qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	What does the International Committee do for newcomers?	The International Committee provides support and assistance to international visitors by hosting social and education events, introducing them to the new area, explaining U.S. government regulations, and answering questions about U.S. culture or anything related to their stay in the country.	"[""International scientific exchange is an important concept in oceanography. These exchanges provide opportunities for interactions between the world's scientists, allow access to data around the globe, and give visibility to literature and theories developed within diverse cultures. Woods Hole Oceanographic Institution fully embraces the philosophy of international scientific exchange, with currently close to 220 foreign nationals at the Institution representing 35 different countries. To amplify our commitment to the international exchange of ideas, WHOI is a designated sponsor of the U.S. Department of State’s Exchange Visitor Program.\nDue to the abundance of foreign nationals within the Institution, WHOI maintains a team of dedicated Foreign National Advisors in Human Resources. One of the team’s primary goals is to ensure departments, employees, guests and students follow the appropriate steps to secure valid authorization to work or conduct research in the United States. Some of the other important roles the Foreign National Advisor plays is to ensure that WHOI stays in compliance with the Exchange Visitor Program and U.S. immigration laws, to be an advocate for the Foreign National during the visa process, and to advise partners within the Institution on visa related matters. When WHOI hires or invites a foreign national to collaborate on a research project, a Foreign National Advisor will be assigned to help facilitate the process.\nWe recognize that most visa-related matters are very specific to an individual’s country of citizenship, and the type and duration of work or research the person is conducting within the United States. While a lot about the visa process is out of WHOI’s control, we do our best to minimize disruption to the scientific research.\nWHOI's International Community\nAccording to the Department of State, the purpose of the Exchange Visitor Program is “to provide foreign nationals with opportunities to participate in educational and cultural programs in the United States and return home to share their experiences, and to encourage Americans to participate in educational and cultural programs in other countries.” WHOI’s International Committee is a great resource to compliment the purpose of the Exchange Visitor Program, and to aid in a foreign national’s assimilation into U.S. culture.\nThe International Committee was formed to provide support and assistance to our international visitors and to help acclimate them to their Institution, our culture and the local area. The Committee provides an array of services such as hosting social and education events, introducing to the new area, explaining U.S. government regulations, and answering generally any question one might have about U.S. culture or anything related to one’s stay in the country.\nWhile you are here, you will learn not only from your colleagues, but also from your interactions with other American people and institutions. We hope you take advantage of the opportunity to learn about a society and culture which may be new to you, and to teach others about the society and culture you represent.\nMass Immunization Requirements for International Visitors\nTo ensure a safe and healthy environment and community, compliance with Massachusetts Immunization Law is a requirement for all incoming international visitors.\nWhile we strive to provide you with the most up-to-date information on immunization requirements, ultimately it is your responsibility to make sure you have the required immunizations.\nFor more information on immunization requirements, please refer to: https://www.mass.gov/service-details/vaccine-information-for-the-public""]"	['<urn:uuid:54d09773-1dd8-40a8-a5d3-6ef1c53354d7>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	8	41	545
2	How do computer systems help track nutrient cycles in water and farming, and what environmental benefits do they provide?	Computer systems help track nutrient cycles through mass spectrometry techniques that detect degradation products in water, allowing scientists to understand chemical processes and sulfur cycling. In farming, programs like Pantheon Farming enable monitoring of nutrient content in soil, manure, and feed, optimizing fertilization while reducing environmental impact. These systems have led to significant environmental benefits, such as reducing fertilization of agricultural land by up to 30% in places like Switzerland, and allowing farmers to optimize spray usage based on weather conditions and pest monitoring.	"['Dr Daisuke Minakata – Sunshine and Organic Molecules in Water\nOrganic molecules dissolved in rivers, lakes, seas and oceans are essential to plant and animal life. Some of these molecules are also degraded and enter a complex cycle of carbon, nitrogen and sulphur containing compounds. Surprisingly, scientists currently have a limited understanding of the fate of these molecules. Dr Daisuke Minakata and his colleagues from Michigan Technological University are involved in an ambitious programme to overcome this critical knowledge gap.\nOrganic Molecules in Water\nOrganic molecules are complex carbon containing structures that are indispensable for the existence of life. ‘Organic material’ is an umbrella term that includes a very wide range of compounds and amino acids – the building blocks of proteins – represent some of the most precious compounds necessary for living organisms. Given the importance of these molecules, it is hardly surprising that NASA is looking for amino acids in the search for evidence of life elsewhere in the universe.\nAmino acids and other organic compounds enter natural bodies of water from multiple sources. Bacteria, algae, plants and animals living in the water itself release significant amounts of organic molecules. In addition, agricultural, human and animal waste products also enter waterways in large quantities, contributing to their amino acid and other organic material content.\nA Poorly Understood Cycle\nThe fate of these compounds present in fresh and saltwater environments is poorly understood. Some organic molecules, including amino acids, are taken up and reused by organisms living in the water, but a significant proportion is broken down through chemical and physical processes and these smaller degradation products enter a complex cycle of nitrogen and sulphur containing molecules. The photochemical process is one important degradation pathway … and this is where things get complicated.\nTheoretically, the energy from sunshine could degrade certain amino acids, making them unavailable for living creatures. However, there is evidence that dissolved organic molecules generate very short-lived reactive intermediates under sunlight irradiation and these reactive intermediates degrade the amino acids – the photosensitising effect of dissolved organic molecules.\nDr Daisuke Minakata from Michigan Technological University was prompted to develop a theory to explain the loss of critical amino acids from water. He assumed that other organic molecules absorb the energy carried by the rays of sunshine and those reactive intermediates transform amino acids into small pieces. According to his theory, such light-activated compounds could act as catalysts, inducing the indirect breakdown of critical amino acids.\nA Need to Simplify Things\nAs Dr Minakata explains, testing this theory was difficult for two reasons. Firstly, dissolved organic molecules form very poorly defined, large complexes that show considerable variability at different locations. Using such complex mixtures in experiments would make the analysis of the results extremely difficult. The other issue was that are more than 20 different amino acids in nature, and these are very different chemically. Finding the degradation products of all of these would be a Herculean task. Thus, to conduct the experiments, the research team needed to simplify things.\nDespite the complexity of the dissolved organic material, scientists had identified the three main mechanisms explaining its catalytic behaviour. Furthermore, they were able to identify smaller single compounds that could mimic this action. Using these mimics in the experiments promised to simplify the analysis of the results.\nThe next decision Dr Minakata and his colleagues made was that they would initially focus their attention on the study of a single amino acid, methionine, that is typically depleted in water under natural conditions. Methionine is one of only five photochemically degradable amino acids and it contains sulfur-atom, a unique element that contributes to the global cycling of sulfur. Sulfur cycling is not as well understood as carbon and nitrogen cycling.\nAn Elaborate Experimental Setup\nNext, the scientists needed to build the appropriate experimental setup. Firstly, the effects of sunlight were replicated using a high-energy xenon arc light source. The water, containing the mixture of methionine and the tested catalytic mimics, needed to be placed under this light in a container that allowed all the wavelength through. However, illuminating a relatively small volume of solution in the specialised quartz containers resulted in an additional problem: the water became very hot. Thus, scientists had no way of establishing which degradation products were produced by light exposure, and which ones resulted from the heat. Understanding the basic science needed a controlled environment in the laboratory as the first step. This meant they needed to build a cooling system that allowed the team to keep the solution at the desired temperature.\nA technique called mass spectrometry was used to detect individual degradation products. After irradiating the samples with the artificial sun, the molecules were exposed to a strong electric field that made them fly along a vacuum tube. Since under these conditions smaller molecules accelerate faster while larger ones move more slowly, measuring the time it takes for a molecule to reach the detector allows its mass to be determined. This allowed the scientists to establish what its chemical structure was likely to be. This may sound complicated – and it certainly was. The group had to resort to the use of some of the most cutting-edge equipment to make these measurements.\nThe Results Start Coming In\nThe initial screening experiments identified common methionine degradation products, and a range of compounds that were produced in smaller quantities during light exposure in the presence of mimics acting as catalysts. Knowing the composition of these degradation products allowed the scientists to work out the possible chemical reactions resulting in their emergence. Once the reaction pathways were identified in this relatively simple experimental system, the scientists started to aim higher. Instead of using chemical mimics acting as catalysts, they started to use a more complex naturally occurring dissolved organic molecule mixture, derived from either the Suwannee River or an Elliot Soil sample, typically used as standard organic molecules.\nWhile these mixtures have the same technical name, humic acid, this is a misleading scientific term, as humic acid represents a very complex mixture of decomposed biological material, that is different at the two sites. Not unexpectedly, the degradation of methionine produced in the presence of the two distinct humic acid samples showed marked differences. Firstly, the light-dependent degradation of methionine was faster, particularly in the presence of the Elliot Soil humic acid. The proportion of produced degradation products also showed considerable differences, both between the two humic acid samples, and compared to the results obtained using simple chemical catalytic mimics. However, the results from simple mimics advance scientific understanding of the roles of different sites in complex mixtures of dissolved organic compounds.\nComputational Chemistry to the Rescue\nAs Dr Minakata tells us, testing the mechanisms causing the degradation of the amino acid caused by such complex chemical mixtures is impossible using conventional experimental approaches. In the analysis of such complex reactions, scientists are increasingly relying on the use of computational chemistry.\nBy the start of the 21st century, scientists had accumulated enough experimental information to enable them to model chemical reactions using computers. By modelling details such as the shape of molecules, their known interactions, the energy needed to introduce changes, or the amount of energy being released during chemical reactions, scientists can predict chemical interactions, including those that have not been studied under experimental conditions.\nAfter entering their experimental data into computer models, Dr Minikata and his colleagues were able to take advantage of this approach to fully understand the fate of methionine in natural water. To a superficial observer, this may sound like a lot of effort to study the fate of a single amino acid. However, deciphering what happens to this vital building block provides is the first step in understanding chemical processes in nature that allow life to flourish in natural bodies of water. Unquestionably, such an understanding is fundamentally important to humanity.\nLISTEN TO THE AUDIO\nMEET THE RESEARCHER\nDr Daisuke Minakata\nDepartment of Civil, Environmental and Geospatial Engineering\nMichigan Technological University\nDr Daisuke Minakata obtained his BSc degree at the Environmental Engineering Department of Kyoto University in 2002. In 2005, he completed his MSc degree at the same institution. After moving to the USA, Dr Minakata studied at the Department of Environmental Engineering of the Georgia Institute of Technology, gaining his PhD in 2010. After graduation, he started working as a Research Engineer at the Brook Byers Institute for Sustainable Systems at the Georgia Institute of Technology. In 2013, he was appointed as an Assistant Professor in the Department of Civil and Environmental Engineering at the Michigan Technological University, where he became an Associate Professor in 2019. He also spent a year as a visiting scholar in Europe, at the Swiss Federal Institute of Aquatic Science and Technology in Dübendorf. Over his career to date, Dr Minakata has mentored over 30 graduate students, published more than 50 peer-reviewed papers, and given more than 25 invited and keynote talks and 70 conference presentations. Dr Minakata has been involved in projects exceeding $7M of which his own fraction is greater than $1.3M. His contributions to environmental science have been recognised with several prestigious awards.\nNational Science Foundation\nMichigan Space Grant Consortium\nB Barrios, D Minakata, Aqueous-phase single-electron transfer calculations for carbonate radicals using the validated Marcus theory, Environmental Science & Technology Letters, 2023, 10, 204–209. DOI: https://doi.org/10.1021/acs.estlett.2c00913\nB Barrios, B Mohrhardt, P Doskey, D Minakata, Mechanistic insight into the reactivities of aqueous-phase singlet oxygen with organic compounds, Environmental Science & Technology, 2021, 55(12), 8054–8067. DOI: https://doi.org/10.1021/acs.est.1c01712\nD Kamath, S Mezyk, D Minakata, Elucidating the elementary reaction pathways and kinetics of hydroxyl radical-induced acetone degradation in aqueous phase advanced oxidation processes, Environmental Science & Technology, 2018, 52(14), 7763–7774. DOI: https://doi.org/10.1021/acs.est.8b00582\nD Minakata, D Kamath, S Maetzold, Mechanistic insight into the reactivity of chlorine-derived radicals in the aqueous-phase UV/chlorine advanced oxidation process: Quantum mechanical calculations, Environmental Science & Technology, 2017, 51(12), 6918–6926. DOI: https://doi.org/10.1021/acs.est.7b00507\nD Minakata, SP Mezyk, JW Jones, et al., Development of linear free energy relationships for aqueous phase radical-involved chemical reactions, Environmental Science & Technology, 2014, 48, 13925–13932. DOI: https://doi.org/10.1021/es504491z\nD Minakata, W Song, J Crittenden, Reactivity of aqueous phase hydroxyl radical with halogenated carboxylate anions: Experimental and theoretical studies, Environmental Science & Technology, 2011, 45, 6057–6065. DOI: https://pubs.acs.org/doi/10.1021/es200978f\nD Minakata, J Crittenden, Linear Free Energy Relationships between the Aqueous Phase Hydroxyl Radical (HO•) Reaction Rate Constants and the Free Energy of Activation, Environmental Science & Technology, 2011, 45, 3479–3486. DOI: https://doi.org/10.1021/es1020313\nD Minakata, K Li, P Westerhoff, Crittenden, Development of a group contribution method to predict aqueous phase hydroxyl radical (HO•) reaction rate constants, Environmental Science & Technology, 2009, 43, 6220–6227. DOI: https://doi.org/10.1021/es900956c\nREPUBLISH OUR ARTICLES\nWe encourage all formats of sharing and republishing of our articles. Whether you want to host on your website, publication or blog, we welcome this. Find out more\nCreative Commons Licence (CC BY 4.0)\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nWhat does this mean?\nShare: You can copy and redistribute the material in any medium or format\nAdapt: You can change, and build upon the material for any purpose, even commercially.\nCredit: You must give appropriate credit, provide a link to the license, and indicate if changes were made.\nMORE ARTICLES YOU MAY LIKE\nDr Jean Decety | Exploring the Dark Side of Morality\nSocial and moral values can inspire change and positive action, yet they can also prompt divisions in society, conflicts and violence. Dr Jean Decety, a Professor at the University of Chicago, recently started investigating the mental and neural mechanisms associated with strong moral convictions and the support for violent acts. His work examines the dark side of morality, pinpointing the brain processes underlying moralisation and its adverse consequences.\nDr Michael Gipp | Unveiling the Past: Using Statistical Computation to Characterise Ancient Climates\nThroughout the earth’s history, the prevailing climate has changed over and over. To gain insights into the history of our planet, scientists use clues uncovered from the earth’s surface. However, not all periods in the earth’s history are equally represented in the geological record. Dr Michael Gipp is using the surprising similarities between computer algorithms and complex natural systems to describe our ancient climates.\nPsychologist Dr David Sperbeck | The Halstead Category Test: Assessing the Impact of Prenatal Alcohol Exposure\nChildren who have been exposed to alcohol while still in the womb often show impairment in executive function – the cognitive domain underpinning diverse skills including attention, memory, learning and self-control. Psychologist Dr David Sperbeck, PhD, has investigated whether a commonly used neuropsychological assessment – the Halstead Category Test – can effectively measure cognitive impairment in this vulnerable group of children. His findings confirm the utility of the measure and also offer important hope for children affected by alcohol prenatally.\nDr Cristina Oliva and Professor Giampiero Favato | Unravelling How COVID-19 Mathematics Impact Behaviour Change\nMathematical models for predicting the spread of COVID-19 directly influenced public health measures around the world, significantly impacting everyone’s day-to-day activities. At Kingston University, Dr Cristina Oliva and Professor Giampiero Favato are leading the way in COVID-19 research, looking at how complex statistical data is communicated to the general public. Their valuable work is helping drive changes in behaviour that could reduce the spread of COVID-19.', 'The Slovene company Datalab has developed a modular IT farm optimization tool that allows the farmer to have all the necessary information always on what is going on in fields, in the stable, or in the orchard.\n""Our key contribution to the circular economy in agriculture is to bring the information about all the processes within the farm into circulation, thus enabling the optimization of operations while at the same time significantly reducing the negative impacts on the environment."" Said Andrej Mertelj, CEO of Datalab.\nComputerization and optimization of farms is possible and yields positive results in all segments: from optimization of spraying, drawing up a feeding plan, management of vineyards and cellars, optimization of utilization of working machines on large farms, design of crop rotation plans, etc. Pantheon Farming supports all these processes and more: the principles of a circular economy can be introduced with materials (eg nitrate cycle: cow-dung-field-food) as well as in processes, by creating information retrieval loops that enable the complete capture of data.\nA typical example is spraying, where in addition to the very needs of plants, the meteorology and quantity of pests are connected to the information loop, which enables the farmer, the fruit farmer or the wine grower to use the spray only when it is necessary. With such an approach, each farm, while optimizing its business, greatly reduces negative environmental impacts. The environmental aspect is one of the more important for the European Commission and is included in the package of measures for the circular economy, which was presented in December last year.\nIt is also very important that the farmer discerns whether the hectare yield under optimal conditions is really such that the cultivation of a particular plant on a given land under given conditions is economically justified.\nThe Pantheon Farming program promotes the introduction of the principles of circular economy, as it solves a whole range of problems that arise in the production of food and greatly burdens the environment.\n1. It promotes the efficient use and exchange of raw materials. Based on the data collected in the support system, a farmer calculates, in advance, how much feed or dung, and at what time intervals he needs it, how much he / she will produce and how much he / she can sell. Another farmer knows when to additionally buy feed or dung.\n2. Analyzing the phosphorus content in dung and feedstock growth increases the quality of the product, and thus the price of both. Not so long ago, this was not possible because there was no appropriate technology - especially sensors and traceability systems - and a system for collecting and processing all key data.\n3. The analysis of the current quantities of nutrients in the soil with the help of a mathematical model predicts how much fertilization and at what time intervals in concrete weather conditions is necessary for the plant to be optimally fertilized.\n4. The orchard module based on photographs of insect traps analyzes and determines whether insects are injurious insects and in which developmental phase they are. These results correlate with the weather forecast and warn when the crop protection in the form of spraying is needed and when not. In this way, the number of needed sprays can be reduced by half.\n5. A solution containing feeding plans in conjunction with feeding robots allows accurate monitoring of how feeding with a particular feed is reflected on the weight of the animal and whether ""only"" quantities of manure grow, which are, for example, also too rich in phosphates.\nIn Switzerland, the authorities implement a good policy example. Less than a farmer brings unnecessary phosphates into the field, the higher is the subsidy for agricultural land. Farmers decide therefore whether a greater quantity of crops resulting from more intensive fertilization really outweighs the deduction they receive at the expense of the land tax. In this way, in Switzerland, the fertilization of agricultural land has also been reduced by up to 30%, since the fertilization according to the principle ""better to be surely sufficiently fertilized"" is well known to have impact in the farmer\'s wallet.']"	['<urn:uuid:62ffea9b-68e2-4786-84ac-c4837da0cb09>', '<urn:uuid:ac6b1b8e-4480-4844-a6fa-cd494a3be673>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	19	84	2897
3	small farms weather protection investment benefits	Small farms, which produce over half the world's food and support 2.5 billion people, can be protected from weather risks while increasing productivity through two key approaches. First, climate-resilient farming techniques, such as stress-tolerant rice varieties developed by IRRI, ensure good harvests even during adverse weather conditions. Second, index insurance programs provide a safety net that encourages farmers to invest in better seeds and fertilizers, leading to higher income in normal years while protecting their investments during bad weather. This combined approach helps farmers both survive bad years and become more productive in good years.	['Earth Month Special\nWith rising temperatures, fiercer typhoons, severe droughts, devastating floods and landslides floods damaging farmlands, appropriate and innovative farming techniques as well as finance are needed for millions of farmers in the developing countries to build their resilience to climate change.\nClimate change is transforming the lives of farmers. Adaptation for them is an increasing daily challenge.\nIn the mountainous region of the Northern Philippines, about 340 kilometers away from Manila, farmers in the World Heritage Site, Ifugao Rice Terraces, farmers are facing evolving problems such as lack of water supply, insufficient rice yield, more lucrative jobs in the cities and pressure from rural development in the midst of the changing climate.\nJose Banggal has grown rice in his almost one hectare piece of land for the past 50 or so years near Barangay Bocos in Banaue, Ifugao. Growing rice has been an integral part of his life and that of his neighbors in the community for decades now. He lamented that things have changed for the farmers in the past few years as crop yields have been decreasing.\n“ Rice farming is no longer profitable and practical nowadays. You see brown patches everywhere in the field as there is inadequate water supply, poor irrigation and we cannot predict the weather anymore. That’s why most of the people here especially the youth prefers most tourist-related jobs such as weaving and wood carving,” Banggal said.\nLike Banggal most farmers across the Philippines are in a dilemma on how to revive the glory of good harvest. They complain of the dramatic consequences of climate change in their day-to-day lives.\nA grim reality\nClimate experts warned that extreme weather patterns pose a threat to the world’s already tough food security challenges, declining incomes for poor farmers, spike in food prices coupled with a growing population.\nIn the Fifth Assessment Report (AR5) from the Intergovernmental Panel on Climate Change (IPCC), it summarizes that the effects of climate change on crops and food production are already evident in several regions of the world, saying that with average temperature increases of 3 to 4 degrees Celsius we will see large negative impacts on farm yields and severe risks to food security, especially in tropic areas.\n“ Major future rural impacts are expected in the near-term and beyond through impacts on water availability and supply, food security, and agricultural incomes, including shifts in the production areas of food and non-food crops across the world,” climate scientist Lourdes Tibig said, citing the AR5 report of the IPCC.\nTibig, also the lead author of the IPCCC AR5 Working Group II said that in the latest IPCC report, climate change impacts are projected to slow down economic growth, make poverty reduction more difficult, further erode food security, and prolong existing and create new poverty traps, the latter particularly in urban areas and emerging hotspots of hunger throughout the 21st century.\n“ In terms of adaptation in agriculture, the use of diversified cropping, use of seasonal forecasts for decision-making at farm levels such as climate-resilience field schools for farmers are vital,” Tibig explained.\nDwindling water supplies\nIn 2050, there will be enough water to help produce the food needed to feed a global population expected to top 9 billion, but overconsumption, degradation and the impact of climate change will reduce water supplies in many regions, according to a report released last week by the Food and Agriculture Organization (FAO) and the World Water Council (WWC).\nUp to 100 percent more food in developing countries will be needed by 2050 to feed the world while agriculture will continue to be the largest user of water globally, said the report which was recently released at the 7th World Water Forum in South Korea. And even with increasing urbanization, much of the global population and most of the poor will continue to earn their living in agriculture. Yet the sector will see the volume of water available to it reduced due to a competing demand from cities and industry.\n“ Water, as an irreplaceable element of achieving this end, is already under pressure by increasing demands from other uses, exacerbated by weak governance, inadequate capacities and under-investment,” in a statement said by FAO deputy director-general natural resources, Maria Helena Semedo, adding that there is a need to revisit public policies, investment frameworks, governance structures and institutions.\nThe report further said that through technology and management practice, farmers, especially smallholders, will need to find ways to increase their output on the limited land and water available. Currently, water scarcity affects more than 40 percent of people across the world due to overconsumption of water for food production and agriculture.\nThe study recommended that policies and investments to enhance adaptation at the watershed and household levels, such as improved water storage facilities, wastewater capture and reuse, as well as research that generates more resilient agricultural production systems for smallholders.\nA climate-resilient agriculture\nFarmers, especially in developing countries like the Philippines, are particularly vulnerable to climate change given their direct reliance on weather-dependent natural resources.\nGreenpeace Food and Agriculture Campaigner Wilhelmina Pelegrina said farmers need assistance to face these challenges in growing our food, as well as support from the government and non-government organizations alike is crucial in bringing-in practical and advance farming systems, ecological farming technologies that work with diversity.\n“ While we agree in the thinking to provide food aid, seeds and organic fertilizers, we strongly urge the Philippine government to step in and set mechanisms for a more-climate resilient ecological farming system,” Pelegrina said. “\nAccording to Pelegrina, many farmers have since abandoned their rice fields leaving un-harvested rice grains which failed to develop due to the dry spell. She cited the latest figure from the Provincial Reduction Management Council of North Cotabato which estimates that the damage caused by severe drought to the town of M’lang in North Cotabato to rice crops is around P42 million. The problem already impacted an estimated 503 hectares of rice land, affecting livelihoods of 630 farmers.\nAcknowledging the problem many farmers face nowadays, the Philippine Rice Research Institute in Science City of Munoz, Nueva Ecija, said more than 4,000 Filipino farmers were trained and many others will continuously to train with the latest technologies on rice farming through its rice agritourism under its FutureRice program.\nUnder the program, farmers learned about the technologies used such as the nutrient diagnostics tools, rice crop management app, machines and monitoring systems in the field. The farm also showcases hybrid rice varieties such as Mestiso 19 and 20, rarely seen collection of traditional varieties, aromatic rice, submerged tolerant rice varieties, and other rice varieties.\n“ Our objective is to prepare the Filipino farmers and extension workers for the future rice farming scenarios and train them on clean, green, practical and smart rice farming,” said Roger Barroga, PhilRice’s FutureRice program leader.\nEarlier this month, officials of the Bill and Melinda Gates Foundation toured the International Rice Research Institute (IRRI) where scientists presented key advances on rice development, including the latest on climate change-ready rice and healthier varieties that aim to help solve micronutrient deficiencies, which afflict about two billion people globally.\nThe climate change-ready rice varieties that have reached millions of farmers in Asia were developed by IRRI and its partners under the BMGF-funded project named Stress-Tolerant Rice for Africa and South Asia.\nThese stress-tolerant varieties, according to IRRI, hold the promise of improving the livelihoods of some of the world’s poorest farmers who till lands and most vulnerable to climate change by ensuring a good harvest, for instance, even after over 14 days of flooding. Non-tolerant rice varieties would normally die after four days of submergence.\nEven the Department of Agriculture and the IRRI are working hand-in-hand in helping the poorest farmers for better varieties and technologies in the face of climate change.\n“ Farmers in the Philippines are facing greater challenges with a continuously growing population and ever-scarer land and natural resources. Climate change will only exacerbate these challenges,” IRRI Deputy Director General Bruce Tolentino said in a statement. “ “Improved agricultural technology developed through advanced scientific research is necessary to overcome these challenges in the long term.”\nPhilippine extension professionals will benefit from training and the use of tools that allow faster information transfer using information and communication technology under the IRRI. In addition, policies formulated through accurate and ground-validated information, as well as knowledge of policies that have spurred growth of rice sectors in neighboring countries will help the country achieve food and nutrition security, IRRI stated.\nFor the meantime, millions of farmers in the Philippines like Banggal will have to struggle to adapt to the effects of climate change while they wait for more technical support, adequate awareness on climate-smart agriculture and sustainable agricultural practices and infrastructure from the government and from various sectors.\nStory and photos by Imelda V. Abano\nphoto credit on Lakbay-Palay photo: PhilRice\n--Asian Developmental Journalist of the Year, 2009\n-- UN Framework Convention on Climate Change (UNFCCC) media fellow 2008-2014\n-- Council of Leaders of the US-based Earth Journalism Network;\n--Board of Director of the US-based Society of Environmental Journalists\nLatest posts by Imelda Abano (see all)\n- In race against time: Why the 1.5C temperature goal matters to climate-vulnerable countries - August 17, 2016\n- Survival of poor, vulnerable countries hinges on low temperature target - August 15, 2016\n- The Paris Agreement on climate change at a glance - July 20, 2016', 'For Farmers, Insurance is an Investment in the Future\nSmallholder farming is big business. Two and a half billion people rely on small farms for their income and they produce more than half the world’s food, assisted by a vast supporting cast of input companies, banks, processing plants, markets, NGOs and research centers.\nHowever, smallholder farming is also extremely vulnerable to the whims of the weather. Most farms are based in developing countries without access to irrigation, so a “bad year” due to a drought or a flood can be catastrophic. In many places, the impact from climate change—such as higher temperatures, stronger storms and changes in rainfall patterns—threaten to make the situation even harder to predict. Farmers around the globe must manage these risks to survive, so a commonly used strategy is to work to become even more productive in the good years, allowing them to offset their losses in the bad.\nOne way to achieve that productivity is to invest in new inputs or technology, but the risk of doing that can often leave farmers even more vulnerable to the weather. For example, a farmer might increase her yields in a normal year by taking out a loan to purchase high-quality seeds and fertilizer. But if a severe drought strikes, there is the chance she will lose everything and go into debt. Furthermore, if the banks believe that the farmer is high-risk, they may not be willing to give her a loan in the first place. This means that, unfortunately, threats like drought—the very reason for adopting these climate-smart practices—also represent a huge risk that make farmers reluctant to invest.\nInsurance is one method to reduce this risk. If the safety of insurance coverage can increase a bank’s willingness to make loans or help farmers feel comfortable making additional investments, then farmers can take advantage of productive opportunities that bring them higher income in most years, knowing that their investment is protected. In other words, insurance can build resilience not only by providing a safety net to help farmers survive and protect their assets, but it can also unlock opportunities to increase productivity in the normal years.\nYet most farmers around the world remain uninsured. This is in part because traditional indemnity insurance, where a payout is based on measured damage, is difficult to apply to the situation. It is extremely expensive to visit every smallholder farmer to assess losses, and it’s incredibly difficult to differentiate between bad management and true losses from something such as drought. Index insurance was created as an alternative approach. The concept is simple: If there’s a big, obvious risk such as a drought that stops farmers from being more productive, then why not instead insure the drought?\nIndex insurance works through allowing farmers to purchase coverage based on some index that’s closely linked to their losses, instead of on directly measured damage. Indices might be based on an aspect of weather that influences yield—such as the wind speed or the total rainfall during a certain window of time—or they could be based on other factors, such as the average yield losses over an entire district.\nA payout is triggered when an index falls above or below a pre-specified threshold. A well-designed index should be closely linked to the loss it’s there to insure, plus it needs to be externally measurable, tamper-proof and easy to understand by the customer. If that is the case, index insurance has the potential to unlock a massive market of smallholder farmers and to increase uptake in new technologies and inputs.\nInsurance Acceptance on the Rise\nIn recent years, the number of farmers who have purchased index insurance contracts has spiraled. In India, more than 35 million farmers are part of a government run index insurance program. Since 2009, more than 800,000 farmers have chosen to purchase insurance across East Africa through the company ACRE (previously Kilimo Salama). In 2015, more than 40,000 of the poorest smallholder farmers voluntarily chose to purchase insurance through the R4 Rural Resilience Program (World Food Program and Oxfam America). The trend is set to continue, with farmers able to purchase premiums in at least 20 countries around the world.\nMany analysts often discuss the “low demand” of index insurance programs, so it’s worth noting that from a commercial perspective, uptake is often capped by logistics rather than farmer demand. Also, the uptake of many “disappointing” projects often exceeds that of comparable products in the U.S. or Europe.\nIndex insurance is no panacea; it isn’t going to singlehandedly solve poverty or offer farmers protection against every risk they face. Very few insurance pilots scale to reach meaningful numbers of farmers, and there are many challenges they have had to overcome in order to do this, including:\nFinding the right situation—Index insurance works best in locations where there is one single overriding risk that stops farmers from being productive, such as drought in Senegal. It’s less useful in places that have more complex risks; for example, it would be less helpful in a region where farmers face risks from flood, drought and disease. Insurance also works well in situations where it can unlock a new option (such as allow access to credit), or to protect an existing asset (such as livestock), as the extra money made by the farmer in good years is then enough to pay the premium. It must also fit in with farmers’ existing risk management strategies in order to be an attractive product. Finding the right opportunities for insurance to thrive takes time and close work with local partners, agricultural experts and farmers themselves.\nBuilding capacity and including farmers in the design process—Everyone prefers a custom-designed product to one that’s off-the-shelf, so it’s unsurprising that index insurance is more successful in situations where farmers can be involved in the design process. Insurance is often a very new idea for farmers, so being included in the design stage allows them time to better understand the product. Farmer-led design has been shown to significantly increase sales, support development and unlock creative new ideas. For example, in Ethiopia, it was a farmer who suggested that people who didn’t have the cash to purchase insurance could pay for it though working on a “food-for-work” program, unlocking an entirely new market.\nReducing basis risk and improving index design—Working out exactly which aspect of the weather most closely relates to a bad year is complicated, and even the most simple of indices will be backed up by a lot of sophisticated analysis. It’s also impossible to perfectly correlate this index with the actual damage on every farm. The difference between an individual farmer’s loss and the payouts from the insurance contract is called basis risk. There is much research on methods to reduce this, from improving our knowledge of spatial statistics to making use of new satellite-based weather products, but this research must be customized for each new project. Communicating basis risk to the customer is equally important; if a farmer knows when he is (or isn’t) covered through a transparent index, it’s easier to trust it and to fit insurance into farm management plans.\nSetting up networks—Index insurance is a unique new area in that a large project will require meaningful collaboration among farmers, local insurers, global re-insurers, banks, input companies, agricultural experts, meteorologists, remote sensing experts, social scientists, economists, NGOs and business experts. It requires time and patience to build up this network and for everyone to learn how to speak the same language.\nFinally, one of the most important lessons stemming from index insurance is that no matter how simple the theory, reality is always complicated and nuanced. Index insurance programs can fill multiple roles with many ideas of success, so simple narratives often miss the subtleties of a situation or the full story behind it. There are many examples where index insurance ended up playing a different role than intended, and programs often take several years to fully develop.\nSo as with any business, perhaps the magic ingredients in index insurance are good communication, patience and an adaptable business plan.']	['<urn:uuid:6e113035-f597-45b7-9505-95a2a89ccc85>', '<urn:uuid:47868a76-0157-4b7b-bb9e-4b6343c1bbd6>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	6	95	2922
4	How do security systems handle device checks and false alerts?	Security systems perform granular scanning of inbound devices, checking for rooted devices, service packs, and latest antivirus software. They can also remotely locate or wipe lost devices. Regarding false alerts, these tools have the capability to deal with false positives, allowing organizations to adjust vulnerability severity levels and customize what alerts are relevant to their specific needs.	"[""To deal with current trend of information security and sophisticated cyber threat we need the most efficient and best suited vulnerability management solution for our infrastructure as well as applications. As vulnerability management deal with people, process and technology; we need to choose each of them carefully. Technology is the pillar which is very vast and we cannot opt for multiple investment on the same. We need to be much cautious while choosing the same. One can take into account following parameters while choosing a vulnerability management solution:\n- Capability in dealing with Asset Inventory: Does the solution provide an asset inventory database? Is it feasible to extend the database schema to support additional fields, such as asset classification? If not, can the technology integrate with other asset management solution/repositories?\n- Coverage capability for multiple environment: Capability on handling multiple Operating system. What’s the breadth and platform coverage of the technology? Many technologies can perform operations against the Windows family of products, but you’ll need technologies that can operate in a heterogeneous environment and can support a variety of platforms, applications, and infrastructure devices.\n- Support for cloud and mobile approach: Does the organization need a vulnerability management tool that scans cloud services, such as software as a service or infrastructure as a service? One need to think of far sighted approach as well.\n- Scalability: What is the scale of scope to be covered in vulnerability management and whether the tool is capable enough to handle the count of scope? Clarity on capability of tool to handle multiple infrastructure devices, applications etc.\n- Ease of Operation: A tool that is incommodious to navigate or presents confusing dashboard information won't be used, at least not to its fullest potential. A vulnerability management tool that requires regular maintenance also becomes a problem for staff that's often already overburdened.\n- Dealing with false positives & severity: Most of the automated tool flag false positives as some vulnerabilities might not be relevant to organizations or one need to edit the severity of vulnerabilities as well. Does the tool possess capability to deal with false positives and severity customization?\n- Integration capability: What is the feasibility of the tool integration into existing patch management, configuration management, intrusion detection, and/or monitoring tools and services?\n- Capability of tool to run non-intrusively: While scanning production infrastructure it is a must to have passive or non-intrusive approach of scan. Whether the tool has capability of safe scan?\n- Workflow & ticketing system: Does the product have a workflow system that allows to assign and track issues? Can it auto-assign tickets based on rule sets defined (i.e., vulnerability, owner, asset classification, etc.)? These are the must have capability for a vulnerability management solution\n- Vulnerability research & Update capability: One need to check; how frequently do the vendor release updates? Does the distribution mechanism leverage industry-recognized security communications protocols? Does the vendor have its own vulnerability research team? How has the vendor responded to vulnerabilities in its own products?\n- Dealing with Zero day vulnerability: Does the tool possess capability to deal with Zero day vulnerability? Do the product possess Predicative analysis of the threat in your environment without the need to perform new scanning?\n- Reporting: Is the reporting detailed and customizable? Can we generate trend report? What are the report types? Are the output format of report reusable on other tools?\n- Remediation Policy enforcement: Does the product provide the capability to designate the selected remediation at varying enforcement levels, from mandatory (required) to forbidden (acceptable risk), via a centralized policy-driven interface?\n- Technical Support: Look for vendors that offer 24/7 support, preferably by phone, and find out if customers can expect an immediate response.\n- Pricing and licensing: Many tools provide different category of licensing. One need to map the requirement in a cost effective manner.\nConsidering above 15 factors will definitely help to choose the best VM solution specific to the Organization.\nAuthored by Sameer Nanda\nRate this article:"", 'We live in a world full of technological buzz terms – cloud computing, software-defined technologies, and now, next-generation security. The challenge with understanding new types of technologies is that the marketing machine usually takes charge before there has been any serious explanation.\nThe term “next-generation” security was born as a direct result of new types of technologies requiring greater levels of security flexibility. What does that mean? Communications happening over the cloud, or over a bring-your-own device, or even through a virtual portal all have new types of requirements that older security platforms just could not meet. Furthermore, at the core of all of these new technologies sits the data center. With so much more reliance around the modern data center infrastructure and everything that it hosts, security platforms had to make that next-level jump.\nNext-generation security technologies are much more than your standard firewall. These are intelligent devices which are application, cloud and user aware. These are special services, new policies and complete virtual appliances logically located throughout an environment.\nSo, what are these new types of security products? Let’s take a look at a few:\n- Security Beyond the Physical: We’ve come far beyond the standard physical firewall. Now, security appliances are being deployed at various nodes within a network – internal, external, at a cloud site, or in a DMZ. Some of these appliances can be physical, while others are completely virtual. The flexibility of virtual security appliances means more control over networks, traffic flow and even policy creation. Furthermore, these appliances can be logically located inside of a network running special policies or at the edge protecting cloud-facing applications. New physical content delivery appliances allow you even virtualize some security services directly on top of the platform. For example, the NetScaler SDX allows for a virtual WebSense service to run directly on to. This allows for things like DLP and even greater application awareness.\n- New Types of Policy Engines. The world of cloud computing requires new types of security engines. Layer 4-7 DDoS protection (volumetric and application-layer), intrusion prevention/detection services (IPS/IDS), and data-loss prevention (DLP) are just a few examples of some advanced protection features. These new engines must scan multiple points within and outside of a network. Furthermore, organizations with heavy regularity compliance measures have to be even more careful with their data. Some healthcare organizations use DLP technologies which scan data leaving and coming in. From there, they scan for patterns, ‘xxx-xx-xxx’ for example, to flag, stop and report malicious data leakages. Next-generation security platforms are designed to help stop data loss by integrating into various technologies – including software-defined networks. These policy engines allow for granular data-flow control as core information flows between the end-user, your data center, and the cloud.\n- Cloud-Ready Endpoint Control. As new devices try to connect into a corporate network, there has to be some means of control. Now, border security devices are being deployed with advanced interrogation engines capable of granularly scanning all inbound devices. Organizations can place certain policy metrics and present only certain content if those policies aren’t met. Checking for rooted devices, the right service pack, or even the latest A/V can all be set as interrogation points. Further control can be derived from the use of mobile/enterprise device management (MDM/EDM) solutions. Having the capability to remotely locate or wipe a stolen or lost device can be very handy. Remember, trends around IT consumerization and mobility are only going to continue growing. This means more users will be utilizing the device that helps them be most productive. It’ll be up to your data center’s next-generation security model to help delivery those resources and keep them secure.\n- Software-Defined Security. Now that security devices are being distributed to multiple points, new types communications methods are being established to create a faster and more secure cloud environment. Closely in conjunction with software-defined networks (SDN) creating site-to-site secure connections is now a must. Many organizations are utilizing a public or hybrid cloud platform which may require a virtual security appliance to be deployed at the provider site. From there a physical or virtual appliance at the corporate site can be used to create a secure, monitored, tunnel into the cloud. Remember, next-generation security platforms are not only cloud and application aware, they provide layer 4-7 networking services and data protection. The idea is to create app-awareness, increase control and create flexibility around your environment to help facilitate an ever-evolving business model.\nThere are going to be lots of different definitions out there for next-generation security. It’s important to understand, however, the core meaning of the technology. Security products have simply evolved beyond the standard firewall platform into something that is capable of supporting numerous different types of services. In many cases these services all work together to bring forward a singular platform – cloud computing, for example. Next-generation technologies will always heavily revolve around security, agility, and the ability to evolve (quickly) to the needs of a growing business. As more distributed technologies take form in the industry, there will be a greater need for dynamic –cloud-aware – security solutions.']"	['<urn:uuid:28490278-90a5-4c3f-9d79-50281f56d716>', '<urn:uuid:3e425a4a-3c81-43e9-89e6-74750c106817>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	10	57	1519
5	world war 2 planes underwater wrecks majuro diving locations	There are several WWII aircraft wrecks around Majuro: a torpedo plane and Grumman Avenger at 115-120 feet near Bokolap Island, a Grumman Duck that crashed near the WWII carrier re-supply airfield, an F6F Hellcat fighter at 115 feet of water that was pushed overboard from an aircraft carrier in 1944, and a B-24 Liberator bomber in 12 feet of water between Laura & Rong-Rong that was damaged during a bombing run and later scuttled by Japanese soldiers.	"['|Home||Photo Gallery||Sportfishing||Diving||Travel Tips||Links||Comments|\n|Diving||Majuro Diving||Arno Diving|\n|Dive Sites around Majuro|\nYou can spend years on Majuro and still find dive sites that you haven\'t\nhad a chance to visit. The lagoon is loaded with variety from coral\npinnacles to deep walls to wrecks -- there is something for everyone.\n|A few minutes from the Yokwe Divers dock lies The Bridge, a favorite for many divers in Majuro. Plate coral splattered with clusters of Black Coral and Tridacna clams embedded in the coral base, drops down a steep wall that exceeds 130 feet. Whitetip and Silvertip sharks cruise the area and Napoleon wrasse hover in the shallow areas. Expect to see schools of Red Snapper and brightly colored Angel Fish as well as Helfrecht\'s Dartfish at this popular dive site.|\n|Aneko Island has both shallow and deep water coral heads that are incredibly large. From 12-90 feet, you will find anemone, cleaner shrimp, resting turtles and a deep water coral garden.|\nKalalen Pass is a favorite location\nfor drift dives. Diving depths run from 30-130 feet at the Pass\nand steep coral walls drop into the crystal clear water.\nPelagic species cruise the currents in search of food and you\ncan expect to see sea turtles, rays, several species of sharks\nand sea turtles on a typical day. Silvertip sharks over 8 feet\nin length have been seen at this location. Kalalen Island,\nlocated adjacent to the Pass, has a lagoon side reef that boasts\nboth hard and soft corals and divers exploring the coral will be\nthrilled as they discover butterfly fish, triggerfish and the\nelusive octopus hiding in the coral gardens. Further east,\nin areas called Second Island, a gradual slope down the\ncoral head drops to a sandy bottom at around 120 feet.\nWhitetip sharks share this area with shrimp gobys and Grass eels\nare commonly sighted.\nThe North Shore outer reef on the ocean side of Kalalen Island is a pristine gradual slope populated by thousands of table corals, anemone, and tens of thousands of tropical reef fish. Schools of fusiliers rain down from the surface as you glide toward the transition from slope to near vertical wall. Sharks, rays, dogtooth tuna, and turtles are also seen here regularly.\nEven further east, Fourth Island offers a popular site for easy second dives as well as for beginning divers because of the extremely calm conditions usually found in this area. Bring your camera because in addition to schools of Kiribati Red Snapper and thousands of tropical reef fish, you can see 3 different species of anemone and anemone fish including the Marshallese Three Striped Clownfish.\nIncoming tides at The Aquarium may offer one of the most exciting dives of your trip. Ranging from 60-130+ feet on the outer reef wall of Kalalen Channel, this natural ""horse shoe"" shaped feature creates an area where tidal flow is compressed, concentrating the flow of rich, open ocean sea water as it enters Majuro Lagoon. This is the place to see Horse Eye Jacks, Black & White and Red snapper, Barracuda, and all manner of reef fish numbering in the MANY tens of thousands. On the sandy ocean floor, you will see sleeping reef sharks and Sting-rays, Gray reef, White-tip, and Black-tip sharks. Schools of Rainbow Runner, Napoleon Wrasse, and huge schools of fusiliers are also common here.\n|Bokolap Island offers an exciting experience on a dive site ranging from 12-120 feet in depth. Beautiful coral heads, 4 species of anemone, clownfish, Harlequin shrimp, 3 species of lionfish, colorful nudibranchs and more fish than you can imagine are here for the viewing. A WWII U.S. torpedo plane sits at 115 feet at this location. A Grumman Avenger is also located in this area at 120 ft. depth. Downed by anti aircraft fire, this Avenger crash-landed on the ocean-side of Bokolap Island, washed over the reef, and sunk inside the lagoon where it rests today. The tail section lies up the rubble slope and is home to a family of three striped Marshallese clown-fish.|\n|If you are looking for extreme visibility, in excess of 140 feet, you will hope for the mild weather conditions that will allow you to dive The Riviera. This northern reef location runs across nearly two miles of untouched coral reef. A drift along the reef will reveal sharks sleeping on the bottom - within your visibility but over the recreational diving limits. This area drops to over 130 feet and is populated by schools of huge red snapper, Mantas and Spotted Eagle Rays and coral reef in every color imaginable.|\n|Just northwest of the Uliga Wall is another dive that is accessible only in the best weather conditions. Known as Shark Street, this is a deep reef on the northeast outer reef of Majuro Lagoon. Divers have reported sightings of 25 or more sharks on a single dive. Thick forests of black coral and schools of Napolean Wrasse make this a thrill for anyone.|\n|Shore Dives are popular with local divers and good shore diving sites can be found on the southern reef wall a few miles west of the Yokwe Divers shop. Weather conditions and local knowledge are important when attempting shore diving which may require entry through breaking waves and a possibility of strong currents. The south shore wall ranges from 20 to over 130 feet in depth and you will be rewarded with schools of Dogtooth Tuna and Grey Reef Sharks. Gorgonian Sea Fans ride the current on the vertical walls and drop-offs of these area - commonly referred to as Mile 14, Mile 15 and Mile 17.|\n|Wreck Diving gives you the best of everything. Explorers enjoy the mystery of the wrecks and the incredible variety of marine life that can be found in every nook and cranny of a sunken structure. Photographers are thrilled by the shafts of light that penetrate through holes and open decks into the dark interiors. In a nation with so much WWII history and a heritage of water transportation as a way of life, you can expect to find a multitude of underwater structures including ships and airplanes - some intentionally sunk and functioning as artificial reefs, others sometimes referred to as ""natural wrecks"".|\n|A sunken freighter, The Kabilok, once sailed between the outer islands and Majuro, hauling copra and supplies. She lies on her side on an 80\' sandy bottom in Majuro Lagoon. A favorite for night dives and underwater photographers, the Kabilok offers safe, interesting penetration into the open cargo hold and is home to colorful sponges, whip coral, and tropical fish of many species. On night dives, beautiful batfish and puffers take refuge in and around the wreck.|\n|Ejit Island (The Parking Lot) at 10-120 feet is the location of a U.S. military dumpsite. A small coral pinnacle marks the spot where Jeeps, Trucks, a Navy Tug boat, and a landing craft were sunk at the end of WWII. The relics, now artificial reefs, are home to colorful sponges, corals, and tropical fish of many species. This area is a favorite for photography and exploration.|\n|The Grumman ""Duck"", just a few miles from the Yokwe Divers dock sits inverted on the bottom. It seems to have crashed on approach to Majuro\'s WWII carrier re-supply airfield, which was adjacent to this site. Used primarily for search & rescue and reconnaissance, there are said to be fewer than 10 surviving Ducks left in the world. This aircraft is also home to hundreds of fish, sponges, and corals. The ""Duck"" is in excellent condition and steeped in the history of this area of operation during WWII.|\nJust 500 meters from our dock at the Marshall Islands Resort, an\nF6F Hellcat sits in 115 of water. It was pushed overboard from\none of the five aircraft carriers that were on Majuro Lagoon in late\n1944. The Grumman F6F Hellcat was the U.S. Navy\'s primary fighter\nbrought into service to battle against the Japanese ZERO. The control stick, rudder pedals, and throttles\nare still intact and the wings are folded back in pre-flight storage\nposition. Hundreds of tropical fish, sponges, oysters, and\ncorals have made their homes there.\nIn the same general area there are other wrecks to be explored. A short 5 minutes from our dock lies the Ratak-Ralik. A dive of 60 - 92 feet will place you on this 120\' freighter that sank in the late 1980\'s. The engine room is accessible and very open penetrations are made through the wheelhouse and the hold. Expect to see thousands of fish on this dive. Within 200 yards the Evangeline offers an upright 85\' wreck with a wheelhouse and exposed hatch into the engine room.\n|Majuro\'s latest find, the Cenpac, was discovered in January 2003. The Cenpac is a 150\' refrigerator ship that was used to haul copra and also passengers and supplies between Majuro and the outer islands. It sank in 135 feet of water about ten years ago. Schools of Spade Fish and Giant Sweetlips call the wreck home.|\n|Midway on the western atoll between Laura & Rong-Rong is a B-24 Liberator, resting in 12 feet of water. A popular site for snorkeling, this classic American bomber was damaged during a bombing run from Kiribati prior to the U.S. occupation of the Marshall Islands. The pilots brought her down on the reef top at low tide. The pilots were captured but the plane remained and was scuttled by the Japanese soldiers who were stationed here at the time. Although the fuselage has broken apart and been buried in the surrounding sand, more than 2/3 of the wing structure is still intact with all four engines and props. The belly machinegun turret is now the host of corals and fish.|\n|A DC-3 has become a very interesting artificial reef, covered in corals and fish and located near shore at Anemonit Island. Penetrations are easily and safely made through the open aft section where the plane was dismantled. The area is dotted with other natural coral heads teeming with life and is a favorite of novice wreck and reef divers.|']"	['<urn:uuid:5cc866e2-6b29-445e-820a-d9cfb7fcdc3b>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	9	77	1681
6	Is the Nebra Sky Disc older than the earliest telescope view of the Pleiades?	Yes, the Nebra Sky Disc from 1600 BC is much older than the first telescope observation of the Pleiades, which was done by Galileo Galilei in 1610.	"[""The Nebra Sky Disc is one of the most interesting archaeological finds in recent years. Dating from 1600 BC, it is a bronze disc about 32 centimetres in diameter with a diagram of the heavens embossed in gold onto it. The disc was found near the town of Nebra in eastern Germany (the state of Sachsen-Anhalt). The disc was made by Bronze Age people, the race that lived in Europe before the arrival of the Celts, and is said to be the oldest chart of the heavens in the world.\nThe disc was found in a cache of bronze goods, including axes and daggers, in a Bronze Age site at the top of a mountain, the Mittelberg. It is thought that the site would originally have had a good view of the skies and the horizon all around, and might have been used as an observatory. The astronomical information on the disc is particular to the latitude of the location where it was found, so it is likely that the disc was made for and used in the site where it was finally hidden.\nDescription and Interpretation of the symbols on the disc\nAt one edge of the disc is an arc which looks like a boat sailing on the sea. The tiny indentations along each side of the arc may represent the oars of the ship. Many ancient peoples imagined the sun as travelling from Western to Eastern horizon after it set in a special ship. This may be a depiction of the Ship of the Sun. If so, it means that the disc should be held in a vertical plane, with this 'boat' at the bottom. In this orientation, the rest of the symbols in the centre of the disc fall into place as a picture of the heavens.\nOn the left and right sides are two long arcs. These span about 80 degrees each. The difference between sunrise on the summer solstice and on the winter solstice is 82.7 degrees at this latitude, as is the difference between the sunsets on the two solstices. The two arcs are said to represent the portions of the horizon where the sun rises during the year. (The gold coating on the left arc, representing sunset, has fallen off and is lost).\nBetween the two arcs are a full circle and a crescent. The crescent obviously represents a crescent moon, while the large circle may be the sun or a full moon. (The gold on the sun/full moon circle is damaged). In the background are 23 stars dotted in an apparently random pattern, and one group of seven stars which is said to represent the Pleiades star cluster (the Seven Sisters or M45). X-Rays indicate that under the gold of the right arc are two more stars, so it is likely that the two arcs were added some time after the other features.\nIt is an astronomical fact that when the crescent moon appears in a particular orientation to the Pleiades, there is an eclipse seven days later. Is the picture on the disc intended to portray this? We'll never know for sure, as there is not enough detail in the picture.\nAround the outside of the disc is a ring of crude holes punched through the metal. It is thought that these are for attaching the disc to something, rather than forming part of the astronomical diagram. Perhaps the disc was stitched to a piece of heavy cloth?\nWhat was the purpose of the disc?\nIf the disc was intended as an astronomical tool, the only thing on it that is accurate is the pair of arcs. With the disc in a horizontal plane, these could be used to examine the position of sunrise and sunset; the cache site was on the top of a hill, a good place for looking at the sun. The site was surrounded by an artificial low bank, which could be used for measuring the position of the sun on the horizon. The position of the sun at sunrise and sunset is a good indication of the time of year and can be used to predict times for planting and harvesting crops; the Bronze Age people were an agricultural society. Alternatively, the disc might have been a teaching tool, explaining the mysteries of the night sky to students.\nThe finding of the disc\nThe disc was found by treasure hunters in 1999. They removed the disc from the site and attempted to sell it to local archaeologists. By the laws of the state in which it was found, it should belong to the state, so they discovered that they couldn't sell it legally. In 2003, they attempted to sell it to a collector in Switzerland, but the 'collector' turned out to be part of a 'sting' operation, working with the Swiss police. The disc was recovered and is now safely in the hands of the legal owner, the state of Sachsen-Anhalt. It is under examination in the Museum of Halle and will eventually go on display there.\nIt has long been known that so-called primitive people had advanced knowledge of the skies: the New Stone Age builders of Newgrange in Ireland aligned the monument with the rising sun at the Winter Solstice in about 3200 BC. Stonehenge, built in about 2000 BC, features many astronomical alignments. The Nebra Sky Disc is the earliest known portable guide to the heavens, and the first example of such knowledge in Central Europe. It shows that the knowledge of the skies was not restricted to one group in one location but existed throughout Europe in the second millennium BC.\nFor a good picture of the disc, look at the Official Nebra Sky Disc website, although the text is in German.\nFor an article on the disc in English, look at here. Note that the picture of the disc in this website has been touched up, with missing golden parts restored."", 'Alcyone, Eta Tauri (η Tau) is a multiple star system located in the constellation Taurus, the Bull. With an apparent magnitude of 2.87, Alcyone is the third brightest star in Taurus, after Aldebaran and Elnath, and the brightest star in the Pleiades cluster (Messier 45), one of the nearest open clusters to Earth. The star lies at an approximate distance of 440 light years from Earth.\nEta Tauri is listed as a multiple star system in the Washington Double Star Catalog (WDS) and the Catalog of Components of Double and Multiple Stars (CCDM). The latter catalog lists the primary component, formally named Alcyone, and three companions. Alcyone B (24 Tauri) is a white (A0) main sequence star with an apparent magnitude of 6.28, located at a separation of 117’’ from the primary component. Alcyone C has the variable star designation V647 Tauri and is classified as a Delta Scuti variable. Its brightness varies from magnitude 8.25 to 8.30 with a period of 1.13 hours. Alcyone D is a white (F3) main sequence star with a visual magnitude of 9.15.\nThe Washington Double Star Catalog lists Alcyone D as a double star with two similar components separated by only 0.3’’. The catalog lists four other fainter stars in the system, none of them brighter than magnitude 11.\nThe primary component in the system is a triple star. The brightest component is a bluish (class B7IIIe) giant star with a mass between 5.9 and 6.1 solar masses and a radius about 9.3 times that of the Sun. With an effective temperature of 12,258 K, the star is 2,030 times more luminous than the Sun, but most of its output is in the invisible ultraviolet part of the spectrum. Alcyone is a Be star and, like all the named Pleiades except Maia, it is a very fast spinner, with a projected rotational velocity of 149 km/s. As a result, it is surrounded by a disk of gas ejected around the equator.\nThe nearest companion is a low-mass star separated by less than 1 milliarcsecond from the primary. The two components orbit each other with a period of a little over four days. The other companion is located at a separation of 0.031 arcseconds (roughly the distance from the Sun to Jupiter). It is half as massive as the primary. The two have an orbital period of about 830 days.\nAlcyone is the brightest member of the Pleiades open cluster. The nine brightest stars in the cluster are named after the Pleiades, the Seven Sisters in Greek mythology – Alcyone (Eta Tauri), Asterope (21 Tauri), Celaeno (16 Tauri), Electra (17 Tauri), Maia (20 Tauri), Merope (23 Tauri) and Taygeta (19 Tauri) – and their parents Atlas (27 Tauri) and Pleione (28 Tauri).\nThe Pleiades open cluster contains more than 1,000 confirmed members and has a total mass of 800 solar masses. The brightest members are hot blue stars of the spectral type B with an estimated age of up to 100 million years. They lie at an average distance of 444 light years from Earth.\nCatalogued as Messier 45 by the French astronomer and comet hunter Charles Messier, the cluster is exceptionally bright and large, occupying an area of 110 arcminutes. It has an apparent magnitude of 1.6 and is the most recognizable star cluster visible to the naked eye. Up to 14 members are visible without binoculars in exceptionally good conditions, but observers normally see six to eight stars.\nThe members will likely keep travelling together through space for another 250 million years and then gradually disperse as a result of gravitational interaction with the interstellar medium.\nIn Greek mythology, the Pleiades caught the eye of Orion, the hunter. After their father Atlas was condemned to carry the heavens on his shoulders, Orion began to pursue the sisters. Zeus intervened, transforming the sisters first into doves and later into stars. The constellation Orion still appears to be pursuing the Pleiades across the sky.\nThe Pleiades have been a familiar feature of the night sky in cultures around the world. The earliest discovered depiction of the cluster is the Nebra sky disk, a Bronze Age artifact found in Germany that dates back to around 1600 BCE. The stars were notably mentioned in the Bible, the Greek poet Hesiod’s Works and Days, and Homer’s Iliad and Odyssey.\nThe first astronomer to observe the Pleiades through a telescope was Galileo Galilei, who discovered that the cluster contained many other stars too faint to be visible to the naked eye. Galilei published his notes along with a sketch in March 1610.\nIn 1782, the French astronomer Edme-Sébastien Jeaurat drew a map of 64 members of the cluster and published it in 1786.\nIn Japan, the cluster was mentioned as Mutsuraboshi, meaning “six stars,” in the 8th century Kojiki (“An Account of Ancient Matters”). Today, the Japanese know the Pleiades as Subaru. The automobile company Subaru uses an image of the six brightest stars in its logo.\nThe name Alcyone (pronunciation: /ælˈsaɪəniː/) comes from Greek mythology. Alcyone was one of the Pleiades, the seven daughters of the Titan Atlas and the sea-nymph Pleione. The seven sisters were nymphs and companions of the goddess Artemis. Alcyone had three children by Poseidon: Hyrieus, Hyperenor, and Aethusa. The name Alcyone (Greek: Ἁλκυόνη) was derived from the Greek αλκυων (alkyon), meaning “kingfisher.” The Pleiad Alcyone is not to be confused with the Thessalian princess Alcyone, who was transformed into a common kingfisher along with her husband Ceyx after he had died at sea.\nThe name was officially approved for the star by the International Astronomical Union’s (IAU) Working Group on Star Names (WGSN) on June 30, 2016. It formally applies only to the component Eta Tauri A.\nThe Chinese know Alcyone as 昴宿六 (Mǎo Xiù liù), meaning “the Sixth Star of Hairy Head.” Hairy Head is a Chinese asterism consisting of the Pleiades stars Asterope, Atlas, Electra, Maia, Merope, Taygeta, and Alcyone. The asterism is one of the seven mansions of the White Tiger.\nAlcyone is easy to find because it is a member of one of the most recognizable features of the night sky. The Pleiades can be found by following an imaginary line extended from the stars of Orion’s Belt – Alnitak, Alnilam and Mintaka – but most observers will likely spot the cluster without any help because it is exceptionally large and bright.\nThe cluster is conspicuous in the sky from October to April. It is invisible in May and June, when it is too close to the Sun.\nAlcyone is located in the constellation Taurus. Representing the celestial Bull, Taurus is one of the largest constellations in the sky, covering an area of 797 square degrees. It is best known for its brightest star, Aldebaran, the 14th brightest star in the sky, and the two bright, large open clusters, the Pleiades and the Hyades. Aldebaran marks the Bull’s eye and the V-shaped Hyades cluster outlines its head.\nTaurus is home to several other well-known deep sky objects: the historic supernova remnant Messier 1, also known as the Crab Nebula, the reflection nebula NGC 1555 (Hind’s Variable Nebula), the planetary nebula NGC 1514 (Crystal Ball Nebula), the colliding galaxies NGC 1409 and NGC 1410, and the open clusters NGC 1647 and NGC 1817.\nThe best time of year to observe the stars and deep sky objects of Taurus is during the month of January.\nThe 10 brightest stars in the constellation are Aldebaran (Alpha Tau, mag. 0.86), Elnath (Beta Tau, mag. 1.65), Alcyone (Eta Tau, mag. 2.87), Tianguan (Zeta Tau, mag. 2.97), Chamukuy (Theta2 Tauri, mag. 3.40), Lambda Tauri (mag. 3.47), Ain (Epsilon Tau, mag. 3.53), Omicron Tauri (mag. 3.61), Atlas (27 Tau, mag. 3.63), and Prima Hyadum (Gamma Tau, mag. 3.654).\nAlcyone – Eta Tauri\n|U-B colour index||−0.34|\n|B-V colour index||−0.09|\n|Distance||443.5 light years (136 parsecs)|\n|Parallax||8.09 ± 0.42 mas|\n|Radial velocity||5.40 km/s|\n|Proper motion||RA: 19.34 ± 0.39 mas/yr|\n|Dec.: -43.67 ± 0.33 mas/yr|\n|Mass||5.9 – 6.1 M☉|\n|Radius||9.3 ± 0.7 R☉|\n|Rotational velocity||149 km/s|\n|Surface gravity||3.047 cgs|\n|Right ascension||03h 47m 29.077s|\n|Declination||+24° 06′ 18.49″|\n|Designations||Alcyone, Eta Tauri, η Tau, 25 Tauri, HD 23630, HR 1165, HIP 17702, GC 4541, GCRV 2135, SAO 76199, FK5 139, PPM 92898, BD+23 541, BDS 1875, CCDM 03474+2407, ALS 15094, IRAS 03445+2357, 2MASS J03472908+2406184, TYC 1800-2202-1, WDS J03475+2406A, Gaia DR2 66714384141781760|']"	['<urn:uuid:3e1b6971-577c-4c1e-9667-65b3bb9b2dce>', '<urn:uuid:46409ecf-331a-4c6e-937a-e03df3bca6a9>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T00:43:36.053682	14	27	2380
7	What role did collective physical gatherings play in advancing workers' rights historically, and how has digital activism changed the nature of collective action in modern times?	Collective physical gatherings were crucial in advancing workers' rights, as shown by the 1892 Manchester march where thousands of workers from different trades united to demand an 8-hour workday and political reforms. These demonstrations created visible solidarity, with workers marching shoulder to shoulder, carrying banners, and gathering in parks to hear speakers advocate for change. In the modern era, digital activism has transformed collective action by making organization and information sharing easier, but it has also introduced challenges. Studies show that while social media can help build networks and relationships between activists, it can also lead to 'slacktivism,' where people engage in token support without meaningful action. Research indicates that public token support through social media doesn't necessarily translate into more substantial forms of activism, unlike the direct physical participation seen in historical labor movements.	"['On 1 May 1892 Manchester workers marched for the first time in a mass labour demonstration for a shorter working week and an independent political voice. It was part of a worldwide movement as unskilled workers organised in mass trades unions and Socialism developed a mass political following.\nMay Day was instituted as an international Labour Day from 1890 onwards. The impetus came in part from a long-running campaign to reduce the working day to 8 hours. In September 1866 the International Workingmen’s Association (otherwise known as the First International) meeting in Geneva passed a resolution adopting 8 hours as a goal. In October 1884 the Federation of Organized Trades & Labor Unions of the United States and Canada also passed a resolution calling for an 8 hour day from 1 May 1886.\nIn May 1886 tens of thousands of workers responded across the United States. The most militant city was Chicago, where on 3 May the police shot dead six strikers at the McCormick Harvesting Machine Company. In response strikers organised a massive rally the following day at Haymarket Square. The rally was peaceful but as it ended somone threw a bomb into police ranks. This was followed by a savage battle in which a number of police died, as well as members of the crowd. There was a political show trial of a number of anarchists, of whom four were convicted and hanged. They become known as the Haymarket Martyrs.\nOn 14 July 1889 the Second International meeting in Paris called workers around the world to march on 1 May 1890 for an 8 hour day.\nThe Congress decided to organize a great international demonstration, so that:\n“in all countries and in all cities on one appointed day the toiling masses shall demand of the state authorities the legal reduction of the working day to eight hours, as well as the carrying out of other decisions of the Paris Congress. Since a similar demonstration has already been decided upon for May 1, 1890, by the American Federation of Labor at its Convention in St. Louis, December, 1888, this day is accepted for the international demonstration. The workers of the various countries must organize this demonstration according to conditions prevailing in each country.”\nThis coincided with a new mood amongst unskilled workers, hitherto ignored or excluded by the trades union movement, which had largely organised skilled male workers only. In July 1888, for instance, women workers at the Bryant & May match factory in East London went on strike and won with the support of Socialists. The following year there was a massive dock strike in London involving thousands of dock labourers, which brought the miles of docks to a halt. The result was victory for the workers leading to higher pay, better working conditions and a new union for unskilled workers, The Dock, Wharf, Riverside & General Labourers Union. The strike was led by trade unionists and Socialists including Tom Mann, John Burns, Ben Tillett and Will Thorne.\nFor the first time Socialist ideas were getting a mass audience. John Burns wrote of the importance of the dispute after it had been won:\n“Still more important perhaps, is the fact that labour of the humbler kind has shown its capacity to organize itself; its solidarity; its ability. The labourer has learned that combination can lead him to anything and everything. He has tasted success as the immediate fruit of combination, and he knows that the harvest he has just reaped is not the utmost he can look to gain. Conquering himself, he has learned that he can conquer the world of capital whose generals have been the most ruthless of his oppressors.”\nOn May Day 1890 there were strikes and marches in many part of the United States and Europe. Frederick Engels wrote:\n“As I write these lines, the proletariat of Europe and America is holding a review of its forces; it is mobilized for the first time as One army, under One Bag, and fighting One immediate aim: an eight-hour working day, established by legal enactment…. The spectacle we are now witnessing will make the capitalists and landowners of all lands realize that today the proletarians of all lands are, in very truth, united. If only Marx were with me to see it with his own eyes!”\nThe London march to Hyde Park was huge, with perhaps 100,000 attending. Engels wrote an account in the newspaper Arbeiter Zeitung:\n“There can be no doubt about that: on May 4, 1890, the English working class joined the great international army. And that is an epoch-making fact. The English proletariat has its roots in the most advanced industrial development and, moreover, possesses the greatest freedom of political movement. Its long slumber — a result, on the one hand, of the failure of the Chartist movement of 1836-50 and, on the other, of the colossal industrial upswing of 1848-80 — is finally broken. The grandchildren of the old Chartists are stepping into the line of battle.”\nThe success in London was repeated in 1891 and Manchester followed with its own march in 1892.\nOn 16 April 1892 The Clarion reported that a “a great labour demonstration” was being planned for Manchester for 1 May. Trade union and labour societies were requested to communicate with Mr James Quinn at the County Forum, 50a Market Street. (The County Forum was a debating society). Organising meetings were to be held every Thursday.\nOn the day before the march Robert Blatchford wrote a millenarian editorial in The Clarion:\n“The people will meet , that is the main thing. We shall see each other face to face, feel each other should to shoulder, hear each other voice to voice, trust each other soul to soul and we shall go away open-eyed and conscious of a change. we shall have felt our strength, imagined our numbers, seen as a vision of the world the golden dawn streak of the day of our deliverance, and our triumph. Manhood suffrage and payment of members! What are these? They are as candles to the sun in comparison with the new LABOUR DAY. …Our labour day as bind us as corn in the sheaf. The sturdy miner, the skilful engineer, the broad-handed navvy, the white-fingered artist, the lusty farmer, the fragile seamstress, the outcasts of the streets, the despised denizens of the slums, the sweater’s slave the hearty sailor. Strong and weak, feeble and brave, old and young, simple and wise, the workers shall band themselves together in fraternity and freedom. They shall march on from this labour day growing ever wiser, nobler and juster until there is honour for those who make more than those who mar, reward for those who labour better than for those who loaf, until snobbery and prejudice, and theft and butchery are banished into the Hell they came from; until Labour shall hold that which it wins, and England shall be the freehold and the home and inheritance of the English.”\nThe procession was to assemble in Stevenson Square at 2.30pm and march to Alexandra Park by way of Oldham Street, Piccadilly, Portland Street, Oxford Street, Stretford Road, Great Jackson Street, Preston Street, Moss Lane and Alexandra Road. The order of procession was advertised as follows\nThe Manchester Fabian Societies\nAmalgamated Society of Railway Servants\nBakers & Confectioners\nThe Labour Church\nShirt & Jacket Makers\nSalford Social Democrats\nNorth East Manchester Labour Electoral\nSpindle & Flyer Makers\nHorsehair & Fibre Workers\nDressers, Dyers & Finishers\nEnginemen & Cranemen\nNavvies & Bricklayers\nNorth Manchester Labour Electoral\nAccording to the report in the Manchester Guardian, a white ensign headed the procession with the slogan “Work for all, Overwork for None”. Other banners stated “Unity is Strength” and “Equality by Right, Justice to All”. Members of the Labour Church carried a banner stating “God Is Our King” while the Social Democratic Federation contingent carried red flags and a red cap on a pole (the symbol of the French Revolution).\nDue to the numbers the procession moved off before the appointed time and was enlivened by at least a dozen bands. The Manchester Guardian noted that watching crowds, especially women, cheered the mottos in favour of the 8 hour day. In Hulme the march was greeted by large crowd and “something like the fervour of enthusiasm”. The march reached the park at about 4pm with at least 60,000 people now present.\nAt the park there were six platforms with a mix of trade union and Socialist speakers who advocated the following political programme;\n1. Formation of an Independent Labour Party\n2. Payment of MPs\n3. Shorter Parliaments\n4. Adult suffrage\n5. Nationalisation of the land.\nOn one of the platforms Robert Blatchford moved the following resolution:\n“That this meeting recognises that the establishment of a working day of not more than 8 hours is the most immediate step towards the ultimate emancipation of the workers and urges upon the Government the necessity of fixing a working day by legislative enactment.”\nOne of the platforms was reserved for Jewish speakers who spoke in Yiddish, including Mr Wess from London. This platform was chaired by Mr R Abrahams, who said that he hoped that next year Jews would be more numerous. There do not seem to have been any women speakers and women’s suffrage was not included as an aim.\nIn 1893 Manchester Council tried to stop the march taking place, turning down an application from 35 trades unions to use Alexandra Park. According to a report in The Clarion, the Corporation Parks Committee had deemed it:\n“inadvisable that Sunday demonstrations should be held in the public parks and considers that unless desired by a considerable section of the Manchester citizens they should not be permitted. This meeting believes the present application is not of such a nature to warrant such permission and therefore declines to grant it.”\nLeonard Hall, chair of the demonstration committee, wrote to The Clarion to state that the march was a labour demonstration open to all and not an Independent Labour Party or Socialist demonstration. After more public protests, permission to use the park was eventually granted.\nManchester was alive with Socialist organisations and activity. In the week before the 1893 march The Clarion carried notices for meetings of the Independent Labour Party in various parts of Manchester and Salford as well as a “gigantic” excursion to Morecambe on Whit Friday, the Social Democratic Federation, Hyde Labour Club, Ashton ILP, Oldham Independent Labour Club, North Manchester Fabians and Manchester Anarchist Group who held public meetings every Sunday in Stevenson Square. Joe Waddington (known as “Clarion” Joe) sold The Clarion, Labour Prophet, Labour Leader, Workman Times, Shafts, A Paper for Women and “Socialistic” literature from his shop at 4a Crown street, Chester Road. The Manchester Labour Press was based at 59 Tib Street.\nThe march took place on 6 May leaving from Stevenson Square at 2.30pm. The Clarion reported that it had been attended by 20,000. In his editorial Blatchford attacked the “city fathers” who had tried to stop them using the park and the Chief Constable who had deployed very large numbers of police who were threatening to the crowd, pushing people off pavements.\nIn 1894 the authorities refused to allow the use of Alexandra Park and instead the march went to Philips Park in Openshaw. The tradition of May Day marches continued for a century until it ceased in the wake of trade union decline and defeat. In recent years the tradition of a May Day labour procession has been revived although the numbers attending are at present but a fraction of those attending in the early years.\nArticle by Michael Herbert.', 'Activism Or Slacktivism? How Social Media Hurts And Helps Student Activism\nOn Oct. 1, 1964, hundreds of University of California-Berkeley students surrounded a police car to protest the arrest of a student. Students stood on top of the car to deliver speeches and sing, “We Shall Overcome,” to a crowd that grew to include roughly two thousand students.\nYears later, on Nov. 21, 2013, students gathered in Saint Louis University’s student union for “The State of St. Louis,” an event planned by a SLU\'s Political Roundtable. Students sat around tables instead of standing on cars, but that’s not the only difference between student activism today and the sit-ins of the past. Student activists today can use social media to promote their organizations online.\nWhile Political Roundtable has 126 supporters on Facebook, roughly 30 students attended their event. Their absence could be attributed to studying for tests or having another commitment, but it could be because of a different phenomenon: slacktivism.\nWhat Is “Slacktivism”?\nSlacktivism is a term for giving token support for a cause, like wearing a pin or “liking” something on Facebook, without being willing to engage in more meaningful support, like donating time or money. And with the presence of social media, being a slacktivist is easier now than ever.\nA recent study from the University of British Columbia found that when people participate in a form of public token support, they aren\'t any more likely to participate in a form of more meaningful support in the future. Someone who ""likes"" a cause on Facebook wouldn\'t be any more likely to donate in the future than someone who had no exposure to the cause at all.\nKirk Kristofferson is one of the authors of the study.\n“What we found is that the social observability of the token support really impacted how likely consumers were to follow up and provide real support for that charity,"" Kristofferson said. ""We found that when consumers gave public support, they were no more likely to provide more meaningful support for the cause than if someone was just randomly asked for the larger request.""\nStudies have shown that when people support a cause with some effort, they’re more likely to support a cause with an even larger effort in the future. For example, someone who participates in a 5k run for a charity is more likely to donate in the future than someone who has no familiarity with the cause at all or has done something minimal like joining their page on Facebook.\nIf college students choose to be activists by sharing a link or liking a status, they may not feel more compelled to take real, tangible effort towards social change.\n“Drawing back from the 60s and sit ins, that was real action. That was people putting forth a strong effort,” Kristofferson said. “Conversely, what social media has done is remove that effort. So, our findings would suggest that it (social media) would be making it more difficult to make a difference.”\nHow Do Students See Social Media With Social Causes?\nBack at SLU, T.K. Smith is on the executive board of Saint Louis University’s Political Round Table. He thinks that social media can convince users they’re doing something good even if all they’re doing is pressing a button.\n“Because we can like something, because we can read an article online, it kind of changes how you feel like you are doing something,” Smith said. “You kind of feel like being more in the know that you are doing more than if you hadn’t \'liked\' it.”\nAnother student at Saint Louis University, Christina LaFon, created a Facebook group with the purpose of sharing articles about feminism with like-minded people.\nBut raising awareness, LaFon said, isn\'t the end goal of social justice, or her own end goal for the group.\n“Raising awareness is a lazy objective. Awareness is a given, action is what you want to promote,” LaFon said. “Eventually, I’d really like for us to be able to provide resources for people to start their own movements.”\n\'Slacktivism\' Still Means Activism... When It\'s Done Right\nAdvocating for a cause on social media does not automatically make you a slacktivist. LaFon, for instance, has been involved in political campaigning and offline advocacy in addition to the awareness she tries to raise online.\nJulie Dixon is Deputy Director of the Georgetown Center for Social Impact Communication, and one of the authors of “Digital Persuasion,” a study that looks at how social media users use their influence to promote social change.\n“It’s unfair to write off everyone who supports on social media as being a slacktivist,” Dixon said.“Because as we’ve seen in other research, there are folks who are active on social media who are deeply involved in other ways.”\nDespite the potential effects of slacktivism student protests do still occur on college campuses. Just last year SLU students helped organize a sit-in and teach-in in protest of then-President Father Lawrence Biondi, and within the last couple years student protests have gained global attention in Egypt, England and Chile.\nIn many ways, Dixon said, social media makes it easier for students to organize in a way that can lead to offline engagement.\nMost importantly, Dixon said to remember that social media isn’t a fundraising tool; it’s a relationship building tool. Social causes are helped by social media when you see what your network cares about and decide to care about that, too.\n“How people hear about causes and how people decide to support and care about them it comes a lot more often from their friends,” Dixon said. “It doesn’t come from this blanket post from the organization- that doesn’t work as much as having a friend say, ‘Hey this is an organization that I really care about and it\'s impacted my life, and you should care about this as well.’”\nSocial media allows users to organize, network and share what they’re passionate about in a way that wasn’t available to student protestors in the 60s. It’s now easier than ever to learn about causes and share information, but it\'s also easier than ever to be a slacktivist.\nThrough research, experts like Dixon and Kristofferson continue to work towards connecting users and organizations in a way that will combat slacktivism and lead to meaningful support that both users and organizations could “like.”\nTake a look at some popular token support campaigns by exploring the infogram below.\nOr, for a look at a campaign committed to combating slacktivism, watch this video from UNICEF Sweden\'s ""Likes Don\'t Save Lives"" Campaign.\nCar Top Rally Audio Courtesy of Pacifica Radio Archives']"	['<urn:uuid:e0bbfca3-820d-47e3-b8a8-62c93b8b0831>', '<urn:uuid:e95551ba-2f49-43b0-9a6f-d5dd2f76a09c>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	26	135	3048
8	time recommendations for tree branch cutting	Most routine pruning to remove weak, diseased or dead limbs can be done at any time during the year with little effect on the tree. However, growth and wound closure are maximized if pruning takes place before the spring growth flush.	"[""Pruning and Removing Trees\nPruning is the most common tree maintenance procedure. Although natural forest trees grow perfectly well on their own planted landscape trees require a higher level of care to maintain their structural integrity and aesthetics. Pruning must be done with an understanding of tree biology. Improper pruning can create lasting damage or even shorten a tree's life.\nOften small trees and shrubs, such as fruit trees, can easily be managed yourself. You can find out more about the right way to prune on our Tree Care Information page.\nPruning and removing larger trees is a dangerous task involving large, heavy structures, dangerous equipment and working at heights. For your safety and the protection of your property these activites should only be undertaken by a competent, qualified and insured arborist.\nOften a professional consultant arborst can provide advice to you regarding trees on your property that means your tree may not need to be pruned or removed, preserving the tree and saving you money.\nIn many places council or state controls may require you to obtain a permit to prune or remove a tree, particularly if the tree is large or a local native species. Check with your local council for controls in your area.\nBecause each cut has the potential to change the growth of the tree, no branch should be removed without a reason. Common reasons for pruning are to remove dead branches, to improve form and to reduce risk. Trees may also be pruned to increase light and air penetration to the inside of the tree’s crown or to the landscape below. In most cases mature trees are pruned as corrective or preventive measures, as routine thinning does not necessarily improve the health of a tree\nWhen to Prune\nMost routine pruning that is undertaken to remove weak, diseased or dead limbs can be accomplished at any time during the year with little effect on the tree. As a rule, growth and wound closure are maximised if pruning takes place before the spring growth flush.\nA few tree diseases can be spread when pruning wounds provide access to pathogens (disease-causing agents). Susceptible trees should not be pruned during active disease transmission periods.\nSpecific types of pruning may be necessary to maintain a mature tree in a healthy, safe and attractive condition.\n- Cleaning is the removal of dead, dying, diseased, weakly attached and low-vigour branches from the crown of a tree.\n- Thinning is selective branch removal to improve structure and to increase light penetration and air movement through the crown. Proper thinning opens the foliage of a tree, reduces weight on heavy limbs and helps retain the tree’s natural shape.\n- Raising removes the lower branches from a tree to provide clearance for buildings, vehicles, pedestrians and to allow for clear sight-lines.\n- Reduction reduces the size of a tree, often for utility line vegetation control clearance. Reducing a tree’s height or spread is best accomplished by pruning back the leaders and branch terminals to secondary branches that are large enough to assume the terminal roles (at least one-third the diameter of the cut stem). Careful and proper reduction helps maintain the form and structural integrity of the tree.\nPruning Young Trees\nProper pruning is essential in developing a tree with a strong structure and desirable form. Trees that receive the appropriate pruning measures while they are young will require less corrective pruning as they mature.\nA good structure of primary branches should be established while the tree is young. These branches, called scaffold branches, are a mature tree's framework. Properly trained young trees will develop a strong structure that requires less corrective pruning as they mature. For most young trees maintain a single dominant leader growing upward. Do not prune back the tip of this leader or allow secondary branches to outgrow the main leader.\nPruning of palms is mostly done to remove dead or dying fronds and/or flowering/fruiting clusters, particularly those that may be a potential risk to the public, such as coconuts. Pruning is usually conducted at least bi-annually. Coconuts may be pruned as often as every 3-4 months to minimise the risk of injury or damage from the heavy fruit falling. Great care should be taken to avoid any damage to a palm's terminal bud or trunk when removing fronds.\nIt is best for the palm if green fronds remain intact. Overpruned palms may have slower growth and may attract pests. Climbing spikes should generally not be used to climb palms for pruning, because they wound the palm trunk.\nDon’t Top Trees!\nThe act of 'topping' is perhaps the most harmful tree pruning practice known. Yet, despite more than 25 years of literature and seminars explaining its harmful effects, topping remains a common practice.\nTopping is the indiscriminate cutting of tree branches to stubs or to lateral branches that are not large enough to assume the terminal role. Other names for topping include 'lopping', 'heading', 'tipping' and 'rounding-over'.\nAlternatives to Topping\nSometimes a tree must be reduced in height or spread, such as for providing utility line clearance. There are recommended techniques for doing so. Small branches should be removed back to their point of origin. If a larger limb must be shortened, it should be pruned back to a lateral branch that is large enough (at least one-third the diameter of the limb being removed) to assume the terminal role. This method of branch reduction helps to preserve the natural form of the tree.\nHowever, if large cuts are involved, the tree may not be able to naturally close over the wounds. Sometimes the best solution is to remove the tree and replace it with a species that is more appropriate for the site.""]"	['<urn:uuid:8fcd60cb-9c1a-4dab-8e3b-f5b8b7549fee>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	6	41	955
9	low power timing devices performance degradation causes	Low power timing devices like MEMS-based TCXOs offer excellent power efficiency, with current draws as low as 4.5 μA at 1.8V for 32.768 kHz models. However, their performance can degrade over time due to crystal aging, which occurs when contaminants either enter or leave the crystal. Other factors affecting frequency error include temperature variations, supply voltage changes, and load variations. Contamination during manufacturing, especially from moisture, is a major concern that requires careful control through processes like nitrogen dry boxes and high-temperature vacuum sealing.	['Power consumption is a concern for any electronic device with a battery. For consumer and commercial mobile/portable products in the near future, the majority will require energy-efficient circuits and components to extend battery life. Time-keeping is essential to saving power in many circuits—for example, starting and ending “sleep modes” to conserve energy when a circuit is not in use.\nFortunately, microelectromechanical-systems (MEMS) and analog technology from SiTime enabled the development of low-power timing components: temperature-compensated crystal oscillators (TCXOs) with a wide range of uses, from sleep clocks to timing devices for Internet-of-Things (IoT) sensors. The firm recently unveiled three products from its Super-TCXO family, with ±5 ppm stability at 32.768 kHz and at factory-programmable frequencies from 1 Hz to 1 MHz, all contained within tiny chip-scale packages (CSPs).\nFor applications where even microamperes of current per active component can make a difference in the power consumption of a systems design, MEMS is an effective means for generating frequencies and saving power. By leveraging the low-power operation of silicon CMOS circuitry, the new Super-TCXOs provide LVCMOS outputs with excellent stability over wide operating temperature ranges. They achieve clock frequencies as high as 1 MHz, and at a fixed frequency (32.768 kHz) which has become popular for numerous timing applications both in consumer and industrial electronic products.\nSiTime has built a strong reputation for its MEMS oscillators, developing miniature, cost-effective alternatives to crystal resonators and oscillators and other oscillator technologies for audio through microwave frequencies. In addition to combining electrical and mechanical devices in its MEMS technology, the firm has also developed sophisticated analog circuitry to maintain frequency stability over time and temperature. This circuitry also pairs with the Super-TCXOs to manage power consumption.\nAlthough each Super-TCXO is supplied in a miniature CSP, it is actually a MEMS resonator with programmable analog circuitry within that package. Analog circuitry includes temperature sensing and regulation, a programmable divider, a low-power fractional-N phase-lock loop (PLL), voltage regulators, sustaining amplifier, and a programmable driver. In effect, SiTime has succeeded in bringing a technology once considered exotic to the masses.\nThe MEMS oscillators are fabricated with the firm’s unique MEMS First process, which includes forming a protective seal around the MEMS resonator. This is achieved by the firm’s EpiSeal process, during which the MEMS resonator is annealed at temperatures exceeding +1000°C. This creates a strong vacuum chamber around the resonator for protection, ease of packaging, improved performance, and long-term reliability.\nThe EpiSeal process grows a polysilicon cap on top of the resonator cavity, eliminating the need for additional cap wafers or custom packaging. As a result, the Super-TCXOs can be treated like any other packaged CMOS device, not as some “exotic” MEMS device. In addition, it allows the integration of additional components and circuitry within the MEMS oscillator package for improved performance and ease of use, plus the elimination of such discrete circuits as temperature compensation outside of the oscillator package.\nAs an example, model SiT1566 is a low-jitter Super-TCXO with 32.768-kHz output and ±5 ppm total frequency stability under all conditions, all inclusive, without overmolding. Its current draw is a mere 4.5 μA for a supply voltage of 1.8 V ±10%. The integrated peak-to-peak phase jitter is typically 1.8 ns RMS and maximum of 2.5 ns RMS, while the RMS period jitter is typically 2.5 ns and worst-case performance of 4.0 ms for 10,000 measurement samples. The peak-to-peak period jitter is typically 20 ns, also for 10,000 samples.\nThe SiT1566’s temperature coefficient is calibrated and corrected over temperature by a portion of the programmable analog circuitry included within each package: an active temperature correction circuit. The miniature MEMS TCXO achieves dynamic temperature frequency response of ±0.5 ppm/s for a temperature ramp of as much as 1.5°C/s. SiT1566 models are available for operating temperature ranges of -20 to +70°C and -40 to +85°C.\nAmazingly, the MEMS resonator and the analog circuitry are housed in a CSP measuring 1.5 × 0.8 × 0.6 mm (see figure) and consuming just 1.2 mm2 of circuit-board area. These MEMS Super-TCXOs are designed to provide significant power savings in electronic products—and in communication networks that rely on precision timing devices such as this—to send different parts of the network into “sleep” modes to conserve power. The SiT1566 becomes active quickly to conserve power, with a power-on start-up time of 300 ms to reach 90% of the final rated output voltage.\nModel SiT1568 is a 32.768-kHz Super-TCXO with in-system auto-calibration. It can compensate for assembly-related frequency errors by connection to a 10-MHz external reference oscillator and running the auto-calibration routine during final system test. The frequency stability of a model SiT1568 can be up to ±25 ppm stability following overmolding and prior to autocalibration, and then reduced to less than ±5 ppm all-inclusive stability after auto-calibration. It has current draw of just 4.5 μA from a supply at 1.8 V ±10%. It is also supplied in the same miniature CSP housing as the SiT1566, with temperature-related specifications similar to those of the SiT1566.\nThe third member of the Super-TCXO family is model SiT1576, with factory-programmable LVCMOS output frequency from 1 Hz to 1 MHz. It is also specified for ±5 ppm total frequency stability under all conditions. Its current draw is a function of frequency for a supply voltage of 1.8 V ±10%, only 2 μA for a 1-Hz output, 4.5 μA for a 33-kHz output, 8 μA for a 100-kHz output, and 20 μA for a 1-MHz output.\nAs with the 32.768-kHz oscillators, the integrated peak-to-peak phase jitter is typically 1.8 ns RMS and maximum of 2.5 ns RMS, while the RMS period jitter is typically 2.5 ns and worst-case performance of 4.0 ms. The peak-to-peak period jitter is typically 20 ns.\nThese LVCMOS clocks are a fraction of the size of traditional quartz crystal oscillators, with extremely low power requirements for wearable and portable electronic products (including for home, industrial, and medical IoT sensors). The short turn-on times and high frequency accuracy can result in accurate setting of network sleep times and reliable saving of power at system levels.\nIn addition, the small size of the package simplifies placement in most circuit and system designs. The MEMS oscillators are capable of driving multiple loads, such as the wakeup clock for Bluetooth circuitry and the timing device for a microprocessor in a battery powered design. In doing so, it can save design time, components, cost, and design size. All three Super-TCXOs products are lead-free and both RoHS- and REACH-compliant. Versions available for use over the two operating temperature ranges: -20 to +70°C and -40 to +85°C.\nLooking for parts? Go to SourceESB.\nDownload this article in .PDF format\nThis file type includes high resolution graphics and schematics when applicable.', 'Crystal Aging and a Counter-Intuitive Tip to Improve System Performance.\nInside Frequency Control\nBut what are the main causes of this frequency error, and what should we be looking for in our oscillators to ensure high levels of systems performance?\nMain Causes of Frequency Error\nThere are four main drivers that cause frequency error in oscillators:\n- Crystal aging\n- Frequency vs. temperature\n- Frequency vs. supply\n- Frequency vs. load\nOf these four drivers, crystal aging is one of the most common failure modes. Very plainly, crystal aging is used to describe long-term frequency change that happens over time caused by changes in either the environment or the crystal itself. A good analogy would be breaking in a new pair of dress shoes. The fit will change more rapidly at the beginning, but eventually they’ll settle in and feel great!\nThere are two types of Crystal aging, positive aging and negative aging. Positive aging occurs when contaminants are driven out of the quartz crystal. Conversely, negative aging occurs when contaminants are driven into the quartz crystal.\nHow Crystal Aging Is Measured\nSo how is crystal aging measured, and what are frequency control experts doing to make sure it doesn’t become a problem in the system down the road?\nPerformance requirements related to crystal aging are maintained to a mil-spec published by the US government. The typical process to meet an aging requirement is to perform a burn in on the oscillator, put the parts into a aging system where it is measured multiple times a day, and then plot and compare these measurements against the mil standard on crystal aging.\nUsing these measurements, frequency control manufacturers can project the aging rate of their devices over the entire life span of a system up to 20 years and beyond.\nContamination as a Crystal Aging Factor\nThere are many factors that can affect crystal aging rate. Chief among these is the amount of contaminants that become sealed inside the resonator’s package. As you can imagine, inside an oscillator package are several interface boundaries between electrical connections, mechanical connections, and the crystal blank itself. Each one of these interfaces is an opportunity to introduce new contaminants into the package.\nPotential sources of out-gassing or contamination can be found in the materials that make up the resonator itself. In fact, the very act of sealing the package can also introduce unwanted sources of contamination. For example, quartz is a getter and readily absorbs moisture, which is extremely detrimental to a vacuum-sealed package.\nThat’s why humidity is tightly controlled during the manufacturing process of high quality frequency control devices. Some common steps to avoid excess moisture include storing and processing components in nitrogen dry boxes, hydrogen firing, and sealing the final resonator under high-vacuum and at elevated temperatures.\nFor instance, on the BG61 oscillator that is flying on the New Horizons spacecraft currently exploring Pluto, the crystal was held under high-temperature vacuum for about a week prior to being sealed under high-vacuum. The BG61 is widely recognized as the most stable crystal in the UNIVERSE! Curious to know just how stable that is… well, it’s 0.0001 ppb or 0.0000000000001 parts per billion. Speaking of billions, Bliley’s ultra-stable crystals are now 5 billion miles from earth!\nTIP: Reduce Temperature Range to Improve System Performance\nSo, what can designers do to insulate themselves from the negative effects of crystal aging? One counter-intuitive design approach is to reduce the required temperature range for the oscillator.\nA common systems engineering approach is to build margin into your component selection to ensure system level performance. If a system must operate across a temperature range from -20 to 70 degrees C, it’s natural for a systems engineer to say “I’m going to buffer myself and spec the oscillator over the full temperature range -40 to 85 degrees C.”\nThe engineer thinks they’ll get a better part since it will be able to operate over an extended temperature range. However, this forces the frequency control manufacturing to ensure their component must operate at up to 95C. These elevated temperatures will accelerate aging (increasing the frequency error) and lower the devices MTBF or mean time between failures.\nWhat began as an attempt to get a superior performing part ends with getting a part that has sub-par performance and will likely have a reduced operational life.\nThe next time you’re designing in a crystal, OCXO, VCXO, or TCXO, remember the effects that aging has on overall clock performance and ways you can compensate within your design.\nIf this article resonated with you (yes, that’s a frequency control pun) follow us on LinkedIn, Facebook, or Twitter to catch more information like this!\n[Editor’s note: This article was originally published on May 9, 2016 and updated on November 23, 2022.]\nTopics: crystal oscillators, Clocks & Crystals\nI just read: https://blog.bliley.com/crystal-aging-and-a-counter-intuitive-tip-to-improve-system-preformance\nand have a question regarding the ‘counter-intuitive design approach’. What are you meaning with the following: However, this forces the frequency control manufacturing to ensure their component must operate at up to 95C.These elevated temperatures will accelerate aging (increasing the frequency error) and lower the devices MTBF or mean time between failures.\nDo manufactures test every single oscillator? And can doing so causing the oscillator to age faster? Or why are they aging faster?\nRob R. – Bliley Technologies\nHi Oskar. Yes, we test and age every oscillator per MIL-PRF-55310 standards to make sure they meet requirement. Oscillators do age faster when exposed to higher temperatures. Hope this helps! Let us know if you have any further questions']	['<urn:uuid:f894a6f2-d827-4c60-91b2-1ea1c340f868>', '<urn:uuid:175e2e2f-1526-41b0-8dca-e4ba511725e7>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	7	84	2040
10	traditional materials british home interior decoration	English interior design style primarily uses natural wood as the main trimming material. The style only allows natural fabrics like brocade, velvet, silk, and satin. The walls typically feature wood paneling combined with expensive fabric wallpaper, while both the floor and coffered ceiling are made of wood. These materials contribute to creating a refined and aristocratic atmosphere characteristic of English interiors.	['Respectable, elegant and refined English style in interior design of apartment, country house, office or salon is often chosen by those who appreciate quality, discreet luxury and ageless charm of classics. Modern interior in English style looks as refined and aristocratic, as it was two or three centuries ago.\nEnglish interior design style is:\n- Natural wood as the primary material for trimming;\n- All shades of brown (from light beige to dark brown tones) in the design of premises. Such shades as wine, purple, crimson, dark green, carmine, a shade of rust, olive, etc are also appropriate. English style is the choice of noble shades (sea green and burgundy, sapphire and dark green), this style does not accept the flashy, acid tones or bright game of contrasts;\n- Interior design in the English style is old fashioned and noble. It is difficult to imagine without the various furniture: chairs and sofas, desks and tables, cabinets and shelves, chests of drawers and cabinets;\n- Symmetry and mostly smooth lines;\n- Geometric pattern, floral ornament on textiles, furniture decoration carved elements;\n- House in the English style is necessarily a house with a fireplace.\nEnglish-style room filled with expensive, high-quality furniture, antique accessories and artwork. This style embodies luxury and elegance, refined taste and elegance.\nEnglish style in the interior (photos below) is perfect for decoration of offices, town houses and elite clubs. This is a style of wealth, exquisite taste and respectability.\nFurniture in the English style is grand and expensive, diverse and recognizable:\n- Comfortable and respectable looks classic English chair with “ears”: wide seat, low armrests and high backrest with going over projections into armrests (called “ears” or “wings”).\n- Chippendale chairs (the famous English cabinetmaker-master of XVIII century.) look a bit pretentious and elegant: the bent legs (often – in the form of a lion’s paw), curved armrests, carved openwork backs. Expensive wood types are often used for their manufacturing. Chairs can be completely wooden (with stuffed seat), soft, that `s mean wrapped to the fabric or leather. Each of these armchairs are real work of art.\n- The chairs in the English style (wide, high-back) also made of wood and decorated with carvings (back, legs).\n- Low wide sofa should be comfortable and soft. Low carved legs, soft curved armrests, wide cushions – such a sofa, voluminous and stylish, will give the room a real English chic.\n- English-style interior implies allocation of bookcases, shelving and shelves, laden with tomes bound in leather.\n- Coffee tables would organically fit into the interior. They can be round, square and triangular, made of wood. You can put a lamp, an ashtray or a vase of flowers on them.\n- Fireplace is an integral part of the interior in the English style.\nEnglish style in the interior of the apartment, office or home is an embodiment of abundance of rich fabrics, beautiful to look at and pleasant to the touch.\n- Curtains should be long, heavy, of dark warm colors, plain or in broad stripes. They can be combined with white light lace curtains and decorative ornamenting cords with tassels and fringe.\n- Bedspreads, blankets and pillows on the chairs, beds and sofas will give comfort to the interior.\n- Luxurious oriental rug brown-red shades can also decorate the living room.\n- Fabric lampshades and pelmets with fringe, embroidered napkins, tapestries on the walls would be appropriate either.\n- Upholstery can be self-colored, striped or with bright oriental pattern (the most common origins – India or China). Use of floral pattern is acceptable, such design will look more comfortable than leather or dark fabrics.\n- English style allows the use of only natural fabrics: brocade, velvet, silk, satin.\nDecor and Trim\nLounge, office or apartment in English style look luxurious and respectable thanks to the abundance of natural wood. Qualitative simulation of this material is permissible, but only for the stylized interiors.\n- The walls of the room, decorated in English style, should be furnished with wood paneling and paintings in massive frames or tapestries. Decoration wallpaper is suitable: English style wallpaper have to be expensive, desirably fabric. Classic option for walls trimming is when the lower part is decorated with wood paneling, middle with wallpaper, and top of the walls are decorated with wooden cornice.\n- English classic style is a wooden floor and a wooden coffered ceiling.\n- Design in the English style implies the presence of a large number of furniture and accessories: candlesticks and porcelain, carved gilded frames of the superfine work for the paintings and mirrors, wall or floor-standing mechanical watches, busts of thinkers and scientists, and a reminder of the colonial greatness of England – Chinese vases and African shells, figurines and jewelry boxes.\n- The house in the English style is filled with soft diffused light: massive chandeliers, cozy floor lamps, table lamps and sconces in the form of lanterns.\nThe history of the emergence of style\nEnglish style was influenced by two epochs of history “Albion”:\n- The era of George V: crisp lines, symmetry of forms, ancient motifs, geometric prints, bright colors, spacious rooms.\n- The era of Queen Victoria: rich colors, an abundance of furniture and decoration, wealth and glamour. This is an era of active colonization, so – a large number of Indian, African, South American and Asian motifs in interiors (carpets and figurines, boxes and vases, dishes and fabrics). Dark brown antique globes, telescopes, hourglass and forged candlesticks will organically complement the interior.\nWhere English style is appropriate?\nEnglish style design ideal for office, dwelling rooms, country club, cottage or office.\nTo order design of the apartment in English style can those people who prefer elegance, old-fashioned chic and truly quality.\n- The kitchen in the English style is a spacious room filled with beautiful pieces of furniture and decorative elements: a massive dining table, chairs with carved backs, spacious closets, porcelain and ceramic vases, live plants, luxury curtains and tablecloth.\n- Living room in the English style – it is a cozy fireplace, a sofa and a spacious seat with blankets and pillows, coffee table and numerous bookshelves. English style of living is underlined by landscapes or family portraits in elaborated gilt frames.\n- Bedroom in the English style is not conceivable without a large four-poster beds, heavy drapes and massive wardrobes.\nWhy English style?\nEnglish style in the interior of your home, office, salon or apartment is tradition and quality, refinement and elegance, respectability and impeccable taste.\nProjects of houses in the English style are characterized by a combination of discreet rigor and elegant luxury, thought-out design and cozy atmosphere of an old-fashioned housing.\nEnglish style can be brave, strict and majestic, or feminine, romantic and sentimental.\n‘Male’ English style is:\n- Dark colors;\n- Reliability, quality and refinement;\n- Demonstration of high social status and the seeking of genuine beauty of natural materials and antiques.']	['<urn:uuid:7d1730bc-c525-4256-9005-5bd2fa4b766a>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	6	61	1151
11	interstitial lung disease symptoms causes diagnosis methods	Interstitial lung disease causes difficulty breathing, shortness of breath, and a dry cough due to scarring of lung tissues. It can be caused by environmental factors. Diagnosis involves multiple tests including infant pulmonary function testing, bronchoscopy, advanced CT scan protocols, blood tests, and lung biopsies performed by experienced teams.	"[""Children's Hospital Colorado Children’s Interstitial Lung Disease (chILD) Program is one of the leading referral centers in the world for children with these rare lung conditions. Research in our hospital and in collaboration with other pediatric ILD centers around the country has led to the recognition and understanding of many new chILD disorders.\nOne of the most important and challenging aspects of chILD is to get the correct diagnosis through sometimes necessary testing. Our center is world renowned for our diagnostic ability and testing facilities, and provides consultation to patients and health care providers around the world. Hear what Dr. Robin Deterding says about the future of chILD (scroll toward the bottom of the page)\nWhy choose Children’s Colorado for the treatment of this condition?\nOur physicians are leaders in the National chILD Research Collaborative, which consists of pediatric lung, pathology and radiology specialists in North America who are committed to improving care for children with these conditions. The Children's Colorado chILD Program has advanced testing programs in ILD for children related to CT scans, infant pulmonary function testing, bronchoscopy (.pdf) and lung biopsies, when needed. Our physicians have grants to study and find new treatments and cures for chILD.\nOur multidisciplinary care team of pediatric lung specialists, pathologists, radiologists, nutritionists, social workers and nurse specialists brings a wide range of experience and expertise. Our goal is to create the most individualized ILD care plan possible for your child and family. Multidisciplinary care is critical to meet the needs of patients with these complex conditions.\nWe also provide resources for healthcare professionals who diagnose and treat chILD.\nSpecific conditions we treat\nThe Children's Colorado chILD Program provides care for a broad range of lung disorders including:\n- Disorders more prevalent in infancy, like surfactant disorders\n- Disorders associated with a normal immune system and response to infections\n- Disorders related to body-wide processes\n- Disorders associated with a weakened immune system like opportunistic infections\n- Disorders that resemble interstitial disease\nDisorders more common in infancy\n- Diffuse developmental (abnormal lung formation) disorders\n- Growth Abnormalities\n- Specific conditions of unknown or poorly understood origin\nThe reasons that these conditions develop are currently unknown, but they have quite specific diagnostic criteria:\n- Surfactant Dysfunction Mutations and Related Abnormalities\nSome of these disorders are more specific to infancy, while others have also been identified in older children and adults.\nDisorders associated with a normal immune system\n- Infectious and post-infectious processes\n- Disorders related to environmental agents, including hypersensitivity pneumonia (caused by exposure to toxic particles like bird feces or hot tubs)\n- Toxic Inhalation\n- Aspiration syndromes\n- Eosinophilic pneumonias\n- Acute interstitial pneumonia/Hamman-Rich syndrome/Idiopathic diffuse alveolar damage\n- Nonspecific interstitial pneumonia\n- Idiopathic pulmonary hemosiderosis (bleeding in the lung)\nDisorders related to body-wide processes\n- Storage Disease\n- Langerhans Cell Histiocytosis\n- Malignant Infiltrates\n- Immune-mediated Disorders\nDisorders associated with a weakened immune system\n- Opportunistic infections\n- Disorders related to therapies like radiation and chemotherapy\n- Disorders related to solid organ, lung and bone marrow transplantation and rejection syndromes\n- Diffuse Alveolar Damage (DAD) of undetermined cause\n- Lymphoid infiltrates related to immune compromise (for non-transplanted patients)\nDisorders that resemble interstitial disease\n- Arterial hypertensive vasculopathy\n- Congestive vasculopathy including veno-occlusive disease\n- Lymphatic disorders, including lymphangiectasis and lymphangiomatosis\n- Pulmonary edema\nOur advanced medical equipment and state-of-the art hospital provide for diagnostic tests including:\n- Infant pulmonary function testing (.pdf)\n- Bronchoscopy (.pdf)\n- Advanced CT scan protocols that limit radiation\n- Combined procedures to reduce anesthesia exposures\n- New blood tests to understand the disease and treatments\n- Chest imaging (high-resolution CT scan)\n- Lung biopsies done by one of the most experienced teams in the country\nMany different types of medications may be used to treat chILD. These medications are frequently delivered by a device to deposit the medication in your lungs."", 'Interstitial lung disease (ILD) is a general category that includes many different lung conditions. All interstitial lung diseases affect the interstitium, a part of the lung’s anatomic structure. The interstitium is a lace-like network of tissues that extends throughout both lungs. The interstitium provides support to the lung’s microscopic air sacs (alveoli). Tiny blood vessels travel through the interstitium, allowing gas exchange between blood and the air in the lungs. The disease affects the lungs in three ways. First, the lung tissue is damaged. Second, the interstitium becomes inflamed. Finally, fibrosis (scarring) begins in the alveoli (air sacs) and interstitium, and the lung becomes stiff, making it difficult to breathe. Interstitial lung disease treatment includes a diverse collection of illnesses with a variety of causes, treatments, and prognoses. Individuals who suffer from interstitial lung disease often have difficulty breathing and moving from place to place due to a consistent feeling of shortness of breath. Disorders classified under interstitial lung disease are often characterized by scarring of the lung’s delicate tissues and a dry, pestering cough. Interstitial lung disease is caused by environmental factors, removing these triggers can slow the progression of the disease and damage to the lungs.\nInterstitial Lung Disease Treatment Market: Dynamics\nRegistries of the epidemiology of different interstitial lung disease have been compiled in several countries. However, they remain scarce due to the difficulties that arise in obtaining a specific diagnosis as many of these conditions are rare. More than 250,000 people have been diagnosed with interstitial lung disease in the United States, and nearly five million people have been diagnosed worldwide. It is estimated that more than 41,000 patients die each year from interstitial lung disease worldwide. The outlook for patients with interstitial lung disease varies depending on the underlying cause of the disease. There is no absolute cure for interstitial lung disease available as yet. A large portion of the treatment is directed to provide temporary relieve to the patients. Failure to provide a complete cure and limited treatment options are considered as major setbacks of the market. Limited drug options with low efficiency are further hindering the growth of the global interstitial lung disease treatment market. UCSF has an active research program conducting both clinical and basic research in the pathogenesis of lung fibrosis. Through the generosity of patients, who collaborate with world-class physician scientists, they have developed a better understanding of the fundamental, cellular and molecular mechanisms of interstitial lung disease. The clinical research involves both observational studies and clinical trials that use the knowledge gained in basic research to study the benefits of novel treatments in patients with lung fibrosis.\nRequest For Report Sample@ http://www.futuremarketinsights.com/reports/sample/rep-gb-3261\nInterstitial Lung Disease Treatment Market: Segmentation\nOn the basis of treatment, the interstitial lung disease treatment market can be segmented into:\n- Cytotoxic Drugs\n- Lung Transplant\n- Oxygen Therapy\n- Pulmonary Rehabilitation\nOn the basis of end user, the interstitial lung disease treatment market can be segmented into:\n- Rehab Centers\nAntibiotics may be given if a bacterial infection is responsible for the condition. Management of the underlying disease process is also essential in a person with interstitial lung disease related to these conditions. If the disease is thought to involve hypersensitivity, then the best therapy is avoidance of this offending substance. Depending on the severity of the symptoms, respiratory support, including supplemental oxygen and even ventilator support, may be required. Respiratory therapists may be part of a treatment team with the goal of improving day to day function. Smoking cessation is an essential part of the treatment for anyone with interstitial lung disease.\nInterstitial Lung Disease Treatment Market: Region-wise Outlook\nGeographically, the market is segmented into North America, Eastern Europe, Western Europe, Asia-Pacific excluding Japan, Japan Latin America, and the Middle East and Africa in which North America holds a major share of the market for interstitial lung disease treatment followed by Europe and Asia-Pacific. However, the increasing incidence of lung disease, coupled with unmet medical needs, is likely to make way for the adoption of interstitial lung disease treatments. The rise in government initiatives to increase awareness regarding lung disorders among consumers in this region is also projected to propel the Asia Pacific market for interstitial lung disease treatment. Countries such as the U.S., France, Britain, Germany and Canada, have accounted for a healthy demand for interstitial lung disease treatment attributed to the increasing number of incidence of the disease in these countries.\nVisit For TOC@ http://www.futuremarketinsights.com/toc/rep-gb-3261\nInterstitial Lung Disease Treatment Market: Key Players\nSome of the hospitals and clinics providing treatment for interstitial lung disease treatment are Cleveland, Barnes-Jewish Hospital, Brigham and Women’s Hospital, Massachusetts General Hospital, Johns Hopkins Hospital, Stanford Health Care, and UCSF Medical Center. Over the past decade, the definition of interstitial lung disease and thus enrollment criteria for this disease have become more specific. However, study design remains a challenge as there is continued debate on what constitutes a clinically meaningful endpoint.']"	['<urn:uuid:704638d3-b7b0-481a-952b-19e3bd843409>', '<urn:uuid:ccbf9714-ea32-4046-9679-8179df36f870>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	7	49	1469
12	Which came first: the first oil well in China or the discovery of oil in Kazakhstan's Dossor field?	The first oil wells in China were drilled much earlier, in 347 AD, reaching depths of 800 feet and using bamboo pipelines for lighting and heating purposes. In contrast, the Dossor field in Kazakhstan was discovered much later, with its first significant oil gush occurring in 1911 from well No. 3 at a depth of 225-226 meters.	"['110 years of the Dossor field\nOn April 29, it was 110 years since a powerful gush of oil was produced from well No. 3 at the Dossor field, from a depth of 225-226 meters of Middle Jurassic sediments.\nThis event was preceded by almost two hundred years of research on the territory of present-day Kazakhstan, which began intensively after the annexation of the Kazakh lands to the Russian Empire in 1731. During this period, government decrees included researchers among the participants of military expeditions to Central Asia, who were assigned the task of studying the territories along the route of the campaigns, with the preparation of detailed reports on minerals. The researchers described that the main occupation of the Kazakhs of that time was nomadic or pasture cattle breeding.\nTechnologist D. Kirpichnikov in 1874 in the ""Description of salt lakes located near the mouth of the Emba River and oil fields to the north-eastern shore of the Caspian Sea"" described the area of Karashungul as follows «…This place is called aulie by the Kyrgyz, a holy place, as a result of which a large number of graveyards are located near the lake. The local population uses the oil flowing from the southern shore as medicine for livestock. It is smeared on cattle when skin diseases appear. For oil production, the Kyrgyz have dug several pits, in which oil accumulates. Over one day, about one bucket of oil accumulates in these pits, which floats in an oily layer above the water that protrudes into the same pits.""\nThe result of numerous expeditions was the appearance of enterprising people in the area in 1892, who made the first applications for oil exploration on Dossor and then, in 1894, on Karashungul. On November 13, 1899, in the area of Karashungul, the first gush of oil on the Kazakh land was obtained. But the oil-bearing Ural-Emba district attracted global attention after the discovery of Dossor. The historic well No. 3 gushed for 30 hours and yielded 16,700 poods of oil. Its quality in chemical composition was superior to that produced in Baku.\nThe application rush began. Petitions for the issuance of application certificates for exploration within the Guryevsky, Temirsky, and Lbishchensky districts of the Ural region started to arrive in abundance in the Samara-Ural Department of Agriculture. If in 1911 only 121 certificates were sold, in 1912 already 1,788 and 2,564 in 1913. As a result of the vigorous application activity, application confusion began in the area, which necessitated the creation of a whole staff of government surveyors with a mining engineer at the head.\nFrom this moment, a new period of exploration work in the trans-Ural steppes begins. Drilling operations after 1911 began to be carried out at a more intensive pace, with the greatest attention being paid to Dossor, which in 1913 had already yielded 117.6 thous. tons of oil.\nOn April 10, 1912, a state act created the Emba oil-producing and trading joint-stock company. On May 12 of the same year, the first general meeting of shareholders is held in St. Petersburg, where the company\'s management is created. From this day, the oil and trade joint-stock company officially begins to work. During this period, the London Stock Exchange issues shares designated to stimulate oil production in Kazakhstan\'s production fields.\nA few years after discovering the Dossor oil field (1911), in 1915, the Makat field, discovered by Nobel company, produces the first oil. The latter event led to the fact that at the turn of 1914-1915, oil production at these two fields for the first time exceeds 200,000 tons.\nDossor after the Civil War\nThe revolution and the ensuing civil war slowed the development of the Emba oil fields. Realizing the importance of oil for the new state\'s economy, the army fought fierce battles to capture the estuaries of the Urals and the Emba. It was necessary to restore production in the oil fields, which were destroyed and partially burned, in the most challenging conditions of shortage of food, freshwater, and qualified specialists. The epidemic was rampant in the fields, which resulted in a substantial turnover among the working staff.\nReuven Friedman, the first Head of Emba, the Chairman of the Oil Fields Department of the Ural-Emba district, had to solve a whole complex of difficult tasks during these years – to restore the fields, build infrastructure, provide production with transport and fodder for horses, and solve the problems of everyday life of oil workers. From the report letter of the first Head of Emba, the Supervisor of the Ural-Emba district oilfields, Friedman, to the management in August 1920 (stylistics and spelling are preserved):\n""...The Dossor field is located 120 versts from Guryev. To Dossor, cargo is delivered along a dirt track on trucks, horses, and camels, water is prepared partly by the accumulation of snow in glaciers, partly by desalination of seawater received from the sea through an oil pipeline, and partly by delivery from wells located 30 versts from the field....\n...By my arrival, the Department did not have a single truck, not a single car, for lack of fodder, the cattle were sent off far into the steppe, and it is already impossible to keep them in the city because of horseflies and mosquitoes. Almost all the automobile property of the former oil companies was looted, there was only one military cargo squadron in the town, which from mid-May, on my order, began to serve the fields… At present, the maximum monthly carrying capacity of the cargo squadron is 1,000 poods, and the maintenance of the squadron costs the Department almost 200,000 rubles per month. As a result, the commercial food supplies have already been consumed, and from time to time, the workers have to starve for the inability to bring enough bread.\n...During the whole summer, nothing is given to the workers except bread, and before my arrival, they were given 18 pounds a month...With my arrival, I ordered them to set up a Red Army ration, however, due to the lack of bread in the city, only 25 pounds had to be given out…Poor nutrition, lack of greens, sugar, and fat affects the health of workers very hard, scurvy is rampant, anemia, general malaise, on this basis, a lot of sick days, constantly the control commission of doctors releases someone from work or transfers to other climatic conditions…\n...In the absence of some specialties, it is impossible to repair apartments in the fields, there is no glass for windows, no stoves, no cast-iron boxes for oil heating, no bricks, no water barrels, and no material for the production of these barrels. There are no drilling parties, there are no metal turners – in short, there is nothing to do the job...»\nThere was not a single issue that did not require the attention of the first Head R. Friedman.\n""...I ask for your request, comrade Friedman, to transfer the Dossor Soviet Hospital under control of the Oil Department, and I attach a list of estimates. With the start of regular work in the fields and the influx of new workers, it is desirable to increase the staff of doctors, add a dentist and one epidemic paramedic,"" the Head of the Dossor Soviet Hospital, doctor Zabrodin, wrote in a letter in August 1920.\nIn conditions where there was a shortage of workers in the fields, young teenagers were involved in the work, who were not allowed to work more than 4 hours, and the hours worked were counted as full-time.\nOil Emba increases production\nDespite the difficulties of these years, on 18.07.1919, the STC Presidium for the Oil Industry approved the Makat-Dossor oil pipeline project.\nIn 1920, for the industrial development of oil fields, the Ural-Emba district Oil Fields Department was created. In the same year, an operational drilling program is planned, within the framework of which it is planned to drill four meridian wells at the Dossor field.\nOil production at Dossor in 1920 from January to September increases six times – from 59,987 to 327,628 poods. If in the first months of 1920 the entire oil production volume was provided by oil free flow, by September, most of it is produced using compressors and borehole pumps.\nIn 1922, the Embaneft Trust was established to develop the Dossor and Makat fields. The production program for the fields of the Embaneft Trust for 1927-1928 provided for the gross production of the Dossor field of 199,599 tons, Makata – 77,870 tons, for the entire area, including gas production. As part of the deep-hole exploration drilling, it was planned to drill 40 wells, of which 21 were in new areas and 19 were in production areas.\nFrom the theses to the report on the Embaneft Trust at the industrial meeting on the deployment of large-scale industry in Kazakhstan: ""Emba oil differs from oils in its special oily qualities. Professor Gurvich once noted these special properties of oils in their oil fractions: low specific gravity and high flash at the same viscosity. Properties that bring Emba products closer to Pennsylvania products....The development of exploration activities in the Ural-Emba region should be given the highest priority, and the Embaneft Trust should be given the opportunity to purchase all the necessary technical equipment abroad to achieve the highest productivity at the lowest cost.""\nIn the second half of the 20s, rotary drilling was used at Emba, which contributed to the development of drilling operations and the growth of the depth of wells, the rate uncovering, and exploration of oil deposits.\nThe deepest well in Europe\nEmba oil workers were the first in the USSR and Europe to master ultra-deep drilling at Dossor and Makat at that time – up to 2500-2800 m. The deepest well No. 304 at 2,800 meters was drilled in Dossor in 1932.\n""The 304th drilling rig surpasses all the drilled wells in Europe in terms of its achieved depths, in addition, the drilling rig has solely technical importance for the Emba fields,"" follows from a letter from the Central Technical Department to the drilling office in Emba on December 9, 1932.\nIn the Bolshaya Emba corporate newspaper No. 120 dated December 5, 1932, an order was published for the Embaneft Trust, in which, in addition to problematic issues on the well, a list of measures to increase motivation for drillers of the 304th drilling rig was published, providing for a revision of the remuneration system in the direction of doubling the calculated coefficient for the entire batch, increasing food rations, issuing textiles, issuing sheepskin coats, boots, felt boots, gloves, underwear, and hats. Also, the Head of the Dossor field was obliged to allocate apartments, etc., for more needy workers of the drilling rig.\nOnly such measures at that time could stop the turnover of qualified personnel from drilling rigs, especially in the summer months, when infections were rampant in the fields. Despite these difficulties, in 1932, Emba drillers, in addition to the 304th, drilled well No. 307 to 962 meters in 92 days, discovered the powerful Koschagyl field, received an influx of light oil in the southern Iskin, uncovered the oil sands at Bis-Bulyuk, built drilling rigs without nails (on bolts), and without sufficient lumber, achieved a reduction in emergency drilling operations in relation to the total drilling time, and achieved many other impressive results.\n""Road of Life"" for oil workers\nOne of the biggest problems of Embaneft in 20-30s was the water supply of oil fields. The nearest distance from Dossor to the Ural River was 85 versts (1 verst is 1.0668 km), and the closest water wells were at a distance of 40 versts from Dossor. These wells of small depth did not serve for long because with the prolonged operation they gave salt water and could not be regarded as sources of water supply. Alternative natural water sources were atmospheric precipitation collected in constructed reservoirs from barrier dams and in winter in equipped ice cellars. The quality of the water was doubtful, of course, but still better than in the wells.\nThe summer of 1924 turned out to be hot and dry, all artificial reservoirs dried up, and the population of the fields – oil workers and their family members – had to drink salt well water, also issued limited, one bucket per day for each family member. Throughout the summer of 1924, until the frosts of 1925, water was delivered to the fields by four tank trucks of 150 buckets, two trips a day, and a camel train of 75 barrels of 30 buckets, one trip in two days. Moreover, the water supply was disrupted at the slightest damage to the road, as a result of which the fields were in a critical situation, under the threat of stopping oil production.\nThe Head of Embaneft at the time, Vladimir Korobovkin, who pushed the issue of building a narrow-gauge railway between Dossor and Guryev, convinced Moscow that a positive solution to this issue would radically solve the problem of water supply. In his report to the management, he argued that the unresolved issues such as transport and water supply worsen the living conditions in the fields, increase production cost, and disrupt the proper operation of the fields.\n""The Trust has decided to proceed with constructing the railway immediately, as soon as this issue is resolved in a positive sense by the SCNE and State Planning Committee. ...every field worker can go to Guryev cheaply and buy everything he needs. The railway will attract more skilled labor, which is now quite difficult to get due to difficult living conditions,"" Korobovkin notes.\nWork on the construction of the railway began in 1924. On December 19, 1926, temporary traffic on the Guryev-Dossor route was launch. The construction of the Guryev-Dossor railway, which ran through the arid desert, was an epoch in the development of the oil-bearing region and was necessary for further progress deep into the desert to expand exploration and operational work on new areas.\nFormation of domestic personnel\nIf at the first fields the Kazakhs acted as unskilled laborers, watchmen, and deliverers of goods, already in the 20-30s, as part of implementing the program of the indigenization of the workforce started, the picture began to change.\nIn 1928-1929, out of 371 employees of the Ural-Emba Department, the number of Kazakhs was 112 people, or 30.1%, of 2277 workers, 1041 were Kazakhs or 45.8%.\nFrom the report of the manager of oil fields R. Novosiltsev, deputy S. Sharipov, and the manager of affairs M. Melikov (stylistics and spelling are preserved):\n""...mining is the main business for the fields, work here for decades without interruptions, and therefore a Kazakh who has entered, say, drilling or oil production has the opportunity to work here for so long that in the process of work such a Kazakh qualifies. In addition, it is easier for a Kazakh steppe-dweller to enroll on a job for the first time in drilling or oil production. Of the large number of workers who have been in these branches of work, a large number of them remain in this business and willingly go to work. In addition, since drilling and operation of wells equally requires training from both Kazakhs and non-Kazakhs, if they are first assigned to work, it is, of course, preferable to attract Kazakhs from the local population to such work.""\nAlready by the 1940s, the ratio of workers in the fields is changing in favor of representatives of the indigenous nationality. Their professionalism is also growing. According to the summary table of the number of Stakhanovites for the Kazakhstanneftekombinat (formed in 1940 from the Embaneft and Aktyubneft trusts – author\'s note), for 182 Stakhanovites of the Dossor Department, there were 152 (83.5 %) employees of Kazakh nationality, in Makat – 242 Kazakhs-Stakhanovites (91 % of the total number of Stakhanovites), Iskine – 108 or 97 %, Koschagyl – 127 or 88 %, Baychunas – 213 or 97 %, etc.\nTo be continued...\nLaura Suleimenova, Head of the Public Relations Service of Embamunaigas JSC', 'The Evolution of Energy: From Torches to Solar Panels\nThroughout history, humans have discovered ways to take various energy sources and use them to their advantage. From the simple task of burning wood for heat, to the monstrous amount of power created from nuclear energy, we have been determined to find the most efficient and economical ways to make our lives easier. But how exactly did we go from primitive torches and fire pits to gushing oil wells and massive solar farms? It has been quite a remarkable journey, and it all started about 800,000 years ago. (Please note that the following dates may be approximations.)\n770,000 B.C. – Humans living in Israel made a miraculous discovery and began realizing the advantages of harnessing fire.\n1,000 B.C – Inhabitants of northeastern China began burning coal for heating and cooking purposes. Over the next several hundred years, coal became popular with other populations such as the Romans and Northern Native Americans.\n400 B.C. – Ancient Greeks and Romans built water wheels in streams and rivers to capture the energy to be used not only as a power source, but for irrigation as well.\n347 AD – China became the birth place of the earliest known oil wells. With depths of 800 feet, the Chinese drilled these wells using extensive bamboo pipelines and used the resource for lighting and heating.\n1,000 – The first windmills were built in Persia and used to pump water and grind grain. By 1300, windmills began to form the modern pinwheel shape in Western Europe, and in the 1590s, the Dutch greatly increased the size to become the most efficient version of the windmill.\n1600s – Europeans slowly began to replace wood with coal and the British discovered that cooking coal transforms it into hot-burning coke.\n1700s – Vast deposits of coal were discovered in North America, making intensive coal mining possible. Coal began to replace other energy sources such as wind and water, and quickly became a main source of energy around the world.\n1820s – The first natural gas well was drilled in Fredonia, NY. For most of the 19th century, natural gas was primarily used as a source of light. The lack of pipeline infrastructure made it challenging to deliver long distances.\n1830s – The electric generator, motor and relay were developed based off of Michael Faraday’s discovery of electromagnetism.\n1850s – The first petroleum rush occurred when the initial commercial oil well was drilled in Titusville, Pennsylvania. Canadian chemist Abraham Gesner discovered how to distill kerosene from petroleum, leading to the spread of kerosene lamps.\n1860s –Fearing that fossil fuels may eventually run out, Augustine Mouchot believed that burning coal could be replaced by the sun’s heat. He developed the first solar powered system to produce steam in order to operate industrial machinery.\n1882 – On September 4, 1882, Thomas Edison flicked the switch in New York that started up America’s very first power plant. That same year, one of the world’s first commercial hydroelectric plants went into operation in Appleton, Wisconsin.\n1892 – The first use of geothermal heat to power nearby buildings occurred in Boise, Idaho.\n1939 – The process of nuclear fission was discovered in Germany. Three years later, the first nuclear fission reactor was designed and built.\n1948 – The world’s largest petroleum deposit was found in the Ghawar oil field in Saudi Arabia.\n1950s – The first nuclear power plants began operating in Shippingport, Pennsylvania and Obninsk, USSR. Eventually, nuclear power will account for roughly 20 percent of all U.S. electricity.\n1970s – The production of U.S. oil began to decline which made dependence on imported oil intensify. Arab oil producers announced an embargo against the U.S., causing the price of a barrel of oil to quadruple.\n1973 – The Trans-Alaska Pipeline was built to transfer oil from Prudhoe Bay to Valdez. Environmental groups protested, concerned about the effects on the surrounding environment and the risk of potential accidents.\n1975 – The construction and operation of large commercial wind turbines began to have an impact on the energy industry. Setting world records for diameter and power output, the U.S. emphasized the potential for wind energy.\n1977 – The U.S. Department of Energy was formed.\n1978 – The world’s first solar powered village was constructed at the Papago Indian Reservation in Schuchuli, Arizona.\n1979 – A nuclear reactor accident at Three Mile Island near Middletown, Pennsylvania caused thousands of nearby residents to evacuate. Costing millions of dollars and years to clean up, the accident led to widespread opposition to nuclear power.\n1980s – The U.S. began to embrace renewable energy, building monumental wind farms in New Hampshire and California, as well as the first large scale solar-thermal power plant in Daggett, California. At the end of the decade, the Exxon Valdez disaster in Alaska became the biggest oil spill in U.S. waters.\n1990s – The U.S. began to import more petroleum than it processed, and hydrogen power began to emerge as Congress passed an Act that encouraged research, development and demonstration.\n1997 – General Motors released over 1,000 electric cars (the EV1).\n2000s – As the U.S. made significant efforts to support hydrogen fuel development, wind energy, solar power, biofuels, and energy grid upgrades, a number of catastrophic events transpired. Climate change, the coal ash spill in Tennessee, the BP oil spill in the Gulf of Mexico, and the Fukushima nuclear crisis in Japan all raised concerns about where the world is headed in terms of energy.\nFrom the discovery of coal to the implementation of solar panels, inventors and explorers have led the energy industry to where it is today. Although these innovations sometimes come with damaging effects, we must continue to improve our energy sources and work to develop the safest and most resourceful technologies. After all, where would we be without those curious individuals who insist on taking it one step further?\nEnergy Curtailment Specialists, Inc.\n- Demand Response\n- Energy politics\n- Energy Today\n- Fossil Fuels\n- Natural Power']"	['<urn:uuid:4aff5668-674e-459d-a086-28b86770432a>', '<urn:uuid:d7563d94-e1c1-45e4-a8e8-2f25141fbb8e>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T00:43:36.053682	18	57	3675
13	woodworking shop dust safety tips cost	For woodworking safety, work areas should be well-lit and organized with no spills or tripping hazards. Air safety is crucial - you should work in areas with good ventilation when using products that produce air pollutants. For dust control specifically, while large shops spend thousands on ventilation, you can build a homemade sawdust-control system for about $100 total ($60 for materials plus $40 for a Shop Vac), which removes 75-85% of generated sawdust.	"['People respect those who can work with wood. No matter who you are or what your skills are like, you can find a project which fits your needs. This article provides information to help you improve your skills and take your woodworking projects to a higher level.\nUse stop blocks to your advantage for more accurate work. When cutting a lot of pieces the same size, no matter how carefully you measure, you often find small variations. Use a pre-cut stop block to make those measurements pinpoint accurate. Then the table saw will stop at just the right length length.\nEven if you have been doing woodworking for some time you may want to consider taking a class. There are different levels of classes available, so you should be able to find one that teaches you something and is challenging. There is never too much that you can know about woodworking.\nAlways put your safety first when it comes to using tools, stains and paints. The more variety you use, the more specifics there are about applications, tool use, times for drying, and more. Put your safety first by reading the instructions, wearing any necessary protective gear, and working in areas with good ventilation whenever using oil-based products or anything that produces air pollutants.\nMake sure your work area is safe, well-lit and organized. Working with woods is difficult work, and it is dangerous work when your work area is dim and there are safety hazards in the area. Make sure there are no spills, tripping hazards and other safety hazards that are a disaster waiting to happen.\nIf you have your own table saw, you will eventually have to crosscut some wood. This can bring your fingers very close to the saw guard and blade. If the saw kicks back, you could be headed to the hospital. It is best to cut a block of wood to push the wood through the saw with.\nAlways remember the rule of measuring twice and cutting once. You are human, which means that mistakes are inevitable. However, you can prevent measurement mistakes from turning into cutting mistakes when you measure two or three times. That is going to spare you serious waste, not just in terms of wasted wood, but also money and time.\nWhen deciding on a woodworking project, make sure you know what tools will be required to finish the project. If you are missing a tool, you may have to stop in the middle of the project and never finish. Many tools are available for rent at home improvement stores, but some you may have to purchase. This purchase could bring you well above your budget.\nOrganize your woodworking space. There are few things more irritating than looking for a tool when you need it, and not making any headway in the process. Missing and misplaced tools can lead to major project delays. Keeping organized is a big help. In fact, pulling out the tools that you’ll need before a project is a smart thing to do.\nIt’s not surprising that woodworking has been around so long. A need for skilled woodworkers has always been there. No matter if you are a casual hobbyist or a woodworking fiend, the tips you learned here can help to improve your skills.', ""Learn how to build this low-cost homemade sawdust-control system and saw table.\nIf you've ever been inside a state-of-the-art woodworking shop, you've probably noticed an unusual lack of airborne sawdust. That's because the large shops spend thousands of dollars on elaborate ventilation systems that remove sawdust from the indoor environment. With clean air filtered in and fine dust particles filtered out, these systems allow workers to breathe easier knowing that there's one less on-the-job health risk. For those of us who work at home or in the garage, it just hasn't been practical to install our own ventilation system . . . until now.\nWhile it may not be economically feasible to install a sawdust-control system for the entire garage, we can assemble a self-contained homemade sawdust-control system for one of the greatest contributors of sawdust in the shop: the table saw. And while prices will vary at lumberyards around the country, $60 should be enough for the wood, hardware, glue and paint needed for this project, give or take a few bucks. Throw in a five-gallon, 1.5-horsepower Shop Vac® for around $40, and a dust-free garage is easily worth the price.\nThe Table Top and Base\nThe saw table is 34 1/2 inches tall. It has outslanting legs with skids connecting each pair on their lower ends. The table surface measures 25 inches by 30 inches with an 11 inch-square drop hole cut in its center. The sawdust evacuation system consists of a boxlike sawdust collector attached to the underside of the table directly under the drop hole, with its outlet spout connected to a 2-horsepower shop vacuum cleaner that draws the sawdust out of it.\nBoth the saw table and sawdust collector are constructed mostly of wood, and the whole structure is mounted on skids to make it movable yet still stable. The legs, braces, sawdust collector and saw are also detachable for easy storage.\nAfter you cut the 25 inches by 30 inches saw table top from a 1 inch-thick sheet of marine grade plywood, round its corners and cut an 11 inch-square drop hole in its center, you'll want to build a base for it. This base acts as additional support and provides sturdy mountings for the legs.\nThe front and rear faces of the base run almost the length of the table at 24 1/4 inches long — the last 5 inches on both ends of these faces should be strengthened (or widened) by adding 2 inches by 5 inches pieces of Douglas fir and expanding the width of the ends to 5 1/2 inches. Reinforce by backing each of them with pieces of 1 inches-thick pine measuring 5 inches by 5 1/2 inches. These expanded and reinforced ends can be assembled using 1 1/2 inches long #5, flat-headed wood screws and 3 inch-long, 2d box nails.\nAngle the entire length of the base by sawing out a 1/2 inch-wide triangular section so that when installed under the tabletop it will give an outward slope of about 15 degrees. This gives the legs a wider stance once they're attached, and makes the table more stable when you're feeding material backward and forward through the saw. Secure these canted faces to the underside of the table with 12 3 inch-long, #8, flat-headed wood screws (six per face) and carpenter's glue.\nCut the two sides of the base to 21 3/4 inches lengths from 2x4 Douglas fir stud stock and attach to the underside of the 25 inches width of the table top at a 90 degrees angle using 3 inch-long, #8, flat-headed wood screws (four per side) and carpenter's glue. Also, attach the sides to the front and rear faces of the base with 3 inch-long, 3/16 inches lag screws (two for each end).\nLegs, Braces and Skids\nThe leg, brace and skid assembly consists of four legs (each 33 1/2 inches long), four braces (one for each side, front and rear) and two skids. The front and rear braces are both 24 1/2 inches long and are permanently attached to the legs with 2 1/2 inch-long 3/8 inches lag screws (four per brace), resulting in two pairs of legs and braces. As for the skids, they measure 32 1/2 inches long and are attached to the bottom ends of each pair of front and rear legs using 3 inch-long, 8d finishing nails.\nJoin the legs and side braces with carriage bolts, wing nuts and cap screws so they can be easily dismantled without using a wrench. Attach the legs to their mounting points on the saw table base with removable 4 inch-long, 1/4 inches cap screws, wing nuts and flat washers (four per pair of legs). All of the cap screws should be 3/8 inches in diameter with sleeves. Then angle the ends of the side braces to match the Outward slant of the legs (lengths on the top and bottom surfaces should be 25 3/4 inches and 27 1/2 inches, respectively) and attach the side braces to the legs on bolts projecting from the brace supports.\nTo permanently mount the brace supports on the backs of the legs (one per leg), use two 3 1/2 inch-long, 3/16 inch diameter lag screws for each support. If you cut the braces at an angle to match the outward slope of the legs, their upper and lower edge lengths should be 3 inches and 4 1/2 inches, respectively. Fit the. supports with two 3 1/2 inch-long, 3/8 inch-diameter carriage bolts inserted from behind through holes and sealed with epoxy cement. Mount the side braces by sliding them into place through their 7/16 inch-diameter, sleeve-lined mounting holes and hold in place with wing nuts and flat washers. The sleeves used to line the mounting holes come from brass shim stock. Simply shape the brass around the appropriate diameter bolts (either 3/16 inches or 3/8 inches diameter) and secure their respective holes with epoxy cement. Substitute copper tubing if needed.\nThe Funnel Dust Collector\nWith the exception of its outlet spout, the sawdust collector is entirely constructed of wood and shaped to funnel sawdust down into the Shop Vac® beneath the table. The entire unit consists of five pieces and two cleats. Join the four sides — cut from 1/2 inches plywood — with 2d finishing nails and carpenter's glue and secure parts labeled Al and A2 inside of B1 and B2. Pieces A1 and A2 should measure 8 3/8 inches long and angle (at 45 degrees) from a 12 1/2 inches width at one end to a 1 1/8 inches width on the other. BI and B2 should measure 8/16 inches long and angle (at 45 degrees) from a 12 1/2 inches width to 2 1/2 degrees (see illustration in the image gallery). Attach the cleats — cut from 1 by 6 pine stock — to the upper outside of B1 and B2. To accommodate your Shop Vac® hose fitting, bore an appropriately sized hole in a 1 inches plywood plug measuring 2 1/2 inches square. Be sure to attach the plug to the construction and let the glue dry before boring your hole.\nThe next step — finding a tube that has a 1 1/8 inches outside diameter (the size required for a spout that would fit snugly inside the intake hose on a Shop Vac®)proved to be a challenge when I was building my own sawdust collector. I ended up using two old medical prescription containers that could be made into tubes by cutting the ends off. One had an outside diameter of exactly 1 1/8 inches, and the other one fit tightly inside the first. After epoxying the two together I finished with a very strong spout. If you don't do it this way, make sure to leave 1 1/2 inches of the tube projecting outside.\nWhen you're finished, attach the sawdust collector to the underside of the saw table with 2 inches-long, 1/2 inch-diameter carriage bolts that are epoxied in place to the saw table (this is the same technique used in setting carriage bolts in the side brace supports on the table). Attach the carriage bolts to the sawdust collector by thrusting their ends through the holes in the collector's 1 inches lip that is formed by the 1 by 6 pine cleats. The sawdust collector should be positioned directly under the drop hole in the tabletop.\nI have used this saw table with its sawdust evacuation system for a little over a year and found it to be quite satisfactory for my purposes. After some research (weighing sawdust collected in the Shop Vac® and swept up from the floor outside the system), I concluded that it removed between 75% and 85% of the sawdust generated by the saw.\nWant more do-it-yourself project, and ideas? www.motherearthnews.com.\nSaw Table and Sawdust-Control System Materials List\n(5) 8 foot-long Douglas fir 2 by 4 studs\n(1) 5 foot length of 1 inches by 6 inches pine (finished 3/4 by 5 1/2 inches)\n(1) 30 inches by 25 inches piece of 1 inches-thick marine-grade plywood\n(1) 11 inches by 10 3/4 inches piece of 1 inches-thick marine-grade plywood\nCap screws: (8) 1/4 inches by 4 1/4 inches each with a wing nut and 2 flat washers\nCarriage bolts: (8) 3/8 inches by 3 1/2 inches each with a wing nut and 2 flat washers; (4) 1/4 inches by 2 inches each with a wing nut and 2 flat washers; (4) 1/4 inches by 1 1/4 inches each with a wing nut and 2 flat washers\nLag screws: (8) 3/8 inches by 1 1/2; (8) 3/16 inches by 3 1/2 inches\nFlat-headed wood screws: (16) #5 by 1 1/2 inches; (8) # 8 by 3 inches\nNails: (16) 2d, 3 inches box nails; (8) 8d, 3 inches box nails; (24) 2d, 1 1/2 inches finishing nails\n(1) 7 5/8 oz. bottle of Elmer's Carpenter's Glue or equivalent\n(1) set of epoxy cement (preferably 2-ton strength)\n(1) pint of gloss enamel\n(1) quart of latex flat paint\nShim Stock (or Copper Tubing)\n24 sq. inches (or 2 feet of tubing)\n*Note: Copper tubing can only be substituted if available in suitable sizes.""]"	['<urn:uuid:42f882f7-c94d-44a4-af47-d436baf72e90>', '<urn:uuid:792fcf38-78ad-42f1-b79d-58ddc4ac3e76>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-13T00:43:36.053682	6	73	2243
14	beginner computer user here when computer programs stop working how do they affect safety and privacy concerns	When computer programs crash, they can have significant privacy and security implications. The crash may contain sensitive user information that could be exposed. Additionally, many software bugs that cause crashes can be exploited by malicious actors to execute unauthorized code or gain elevated system privileges. For example, certain types of crashes like buffer overflows can be exploited to allow replication of viruses or access to normally inaccessible data. Hackers can take advantage of these vulnerabilities through various means, including malicious code delivery through email attachments or compromised networks, potentially leading to data breaches, privacy violations, and financial losses.	"['In computing, a crash, or system crash, occurs when a computer program such as a software application or an operating system stops functioning properly and exits. The program responsible may appear to hang until a crash reporting service reports the crash and any details relating to it. If the program is a critical part of the operating system, the entire system may crash or hang, often resulting in a kernel panic or fatal system error.\nMost crashes are the result of executing invalid machine instructions. Typical causes include incorrect address values in the program counter, buffer overflow, overwriting a portion of the affected program code due to an earlier bug, accessing invalid memory addresses, using an illegal opcode or triggering an unhandled exception. The original software bug that started this chain of events is typically considered to be the cause of the crash, which is discovered through the process of debugging. The original bug can be far removed from the code that actually crashed.\nIn earlier personal computers, attempting to write data to hardware addresses outside the system\'s main memory could cause hardware damage. Some crashes are exploitable and let a malicious program or hacker to execute arbitrary code allowing the replication of viruses or the acquisition of data which would normally be inaccessible.\nAn application typically crashes when it performs an operation that is not allowed by the operating system. The operating system then triggers an exception or signal in the application. Unix applications traditionally responded to the signal by dumping core. Most Windows and Unix GUI applications respond by displaying a dialogue box (such as the one shown to the right) with the option to attach a debugger if one is installed. Some applications attempt to recover from the error and continue running instead of exiting.\nTypical errors that result in application crashes include:\n- attempting to read or write memory that is not allocated for reading or writing by that application (segmentation fault) or x86 specific (general protection fault)\n- attempting to execute privileged or invalid instructions\n- attempting to perform I/O operations on hardware devices to which it does not have permission to access\n- passing invalid arguments to system calls\n- attempting to access other system resources to which the application does not have permission to access\n- attempting to execute machine instructions with bad arguments (depending on CPU architecture): divide by zero, operations on denormal number or NaN (not a number) values, memory access to unaligned addresses, etc.\nCrash to desktop\nA ""crash to desktop"" is said to occur when a program (commonly a video game) unexpectedly quits, abruptly taking the user back to the desktop. Usually, the term is applied only to crashes where no error is displayed, hence all the user sees as a result of the crash is the desktop. Many times there is no apparent action that causes a crash to desktop. During normal function, the program may freeze for a shorter period of time, and then close by itself. Also during normal function, the program may become a black screen and play the last few seconds of sound (depending on the size of the data buffer) that was being played repeatedly before it crashes to desktop. Other times it may appear to be triggered by a certain action, such as loading an area.\nCrash to desktop bugs are considered particularly problematic for users. Since they frequently display no error message, it can be very difficult to track down the source of the problem, especially if the times they occur and the actions taking place right before the crash do not appear to have any pattern or common ground. One way to track down the source of the problem for games is to run them in windowed-mode. Windows Vista has a feature that can help track down the cause of a CTD problem when it occurs on any program. Windows XP included a similar feature as well.\nWeb server crashes\nThe software running the web server behind a website may crash, rendering it inaccessible entirely or providing only an error message instead of normal content.\nOperating system crashes\nAn operating system crash commonly occurs when a hardware exception occurs that cannot be handled. Operating system crashes can also occur when internal sanity-checking logic within the operating system detects that the operating system has lost its internal self-consistency.\nSome operating systems, e.g., z/OS, have facilities for Reliability, availability and serviceability (RAS) and the OS can recover from the crash of a critical component, whether due to hardware failure, e.g., uncorrectable ECC error, or to software failure, e.g., a reference to an unassigned page.\nSecurity and privacy implications of crashes\nDepending on the application, the crash may contain the user\'s sensitive and private information. Moreover, many software bugs which cause crashes are also exploitable for arbitrary code execution and other types of privilege escalation. For example, a stack buffer overflow can overwrite the return address of a subroutine with an invalid value, which will cause a segmentation fault when the subroutine returns. However, if an exploit overwrites the return address with a valid value, the code in that address will be executed.\nWhen crashes are collected in the field using a crash reporter, the next step for developers is to be able to reproduce them locally. For this, several techniques exist: STAR uses symbolic execution, MuCrash mutates the test code of the application that has crashed, and EvoCrash performs evolutionary search.\n- ""Crashing Privacy: An Autopsy of a Web Browser\'s Leaked Crash Reports"". Retrieved 28 September 2018.\n- ""Analyze Crashes to Find Security Vulnerabilities in Your Apps"". Msdn.microsoft.com. 26 April 2007. Retrieved 26 June 2014.\n- ""Jesse Ruderman » Memory safety bugs in C++ code"". Squarefree.com. 1 November 2006. Retrieved 26 June 2014.\n- Chen, Ning; Kim, Sunghun (2015). ""STAR: Stack Trace Based Automatic Crash Reproduction via Symbolic Execution"". IEEE Transactions on Software Engineering. 41 (2): 198–220. doi:10.1109/TSE.2014.2363469. ISSN 0098-5589.\n- Xuan, Jifeng; Xie, Xiaoyuan; Monperrus, Martin (2015). ""Crash reproduction via test case mutation: let existing test cases help"": 910–913. doi:10.1145/2786805.2803206. Cite journal requires\n- Soltani, Mozhan; Panichella, Annibale; van Deursen, Arie (2017). ""A Guided Genetic Algorithm for Automated Crash Reproduction"": 209–220. doi:10.1109/ICSE.2017.27. Cite journal requires\n|Wikimedia Commons has media related to Computer errors.|', 'In the realm of cybersecurity, exploits are tools or techniques that hackers use to take advantage of software or hardware vulnerabilities. These vulnerabilities are essentially weaknesses in a system’s defenses, which can be exploited to gain unauthorized access, manipulate data, or even take control of a target system. In this blog post, we will delve into the world of exploits, shedding light on what they are, how they work, and the importance of staying vigilant to protect against them.\nWhat is an Exploit?\nAn exploit is a piece of software, code, or technique designed to take advantage of a specific vulnerability in a computer system. Vulnerabilities can be found in various forms, including:\n- Software Bugs: Software is developed by humans and is rarely without flaws. Bugs and coding errors can create openings for exploits. These include buffer overflows, privilege escalation, and more.\n- Misconfigurations: System administrators may overlook certain security settings or misconfigure software, which can lead to vulnerabilities.\n- Weak Passwords: Weak, easily guessable passwords can be exploited through brute force attacks or password cracking.\n- Outdated Software: Software that has not been updated or patched is often vulnerable to known exploits. Hackers can leverage these known weaknesses.\n- Zero-Day Vulnerabilities: These are vulnerabilities that are not publicly known, giving hackers a unique opportunity to exploit them before they are patched by the software developer.\nHow Do Exploits Work?\nExploits are typically used by attackers to gain unauthorized access to a system, escalate privileges, or perform other malicious actions. The process generally involves the following steps:\n- Identification: Hackers identify a vulnerability in a target system or software. This can be done through security research, reverse engineering, or even simply by stumbling upon a weakness.\n- Exploitation: Once a vulnerability is identified, an exploit is developed or obtained to take advantage of it. This may involve crafting malicious code that triggers the vulnerability or using pre-existing exploit code.\n- Delivery: The exploit code needs to be delivered to the target system. This can be done through various means, such as email attachments, malicious websites, or compromised networks.\n- Execution: When the exploit code is executed on the target system, it triggers the vulnerability, allowing the attacker to gain unauthorized access or perform other malicious actions.\n- Post-Exploitation: After gaining access, the attacker may continue their actions, such as exfiltrating data, installing malware, or maintaining persistence within the compromised system.\nThe Importance of Staying Vigilant\nUnderstanding exploits is essential for both individuals and organizations. Here are a few reasons why vigilance is crucial:\n- Data Security: Exploits can lead to data breaches, compromising sensitive information. Understanding how exploits work can help protect your personal and business data.\n- Privacy: Unauthorized access to devices or networks can result in the violation of personal and corporate privacy. Vigilance can help prevent such breaches.\n- Financial Loss: Cyberattacks can lead to significant financial losses through fraud, theft, and costly remediation efforts.\n- Reputational Damage: A successful exploit can tarnish an individual’s or organization’s reputation, causing long-term damage.\nExploits are a constant threat in the digital age, and understanding them is the first step in defending against them. Staying updated with security best practices, regularly patching software, and implementing strong password policies are all essential steps in minimizing the risk of falling victim to exploits. In a world where cyber threats are ever-evolving, continuous vigilance is key to safeguarding your digital assets and privacy.']"	['<urn:uuid:c0370aec-0f34-4e2f-8e97-1d3b0e65bf4e>', '<urn:uuid:828747b5-c8b2-45cd-b9d3-e1e5f58ba2c1>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	17	98	1613
15	traditional artisan crafts preservation challenges economic viability how global market access and local community involvement impact survival heritage businesses	Traditional artisan crafts face dual challenges and opportunities in preserving heritage while maintaining economic viability. On the preservation side, successful approaches involve active community participation and capability building, as demonstrated by Otaru's heritage management and Lebanon's craft organizations. The economic dimension shows that while industrial development and cheap imports threaten traditional crafts, new opportunities emerge through global market access via online platforms and social media. Organizations like L'artisan du Liban and UNIDO help artisans adapt by providing technical support, design updates, and marketing assistance, enabling them to reach international markets while maintaining authentic cultural elements.	['Management of cultural heritage is an issue that is attracting increasing attention worldwide, as emphasis shifts from pure conservation issues to issues of future sustainability of heritage sites. Experience seems to suggest that it is difficult to preserve heritage sites without embedding them into a cultural vital social and economic environment, whereas at the same time there is the risk that, by putting excessive focus on the economic dimension, they are turned into theme parks with gradual loss of authenticity and progressive disintegration of their cultural and historical value. This paper tackles these issues through a comparative study of two Japanese heritage cities, Otaru and Yubari, both in the Hokkaido Island of the Japanese archipelago. The case study is of interest in that the two cities are representative of two opposite attitudes toward heritage management: in Otaru, the revaluation of local cultural assets has been conducted through the involvement of the local population and a socially inclusive approach, whereas in Yubari a top-down, financially driven approach has been developed in order to improve the city’s attractiveness for tourists. The results of such opposite strategies have gained international attention.\nThe Municipality of Yubari has gone bankrupt and its tourist attraction policy has been largely ineffective, whereas the bottom up approach of the City of Otaru has led to a rejuvenation of the city, that is now attracting tourists from abroad and has witnessed a considerable improvement in the local quality of life and in the sustainable use of local cultural heritage. Recent, successful new policy developments in Yubari to rescue the town from its deep crisis by means of direct dialog with citizens provide further evidence that active involvement of the local population makes the difference. This comparative analysis suggests that socially inclusive policies seem to be an essential component of viable heritage management models, and that failure to integrate such dimension into the strategic approach may be a fatal flaw.\nArticle by Guido Ferilli, Pier Luigi Sacco, Kunihiro Noda\nPublished in City, Culture and Society 6 (2015) 135e143\nFrom the Conclusions (excerpt)\n(…) The debates on the efficacy of culture as a powerful tool of local economic and social revitalization are increasingly frequent, especially in this period of global economic crisis and of obsolescence of traditional growth models and schemes. What the comparison between the Otaru and Yubari case studies tells us, is that it is not amatter of whether or not culture ‘works’. In order for culture to play its developmental role, it is fundamental that one properly understands the micro-mechanisms through which economic and social value added is produced e and such mechanisms have inherently to do with capability building, direct active participation of the local community, and collective learning processes.\nIf these aspects are overlooked, and culture and cultural tourism are merely thought as an industrial sector in a traditional sense and are managed accordingly, in terms of top-down sector policies, failure is most likely. But if the micro-social dimension of culture is properly understood and addressed, culture may become a very powerful developmental tool, not only despite the crisis, but to a large extent because of the crisis, as the Otaru case study powerfully illustrates. The crisis is a concrete occasion for the community to self-organize and leverage its full potential by designing creative responses to the current needs, thanks to its stock of human and social capabilities, provided that such capabilities have been cultivated and inclusively developed through time. This is a lesson that should be given all due attention in times like ours.(…).\nRead the whole article clicking here.\nMain Photo Credits: click here.', 'Carpets, cutlery, glass, soap, furniture—these traditional Lebanese crafts have a valued place in the country’s—and the region’s—history. Industrial development in the last half of the 20th century has, inevitably, affected Lebanon’s traditional artisans. One the one hand, it has driven demand down for artisanal crafts that are usually more expensive than mass-produced imports. On the other, for local artisans who have weathered weary economic waters, access to a new global market has been made easier, a result of improved online marketplaces, social media, and internet-based communication. There are also multiple initiatives underway in Lebanon that seek to ensure Lebanese artisans find their place in increasingly crowded local and global markets. For traditional artisan crafts in particular, local and international organizations have worked to improve their sustainability by providing technical, industrial, design, and marketing support, and by providing a place for craftsmen to market their goods.\nThe United Nations Industrial Development Organization (UNIDO) and L’artisan du Liban are two organizations that have helped keep Lebanese artisanry alive by providing production support, serving as design catalysts, and offering a place for local craftsmen to market their wares. Established in 1979, L’artisan du Liban was Lebanon’s first social enterprise, and it sought to keep artisans active and safekeep artistry and heritage by providing local craftspeople a marketplace for their goods. UNIDO has been active in Lebanon since 1989, and supports sustainable development across multiple sectors, including its work with craftspeople. Both organizations have played similar, but distinct, roles in sustaining local artisanry.\nDespite a strong history of design generation and export in Lebanon, recent economic stagnation has made it difficult for some traditional artisans to compete with cheaper imports from places like China. For example, where 10 years ago there were several glassblowers in Lebanon, today only one family, the Khalifehs in Sarafand, remain. The period between 2011 and 2018 marked a 37 percent increase in imported glass and glassware, according to data by Blominvest Bank, with which Lebanese glassblowers had to compete.\nArtisans across the handicraft spectrum have had to adjust to shifting market trends and find new ways to make their products attractive to consumers in a modern market. Driven, in part, by shifting market trends and demands, artisans have used several tactics to stay ahead of the game, from introducing subtle, more modern-looking design twists, to adopting new materials and packaging methods to make traditional goods more marketable.\nThe rapidity of changing trends, a clientele more aware of global trends and buying options, and the combination of rising ease of travel and digital advancements have accounted for the largest market shifts, says Hadi Maktabi, owner and curator of Hadi Maktabi carpets, who holds a PhD in Islamic Art from Oxford. Difficulty in identifying a unified “Lebanese taste” has also made marketing to a local audience challenging. And it is in this climate that producers and artisans must decide which model they want to adopt, whether that be identifying a niche within a larger market and catering to it, or following the “supermarket model,” which Maktabi defines as being largely mass-produced, cheaper wares made abroad that appeal to a broad audience (think IKEA).\n“Fifteen years ago, it was the supermarket model, and you had five to 10 big dealers who catered to everything,” Maktabi says, referring to the carpet industry. He argues that where trends in the 1990s shifted every 10 years or so, recently they have begun changing every two to five years.\nThe advent of the internet and improved digital marketplaces have sped up the introduction of new styles over the last 20 years. Rapidly growing global markets also meant the Lebanese market was flooded with more affordable, modern products that were designed in Europe, but were produced in places with cheap labor supply. “Most people working on this [supermarket model] side are dealing with products mass produced in China and India, and then selling them here,” Maktabi says. “But what they’re selling now is not craft, it’s just a product.”\nCarve out a niche\nNow, within the last few years, more have tried to carve out a niche in a crowded market—like Maktabi’s focus on antique carpets and textiles—specializing in providing a specific product. Complicating the matter, on the local front, he argues, is the rising European influence and the need for the Lebanese to find their place within that trend. Even local geographical considerations play a role in this. “Drive a few kilometers south of Beirut, and it’s like entering a different time period,” Maktabi says. Torn between the occidental and oriental, this clash of cultures has made designers and artisans alike, who choose to follow the niche market approach, have to define a narrow target audience.\nBut those specialists, specifically some traditional artisans, have needed help finding a viable market for their niche crafts. A few kilometers south of Beirut, Houssam Outabashi is found in Ouzai with multiple workshops lining the street. Here, Outabashi, a master in traditional marquetry and inlay techniques, can look at a piece of mother of pearl and name its country of origin by its color. Marquetry is a process by which small pieces of different types of wood are bundled to form a pattern, and then shaved in thin layers, while inlay design is a process in which chunks of wood are carved out and replaced with the shimmery pieces cut from sheets of Mother of Pearl to create intricate designs.\nOutabashi specializes in the traditional styles of his craft, however, he has started modernizing some of his designs. L’artisan du Liban has provided him with support to help preserve his craft, which goes back as far as the 1800s through generations of his family. Nour Najm, creative director of L’artisan du Liban, says they work with Outabashi, designing objects that Outabashi creates by hand and then are sold in L’artisan du Liban store.\nFurther south in Sarafand, L’artisan similarly works with the Khalifeh family who create glassware out of recycled glass—which in a country that has an excess of garbage, is remarkable. The Khalifeh’s only turn on the oven five to six times a year, but can turn it on up to 10 times when there is an order to be filled—otherwise it is a resource drain. When it is on, six to eight people work in shifts around the clock for 15-20 days to fill an order. Najm says L’artisan makes sure to place a large order with the Khalifeh’s each time they turn on the oven.\nIn the small, run-down warehouse where the Khalifeh family makes their glass, Najm is thinking about what she can do to give the glassware a modern twist—for her, the answer is color. With colored glass she bought from the US, the glassblowers are experimenting with different techniques to potentially incorporate color into their traditional designs. Najm says they have introduced a lot of small details to modernize traditional designs and help make them competitive in today’s markets. “Small twists change everything,” she says.\nReimagining the craft\nUNIDO has also worked with local craftsmen to help them update traditional designs and help artisans peddle their crafts. For example, UNIDO worked with Jezzine cutlery craftspeople—as well as local soap makers and tark el-fouda (embroidery) craftspeople—to help them modernize designs and industrialize production. Two years ago, UNIDO launched a program in partnership with the Ministry of Industry and funded by the Austrian government to help preserve traditional artisanry and improve livelihoods of artisans in these sectors, says Nada Barakat, national project coordinator at UNIDO.\nJezzine cutlery, for example, was once thought of as a gift that sat in a wooden box unused; the product had to be re-imagined into something people would by to use and enjoy. Barakat stressed the importance of marketing: they did away with the old wood box and started packaging the sets in cardboard, which cut down costs and made the sets more practical. To better market the soap, they did the opposite and introduced an attractive olive wooden box as packaging. Jezzine cutlery, which was traditionally made out of olive wood and featured bird-like motifs on the handles is now sometimes made out of resin, but maintains the older features with a modern edge. Creating the mold for the resin-based handles made the production process and end-product marginally cheaper, but consumers can still buy the cutlery with the traditional wooden handles as well.\nBarakat says that while the collection is primarily available to local markets, negotiations are underway with Coincasa, an Italian retail outlet, to market the collection there. L’artisan du Liban has a slightly larger global reach with their online store that opened last year. Najm says that less than 10 percent of their sales are global, but they have clients all over Europe and in the US, and they attend yearly trade fairs in Paris.\nBoth entities—UNIDO and L’artisan du Liban—have worked to keep Lebanese artisanry alive and are beginning to introduce traditional local crafts in international markets. Though industrial development made traditional crafts more expensive, recent globalizing trends and improved digital markets may help some local artisans find a viable market for their goods abroad. While it is too early to tell what the future holds for Lebanese crafts in the international market, at least here at home some local artisans have found the support needed to keep centuries’ old traditions alive.']	['<urn:uuid:9e107902-ed3e-4a78-8e9a-1880d1663e59>', '<urn:uuid:ffd2d814-8849-4e3e-851e-e1aee977602a>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	19	95	2155
16	What methods do train enthusiasts use to spot trains, and what crucial safety guidelines must they follow at railroad crossings?	Train enthusiasts can either write down train numbers in a notebook or take photographs/videos of trains. For photography, a decent camera with good night capability and zoom is recommended, while videography requires at least 1080p resolution. As for safety at railroad crossings, drivers must come to a complete stop at least 15 feet from the track when flashing red lights are activated, crossing gates are lowered, a flagman signals to stop, a stop sign is posted, or a train is clearly visible. Additionally, one should never stop on railroad tracks, and should look carefully in both directions before crossing, even during daylight hours.	['Guide to Getting Started with Train Spotting\nTrain spotting is a popular hobby. Just like any other, it can be done in your spare time and doesn’t have to take over your life. It’s a fantastic lifestyle choice though, because it is one which enables you to travel to lots of different places and see the very best of Great Britain, or even the world if you choose to.\nWhen we say “train spotting”, don’t feel limited to a book and pen, because many railway enthusiasts also spend their time photographing and videoing trains. Videos and photos can be shared online on personal portfolios such as YouTube or Flickr, or just developed and put into a personal collection or photo album. Some train spotters also have a seperate notebook and the intention of traveling behind as many classes of trains as possible, called “train bashing”.\nThere are many different types of train in the UK. Nowhere near all of them can be seen from your local railway station, and you’ll need to travel far and wide if you want to tick off / photograph / video all of the different classes of trains.\nThis handy guide that we have written will help you to get the most out of the hobby of train spotting.\nGetting Started with Train Spotting\nThe first thing you need to decide upon is what you want to spot / phot (spotting whilst photographing) trains, wagons, or both. Trains fall into several different types – these are: multiple units (electric, diesel, hybrid); locomotives (diesel, electric, steam); and on-track plant. Wagons and coaches also come in lots of different kinds, vehicle carrying wagons, petroleum tankers, shipping container flats, Network Rail maintenance wagons are just a small handful of wagons that can often be found on the UK rail network today – if you know where to see them.\nYou also need to decide on what method of spotting you’re going to take. Are you simply going to write down numbers of trains in a notebook, or are you going to take photographs of the trains to save as physical evidence of your sightings? Whichever route you take, you’ll need to be prepared.\nIf you’re going down the route of ticking off as many different classes of trains, and their subclasses and or numbers, it goes without saying that a book containing a comphrehensive list of all of today’s trains and their coaching stock formations, numbers and construction details would come in very handy. We recommend Platform 5 publications. Platform 5 books are updated yearly and contain a list of every number of every train class in operation on the UK rail network. You can view the Platform 5 book range, and lots of other railway books at rail-books.co.uk\nIf you’re going down the route of railway photography, you’ll need a decent camera. When buying a camera, make sure that it works well at night time, as some rare railway workings often run at night. A camera with a good zoom range is also handy. If you’re going down the route of videography and recording videos, you’ll want a camera that is at least 1080p. 1080p is Full HD resolution, and is good enough quality to capture all the details. Video cameras don’t always work well in the dark, so a high end mobile phone such as the Google Pixel 3 might be a good choice if your primarily going for night videos.\nHow to be a train spotter\nTo be a train spotter, you need to know what you’re spotting. Trains are classified into types called “classes”. Trains usually have a number on the front of them, or at the side near the front. The last 3 digits of the number on the front will be the train’s number, whilst any numbers appearing before those last 3 digits, are the train’s class. For example a train which has 377442 on the front of it would be a class 377, and the train’s number is 377442. The number first succeeding the class number is the train’s sub class. Train classing information is detailed below in coloured guide.\nThe red digits are the train’s class. The example above is a class 377. The blue digit is the train’s sub class often written as being a class 377/4. It is class 377, number 377442.\nThe above example is a class 09 locomotive. It is sub class 09/0. It’s number is 019 (09019).\nClasses 0 – 99 are diesel or electric locomotives.\nClasses 100 – 299 are diesel multiple units\nClasses 300 – 799 are electric multiple units\nClasses 800 onwards are hybrid units.\nTrack machines are not classed the same as locomotives and multiple units. Track machines and on track plant numbers are preceeded by “DR”. You can see all track machine numbers on our on track plant identification guide from the main menu under enthusiast resources, which lists most track machines and their numbers. The numbering system given to locomotives and multiple units in today’s railways is called TOPS. Steam locomotives are not referred to by their TOPS numbers in the railway enthusiast community, but rather their pre-TOPS numbers because they are easier to tell apart. Northern Ireland trains do not use the same numbering system as the UK railway, nor do their timetables show on Rail Record live train times, so train spotting in Northern Ireland adds a bit of a challenge.\nDifferent train operators have different classes of trains, so if you want to see every class of train, you’ll need to travel far and wide across the country.\nOne of the enjoyable things of trainspotting is liveries. Different train operating companies have their own liveries, some of which can be specially branded. Southern Railway’s class 313201 has been repainted into it’s original BR colours as a commemoration anniversary of its 20 years in service. Freight operating companies often paint their locomotives into their customer’s branding, for example 66783 has been painted into Biffa bins livery, Freightliner 66522 has been painted into a special Shanks group livery. Other locomotives are painted into special colours, such as 66789 (original British Rail large logo colours), 66720 (a children painted rainbow), 66746 Royal Scotsman livery, you just have to find them.\nIf you’re going to spot wagons, these are broken down in to 3 letter classes, not numerical classes. A lot of maintenance wagons are also nicknamed after sea animals. A comprehensive list of wagons and their fish names can be found on the LTSV website. Wagons are numbered, similarly to locomotives and multiple units.\nUK Train Schedules and Timetables\nRail Record provides a list of most train services in Great Britain (but not Northern Ireland). Live train times are available for most train services to help you when train spotting. Rail Record live train times gives you information about passenger, non passenger and freight train services, so it is a useful resource. Trains which run on the Great British railway network are allocated a 4 digit signalling ID, also referred to as a headcode. Headcodes are used to identify specific workings on the network.\nTrain schedules are either WTT, VAR, STP, VSTP, or CANX. WTT timetabled services are services which run regularly so a permanent entry is made in the working timetable. VAR services are variations of the WTT timetabled service, with timings or routing slightly altered to the requirements of other services. STP timetables are short term planned timetables, for movements such as diversions, short term freight services and engineering trains. VSTP schedules are very short term planned services, usually running within 48 hours of the timetable being made. CANX are cancelled trains.\nFinding particular trains and locomotives (train allocations)\nOne of the hardest, but most rewarding elements of train spotting is finding a locomotive, multiple unit or piece of rolling stock that you’ve required for a long time. Perhaps there is one locomotive or multiple unit that just keeps avoiding you, the last one of the train series that you need to tick off…\nLocomotive and train service allocations are a well guarded secret, because the internal train allocations systems aren’t publicly available. Rail Record live train times is testing a feature which enables train spotters to share their sightings of locomotives and multiple units by uploading their first hand information reports from what they’ve also seen on their travels, so other train spotters and railway enthusiasts can also benefit. Some websites such as WWRail Forum have a section dedicated to uploading TOPS lists, we do not offer this because we respect the privacy and anonymity of train operating companies workings.\nBelow, we have compiled a list of important websites and resources that will definitely help you in getting started and finding those popular locomotives.\nGroups.io / Yahoo! groups email list website\nWebsites such as groups.io and groups.yahoo.com have railway groups committed to sharing sightings and useful information about different things. It is well worth registering for a groups.io and yahoo groups account because its free. You’ll need to search for groups in your area. Information and sightings can be uploaded to the website by email. You’ll receive updates from the groups by email (can be disabled), as well as view the “gen” through the website. Below we have listed a few that we think you’ll find useful.\n- SE-Gen (groups.io) – a group dedicated to sightings in the South East of the UK, from Bedford down to Portsmouth and Kent. Includes information about RHTT allocations, rare movements on the mainline and heads up about future uncommon workings.\n- uk-rail (groups.io) – a UK wide sighting group, information similar to SE-Gen but is for whole of UK\n- WRGen – Rail information and sightings for the Wessex area\n- Midlands-Gen (Yahoo Groups) – Rail information and sightings for the Midlands area\n- Network-Rail-Gen (groups.io) provides occasional sighting information about test trains, often run by 1960s diesel locomotives, and some newer freight locomotives.\nRailway forums are also a useful resource (such as railforums.co.uk) and WNXX forum, because they often have boards dedicated to questions/answers where you can ask about particular train workings, and if anyone knows, they’ll give you information about the whereabouts of trains you need. Other resources that may come in handy:\n- Various Facebook gen groups, such as The Real Railway Gen Group. (Free).\n- Freightmaster – hand compiled train timetables for services that often run, and the locomotive class that runs them (paid).\nIt’s important to emphasize that you should always stay safe when train spotting. Below are some important tips.\n- Do not cross the yellow line at stations. This includes legs of tripods and legs of human beings.\n- Do not wave with both arms in the air – this should only be done in an emergency and you need the train to stop for safety reasons.\n- Advise station staff of your presence. Make sure that platform staff know your intentions so that you’re not acting suspiciously.\n- Try to keep out of the way of commuters. Most train spotters go to the far end of the platform for good shots, but should not pass any signs at the ends of the platform.\n- When using foot crossings, be aware that trains can come in either direction at any time.\n- You should always follow the advice of staff on the platform. Whilst most stations will be train spotter friendly (Eastleigh is a good example that welcomes train spotters), but if staff ask you to leave the station, you should do so.\nWe hope that this guide has been useful. If you’d like any further information, please do not hesitate to comment below.', 'Rail Safety Week: Stop. Trains Can’t\nRail Safety Week (RSW) is being held in Sept. 22-28 to empower people to keep themselves safe near tracks and trains.\nThe Utah Department of Public Safety, Utah Operation Lifesaver, the U.S. Department of Transportation, the Utah Transit Authority, the Utah Department of Transportation are working together to remind motorists, cyclists, and pedestrians about the lifesaving importance of railroad safety.\nReducing injuries and fatalities as Utahns travel across all modes is a top priority for state and federal agencies promoting safety. In the past 10 years (2009-2018) there have been 134 collisions at highway-rail grade crossings resulting in 28 fatalities and 40 injuries across the state. These safety advocates are working to reduce the risks of trespassing on or crossing rail-grade tracks, promote safe behavior on and around tracks, and reduce rail-related injuries and fatalities to zero .\nThe U.S. Department of Transportation’s safety initiative, “Stop. Trains Can’t.”, reminds drivers, bicyclists, and pedestrians to look for the train and observe crossing devices. Trains cannot stop quickly enough to avoid a crash; everyone must stop and proceed with caution. State and federal partners are working with the rail, transit, and highway industry to improve multi-modal safety, but the public must make cautious decisions when nearing and crossing tracks to reach zero fatalities.\nWatch a media story from Rail Safety Week kickoff here: https://kutv.com/news/local/rail-safety-week-kicks-off-in-utah-after-spike-in-deadly-train-crashes\nThe Stop. Trains Can’t campaign aims to help reduce collisions, deaths, and injuries at railroad crossings, which occur approximately every four hours. Because trains can take so long to stop, even in an emergency, it is ultimately the driver’s responsibility to avoid a collision with a train. Read on for more safety tips and facts.\n- In 2018 alone, 270 people were killed at railroad crossings. Over the last five years (2014-2018), there were 798 fatalities involving motor vehicles at railroad crossings.\n- Although rail crossing incidents and fatalities have been on the decline for decades, the number of drivers going around gates at railroad crossings has spiked in recent years.\n- In 2018 alone, 99 people were killed because a driver went around a lowered gate, marking a 10-year high.\n- From 2014–2018, 1,538 drivers went around a lowered gate and were struck by a train, accounting for 14 percent of all collisions. These deaths were caused by risky driving behaviors and poor decision-making, which means these deaths could have been prevented.\nTrains Always Have the Right of Way\n- By law, trains have the right-of-way at all railroad crossings.\n- Trains cannot swerve, stop quickly, or change direction to avert collisions. A train traveling at 55 miles per hour takes a mile or more to stop.\n- State highway traffic safety laws require all drivers to slow, yield, or stop until the train has cleared the roadway and it is safe to cross.\n- It is illegal to go around a lowered crossing gate or to ignore signs or flashing lights posted at a railroad crossing.\nKnow the Laws\n- Of the 130,200 public railroad crossings in the United States, roughly 54 percent are “active” crossings that include warning devices such as gates, bells, or flashing lights to alert drivers of an approaching train. But 46 percent are “passive” crossings, and only signs and markings are present at these crossings.\n- While warning devices do improve safety at railroad crossings, they do not prevent all collisions. Approximately 60 percent of all collisions at railroad crossings occur when active warning devices are present.\n- Motorists must come to a complete stop at least 15 feet from the track if: 1) flashing red lights are activated, 2) a crossing gate is lowered, 3) a flagman signals you to stop, 4) a stop sign is posted, or 5) a train is clearly visible or a train whistle is audible.\n- Ignoring signage or attempting to go around a lowered crossing gate can have deadly consequences. It is never worth risking your life or the lives of your passengers by ignoring the law or racing a train.\n- The best way to avoid a collision with a train is to understand and follow the warning signage, and to always stop for a train.\nUse Caution at Every Railroad Crossing\n- When approaching a railroad crossing, slow down, look, and listen for a train on the tracks, especially at “passive” crossings.\n- Look carefully in both directions before crossing a railroad track—even during the day. Sixty-seven percent of railroad crossing collisions occur in clear weather conditions.\n- Do not rely on past experiences to guess when a train is coming. Trains can come from either direction at any time.\n- Never race a train. It is easy to misjudge a train’s speed and distance from the crossing. A train traveling at 55 miles per hour takes a mile to stop—the length of 18 football fields or more — after applying the emergency brakes.\n- Before entering a railroad crossing, check that there is enough room on the other side of the tracks for your vehicle to cross completely and safely. Be aware that you may need to cross multiple sets of tracks at some railroad crossings.\n- Never stop on the railroad tracks. Keep moving once you have entered the crossing, and to avoid stalling, never shift gears on the tracks.\n- If your vehicle stalls on a railroad track, quickly move away from the track and your vehicle at a 45-degree angle. Call the number on the Emergency Notification System sign, or, if the sign is not visible to you, dial 911 for help.']	['<urn:uuid:b1e40b69-4e82-44e7-abd1-73b5ca30ac60>', '<urn:uuid:d3704bf9-518a-4912-8f47-9f83d681e5b6>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	20	103	2872
17	What makes Breakneck Ridge such a challenging hiking destination and why should one be cautious when attempting to climb it?	Breakneck Ridge is a challenging hike that requires careful preparation as it features steep friction slabs, boulder piles, and 1,400 feet of elevation gain in just over a mile. The trail has both impressive views and precipitous drop-offs, making it unsuitable for inexperienced day-trippers, young children, dogs, or inappropriate footwear like flip-flops.	"[""There will be six trailhead access points to the HHFT which will include restrooms, orientation kiosks, nearby parking, shuttle access and other amenities when completed. A few of these, along with additional immersive meanders on the trail route, can be visited and enjoyed now.\nLEARN MORE ABOUT EACH ENTRANCE LOCATION:\nPART OF THE CULTURAL FABRIC OF COLD SPRING, AN ARCHETYPAL RIVERFRONT VILLAGE\nDockside Park features open river views and shoreline access, just north of the Cold Spring riverfront dock where locals can be seen catching striped bass or enjoying views from wrought iron benches, and the village’s beloved bandstand.\nThis park marks the western bank of a historic village that delights visitors with its eclectic nineteenth-century architectural styles, artisanal shops, and restaurants for diners of all persuasions.\nDockside Park will be the southern gateway of the Fjord Trail. Village concerns in Cold Spring about the impacts of over-visitation to Breakneck Ridge gave rise to the Fjord Trail as a community project.\nWhen visiting Dockside Park and Cold Spring, make sure to also check out the nearby West Point Foundry Preserve.\nLittle Stony Point\nEXACTLY AS IT SOUNDS!\nA small peninsula in Philipstown with a lot of rock (gneiss, to be precise), Little Stony Point is the perfect place for a short hike with beautiful river views.\nIt’s also a very agreeable cool-down after the strenuous hike across the street, Mount Taurus (a.k.a. Bull Hill, but “hill” is understated, in a really big way, considering the degree of exertion this hike demands).\nLittle Stony Point has a cool cave and a pretty sand beach with a head-on view of Breakneck Ridge, the much bigger piece of rock up the road.\nA natural playscape for children will be one of the Fjord Trail’s future projects at Little Stony Point—but why wait? The one-mile loop at Little Stony Point is just right for kids who might not be ready for a full day’s hiking.\nTHE LEGENDARY HIKE BEHIND THE FJORD TRAIL'S GENESIS\nThis is the hike you’ve heard about: the one that sees nearly 200,000 visitors a year, many of whom learn quickly that you need to be prepared for this one.\nLocated in Fishkill and part of the 8,000-acre Hudson Highlands State Park Preserve, Breakneck Ridge is neither for the faint of heart nor the inexperienced day-tripper. Steep friction slabs, boulder piles, and 1,400 feet of elevation gain in a little over a mile mean you’ll want to have your lungs on good for Breakneck Ridge. Best to leave the young kids, dogs, and flip-flops at home. There are both impressive views and precipitous drop-offs to be encountered along the way so hike with care on this challenging route.\nAnd really importantly, everywhere you go and every step of the way, minimize your impact by abiding by the principles of Leave No Trace. Breakneck Ridge is a cherished natural landmark that has seen a lot of crowds over the years, leaving behind serious erosion and other evidence of heavy use. The new Nimham Trail and relocated Breakneck Trailhead will greatly reduce the environmental impact the area has experienced in recent years; but you need to do your part by sticking to established trails and treating this remarkable site with care.\nOf note: while the improved trailhead and overlooks at Breakneck will be part of the future Fjord Trail, the remainder of the hike is in Hudson Highlands State Park Preserve, and thus managed by OPRHP.\nPlease consider visiting other destinations in the project area—given the crowds here, you might be glad you did.\nNotch, Wilkinson Memorial, and Nimham Trails\nSOLITUDE AND BIRDSONG ALONG A CONNECTOR TRAIL\nThe Notch Trail is in the Hudson Highlands State Park Preserve, an approximately 12-mile out-and-back hike with its trailhead on Route 9D. Most of today’s Notch Trail will not be part of the HHFT; as with Breakneck Ridge, the Notch Trail is managed by OPRHP—but the improved trailhead will be part of the Fjord Trail and the hub of HHFT’s planned Forest area.\nThe Notch Trail can be reached four ways: from South Beacon Mountain, Breakneck Ridge, the Notch trailhead on Route 9D, or the Wilkinson Memorial trailhead, also on Route 9D. The Wilkinson Memorial trailhead will also take you to the Nimham Trail, which HHFT constructed in 2021 in partnership with NYS Parks.\nMadam Brett Park\nBIRDWATCHING RIGHT IN THE CITY OF BEACON\nScenic Hudson’s Madam Brett Park combines a magical mix of natural beauty and Hudson Valley industrial history.\nA boardwalk takes visitors along Fishkill Creek, passing a sometimes-raging waterfall and built opportunities for observing wildlife. History buffs will enjoy the nearby homestead of Catheryna Brett, who established a successful grist mill in the early eighteenth century and went on to be a formidable real estate tycoon.\nThe now-defunct Tioranda Hatworks, just adjacent to the boardwalk at Madam Brett Park, is a colorful relic of a time when Beacon was the hat-making capital of America.\nThe Fishkill Marsh, between Madam Brett Park and Dennings Point, is home to a diverse array of amphibians, waterfowl, and other aquatic species—make sure to bring binocularsand your favorite wildlife identification app!\nA HUDSON RIVER PENINSULA\nIt may look an awful lot like a mirror image of Florida on Google Maps, but Dennings Point, up close, is pure forest. Right in the City of Beacon, Dennings Point is a short loop hike with opportunities for river access. It is home to the Beacon Institute for Rivers and Estuaries of Clarkson University—leaders in research and education focusing on healthy water.\nPair Dennings Point with a hike at Scenic Hudson’s Madam Brett Park for a day of ecological diversity combining forest, marsh, and the Fishkill Creek—and some history at the 1879 Tioronda Hat Works ruins. Or connect Dennings Point to Scenic Hudson’s Long Dock Park via the Klara Sauer Trail, also known as the Riverfront Trail, for a longer excursion.\nThe connections you can make between different parks and trails in this area, along with creating a safe thoroughfare for non-motorized travel between Cold Spring and Beacon, laid the groundwork for the Fjord Trail.\nLong Dock Park\nTHE NORTHERN GATEWAY OF THE FJORD TRAIL\nThis Scenic Hudson park should be a destination on any visit to the City of Beacon: home to galleries, restaurants, and the renowned contemporary art museum Dia: Beacon. Long Dock Park offers year-round recreational opportunities from biking to cross-country skiing and kayaking.\nA restored nineteenth-century barn is a Scenic Hudson headquarters for educational and artistic programming. And to think: this community open space on the river was once an industrial hub, from railroad depot to salt stockpile to automotive junkyard.\nPhotography: Ruben Martinez Barricarte\nIllustration: Donna Calcavecchio\nPhotography (Dockside Park panel): Jaroslav Bendl\nPhotography (Dockside Park panel): Russell Cusick\nPhotography (Little Stony Point panel): Ty Cole\nPhotography (Notch, Wilkinson Memorial and Nimham Trails panel): Meredith Heuer\nPhotography (Madam Brett Park panel): Meredith Heuer\nPhotography (Dennings Point panel): Pierce Johnston\nPhotography (Long Dock Park panel): Meredith Heuer""]"	['<urn:uuid:c3f60bad-5a98-41bf-b613-8d556567c956>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	20	52	1159
18	I love ballet and architecture. How is performance affected by both artistic design and building acoustics?	In modern performances, both artistic elements and acoustics play crucial roles. In ballet, this is exemplified by productions like Black Swan White Swan where artistic elements include contemporary costumes, barefoot dancers, and gesture-laden movement language, all performed on a stage with mirrored panels and projected images of shimmering lakes. As for acoustics, concert venues like Walt Disney Concert Hall use Douglas fir wood walls and oak floors to enhance sound quality, while employing vineyard-shaped designs with terraced seating to optimize sound reflection. Advanced technologies, such as LARES systems and computer modeling, are used to ensure optimal sound distribution and acoustic experience across performance spaces.	['Grand Rapids Ballet Company\nBlack Swan White Swan\nGrand Rapids, Peter Martin Wege Theatre\n12 May 2012\nSave Tchaikovsky’s brilliant original score there was little choreographer Mario Radacovsky’s psychological ballet Black Swan White Swan had in common with traditional productions of Swan Lake. No feather-adorned tutus, no clack of pointe shoes hitting the stage floor and no love story of prince and a beautiful swan that transcends death. His contemporary version of the Marius Petipa classic instead had its dancers in modern-day costume, often barefoot, and told the story of a self-absorbed man who, faced with his own mortality, sought refuge in romantic relationships with two very different women only to be tormented by feelings of guilt and betrayal over his infidelities with them.\nThe world premiere of the black-and-white themed two-act ballet by Grand Rapids Ballet Company (GRBC) at the Peter Martin Wege Theatre began with a darkened stage on which the moving image of the surface of a shimmering lake was projected. The lake’s waters gradually became more turbulent as apparition-like circles of white light rose from its depths.\nThe mesmerizing scene then shifted to two others that served as setup for the ballet’s storyline and where we were introduced to its main characters.\nIn the first, a brief encounter between Nicholas Schultz as Siegfried and Laura McQueen Schultz as the White Swan (she looking more like a secretary than a swan) left the impression that Siegfried was in poor health as he collapsed to the floor (repeating the gesture at intervals throughout the first act) and that whatever ailed him, as well as his inner demons, were being manifested in the form of the sorcerer, Von Rothbart. Dancer Kyohei Giovanni Yoshida was riveting in his portrayal of the clingy, jealous and sinister Von Rothbart who seemed to have an equal measure of aggression and desire for Siegfried. His dancing was a potent blend of skill and power.\nThe second found Siegfried at a dance party where he encountered Black Swan, Dawnell Dryja. Radacovsky’s contemporary dance choreography for the scene had its five couples rocking back and forth in each other’s arms in a slow dance then picking up the pace to a comedic level while the Black Swan fawned over a now-melancholy Siegfried while Von Rothbart inserted himself between the pair, grabbing at Siegfried or climbing onto his back.\nApart from the ballet’s leads, GRBC’s dancers appeared at times uneasy in Radacovsky’s gesture-laden contemporary movement language; their dancing lacked energy and crispness.\nToward the end of the first act however, the ballet and the dancers’ performances seemed to come into their own thanks to a striking dreamlike scene where a corps of fourteen swans, barefoot and in white leotards, performed choreography that blended references to Petipa’s production with twisty, off-balance movement. The swans each danced in individual spotlights atop the shimmering lake projection that harked back to the ballet’s beginning and brought home the notion that the swans were in fact the apparition-like lights that rose from the lake’s depths. Backed by a wall of mirrored panels that multiplied the number of Swans, Siegfried and Von Rothbart slowly stalked each other as the swans danced, creating a marvelous juxtaposition of growing tension and animated beauty.\nA lightning-quick dance of the four cygnets and a playful and acrobatic pas de deux between Siegfried and the White Swan followed to end the act. McQueen Schultz as the White Swan was radiant and her performance in the pas deft.\nThe ballet’s second act saw Radacovsky’s choreography abandon its sometimes cutesy elements in favor of rich, emotional and more technically challenging movement. It opened with Siegfried and Von Rothbart in another battle of wills in which both dancers shone.\nThe ballet then climaxed in the Black Swan pas de deux where a seductive and fierce Dryja partnered by Schultz wowed the audience. Darting legs, sharp turns and daring lifts were thoroughly enthralling.\nThe rest of the ballet saw Siegfried seemingly defeat Von Rothbart only to have him return when the White and Black Swans met each other and exposed Siegfried’s infidelities. The ballet ended with Siegfried and Von Rothbart lying lifeless on the stage as the projected image of a swan in flight surrounded them and GRBC’s swan corps slowly flapped their arms to Tchaikovsky’s stirring music.\nPerhaps not a production for the ballet purist and in some ways a flawed one, Radacovsky’s ballet was nonetheless a triumph. For GRBC, now in its 40th season and the second under new artistic director and former Pacific Northwest Ballet prima ballerina, Patricia Barker, Black Swan White Swan was bold and successful step in the new direction Barker is taking the company from a little-known regional ballet company to one in the national spotlight.', 'Often the acoustics pose challenges for architects and motivates them to use their imagination and creativity. Architecture forms may integrate use of advanced technologies in order to enhance acoustics in modern concert halls. The key role of advanced technologies that are used in architecture is to enhance acoustic experience at a live performance concert venue. Acoustic design has evolved over the years to allow sound engineers and architects to avoid any sound disasters. Electronic technologies and computer aided software have the power to solve many acoustical problems by allowing them to access the acoustic performance as early as design stage. Technologies such as ray tracing that use a computer technique to visualize how sound waves travel through space, and auralization that helps architects to hear how buildings may sound in the design phase have contributed in the enhancement of acoustics to a great deal (Brensilver, 2004). The current essay analyzes two concert halls/outdoor venue space designed by Frank Gehry including Chicago’s Jay Pritzker Pavilion, and Walt Disney Concert Hall in Los Angeles.\nChicago’s Jay Pritzker Pavilion\nChicago has always been famous for its skyscraper buildings and beautiful architecture. The city has been able to attract many visitors to Millennium Park that features an open concert stage that has been designed by architect Frank Gehry (Cornwell, 2006). Jay Pritzker Pavilion as a part of the architecture has a soaring, curving stainless steel body with an open concert value. The pavilion has a 120 foot high billowing headdress made of brushed steel ribbons that make the stage similar to a post-modern proscenium (Cornwell, 2006). It is favorable for large audiences with 4000 fixed seats and a large lawn to accommodate 7000 more people. The key attraction is the innovative sound system which is first of its kind with an ability to mimic the effects of an indoor concert hall by using the technique of distributing sound equally over fixed seats and lawn. The three part structure including a stage house with perforated Douglas fir for the orchestra and choir, a steel plated shell designed to deflect and contain sound and intersecting steel pipes which extend over 90m in the lawn with lights and speakers allow an even distribution of digital sound in the hall (Webb, 2004). The hall has an added acoustic enhancement system that provide virtual walls and also has the capability to mask city noise. Speakers are placed after every 60 feet for the lawn audience consisting of enhancement as well as reinforcement speakers. The lateral speakers supplement sound while directing it towards the center for wall like reflections. Sound also extends to the lawn and also comes back to the performers. The hall has a computerized delay system that synchronizes sound from different locations at different time intervals (Napoleon, 2000). Since the stadium acoustics distribute the sound evenly over fixed seats and lawns, audience can choose to be seated according to their choices (Malnar & Hays, 2006). The trellis with speakers over the audience allows sound to reach a large number of people (Dawson).\nIt is considered as one of the most complex concert venue in US with a capacity to hold more than 12000 people at the same time. The technological innovation used in this hall is known as LARES system that creates reflected and reverberant energy which surrounds an audience in an indoor venue (Delacoma, 2005). Pritzker Pavilion is the first outdoor concert hall in US to adopt LARES system in US. It is with these LARES system sound technology, that sound can effectively reach a large audience in the hall. LARES system cancels any echoing effects and makes the listeners feel as if they are listening to the sound such as live orchestra from a huge distance (Chicagoarchitecture.info, 2012). Sound amplification in the outdoor concert area wonderfully enhances the reach of sound (Rhein, 2004). Douglas fir around seats of orchestra and chorus allow them to hear each other with clarity (Kamin, 2004).\nWalt Disney Concert Hall in Los Angeles\nWalt Disney Concert Hall is located within a two layered plastered box that serves the purpose of acoustic shell and sound proofing (Goldberger, 2003). The walls and ceilings of the hall have Douglas fir wood with oak floor to enhance acoustics (The American Institute of Architects, 2015).\nWhile designing larger halls, it is a challenge for the architects to seat more people and also provide a stylish design with quality acoustics. Using shoebox like designs won’t work since sidewalls shift farther from each other and thus reflections take longer to reach certain seats in the hall. Architect Frank Gehry designed a vineyard shaped hall with terraced seating, central stage for higher capacity and higher interaction between sound players and the audience. A vineyard shaped auditorium was designed on the principle that concave shape focuses sound while convex scatters it (Krieger Products, 2015). The walls around Disney Hall are an optical illusion since they are made of transparent material to allow sound to easily pass through and reflect. 3D computer models and complex software were used to fine tune the concert hall (Vankin, 2013).\nThe acoustics of the hall are clear, transparent and preferable for large scale symphonic music as well as small to mid-scale works (Rahilly, 2009). Creators of Walt Disney Concert Hall used methods such as ultrasonic sound generators to test how sound waves travel and reflect in the halls (Backscatter, 2013). According to acoustician Yasuhisa Toyota, a balance of acoustics involves a correlation between the science of architecture and music art (Anderson, 2015).\nDuring the construction of Disney Hall, acoustic tests using one-tenth scale model were used. Each aspect had to be reduced by the same degree which demanded that frequency of sounds should be made ten-fold in order to reduce the wavelength to tenth of the usual and nitrogen was used to expel oxygen and water that absorb high frequency sounds (Walt Disney Concert Hall, 2015).\nOn the basis of the above discussion it is clear that advanced technologies can play an instrumental role in facilitating the use of innovative acoustic solutions that enable architects to offer exceptional sound experience to audience in concert halls. Leading architects have utilized various technologies to assess the quality of acoustics in their designs so as to modify the structures at an early stage of development. The practical examples of Chicago’s Jay Pritzker Pavilion, and Walt Disney Concert Hall in Los Angeles are brilliant cases of superior acoustics while supported by innovative designs.\nAnderson, A. (2015, May 14). The Sound of Music: acoustician Yasuhisa Toyota. Retrieved from Internationalartsmanager.com: http://www.internationalartsmanager.com/2015/05/the-sound-of-music-acoustician-yasuhisa-toyota/\nBackscatter. (2013, October 2). Mr. Yasuhisa Toyota-The Honest Engineer of Disney Hall. Retrieved from Rettacs.org: http://rettacs.org/mr-yasuhisa-toyota-the-honest-engineer-of-disney-hall/\nBrensilver, D. A. (2004). With Acoustics, No Longer Just Playing it by Ear. Architecture, 93(6), 28.\nChicagoarchitecture.info. (2012). The Pritzker Pavilion. Retrieved from Chicagoarchitecture.info: http://www.chicagoarchitecture.info/Building/1248/The-Pritzker-Pavilion.php\nCornwell, T. (2006, October 11). Millennium Park Burnushes Downtown. National Mortgage News, p. 22.\nDawson, J. (n.d.). Jay Pritzker Pavilion, Millennium Park. 16.\nDelacoma, W. (2005). The Jay Pritzker Pavilion Millennium Park Chicago, IL. Retrieved from Lareslexicon.com: http://www.lares-lexicon.com/millenium/millenium.html\nGoldberger, P. (2003). Good Vibrations the Skyline. The New Yorker, 79(28), 108-109.\nKamin, B. (2004, July 18). Jay Pritzker Pavilion; Randolph and Columbus Drives; Frank Gehry, Los Angeles. Chicago Tribune.\nKrieger Products. (2015). The Walt Disney Concert Hall: An Acoustical Wonder. Retrieved from Kriegerproducts.com: http://www.kriegerproducts.com/successstories/the-walt-disney-concert-hall/\nMalnar, J. M., & Hays, D. L. (2006). Millennium Park, Chicago: A Sensory Delight, Part 2. The Senses and Society, 1(2), 259-266.\nNapoleon, D. (2000). Indoors out. Entertainment Design, 34(11), 25-26.\nRahilly, L. (2009). The Architecture of Acoustics. Retrieved from Listenmusicmag.com: http://www.listenmusicmag.com/feature/the-architecture-of-acoustics.php?page=1\nRhein, J. v. (2004, July 18). Pavilion’s sound a work in progress. Retrieved from Chicagotribune.com: http://www.chicagotribune.com/entertainment/music/chi-0407180378jul18-story.html\nThe American Institute of Architects. (2015). Walt Disney Concert Hall. Retrieved from The American Institute of Architects: http://www.aia.org/cities/los-angeles/all-stories/disneyconcerthall/index.htm\nVankin, D. (2013, September 20). Disney Hall: Yasuhisa Toyota’s fluid, innovative approach to sound. Retrieved from Los Angeles Times: http://articles.latimes.com/2013/sep/20/entertainment/la-et-cm-walt-disney-concert-hall-yasuhisa-toyota-20130922\nWalt Disney Concert Hall. (2015). Acoustics. Retrieved from Wdch10.laphil.com: http://wdch10.laphil.com/wdch10/wdch/acoustics.html\nWebb, M. (2004). View from Chicago. The Architectural Review, 216(1293), 38-39.']	['<urn:uuid:62f162e6-303e-48df-80da-8ac39492fc6e>', '<urn:uuid:f6eae080-3bad-4b2c-95cd-08c72a3c7185>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	16	104	2122
19	I'd like to prepare a nice roast beef this weekend but I'm not sure about cooking times - how can I tell how long to cook it based on my preferences?	For rare beef, roast 20 minutes per 450g plus 20 minutes. For medium results, cook the meat 25 minutes per 450g plus 25 minutes. For a well done roast, cook 30 minutes for every 450g plus 30 minutes. For reference, internal temperatures should be 145°F for medium rare and 160°F for medium.	['Place in the center of the oven and bake for 20 minutes, then lower the temperature to 170C for a ventilated oven or 180C for a fanless oven. Continue roasting for 20 minutes for 500g, reaching a core temperature of at least 56C for medium rare roasts.\nHow do you cook a 500g beef joint?\nCalculate your cooking time for medium-rare with 20 minutes for 500g or for medium use of 25 minutes for 500g. For beef with or without bones, cook at 240C \\/ 220C fan \\/ gas 9 for 20 minutes, then lower to 180C \\/ 160C fan \\/ gas 4 (don’t forget to subtract this 20 minutes from the time you just calculated).\nHow long does beef silverside take to cook?\nWeigh the roast (maybe stuffed) to calculate the cooking time. If you like rare beef, roast 20 minutes per 450g plus 20 minutes, for medium results cook the meat 25 minutes per 450g plus 25 minutes and for a well done roast 30 minutes for every 450g plus 30 minutes.\nHow long do you cook roast beef for per pound?\nRoast for about 13 to 15 minutes per pound for the rare, 17 to 19 minutes for the medium, and 22 to 25 minutes for the fine. Check meat with a thermometer to make sure it’s at the desired temperature: 145 for medium rare, 160 for medium.\nHow do you cook beef so it’s tender?\n8 Simple Ways to Make Tough Meat Tender\n- Physically tenderize the meat. For tough cuts like steak, plunging can be a surprisingly effective way to break down those tough muscle fibers.\n- Use a marinade. …\n- Don’t forget the salt. …\n- Let it come up to room temperature. …\n- Cook it low-and-slow. …\n- Hit the right internal temperature. …\n- Rest your meat. …\n- Slice against the grain.\n26 янв. 2018 г.\nWhat is the best way to cook a small joint of beef?\nIt’s important to cover your mini roast with foil and rest for 10-15 minutes before carving.\nPlace in the centre of the oven;\n- Rare – cook 20 minutes per 450 g plus 20 minutes.\n- Medium – cook 25 minutes per 450g plus 25 minutes.\n- Well done – cook 30 min per 450g plus 30 min.\nShould you cover beef with foil when roasting?\nRoast your beef, uncovered, until you like. Remove from oven, cover with aluminum foil and let stand 15 minutes before carving. Roasts over 8 pounds should be lightly covered halfway while roasting to prevent browning.\nShould I cover silverside when cooking?\nCover with aluminum foil and cook for about 1 hour 30 minutes over medium heat, rinsing occasionally and adding a little water if needed. This will keep the meat moist. Cover with aluminum foil each time. Leave the meat for the last 15 minutes of cooking.\nWhy is my silverside beef tough?\nPan-fried beef is the back meat of a large and lean beef with little fat. Depending on how you prepare it, there is a risk that the meat will be tough, but with Joule you can guarantee that it will be tender and moist.\nIs silverside and topside the same?\nSilverside is leaner than Topside and can be used as an inexpensive grill, but lean meats work much better in slow cooking. Traditionally sold rolled and tied, Topside makes a great roast with full beef flavor.\nHow do you keep beef moist when roasting?\nPlace a few sprigs of fresh thyme on the grill and pour in the beef broth. Using this amount of broth will help the roast beef stay moist and tender. It also soaks into vegetables during roasting for great flavor. Cover the roasting pan and place in a 350F oven for 3 hours.\nDoes Roast get more tender the longer you cook it?\nUnlike most other cooking styles, the meat will be softer when you cook it in the slow cooker. IF MY POT ROAST IS STILL HARD WHAT SHOULD I DO? Close the lid and let the roast cook longer.\nShould I sear beef before roasting?\nMarinating meat is a necessary step if you want to prepare grilled dishes, steaks, ribs and more. When you grind meat, you caramelize the meat’s natural sugars and brown the proteins, forming a rich brown crust on the surface of the meat that enhances the delicious flavor of the finished dish.\nHow do you keep meat moist when cooking?\nTo avoid it, keep the meat moist, by marinating or carefully cooking it at a lower temperature for a shorter time. Blackened foods are safe because they are protected by butter and rubbing. To prevent the meat from shrinking into small sausages during cooking, cut the meat along the grain.\nWhat is the best temperature for slow cooking beef?\nBest cooked at 150-170C, recipes that call for slow cooking are more difficult to control with a home oven. First, brown the meat to give it a little color, cover the lid on low heat, then increase the heat lastly so that the meat is golden brown.\nWhy is my roast beef tough?\nIf the pot is not cooked thoroughly, it will be tough and flexible. Test your roast with a fork before removing it from the pot. Another problem is overcooking the pot, resulting in dry meat. To avoid this, make sure you have enough braising liquid in the pan and don’t let it dry out.']	['<urn:uuid:56ff0d62-d1c2-4b09-8df8-64aac05acd43>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	31	52	914
20	sprinkler system spring startup winter protection steps	For spring startup, wait until no chance of overnight freezing (late April/early May), then slowly turn on main water valve, inspect property for damage, check wire connections, test each zone manually, and look for leaks. For winter protection, shut off water before freezing temperatures, drain the system, install freeze sensors, insulate valves and above-ground piping, and winterize the automatic controller by disconnecting the COM wire.	['Before we jump into springbiding the irrigation system, it’s important that you’re comfortable with knowing your system was winterized, completely drained of water and shut down properly at the end of last years’ irrigation season. If you’re unsure, call an irrigation specialist to assist with the spring startup. It will save you time, money, and frustration.\nWhile spring has sprung and the ground is beginning to thaw, it is only when there is no chance of overnight temperatures dropping below zero degree (late April or early May) that it’s safe to say, “inspect the irrigation” not “flip the switch.” Don’t be too quick to bypass a few important steps to ensure your system has made it through a grueling winter.\nYour irrigation system was expensive and has great resilience – taking it for granted might prove disastrous. It’s made up of multiple components that precisely interconnect with each part in order to operate in a hands-free environment. This is an investment worth the few steps it takes to ensure its daily operation can deliver without hiccup. Below is a list of steps to effectively manage your spring start up.\nStarting up Your Underground Irrigation System\nFIRST THINGS FIRST:\nA) Be prepared to devote a couple hours to supervise the system’s initial operation. This is the time to manually control each zone ensuring that all sprinklers are popping up and operating in normal range.\nB) Test the ground by inserting a tool to penetrate to a depth of at least one foot. If you broke out in a sweat without getting past a few inches, go grab a drink and a few days’ rest, the irrigation isn’t ready to operate either.\nPREPARE TO START UP\n1) Main Water Valve:\nTurn on the main water valve slowly counterclockwise. Remember: Lefty-Lucy, Righty-tighty. This valve could be located outside after the backflow preventer or in the mechanic room, follow the pipe from the cold-water line at the meter to where it goes through the wall outside.\n2) Property Inspection:\nDo a walk-about to inspect each sprinkler before allowing the water to pass through to each zone. Check for signs of damage, missing nozzles, and that each sprinkler is in good alignment and accounted for.\n3) Wire Connection:\nInspect and tighten up all wire connections to each electric valve at the manifold, making sure the wires are protected with gel filled wire connectors.\nEnsure the wires are securely connected at the controller for all valves, rain sensor, and other wired components.\n4) Manual Override\nSet the controller to ‘Manual operation’ then open each zone valve one at a time and watch that all sprinklers are functioning as they should.\n5) Adapters & Switches\nSecure any relay switches for pumps, adapters for power, and wifi plugins.\n6) Check for Leaks\nInspect for any leaks at the manifold, along the distance of the zone pipe and up through each sprinkler. If there are leaks, you know what to do. X the spot, flag it, tag it or spray the area that needs repairs. shut the system down and repair or replace as needed.\n7) Automatic Watering\nOnce you’re sure everything is in normal working condition, set the timer back to ‘Automatic Watering’. Spend the time to supervise the complete cycle of each zone. If some heads are spitting instead of spraying, check if they need to be cleaned instead of replaced.\n8) Enjoy!! You’re all set for another beautiful irrigation season.\nPS; If you want to add one or more zones to your system, go to www.irrigationpackages.com', 'By the editorial team at Sprinkler Supply Store\nWinter is upon us, and before long, we’ll be saying hello to coats, scarves, and mittens. While it may not seem time to think about gardening, this season is a critical time to do the things you did not get a chance to accomplish during the growing season. It’s also the perfect period to prepare for the onslaught of winter and protect your garden from damage during the season’s freezing temperatures.\nOne of the major considerations during this time is preparing your sprinkler system to withstand the colder months. You won’t want to welcome spring with unwelcome surprises, like repairing your irrigation system when it has frozen over and got its pipes cracked, damaged, and broken into pieces. Even worse, you won’t discover this fiasco until spring!\nShort of calling a sprinkler winterization service, you will find that many of these preparations are easily manageable on your end. All you need is ample knowledge and these valuable tips in winterizing your sprinkler system.\nShut Off the Water and Timer\nYou won’t need to water your lawn during winter most of the time, especially if it snows heavily in your area. Therefore, start your sprinkler system winterization by shutting off the water and sprinkler timer (if you have one). Shut off the water before the temperature drops to below freezing. Doing so will protect your system from damage. Water expands when it freezes, so you don’t want any in your pipes and valves lest they crack.\nDepending on where you live, you may have to turn off your sprinkler timer entirely or leave it in rain mode. Lawns in areas that experience heavy snow will not need watering, so you should turn off your water and timer before the cold sets in. If your area experiences lighter winters, you may still need to water your lawn, so keep the timer on rain mode.\nDrain Your Sprinkler System at the Right Time\nOnce the freezing temperature approaches, you’ll know it’s time to drain your sprinkler system. Do this during the fall when you’re expecting the first frost, just early enough to ensure that you’re not leaving it at risk for damage. When you shut down the flow of water early on, allowing it a few weeks of hot and dry weather, you may be ensuring that your sprinkler system will survive the onslaught of the winter season.\nBefore doing any sort of sprinkler work or winter maintenance, make sure to wear proper protective equipment, such as high-grade nitrile gloves, coveralls, goggles, and boots. Draining your irrigation system can get messy, so you don’t want to be wearing nice clothes.\nDraining your sprinkler system can be done in several ways, depending on the type of valves and system in place. For one, automatic valves require you to turn off the water supply to the system first before turning on the sprinkler head and letting the water out. However, manual valves would need you to shut off the water and open the valve.\nOthers try what they call the “blow-out” method. This trick requires you to force compressed air into the piping. This will push the water out of the sprinkler heads. Just a word of caution, though. This method requires expertise because it can lead to a damaged system if done wrong. If you’re unsure about what you’re doing, it’s best to leave this job to professionals to ensure effectiveness and safety.\nInstall a Freeze Sensor\nYou might also want to consider installing a sensor that can detect freezing temperatures. Not only does this allow you to save water and money, but it’s also a reliable tool that can suspend irrigation when the phenomenon unfolds. A freeze sensor can protect your plants from overwatering and damage while reducing the risk of ice on driveways and sidewalks, alleviating possible injuries and accidents.\nThe sensor often detects both rain and freezing temperatures. Usually wireless, the freeze sensor is mounted to your house, unblocked from tree limbs, leaves, and other obstructions. It can communicate with your sprinkler system’s controller.\nEvery year, it’s advisable to inspect if your sensors are still functioning properly. The lead-up to winter is the best time to do this. One of the best ways to ensure your sensors are working well is to clean out the debris forming around the sensors, such as dirt, cobwebs, and leaves. You should also check the battery before testing the sensor.\nInsulate Your Sprinkler System\nApart from shutting off the water supply to your irrigation system, it’s also essential that you take steps to protect the shutoff valve against freezing. You can do this by wrapping the valve with insulation, such as foam insulation and a plastic bag. This will help protect the valve from freezing temperatures.\nYou should also keep in mind to do the same for any above-ground piping. You may consider using self-sticking foam insulating tape. There are also insulating tubes for this purpose.\nWinterize Your Automatic Controller\nAutomatic controllers need to be winterized too. This requires you to disconnect the standard wire from the controller. Find the wire marked as “COM” or just “C.” In the case of outdoor mounted controllers, leave the power on and the dial off. The enclosure will be kept warm through the heat emanating from the transformer, keeping condensation from forming.\nOf course, you must invest in a quality controller that gets the job done while allowing you to save on costs, just like the Hunter Pro C controller, for example.\nProtect Outside Faucets\nExterior faucets are common in many household gardens. During the winter season, shut down exterior faucets from the water supply. You might also want to consider removing and draining garden hoses attached to these faucets to avoid damaging them. While turning the valves off is recommended, you need to leave the taps open as this trick can break the air vacuum and allow water to drain thoroughly.\nDoing this will let all the water between the shutoff valve and outside faucets drain, reducing the chances of freeze damage.\nWrapping It Up\nA good sprinkler system is essential to keep your garden in pristine condition during the hot and dry summer season. However, once fall is around the corner, you will face a looming deadline that compels you to prepare for the winter months. After all, cold temperatures can put your sprinkler system in danger of freezing and getting damaged in the process.\nThat is why a proper and thorough winterization preparation is necessary. It’s the only way to ensure your sprinkler system will remain in great shape throughout the changing seasons. You need to remember that this process requires timing, effort, and commitment, year in and out. It’s not something that you can skip unless you’re prepared to face some nasty surprises come spring, not to mention mounting costs to repair the damage.']	['<urn:uuid:36e499cd-c0de-4d1f-bdb4-a4e9cd4eb4b4>', '<urn:uuid:831904de-45c6-4f51-829e-76a5824e46fb>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	7	65	1738
21	I own a Vortex Smart - what's the steepest hill it can handle?	The T2180 Vortex Smart has a maximum slope capability of 7%	['Virtual speed is a difficult feature to explain. The necessity of implementing Virtual Speed in the Tacx applications and third party applications are basically coming from the limitations of an indoor trainer compared to the wide landscape and/or performance range of riding outside.\nEvery indoor trainer has its own specifications (e.g. maximum slope, maximum power, inertia). Amongst indoor trainers there are many differences, e.g. a T2180 Vortex Smart has a maximum slope of 7% and a T2800 NEO Smart a maximum slope of 25%.\nIt is understandable that when the cyclist is simulating the Mont Ventoux with a maximum gradient of 11%, that this can be complicated for a T2180 Vortex Smart. In order to solve this Virtual Speed is developed.\nVirtual Speed is that the application being used is displaying a different speed (Virtual Speed) compared to the actual speed of the rear wheel, in the situation where the track being simulated is outperforming the indoor trainer. The difference in speed is based on a calculation taking all performance and resistance factors in account (e.g. gradient, speed, inertia, air resistance etc.). This allows the cyclist to simulate a track that is outperforming his indoor trainer.\nTrainer being used is a T2180 Vortex Smart\n- 25 km/h on a 1% slope, speed shown in the app is also 25 km/h (in breaking zone)\n- 25 km/h on a 5% slope, speed shown in the app is 20 km (out the breaking zone)\n- 15 km/h on a 9% slope, speed shown in the app is 5 km/h* (Virtual Speed)\n*This is an example, actual speed could be different in the application.\nNote: The maximum percentage gradient is calculated with a body weight of 75kg in reference to the strength of the resistance unit with a speed of 10km/h\nNote: Virtual Speed is being calculated in the application. Not all applications are provided with this feature\nAscending and descending\nVirtual Speed applies to ascending, but also to descending. When the cyclist is riding downhill, the rear wheel maybe only have a speed of 10 km/h (6 mi/h), but the Virtual Speed is showing 60 km/h (37 mi/h).\nAccelerate and decelerate\nWhen accelerating and decelerating on an indoor trainer there could be a difference between the actual acceleration on your indoor trainer compared to a similar situation riding outside using the same accelerating force. This is dependent on the inertia of the flywheel and the current speed the cyclist is cycling with. Also to cancel out these inequalities Virtual Speed is being used in the application (the Virtual Speed slowly accelerates when you start from 0 to 25 km/h on a trainer that is not able to simulate inertia correct).\nNote: The T2800 NEO Smart is designed with a Virtual Flywheel with a very large controllable mass inertia, meaning that the NEO Smart is able to simulate at different speeds realistic inertia effect. Virtual Speed in combination with a NEO Smart is hardly being used\nHandlebar computers and watches\nSmart trainers can also be connected to watches and handlebar computers, showing power, cadence and speed. Due to the lack of the Virtual Speed feature in these devices a difference in speed reading is possible when comparing the applications with another device.\nERG training (power based training, Watt, FTP, HR):\nSpeed is not important in these trainings. In the background the program needs to understand what the distance for calculation so it just takes an average speed. If you ride faster it needs to correct the Virtual Speed. It will always be Virtual Speed in the program. This also counts for the NEO.']	['<urn:uuid:765300d4-29e6-4343-9313-cdb075563f01>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	13	11	600
22	merlin hunting techniques prey capture strategies	Merlins employ diverse hunting strategies to capture prey. They hunt birds in the air, earning the nickname 'bullet hawk' for their fast, direct flight. They may use vehicles or large mammals as cover to flush out prey, and sometimes hunt cooperatively in pairs - one bird flying low to flush prey while the second flies above for the capture. They typically feed in early morning and late afternoon, successfully catching one of every two targets. When prey tries to escape by flying in circles, merlins engage in a 'ringing flight,' following the prey upward until it tires.	['By David Spector Gazette Contributing Writer\nThe science of ecology often entails studying the distribution and abundance of species: Why are they where they are, and what determines their population size? On a single day of bird watching, I had the opportunity to observe two species whose presence and numbers led to very different answers to these questions, even as their lives intersected.\nA year ago, I took a drive through some farm fields in Hadley, which provide habitat for open-country species of birds. These include both breeding birds and those that come down from farther north to spend the winter here. That day I saw a flock of a dozen horned larks. These plump little birds, with distinctive bold facial markings and tufts of feathers forming little “horns” at the corners of the face, make pleasingly comical sights from the human perspective. In flight they present a more serious image, flocks of sleek birds swarming over the land.\nThe horned lark is the only member of the lark family native to the Americas, although there are some 96 species in the Old World. The breeding population of this species is quite small in western Massachusetts, but the birds’ numbers swell during migration and in winter when locals are joined by many more that breed on the Arctic tundra.\nAs I left the fields, I stopped the car to take one last scan for birds, and I noticed what looked like a large ash falling from the sky. I was concerned that a fire might be breaking out nearby until I took a closer look and realized that the “ash” was a feather, one of several drifting down around my car. I looked up and saw, atop a nearby utility pole, a horned lark being plucked by a merlin, a small bird-eating falcon.\nThe merlin hunts birds in the air, and is known to hawk-watchers as the “bullet hawk” for its fast, direct flight. It might not be coincidental that I saw the lark becoming a meal. Merlins have been seen hunting in the company of large mammals or vehicles and preying on birds flushed into flight. The merlin I saw may well have followed my car, using it both as cover and as a beater to put prey into the air.\nA few decades ago, merlins were uncommon migrants here, shooting through between their northern breeding grounds and their more southern wintering range. Recently, though, a few merlins have wintered in the Connecticut Valley as the species’ winter range has expanded north. Also, merlins are breeding more to the south than previously known, and last year the first breeding record for the species in western Massachusetts was reported in Northampton. In the Canadian part of the merlin’s range, the falcon has now moved beyond the classic wilderness and started nesting in cities.\nSo why are merlins expanding their range and habitat in these ways? The reasons may be many, but at least one is related to human activity: the spread of fruit-bearing plants, including invasive plants like oriental bittersweet and deliberately planted ornamentals like crabapples and cherries. These have supported expanding populations of fruit-eating birds like robins and waxwings that in turn provide prey for the merlins.\nAnother question, yet to be answered: Will the nesting population of merlins become established and expand in western Massachusetts? We have two habitats the bird finds suitable. The far northwest corner of our state offers patches of boreal (spruce/fir) habitat similar to the classic northern wilderness breeding areas of the species, and places like Northampton provide an urban environment that matches the merlins’ more recent choices. Only time will tell whether these are sufficient to support an ongoing presence of these falcons in our midst.\nDavid Spector is a former board president of the Hitchcock Center for the Environment and teaches biology at Central Connecticut State University.\nEarth Matters, written by staff and associates of the Hitchcock Center for the Environment at 525 South Pleasant St., Amherst, appears every other week. For more information, call 413-256-6006, or write to us.Click here to return to full list of Earth Matters articles.', 'by Nina Harfmann\nMerlin, Falco columbarius\nAs their range increases to the south, Merlins, the fierce little falcons once known as “pigeon hawks” for their resemblance to pigeons in flight, will be increasingly more than rare visitors to the Buckeye State. When perched in a suburban neighborhood, a Merlin, lacking the strong facial pattern of most other falcons, could be confused with an immature Sharp-shinned Hawk—until you recognize its large deep brown eyes! When seen overhead, a Merlin, with its strongly tapered, checkerboard-patterned wings, could be confused with an American Kestrel—until you note the shorter tail and heavily streaked breast. Or when seen from the back, a Merlin, with slate blue wings and cap (male), could again be confused with a Kestrel—until you notice that the tail isn’t rusty and has narrow white bands, instead.\nYes, this bird deserves a second look—and if you’re lucky, you might just get the chance!\n- Although the Merlin is considerably smaller than the Peregrine Falcon and easily recognizable by its shorter wings and faster wing beats, it is only inches larger than the smallest of North American falcons, the American Kestrel. (Kestrel, 8-11 in./Merlin, 9-12 in.) However, if one were to weigh both a Merlin and a Kestrel, the Merlin would tip the scales at up to 3 times the Kestrel’s weight—its heavier body evidence of a more muscular build and the source of the speed and power behind the Merlin’s forceful flight and spectacular aerial maneuvers.\n- Merlins generally feed in the early morning and late afternoon—mostly upon small birds that they take mid-air, either by dashing from a perch in a tail chase or flying low, hugging the ground in a surprise attack. Under good conditions, this fast flier will likely catch one of every two targets. The few that escape the initial effort may attempt to “outrun” the hunter by flying in circles, as both predator and prey climb into the sky in a rising chase called a “ringing flight.” But the songbird quickly tires. And the Merlin takes his prey. A male and female pair may also hunt cooperatively—one bird flying low to flush prey, while the second flies above for the capture. In addition to the almost 900 small birds devoured each year, a Merlin also eats dragonflies and moths on the wing, small mammals (bats) and an occasional lizard.\n- Historically, Merlins have bred to the north and west of Ohio in and around open areas and at the edges of conifer woods, only passing through the state as migrants en route to wintering grounds in the southern United States and northern South America. In recent years, these supreme winged predators have been documented to be winter residents in Ohio, as well—snapping up hapless House Sparrows (3/4 of their diet!) throughout the winter months while perched atop an aged shade tree within the spacious sanctuary of an urban cemetery. In addition, within the last decade, Ohio has seen multiple reports of nests and nesting behavior within several counties, as Merlins adapt to more urban and suburban areas and the wealth of small songbirds provided by these open areas with large overstory trees.']	['<urn:uuid:556d7881-e749-479e-b6a0-4ff890ad6d32>', '<urn:uuid:ae7c9d35-3d93-4f80-b788-20ebcb9a8da1>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	6	97	1213
23	3d printed zeus statue stratasys details	The Zeus statue was recreated using the Stratasys Fortus 900 3D printer, one of the most powerful available, using fused deposition modeling (FDM) technology. The replica was printed in thermoplastic and stands 6 feet tall, significantly smaller than the original 42-foot statue. The project was a collaboration between Stratasys Ltd. and Kennesaw State University's 3D Center, with Kennesaw's contribution including the eagle on the scepter and a small statue of Nike held by Zeus.	['Zeus must be wondering if he is about to regain his stature of antiquity. For more than a thousand years he was considered the father of the gods who ruled from his throne at the top of Mount Olympus, the highest mountain on the Greek mainland located 50 miles southwest of Thessaloniki.\nHis mighty presence is about to be resurrected at the Millennium Gate Museum in Atlanta‘s Atlantic Station in celebration of the 20th anniversary of the 1996 Atlanta Centennial Olympic Games. Although Zeus must have been disappointed that those games hadn’t been hosted in Greece’s capital, Athens, the upcoming exhibition – The Games: Ancient Olympia to Atlanta to Rio — gives full-credit to the country’s historic role.\nMinnesota firm Stratasys Ltd. and Kennesaw State University‘s 3D Center. The feat is remarkable in that the statue, one of the “seven wonders” of the ancient world, was recorded only in written descriptions and illustrations on coins.\nIn an attempt to replicate the figure, which was made of wood, ivory and gold, the parts have been printed in thermoplastic, using a 3D model approximating its design. Zeus has been cut down to size, however. Originally at 42 feet tall, the 3D version has him at only 6 feet.\nBut he is at the forefront of what far-sighted museum curators see as the beginning of new teaching tools and the historic restoration of artifacts lost through the ravages of time or the destruction by militants such as Islamists who have destroyed the cultural heritage in Iraq and Syria by attacking archeological sites with bulldozers and explosives.\nThe Stratasys Fortus 900 3D printer is one of the most powerful available and recreated the Zeus statue using fused deposition modeling (FDM) technology. Its roots stretch back to 1989 when Scott Crump and his wife Lisa founded the Stratasys company in Eden Prairie, Minn., after Mr. Crump decided to make a toy frog for his young daughter using a glue gun loaded with a mixture of polethylene and candle wax.\nRandy Emert, director of Kennesaw’s Center for Additive Manufacturing and the 3D Center, told Global Atlanta that Kennesaw’s contribution to the Zeus statue includes an eagle on a scepter and a small statue of Nike which are held by Zeus. The Kennesaw students also are learning to use the technology by creating a separate head of Zeus.\nHe said that the university’s 3D Center introduces students to the vast employment opportunities that are opening up due to 3D technology, which is playing an ever growing role in new media arts, game design and mechanical engineering.\nRodney Mims Cook Jr., founder and president of the National Monuments Foundation, an organization that is responsible for the design and construction of the Millennium Gate Museum, told Global Atlanta that the Zeus project was in keeping with the spirit of the 1996 Summer Olympic Games by underscoring cooperation among local governments, universities, business and non-profits.\nThe University of Georgia, the Georgia Institute of Technology, Savannah College of Art and Design, the Carlos Museum at Emory University, Oglethorpe University in addition to Kennesaw are participating in the project. It also is scheduled to travel in Georgia to Rome, Columbus, and Athens where it will be on display at the Darlington School, The Columbus Museum and the University of Georgia.\nPerhaps to Zeus’ dismay, the entire exhibition is not to be focused on his statue alone. Also to be included are classical artifacts to be contributed from the Hearst Castle in California, which are more than 2,500 years old and tell the story of the Olympic Games and Ancient Olympia, athletic competition and Greek mythology and politics.\nMr. Cook also was enthusiastic about the participation of the Hearst Castle because of the connection between Atlanta and the Hearst family. William Randolph Hearst bought a newspaper in Atlanta in 1912 extending the reach of his publishing empire across the United States. The Atlanta connection eventually led to his donating enough land to triple the size of Oglethorpe University.\nA classicist at heart, Mr. Cook’s interest in 3D technology includes a commitment to educational and restoration projects. This exhibition rests at the intersection of art and technology, he told Global Atlanta, “Teaming with Stratasys and KSU’s 3D Center, our hope is to deliver creations far too few have even tried to attempt.”\nAgain, in the spirit of the Olympics, Mr. Cook expects the technology to “test human limits,” and as the 1996 Summer Olympic Games did, “Give us a shot in the arm and an international space that we never had. It served the city by demanding what was expected of us and met the aspirations of our forefathers.”\nMeanwhile, Athens, Ga.-based sculptor and painter Stan Mullins, along with two craftsmen from Gravity Works LLC, also in Athens, have been putting the finishing touches on the throne upon which Zeus is to be placed for the exhibition.\nThe exhibition is to open on Saturday, Aug. 20, extending until Jan. 2, 2017, and provides insights into the scope of the games in ancient times, and the values that have been brought back to life in modern times.\nThe earliest records of the Olympic Games date back to 776 B.C. when they were a religious festival held on the fertile ground of a rural sanctuary at the base of the mountain. According to ancient legend, all hostilities between the Greek city states ceased for the duration of the games, which came to represent the Olympic spirit of global brotherhood that continues to this day.\nIn stark contrast to the Rio Olympics, the games were small in scale in their earliest years. In addition to sacrifices and other religious rites, there was only one sporting event, a footrace of 200 yards, a distance which was called a “stade” from which the word “stadium” is derived.\nAs the games evolved over the centuries, however, they became the center of Greek life and Olympia became the hub of the Greek-speaking world.\nWhile the athletic contests multiplied so did grand processions and banquets honoring the gods and heroes. The sacrifices also grew exponentially including hundreds of oxen.\nDespite all of the pageantry and the proud strutting of masculinity, the most spectacular sight at Olympia was the gold and ivory cult statue of Zeus enthroned and crowned with a sculpted wreath of olive sprays. A golden robe carved with animals and lilies covered its body. Its sandals were made of gold. In its right hand it held a statue of the crowned Nike, the goddess of victory, and in its left hand was a scepter inlaid with many metals supporting an eagle.\nThe throne itself was no less impressive, decorated in gold with precious stones, ebony and ivory.\nOne Greek orator was reported to have said that a glimpse of the statue was enough to make one forget all early troubles. A Roman general said that once when he saw the statue he felt he had seen a living god.\nBut even Zeus’ command of the known world didn’t last forever.\nIts demise is cloaked in obscurity, but Christianity is generally accredited with its end. In the early fourth century A.D., Emperor Constantine decreed that gold be stripped from all pagan shrines after he converted to Christianity.\nBy the end of the century the Olympics were viewed as a pagan rite and abolished altogether. It was then that the statue perhaps was moved by a wealthy Greek to Constantinople where it became part of his private collection of classical art.\nThere is some controversy about whether its final end occurred when fires swept Constantinople in 475 A.D. and in 425 A.D.\nWhatever the immediate cause of its end, there are only few traces of this “Wonder of the World” that remain. These are limited to a few approximate versions on Greek and Roman coins and the account of a Roman geographer and traveler on which the 3D printing restoration is based..\nSynovus Financial Corp. a financial services company based in Columbus, Ga., is sponsoring the exhibition.']	['<urn:uuid:954c8226-0fc7-4261-b9cc-37faaed551f1>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	6	74	1335
24	watch movement power generation without battery	Spring Drive is powered solely by a mainspring that drives a series of gears. At the end of these gears, a rotor generates a small electrical charge that activates an electronic circuit and quartz oscillator. This innovative system regulates a spring-driven movement with electronic technology without requiring batteries or external power sources, achieving unprecedented accuracy for a mainspring-wound watch.	"[""9R SPRING DRIVE MOVEMENT\nAll Spring Drive movements already offer a remarkable 72 hours of power reserve. The challenge was clear. Could this be extended still further?\nThe team at the Micro Artist Studio thought so and the Grand Seiko 8-Day Spring Drive is the result. To maintain perfect timekeeping for up to eight days (192 hours) requires greatly increased power, which is provided by a linked series of three barrels. This remarkable caliber, 9R01, has also been designed throughout to maximize energy efficiency, reducing friction between parts that would otherwise lead to loss in the transmission of the energy derived from the mainspring.\n±0.5 seconds per day (±10 seconds per month)\nApproximately 192 hours\nPower reserve display function, Independent hour-hand adjustment function\nThe 3 barrels for increased power and reduced energy transmission loss\nIn order to realize the eight-day (192 hours) power reserve of Caliber 9R01, three barrels are used. They are arranged in sequence for expanded capacity.\nThe whole movement has been designed to reduce friction between parts that would otherwise lead to loss in the transmission of the energy derived from the mainspring. Precise manufacturing and polishing delivers reduced friction, as does the removal of the intermediate wheel which increases the efficiency of the energy transfer by 10%.\nA solid, one-piece bridge for enhanced durability\nIn the assembly of most watch movements, the gears and other components are placed on the main plate which acts as a base and these parts are then held in place on the main plate by several bridges. For Caliber 9R01, however, Grand Seiko developed a unique system using a single, one-piece bridge. This system delivers enhanced durability and is, we believe, a unique design.\nThe thickness of this one-piece bridge is some three times greater than any other Spring Drive caliber. Made possible by very high precision engineering, it provides a totally solid base on which all the components are secured in place more precisely.\nIn addition, the greater thickness of the bridge enhances the durability of movement as the gear shafts are better protected in the event of impact or other shock.\nThe Micro Artist Studio's first Spring Drive watch\nCaliber 9R01 is the first Grand Seiko Spring Drive movement to be created by the Micro Artist Studio in Shio-jiri in the mountains of central Japan. This remarkable Studio has been responsible for many of the company’s great achievements, including the Credor Spring Drive Sonnerie and Minute Repeater.\nWhat makes 9R Spring Drive special?\nSpring Drive, one of the great innovations in modern watch making history, regulates a spring-driven movement with state-of-the-art electronic technology that functions without batteries or other external power source. Its sole power source is a mainspring, which drives a series of gears. A rotor, connected to the end of these gears, generates a small electrical charge that activates an electronic circuit and quartz oscillator. Its accuracy is unprecedented for a watch wound by a mainspring. The development of the 9R Spring Drive movement was possible only because Grand Seiko is one of the few manufacturers with expertise in both mechanical and electronic watches.""]"	['<urn:uuid:07902c25-6fa3-4fb6-b4c5-fe8e75d15486>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	6	59	517
25	I read articles online about mental health. Can those count as learning?	Yes, home study programs, including online activities, can count for continuing education clock hours. They must meet NBCC guidelines for continuing education. You'll need documentation including your name, date attended, hours completed, program title and signature of presenter/provider.	"['Continuing education is an integral part of professional practice. There are many ways you can earn continuing education credit. Any of the following activities will qualify for continuing education clock hours to recertify your board certification, as long as the activity relates to an approved content area.\nFor any continuing education activity, you must have proper documentation, as described below, in order to claim credit. You can use the NBCC continuing education credit verification form for activities that do not provide a certificate of completion.\nAll courses must be graduate level. Courses may be audited. A transcript, grade report or verification form is required for documentation.\n1 semester hour = 15 clock hours\n1 quarter hour = 10 clock hours\nNBCC ACEP approval is not required; however, the courses must meet NBCC guidelines for continuing education. A certificate, verification form or letter verifying attendance is required for documentation. The documentation must include your name, date attended, hours completed, program title and signature of presenter/provider.\nAny amount of your continuing education clock hours can come from home study programs, including online activities. The same standards for selecting live programs apply to selecting home study programs. That is, they do not have to be NBCC ACEP approved, but they must meet NBCC guidelines for continuing education. A certificate, verification form or letter verifying attendance is required for documentation. The documentation must include your name, date attended, hours completed, program title and signature of presenter/provider.\nList the dates for which credit was granted by your college or university for work done toward the completion of your dissertation. List the title of your dissertation and the name of your college or university. A transcript, grade report or verification form is required for documentation.\nContinuing education clock hours can only be granted for formal consultation/supervision with a clinical focus received on a regular basis. Credit cannot be granted for supervision you provide to others. List the start and end dates and the site where you received supervision, as well as the name and credentials of your supervisor. A signed letter from your supervisor is required for documentation. Supervision for academic credit should be listed under coursework. A maximum of 25 contact hours can be granted in a certification cycle for supervision taken for academic credit. This supervision must appear on your transcript or grade report.\nTo qualify for continuing education credit, you must hold a leadership position for a minimum of one year and be within your current certification cycle. The position must be in a state or national counseling organization or state counseling licensure/certification board and must be unpaid. List the position, the start and end dates, and the name of the organization. A signed letter from the organization with the dates and type of service is the required documentation.\nVolunteer hours may be earned from a national, state, or humanitarian organization. The mission of the organization must fit into an approved content area. A certificate, verification form, or letter verifying attendance and a written statement explaining how the activity assists in your direct service to clients is required for documentation. The documentation must include your name, date attended, hours completed, title of the activity, and signature of a presenter/provider. Activities are considered on a case-by-case basis.\nClock hours earned for activities on topic areas that do not fit within NBCC’s nine counseling topic areas may be considered if the activity helps in your direct service to clients. A certificate, verification form, or letter verifying attendance and a written statement explaining how the activity assists in your direct service to clients is required for documentation. Programs are considered on a case-by-case basis.\nA passing score on the NCE or NCMHCE may be used in lieu of the required continuing education hours in the recertification and reinstatement processes.\nRemember to check the calendar of events for upcoming NBCC-approved continuing education events in your state.\nIt is not required that your continuing education clock hours come from NBCC Approved Continuing Education Providers; however, your activities must fall within at least one of these approved content areas.\nContinuing education programs in this content area provide an advanced understanding of the counseling processes.>\nFoundational and well-established counseling theories, principles and techniques and their application in mental health settings; diagnosis and treatment of mental disorders; wellness and prevention within counseling and treatment; crisis intervention techniques for counselors to use in response to disaster or other rapid-onset trauma-causing events, including psychological first aid strategies; psychophysiological awareness and mindfulness in the counseling process; distance counseling; and biofeedback used in a mental health setting.\nContinuing education programs in this content area provide an advanced understanding of the nature and needs of individuals at developmental levels and are relevant to professional counselors and the counseling profession.\nFoundational and well-established theories, including personality theory, life span theory, learning theory and human development theory; end-of-life issues within the scope of practice of mental health professionals; and information related to developmental crises, disabilities, and situational, transitional and environmental factors that affect behavior.\nContinuing education programs in this content area provide an understanding of the issues and trends in a multicultural and diverse society that impact professional counselors and the counseling profession.\nMulticultural competency as a professional counselor; societal subgroups; societal changes and trends; mores and interaction patterns; major societal concerns; social justice; and cross-cultural effects, such as acculturation, assimilation and multicultural and pluralistic trends.\nContinuing education programs in this content area provide an advanced understanding of therapeutic group development, dynamics and counseling theories and group counseling methods and skills.\nGroup counseling theories and group counseling leadership styles, developmental stages in group counseling and related principles of group dynamics, group counseling methods and skills, and group members’ roles and behaviors within a mental health setting.\nContinuing education programs in this content area provide an advanced understanding of career counseling, development and related life factors.\nTheories, principles and techniques of counseling as applied to work and career; career counseling processes and resources, including those applicable to specific communities and populations; career/life planning and decision-making models; life-work role transitions, including outplacement and retirement counseling; career development program planning, resources and program evaluation; assessment of workplace environment for purposes of job placement; trends in job search; and career avocational, educational, occupational and labor market information, and resources and career information systems.\nContinuing education programs in this content area provide an advanced understanding of approaches to assessment and evaluation in counseling practice.\nDiagnostic interviewing; clinical assessment and interpretation for use in treatment planning; ethical strategies for selecting, administering and interpreting assessment and evaluation instruments; information on the current edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM); implementation and interpretation of standardized assessment instrument protocols; new and existing assessments; issues related to assessment development; legal issues related to counselor authority to administer and interpret tests and inventories that assess psychopathology, abilities, interests and career options; and information on basic classification, indication and contraindications of commonly prescribed psychopharmacological medications, in order to make appropriate referrals for medication evaluations and to identify the side effects of medication.\nContinuing education programs in this content area provide an advanced understanding of research methods, statistical analysis, needs assessment and evaluation, and ethical and legal considerations in research.\nCritical evaluation of research findings and its clinical implications, development of measurable outcomes for counseling interventions, foundational and well-established research design and methods, statistics in research, research report development and information on disseminating research findings, and critical evaluation of research findings.\nContinuing education programs in this content area provide an understanding of various aspects of professional functioning as graduate-level counselors.\nNational Board for Certified Counselors (NBCC) or American Counseling Association (ACA) ethical codes and conduct standards in professional counseling and/or counseling research; ethical decision-making models directly related to counseling practice; NBCC certification, state licensure and related mental health public policy; professional counseling organizations and associations, limited to NBCC, the Council for Accreditation of Counseling & Related Educational Programs (CACREP), and ACA and its branches and divisions; roles and processes related to counselor advocacy on behalf of the profession and the client; the counselor’s role and function as a member of an interdisciplinary team; starting and maintaining a counseling practice, including business-planning, quality control, service delivery, billing and reimbursement, fee-setting, capacity-building, referrals and ethical marketing; effective and ethical utilization of social media; counselor self-assessment to address areas for ongoing professional growth; critical analysis of emerging issues in the counseling profession; clinical supervision models, practices and processes for professional counselors; the counselor’s role and function as a professional consultant; the counselor’s role and function as a forensic interviewer; legal issues in professional counseling and/or counseling research; self-care strategies and burnout prevention appropriate to the counselor role; and grant writing for counselors and counseling programs.\nContinuing education programs in this content area provide psychoeducational information for counselors to enhance their ability to promote optimal wellness related to client mental health.\nComplementary and alternative medicine (CAM), community-based wellness initiatives, conflict resolution, sex education, sleep hygiene, nutrition, and anger management.\nClock hours earned for activities on topic areas that do not fit within NBCC\'s nine counseling topic areas may be considered if the activity helps in your direct service to clients. A certificate, verification form, or letter verifying attendance and a written statement explaining how the activity assists in your direct service to clients is required for documentation.\nPrograms are considered on a case-by-case basis.\nNCCs have immediate access to NBCC Approved Continuing Education Providers (ACEPs) and continuing education programs via the ACEP Directory. When registering for a course offered by an ACEP, you know that the CE provider has been vetted and that their programs offering NBCC credit will meet your NCC recertification requirements.\nRemember to check the calendar of events for upcoming NBCC-approved continuing education events in your state.\nNBCC collaborates with Psychology Today to identify featured articles that are eligible for continuing education for NBCC’s certifications. Your board certification allows you to earn continuing education clock hours for reading featured articles and completing an assessment through i-counseling. Once completed, print out the certificate and upload it to your ProCounselor account.\n""Protect Yourself from Emotional Contagion""\n""What Makes Female Psychopaths Different?""\n”The New Science of Sleep”\n”A Different Mix”\n”Silencing Your Inner Critic”\n”The Puzzle of the Unbalanced Mind”\n”Orchids and Dandelions”\n”Seeing Beyond Depression”\n”7 Extraordinary Feats Your Brain Can Perform”\n”Tangled in Turmoil”\n""10 Signs You Know What Matters”\n“The Tough-to-Treat Kid”\n“From Loss to Love”\n""The Stealthiest Predator""\n""To Love and To Envy""\n""Your Elastic Mind""\n""The Most Extraordinary Amnesia""\n""Where No One Hears a Cry for Help""\n""The Comparison Trap""\n""The High Cost of Caring""\n""The Big Data Bull\'s-Eye""\n""The Hardest Word""\n""The Mad Genius Mystery""\n""Down with Extremes!""\n""A Doorway to Change""\n""The Vlogging Cure""\n""Up From Chaos""\n""The Mystery of Motivation""\n""When the Body Speaks""\n""The Fraud Who Isn’t""\n""No Known Expiration Date""\n""New Reflections on Narcissism""\n""Love Beyond Gender""\n""Right Where You Belong""\n""Breaking the Code of the Streets""\n""Rewrite Your Life""\n""Duet Ex Machina""\n""Moment of Impact""\n""It’s Not All About You!""\n""How to Pick a President""\n""The 10-Second Take""\n""Sympathy for the Deviant""\n""The Einstein of Love""\n""The Drama of Deception""\n""The Voice of Reason""\n""The New Survivors""\n""Building the Perfect Day""\n""Beyond Happiness, The Upside of Feeling Down""\n""Are These Rules Worth Breaking?""\n""A Death by Design""\n""Just Say It""\n""Seeing the Light""\n""Partners in Crime""\n""Parents Just Don’t Understand""\n""The Night Shift""\n""Dreams of Glory""\n""The Beat (Up) Generation""\n""Meet the Supertaskers""\n""Love and Power""\n""The Power of No""\n""The Problem with Rich Kids""\n""Kings and Queens of Chaos""\n""When Virtue Becomes Vice""\n""Love in the Time of Autism""\n""What Happy People do Differently""\n""Confessions of a Sociopath""\n""The Enemies of Invention""\n""The Against-All-Odds Club""\n""Louder Than Words""\n""Living a Lie""\n""The Mixed Bag Buddy""\n""What\'s in a Face""\n""The Everyday Fear Zone""\n""Lessons for Living""\n""From Promise to Promiscuity""\n""The High Art of Handling Problem People""\n""The Atheist at the Breakfast Table""\n""The Perfect Amount of Stress""\n""Lighten Up and Terrorized by the Tongue""\n""How to Save Your Brain""\n""The Boom and Bust Ego""\n""The Uses and Abuses of Optimism""\n""Your Backup Brain""\n""Self-Sabotage : The Enemy Within""\n""The Art of Influence""\n""How to Spot a Narcissist""\n""Sense and Sensitivity""\n""To Know Or Not To Know""\n""The Idea That Wouldn\'t Die""\n""A Chic Critique""\n""The American Nightmare""\n""Secrets of Special Agents""\n""Revenge of The Introvert""\n""The New Quitter""\n""Get Lucky: Five Principles for Making the Most of Lifeâ€™s Twists, turns, and Coincidences""\n""Go Ahead, Obsess!""\n""The Expectations Trap""\n""Portrait of a Hunger Artist""\n""Hypochondria: The Impossible Illness""\n""Who has a Difficult Mother?""\n© 2019 | National Board for Certified Counselors, Inc. and Affiliates']"	['<urn:uuid:0c2fa361-7a5f-47c1-bd64-3eb78c92d40d>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	12	38	2115
26	Do pull printing and old quarto printing both use pre-specified printers?	No. Pull printing allows collecting prints from any printer of choice, while old quarto printing used a specific press with a fixed platen and forme for each page.	['Sometimes in rare book cataloguing you come across something that requires you to flex you analytical bibliography muscles. It can be amazing what you can gather from the study of the physical form of a particular volume.\nIn the following case we managed to learn quite a bit about the printing practices in Cologne during the 1470s from the study of one page.\nSo, one day I was merrily cataloguing CRC Inc.S.16/2 (De excidio Troiae historia. Not printed after 1472) when I turned a page and found this:\nNot actually a bad photograph, but a badly printed page. Possibly what is known as a “slur” where the platen (we’ll get into that later) moves during the printing process and causes the ink to smear. But more likely the platen was lowered twice on the same page, whether on a one- or two-pull press is open to debate.\nSo far, so what. ¯\\_(シ)_/¯\nWell, it occurred to me that there was only one mis-printed page. In the printing process there will always be a partner page printed on the same sheet, which is then folded. So, I checked the partner of our mis-printed page and found that it wasn’t blurred. This book was a quarto which meant, as I’m sure you’re all thinking, that that was impossible.\nOkay, so a lot of jargon there. Let me break this down.\nThis is a diagram of a hand-pulled press. Showing the frisket, tympan, forme, press stone and the aforementioned platen.\n(Public domain image made available by Smithsonian Libraries (AE25.E53X 1851 Plates, t.7, “Imprimerie en caracteres,” plate 15))\n- Frisket: Used to hold the paper in place on the tympan and to mask off areas that you don’t want printed.\n- Tympan: Holds the paper using small pin-like pieces of metal.\n- Forme: The name given to the frame that the type is tightly packed into.\n- Press Stone: The frisket and tympan are folded onto the press stone.\n- Platen: Is the part of the press that applies the pressure to the paper on the forme.\nThe illustration above is actually a two-pull press. In this case the press is set up for a quarto sheet with four pages to be printed. The stone is rolled under the platen once, the platen is pressed down printing two pages, then it’s lifted and the stone is rolled further in and the platen is lowered again, printing the final two pages.\nIn the case of a one pull press, the platen is lowered once. If it’s a folio then one page is printed, if it is a quarto then two pages are printed. After it’s printed, the forme is reset with the next page(s) to be printed.\nNow the complicated bit.\nLet’s talk about formats. Folio, quarto, octavo, etc.\nThe format of a book is determined by how many pages are printed on a sheet and how many times that sheet is folded.\nSo, for example, one sheet of paper is printed on both sides, then folded once.\nThis is a folio. It’s folded once along the y-axis. Giving two leaves or four pages.\nFront of sheet\nBack of sheet\nThe Folger Library has an interesting website that lets you play with Shakespeare’s First Folio where you can assemble sheets into “gatherings”.\nThis is a quarto. It’s folded twice. First the y-axis, then the x-axis giving four leaves or eight pages.\nFront of sheet\nBack of sheet\nCheck out this video to see how it’s done.\nOctavos are folded three times, giving you eight leaves or sixteen pages. And so on …\nOkay now that we’re all experts on formats, let’s stampede over to chain lines.\nChain lines are formed during the paper making process. The mould used to make the paper is dipped into a vat of pulped linen and the water is sieved away leaving behind an impression of the mould.\nCheck here to see the process.\nThe mould consists of wire sewn onto supports, it’s these supports that leave the chain line impressions.\nHere’s a paper mould.\nThe thicker, vertical lines you can see are imparted onto the sheet of paper during the paper-making process and will end up looking something like this.\nChain lines help to determine the format of a volume. With a folio the sheet is folded once along the y-axis, therefore the chain lines will be vertical on the page. If there is a watermark (and there isn’t always!) it is placed on the right-hand side of the sheet.\nIn the example below there’s a watermark on the right-hand sheet and a countermark on the left. When the sheet is folded the chain lines will be vertical and the watermark will be in the centre of the page.\nWith a quarto the sheet is folded once along the y-axis, then once along the x-axis therefore the chain lines will be horizontal on the page. The watermark will be in the gutter, often difficult to see, especially in tightly bound books.\nPhew! Okay, we now have all that knowledge, so here’s why that blurry page is so weird. The chain lines and watermarks in the book show that it is a quarto. And if you remember from before, quartos are printed either two or four pages at a time, so how can there be only one mis-printed page on a sheet? The conjugate page should be mis-printed as well.\nWhen the platen lowered the mis-printed page should have had a mis-printed partner:\nIf the red page is the mis-printed page, then the green page must be mis-printed because the platen would be lowered on the both at the same time.\nNo such mis-printed partner existed.\nThis volume was printed before 1476, we know this because the rubricator (someone who would emphasise areas of the text with red ink) very kindly dated his rubrication. So, it’s a very early quarto. What if the printer viewed printing a quarto like printing a small folio?\nPossibly they used a half sheet and imposed the quarto as a folio, and then printed it a page at a time. That would allow for only one page to be mis-printed. We checked the watermarks and chain lines and established that these were indeed half sheets.\nCalls went out on Twitter; colleagues were asked for their opinions. Robert MacLean at the University Glasgow put us on to Karina de la Garza-Gil at the University of Cologne who confirmed that the common practice for Cologne printers at that time was to print quartos in half sheets one page at a time.\nAll that was left was to work out how it happened.\nThere is no smearing of the ink, and the first printing is sharp if faint. This makes it unlikely that anything twisted or moved, so perhaps the printer lowered the platen once and changed their mind before lowering it with the required force a second time.\nThe final mystery: was it a one or two pull press? It would be pure speculation to decide either way. Arguments could be made for either. At this point you really need to be able to read the mind of a printer from five hundred years ago. What we do know is that printing quartos on full sheets on a two-pull press became common a few years after this particular book was printed.\nIn the end what this does show, is how much information can be gleaned from analysing the physical properties of a book. From one mis-printed page we established the printing practices in Cologne from five hundred years ago.\nAnd that end’s the tale of the blurry page!', 'Pull printing, private cloud printing with WeP\nWhat is pull printing?\nPull printing is a system where users select and release (pull) their print jobs when they arrive at the printer of their choice, rather than sending a print job from their PC (push) to a pre-specified printer or MFP (multifunction printer) for immediate printing.\nHow does pull printing work?\nUnlike the more traditional push printing – where someone prints a document, and their PC (or the print server if one is in use), sends or pushes the job straight to the printer or MFP in question. The job prints immediately or as soon as it can be processed by the queue, and the owner collects it. This is the typical experience most people will think of when it comes to printing.\nPull printing, by contrast, works as follows: someone sends a job to their printer as normal, but the Managed Print Software (MPS) system does not print it immediately – instead the job is stored in a queue in a secure database on the server. When the person arrives at the printer of their choice in order to collect their job, they have to authenticate with the MPS system, upon successful completion of which, the MPS system releases only that persons’ selected print job. The location of the printer is largely irrelevant: the software automatically directs the job to whichever printer the person has authenticated at. Authentication can be done in a variety of ways, from RFID cards to PINs or passwords, biometrics, QR codes or NFC. Many organizations already have magnetic or RFID physical access control systems in place, these existing cards can usually be used for pull printing authentication as well.\nSecurity and waste elimination\nWhen considering the effectiveness of pull printing, the fact that security surrounding printouts is substantially improved immediately becomes clear. Only an authenticated employee can collect a job, and they have to be physically standing in front of the printer before their job even starts printing. There are other significant benefits too, though, such as the fact that paper and toner wastage is dramatically reduced. Countless print jobs are certainly sent to the incorrect printers, or simply printed out and then forgotten on a daily basis in organizations everywhere. Preventing this wastage by implementing authenticated pull printing will add up to significant cost savings when taking economies of scale into account, not to mention the positive effects on the environment through the reduced paper, cartridges, toner, and energy wastage.\nWhat are the benefits of WeP secure pull printing?\n- Secure, monitor and track all print, copy and scan tasks by user across an entire organization\n- Eliminate waste of consumables – cartridges, toner and paper\n- Use less energy\n- Reduce printer numbers\n- Improve efficiency of your print strategy\n- Reduce printing costs\n- Reduce overall cost of ownership\n- Simplify print management\nSentinel is compatible with printers, copiers, MFPs – multifunction printers from all the major brands of printer manufacturers including Toshiba, Brother, HP, Samsung, Ricoh, Sharp, Konica Minolta, Xerox, Lexmark, Develop, etc.\nCloud Printing with WeP\nThe nature of secure pull printing means that it is possible to send a job to print without knowing beforehand where you will collect it. Print jobs can be submitted from PC’s, laptops, mobile phones, tablets or any other mobile devices; they are then stored securely in a private queue on the Sentinel server or cloud. They can then be collected at any supported printer – whether in the room next door or a branch office on another continent. This is the power of WeP private cloud printing – hassle-free dynamic printing for users on the move.\nHow Does WeP Private Cloud Pull-Printing Work?\n- Send one or more documents to print.\n- When you are ready to retrieve the printouts, identify yourself at a Sentinel printer by typing a PIN code on the printer or swiping a magnetic or RFID card. Once the Sentinel server recognizes you via your RFID employee card, ID card or PIN code, it will list all the print jobs waiting for you.\n- Select a job to print, delete, view, hold, transfer to another printer, etc. The instruction is sent to the server for action. No one but you sees your documents.\n- Because documents are only released to users after they input authorization information, sensitive documents are not left on output trays or picked up by the wrong person.\n- The central Web-based server allows individual users to control their print jobs. The “follow me” feature allows users to transfer print jobs from one office printer to another by self-identifying at their device of choice.\n- Your time will no longer be invested in connecting different servers and print server definitions for your many subsidiaries. The system becomes inherently more stable.']	['<urn:uuid:d0634cd3-120e-4a97-9faa-c8766edf6cb1>', '<urn:uuid:04668809-0b41-4c0d-a7b4-6008cf296cab>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T00:43:36.053682	11	28	2075
27	How do the earliest clinical research experiments of new potential treatments for diseases take place before testing in humans?	Scientists usually do years of experiments in the laboratory and in animals before testing experimental treatments in people. Most of this early research occurs at universities and medical centers across the country, and the National Institutes of Health funds much of this basic research.	"[""Participating in Clinical Trials\nAbout Clinical Trials\nA Research Study With Human Subjects\nA clinical trial is a research study that involves human subjects. The purpose of a clinical trial is to find out if an experimental drug, therapy, medical device, lifestyle change, or test will help treat, find, or prevent a disease. A clinical trial may compare experimental products or tests to those already available or may compare existing treatments with each other in order to determine which one is safer or more effective.\nThe five main types of clinical trials are treatment, screening, diagnostic, prevention, and supportive care trials.\nIn treatment trials, researchers may\n- gather information about experimental treatments, their risks, and how well they work\n- compare existing therapies to decide which one, or a combination of them, is the best treatment for a disease\n- evaluate treatment methods such as surgical techniques, psychiatric therapy, or radiotherapy.\nScientists usually do years of experiments in the laboratory and in animals before they even consider testing an experimental treatment in people. Most of this early research occurs at universities and medical centers across the country. The National Institutes of Health funds much of this basic research.\nIn screening trials, researchers study ways of finding a disease before symptoms occur. These methods, often called screening tests, can include\n- imaging tests that produce pictures of what is inside the body\n- laboratory tests that check samples of blood, urine, or other body tissues\n- genetic tests that look for genes linked to some types of disease.\nIn diagnostic trials, researchers evaluate new tests that could identify a disease in its early stages. Usually, trial participants must show signs of the disease or condition before they can join this type of trial.\nIn prevention trials, researchers study ways to reduce the risk of getting a disease or a specific medical problem. These trials find out if\n- lifestyle changes, such as exercising more, getting more sleep, keeping mentally active, or eating nutritious foods, can prevent a problem\n- taking certain medicines, or vitamins, or getting vaccines will prevent a disease in people who have never had the disease or prevent a disease from returning.\nSupportive Care Trials\nIn supportive care trials, researchers look for ways to make life better for people living with a life threatening disease or chronic health problem. The goal may be to maximize comfort or to manage side effects of treatment. Researchers may study the role of caregivers, support groups, and various types of social interventions. Supportive care interventions are not intended to treat or cure a disease.\nPhases of Clinical Trials\nClinical trials of drugs are usually described based on their phase. The U.S. Food and Drug Administration typically requires Phase 1, 2 and 3 trials to be conducted to determine if the drug can be approved for use.\n- A Phase I trial tests an experimental treatment on a small group of often healthy people (20 to 80), to judge its safety and side effects, and to find the correct drug dosage.\n- A Phase II trial uses more people (100 to 300). While the emphasis in Phase 1 is on safety, the emphasis in Phase 2 is on effectiveness. This phase aims to obtain preliminary data on whether the drug works in people who have a certain disease or condition. These trials also continue to study safety, including short-term side effects. This phase can last several years.\n- A Phase III trial gathers more information about safety and effectiveness, studying different populations and different dosages and using the drug in combination with other drugs. The number of subjects usually ranges from several hundred to about 3,000 people. If the U.S. Food and Drug Administration agrees that the trial results are positive, they will approve the experimental drug or device.\n- A Phase IV trial for drugs or devices takes place after the U.S. Food and Drug Administration approves their use. A device or drug's effectiveness and safety are monitored in large, diverse populations. Sometimes the side effects of a drug may not become clear until more people have taken it over a longer period of time.""]"	['<urn:uuid:e8aa8107-351f-46a5-8ec2-26da914695e8>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	19	44	693
28	how many slaves weeks parker bequeath daughters	Weeks Parker bequeathed numerous slaves to his daughters through trusts after his wife Sabra's death. To Margaret H. Battle, he left slaves including Lucindy, Stephen, Turner, Lewis, George, Marina, Tony, Matilda, Caroline, William, Holly, Big Hardy, Ben, Cena, Moses, Syphax, Little Hardy, Jim, Lucy and Little Jim. To his other daughter Henrietta Battle, he left slaves including Barbara, Sarah, Luke, Ned, Sophia, Elick, Harrison, Milly, Jeffrey, Dorcas, Silas, Bill, Lou, Julia, Randal, Will and Abner.	['Joel Craig and Sharlene Baker’s As You May Never See Us Again: The Civil War Letters of George and Walter Battle, 4th North Carolina Infantry (2004) features the annotated missives of two sons of Amos Johnston Battle, a prominent (and peripatetic) Baptist minister who spent his last years, including the war era, in Wilson County. The letters contain only one presumed mention of the family’s slaves — a single reference to a Church, who was charged with delivering certain items to the letter’s writer. A footnote appended to the passage states: “The boy ‘Church’ has been referred to by some as one of the Battle’s [sic] slaves. Whether this is referring to the Raleigh [North Carolina] Battle’s or the Wilson Battle’s is unclear. However, if the Rev. Battle did own slaves in the midst of the war it might mean that he was not the abolitionist as previously thought.”\n… Who thought Amos J. Battle was an abolitionist?\nHe is listed in neither the 1850 and 1860 federal slave schedules, but his wife Margaret H. Battle is listed with 32 slaves in 1860. (Hugh Johnston noted that Amos Battle’s “wife owned a small farm north of Wilson not far from the Barnes plantation.”) She is not listed in the 1850 slave schedule, and the sudden acquisition of that many slaves suggests inheritance.\nMargaret Hearne Parker Battle’s father Weeks Parker died in January 1844 in Edgecombe County, leaving a widow and three children. (One predeceased him.) The 88 pages of his estate file span more than a decade, and Emancipation eventually intervened to prevent a final distribution. Included, however, is a listing of those slaves apportioned to daughter Margaret H. Battle and her children, apparently dating from the late 1850s: Old Ben, Old Seny, Big Hardy, Lucinda, Stephen, Turner, Hilliard, Mary, Adeline, William, Lena, Alice, William “usually called Reuben,” Little Ben, Harriet, Marina, Sally, Smith, Maria, Little Hardy, Betty, Jim, Moses, Syphax, Toney, Louis, Allen, George, Matilda, Lizzie.\nWeeks Parker had executed his will on 31 July 1843. The document mentions his wife Sabra [Irwin Hearn]; son Simmons B. Parker; deceased son Dr. John H. Parker, who had migrated to Florida; and daughters Henrietta, wife of Benjamin Battle, and Margaret, wife of Amos J. Battle. [Benjamin Dossey Battle was Amos’ brother.]\nWeeks designated son Simmons as his executor and trustee. He bequeathed certain slaves — Polly, Godwin, Old Ned, Winny, Hardy, Charlotte and her child Cintha, and Nelly — to pass to Simmons after wife Sabra’s death, and mentioned that he had already given Simmons 14 slaves in a deed of gift. He also directed Simmons to sell the land and slaves in Florida inherited from son John’s estate. (And tweaked this last provision in a codicil.)\nWeeks’ bequests to his daughters are curious though. After Sabra’s death, Simmons was to hold in trust slaves Lucindy, Stephen, Turner, Lewis, George, Marina, Tony, Matilda, Caroline, William, Holly, Big Hardy, Ben, Cena, Moses, Syphax, Little Hardy, Jim, Lucy and Little Jim “for the sole and separate use and benefit of daughter Margaret H. Battle wife of Amos J. Battle during her natural life free from the management and control of her present or any future husband.” Similarly, he directed that Simmons hold in trust after Sabra’s death slaves Barbara, Sarah, Luke, Ned, Sophia, Elick, Harrison, Milly, Jeffrey, Dorcas, Silas, Bill, Lou, Julia, Randal, Will and Abner for the benefit of daughter Henrietta Battle. Why the specific attempt to keep Amos Battle’s hands off his wife’s property? Was he in fact an abolitionist likely to try to free them? Or were Weeks’ concerns more prosaic?\nSimmons and his mother went into court to have Weeks’ will admitted to probate, and the skirmishes began. The two sets of Battles teamed up to claim that they had not been notified prior to probate and that the will’s codicil had been made under undue influence. Simmons and the other trustees admitted that Battles may not have been given formal notice, but claimed that they knew anyway. They also charged Amos Battle with having taken a slave named Jim to Wilmington. The Battles fired a second volley with a claim that Simmons was in “extreme bad health” and “great physical inability” and “utterly incapable of carrying out his duties” as a trustee. Simmons responded meekly, acknowledging that he had been shot in the chest many years before and had never recovered, a circumstance that sometimes completely debilitated him. He agreed to surrender his trusteeship. Replacement trustee Nathan Matthewson, too, stepped down, and was replaced by Benjamin Oliver of Duplin County. In one of Oliver’s reports, he advised the court that he had sold for $600 a slave named Jim “in consequence of grossly bad behavior and general bad deportment.” The buyer was Wyatt Moye. [In 1848, Moye, as Senator from Edgecombe County, introduced a bill in the Senate to “incorporate Toisnot Depot and Hickory Grove in the County of Edgecombe into a town by the name of Wilson.” The bill passed its third reading and was ratified on January 29, 1849.] With the funds received, Oliver then spent $500 to purchase Lilah from a Dr. Arrington. (She later gave birth to a son Charles.) In 1849, Oliver moved to Bladen County and resigned his trusteeship; Uriah Vaughan of Hertford County — where Margaret then lived — was appointed in his stead.\nIn the mid-1850s, Margaret, Amos and their children moved to the town of Wilson, where Sabra Parker bought them a house and lot. In another plaintive petition for yet another trustee, submitted in September 1856, Margaret complained that she had no other property and that the family was “dependent on their own exertions for a support” as their trust fund was inadequate. The younger children were chiefly supported by Margaret’s “exertions” [she was an innkeeper], while the creditors of her husband Amos, “who is greatly embarrassed,” tried to take her earnings at every opportunity.\nAnother source shines light on the Battle family’s financial situation. In 1911, Amos and Margaret’s youngest son, Jesse Mercer Battle, published memoirs titled Tributes to my Father and Mother and Some Stories of My Life. In the chapter on his mother, he recalled that his “mother’s family lived in Wilson, N.C. We lived in a large house, and it was called ‘The Battle House.’” There, to her humiliation, his mother took in boarders and other passers-through to earn money for the family’s keep. His father, though “rich in lands and negroes,” gave away his wealth to the point that his younger sons’ educations were neglected. The chapter on Amos J. Battle goes further. Amid fifty hagiographic pages limning his father’s Christlike-ness, Jesse reveals that “his money, his lands, his negroes, his stocks, his bonds, his personal property of every description went as his free will offering to the Church as a whole, and to anyone of its members individually, or to those who were not members.” (This was not offered ironically, and there is no attempt to square Battle’s slaveholding with his Christian values.)\nAh. So. And therein lies the motive for Weeks Parker’s determined attempt to keep his wealth out of pious Amos Battle’s hands.\nJesse Battle’s memoir also provides a peek at the family’s slaves and demonstrates that the thirty or so inherited from Weeks did not define the extent of Margaret’s holdings. “Negroes were my companions,” he wrote. “I played with them, and spent my time with them all day, till I was about seven years old, when I was started to school. I knew my alphabet and how to read a little. This start on my way to an education was given to me by a good old colored woman I called Mammy. (Her name was Dinah.) … This good woman remained with our family till 1865, when the Civil War ended, when she left us and moved down to Greenville, N.C., where her husband, whose name was ‘Shade,’ lived. After the emancipation of the slaves she said that she could never enjoy her ‘freedom’ as long as she lived with her master and mistress.” Jesse elsewhere mentioned that Dinah had lived with the family at a farm called Walnut Hill, “about three miles from Wilson N.C., on the railroad toward Rocky Mount.”\nWill Book F, Edgecombe County, North Carolina Probate Records, 1735-1970, familysearch.org; Estate of Weeks Parker (1844), Edgecombe County, North Carolina Estate Files, 1663-1979, familysearch.org; other sources as named.']	['<urn:uuid:4fd54425-79f5-4d64-9d15-2827fdef2f79>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	7	75	1402
29	potential damages wishcycling ewaste recycling systems	Both wishcycling and e-waste recycling face significant challenges that can damage recycling systems. Wishcycling wastes time by jamming machinery at recycling plants, costs money (estimated $250,000 annually in London, Canada), and can contaminate entire batches of recyclable materials. Similarly, improper e-waste disposal leads to environmental contamination with hazardous materials like heavy metals and polychlorinated biphenyls (PCBs), creating health risks and requiring complex remediation efforts. Both issues highlight the importance of proper waste management and following local recycling guidelines to protect recycling systems and environmental health.	['We’ve all been there. You’re standing in front of the recycling bin with something in your hand — perhaps a beverage cap, crinkly cellophane wrapping, the tinfoil lid from a sour cream container, or a pizza box. You’re not entirely sure whether you can recycle it, but you think you can.\nYou’d rather err on the side of caution than waste, so you toss the item into the recycling and figure that if it’s not suitable, the recycling facility can sort it out. No harm, no foul — right?\nUnfortunately, no. Gather ’round while I explain how wrong we are for doing this.\nRecyclers have a word for this kind of well-intentioned, but ultimately harmful, behavior: It’s called “wishcycling.” The apt term seems to have been coined by Bill Keegan, president of a recycling company in Shakopee, Minnesota, and it’s loosely defined as “the practice of tossing questionable items in the recycling bin, hoping they can somehow be recycled.”\nIt’s incredibly common, but also incredibly damaging to the very recycling programs you’re trying to support with your eco-friendly efforts. The issue with wishcycling is that, although it seems like it’s a great idea, tossing potentially unrecyclable items into your blue bin ends up costing time, money and even sometimes creating far more waste. Here’s how.\n1. Wishcycling Wastes Time\nMost recycling plants these days rely on complex machinery to sort and process our recyclables. These machines are designed to recognize and appropriately divert a specific set of items — namely, the items accepted by your local recycling program. When things like cling plastic film, shredded paper, or bottle caps get tossed in with proper recycling items, it can jam up the machinery. It’s not uncommon to have workers spend up to a few hours fixing the issue, grinding the whole operation to a halt, and temporarily preventing any other items from being recycled.\n2. Wishcycling Wastes Money\nWe’ve all heard the saying “time is money,” right? Well, the time wasted on fixing clogged machinery and having an entire recycling plant put on pause while the issue is fixed ends up costing taxpayers money — and potentially jeopardizing recycling plant operators’ relationship with the commodities market.\nIf recyclers bale materials with high rates of contamination, markets will not purchase them, which effectively makes recycling unsustainable,” Lara Rezzarday, spokeswoman for Waste Management in Denver, told the Coloradoan. “In order for recycling programs to be sustainable, it is important that we all maintain material stream integrity and quality.”\nDirector of Environment, Fleet and Solid Waste for London, Canada, Jay Stanford estimated the annual costs of improper recycling and wishcycling to be around $250,000 (in 2015). That’s a hefty price tag for this crossed-fingers approach to recycling.\n3. Wishcycling Can Create More Waste\nRecycling operations work by collecting, sorting, and then selling their recycled material. Recycling is typically done in batches, and if a nonrecyclable item works its way in — by confusing the machinery or eluding hand-sorting — it risks contaminating the entire batch. What does that look like? An Earth911 article on what not to put in your recycling bin explains:\nWhen paper products are recycled, they are mixed with water and turned into a slurry. Since we all know water and oil don’t mix, the issue is clear. Grease from pizza boxes causes oil to form at the top of the slurry, and paper fibers cannot separate from oils during the pulping process. Essentially, this contaminant causes the entire batch to be ruined. This is the reason that other food-related items are nonrecyclable (used paper plates, used napkins, used paper towels, etc.).”\nEditor’s note: Although a recent study determined that recycling pizza boxes is possible, please follow the guidance provided by your recycling provider regarding what you can put in the bin.\nBeat Wishcycling by Learning the Local Recycling Guidelines\nSo, while you think you’re helping out by tossing anything and everything that could possibly, maybe, sort of be recycled into your blue bin, you may actually be dooming literal tons of other, properly recycled, items to the trash.\nWhat’s a wishcycler to do? First of all, familiarize yourself with your city’s recycling program and its unique guidelines — almost every program is different. Do a quick Google search, use the Earth911 recycling search, or call your local municipal office if you are having trouble finding specific guidelines.\nPrint the guidelines and keep them taped on your fridge or beside your recycling bin for easy reference. Just because some plastics are accepted doesn’t mean all plastics are accepted; just because you can recycle glass bottles doesn’t mean you can also toss in their metal or plastic bottle caps.\nGetting familiar with the requirements of your local recycling program means that you can finally make the most of your good intentions and happily start recycling the right way.\nOriginally published on January 11, 2017, this article was updated in June 2021.', 'Adverse Human and Environmental Health Effects of Electronic Wastes: A Critical Review and Consequence Analysis\nIncrease in obsolete electronics is a direct consequence of rapidly changing technologies and an upsurge in the global demand for new electronic devices. This has led to large quantities of electronic products discarded as electronic waste (e-waste), creating enormous environmental health challenges impacting the management of the product life cycle (PLC) of the global product chain for manufactured electronics. The environmental health impacts of e-wastes vary widely across geography. Major contributors of e-waste production as well as major e-waste importing countries are identified in this study. The “Chemicals in Products (CiP)”, their fate and distribution in the environment, based on the local geology of the land use, and toxicological effects in the human body upon exposure, are investigated. The study outcomes present the need for recommended actions and prospective solutions for mitigating the associated hazards from CiP and the risks of potential adverse effects to humans and the environment. Effective handling and PLC management of e-waste can be achieved through proper implementation, permeable reactive barriers use in incinerators, oil-water solvent systems for contaminant removal and revision of existing regulations. These recommendations will help protect the health of children and adults in susceptible areas and improve electronic products management and environmental sustainability.\nElectronic Waste, Heavy Metals, Polycyclic Aromatic Hydrocarbons, Polychlorinated Biphenyls, Contamination, Exposure, Toxicology, Remediation\nUnited States Census Bureau - USCB (2017). U.S. and World Population Clock. Retrieved December 30, 2017 from https://www.census.gov/popclock/\nGSMA Intelligence (2017). Global Mobile Trends September 2017. Retrieved December 30, 2017 from https://www.gsmaintelligence.com/research\nPerez-Belis, V., Bovea, M. D., Ibanez-Fores, V., (2015). An in-depth literature review of the waste electrical and electronic equipment context: trends and evolution. Waste Manag. Res. J. Int. Solid Wastes Public Clean. Assoc. ISWA 33 (1), 3-29.\nAwasthi, A. K., Zeng X., Li J. (2016). Environmental pollution of electronic waste recycling in India: A critical review. Environmental Pollution 211 (2016) 259-270.\nKinhal, V. (2017). Highest e-waste generating countries in the world. World Atlas. Retrieved November 22, 2107 from http://www.worldatlas.com/articles/highest-e-waste-generating-nations-in-the-world.html\nWorldometers (2017) - Current World Population. Retrieved December 30, 2017 from http://www.worldometers.info/world-population/\nUnited Nations Environment Programme [UNEP]. (2015). Illegally Traded and Dumped E-Waste Worth up to $19 Billion Annually Poses Risks to Health, Deprives Countries of Resources, Says UNEP report. Retrieved November 22, 2017 from http://web.unep.org/newscentre/illegally-traded-and-dumped-e-waste-worth-19-billion-annually-poses-risks-health-deprives-countries\nSthiannopkao, S., Wong, M. H., 2013. Handling e-waste in developed and developing countries: Initiatives, practices, and consequences. Sci. Total Environ. 463-464, 1147-1153.\nBalde, C. P., Wang, F., Kuehr, R., Huisman, J., (2015). The Global E-waste Monitor 2014. Quantities Flows and Resources. United Nations University, IAS - SCYCLE, Bonn, Germany, pp. 1-41. Institute for the Advanced Study of Sustainability. Retrieved December 30, 2017 from http://i.unu.edu/media/ias.unu.edu-en/news/7916/Global-E-waste-Monitor-2014- small.pdf.\nPerkins, D. N., Brune-Drisse, M., Nxele, T., & Sly, P. D. (2014). E-waste: A global hazard. Annals of Global Health, 80 (4), 286-295. doi: 10.1016/j.aogh.2014.10.001\nLi, J., Zeng, X., Chen, M., Ogunseitan, O. A., Stevels, A. L. N., (2015). “Control-Alt- Delete”: rebooting solutions for the e-waste problem. Environ. Sci. Technol. 49 (12), 7095-7108.\nRobinson, B. H. (2009). E-waste: An assessment of global production and environmental impacts. Science of the Total Environment, 408 (2), 183-191. doi: 10.1016/j.scitotenv.2009.09.044\nZheng, H., Hu, G., Xu, Z., Li, H., Zhang, L., Zheng, J., He, D. (2015). Characterization and distribution of heavy metals, polybrominated diphenyl ethers and perfluoroalkyl substances in surface sediment from the dayan river, south china. Bulletin of Environmental Contamination and Toxicology, 94 (4), 503-510. doi: 10.1007/s00128-015-1479-7\nYuan, J., Chen, L., Chen, D., Guo, H., Bi, X., Ju, Y., Chen, X. (2008). Elevated Serum Polybrominated Diphenyl Ethers and Thyroid-Stimulating Hormone Associated with Lymphocytic Micronuclei in Chinese Workers from an E-Waste Dismantling Site. Environmental Science & Technology, 42 (6), 2195–2200. doi: 10.1021/es702295f\nZhang, Y., Luo, X., Mo, L., Wu, J., Mai, B., & Peng, Y. (2015). Bioaccumulation and translocation of polyhalogenated compounds in rice (Oryza sativa L.) planted in paddy soil collected from an electronic waste recycling site, South China. Chemosphere, 137 (Journal Article), 25–32. doi: 10.1016/j.chemosphere.2015.04.029\nZeng, X. L., Song, Q. B., Li, J. H., Yuan, W. Y., Duan, H. B., Liu, L. L., (2015). Solving e-waste problem using an integrated mobile recycling plant. J. Clean. Prod. 90, 55-59.\nZhang, Y., Huo, X., Cao, J., Yang, T., Xu, L., & Xu, X. (2016). Elevated lead levels and adverse effects on natural killer cells in children from an electronic waste recycling area. Environmental Pollution, 213, 143–150. doi: 10.1016/j.envpol.2016.02.004\nHe, K., Sun, Z., Hu, Y., Zeng, X., Yu, Z., & Cheng, H. (2017). Comparison of soil heavy metal pollution caused by e-waste recycling activities and traditional industrial operations. Environmental Science and Pollution Research, 24 (10), 9387. doi: 10.1007/s11356-017-8548-x\nUS Environmental Protection Agency [EPA]. (2017). Basic information about landfills. Retrieved November 29, 2017 from https://www.epa.gov/landfills/basic-information-about-landfills\nBreivik, K., Armitage, J. M., Wania, F., & Jones, K. C. (2014). Tracking the global generation and exports of e-waste. do existing estimates add up? Environmental Science & Technology, 48 (15), 8735.\nVos, S. (2012). Electronic waste disposal. Duke university: Nicholas school of the environment. Retrieved November 29, 2017 from http://sites.nicholas.duke.edu/loribennear/2012/11/15/electronic-waste-disposal/\nPickren, G. (2014). Political ecologies of electronic waste: Uncertainty and legitimacy in the governance of E-waste geographies. Environment and Planning A, 46 (1), 26-45. doi: 10.1068/a45728\nGallo, D. T. (2013). Broad Overview of E-Waste Management Policies in the U.S. Environmental Protection Agency. Retrieved November 29, 2017 from https://www.epa.gov/sites/production/files/2014-05/documents/overview.pdf\nDasgupta, D., Debsarkar, A., Hazra, T., Bala, B. K., Gangopadhyay, A., & Chatterjee, D. (2017). Scenario of future e-waste generation and recycle-reuse-landfill-based disposal pattern in india: A system dynamics approach. Environment, Development and Sustainability, 19 (4), 1473-1487. doi: 10.1007/s10668-016-9815-6\nWang, H., Yu, Y., Han, M., Yang, S., Li, Q., & Yang, Y. (2009). Estimated PBDE and PBB congeners in soil from an electronics waste disposal site. Bulletin of Environmental Contamination and Toxicology, 83 (6), 789-793. doi: 10.1007/s00128-009-9858-6\nUS Environmental Protection Agency [EPA]. (n.d.). PCBs questions and answers. Retrieved December 1, 2017 from https://www3.epa.gov/region9/pcbs/faq.html\nWilson, D. (2015). Toxins in WEEE (E-waste). University of Washington. Retrieved December 1, 2017 from http://ewaste.ee.washington.edu/students/impacts/\nHong, W., Jia, H., Ding, Y., Li, W., & Li, Y. (2016). Polychlorinated biphenyls (PCBs) and halogenated flame retardants (HFRs) in multi-matrices from an electronic waste (e-waste) recycling site in northern china. Journal of Material Cycles and Waste Management, doi: 10.1007/s10163-016-0550-8\nRabodonirina, S., Net, S., Ouddane, B., Merhaby, D., Dumoulin, D., Popescu, T., & Ravelonandro, P. (2015). Distribution of persistent organic pollutants (PAHs, me-PAHs, PCBs) in dissolved, particulate and sedimentary phases in freshwater systems. Environmental Pollution, 206, 38-48. doi: 10.1016/j.envpol.2015.06.023\nAgency for Toxic Substances and Disease Registry [ATSDR]. (2015). Public health statements for PCBs. Retrieved December 2, 2017 from https://www.atsdr.cdc.gov/phs/phs.asp?id=139&tid=26\nWang, Y., Luo, C., Wang, S., Cheng, Z., Li, J., & Zhang, G. (2016). The abandoned E-waste recycling site continued to act as a significant source of polychlorinated biphenyls: An in situ assessment using fugacity samplers. Environmental Science & Technology, 50 (16), 8623.\nLi, L., Li, Y., Richardson, J. B., Mark Bricka, R., Niu, X., Yang, H., & Jimenez, A. (2009). Leaching of heavy metals from E-waste in simulated landfill columns. Waste Management, 29 (7), 2147-2150. doi: 10.1016/j.wasman.2009.02.005\nKyere, V. N., Greve, K., & Atiemo, S. M. (2016). Spatial assessment of soil contamination by heavy metals from informal electronic waste recycling in agbogbloshie, ghana. Environmental Health and Toxicology, 31, e2016006. doi: 10.5620/eht.e2016006\nIsimekhai, K. A., Garelick, H., Watt, J., & Purchase, D. (2017). Heavy metals distribution and risk assessment in soil from an informal E-waste recycling site in Lagos state, Nigeria. Environmental Science and Pollution Research, 24 (20), 17206-17219. doi: 10.1007/s11356-017-8877-9\nBlanco, A., Salazar, M. J., Vergara Cid, C., Pignata, M. L., & Rodriguez, J. H. (2017). Accumulation of lead and associated metals (Cu and Zn) at different growth stages of soybean crops in lead-contaminated soils: food security and crop quality implications. Environmental Earth Sciences, 76 (4), 1–11. doi: 10.1007/s12665-017-6508-x\nWu, J.-P., Luo, Y., Luo, X.-J., Zhang, Y., Chen, S.-J., Mai, B.-X., & Yang, Z.-Y. (2008). Bioaccumulation of polybrominated diphenyl ethers (PBDEs) and polychlorinated biphenyls (PCBs) in wild aquatic species from an electronic waste (e-waste) recycling site in South China. Environment International, 34 (8), 1109–1113. doi: 10.1016/j.envint.2008.04.001\nTao, W., Zhou, Z., Shen, L., & Zhao, B. (2015). Determination of dechlorane flame retardants in soil and fish at guiyu, an electronic waste recycling site in south china. Environmental Pollution, 206, 361-368. doi: 10.1016/j.envpol.2015.07.043\nAckah, M. (2017). Informal E-waste recycling in developing countries: review of metal(loid)s pollution, environmental impacts and transport pathways. Environmental Science and Pollution Research, 24 (31), 24092–24101.\nGrant, K., Goldizen, F., Sly, P., Brune, M., Neira, M., van den Berg, M., & Norman, R. (2013). Health consequences of exposure to e-waste: a systematic review. Lancet Global Health, 1 (6), E350–E361. doi: 10.1016/S2214-109X(13)70101-3\nOlawoyin R. (2017). Exposure to Toxic Pollutants: Assessing Potential Human Health Risk. Professional Safety, 62 (2), 40-45.\nSong, Q., & Li, J. (2014). A systematic review of the human body burden of e-waste exposure in China. Environment International, 68 (Supplement C), 82–93. doi: 10.1016/j.envint.2014.03.018\nOlawoyin R, Heidrich B, Oyewole SA, Okareh OT, McGlothlin CW, (2014). Chemometric analysis of ecological toxicants in petrochemical and industrial environments. Chemosphere 112, 114–11\nBoston University School of Public Health [BUSPH] (2017). Exposure Assessment: Introduction to Basic Concepts. Retrieved December 30, 2017 from http://sphweb.bumc.bu.edu/otlt/mph-modules/exposureassessment/exposureassessment3.html\nXing, G. H., Chan, J. K. Y., Leung, A. O. W., Wu, S. C., & Wong, M. H. (2009). Environmental impact and human exposure to PCBs in Guiyu, an electronic waste recycling site in China. Environment International, 35 (1), 76–82. doi: 10.1016/j.envint.2008.07.025\nYang, Y., Xue, M., Xu, Z., & Huang, C. (2013). Health risk assessment of heavy metals (Cr, Ni, Cu, Zn, Cd, Pb) in circumjacent soil of a factory for recycling waste electrical and electronic equipment. Journal of Material Cycles and Waste Management, 15 (4), 556–563. doi: 10.1007/s10163-013-0120-2\nTao, X.-Q., Shen, D.-S., Shentu, J.-L., Long, Y.-Y., Feng, Y.-J., & Shen, C.-C. (2015). Bioaccessibility and health risk of heavy metals in ash from the incineration of different e-waste residues. Environmental Science and Pollution Research, 22 (5), 3558–3569. doi: 10.1007/s11356-014-3562-8\nChan, J. K. Y., Man, Y. B., Wu, S. C., & Wong, M. H. (2013). Dietary intake of PBDEs of residents at two major electronic waste recycling sites in China. Science of The Total Environment, 463–464 (Supplement C), 1138–1146. doi: 10.1016/j.scitotenv.2012.06.093\nFu, J., Zhou, Q., Liu, J., Liu, W., Wang, T., Zhang, Q., & Jiang, G. (2008). High levels of heavy metals in rice (Oryzasativa L.) from a typical E-waste recycling area in southeast China and its potential risk to human health. Chemosphere, 71 (7), 1269–1275. doi: 10.1016/j.chemosphere.2007.11.065\nLuo, C., Liu, C., Wang, Y., Liu, X., Li, F., Zhang, G., & Li, X. (2011). Heavy metal contamination in soils and vegetables near an e-waste processing site, south China. Journal of Hazardous Materials, 186 (1), 481–490. doi: 10.1016/j.jhazmat.2010.11.024\nLeung, A. O. W., Duzgoren-Aydin, N. S., Cheung, K. C., & Wong, M. H. (2008). Heavy Metals Concentrations of Surface Dust from e-Waste Recycling and Its Human Health Implications in Southeast China. Environmental Science & Technology, 42 (7), 2674–2680. doi: 10.1021/es071873x\nWu, C.-C., Bao, L.-J., Tao, S., & Zeng, E. Y. (2016). Dermal Uptake from Airborne Organics as an Important Route of Human Exposure to E-Waste Combustion Fumes. Environmental Science & Technology, 50 (13), 6599–6605. doi: 10.1021/acs.est.5b05952\nXing, G. H., Liang, Y., Chen, L. X., Wu, S. C., & Wong, M. H. (2011). Exposure to PCBs, through inhalation, dermal contact and dust ingestion at Taizhou, China – A major site for recycling transformers. Chemosphere, 83 (4), 605–611. doi: 10.1016/j.chemosphere.2010.12.018\nKim, K.-H., Jahan, S. A., Kabir, E., & Brown, R. J. C. (2013). A review of airborne polycyclic aromatic hydrocarbons (PAHs) and their human health effects. Environment International, 60 (Supplement C), 71–80. doi: 10.1016/j.envint.2013.07.019\nWang, J., Chen, S., Tian, M., Zheng, X., Gonzales, L., Ohura, T., Simonich, S. L. M. (2012). Inhalation Cancer Risk Associated with Exposure to Complex Polycyclic Aromatic Hydrocarbon Mixtures in an Electronic Waste and Urban Area in South China. Environmental Science & Technology, 46 (17), 9745–9752. doi: 10.1021/es302272a\nLuo, P., Bao, L.-J., Li, S.-M., & Zeng, E. Y. (2015). Size-dependent distribution and inhalation cancer risk of particle-bound polycyclic aromatic hydrocarbons at a typical e-waste recycling and an urban site. Environmental Pollution, 200 (Supplement C), 10–15. doi: 10.1016/j.envpol.2015.02.007\nGuo, Y., Huo, X., Wu, K., Liu, J., Zhang, Y., & Xu, X. (2012). Carcinogenic polycyclic aromatic hydrocarbons in umbilical cord blood of human neonates from Guiyu, China. Science of The Total Environment, 427–428 (Supplement C), 35–40. doi: 10.1016/j.scitotenv.2012.04.007\nZheng, J., He, C.-T., Chen, S.-J., Yan, X., Guo, M.-N., Wang, M.-H., Mai, B.-X. (2017). Disruption of thyroid hormone (TH) levels and TH-regulated gene expression by polybrominated diphenyl ethers (PBDEs), polychlorinated biphenyls (PCBs), and hydroxylated PCBs in e-waste recycling workers. Environment International, 102 (Supplement C), 138–144. doi: 10.1016/j.envint.2017.02.009\nWorld Health Organization [WHO]. (2010). Preventing Disease Through Healthy Environments. Retrieved on December 2, 2017 from http://www.who.int/ipcs/features/10chemicals_en.pdf?ua=1\nGuo, Y., Huo, X., Li, Y., Wu, K., Liu, J., Huang, J., Xu, X. (2010). Monitoring of lead, cadmium, chromium and nickel in placenta from an e-waste recycling town in China. Science of The Total Environment, 408 (16), 3113–3117. doi: 10.1016/j.scitotenv.2010.04.018\nLiu, J., Xu, X., Wu, K., Piao, Z., Huang, J., Guo, Y., Huo, X. (2011). Association between lead exposure from electronic waste recycling and child temperament alterations. Neuro Toxicology, 32 (4), 458–464. doi: 10.1016/j.neuro.2011.03.012\nWang, X., Miller, G., Ding, G., Lou, X., Cai, D., Chen, Z., Han, J. (2012). Health risk assessment of lead for children in tinfoil manufacturing and e-waste recycling areas of Zhejiang Province, China. Science of the Total Environment, 426, 106–112. doi: 10.1016/j.scitotenv.2012.04.002\nZhang, K., Schnoor, J. L., & Zeng, E. Y. (2012). E-waste recycling: Where does it go from here? Environmental Science and Technology, 46 (20), 10861–10867. https://doi.org/10.1021/es303166s\nUnited Nations Environment Management Group [UNEMG]. (2017). United Nations System-wide Response to Tackling E-waste. Retrieved November 22, 2017 from https://unemg.org/images/emgdocs/ewaste/E-Waste-EMG-FINAL.pdf\nOngondo, F. O., Williams, I. D., & Cherrett, T. J. (2011). How are WEEE doing? A global review of the management of electrical and electronic wastes. Waste Management, 31 (4), 714–730. doi: 10.1016/j.wasman.2010.10.023\nLong, Yuyang & Feng, Yi-Jian & Cai, Si-Shi & Hu, Li-Fang & Shen, Dong-Sheng. (2014). Reduction of heavy metals in residues from the dismantling of waste electrical and electronic equipment before incineration. Journal of Hazardous Materials. 272. 59–65. 10.1016/j.jhazmat.2014.02.048.\nBeiyuan, J., Tsang, D. C. W., Yip, A. C. K., Zhang, W., Ok, Y. S., & Li, X.-D. (2017). Risk mitigation by waste-based permeable reactive barriers for groundwater pollution control at e-waste recycling sites. Environmental Geochemistry and Health, 39 (1), 75–88. doi: 10.1007/s10653-016-9808-2\nYe, M., Sun, M., Wan, J., Fang, G., Li, H., Hu, F. Orori Kengara, F. (2015). Evaluation of enhanced soil washing process with tea saponin in a peanut oil–water solvent system for the extraction of PBDEs/PCBs/PAHs and heavy metals from an electronic waste site followed by vetiver grass phytoremediation. Journal of Chemical Technology & Biotechnology, 90 (11), 2027–2035. doi: 10.1002/jctb.4512']	['<urn:uuid:8af4f6c8-5675-4426-b9ca-9a5cd5ee47e0>', '<urn:uuid:292e510a-9722-4c47-9ed5-81cd75880cfd>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-13T00:43:36.053682	6	85	3285
30	I've been diagnosed with a spinal cord injury and I'm interested in staying active. What are the health benefits of physical activity, and what should I monitor about my shoulder health during exercise?	Physical activity offers numerous benefits: it improves posture, self-esteem, cardiovascular fitness, flexibility, and helps maintain a healthy weight. It also builds strong bones and muscles, improves mood, promotes relaxation, and provides opportunities for friendships and independence. Regarding shoulder health, it's crucial to monitor because individuals with spinal cord injury have high mechanical loading on their shoulders due to upper extremity dependency. This frequent loading increases the risk of tissue overload and shoulder complaints, which can affect functioning and quality of life. Movement patterns can change due to fatigue, potentially increasing the risk of shoulder pain. It's recommended to maintain sufficient shoulder capacity through appropriate exercise and to watch for signs of fatigue during activities.	"[""Physical activity is vital for everyone, and especially for people living with Spinal Cord Injury. Physical activity will help to:\n- Improve posture\n- Improve self-esteem\n- Prevent constipation\n- Improve cardiovascular fitness\n- Maintain and develop flexibility\n- Build strong bones and muscles\n- Improve mood and help relaxation\n- Promote healthy growth and development\n- Help achieve and maintain a healthy weight\n- Improve balance and develop coordination skills.\n- Provide opportunities for friendships and independence\nFitness and injury prevention\nThe fitter you are, the more you will enjoy being active. You will also be less likely to suffer from injury.\nTo improve physical fitness, it is advised that everyone needs at least one hour (and up to several hours) of moderate to vigorous physical activity every day.\nA combination of moderate and vigorous activities (“huff and puff”) is recommended. Try a variety of activities that are fun and suit your interests, skills and abilities.\nGood exercise routines can stay with you throughout life.\nWe should not spend more than two hours a day using electronic media for entertainment, for e.g. computer games, TV or the internet, particularly during daylight hours.\nA 5-10 minute warm up will help prepare the body for the activity. The warm up should consist of stretching activities, some continuous movement for 2-3 minutes e.g. arm/upper body movements and movements/games related to the activity.\nA 10 minute cool down should also follow the activity to remove waste products from the muscles so they don't become sore and stiff. The cool down should be some continuous movement for 2-3 minutes e.g. slower chair pushing/running and stretching the muscles that were used in the activity.\nBuilding strong upper body and limbs\nPhysical activity can help to build strong arms and torso. People living with Spinal Cord Injury need a strong upper body to help with balance; to move or transfer from their wheelchair; to propel the wheelchair and in some situations to walk using equipment.\nWays to use your upper body, build strength and improve fitness include:\n- Walking the dog\n- Rowing, canoeing and paddling\n- Sports such as basketball and tennis\n- Gym programs such as ‘Burn Rubber Burn.’\n- Using a bike with hand pedals – your OT/Physio can assist with adaptations.\n- Pushing in your wheelchair over a distance, using long arm movements. Speak to your therapist about the best way to move the wheels of the wheelchair\nPropelling your wheelchair (Information reproduced by kind permission of Paraquad NSW)\n- The most energy efficient way to push your wheelchair is using a semi-circular propulsion technique.\n- Use long smooth strokes to decrease the amount of times you need to push.\n- Take regular rest breaks and rest following fatigue to let your arms recover.\n- Maintain your wheelchair regularly to reduce rolling resistance from clogged castors and stored grime.\n- If using pneumatic tyres they should be kept at high pressure to make pushing easier.\nA semi-circular push stroke is where your hand falls below the push rim between push strokes.\nA few things to watch out for:\nDue to your spinal cord injury you most likely do not have normal sensation. Burns and abrasions can occur easily from hot and/or rough surfaces eg. slippery dips and sand. If you have poor balance when walking a wheelchair can be used successfully for sport, games and other physical education activities at home, school and in the community. Consult with your Spinal Cord Injury Service if you are unsure about what activities you should avoid.\nSports to try!\n- Bike riding\n- Canoeing and paddling\n- Swimming – Rainbow Club\n- Sailing – Sailability\n- Horse riding – Riding for the Disabled Association of Australia\n- Wheelchair Junior Sports – try all sorts of sports under the guidance of experienced coaches - basketball, rugby, tennis, swimming, athletics, canoeing, winter sports, or sports camps. NSW Wheelchair Sports Association Inc.\n- Anything that interests you!\nYou can find further information about being active at:\n- ‘Burn Rubber Burn’ is a health and fitness program run through PCYC for people over 12 years of age with physical disabilities:\n- “Armed for Life: Prevention Strategies and Management of Upper Limb Overuse Syndrome(s) for Persons with a Spinal Cord Injury (SCI): Strategies for upper limb preservation in daily activities following SCI.”\n- “Armed for Life: Prevention Strategies and Management of Upper Limb Overuse Syndrome(s) for Persons with a Spinal Cord Injury (SCI): Wheelchair Set up and Transfers.”\n- The Sydney Children’s Hospital Network “Pressure relief technique and spinal cord injury” fact sheet\n- NSW Wheelchair Sports Association Inc.\nThings to remember:\n- Physical activity improves fitness and helps to maintain a healthy weight.\n- Any activity that sees you expend energy is good!\n- Upper body strength assists in independence.\n- Sport provides opportunities for friendship and fun.\n- Be active every day to keep healthy.\nWritten by The Children's Hospital at Westmead, Sydney Children's, Randwick, Kaleidoscope Hunter Children's Health Network and Northcott"", 'Shoulder Health and Mobility\nDue to their upper extremity dependency in daily life, the mechanical loading on the shoulder in individuals with spinal cord injury (SCI) is frequent and high. This increases the risk of tissue overload and shoulder complaints, thereby affecting functioning, participation and quality of life. This is confirmed by results from the Swiss Spinal Cord Injury Cohort Study (SwiSCI).\nDue to its anatomical structure, the human shoulder enables a large range of motion while joint stability is mainly controlled by muscles, rendering the upper extremity vulnerable to complaints.\nIn order to diminish the shoulder complaints, we aim our research at:\n- studying underlying mechanisms of shoulder load, in our movement laboratory,\n- quantifying and relating load, capacity and tissue change,\n- development of methods in monitoring shoulder load in daily conditions.\nBased on increased insight in the relation of load, capacity and tissue change, injury prevention programs can be optimized, in an open and bilateral discussion with clinical experts.\nWe currently have a strong national and international collaboration with the Swiss Paraplegic Centre, the Vrije University in Amsterdam, University Medical Center in Groningen and are continuously expanding this collaboration with other research groups, clinical disciplines and the SCI population for their valuable input.\nWith these combined efforts we work towards a quality management system for upper extremity functioning, aiming at long-term preservation of shoulder health in SCI individuals.\nExamples of projects in relation to shoulder health:\n- Handcycling versus handrim wheelchair (completed project)\n- Muscle fatigue induced by wheelchair propulsion (current project)\n- Wearables & big data (future project)\nHandcycling versus handrim wheelchair propulsion\nSeveral factors are involved in the development of shoulder pain, but the repetitive load of wheelchair propulsion is seen as one of the major risk factors. However, mobility devices such as the wheelchair are important since they support people with spinal cord injury (SCI) to live an independent life. Therefore, alternative or additional mobility devices to the manual handrim wheelchair should be considered. Since upper extremity mobility is usually the only means of physical exercise for SCI individuals, a manual device is preferable to maintain regular physical activity.\nThe handbike is one of these alternative mobility devices. It is increasingly used for commuting, recreation and sports. Compared to handrim wheelchair propulsion, the usage of a handbike is physiologically (heart rate, breathing, energy expenditure) more efficient and less straining. Whether also the mechanical load at the shoulder is lower than during wheelchair propulsion has not been investigated.\nThe focus of this project was on the biophysical benefits of handcycling in order to investigate the general assumption that the handcycle is indeed a good device for the prevention of shoulder problems. Additionally, we studied the handcycle-user interface. This will allow for practical advice on how to best adjust the handcycle to its user in order to further reduce the load on the shoulder joint.\nThe figure below shows the difference in shoulder load between handcycling and wheelchair propulsion. Glenohumeral contact force is the force with which the shoulder joint is compressed during the propulsion. The compression is a result of the exerted force by the hand and of the contractions of all muscles around the shoulder to perform the required movement and to stabilize the shoulder joint while doing so. (For example, 10 Newton (N) is the force needed to hold a mass of 1 kg against gravity).\nFrom the graphs it can be concluded that the load on the shoulder is much lower for handcycling.\nMuscle fatigue caused by wheelchair propulsion\n30% to 70% of individuals with spinal cord injury (SCI) suffer from shoulder pain. This has an enormous impact on functioning, independence and quality of life. The treatment is often unsatisfying and the pain problem remains.\nEarly diagnosis and early intervention of shoulder problems can markedly reduce the impact of shoulder pain. Although the exact mechanism remains unclear, several known factors contribute to the development of shoulder pain.\nOne of these factors is the mechanical load on the shoulder in relation to its capacity, for instance: how much force is needed to propel a wheelchair, and what is the maximum propelling force an individual can deliver? The lower the maximum, the sooner muscles will be fatigued. Improving the capacity to a sufficient level will have a positive effect on the prevention of shoulder pain.\nIt is also known that movement patterns can change due to fatigue, by a change in the activation or coordination of muscles. A less optimal movement pattern during wheelchair propulsion, a weight relief lift or a transfer can increase the risk of shoulder pain. Movement patterns are measured in our motion lab.\nFinally, prolonged activities can also lead to tissue change, for instance thickness and appearance of biceps tendon. Such changes in tendon thickness can be observed and quantified by Ultrasound imaging, not directly during wheelchair propulsion, but shortly after a fatiguing exercise, or during a weight relief lift.\nHow these mentioned changes, due to fatigue, are related to wheelchair propulsion and mechanical load at the shoulder is not known. It is, however, required information for the further development of training and prevention programs.\nQuality management system of upper extremity functioningWearables & Big Data\nFundamental research is required to gain insight in working mechanisms of upper extremity function, for which controlled conditions in motion labs are essential.\nIn addition questionnaires are used to gather information on lived experience from larger groups of participants.\nBoth approaches aim at extracting information from real life, and after analysis and discussion, generalize conclusions back to real life. Both approaches are, however, also just snapshots of what happens in real-life.\nWith the rapid technological development in the past two decades a third approach comes into reach. A broad variety of wearable sensors and smartphones might enable the desired longitudinal monitoring of upper extremity functioning in real life, to obtain an overview of real life demands and clarify dose-response relations among:\n- physical activities,\n- biomechanical load on the upper extremity, up to the level of musculoskeletal modelling,\n- long-term effect on tissue change,\n- performance of function.\nBesides, these non-intervening, discrete and wearable devices can potentially also be used as feedback devices, during rehabilitation or after discharge in real life in order to:\n- optimize or maintain optimal wheelchair propulsion style,\n- indicate over- or underuse with respect to individual capacity,\n- support or coach in adherence to «healthy» performance.\nTo enable the development of such an upper extremity quality management system, we focus our research on:\n- development of methods and algorithms (e.g. to estimate biomechanical load in real life),\n- testing usability of equipment (e.g. Smart Watches, Internet of Things modules) to enable large scale measurement in real life,\n- creation of safe and adequate infrastructure (where to store and process data, who can access data).']"	['<urn:uuid:04c7f3e7-6f1a-4641-8574-c267979bc294>', '<urn:uuid:9075a208-866c-4118-a3af-98c04d46c992>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	33	114	1968
31	methods prevent ice formation aircraft surface engine moisture temperature control cost effective	There are multiple methods to prevent ice formation. Virginia Tech developed a system using grooved aluminum surfaces with sacrificial ice stripes that cover 10% of the surface, keeping 90% ice-free without electricity. For aircraft engines, anti-icing systems use electrically heated mats, circulated hot engine oil, or bleed air from the engine to prevent ice formation in air intake lips and guide vanes. These methods are more environmentally friendly compared to traditional approaches like antifreeze chemicals or heating systems that rely on fossil fuel power.	"['Developed by Virginia Tech (VT) researchers, the system can prevent moisture and ice from appearing on 90 percent of its surface. It also saves on electrical costs and is environmentally friendly to boot.\nFor the individual driver and homeowner, the anti-frosting surface promises to put an end to the chore of having to defrost or scrape windows during freezing mornings. For companies and consumers, it might just save billions of dollars in efficiency, maintenance, and repair.\nAircraft protected by the surface will no longer require extensive defrosting treatment that forces airlines to delay flights for hours. Power outages caused by excessive buildup of ice will also be greatly reduced. (Related: Innovative technology will allow dementia patients to live independently in their own homes.)\nThe VT researchers went for a straightforward design for the anti-frosting surface. Using untreated aluminum as the base material, they created ice stripes on an array of tiny grooves that rose from the surface.\nThe grooves are meant to be sacrificial areas. Stripes of ice will form on top of these grooves. The temperature of this intentional ice will form low pressure zones.\nMoisture present in the air is drawn to the low pressure zones of the intentional ice. Because of this suction effect, the intermediate areas that overlap the zones will remain free of frost. This effect was shown to work in conditions with moisture-laden air and close to freezing temperatures.\nThe sacrificial ice stripes comprise just 10 percent of the entire surface area of the anti-frosting material. The rest of the surface stays dry and ice-free. It does not use any electricity to achieve this effect.\n""The real power of this concept is that the ice stripes themselves are the chemistry, which means the material we use is irrelevant,"" explained VT researcher Jonathan Boreyko. ""As long as you have that proper pattern of sacrificial ice, the material you use could be virtually anything. So there are a lot of possibilities.""\nThe new anti-frosting technology will probably be welcomed by the HVAC (Heating, Ventilating, and Air Conditioning) industry. Similar micro-fin patterns can be found on the outdoor parts of heat pumps, fan systems, and other heat exchange products.\nThose patterns can be altered to resemble the grooves on the VT-derived surface to further protect those heating systems from frost. Other items that can mount this surface include airplane wings, aerospace materials, and windshields.\nThe researchers have already obtained a patent for their invention. They brought up another welcome benefit of their technology: Environmental consciousness.\nConventional means of defrosting ice-laden objects are known to indirectly or directly harm the environment. Heating systems use electricity, which often comes from fossil fuel power plants that spew air pollution.\nAntifreeze chemicals are suspected to have negative, long-lasting effects on plants, wildlife, and humans. Yet thousands of gallons of these synthetic chemicals are spent on just a single wing of an airplane. The antifreeze either contaminate groundwater or turn into airborne droplets that get inhaled by people.\nEven good old-fashioned salt will leave residues and loses effectiveness as it gets diluted. But the VT researchers are assured that their anti-frosting surface can do a better job of keeping windshields – and the rest of the world – clean.\nFor more stories about the newest anti-frosting technologies, visit Inventions.news.', ""If you wish to contribute or participate in the discussions about articles you are invited to join SKYbrary as a registered user\nIce Formation on Aircraft\nThis article aims to address the basics of ice formation on aircraft and in their engine air inlets. It does not consider the formation of frost which is created by sublimation, the process by which water vapour freezes directly onto sub zero surfaces. With this exception, the formation of ice attached to the external surface of an aircraft or to the surfaces within its engine air intakes, requires that liquid water drops impact the surface involved. Usually, that moisture must be supercooled, that is it must be in liquid form below 0°C273.15 K\n491.67 °R. There are a couple of specific variations in this general scenario which will be considered later:\n- issues relating to the difference between the temperature of the aircraft skin and the temperature of the air through which it is, has been or will be passing, especially during climb and descent and in the temperature range +/-10 degrees Celsius.\n- issues arising from the temperature and/or pressure difference between the ambient air and air within engine air inlets, most often reductions in air pressure or increases in air temperature.\nIce from Supercooled Moisture\nIce which accretes on the external parts of an aircraft is most often the result of the impact of supercooled water droplets of various sizes on that aircraft. This may happen within cloud or when flying through precipitation. The reason why water droplets do not all freeze as soon as the ambient temperature falls below 0°C273.15 K\n491.67 °R is the release of latent heat as water changes state to ice. So much latent heat is released that the change of state is slowed down so that it takes place progressively as temperature continues to fall. This continues until, by about -20°C253.15 K\n455.67 °R, most of the by now supercooled water has turned to ice. Ice grains which have already fully formed and are dry when they impact an aircraft do not adhere but simply bounce off. Therefore, the relative severity of ice accretion can be expected to progressively decrease as ambient temperature reduces below 0°C273.15 K\n491.67 °R so that little, if any, risk of accretion remains below -20°C253.15 K\n455.67 °R. Two factors are important in respect of supercooled water droplets:\n- the extent of their presence, which will affect the rate of any ice accretion; and\n- their size, which will affect the severity of that accretion by adversely influencing its rate.\nBoth the quantity and, perhaps unsurprisingly, the droplet sizes of supercooled water in cloud are greatest at temperatures just below 0°C273.15 K\n491.67 °R and both decrease as temperature falls. The size of the supercooled water droplets is very important in respect of the potential to induce ice accretion. Larger droplets have greater inertia and are less influenced by the airflow around the aircraft than smaller droplets so they will impinge on more of the aircraft surface than smaller droplets. This is especially the case in respect of the in-flight air flow around the leading edge of wings and empennage. It is also the larger droplets which produce clear or glaze ice which is well recognized as the ice form of most concern and is often also the most difficult to detect visually.\nClouds and Supercooled Moisture\nThe majority of supercooled droplets in clouds are between 1 micron (0.001mm) and 50 microns (0.05 mm) in diameter. [For comparison, the thickness of the average human hair is approximately 100 microns]. Layer (stratiform) clouds typically contain average droplet diameters of up to 40 microns. Vertically developed (cumuliform) clouds of moderate scale typically have average droplet diameters of up to 50 microns (0.05mm) but large Cumulonimbus clouds often contain much more liquid water, including large quantities in droplets with diameters up to and beyond 100 microns (0.1mm).\nFreezing Rain and Freezing Drizzle\nPrecipitation droplets that are clear of cloud are much larger than those within cloud and, if they are supercooled, they are described as creating Freezing Drizzle where droplets have a diameter of between 50 and 500 microns (0.05mm and 0.5mm) and Freezing Rain where the droplets exceed 500 microns (0.5mm) in diameter. Freezing Rain often has much larger droplets of 2mm diameter or more, although if they get much beyond 6mm in diameter, they will tend to break up.\nFreezing rain below cloud forms when rain droplets are supercooled by passage through a layer of air which has a sub-zero temperature. Since air temperatures normally increase as altitude reduces, freezing rain implies the existence of an air temperature inversion. Such conditions can occur below an advancing warm front or a warm occlusion where a relatively warm air mass is overrunning colder air. The existence of freezing rain normally means that there will be warmer air (above 0°C273.15 K\n491.67 °R) above.\nThe smaller supercooled droplets of freezing drizzle can also form in this way but it is generally considered that they more commonly arise by a different process called the collision-coalescence process. When, through condensation, some droplets in a cloud grow to approximately 30 micrometers in diameter, they begin to settle, falling fast enough so that they collide with some smaller droplets. If the droplets then coalesce, a larger droplet is produced and this now has an even better chance of ‘capturing’ smaller droplets. Under favorable conditions, this process can produce drizzle-size drops in a supercooled cloud, usually near the top, where the largest droplets generally are found in any cloud. Data capture has varied but some studies have reported that freezing drizzle in non-convective clouds forms more than 80 percent of the time by the collision-coalescence process. So, importantly, when in freezing drizzle, it cannot be assumed that warmer air (above 0°C273.15 K\n491.67 °R) will exist above it.\nThe term ‘icing severity’ is essentially about the rate at which significant ice accretion occurs. The descriptions of supercooled water droplets so far have been in terms of their size. These comparative diameters are important - typical drizzle droplets have a diameter 10 times that of typical cloud droplets and typical rain droplets have a diameter 100 times that of typical cloud droplets. Size as described by diameter is, though, not what matters most in terms of the potential for ice accretion through impact. What really makes the difference is the volume (or mass) of water contained in a droplet since this is what controls the amount of water which will impact the aircraft and also how far aft of the airflow stagnation point ahead of a leading edge surface that droplets will strike the aircraft. This latter point is of considerable importance since large droplets may impact far beyond the leading edges in areas which are not anti iced or de iced and may also turn to ice as they are flowing aft in contact with the surface initially hit.\nIt is important to note that the volume of a droplet is not proportional to its diameter but to approximately the cube of half the diameter (i.e. the radius). Therefore, if 20 microns (0.02 mm) is taken as a typical diameter for a cloud droplet and 2000 micrometres (2mm) is taken as the typical diameter of a freezing rain droplet, then although the diameters of these droplets differ by a factor of only 100, their volume, and therefore their mass, differ by a factor which is of the order of 1,000,000.\nIt is this vastly greater mass of supercooled water droplets in freezing precipitation compared to those in cloud, even cumulonimbus cloud, which precludes any aircraft undertaking a significant period of sustained flight - and in most cases any flight - in freezing precipitation clear of cloud.\nDifferences between ambient and aircraft skin temperature\nThere are a number of factors which vary the propensity for ice to accrete on an aircraft:\n- When any aircraft moves through the air, the resultant kinetic hearting due to both compression at points of air obstruction and friction at surfaces of air passage raises the aircraft skin temperature above that of the ambient air in the vicinity. The extent to which this occurs, and therefore the extent of its significance to ice formation, has been calculated to be directly proportional to the square of 1/100 x the true airspeed in knots. Pilots will be aware of the consequent difference between SAT and TAT which can be observed to increase with speed since the effect of TAS increase is greater than the opposite effect of air density decrease.\n- If an aircraft is climbing into colder air and frozen or semi frozen deposits are resting on the upper surfaces of the wings or horizontal tailplane surfaces or in related flight control hinge gaps, the possibility exists that these deposits might freeze fully and attach to the airframe in situ. However, this will usually only be a significant problem if the climb is made concurrently into conditions likely to produce independently significant ice accretion.\n- When an aircraft is descending from high altitude, the increase in the structure surface temperature as the ambient air warms is likely to lag behind the ambient air temperature increase, especially in the case of wings used as fuel tanks which still contain a significant quantity of fuel. This will particularly apply where fuel is ‘tankered’ for use on the next flight sector in preference to uplifting at the initial destination. The significance of fuel is its tendency for temperature to recover from prior cold soak more slowly than the structure containing it. One particular effect of this is the formation of ‘fuel ice’ on the lower surface of a wing because this is where the cold fuel is in direct contact with the wing structure. Such clear ice is often still present during the subsequent turnaround even when OAT is several degrees above freezing.\nEngine Air Inlet Icing\nAll aircraft certificated for flight in icing conditions are fitted with anti-icing systems. These prevent air intake lip and guide vane ice formation by use of electrically heated mats, circulated hot engine oil or bleed air extracted from the engine. However, further inside an air inlet, ice can form in unprotected areas even when an aircraft is not flying in icing conditions as presently defined for engine certification purposes. There are two such circumstances: the cooling of moist air above freezing temperature by pressure reduction in piston engines; and the melting of ice crystals which are warmed after entering turbine engines.\nIn the first case, moist air entering a piston engine intake at a temperature above 0°C273.15 K\n491.67 °R is then accelerated by a suction effect through a reduced channel. This ‘venturi’ effect reduces its temperature causing the moisture load to condense out and be precipitated on the walls of the air channel as ice. See Piston Engine Induction Icing for more detail. This article also describes two other types of induction icing which occur in piston engines.\nIn the second case, high densities of very small ice crystals in very cold, high altitude air are ingested into high bypass gas turbines. They then either temporarily freeze and detach as larger pieces of ice causing engine airflow disruption, or cause mechanical damage either as pieces of ice or via discrete throughputs of liquid water. For further information, refer to High Level Ice Crystal Icing: Effects on Engines.\nThere are no definitions for absolute icing severity in aviation forecasting. However, the relative qualifications light, moderate and severe are generally used (in respect of airframe icing risk only) in a way which has at least reasonable consistency regionally in the context of the type of forecast in which they are used. It is understood by most forecasters that a term like ‘light icing’ will be interpreted rather differently by a commercial transport flight crew and a private pilot wondering whether they are likely to be able to undertake their planned flight clear of all icing conditions. As a consequence, low level icing forecasts are usually presented in such a way that they are specifically accessible to pilots of light aircraft vulnerable to the effects of any icing. However, the forecasts provided primarily for commercial air transport will use moderate and severe icing in terms which reflect their likely significance to aircraft which are certificated for routine flight in ‘icing conditions’ because they are equipped with appropriate ice protection systems. This use of higher level forecast presentations by general aviation can produce misunderstandings about the icing conditions that might be expected, and sometimes lead to inappropriate flight planning. The point is that the relative forecasts of icing severity cannot and do not take account of their user and the aircraft they are flying, so the interpretation of forecasts is as much a matter of judgement as monitoring the apparent icing reality once in the air.\nWith all the above caveats, a brief look at the usual ‘descriptions’ and ‘definitions’ of icing conditions used by forecasters may still be helpful. The descriptions all assume that an aircraft is certificated for “flight in icing conditions”.\n- Light Icing is often described as conditions such that ‘no change of course or altitude is necessary and no loss of airspeed occurs. It has been more rigorously defined by some as a rate of ice accretion per hour on outer wing of between 0.25 inch and 1 inch (0.6 to 2.5 cm).\n- Moderate Icing has been typically described as ice accretion which continues to increase but not at a rate sufficient to affect the safety of the flight unless it continues for an extended period of time, but air speed may be lost. A definition based upon an ice accretion rate per hour on the outer wing of 1 to 3 inches (2.5 to 7.5 cm)\n- Severe Icing has been variously described as ice accretion:\n- in which either the icing rate or ice accumulation exceed the tolerance of the aircraft;\n- which continues to build and begins to seriously affect the performance and manoeuvrability of an aircraft;\n- at a rate such that ice protection systems fail to remove the accumulation of ice and ice accumulates in locations not normally prone to icing;\n- such that an immediate exit from the condition is necessary to retain full control of the aircraft.\nIt is generally accepted that, although aircraft certification for flight in icing conditions rarely includes any stated restrictions, no aircraft is approved for flight in severe icing conditions, and that severe icing conditions may occur at any ice accumulation rate.\nIn North America, the terms clear, rime or mixed are more often used in forecast material than elsewhere and are both intended and taken as a proxy for droplet size regardless of other factors such as temperature and liquid water content. In this use, a forecast of rime icing indicates smaller drop sizes and a forecast of mixed or clear icing indicates larger drop sizes but with only a vague and undefined boundary between the two.\nFinally, a “clean aircraft” at rotation is an essential requirement for flight in or into icing conditions for those aircraft so approved. It is achieved by the use of appropriate ground de-icing or anti-icing fluids which have a sufficient holdover time for the prevailing conditions. Anti Icing is nearly always achieved by thickened fluids which adhere to the airframe and then progressively shear off during the take off roll so that they have all been shed by 100KIAS. There is no approved fluid protection against freezing rain or freezing drizzle conditions and so departure in them is generally not possible.\n- In-Flight Icing\n- Icing - Collection Efficiency\n- Aircraft and In Flight Icing Risks\n- Piston Engine Induction Icing\n- Freezing Rain\n- Aircraft Ground De/Anti Icing\n- Ice Protection Systems\n- High Level Ice Crystal Icing: Effects on Engines\n- Extract from Transport Canada Aviation Safety Letter 1/2007: The Adverse Aerodynamic Effects of Inflight Icing on Airplane Operation\n- Aircraft Icing Handbook, Version 1 by Civil Aviation Authority of New Zealand\n- Appendix C 'Icing Conditions' to CFR 14 Part 25, FAA, 2014""]"	['<urn:uuid:66772075-7054-41c1-a872-b60125260d6f>', '<urn:uuid:feb0abc0-d768-40b0-ab94-269ded9eeac1>']	factoid	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-13T00:43:36.053682	12	84	3242
32	I'm doing research about engines and fuel efficiency. What makes diesel engines more efficient than regular gasoline engines, and how does temperature affect their performance?	Diesel engines are more thermally and combustion efficient than gasoline engines for several reasons. Due to diesel's higher energy density (15-25% greater than gasoline), there is more energy per volume to power vehicles. Additionally, diesel's stability and compression resistance allows for higher compression ratios, which improves efficiency. When considering temperature effects, the Carnot cycle principles show that higher temperatures lead to better efficiency. In diesel engines, compression raises fuel temperature until combustion occurs, unlike spark-ignited gasoline engines. The compression process makes diesel engines about 20% more efficient than gasoline engines, even accounting for diesel's slightly lower calorific value (45.5 MJ/kg vs 45.8 MJ/kg for gasoline).	['Carnot Cycle – Carnot Heat Engine\nThe second law of thermodynamics places constraints upon the direction of heat transfer and sets an upper limit to the efficiency of conversion of heat to work in heat engines. So the second law is directly relevant for many important practical problems.\nIn 1824, a French engineer and physicist, Nicolas Léonard Sadi Carnot advanced the study of the second law by forming a principle (also called Carnot’s rule) that specifies limits on the maximum efficiency any heat engine can obtain. In short, this principle states that the efficiency of a thermodynamic cycle depends solely on the difference between the hot and cold temperature reservoirs.\nCarnot’s principle states:\n- No engine can be more efficient than a reversible engine (a Carnot heat engine) operating between the same high temperature and low temperature reservoirs.\n- The efficiencies of all reversible engines (Carnot heat engines) operating between the same constant temperature reservoirs are the same, regardless of the working substance employed or the operation details.\nThe cycle of this engine is called the Carnot cycle. A system undergoing a Carnot cycle is called a Carnot heat engine. It is not an actual thermodynamic cycle but is a theoretical construct and cannot be built in practice. All real thermodynamic processes are somehow irreversible. They are not done infinitely slowly and infinitesimally small steps in temperature are also a theoretical fiction. Therefore, heat engines must have lower efficiencies than limits on their efficiency due to the inherent irreversibility of the heat engine cycle they use.\nCarnot Cycle – Processes\n- isentropic compression – The gas is compressed adiabatically from state 1 to state 2, where the temperature is TH. The surroundings do work on the gas, increasing its internal energy and compressing it. On the other hand the entropy remains unchanged.\n- Isothermal expansion – The system is placed in contact with the reservoir at TH. The gas expands isothermally while receiving energy QH from the hot reservoir by heat transfer. The temperature of the gas does not change during the process. The gas does work on the surroundings. The total entropy change is given by: ∆S = S1 – S4 = QH/TH\n- isentropic expansion – The gas expands adiabatically from state 3 to state 4, where the temperature is TC. The gas does work on the surroundings and loses an amount of internal energy equal to the work that leaves the system. Again the entropy remains unchanged.\n- isothermal compression – The system is placed in contact with the reservoir at TC. The gas compresses isothermally to its initial state while it discharges energy QC to the cold reservoir by heat transfer. In this process the surroundings do work on the gas. The total entropy change is given by: ∆S = S3 – S2 = QC/TC\nAn isentropic process is a thermodynamic process, in which the entropy of the fluid or gas remains constant. It means the isentropic process is a special case of an adiabatic process in which there is no transfer of heat or matter. It is a reversible adiabatic process. The assumption of no heat transfer is very important, since we can use the adiabatic approximation only in very rapid processes.\nIsentropic Process and the First Law\nFor a closed system, we can write the first law of thermodynamics in terms of enthalpy:\ndH = dQ + Vdp\ndH = TdS + Vdp\nIsentropic process (dQ = 0):\ndH = Vdp → W = H2 – H1 → H2 – H1 = Cp (T2 – T1) (for ideal gas)\nIsentropic Process of the Ideal Gas\nThe isentropic process (a special case of adiabatic process) can be expressed with the ideal gas law as:\npVκ = constant\np1V1κ = p2V2κ\nin which κ = cp/cv is the ratio of the specific heats (or heat capacities) for the gas. One for constant pressure (cp) and one for constant volume (cv). Note that, this ratio κ = cp/cv is a factor in determining the speed of sound in a gas and other adiabatic processes.\nAn isothermal process is a thermodynamic process, in which the temperature of the system remains constant (T = const). The heat transfer into or out of the system typically must happen at such a slow rate in order to continually adjust to the temperature of the reservoir through heat exchange. In each of these states the thermal equilibrium is maintained.\nIsothermal Process and the First Law\nThe classical form of the first law of thermodynamics is the following equation:\ndU = dQ – dW\nIn this equation dW is equal to dW = pdV and is known as the boundary work.\nIn isothermal process and the ideal gas, all heat added to the system will be used to do work:\nIsothermal process (dU = 0):\ndU = 0 = Q – W → W = Q (for ideal gas)\nIsothermal Process of the Ideal Gas\nThe isothermal process can be expressed with the ideal gas law as:\npV = constant\np1V1 = p2V2\nOn a p-V diagram, the process occurs along a line (called an isotherm) that has the equation p = constant / V.\nSee also: Boyle-Mariotte Law\nCarnot Cycle – pV, Ts diagram\nThe Carnot cycle is often plotted on a pressure- volume diagram (pV diagram) and on a temperature-entropy diagram (Ts diagram).\nWhen plotted on a pressure-volume diagram, the isothermal processes follow the isotherm lines for the gas, adiabatic processes move between isotherms and the area bounded by the complete cycle path represents the total work that can be done during one cycle.\nThe temperature-entropy diagram (Ts diagram) in which the thermodynamic state is specified by a point on a graph with specific entropy (s) as the horizontal axis and absolute temperature (T) as the vertical axis, is the best diagram to describe behaviour of a Carnot cycle.\nIt is a useful and common tool, particularly because it helps to visualize the heat transfer during a process. For reversible (ideal) processes, the area under the T-s curve of a process is the heat transferred to the system during that process.\nCarnot Cycle Efficiency\nSince energy is conserved according to the first law of thermodynamics and energy cannot be be converted to work completely, the heat input, QH, must equal the work done, W, plus the heat that must be dissipated as waste heat QC into the environment. Therefore we can rewrite the formula for thermal efficiency as:\nSince QC = ∆S.TC and QH = ∆S.TH, the formula for this maximum efficiency is:\n- is the efficiency of Carnot cycle, i.e. it is the ratio = W/QH of the work done by the engine to the heat energy entering the system from the hot reservoir.\n- TC is the absolute temperature (Kelvins) of the cold reservoir,\n- TH is the absolute temperature (Kelvins) of the hot reservoir.\nSee also : Causes of Inefficiencies\nExample: Carnot efficiency for coal-fired power plant\nIn a modern coal-fired power plant, the temperature of high pressure steam (Thot) would be about 400°C (673K) and Tcold, the cooling tower water temperature, would be about 20°C (293K). For this type of power plant the maximum (ideal) efficiency will be:\nηth= 1 – Tcold/Thot = 1 – 293/673 = 56%\nIt must be added, this is an idealized efficiency. The Carnot efficiency is valid for reversible processes. These processes cannot be achieved in real cycles of power plants. The Carnot efficiency dictates that higher efficiencies can be attained by increasing the temperature of the steam. This feature is valid also for real thermodynamic cycles. But this requires an increase in pressures inside boilers or steam generators. However, metallurgical considerations place an upper limits on such pressures. Sub-critical fossil fuel power plants, that are operated under critical pressure (i.e. lower than 22.1 MPa), can achieve 36–40% efficiency. Supercritical designs, that are operated at supercritical pressure (i.e. greater than 22.1 MPa), have efficiencies around 43%. Most efficient and also very complex coal-fired power plants that are operated at “ultra critical” pressures (i.e. around 30 MPa) and use multiple stage reheat reach about 48% efficiency.\nSee also: Supercritical Reactor', 'Latest Advances in Diesel Technology And What is To Come\nDiesel, in relation to other fossil fuels, is an exceptionally dense fuel. There are a variety of properties that are inherent in dense fuels. Because of the inherent properties of diesel, diesel engines are extremely energy efficient both respect to combustion efficiency and thermal efficiency.\nHowever, while diesel is extremely energy dense and diesel engines are more thermally efficient and have higher combustion efficiency rates than lighter fuels with low energy density, capturing the full potential of diesel can be difficult.\nDiesel engines — while considerably more efficient and less polluting than a gasoline engine and alternative fuel engines — still have limitations. But, the same inherent traits of diesel that make diesel engines efficient give it the potential — as technologies advance — to generate even higher efficiency rates and lower emissions.\nThe properties of diesel — with respect to fuel efficiency, combustion efficiency, and thermal efficiency rates — govern the energy and emissions outputs of diesel engines according to several laws of physics. Ideal Gas Law and the first and second laws of thermodynamics determine the environmental and economic value of diesel engines. As technologies improve efficiency and output rates should increase according to the laws of physics.\nWhile we can develop technologies to increase the efficiency in which we use diesel, the laws of physics also tell us that gasoline and alternative fuels have limitations with respect to how much more we can improve the efficiency of the engines that combust them.\nThe principal reason gasoline has greater potential to improve is because of its energy density and stability. Gasoline and alternative fuels are much lighter and less energy dense fuels and for that reason, improving gasoline and alternative fuel engines is extremely difficult.\nIn other words, diesel is the ideal fossil fuel, potentially.\nThe energy density and stability of a fuel determine potential fuel efficiency and potential energy output.\nEfficiency Potential of Not Just Diesel, but Diesel Engines\nOf all liquid fossil fuel combustion engines, diesel engines are the most thermally efficient and have the highest rates of combustion efficiency. Furthermore, diesel engines produce the least amount of emissions, save nitrous oxides. There are two reasons diesel engines are the best fossil fuel combustion engines with respect to efficiency and emissions: the energy density and compression resistance of diesel fuel. Because of energy density and the compression resistance of fuel, diesel engines have higher rates of both combustion efficiency and thermal efficiency.\nDiesel has a High Energy Density\nDiesel engines highly fuel efficient, for one, because on a volume scale — gallon, liter, square foot or meter — diesel has a much higher energy density than most other solid, liquid, and gas-state fossil fuels. Diesel certainly has a higher energy density than gasoline, natural gas (methane), and propane.\nHigher energy density means there is more energy per volume unit of measure — more energy per gallon — to push a vehicle down the road. The reason diesel has a higher energy density than other fossil fuels is that the hydrocarbons in diesel — the valuable components in every fossil fuel that ignites/burns/combusts — are made of long and complex molecules, molecules with very high carbon-to-hydrogen ratios.\n“The calorific value of diesel fuel is roughly 45.5 MJ/kg (megajoules per kilogram), slightly lower than petrol which is 45.8 MJ/kg. However, diesel fuel is denser than petrol and contains about 15% more energy by volume (roughly 36.9 MJ/liter compared to 33.7 MJ/liter). Accounting for the difference in energy density, the overall efficiency of the diesel engine is still some 20% greater than the petrol engine, despite the diesel engine also being heavier.”\nThe greater the number of carbon atoms in a hydrocarbon, in relation to hydrogen atoms, the higher the fuel density of that fuel. On a volume scale, the energy density of diesel is between 15% and 25% greater than gasoline. The difference is dependent on whether or not the gasoline is low or high octane. The higher the octane of gasoline, the lower it’s energy density. The reason being, the additives used to increase gasoline octane have lower energy densities than gasoline.\nIn other words, octane additives dilute gasoline.\nDiesel naturally has exceptional compressive resistance because it is a heavy fuel, stable fuel made of large, long hydrocarbon molecules.\nDiesel has High Compression Resistance\nThe second reason diesel is highly efficient with respect to fuel efficiency is because diesel is a very heavy fossil fuel. As such, diesel is a very stable fuel. The stability of diesel is the reason diesel engines with high compression ratios are possible. Compression ratio plays into both fuel efficiency and emissions. Compression ratio is particularly important with respect to reducing emissions. The higher the compression ratio, the lower the emissions.\nDiesel Engines are More Thermal Efficient than Other Fossil Fuel Engines\nThe third reason diesel engines are more efficient than any other liquid fossil fuel engine is that of thermal efficiency. Thermal efficiency is the total amount of energy generated by an engine’s combustion of fuel that becomes mechanical energy, an energy that pushes a vehicle down the road. The thermal efficiency of diesel engines is far greater than that of any other type of liquid fossil fuel engine.\nThe thermal efficiency of diesel engines is, in part, due to the energy density and compression resistance of diesel fuel.\nTechnologies that Increase Diesel Engine Compression Ratio\nThere are a number of means by which to increase the fuel efficiency and combustion efficiency — one of the most important variables with respect to emissions — of diesel engines. One of the simplest means of increasing fuel and combustion efficiency is to increase the compression ratio of an engine.\nWhat Compression Ratio Is\nCompression ratio is a measure of how much an engine compresses a gas or vapor-state fossil fuel once the fuel is in the combustion chamber. In piston/cylinder engines, the compression ratio is a measure of the difference between two piston positions, top and bottom center. The ratio difference between when a piston is at bottom center — when there is the most volume in an engine’s cylinder — and when the piston is at top dead center — when the cylinder has the least volume — is compression ratio.\nThe higher the compression ratio of an engine, the more it compresses a vaporized or gas-state fuel. The more a fuel is compressed, the hotter it gets prior to combustion. The hotter a fuel is before compressions, the more efficiently it combusts.\nThe weight and stability of diesel fuel mean it is highly resistant to compression. The compression resistance of diesel means that when it does ignite — being highly compressed, thus extremely hot, — diesel burns far more completely than other fuels with less compression resistance.\nTechnologies that Increase the Compression Resistance of Diesel\nAlready highly resistant to compression, there are technologies available that can further increase the compression resistance of diesel. By increasing the compression resistance of diesel, again, it is possible to increase both the fuel and combustion efficiency of diesel. There are a variety of technologies that increase the compression ratio of a diesel engine.\nSome technologies that increase the compression resistance of diesel are old concepts with new advances. Others are new concepts that have yet to take hold in the diesel engine sector. Even others are old concepts that have potential, theoretically but have yet to be fully developed. And there are also new, proven technologies that work well with respect to increasing a diesel engine’s compression ratio.\nThe easiest way to increase the compression ratio of a diesel engine is with cetane additives. The same way additives can increase the octane of gasoline, cetane additives increase the compressive resistance of diesel fuel. The difference is gasoline engines are set at a particular compression ratio while diesel engines have the potential for variable compression ratios.\nOnce gasoline engine piston reaches a preset compression ratio, the engine’s sparks fire and drives the piston down. But, diesel engines are not spark fired. Diesel engines are compression engines. That means the diesel does not combust in the combustion chamber until it — the diesel — reaches a certain temperature. The temperature rise in diesel is a product of compression. A diesel engine compresses — therefore heats — diesel until it finally combusts.\nIncreasing the flash point temperature of diesel — the temperature at which it combusts — increases the compression ratio.\nDrawbacks of Cetane Additives\nWhile increasing the compression ratio increases the combustion efficiency of a fuel, which increases total energy output and the total amount of the hydrocarbons that combust, it is a measure of the total fuel available. Combustion efficiency is a percentage of the total hydrocarbons in a combustion chamber that burn up. The problem is, cetane additives actually decrease the total hydrocarbons in a fuel.\nIn other words, while the compression ratio increases the combustion efficiency of the total energy available, cetane additives reduce the total amount of energy available. So while a greater percentage of the fuel available burns up during combustion, cetane additives mean a lower amount of total fuel in a combustion chamber at any given time.\nCommon Rail Fuel Injection\nThough the Common Rail Fuel Injection System has been used in diesel trucks and pickups since the mid-2000s — and since 1997 in diesel cars and marine engines, — it is a new technology relative to the age of diesel engines. The Common Rail Fuel Injection System was a major breakthrough both with respect to fuel efficiency and emissions.\nFrom the 1950s until the new millennium, diesel fuel systems used direct injection. “Direct injection systems pump diesel through injectors mounted directly in the cylinder and feeds fuel to a combustion chamber machined into the top of the cylinder itself.” Direct injection systems have a low-pressure fuel pump that feeds each injector. The problem with traditional direct injection is that large droplets of diesel feed into the combustion chamber.\nThe larger the individual droplets of fuel, the less completely they burn. By reducing the size of the droplets fed into a diesel combustion chamber, the total surface area of each droplet increases which promotes oxygenation. The greater the rates of oxygenation of each droplet, the more complete each droplet ignites/combusts/burns.\nAnd that is what a Common Rail Fuel Injection System does, dramatically reduces the size of the droplets of diesel that are fed into the cylinders of the engine. By vaporizing the air/fuel mixture, a Common Rail Fuel Injection System increases combustion efficiency dramatically, but nothing compared to Supercritical Fuel Injection\nSupercritical Fuel Injection\nThe combustion efficiency generated by a Supercritical Fuel Injection System is greater than Common Rail Fuel Injection System. Combustion efficiency means greater fuel efficiency and lower emissions, much lower emissions. “Researchers in New York have demonstrated a supercritical diesel fuel-injection system that can reduce engine emissions by 80 percent and increase overall efficiency by 10 percent.”\nWhile diesel engines are almost always more fuel efficient than gasoline engines, they have a tendency to produce more oxides of sulfur and oxides of nitrogen. Additionally, because diesel is a heavy fuel with extremely complex hydrocarbon molecules, the combustion efficiency is less than perfect. It is the combustion efficiency of diesel engines that a Supercritical Fuel Injection system improves.\nAccording to George Anitescu, a research associate at the Department of Biomedical and Chemical Engineering at Syracuse University in New York state, a Supercritical Fuel Injection System puts diesel in a state that is between liquid and gas.\n“The high molecular diffusion of supercritical fluids means that the fuel and air mix together almost instantaneously. So instead of trying to burn relatively large droplets of fuel surrounded by air, the vaporized fuel mixes more evenly with air, which makes it burn more quickly, cleanly, and completely. In a sense, it is like an intermediate between diesel and gasoline, but with the benefits of both.”\nUnfortunately, Supercritical Fuel Injection is still in the design and development state. There are several issues engineers must contend with prior to Supercritical Fuel Injection becoming commercially viable. For one, in order for diesel to reach a supercritical state, its temperature must be around 450 degrees Celsius. Such temperatures can cause diesel to solidify and choke. “Coking occurs when hydrocarbons in the fuel react, producing sticky deposits that can lead to fuel-system failures.” While there are a number of solutions to the problems presented when attempting to move diesel into a supercritical state, Supercritical Fuel Injection is not yet a complete practical application.\nWhile they have been around for a while, fuel catalysts are still an aftermarket product. Scientists and engineers — even laymen — have been aware of the problem of oxygenizing diesel fuels. As far back as 1936, Chinese fishermen tapped magnets around fuel lines in order to break apart the hydrocarbon clusters inherent in diesel fuel. Breaking up the hydrocarbon clusters increases the oxygenation potential of diesel. In the 1980s, ranchers in Wyoming made national news doing the same, though on gasoline engines. The Washington Post reported,\n“Although cow magnets have been sold in farming areas for half a century, the market has been limited until recently. Now it’s booming as never before since word got around that somehow the magnetic field can boost a car’s gas mileage by improving combustion if a pair of cow magnets is fastened to a car’s gas line close to the carburetor.”\nWhether the magnets truly helped the Chinese fishermen and Wyoming’s cowboys achieve higher rates of fuel efficiency is debatable. What is not is the fact that — in principle — they were onto something. They knew that by increasing the oxygen content in a diesel fuel mixture, they could increase fuel efficiency.\nModern fuel catalysts — made of similar precious metals as those found in catalytic converters — are also engineered to increase diesel fuel oxygenation. But unlike injectors that oxygenate diesel by increasing the surface area of diesel fuel droplets, fuel catalysts change the physical state of diesel on a molecular level.\nBy depolarizing the charges that bind diesel fuel hydrocarbons into clusters, fuel catalysts expose the individual hydrocarbon molecules and molecule chains to oxygen. Again, the greater the oxygenation of a fossil fuel, the more completely it burns. The Rentar Fuel Catalyst, for example, increases fuel efficiency by between 3% and 8% on over the road vehicles and to a greater extent on heavy equipment, machinery, and marine engines.\nWithout question, the latest advances in diesel technology are making the world’s most efficient fossil fuel produce even more energy and, at the same time, reducing the emissions associated with diesel.']	['<urn:uuid:f22c4ee2-92f0-4cc1-8e71-3d2f4366a8a2>', '<urn:uuid:2be7f07b-5cdf-482c-aef9-c78dcbecd24c>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	25	105	3803
33	coastal fog formation redwoods survival research	Coastal fog forms through a complex process involving California's unique geography, ocean currents, and temperature differentials. When spring sun warms the Central Valley, it creates a low-pressure system that interacts with high-pressure systems from the Pacific Ocean, while northwest winds collect moisture that condenses over cold coastal waters. Scientists are actively studying how redwoods might survive these changing conditions. Research teams from universities are monitoring tree growth, examining past climate responses through tree rings, and conducting greenhouse experiments to understand how seedlings from different areas might react to climate changes. Their findings will help develop strategies for protecting cooler and moister habitats where trees can survive if their current range becomes too warm or dry.	"[""by Kyle Pinjuv\nFog: Some embrace the cool reprieve it offers from hot summer temperatures. Some feel betrayed by its presence as they approach the beach in shorts and a tank-top with summery intent only to be forced to spend the day wrapped in a towel shivering in a cloud. Fog limits your vision and inspires the creative mind. It reduces contrast during movement, changing our perception of reality and literally causing us to move differently through space. It moves border-free without discrimination over land and sea. We cannot own it or control it. The fog is the San Francisco Bay Area’s breath. It is the gift that allows our iconic redwood forests to flourish. It is itself a coastal icon.\nFog along the coastal belt is a result of our region’s unique geography, ocean currents, wind and air temperature both inland and offshore. It all begins as the spring sun warms the Central Valley of California: The heated air rises creating a low pressure system along the ground in the Valley, while at the same time, warm air moves north from the equator over the cool Pacific Ocean creating a high pressure system. The high pressure is then pulled inland toward the low pressure, balancing the two systems. While this balancing act is occurring, wind from the northwest, full of moisture gathered while traveling over thousands of ocean miles, comes into contact with the cold upwelled waters off the California coastline. The air is cooled, causing moisture to form into visible droplets. Eventually, the accumulation of billions of these drops occurs and produces what we call fog. The fog is pulled with the balancing high and low pressure systems from the coast as far inland as the Central Valley.\nThis complex process of fog formation is part of a larger ecological process that allows our beloved coastal redwood habitats to thrive. Summer is the growing season for coastal redwoods and they rely on the cool, moisturized air to collect water in an otherwise dry season. Approximately 30–40 percent of the water supply used by redwoods comes from fog. With the trees’ great height, they rely on pulling moisture both from the ground and from the air. Redwoods are one of only a few tree species that can move water in both directions. The fog also contributes to the greater coastal redwood ecosystem by soaking the ground up to 35 centimeters deep!\nThere have been studies throughout Northern California looking at how climate change reduces the differential of air and ocean temperatures resulting in the decrease of total days of fog per year. Without the fog, coastal redwoods are more vulnerable to drought and die back at the tops of the trees. With the drying of soils and hot summer temperatures, redwood seedlings are especially susceptible to drought conditions. Many organizations and agencies will be keeping a close eye on fog reduction as it has the ability to greatly impact our coastal ecosystems.\nWe are so often grateful for the landscapes of Sonoma County — and rightfully so — but every so often it is important to take a broader view of why this region’s land is so special. Many times, the outside forces that know no political boundaries, such as the wind, rain and fog, allow this region to flourish. It is our duty as stewards of the land to not only protect that which can be bought and sold, but also to care for our seas and our air. Take away the health of our shared planetary resources and we are left with little to protect.\nKyle Pinjuv is the stewardship associate project manager at Sonoma Land Trust.\nSonoma Land Trust is a local nonprofit based in Santa Rosa, CA, that conserves scenic, natural, agricultural and open lands in Sonoma County for the benefit of the community and future generations. This blog focuses on SLT's stewardship team, whose members do hands-on work to directly protect, restore, and safeguard the land for generations to come."", 'As the planet warms, will the progressive loss of coastal fog, which has declined over 30%, doom these beauties? KQED follows a team of UC Berekely researchers as they climb up into the crown of a huge old-growth redwood in Montgomery Woods to install monitoring equipment.\nRecent advances enable Save the Redwoods League and a team of pioneering scientists to unlock the record of environmental changes stored in redwood tree rings. From individual trees to whole forests, we will study redwood growth, vulnerabilities, early indicators of stress and how these trees might respond to predicted climate changes.\nSave the Redwoods League has united leading scientists Stephen C. Sillett and Robert Van Pelt of Humboldt State University and Todd Dawson and Anthony Ambrose of the University of California, Berkeley, to launch the Initiative. Their studies will yield results that quantify redwoods’ vulnerabilities to climatic changes and their capacities to mitigate these changes via photosynthesis, fog interception, wood production and carbon sequestration. They are uniquely qualified, in part, because they have developed many of the methods to obtain the study’s data.\nThe scientists are:\n- studying whole-tree and whole-forest rates of annual wood production back 1,000 years in forest plots throughout the redwood ranges. These measurements will help the team predict tree and forest growth in response to changing climates\n- reconstructing past climates to learn how redwoods responded to environmental conditions\n- assessing how redwoods are responding to current conditions\n- planning to manipulate temperature, carbon dioxide and water in greenhouse experiments to examine how redwood seedlings and saplings from different parts of the ranges might react to climatic changes\nStrategies for Survival\nPossible ways the Initiative findings could help redwoods survive in the future include\n- Protecting cooler and moister habitats so the trees will have a place to grow if their current range becomes too warm or dry\n- Guiding economic incentives for sustainable logging practices\n- Developing more sophisticated water and buffer management in the forecasted redwood range, in partnership with redwood forest landowners\n- Informing new public policy\nOutcomes of the Initiative\n- Gain the critical data necessary to develop strategies for helping redwoods adapt to rapid environmental changes, thereby protecting the diverse ecosystems they define\n- Understand how restoration techniques can utilize carbon storage potential in the redwoods\n- Focus our conservation work in the places that need it most\n- Preserve watersheds to benefit redwood ecosystems, wildlife and people\n- Restore forests for the future\n- Gather data to inform sound public policy and conservation management strategies\n- Create a model for studying and protecting other forest ecosystems in California, the Pacific Northwest and the world\n- Inspire renewed hope for future generations to experience the wonder of ancient redwood forests\nIntegrity, Power & Passion\nThe Integrity of the Initiative lies in the integrated nature of the collaboration. The best redwood scientists in the world will develop and use established protocols and cutting-edge technology, linking ground based data, watershed plots and airplane based LiDAR data to scale up the findings.\nThe Power of the Initiative is that the data will inform models to understand the past, analyze impacts of current environmental changes, and help develop adaptive strategies for the future.\nThe Passion for the Initiative is reflected in the League’s more than 90-year history of success in protecting the redwoods. We must protect the existing redwood forests and work to ensure the future of the species.\nWith your help, the Redwoods and Climate Change Initiative will succeed!']"	['<urn:uuid:58655171-6799-418b-a0ac-f44bec2058ef>', '<urn:uuid:ec634f65-8d3f-4dc8-b2ed-09368a569984>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	6	115	1248
34	55th fighter group arnhem bridge defense outcome	The 55th Fighter Group was involved in the Arnhem operations in September 1944, providing air patrol in the Arnhem sector to support the airborne invasion of Holland. However, the battle for the Arnhem bridge ultimately ended in failure for the Allied forces. Despite British and Polish paratroopers attempting to capture and hold the bridge over the Rhine, which later became known as the John Frost Bridge, they were unable to maintain control. The bridge proved to be 'one bridge too far' in Operation Market Garden, resulting in a disastrous outcome for Allied forces.	"[""55th Fighter Group\nConstituted as 55th Pursuit Group (Interceptor) on 20 Nov 1940. Activated on 15 Jan 1941. Trained with P-43's. Redesignated 55th Fighter Group in May 1942. Converted to P-38's and prepared for combat. Moved to England, Aug-Sep 1943. Assigned to Eighth AF. Began operations with P-38's on 15 Oct 1943; converted to P-51's in Jul 1944. Engaged primarily in escorting bombers that attacked such targets as industries and marshalling yards in Germany, and airfields and V-weapon sites in France. Provided cover for B-17's and B-24's that bombed aircraft plants during Big Week in Feb 1944, gun emplacements during the St Lo breakthrough in Jul 1944, and transportation facilities during the Battle of the Bulge, Dec 1944-Jan 1945. Also patrolled the air over the Channel and bombed bridges in the Tours area during the invasion of the Continent in Jun 1944; patrolled the Arnhem sector to support the airborne invasion of Holland in Sep 1944; strafed trucks, locomotives, and oil depots near Wesel when the Allies crossed the Rhine in Mar 1945. Received a DUC for eight missions to Germany between 3 and 13 Sep 1944 when the group not only destroyed enemy fighters in the air to protect the bombers it was escorting, but also descended to low levels, in spite of intense antiaircraft fire, to strafe airdromes and to destroy enemy aircraft on the ground. Received second DUC for operations on 19 Feb 1945 when the organization flew a sweep over Germany to hit railway tracks, locomotives, oil cars, goods wagons, troop cars, buildings, and military vehicles. Flew last combat mission on 21 Apr 1945. Moved to Germany in Jul 1945 as part of the occupation forces. Assigned to United States Air Forces in Europe. Trained with P-51 and P-80 aircraft. Inactivated in Germany on 20 Aug 1946.\nRedesignated 55th Reconnaissance Group (Very Long Range, Mapping). Activated in the US on 24 Feb 1947. Assigned to Strategic Air Command. Redesignated 55th Strategic Reconnaissance Group in Jun 1948. Aircraft included RB-17's and B-29's and RB-29's. Inactivated on 14 Oct 1949.\nRedesignated 55th Strategic Reconnaissance Group (Medium). Activated in Puerto Rico on 1 Nov 1950. Assigned to Strategic Air Command. Equipped with RB-29 and RB-50 aircraft. Inactivated on 16 Jun 1952.\nSquadrons. 7th Geodetic: 1949. 37th: 1941-1943. 38th: 1941-1946; 1949; 1950-1952. 54th: 1941-1942. 338th: 1942-1946; 1949; 1950-1952. 343d: 1943-1946; 1947-1949; 1950-1952.\nStations. Hamilton Field, Calif, 15 Jan 1941; Portland, Ore, 21 May 1941; Paine Field, Wash, 10 Feb 1942; McChord Field, Wash, 22 Jul 1942-23 Aug 1943; Nuthampstead, England, 14 Sep 1943; Wormingford, England, 16 Apr 1944; Kaufbeuren, Germany, 22 Jul 1945; Giebelstadt, Germany, 29 Apr-20 Aug 1946. MacDill Field, Fla, 24 Feb 1947; Topeka AFB, Kan, 30 Jun 1948-14 Oct 1949. Ramey AFB, PR, 1 Nov 1950-16 Jun 1952.\nCommanders. Capt Kenneth S Wade, 15 Jan 1941; Maj James W McCauley, 1 May 1941; Lt Col Karl K Bowen, 1 May 1942; Maj Jack S Jenkins, 1 Aug 1942; Maj Ernest W Keating, 13 Nov 1942; Lt Col Frank B James, 15 May 1943; Col Jack S Jenkins, 6 Feb 1944; Col George T Crowell, 10 Apr 1944; Lt Col Elwyn C Righetti, 22 Feb 1945; Col Ben Rimerman, 22 Apr 1945; Lt Col Jack W Hayes Jr, 21 May 1945; Lt Col Horace A Hanes, Jul 1946-unkn. Capt Daniel W Burrows, 24 Feb 1947; Lt Col Albert M Welsh, 20 May 1947-unkn; Lt Col George Humbrecht, 26 Oct 1948-unkn; Col Herbert K Baisley, unkn-1949. Col Richard T King, 1 Nov 1950; Brig Gen Sydney D Grubbs Jr, 20 Dec 1950; Col Alfred K Kalberer, 18 Feb-16 Jun 1952.\nCampaigns. American Theater; Air Offensive, Europe; Normandy; Northern France; Rhineland; Ardennes-Alsace; Central Europe.\nDecorations. Distinguished Unit Citations: ETO, 3-13 Sep 1944; Germany, 19 Feb 1945.\nInsigne. Shield: Azure, on a fess indented or a similar bar gules. Motto: Pursuit To Defend. (Approved 18 Feb 1942. This insigne was replaced 4 Feb 1954.)\nData from Air Force Combat Units of World War II By Maurer, Maurer, Published 1986"", 'Arnhem Berömda Bron Över Floden Rhen, Holland Stockfoton\nOperation Market Garden: a massive airborne strike designed to open a c The Allies decided to launch an airborne assault on Arnhem to secure bridges across the Rhine River to end the war, and the operation took place in September 16 Dec 2016 Aerial reconnaissance photo of the Arnhem Bridge taken by the Royal Air Force on 19 September, 1944 showing signs of the British defense 19 Oct 2011 The first bridges could be seized successfully, but the bridge in Arnhem was “one bridge too far”, so that the Allied forces suffered a disastrous 29 Sep 2010 Arnhem Bridge - September 17, 1944 - September 19, 1944 · Historical Background: The largest Airborne drop ever designed, Operation Market 16 Aug 2016 Almost 12,000 parachute and glider-borne troops landed to seize the bridge over the Rhine at Arnhem in Holland. Over nine days of fierce 26 May 2018 Further on, the British 1st Airborne Division and the Polish brigade would drop near Arnhem to capture the great road bridge over the Neder 30 Nov 2016 Arnhem bridge across the Rhine proved an insurmountable goal for the ill-fated British 1st Airborne during Operation Market-Garden. 18 Sep 2019 The loss of the bridge at Son, and the inability of 82nd Airborne to take Nijmegen swiftly enough, are both examples of a number of failings that Aerial view of the bridge over the Neder Rijn, Arnhem.jpg Slaget vid Arnhem var en del av Operation Market Garden som utkämpades i och omkring de De flygburna truppernas uppgift var att säkra bron över Rhen vid Arnhem och de på att ingenjörstrupperna skulle bygga upp en Bailey bridge över kanalen. Includes: ""1944"", ""A Bridge Too Far"" and more!\nNote: This is not a historically accurate game! 2009-10-27 2016-11-30 John Frost Bridge (John Frostbrug in Dutch) is the road bridge over the Lower Rhine at Arnhem, in the Netherlands. The bridge is named after Major-General John Dutton Frost (1912–1993), who commanded the British forces that reached and defended the bridge during the Battle of Arnhem in September, 1944. The bridge was made more famous… Massed pipes and drums are marching down the John Frost bridge, towards remembrance monument during 65th remembrance of the Battle of Arnhem (Operation Marke Arnhem was the site of the failed attempt to take the bridge of the Rhine during World War 2. In September 1944 thousands of mainly British and Polish paratroopers were dropped in enemy territory outside Arnhem during operation \'Market Garden\'. This museum is close to Arnhem road bridge.\nPanzer Cmp - Market-Garden \'44 – Appar på Google Play\nThe photos in this section are excellent, showing the fighting in the area and photos taken after the battle in 1945. 2 dagar sedan · After reaching the furthest bridge, over the Lower Rhine at Arnhem, the way into Germany\'s industrial heartland of the Ruhr would be clear.\noperation \'market garden\' - the battle for arnhem - Pinterest\nMarkerad betygsstjärna Omarkerad betygsstjärna. Antal reservationer: 0. ArnhemJönköping University. Skövde Arnhem.\nIf you continue to use this website without changing your cookie settings or you click ""Accept"" below then you are consenting to this. Marketplace.\nBranemark implant screwdriver\nOriginaltitel: A bridge at Arnhem. Innehållsbeskrivning. This 24-hour fire station in Arnhem (The Netherlands) consists of a commercial building with office space, workshops, warehouses, parking vehicles and various Utförlig information. Utförlig titel: En bro vid Arnhem, [Elektronisk resurs], Charles Whiting; Originaltitel: A bridge at Arnhem; Medarbetare: Knut Rosén.\n· imusic.se. Arnhem: The Farthest Bridge_boxshot key bridges along the main highway leading through Holland, the Arnhem bridge being the most distant of them all. Arnhem bridge - John Frost bridge - scenic of World War 2 battle during Market-Garden airborne operation. The bridge was captured during the operation by the\nRoy. 4to., First Edition, with very numerous photographs and maps (a number full-page) in the text; red cloth, backstrip lettered in silver, a fine copy in unclipped\nUtförlig information. Utförlig titel: En bro vid Arnhem, [Elektronisk resurs]; Originaltitel: A bridge at Arnhem; Språk: Svenska. Originalspråk: Engelska\nEn bro vid Arnhem [Elektronisk resurs].\nAdvokatfirman kronan oskarshamn\nBridge to Bridge Arnhem. 16 Aug 2016 The operation was split into two sub-operations: Market – the airborne forces, the First Allied Airborne Army, who would seize bridges Garden – Lance Gerrard-Wright in front of the John Frost Bridge in Arnhem, named after Major-General John Dutton Frost, who commanded the British forces. This meant 20 Sep 2019 Eventually, the survivors were evacuated across the river, but with Arnhem bridge still in German hands, the operation was a failure. Nearly 17 Oct 2019 Farthest north, the British 1st Airborne Division landed at Arnhem to secure bridges across the Nederrijn, supported by men of the Glider Pilot Bloomsbury presents The Devil\'s Bridge by Anthony Tucker-Jones, read by Roger May. In the late summer of 1944, SS-Obergruppenführer Wilhelm \'Willi\' Bittrich Ride to the infamous bridge at Arnhem. This veritable feast of history concentrates on Operation Market Garden but happens to pass by some other pretty Arnhem Bridge.\nIt includes maps, good action reports and order of battles on the German units involved in the fighting.\nEn tung lastbil reflex\nmagisterexamen vs masterexamen\nrtv orkester dirigent\netik i socialt arbete - etisk kod för socialarbetare antagen av akademikerförbundet ssr.\nJa jag vet, en suddig bild, men maten och ölen var jättegod, Arnhem\nAndra broar på Arnhem — Arnhem har tre broar över Niederrinen: John Frost Bridge, Nelson Mandela Bridge och Andrei Sakharov Bridge. av F Rosgren · 2018 — 2 Ryan, Cornelius, A bridge too far, Hamish Hamilton, London, 1974, s. vii. communications during the early part of the Battle of Arnhem. Slaget vid Arnhem var en del av Operation Market Garden som utkämpades i och omkring de holländska William Goldman\'s Story of a Bridge Too Far. De allierades insats med fallskärmsjägare vid Arnhem i september 1944 är han blivit för sitt sista storverk om andra världskriget, A Bridge Too Far – En bro för Airborne at the bridge ligger inom gångavstånd från detta stora hotell. För din bekvämlighet, finns det Wi-Fi, arbetsbord och IDD-telefon.\npeptonic medical avanza\n- Tele wizja ru\n- Miljöbalken innehållsförteckning\n- Paula blomqvist uppsala\n- Första hjälpen vandring\n- Lappland eco shell\n- Första tv spelet\n- Benjamin eidem model\nJohn Frost Brug Arnhem by JaapMechielsen - Pinterest\nThis 24-hour fire station in Arnhem (The Netherlands) consists of a commercial building with office space, workshops, warehouses, parking vehicles and various Utförlig information. Utförlig titel: En bro vid Arnhem, [Elektronisk resurs], Charles Whiting; Originaltitel: A bridge at Arnhem; Medarbetare: Knut Rosén.\nHotell i Arnhem Hotel Haarhuis - TiCATi.com\nThis year marks the 75th anniversary of the battle for the bridge at Arnhem – of The battle of Arnhem IN 9 DAys.\nDue to several reasons Frost had to retreat, and the bridge was again in German control. The movie A Bridge Too Far is about this ""Arnhem: The Third Bridge is an Advanced Squad Leader module that depicts the battle for the bridge made famous in print and film by Cornelius Ryan\'s \'A Bridge Too Far\'.']"	['<urn:uuid:bf00680d-16a0-481b-b9fa-396e43779852>', '<urn:uuid:502f7c57-ad67-4aff-bf71-c80c2212f1f3>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	7	93	1850
35	How did early European explorers like Thomas Mitchell view the unmapped regions of Queensland during their expeditions in the 1800s?	Early European explorers viewed unmapped regions of Queensland as 'dark' and dangerous places full of uncertainty and savagery. They believed it was their role to draw these places into the 'light' of European rationality through mapping them. Thomas Mitchell, who had fought in the Peninsular wars, saw exploration as glorious work meant to 'shed the light of civilisation' on unknown regions.	"[""- Quintessential Queensland\n- Perceptions: how people understand the landscape\n- From runs to closer settlement\n- Geological survey of Queensland\n- Mapping a new colony, 1860-80\n- Mapping the Torres Strait: from TI to Magani Malu and Zenadh Kes\n- Order in Paradise: a colonial gold field\n- Queensland atlas, 1865\n- Queensland mapping since 1900\n- Queensland: the slogan state\n- Rainforests of North Queensland\n- Queenslanders: people in the landscape\n- Aboriginal heroes: episodes in the colonial landscape\n- Australian South Sea Islanders\n- Cane fields and solidarity in the multiethnic north\n- Colonial immigration to Queensland\n- Greek Cafés in the landscape of Queensland\n- Hispanics and human rights in Queensland’s public spaces\n- Italians in north Queensland\n- Lebanese in rural Queensland\n- Queensland clothing\n- Queensland for ‘the best kind of population, primary producers’\n- Too remote, too primitive and too expensive: Scandinavian settlers in colonial Queensland\n- Movement: how people move through the landscape\n- Air travel in Queensland\n- Bicycling through Brisbane, 1896\n- Cobb & Co\n- Journey to Hayman Island, 1938\n- Law and story-strings\n- Mobile kids: children’s explorations of Cherbourg\n- Movable heritage of North Queensland\n- Passages to India: military linkages with Queensland\n- The Queen in Queensland, 1954\n- Transient Chinese in colonial Queensland\n- Travelling times by rail\n- Pathways: how things move through the landscape and where they are made\n- Aboriginal dreaming paths and trading ways\n- Chinese traders in the nineteenth century\n- Introducing the cane toad\n- Pituri bag\n- Press and the media\n- Radio in Queensland\n- Red Cross Society and World War I in Queensland\n- The telephone in Queensland\n- Where did the trams go?\n- ‘A little bit of love for me and a murder for my old man’: the Queensland Bush Book Club\n- Separation: divisions in the landscape\n- Asylums in the landscape\n- Brisbane River\n- Changing landscape of radicalism\n- Civil government boundaries\n- Convict Brisbane\n- Dividing Queensland - Pauline Hanson’s One Nation Party\n- High water mark: the shifting electoral landscape 2001-12\n- Hospitals in the landscape\n- Indigenous health\n- Palm Island\n- Secession movements\n- Separate spheres: gender and dress codes\n- Separating land, separating culture\n- Stone walls do a prison make: law on the landscape\n- The 1967 Referendum – the State comes together?\n- Utopian communities\n- Whiteness in the tropics\n- Conflict: how people contest the landscape\n- A tale of two elections – One Nation and political protest\n- Battle of Brisbane – Australian masculinity under threat\n- Dangerous spaces - youth politics in Brisbane, 1960s-70s\n- Fortress Queensland 1942-45\n- Grassy hills: colonial defence and coastal forts\n- Great Shearers’ Strike of 1891\n- Iwasaki project\n- Johannes Bjelke-Petersen: straddling a barbed wire fence\n- Mount Etna: Queensland's longest environmental conflict\n- Native Police\n- Skyrail Cairns (Research notes)\n- Staunch but conservative – the trade union movement in Rockhampton\n- The Chinese question\n- Thomas Wentworth Wills and Cullin-la-ringo Station\n- Imagination: how people have imagined Queensland\n- Brisbane River and Moreton Bay: Thomas Welsby\n- Changing views of the Glasshouse Mountains\n- Imagining Queensland in film and television production\n- Literary mapping of Brisbane in the 1990s\n- Looking at Mount Coot-tha\n- Mapping the Macqueen farm\n- Mapping the mythic: Hugh Sawrey's ‘outback’\n- People’s Republic of Woodford\n- Poinsettia city: Brisbane’s flower\n- The Pineapple Girl\n- The writers of Tamborine Mountain\n- Vance and Nettie Palmer\n- Memory: how people remember the landscape\n- Anna Wickham: the memory of a moment\n- Berajondo and Mill Point: remembering place and landscape\n- Cemeteries in the landscape\n- Landscapes of memory: Tjapukai Dance Theatre and Laura Festival\n- Monuments and memory: T.J. Byrnes and T.J. Ryan\n- Out where the dead towns lie\n- Queensland in miniature: the Brisbane Exhibition\n- Roadside ++++ memorials\n- Shipwrecks as graves\n- The Dame in the tropics: Nellie Melba\n- Vanished heritage\n- War memorials\n- Curiosity: knowledge through the landscape\n- A playground for science: Great Barrier Reef\n- Duboisia hopwoodii: a colonial curiosity\n- Great Artesian Basin: water from deeper down\n- In search of Landsborough\n- James Cook’s hundred days in Queensland\n- Mutual curiosity – Aboriginal people and explorers\n- Queensland Acclimatisation Society\n- Queensland’s own sea monster: a curious tale of loss and regret\n- St Lucia: degrees of landscape\n- Townsville’s Mount St John Zoo\n- Transformation: how the landscape has changed and been modified\n- Empire and agribusiness: the Australian Mercantile Land and Finance Company\n- Kill, cure, or strangle: Atherton Tablelands\n- National parks in Queensland\n- Pastoralism 1860s–1915\n- Prickly pear\n- Repurchasing estates: the transformation of Durundur\n- Sunshine Coast\n- The Brigalow\n- Walter Reid Cultural Centre, Rockhampton: back again\n- Survival: how the landscape impacts on people\n- Brisbane floods: 1893 to the summer of sorrow\n- City of the Damned: how the media embraced the Brisbane floods\n- Depression era\n- Did Clem Jones save Brisbane from flood?\n- Droughts and floods and rail\n- Missions and reserves\n- Queensland British Food Corporation\n- Rockhampton’s great flood of 1918\n- Station homesteads\n- Tropical cyclones\n- Wreck of the Quetta\n- Pleasure: how people enjoy the landscape\n- Bushwalking in Queensland\n- Cherbourg that’s my home: celebrating landscape through song\n- Creating rural attractions\n- Queer pleasure: masculinity, male homosexuality and public space\n- Railway refreshment rooms\n- Regional cinema\n- Schoolies week: a festival of misrule\n- The sporting landscape\n- Visiting the Great Barrier Reef\n‘Maps break down our inhibitions, stimulate our glands, stir our imagination, loosen our tongues’ thus spoke Carl Sauer in an essay entitled Education of a Geographer in 1956. Sauer goes on to say that maps are the ‘language of geography’ which cut across language barriers.\nMost of us would agree that there is something evocative about maps. Great journeys begin with maps. By tracing the roads, rivers and mountains with our fingers, the entire landscape is accessible to our imagination, fuelling the anticipation for adventure and the possibility of discovery.\nHowever, maps also present us with a view of the world that is limited. Compressed into a two dimensional format or confined to within the borders on an aerial photograph or satellite image the language is objective, scientific and domineering over the landscape, juxtaposed to a ‘horizontal’ view of ‘being’ in and experiencing the landscape itself.\nIn this sense maps are compressed representations of reality. Some would even go further to say that our reality is constructed. Our sense perceptions are like light entering through a stained glass window, where the mind acts as organiser and filter.\nHow we construct the world around us and place ourselves in it is by no means the same for all people and cultures, thereby placing a formidable task for anyone attempting to comprehend how the landscape is perceived and understood.\nAsk a farmer, an accountant or an environmentalist to describe the same landscape and their responses would probably differ and be determined by how they related to the landscape itself, not just in terms of economic activity but something more quintessential.\nMarcel Proust in Remembrance of Things Past referred to our most intimate memories being bound within the physical world around us like the souls of Celtic mythology; the right incantation would release them from their imprisonment whether it be a rock, tree or river. Encounters with the landscape are likely to do the same, evoking some form of memory or aesthetic experience.\nAs a native Tasmanian writing about Queensland and who sometimes suffers nostalgia, my thoughts turn to home when I see a chain of hills or forest, perhaps even one that’s clear felled. I love winters here precisely because they are like the Tasmanian summers and autumn. But I also appreciate Queensland’s unique landscapes and where I go after Queensland thoughts will bring me back here to this place or other places I have been.\nThroughout this atlas there are numerous maps of Queensland representing its landscapes in different ways. In the theme ‘Perceptions’, there are maps that indicate how the landscape is perceived and understood in its own right, and while maps remain silent to our subjective experiences, the process of mapping itself is a useful place to begin to understand the cultural interface between people and landscapes and the way people relate to their landscapes and place themselves in it.\nExciting sheets of white paper\nAs a boy, Joseph Conrad the Polish–born English novelist held a great passion for geography which developed into a penchant for pouring over the great empty map spaces that dominated the world at the time. Europe had been expanding its Empire since the renaissance. Geographical knowledge closely coincided with this yet by the 1860s when Conrad was just ten years old much remained unknown about the world.\nOne such place that caught Conrad’s eye was the African continent. The outline had been known for centuries but the interior remained as ‘exciting white sheets of paper regions unknown’. There young Conrad envisaged the work of great explorers nibbling and attacking the edges of the cartographic emptiness ‘conquering a bit of truth here and a bit of truth there, and sometimes swallowed up by the mystery their hearts were so persistently set on unveiling’.\nSuch a statement might have applied to a number of explorers who passed through what would later be Queensland, though Leichhardt seems to reflect this the most when during an expedition in 1848 he vanished without a trace.\nIn light of such intrepid explorers it suffices to say that in 1846 when Major Thomas Mitchell set out on an expedition to explore central Queensland, he was moving into unknown territory and a blank space on the map he was hoping to fill. The places he was entering were considered ‘dark’ and dangerous, full of uncertainty and savagery and it was the role of geography and exploration to draw these places into the ‘light’ of European rationality; in other words to map them.\nExploration in the nineteenth century concerned the systematic gathering of geographic ‘facts’. Plants and animals were collected while journals recorded explorers’ descriptions of the landscape and encounters with Indigenous people. Importantly attempts were made to accurately position themselves on the map according to lines of latitude and longitude of which explorers had various proficiencies.\nJoseph Conrad described this rational and scientific approach to exploration as a kind of ‘militant geography’ of which a military style conquest over the landscape wasn’t far off in the minds of explorers. Thomas Mitchell who had experienced pitched battles against the French during the Peninsular wars remarked in 1839 that victory on the battlefield did not surpass the glory of exploration ‘the purposes of which was to shed the light of civilisation’ on regions formerly unknown.\n‘Militant geography’ was closely aligned with the conceptualisation of empty cartographic space which shaped impressions of Queensland as it did in the other colonies highlighting a distinct dichotomous perception of the landscape, as rational or irrational, savage or civilised.\nWhile cartographic representations were nominally ‘objective’, the conceptualisation of landscape was firmly from a European cultural perspective. ‘Exciting sheets of white paper’ were a cultural construct, which placed colonies geographically peripheral to Europe and presented them as mysterious and open to exploration and invasion. Landscapes became conceptually ‘available’ and the fact that this coincided with Imperial interests was no coincidence.\nEncountering Indigenous people and their landscape\nIn 1847 while exploring the upper reaches of the Barcoo River, central west Queensland, a curious incident happened to Edmund Kennedy not unlike other kinds of early Indigenous-European contact. Having observed Kennedy’s party and their strange animals from a distance, a group of Aboriginal people made gestures beckoning them to stay. However, on approaching them, this was followed by a stern indication that they were to ‘move along’.\nThe message was blunt and clear, they were trespassing. However, this move struck members of the expedition as both insulting and absurd. They were familiar with Indigenous hostility. But why act in such a contrary manner?\nTo understand the context of this encounter it is worthwhile mentioning Kennedy had crossed this country before and was returning along the same route. He noted that previously most Aboriginal people had been wary of him, however, on his return many would come up to them without fear, and in many cases, tell them to leave.\nWhile such encounters are fraught with difficulties in interpretation, I believe the incident in question was an example of Indigenous confidence, dare I say arrogance that the landscape around them was their own. They were not inviting Kennedy to stay and changing their minds, they were trying to get Kennedy’s party to stop, to tell them in person to leave and express their displeasure that he had trespassed, twice no less.\nThis interpretation appears vindicated by the next day’s events where they came across two elderly Aboriginal women. They had evidently been collecting yams as they carried with them digging sticks, yet they also took the opportunity to approach the men and were ‘abusive’ and ‘bullied’ them as they left. However, these assertions of territoriality and self-assured behaviour in confronting Europeans would diminish with the advance of Squatters and violent repression of Indigenous people.\nThe way the landscape has been mapped by explorers excluded alternative conceptualisations of the land; particularly those of Indigenous people which is an irony as many explorers relied on their direction and assistance. Their voices remain silent in the colonial literature which often necessitates a re-reading of the historical text in light of more recent scholarly research into Indigenous culture and race relations.\nThough occupying the same space it is clear that Indigenous people and explorers mapped the landscape differently. Europeans found themselves in an unfamiliar landscape though one that they were closely observing for its productive purposes. Science allowed it to be navigated and conceptualised in a rational manner. Landscapes were imagined and fixed into a wider geographical matrix of relative distances, heights above sea level and location of the earth’s surface according to latitude and longitude.\nFor the Indigenous viewer the landscape was deeply contextualised wherein ancestral beings lay quiescent in mountains, rivers, caves and other features of the landscape. This was a very different conceptualisation of landscape which formed a topographical and spiritual narrative that was both map and story. Knowledge of the ancestral Dreaming provided rights and access to Country, a geographical, spatial and spiritual knowledge that the white man was ignorant of which emphasised his trespass and intrusion.\nThis way of knowing the world formed an integral part of Indigenous culture and interaction with the landscape, yet this was a cultural invention and projection of human values onto the landscape as much as European ‘scientific’ methods were.\nQueensland on the map\nSince early ocean explorers gazed upon Queensland’s coast and speculated what lay beyond the fringe of beaches, forests and mountains they saw, geography, exploration and the maps that have been produced have defined and constructed Queensland’s landscapes, albeit from a Western perspective.\nCook, Flinders and King were among the first navigators who gave a detailed outline of Queensland’s coast. They were in turn followed by hydrographers who provided detail of its reefs, shoals and harbours. The process of mapping and inscription was complimented by inland explorers who were in turn followed by land surveyors who delineated Queensland’s boundaries, subdivided the land and planned the cities.\nAerial photography and the outbreak of war in the Pacific created new imputes for mapping leading to some of Queensland’s first detailed topographic maps while the development of remote sensing GIS technologies have taken precision mapping to an even higher level and opened up new areas of geographical research.\nThe establishment of the SLATS (Statewide Land Cover and Trees Survey) programme by the Queensland government in the 1990s was a significant landmark, processing satellite images that enabled land cover to be monitored state-wide. Concurrently the development of GIS technology has enabled this extensive matrix of data to be analysed and understood in greater complexity.\nHowever, are we any closer to understanding our landscapes? Or have these mapping traditions for all their benefits, inadvertently disassociated us by accentuating a narrow positivist perception of landscape that emphasises material process over cultural understanding?\nWithout a contextualised understanding of our landscapes, the avenues for connection with the world around us are limited. This atlas critically examines people’s place in the landscape. The landscape is a library and it is only by broadening our understanding beyond narrow spatial definitions, that we may truly encounter the landscape in its deeper sense.\nMaps are a wonderful place to start forging these connections yet we will remain alienated without actually placing ourselves in the landscape. In other words go where your curiosity draws you! By doing this you are taking and experiencing something of the landscape that is both unique and your own as well revealing to the intimate possibilities of experience. These are the truly evocative features of landscape which maps conceal from us as much as they show.\nJohn Leighly (ed) Land and life: a selection from the writings of Carl Ortwin Sauer, Berkeley, University of California Press, 1963\nJoseph Conrad, Tales of hearsay and last essays, London, Dent and Sons, 1926\nThomas Mitchell, Journal of a journey into the interior of Tropical Australia in search of a route from Sydney to the Gulf of Carpentaria, London, Longman, Brown and Longmans, 1848\nThomas Mitchell, Three expeditions into the interior of Eastern Australia: with descriptions of the recently explored regions of Australia Felix, and of the present colony of New South Wales, London, T. & W. Boone, 1839\nEdgar Beale, Kennedy, the Barcoo and beyond, 1847: the journals of Edmund Besley Court Kennedy and Alfred Allatson Turner with new information on Kennedy’s life, Hobart, Blubber Head Press, 1983""]"	['<urn:uuid:2e389b05-a3cf-4b71-98d2-b3952351c684>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	20	61	2990
36	michigan sulfide mine water treatment method eagle project reverse osmosis cost dollars	The Eagle Project mine uses a $10-million water treatment plant with reverse osmosis technology to clean any water that comes in contact with mining activities. The treated water is either recycled into the mining process or returned to the ground.	"['Special Report: Pollution, Poverty, People of Color\nCommunities across the US face environmental injustices\nHead in any direction on Michigan’s Upper Peninsula and you will reach gushing rivers, placid ponds and lakes – both Great and small.\nAn abundant resource, this water has nourished a small Native American community for hundreds of years. So 10 years ago, when an international mining company arrived near the shores of Lake Superior to burrow a mile under the Earth and pull metals out of ore, the Keweenaw Bay Indian Community of the Lake Superior Band of Chippewa had to stand for its rights and its water.\nAnd now, as bulldozers raze the land and the tunnel creeps deeper, the tribe still hasn’t backed down.\n“The indigenous view on water is that it is a sacred and spiritual entity,” said Jessica Koski, mining technical assistant for the Keweenaw Bay community. “Water gives us and everything on Earth life.”\nThe Keweenaw Bay Indians are fighting for their clean water, sacred sites and traditional way of life as Kennecott Eagle Minerals inches towards copper and nickel extraction, scheduled to begin in 2014.\nTribal leaders worry the mine will pollute ground water, the Salmon Trout River and Lake Superior, and strip the spiritual ambiance from their historical sites. Meandering through the Huron Mountains before spilling into Lake Superior, the river is home to endangered coaster trout as well as other fish that the tribe depends on for food.\nThe Keweenaw Bay community’s L’Anse Reservation, home to 1,030 people, is both the oldest and the largest reservation in Michigan and sits about 30 miles west of the river. The struggle of this small community in remote, sleepy northernmost Michigan mirrors that of its native ancestors.\nAccording to the U.S. Bureau of Indian Affairs, there are 565 recognized Native American tribes. About 5.2 million people identified themselves as Native American or Alaska Native in the 2010 U.S. Census. But that sliver of the country’s population – 1.7 percent - historically has faced an unfair burden of environmental justice issues.\nSince early European immigration there have been palpable culture clashes with Native Americans – with the indigenous people often on the losing end. Infectious diseases, forced assimilation and land grabs marred early relations.\nBut as the nation grew larger, the environmental justice issues did, too. Native American reservations have been targeted as places to dump industrial waste, and to mine both uranium and coal, leading to polluted rivers, lakes and tribal lands across the country. Some tribes have turned to waste storage or mining as revenue generators.\nNative Americans continue to battle poverty, joblessness and low incomes. About 28.4 percent of American Indians and Alaska Natives – nearly twice the national rate – lived in poverty in 2010. Their unemployment hovers around 49 percent, according to the Bureau of Indian Affairs’ most recent labor force report in 2005.\nLow income and environmental threats often go hand-in-hand, said Kyle Whyte, an assistant professor of philosophy at Michigan State University who studies Native American environmental justice issues.\nNative Americans are even more vulnerable than other disadvantaged groups because of their reliance on natural resources for survival, he said. The top environmental justice issues still plaguing their communities are lack of healthy foods and water, and protection of sacred sites – all at play in northern Michigan.\nFor the 3,552 members of the Keweenaw Bay tribe, it’s more than just water at stake. “It is a living thing that provides for us – physically and spiritually,” Koski said.\nWhyte said this view of water and the surrounding area is unique to tribes and should guide governance. “Part of it is admitting that some groups have a different conception of sacredness than we do,” he said.\n""Almost more pure than rainfall""\nThe newest controversy is over the Eagle Project, an underground nickel and copper mine just west of Marquette, Mich., a few miles inland from the shores of Lake Superior. Mine development began in 2010. It is now 75 percent complete and is scheduled to operate in 2014, according to Kennecott Eagle Minerals, owner, developer and future operator of the mine. The tribe, however, hopes to derail it with pending lawsuits.\nThe concerns about water contamination stem from the method, sulfide mining, which extracts metals from sulfide ores. When the sulfide ores are crushed, the sulfides are exposed to air and water, which catalyzes a chemical reaction that produces highly toxic sulfuric acid. The acid can then drain into nearby rivers, lakes and ground water sources – a phenomenon called acid mine drainage.\n“Water is the top environmental concern,” Koski said. “In addition to ourselves, all of the plants and wildlife rely on that water, and we have treaty rights for hunting, fishing and gathering.”\nCompany officials say they have addressed environmental concerns.\nAny water, including rain and snow, that comes in contact with mining activities is sent to a $10-million water treatment plant that will use a cleansing technology called reverse osmosis, said Daniel Blondeau, communications and media relations advisor at Rio Tinto, the London-based mining company that owns Kennecott.\nThe water is then either recycled into the mining process or returned into the ground. Blondeau said mining effluent will be tested every day and results will be sent to the state monthly.\nThose tasked with keeping Michigan’s water clean say they are confident that this treatment method, already used in many places to purify drinking supplies, will work.\n“We actually really don’t expect a lot of water in the mine … I mean there will be some,” said Hal Fitch, director of the office of oil, gas and minerals for the Michigan Department of Environmental Quality. “Once that water goes through reverse osmosis treatment, it comes out almost more pure than rainfall … In fact, they have to have a roof over treatment plants so the treated water isn’t contaminated by rainwater.”\nExperts tout the reverse osmosis and reuse as an example of technology overcoming environmental obstacles.\n“The way Eagle will process the material, there will be no smelting onsite, so there’s very little likelihood for contamination,” said Klaus Schulz, a senior research geologist with the U.S. Geological Survey. “As long as they aren’t putting tailings (leftover mining material) into the streams there should be no problems.”\nTailings will be stored in an offsite temporary holding area double-lined with leak detection and collection systems, according to Kennecott officials. The tailings will also be mixed with limestone to neutralize the acid potential.\nSchulz said historically mishandled tailings have been to blame for contamination.\n“The reverse osmosis is leaps and bounds over what used to happen, which was water being dumped in lakes and rivers,” Schulz said. “Plus they’ll be recycling and reutilizing the water in the process, which lessens the withdrawal.”\nWestern states have seen the most sulfide mine contamination, Schulz said. Two of the most well-known examples are the Summitville and Gilt Edge mines.\nIn the 1980s, the Summitville Mine in southwestern Colorado contaminated the Wrightman Fork tributary and the Alamosa River. The acid drainage stemmed from poor holding areas and tailing leakage. Ground water in that area is not used for drinking. But the Alamosa River below the site still cannot support aquatic life.\nThe Gilt Edge Mine in South Dakota was a gold mine that an insolvent company abandoned in the late 1990s, leaving behind 150 million gallons of acidic heavy-metal-laden water, as well as millions of cubic yards of acid-generating tailings. The Strawberry and Bear Butte creeks have been contaminated.\n“Sure, historically there have been issues, but there are techniques today to deal with all of that,” Schulz said.\nNow-shuttered Wisconsin mine\nUnder the Treaty of 1842, the Chippewa gave the U.S. government land bordering Lake Superior in what is now the western half of Michigan’s Upper Peninsula and northeast Wisconsin. The tribes were paid and allowed to continue hunting, fishing and gathering on the ceded land.\nKennecott now owns about 1,600 acres, including the mine site, within that territory given to the government 170 years ago. Over its seven- to eight-year lifespan, the mine will produce 300 million pounds of nickel and 250 million pounds of copper, and directly employ about 300 people, according to Kennecott estimates.\nIn recent years, the land surrounding Lake Superior has been a hotspot for companies seeking to mine, process and sell metals. A similar copper and nickel sulfide mine proposal in St. Louis County, Minn., by Polymet Mining, has come under similar attacks by residents concerned about the water supply.\nThe Eagle mine will be the first to use sulfide extraction in Michigan. The state has had copper mines in the past but it was native copper, not copper tied up in sulfide, Schulz said.\n“There are no examples they can point to of sulfide mines that haven’t caused pollution,” Koski said.\nBut Kennecott points to its now closed Flambeau Mine that operated in Rusk County, Wis., from 1993 to 1997. Reclamation of the copper and gold mine was completed in 1999, when it was filled back in.\n“We have not found any violations of mining permits or state law, have not issued any violations at the Flambeau in compliance,” said Phil Fauble, mining coordinator with the Wisconsin Department of Natural Resources.\nKennecott is responsible for the Flambeau site in perpetuity. During mine backfill and site cleanup, the company found a few areas where there was copper contamination. The company took care of the contamination right away, Fauble said. His department hasn’t yet completed studies to see if these areas could harm wildlife or people.\nEmily Whittaker, executive director of the Yellow Dog Watershed Preserve, an environmental group that is also fighting the Eagle mine, said Flambeau is a reason not to trust Kennecott. She pointed to an ongoing lawsuit brought by the Wisconsin Resource Protection Council that charges Kennecott under the Clean Water Act.\nWhittaker said the preparation for mining has already altered the environment in Michigan. Road widening for trucks is probably to blame for increased sedimentation of the Salmon Trout River, she said. Portions of the mine will be drilled directly below the river.\n“Our main concern is the condition of the environment. Our secondary concern is the communities that depend on this environment,” Whittaker said. “Water is the lifeblood of this area.”\nHunting, fishing and blueberry gathering have already been hampered because development has gobbled up prime land, Koski said.\n“Right now it’s an access issue, with the complete bulldozing,” Koski said. “If and when the mine opens, it will be pollution impacting wildlife and treaty-protected resources.""\n“It is still very sacred to us.”\nOne company compromise – providing access to Eagle Rock – hasn’t mollified tensions. Eagle Rock is a spiritual gathering place.\n“We went there early afternoon on a Saturday, had to show our ID’s … show our tribal cards,” Koski said. “We had to put on bright green and red vests, wear safety goggles … there were cranes everywhere … we were escorted by two company officials.”\nBlondeau said the state requires the company to protect the rock’s surface and prohibits any mining activities on the rock. But to the Keweenaw Bay community, these requirements haven’t protected the rock’s essence.\n“We used to drive up freely without permission or being escorted, now there is a high berm and a barbed wire fence,” Koski said. “We aren’t able to stay the night or do any traditional fasting. The whole integrity of the site is disturbed. But it is still very sacred to us.”\nFitch said that Kennecott has provided adequate access. He referred to the site as “the so-called Eagle Rock.”\n“It never had that name before the project, but I guess there’s an oral tradition,” Fitch said.\nIt’s these kinds of communication breakdowns that are at the heart of persistent Native American environmental justice issues across the country, Whyte said.\n“When a tribe expresses its own knowledge and conceptions of things like the environment to companies or officials, and it is respected, justice issues like being able to protect sacred sites and clean water are often handled in a fair way,” Whyte said.\nHe said the state and federal government has a mixed record on handling Native American issues, and he wasn’t so sure that this open communication was happening in the upper reaches of Michigan – especially over Eagle Rock.\n“I think it’s fair to question whether the company, federal and state governments are truly respecting the unique caretaking practices of the tribe for that space,” Whyte said.\nLittered with litigation\nThe only federal regulatory hurdle Kennecott faced was approval of the Environmental Protection Agency’s underground injection control program. After reviewing the treatment plans, the EPA determined a federal permit wasn’t necessary.\nThe Keweenaw community initially reached out to the EPA but did not get very far, Koski said. The EPA acknowledges that environmental justice issues persist in Native American communities, even accepting some blame.\n“The environmental justice issues facing Native American communities range from direct environmental, public health, cultural and sacred sites impacts, to lack of meaningful involvement and fair treatment in the governmental decision-making processes,” according to an emailed response by the EPA’s press office.\nThe EPA did not comment on relations with the Keweenaw Bay Indian Community.\nFitch said the state has included the tribe every step of the way.\n“They were there when we went over the rules and didn’t say much. We had meetings at the governor’s office with them and they didn’t say much,” Fitch said. “Now they come out and they’re critical, and that bothers me.”\nKoski said the tribe has opposed the mine since talks began. Failing to make much progress at the state or federal level, the Keweenaw Bay community has reached out to the United Nations, meeting with James Anaya, the UN\'s special rapporteur on the rights of indigenous peoples.\nAs the project pushes on, it remains littered with litigation.\nCurrently, the Keweenaw Bay Indian Community and three organizations – the National Wildlife Federation, Huron Mountain Club and Yellow Dog Watershed Preserve – are waiting to hear whether Michigan’s Court of Appeals will hear their challenge of a 2006 state permit to build the mine.\nIn addition, the Huron Mountain Club, a private landowners club, filed a lawsuit in April contending Kennecott didn’t get permits from the U.S. Army Corps of Engineers. The lawsuit claims that the permits are necessary since the project could harm the Salmon Trout River, wetlands, endangered species and sacred Native American sites.\nSo far lawsuits and knocking on government doors have proven fruitless and costly, Koski said. But the tribe will continue its steady drumbeat of opposition, pushing industry and government to respect their land, their water, their beliefs and their rights.\n“An elder recently gave me a bracelet symbolizing a rainbow,” Koski said. “She said she saw that things were going to be changing.\n“There is still a lot of hope.”\nThis article originally ran at Environmental Health News, a news source published by Environmental Health Sciences, a nonprofit media company.']"	['<urn:uuid:145a85d3-d026-47f4-857f-b1e9f548e403>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	12	40	2503
37	What exactly is cryptocurrency and how do people own and control their digital coins?	Cryptocurrency is a blockchain-based digital currency where ownership is proven through digital cryptographic signatures. When you own cryptocurrency, you don't actually have digital tokens stored on your computer. Instead, you have cryptographic keys that give you permission to transfer cryptocurrency tokens that exist on the public blockchain. Using public key cryptography, you can prove you own the tokens and transfer them without revealing sensitive information that could compromise your access to the tokens.	['What is blockchain and cryptocurrency?\nBlockchain is simply a method of accounting that can be done by a distributed network rather than a centralised processing centre. Records are compiled into batches called blocks and then verified by one of the computers on the network. The first computer to verify the records in a block as being valid cryptographically sign the block and distribute it to the rest of the network.\nUpon receiving this new block, the rest of the network confirms the validity of the records and the block as a whole. If the checks pass, then each computer accepts that block as the most recent changes to the ledger and then proceeds to work on validating the next batch. This linked chain of batched records is what is referred to as a blockchain.\nThe computer that was able to validate and distribute the next block of records before anyone else and have it accepted by the network is receives a reward which is what incentivises the distributed network to participate.\nAll records in a block are amendments to the existing ledger and it is not possible to delete or directly update a record from a previous block. This gives blockchains a perfect level of audibility compared to a lot of centralised systems and the difference can be visuallised as such.\nA traditional accounting ledger is mostly like a spreadsheet.\nIf Alice wants to send 2 bananas to Bob, in a centralised and trusted processing system we would simply update the values in the spread sheet to reflect the change because we accept our system is acting in good faith.\nA blockchain works more like a spreadsheet workbook with multiple tabs.\nIf Alice wants to send 2 bananas to Bob using a blockchain, instead of altering the existing record, we would open a new tab and record only what information has changed.\n|Block 1||Block 2|\nThis means there is no longer a single record where an account balance is store, but Bob’s current balance of bananas is instead calculated by inspecting the blockchain and calculating how many banans bob has received and subsequently sent. If a third transaction were to occur, we would simply open a third tab (or block) and again record the changes, and so on.\nThis method provides a robust framework for creating a perfectly audit-able ledger of records and combined with some of the other cryptographic inherent to blockchain provide a safe and secure method to allow decentralised validation of transactions without the possibility of fraudulent records entering the ledger unnoticed.\nThis style of publicly validated transaction ledger was invented by the person (or persons) operating under the pseudonym of Satoshi Nakamoto in 2009. The first application of blockchain technology was as an accounting ledger for the digital currency Bitcoin.\nBitcoin is a blockchain based digital currency otherwise referred to as cryptocurrency. Cryptocurrencies use digital cryptographic signatures to prove ownership over coins which exist on the public blockchain hence the term.\nAs an owner of cryptocurrency, you never physically have a file on your computer which is the cryptocurrency tokens as such. What you do have on your computer instead are cryptographic keys which grant you access to transfer cryptocurrency tokens which exist on the public blockchain. Using the magic of cryptography, this transfer can be independently validated as originating by the owner of the tokens by a third party without needing to reveal any information which could lead to the third party gaining access to the tokens. This proof system is commonly known as public key cryptography and is extremely well understood proven as sound.\nMost blockchains today are focused on being ledgers for value transactions of cryptocurrencies. However it is not the only purpose that a blockchain can serve. Some blockchains are used to record logistics information, manage decentralised services, voting systems with many more uses being researched and developed all the time.\nQuestions & Answers\nInland Revenue has released a Questions & Answers article covering their position on cryptocurrency and tax\nInland Revenue is extending its guidance on cryptocurrencies to cover their use in employee share schemes\nScott Mason from Findex unpacks the Inland Revenue guidance on cryptocurrency taxation']	['<urn:uuid:22ab18ba-3712-4e00-a39c-cafd882eb192>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	14	73	696
38	Do anorexia and bulimia patients differ in their body weight?	Yes, people with bulimia typically fall within the normal weight range, though their weight may fluctuate. In contrast, people with anorexia are characterized by emaciation and unwillingness to maintain a normal or healthy weight.	['Bulimia Nervosa is a serious, potentially life threatening mental illness.\nA person with bulimia has not made a “lifestyle choice”, they are actually very unwell and need help.\nThe reasons for developing Bulimia will differ from person to person; known causes include genetic predisposition and a combination of environmental, social and cultural factors.\nBulimia is characterised by repeated episodes of binge eating followed by compensatory behaviours. In addition, people with bulimia place an excessive emphasis on body shape or weight in their self-evaluation. This can lead to the person’s sense of self-esteem and self worth being wholly defined by the way they look.\nA person with Bulimia can become lost in a dangerous cycle of eating out of control and attempts to compensate which can lead to feelings of shame, guilt and disgust. These behaviours can become more compulsive and uncontrollable over time, and lead to an obsession with food, thoughts about eating (or not eating), weight loss, dieting and body image.\nThese behaviours are often concealed and people with Bulimia can go to great lengths to keep their eating and exercise habits secret. As a result, Bulimia can often go undetected for a long period of time.\nMany people with Bulimia experience weight fluctuations and do not lose weight, they can remain in the normal weight range, be slightly underweight, or may even gain weight.\nBinge eating involves two key features:\n- Eating a very large amount of food within a relatively short period of time (e.g. within two hours)\n- Feeling a sense of loss of control while eating (e.g. feeling unable to stop yourself from eating)\nCompensatory behaviours are used as a way of trying to control weight after binge eating episodes. They include:\n- Misusing laxatives or diuretics\n- Excessive exercise\n- Use of any drugs, illicit, prescription and/or ‘over the counter’ inappropriately for weight control (inappropriate use refers to use that is not indicated and for which the drug has not been prescribed).\nHaving awareness about Bulimia and its warning signs and symptoms can make a marked difference to the severity and duration of the illness. Seeking help at the first warning sign is much more effective than waiting until the illness is in full swing. If you or someone you know is exhibiting some or a combination of these signs it is vital to seek help and support as soon as possible.\nThe warning signs of Bulimia can be physical, psychological and behavioural. It is possible for someone with Bulimia to display a combination of these symptoms.\n- Frequent changes in weight (loss or gains)\n- Signs of damage due to vomiting including swelling around the cheeks or jaw, calluses on knuckles, damage to teeth and bad breath\n- Feeling bloated, constipated or developing intolerances to food\n- Loss of or disturbance of menstrual periods in girls and women\n- Fainting or dizziness\n- Feeling tired and not sleeping well\n- Preoccupation with eating, food, body shape and weight\n- Sensitivity to comments relating to food, weight, body shape or exercise\n- Low self esteem and feelings of shame, self loathing or guilt, particularly after eating\n- Having a distorted body image (e.g. seeing themselves as overweight even if they are in a healthy weight range for their age and height)\n- Obsession with food and need for control\n- Depression, anxiety or irritability\n- Extreme body dissatisfaction\n- Evidence of binge eating (e.g. disappearance or hoarding of food)\n- Vomiting or using laxatives, enemas, appetite suppressants or diuretics\n- Eating in private and avoiding meals with other people\n- Anti social behaviour, spending more and more time alone\n- Repetitive or obsessive behaviours relating to body shape and weight (e.g. weighing themselves repeatedly, looking in the mirror obsessively and pinching waist or wrists)\n- Secretive behaviour around food (e.g. saying they have eaten when they haven’t, hiding uneaten food in their rooms)\n- Compulsive or excessive exercising (e.g. exercising in bad weather, continuing to exercise when sick or injured, and experiencing distress if exercise is not possible)\n- Dieting behaviour (e.g. fasting, counting calories/kilojoules, avoiding food groups such as fats and carbohydrates)\n- Frequent trips to the bathroom during or shortly after meals which could be evidence of vomiting or laxative use\n- Erratic behaviour (e.g. spending large amounts of money on food)\n- Self harm, substance abuse or suicide attempts\nThe risks associated with Bulimia are severe. People with Bulimia may experience:\n- Chronic sore throat, indigestion, heartburn and reflux\n- Inflammation and rupture of the oesophagus and stomach from frequent vomiting\n- Stomach and intestinal ulcers\n- Chronic irregular bowel movements, constipation and/or diarrhoea due to deliberate misuse of laxatives\n- Osteoporosis– a condition that leads to bones becoming fragile and easily fractured\n- Loss of or disturbance of menstrual periods\n- Increased risk of infertility for all genders\n- Irregular or slow heart beat which can lead to an increased risk of heart failure\nTherapies to be considered for the treatment of Bulimia Nervosa include:\n- Psychological Treatments\n- Evidence-based self-help programs\n- Cognitive behavioural therapy for Bulimia Nervosa\n- Interpersonal psychotherapy\n- Maudsley family-based treatment (for children and adolescents)\n- Dialectical behaviour therapy\n- Guided imagery\n- Crisis intervention\n- Stress management\nAntidepressants (specifically selective serotonin reuptake inhibitors; e.g. SSRI) may also be prescribed for someone who is suffering from Bulimia Nervosa.\nRecovery from Bulimia Nervosa is possible\nIt is possible to recover from Bulimia, even if you have been living with the illness for many years. The path to recovery can be very challenging. People with Bulimia can become entangled in a vicious cycle of eating and exercise behaviours that can impact their ability to think clearly and make decisions. However, with the right team and a high level of personal commitment, recovery is an achievable goal.\nIf you suspect that you or someone you know has Bulimia, it is important to seek help immediately. The earlier you seek help the closer you are to recovery. While your GP may not be a specialist in eating disorders, they are a good ‘first base.’ A GP can provide a referral to a practitioner with specialised knowledge in health, nutrition and eating disorders.\nFor support, information, access to resources or referrals, you can also contact Butterfly’s National Helpline on 1800 33 4673 (8am-midnight, AEST, 7 days a week), email firstname.lastname@example.org, or use our Helpline chat.', 'What Are Eating Disorders?\nAn eating disorder is marked by extremes. It is present when a person experiences severe disturbances in eating behavior, such as extreme reduction of food intake or extreme overeating, or feelings of extreme distress or concern about body weight or shape.\nTypes of eating disorders:\n- Anorexia Nervosa\n- Bulimia Nervosa\n- Binge-Eating Disorderâ€¨\nAnorexia nervosa is characterized by emaciation, a relentless pursuit of thinness and unwillingness to maintain a normal or healthy weight, a distortion of body image and intense fear of gaining weight, a lack of menstruation among girls and women, and extremely disturbed eating behavior. Some people with anorexia lose weight by dieting and exercising excessively; others lose weight by self-induced vomiting, or misusing laxatives, diuretics or enemas.\nMany people with anorexia see themselves as overweight, even when they are starved or are clearly malnourished. Eating, food and weight control become obsessions. A person with anorexia typically weighs herself or himself repeatedly, portions food carefully, and eats only very small quantities of only certain foods. Some who have anorexia recover with treatment after only one episode. Others get well but have relapses. Still others have a more chronic form of anorexia, in which their health deteriorates over many years as they battle the illness.\nAccording to some studies, people with anorexia are up to ten times more likely to die as a result of their illness compared to those without the disorder. The most common complications that lead to death are cardiac arrest, and electrolyte and fluid imbalances. Suicide also can result.\nMany people with anorexia also have coexisting psychiatric and physical illnesses, including depression, anxiety, obsessive behavior, substance abuse, cardiovascular and neurological complications, and impaired physical development.\nOther symptoms may develop over time, including:\n- thinning of the bones (osteopenia or osteoporosis)\n- brittle hair and nails\n- dry and yellowish skin\n- growth of fine hair over body (e.g., lanugo)\n- mild anemia, and muscle weakness and loss\n- severe constipation\n- low blood pressure, slowed breathing and pulse\n- drop in internal body temperature, causing a person to feel cold all the time\nTreating anorexia involves three components:\n- restoring the person to a healthy weight;\n- treating the psychological issues related to the eating disorder; and\n- reducing or eliminating behaviors or thoughts that lead to disordered eating, and preventing relapse.\nSome research suggests that the use of medications, such as antidepressants, antipsychotics or mood stabilizers, may be modestly effective in treating patients with anorexia by helping to resolve mood and anxiety symptoms that often co-exist with anorexia.â€¨Different forms of psychotherapy, including individual, group and family-based, can help address the psychological reasons for the illness. Some studies suggest that family-based therapies in which parents assume responsibility for feeding their afflicted adolescent are the most effective in helping a person with anorexia gain weight and improve eating habits and moods.\nOthers have noted that a combined approach of medical attention and supportive psychotherapy designed specifically for anorexia patients is more effective than just psychotherapy. Unfortunately, no specific psychotherapy appears to be consistently effective for treating adults with anorexia.\nBulimia nervosa is characterized by recurrent and frequent episodes of eating unusually large amounts of food (e.g., binge-eating), and feeling a lack of control over the eating. This binge-eating is followed by a type of behavior that compensates for the binge, such as purging (e.g., vomiting, excessive use of laxatives or diuretics), fasting and/or excessive exercise.â€¨Unlike anorexia, people with bulimia can fall within the normal range for their age and weight. But like people with anorexia, they often fear gaining weight, want desperately to lose weight, and are intensely unhappy with their body size and shape. Usually, bulimic behavior is done secretly, because it is often accompanied by feelings of disgust or shame. The binging and purging cycle usually repeats several times a week. Similar to anorexia, people with bulimia often have coexisting psychological illnesses, such as depression, anxiety and/or substance abuse problems. Many physical conditions result from the purging aspect of the illness, including electrolyte imbalances, gastrointestinal problems, and oral and tooth-related problems.\nOther symptoms include:â€¨\n• chronically inflamed and sore throatâ€¨\n• swollen glands in the neck and below the jawâ€¨\n• worn tooth enamel and increasingly sensitive and decaying teeth as a result of exposure to stomach acidsâ€¨\n• gastroesophageal reflux disorderâ€¨\n• intestinal distress and irritation from laxative abuseâ€¨\n• kidney problems from diuretic abuseâ€¨\n• severe dehydration from purging of fluids\nTo reduce or eliminate binge and purge behavior, a patient may undergo nutritional counseling and psychotherapy, especially cognitive behavioral therapy (CBT), or be prescribed medication. Some antidepressants, such as fluoxetine (Prozac), which is the only medication approved by the U.S. Food and Drug Administration for treating bulimia, may help patients who also have depression and/or anxiety. It also appears to help reduce binge-eating and purging behavior, reduces the chance of relapse, and improves eating attitudes.\nCBT that has been tailored to treat bulimia also has shown to be effective in changing binging and purging behavior, and eating attitudes. Therapy may be individually oriented or group-based.â€¨\nBinge-eating disorder is characterized by recurrent binge-eating episodes during which a person feels a loss of control over his or her eating. Unlike bulimia, binge-eating episodes are not followed by purging, excessive exercise or fasting. As a result, people with binge-eating disorder often are overweight or obese. They also experience guilt, shame and/or distress about the binge-eating, which can lead to more binge-eating.\nObese people with binge-eating disorder often have coexisting psychological illnesses including anxiety, depression, and personality disorders. In addition, links between obesity and cardiovascular disease and hypertension are well documented.\nTreatment options for binge-eating disorder\nTreatment options for binge-eating disorder are similar to those used to treat bulimia. Fluoxetine and other antidepressants may reduce binge-eating episodes and help alleviate depression in some patients.\nPatients with binge-eating disorder also may be prescribed appetite suppressants. Psychotherapy, especially CBT, is also used to treat the underlying psychological issues associated with binge-eating, in an individual or group environment.â€¨']	['<urn:uuid:94230ce9-461d-419a-8076-6807a37b9e57>', '<urn:uuid:aa212d0c-37fe-485a-9796-8fd2b7313841>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T00:43:36.053682	10	34	2070
39	best learning method classroom online active vs passive study proven results	Research demonstrates that active learning methods are more effective than passive learning. Studies show that students who engage in activities and discussions, rather than just passively listening, perform better across subjects like science, engineering, and mathematics. This principle has been successfully applied in both digital and traditional settings - learning apps that incorporate active engagement (like spaced repetition and interactive questioning) show better retention results, with one study showing 69% longer retention periods. Additionally, research-based instructional strategies that promote active student participation through class discussions have been proven to improve learner outcomes.	"['Remembering is one of the core objectives of learning and studying. This is true for those who are studying vocabulary for a foreign language and for those who are learning the rules of the road to pass their driving test. The benefits of the spacing effect are indisputable, i.e., instead of cramming for the exams, it is much better for long-term retention to review the material at spaced intervals. This was first pointed out in the 1890s and science has come a long way in describing the effect of passing time on memory more and more accurately.\nHowever, the above observation does not necessarily lend itself to easy practice. There is a long way between knowing that spacing is good for retention and practically designing a pedagogical structure around it, e.g., textbooks written for students still do not usually account for spaced repetition in any way. People have been attempting to use the spacing effect in practice, starting with physical flashcards and Leitner boxes.\nNow, in the digital era, with ubiquitous computers and learning apps (e.g. Anki, Duolingo, etc.), one can keep detailed records of when learners review each learning item and how good their recall was. These apps and websites use various heuristics to optimize the teaching schedule for each learner and the heuristics often intuitively incorporate some notion of the spacing effect.\nIn our new research study, ""Large-scale randomized experiment reveals that machine learning-based instruction helps people memorize more effectively"", we formalized the problem and presented a theoretically grounded optimal solution that provides a basis for the intuition behind those heuristics.\nIn previous work, we have shown how we can mathematically model a simplified version of the cognitive processes behind forgetting and uncover the optimal point in time for study for each item for a learner, given his/her past reviews. However, with learning apps, learners are in control of their learning experiences. So while knowing the optimal point in time for study is useful, it is unlikely that the learner will initiate their learning session exactly when the time is optimal. Hence, we designed \'Select\', an algorithm that selects the optimal questions for the users to study when they choose to initiate a session. ‘Select’ is a data-driven algorithm wherein the parameters (adjusting to correct/incorrect recall, and initial difficulty of items) are learned by looking at past tests which users have performed on the app. The algorithm itself follows the intuition of prioritizing the review of items that were closest to being forgotten. It adapts to the learners by keeping track of individualized forgetting rates per learner and per item and can be easily deployed with any learning app where the learners initiate the learning sessions.\nWe deployed our algorithm via a popular app that helps users prepare for the driving license exam (iTheorie Führerschein, https://swift.ch). We conducted a randomized trial experiment to compare ‘Select’ against two baseline algorithms. One of the baselines randomly selected the question to pose to the user (named ‘random’) while the other presented questions in their ascending order of previously correct answers and difficulty in a round-robin fashion (named ‘difficulty’). This experiment was run with about 50 thousand learners, which makes it one of the largest studies of its kind.\nWe found the learners who were given questions to review, as recommended by ‘Select’, did improve their retention, all while controlling for the number of days they had used the app and the number of reviews they had done. In particular, we found that learners who were assigned questions by ‘Select’ would remember the content over ~69% longer than the `difficulty` baseline. We also found an interesting interaction between engagement and learning-algorithm selection that can be explored further: more people who were given the ML-algorithm driven reviews, stopped using the app in the initial 3 days, but those who continued using the app were more engaged when compared to learners receiving reviews using other algorithms.\nOur results have direct implications for the learning of large sets of paired-associate items by young learners using machine learning-based instruction. However, more research at the intersection of cognitive sciences and machine learning is needed to generalize our results to different populations of learners, different materials, other tasks, and on the engagement of learners.\nTo learn more, read our research paper, ""Large-scale randomized experiment reveals that machine learning-based instruction helps people memorize more effectively"", published by npj Science of Learning.\n* Our dataset and analysis are available at Networks-Learning/spaced-selection.\nPlease sign in or register for FREE\nIf you are a registered user on Neuroscience Community, please sign in', 'Research proves that engaged students learn more.\nWe have created a real-time actionable measure of student engagement coupled with personalized Tips and research-based instruction strategies (RBIS) which allow educators to immediately improve learner engagement and educational practice in accordance with learning theory research on effective teaching practices. Research is embedded into our Student Engagement Survey (SES) and is the foundation of our RBIS.\nSES was developed through a literature review, an expert panel, and pilot testing. SES reliability was tested using Cronbach’s alpha (Cronbach, 1951). This process yielded a valid and reliable overall student engagement scale and nine categories, to help assess student engagement weaknesses in time to revise an educator’s instructional approach within a semester.\nWe created a definition for each category, a best practice/instructional goal, and questions directly linked to the evidence base of educational research. For example, our cognitive category includes a question on student perception of the amount of active learning in a lesson (i.e., the process of learning through activities and/or discussion in class, as opposed to passively listening to an expert or reading).\nResearch demonstrates that active learning increases student performance in science, engineering, and mathematics (Scott Freeman, et. al., 2014). The SES question on the cognitive category linked to active learning is shown below.\nI learned through activities and/or discussion in class, as opposed to passively listening to my teacher during this class lesson.\nStrongly Agree, Agree, Somewhat Agree, Disagree, Strongly Disagree\nResearch demonstrates that active learning increases student performance in science, engineering, and mathematics (Scott Freeman, et. al., 2014). The SES question on the cognitive subscale linked to active learning is shown below.\nResearch-Based Instructional Strategies (RBIS)\nRBIS are instructional practices where research has proven their effectiveness in improving learner outcomes. We have compiled a database of RBIS which will improve the score for each question on our Student Engagement Survey, such as the active learning strategy shown below:\nActive Learning Strategy: Promote active student learning through class discussion.\nOur Tip Masters and Instructional Coaches have access on the LessonLoop platform to our RBIS database and incorporate relevant instructional strategies into their coaching Tips. We continuously review education research sites and journals to add new instructional strategies with a focus on these sources:\nOur Research Luminaries\nWe gratefully applaud the work of John Hattie and Robert Marzano who have pioneered and made possible the increased incorporation of educational research into instructional practice by synthesizing and translating a tremendous amount of complex social science into easily understandable strategies.\nBill & Melinda Gates Foundation (2015). Teachers know best: What educators want from digital instructional tools 2.0. http://k12education.gatesfoundation.org/resource/what-educators-want-from-digital-instructional-tools-2-0/\nBrenner, L. (2015, October 27). 3 ways to increase student engagement in your classroom. ISTE. https://www.iste.org/explore/Innovator-solutions/3-ways-to-increase-student-engagement-in-your-classroom\nBryk, A. S., Gomez, L.M., ,Grunow, A. , LeMahieu, P.G. (2015). Learning to improve: How America’s schools can get better at getting better. Harvard Education Press.\nCarini, R.M., Kuh, G.D., & Klein, S.P. (2006). Student engagement and student learning: Testing the linkages. Research in Higher Education, 47(1), 1-32. http://www.jstor.co/stable/40185882\nCAST (2018). Universal Design for Learning Guidelines version 2.2. http://udlguidelines.cast.org\nChen, P.D., Lambert, A.D., & Guidry, K.R. (2010). Engaging online learners: The impact of Web-based learning technology on college student engagement. Computers & Education, 54, 1222-1232. https://www.sciencedirect.com/science/article/pii/S0360131509003285?via%3Dihub\nChi, M. T.H., & Wylie, R, (2014). The ICAP framework: Linking cognitive engagement to active learning outcomes. Educational Psychologist, 49(4), 219–243, 2014. https://doi.org/10.1080/00461520.2014.965823\nClark, R. C., & Mayer, R. E. (2016). e-Learning and the Science of Instruction, Proven Guidelines for Consumers and Designers of Multimedia Learning. Wiley Online Library. https://onlinelibrary.wiley.com/doi/book/10.1002/9781119239086\nDavis, T. & Thomas F. V. (2020). The impact of multimedia in course design on students’ performance and online learning experience: A pilot study of an introductory educational computing course. Online Learning Journal, 24(3). https://olj.onlinelearningconsortium.org/index.php/olj/article/view/2069\nDigital Promise (2020). Suddenly online: A national survey of undergraduates during the COVID-19 pandemic. https://digitalpromise.org/wp-content/uploads/2020/07/ELE_CoBrand_DP_FINAL_3.pdf\nDixson, M. (2010, June). Creating effective student engagement in online courses: What do students find engaging? –– Vol. 10, No. 2, 1-13, Journal of the Scholarship of Teaching and Learning, Vol. 10, No. 2. ERIC. https://eric.ed.gov/?id=EJ890707\nDixson, M. D. (2015). Measuring student engagement in the online course: The online student engagement scale (OSE). Online Learning, 19(4), 143-157. https://olj.onlinelearningconsortium.org/index.php/olj/article/view/561\nFredricks, J. A., Blumenfeld, P. C., & Paris, A. H. (2004). School Engagement: Potential of the Concept, State of the Evidence. SAGE Journals. https://journals.sagepub.com/doi/10.3102/00346543074001059\nFredericks, J. A., & McColskey, W. (2012). The measurement of student engagement: A comparative analysis of various methods and student self-report instruments. In S.L. Christenson et al. (eds.) Handbook of Research on Student Engagement (763-782t. Springer.https://doi.org/10.1007/978-1-4614-2018-7_37\nFollman, J. (1995). Elementary public school pupil rating of teacher effectiveness. Child Study Journal, 25(1), 57–78.\nGawande, A. (2010). The checklist manifesto: How to get things right. New York: Picador.\nGoodenow, C. (1993). Classroom belonging among early adolescent students: Relationships to motivation and achievement. Journal of Early Adolescence, 13(1), 21-43.\nHandelsman, M.M., Briggs, W.L., Sullivan, N. & Towler, A. (2005). A measure of college student course engagement. The Journal of Educational Research, 93(3), 184-191.\nHanover Research. (2013). Student perceptions surveys and teacher assessments. Washington, DC. Retrieved fromhttp://dese.mo.gov/sites/default/files/Hanover-Research-StudentSurveys.pdf.\nKyriakides, L. (2005). Drawing from teacher effectiveness research and research into teacher interpersonal behaviour to establish a teacher evaluation system: A study on the use of student ratings to evaluate teacher behavior. Journal of Classroom Interaction, 40(2), 44–66. Retrieved from http://www.eric.ed.gov/PDFS/EJ768695.pdf\nMarzano, R.J. (2003). What works in schools: Translating research into action. Alexandria, VA: Association for Supervision and Curriculum Development.\nMarzano. R.J., Pickering, D.J., Pollock, J.E. (2000). Classroom instruction that works; Research-based strategies for increasing student achievement. Alexandria, VA; Association for Supervision and Curriculum Development.\nMcLeod, S. (2019). The Zone of Proximal Development and Scaffolding. Simply Psychology. https://www.simplypsychology.org/Zone-of-Proximal-Development.html\nMET Project. (2012). Asking students about teaching: Student perception surveys and their implementation (Policy & Practice Brief). Seattle, WA: Bill & Melinda Gates Foundation. Retrieved from http://www.metproject.org/downloads/Asking_Students_Practitioner_Brief.pdf.\nNAIS Report on the 2017 High School Survey of Student Engagement. (2017). NAIS. https://www.nais.org/articles/pages/research/2017-high-school-survey-of-student-engagement-(hssse)/\nNational Association of Independent Schools. (2017), NAIS report on the 2017 high school survey of student engagement. https://www.nais.org/articles/pages/research/2017-high-school-survey-of-student-engagement-(hssse)/\nProject Tomorrow. (2010). Unleashing the future: Educators “speak up” about the use of emerging technologies for learning. https://tomorrow.org/speakup/pdfs/SU09UnleashingTheFuture.pdf\nQuality Matters. (2019). Course design rubric standards. (fifth Edition). https://www.qualitymatters.org/qa-resources/rubric-standards/k-12-secondary-rubric\nRajabalee, B., Rennie, F., & Santally, M. (2019). A study of the relationship between students’ engagement and their academic performances in an eLearning environment. E-Learning and Digital Media, 17(1), 1-20. https://eric.ed.gov/?q=rajabalee&id=EJ1187889\nReyes, M. R., Brackett, M. A., Rivers, S. E., White, M, & Salovey, P. (2012). Classroom Emotional Climate, Student Engagement, and Academic Achievement. Journal of Educational Psychology. https://psycnet.apa.org/fulltext/2012-05721-001.pdf?auth_token=b98fe6dfa9f78edb1c367b779f352ce239fad8ff\nScott Freeman, Sarah L. Eddy, Miles McDonough, Michelle K. Smith, Nnadozie Okoroafor, Hannah Jordt, and Mary Pat Wenderoth, PNAS June 10, 2014 111 (23) 8410-8415; first published May 12, 2014. https://doi.org/10.1073/pnas.1319030111\nSoland, Jim, Kuhfeld, Megan, Tarasawa, Beth, Johson, Angela, Ruzek, Erik, and Liu, Jing (2020) The impact of Covid-19 on student achievement and what it may mean for educators, Brookings Institute. https://www.brookings.edu/blog/brown-center-chalkboard/2020/05/27/the-impact-of-covid-19-on-student-achievement-and-what-it-may-mean-for-educators/\nTaylor, L., & Parsons, J. (2011). Improving student engagement. Current Issues in Education, 14(1). https://cie.asu.edu/ojs/index.php/cieatasu/article/view/745.\nTeachers Know Best: What Educators Want From Digital Instructional Tools (2.0) — Report. (2015). Bill & Melinda Gates Foundation. http://k12education.gatesfoundation.org/resource/what-educators-want-from-digital-instructional-tools-2-0/\nTheobald, E. J., Hill M. J., Tran, E., Agrawal, S., Arroyo, E. N., Behling, S., Chambwe, N., Cintrón, D. L., Cooper, J. D., Dunster, G., Grummer, J. A., Hennessey, K., Hsiao, J., Iranon, N., Jones II, L., Jordt, H., Keller, M., Lacey, M. E., Littlefield, C. E., … Freeman, S. (2020, March). Active learning narrows achievement gaps for underrepresented students in undergraduate science, technology, engineering, and math. Proceedings of the National Academy of Sciences. https://www.pnas.org/content/117/12/6476\nToshalis, E. & Nakkula, M. J. (2012). Motivation, engagement and student voice. Students at the Center Hub. https://studentsatthecenterhub.org/resource/motivation-engagement-and-student-voice/\nTrowler, V. & Trowler, , P. (2010). Student engagement evidence summary. The Higher Education Academy. https://www.heacademy.ac.uk/system/files/StudentEngagementEvidenceSummary_0.pdf\nWilkerson, D. J., Manatt, R. P., Rogers, M. A., & Maughan, R. (2000). Validation of student, principal, and self-ratings in 360 degree feedback for teacher evaluation. Journal of Personnel Evaluation in Education, 149(2), 179–192.\nWillingham, W.W., Pollack, J.M. & Lewis, C. (2002). Grades and test scores: Accounting for observed differences. Journal of Educational Measurement, 39(1). 1-37. https://www.jstor.org/stable/1435104?seq=1#metadata_info_tab_contents\nZepke, N., & Leach, L. (2010). Improving student engagement: Ten proposals for action. Active Learning in Higher Education, 11(3), 167-177.https://journals.sagepub.com/doi/pdf/10.1177/146978741037968\nIcons from Flaticon; Stock images from Unsplash']"	['<urn:uuid:5803135a-3397-4934-8f61-f3273c98762b>', '<urn:uuid:83d1a969-602e-4f02-a123-1364923f0e84>']	open-ended	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-13T00:43:36.053682	11	92	2134
40	As a nematode researcher, I'm intrigued by how these organisms interact with fungi in nature, and how this differs from their commercial applications. What are the natural predator-prey dynamics between nematodes and fungi, and how do beneficial nematodes interact with other organisms in controlled applications?	In nature, there's a complex predator-prey relationship where fungi actually prey on nematodes using various trapping mechanisms. Some fungi create sticky lollipop-like structures or lassos made of three hyphal cells that inflate within seconds to trap nematodes. Once caught, the fungi develop specialized cells called haustoria that penetrate and devour the worm's innards. This behavior is found in over 200 fungal species and dates back at least 100 million years. However, nematodes have evolved defense mechanisms - when touched at their front end, they can quickly reverse direction, allowing about 80% to escape the fungal traps. In contrast, when used as biological control agents, beneficial nematodes become the predators rather than the prey. They actively hunt soil-dwelling insect pests, entering through body openings and releasing bacteria that kill the insects within 48 hours. These beneficial nematodes are harmless to humans, animals, plants, and healthy earthworms, only targeting pest insects. They can control over 200 species of pest insects from 100 different families. After eliminating the pest population, the beneficial nematodes naturally die off and biodegrade.	"['This is the last of three installments celebrating the Week of the Fungi on STC.\nThe space around a single soil particle can be one hell of a battlefield. Here, nematode worms consume bacteria, bacteria fight back and kill them, fungi suck the worms dry, and so on. Not a moment of peace. These battles are as intense as they are intricate, yet we know little about all this drama. Honest, if it weren’t for C. elegans, nematodes would hardly be in my consciousnessdespite their enormous abundance and ecological importance in soils and waters. They are found nearly everywhere. They are major players in the McMurdo Dry Valleys of Antarctica, and have recently been found one kilometer down in a gold mine, where they feed on bacterial biofilms growing on the walls. Nematodes have their bard: a delicious 1914 article by N.A. Cobb graciously extols their glory. But this being Fungus Week at STC, I choose to relate a nematode story of life in the soil that involves fungi.\nIn the ground, fungi and water molds are adept at feeding off nematodes. Like all carnivores, these predators must first catch their prey. They use a variety of strategies for doing this. Fungi make gooey structures, some lollipop-like, to which the worms stick irreversibly, while certain water molds shoot a projectile consisting of a spore into passing nematodes. All these mechanisms are exceedingly effective, but one stands out for its visual horror. Here, the fungus makes a lasso out of three hyphal cells. When a nematode sticks its body into this noose, the fungal cells inflate within a second or two, trapping the worm in an inescapable embrace. In every case, whatever the trapping mechanism used, the fungus makes specialized cells called haustoria, which penetrate into the worm and develop within it, eventually devouring its innards. Fungal cells then emerge and the spores they make are released to spread in the environment.But why do the worms stick their front end in the trap? There is experimental evidence that they respond to some chemotactic bait made by the fungi, although the compounds involved have not yet been fully characterized. (For an action-packed video of this drama, click here.)\nSo, how come not all the nematodes are eaten up? This is a lions-and-zebras question, with one distinction: the fungi (the “lions”) are not obligatory carnivores and do not depend for their livelihood on eating nematodes (the “zebras”). And the nematodes are not totally defenseless. They have developed mechanisms to ensure that at least some will escape the fungal traps. It turns out that when you gently touch the front of one of these worms, it will quickly reverse direction. The short time between worm entry into the fungal ring and its constriction allows some worms to retract and escape the fatal squeeze. In fact, some 80% of the worms escape unharmed. A recent study shows that C. elegans mutants that are defective in the response to touch are more likely to be captured by the fungus. This being C. elegans about whose neurobiology so much is known, the details of the neural circuit involved in this behavior has been elucidated. C. elegans aficionados will enjoy reading the exciting details of this study. These results suggest that selective pressures imposed by predacious fungi have shaped the evolution of the C. elegans “escape behavior.” In the authors’ words: Because we know the molecular and neural underpinnings of the C. elegans escape response in exquisite detail, comparative studies of soil nematodes provide a powerful model to study the mechanisms that underlie the evolution of escape behaviors.\nBack to the fungi. “Nematophagy” among the fungi goes back a long ways, at least to the age of the dinosaurs. Well preserved fossils in 100 million year-old amber show the characteristic constricting rings and other features of today’s carnivorous fungi. In the present world, nematophagous behavior is found in over 200 species, distributed in the main fungal branches (Zygomycota, Basidiomycota, and Ascomycota). Included are such edible (to humans) kinds as the oyster mushrooms and the inky caps. The next mushroom you eat may be a carnivore!\nThis is not all. Nematophagy has made it into science fiction. In a post in Kathie Hodge’s terrific Cornell Mushroom Blog, I found the following bit of essential information: Science fiction fans will recall the famous Piers Anthony novel Omnivore, which features a planet where all organisms in all ecological niches evolved from fungi. In one dramatic scene, the heroes are menaced by gigantic nematodes and then rescued by the constricting ring traps of a heroic Arthrobotrys. Fungi to the rescue!', 'Beneficial Nematodes to eliminate grubs and insect pests\nSafe Biological Pest Control\nAsk a Question Home Nematodes Biology fact sheet 520-298-4400 Order Your Nematodes Here! email@example.comIntegrated Pest Management IPM\nNATURAL PEST CONTROL WITH BENEFICIAL NEMATODES\nBiological Control Of Pest Insects With Nematodes. Beneficial Nematodes naturally occur in soil and are used to control soil pest insects and whenever larvae or grubs are present. Like all of our products, it will not expose humans or animals to any health or environmental risks. Beneficial nematodes only attack soil dwelling insects and leave plants and earthworms alone. The beneficial nematodes enters the larva via mouth, anus or respiratory openings and starts to feed. This causes specific bacteria to emerge from the intestinal tract of the nematode. These spread inside the insect and multiply very rapidly. The bacteria convert host tissue into products which can easily be taken up by the nematodes. The soil dwelling insect dies within a few days. Beneficial nematodes are a totally safe biological control in pest insects. The Beneficial nematodes are so safe the EPA has waived the registration requirements for application.NATURE\'S BEST WAY OF KILLING Grubs and Japanese Beetles. We ship Beneficial Nematodes to USA and Canada. Call 520-298-4400 for information to place order for shipment to Canada. For USA visit our website www.buglogical.com\nThough they are harmless to humans, animals, plants, and healthy earthworms, beneficial nematodes aggressively pursue insects. The beneficial nematodes can be used to control a broad range of soil inhabiting insects and above ground insects in their soil inhabiting stage of life. More than 200 species of pest insects from 100 insect families are susceptible to these nematodes. When they sense the temperature and carbon dioxide emissions of soil-borne insects, beneficial nematodes move toward their prey and enter the pest through its body openings. The nematodes carry an associated bacterium (Xenorhabdus species) that kills insects fast within 48 hours. The bacteria is harmless to humans and other organisms and cannot live freely in nature. Several generations of nematodes may live and breed within the dead insect, feeding on it as a food source. When the food source is gone, they migrate into the soil in search of a new host. When the pest population is eliminated, the beneficial nematodes die off and biodegrade. Beneficial nematodes are so effective, they can work in the soil to kill the immature stages of garden pests before they become adults.\nBeneficial nematodes infest grubs and other pest insects that are known to destroy lawns and plants.\nThe Nematodes are effective against grubs and the larval or grub stage of Japanese Beetles, Northern Masked Chafer, European Chafer, Rose Chafer, Fly larvae, Oriental Beetles, June Beetles, Flea beetles, Bill-bugs, Cut-worms, Army worms, Black Vine Weevils, Strawberry Root Weevils, Fungus Gnats, Sciarid larvae, Sod Web-worms, Girdler, Citrus Weevils, Maggots and other Dip-tera, Mole Crickets, Iris Borer, Root Maggot, Cabbage Root Maggot and Carrot Weevils.\nBeneficial nematodes belong to one of two genera: Steinernema and Heterorhabditis are commercially available in the U.S. Steinernema is the most widely studied beneficial nematode because it is easy to produce. Heterorhabditis is more difficult to produce but can be more effective against certain insects, such as th white grubs, and Japanese beetles.How beneficial nematodes work: The life cycle of beneficial nematodes consists of six distinct stages: an egg stage, four juvenile stages and the adult stage. The adult spends its life inside the host insect. The third juvenile stage, called a dauer, enters the bodies of insects (usually the soil dwelling larval form. Some nematodes seek out their hosts, while others wait for the insect to come to them. Host seeking nematodes travel through the soil the thin film of water that coats soil particles. They search for insect larvae using built-in homing mechanisms that respond to changes in carbon dioxide levels and temperature. They also follow trails of insect excrement. After a single nematode finds and enters an insect through its skin or natural openings, the nematode release a toxic bacteria that kills its host, usually within a day or two. In less than two weeks the nematodes pass through several generations of adults, which literally fill the insect cadaver. Steinernema reproduction requires at least two dauer nematodes to enter an insect, but a single Heterorhabditis can generate offspring on its own. The nematodes actively searches for insect larvae. Once inside the larva the nematodes excretes specific bacteria from its digestive trac before it starts to feed. The bacteria multiply very rapid and convert the host tissue into products that the nematodes take up and use for food. The larva dies within a few days and the color changes from white-beige to orange-red or red-brown. The nematodes multiply and develop within the dead insect. As soon as the nematodes are in the infectious third stage, they leave the old host and start searching for new larvae. Infected grubs turn color from white-beige to red brown 2-4 days after application and becomes slimy. After a few weeks, dead larvae disintegrate completely and are difficult to find.\nBeneficial nematodes are also very effective against termites, German cockroaches, flies, ant, and fleas.\nAPPLICATION: Beneficial Nematodes are very easy to use. Mix with water and spray or sprinkle on the soil along garden plants or lawn. Put the contents of the Beneficial nematodes in a bucket of water and stir to break up any lumps, and let the entire solution soak for a few minutes. Application can be made using a water-can, irrigation system, knapsack or sprayer. On sprayer use a maximum pressure to avoid blockage, all sieves should be removed. The sprayer nozzle opening should be at least 1/2 mm. Evenly spread the spraying solutions over the ground area to be treated. Continuous mixing should take place to prevent the nematodes from sinking to the bottom. After application keep the soil moist during the first two weeks for the nematodes to get establish. For a small garden the best method is using a simple sprinkling or water can to apply the Beneficial nematodes to the soil. Apply nematodes before setting out transplants; for other pest insects, Japanese Beetles and grubs, apply whenever symptomatic damage from insects is detected. Best to apply water first if soil is dry.Application and amount for 50 and 100 Mil. Nematodes. The 50 Mil. + nematodes are packed in an inert carrying material that will dissolve in water when mixed. You can use a watering can, pump sprayer; hose end sprayer and irrigation system, backpack sprayers, or motorized sprayer. The 50 and 100 Mil. Nematodes mix ½ teaspoon per gallon of water. The Large yard size: 1/2 Acre Size (50 Million) you can use up to 50 Gallons of water The Acre size 100 Mil. Nematodes you can use up to 100 Gallons of water. o For covering up to 800 square feet, place approximately 1½ teaspoons of dry nematodes in the hose end sprayer container, and then fill to the 3 gallon mark. (for Garden size) o Evenly spread the solution over the ground areas to be treated. o Continuous mixing should take place to prevent the nematodes from sinking to the bottom of the container. To avoid blockages, remove all filters from the sprayer. o You can sprinkle the soil with water again after application to move the nematodes into the soil. o Apply nematodes as soon as possible for best product performance. o Keep the soil most for the first week after application. Application for the 10 Mil. garden size Nematodes. The 10 Mil. nematodes are placed in a sponge. Place the entire sponge in a gallon of water, squeeze for a few minutes to get the nematodes out of the sponge and into the water. Discard the sponge and pour the gallon of water into the sprayer or water can and apply to the soil.Proper storage and handling is essential to nematode health. Always follow the package instructions for the best method of mixing nematodes. Formulations vary depending on the species and target insect. Nematodes can be stored in the refrigerator up to a month (not the freezer) before they are mixed with water, but once the nematodes are diluted in water, they cannot be stored. Also, nematodes shouldn’t be stored in hot vehicles, or left in spray tanks for long periods of time.\nNematodes need moisture in the soil for movement (if the soil is too dry or compact, they may not able to search out hosts) and high humidity if they are used against foliage pests. Watering the insect-infested area before and after applying nematodes keeps the soil moist and helps move them deeper into the soil. Care should be taken not to soak the area because nematodes in too much water cannot infect.Exposure to UV light or very high temperatures can kill nematodes. Apply nematodes in the early evening or late afternoon when soil temps are lower and UV incidence is lower as well (cloudy or rainy days are good too). Nematodes function best with soil temperatures between 48Fº and 93Fº day time temperatures.\nApplication is usually easy. In most cases, there is no need for special application equipment. Most nematodes species are compatible with pressurized, mist, electrostatic, fan and aerial sprayers! Hose-end sprayers, pump sprayers, and watering cans are effective applicators as well. Nematodes are even applied through irrigation systems on some crops. Check the label of the nematode species to use the best application method. Repeat applications if the insect is in the soil for a longer period of time. There is no need for masks or specialized safety equipment. Insect parasitic nematodes are safe for plants and animals (worms, birds, pets, children). Because they leave no residues, application can be made anytime before a harvest and there is no re-entry time after application.How to use beneficial nematodes: For the home gardener, localized spraying is probably the quickest and easiest way to get the nematodes into the soil. Producers ship beneficial nematodes in the form of dry granules, powder type clay, and sponges. All of these dissolve in water and release the millions of nematodes. Each nematode ready to start searching for an insect in your lawn or garden. Nematodes should be sprayed on infested areas at the time when pests is in the soil. Timing is important, or else you will have to repeat the application. Northern gardeners should apply the nematodes in the spring, summer and fall, when the soil contains insect larvae. Most of the beneficial nematodes are adaptive to cold weather. In fact , the very best time to control white grubs is in the spring and fall. If your in a warmer climate, beneficial nematodes are most effective in the summer.\nFertilizers should be avoided roughly 2 weeks prior to and after nematode application, because they may be adversely affected by high nitrogen content.\nSome pesticides work well with nematodes when their mutual exposure is limited while other pesticides may kill nematodes. Check labels or specific fact sheets to find out. Some chemicals to avoid are bendiocarb, chlorpyrifos, ethoprop, and isazophos. Fungicides to avoid are anilazine, dimethyl benzyl, ammonium chloride, fenarimol, and mercurous chloride. The herbicides, 2,4-D and trichlopyr and nematicide, fenamiphos, should be avoided as well.\nDuring hot weather release nematodes in the evening or afternoon when temperature is cooler. Release once or twice a year or until infestation subsides. Nematodes are shipped in the infectious larvae stage of their life cycle and can be stored in the refrigerator for up to 4 weeks. Always release very early in the morning or late in the late afternoon.\nWhy are these organisms beneficial?\nBeneficial nematodes seek out and kill all stages of harmful soil-dwelling insects. They can be used to control a broad range of soil-inhabiting insects and above-ground insects in their soil-inhabiting stage of life.\nParasitic nematodes are beneficial for eliminating pest insects. First, they have such a wide host range that they can be used successfully on numerous insect pests. The nematodes\' nonspecific development, which does not rely on specific host nutrients, allows them to infect a large number of insect species.\nNematodes enter pest bugs while they are still alive, then they multiply inside the bugs (which eventually die) and finally burst out of the dead bodies. The number of nematodes inside a single bug (depending on the species) ranges from 5,000 to 10,000. Although you can barely see one young nematode with your naked eye, large groups of these tiny wigglers pouring out of the dead insects are easy to see. Then the nematodes wriggle off to find other insects to ""invade,"" starting the whole cycle all over again.\nSecond, nematodes kill their insect hosts within 48 hours. As mentioned earlier, this is due to enzymes produced by the Xenorhabdus bacteria.\nAlso, the infective juveniles can live for some time without nourishment as they search for a host.\nFinally, there is no evidence that parasitic nematodes or their symbiotic bacteria can develop in vertebrates. This makes nematode use for insect pest control safe and environmentally friendly. The United States Environmental Protection Agency (EPA) has ruled that nematodes are exempt from registration because they occur naturally and require no genetic modification by man. Beneficial nematodes can be an excellent tool in the lawn and garden to control certain pest insects. They can be used with organic gardening and are safe for kids and pets.\nWhat is a nematode? Nematodes are microscopic, whitish to transparent, unsegmented worms. They occupy almost every conceivable habitat on earth, both aquatic and terrestrial, and are among the most common multicelled organisms. Nematodes are generally wormlike and cylindrical in shape, often tapering at the head and tail ends; they are sometimes called roundworms or eelworms. There are thousands of kinds of nematodes, each with their particular feeding behavior -- for example, bacterial feeders, plant feeders, animal parasites, and insect parasites, to name a few.\nInsect-Parasitic Nematodes. Traditionally, soil-inhabiting insect pests are managed by applying pesticides to the soil or by using cultural practices, for example, tillage and crop rotation. Biological control can be another important way to manage soil-inhabiting insect pests. A group of organisms that shows promise as biological control agents for soil pests are insect-parasitic nematodes. These organisms, which belong to the families Steinernematidae and Heterorhabditidae, have been studied extensively as biological control agents for soil-dwelling stages of insect pests. These nematodes occur naturally in soil and possess a durable, motile infective stage that can actively seek out and infect a broad range of insects, but they do not infect birds or mammals. Because of these attributes, as well as their ease of mass production and exemption from EPA registration, a number of commercial enterprises produce these nematodes as biological ""insecticides.""\nHow to order Beneficial Nematodes: All nematodes are not the same. Buglogical nematodes are more tolerant of high tempertures than any other brands. It is best to order biological control nematodes and have them delivered directly to you from a reliable source.. This helps insure that the nematodes you are buying are still alive. Nematodes do not live very long in storage. Therefore, buying nematodes that are stocked on a store shelf is very risky.\nSuppliers: Buglogical Control Systems, Inc. PO Box 32046, Tucson, AZ 85751-2046 Phone: 520-298-4400\nBedding, R.A. and L.A. Miller. 1981. Use of a Nematode, Heterorhabditis heliothidis to Control Black Vine Weevil, Otiorhynchus sulcatus, in Potted Plants.Ann.Appl.Biol. 99:211-216.\nDavidson, J.A., S.A. Gill, and M.J. Raupp. 1992. Controlling Clearwing Moths with Entomopathogenic Nematodes: The Dogwood Borer Case Study. J. of Arboriculture. 18(2):81-84.\nGeorgis, R. and G.O. Poinar. 1989. Field Effectiveness of Entomophilic Nematodes Neoaplectana andHeterorhabditis.Pages 213-224, In A.R. Leslie and R.L. Metcalf (eds.). Integrated Pest Management for Turfgrass and Ornamentals.United States Environmental Protection Agency, Washington, DC.\nGill, S., J.A. Davidson, and M.J. Raupp. 1992. Control of Peachtree Borer Using Entomopathogenic Nematodes. J. of Arboriculture.18(4):184-187\nKaya, H.K. 1985.Entomogenous Nematodes for Insect Control in IPM Systems. Pages 283-303, In M.A. Hoy and D.C. Herzog (eds.).Biological Control in Agricultural IPM Systems,New York: Academic Press.\nKaya, H.K. and L.R. Brown. 1986.Field Application of Entomogenous Nematodes for Biological Control of Clear-Wing Moth Borers in Alder and Sycamore Trees. J. of Arboriculture. 12(6):150-154.\nOwen, N.P., M.J.Raupp, C.S. Sadof, and B.C. Bull. 1991. Influence of Entomophagus Nematodes and Irrigation on Black Vine Weevil in Euonymus fortunei (Turcz.) Hard. Mazz.Beds.J.Environ.Hort.9(3):109-112.\nPoinar, G.O. 1986. Entomophagous Nematodes. Pages 95-121, In H.Franz(ed.).Biological Plant and Health Protection, Fortschritte der Zoologie, Bd.32.G.Fischer Verlog, Stuttgart, New York. Reprint.\nRutherford, T.A., D. Trotter, and J.M. Webster. 1987. The Potential of Heterorhabditid Nematodes as Control Agents of Root Weevils. The Canadian Ent. 119:67-73.\nShetlar, D.J. 1989. Entomogenous Nematodes for Control of Turfgrass Insects with Notes on Other Biological Control Agents.Pages 225-253, InA.R. Leslie and R.L. Metcalf (eds.) Integrated Pest Management for Turfgrasses and Ornamentals. United States Environmental Protection Agency, Washington, DC.']"	['<urn:uuid:f56c914e-733e-4c60-9e72-2084f1d5a39f>', '<urn:uuid:0ed9232d-2cef-49aa-97cd-48e2b47d3b63>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	45	175	3565
41	What are the benefits of growing mint in a balcony garden, and what potential pest problems should I watch out for with this herb?	Mint is a hardy and productive herb that thrives in balcony gardens with several benefits: it's tough and tenacious, produces abundant leaves even in small containers, doesn't require much sun, and only needs plenty of water and occasional fertilizer. However, like other garden plants, mint can be vulnerable to certain pests. Particularly concerning are aphids, which cluster on leaf undersides and new growth, sucking plant juices and causing leaves to become distorted and yellow. These pests reproduce rapidly and can attract ants, which protect them in exchange for honeydew secretion, leading to black sooty fungus growth on leaves.	['The Balcony Garden for Beginners\nThe high-rise veggie patch: the future of homegrown.\nTimes have changed for the Aussie city-dweller. The quarter acre block is a dream of decades ago. Apartment living is in.\nBut life without a backyard doesn’t mean a lifelong sentence of store-bought iceberg lettuce, droopy supermarket-shelf herbs and overpriced organics. It’s possible to create a thriving edible garden in the confines of the concrete jungle. All you need is a balcony – even a window ledge will do!\n1. Follow the Sun: Before you go and buy plants of any kind, you’ll need an idea of how much sunlight your balcony gets and at which time of day. You can then pick out plants that best suit your space – either full sun, partial sun or mostly shade. If animals such as birds, possums and cockroaches are making your home also theirs, tomato’s and fruits are best to avoid growing in this environment.\n2. Containers: Particularly if you’ll be hanging planters over balcony and window edges, bear in mind that ledges can only withstand a maximum weight-bearing load. Stick to lightweight plant pots and try recycling plastic containers from around the house. Whatever you choose, make sure they have holes in the bottom to allow adequate drainage. Glass jars and old tins are also ideal to use when propagating mint and basil at home. Simply cut a healthy and thick stem of the plant (around 15cm in length) and cut just below a node which is the bump in the stem where the leaves shoot from. Then, gently remove one-third to half of the leaves and place the stem in a clean jar or glass of water, out of direct sunlight. Finally, watch your new plant grow!\n3. Vertical Gardening: If you have a super-small balcony, building up a collection of potted plants can become a clutter-nightmare. That’s where the miracle of vertical garden designer can help! Nurseries like Flower Power now offer rather attractive wall-mountable systems. You could also recycle an old wooden pallet - just make sure it stays well secured.\nHerbs and Vegetables – What to Grow:\nThe right plants for your apartment veggie patch will depend on your local climate and the amount of space and sunlight at your disposal. Below are a few good all-rounders that will work for most balcony gardens.\nMint is a tough, tenacious herb as long as it’s given plenty of water and occasional fertiliser. It produces abundant leaves even in small containers and doesn’t mind modest amounts of sun.\nParsley is one of the easiest herbs to grow in containers. It’s not picky about soil type and does equally well in full sun and partial shade.\nOregano isn’t fussy about soil and will cope with being watered just a couple of times a week. It does, however, need a good amount of sunlight and needs to be picked correctly to stop the plant from going woody.\nGarlic is super easy to grow and even a small pot placed on a windowsill will quickly yield several large, juicy bulbs.\nSalad Leaves: Green salad leaves such as rocket, spinach, cress and oak lettuce thrive in planter boxes, do well in full or partial sun and can be cut back several times for multiple harvests.\nChilli Plants: They require no love yet yield 4 months a year. They are happy being dormant and always come back to life.\nTomatoes: Most tomato varieties do well in containers, provided they get plenty of sun. Trailing varieties are great for hanging pots. Consider adding a lavender plant in with your tomato plants to attract more bees to pollinate the fruits.\nEggplant: Eggplants will happily grow in larger containers but require a good amount of sun. They’re super productive too – a healthy plant will produce several harvests.\nSo there you have it. Your high-rise veggie patch is waiting to be tended to. Start with a few of the suggestions above and taste the difference!', 'Garden Pests Identification is essential because these pests can chew on plants and damage your garden. These are some of the common garden pests. After proving that gardening is good for health now its time to keep your garden healthy.\nSpider mites are common garden pests that thrive well in hot, dry, and dusty conditions. Residing on the undersides of the leaves of plants, these mites congregate in dense colonies in protective silk webs.\nThey cause damage to the plants by puncturing their cells to feed on them. This, in turn, shows up as a silvering or stippled effect on the leaf top. These garden pests feed on a number of species of plants including ornamental and houseplants as well.\nAlthough various garden insects are known as leaf miners, the most common are the larva of tiny flies that reside in and eat the leaf tissues, creating visible mines between its upper and lower layers.\nThey are identified as garden pests as they cause immense damage garden plants. They can be effectively taken care of by either planting trap crops or encouraging natural parasites.\nScales are sap-sucking garden pests that attach themselves with the host plants in order to feed. They secrete a waxy coating, which makes them look like fish scales. Usually, they appear as shell-like bumps, and before one can realize, they build up huge colonies and start destroying the plants.\nScales are serious pests in the garden, which can only be controlled using horticultural oils, as their waxy covering protects them well from contact insecticides.\nFound in moist and warm climates, mealybugs are unarmored scale pests in the garden that feed on the juices of plants and act as a vector of several plant diseases. While sucking plant juices, these garden pests secrete a powdery wax for their protection and require garden fungus identification. You should properly follow the indoor gardening tips to keep insects away.\nThey are a major menace as they infest and damage various types of plants including mango, grapes, sugarcane, mulberry, pineapple, ferns, sunflower, papaya and orchids. The tenacity of these garden insects increases in the presence of ants, as ants protect from predators and parasites.\nThrips are fringe-winged minute insects that feed on a large variety of plants. They are one of the most commonly found pests in the garden and are considered hazardous. These garden pests feed on them; they cause discoloration, deformities, and reduced marketability of the crops.\nThey are one of the prominent vectors of several plant diseases and are known for transmitting more than 20 plant-infecting viruses.\nAlso known as the plant lice, aphids are small, soft-bodied sap-sucking insects, which are one of the most destructive garden pests in temperate regions. They usually cluster on the tips of new growth and leaf undersides and starting sucking plant juices, which cause the leaves to become distorted and yellow.\nAphids share a unique relationship with ants, as the later farms and protects them in order to extract honeydew secreted by the former. This process is known as tending, and it causes black sooty fungus to grow on leaves. Aphids reproduce at a rapid rate, which makes them quite menacing. They also spread diseases among plants.\nWhiteflies are tiny garden insects that commonly feed on the undersides of plant leaves and stems. These menacing garden pests carry and spread garden diseases, thereby posing as a great agricultural threat globally. These pests are really dangerous specially for flower plants.\nThese flies feed by sucking into the phloem of plants and while doing so release a toxic saliva that decreases the plants’ overall turgor pressure. These hazardous pests in the garden are known for congregating in large numbers, which causes rapid degeneration in susceptible plants.\nControlling the outburst of whiteflies is a challenging problem, for these tiny garden pests have developed resistance towards chemical pesticides. Therefore, an integrated approach with biological control methods is required for achieving the same.\nSo, make sure that your garden is protected and identify garden pests.\nAlso Read: A Beginner’s Guide To Container Gardening']	['<urn:uuid:55d48d56-379b-4fb2-92d5-46b10945016f>', '<urn:uuid:a5ed2f26-1f7c-4d82-a6d8-a2f89bf82a9c>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	24	98	1338
42	What preparation and backfilling steps ensure safe underground construction?	Before construction, you must install landscape fabric in trenches, overlapping edges by 3-4 inches, and prepare the trench bottom with an inch of leveled, compacted sand. For backfilling, the material must be free from organic matter, rocks, concrete, and frozen material. Services should be placed back in original positions on a compacted AASHTO density backfill bed to prevent settlement. Use squeegee, well-graded sand, loam soil, or Class D bedding material. Don't encase utilities in wet concrete as it complicates future excavations. Additionally, all mobile and static loads must stay 1000mm away from excavation edges.	['Is a sudden slope in your yard preventing you from achieving the landscape you want? One way to even the score is by building a retaining wall. By replacing the slope and leveling the yard to either side of the wall, you can create two distinct tiers for split level form and function.\nLandscape blocks are some of the best things to happen to walls in a long time. Made of precast concrete, they have textured faces that make them look a lot more like stone than concrete. The blocks are durable and can easily be cut into smaller pieces to offset courses as you build your wall. They require neither footing nor mortar, and stacking them automatically creates a wall that leans into the hillside for strength.\nLandscape blocks come in two major varieties, lipped and pinned. The retaining wall in this project uses lipped blocks. For any wall four feet or higher, consult a professional rather than tackle it yourself. The Home Depot offers installation services for many products. And any time you plan a project that involves digging or reshaping your land, be sure to call 811 first to find out the location of utilities in your yard.\n- 1-2 days for a small length of wall\n- Construction adhesive\n- Drainpipe (optional)\n- Landscape blocks\n- Landscape fabric\n- Mason’s line\n- Sand (optional)\n- Beam level\n- Circular saw with a masonry blade\n- Rubber mallet\n- Torpedo level\n1. Dig and level a trench\nLay out the wall with batterboards and mason’s line. Dig a trench as wide and deep as the manufacturer recommends. Line the bottom with an inch or so of sand, rake it level, and compact it with a tamper. Place some of the soil you remove on the downhill side of the trench to level the slope. Line the back and bottom of the trench with landscape fabric. Unroll enough fabric to cross back over the trench later, plus about a foot more than the height of the wall. If you have to piece together lengths of landscape fabric, overlap any adjoining edges by 3 to 4 inches.\n2. Lay the first course\nSome manufacturers instruct you to set the first block lip up; others instruct lip down. Follow the directions. Lay the first block and check its level from side to side and front to back. Tap with a rubber mallet to make small adjustments. Put sand under the block to solve bigger problems. Lay the remaining blocks one at a time. Make sure each block is level, checking both side to side and front to back. Put the level across the new block and at least one of its neighbors to make sure the tops of the blocks are level with each other. The spaces at the ends of the course may not require a full block. Buy smaller blocks or trim larger ones to fit.\n3. Lay the second course\nIf the first course ended with a full block, begin the second course with a half block. If you started the first course with a partial block, trim a full block so it overlaps the seam below by at least 3 inches. (Check the manufacturer’s instructions.) Lay a second course of blocks with their lips pointing down, over the back of the wall. Position each block so it spans a joint in the course below. As you work, verify that the blocks are level; if not, shim the low end with asphalt shingle or some landscape fabric. As you come to the end of the wall, trim a block to fit in the opening. When you begin building the rest of the wall, alternate so one row begins with a full block and the next with a half block.\n4. Lay a third course and some drainpipe\nNot all block walls require a drainpipe, though it’s never a bad idea. Lay the third course the same way you laid the second course, leaving an exit point for the drainpipe at one end of the wall. Add a bit more gravel behind the wall to create a flat surface, and tamp the gravel. Tuck a drainpipe behind the wall. Fill in completely behind the wall to the top of the course and tamp. Continue building the wall, adding gravel and tamping after every course. When you’re a course or two below the top of the wall, fold the landscape fabric over the gravel and trim off any excess.\n5. Lay capstones\nThe keystone shape of the blocks leaves triangular gaps between the stones, which are covered up with special blocks called capstones. Apply a bead of construction adhesive along the top of the wall and set the capstones in place. Put topsoil in the space between the landscape fabric and the top of the wall and fill what remains of the trench in front of the wall. Rake to make a flat surface and tamp.\nGot questions about this article or any other garden topic? Go here now to post your gardening ideas, questions, kudos or complaints. We have gardening experts standing by to help you!', 'Is it Safe and Permissible to Dig a Hole in your Backyard without Consulting Local Authorities?Rules and Regulations for Excavations on your Private Property | Depths of Underground Utility Services\nSo you have a little project that you want to do on your private property, and this project involves trench excavations, but before you proceed with your project, can you legally dig a hole in your backyard without permission from local authorities and utility operators?\nBefore we look at rules and regulations regarding excavations and digging trenches in your backyard, we should look at reasons why you would want to dig a hole in your backyard.\nYou have been talking to a neighbour or friend who has an amazing garden, a beautiful landscaping feature like a water fountain, koi pond, stone/metal sculpture, pathway lamp posts, curb mailbox, tropical plant, palm trees or an outdoor structure like a pergola, gazebo, screen wall, boundary wall, a rustic outdoor shower cubicle, barbeque area or fireplace. Maybe it’s a septic tank, stormwater gulley, manhole or inspection chamber. All these outdoor home improvements, additions and works will require you to dig a hole, trench or excavation of some sort. The width and depth of the excavation will depend on what you are building or what you want to install. If you have building drawings of the structure, the draughtsman, engineer, architectural technician or landscape designer should show you the dimensions of the structure, i.e. the plan view, elevations and cross section details of the foundations that need to be dug. This information is required by local building authorities, so make sure you have the drawings and other information about your site at hand. Local building authorities need to know the depth and width of your excavations, as well as the site conditions before they give you a permit or permission to dig holes and pits on your property.\nPermit To Dig in Your Backyard\nRegardless of where you live in the United States, digging a hole in your home yard comes with risks that may cost you legally or financially. You do not just stand a chance to violate local ordinances and get fined, but digging holes arbitrarily on your private property may pose a hazard to your health and physical well being. Your neighbours as well as the whole community may be affected by your actions. Hazards of digging holes in your backyard without consulting local authorities and utility operators include damaging underground water, sewer and gas pipes. Damaged water and sewer pipes can cause flooding in the surrounding environment. Hitting buried gas pipes can cause fire outbreaks and air poisoning. Broken electricity cables and wires left exposed pose a risk of electrocution and fire outbreaks.\nAccording to CGA (Common Ground Alliance) and city council ordnances in various parts of the United States, you need to call 811 before you dig a hole on your property. There is no legal minimum depth that you can dig before calling 811. This is due to the fact that environmental conditions such as ground settlement, rainfall, floods, surface runoff, soil erosion and deposits can change the depth of topsoil, reducing or increasing the cover for underground services.\nHow to Contact 811 for Backyard Excavations\nTo call 811, simply visit the CGA site, choose your state and submit an online application for permission to dig holes on your property. Your state 811 call centre will create a ticket that allows you to follow up the progress on your application. Alternatively, you can pick up your phone and call 811 directly to speak with the call centre agents. The call centre agents will submit your request and notify you of utility operators who have responded to your application for excavating in your home yard. After submitting your request, you have to wait for responses from utility operators of underground services such as drainage pipes, gas pipelines, water supply lines, telecommunication and electricity cables. They provide feedback through your local 811 call centre, so you will only interface with the call centre.\nIt may take 2 to 3 days for utility operators to respond to your application, but the time varies with each state. Once your request is processed, the utility operators will send out their inspectors to come and mark the positions of underground services in your yard.\nRemember, you cannot start digging until all utility operators have responded to your request and marked the positions of all underground services. Do not remove the paint markings or any flags that are used to locate underground services. If you lose the markings in the course of your construction activities, you will be required to submit another application for re-marking.\nRules and Regulations for Excavations on your Private Property\nAfter you have obtained permission to dig holes inside your residential lot, there are a whole lot of rules and regulations that you have to abide with. Caution and care should be taken when digging near utility markings. OSHA and CGA provide health and safety guidelines for digging trenches, holes and excavations.\nDistance from Utility Line or Service / Tolerance Zone\nState building codes recommend digging 18 to 24 inches (457mm to 610mm) from the utility and its marking to avoid damage to the utility line. The homeowner and the contractor are prohibited from using mechanical equipment when digging within this perimeter, which is known as the tolerance zone.\nYou can only dig within the tolerance zone using hand tools and vacuum excavators which give you a high degree of control and precision. A vacuum excavator is the best option for “soft digging”. It breaks the earth and loosens soil particles by blowing air or water at high pressure. With this method, there is no risk of damage to underground pipes. On the other hand, hand tools can cause damage to buried lines if delicate and proper techniques are not used. While a hand tool will be useful for breaking hard surfaces such as concrete paving and slabs, you must verify that there are no service pipes or conduits embedded inside the concrete or too close to the slab.\nDig Alongside the Service Line, Not on Top\nAs a rule, you must not use a hand tool to dig on top of or over a buried service line. Dig alongside the utility line on both sides parallel to the line, and observe the tolerance zone stated above.\nScan for Underground Services Every 300mm of your Dig\nBefore digging alongside the buried utility line, you must use a CT Scanner to locate the depth of the service line as well as other possible uncharted services which might exist under the spot or working area. This device is also known as a Cable Locator or Cable Detector. However, you are not only going to use this device on your first dig, but you have to repeat the scanning procedure every 300mm (12 inches).\nThere are different types of scanners for detecting specific underground materials. Metal locators can only detect metal pipes and electric cables. They cannot detect non-metallic pipes such as plastic, PVC, clay, fibre cement (asbestos cement), HDPE and cement.\nTo ensure that non-metallic underground services are not missed in the cause of prospecting and scanning, you have to use a GPR device (Ground Penetrating Radar). A ground penetrating radar device can detect all types of buried lines, both metallic and non-metallic.\nHow Deep Can You Dig Without Calling 811?\nHow deep can you legally dig in your backyard? As has been said previously, there is no minimum or maximum legal depth of which you can dig holes in your backyard residential lot without calling 811 or consulting the local building authorities, meaning that you have to call 811 before digging any kind of hole. However, for practical purposes (leaving the legal requirements aside), it is generally safe to dig holes no more than 300mm deep (12 inches) on your property, and it’s much safer and non-risky to dig holes no more than 100mm deep (4 inches). It will be quite safe as long as you are digging within your residential lot, and not outside the lot, for example on street paving etc. Digging holes on your street curb and paving will require a permit in most cases, depending on your state laws.\nEarthwork Support for Deep Trenches\nEarthwork support to prevent the risk of collapse must be installed inside a trench with a minimum depth of 1.5m (5 feet). Timber shoring is a common method of preventing the walls of a trench from collapsing onto the worker. Trench boxes and sloping are other protective measures that can be used. You cannot work inside in a trench that is 1.5m or deeper if there is no earthwork support.\nPermit for Deeper Trenches\nA permit will be required for trenches that are 5 feet (1,520mm) or deeper. Workers may not enter or start working in such trenches unless they have obtained a permit from DOSH which satisfies the requirements for earthwork support and safe access in and out the trench.\nSafe Access and Exit from Excavation\nThe contractor or anybody responsible for digging pits and trenches on your residential lot must provide a safe means of access and exit for workers. This is required for trenches which are 4 feet (1,220mm) or deeper. Safe means of access and exit can be provided by making use of steps, ladders and ramps.\nSafe Load / Traffic Distance from Excavation\nAs a rule, all mobile and static loads should not encroach within 1000mm (3.3 feet) from the excavation or trench. No machinery, vehicles, equipment, materials or people should stand within 1000mm from the excavation. Material excavated from the trench should be stored outside the 1000mm perimeter on both sides of the excavation.\nProtection of Exposed Utility Services\nOnce underground services have been safely exposed, they should be protected from damage by people, machines, tools, vehicles and weather elements. Make sure that the services are supported as required, but you cannot remove them without a permit.\nProtection of People from Exposed and Hazardous Utility Services\nExposed services can be hazardous especially if they are not protected or if they are damaged by workers. In order to prevent hazards and provide a safe environment for both workers and the community, you may be required to disconnect the services temporarily. If your neighbours and the community are going to be affected by this, they should be notified in advance.\nDamaging a gas pipeline with your tools will cause hazardous fumes to escape and ignite fires. Damaged electrical cables pose a risk of electrocution and fire outbreaks. Hitting a water supply pipe or drainage pipe can cause flooding in the yard. State and city authorities impose fines for damage to utility services.\nSafe Backfilling of Trenches and Excavations\nWhen you are done with your work or outdoor project, you have to place and align the services back in their original position. Make sure that the backfill bed on which the pipe is lying is compacted to a suitable AASHTO density to avoid earth settlement. The trench backfilling must be free from organic material, rocks, concrete, bricks, stones, frozen material and other hardcore material. Do not encase, cover up, bury or embed the service pipes in wet concrete because it will be difficult and hazardous to break up the hardened concrete during a utility dig-up.\nBackfill the exposed utility trenches with squeegee or well-graded sand, loam soil, silt sand or any Class D bedding from excavated material.\nDepths of Underground Utility Service Pipes and Cables\nIt’s important to know the depth at which various underground utility services are buried.\nElectricity Conduits / Cables:\nThe minimum depth of underground electricity cables is 450mm (18 inches), and they can be as deep as 1000mm (40 inches) or more depending on the voltage they are carrying. The higher the voltage, the deeper the cables should be, but the depth may vary depending on the method of wiring, type of wire sheathing (conduit) and zoning/land use requirements in a state.\nUnderground gas pipelines are buried at various depths, depending on their location, land use, zoning, as well as the type of pressure they are carrying. Gas pipes on private residential lots are buried at a much lower (shallow) depth than those laid under or near roads, highways and pedestrian walks. On private residential lots and building entrances, gas pipes are laid 375mm (15 inches) under the ground. On pedestrian sidewalks, they are found 600mm (24 inches) under the ground, and on roads and highways, they will be buried much deeper at 750mm (30 inches).\nHigh-pressure gas pipelines are buried much deeper than low-pressure pipes due to the significant damage that they can cause in enclosed and confined spaces in the event of a rupture. The minimum depth of underground high-pressure gas pipes is 900mm (35 inches).\nWater Supply and Drainage Pipes:\nSite water reticulation pipes and waste/rainwater drainage pipes must be laid deep enough to withstand freezing caused by temperature drops at night, in the morning and winter season. As a result, main water pipes, drains and sewerage pipes should be buried 4 to 5 feet under the ground, but a minimum depth of 900mm (3 feet) is the standard. On a private residential lot, the minimum depth can be reduced to 750mm (2.5 feet).\nWater and drainage pipes can be installed at a much shallower depth in regions with warm, sunny and hot climates where the depth of soil freezing is low (closer to the surface). In cold climates, the depth of soil freezing is very high, which means that frost will penetrate deeper into the ground.\nMajor Hazard Pipelines:\nNatural gas pipelines and other pipelines which carry oil, highly explosive gas and liquids are known as major hazard pipelines due to their extremely high risk of causing explosions and fires when the escaping gas is ignited. As a result, they are buried deeper than 900mm (3 feet). Under trafficked areas such as highways and railway lines, the pipes are laid even deeper. The minimum cover for pipes under railway crossings is 5.5 feet (168 metres) for cased pipes, and 10 feet (305 metres) for uncased pipes.\nTelecom cables running under the road and highway are installed 600mm (2 feet) deep under the surface of the ground.\nInternet Optic Fibre Cables:\nUnderground internet cables are buried in a trench with a minimum cover of 450mm (1.5 feet). A much lower minimum cover of 300mm (1 foot) is required when the cables are running under a concrete path or driveway.\nIn cold and freezing climates, the cables are laid deeper, at a minimum depth of 762mm (30 inches) and up to 1067mm (42 inches). The depth is also much higher in conditions where the fibre optic cables are running under roads, highways and railway tracks.']	['<urn:uuid:63d57dc7-4a85-4e4c-bfdf-7a423e113bfc>', '<urn:uuid:cba7d25c-9b83-4aef-9ef8-4ebeb522bf35>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	9	94	3327
43	medical scan tools accuracy compare xray versus ultrasound lung problems fluid air detection	X-rays and ultrasound have different levels of accuracy in detecting lung problems. For pleural effusion (fluid), X-rays show shadows/density changes where fluid accumulates, though these shadows may persist longer than the actual fluid due to remaining proteins. Regarding pneumothorax (air), ultrasound has proven superior to X-rays, especially in trauma cases. Studies demonstrate that ultrasound has a sensitivity of 92%-100% in detecting pneumothorax, while supine chest X-rays only have 36%-75% sensitivity. Ultrasound can detect specific signs like absence of lung sliding and comet tail artifacts, making it particularly effective for anterior pneumothoraces. However, ultrasound requires proper technique, including examination of multiple chest areas and correct probe placement between ribs.	"['Doctor insights on:\nWhat Are The Key Symptoms Of Pleural Effusion\nShortness of breath: The key symptom of a pleural effusion is shortness of breath ... Since fluid is denser than air, and since the chest cavity is limited by the rib cage (cannot not expand much), then accummulation of pleural fluid (or effusion) will cause the lungs to be compressed ... As more and more pleural effusion accummulates, then more lung is compressed, and shortness of breath gets worse ... ...Read moreSee 1 more doctor answer\nFluid around lungs has many causes. It can be exudate(thick i.E pus from infection, malignancy etc) or transudative(heart failure). It may be treated based on the problem found by sampling(thoracentesis). Labs on the fluid help the clinician determine the etiology. For recurring pleural fluid, sometime pleuradesis is necessary to hep prevent recurrance. Need ...Read more\nDoctor told me that symptoms of pleural effusion wont go away in X RAY even after treatment is it possible?\nIt depends: So pleural effusion is seen on a chest x. ray, and after treatment (ie drainage), there should be a difference, however, if a cxr is taken day/ month later the effusion may have recurred. I do not have enough room to go into any further details however if a pleural effusion is seen on a chest x ray and it is large enough that it can be tapped, then it should be tapped for diagnosis & treatment ...Read more\nDoctor told me that symptoms of Pleural effusion won\'t go away in X RAY even after treatment is it possible?\nX-Ray does not show: symptoms, only density/ability to absorb X-Rays so they do not reach the film. Basically standard X-Ray images are just shadows, ""shadograms"" if you will, using higher frequency (above visible light) electromagnetic radiation. The shadows, on X-Rays, of pleural effusions/lung densities, etc. may linger longer than the effusion itself due proteins which take longer for body to remove than water. ...Read moreSee 1 more doctor answer\nShortness of breath: It depends on what is causing it and how large it is. The more fluid the more it compresses and collapses the lung. The collapsed lung can not breathe for you. Effusions multifactorial, inflammation, heart failure, pneumonitis, cancer, post obstructive pneumonia, heart failure , kidney failure, hypoalbuminemia, granulomatosis etc. Shortness of breathatrestorexcertioninability to breathe laying flat. ...Read moreSee 1 more doctor answer\nPleural effusion: 31 F asks: Is a pleural effusion always dangerous? Does it need to always be drained. What are the symptoms? ANS: yes, no & SX can be none to pain when take deep breath, SOB or to coughing up blood but depends on cause. So ask your Drs what they think is cause. Once this is known then advice may range from no worries! to do not begin any long books! Dr. humor here! But ask them they know u best. ...Read more\nAsbestos - how long does if it hapensfor pleusry or diffuse pleural effusion symptoms to appear after exposure. I was xposed 3mo ago. Could I have it.\nLikely to take years: Asbestos is a mineral that was widely used in the construction, automotive, and other industries. Currently it is illegal to use. Factors involved in how asbestos exposure affects an individual include the amount and length of time a person was exposed to asbestos, as well as underlying health. In your case, a single limited exposure is not likely to cause early or high risk of late lung problems. ...Read more\nSymptoms or signs?: Symptoms refers to what the PATIENT is feeling; signs are what is seen by doctors. An X-ray can still show the lingering signs of a pleural effusion, especially if the pleural effusion is caused by ""trapped"" lung, cancer, or a severe infection. So, the ""solution"" depends on the CAUSE of the pleural effusion & what was done to treat it. Talk to ur doctor / pulmonologist for advice. Diagnosis is key ...Read more\nHI Doctor, i had pleural effusion almost a year ago, Doctor diagnosed it and treated me for 6 months. Bt its symptoms r still on x ray.\n- Talk to a doctor live online for free\n- Malignant pleural effusion symptoms\n- What is pleural effusion caused by?\n- What is a pleural effusion?\n- Ask a doctor a question free online\n- Pleural effusion\n- What does it mean to have a small pleural effusion?\n- What are the key symptoms of dermatosclerosis?\n- What are the key symptoms of ant bite?\n- Talk to a pulmonologist online for free', 'Focus On: Ultrasound Detection of Traumatic Anterior Pneumothorax\nBy Arun Nagdev, MD, and Michael Murphy, MD\nAfter reading this article, the physician should be able to:\n- Recognize the primary ultrasonographic finding of normal lung tissue (lung sliding).\n- Recognize the ultrasonographic findings present in a pneumothorax (absence of lung sliding, absence of comet tails).\n- Perform an ultrasonographic exam of the chest for the detection of a pneumothorax.\nEarly detection of a pneumothorax may be important in the clinical management of a trauma patient. Supine chest radiographs in the immobilized trauma patient have been shown to be insensitive.1 As bedside ultrasound is more frequently utilized by emergency physicians, detection of pneumothoraces at the bedside by ultrasound has become a valuable aspect of the initial assessment of the trauma patient.\nAdded to the FAST (Focused Assessment with Sonography in Trauma), rapid evaluation of the chest for the detection of a pneumothorax can offer the emergency physician pertinent clinical information to aid in the management of the trauma patient with undifferentiated injuries.2\nInitial clinical research in ultrasonographic evaluation of the chest in the critically ill patient originated in European intensive care units. These studies demonstrated the superiority of bedside ultrasound in the detection of anterior pneumothoraces, compared with supine chest radiographs (CXR).1,3,4 Although clearly demonstrating increased sensitivity, these initial studies were performed on nontrauma patients.\nRecent data from acutely injured patients support the initial studies by demonstrating that bedside ultrasound is more sensitive than chest radiography in the detection of an anterior pneumothorax when computed tomography (CT) is used as the gold standard. In summary, these studies demonstrate that ultrasound has a sensitivity of 92%-100% in detecting pneumothorax, compared with the supine CXR, which has been shown to have a sensitivity of 36%-75%.5,6\nThis article will focus on the ultrasound detection of pneumothorax mainly in the setting of trauma. The ultrasound examination techniques described may certainly be extended to use in the diagnosis of pneumothorax in the nontrauma patient.\nIn the normal state, the closed pleural system of the lung is made up of physiologic pleural fluid contained between the visceral and parietal pleura making up the pleural sac. Air introduced into this closed system leads to a pneumothorax by separating the normally apposed visceral and parietal pleura. This separation can potentially alter the mechanisms of respiration.\nIn the supine trauma patient without previous pleural disease, pathologic air within the pleural space tends to rise to the anterior chest wall at the paracardiac regions and anterior costo-diaphragmatic sulci. It is at these anterior sites that bedside ultrasound is ideal (see image 1).\nThe low-frequency, curvilinear probe (2-5 MHz) has been used by most authors for the detection of anterior pneumothoraces (see image 2, item A). This probe is found on most bedside ultrasound machines and is recommended for use during FAST examinations.\nThe curvilinear probe allows for the continuation of the FAST exam to the thoracic cavity, negating the need for a probe change during the exam. In our experience, using the high-frequency, linear probe (5-10 MHz) allows the novice physician to obtain rapid proficiency in the chest ultrasound exam for detection of pneumothorax (see image 2, item B).\nAfter sufficient experience with pattern recognition of positive and negative ultrasonographic signs of a pneumothorax with the linear, high-frequency probe, the clinician may then utilize the curvilinear, low-frequency probe for the complete evaluation of the abdomen and thorax--thus adding the lung component to the standard FAST exam, commonly termed the Extended FAST or EFAST.2\nThe bedside chest ultrasound exam for the detection of anterior pneumothoraces is performed by placing the probe perpendicular to two ribs spaces in the anterior chest region (see image 2, item C). The hyperechoic (white) ribs, with posterior shadows, act as fixed anatomical landmarks (see image 2, item D). This defined interspace, just deep to the intercostal muscles and soft tissue, is the location of the pleural line.\nSliding of the pleural line indicates the lack of air between the visceral and parietal pleural, and ""rules out"" an anterior pneumothorax. When pathologic air accumulates in between the parietal and visceral pleura, ultrasound waves are able to image the superficial parietal layer, but cannot visualize the visceral layer. This is because of the small collection of air in the pleural space that causes ultrasound waves to scatter. This is represented as ""lack of lung sliding"" and can be interpreted as a pneumothorax.\nOnce the pleural space is interrogated adequately, we recommend utilizing M-mode as an adjunctive imaging modality to confirm the presence or absence of a pneumothorax. Motion mode, or ""M-mode,"" allows the clinician sonographically to evaluate a single line and determine the presence of motion along that singular line.\nAgain, by leaving the probe in same perpendicular orientation over the rib interspaces, the operator can determine, in a static method, the presence of lung sliding. Lack of motion in the soft tissue region is displayed as horizontal lines, while motion is displayed as unorganized ""static"" (see image 3).\nCurrently, no consensus exists regarding the number of interspaces that need to be interrogated to exclude an anterior pneumothorax. In a supine patient, we recommend scanning 2-3 interspaces in the midclavicular line in the most anterior region of the chest (usually around the nipple line/4th-5th rib space). Also, we recommend evaluating both hemithoraces to differentiate normal from abnormal lung tissue if a pneumothorax is present.\nSecondary Signs of an Anterior Pneumothorax\nIn ""B-mode,"" or ""brightness mode,"" another sign termed ""comet tail"" artifact can aid the physician in the ultrasound examination of the chest for the detection of a pneumothorax.\nComet tail artifacts are ""ray-like"" projections off the pleural line thought to be created when ultrasound waves hit the interface between the apposing pleural and visceral layers of the lung (see image 4). These artifacts are not seen when pathologic air is present between the normally apposed pleural layers. Lack of comet tail artifacts can indicate the presence of a pneumothorax.\nThe point where normal lung interfaces with air of a pneumothorax in the pleural space is defined as the ""lung point."" In ""B-mode,"" the physician can scan the anterior chest in search of the dynamic transition between normal pleural sliding and absence of pleural sliding (pneumothorax). This dynamic point moves during the respiratory cycle, and can be found at various locations in the chest. Small pneumothoraces tend to have transitions in the anterior chest, while larger pneumothoraces have areas of transition on the lateral aspect of the chest.\nVisualization of the lung point is pathognomonic for the presence of a pneumothorax. Experience is necessary to recognize this specific sign, and it may be necessary to interrogate numerous interspaces to locate the lung point. This limits the recognition of lung point during a rapid EFAST examination.\nLack of lung sliding and comet tail artifacts may not always indicate a pneumothorax. Recently intubated patients may have a mainstem bronchus intubation preventing adequate aeration of one lung and not demonstrate either lung sliding or comet tail artifacts, giving the operator a false impression of pneumothorax.\nAlso, when evaluating the paracardiac regions on the left chest, care must be taken to identify the pleural line. The heart rises and falls with the movement of the diaphragm, and this motion may be misinterpreted as a ""lung point,"" especially if the probe marker is pointed caudad instead of cephalad (as recommended).\nIn stable trauma patients with ultrasonographic signs of a pneumothorax and a negative supine chest radiograph, we recommend a repeat upright plain film after clearance of cervical immobilization for confirmation of an occult pneumothorax. In patients where cervical immobilization cannot be removed, we recommend computed tomography of the chest to delineate the pneumothorax early in trauma care (before transportation or intubation).\nContinuation of the FAST exam to include the chest for the rapid determination of a pneumothorax can be useful for the emergency physician.\nThe provider must interrogate the most anterior aspect of the chest wall. The operator must identify pertinent landmarks (rib with resultant shadows) and determine the location of the pleural line. Lack of pleural movement can indicate air between the parietal and visceral pleura, allowing for rapid identification of a pneumothorax. This technique has been shown to be more sensitive for the recognition of anterior pneumothoraces in trauma patients, compared with supine chest radiographs.\nFurther management should be based on the clinical situation.\n- Lichtenstein D.A., Meziere G., Lascola N., et al. Ultrasound diagnosis of occult pneumothorax. Critical Care Med. 2005;33:1231-8.\n- Kirkpatrick A.W., Sirois M., Laupland K.B., et al: Hand-held thoracic sonography for detecting post-traumatic pneumothoraces: The extended focused assessment with sonography for trauma (EFAST). J. Trauma 2004;57:288-95.\n- Lichtenstein D.A., Menu Y.: A bedside ultrasound sign ruling out pneumothorax in the critically ill: Lung Sliding. Chest 1995;108:1345-8.\n- Lichtenstein D., Mezierre G., Biderman P., et al. The comet-tail artifact: An ultrasound sign ruling out pneumothorax. Intensive Care Med. 1999;25:383-8.\n- Blavais M., Lyon M., Duggard S.: A prospective comparison of supine chest radiography and bedside ultrasound for the diagnosis of traumatic pneumothorax. Acad. Emerg. Med. 2005;12:844-9.\n- Soldati D., Testa A., Sher S., et al. Occult traumatic pneumothorax: diagnostic accuracy of lung ultrasonography in the emergency department. Chest 2008;133:204-11.\nDr. Arun Nagdev is an assistant professor, department of emergency medicine, at the Warren Alpert School of Medicine of Brown University. Dr. Michael Murphy is an attending physician, department of emergency medicine, at the Warren Alpert School of Medicine of Brown University. Medical Editor Dr. Robert C. Solomon is an attending emergency physician at Trinity Health System in Steubenville, Ohio, and clinical assistant professor of emergency medicine at the West Virginia School of Osteopathic Medicine.\nIn accordance with the Accreditation Council for Continuing Medical Education (ACCME) Standards and American College of Emergency Physicians policy, contributors and editors must disclose to the program audience the existence of significant financial interests in or relationships with manufacturers of commercial products that might have a direct interest in the subject matter.\nDr. Nagdev, Dr. Murphy, and Dr. Solomon have disclosed that they have no significant relationships with or financial interests in any commercial companies that pertain to this educational activity.\n""Focus On: Ultrasound Detection of Traumatic Anterior Pneumothorax"" has been planned and implemented in accordance with the Essential Areas and Policies of the Accreditation Council for Continuing Medical Education (ACCME).\nACEP is accredited by the ACCME to provide continuing medical education for physicians. ACEP designates this educational activity for a maximum of one Category 1 credit toward the AMA Physician\'s Recognition Award. Each physician should claim only those credits that he or she actually spent in the educational activity.\n""Focus On: Ultrasound Detection of Traumatic Anterior Pneumothorax"" is approved by ACEP for one ACEP Category 1 credit.\nACEP makes every effort to ensure that contributors to College-sponsored programs are knowledgeable authorities in their fields. Participants are nevertheless advised that the statements and opinions expressed in this article are provided as guidelines and should not be construed as College policy.\nThe material contained herein is not intended to establish policy, procedure, or a standard of care. The views expressed in this article are those of the contributors and not necessarily the opinion or recommendation of ACEP. The College disclaims any liability or responsibility for the consequences of any actions taken in reliance on those statements or opinions.']"	['<urn:uuid:21b55b5c-e31c-4174-bfcd-5a148df70ea2>', '<urn:uuid:9a60e928-d1cc-457d-9159-1b33f3172925>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	13	108	2641
44	how to prevent carrot fly pest naturally when growing carrots	There are several methods to prevent carrot fly pest: 1) Use very fine insect netting to keep the flies away, 2) Plant carrots in a raised bed since carrot flies only fly low to the ground, 3) Plant spring onions around the edges of your carrot plot as they help deter the pests.	"[""Growing carrots from seed\nBefore we go any further though, please look at the photos which both show carrots being grown well above ground. This is to deter the carrot fly pest which only flies at just a few inches above ground.\n1) Carrots grow best in soil which has not been recently composted, or if compost has been used it should be very well rotted otherwise the end result will look like something which has been spawned from a nuclear reactor site with lots of roots forking from the main one.\n2) Carrots are grown best in a light sandy soil, so if your soil is heavy or clay based then dig in plenty of fine builder's sand, the light coloured sand is best we find. All Purpose compose which you can buy from any garden centre is perfectly OK to use, either on it's own or mixed with soil and sand. Sharp sand is excellent too.\n3) You will find that some of your crop will show above ground and that part may turn green. Greening of the top of the carrot is caused by sunlight. Heavy rain can wash away the soil from carrot tops exposing them to the sun. The green colour is the chlorophyll pigment. Mound up the soil around the shoulders of the carrots to prevent exposure to the sun.\nSowing carrot seeds:\nA typical packet of carrot seeds contains in the region of 2000 seeds and these can be sown in your garden in one of several ways.\n1) By broadcasting: basically this means you get a handful of seed and scatter them onto your already prepared and pre-watered patch of soil. Cover them with a light coating of new soil and leave them to grow. This method means that you will have a great many shoots appearing very close together and they need to be thinned out to leave the strongest growing on to become part of your salad.\n2) The more time consuming method is to dib 1 CM deep holes in the soil and drop in an individual seed. Cover and leave to grow as normal. You will still have to do some thinning out because it is a virtual impossibility to drop just 1 seed into every hole. But at least with this method most of your seeds will grow at predetermined distances away from each other.\nWell, all the above should get you a good crop but there are other little things such as carrot fly which can ruin a whole crop, but we tackle that problem on one of our pages in the pests/diseases section. However, if you follow the tips below you should avoid the problem\n1) Carrot flies fly close to the ground and lay their eggs near to the top of you carrots. To overcome this you need to buy some very fine insect netting which will keep them away.\n2) Make a raised bed as high as you can and plant your seeds in there. As the pests only fly low they will not be able to infect your crop.\n3) When you plant your carrot seeds then also plant plenty of spring onion seeds around the edges of your plot and these will help to deter the pests.""]"	['<urn:uuid:e5790b0d-9115-4cd8-8efb-0995b2855b32>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	10	53	546
45	Can government officials avoid punishment for international crimes?	No, government officials cannot avoid punishment for international crimes. The Nuremberg Principles clearly establish that being a Head of State or responsible government official does not relieve someone from responsibility under international law. Additionally, unlike regular crimes which have statutes of limitations, international crimes like war crimes and crimes against humanity have no time limit for prosecution, so officials cannot escape justice by waiting out a limitations period. Even following orders from superiors is not a valid defense, unless the person truly had no moral choice in their actions.	"['Statute of limitations\nStatutes of limitations are laws passed by legislative bodies in common law systems to set the maximum time after an event within which legal proceedings may be initiated. When the period of time specified in a statute of limitations passes, a claim might no longer be filed, or, if filed, may be liable to be struck out if the defence to that claim is, or includes, that it is statute barred as having been filed after the limitations period.\nThe intention of these laws is to facilitate resolution within a ""reasonable"" length of time. What period of time is considered ""reasonable"" varies from country to country, and within countries such as the United States from state to state, and within countries and states from civil or criminal action to civil or criminal action. Some countries, and some crimes (depending on the country), have no statute of limitations whatsoever. In civil law (legal system) systems, similar provisions are typically part of their civil or criminal codes and known collectively as periods of prescription. The cause of action dictates the statute of limitations, which can be reduced (or extended) to ensure a fair trial.\nWhen a statute of limitations expires in a criminal case, the courts no longer have jurisdiction. Analysis of a statute of limitations includes the examination of any associated statute of repose, tolling provisions, and exclusions.\nCommon law legal systems can include a statute specifying the length of time within which a claimant or prosecutor must file a case. In some civil jurisdictions (e.g., California), a case cannot begin after the period specified, and courts have no jurisdiction over cases filed after the statute of limitations has expired. In some other jurisdictions (e.g., New South Wales, Australia), a claim can be filed which may prove to have been brought outside the limitations period, but the court will retain jurisdiction in order to determine that issue, and the onus is on the defendant to plead it as part of their defence, or else the claim will not be statute barred.\nOnce filed, cases do not need to be resolved within the period specified in the statute of limitations.\nThe purpose and effect of statutes of limitations are to protect defendants. There are three reasons for their existence:\n- A plaintiff with a valid cause of action should pursue it with reasonable diligence.\n- A defendant might have lost evidence to disprove a stale claim.\n- A long-dormant claim has more cruelty than justice (Halsbury\'s Laws of England, 4th edition).\nThe limitation period generally begins when the plaintiff’s cause of action accrues, or they become aware of a previous injury (for example, occupational lung diseases such as asbestosis). In Classical Athens, a five-year statute of limitations was established for all cases except homicide and the prosecution of non-constitutional laws (which had no limitation). Demosthenes wrote that these statutes of limitations were adopted to control ""sycophants"" (professional accusers).\nStatute of repose\nA statute of limitations is a type of statute of repose which may be extended for a variety of reasons (such as the minority of the victim). Other statutes of repose limit the time within which an action may be brought based upon when a particular event occurred (such as the completion of construction of a building or the purchase of manufactured goods), and often do not permit extensions.\nFor example, if a person receives an electric shock due to a wiring defect which occurred during construction of a building, the builder is liable for damages if the suit is brought within a certain number of years after construction was completed. After that, any injury is considered due to the natural degradation of the structure or a lack of proper maintenance rather than the builder\'s actions.\nStatutes of repose are sometimes controversial; manufacturers contend that they are necessary to avoid unfair litigation and encourage consumers to maintain their property. Alternatively, consumer advocates argue that they reduce incentives to manufacture durable products and disproportionately affect the poor, because manufacturers will have less incentive to ensure low-cost or ""bargain"" products are manufactured to exacting safety standards.\nTolling and the discovery rule\nMany jurisdictions suspend, or toll, the limitation period under certain circumstances—for example, if the aggrieved party (plaintiff) is a minor or has filed a bankruptcy proceeding. In those instances, the running of limitations is tolled (paused) until the condition ends. Equitable tolling may also be applied if an individual may intimidate a plaintiff into not reporting or has been promised a suspended period.\nThe statute of limitations may begin when the harmful event (such as fraud or injury) occurs or when it is discovered. The Supreme Court of the United States has described the ""standard rule"" of when the time begins as ""when the plaintiff has a complete and present cause of action"", a rule in existence since the 1830s. A ""discovery rule"" applies in other cases (including medical malpractice), or a similar effect may be applied through tolling. As discussed in Wolk v. Olson, the discovery rule does not apply to mass media such as newspapers and the Internet; the statute of limitations begins to run at the date of publication. In 2013 the Supreme Court of the United States ruled unanimously in Gabelli v. SEC that the discovery rule does not apply to U.S. Securities and Exchange Commission\'s investment-advisor-fraud lawsuits, since one purpose of the agency is to root out fraud.\nIn private civil matters, the limitations period may generally be shortened or lengthened by agreement of the parties. Under the Uniform Commercial Code, the parties to a contract for sale of goods may reduce the limitations period to one year but not extend it.\nLimitation periods known as laches may apply in situations of equity; a judge will not issue an injunction if the requesting party waited too long to ask for it. Such periods are subject to broad judicial discretion.\nFor U.S. military cases, the Uniform Code of Military Justice (UCMJ) states that all charges except those facing court-martial on a capital charge have a five-year statute of limitations. In all UCMJ proceedings except those headed for general court-martial, if the charges are dropped there is a six-month window in which they can be reinstated. If six months have passed without reinstatement, the statute of limitations has run out.\nIn civil law countries, almost all lawsuits must be brought within a legally-determined period known as prescription. Under Italian and Romanian law, criminal trials must be ended within a time limit.\nIn criminal cases, the public prosecutor must lay charges within a time limit which varies by jurisdiction and varies based on the nature of the charge; in many jurisdictions, there is no statute of limitations for murder. Over the last decade of the 20th century, many United States jurisdictions significantly lengthened the statute of limitations for sex offenses, particularly against children, as a response to research and popular belief that a variety of causes can delay the recognition and reporting of crimes of this nature.\nCommon triggers for suspending the prescription include a defendant\'s fugitive status or the commission of a new crime. A criminal may be convicted in absentia. Prescription should not be confused with the need to prosecute within ""a reasonable delay"" as obligated by the European Court of Human Rights.\nUnder international law, genocide, crimes against humanity and war crimes are usually not subject to the statute of limitations as codified in a number of multilateral treaties. States ratifying the Convention on the Non-Applicability of Statutory Limitations to War Crimes and Crimes Against Humanity agree to disallow limitations claims for these crimes. In Article 29 of the Rome Statute of the International Criminal Court, genocide, crimes against humanity and war crimes ""shall not be subject to any statute of limitations"".\nFraud upon the court\nWhen an officer of the court is found to have fraudulently presented facts to impair the court\'s impartial performance of its legal task, the act (known as fraud upon the court) is not subject to a statute of limitation. Officers of the court include lawyers, judges, referees, legal guardians, parenting-time expeditors, mediators, evaluators, administrators, special appointees and any others whose influence is part of the judicial mechanism. Fraud upon the court has been defined by the 7th Circuit Court of Appeals to ""embrace that species of fraud which does, or attempts to, defile the court itself, or is a fraud perpetrated by officers of the court so that the judicial machinery cannot perform in the usual manner its impartial task of adjudging cases that are presented for adjudication"". In Bulloch v. United States, the 10th Circuit Court of Appeals ruled: ""Fraud upon the court is fraud which is directed to the judicial machinery itself and is not fraud between the parties or fraudulent documents, false statements or perjury ... It is where the court or a member is corrupted or influenced or influence is attempted or where the judge has not performed his judicial function—thus where the impartial functions of the court have been directly corrupted.""\nCrimes considered heinous by society have no statute of limitations. Although there is usually no statute of limitations for murder (particularly first-degree murder), judges have been known to dismiss murder charges in cold cases if they feel the delay violates the defendant\'s right to a speedy trial.\nIn tort law, if a defendant commits a series of illegal acts against another person (or in criminal law if someone commits a continuing crime) the limitation period may begin to run from the last act in the series. In the 8th Circuit case of Treanor v. MCI Telecommunications, Inc., the court explained that the continuing-violations doctrine ""tolls [freezes] the statute of limitations in situations where a continuing pattern forms due to [illegal] acts occurring over a period of time, as long as at least one incident ... occurred within the limitations period."" Whether the continuing-violations doctrine applies to a particular violation is subject to judicial discretion; it was ruled to apply to copyright infringement in Taylor v. Meirick (712 F.2d 1112, 1119; 7th Cir. 1983) but not in Stone v. Williams (970 F.2d 1043, 1049–50; 2d Cir. 1992).\nFor crimes other than summary conviction offences, there is no statute of limitations in Canadian criminal law. For indictable (serious) offences such as major theft, murder, kidnapping or sexual assault, a defendant may be charged at any future date; in some cases, warrants have remained outstanding for more than 20 years. The sole exception is a charge of treason, which must be brought within 3 years.\nCivil law limitations vary by province, with Ontario introducing the Limitations Act, 2002 on January 1, 2004.\nThe Limitations Act of 1958 allows 12 years for child survivors and the disabled to make a claim, with age 37 the latest at which a claim can be made. The police submitted evidence to a commission, the Victorian Inquiry into Church and Institutional Child Abuse (in existence since 2012) indicating that it takes an average of 24 years for a survivor of child sexual abuse to go to the police. According to Attorney General Robert Clark, the government will remove statutes of limitations on criminal child abuse; survivors of violent crime should be given additional time, as adults, to deal with the legal system. Offenders of minors and the disabled have used the statute of limitations to avoid detection and prosecution, moving from state to state and country to country; an example presented to the Victorian Inquiry was the Christian Brothers.\nAn argument for abolishing statutes of limitations for civil claims by minors and people under guardianship is ensuring that abuse of vulnerable people would be acknowledged by lawyers, police, organisations and governments, with enforceable penalties for organisations which have turned a blind eye in the past. Support groups such as SNAP Australia, Care Leavers Australia Network and Broken Rites have submitted evidence to the Victoria inquiry, and the Law Institute of Victoria has advocated changes to the statute of limitations.\nThe statute of limitations for criminal offences is governed by Sec. 468 of the Criminal Procedure Code.\n|Wikiquote has quotations related to: Statute of limitations|\n- Adverse possession\n- Equitable tolling\n- Limitation Act 1980 (England and Wales)\n- Nullum tempus occurrit regi\n- Statute of Limitations in Ireland\n- Limitation Periods in the UK\n- Statute of repose\n- Tort reform\n- ""Statute of Limitations"". California Court Judicial Branch. Public Access Records. Retrieved 6 June 2014.\n- California Courts, Judicial Council. ""Public Access Records"". California Courts. Rewriting Amendments. Retrieved 6 June 2014.\n- Special Historic Session. ""Opening Remarks:HistoricSpecial section"" (PDF). www.courts.ca.gov. Supreme Court Of California. Retrieved 6 June 2014.\n- Allen, Danielle S. (2003). The World of Prometheus: The Politics of Punishing in Democratic Athens. Princeton University Press. p. 154. ISBN 0-691-09489-6.\n- Gabelli v. Securities and Exchange Commission.\n- Macy J. (2013). Opinion analysis: That which does not kill the SEC may make the agency stronger. SCOTUSblog.\n- ""La prescrizione del reato dopo la ex-Cirielli"". Diritto Penale. Retrieved 5 June 2013.\n- ""Codul Penal, Articolul 180 – Prescripția"". Retrieved 9 October 2013.\n- Ridley, Yvonne. ""Bush Convicted of War Crimes in Absentia"". www.foreignpolicyjournal.com. Foreign Policy Journal. Retrieved 12 October 2014.\n- Kenner v. C.I.R., 387 F.3d 689 (1968); 7 Moore\'s Federal Practice, 2nd ed., p. 512, ¶ 60.23\n- Bulloch v. United States, 763 F.2d 1115, 1121 (10th Cir. 1985)\n- ""Sixth Amendment - Limited Protection Against Excessive Prosecutorial Delay"". Northwestern University School of Law. The Journal of Criminal Law and Criminology. Retrieved 12 October 2014.\n- ""Criminal Procedure"". thecanadianencyclopedia.ca. The Canadian Encyclopedia. Retrieved 2 October 2014.\n- ""Criminal Code"". Section 46: Government of Canada. Retrieved 16 July 2015.\n- ""Limitation Periods in Canada\'s Provinces and Territories"" (PDF). fenninsurance.com. Olga Gil Research Services. Retrieved 30 October 2008.\n- ""Limitations Act, 2002, S.O. 2002, c. 24, Sched. B"". E-laws.gov.on.ca. Retrieved 2014-07-31.\n- ""Should Britain have a Statute of Limitations on sex crimes?"". Theopinionsite.org. 2011-03-19. Retrieved 2014-07-31.\n- ""Parliamentary Inquiry On The Handling Of Child Abuse By Religious And Other Non-Government Organisations"" (PDF). Parliament of Victoria. Retrieved 2014-07-31.\n- ""Parliament of Victoria"". Inquiry Into The Handling of Child Abuse by Religious and Other Organizations. Retrieved 9 May 2013.\n- ""Victoria ends statutory time limit on historical child sex abuse cases"". www.theaustralian.com.au. Australian Associated Press. Retrieved 12 October 2014.\n- ""Inquiry Into the Handling of Child Abuse by Religious and Other Organisations"" (PDF). Parliament of Victoria. Retrieved 2014-07-31.\n- ""Waller Legal response"" (PDF). Parliament of Victoria. Retrieved 2014-07-31.\n- ""A Submission by Care Leavers Australia Network (CLAN) to the Inquiry into the handling of child abuse by religious and other organisations"" (PDF). Parliament of Victoria. Retrieved 2014-07-31.\n- ""Broken Rites response"" (PDF). Parliament of Victoria. Retrieved 2014-07-31.\n- ""Inquiry Into the Processes by Which Religious and Other Non-government Organisations Respond to the Criminal Abuse of Children by Personnel Within Their Organisations"" (PDF). Parliament of Victoria. Retrieved 2014-07-31.', 'Principles of the Nuremberg Tribunal, 1950 No. 82\nPrinciples of International Law Recognized in the Charter of the Nuremberg Tribunal and in the Judgment of the Tribunal. Adopted by the International Law Commission of the United Nations, 1950.\nIntroductory note: Under General Assembly Resolution 177 (II), paragraph (a), the International Law Commission was directed to ""formulate the principles of international law recognized in the Charter of the Nuremberg Tribunal and in the judgment of the Tribunal."" In the course of the consideration of this subject, the question arose as to whether or not the Commission should ascertain to what extent the principles contained in the Charter and judgment constituted principles of international law. The conclusion was that since the Nuremberg Principles had been affirmed by the General Assembly, the task entrusted to the Commission was not to express any appreciation of these principles as principles of international law but merely to formulate them. The text below was adopted by the Commission at its second session. The Report of the Commission also contains commentaries on the principles (see Yearbook of the Intemational Law Commission, 1950, Vol. II, pp. 374-378).\nAuthentic text: English Text published in Report of the International Law Commission Covering its Second Session, 5 June-29 Duly 1950, Document A/1316, pp. 11-14.\nAny person who commits an act which constitutes a crime under international law is responsible therefor and liable to punishment.\nThe fact that internal law does not impose a penalty for an act which constitutes a crime under international law does not relieve the person who committed the act from responsibility under international law.\nThe fact that a person who committed an act which constitutes a crime under international law acted as Head of State or responsible Government official does not relieve him from responsibility under international law.\nThe fact that a person acted pursuant to order of his Government or of a superior does not relieve him from responsibility under international law, provided a moral choice was in fact possible to him.\nAny person charged with a crime under international law has the right to a fair trial on the facts and law.\nThe crimes hereinafter set out are punishable as crimes under; international law:\n1. Crimes against peace:\n1. Planning, preparation, initiation or waging of a war of aggression or a war in violation of international treaties, agreements or assurances;\n2. Participation in a common plan or conspiracy for the accomplishment of any of the acts mentioned under (i).\n2. War crimes:\nViolations of the laws or customs of war which include, but are not limited to, murder, ill-treatment or deportation to slave-labor or for any other purpose of civilian population of or in occupied territory, murder or illtreatment of prisoners of war, of persons on the seas, killing of hostages, plunder of public or private property, wanton destruction of cities, towns, or villages, or devastation not justified by military necessity.\n3. Crimes against humanity:\nMurder, extermination, enslavement, deportation and other inhuman acts done against any civilian population, or persecutions on political, racial or religious grounds, when such acts are done or such persecutions are carried on in execution of or in connection with any crime against peace or any war crime.\nComplicity in the commission of a crime against peace, a war crime, or a crime against humanity as set forth in Principles VI is a crime under international law. Index\nWWW URL: http://deoxy.org/wc-nurem.htm\nThe Commission of Inquiry for the International War Crimes Tribunal']"	['<urn:uuid:c5074c85-e14d-4b9b-8920-ddee256e4b13>', '<urn:uuid:fe04b3a6-750b-473e-b7a2-e8cccf33bacf>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	8	89	3064
46	How does breathing in swimming compare to body tension in gymnastics?	In swimming, proper breathing technique involves exhaling when your face is in the water and taking a full breath when rolling to the side. Many swimmers have issues due to holding their breath or partially exhaling, leading to panic and fatigue from lack of oxygen rather than physical effort. In gymnastics, good body tension in the major muscles is crucial for maintaining effective balances. Both are fundamental aspects that affect performance - breathing enables efficient swimming without premature fatigue, while body tension allows proper execution of gymnastic balances.	"['Presentation on theme: ""SKILLS & TECHNIQUES Atechnique is a way of performing a skill Askill is a movement with a purpose Shooting Passing Dribbling Catching Marking Set Shot.""— Presentation transcript:\nSKILLS & TECHNIQUES Atechnique is a way of performing a skill Askill is a movement with a purpose Shooting Passing Dribbling Catching Marking Set Shot Jump Shot Lay-up Skill: Shooting\nSkills Skills should be carried out with maximum efficiency and performed with minimum effort. A skilful performer is one who can control physical movement and can anticipate what is going to happen next as different skills and techniques are completed. 1) Write down some major skills and techniques involved in the activities in your course. 2) Write down a sports person that you think is the best in the world in their activity. Explain why you think they are so skilful.\nSimple Skills EASY/SIMPLE SKILLS Few Sub-routines Physically Undemanding Little co-ordination needed Few judgments to be made Simple order of movements One movement at a time Easy Environment e.g. Indoors e.g. Set Shot\nSimple Skills PUSH PASS CHEST PASS HEAD- STAND HEADING SET SHOT HIGH SERVE SIMPLE SKILLS\nComplex Skills COMPLEX SKILLS Many Sub-routines Physically Demanding Co-ordination is vital Many judgements to be made Many movements at the same time e.g. Lay-up in Basketball\nButterfly stroke Butterfly stroke High jump High jump Basketball lay-up Basketball lay-up Volley Volley Goal Keeper save Complex Skills\nEffects of Skills & Techniques on Performance Simple skills are made up of physical actions which are common to many activities such as kicking, jumping, striking, throwing, stretching and rotating. You learn these basic skills (moves) as you develop. Your skill level will affect how well you can refine skills through practice, use skills at the correct time, and make decisions about when to use basic and complex skills.\nEffects of Skills & Techniques on Performance (EXAMPLE) When drawn to the net in badminton, a basic skill is to return the shuttle high to back of the opponents court with an underarm clear. A complex skill would be a tight net shot. Choose one skill from one activity. Explain how you refined the skill through practice.\nWhat makes a skilled performance? A skilled performance shows three characteristics: a.Sequences of movements are carried out in a fluent, controlled way often with minimum effort b.Correct options are selected c.Skills & techniques are used which reflect the performers ability and experience\nSkill Learning Learning skills involves considering: Safe practice Methods of practice How best to practise with a partner and in a group\nSafe Practice This involves ensuring practice is at the right level for your ability and completing a warm up before practice and a cool down after practice. It also involves understanding and abiding by the safety rules. E.g. Swimming * Class rules set by the teacher * Swimming pool rules * Working co-operatively with your partner or class\nLearning Skills Learning Skills Gradual Build Up is when you learn a skill bit by bit e.g. Diving 1. Seated Dive 2. Kneeling Dive 3. Crouched Dive 4. Full Dive Whole Part Whole is when you try the whole skill, then, break it down and concentrate on part, before putting it all together again Repetition Drills Shadowing is when you shadow your partner and copy what they are doing. e.g. Movements in Badminton Solo/Partner/Group Conditioned Games\nMethods of Practice There are four main methods of practice that you need to know for physical education: GRADUAL BUILD UP WHOLE PART WHOLE PASSIVE/ACTIVE PRACTICES REPETIITIVE PRACTICES\nGradual Build Up Gradual build up is a useful practice method for learning complex skills. You learn the skill bit by bit. You build the skill up in stages. Often used when the skill is too difficult or dangerous to try. Gradual build up is a useful practice method as it allows you to: Make practices more demanding in small steps Develop confidence Make practices challenging but achievable Gear practices to your stage of learning\nGradual Build Up The best example for using the gradual build up method for an individual activity is the flight dive. Describe three different stages of building up to a flight dive in swimming\nGradual Build Up Gradual build up practice method can also be used in team activities. E.g. Stage 1: In indoor hockey you are working on attack. 4 v 1. The object of the practice is to get past the defender. You must describe how you would gradually build up this practice to make it more challenging.\nWhole Part Whole Whole part whole is often used by performers who already have some experience of the activity. It works best when you can perform a version of the whole skill already. E.g. Front Crawl - Swimming Whole: Try the whole stroke i.e. using both arms and legs Part: With a float in hands, practise legs only to improve leg kick Whole: Return to the whole stroke again to see if stroke has improved.\nPASSIVE/ACTIVE PRACTICES These are often useful to practise with a partner as they can vary the degree of opposition required to help you improve. E.g. Basketball – lay up shot 1. Solo practice ensuring foot pattern and ball placement is correct. 2. With a partner – receive pass from partner and perform lay-up shot. 3. Partner passes and then uses passive defence to mark you 4.Partner passes and then becomes an active defender who is trying to get ball.\nLearning Skills Tasks 1.Choose one method of practice from list A and one from list B. Explain why you would use these methods of practice.(4) List AList B Shadow practice Passive/Active Practice Whole-part-wholeSmall sided games Cooperative practiceRepetitive Practices 2.Choose two activities and describe, in detail, how you used these methods of practice to develop a skill. (Give specific details of a practice that you have used). (4)\nLearning Skills Tasks 3.Gradual build up is one method of learning and developing skills. i)What is meant by gradual build up?/2 ii)Explain why you would use this method/2 4.Select two different skills where you used gradual build up. Choose an individual activity, a skill or technique. Describe how you learned to perform this skill or technique. Firstly I, /1Then I, /1 Finally I, /1 5.Choose a Team activity, a skill or technique. Describe how you learned to perform this skill or technique. Firstly I, /1Then I, /1 Finally I, /1\nLearning Skills Exam Questions 1.Whole – part – whole and gradual build-up are two different methods of learning and developing skills. Explain why you used each method. (2)x2 2.Choose an activity. Choose a skill/technique that could be improved using whole/part/whole. Describe what you did to improve this skill/technique using whole/part/whole. (2) ii)Now choose a different method of developing skills. (1) iii)Describe how you used this method to improve a skill/technique in an activity of your choice. (2) iv)Why was this an appropriate way to develop this skill/technique. (1)\nMovement Analysis One effective approach for breaking down a skill or technique is through analysing your: P reparation is the first part of the movement A ction is the middle part of the movement R ecovery is the end of the movement You would use an observation schedule to achieve this\nMovement Analysis PHASE OF ACTION FEATURES OF MODEL PERFORMANCE MY PERFORMANCE Preparation Starts from base. Performer tracks path of shuttle and begins moving towards place shuttle will be played from. While moving, body turns side-on to net. Racquet is taken up and back behind head. Weight shifts mostly onto back foot. Back shoulder drops. Front arm balances racquet arm (both arms are raised). An observation schedule would be used to analyse your performance. E.g. Overhead Clear\nMovement Analysis PHASE OF ACTION FEATURES OF MODEL PERFORMANCE MY PERFORMANCE Action Shoulder, arm and racquet are brought forward at speed to help generate power. Action resembles throwing action. Weight is transferred forward from back foot to front foot to coincide with moment of impact. Impact is with open racquet face above racquet shoulder. Performer strikes through shuttle and body weight continues to move forward (a smooth continuous action leads naturally into recovery).\nMovement Analysis PHASE OF ACTION FEATURES OF MODEL PERFORMANCE MY PERFORMANCE Recovery Racquet comes down and across body in recovery phase. Forward movement at end of stroke leads to base and recovery of ready position.\nMovement Analysis Benefits of Using an Observation Schedule i. Identifies Strengths & Weaknesses of a skill ii. Pinpoints specifically where weakness lies iii. Practice programme can be designed from the information received iv. You can use an observation schedule at the beginning and end of a training block to see if your technique has improved\nStages of Learning There are three important stages in learning and developing skills: Planning Stage (Cognitive) Practice Stage (Associative) Automatic Stage (Autonomous)\nPlanning Stage During the planning stage, you find out what the skill involves. You establish what the parts of the skill are and make your first attempts at learning each part. Errors are likely to be common at this stage in learning.\nPractice Stage During the practice stage you link together all the required subroutines of the skill. Simple skills will require less practice than complex skills. Quality practice will reduce the number of mistakes made during performance.\nAutomatic Stage At this stage, most key subroutines have become automatic in the performance. At the automatic stage errors are less likely. You will be able to do it without thinking. Your success rate with an automatic skill will be high and your mind will be free to concentrate on other aspects of the game. Choose one skill from an activity from the course. Explain, in detail, the characteristics of performance at each stage of skill learning.\nOvercoming Problems in Skill Learning Identify your performance strengths and weaknesses Identify your performance strengths and weaknesses Compare with a classmates ability Compare with a classmates ability Compare with a model performer Compare with a model performer Highlights fitness needs of performance Exact requirements of different skills Effectiveness of decision making Highlights specific practice to overcome skill weaknesses\nPrinciples of Effective Practice For your practice to be effective you should set clear objectives. This will enable you to refine performance. You must consider your existing level of ability and current strengths and weaknesses and what aspects of your performance you want to improve. You must also consider: Work/Rest Ratio Progression\nPrinciples of Effective Practice WORK/ REST RATIO PREVIOUS EXPERIENCE IN THAT ACTIVITY YOUR LEVEL OF PRACTICAL ABILITY DIFFICULTY OF THE SKILL INVOLVED PHYSICAL DEMANDS OF THE PRACTICE WORK/REST RATIO IS ONE OF THE KEY ISSUES IN MAKING SKILL TRAINING SPECIFIC TO YOUR NEEDS\nPrinciples of Effective Practice PRACTICES ARE LINKED TO YOUR CURRENT PERFORMANCE LEVEL PROGRESSION MAKE PRACTICES MORE CHALLENGING WHEN YOU ARE READY WORK AT A SUITABLY DEMANDING LEVEL AT ALL TIMES HIGH QUALITY PRACTICE FOR A SHORT PERIOD OF TIME PROGRESSION WILL ENSURE THAT YOUR PERFORMANCE DOES NOT SUFFER FROM THE EFFECTS OF BOREDOM AND FATIGUE\nPrinciples of Effective Practice Practising Under Pressure With skills and techniques that you can complete with a high degree of control and fluency it is important to complete practice under pressure i.e. as you improve practices must become more challenging. This will make practice relate to performing in competition. In team activities this could involve increasing the demands of opponents.\nFeedback Feedback is information received by a performer about their performance. Positive feedback helps your performance improvement Effective feedback needs to be positive Feedback needs to be precise and accurate One or two points of feedback given at a time Feedback should be given as soon as possible after performance\nFeedback FEEDBACK INTERNAL Is how you feel about your own performance.E.g.You took a shot that was unsuccessful because the ball hit the front of the ring. You would know that next time you shoot you would have to shoot with more power.\nFeedback Exam Questions 1)Choose an activity and a skill/technique. Select one type of feedback which helped you identify a problem. Describe how this feedback helped you to identify a problem. (2) ii)For the same skill choose a different type of feedback. Describe how this feedback helped you to identify a problem. (2) iii)Explain how feedback helped you to improve your performance in your chosen activity. (2) iv)Explain what is meant by internal feedback. (1) v)Explain what is meant by external feedback 2)Feedback is an important aspect of developing skills to an automatic stage. What is meant by feedback? (1) ii)How does feedback help you to develop a skill towards the automatic stage? (1)\nCo-operation It is important that you can effectively co-operate with class mates. This involves practising together and talking about how best complete tasks. Co-operation also involves observing a partner and recording information. When participating as part of a team working co- operatively involves accepting responsibility and recognising you role within a team.\nMechanical Principles Learning Outcomes What is balance, are there different types of balance?What is balance, are there different types of balance? Centre of gravityCentre of gravity Body tensionBody tension Transfer of Weight and benefits to performance if completed effectivelyTransfer of Weight and benefits to performance if completed effectively Force and the application of force.Force and the application of force. FrictionFriction Follow throughFollow through\nBalance Balance is the ability to retain the centre of gravity over your base of support. There are two types of balances: 1) Static balances 2) Dynamic balances\nCentre of Gravity Centre of gravity is to do with stability. i.The heavier you are, the more stable you will be ii.To be balanced your centre of gravity should be inside your base iii.A low position with a wide base is more balanced\nCentre of Gravity\nBody Tension For technique to be effective it is helpful if the major muscles involved in balances have good body tension. In gymnastics explain how good body tension helped to improve the quality of your balances?\nTransfer Of Weight The transfer of weight allows you to move your body from one place to another. How well you transfer your weight involves considering how well balanced you are when performing. To transfer your weight in any direction, you must apply a force in the opposite direction. The force is created when you contract your muscles. At times, this can be single actions such as throwing the javelin. At other times, for example when running, simple actions are repeated. Name 2 activities, describe one technique which requires a single transference of weight and describe another technique which requires repeated transference of weight\nApplication of Force When performing different skills and techniques different forces are applied and resisted. For every action there is an equal and opposite reaction (Newtons third law)\nApplication of Force If speed is required then the greater force applied the better. Differences in the mass of the body will affect performance. If the force applied to the sprinting block (100m sprint) is the same the athlete with a smaller mass will accelerate at a quicker rate.\nForce You must apply force in one direction to move in the opposite direction. Look at the following examples. Explain where force is applied and what direction the person or equipment moves. A. C B\nFriction This is when contact between two surfaces cause a slowing down or gripping effect. This can be an advantage or disadvantage depending on the activity and the skill or technique you are trying to complete. Adv.Football boots getting a good grip on the ground to stop the player slipping and allow them to change direction quickly DisAdv.In cycling when the wind slows you down Choose four activities explain how friction affects your performance.\nRotation In different activities you rotate (turn) in order to carry out skills and techniques. When throwing the discus in athletics you turn around in a spinning back-to-front movement to generate power. This is a complex technique. You build up speed before throwing by quickly turning until you have regained balance. You can also rotate by performing a forward roll. Name two skills in which you turn head over heals.\nResistance When you apply force there will be resistance. Like friction, resistance can either be an advantage or disadvantage. Adv. When sprinting in athletics the starting blocks are an advantage. They provide resistance. DisAdv.If you are running into a headwind this slows you down. Resistance can be reduced or increased depending on your technique. In swimming if you keep your head out of the water when doing the front crawl this creates a greater resistance with the water and slows you down. Reduce resistance by putting head level with the water (streamlined)\nFollow through When completing different skills and techniques it is important to have a good follow through. A good follow through is part of effective performance and follows on from the preparation and action phases in different skills and techniques. An example would be kicking a ball in football, the follow through is in the direction of the shot. In other actions the follow through involves body rotation. The ball and socket joints in the hips and shoulder help rotation in this striking action.\nMechanical Principles A. What is the importance of transfer of weight when completing an overhead clear? B. When completing a headstand what affect would having a small base have your performance? C. Why is body tension important when completing static balances? D. Where must your centre of gravity be maintained to hold a balance? E. Give an example of a dynamic balance? F. Name two activities where equipment is used to reduce friction? G. In swimming, how did improving your technique, reduce your resistance in the water? H. Describe one technique in three different activities where follow through is important (use kicking, striking and throwing)?', 'Your weakness is your strength! Cliché? Yes, but like many clichés, this maxim contains an element of truth, since every imperfection presents an opportunity for improvement. When it comes to swimming, ironing out wrinkles in your stroke can yield huge dividends in terms of improved efficiency and lowered splits. Regardless of your level of proficiency in the water, improvement is always an option, the most talented swimmers can improve by continually tweaking and refining their strokes. The off season is the time to spend on technique not yardage.\nBefore we can discuss technique, we need to go back to the most basic element in swimming, breathing! This is generally where most problems begin in the water. When you’re swimming freestyle, it’s critical to exhale when your face is in the water so you are able to take a full breath when you roll to the side. Do not think about the inhale, unless you have gills, you will always inhale when you rotate your head, it’s a natural reaction. When swimmers say they swallow water, 9 out of 10 times it has to do with timing, not forgetting to inhale. However, because they can’t relax in the water, many swimmers resort to holding their breath or partially exhale, subsequently they cannot take in a complete lungful of air. This starts the downward spiral to panic, these swimmers have to breath every stroke, they go hypoxic very quickly, and tire not from the effort but merely from lack of oxygen. The way I like to begin correcting this is in our warm up and cool down. Here the swimmer is relaxed enough to smooth out breathing using certain drills to help with technique.\nNow that we have solved the breathing issue lets go back to the common flaws I generally see in swimming:\nCrossover is a problem for many swimmers. I find the main reason is because they do not realize they are doing it. We use video analysis in our clinics for this very reason. If I had a quarter for every time a swimmer has said to me “I thought I was doing that”, I’d be a rich woman today. Crossover happens when your hand enters the water at the beginning of each stroke and passes through your midline. It is important for swimmers to not cross the body’s imaginary midline running from head to toe. Crossing over puts strain on the shoulder joint and makes your body fishtail or swing through the water, increasing drag.\nEntering too early:\nAn early hand entry at the beginning of each stroke in most cases causes the swimmer to cut through the water in a downward motion, usually next to the head. The swimmer needs to focus on extending the arm forward near the surface. The down-motion of the arm causes the swimmer to pull through with a straight arm generating very little power in the most important part of the stroke, “the catch”.\nIf swimming short distances is your thing then a short finish will boost your stroke rate, but since most of the swimmers I work with are swimming longer distances, the full extension at the end of the pull phase of the stroke is more efficient. When the swimmers triceps get tired the natural reaction is shortening of their stroke, pulling their hands out of the water to early, at their waist rather than by their upper thigh. As your arms complete their underwater pull, strive for a fully extend arm behind you, by your side, so that your thumbs graze the side of your thighs below your suit-line. Too often I see swimmers begin bending their elbows the end of their stroke, they pull their arms out of the water before allowing them to finish their path. Shortening your stroke results in a less of efficient stroke, uses more energy because the swimmer is taking more strokes per lap (essentially spinning their wheels).\nDropping the elbow:\nMany times, swimmers drop their elbows after their hands enter the water in the beginning of each stroke (remember the elbow should be high while the fingertips point down at the bottom of the pool (“over the barrel”). This freestyle flaw zaps more speed from the swimmer then any of the other mentioned flaws. This happens because all of your power and speed come from under the water. In simple terms all your force is being directed down instead of pushing the swimmer through the water.\nNo long-axis rotation:\nWhat the heck is that? I like to call this “flat” swimming. Basically the swimmer does not rotate from side to side. Very common with triathletes because cycling and running are forward motion sports with little rotation. Consequently the pull is shortened, the length of the stroke becomes shorter, and drag becomes an issue.\nSlapping and overextending entry:\nDo you ever stand by the pool and hear a loud swimmer? This is not a good thing. To me it usually indicates that the swimmer is trying to lengthen the stroke from the top. In reality a long stroke comes from extending underwater and rotating or rolling form side to side. Overextending in the entry phase of swimming will cause the swimmer to push the body down in the water and create a straight arm pull. I try to get swimmers to feel over extension to reach their “sweet spot.”\nIf you do any of these try to back down on your yardage, and focus on technique or you might want to join in on one of my swim clinics!']"	['<urn:uuid:14166bfe-6d42-4a66-9f24-aab752bfc24d>', '<urn:uuid:916688a0-405d-4294-9bb8-944ea09414eb>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T00:43:36.053682	11	88	3881
47	As someone studying Holocaust survivor testimonies, I'm curious about Si Frumkin's work with the Shoah Foundation - what was his contribution to Steven Spielberg's project in 2007?	In 2007, Si Frumkin interviewed nearly one hundred Holocaust survivors for Steven Spielberg's Shoah Foundation.	"['Scope and Content\nOrganization and Arrangement\nPhysical Characteristics and Technical Requirements\nTitle: Si Frumkin papers\nDate (inclusive): 1969-2010\nCollection number: 1888\nFrumkin, Si, 1930-2009.\n56 boxes (28 linear ft.)\nAbstract: Si Frumkin was born in Kaunas/Kovno, Lithuania on November 5, 1930. He survived the Dachau concentration camp and emigrated\nto the U.S. in 1949. In 1968 he founded the Southern California Council for Soviet Jews (SCCSJ.) He frequently spoke on Holocaust\nissues at the Wiesenthal Center\'s Museum of Tolerance and founded the Association of Holocaust Survivors from the former Soviet\nUnion. The collection consists mostly of newsletters, press releases and photographs highlighting the activities of the SCCSJ\nand the Union of Councils for Soviet Jews for more than 40 years.\nLanguage: Finding aid is written in\nUniversity of California, Los Angeles. Library Special Collections.\nLos Angeles, California 90095-1575\nPhysical location: Stored off-site at SRLF. Advance notice is required for access to the collection. Please contact UCLA Library Special Collections\nfor paging information.\nRestrictions on Access\nOpen for research. STORED OFF-SITE AT SRLF. Advance notice is required for access to the collection. Please contact UCLA Library\nSpecial Collections for paging information.\nRestrictions on Use and Reproduction\nProperty rights to the physical object belong to the UC Regents. Literary rights, including copyright, are retained by the\ncreators and their heirs. It is the responsibility of the researcher to determine who holds the copyright and pursue the copyright\nowner or his or her heir for permission to publish where The UC Regents do not hold the copyright.\nProvenance/Source of Acquisition\nGift of Ella Frumkin, 2011.\nProcessed by William M. Katin in the Center for Primary Research and Training (CFPRT), with assistance from Megan Hahn Fraser,\nSummer 2011. The processing of this collection was supported by the generous donation of Zev Yaroslavsky.\n[Identification of item], Si Frumkin papers (Collection 1888). UCLA Library Special Collections, Charles E. Young Research\nUCLA Catalog Record ID\nSi Frumkin (born Simas Frumkinas on November 5, 1930), businessman and political activist, was born in Kaunas/Kovno, Lithuania,\nthe son of Mykolas and Zila (nee Waisapel) Frumkinas, who owned a Willys Overland automobile and NSU motorcycle dealership.\nFrumkin was ten years old when the Kaunas Ghetto was established on July 10, 1941. He and his parents lived there until the\nNazis closed the camp on 8 July 1944, and Frumkin and his father were deported to Dachau. His mother was separated from the\nfamily and sent to Poland. Frumkin and his father became slave laborers constructing the subterranean factories to manufacture\nthe Messerschmitt 262 for the Phillip Holzmann\'s plant called ""Diana II."" On April 7, 1945, twenty days before the liberation\nof Dachau, Frumkin\'s father died.\nThe Jewish Brigade (a unit of Jewish men in the British Army) transported Frumkin to an Italian Displaced Persons camp in\nModena, Italy. He received a scholarship in order to attend schools in Italy, Switzerland, and England after the war. He was\nlater reunited with his mother who had survived the war and relocated to Venezuela.\nHe immigrated to the United States in 1949 to study at New York University, earning a Bachelor\'s Degree in 1953. He moved\nto Los Angeles, where his mother and stepfather (a survivor from the Warsaw Ghetto) had since moved. They built Universal\nDrapery Fabrics at 560 San Julian Street in Los Angeles into a successful business.\nFrumkin earned a Master\'s Degree in history from California State University, Northridge in 1964 and founded the Southern\nCalifornia Council for Soviet Jews (SCCSJ) in 1968. With Zev Yaroslavsky (later Los Angeles City Councilman and a member of\nthe Los Angeles County Board of Supervisors), Frumkin demonstrated against Soviet cultural events held in Los Angeles, including\nthe Moiseyev Ballet, Lynus Pauling receiving the Lenin Peace Prize, the Osipov Balalaika performance, and Leonid Brezhnev\'s\nvisit with President Nixon in San Clemente, in order to expose the plight of Jews not allowed to emigrate from the USSR. In\n1973 he was accompanied by Yaroslavsky on a visit to Moscow, where they met with Soviet dissidents.\nFrumkin began weekly speaking engagements for the Wiesenthal Center\'s Museum of Tolerance, and he provided expert court testimony\nfor Soviet Jews facing possible deportation. In 1991 he went to China on behalf of the Simon Wiesenthal Center, participated\nin a 1996 American Gathering of Jewish Holocaust Survivors meeting in Washington, D.C., and in 2007 he interviewed nearly\none hundred Holocaust survivors for Steven Spielberg\'s Shoah Foundation. He was one of the leading figures in the Russian\nspeaking community in California and the United States.\nFrumkin died May 15, 2009. Further information about his life and work can be found at\nScope and Content\nThe collection consists of two large series and four small series.\nSeries 1. Political Commentary\nAbout 40 per cent of this collection is found in the first series consisting of essays written to encourage the American Government\nto pressure the Soviets to allow more Soviet Jews to emigrate to Israel or the U.S. The emphasis is on political commentary\non current events with a Jewish theme. These writings, originally entitled ""Si\'s Scribbles,"" were on occasion published in\nnewspapers, and beginning in December 1992 they were published in a newsletter format entitled\nGraffiti for Intellectuals. The\nGraffiti newsletter also included reprinted articles from the\nLos Angeles Times,\nJerusalem Post, and other media. Themes include the Jackson-Vanik amendment, Soviet limitation of Jewish emigration, the status of Jewish\nrefusniks in the USSR, and the slowness of the Claims Conference in disbursing restitutions for deprivations caused by the\nAn important feature of this series is Frumkin\'s translation of Soviet dissident literature from the 1970s and 1980s, often\nGraffiti. Authors include: Mihajlo Mihajlov, Lev Navrozov, Lev Alburt, Valery Soyfer, Michail Makarenko, Genrikh Shakhnovich, Vladimir\nMatlin, and Vadim Belotserkovsky. It is unknown whether Frumkin\'s translations are the only edition of the essays that are\nextant for those unable to read Cyrillic. Other Russian literary activity referred to in\nGraffiti includes the translation of Leon Uris\'s Exodus from English to Russian by inmates in the Dubrovlag Camp in Soviet Mordovia\nOne example of Frumkin\'s abiding concern for the Russian émigré population of greater L.A. is demonstrated by his deep friendship\nwith Alexander Polovets. Polovets was the editor of the Russian-American newspaper Panorama, the largest independent Russian-language\nweekly newspaper outside of Russia, a publication which originated in July 1980. Thus there are numerous articles shared by\nA second important theme of this first series is Frumkin\'s involvement in Holocaust memorial activities. This theme is seen\nin the essays Frumkin wrote, and in newspaper articles covering events such as his service as the master of ceremonies for\nthe dedication of the Babi Yar memorial in Plummer Park, his weekly talks on his Dachau experiences presented at the Museum\nof Tolerance, and his interviewing Russian speaking survivors for Steven Spielberg\'s Shoah Foundation. In addition, a conference\nfolder indicates that Frumkin participated in Phil Blazer\'s ""Addressing the Cycle of Pain"" Conference of Holocaust survivors\nin Berlin during November 1991.\nSeries 2. Press Releases\nThis series contains press releases about the efforts of the Southern California Council for Soviet Jews (SCCSJ), such as\ncandlelight walks, encouraging Soviet Jews by mailing them Passover and Rosh Hashanah cards, and their protests of Russian\ncultural exchanges in order to draw attention to the Soviet restrictions on emigration.\nFrumkin developed a ""prisoner of the month"" campaign, thus the press releases may now serve as an index to Soviet refuseniks\nduring the 1970s and 1980s, including such notables as Anatoly Sharansky, Ida Nudel, Vladimir Slepak and Alexander Voronel.\nIn addition, Frumkin compiled reports of his trips to the USSR in which he met with Soviet Jews, excerpts of which are contained\nin this series.\nFrumkin enlisted the aid of politicians in enabling Soviet Jews to emigrate. The working relationships that he developed with\nCalifornia politicians can be seen in letters from those who acknowledged receipt of his material, including Senators Barbara\nBoxer and Pete Wilson, Congressmen Henry Waxman and Howard Berman, Los Angeles Mayors Sam Yorty and Tom Bradley, and Los Angeles\nCity Councilman Joel Wachs. Zev Yaroslavsky\'s work is also represented in the collection, which contains items such as his\npress releases as the leader of the UCLA student movement promoting Jewish emigration from the USSR.\nSeries 3. Research Files\nAbout 45 per cent of the collection consists of research materials, mostly clippings from U.S. daily newspapers and American\nJewish publications on topics such as anti-Semitism, emigration and the conviction of Jonathan Pollard, the American Jewish\nanalyst convicted of giving U.S. military secrets to the Israelis. Frumkin\'s translation into English of Soviet refusenik\nliterature is not solely confined to series one, but also appears in this series as the entire run of the publication\nTHEM and also\nWitness, publications of short duration in which Frumkin collaborated with Alexander Polovets as the editor-in-chief. Also included\nare lists of names and addresses of Soviet Jews seeking to emigrate as well as material from the Union of Councils for Soviet\nSeries 4. Photographs\nPhotographs from events such as the annual candlelight walks and the picketing of Soviet cultural events outside the Shrine\nAuditorium are present in this series. Many of the politicians lending support to Soviet Jewish emigration mentioned in Series\n2 are also depicted, including Secretary of Interior James Watt, Senator Henry ""Scoop"" Jackson, Congressmen Alphonso Bell\nand Bob Dornan, and L.A. City Councilmen Joel Wachs, Tom Bradley, and Zev Yaroslavsky.\nSeries 5. Soviet Holocaust Interview Translations\nIn 2008, Frumkin translated 21 of the Holocaust narratives from the 2004 Russian book,\nVictims of the Holocaust Tell Their Stories. Some of the personal memories included are those from Vilya Ira, Ida Anapolskaya, and Alexander Anapolski. In 2010, Frumkin\'s\nwidow Ella Frumkin edited the recollections along with additional translations for the publication of the book\nThe Holocaust DID Happen.\nSeries 6. Pocket Calendars\nThe final series contains pocket calendars, an address book and a film made for the occasion of Frumkin\'s 70th birthday, in\nwhich many people he assisted or worked with express their gratitude to him.\nOrganization and Arrangement\nThe original order of Frumkin\'s filing system in Series 1 through 3 has been maintained.\n- Series 1. Political Commentary 1984-2009\n- Series 2. Press Releases 1969-1998\n- Series 3. Research Files 1989-1996\n- Series 4. Photographs 1969-1989\n- Series 5. Soviet Holocaust Interview Translations 2008-2010\n- Series 6. Pocket Calendars 1971-1988\nPhysical Characteristics and Technical Requirements\nCOLLECTION CONTAINS DIGITAL MATERIALS: Special equipment or further processing may be required for viewing. To access digital\nmaterials you must notify the reference desk in advance of your visit.\nThe following terms have been used to index the description of this collection in the library\'s online public access catalog.\nFrumkin, Si, 1930-2009. --Archives.\nSouthern California Council for Soviet Jews --Archives.\nHolocaust survivors --Archival resources.\nGenres and Forms of Material']"	['<urn:uuid:0e2847b6-2ed4-47fb-84f7-c4a8f91ed7a3>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	27	15	1798
48	comparison cream cheese heavy cream whipping cream properties percentages usage desserts	Cream cheese contains 33% milk fat and is a fresh cheese made from cream, vital for many dessert recipes where substituting lower fat versions should be done carefully. Heavy whipping cream (also called heavy cream) contains between 36-49% milk fat and can be whipped easily. Light whipping cream contains 30-36% milk fat and can also be whipped, though with more difficulty. Both types of whipping cream are commonly used in dessert recipes, such as in cream cheese fillings where heavy cream is gradually added to create thick, creamy textures.	['Cream Cheese Swirl Tart With Chocolate Crust\nThis eye-catching tart nods to a classic fruit-topped cheesecake, but is far easier to put together and starts with a cocoa-and-oat streusel press-in crust, instead of traditional graham crackers. Fruit preserves are folded into the tangy no-bake filling to create swirls. You can serve the tart immediately, or keep it in the refrigerator for up to 24 hours. Using a pan with a removable base allows you to unmold the tart and serve it on a platter, but you could also use a glass or ceramic pan and serve the dish directly from the pan. Serve alone, or top with fresh berries.\n- Serves: 9 persons\n- ¾cup/95 grams all-purpose flour\n- ½cup/40 grams old-fashioned oats\n- ¼cup/55 grams light brown sugar\n- 3tablespoons unsweetened cocoa powder\n- ½teaspoon fine sea salt\n- ¼teaspoon baking powder\n- ½cup/115 grams cold unsalted butter (1 stick), cut into 1/2-inch/1-centimeter cubes\n- 6ounces/170 grams cream cheese, at room temperature\n- ¾cup/90 grams confectioners’ sugar\n- 1 ¼cups/300 milliliters cold heavy cream\n- 1teaspoon vanilla extract\n- ¼teaspoon almond extract\n- ½cup/120 milliliters jam or preserves (such as strawberry, raspberry, blackberry or cherry)\nStep 1Make the crust: In a medium bowl, stir the flour, oats, brown sugar, cocoa powder, salt and baking powder to combine. Add the butter to the bowl and mix with your hands (or a fork or pastry cutter) until the butter is incorporated and the mixture starts to look crumbly. (Try to work quickly, so the butter stays chilled.) Add 2 tablespoons cool water and mix until the mixture comes together to form large clumps. (It should hold together firmly when squeezed.) If the mixture looks dry, add more water, 1 tablespoon at a time, until it comes together. If it feels too soft, transfer to the refrigerator to firm up for a few minutes.\nStep 2Using your fingers, press the prepared streusel into the bottom and up the sides of a 9-inch tart pan with a removable base. Work your way around the edges of the tart pan, pressing firmly to make sure it is evenly thick. Break away any excess that pops over the edge of the pan so that it is relatively flush. Crumble the excess streusel into the base of the pan, and press it into an even layer.\nStep 3Transfer the crust in the tart pan to the freezer while you heat the oven to 350 degrees.\nStep 4Transfer the chilled crust to a baking sheet and bake until it appears set and dry to the touch in the center, 20 to 23 minutes. Cool completely.\nStep 5While the crust cools, prepare the filling: In the bowl of an electric mixer fitted with the whip attachment (or if using a hand mixer, in a large bowl), whip the cream cheese and confectioners’ sugar on medium speed until light and fluffy, 2 to 3 minutes. Scrape down the sides of the bowl with a silicone spatula.\nStep 6With the mixer running on medium-low speed, gradually add the cream a little at a time, allowing each addition to incorporate before adding more. Continue to whip on medium-high speed until the mixture is thick and creamy, 3 to 4 minutes. Scrape down the sides of the bowl well. Add the vanilla and almond extract and whip until well combined.\nStep 7In a small pot, heat the jam until warm; this should loosen the mixture considerably. If the jam contains large pieces of fruit, you can use a fork or potato masher to crush them slightly, once the mixture is warm. Strain the jam through a fine-mesh sieve, pressing firmly to extract as much as possible. Discard any solids or seeds that remain in the strainer.\nStep 8Drizzle about 1/2 of the warm, strained jam over the cream cheese filling. (Don’t worry about adding the warm mixture to the cold filling; it will set up as it hits the cold mixture). Gently fold it a few times to loosely combine the two. Scoop the filling into the cooled crust and spread into a relatively even layer. Drizzle the remaining jam over the surface of the filling. Use a small offset spatula or the back of a small spoon to swoop across the filling back and forth a few times to lightly swirl the jam into the filling, leaving some large streaks of jam. (Note: If you mix too much, the two mixtures will combine and you’ll end up with a tinted filling rather than a streaky filling.) Serve immediately, or chill for up to 24 hours, loosely covered, before serving.\nStep 9To remove the tart from the pan, support the base with one hand and push upward to release it from the outer ring. If desired, run a small offset spatula around the bottom to help release the tart from the base and slide it onto a serving platter. (Alternately, you can leave the tart on its base and simply set on top of a serving platter.)', 'This post is one I was going to research and do later in the year. But…I’m in the middle of creating a new dessert: “Butterscotch Blondie Cheesecake.” And various recipes for butterscotch and cheesecake call for multiple types of dairy products: milk, heavy cream, whipping cream, heavy whipping cream, etc. And I’ve been wondering how much I really, really need to buy both heavy cream and whipping cream. Seriously? Aren’t they the same thing? It turns out the answer is….no.\nHigh-quality and the correct ingredients are the difference between a delicious recipe and a…well, disaster. Or at least a low-quality product. Understanding the differences between ingredients is important. Which brings us to the many dairy products we encounter while cooking and baking. I am not covering: cheeses (excepting cream cheese), ice creams, eggs, or non-dairy milks/cream in this post. I will in the future!\nClotted cream (breakfast cream): contains minimum fat content of 55%, ranges up to 64%. Produced by indirectly heating (steam or water bath) full-cream milk and then allowing it cool naturally in shallow pans. The clots that form on top are the “clotted cream.” Generally used in the Britain.\nCream cheese: contains 33% milk fat (in U.S.; other countries may require higher percentage). Fresh cheese made from cream. In many recipes, the fat in the cream cheese is vital to the end product, so substituting lower fat or non-fat cream cheese should be done carefully (or not at all).\nCreams: higher butter-fat layer skimmed from top of milk. Sometimes called sweet cream to distinguish it from whey cream used in cheese-making.\n• Half & Half: blend of whole milk and cream. Contains between 10.5% and 18% milk fat. Cannot be whipped.\n• Light cream (coffee cream, table cream): Contains between 18% and 30% milk fat; usually in the range of 20%. Cannot be whipped.\n• Light whipping cream: Contains between 30% and 36% milk fat. Can be whipped (higher fat content, easier to whip). If a recipe calls for light whipping cream, heavy whipping cream may be used. However, if a recipe specifies heavy whipping cream, that is what may be used.\n• Heavy whipping cream (heavy cream): contains between 36% and 49% milk fat (according to American requirements…generally on the lower end of that spectrum). Can be whipped (higher fat content, easier to whip).\n• Double cream: contains 42-48% milk fat. Only available in Europe.\n-Notes: Ultra-pasturized versions of these are most common…but don’t whip as well and can have a slight “burnt” aftertaste. Finding “regular” pasteurized versions is becoming harder, but the taste is better. Above percentages are U.S. requirements; they vary by country. Ultra-pasturized heavy cream may “not work” if frothing or stiff peaks are required.\nCreme fraiche: contains 30% to 45% milk fat. It is made in similiar manner to sour cream but with different bacteria; less sour than sour cream. (This is the American & English definition; in France, creme fraiche is liquid, while creme epaisse is the thick cream referred to here.)\nEvaporated milk (dehydrated milk, condensed milk, concentrate milk): canned milk product in which approximately 60% of water content has been removed; does not need to be refrigerated. Available in whole and non-fat. Do NOT confuse with sweetened condense milk (which is generally right next to it on the grocery store shelf.)\nMilk: the fat in milk contributes to structure of baked goods; differences in fat content (2% versus whole, for example) will result in differences in flavor and texture.\n• Whole milk: 3.5% milk fat.\n• 2% milk: 2% milk fat (obviously).\n• 1% milk: 1% milk fat (obviously).\n• Skim milk: .1% to .5% milk fat.\nPowdered milk: made by evaporating milk to dryness. A powder with long shelf life and no need of refrigeration. Used in the production of many food items by manufacturer’s, occasionally used by the home cook.\nSour cream: contains 14% milk fat and is made by “souring” cream with an acidfier. Basically creme fraiche, but leaner and firmer.\nSweetened condensed milk: thick, sweet, canned milk product sold in cans; does not need to be refrigerated. It is produced by removing approximately half of the water from whole milk, then adding up to 40% sugar. (There is also a product called condensed milk [unsweetened] but rarely used anymore).\nWhipped cream: cream (with more than 30% fat) that has been whisked/had air mixed in to form a soft solid (versus natural liquid state).\nYogurt: “‘Yogurt’ is the Turkish word for milk that has been fermented into a tart, semisolid mass” (McGee.) Today’s industrial version of yogurt is cultured using only 2 strains of bacteria and can be cultured from whole, low-fat, or nonfat milk.\nYogurt cheese: made by draining yogurt in cheesecloth. Low-fat alternative to cream cheese.\nNotes: 1) milk fat & butterfat are used interchangeably above. ; 2) The higher the fat content of the cream, the richer it will taste and the less likely it will be to curdle when heated.\nSources: McGee, Harold. (2007). On Food and Cooking: The Science and Lore of the Kitchen. Published by: Scribner.']	['<urn:uuid:c59b96a8-e081-4184-9920-c6c86f76675b>', '<urn:uuid:fa6f4761-75af-4ee0-a267-f3975df7c110>']	open-ended	direct	long-search-query	similar-to-document	three-doc	expert	2025-05-13T00:43:36.053682	11	89	1681
49	visiting church holy sepulchre jerusalem what time opens closes summer months	The Church of the Holy Sepulchre opens at 05:00 and closes at 20:00 from April to September.	['On the top of the hill in Jerusalem known as Golgotha stands the holiest Christian shrine in the Old City, the Church of the Holy Sepulchre – the site of Christ’s crucifixion, burial and resurrection.\nDespite its rather unimpressive exterior – it does seem somewhat squeezed between a variety of other buildings and edifices in the Christian Quarter of the Old City – this Church has to be on your list of Christian sites to see in the Holy Land, however seriously you take your faith.\nToday the church is shared by five different Christian communities; Roman Catholic, Greek Orthodox, Coptic, Syrian Orthodox and Armenian. This happy co-existence has been long in the making and the Church of the Holy Sepulchre has a long colorful history which you can revel in when you visit what is a powerful, spiritual and one-to-last-in-the-memory site.\nThe hill where the Church of the Holy Sepulchre now stands has been a site of worship since as early as the 2nd century with the first building believed to have been a Temple dedicated to Aphrodite.\nThis was replaced by a Basilica at the behest of Emperor Constantine I, sometime in the mid 320s. It was during the excavations for this building that the evidence that supports the shrine today was found, namely the True Cross and the tomb. The plans were then redrawn to accommodate the finds and the new Basilica was consecrated as the church to commemorate the death and resurrection of Jesus Christ.\nIn 1009 the Church of the Holy Sepulchre was destroyed on the orders of Fatimid caliph Al-Hakim bi-Amr Allah after having enjoyed 600 hundred years as a shrine protected on the order of a succession of Muslim and Moorish rulers.\nThere then followed a period of often violent squabbling among all arms of the Christian church which continued for centuries until the 1852 Status Quo sharing agreement. The agreement still stands today and has resulted in various parts of the Church of Holy Sepulchre being dedicated to the different sects and has also resulted in something to watch with fascination when you visit.\nIn the late afternoon, each group with a shared ownership of the church proceeds in turn from the Calvary to the tomb. As a watching tourist you can enjoy it all; there might be a similarity in the swinging censers (incense holders) and candle carriers but there are distinct differences in the robes, chanting/incantations and hymn singing.\nThe authenticity of the shrine aspect of the Church of the Holy Sepulchre makes it the place of reverent gorgeousness that it is today and whether you believe or not, you can’t help yourself from feeling the power of worship of fellow Christians as you gaze upon the Stone of Unction, marking the spot where Christ’s body was prepared for burial. Don’t be surprised if the person next to you rubs something on the stone to absorb its sanctity.\nAs you climb the narrow steps to the Calvary you’ll find yourself in the most lavishly decorated part of the Church of the Holy Sepulchre. Not surprising really, as this site is regarded as the site of the crucifixion.\nIn this area you will find The Rock of Calvary (12th Station of the Cross) in the Greek Orthodox altar under glass, the Roman Catholic Chapel of the Nailing of the Cross (11th Station of the Cross) and towards the Eastern Orthodox chapel, a statue of Mary (the 13th Station of the Cross).\nAfter you’ve seen the Calvary, make your way to the Rotunda under the larger of the two domes to the stunningly constructed Aedicule which contains the Holy Sepulchre itself.\nHere you can see the Angel’s Stone, a fragment of the rock believed to have sealed Jesus’s burial tomb and the tomb itself.\nThe innermost room of the Aedicule is covered in striking medieval marble and there are some beautiful examples of iconic art. To the rear of the Aedicule you’ll find the Coptic Orthodox altar in a delightful chapel of lattice ironwork and if you’re interested in the Roman Catholic section of the Church of the Holy Sepulcher you’ll locate it on the southeastern side of the Rotunda in the Chapel of the Apparition.\nThere are also chapels in practically every corner of the Church of the Holy Sepulchre, but there’s a few of major significance that really should be on your list of things to see, including these three:\nYou can also visit the Prison of Christ, although the attribution of this site is still hotly contested amongst the Franciscans, Greek Orthodox and Armenians.\nThe Church of the Holy Sepulcher is a working church so all the usual requisites of a Christian church can be seen as you make your way around it. You can even make your way to the roof from the courtyard of the Greek Orthodox Patriarchate for stunning views of Jerusalem.\nJust remember, as a tourist visiting the Church of the Holy Sepulcher you are in a place of immense religious significance, so please dress (no shorts are advised but we got in with no problem, and long trousers in the famous Israeli summers isn’t much fun) and act accordingly.\nIf you want to know where you’re heading within the Church of the Holy Sepulchre, before you go there you might want to print this image out and take it with you.\nAddress: Suq Khan e-Zeit and Christian Quarter Rd in the Old City’s Christian Quarter.\nOpening Hours: Throughout the year it opens at 05:00. It closes at 20:00 from April-September, and at 19:00 from October-March.']	['<urn:uuid:529485f2-751a-4fb6-8822-afe9839375cf>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	11	17	931
50	earliest latino groups activities new york city first arrived	The earliest Latino presence in New York began with Puerto Rican and Cuban patriots who were plotting to overthrow Spanish rule in their islands. These early revolutionary clubs were composed of both Puerto Ricans and Cubans working together.	"['Agustin Laos-Montes, Arlene Davila, eds. Mambo Montage: The Latinization of New York. New York: Columbia University Press, 2001. xiv + 493 pp. $25.50 (paper), ISBN 978-0-231-11275-8.\nReviewed by Evelyn Gonzalez (Department of History, William Paterson University of New Jersey)\nPublished on H-Urban (July, 2002)\nLatins in Manhattan\nLatins in Manhattan\nIn the last ten years or so, there has been a veritable explosion of research and writing by Latinos and about Latinos in New York and the United States. A new crop of Latino scholars has resurrected the history of early Latino migrants to New York City, focused on the Latino condition in general, and created new forums for their findings. Mambo Montage: The Latinization of New York is the latest addition in this growing new field of ""Latino Studies."" It is divided into four sections--loosely defined as the Production of Latinidad, Expressive Cultures, Latino/a Identities, and Latinizing Cityscapes--and is written for a scholarly audience. Its eighteen articles seek to portray the Latino/Hispanic experience in all its complexities, ramifications, and contradictions. These entries are interrelated and interdisciplinary and range in quality from the excellent essay on Latino literature and identity by Juan Flores to the unfocused, meandering account of Latin dance classes by Karen Beckstein.\nMambo Montage, editor Agustin Lao-Montes tells us, ""is an attempt to situate the historical production of Latino American identities, cultural expressions, and social movements in the context of New York City"" (p. 3). The important concept here is ""latinidad,"" which is both the unifying theme of the book and ""a useful category of historical analysis"" (p. 36, n. 9). Latinidad is defined as a common Latino identity for all Latin peoples in the United States, be they Mexican, Puerto Rican, Cuban, Dominican, or Columbian, from the Caribbean or Latin America. Latinidad is particularly relevant in New York because the city has the most diverse Latino population and is also the ""symbolic center of Latin culture"" (p.2), hence the ""mambo montage"" of the title. Such a pan-Latino identity only began developing in the late 1960s, hence my use of the ""Latino/Hispanic"" label to encompass the time frame covered by the book.\nLatinos, however, ""remain generally invisible"" (p. 366), because their history ""has been poorly documented"" (p. 366). Luis Aponte-Pares believes ""this loss compounds the view that Latinos are all immigrants, [and] that they have not contributed to the development of New York City"" (p. 366). Mambo Montage will have none of that. Its articles reveal a long history of Latinos in New York, beginning with Puerto Rican and Cuban patriots plotting to overthrow Spanish rule in their islands and concluding with the Hispanic marketing industry, Spanish language rights, and the development of a pan-Latino identity, that is to say, latinidad. Along the way, the articles cover Latino baseball teams in the Jim Crow era, Puerto Rican and Brazilian women in labor unions, and Latino activism and social movements, laced with discourses on Latino art, literature, music, dance, and food. In the process, these entries spell out the differences between New York\'s Latino/Hispanic groups. Each has arrived at different times, under different conditions, for differing reasons, and from distinct countries that have their own histories, cultures, and racial mix.\nHow is a common identity possible in this context? The answer lies in another important theme of the book, the centrality of race in the Hispanic/Latino experience. In the words of Lao-Montes, ""everyday racism is the most immediate source in the formation of a pan-Latino consciousness"" (p. 10). Race is key because Puerto Ricans, the first large Latino group in the city, encountered a rigid black/white racial dichotomy when they arrived. Many were white, but those who were not shared the indignities and exclusion that white America inflicted on American Blacks. Old-stock and Euro-ethnic New Yorkers quickly imposed a non-white ""other"" status on all Puerto Ricans, whatever their skin color. New to such racial discrimination, Puerto Ricans clung to their culture, their Spanish language, and their neighborhoods, and took solace in their Puerto Rican heritage. Thus, self-identification from below (by Puerto Ricans) and ""the practices of othering"" (p. 17) from above (to Puerto Ricans) fostered a Latino identity. As Lao-Montes explains, ""One of the main racial ideologies of latinidad defines Latinos as a third race, as it were, in between black and white"" (p. 9).\nThat ""perception of Latinos as a mestizo race"" (p. 9), that ""so-called browning of America"" (p. 9), is at the core of two-thirds of the essays. Articles by Adrian Burgos, Jr., by Ramon Grosfoguel and Chloe S. Georas, and by Jose Itzigsohn and Carlos Dore-Cabral discuss how Latino ball teams stressed ""Spanish"" to evade the color line; how Dominicans in New York were lumped with Puerto Ricans into that ""non-white"" category while the mostly white, anti-Castro Cuban refugees were not; how Dominicans chose to identify themselves as Latino rather than white or black; and how Latinos were discriminated against, stigmatized, and finally marginalized by white America. Jocelyn Solis adds Mexican identity and immigration status to the mix, maintaining that Mexicans (the newest and fastest growing segment of Latinos in the city) were stigmatized because those from Latin America had long been considered ""a foreign other"" (p. 342) in the United States and undocumented Mexicans even more so by their illegal status. Latinos are not just another ethnic group in this formulation. Grosfoguel and Georas, for one, posit that equating latinidad with ethnicity denies racism and blames Latinos for their marginalized condition in the city and the nation, thus refuting earlier depictions of Puerto Ricans, and by extension other Latinos, as the latest wave of immigrants to New York who would in time assimilate into mainstream America.\nOther factors also promoted a Latino identity. To improve their lot, Latinos of all stripes engaged in community organizations, electoral politics, and protest groups, which in the context of New York meant Puerto Ricans, Dominicans, Columbians, and sundry others had to work together as Latinos, not as disparate nationalities. Six articles outline Puerto Rican and Latino activism and community involvement that stretched from the very early revolutionary clubs composed of Puerto Ricans and Cubans, through the social and political militancy of the Young Lords and the neighborhood action committees, and finally to the current ongoing struggles against police brutality. The activism of the 1960s and 1970s has been explained better and more fully elsewhere, but Luis Apointe-Pares, Liz Sevcenko, Arlene Davila, and joint authors Elsa Cardalda Sanchez and Amilcar Tirado Aviles explore identity politics in the organizing of Latino Gays, latinizing the Lower East Side\'s name to ""Loisaida"" to claim the neighborhood and keep gentrification at bay, and creating neighborhood museums and workshops for Puerto Rican and Latino artists. Language is key as well because the Hispanic marketing industry targets Latinos ""as a common people and a market"" (p. 411). Arlene Davila argues that to earn and stay in business, this industry actively promotes ""the permanence of the Spanish language as the basis for Latino/Hispanic identity"" (p. 411).\nWhat is apparent in Mambo Montage is that Latinos are still marginalized on all sides. Puerto Ricans in New York City (aka Nuyoricans or the more recently coined, DiaspoRicans) are particularly vulnerable here. There is no hyphenated Puerto Rican-American, relates Juan Flores, hence Puerto Ricans live off the hyphen, unrecognized by mainstream society on the mainland or the island. Flores is specifically concerned with Nuyorican writers who are ""dismissed and ignored by the Puerto Rican government and literary establishment"" (p. 195), unlike other Latino authors in the United States, whose countries of origin provide recognition and consular support and whose novels are often taken as representative of the Latino experience in New York City even though that experience was really a Puerto Rican one. Aponte-Pares asserts that this exclusion also extends to Latino Gays, who are stigmatized by the dominant Macho Latin culture, yet not accepted by the Gay movement in New York.\nA similar dynamic is at work elsewhere. Wilson Valentin-Escobar details how the lyrics of Puerto Rican born singer Hector Lavoe expressed the hopes and sorrows of diasporic Puerto Ricans and Latinos living in New York. They in turn made Lavoe\'s Salsa into a music that ""critique[d] Puerto Rican second-class citizenship ... and affirmed their cultural identity"" (p. 214). Lavoe died in 1993 and, according to his wishes, was buried in The Bronx, the city\'s most Latino borough. Island Puerto Ricans, however, demanded that Lavoe\'s body be re-interred in Puerto Rico, in effect dismissing New York\'s Puerto Rican community as not really Puerto Rican. Likewise, Raquel Z. Rivera describes how South Bronx Puerto Rican youths who participated with Blacks in the creation of Hip-Hop music found themselves caught between Latino and Black culture. As Hip-Hop became more attuned to the Black ghetto scene in the 1980s and rapping in English took precedence over the Latino rhythmic contribution, Puerto Rican Hip-Hop artists seemed an anomaly, seen as neither Latino nor Black. In the same way, as Davila related, Puerto Rican artists created El Museo Del Barrio only to find themselves subordinated to and virtually ignored by established cultural institutions in New York, the United States, and Latin America. Meanwhile, the Hispanic marketing industry, Davila continues, only employs Latin American professionals (those ""not yet \'tainted\' with U.S. culture"" [p. 415]), never Latinos from the United States. Presenting itself ""as the authentic \'spokesman\' for the entire U.S. Latino population,"" the industry contributes to and winds up profiting ""from Latinos\' marginality in greater society"" (p. 416).\nOn the whole, latinidad works well as a unifying theme in Mambo Montage, but there are difficulties with the concept in and of itself. For one, it glosses over the differences between Latino groups that despite the growing Latino indentification are real and concrete and are kept alive by continuing inflows of new migrants. Cubans are not Puerto Ricans and vice versa. For another, like the Hispanic marketing industry, latinidad assumes the existence of a static Latino community (always there, always Spanish-speaking, and always the ""other"") and ignores a larger bilingual Latino community that has moved out of ""El Barrio"" but not away from its roots. This cohort of Latinos is socially aspiring, increasingly educated, and English-speaking, yet according to Vilma Santiago-Irizarry, insists on bilingualism in city signs, government forms, and education to counter negative images of Latinos, in effect using the Spanish language as a form of empowerment instead of marginalization.\nIn the final analysis, Mambo Montage is an important book. Its articles convey much new information about the Latino/Hispanic community in New York and explore important but long-ignored issues. The inclusion of music, literature, and the arts fills in the Latino/Hispanic experience even more, while the footnotes and bibliographical references alone are invaluable for the latest works in the field. That said, however, some of the entries are not as well researched or as clearly written as they should be. There are too many asides just accepted as given without explanation or footnote references. The otherwise fine article by Grosfoguel and Georas, for example, mentions that white America associates Puerto Rican identity with ""laziness, criminality, stupidity, and a tendency toward uncivilized behavior"" (p. 98), without showing how such an association came about. Crying racism is not enough. One must have evidence, or at the very least the scholarship on which the assertion is based. Such tidbits are sprinkled throughout the book and point out the need for further research in the field. Similarly, much of the writing is numbingly abstract. The reader has to plow through sections of hermetic prose and theoretical constructs with little factual grounding. This limits the potential audience of Mambo Montage. And this is unfortunate because this book is worthwhile and should be read by many.\n. See for example, James Jennings, ed., Blacks, Latinos, and Asians in Urban America (Westport, Conn. and London: Praeger, 1994); Andres Torres, Between Melting Pot and Mosaic: African Americans and Puerto Ricans in the New York Political Economy (Philadelphia: Temple University Press, 1995); Gabriel Haslip-Viera and Sherrie L. Baver, eds., Latinos in New York: Communities in Transition (Notre Dame, Ind. and London: University of Notre Dame Press, 1996); Clara E. Rodriguez, ed., Latin Looks: Images of Latinas and Latinos in the U.S. Media (Boulder, Colo.: Westview Press, 1997); Andres Torres and Jose E. Velasquez, eds., The Puerto Rican Movement: Voices From the Diaspora (Philadelphia: Temple University Press, 1998); and Juan Flores, From Bomba to Hip-Hop: Puerto Rican Culture and Latino Identity (New York: Columbia University Press, 2000).\n. Chapter 6, Juan Flores, ""Life Off the Hyphen: Latino Literature and Nuyorican Tradition""; and chapter 17, Karen Beckstein, ""Taking Class Into Account: Dance, the Studio, and Latino Culture.""\n. Chapter 2, Adrian Burgos, Jr., ""\'The Latins From Manhattan\': Confronting Race and Building Community in Jim Crow Baseball, 1906- 1950""; chapter 3, Ramon Grosfoguel and Chloe S. Georas, ""Latino Caribbean Diasporas in New York""; and chapter 11, Jose Itzigsohn and Carlos Dore-Cabral, ""The Manifold Character of Panethnicity: Latino Identities and Practices Among Domincans in New York City.""\n. Chapter 12, Jocelyn Solis, ""Immigration Status and Identity: Undocumented Mexicans in New York.""\n. For Puerto Ricans as an ethnic group, see Nathan Glazer and Daniel P. Moynihan, Beyond the Melting Pot: The Negroes, Puerto Ricans, Jews, Italians, and Irish of New York City, 2nd ed., (New York and London: The M.I.T. Press, 1970); and Oscar Handlin, The Newcomers (Cambridge, Mass.: Harvard University Press, 1959.\n. See Torres and Velazquez, eds., The Puerto Rican Movement.\n. Chapter 13, Luis Aponte-Pares, ""Outside/In: Crossing Queer and Latino Boundaries""; chapter 10, Liz Sevcenko, ""Making Loisaida: Placing Puertorriquenidad in Lower Manhattan""; chapter 5, Arlene Davila, ""Culture in the Battlefront: From Nationalist to Pan-Latino Projects""; and chapter 9, Elsa B. Cardalda Sanchez and Amilcar Tirado Aviles, ""Ambiguous Identities! The Affirmation of Puertorriquenidad in the Community Murals of New York City.""\n. Chapter 15, Arlene Davila, ""The Latin Side of Madison Avenue: Marketing and the Language That Makes Us \'Hispanics\'.""\n. Flores, ""Life Off the Hyphen.""\n. Flores is referring to Cuban-American author Oscar Hijuelos, whose novel The Mambo Kings Play Songs of Love won the Pulitzer prize for fiction in 1990 and depicted the Latin music scene of 1950s New York as solely Cuban, even though the Cuban exodus from Castro\'s Cuba began much later.\n. Aponte-Pares, ""Outside/In.""\n. Chapter 7, Wilson Valentin-Escobar, ""Nothing Connects Us All But Imagined Sounds: Performing Trans-Boricua Memories, Identities, and Nationalisms Through the Death of Hector Lavoe.""\n. Chapter 8, Raquel Z. Rivera, ""Hip-Hop, Puerto Rican, and Ethnoracial Identities in New York.""\n. Davila, ""Culture in the Battlefront.""\n. Davila, ""The Latin Side of Madison Avenue.""\n. Chapter 18, Vilma Santiago-Irizarry, ""Deceptive Solidity: Public Signs, Civic Inclusion, and Language Rights in New York City (and Beyond).""\nIf there is additional discussion of this review, you may access it through the list discussion logs at: http://h-net.msu.edu/cgi-bin/logbrowse.pl.\nEvelyn Gonzalez. Review of Laos-Montes, Agustin; Davila, Arlene, eds., Mambo Montage: The Latinization of New York.\nH-Urban, H-Net Reviews.\nCopyright © 2002 by H-Net, all rights reserved. H-Net permits the redistribution and reprinting of this work for nonprofit, educational purposes, with full and accurate attribution to the author, web location, date of publication, originating list, and H-Net: Humanities & Social Sciences Online. For any other proposed use, contact the Reviews editorial staff at firstname.lastname@example.org.']"	['<urn:uuid:1631c76a-afa6-43e3-a9a7-9905ea8a595c>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	9	38	2541
51	curious about art history did arcimboldo and dix share similar painting themes or subjects	While working in different time periods, both artists were known for their unique approaches to portraiture. Arcimboldo created composite portraits using objects like produce and books, often incorporating humor and metaphorical meanings to mock the wealthy elite. Similarly, Dix was known for his satirical portraits, particularly of celebrities from Germany's intellectual circles, though his were characterized by a darker, more savage style reflecting the harsh realities of interwar German society.	"[""Said to be 400 years ahead of his time, his paintings were considered to be incredibly modern. Today, many first-time viewers of his work assume he is a current artist using computer graphics to produce the creations. Arcimboldo combined two traditional fine painting genres in his 16th-century works, including still life and portraits. He did not become famous in his time during which religious scenes and traditional paintings were the norm, but for his unique composites of produce and objects, which added a distinct touch of humour to the art.\nHis Early Life\nGiuseppe Arcimboldo was born and raised in Milan at a period during which the Italian Renaissance was beginning to fade. His town of birth was linked to the Italian traditions and was also an artistic hub. His father, Biagio Arcimboldo, was also an artist, famous for his religious paintings in the styles of the Renaissance. He began his career as a cartoon designer, primarily on stained glass windows, stationed at the Milan Cathedral at the age of 21. In 1549, his career took a different turn, after he was contracted as a stained-glass window painter for the Duomo and in 1551, he was contracted to paint the courts of arms for Ferdinand I. He then painted the cathedral of Monza in 1558 and drew the Virgin tapestry cartoon, which to date, is still on display in Lombardi Como Cathedral.\nIt was in 1561 when Giuseppe Arcimboldo earned himself a position in Maximilian II as a court theatre painter after moving to Prague. His artwork was composed of jokes, metaphorical meanings and puns that were loved by his audience. He would then go on to live with the powerful archduke Hapsburg family in Vienna for 20 years before returning to Milan in 1587. Despite returning to his hometown, he kept close contact with the Hapsburgs and continued painting for them until he died.\nWhile at the Hapsburg courts, he was loved and famed for his unique signature style of painting, which was considered intellectual and comical. This style was first seen in 1569 when Arcimboldo released his figurative portraits. One of the sets was named The Four Seasons, featuring fall, summer and winter. The Four Elements was the second portrait unveiled, personalising air, the earth, fire and water. Although for the Renaissance incorporating individuals to personify abstract ideas was normal, Giuseppe Arcimboldo married natural items in his portraits to appear like human heads. It is worth noting that during his time, artists were executed when they did not please the commissioners. Arcimboldo was lucky; his patron loved his works.\nStyle and Character\nMost of Arcimboldo’s artwork is surrealistic and not representational. Largely, its compositions use items from nature, painted or used as facial features. He, for instance, uses leaves as hair, apples as cheeks, cucumber as a nose and so on, making the portrait look human from a distance. He has also used controversy in his paintings. This is better explained in his oil on canvas painting known as The Librarian. Here, the artwork features a human character, created from books and other objects. This painting is believed to have been mockery on the wealthy elite. During his time, only the wealthy could afford to purchase a collection of books. Apparently, these people were considered to have more wealth than brains, given than most of them could not comprehend the content of the books they had.\nGiuseppe Arcimboldo's style was purely under the category of mannerism. This is whereby artists showed the relationship between natural objects and humans in their artwork. It was a popular style between 1510 and 1590. There were other artists who leaned towards the mannerism style, such as Leonardo da Vinci. Most of them, unlike Arcimboldo, produced content that addressed the demands of the day and the urges of the wealthy. Most of Arcimboldo's paintings were significant to the Hapsburg family. The figure of the earth, for instance, is painted with a lion skin, while that of winter has a cloak with an M to represent Maximillian. These paintings pleased Maximillian so much that in 1571, he hosted a festival in their honour.\nGiuseppe Arcimboldo's paintings challenged many artists of his time, and by the time he moved to the Habsburg court, he had all the skills of a professional Renaissance expert. It was at the court where his progressive and profound thinking was cultivated as he worked with zoologists, alchemists, astronomers and other elite professionals. Habsburg was the most recognised centre for sciences and arts in Europe. Arcimboldo thrived as a designer, an organiser and a portrait painter. He received a worthy salary for his merits, allowing him to lead a successful career.\nDue to the invasion of the Swedish Prague in 1648 during the Thirty Years’ War, most of his works were lost. For this reason, most of those pieces, including his religious paintings and conventional portraits, did not get the spotlight they deserved. It is his unique mannerist paintings that have made their way into the films, album covers, novels and comics. Art critics still debate whether Giuseppe Arcimboldo's works were a result of a disturbed mind or whimsical thinking. Most scholars are of the view that with his fascination with puzzles and riddles, he could not have suffered from mental imbalance. Today, Arcimboldo is elevated to the ranks of the best 16th-century artists.\nThe last emperor Giuseppe Arcimboldo worked for was Rudolf I for 11 years. This period marked the peak of his career, particularly due to Rudolf's love for horticulture, exotic creatures and botany. He found the ultimate freedom to include extraordinary observations of animals and plants fetched from Europe by Rudolf's agents. It was during this period that he painted a portrait of himself as Four Seasons and that of Rudolf II. His illustrious career was highly celebrated by his Italian Contemporaries, who honoured him with manuscripts and poetry. Before leaving for Prague, Rudolf II made Arcimboldo count palatine for his faithful and long service, and also awarded him 1,500 Rhenish guilders.\nGiuseppe Arcimboldo died a year later in 1593 from kidney stones at the age of 66. Arcimboldo received notable recognition during his lifetime, but many historians did not take his art seriously. Instead, they viewed the artist as just a quirky painter. However, in the rise of Surrealism in the 1920s and the Dada anti-art movement, his visual expressionism was revived. His works were rediscovered in the 20th century by Salvador Dali and other Surrealist artists. Some of his popular paintings today include:\n- The 1566 Librarian oil on canvas\n- 1573 Autumn, oil on wood\n- 1573 Winter, oil on wood\n- 1573 Spring, oil on wood\n- 1573 Summer, oil on wood\n- 1591 Emperor Rudolf II as Vertumnus, oil/wood\n- 1590 The Gardner, oil on panel\nArcimboldo has been known to influence many artists, including Istvan Orosz, Sandro del Prete, Shigeo Fukuda, Octavio Ocampo and Jan Svankmejer films. Most of Giuseppe Arcimboldo’s works are displayed in famous museums around the world, including Uffizi Florence, Vienna's Kunsthistorisches Museum, Louvre Paris and many museums in Italy, the USA and Sweden. It is hard to find, from all of the most famous artists from the past few centuries, anyone who is quite as unique as Giuseppe Arcimboldo."", 'Otto Dix and die Neue Sachlichkeit, 1918-25 (Studies in the fine arts. The avant-garde) ebook\nby Brigid S. Barton\nStart by marking Otto Dix and Die Neue Sachlichkeit .\nStart by marking Otto Dix and Die Neue Sachlichkeit, 1918-1925 as Want to Read: Want to Read savin. ant to Read. Otto Dix and Die Neue. Details (if other): Cancel. Thanks for telling us about the problem. Otto Dix and Die Neue Sachlichkeit, 1918-1925. by. Brigid S. Barton.\nOtto Dix was a remarkable artist of the 20th century Germany\nOtto Dix was a remarkable artist of the 20th century Germany. He occupied a lead position in the New Objectivity movement, turning away from the ideas of Romanticism and Expressionism toward a more acidic and non-sentimental perspective to reflect the harsh realities of the interwar German society. Though being a representative of the anti-expressionist movement, Dix incorporated numerous styles into his paintings and etchings, including Expressionism.\nResults from Google Books.\nStart studying Die Neue Sachlichkeit: 1918-1933. Die Neue Sachlichkeit. Expressionist movement founded in Germany in the aftermath of WW1. Learn vocabulary, terms and more with flashcards, games and other study tools. Characterized by Realistic style combined with cynical and socially critical philosophical stance. Weimar Republic: after WW1- Treaty of Versailles, republic comes in and there is corruption in the government, before the Nazis come into power. Otto Dix. Father was a factory worker, mother was a seamstress. Exposed to art early on. Unique style depicting war.\nNeue Sachlichkeit, (German: New Objectivity), a group of German artists in the 1920s . Art Encyclopedia - Die Neue Sachlichkeit. Article Contributors.\nNeue Sachlichkeit, (German: New Objectivity), a group of German artists in the 1920s whose works were executed in a realistic style (in contrast to the prevailing styles of Expressionism and Abstraction) and who reflected what was characterized as the resignation and cynicism of the post-World Wa. Although many Neue Sachlichkeit artists continued working in representational styles after the 1920s, the movement itself ended with the rise of Nazism. Learn More in these related Britannica articles: Metaphysical painting.\nWilhelm Heinrich Otto Dix (German: ; 2 December 1891 – 25 July 1969) was a German painter and printmaker, noted for his ruthless and harshly realistic depictions of German society during the Weimar Republic and the brut.\nWilhelm Heinrich Otto Dix (German: ; 2 December 1891 – 25 July 1969) was a German painter and printmaker, noted for his ruthless and harshly realistic depictions of German society during the Weimar Republic and the brutality of war. Along with George Grosz and Max Beckmann, he is widely considered one of the most important artists of the Neue Sachlichkeit.\nOtto Dix, German, 1891 – Neue Sachlichkeit /New Objectivity, Metropolis .\nOtto Dix, German, 1891 – Neue Sachlichkeit /New Objectivity, Metropolis (Triptych), middle panel detail. Metropolis (German: Großstadt) is a 1928 painting by the German artist Otto Dix. It is a triptych with three nighttime city scenes from the Weimar Republic. Stifled by conservative social mores, iearly century avant-garde German artists embraced emotion, abstraction, and sexual freedom. Paintings for me. What others are saying. Joseph J Abhar - Christian Schad, Sonja, 1928.\nOtto Dix painted nudes, prostitutes, and often savagely satirical portraits of. .\nOtto Dix painted nudes, prostitutes, and often savagely satirical portraits of celebrities from Germany\'s intellectual circles in dark and allegorical works. His works are key parts of the Neue Sachlichkeit (""New Objectivity"") movement, which also attracted George Grosz and Max Beckmann in the mid 1920s. It is said that Dix painted this portrait after seeing von Harden in the street and exclaiming, ""I must paint you, I simply must! You represent an entire epoch.\nOtto Dix and the First World War: Grotesque Humor, Camaraderie and Remembrance. CDN$ 13. 8(4 used & new offers). Otto Dix and die Neue Sachlichkeit, 1918-25. Arts & Photography. by Michael Mackenzie. Eligible for FREE Shipping. CDN$ 7. 8(2 used & new offers). Bitter Witness: Otto Dix and the Great War. Mar 13 2003. by Linda F. McGreevy. Artists, A-Z. ( D-F ).\nOtto Dix was a German artist known for his grotesque portrait paintings and ghoulish visions of wa. In 1909, Dix began his study at the Dresden Academy of Fine Arts\nOtto Dix was a German artist known for his grotesque portrait paintings and ghoulish visions of war. A member of the New Objectivity movement along with George Grosz and Max Beckmann, Dix was heavily influenced by his time serving in the hellish trenches of World War . In 1909, Dix began his study at the Dresden Academy of Fine Arts. There was a huge creative output in the city, with a well-established and internationally renowned art and music scene that hosted large exhibitions and events.']"	['<urn:uuid:1dd6166a-829f-4050-94be-305a81952bf2>', '<urn:uuid:4e62caeb-8dd5-4d5c-96c4-911501b716e2>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T00:43:36.053682	14	70	1997
52	I'm new to investing and wondering about block trades and gold investments. How big does a block trade need to be to qualify, and what makes gold different from other alternative investments?	According to NYSE and Nasdaq criteria, a block trade must involve at least 10,000 shares or exceed $200,000 in value. Regarding gold's unique characteristics compared to other alternative investments, it has shown lower correlation and beta to global stock/bond portfolios than other alternatives, and its performance is driven by its own market fundamentals like jewelry demand, technology use, and central bank purchases. While gold can be volatile, its peak volatility matches global equities at around 33%, making it less volatile than other alternatives like REITs, which reached 71% volatility during the 2008 financial crisis.	"['A block trade is a significant transaction involving the buying or selling of securities that is conducted privately, away from the public markets. Its purpose is to minimize the impact on the price of the security. Typically, block trades are carried out by hedge funds and institutional investors through investment banks and other intermediaries. However, high-net-worth accredited investors may also have the opportunity to participate in these trades.\nAccording to the criteria set by the New York Stock Exchange and the Nasdaq, a block trade involves a minimum of 10,000 shares of stock or a value exceeding $200,000. For instance, in the present scenario, a block trade is taking place involving GOLD. The trade consists of 964,472 shares being exchanged at a price of $16.32 per share, which is slightly higher than the ask price of $16.31. To maintain confidentiality and disguise the true magnitude of the trade, it is likely that different brokers will be involved in executing this transaction.\nIt should be noted that the specific exchange on which this block trade is occurring has not been specified in the information provided. Nonetheless, the CME Group offers the opportunity to execute block trades for futures, options, or combination transactions that are permitted to be conducted separately from the public auction market.\nBarrick Gold Corporation\nUpdated on: 28/09/2023\nDebt to equity ratio: Buy\nPrice to earnings ratio: Strong Sell\nPrice to book ratio: Strong Buy\nDCF: Strong Buy\n12:00 PM (UTC)\nDate:28 September, 2023\n|Analyst / firm||Rating|\nBank of America Securities\nGOLD Stock Performance: Mixed Results on September 19, 2023 with Positive Turnaround in Earnings Growth\nOn September 19, 2023, GOLD stocks experienced mixed performances. The previous close for GOLD stocks was at $16.68, and today’s open was slightly higher at $16.72. Throughout the day, the stock fluctuated within a range of $16.20 to $16.75. The trading volume was 14,976,724, which is higher than the average volume of 13,188,653 over the past three months.\nLooking at the company’s financials, GOLD has shown a significant decline in earnings growth over the past year, with a decrease of 78.54%. However, there has been a positive turnaround this year, with earnings growth of 18.52%. Looking ahead, the projected earnings growth for the next five years is a modest 2.00%. The revenue growth for the previous year was -7.40%, indicating a decline in sales.\nIn terms of valuation, the P/E ratio is not available (NM), suggesting that the company may have negative earnings. The price/sales ratio is 2.74, which indicates that investors are willing to pay $2.74 for every dollar of sales generated by the company. The price/book ratio is not provided, so it is difficult to assess the company’s value based on its book value.\nTaking a closer look at the individual stock performances, Newmont Corporation (NEM) experienced a decrease of $0.16, which translates to a decline of 0.41%. Agnico Eagle Mines Ltd (AEM) also saw a decrease of $0.50, representing a decline of 1.00%. Gold Fields Ltd (GFI) had a smaller decrease of $0.08, resulting in a decline of 0.65%.\nLooking ahead, the next reporting date for GOLD stocks is set for November 9, 2023. Analysts forecast an EPS of $0.27 for this quarter. The net profit margin for GOLD stocks is 3.91%, indicating the company’s ability to generate profit from its revenue.\nGOLD operates in the Non-Energy Minerals sector and falls under the Precious Metals industry. The corporate headquarters of GOLD is located in Toronto, Ontario.\nOverall, GOLD stocks had a mixed performance on September 19, 2023. While there was a slight increase in the stock price at the opening, it fluctuated within a range throughout the day. The company’s financials show a decline in earnings growth over the past year, but there has been a positive turnaround in the current year. The next reporting date will provide further insights into the company’s performance.\nAnalysts and Investors Bullish on Barrick Gold Corp Stock Performance: Promising Investment Opportunity in 2023\nOn September 19, 2023, Barrick Gold Corp saw its stock performance being discussed by analysts and investors. According to data from CNN Money, 21 analysts provided 12-month price forecasts for the company’s stock. The median target price was estimated to be 22.25, with a high estimate of 30.00 and a low estimate of 17.20.\nThe consensus among 25 investment analysts polled is to buy stock in Barrick Gold Corp. This rating has remained unchanged since September.\nBarrick Gold Corp reported earnings per share of $0.27 for the current quarter. Additionally, the company’s sales for this period amounted to $3.4 billion.\nInvestors and analysts eagerly await the reporting date of November 9, when Barrick Gold Corp will release more detailed financial information for the current quarter.\nThe positive sentiment among analysts and investors, as well as the projected increase in stock price, suggests that Barrick Gold Corp is currently viewed as a promising investment opportunity.\nHowever, it is important for investors to conduct their own research and consider various factors before making any investment decisions.\nIt is worth noting that the information provided is based on data from September 19, 2023, and stock prices and forecasts are subject to change based on market conditions and other factors. Therefore, it is advisable to consult up-to-date sources and seek professional advice before making any investment decisions.', ""A lot can change in a year. As global economies begin to reopen in response to wider vaccine distribution, risk assets continue to take center stage among many investor discussions. However, amid this continued bullish sentiment lies a backdrop of certain rising risks — namely, stretched equity market valuations and bond yields remaining below inflation, which have created a potential asymmetrical return profile.\nThese factors emphasize the importance of diversification, a trait that gold — the original liquid alternative — has historically provided. Gold is one of the oldest financial instruments, dating back thousands of years, and may offer unique risk mitigation and diversification characteristics for portfolios compared to other liquid alternatives.\nFollowing a strong 2020, with gold reaching an all-time high of US$2,067/oz last summer,1 gold performance has remained subdued year to date, as shown below. Some have claimed that other liquid alternatives which have outperformed recently, or even cryptocurrencies such as Bitcoin, should replace gold’s role in portfolios as the preferred diversifier and potential portfolio hedge.\nFigure 1: Year-to-Date Performance\nAdding liquid alternative asset classes such as real estate investment trusts (REITs), natural resources, global infrastructure, liquid hedge strategies or broad commodities may potentially help investors mitigate risk in a traditional 60/40 portfolio. But gold, the original liquid alternative, has historically shown it may serve portfolios more effectively.\nProtecting Portfolios During Tail Events\nIn a fervent bull market, like the one we find ourselves in now, mitigating risk may not be front of mind. Yet, it remains incredibly important. More so now given the asymmetrical return profile for stocks and bonds that features less of a fundamental backstop (high valuations, low yields) if a correction does occur, even though the reopening of economies will continue to be the driver of growth in the near term.\nThis aspect of persistent downside risk mitigation is on display in the chart below, illustrating that each time equities dropped more than 10% over the last decade, a 10% portfolio allocation to gold would have reduced portfolio drawdowns by 161 basis points (bps) on average compared to a traditional 60/40 portfolio, while the same allocation to most other liquid alternatives would have had the opposite effect — increasing average drawdowns.\nFigure 2: Gold Reduced Drawdowns During Times of Market Stress\nA True Alternative to Risk Assets\nGold has not only exhibited a low correlation to global stock/bond portfolios, but exhibited a low beta as well. While correlation showcases directional tendencies in movement of two assets, beta attempts to measure the explanatory power and quantify possible cause and effect.\nGold’s performance is explained by more than just the vicissitudes of global equity and debt markets. Gold is driven by its own market’s fundamentals — gold demand in the form of jewelry, technology, central banks, and investment along with changes in gold supply globally. Compared to gold, other liquid alternatives and Bitcoin have higher betas to global stock/bond portfolios; this shows that their performance is determined by these markets. This may result in a less effective source of portfolio diversification.\nFigure 3: Gold Is an Effective Portfolio Diversifier\nVolatility Can Lead to Bad Investment Decisions\nUnderstanding the value at risk when adding a liquid alternative investment is important because high volatility can create fear and uncertainty which may lead to irrational investment decisions. Before the emergence of Bitcoin and other cryptocurrencies, investors were sometimes hesitant to allocate to gold due to its potential volatility. Gold can at times exhibit significant volatility but, as shown in the table below, even at its most volatile, gold ranks on par with global equities which, in theory, should be less volatile since it’s a basket of many stocks while gold is a single asset. As expected, Bitcoin’s volatility is much greater than liquid alternatives and nearly five time greater than gold over the last 12 months. While Bitcoin volatility has decreased from 174% to 86% recently, it is still higher than the peak volatility gold and liquid alternatives experienced at the height of the 2008 financial crisis. During this period, REITs were most volatile with a 1-year annualized volatility peaking at 71% in 2009.2 Meanwhile, gold matched global equites in 2009, peaking at 33% rolling 1-year annualized volatility.3\nFigure 4: Gold Looks Relatively Tame When Comparing Annualized Volatility\nMaintaining Gold for Risks Ahead\nAfter being down as much as 10% this year (driven by rising yields and a stronger US dollar), peak bearishness in gold may have been met. In Q2, gold-backed ETFs experienced positive inflows which could indicate that those investors who opened a gold position in 2020 to only hedge COVID-19 disruptions may have already closed those positions by now. Remaining ETF holders may be a combination of strategic and tactical investors seeking diversification and protection against future market risks, whether it is inflation, a weakening dollar, or monetary policy missteps.\nFigure 5: Global Appetite for Gold-backed ETFs Continues to Grow\nThere are ideally three main portfolio construction goals: capital appreciation, income, and downside risk management. In a bull market, it is sometimes hard to focus on the third. Yet, it is needed and alternative investments should be considered. However, not all alternative assets offer the proper amount of diversification. Overall, whether an investor is considering adding alternatives to their asset allocation, has already done so, or is primarily focused on traditional assets, gold — the original liquid alternative — may be a potentially strong complement to traditional asset allocation models and potentially enhance portfolio performance over time.\n1Bloomberg Financial L.P., and State Street Global Advisors. Gold reached all-time high in US-dollar terms on August 6, 2020. 2Bloomberg Financial L.P., and State Street Global Advisors, data as of June 30, 2021. 3Bloomberg Financial L.P., and State Street Global Advisors, data as of June 30, 2021.\nBasis Point (bps)\nA unit of measure for interest rates, investment performance, pricing of investment services and other percentages in finance. One basis point is equal to one-hundredth of 1 percent, or 0.01%.\nBloomberg Barclays Global Aggregate Bond Index A benchmark that provides a broad-based measure of the global investment-grade fixed income markets. The three major components of this index are the US Aggregate, the Pan-European Aggregate, and the Asian-Pacific Aggregate Indices. The index also includes Eurodollar and Euro-Yen corporate bonds, Canadian government, agency and corporate securities, and USD investment-grade 144A securities.\nFTSE NAREIT All Equity REITS Total Return Index The index is a free-float-adjusted market capitalization-weighted index that includes all tax-qualified REITs listed in the NYSE, AMEX, and NASDAQ National Market.\nHFRX Global Hedge Fund Index The HFRX Global Hedge Fund Index is designed to be representative of the overall composition of the hedge fund universe. It is comprised of all eligible hedge fund strategies, including but not limited to convertible arbitrage, distressed securities, equity hedge, equity market neutral, event driven, macro, merger arbitrage, and relative value arbitrage. The strategies are asset weighted based on the distribution of assets in the hedge fund industry.\nMSCI All Country World Index Captures large and mid cap representation across 23 Developed Markets (DM) and 26 Emerging Markets (EM) countries. With 3,050 constituents, the index covers approximately 85% of the global investable equity opportunity set.\nS&P GSCI Total Return Index The S&P GSCI Total Return Index in USD is widely recognized as the leading measure of general commodity price movements and inflation in the world economy. The index is calculated primarily on a world production-weighted basis comprised of the principal physical commodities futures contracts.\nSpot Gold Price The price in spot markets for gold. In US dollar terms, spot gold is referred to with the symbol “XAU,” which refers to the price of one troy ounce of gold in USD terms.\nS&P Global Infrastructure Net Total Return Index The S&P Global Infrastructure Index is designed to track 75 companies from around the world chosen to represent the listed infrastructure industry while maintaining liquidity and tradability. To create diversified exposure, the index includes three distinct infrastructure clusters: energy, transportation, and utilities.\nS&P Global Natural Resources Total Return Index The index includes 90 of the largest publicly-traded companies in natural resources and commodities businesses that meet specific investability requirements, offering investors diversified, liquid and investable equity exposure across 3 primary commodity-related sectors: Agribusiness, Energy, and Metals & Mining.\nS&P 500® Index The S&P 500 is widely regarded as the best single gauge of large-cap U.S. equities and serves as the foundation for a wide range of investment products. The index includes 500 leading companies and captures approximately 80% coverage of available market capitalization.\nThe information provided does not constitute investment advice and it should not be relied on as such. It should not be considered a solicitation to buy or an offer to sell a security. It does not take into account any investor's particular investment objectives, strategies, tax status or investment horizon. You should consult your tax and financial advisor.\nThe whole or any part of this work may not be reproduced, copied or transmitted or any of its contents disclosed to third parties without SSGA’s express written consent.\nCommodities and commodity-index linked securities may be affected by changes in overall market movements, changes in interest rates, and other factors such as weather, disease, embargoes, or political and regulatory developments, as well as trading activity of speculators and arbitrageurs in the underlying commodities.\nInvesting in commodities entails significant risk and is not appropriate for all investors.\nDiversification does not ensure a profit or guarantee against loss.\nFor EMEA Investors: The information contained in this communication is not a research recommendation or ‘investment research’ and is classified as a ‘Marketing Communication’ in accordance with the Markets in Financial Instruments Directive (2014/65/EU) or applicable Swiss regulation. This means that this marketing communication (a) has not been prepared in accordance with legal requirements designed to promote the independence of investment research (b) is not subject to any prohibition on dealing ahead of the dissemination of investment research.\nETFs trade like stocks, are subject to investment risk, fluctuate in market value and may trade at prices above or below the ETFs net asset value. Brokerage commissions and ETF expenses will reduce returns.\nStandard & Poor's®, S&P® and SPDR® are registered trademarks of Standard & Poor's Financial Services LLC (S&P); Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC (Dow Jones); and these trademarks have been licensed for use by S&P Dow Jones Indices LLC (SPDJI) and sublicensed for certain purposes by State Street Corporation. State Street Corporation's financial products are not sponsored, endorsed, sold or promoted by SPDJI, Dow Jones, S&P, their respective affiliates and third party licensors and none of such parties make any representation regarding the advisability of investing in such product(s) nor do they have any liability in relation thereto, including for any errors, omissions, or interruptions of any index.\nDistributor: State Street Global Advisors Funds Distributors, LLC, member FINRA, SIPC, an indirect wholly owned subsidiary of State Street Corporation. References to State Street may include State Street Corporation and its affiliates. Certain State Street affiliates provide services and receive fees from the SPDR ETFs. ALPS Distributors, Inc., member FINRA, is the distributor for DIA, MDY and SPY, all unit investment trusts. ALPS Portfolio Solutions Distributor, Inc., member FINRA, is the distributor for Select Sector SPDRs. ALPS Distributors, Inc. and ALPS Portfolio Solutions Distributor, Inc. are not affiliated with State Street Global Advisors Funds Distributors, LLC.\nTHIS SITE IS INTENDED FOR QUALIFIED INVESTORS ONLY.\nNo Offer/Local Restrictions\nNothing contained in or on the Site should be construed as a solicitation of an offer to buy or offer, or recommendation, to acquire or dispose of any security, commodity, investment or to engage in any other transaction. SSGA Intermediary Business offers a number of products and services designed specifically for various categories of investors. Not all products will be available to all investors. The information provided on the Site is not intended for distribution to, or use by, any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation.\nAll persons and entities accessing the Site do so on their own initiative and are responsible for compliance with applicable local laws and regulations. The Site is not directed to any person in any jurisdiction where the publication or availability of the Site is prohibited, by reason of that person's nationality, residence or otherwise. Persons under these restrictions must not access the Site.\nInformation for Non-U.S. Investors:\nThe products and services described on this web site are intended to be made available only to persons in the United States or as otherwise qualified and permissible under local law. The information on this web site is only for such persons. Nothing on this web site shall be considered a solicitation to buy or an offer to sell a security to any person in any jurisdiction where such offer, solicitation, purchase or sale would be unlawful under the securities laws of such jurisdiction.\nBefore investing, consider the funds' investment objectives, risks, charges and expenses. To obtain a prospectus or summary prospectus which contains this and other information, call 1-866-787-2257, download a prospectus or summary prospectusnow, or talk to your financial advisor. Read it carefully before investing.\nNot FDIC Insured * No Bank Guarantee * May Lose Value""]"	['<urn:uuid:8074c8dd-940e-423e-ba27-9b7b26fbde95>', '<urn:uuid:e2f66c96-ce1a-40da-b5f7-dbe1f1f7ca64>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	32	94	3090
53	early earth chemical life formation prospects challenges	The emergence of life on Earth appears promising as it started almost immediately once conditions allowed it 3.8 billion years ago, and the building blocks of life are more common than previously thought. However, significant challenges exist in the prebiotic chemistry, as laboratory reactions that work well would have been frustrated under early Earth conditions due to numerous interfering side reactions and the presence of contaminants like ammonia and amino acids.	"[""The Scientific Quest Continues\n“What problem with prebiotic Earth was solved by the emergence of life?”—D. Eric Smith\nCould the answer lie in the flow of energy?\nLightning is the Earth’s solution to the problem of regulating electrical energy as it builds up in the atmosphere. Hurricanes solve the problem of releasing heat energy that builds up in warm tropical oceans. In the same way, the emergence and evolution of life may be the Earth’s most efficient solution to the problem of dissipating the energy constantly generated inside it.\nUltimately, origin-of-life research could reveal the extent to which the emergence of life and biological processes are governed by universal principles on the order of the laws of motion, gravity, and thermodynamics. It has already provided important insights into how biological systems produce and regulate growth and change.\nGlobal climate change\nClimate affects biology, and biology affects climate. Until recently, models for studying climate change have largely ignored energy flows through biological and ecological systems. Incorporating these interconnections will enhance the models.\nCures for diseases\nWhile studying life’s origin, Harold Morowitz and his colleague Vijayasarathy Srinivasan stumbled upon a clue to a medical mystery—how Mycobacterium tuberculosis bacteria survive for years inside the human respiratory system, imprisoned by the body’s immune cells and deprived of the oxygen and nutrients they normally depend on.\nTwo ancient genes possessed by the bacteria may provide the answer. The genes are crucial players in the reductive citric acid cycle, but in the oxygen-rich environment in which the bacteria normally live, only the oxidative citric acid cycle would be needed. Why then have these genes persisted? The answer could be that they’re the organism’s secret weapon, allowing it to switch back to the reductive cycle when trapped inside the body. If correct, this could open new avenues to therapies for tuberculosis and other diseases.\nLife on other planets\nShedding light on the emergence of life on Earth will help us come up with an answer to another question with profound implications: Is there life elsewhere in the universe?\n• The building blocks of life are easier to make and more common than once thought.\n• Many scientists believe that life emerged on Earth almost immediately once the environment allowed it around 3.8 billion years ago.\n• Life forms on Earth have been found to thrive under a wide range of extreme conditions.\n• As the number of planets discovered around other stars grows rapidly, it appears increasingly likely that Earth-like conditions, extreme or not, are common throughout the universe.\nCould this mean that life on Earth and throughout the universe is inevitable?\nHarold Morowitz | view bio\nMorowitz, a biophysicist at the Santa Fe Institute and George Mason University’s Krasnow Institute, is preparing laboratory experiments to test the catalytic properties of transition metal-ligand complexes common when life began.\nD. Eric Smith | view bio\nMorowitz’s colleague D. Eric Smith is using advanced statistical mathematics and computational modeling to determine the chemical pathways on early Earth most likely to have led to self-replicating and metabolic processes.\nPenny J. Boston | view bio\nPenny J. Boston, a speleologist at the New Mexico Institute of Mining and Technology, is conducting research on the geomicrobiology of caves and mines, astrobiology, and extraterrestrial speleogenesis.\nRobert M. Hazen | visit website\nThe role of minerals in the origin of life is the focus of recent research by Earth scientist Robert M. Hazen at George Mason University and the Carnegie Institution’s Geophysics Laboratory.\nGeorge Cody and colleagues in the Geophysics Laboratory of the Carnegie Institution of Washington, D.C., are performing experiments at high temperatures and pressures on potentially prebiotic chemistry at high temperatures and pressures to see how chemical reactions could have produced the reductive citric acid cycle at thermal vents.\nAt the University of Illinois Urbana-Champaign, physicist Nigel Goldenfeld and Carl Woese (discoverer of the archaea), molecular biology and evolution researcher were working with colleagues to investigate the role of horizontal gene transfer, or gene sharing, in producing the Universal Genetic Code.\nRogier Braakman | visit website\nBraakman is an Omidyar Fellow at the Santa Fe Institute, studying the logic and evolution of early bacterial metabolism and the origin of life using computational and theoretical methods.\nGeochemist Michael Russell, a research scientist in the Planetary Chemistry and Astrobiology Group at NASA’s Jet Propulsion Laboratory, is investigating the emergence of life and oxygenic photosynthesis in hydrothermal systems on wet, rocky, sunlit planets.\nAriel D. Anbar | view bio\nAriel D. Anbar, a biogeochemist in the School of Earth & Space Exploration and the Department of Chemistry & Biochemistry at Arizona State University, is interested in the past and future evolution of the Earth as a habitable planet and how this can inform the search for inhabited worlds beyond Earth.\nShelley Copley | view bio\nAt the University of Colorado, with the goal of developing a model for the origin of the genetic code, Shelley Copley and her research group are investigating precisely how RNA may have evolved from early chemical reaction networks.\nAt the University of Illinois Urbana-Champaign, Zaida Luthey-Schulten and her group are performing artificial life research that includes computer simulations of ribosomes, the cell's protein-building machinery; the differences in ribosome structure in the three main branches of the Tree of Life; and their role in the early evolution of protein synthesis.\nThe goal of other artificial life researchers is to synthesize simple living organisms in the laboratory. At Los Alamos National Laboratory, Steen Rasmussen is attempting to assemble a proto-cell.\nLarry S. Crumpler | view bio\nCrumpler is a research curator at the New Mexico Museum of Natural History & Science, studying New Mexico volcanoes as well as geology and volcanism on other planets. He is a member of the Mars Exploration Rover science team."", 'I remember watching Dudley Do-Right on Saturday mornings as a kid. A member of the Canadian Mounties, this cartoon hero displayed more heroism than smarts as week after week he took on his archenemy Snidley Whiplash. With sheer luck on his side, Dudley (and his horse named Horse) somehow managed to rescue his love interest Nell Fenwick from Whiplash\'s dastardly schemes.\nRecent work by chemists from the University of Manchester (U.K.) appears to have rescued one of the leading ideas for the origin of life, the RNA world hypothesis, from certain doom. Instead of relying on luck, these researchers applied ""out-of-the-box"" thinking. Through painstaking laboratory experiments, they discovered a novel, straightforward chemical route to one of the key building block materials required by the RNA world hypothesis.\nThis work not only makes an evolutionary explanation for the beginning of life more plausible, it also opens up a new avenue of research for the origin-of-life community. But, as exciting as this breakthrough appears, does it really bailout the RNA world scenario?\nMany origin-of-life investigators think that RNA was the first replicator and information-harboring molecule, predating both DNA and proteins. As such, RNA took on the contemporary biochemical function of both DNA and proteins by operating as a self-replicator that catalyzed its own synthesis. According to the RNA world hypothesis, over time numerous RNA molecules possessing a wide range of catalytic activity emerged. Eventually, this environment transitioned to an RNA-protein world that–with the addition of DNA–finally gave way to contemporary biochemistry. (For a more detailed discussion on the central importance of the RNA world hypothesis to the origin-of-life paradigm, please see here.)\nValidating the RNA World Hypothesis\nIn order to substantiate the RNA world scenario researchers need to establish the validity of several processes. These include:\n- reasonable prebiotic chemical routes that will generate the building blocks (nucleobases, ribose, and phosphate) of RNA;\n- reasonable prebiotic routes that will assemble these building blocks into ribonucleotides;\n- a reaction scheme that will chemically activate the ribonucleotides;\n- reasonable prebiotic routes that will assemble RNA from its building blocks into molecular chains long enough to form ribozymes.\nChallenges Facing the RNA World Hypothesis\nScientists have identified possible routes to make ribose and the nucleobases, but these pathways have questionable relevance for the origin of life. While the reactions work well in the laboratory, the conditions of early Earth would have frustrated such processes.\nFor example, the only known plausible prebiotic route to ribose (and all sugars) is the Butlerow reaction (also known as the formose reaction). This reaction begins with the one-carbon compound formaldehyde, which researchers think would have been present on early Earth. In the presence of an inorganic catalyst (calcium hydroxide, calcium oxide, alumina clays, and so on), formaldehyde reacts with itself. The resultant products generate two, three, four, five, six, or more carbon sugars.\nThough this route to ribose and other sugars exists, most researchers question its applicability to the origin-of-life scenario. Numerous side reactions dominate formose chemistry. As a consequence, this reaction yields over forty different sugar species with ribose as a minor component. If this reaction did operate on early Earth, it could never have yielded enough ribose to support an RNA world. Additionally, laboratory formose reactions are free of contaminants that would likely be present on early Earth. Ammonia, amines, and amino acids, for example, react with formaldehyde and the products of the formose reaction. On primordial Earth these side reactions would have consumed key reactants and frustrated the formation of ribose and other sugars.\nDecomposition negatively affects ribose formation. Sugars decompose under alkaline and acidic conditions and are susceptible to oxidation. Even within a neutral pH range, sugars decompose. At 212 °F (100 °C), under neutral conditions, ribose\'s half-life is seventy-three minutes. At 32 °F (0 °C), ribose has a half-life of forty-four years.\nSimilar problems confront origin-of-life researchers as they attempt to account for the genesis of the nucleobases. For example, chemists have discovered two possible pathways that produce cytosine. One route involves a reaction between cyanoacetylene and cyanate, and the other reaction begins with cyanoacetalydehyde and urea. These four compounds represent essential ingredients of early Earth\'s supposed prebiotic soup.\nChemist Robert Shapiro demonstrated, however, that the two chemical routes lack any relevance. He points out the unlikelihood that cyanoacetylene, cyanate, cyanoacetaldehyde, and urea existed at sufficient levels on primordial Earth to effectively produce cytosine. Even if these compounds had occurred at appropriate levels, interfering chemical reactions would have consumed them quickly before cytosine could form. Cyanoacetylene rapidly reacts with ammonia, amines, thiols, and hydrogen cyanide. Cyanate undergoes rapid reaction with water. In the presence of water, cyanoacetaldehyde decomposes into acetonitrile and formate. When cytosine does form, it rapidly decomposes. At room temperature and with a neutral pH, cytosine breaks down, losing half its molecules in 340 years. At 32 °F (0 °C), its half-life is seventeen thousand years.\nAnd even if a ready supply of ribose, nucleobases, and phosphates were present on early Earth, these compounds won\'t react with each other to form ribonucleotides.\nThe weight of such problems is so burdensome that I heard the late Leslie Orgel actually say at a scientific conference that ""it would be a miracle if a strand of RNA ever appeared on the early Earth.""\nTraditionally, researchers divided possible prebiotic reactions into those that lead to sugars and those that yield nucleobases. Then they tried to find a way for the two products to eventually form ribonucleotides. But the Manchester chemists took a different tact. They looked for ways that the two prebiotic routes could intermingle.\nThis novel approach led to a breakthrough. The researchers discovered that activated ribonucleotides could readily form in the laboratory in just a few simple steps. The reaction of cyanamide and glycoaldehyde to form 2-amino-oxazole comprises a key to this reaction sequence. (Cyanamide is a material traditionally viewed as part of the chemical pathway to some of the nucleobases and glycoaldehyde is the first product in the formose reaction.) In turn, 2-amino-oxazole reacts with glyceraldehyde (formed when glycoaldehyde reacts with formaldehyde) to form a sugar derivative, pentose amino-oxazoline. This compound reacts with cyanoacetylene (one of the starting materials in the prebiotic synthesis of cytosine) to generate anhydroarabinonucleoside, which can react with pyrophosphate and urea to form an activated ribonucleotide. The new ribonucleotide is poised to react with other activated ribonucleotides to form RNA chains.\nAs promising as this chemistry is, the researchers noted a serious problem. In unbuffered reactions (in which the pH isn\'t controlled), a large number of unwanted products result at each step in the pathway. As a consequence, 2-aminooxazole is present in the system only at low levels. The byproducts interfered with the remainder of the pathway and thwarted the generation of activated ribonucleotides.\nIn the face of this issue, the chemists discovered that including phosphate in the reaction mixtures eliminated many of the unwanted byproducts. Phosphate functions as both a catalyst and a buffer, controlling the pH of the reaction mixture. In other words, one of the key reactants in the last stage of the chemical route plays a role in earlier reactions, facilitating the production of ribonucleotides. The team also found that exposing the final reaction mixture to UV radiation selectively destroys unwanted byproducts as well, helping to increase the relative amount of ribonucleotides in the final product mixture.\nInstead of thinking about sugar and nucleobase chemistries as separate, scientists from Manchester allowed the two chemistries to intermingle. This conceptual breakthrough allowed them to discover a very simple chemical route to produce activated ribonucleotides, which are chemically complex materials. In addition, the chemists allowed reactants from the final steps of the chemical pathway to intermingle with reactants in the early stages of the process. This, too, represents a conceptual advance in origin-of-life studies, and provides a reasonable way to ""clean up"" unwanted side reactions that otherwise would interfere with the production of the ribonucleotides. The use of UV radiation to ""purify"" the final product, too, is reasonable. UV radiation would have impinged on early Earth.\nWithout question, these chemists have made an important contribution to the evolutionary paradigm. Their work paves the way for others to approach problems in prebiotic chemistry in an unconventional way that might lead to other key advances. Still, have they really rescued the RNA world hypothesis?\nNext time I will summon the help of another Saturday morning cartoon legend to demonstrate how this work actually turns the tables on chemical evolution and unwittingly provides support for intelligent design.\n|Part 1 | Part 2|']"	['<urn:uuid:fc6c5a1f-3165-4547-ad30-dc6a95a1dc16>', '<urn:uuid:6af678f2-8ba8-4afe-b3d9-71d79130bcef>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	7	71	2383
54	pregnancy safety chamomile side effects	While chamomile is generally considered safe for pregnant women, it should be used with caution as it may lead to miscarriage, especially when consumed as essential oil or tincture. Both German and Roman chamomile can cause allergic reactions in people allergic to plants in the Asteraceae family, with symptoms including stomach cramps, swollen lips and eyes, itching, hives, and throat tightness.	"['- German Chamomile Scientific Name\n- Other Names for German Chamomile\n- German Chamomile Description\n- German Chamomile Cultivation\n- Matricaria recutita Range of Distribution\n- Matricaria recutita Uses\n- Matricaria recutita Health Benefits\n- Dosage of Matricaria recutita\n- Side Effects of Matricaria recutita\n- Profile of German Chamomile in Homeopathy\n- German Chamomile Drug Interactions\n- Where to buy German Chamomile\n- German chamomile pictures\nGerman chamomile is an annual plant belonging to the Asteraceae family. The flower is known for its medicinal properties. The flower is grown in many parts of the world, such as Europe and Asia.\nGerman Chamomile Scientific Name\nThe scientific name for this plant is “Matricaria recutita”.\nOther Names for German Chamomile\nThe flower is also known by several other names such as\nPicture 1 – German chamomile\n- Matricaria suaveolens\n- Matricaria chamomilla\n- Chamomilla recutita\n- Chamomilla chamomilla\n- Blue Chamomile\n- Camomille Allemande\n- Echte Kamille\n- Fleur de Camomile\n- Hungarian Chamomile\n- Kleine Kamille\n- Manzanilla Alemana\n- Matricariae Flos\n- Pin Heads\n- Sweet False Chamomile\n- True Chamomile\n- Wild Chamomile\nGerman Chamomile Description\nThe German Chamomile plant has small and dainty flowers that look much like daisies. These flowers are arranged in paniculate capitula. The white ray-like petals are all spread around a raised disc-shaped yellow center. The florets are equipped with a ligule. The width of a petal is less than one inch. The flower receptacle is swollen, hollow and lacking in scales. The stems on which the flowers grow are long, smooth and thin and light green in color. The stem has several branches. The narrow and long leaves are tripinnate or bipinnate. Sometimes these plants can grow near the ground and at other times they can be found in herb gardens. The plant can reach up to a height of 3 feet. The flowers of this plant have a sweet fragrance. German chamomile plant has a fibrous root system.\nGerman Chamomile Cultivation\nGerman Chamomile seeds require light for germination. Hence it is a delicate process to grow these plants from seed. Generally direct seeding is used to cultivate these plants. For proper growth and survival, the seeds need to be grown on open soil. It is important that seeds of superior quality are sown for vegetative propagation. The growing season lasts for two months. The seeds take one or two weeks to germinate. Seeds can be planted outdoors around the month of August. Alternatively, these plants can be grown initially in propagation trays around March. The number of seeds per gram of soil should range between 10,000 and 18,000. After the plants reach a certain level of maturity, they can be shifted to outdoors. Spraying chamomile tea on the plants aids in calcium absorption from the compost piles where they are planted. When they are grown outdoors, the seeds can be spread gently around some good soil. Individual plants should have a space of six inches between them. The plants are sturdy and strong after being firmly established. The plant can be grown in outdoors for gardening, landscaping and mass planting and can also be grown out of containers. It is however, advisable not to grow these plants in containers.\nThe plant can be grown on a wide variety of soils. However, a well-drained and sandy soil is preferred for growing German chamomile. The pH content of the soil should be between 7.0 and 7.5. The plant should have access to plenty of sunlight for healthy development. The plant needs to be watered regularly. However, care should be taken not to overdo the watering process.\nThe soil used for this plant should be tested before plantation. Depending on the results of the tests, certain minerals like phosphorus, nitrogen and potassium may need to be added. The major nutrients required by this plant for proper growth and development are as follows:\n- Carbon (C)\n- Oxygen (O)\n- Hydrogen (H)\n- Phosphorus (P)\n- Potassium (K)\n- Nitrogen (N)\n- Sulfur (S)\n- Calcium (Ca)\n- Magnesium (Mg)\nThe best time to grow these plants is in autumn or during earlier months after winter. The rate of germination depends on when the seeds are sown. Plants which are sown late will have a lower germination rate. As a result, these plants will develop less number of flower stalks and growth of flowers will be less.\nMatricaria recutita Range of Distribution\nThe German chamomile plant is native to a wide variety of regions all across the globe. Commercially, these plants are grown in\n- All over Europe, especially in countries like Hungary, France, Germany, Italy, Greece and Spain\n- Parts of former USSR, such as Ukraine, Belarus and Moldova\n- North African countries like Egypt, Morocco and Ethiopia\n- South Africa (Free State, Lesotho, Eastern Cape, KwaZulu-Natal, Gauteng and Mpumalanga provinces)\n- Temperate regions of Asia, comprising of countries such as North India, Pakistan, Afghanistan, Japan and Turkey\n- East Coast of North America\n- South American countries like Brazil and Argentina\n- Australia and New Zealand\nThe Matricaria recutita flower plant is also grown in the icy regions ranging between North Caucasus and South Siberia.\nMatricaria recutita Uses\nThe German chamomile is extensively used for its positive health effects. It has antispasmodic, anti-cholesterol, anti-inflammatory, anxiolytic and antimutagenic properties. The plant includes the following chemical components:\n- Volatile oils like Bisabolol oxides, α-Bisabolol and Matricine\n- Flavonoids including quercitin and apegenin\nThe flower extract of this plant is used to make essential oils, ointments and capsules. The dried herb of this plant is also used to make chamomile tea. Chamomile tea prevents the growth of fungus. It is thus often spread over seedlings to prevent damp and fungus. The plant is also used for cosmetic purposes. It is used to prepare a hair rinsing solution for blonde hair. In agriculture, this plant is known to facilitate the growth and overall health of plants used for the production of essential oils. The plant helps in increasing the production of these essential oils. It strengthens the scent and flavor of herb plants like basil, oregano, sage and spearmint. This plant is also used to manufacture herb beers.\nGerman chamomile is also used in aromatherapy. The sweet fragrance of these flowers is believed to elevate the mood and bring relaxation to a person’s mind and body. Research shows that chamomile has a calming effect on our brain waves. Chamomile oil is used in aromatherapy to treat insomnia and sleep disorders.\nMatricaria recutita Health Benefits\nThe Matricaria recutita plant is popular for its beneficial health aspects. The plant is used to treat various health problems, such as\n- Gum inflammation (gingivitis)\n- Sore throats\n- Chest colds\n- Stomach ulcers\n- Inflammatory bowel disease (ulcerative colitis)\n- Minor first-degree burns\n- Children health issues like chickenpox, colic and diaper rash\nApart from the above mentioned health issues, this plant also offers relief from inflammation. It facilitates a speedy recovery of wounds.\nIt offers relief from anxiety and calms the nerves. The plant extracts act as a sedative, helping us to have a good night’s sleep.\nAntispasmodic properties of this plant help in reducing muscle spasms. It also relaxes the intestinal muscles and gives relief from diarrhea, indigestion, stomach cramps, gas and irritable bowel syndrome.\nExtracts from German chamomile are used to make creams or ointments which have a soothing effect on the skin. In European countries, it is often used to this effect and also to treat eczema. It has strong tissue regenerating properties.\nThe oil of this plant helps in reduction of swelling. It also prevents the growth of viruses, bacteria and fungi. The oil is also an important antidote to premenstrual syndrome (PMS) as well as other menopausal and menstrual problems such as menstrual cramps.\nThe German chamomile oil has the following therapeutic properties:\nResearch on the benefits of German chamomile has also been conducted on rats. The results indicate that the plant aids in reduction of body weight.\nDosage of Matricaria recutita\nAdults can drink the chamomile tea 3 or 4 times in a day between meals. The tea should be prepared in the following way: Two to four grams of dried chamomile herb should be added on a cup of simmering water and kept for around 10 to 15 minutes.\nCapsules with strength ranging between 300 to 400 mg should be taken thrice daily.\nLiquid chamomile extract can be consumed up to 1 to 1.5 teaspoons thrice daily.\nAround 100 to 150 drops of chamomile tincture containing 45% alcohol can be mixed in hot water and consumed three times daily.\nChamomile tea can be cooled and used for gargling. An oral rinse may be prepared by adding 10 to 15 drops of chamomile liquid in 100 milliliters of moderately hot water. The oral rinse may be used thrice daily.\nA few drops of chamomile essential oil can be added to hot water. The steam emitting from this mixture can be inhaled to soothe a cough.\nA bath using chamomile oil can give relief from hemorrhoids and insect bites. This can soothe the skin and heal cuts and eczema. The bath can be prepared by adding 5 to 10 drops of chamomile oil or a quarter of a pound of dried flowers to a tub of water.\nPowdered chamomile herb can be mixed with water to create a paste. This paste can then be applied to heal skin inflammations and infections.\nA cream containing 3 to 10% of crude drug chamomile can be applied as an antidote to eczema, psoriasis or flaky and dry skin.\nFor children under the age of 18, the dosage is half of the adult dose. To deal with pediatric problems or with colic, chamomile tea can be consumed daily. The consumption limit for children however, should not exceed more than half cup or 1-2 ounces of chamomile tea.\nThe dosage for chamomile supplements, like any other herbal nutrition supplements, should be as per the recommendations made by doctors. The dosage varies, depending on the medicinal purposes for which it is used.\nSide Effects of Matricaria recutita\nGerman chamomile is widely used by people to avail its therapeutic qualities. However, certain side effects can be triggered by the use of this oil. The oil when used can exaggerate certain health issues. The oil can also react badly with certain other medications, herbs or supplements. It is thus advisable to consult a doctor while consuming chamomile and its products.\nAsthma patients should avoid consuming chamomile as it may worsen the existing asthmatic symptoms.\nChamomile oil and extracts are generally considered safe for pregnant and lactating women. However, a doctor needs to be consulted to supervise the dosage or if it is suitable at all for the pregnant woman to drink chamomile tea or use chamomile products during her pregnancy. This is because consumption of chamomile oil may lead to miscarriage in pregnant women. Especially, chamomile should not be consumed at this stage as an essential oil or a tincture preparation.\nVomiting may be induced by drinking too much of chamomile tea of high concentrations.\nConsumption of chamomile may lead to allergic reactions for people who are already allergic to other Asteraceae species such as daisies, asters, ragweed or chrysanthemums.\nProfile of German Chamomile in Homeopathy\nGerman chamomile acts as a good homeopathic remedy for people who find it difficult to cope up with pain. Some individuals may find it difficult to have control over their emotions and might often experience inner conflict, hostility and irritation. They might experience unpredictable moods, a hot temper and an active dislike of being touched. They feel stressful if they feel like their space is being invaded or if someone interferes about their matters. Often such behavior is displayed by children and such conditions are medically termed as Attention Deficit Hyperactivity Disorder or ADHD. German chamomile can give relief from such stressful mental conditions.\nGerman Chamomile Drug Interactions\nThe German chamomile plant or its extracts can react adversely when it interacts with certain other drugs. Hence users of these drugs should consult a physician before consuming chamomile oil, herbs, flowers or ointments.\nThe following is a list of drugs with which the chamomile plant or its extracts can have a negative reaction:\n1. Sedatives or drugs having a sedative effect, such as\n- Anticonvulsants, such as Valproic acid (Depakote) and Phenytoin (Dilantin)\n- Benzodiazepines, such as Diazepam (Valium) and Alprazolam (Xanax)\n- Drugs meant for the treatment of insomnia, for e.g. Zaleplon (Sonata), Ramelteon (Rozerem), Eszopiclone (Lunesta) and Zolpidem (Ambien)\n- Tricyclic antidepressants like Amitriptyline (Elavil)\n- Sedative herbs such as catnip, valerian and kava\n2. Certain liver enzymes decompose Chamomile as well as other drugs. Chamomile can enter into a reaction with such drugs.\n- Birth control pills\n- Antihistamine drugs like Fexofenadine\n- Statins, which help in lowering the cholesterol levels of the body\n- Certain antifungal drugs\nWhere to buy German Chamomile\nGerman chamomile essential oils and ointments can be found in any aromatherapy shop.\nGerman chamomile capsules are found in retail medicine and healthcare shops.\nChamomile flowers are available in shops packed in pouches. The flowers can be bought from florists. Alternatively, they can be ordered from flower shops for home delivery.\nBeverages made from this plant such as chamomile tea can be found in grocery stores selling tea. Herb beers can be found in any alcohol shop selling beers.\nThe various chamomile products can also be brought online at reasonable prices.\nGerman chamomile pictures\nBelow are some pictures showing the German chamomile plant, with its flowers having white petals.\nThe German Chamomile plant has long been in use for its beneficial properties. In folk medicine, the plant has always been connected with female health. In fact, the word “Matricaria” essentially means “Caring for the womb”. Besides female health, its use in relieving mental stress, coupled with its sweet fragrance has made it a popular choice as herbal health supplement for millions.', ""Chamomile is an alternative herbal treatment for anxiety and tension, various digestive disorders, muscle pain and spasm, and menstrual cramps. Learn about the usage, dosage, side-effects of Roman Chamomile.\nBotanical Name:Chamaemelum nobile\nCommon Names: Roman Chamomile\n- Plant Description\n- What's It Made Of?\n- Available Forms\n- How to Take It\n- Possible Interactions\n- Supporting Research\nThere are two plants known as chamomile: the more popular German chamomile (Matricaria recutita) and Roman, or English, chamomile (Chamaemelum nobile). Both belong to the Asteraceae family, which also includes ragweed, echinacea, and feverfew. Both have been used traditionally to calm frayed nerves, to treat various digestive disorders, to relieve muscle spasms and menstrual cramps, and to treat a range of skin conditions (including minor first degree burns) and mild infections. Chamomile can also be found in a variety of face creams, drinks, hair dyes, shampoos, and perfumes.\nMost research on chamomile has been done with the closely related plant, German chamomile, which has similar, but not identical, active ingredients. Roman chamomile has not been used in studies of people as much as German chamomile, so claims about its use for specific health conditions are based on clinical experience and will have to be verified through future research. Nevertheless, Roman chamomile is an ingredient in many teas, ointments, and other types of medicinal preparations.\nTraditionally, Roman chamomile has been used to treat nausea, vomiting, heartburn, and excess intestinal gas that can happen when feeling nervous. It is widely valued for its tension-relieving properties. As legend has it, Peter Rabbit's mother used Roman chamomile tea to calm him down after his adventures in Mr. MacGregor's garden. This herb may also reduce inflammation associated with cuts or hemorrhoids, and may ease the discomfort associated with conditions such as eczema and gingivitis (swollen gums). The traditional uses of Roman chamomile, again while not studied scientifically are quite similar to the uses for German chamomile.\nRoman chamomile originates in northwestern Europe and Northern Ireland, where it creeps close to the ground and can reach up to one foot in height. Gray-green leaves grow from the stems, and the flowers have yellow centers surrounded by white petals, like miniature daisies. It differs from German chamomile in that its leaves are thicker and it grows closer to the ground. The flowers smell like apples.\nChamomile teas, ointments, and extracts all start with the white and yellow flower head. The flower heads may be dried and used in teas or capsules or crushed and steamed to produce a blue oil, which has medicinal benefits. The oil contains ingredients that reduce swelling and may limit the growth of bacteria, viruses, and fungi.\nRoman chamomile is available as dried flowers in bulk, tea, tinctures, and in creams and ointments.\nThere are no known scientific reports regarding the appropriate pediatric dose of Roman chamomile. For this reason, children should not take this herb.\nRoman chamomile can be taken a number of ways. A cup of hot chamomile tea may help soothe an upset stomach or help those who suffer from insomnia. The oral dosages listed below should help relieve stomach discomfort; chamomile has also been used for reducing menstrual pain and the swelling of gums in the case of gingivitis. The ointment and bath recommendations are for skin conditions.\n- Tea: Pour one cup of boiling water over 1 heaping tablespoon of dried herb, steep 10 to 15 minutes.\n- Liquid extract (1:1, 70% alcohol) 20 to 120 drops, three times per day\n- Bath: Add two teabags or a few drops of Roman chamomile essential oil to a full tub of bathwater to soothe hemorrhoids or skin problems\n- Cream/Ointment: Apply cream or ointment containing 3% to 10% chamomile content\nThe use of herbs is a time-honored approach to strengthening the body and treating disease. Herbs, however, contain active substances that can trigger side effects and interact with other herbs, supplements, or medications. For these reasons, herbs should be taken with care, under the supervision of a practitioner knowledgeable in the field of botanical medicine.\nChamomile is considered generally safe by the FDA. Roman chamomile contains an ingredient, anthemic acid, which can induce vomiting if taken in high doses. Highly concentrated tea may therefore cause vomiting.\nThose who are allergic to ragweed or other plants in the Asteraceae family (including echinacea, feverfew, and chrysanthemums) should avoid chamomile. Allergic reactions are somewhat common, actually, and may include stomach cramps, tongue thickness, swollen lips and eyes (called angioedema), itching, hives, throat tightness, and even shortness of breath. The latter two symptoms are medical emergencies and medical care should be sought urgently.\nIf you are currently being treated with any of the following medications, you should not use Roman chamomile without first talking to your healthcare provider.\nBecause of its calming effects, chamomile should not be taken in conjunction with sedative medications (particularly those that belong to a class called benzodiazepines such as alprazolam and lorazepam) or alcohol.\nPatients taking blood-thinning medications such as warfarin should use Roman chamomile only under the careful supervision of a healthcare practitioner. Although not proven scientifically, this herb may, in theory, enhance the effects of the medication.\nback to: Herbal Treatments Homepage\nBlumenthal M, ed. The Complete German Commission E Monographs. Boston, Mass: Integrative Medicine Communications; 1998:320-321.\nBriggs CJ, Briggs GL. Herbal products in depression therapy. CPJ/RPC. November 1998;40-44.\nCauffield JS, Forbes HJM. Dietary supplements used in the treatment of depression, anxiety, and sleep disorders. Lippincott's Primary Care Practice. 1999;3(3):290-304.\nErnst E, ed. The Desktop Guide to Complementary and Alternative Medicine: An Evidence-Based Approach. New York, NY: Mosby;2001:110-112.\nFoster S, Tyler VE. Tyler's Honest Herbal. New York, NY: The Haworth Herbal Press; 1999:105-108, 399.\nHeck AM, DeWitt BA, Lukes AL. Potential interactions between alternative therapies and warfarin. Am J Health Syst Pharm. 2000;57(13):1221-1227.\nLeung A, Foster S. Encyclopedia of Common Natural Ingredients Used in Food, Drugs, and Cosmetics. 2nd ed. New York, NY: Wiley & Sons; 1996.\nMcGuffin M, Hobbs C, Upton R, Goldberg A. American Herbal Products Associations's Botanical Safety Handbook. Boca Raton, Fla: CRC Press; 1996:27.\nMiller L. Herbal medicinals: selected clinical considerations focusing on known or potential drug-herb interactions. Arch Intern Med. 1998;158(20):2200-2211.\nNewall CA, Anderson LA, Phillipson JD. Herbal Medicines: A Guide for Health Care Professionals. London, England: The Pharmaceutical Press; 1996:72 73.\nO'Hara M, Kiefer D, Farrell K, Kemper K. A review of 12 commonly used medicinal herbs. Arch Fam Med. 1998:7(6):523-536.\nRobbers JE, Tyler VE. Tyler's Herbs of Choice: The Therapeutic Use of Phytomedicinals. New York, NY: The Haworth Herbal Press;1999:69-71.\nRotblatt M, Ziment I. Evidence-Based Herbal Medicine. Philadelphia, Penn:Hanley & Belfus, Inc. 2002:119-123.\nback to: Herbal Treatments Homepage\nStaff, H. (2008, December 11). Roman Chamomile, HealthyPlace. Retrieved on 2021, December 4 from https://www.healthyplace.com/alternative-mental-health/herbal-treatments/roman-chamomile""]"	['<urn:uuid:ae6cdbc8-f064-4184-8c68-f3c87ef96ba0>', '<urn:uuid:c267a9fd-1e46-4b3b-9e77-8b4e97f28c58>']	factoid	direct	short-search-query	similar-to-document	three-doc	novice	2025-05-13T00:43:36.053682	5	61	3451
55	What charging approach is better: street charging or home charging?	Neither is definitively better - both serve different needs. Home charging is simple for those with dedicated parking and electrical service, while street charging is crucial for the 20% of Americans who are 'garage orphans' living in apartments or condominiums. Cities need both to meet clean air and carbon goals.	['Why Cities Should Support Right-of-Way Charging\nThroughout history, cities have invested in programs and infrastructure to support the efficient movement of people and goods. Once upon a time, this meant regularly cleaning up after horses and providing tethering rings. Over time, streetcar tracks ran down city streets. Then those were paved over to make way for cars to drive and park. As cities began focusing more on livability and embraced “new urbanism” approaches to revival, streetcar tracks began to return alongside dedicated bus lanes, bike racks, and dedicated bike lanes.\nCities now face a number of new demands on their public rights of way. It will take new forms of urban design and management to accommodate the next generation of mobility services like bike share systems, car sharing, micro-mobility services, and rideshare services. Broader smart city technologies like parking management, way-finding, and citizen engagement technologies will also factor into the design process.\nCities that are serious about tackling climate change are also recognizing that they need to electrify everything that moves. This includes private passenger cars, and requires making room for electric vehicle charging infrastructure in the public right of way.\nCities that have been most successful in promoting cycling, like my home town of Portland, have invested in bike racks, dedicated bike lanes, and other infrastructure that is ubiquitous, convenient, and highly visible. Likewise, cities that want to reduce air pollution and meet carbon goals by accelerating transportation electrification need to invest in electric vehicle charging infrastructure that is ubiquitous, convenient, and highly visible, using public rights of way.\nWhy Cities Should Support Right-of-Way Charging\nTo meet clean air and carbon goals.\nCities that are serious about reducing carbon pollution from transportation need to promote walking and biking, expand transit and micro-mobility services, manage development, and use pricing to reduce traffic and parking congestion.\nMany of these steps are designed to reduce the use of single occupancy cars. At the same time, though, cities will also need to electrify everything that moves, including those passenger cars. Just as our approach to solid waste requires a “reduce, reuse, recycle” approach, city transportation policy needs to pursue a “both-and” strategy. Making it easier to use an electric car does not conflict with encouraging alternative transportation options, any more than making it easier to recycle conflicts with discouraging single-use packaging.\nOut of sight, out of mind.\nResearch by UC Davis in California shows that most consumers do not realize that electric vehicles are available, and that public awareness has barely budged over the last six years. Highly visible charging on the street can be a very effective way to increase awareness. Cities that are serious about promoting cycling paint colorful bike lanes, replace car parking with large bike racks, and encourage drivers to “start seeing bicycles.” We need to help gas-car drivers “start seeing electric cars.”\nCare for your “garage orphans.”\nCharging at home is simple if you own a home with a dedicated parking spot and electrical service. However, some 20% of Americans, and an even higher percentage of low- and moderate-income drivers, are “garage orphans” who live in apartments or condominiums. Urban policy to promote sustainability and density are steadily increasing this percentage, while also steadily reducing the amount of parking provided in new construction. Public charging for these drivers will be increasingly important in cities.\nElectrify shared mobility.\nDelivery vehicles, carshare services, and drivers for rideshare services like Uber and Lyft can easily travel 50,000 miles a year or more, some five times more than an average commuter. If cities want these services to electrify, it is important to recognize that time is money for these drivers, that they will need to charge multiple times in a day, and will want to be able to do so as quickly as possible. They will need fast charging that is centrally located and easily accessed.\nRecommendations for Cities\nAs part of our work with multiple cities through the Bloomberg American Cities Climate Challenge, Forth has recently published a report that offers a toolkit for cities seeking to provide ubiquitous, convenient, highly visible, and affordable charging options in busy urban areas. Forth’s report includes a wealth of additional detail, but our recommendations begin with a few basic suggestions.\nStart with high level policy.\nTo meet our climate goals, all new cars sold must be electric within the next decade. Cities should start by adopting a policy recognizing this imperative and committing the city to support this transition. City departments can then develop specific policies and programs to meet this goal, but with clear direction and support from political leadership.\nThink bike racks, not gas stations.\nPublic electric vehicle charging generally won’t be profitable until there are far more electric cars using it, yet cities sometimes approach charging companies as if they were entering franchise negotiations with profitable technology companies, or regulating undesirable gas stations. To create the best environment for creative problem solving, cities should think of public charging as an important public service that they provide their citizens through public-private partnerships.\nTo make charging easier, cities will need to collaborate across internal departments, but will also need to work closely with their local electric utility and with private charging companies. Mobility technologies and needs are evolving quickly, creating new challenges and opportunities. Convening a cross-functional stakeholder group can be a great way to brainstorm ideas and to shape and evaluate programs.\nCharging can be integrated with high-efficiency lighting, mounted on power poles, combined with parking control systems, integrated into multimodal “smart mobility hubs,” or used to offer Wi-Fi and other smart city services. Some companies offer free charging services supported by advertising, others operate smart networks that generate valuable data, and others simply sell non-networked hardware. In short, there are a variety of technologies and business models that should enable cities to develop a strategy that supports their other mobility priorities.\nWhen a city is ready to proceed, it should create clear and simple parameters and processes to minimize the transaction costs for participants. Small pilot programs that are clear, fast, and well defined are nearly always better than sweeping plans that are vague and overly ambitious without implementation strategies behind them. Simple applications with the potential to request additional information are generally better than asking participants for comprehensive information up front. Sometimes, the most impactful thing a city can do is simply get out of the way by reducing permitting or planning barriers, or identifying specific locations for innovation.\nThink big, but start small.\nMeeting our carbon reduction goals will require all new cars sold in the US to be electric within the next decade or two. On the one hand, this means that cities need to be prepared to accommodate and charge a fleet that is 100% electric. On the other hand, it means that there is time to plan. Cities do not need to rush to tear up streets just to install charging. They can begin to incorporate charging into their long-term plans for the urban streetscape, incorporating charging naturally as streets are renovated along with lighting, sidewalks, bike lanes, and other aspects of modern urban mobility.\nIn many progressive cities, it is increasingly easy to find a well-marked bike lanes and convenient bike parking corrals in place of car parking spots. To address the climate crisis, cities need to make it just as easy to find electric car charging. A key element in that strategy will be making that charging available at street parking spaces. We hope our report provides helpful case studies and recommendations to cities as they innovate towards that goal.\nLeave your comment below, or reply to others.\nPlease note that this comment section is for thoughtful, on-topic discussions. Admin approval is required for all comments. Your comment may be edited if it contains grammatical errors. Low effort, self-promotional, or impolite comments will be deleted.\nRead more from MeetingoftheMinds.org\nSpotlighting innovations in urban sustainability and connected technology\nDedicated anti-trafficking actors across the nation are trying to build better systems in big jurisdictions like New York, San Francisco, and Los Angeles, and in smaller but scrappy jurisdictions like Waco, Texas and Boaz, Alabama. They all share the same need, for stronger interconnectedness as an anti-trafficking field, and more collaboration.\nThe Forging Freedom Portal is a one-stop shop where a police officer planning a victim-centered operation can connect with their law enforcement counterparts, and the right service providers ahead of time, collaborating to make sure they’re planning for the language skills, social services, and legal support that victims may need. The portal is a place where the people who care most about ending human trafficking, who are doing the hard work every day on the ground, can learn from each other and share best practices to raise the collective standard of this work.\nMaximizing both the mobility and safety of road users at urban and suburban intersections is of utmost importance to city leaders and citizens today. Trends such as micromobility, connected and automated vehicles, and an explosion of available data, coupled with increasing numbers of bikes and pedestrians on our streets, result in both challenges and opportunities.\nThe increasing ability to provide intersection connectivity, edge computing and cloud storage, along with growing tool sets, such as Signal Performance Measures (SPM) and advanced video detection, provide new and exciting opportunities to traffic engineers. Possible combinations of Vision Zero intersection solutions, Near-Miss analyses, and the ability to make real-time operating decisions at our intersections can be overwhelming. Still, they must be embraced to ensure public officials are accountable to the traveling public.\nI caught up with Joe Bergera – CEO of Iteris – recently and we discussed a cloud-first strategy for cities and the benefits, particularly during the pandemic. Organizations, cities and companies that have replicated some of their business processes in the cloud have navigated the pandemic quite well. We discuss why that is, and what can be done to help other cities during this time.', 'Frequently Asked Questions About Electric Vehicles\nWhat is the difference between a conventional hybrid, a plug-in hybrid, and a battery electric vehicle?\nConventional hybrids, such as the Toyota Prius, are primarily gas-powered cars that use electric motors to help out the internal combustion engine.\nPlug-in hybrids (PHEV), such as the Chevrolet Volt, have a battery-powered electric motor that can function independently with enough range for a daily commute. They also have a conventional gas engine to extend the vehicle’s range.\nBattery electric vehicles (BEV), such as the Nissan Leaf, are powered solely by a battery and electric motor.\nHow far will an EV go on a charge?\nToyota Prius Prime\nPlug-in hybrids (PHEVs) like the Toyota Prius Prime, Chevy Volt, and Kia Niro can generally travel 20-50 miles on one charge. The combined gas and electric ranges of PHEVs are between 460-590 miles.\nBattery electric vehicle (BEVs) like the Chevy Bolt, Nissan Leaf, and Honda Clarity can generally travel 80-240 miles on one charge.\nWhat if I need to travel further?\n- A) Plug-in- Hybrids (PHEVs) offer the green and fun driving experience of an EV on short trips with the convenience of extended range up to 400+ miles\n- B) Power up your Battery Electric Vehicle (BEV) at a Level 3 DC fast charger to replenish range up to 70-80% in 30-60 minutes.\nHow long do EV batteries last? Do they have warranties?\nThe usual battery replacement time is about 10 years. Battery replacement cost will depend on the size of the battery. Each manufacturer offers different battery warranties—for example, Nissan and Chevrolet offer an 8-year/100,000 mile warranty on their battery packs and Hyundai offers it for life. There will be some battery degradation over time that warranties do not cover.\nHow much does it cost to charge an EV?\nRCEA’s network of charging stations cost $0.50 per charging session + $0.25 per kWh.\nOther charging stations like those at casinos or hotels will have their own pricing schemes. Visit charging maps like Plugshare to see how much it will cost to charge at any nearby station.How long does it take to charge an EV?\nElectric vehicles take 10+ hours to recharge on a Level 1 charger. Level 2 chargers can fully charge an electric vehicle in 4 to 8 hours. Level 3 chargers, also known as DC fast chargers, are most often used by people traveling long distances and can charge an electric vehicle in 20 to 60 minutes.\nCharging Best Practice Tips (*)\n- Limit DC fast charging. Repeated DC fast charging to the high end of state of charge (past 80% or so) may accelerate battery degradation. Try to use DC fast charging to reach a very high state of charge only if needed to reach a destination. (*)\n- Charge small. Generally, many small charges are better for battery than a few larger charges. A deep discharge cycle constitutes a full chemical reversal inside the cell, whereas a shallow cycle is not a complete utilization of the chemical resources in the cell. (*)\n- Assess when to charge your battery. Deciding whether to leave your battery always plugged in depends on the charging system and the EV model. Some recommend it, to keep the battery thermal management system functioning optimally. If you have time-of-use pricing from your utility, you may wish to schedule charging to take advantage of low rates. In extreme weather, you may also approach things differently. (*)\nShould we leave the battery plugged in all day or only charge overnight? (*)\nLeaving your battery always plugged in depends on the charging system and the EV model. Some recommend it, to keep the battery thermal management system functioning optimally. If you have time-of-use pricing from your utility, you may wish to schedule charging to take advantage of low rates. In extreme weather, you may also approach things differently.\nHow do you find EV charging stations?\nThere are a number of free public charging apps available that could help you find an EV Station.\nHere are a few:\nWhat happens when your EV runs out of battery “juice”?\nIn some markets, AAA offers a mobile charging service from their trucks. In all other markets, you’ll need a tow or a push. Electric Vehicles need to be towed by a flat-bed tow truck.\nHere are some local towing companies and their contact numbers:\nHow much does it cost to install a home charger?\nIf you want to charge an EV at home, you can actually do it without installing a home charger. Most EVs come with an adapter that allows you to plug into any 110V outlet.\nIt takes about 20 hours to go from completely empty to full using a 110V outlet, but if you are charging every night and have a relatively short commute it won’t take you that long to recharge your car every time.\nIf you have an older home, say older then 1970, you will likely want an electrician to visit your house to verify you have the electrical capacity to charge an EV.\nSome folks want to charge at home at a faster rate. It is possible to install a Level 2 charger at your home. The costs vary, but here are some ball park estimates:\n- To hire an electrician for the installation, you should expect to pay anywhere from $65-$85 and hour, for about $500 total. While you might be tempted to do this yourself, hiring a professional is the safest way to go.\n- There are a wide array of EV charging station options available, but expect to pay between $500-$700. If you want a charger with higher power or longer cables expect to pay at the higher end of the range.\nThis brings the estimated total to $1200 to install a L2 charger at home.\nWhat about the use of EVs as Home Battery for Solar Energy Storage? (*)\nThis is a technically feasible option for providing grid services or emergency backup power. More generally, timing EV charging to when a home or building’s solar panels are generating electricity is effectively a way to store that solar power for later use, even without the ability to return it from the vehicle to the home.\nIn most cases, with net metering, storage is not specifically needed to accommodate the levels of solar power currently on the electricity grid. But EVs could provide this capability if that changes. Vehicle-to-Home and Vehicle-to-Grid technology is still under development but will likely become commercially available within the next few years.\nWhen can I purchase an EV with more cargo space?\nNow! EV models on the market today have cargo space that is comparable with others in their class. There are hybrid minivans, SUVs, and trucks available from a variety of manufacturers and more are announced every year.\nWhat kind of services do EV’s require?\nUnlike conventional cars, in fully electric vehicles there are no oil changes, spark plugs, timing belts, fuel filters, head gaskets, radiators or fuel lines. The main wear items will be tires, brakes, windshield wipers, and the occasional cabin air filter- all relatively cheap in comparison!\nWhat incentives are available?\nRCEA is currently offering an EV rebate for CCA customers.\nView our Electric Vehicle Incentives Sheet (PDF) for more information about available federal, state, and utility incentives.\n(*)= Information sourced from Plug In America.\nWhat is a fuel cell electric vehicle (FCEV)?\nAn FCEV uses compressed hydrogen gas as its fuel, which is then fed into a fuel cell in the car to combine with oxygen to form water. This process generates electricity, which is harnessed to run a motor, and water vapor, which is the only emission from an FCEV.\nWhat are the advantages of FCEVs?\nThe state of California classifies FCEVs as “zero emission vehicles”. FCEVs are quiet and fuel efficient, just like EVs, however, they have much longer ranges (up to 360 miles!) and take only 3-5 minutes to refuel.\nWhat FCEV models are available?\nWhere can I buy an FCEV?\nYou can only buy FCEVs in areas with hydrogen fueling stations. Currently, most fueling stations are in Los Angeles and the Bay Area. Fueling stations are coming soon to the New York City, Connecticut, and Boston areas as well.\nWill there be hydrogen fueling stations in our region any time soon?\nWe hope so! In 2017 RCEA worked on a grant from the California Energy Commission to identify sites for hydrogen fueling stations along the Interstate 5 corridor. Redding and Eureka have been identified as priority sites for near-term station development.\nHow is hydrogen made?\nThere are a couple of ways. The first is through a process called electrolysis. This involves running an electric current through water to separate it into its components, hydrogen and oxygen. Clean power, such as solar energy, is frequently used to run this process. The other is called steam-methane reformation, which reacts steam with natural gas and a catalyst to make hydrogen. 33% of hydrogen used in CA is legally required to be produced from renewable resources.\nHow can I learn more about FCEVs and hydrogen?\nThere are lots of resources about hydrogen and FCEVs online, and a great place to start is with the statewide experts in FCEVs, the California Fuel Cell Partnership.\nWhen hydrogen is produced via stream methane formation, what happens to the by products?\n33% of hydrogen in California must be produced via renewable energy, but steam methane reformation is another common production method.\nSteam methane reformation applies extreme heat to methane (CH4) to produce hydrogen via the following chemical reaction:\nCH4 (methane) + H2O (water) <-> CO (carbon monoxide) +H2 (hydrogen)\nHydrogen is the obvious product we harness for FCEVs. The carbon monoxide is typically burned to extract extra energy and produces carbon dioxide upon combustion.\nThere are innovative carbon monoxide clean-up systems used in some factories which produce hydrogen but generally the CO ends up as CO2.\nNorth Coast and Upstate Fuel Cell Vehicle Readiness Project\nIn March 2019, RCEA and SERC completed the North Coast and Upstate Fuel Cell Vehicle Readiness Project (PDF). This project supports the successful introduction of fuel cell vehicles throughout the eight-county North Coast and Upstate regions by catalyzing a regional fuel cell vehicle market and planning for the deployment of hydrogen fueling infrastructure.\nVisit the RCEA Resources for Emergency Responder page for hydrogen and fuel cell resources related to safety and permitting.\nVisit the RCEA Resources for Fleet Operators (PDF) to find resources related to incentives, available FCEV models, and training resources for fleet operators and managers. If you would like to receive a free fleet assessment to determine how much it would cost to incorporate a fuel cell electric vehicle, or other electric vehicle, into your fleet please call RCEA at 707-269-1700.\nInstalling a Home Charger\nAutumn 2019 Newsletter – Installing a Home Charger\nThis newsletter provides a brief overview of frequently asked questions regarding installing a home EV charger, including who can install a charger, what permitting requirements are, and what type of charger one should install. The Owning a Charging Station page has more information.\nSpring 2019 Newsletter (PDF) – Electric Bikes\nElectric Bikes, or E-bikes, have a variety of benefits for riders of all ages. Read more about the types of e-bikes, what they can provide for riders, and how you may go about acquiring one. Our Electric Bikes and Rebate page has more information.\nEV Frequently Asked Questions\nWinter 2018 Newsletter – EV Frequently Asked Questions\nIf you are looking for information about electric vehicles, this newsletter provides details on plug-in hybrids, EV battery range, lesser known EV models and more. Read more information on our EV FAQ page.\nEV Travel Tips\nAutumn 2018 Newsletter – EV Travel Tips\nThis newsletter details some quick tricks to making longer road trips easier with an electric vehicle. Find more details about charging your EV in and out of the county on our Local Charging Stations page.\nMeet a Local EV Driver\nSpring 2018 Newsletter – Chargers, Incentives, and Interview with a Local EV Driver\nThis installment of the PEV newsletter includes information on DC fast chargers added to Humboldt County, incentives for EVs, and an interview with a local EV driver.\nThese newsletters contain information that has been updated in more recent newsletters.\nRCEA Electric Vehicle Buying Presentations\nAugust 26, 2020 Webinar with Lee Valenzuela, RCEA Transportation Specialist\nRCEA Transportation Specialist Lee Valenzuela will talk about the different types of electric vehicles, financial incentives and assistance, consumer benefits, environmental benefits, fuel and maintenance savings, and charging options.\nRCEA plans to roll out a local EV rebate this fall that will be an add-on to the California Clean Vehicle Rebate Project. Details will be provided during the webinar.\nJune 13, 2019 at the Eureka Chamber of Commerce with Aisha Cissna, RCEA Transportation Specialist']	['<urn:uuid:5daf1e4d-e779-4953-aafd-50012b3593cc>', '<urn:uuid:cc1052f0-bee8-4e91-af1d-5dcca1a69275>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T00:43:36.053682	10	50	3799
56	How does the Eder river's gold mining history compare with current hydroelectric power plant monitoring techniques in terms of resource exploitation?	The Eder river has a historical gold mining tradition dating back to 1308, with gold panning occurring mainly between Affoldern and Fritzlar, even leading to ducat minting in the 18th century. In contrast, modern resource management involves sophisticated monitoring systems for hydroelectric installations, including wireless weather sensors, acoustic sensors, and MEMS (Micro Electro Mechanical Systems), with data being collected in a 'Data Lake' cloud platform for analysis and maintenance optimization.	"[""Eder facts for kids\nUser:RMCD bot/subject notice\nQuick facts for kidsEder\nThe Eder in Edersee when the lake dried out in 2003\nMap of the Eder, its tributaries and water catchment area\n634 m (2,080 ft)\n142 m (466 ft)\n|Length||176.1 km (109.4 mi)|\n|Progression||Fulda→ Weser→ North Sea|\n|Basin size||3,362 km2 (1,298 sq mi)|\nThe river was first mentioned by the Roman historian Tacitus. In his Annals, he describes the Roman campaign against the Chatti under the command of Germanicus in 15 AD. Forty-five thousand soldiers of the Roman army destroyed the major centre of the Chatti, Mattium, directly after they crossed the Adrana (Eder).\nIn the Middle Ages, the river was known by the names; Aderna, Adarna, Adrina.\nOn the banks of the Eder, in the town of Schwarzenau, near Bad Berleburg, a religious group was founded in August 1708; the Schwarzenau Brethren. Eight adults were completely baptised thrice in the Eder. This group emigrated to America where they are still to be found.\nAs late as up to the end of the 19th Century, the river was also known in local dialect as Edder. For instance, in Felsberg-Gensungen, the pharmacy is known as the Edder-Apotheke.\nGold in the Eder\nSediments of the Eder contain a proportion of gold. The majority of this gold is said to originate from Eisenberg (Korbach), which contains one of the largest reserves of gold in Middle Europe. Gold is eroded out of the Eisenberg, for example, by the Itter stream, which, since the Edersee dam was built, flows into the Edersee lake.\nGold panning in the Eder has been known since 1308. The main historical area for panning the gold is between Affoldern and Fritzlar. In the 14th Century, the Teutonic Order panned gold out of the Eder sediments near Obermöllrich. In the 18th Century, even ducats were minted from Eder gold; they are collector's items today.\nUp to the 1970s, school children from Duisburg, who stayed at a nearby holiday camp, together with a teacher from Marienhagen, part of the town of Vöhl, went panning for gold in the sediments of the River Itter. Panning for gold along the Eder is still popular.\nThe river rises from the Ederkopf mountain in the Rothaar mountain range in eastern North Rhine-Westphalia, near the springs of the Lahn and Sieg rivers. However, unlike the Lahn and Sieg, which are both tributaries of the Rhine, the Eder flows east and north, into the river Fulda at Edermünde, south of Kassel. The Fulda then confluences with the Werra River at Hann. Münden to form the Weser River, which flows in to the North Sea.\nThe Edersee Dam is situated below the town of Waldeck. It is constructed of rock and concrete, and is 47 metres (154 ft) high and 400 metres (1,300 ft) long. It was completed in 1914; in 2014 its 100th birthday was celebrated. It forms the Edersee lake, which is 27 kilometres (17 mi) long and contains 200 million cubic metres of water. This is used to generate hydroelectricity and to regulate water levels for shipping on the Weser river.\nAt low water in late summer, and during dry years, the remnants of three villages (Asel, Bringhausen, and Berich) and a bridge across the original river bed, which was submerged when the lake was filled in 1914, can be seen. Descendants of those buried there go to visit the graves of their ancestors.\nDestruction of the Eder dam\nOn the night of the 17 May 1943, Avro Lancaster bombers of the RAF 617 Squadron used specially-developed bouncing bombs (codename: Upkeep) that were engineered by Barnes Wallis. They were used to destroy the Möhne and Eder dams, as part of Operation Chastise. The dam was repaired and in use again before the end of the year. The story of the raid was documented by the 1955 film called The Dam Busters.\nTributaries of the Eder\nThe most important tributaries of the Eder are (listed first in Eder-kilometres from source to the Fulda):\n(discharge rate; l/s)\n(m ü. NHN)\n|German river numbering system|\n- rivers that used to flow to the Eder, but now flow in to the Edersee lake.\nEder Facts for Kids. Kiddle Encyclopedia."", ""“Hydroelectric power isn’t just the main renewable power source for the sheer quantity of energy it generates, but also, for the quality it guarantees in assuring the reliability and integrity of the whole grid network. Moreover, it allows for a manifold use of its waters - be them for drinking, irrigation and recreational uses - and it proves a fundamental asset in regulating hydraulic overflows thus safeguarding the populace from potential hazards. This is an incredible wealth that must be looked after with digitalizion and refurbishment programs.”\nHydroelectric energy innovation\nIn Italy, hydroelectric energy was among the architects of economic development in the 1950s and 1960s, when plants powered by the strength of water significantly contributed to the reconstruction of the country.\nToday, the strength of water helps to generate 42% of the total energy produced by renewable sources in Italy, for a total installed capacity of 18.5 GW, equal to 46 TWh/year.\nThese are significant numbers, sure to rise in the coming years, thanks to plant renovation programmes.\nAccording to a study by Althesys, recently presented in Rome and titled 'Hydropower creates value for Italy', the Italian hydroelectric fleet has a still unfulfilled potential of about 1800 MW in 2020 and 5800 MW in 2030, equivalent to a production increase of 1 TWh in 2020 and 3.4 TWh in 2030.\nThis potential, however, needs to be unleashed through an integrated programme of investments, which Althesys’ calculations estimate at around 5.5 billion euro.\nTo date – according to the study - just 42% of the capacity built before 1960 has been modernized, with 6.7 GW of installed power still to be renovated and upgraded.\n“Hydropower, the main renewable source in Italy, with almost half of its production, is key to reaching the UN’s goals for 2030. The raising of the bar to 32% of European renewable goals is pushing our country to go beyond the National Energy Strategy and put a special plan for renewables in place for the coming decade.”\nDrones for monitoring and maintenance of hydroelectric power plants\nAlongside the renovation and modernization of hydroelectric plants to promote their energy efficiency, Enel Green Power is committed to a vast programme aimed at introducing innovative technologies to improve the safety and technical-economic performance of the fleet.\nOne example is drones and robots to facilitate inspections in remote or confined places and optimise the resulting maintenance work. A project started in Italy but that includes EGP’s plants around the world.\nIn Chile, for example, we use robots to carry out inspections of penstocks – difficult for workers to access – and to take measurements of basin bottoms, to check their pitting.\nBut, in this area, innovation for Enel Green Power’s plants comes above all from the sky. A series of tests is underway to use drones to check – from inside and out – the health status of a plant or dam, in order to increase general plant safety and optimise maintenance processes.\nIn Italy, together with ENAC, the national body for flight certifications, we’ve launched a project to use unmanned aerial vehicles (UAV) in so-called Beyond Visual line of Sight (BVLOS) mode, for the monitoring of certain channels in the country.\nIt’s a very promising and useful project for the world of drones, given that it’s helping ENAC test the need and opportunity to extend current UAV regulations, fundamentally based on the Remote Pilot Aircraft Regulation, to include the possibility of carrying out specialised operations beyond the limits of the Visual Line of Sight and to modify the related regulations.\nThe Challenge of Digitalization\nThe use of drones and robots is part of EGP’s broader commitment to transform its hydropower plants – like what it has done with solar and wind – into a winning model of digitalization.\nOn a global scale, digitalization can be fundamental for Enel Green Power in all its Operation & Maintenance (O&M) activities, in order to reduce intervention and maintenance costs, achieve greater safety and, consequently, an increase in production efficiency.\nFor a hydroelectric plant, digitalization means the integration of different technologies, linked, for example, to the Internet of Things (IoT), to wireless sensors, able to provide a large amount of data (Big Data), reducing installation costs, testing new protocols for connectivity and allowing for the collection of data even in remote, not always equipped, places, aimed at smart diagnostics and predictive maintenance.\nExamples of technologies tested or in the monitoring phase are acoustic sensors, MEMS (Micro Electro Mechanical Systems), wireless sensors with Energy Harvesting solutions for weather stations, in correspondence with intake works, or to monitor the status of plant machinery.\nThen there are the low-cost wireless weather sensors, able to self-power and so perfect for remote, difficult-to-access areas. The goal, also in this case, is to increase plant safety and improve and optimise O&M processes, in order to reduce costs. At the moment, wireless weather sensors have been installed and tested in Chile, with twelve stations, and in Italy, with eleven.\nThe numerous data collected from plants are sent to our cloud platform – a real “Data Lake” – and analysed with algorithms developed and customised for hydroelectric, with the goal of having an accurate picture of the plant’s health status and allow for more effective maintenance.\nOur Commitment to Sustainable development\nNext to innovation and digitalization, EGP is always committed to the well-being of local communities, in line with the sustainable development goals in the UN’s 2030 Agenda.\nTo reach these ambitious goals, we put new solutions and new ideas to work for the environment, like the ones we are searching for on our Open Innovability platform for a new sustainability project in Colombia, where we want to reduce pollution in the Rio Bogotá, in the El Muña basin, near our Paraíso and Guaca hydropower plants.\nUnfortunately, the river is one of the most contaminated in South America and so we called on innovators, start-ups and companies to come up with a technologically advanced and low-cost solution to reduce emissions and purify the air in the most critical areas of the basin.\nIn a world that’s moving faster and faster, EGP is in the front lines to add value to its internal skills and search for new ideas in order to stimulate the essential combination of innovation and sustainability.""]"	['<urn:uuid:fda45230-5e5a-43e0-b1b4-984fa13abf74>', '<urn:uuid:674d25a7-703b-48d9-9c7a-e76c81868da0>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	21	70	1748
57	lead poisoning franklin expedition deaths evidence	The lead poisoning theory was first proposed when forensic anthropologist Owen Beattie exhumed remains from Beechey Island in 1984. While initially thought to be the main cause of deaths, scientists later doubted that tin cans could produce enough lethal lead. The lead levels found in the three dead sailors on Beechey were actually not unusual for sailors of that time. Current research indicates that lead poisoning cannot be seen as the sole or primary cause of the expedition's downfall, though it may have been one contributing factor.	['Images: Edwin Landseer & Sheena Fraser McGoogan\nFrom the Toronto Star, August 19, 2023\nBy Ken McGoogan\nThe Franklin search season is almost upon us. Last year, during eleven days in early September, Parks Canada underwater archaeologists retrieved 275 artifacts from HMS Erebus. That ship was one of the two recently located wrecks from the 129-man expedition that, led by Sir John Franklin, disappeared into the Arctic in 1845. This year, as soon as weather permits, Parks Canada will resume its investigations.\nMeanwhile, late in July, National Geographic magazine published a massive article online, complete with photos and videos, headlined: “Seeking to solve the Arctic’s biggest mystery, they ended up trapped in ice at the top of the world.” It features Tom Gross, who has been scouring King William Island for a burial vault that he believes holds Franklin’s body — and possibly logbooks or documents. With Jacob Keanik, an Inuk who lives on the island, Gross will resume his search any day now.\nBooks about Franklin and his doomed expedition, which already number in the hundreds, continue to appear at a rapid rate. (See below for where to begin.)\nOutstanding documentaries have treated the expedition, among them Franklin’s Lost Ships (Andrew Gregg, 2016) and Passage (John Walker, 2008). A furiously active Facebook group, Remembering the Franklin Expedition, boasts more than 3,400 members.\nBut why does this expedition, which disappeared into the Arctic almost 180 years ago, continue to attract attention. First, the catastrophe cost the lives of 129 men, making it the worst in exploration history.\nSecond, some final survivors resorted to cannibalism, which prompted international headlines. Third, the formidable Jane Lady Franklin made a mythical hero of her late husband as “discoverer of the Northwest Passage.”\nFourth and above all, here we have, as National Geographic put it, the “Arctic’s biggest mystery.” The expedition was expected to solve the riddle of the Northwest Passage. What happened?\nResearchers have been seeking answers since 1848. A written document found in 1859 – the Victory Point Record — filled in a few blanks. The expedition crossed Davis Strait, entered Lancaster Sound, probed Wellington Channel northwards, and then spent the winter of 1845–46 on Beechey Island. Here, Franklin buried three crewmen.\nIn 1846, when the ice melted, the two ships sailed south into Peel Sound. On September 12, they got trapped in the ice off the northwest coast of King William Island. Nine months later, on June 11, 1847, Sir John Franklin died.\nThen, on April 22, 1848, 105 men departed from the two ships, which were still beset in the ice. By that time, nine officers and fifteen seamen had died. Given three men buried on Beechey, twenty-one died between spring 1846 and April 1848.\nA previous Royal Navy expedition had spent four winters trapped in the Arctic (1829-1834). Almost all men had escaped back to England. Why, with Franklin, did so many die so early? The Inuit reported seeing disoriented men trekking along the coast of King William Island and dying in hospital tents. What?\nIn 1984, on Beechey Island, forensic anthropologist Owen Beattie exhumed the remains of the first three sailors to die. In Frozen in Time, with co-author John Geiger, he suggested that lead poisoning, contracted from the solder used to seal cans of preserved food, killed some men and sickened the rest.\nThis view prevailed for two decades. But then some scientists doubted that tin cans could produce enough lead to be lethal. Others suggested that maybe the lead came from the ship’s pipes. Researchers then determined that, for sailors of that time, the lead levels of the three dead on Beechey were not unusual.\nIn April 2014, three British scientists—Keith Millar, Adrian Bowman, and William Battersby—published a statistical analysis in Significance magazine, arguing that a combination of factors killed the sailors. In his introduction to May We Be Spared to Meet on Earth, a collection of letters, American expert Russell Potter summarized, noting that “lead poisoning can no longer be seen as the sole or primary cause of the expedition’s downfall, though it certainly may have been one factor.”\nA few writers had pointed to botulism as an alternative root cause. But as Potter observes, this “seems highly unlikely” as the poorly sealed tin cans admitted oxygen, which along with salt and nitrates would have constrained botulism. Early letters sent home from Greenland find the sailors raving about the soups and, years later, Inuit who opened some tin cans enjoyed their contents very much.\nBottom line: leading researchers have ruled out lead poisoning and botulism as the root cause of the catastrophe. Fact remains: 37 per cent of the officers had died by 1848, but just 14 per cent of the regular seamen. Why such disproportion?\nWhat galvanized 105 men to leave the ships (though some of\nthem, clearly, later reboarded)? Why were so many so sick that they could not walk straight? Why did some appear to be sheltering (and dying) onshore in a hospital tent? Questions like these keep many of us searching for answers – whether underwater, on land, or at our desks.\n(This fall, with Douglas & McIntyre, Ken McGoogan will publish Searching for Franklin: New Answers to the Great Arctic Mystery.)\nDon’t know where to begin your quest? These five books point the way:\nFrozen in Time: The Fate of the Franklin Expedition, by Owen Beattie and John Geiger. Greystone, 1987. Advances the lead poisoning theory after three bodies exhumed.\nUnravelling the Franklin Mystery: Inuit Testimony, by David E. Woodman. McGill-Queen’s, 1991. Extensive analysis of Inuit oral testimony, often graphic.\nIce Blink: The Tragic Fate of Sir John Franklin’s Lost Polar Expedition, by Scott Cookman. John Wiley, 2000. Rejects lead poisoning, points to botulism.\nMay We Be Spared to Meet on Earth: Letters of the Lost Franklin Arctic Expedition, ed. Russell Potter and others. McGill-Queen’s, 2022. Brings sailors to life in their own words, rejects lead poisoning and botulism.\nDead Reckoning: The Untold Story of the Northwest Passage, by Ken McGoogan. HarperCollins Canada, 2017. Highlights the role of the Indigenous peoples in Arctic exploration.']	['<urn:uuid:95368991-faa2-495b-846a-4d497a9d1995>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	6	87	1013
58	In my work on financial market interconnectedness, I need to analyze both bond and CDS markets - how does your research approach handle data from these two markets to identify risk transmission patterns?	The research uses intraday data from both bond markets and CDS markets to identify which market was leading in the contagion (systemic risk spillover) within the network. For the bond market, credit risk data (asset swap spreads) are calculated on 30-minute intervals. The CDS data is specifically tested for intraday patterns (volatility smiles). This dual-market approach enables the identification of whether risk transmission takes place through the bond market or the derivative market from one country to another.	['FV-84 | Systemic Risk Dynamics in Euro Area Sovereign Debt Markets\nProf. H. Zimmermann, Dr. K. Ters\nIn this project, we will employ a network model based on the methodology of a VAR Variance decomposition in order to understand the dynamics of sovereign risk interconnectedness. Macroprudential regulation views systemic risk as dependent on collective behaviour (endogenous), therefore, a network methodology is essential. We will investigate the magnitude and dynamics within the network before the unconventional policy or regulatory measure (called “events”) and compare it with the dynamics estimated in a short time window after the measure was adapted. We are able to work with very short time windows around an event, as we operate with intraday data for bond markets and CDS markets. Also, having data for credit risk from both, the cash and derivative market, enables us to identify which market was leading in the contagion (systemic risk spillover) within the network. E.g. was the risk transmission taking place through the bond market or the derivative market from one country to another?\nThe credit risk data for the bond market (asset swap spreads) have been calculated on an intraday basis (30 minute intervals). The CDS data has been tested for intraday patterns (volatility smiles). All relevant events have been collected including their time stamps for our intraday analysis. The coding of the partial correlation is finalized. However, we have decided to add a network structure based on impulse responses from a Panel Vector Autoregressive model (PVAR). This coding is still in process. We have decided to change from the formerly proposed VAR structure to a PVAR due to its superior performance within our problem setup. PVARs have the same structure as VAR models, in the sense that all variables are assumed to be endogenous but with the difference that a cross-sectional dimension is added to the representation. The PVAR has several advantages over individual country VARs in a time series framework. By analysing a panel of countries, we can more accurately model contagion from one country to another since the panel approach captures country-level heterogeneity. The network representation will be based on the PVAR impulse responses to a one unit shock of each respective sovereign entity. The arrow representation within the network will follow the same logic as for the partial correlations.\nFurthermore, we have also estimated certain events already such as the impact of different economic adjustment programs during the euro area sovereign debt crisis. Interestingly, we have found differing dynamics of risk transmissions. The first economic adjustment program for Greece had a much stronger impact in terms of contagion transmission onto other GIIPS countries in contrast to for example the second Greek economic adjustment program (EAP). We believe that this is due to the fact, that the first EAP was not executed under the European Financial Stability Facility. This gives some evidence, that the EFSF was able to decrease contagion risk amongst GIIPS countries.\nDue to the problems and delays caused by the corona virus, our estimated completion of the project will be end-summer 2021.\nPublications, Presentations, Conferences\nAs the research paper is not yet finalized, there has not been a submission made yet to an academic journal or conference.']	['<urn:uuid:39351e6a-112a-4397-b8dc-9743a23fdae3>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	33	78	533
59	What kind of energy is used to transport PCB materials?	The energy for transporting PCB materials comes from fossil fuels including petroleum, natural gas, propane, biofuel, and electricity.	['Increasingly, PCB manufacturers are focused on minimizing their environmental impact. They achieve this by adopting energy-efficient equipment and implementing waste reduction strategies.\nUsing additive processes instead of the subtractive approach to manufacture circuit boards reduces the need for chemical etching, which is very energy-intensive. Also, applying design for manufacturing (DFM) principles at the prototype stage decreases material usage.\nThe production of PCBs generates a lot of waste. This waste includes chemical cleaning solutions, offcuts, and other materials that can be hazardous to the environment. To minimize the amount of waste produced, PCB manufacturers are looking at alternative, eco-friendly products that can replace harmful substances.\nAdditionally, waste generated during manufacturing includes contaminated rinse water and various chemicals used in manufacturing. As such, PCBs must be disposed of using regulated processes. It ensures that the environmental risks of these substances are minimized.\nResponsible e-waste disposal practices include proper landfill management, recycling, and incineration. Responsible PCB recycling programs reduce the need for new natural resources by repurposing old electronics and help lower the global footprint. Recycling also aids in reducing secondary persistent organic pollutants, including dioxins and volatile organic compounds (VOCs), which are air pollutants. Strict guidelines that regulate ash disposal and leachate quality factors during processing are used to achieve this. Go to this website to find out more about PCB recycling initiatives.\nThe manufacturing process for PCBs consumes a significant amount of energy, especially in high-temperature reflow soldering ovens. Using more energy-efficient machinery and equipment minimizes this consumption, reducing the environmental impact of the manufacturing process.\nTransporting the materials used in the production of PCBs requires a significant amount of energy. This energy is primarily obtained from fossil fuels such as petroleum, natural gas, propane, biofuel, and electricity.\nManufacturers need to adhere to a wide range of environmental regulations and standards when manufacturing PCBs. Some of the most essential requirements include adhering to the Restriction on Hazardous Substances (RoHS) regulations, which limit using certain toxic materials like lead, mercury, cadmium, and flame retardants in electronic products. Manufacturers can meet these regulations by implementing sustainable practices like optimizing material usage, minimizing waste, and designing for disassembly. They can also reduce the need for product replacement by encouraging e-waste management programs. Moreover, they can achieve cost savings through more efficient production processes.\nEnvironmentally Friendly Materials\nThe core of electronic gadgets, from computers to medical equipment and automotive systems, is printed circuit boards (PCBs). Unfortunately, conventional methods for making PCBs often involve dangerous substances and produce a lot of waste. Green initiatives in the PCB business aim to minimize these problems through environmentally friendly materials, production procedures, and responsible waste management.\nSustainable sourcing of raw materials is another critical aspect of PCB sustainability. Manufacturers are exploring options for recyclable substrates like bamboo and bio-resins. These renewable materials reduce long-term environmental waste and align with the principles of the circular economy.\nOther eco-friendly material changes in PCB manufacturing include lead-free solder and low-VOC laminates to reduce toxic chemicals and VOC emissions. Also, local sourcing of materials results in reduced transportation costs and emissions. Finally, the design of a circuit board must be optimized for manufacturing to limit material usage. It minimizes waste and energy usage while ensuring the circuit board has everything.\nSeveral hazardous wastes are produced throughout the process of PCB manufacture. If improperly disposed of, they may end up in landfills or cause environmental contamination. To prevent this, recycling is an integral part of the PCB industry. Non-metallic PCB components and other materials, such as copper and tin, can be salvaged for other purposes. In addition, metals like nickel and silver can also be recycled to avoid the use of new resources.\nOther environmental considerations in PCB manufacturing include energy-efficient production processes and responsible e-waste management programs. Many manufacturers have begun using closed-loop systems to minimize the volume of discarded materials, significantly reducing the industry’s overall carbon footprint.\nManufacturers also focus on design for manufacturing (DFM) principles to ensure that each board includes all the necessary elements. It helps to reduce excess waste caused by reworking and redesigns. Furthermore, technologies like 3D printing enable the production of complex circuits without the need for multistep subtractive procedures. It is another significant step toward sustainability for the electronics industry.']	['<urn:uuid:17ba214d-027a-4839-a338-6c08ea31c141>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	10	18	704
60	what problems could advanced artificial intelligence cause in future and how prevent them	Advanced AI systems could pose several risks including misuse, economic disruption, disinformation, bias, discrimination, addiction, and overreliance. There are also concerns about superintelligent AI systems that could potentially outthink their human creators. To prevent these problems, researchers are working on 'alignment' - ensuring AI performs as humans intend. OpenAI is specifically developing a human-level automated alignment researcher to control future AI products, with goals including evaluating other AI systems, automating the search for problematic behavior, and stress-testing alignment methods. However, completely eliminating risks may be impossible, so ongoing monitoring, refinement, and adaptation are necessary, along with collaboration between developers, users, and affected communities.	"['Eliminating bias in AI may be impossible—a computer scientist explains how to tame it instead\nWhen I asked ChatGPT for a joke about Sicilians the other day, it implied that Sicilians are stinky.\nAs somebody born and raised in Sicily, I reacted to ChatGPT\'s joke with disgust. But at the same time, my computer scientist brain began spinning around a seemingly simple question: Should ChatGPT and other artificial intelligence systems be allowed to be biased?\nYou might say, ""Of course not!"" And that would be a reasonable response. But there are some researchers, like me, who argue the opposite: AI systems like ChatGPT should indeed be biased—but not in the way you might think.\nUncovering bias in AI\nComputer scientists say an AI model is biased if it unexpectedly produces skewed results. These results could exhibit prejudice against individuals or groups, or otherwise not be in line with positive human values like fairness and truth. Even small divergences from expected behavior can have a ""butterfly effect,"" in which seemingly minor biases can be amplified by generative AI and have far-reaching consequence.\nBias in generative AI systems can come from a variety of sources. Problematic training data can associate certain occupations with specific genders or perpetuate racial biases. Learning algorithms themselves can be biased and then amplify existing biases in the data.\nBut systems could also be biased by design. For example, a company might design its generative AI system to prioritize formal over creative writing, or to specifically serve government industries, thus inadvertently reinforcing existing biases and excluding different views. Other societal factors, like a lack of regulations or misaligned financial incentives, can also lead to AI biases.\nThe challenges of removing bias\nIt\'s not clear whether bias can—or even should—be entirely eliminated from AI systems.\nImagine you\'re an AI engineer and you notice your model produces a stereotypical response, like Sicilians being ""stinky."" You might think that the solution is to remove some bad examples in the training data, maybe jokes about the smell of Sicilian food. Recent research has identified how to perform this kind of ""AI neurosurgery"" to deemphasize associations between certain concepts.\nBut these well-intentioned changes can have unpredictable, and possibly negative, effects. Even small variations in the training data or in an AI model configuration can lead to significantly different system outcomes, and these changes are impossible to predict in advance. You don\'t know what other associations your AI system has learned as a consequence of ""unlearning"" the bias you just addressed.\nOther attempts at bias mitigation run similar risks. An AI system that is trained to completely avoid certain sensitive topics could produce incomplete or misleading responses. Misguided regulations can worsen, rather than improve, issues of AI bias and safety. Bad actors could evade safeguards to elicit malicious AI behaviors—making phishing scams more convincing or using deepfakes to manipulate elections.\nWith these challenges in mind, researchers are working to improve data sampling techniques and algorithmic fairness, especially in settings where certain sensitive data is not available. Some companies, like OpenAI, have opted to have human workers annotate the data.\nOn the one hand, these strategies can help the model better align with human values. However, by implementing any of these approaches, developers also run the risk of introducing new cultural, ideological or political biases.\nThere\'s a trade-off between reducing bias and making sure that the AI system is still useful and accurate. Some researchers, including me, think that generative AI systems should be allowed to be biased—but in a carefully controlled way.\nFor example, my collaborators and I developed techniques that let users specify what level of bias an AI system should tolerate. This model can detect toxicity in written text by accounting for in-group or cultural linguistic norms. While traditional approaches can inaccurately flag some posts or comments written in African-American English as offensive and by LGBTQ+ communities as toxic, this ""controllable"" AI model provides a much fairer classification.\nControllable—and safe—generative AI is important to ensure that AI models produce outputs that align with human values, while still allowing for nuance and flexibility.\nEven if researchers could achieve bias-free generative AI, that would be just one step toward the broader goal of fairness. The pursuit of fairness in generative AI requires a holistic approach—not only better data processing, annotation and debiasing algorithms, but also human collaboration among developers, users and affected communities.\nAs AI technology continues to proliferate, it\'s important to remember that bias removal is not a one-time fix. Rather, it\'s an ongoing process that demands constant monitoring, refinement and adaptation. Although developers might be unable to easily anticipate or contain the butterfly effect, they can continue to be vigilant and thoughtful in their approach to AI bias.', 'The AI giant predicts human-like machine intelligence could arrive within 10 years, so they want to be ready for it in four.\nOpenAI is seeking researchers to work on containing super-smart artificial intelligence with other AI. The end goal is to mitigate a threat of human-like machine intelligence that may or may not be science fiction.\n“We need scientific and technical breakthroughs to steer and control AI systems much smarter than us,” wrote OpenAI Head of Alignment Jan Leike and co-founder and Chief Scientist Ilya Sutskever in a blog post.\nOpenAI’s Superalignment team is now recruiting\nThe Superalignment team will devote 20% of OpenAI’s total compute power to training what they call a human-level automated alignment researcher to keep future AI products in line. Toward that end, OpenAI’s new Superalignment group is hiring a research engineer, research scientist and research manager.\nOpenAI says the key to controlling an AI is alignment, or making sure the AI performs the job a human intended it to do.\nThe company has also stated that one of its objectives is the control of “superintelligence,” or AI with greater-than-human capabilities. It’s important that these science-fiction-sounding hyperintelligent AI “follow human intent,” Leike and Sutskever wrote. They anticipate the development of superintelligent AI within the last decade and want to have a way to control it within the next four years.\nSEE: How to build an ethics policy for the use of artificial intelligence in your organization (TechRepublic Premium)\n“It is encouraging that OpenAI is proactively working to ensure the alliance of such systems with our [human] values,” said Haniyeh Mahmoudian, global AI ethicist at AI and ML software company DataRobot and member of the U.S. National AI Advisory Committee. “Nonetheless, the future utilization and capabilities of these systems remain largely unknown. Drawing parallels with existing AI deployments, it’s clear that a one-size-fits-all approach is not applicable, and the specifics of system implementation and evaluation will vary according to the context of use.”\nAI trainer may keep other AI models in line\nToday, AI training requires a lot of human input. Leike and Sutskever propose that a future challenge for developing AI might be adversarial — namely, “our models’ inability to successfully detect and undermine supervision during training.”\nTherefore, they say, it will take a specialized AI to train an AI that can outthink the people who made it. The AI researcher that trains other AI models will help OpenAI stress test and reassess the company’s entire alignment pipeline.\nChanging the way OpenAI handles alignment involves three major goals:\n- Creating AI that assists in evaluating other AI and understanding how those models interpret the kind of oversight a human would usually perform.\n- Automating the search for problematic behavior or internal data within an AI.\n- Stress-testing this alignment pipeline by intentionally creating “misaligned” AI to ensure that the alignment AI can detect them.\nPersonnel from OpenAI’s previous alignment team and other teams will work on Superalignment along with the new hires. The creation of the new team reflects Sutskever’s interest in superintelligent AI. He plans to make Superalignment his primary research focus.\nSuperintelligent AI: Real or science fiction?\nWhether “superintelligence” will ever exist is a matter of debate.\nOpenAI proposes superintelligence as a tier higher than generalized intelligence, a human-like class of AI that some researchers say won’t ever exist. However, some Microsoft researchers think GPT-4 scoring high on standardized tests makes it approach the threshold of generalized intelligence.\nOthers doubt that intelligence can really be measured by standardized tests, or wonder whether the very idea of generalized AI approaches a philosophical rather than a technical challenge. Large language models can’t interpret language “in context” and therefore don’t approach anything like human-like thought, a 2022 study from Cohere for AI pointed out. (Neither of these studies is peer-reviewed.)\n“Extinction-level concerns about super-AI speak to the long-term risks that could fundamentally transform society and such considerations are essential for shaping research priorities, regulatory policies, and long-term safeguards,” said Mahmoudian. “However, focusing exclusively on these futuristic concerns may unintentionally overshadow the immediate, more pragmatic ethical issues associated with current AI technologies.”\nThose more pragmatic ethical issues include:\n- And potential bias in AI algorithms.\nThese are already relevant to the way people use AI in their day-to-day lives, she pointed out.\n“It is crucial to consider long-term implications and risks while simultaneously addressing the concrete ethical challenges posed by AI today,” Mahmoudian said.\nSEE: Some high-risk uses of AI could be covered under the laws being developed in the European Parliament. (TechRepublic)\nOpenAI aims to get ahead of the speed of AI development\nOpenAI frames the threat of superintelligence as possible but not imminent.\n“We have a lot of uncertainty over the speed of development of the technology over the next few years, so we choose to aim for the more difficult target to align a much more capable system,” Leike and Sutskever wrote.\nThey also point out that improving safety in existing AI products like ChatGPT is a priority, and that discussion of AI safety should also include “risks from AI such as misuse, economic disruption, disinformation, bias and discrimination, addiction and overreliance, and others” and “related sociotechnical problems.”\n“Superintelligence alignment is fundamentally a machine learning problem, and we think great machine learning experts — even if they’re not already working on alignment — will be critical to solving it,” Leike and Sutskever said in the blog post.']"	['<urn:uuid:520dba0f-e9f6-41d5-a9db-9291d2bd4272>', '<urn:uuid:0b9f31c7-9c48-4791-b322-17a25e100b04>']	open-ended	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-13T00:43:36.053682	13	103	1688
61	Which bird lays bigger eggs - the Common Eider duck or the California Condor?	The California Condor lays bigger eggs. California Condor eggs are about 4.5 inches long, almost 3 inches wide, and weigh about 11 ounces. While the exact size of Common Eider eggs is not specified in the documents, we know they lay 4-5 eggs per nest, and being a duck species, their eggs would be significantly smaller than those of the California Condor, which lays among the largest eggs of any U.S. bird species.	['The Common Eider Somateria mollissima is the largest duck in the northern hemisphere. It weighs an average of 1 800 g, but its weight can vary from 850 to 3 025 g depending on race, sex, and time of year. There are four Common Eider races in North America; subtle differences in body size and bill structure distinguish each race from the other.\nThe plumage of the Common Eider varies considerably. It passes through several stages while the bird is growing to maturity, and after the bird reaches adulthood at about three years old, the plumage alternates between two colours each year as a result of moulting, or the replacement of old feathers with new. In addition, the male’s plumage differs from the female’s.\nBetween the ages of three weeks and three years, male Common Eiders moult their feathers eight times, changing their colour from a juvenile blackish brown to an adult olive-brown and white in winter and a striking black and white, with a small area of light emerald green on the back and sides of the head, during the breeding season. Changes in female plumage are less dramatic: from a juvenile blackish brown, the duck becomes rusty-to-tan. The female’s summer colours provide good camouflage in the vegetation and rocks of the offshore islands on which she breeds.\nCommon Eiders can live 20 years, one of the longest lifespans among sea ducks. However, the expected lifespan for eider populations which are heavily harvested may be much shorter.\nSigns and sounds\nDucklings utter a number of sounds, ranging from a high-pitched note of contentment, which they give especially when they are feeding in the water, to a distress call—a monosyllabic piping.\nWhen they are alarmed, adult Common Eiders emit a series of hoarse kor-korr-korr notes. When courting, drakes give a haunting call much like the cooing of pigeons, which can travel great distances across water on calm days.\nFemales are less vocal than males. They produce a series of throaty calls during courtship and feeding and an abrupt cluck-cluck-cluck when defending their ducklings from avian predators such as herring gulls.Back to top\nHabitat and Habits\nOf all sea ducks, the Common Eider is the most closely tied to marine habitat. It lives in arctic and subarctic coastal marine areas, where it frequents coastal headlands, offshore islands, skerries, and shoals. The Common Eider rarely leaves the water in the winter, and some races remain as far north as there is open water.\nThe four races of Common Eiders in North America have different breeding ranges. The southern race Somateria mollissima dresseri breeds from Maine to Hamilton Inlet on the Labrador coast; the northern race Somateria mollissima borealis breeds from northern Labrador to Ellesmere Island in the eastern Canadian Arctic; the Hudson Bay race Somateria mollissima sedentaria remains all year within Hudson Bay; and the Pacific race Somateria mollissima v-nigra breeds from Coronation Gulf in the MacKenzie District of the Northwest Territories to the south side of the Alaskan peninsula. Three subspecies are found outside North America: one in northwest Europe, one in Iceland and a third in the Faeroe Islands north of Great Britain.\nThe Common Eider belongs to the sea duck tribe (Mergini), which contains closely related ducks, all of which use marine habitats to some degree. The King Eider Somateria spectabilis, Spectacled Eider Somateria fischeri, and Common Eider all belong to the same genus, and hybridization is known to occur between Common and King Eiders.\nEider ducks are gregarious, travelling and feeding in flocks numbering from tens to thousands.\nYoung Common Eiders often benefit from the care of “aunts,” which are nonbreeding females. These “aunts” gather around nests containing hatching eggs or newly hatched young and accompany the ducklings to the water with their mother and help to protect the young from predators.\nAnother noteworthy practice is the Common Eiders’ habit of gathering in compact flocks at night, sometimes offshore, at other times in the protection of a headland or cove. Folklore claims that under very cold conditions some eiders move around the outer ring of the flock in order to keep the water from freezing. Although this behaviour has not been documented by scientists, it is known that in winter when the temperature drops eiders protect themselves in other ways. They minimize their energy expenditure by becoming inactive, not feeding and, presumably to insulate themselves, gathering in groups so dense that individual ducks cannot be counted.\nDistribution of the Common Eider\nCommon Eiders breed along much of the coast of northern North America, south to Maine in the east and south to the Alaska Peninsula in the west. In winter the various races shift southward, even as far as Florida on the eastern seaboard and to the coast of Washington in the Pacific. However, the bulk of the Atlantic coast eiders winter in Newfoundland and Labrador and on Cape Cod, Maine, and most of the Pacific eiders winter in the Aleutian Islands in Alaska.\nThe four races of Common Eider that breed in Canada follow a wide range of migration patterns. Most eiders migrate in spring and fall, some travelling long distances, others short; some populations remain in the same area year-round.\nCommon Eiders that migrate in spring travel rapidly. Most birds fly along the coast, though some individuals are known to cross over significant tracts of land, such as the Nova Scotia–New Brunswick border region or the isthmus of the Avalon Peninsula on the island of Newfoundland. They travel in compact flocks of a few to thousands, flying low to the water and travelling at speeds of 60 to 70 km per hour.\nFall and winter migration is slower and more leisurely. During this migration, eiders rarely cross land, usually only flying over projecting points of land or headlands, and then only under certain weather conditions, such as snow and onshore winds. The eiders of the inner Gulf of St. Lawrence are an exception, many of them moving first southwest into the St. Lawrence estuary a little downstream of Quebec City and then flying over much of the state of Maine.\nEiders begin to migrate south in the late fall. The timing is largely influenced by freeze-up and by the growth of pack ice, which occurs progressively later as one proceeds south and is a more important influence in the northwest Atlantic than elsewhere. There is generally a supplanting effect: birds from farther north replace those that have bred and migrated south.\nIn June and July, males, or drakes, and nonbreeders make moult migrations to replace their worn-out plumage with new plumage in an area where they will be protected from weather and predators. They often move several hundred kilometres north from their breeding areas. During the moult, the eiders are unable to fly for three to four weeks, and the ducks lose weight as energy is used to grow new feathers. By mid-September, the drakes have resumed flight and are ready to return to the wintering grounds. However, they travel separately from the females and young, and often arrive later at the wintering grounds than these two groups, even though the adult females moult later, in August and September.\nEiders feed during the day by diving to the bottom in waters from 3 to 20 m deep to take mussels, clams, scallops, sea urchins, starfish, and crabs, which are swallowed whole and crushed in the large gizzard.\nIn winter, when daylight is short, more than half the daytime hours are spent in feeding. The ducks feed in shoal waters off headlands and offshore islands and skerries. Flocks move together at the same rate, the ducks at the front of a flock diving first and the rest following sequentially. After 15 to 30 minutes of intensive feeding, flocks move offshore to rest, preen, and digest the contents of the gullet. The feeding sequence is then repeated.\nDuring spring migration, and when the eider ducks arrive near their breeding places, much time is spent feeding, and the birds accumulate fat. These stores are particularly important for the breeding females, or hens, which rely on the reserves through the incubation period. Unlike many ducks, the hen does not feed once she starts sitting on her eggs.\nThe young apparently eat insects during their first week of life.\nEiders return to the breeding islands along the northern coasts as soon as shorefast ice or pack ice starts to dissipate. Many eider ducks are paired when they arrive on the breeding grounds, although some pairing occurs there. Some races remain paired for several years, others do not. Courtship is very intense in spring, with males making displays for the females which include the upward tossing of the head, cooing, neck-stretching and wing-flapping. Courtship continues after pairing in order to maintain pair bonds.\nSome female eiders may breed in their second year of life, but males do not breed until they are three years old. Many females will not breed in some years. Common Eiders breed mainly on small offshore marine islands or isolated spits and points that are free of mammalian predators. Within a couple of weeks of arriving at the breeding grounds, the birds make prospecting flights and visits to choose a suitable nesting place. Often females will use the same nesting site for a number of years, while others choose new nest sites each year. They nest in early summer in dense colonies of tens to 10 000 or more; nesting starts progressively later as one proceeds farther north. There is one brood per season.\nOnly the female prepares the nest. In some races, the male stays with the female for a while; in others, he does not. When he remains, the male defends the female from other eiders and from gulls and ensures that she does not mate with other males.\nThe female begins laying the eggs a couple of days after the nest is ready. There are usually four or five eggs per nest, and generally, one egg is laid per day. When the second or third egg has been laid, the female lines her nest with down plucked from her body. While laying the egges, some females will leave the nesting colony, possibly to feed before they return to the nest to incubate, or sit on the eggs, continuously. Once incubation begins, the female only leaves the nest for a little as five minutes every two or three days to drink, but not to eat. During early egg-laying, if the male is still in the vicinity, he accompanies the female on her breaks. By mid-incubation, most males have left the colony on their moult migrations. Incubation lasts from 21 to 24 days, and about 50 to 70 percent of the eggs hatch successfully.\nThe downy newborns leave the nest within 24 hours, and they feed themselves. Within one hour of entering the water, they can dive competently. Young first fly when they are 60 days old. Generally, few survive to fly; many are lost to predators, exposure, or starvation in their first week of life. In good years, one duckling per adult pair may survive for the fall flight. On the other hand, adults are often long-lived, and estimated annual survival rates vary from 80 to 95 percent. This low reproductive success, which is compensated by high adult survival, is very characteristic of eiders and other sea ducks such as scoters and Long-tailed Ducks. Most other ducks breed more successfully but lose 40 to 50 percent of adults each year.\nThe mother’s relationship with her ducklings ends when she leaves for the moult migration in the autumn. In the fall migration, groups of young may travel together and arrive before the adults on the winter range.\nThe main predators of Common Eiders are large gulls, ravens, American crows, and jaegers, which prey on the eggs and the downy young. Because they nest mostly on small islands, Common Eiders have few mammalian predators. Sometimes arctic foxes or red foxes may inflict some damage, and if polar bears come ashore at nesting colonies, they can destroy all the nests and kill many nesting females.\nCommon Eiders also fall victim to starvation in years of low food supply, and sometimes brooding females will die of starvation during the incubation period when they do not leave their nests.\nHunting has taken a heavy toll. Before the introduction of hunting regulations, breeding stock of Common Eider were greatly reduced, to the point of some local extinction in widely scattered breeding areas. Fortunately, this bird responds well to adequate protection, and not only has reoccupied lost areas but also, in recent decades, has extended its breeding range in western Europe, the British Isles, Atlantic Canada, and elsewhere.\nSuch recoveries in eastern North America began quite soon after the enactment in 1916 of the Migratory Birds Convention between Canada and the United States, which limits hunting of migratory birds. The Common Eider was given special recognition at that time because of the greatly diminished breeding stocks along the eastern seaboard. Eider ducks in most areas recovered dramatically at rates of up to 12 percent per year; for example, in Maine, the number of nests increased from fewer than 100 in one colony in 1910 to more than 20 000 nests in over 75 colonies by 1970. Scientists have not determined how many Common Eiders live in Canada today.\nIn Newfoundland and southern Labrador, breeding stocks have yet to recover to any extent. This is partly because, until 1949, when Newfoundland entered Confederation, these areas were not bound by the provisions of the Migratory Birds Convention Act and partly because of the lack of adequate public education and enforcement of hunting regulations under the Act. Illegal hunting and taking of eggs during the spring and early fall, when the local breeding population is present, has kept the number of local breeders low.\nAnother cause for concern is the harvest in southwestern Greenland of the wintering northern Common Eider (S. m. borealis), which breeds in the eastern Arctic of Canada and in west Greenland. It is thought that the harvest may not be sustainable and that the population there may be in decline.\nIn addition, the Pacific race of common eider (S. m. v-nigra) has experienced dramatic population declines since the 1980s for unknown reasons.\nIn Canada, recreational hunting of eider is permitted during a winter hunting season, even in Newfoundland and southern Labrador. This is because the eiders present on the Newfoundland and Labrador coasts and in the Gulf of St. Lawrence during the winter are not local breeders: they belong to larger Arctic breeding populations.\nIncreased focus on public education and enforcement and the promotion of other uses of eiders, such as the harvesting of down, is laying the groundwork for a recovery of breeding stocks. Eider down is one of the lightest and most effective insulators known. It commands high prices on the world market and is used in products such as parkas, sleeping bags, and comforters. The down can be collected without damaging the ducks or their eggs and nests. Eider down harvesting occurs in the Gulf of St. Lawrence and in Nunavut, and recently there has been interest in the potential of this activity in Newfoundland and Labrador. In Iceland where there are significant eider down farms and the eider down harvest is a multimillion dollar industry, eiders are protected against hunting all year around.\nBellrose, F.C. 1976. Ducks, geese and swans of North America. Stackpole Books. Harrisburg, Pennsylvania.\nGodfrey, W.E. 1986. The birds of Canada. Revised edition. National Museums of Canada, Ottawa.\nGoudie, R. I., G. J. Robertson, and A. Reed. 2000. Common Eider (Somateria mollissima). In A. Poole and F. Gill, editors. The Birds of North America, no. 546. The Academy of Natural Sciences, Philadelphia, Pennsylvania.\nPalmer, R.S. 1976. Handbook of North American birds. Volume 3. Yale University Press, New Haven, Connecticut.\nReed, A., editor. 1986. Eider ducks in Canada. Report Series No. 47. Canadian Wildlife Service, Ottawa.\n© Her Majesty the Queen in Right of Canada, represented by the Minister of the Environment, 1973, 1989, 2003, 2005. All rights reserved.\nCatalogue number CW69-4/72-2003E-IN\nText: Ian Goudie\nRevision: Ian Goudie, 1996; Grant Gilchrist, 2003\nEditing: Maureen Kavanagh, 2003, 2005\nPhotos: Grant Gilchrist', 'The egg has long been a symbol of rebirth and renewal, which goes a long way toward explaining the connection between eggs and the holiday of Easter. In honor of the upcoming Easter holiday, I thought I’d devote this week’s column to some “egg-citing” egg trivia. I’ll try to avoid any additional “egg-sasperating” puns.\nOf course, the familiar Easter egg has traditionally been provided by chickens, although the eggs of other birds, such as ducks, geese and turkeys, are occasionally incorporated into such festivities as egg fights and egg hunts.\nProducing eggs encased in a thin calcium shell is one of the ways that the birds are different from other lifeforms. For example, mammals carry their eggs inside the body with the exception of the oddball echidnas and the platypus. Amphibians lay eggs, but their eggs are soft and must usually be deposited in the water to ensure they do not dry out. Insects also lay eggs. It’s only the birds, however, and a few reptiles that have evolved the hard-shelled egg as a more durable means of reproduction.\nThe shape and size of those eggs varied greatly. Among the world’s 10,000 species of birds, the ostrich lays the largest eggs of any bird. Closer to home, the California condor almost certainly lays the largest eggs of any U.S. bird species. According to the website, All About Birds, eggs of the California Condor eggs are about 4.5 inches long and almost 3 inches wide. These whopper eggs weigh about 11 ounces. By contrast, a large chicken eggs weighs only two ounces.\nThe eggs produced for one nesting attempt are referred to as a clutch. The number of eggs per clutch varies among different species. Among songbirds, some rather small birds lay large clutches of eggs. For instance, the kinglets, which are tiny birds barely bigger than hummingbirds, are champion egg layers. The ruby-crowned kinglet can lay as many as a dozen eggs in a clutch, while the golden-crowned kinglet may lay as many as 11 eggs per clutch. The house wren, which is also a rather diminutive bird, can lay as many as 10 eggs. Several North American wrens produce large clutches of eggs. The marsh wren and sedge wren are known to lay as many as 10 eggs per clutch. The winter wren is almost their equal with clutch sizes that can include nine eggs. Of course, these same birds may lay more average clutches of between three and six eggs.\nThe sora, a small, secretive species of rail, lays 10 to 12 eggs in a nest well-concealed in marsh vegetation. The sora has been known to produce exceptional clutches with as many as 18 eggs.\nWaterfowl are known to be good egg layers. The mallard hen may lay a clutch of eight to 13 eggs. The redhead hen typically lays only seven to eight eggs, but has parasitic tendencies that include depositing some eggs in the nests of other redheads. On occasion, multiple redhead hens get a little compulsive about slipping their eggs into a communal “dump” nest. These type of nests have been found containing as many as 80 or more eggs, but such clutches are impossible for a single hen to incubate and the eggs are usually lost. The wood duck, which will nest in specially designed bird houses placed near a source of water, can lay as many as 14 eggs in a single clutch.\nOther birds concentrate their efforts on laying only a single egg or perhaps a pair of eggs. Many species of albatross lay only a single egg. The parents will dedicate a long period incubating the egg and then tend to the needs of the solitary offspring once the egg hatches.\nHummingbirds typically lay a pair of eggs. The poor female is soon abandoned by her mate and must build a nest, incubate her two eggs, and care and feed her young without any paternal assistance whatsoever. Hummingbirds, having high metabolisms, would find it impossible to feed themselves and any more than two young in a nest.\nPenguins lay only a few eggs. Many species lay a pair of eggs, but the king penguin and emperor penguin are single-egg producers. In their harsh environment, these penguins would find meeting the needs of more than one offspring at a time quite impossible.\nLet’s talk color. Eggs come in a variety of colors. Some eggshells also feature intricate patterns and splotches.\nThe killdeer, which usually lays four eggs, produces buff-colored eggs with dark mottling on the shell. These eggs blend remarkably with gravel and other rocks, which helps the ground-nesting killdeer hide their nests from would-be predators.\nThe American robin is famous for producing a clutch of beautiful blue eggs. The coloration of the eggs have even given rise to the descriptive phrase “robin’s egg blue.” This particular shade of blue is described as a variable one that leans to greenish-blue that is paler than turquoise and more blue than aqua.\nThe American robin’s blue eggs are not unusual among the thrushes, the group of birds of which the robin can claim kinship. For instance, the Eastern bluebird’s eggs are pale blue or sometimes white. The wood thrush produces eggs that are more to the turquoise shade of blue while the veery’s eggs can vary between green and pale blue.\nThe blue tint to the eggs of robins and other thrushes is caused by a pigment that gets applied to the shell as the female lays the eggs. It’s like the female robin has an internal egg-dyeing mechanism. In an article published in May of 2012, Science Daily reported that a study conducted by Queen’s University also found that the brighter blue the egg, the more the male robin will apply himself to caring for his offspring.\nAmong the birds that use my birdhouses for nesting, I’ve always thought the eggs of Carolina chickadees are exquisite. The eggs are pale white with a hint of pinkish coloration. A fine mottling of brown spots dot the shell. The amount of mottling can vary from egg to egg in the same clutch. These tiny eggs produced by a tiny bird are kept safe and sound in a nest of moss and other plant materials with a soft lining of animal hairs. Chickadees lay between three and 10 eggs, but between five and eight eggs per clutch seems more usual.\nAs mentioned, the ostrich lays the largest egg. Ostrich clutches are also impressive. The wild African ostrich, the largest bird in the world, lays 12 to 18 eggs. The ostrich has been domesticated, however, and hens have been conditioned to produce more eggs than would ever be expected in the wild.\nThe ostrich belongs to the ratite family, which includes other large birds such as the emu, rhea and cassowary, as well as five species of kiwi, which are considerably smaller than other ratites. The kiwi does have one interesting claim to fame when it comes to eggs. Kiwi are notable for laying eggs that are extremely large in relation to their body size. A kiwi egg may equal 15 to 20 percent of the body mass of a female kiwi. Perhaps not surprisingly, most kiwis lay only a single egg per clutch. The brown kiwi, however, normally lays two eggs per clutch.\nAepyornis, which was a giant, flightless ratite native to Madagascar and now extinct, produced the largest egg of any bird. Also known as the elephant bird, Aepyornis produced an egg with a volume equal to slightly more than 150 chicken eggs. These giant eggs boasted a circumference of 3 feet, 3 inches and reached about 13 inches in length. Research indicates that humans drove these enormous birds into extinction. One theory is that humans feasted so heavily on the giant eggs produced by Aepyornis that the birds were unable to sustain their species.\nHumans have long turned to eggs as a source of nourishment, but we do not feed exclusively on eggs. Animals that feed primarily on eggs are classified by experts as ovivores. Some ovivores include fish, snakes, insects and, yes, birds. Some birds have become quite adept at preying on the eggs of other birds. Blue jays and American crows are known for robbing nests for both eggs and nestlings of other birds.\nThe bird egg has become firmly interwoven into human culture and traditions. According to the website Aghires.com, estimates indicate that about 180 million eggs are purchased each year in the United States specifically for the Easter holiday. Chicken hens must feel really overworked at this time of the year.\nOf course, there are also the Easter traditions of chocolate and peanut butter eggs. I suppose that eases the pressure slightly on the overworked hens.\nI’m reminding readers that I want to hear from them when they see their first hummingbird of spring. Email me your observations at firstname.lastname@example.org or post them on my Facebook page at http://www.facebook.com/ahoodedwarbler. Please include the date and the approximate time of your sighting.\nObservations will be accepted through Sunday, April 18. Winners will be announced in this column on April 28.']	['<urn:uuid:9e7f3d1e-ef9e-4c2e-9729-b31cf0eb9fce>', '<urn:uuid:a8250970-9e4c-41e5-bc28-e7b7b1a21677>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T00:43:36.053682	14	73	4222
62	bionic eye microbot retinal therapy applications	Bionic eyes and microbots have multiple applications in retinal therapy. The Argus II bionic eye system uses a retinal implant with 60 electrodes to help patients with retinitis pigmentosa see in black and white. For surgical applications inside the human eye, microbots can be tracked using a mechatronic vitreoretinal ophthalmoscope that images the entire retina. This tracking system uses trained color histograms and colorspace evaluation to distinguish the microbot from the rest of the eye, despite challenges with image clarity and lighting.	"['Could bionic eyes restore sight to the blind and give the U.S. military super sight?\nBionic implanted eyeballs, “Star Trek”-style visors, telescopic contact lenses ... these are just a few of the many exciting projects underway to both restore and provide enhanced sight.\nSignificant strides have been made in tech that will restore and transform lives - replacing white canes, service animals, braille machines and more for the visually impaired.\nThere has been a lot in the media about prosthetic breakthroughs for U.S. veterans, but what about vision? Last year the Blinded Veterans Association told the House and Senate Committees on Veterans Affairs that there are an estimated 131,580 legally blinded veterans in the U.S., citing data from the Depatment of Veterans Affairs.\nTechnology is being increasingly harnessed to overcome blindness. So far, much of the key progress has been restricted to restoring sight for those with a specific type of visual impairment – in particular retinitis pigmentosa – an inherited condition that involves the loss of cells in the retina and causes a decline in vision.\nBIONIC EYE IMPLANTS\nOne of the first and most promising “bionic eyes” is the Argus II made by Second Sight, which is geared toward patients with retinitis pigmentosa.\nThe system has two parts: a very high tech retinal implant and a camera mounted on eyeglasses or shades. The “bionic eye” is surgically implanted in, and on, the eye. It has an antenna, an electronics case, and an electrode array.\nThe camera processes what it sees and sends it to a small computer that the person wears. The data is processed and translated into instructions that are sent wirelessly to the antenna in the implant.\nThe retinal implant has 60 electrodes in it. These electrodes provide information to the optic nerve and the optic nerve relays the data to the brain. The optic nerve understands this data as shapes, light and movement.\nThis vision is not yet like normal sight and it will not restore vision to 20/20. But with Argus II, folks who were once sightless can see in black and white – they can read a book and see their homes and loved ones for the first time. More advances are in the pipeline for Argus II to restore color as well as resolution and brightness.\nArgus II bionic eyes require functioning retina so many, including many visually impaired veterans, can’t take advantage of that tech – Second Sight’s Orion technology could be the solution.\nPLUGGING INTO THE BRAIN\nBy skipping the optic nerve and directly plugging into the visual cortex, Orion could hold enormous potential for veterans who have visual impairment due to trauma.\nIn fact, this approach could potentially help those blinded by cancer or glaucoma.\nThis new device bypasses the retinal layer and implants electrodes directly onto the visual region of the brain.\nSecond Sight announced a major breakthrough for its Orion I project late last year. In a trial at UCLA, the very first of these devices to directly plug into the brain, a wireless visual cortical stimulator, was implanted in a human subject. The test was a success and restored vision to a 30-year old patient with no major side effects.\n‘STAR TREK’-STYLE VISOR\nEver seen “Star Trek?” One American company has created a sort of real-life version of character Geordi La Forge’s “visor.”\nWith the eSight 3 device, the wearer can see full-color video images without a time lag. Wherever the user looks and whatever he or she looks at, the high-speed, high-def camera captures it for them.\nAdvanced algorithms are used for the video feed. The video is then displayed on two screens in front of the wearer’s eyes. The video image is provided in a way that overcomes their vision loss.\neSight isn’t a cure-all at this point. If the retina damage is too severe, then it may not work. It tends to be more helpful with macular degeneration, for example, than glaucoma. The company says the technology has about a 50 percent chance of working with all conditions.\nGIVING SOLDIERS SUPER SIGHT\nAdvances in this field are also creating the potential to give US warfighters super vision.\nOne exciting example is a new contact lens funded by DARPA, and made by École Polytechnique Fédérale de Lausanne, that gives the wearer the ability to zoom like a telescope.\nThe scleral lens has thin aluminum mirrors built into it that work with special liquid crystal glasses. These glasses are connected to an electronic system.\nThink “Mission Impossible.” If you blink your right eye, then it allows magnifying – but if you wink your left then the vision is normal. If you blink normally, it doesn’t trigger the magnifying mode.\nIn addition to the contact lens, other projects have made great headway. Even Second Sight bionic eyes can see in IR with a specific input device.\nAugmenting soldiers with vision-enhancing tech could provide advantages for ground troops and special operations in particular. Warfighters could switch between seeing in night vision, infrared, thermal, zoom, telescopic and more. Whether worn or implanted, it would provide enhanced capabilities that remove the weight of carrying optics and the time lost shifting optics by switching instead at the speed of thought.\nJust one specific illustration of how helpful this could be is explosives. If the amazing advances in explosive detection could be miniaturized and adapted for military bionic eyes, then warfighters with enhanced vision could scan and spot these hidden IEDs before they could strike – putting an end to injury and death due to IEDs.\nMeet a Green Beret who was blinded in combat, but still serves, shoots with remarkable accuracy and explored Antarctica with Prince Harry at Tactical Talk this week.\nAllison Barrie consults at the highest levels of defense, has travelled to more than 70 countries, is a lawyer with four postgraduate degrees and now the author of the new book ""Future Weapons: Access Granted"" covering invisible tanks through to thought-controlled fighter jets. You can click here for more information on FOX Firepower columnist and host Allison Barrie and you can follow her on Twitter @allison_barrie.', 'Distinct from this approach is a screw shaped microbot with\nembedded magnetic material that physicians could some\nday drive through tissue wirelessly. This approach can\nenable moving through soft tissue and not just inside veins.\nThis approach requires the helix to make only one full\nrotation to move the length of the microbot through\nhuman tissue. The other helix-propelled bots must turn the\nhelix several times to move through human fluid.\nAnother propulsion method for microbots is the\ntraveling wave. Traveling wave propulsion looks like a fish\ntail swimming behind the microbot. Traveling waves are\nmore efficient at propulsion than propellers, assuming the\nefficiency of the actuation is equal to that of the propeller\nsystem. However, there are fabrication, power, and control\nissues related to creating distributed actuation for the\ntraveling wave method. Methods that create traveling\nwaves without distributed actuation are more feasible but\nMini Medical Robots\nIn another approach to microbots, researchers are\nadding active locomotion and telemetry to capsule\nendoscopy technology. According to IRIS, the size of the\ncapsule cannot be larger than what a person can swallow.\nIn this new method, the larger robot would be swallowed\nin individual pieces that would assemble themselves inside\nthe gastrointestinal tract. Self-assembly is the biggest\nchallenge to this approach.\nBy using magnets on the faces of the modules that\nInstitute of Robotics and Intelligent Systems\nResearch of Professor Bradley J. Nelson\n12 SERVO 07.2011\nThis is an image of the Octomag. In the background,\nyou can see an image as viewed through an ophthalmic\nmust connect to one another, researchers hope to solve the\nself-assembly problem. The results are promising for a\nsnaking robot that can assemble itself and make its way\nthrough the small intestine.\nThe self-assembling robot would have sensors on its\nfront end to detect pathologies that would wirelessly\ncommunicate to the surgeon or physician. A module\ndesigned for specific tasks would execute commands to\naccomplish the task once an actuation module moves the\nrobot into the right place. Sensors and actuators that work\non individual pill sized devices are already being developed.\nEach sensor or actuator could eventually constitute a\nmodule in the larger robot discussed here.\nEye See the Robot\nOne application of microbots is retinal therapies where\nthe robot could help the surgeon operate inside the human\neye. The primary challenge to this approach is localization.\nBecause researchers and clinicians can easily see what is in\nthe eye, they can look through the eyeball to localize the\nrobot. Using high quality imaging of the rear of the human\neye, they can develop algorithms to track and localize the\nmicrobots. However, there is no existing research that\nexplains what the effects of this will be.\nIRIS researchers have developed a mechatronic\nvitreoretinal ophthalmoscope that can take an image of the\nentire retina despite the obstruction of view created by the\nother parts of the eye. They can use this device to image\nand follow objects in the retina. There is on-going research\nto discover how much illumination will be necessary to\nfollow objects in the human eye.\nThe imaging device must be able to differentiate the\nmicrobot from the rest of the retina. The mechatronic\ndevice must use several lenses to see beyond the other\ncomponents of the eye to keep the retina in view. Images\nare still blurry much of the time. The light that reaches the\neye’s interior can decrease the quality of the images taken\ndue to uneven brightness. The microbot has no distinctive\ncolors to distinguish it from the rest of the eye.\nIRIS researchers have surmounted many of these\nobstacles by using trained color histograms, colorspace\nevaluation, and maximum separability thresholding.\nAccording to IRIS, a real time color tracker in the right color\nspace assures that the microbot can be separated from the\nrest of the eye.\nProduction-quality microbots are on the horizon. Soon,\na variety of surgeries will be possible using these\nmagnificent miniaturized microbots. SV']"	['<urn:uuid:66f85023-683f-43be-ad1f-0a9fe160ac16>', '<urn:uuid:9906372f-3ed2-47aa-93fe-e8956ee45584>']	open-ended	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-13T00:43:36.053682	6	82	1676
63	What role do precise calculations and proper documentation play in both employment garnishment proceedings and veterinary toxicity cases?	Both scenarios require extremely precise calculations and proper documentation. In garnishment cases, employers must make accurate disclosures about amounts owed and follow specific formulas for withholding, as demonstrated in the Michigan Court Rules. Failing to calculate correctly can result in the employer becoming liable for the full judgment amount. Similarly, in veterinary toxicity cases, precise calculations are crucial - as shown in Maple's case, where the veterinarian had to calculate the exact toxicity potential using the formula of 1IU = 0.025mcg and 1mcg = 0.001mg to determine if the 20 gel caps of either 400IU or 5,000IU would reach toxic levels. In both cases, proper documentation and monitoring are essential - garnishment requires proper legal documentation while toxicity cases require ongoing monitoring of blood work and vital signs.	"[""The Michigan Court of Appeals has published another employment related case, this time about the whacky world of garnishments. I say whacky because an employer can incur the employee’s debt if the garnishment withholding is not handled properly.\nThat’s what happened in Premiere Property Services, Inc v Crater. Let’s take a look so that you don’t make the same mistake.\nThe plaintiff, Premiere Property Services, served a “writ of periodic garnishment” on garnishee defendant True North Painting in order to satisfy a judgment it had obtained against the defendants, Matthew Crater, Fresh Outlook Painting and Better Brush Painting.\nBefore we get too deep into the case details, let’s take a step back and see how this is supposed to work for those who aren’t familiar with the process.\nPremiere Property would have filed a previous lawsuit against Matthew Crater, Fresh Outlook and Better Brush and succeeded in obtaining a judgment in that litigation. But obtaining a judgment is not the end of the story. The plaintiff must collect on the judgment. Sometimes a defendant will just pay the plaintiff the amount of the judgment (or negotiate a lower amount so that the plaintiff doesn’t have to jump hurdles or defend the judgment on an appeal, etc.).\nBut, if the defendant does not pay, the plaintiff takes action such as placing a lien on property or garnishing the earnings of the defendant. That’s how “garnishee defendant” True North Painting became involved, and it did not end well for the company.\nThe process is described in the Michigan Court Rules, Subchapter 3.100 and is regulated by statute, MCL 600.4001, et seq. But, generally, a “periodic” garnishment is required when the total judgment cannot be collected all at once. This is often the case when the garnishment is against an employee’s paycheck because a formula limits the amount employers can withhold from each paycheck. Let’s follow the employer/employee example to keep it simple.\nWhen served with a writ of garnishment, the garnishee (employer) is required to make certain disclosures about the amounts it owes (wages) to the original defendant (employee). Based on the disclosures and the formula, the amount of the periodic garnishment (money to be withheld from the employee’s paycheck) is established and paid “periodically” by the employer to the original plaintiff. But, if the garnishee/employer fails to follow the process correctly, it (instead of the employee) can end up owing the amount of the judgment to the plaintiff.\nLet’s go back to our case and pick up where plaintiff Premiere served garnishee True North Painting with the writ of garnishment. True North acknowledged in its disclosures that the three defendants, Crater, Fresh Outlook and Better Brush, were its subcontractors and that it owed money to them.\nHowever, instead of withholding the full amount of the garnishment, it only withheld 25% of it ($7,610.62) for the plaintiff and paid the remaining 75% ($22,746.64) directly to Crater. Apparently, True North was under the mistaken belief that the amount was payable to Crater as “wages” owed to him as an employee. Crater then filed for bankruptcy. This placed a stay on the proceedings.\nSadly, several trees likely died as a result of the court filings that followed. Because the other two defendants did not file for bankruptcy, Premiere argued that it could still garnish the $22,746.64 from True North to Better Brush.\nIn the end, the lower court ruled that it would be inappropriate to order True North to pay plaintiff the $22,746.64 it had already paid to Crater since it could seek payment directly from Crater in the bankruptcy court (at pennies on the dollar).\nThe Michigan Court of Appeals saw things differently. Ignoring the bankruptcy, it found that True North was liable because it had disregarded the writ of garnishment by paying Crater. A writ of periodic garnishment continues until the original judgment is satisfied.\nA garnishee is liable for the amount of the garnishment to the extent it owes a debt to the defendant(s). The lower court found that once Crater had been paid, there was no further debt owed to the defendants and, therefore, nothing from which True North could withhold. But this is the same as saying that “a garnishee who violates a writ of garnishment by making payment directly to the defendant cannot be held liable because it is no longer in possession of an obligation owed to the defendant” and is erroneous circular reasoning.\n“Garnishees’ duties and obligations under the rules and their potential liability to the plaintiffs attach at the time they are properly served the writ. They then become responsible for the timely performance of the specific duties imposed…at the risk of default judgment against them which may be executed against their own funds or property…” Chayka v Brown, 92 Mich App 360 (1979).\nWhile it’s true that a garnishee’s liability is determined by the obligation it owes to the defendant (often the paycheck, but in this case the contract amount), the lower court was simply wrong by concluding that a garnishee can “escape all liability by turning over property and paying obligations to the defendant in violation of the writ.” The appellate court said this would be an “absurd” result.\nThe fact that True North had made a good faith mistake and failed to accurately understand it legal responsibilities does not deny plaintiff its recovery. “Allowing a garnishee to transfer garnished funds to a defendant without risk of liability could greatly frustrate” a key mechanism for prevailing parties to enforce their judgments.\nWhile the bankruptcy applied to the garnishment directed at True North as against Crater, Better Brush and Fresh Outlook were not in bankruptcy. Thus, the case was remanded to the lower court to determine whether the payments made to Crater were actually owed to Better Brush or Fresh Outlook and, therefore, unaffected by the bankruptcy.\nYou can see that an “inadvertent noncompliance” with the writ of garnishment can be a costly error. It is crucial to get it right!\n- Of Counsel\nAn of counsel attorney in the firm’s Detroit office, Claudia D. Orr exclusively represents and advises employers and management in employment and labor law matters.\nMs. Orr has an ever-growing practice in Alternative Dispute ...\nAdd a comment\n- Employment Liability\n- Labor Law\n- Employment Discrimination\n- Human Resources\n- Equal Employment Opportunity Commission (EEOC)\n- Wage & Hour\n- Employment Agreement\n- Department of Labor (DOL)\n- Fair Labor Standards Act (FLSA)\n- Regulatory Law\n- OSHA Issues\n- Department of Justice\n- Family Medical Leave Act (FMLA)\n- Americans With Disabilities Act (ADA)\n- Hostile Work Environment\n- National Labor Relations Act\n- Title VII\n- Business Risk Management\n- Noncompete Agreements\n- Sick Leave\n- Medicare Issues\n- Workplace Harassment\n- Minimum Wage\n- National Labor Relations Board\n- Paid Medical Leave Act (PMLA)\n- Workers' Compensation\n- Transgender Issues\n- Unemployment Benefits\n- Whistleblower Protection Act\n- Sexual Harassment\n- Civil Rights\n- Social Media\n- Class Actions\n- Retail Liability\n- Emergency Information\n- Uniformed Services Employment and Reemployment Rights Act (USERRA)\n- Department of Education (DOE)\n- Tax Law\n- Title IX\n- Medical Marijuana\n- Right to Work\n- Health Insurance Portability and Accountability Act (HIPAA)\n- Union Organizing & Relations\n- DOJ Issues Guidance on ADA, Opioid Crisis Issues\n- Congress Passes Law, With Retroactive Effect, to Invalidate Forced Arbitration Provisions at the Employee’s Election\n- U.S. Supreme Court Temporarily Blocks Implementation of Vaccine Requirement for Large Businesses\n- Contractual Limitations Periods STILL Alive and Well... on job Applications!\n- Federal Appellate Court Takes Brakes Off COVID-19 Vaccine Mandate Carousel\n- Employers: Hang on for Another Spin Around the Vaccine Mandate Carousel\n- Stray Comments can Lead to Employer Liability\n- President Biden's COVID-19 Vaccine Mandates Face Uncertain Future\n- Employment Noncompete Agreements Enforced Badly\n- Feds Release New COVID-19 Vaccination Rules for Large Employers, Medicare/Medicaid Providers"", 'Reference, full author’s credit and bibliography: https://www.iowaveterinaryspecialties.com/student-scholars/vitamin-d-toxicosis\nVitamin D Toxicosis\nWritten by: Lena R.\nA 7 year old, female spayed, 62lb Labrador Retriever named Maple presented for ingestion of 20 gel caps of Vitamin D-3 supplement of unknown concentration- either 400IU or 5,000IU per cap. Maple had vomited three times at home, with some of the caps contained within the vomitus, and the owners called Pet Poison Helpline.\nOn physical exam, Maple appeared healthy, her abdomen was not painful on palpation, and her heart rate was within normal limits. She was not lethargic or depressed at that time.\nDid Maple consume enough of the supplements to cause clinical signs? Symptoms of Vitamin D toxicosis can be seen with doses as low as 0.1mg/kg, and a lethal dose in a mature dog can be as low as 2mg/kg (4). Maple weighed 28kg on entry. 1IU = 0.025mcg, and 1mcg = 0.001mg (3).\nIf the gel caps were 400IU: 400IU x 0.025 = 10mcg x 0.001 = 0.01mg. If she ate 20 gel caps, the total dose she consumed would be 0.2mg. For a 28kg dog, that dose would be non-toxic at 0.007mg/kg.\nIf the gel caps were 5,000IU: 5,000IU x 0.025 = 125mcg x 0.001 = 0.125mg. If she ate 20 gel caps, the total dose she consumed would be 2.5mg. For a 28kg dog, that dose would be non-toxic at 0.08mg/kg.\nHowever, with the impossibility of knowing exactly what Maple consumed, the safest choice was to hospitalize her with presumed Vitamin D toxicosis.\n- Hospitalize for 72 hours with IV saline to diurese the kidneys.\n- UAA gel with sorbitol.\n- CBC, blood chemistry panel, iStat to check free calcium, and urinalysis.\n- Recheck iStat free calcium and phosphorus every 12 hours.\n- Recheck CBC/chem every 24 hours to monitor renal values.\n- Oral medications: furosemide, to promote calcium excretion by the kidneys (2), and famotidine, as a gastrointestinal protectant.\n- Optional: adding oral prednisone (a corticosteroid), to reduce bone reabsorption of calcium and decrease absorption of calcium in the GI tract (3).\nMaple was hospitalized on IV fluids for 72 hours. Her total calcium and phosphorus levels remained within normal range for the duration of her treatment, and her renal values (BUN and creatinine) were within normal limits on exit. She was discharged with the recommendation to recheck renal values after an additional 48 hours, to ensure there would be no lasting effects from the incident.\nVitamin D toxicosis is a potentially life-threatening condition that causes increased reabsorption of calcium leading to renal damage. Clinical signs include abdominal pain, renal pain on palpation, depression, bradycardia (increased heart rate), vomiting and diarrhea (1). Once blood work is performed, findings include hypercalcemia, hyperphosphatemia, hypokalemia, and azotemia.The most common cause of vitamin D toxicity in dogs is ingestion of cholecalciferol rodenticides (4).\n- Tilley, Lawrence P., Francis W. K. Smith, and Lawrence P. Tilley. “Vitamin D Toxicity.” Blackwell’s Five-minute Veterinary Consult: Canine and Feline. Ames, IA: Blackwell, 2007. 1317-319. Print.\n- Plumb, Donald C. “Furosemide.” Plumb’s Veterinary Drug Handbook. Stockholm, WI: PharmaVet, 2011. 454-57. Print.\n- “Cholecalciferol: Rodenticide Poisoning.” Veterinary Manual. Mereck, Oct. 2014. Web. 02 Aug. 2016.\n- Peterson, Michael E., and Patricia A. Talcott. “Chapter 38: Cholecalciferol.” Small Animal Toxicology. St. Louis, MO: Saunders/Elsevier, 2006. 489-98. Print.\nEnter your email address for a free PDF of this article including its images.Enter your Email Address\nMy Favorite Amazon Recommendations\nItems and content will not show in ""Reader View"" check your browser.\nPraziPro for Flukes\nBuy Some Good Koi?\nBest Food, Ever\nPro Air 60 Aerator is a VERY high output air pump pushing my whole fish room (17+ drops), and I have one at home driving everything there, and I have one as a back up.\nTopical For Koi and Pond Fish Ulcers\nKoi Health & Disease\nI have over ten of these Titanium Heaters in my fish room and at home. They\'re a paradigm shift in aquarium heating. They\'re titanium and 400W for under $30! Whaaaaaaaaat?\nCyroPro by Hikari is safe and easy for Anchor Worms and Fish Lice.\nWhatever heaters you use, back yourself up with a temperature controller, it\'ll turn on, and off your heaters. If your heater seizes ""on"" at least the thermostat will stop a tragedy.\nLifeGuard by Tetra is a tablet version of a tame \'chloramine-t\' or \'halamid\' compound that\'s easy to get and good on bacterial infections, in baths.\nFor me, filtration-of-choice\nRubber sided, round, nettable tanks\n1200 Watt heater, on a thermostat held securely above the waterline, works GREAT as a large tank heater. That\'s all I\'ll say because there\'s risk of electrocution and all that. But then, even in a bucket you could get killed. Sound like fun?\nWhat Does Ajax Eat?']"	['<urn:uuid:88839e0c-cf48-48b3-b2ca-f73a7f573fa2>', '<urn:uuid:a40821cc-bd77-4e4d-88c2-e7b180d8a4e9>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	18	128	2106
64	Working with emerging technologies like smartwatches, I'm wondering how ROSI calculations can be applied to evaluate security measures for wearable devices. What insights can it provide?	ROSI calculations can be effectively applied to evaluate security measures for wearable technology like smartwatches, which face similar security vulnerabilities as standard wireless network resources. When considering security investments such as third-party antimalware applications and advanced security features for these devices, ROSI helps determine if the investment in security control technology will result in savings compared to predicted losses without protection. For low-cost security applications on wearable devices, ROSI calculations may indicate that these security measures can be very cost-effective, providing substantial savings against potential future losses from device security breaches.	['Question 1. Information systems have become essential components in most organizations since they enhance efficiency in completing business and organizational operations. The security of this information systems is equally important to ensure their optimum functionality. Most organizations have heavily invested in information systems ranging from enterprise resource planning systems (ERP) to customer relationship management systems among others. These investments will be futile if their security is not guaranteed. Consequently, it is the role of the Computer Emergency Response Teams (CERT) together with the CISO to identify the security measures to be implemented in the organization. Information security efforts and implementation must be factored in the budget to meet various activities and procedures that go into the implementation. It is therefore important to consider the returns on investment that will be realized from information security.\nCERTs need to determine their cost-effectiveness in order to justify their budget usage as well as provide supportive claims for the next budgetary allocations. According to ENISA (2012), organizations mainly face challenges while accurately determining the cost and effectiveness of the respective information security activities. This is attributed to the fact that information security investment does not yield profits but it is a loss prevention investment. Despite this challenge, it is important that the organizations have a clear understanding of the benefits and the importance of investing in the security of information systems. Hence the need for Returns on security on investment calculation (Cavusoglu, Mishra & Raghunathan, 2004).\nReturn on Investment (ROI) calculation is essential for justification of every budgetary allocation in any organization (Cavusoglu, Mishra & Raghunathan, 2004). Security is an investment in both public and private entities. Thus, the return on security investment (ROSI) must be done to determine and justify budgetary allocation for information system security. The ENISA work program states that the executive decision makers should know the impact of security. This will assist them to understand how much they should spend on security and how much the lack of security is going to cost. Additionally, they should identify the most cost-effective solutions that can be adopted (ENISA, 2012). This is only possible with a ROSI calculation.\nAccording to Sonnenreich, Albanese & Stout (2006), ROSI calculation will provide substantial quantitative answers to fundamental questions such as the amount an organization is paying for cyber security, the impact of the lack of cyber security in the organization and whether the security investment is enough. The method uses estimated potential loss (ALE), estimated risk mitigation and cost of the solution variables. This metric can also be applied to the Big Data Analytic (BDA) technology to determine its cost and the potential returns on investment (ENISA, 2012). ROSI calculations can be used to determine whether and organization particularly security companies can implement the technology and be able to realize returns on investment. Most notably, it enables decision makers in this respect to identifying the improvements to be gained from the technology and the losses from lack of implementation.\nROSI calculation has some limitations that are mainly centered on the drawbacks of estimation and the Gordon and Loeb model of calculation. ROSI calculation is based on estimations which make it difficult to estimate the cost of cyber security incidents which vary depending on the environment. Additionally, the ROSI calculation can be manipulated very easily to serve the interest of the user. Secondly, the Gordon and Loeb model popularly used in the metric is an approximate model hence the resulting numbers must be treated with caution (ENISA, 2012).\nCavusoglu, H., Mishra, B., & Raghunathan, S. (2004). A model for evaluating IT security investments.Communications of the ACM,47(7), 87-92.\nEuropean Network and Information Security Agency. (2012). Introduction to Return on Security Investment: Helping CERTs assessing the cost of (lack of) security. Heraklion, Crete, Greece: Author. Retrieved from https://www.enisa.europa.eu/activities/cert/other-work/introduction-to-return-on-security-investment/at_download/fullReport\nQuestion 2. Return on security investment (ROSI)-a practical quantitative model.Journal of Research and Practice in Information Technology,38(1), 45-56.Since network security products do not generate revenue, it is necessary to evaluate their worth by considering money saved from potential loss.The concept of Return on Security Investment (ROSI) provides a quantitative value in reference to the benefits attained through an investment in network security technology.“ROSI is calculated by the amount of risk reduced, less the amount spent, divided by the amount spent on controls, resulting in the net amount of risk per amount of control,” (Lindstrom, 2017).This value provides an indicator if the cost for security controls will outweigh the potential liability for penalties incurred due to network security breaches throughout the year.\nCybersecurity technologies are the primary line of defense for network intrusions.It is essential that an organization remain abreast with the latest security software and strategies to defend against emerging security threats.Acquiring the most effective technology requires a cost benefit analysis to ensure a reasonable correlation of mitigated risk is met.ROSI is used to evaluate whether the amount of potential money saved is higher or lower than the recommended security control investment, (Schneier, 2008).This resulting loss or gain will provide guidance as to whether it is cost effective to purchase the recommended security measure or not.\nNetwork security threats are imminent to organizations and network resources everywhere.Increased accessibility of a network resource is generally parallel to an increase in security vulnerabilities.It is essential that all input values be as accurate as possible to assume an effective value of investment.“Unfortunately, the cost of cyber security incidents and annual rate of occurrence are hard to estimate and the resulting numbers can vary highly from one environment to another.These approximations are often biased by our perception of the risk and the ROSI calculation can be easily manipulated,” (ENISA, 2012).The routine probability for error limits the potential effectiveness of the ROSI metric and should be analyzed on a case by case bases.\nThe ROSI calculation may even be utilized to evaluate the benefits of acquiring appropriate security measures for wearable technology.Take the smartwatch for example, as this product endures similar security vulnerabilities to a standard wireless network resource. There are third party antimalware security applications that may be purchased in addition to the use of advanced security and password features on the device, (Thomas, 2015).It is still necessary to confirm whether this investment in security control technology will yield savings or excess expenditures compared to the predicted losses without it.In this case, the resulting ROSI value may indicate that these low costing security applications may prove to be very cost effective, providing substantial savings against any potential futures losses incurred by a device security breach.\nENISA. (2012). Introduction to return on security investment. Retrieved from\nLindstrom, P. (2017). Return on security investment: the risky business of probability. Retrieved\nSchneier, B. (2008, September 2). Schneier on security: security ROI. Retrieved from\nThomas, K. (2015, April 15). How secure is your smartwatch? Retrieved from']	['<urn:uuid:a6dac902-24ff-4053-b29f-22183510940a>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	26	91	1127
65	What kind of jobs can people with history degrees get?	History graduates can work in journalism, law, education, government service, military, business, politics, museums, cultural organizations, non-profits, communications and public relations.	['Student Career Information & Learning Outcomes\nHistory as a field of study develops habits of mind, values of civic engagement and citizenship, and specific skills that have served our students well in their later avocations and endeavors. Recent graduates have found work in journalism, law, all levels of education, government service and the military, business, politics, curatorial and management roles in museums, cultural organizations, and other non-profits, and communications and public relations. To these careers our students bring their passion for service to our communities, critical thinking skills to tackle perplexing and complicated problems to make creative solutions, and written and oral communication expertise and research abilities.\nPlease use this link to connect with UB Career Services. The department partners with Career Services to help students obtain internships, especially those that provide experiential learning but are not explicitly associated with history, like with our public history internships. Career Services can assist with developing internships and mentor relationships with local alumni in many fields.\nPlease see the American Historical Association for a general guide to careers for history majors.\nThe undergraduate and graduate programs in History enable students to explore the past with open-mindedness, thoroughness, and rigor; interrogate explanations and interpretations of the past; pursue original research in order to develop a deeper understanding of past events, people, and societies; and develop critical thinking, writing, and reading skills vital to future studies and careers.\nOver the course of the major, students will develop their historical knowledge, habits of thinking, and skills through the required courses, summarized in these learning outcomes. Students take courses that are organized by “level” and by breadth requirements that are outlined in the curriculum maps. Learning outcomes and assessments of individual courses draw on and the plan for assessment of student achievement corresponds to these outcomes.\n- Develop a wide range of historical information\n- identify the key events which express/define change over time in a particular place or region\n- identify how change occurs over time\n- explain historical continuity and change\n- describe the influence of political ideologies, economic structures, social organization, cultural perceptions, and natural environments on historical events\n- discuss the ways in which factors such as race, gender, class, ethnicity, region and religion influence historical narratives\n- Recognize the past as distinct from the present\n- explain how people have existed, acted and thought in particular historical periods\n- explain what influence the past has on the present\n- Emphasize the complex nature of past experiences\n- interpret the complexity and diversity of situations, events and past mentalities\n- compare eras and regions in order to define enduring issues\n- Emphasize the complex and problematic nature of the historical record\n- recognize a range of viewpoints\n- compare competing historical narratives\n- challenge arguments of historical inevitability\n- analyze cause-and-effect relationships and multiple causation\n- Develop skills in critical thinking and reading\n- evaluate debates among historians\n- differentiate between historical facts and historical interpretations\n- assess the credibility of primary and secondary sources\n- Develop research skills\n- formulate historical questions\n- obtain historical data from a variety of sources\n- identify gaps in available records\n- Develop the ability to construct reasonable historical arguments\n- write a well-organized historical argument\n- support an interpretation with historical evidence from a variety of primary and secondary sources\n- Develop oral communication skills\n- present research and findings in clear and compelling fashion\n- develop critical skills of analysis through discussion of primary and secondary sources\nWe have developed these outcomes using an example from Utah State University, the American Historical Association’s on-going project on assessment, and SUNY guidelines.']	['<urn:uuid:b8a98e07-7175-4eb1-819f-6d3931feaef2>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	10	21	606
66	compare frequency check transmission levels vs investment mix	For transmission fluid, it's important to check levels regularly through manual inspection, while watching for warning signs like rough shifting or unusual noises that indicate low fluid. Any leaks should be addressed immediately. For investment portfolio checks, an annual review is typically sufficient, ideally at year-end. However, while transmission fluid issues require immediate attention to prevent severe damage, investment rebalancing can wait until allocations drift 5-10 percentage points from targets. The transmission requires more frequent monitoring because its fluid is critical for ongoing operation, while investment portfolios can tolerate more variation before requiring adjustment.	"[""How should I decide whether it's time to rebalance? Should I be rebalancing every year?Answer:\nInvestors often wrestle with whether to take a strategic (that is, buy-and-hold) or tactical (active and hands-on) approach to asset allocation. Regular readers know that I generally favor the strategic approach, and I outlined some specific guidelines in this article\n. But rebalancing--the process of putting your portfolio's asset allocations back in line with your strategic targets--is a disciplined way to add a tactical element to a buy-and-hold asset-allocation strategy. Rebalancing helps ensure that you peel money away from your winners and add to your losers, and it's one of the best ways to improve your portfolio's return and improve its risk level.\nBut even investors who are sold on the merits of rebalancing often struggle with how often to do it. David Swensen, the manager of Yale's endowment, has argued in favor of frequent, even daily, rebalancing. That approach doesn't translate to the world of retail investors, however. Individual investors may have to pay taxes and transaction costs to rebalance, and such a rigorous rebalancing program would take up a ridiculous amount of time, too.\nSome investors and advisors rebalance on a set schedule based on the calendar--every December, for example. The big advantage of rebalancing at year-end is that it enables you to be systematic about netting tax losses and gains versus one another in an effort to reduce your investment-related taxes.\nThe downside of rebalancing on a set calendar-year schedule, however, is that you may incur tax and transaction costs just to alter your asset mix by a few percentage points. For that reason, I favor a hybrid approach to rebalancing. Plan to conduct a thorough portfolio checkup\nevery year, ideally at year-end. But rebalance only when your stakes in stocks, bonds, or cash are 5 or 10 percentage points out of whack with your target ranges. At the same time you rebalance to bring your stock/bond/cash mix back in line with your targets, you can also address any dramatic investment-style shifts that have occurred in your portfolio--for example, if growth stocks or funds are crowding out the value-oriented stocks in your portfolio.\nIf you've created an Investment Policy Statement--and I think that's a worthwhile starting point for every investment plan--that's a good place to articulate your rebalancing policy. Say, for example, you've decided to rebalance only when your stock/bond/cash weightings drift 5 percentage points above or below your targets. If your asset-allocation target for your retirement portfolio is 55% stock, 40% bonds, and 5% cash, you'd set out the ranges as follows:\n� Stocks: 50% to 60%\n� Bonds: 35% to 45%\n� Cash: 0% to 10%\nWhether you allow your portfolio's allocations to the core asset classes (stocks, bonds, and cash) to diverge 5 or 10 percentage points from their targets depends on your personality and your time horizon. If you're a laissez-faire investor and you have a long time horizon for your money, you can choose to take action only if your asset allocations swing 10 percentage points above or below your targets. But if you like to be more hands-on or you're already retired, it's fine to rebalance when your allocation to any one asset class is 5 percentage points above your range.\nSome investment policy statements include sub-asset-class breakdowns, setting parameters for large-, mid-, and small-cap stock exposure; you can also break out U.S. and foreign stock exposure. There's nothing wrong with that, but you don't need to get too fancy. Setting your exposure to the broad asset classes and keeping your portfolio in line with those parameters is most important.See More Articles by Christine Benz\n|New! 30-Minute Money Solutions|\n|Need help picking up the pieces in this turbulent market? 30-Minute Money Solutions by Morningstar director of personal finance Christine Benz simplifies the daunting task of getting your financial house in order. Written for novice and experienced investors alike, this book offers manageable, step-by-step solutions for tackling money challenges and building a comprehensive financial plan in simple 30-minute increments. Learn more.|\n|Order Your Copy Today--$16.95|"", 'Accidentally draining transmission fluid can spell disaster for your vehicle’s performance and longevity. Whether you’re a seasoned DIY mechanic or a casual car owner, this dreaded mishap can happen to anyone. In this comprehensive blog article, we will delve into the causes, consequences, and solutions of accidentally draining transmission fluid. By understanding the importance of transmission fluid and how to handle this situation, you can prevent irreversible damage to your vehicle’s transmission system.\nBefore we dive into the details, let’s start by understanding what transmission fluid does. Transmission fluid acts as a lubricant, coolant, and hydraulic fluid within the transmission system. It ensures smooth gear shifting, reduces friction, and dissipates heat generated during operation. Now, imagine what happens when this vital fluid is accidentally drained. The consequences can be severe, ranging from decreased performance and increased wear and tear to complete transmission failure.\nCauses of Accidentally Drained Transmission Fluid\nAccidentally draining transmission fluid can occur due to various causes. Understanding these causes will help you avoid such mishaps in the future. Here are some common causes:\n1. Improper Fluid Disposal\nImproperly disposing of transmission fluid during maintenance or repairs can lead to accidental drainage. It is crucial to follow proper disposal procedures to prevent any mishaps.\n2. Accidental Removal during Maintenance\nDuring routine maintenance tasks, such as changing the oil or replacing the filter, it is possible to accidentally remove the transmission fluid instead of the intended fluid. This can occur if the wrong drain plug or fluid reservoir is opened.\n3. Damage Caused by External Factors\nExternal factors, such as road debris or accidents, can cause damage to the transmission system, leading to fluid leakage or accidental drainage. It is important to inspect the transmission system regularly for any signs of damage.\n4. Human Error\nMistakes happen, and human error can also lead to accidentally draining transmission fluid. It could be as simple as forgetting to reattach the drain plug or accidentally loosening the wrong component during repairs.\nSigns and Consequences of Drained Transmission Fluid\nKnowing the signs of drained transmission fluid is crucial to addressing the issue promptly. Here are some common signs that indicate a problem with your transmission system:\n1. Rough Shifting\nIf you notice that your vehicle is shifting gears roughly or experiencing delays when changing gears, it could be a sign of drained transmission fluid. The lack of proper lubrication can cause the gears to grind and result in an unpleasant driving experience.\n2. Slipping Gears\nAnother indicator of drained transmission fluid is slipping gears. You may notice that the engine revs higher than normal, but the vehicle fails to accelerate as it should. This can occur because the transmission is not receiving enough hydraulic pressure to engage the gears properly.\nA drained transmission fluid can lead to overheating of the transmission system. Heat is generated during the operation of the transmission, and the fluid helps to dissipate this heat. Without sufficient fluid, the transmission can overheat, resulting in potential damage.\n4. Unusual Noise\nWhen the transmission fluid is low or drained, you may hear unusual noises coming from the transmission system. These noises can include whining, buzzing, or humming sounds. It is important not to ignore these noises as they can be indicative of a more significant problem.\nThe consequences of driving with drained transmission fluid can be severe. Here are some potential consequences:\n1. Increased Wear and Tear\nTransmission fluid plays a crucial role in reducing friction and providing lubrication to the various moving parts within the transmission system. Without proper lubrication, increased wear and tear can occur, leading to premature failure of components.\n2. Reduced Performance\nDriving with drained transmission fluid can significantly impact your vehicle’s performance. You may notice a decrease in acceleration, difficulty in shifting gears, and overall sluggishness in the transmission’s response.\n3. Complete Transmission Failure\nIf the issue of drained transmission fluid is not addressed promptly, it can ultimately result in complete transmission failure. This can be a costly repair that may require a complete transmission replacement.\nImmediate Steps to Take after Accidentally Draining Transmission Fluid\nWhen you realize that your transmission fluid has been accidentally drained, it is crucial to take immediate action to prevent further damage. Here are the steps you should follow:\n1. Stop the Vehicle\nAs soon as you notice the issue, safely pull over and park your vehicle. Continuing to drive with drained transmission fluid can cause further damage to the transmission system.\n2. Avoid Driving Further\nOnce you have parked your vehicle, it is advisable to avoid driving it any further until the issue is resolved. Driving without transmission fluid can cause severe damage to the transmission system.\n3. Seek Professional Help\nIt is recommended to seek assistance from a professional mechanic or a reputable automotive service center. They have the expertise and equipment to assess the situation accurately and provide appropriate solutions.\nRefilling Transmission Fluid: Do’s and Don’ts\nRefilling the drained transmission fluid correctly is crucial to avoid further complications. Here are some do’s and don’ts to keep in mind:\n1. Use the Correct Type of Fluid\nIt is essential to use the correct type of transmission fluid specified by your vehicle’s manufacturer. Using the wrong fluid can lead to compatibility issues and possibly cause damage to the transmission system.\n2. Check the Fluid Level\nBefore adding new fluid, it is important to check the fluid level in your transmission. Refer to your vehicle’s manual for the correct procedure to check the fluid level accurately.\n3. Avoid Overfilling\nWhile adding new fluid, it is crucial not to overfill the transmission. Overfilling can lead to excessive pressure within the system, causing leaks and other issues.\n4. Follow Proper Refilling Procedure\nEach vehicle may have a slightly different procedure for refilling the transmission fluid. Refer to your vehicle’s manual for the correct steps and follow them meticulously to ensure proper filling.\nAssessing and Repairing Damage Caused by Drained Transmission Fluid\nDriving with drained transmission fluid can lead to significant damage to the transmission system. Here’s how to assess the extent of the damage and explore potential repair options:\n1. Seek Professional Assessment\nAfter refilling the transmission fluid, it is advisable to seek professional assistance to assess the damage caused by driving with drained fluid. They can perform diagnostic tests and inspections to determine the extent of the damage.\n2. Minor Repairs\nIf the damage is minimal, such as worn-out seals or minor leaks, the repairs may involve replacing the damaged components or fixing the leaks. This can be a more affordable option compared to complete transmission replacement.\n3. Transmission Replacement\nIn severe cases where the transmission has suffered extensive damage, a complete transmission replacement may be necessary. This can be a costly repair, and it is essential to weigh the cost against the value of your vehicle.\n4. Consider Rebuilt or Remanufactured Transmissions\nIf you opt for a transmission replacement, consider rebuilt or remanufactured transmissions as an alternative to brand new ones. These transmissions are often more cost-effective while still providing reliable performance.\nPreventive Measures for Avoiding Accidental Fluid Drainage\nPrevention is always better than cure. Here are some preventive measures to avoid accidentally draining your transmission fluid:\n1. Use a Drain Plug with a Safety Catch\nWhen performing maintenance tasks that involve draining fluids, consider using a drain plug with a safety catch. This catch prevents the plug from falling into the drain pan, reducing the chances of accidentally draining the wrong fluid.\n2. Double-Check Fluid Levels\nBefore performing any maintenance or repairs, double-check the fluid levels in your vehicle. This ensures that you don’t accidentally drain the transmission fluid while intending to work on other components.\n3. Regular Inspections\nRegularly inspect your transmission system for any signs of damage or leaks. This can help you identify potential issues before they escalate and prevent accidental fluid drainage.\n4. Follow Maintenance Schedules\nAdhere to your vehicle’s recommended maintenance schedules. Regularly changing the transmission fluid and performing other necessary maintenance tasks will help keep your transmission system in optimal condition.\nDIY vs. Professional Assistance: Making the Right Choice\nWhen faced with drained transmission fluid, it’s important to consider whether to attempt repairs yourself or seek professional assistance. Here are some factors to help you make an informed decision:\n1. Mechanical Expertise\nIf you have extensive mechanical knowledge and experience working on vehicles, youmay feel confident in attempting DIY repairs for drained transmission fluid. However, if you are unsure about your abilities or lack experience in transmission repairs, it is best to seek professional assistance. Transmission systems are complex, and incorrect repairs can worsen the issue or cause further damage.\n2. Available Tools and Equipment\nRepairing a drained transmission fluid issue may require specialized tools and equipment. Consider whether you have access to the necessary tools or if it would be more practical to have a professional repair shop handle the task. Professionals are equipped with the right tools and can ensure proper repairs and fluid replacement.\n3. Complexity of the Repair\nThe complexity of the repair should also factor into your decision. If the issue is minor, such as a simple fluid refill, it may be within your capabilities to handle it yourself. However, if there are underlying problems or if the transmission requires extensive repairs, it is advisable to let professionals handle the job to ensure accuracy and prevent further damage.\n4. Warranty Considerations\nIf your vehicle is under warranty, attempting DIY repairs for drained transmission fluid may void the warranty. It is essential to review your warranty terms and consult with the dealership or manufacturer before proceeding with any repairs yourself. In such cases, professional assistance is often the recommended route.\nCommon Myths and Misconceptions about Drained Transmission Fluid\nThere are various myths and misconceptions surrounding drained transmission fluid. Let’s debunk some of these common misconceptions and provide accurate information:\n1. Myth: Driving without Transmission Fluid Is Fine for Short Distances\nThis is a dangerous myth. Driving without transmission fluid, even for short distances, can cause significant damage to the transmission system. The lack of lubrication and cooling properties of the fluid can lead to increased friction, overheating, and premature wear.\n2. Myth: Topping Up with Any Fluid Will Suffice\nUsing the correct type of transmission fluid is crucial. Different vehicles require specific types of fluid to ensure proper operation and prevent damage. Using the wrong fluid can lead to compatibility issues, reduced performance, and potential damage to the transmission system.\n3. Myth: Adding More Fluid Will Fix the Issue\nIf you have accidentally drained transmission fluid, simply adding more fluid will not resolve the problem. It is essential to address the cause of the drained fluid and properly refill the transmission following the manufacturer’s guidelines. Ignoring the underlying issue can lead to further damage and costly repairs.\nTransmission Fluid Maintenance: Best Practices\nMaintaining proper transmission fluid levels and quality is crucial for the longevity and performance of your vehicle’s transmission system. Here are some best practices to consider:\n1. Regular Fluid Checks\nRegularly check the transmission fluid level using the dipstick or the recommended procedure outlined in your vehicle’s manual. This allows you to detect any potential leaks or low fluid levels early on.\n2. Fluid Replacement Intervals\nFollow the manufacturer’s recommended fluid replacement intervals. Over time, transmission fluid can deteriorate, lose its properties, and become less effective. Regular fluid changes help maintain optimal performance and protect the transmission system.\n3. Follow the Manufacturer’s Recommendations\nAlways adhere to the manufacturer’s recommendations regarding the type of transmission fluid to use, as well as any specific maintenance procedures. This ensures compatibility and helps maintain the warranty (if applicable).\n4. Address Leaks Promptly\nIf you notice any fluid leaks from your transmission system, address them promptly. Leaks can lead to low fluid levels and, if left unattended, can cause damage to the transmission system. Have leaks repaired by a professional mechanic to prevent further issues.\nFrequently Asked Questions (FAQs)\nHere are answers to some frequently asked questions about accidentally drained transmission fluid:\n1. Can I Drive with Low Transmission Fluid?\nDriving with low transmission fluid is not recommended. Low fluid levels can lead to poor lubrication, increased friction, and overheating, which can cause significant damage to the transmission system. It is best to address the issue promptly and refill the fluid to the proper level.\n2. How Do I Check the Transmission Fluid Level?\nTo check the transmission fluid level, park your vehicle on a level surface, and ensure the engine is running. Locate the transmission dipstick, usually labeled and located near the engine. Pull it out, wipe it clean, reinsert it fully, and then pull it out again. The fluid level should be between the two marked lines on the dipstick.\n3. Can I Refill the Transmission Fluid Myself?\nRefilling the transmission fluid can be done yourself if you have the tools, correct fluid, and knowledge of the proper procedure. However, it is important to follow the manufacturer’s guidelines and ensure you refill the fluid to the proper level. If in doubt, it is best to seek professional assistance.\n4. How Often Should I Change the Transmission Fluid?\nThe frequency of transmission fluid changes can vary depending on the vehicle and driving conditions. It is generally recommended to change the fluid every 30,000 to 60,000 miles, but always refer to your vehicle’s manual for the manufacturer’s specific recommendations.\nIn conclusion, accidentally draining transmission fluid can have serious consequences for your vehicle’s transmission system. By understanding the causes, consequences, and solutions discussed in this comprehensive blog article, you can take the necessary steps to prevent and address this issue effectively. Remember, proper maintenance, immediate action, and seeking professional assistance when needed are key to maintaining a healthy transmission system and ensuring your vehicle’s optimal performance and longevity.\nRelated video of Accidentally Drained Transmission Fluid: Causes, Consequences, and Solutions\nWelcome to. my blog, jack !']"	['<urn:uuid:c60106ea-a642-49e4-8a05-ebdde9970720>', '<urn:uuid:f8c05b94-9d7c-4f44-9872-3861e00a89be>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T00:43:36.053682	8	94	2989
67	As a leadership consultant, I'm curious about the connection between delegation and accountability - when a manager delegates decision-making to their team, are they still accountable for the outcomes?	Yes, delegating freedom and decision-making responsibility to a team absolutely does not absolve the manager of accountability. In fact, if everything goes well, the team must get the credit, but if it all goes horribly wrong, the manager must take the blame.	"['The manager suggests a provisional decision and invites discussion about it. Not surprisingly the team must be mature and competent, and capable of acting at what is a genuinely strategic decision-making level.\nDelegating freedom and decision-making responsibility to a team absolutely does not absolve the manager of accountability. Others are much more ""hands off. The manager presents the decision along with some of the background which led to the decision.\nThe studies determined that motivation is not primarily a result of financial incentives in isolation of social factors. The manager makes the decision as in 1 above, and then explains reasons for the decision to the team, particularly the positive benefits that the team will enjoy from the decision.\nManager presents the problem or situation, get suggestions, then decides — team is free to come up with options; manager decides on those options 6.\nThere is a rising scale of levels of delegated freedom that you can use when working with your team.\nThis means that the leader acts mostly like a dictator, telling the team members what to do and leaving very little or no room for negotiation.\nSearch Tannenbaum-Schmidt Leadership Continuum Many leadership models and theories like to lump leadership strategies into just two or three categories and analyze them from there. The manager presents the decision with background ideas and invites questions.\nIf everything goes well, the team must get the credit; if it all goes horribly wrong, the manager must take the blame. Sells Sliding a bit up the scale, this point is marked by leadership which is still direct, but also allows for a little bit of back and forth between the leader and the team.\nTo learn more about The Tannenbaum-Schmidt Leadership Continuum and how to choose the leadership style that best suits you, read the article that accompanies this video.\nA manager is characterized according to degree of control that is maintained by him. Key Points The Tannenbaum-Schmidt Leadership Continuum describes a spectrum of leadership behaviors from autocratic to democratic.\nThe behavior of the leader is influenced by his personality, background, knowledge, and experience. The Tannenbaum and Schmidt Continuum is often shown as a simple graph: The leader is effectively inviting the team to problem-solve with him or her.\nThe personality of the subordinates and their expectations from the leader influences their behavior. The manager presents the situation or problem, gets suggestions, then decides.\nThe manager presents the decision with background ideas and invites questions. Within those extremes, the Tannenbaum-Schmidt Leadership Continuum highlights seven points along the way which can be used to describe various leadership styles.\nThe \'Abdicates\' style represents a point where the leader essentially relinquishes any involvement and trust the team to get the job done from start to finish.\nThe manager presents the situation, and maybe some options, to the team. The manager presents the situation, and maybe some options, to the team. Taylor left the discovery of the psychological aspects of management for others.The Tannenbaum and Schmidt Continuum is a simple model of leadership theory which shows the relationship between the level of freedom that a manager chooses to give to a team, and the level of authority used by the manager.\n*Graphic representation of the Tannenbaum-Schmidt Continuum originally published in Harvard Business Review.\nFrom ""How to Choose a Leadership Pattern,"" by. Leadership behaviour changing on a continuum. Tannenbaum and Schmidt produced one of the best known works in the field of leadership style research in the late \'s through to the early \'s.\nThey called this work the Continuum of Leadership Behaviour. Tannenbaum and Schmidt Continuum model shows the relationship between the levels of freedom that a manager chooses to give to a team, and the level of authority used by the manager.\nAs we already discussed in Situational leadership, number of parameters goes into choosing the managerial style: manager’s competence, subordinate’s.\nThe Tannenbaum and Schmidt Continuum recognises that the chosen leadership style depends on a variety of factors, including the leader\'s personality, the perceived qualities of subordinates.\nR. Tannenbaum and W, H. Schmit shows a wide range of leadership style on a continuum that moves from authoritarian to free-rein. Depending on the situation and many factors; manage must determine whether directive leadership, participative leadership or something in between is the best.Download']"	['<urn:uuid:952b3782-141b-4b67-838e-8be8bc861762>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	29	42	715
68	As a cyber security manager, what is the duration of Cyber Protect awareness sessions?	The cyber awareness sessions are approximately 1.5 hours long, including time for questions.	['The Cyber Risk\nIf you’re a small or medium-sized enterprise (SME) then there’s around a 1 in 3 chance that you’ll experience a cyber security breach.\nFalling victim to cyber crime can be devastating for your business, and when you consider that SMEs make up 99.9% of Britain’s 5.5 million private sector businesses, it represents a real threat to the whole of the UK economy.\nAs part of the UK’s National Cyber Security Strategy the Cyber Protect network has been created to help improve the ability for UK PLC to defend itself against cyber attack.\nFREE CYBER AWARENESS PRESENTATIONS\nThe Cyber Protect network exists across the UK and provides FREE cyber awareness sessions to organisations including business, education, public services, local government and the charitable sector*.\nWe can deliver these sessions in person, if you can provide a suitable venue, or virtually. For virtual presentations, the limit for MS Teams sessions is 250 attendees, for Zoom it is 500 attendees. The presentation is delivered by fully trained Police Officers and Police Staff from the Cybercrime Unit – for FREE.\nOur cyber awareness sessions are approximately 1.5 hours (including questions). From experience, your staff are more likely to learn by engaging them for the benefit of their personal lives, achieving behaviour change and then bringing that benefit back to the workplace. As such, we pitch our sessions for their personal life, with explanation of their role in cybersecurity at work. These sessions are for an everyday user, avoiding jargon where we can and avoiding technical complexity. We cover:\n- Current Cyber Threats\n- The motivation of cyber criminals\n- Data Breaches\n- Securing accounts – passwords and 2FA\n- Improving our privacy online\n- Securing devices\n- Public WiFi risks\n- Response Planning\n- Cyber Drills\n- Further learning\nOur leadership sessions are approximately 1.5 hours (including questions). They are pitched for company leaders and decision makers and are limited in technical complexity. We cover:\n- Current Cyber Threats & Trends\n- The motivation of cyber criminals\n- How criminals exploit weak passwords\n- Data Breaches\n- Detecting Suspicious Events\n- The importance of 2FA\n- Denial of Service attacks\n- Supply chain threats\n- Internet of Things (IoT) and business\n- Industrial Control Systems\n- Asset control\n- Legacy equipment\n- Strategic considerations\n- Response Planning\n- Cyber Drills\n- Staff training\n- Further learning\n“Jon called to say thank you for saving his company £125,000 through an attempted invoice fraud. Staff were reminded of the event that they had attended 6 months earlier. If they hadn’t gone to the event SEROCU Cyber Protect presented at we would have been stung.”Jon, Sussex\nI thought your presentation and communication style was excellent and we really appreciate you supporting us and our clients.Paul, UK Banking sector\n“It is important for you to know that your efforts do have a positive effect out here in the business world. I hope you continue the great work you do.”Brian, Sussex\nI have had numerous positive comments from the members of staff who attended the CPD session after school. It has certainly given us a lot to think about and some clear action points both in the shorter and longer term.Headteacher P, Sussex\n* Audiences: While we would like to be in a position to support all organisations, our team is small and therefore we have to limit our proactive services for businesses to micro-, small- and some medium-enterprises. For medium sized enterprises, availability will depend on turnover and your ability to procure these services privately. Any victim of cybercrime, regardless of size, who reports the incident to Action Fraud is eligible for some Cyber Protect services.\nDECISIONS & DISRUPTIONS EXERCISE\nThis exercise is targeted towards senior decision makers within companies, but it can also be of benefit to other staff with varying levels of technical knowledge. It creates an immersive experience in which participants have to use their problem solving, collaboration, leadership and communication skills.\nThe exercise takes approximately 1.5 hours and the target audience is senior leaders and decision makers. The game works best with 5-9 players.\nThis exercise is FREE and available to the same audiences as our presentations.\nGet in Touch\nPlease contact the relevant Police Cyber Crime Unit direct to access any of the services …\nPlease be aware that our primary business audience is micro-, small- and some medium-enterprises. For medium sized enterprises, availability will depend on turnover and the ability to procure these services privately.Any victim of cybercrime, regardless of size, who reports the incident to Action Fraud is eligible for some Cyber Protect services.\nCyber Resilience Centre for the South-East\nUsing a combination of police officers and cyber talent, the Cyber Resilience Centre for the South-East supports and helps protect small business, SMEs and supply chain businesses and third sector organisations in the region against cyber crime.\nCovering the areas of Thames Valley, Oxfordshire, Berkshire, Buckinghamshire, West Sussex, East Sussex, Surrey, Hampshire, and the Isle of Wight, they are working with local Universities and the Police forces, which provides them with access to the latest local as well as national information on emerging cyber threats, criminal trends, best practice for cyber resilience and new technology to provide you with timely guidance to prepare and protect your business, staff and clients from cyber criminals.\nThe Cyber Resilience Centre for the South-East provides affordable cyber security testing and training services, with the opportunity to learn how to procure private sector cyber security products, services, or resources. Whether it’s just a health check for your company or its employees, they can help with whatever your cyber needs.\nA trusted resource, they are also a straightforward place to find IASME approved Cyber Essentials and Cyber Essentials Plus Certifiers in the South East. These are recognised nationally as Trusted Partners.\nThe Cyber Resilience Centre for the South-East is modelled on a successful structured collaboration acclaimed by the National Police Chiefs’ Council (NPCC). It is part of a nationwide network of not for profit centres set up by the Home Office and Business Resilience International Management.\nThe Cyber Resilience Centre for the South-East provides free of charge membership and further paid membership opportunities. Membership is not just for IT or Tech companies – it is highly relevant and beneficial to all sizes and types of organisations.\nPolice CyberAlarm is a free tool to help members understand and monitor malicious cyber activity. This service is made up of two parts; monitoring and vulnerability scanning.\nPolice CyberAlarm acts as a “CCTV camera” monitoring the traffic seen by a member’s connection to the internet. It will detect and provide regular reports of suspected malicious activity, enabling organisations to minimise their vulnerabilities. The data collected by the system does not contain any content of the traffic. The system is designed to protect personal data, trade secrets and intellectual property.\nMembers of Police CyberAlarm will become part of the wider UK cyber defence network, sharing collected data with Police for analysis at local, regional and national levels to identify trends, react to emerging threats and identify, pursue and prosecute cyber criminals.\nVulnerability Scanning can be added and used to scan an organisations website and external IP addresses, providing regular reports of all known vulnerabilities.\nThere is no charge from the Police to use this service.\nPolice CyberAlarm members install a CyberAlarm Virtual Server on their premises which will be used to collect and process traffic logs from their firewall/internet gateway. Police CyberAlarm Virtual Server is easy to install using the downloadable virtual appliance, offering one-click installation. The Virtual Server will run on low-specification hardware. While full installation instructions are provided, you may require technical support to configure the connection to your firewall. You will require a dedicated firewall appliance – all in one small business solutions may not have the capability of sending logs to the Virtual Server (or any log aggregation / SIEM solution).\nUse SignUp code SEROCU to register:\nExercise in a Box\nNo cyber defence is 100% effective. Recognising that fact, and preparing for an incident in advance, is key to mitigating the impact – and may be what enables your business to survive.\nPlanning for incident response should be a continual and dynamic process. To get started, think about the incidents that might affect your business – ransomware, business e-mail compromise, Distributed Denial of Service (DDoS), malicious insider. You should develop individual responses (‘playbooks’) for each incident – and don’t forget, we are here to help.\nThe National Cyber Security Centre created ‘Exercise in a Box’ in response to feedback that conducting cyber drills was expensive and difficult. It is a free online cyber exercising tool which guides participants through different cyber incidents. This will allow you to test your response plans and develop their maturity.\nIf you’d like us to assist you in completing the exercises, either as observers or advisors, please get in touch:\nNetwork Defenders – join CiSP!\nCiSP is the Cyber security Information Sharing Partnership. CiSP is a joint industry and government initiative set up to exchange cyber threat information in real time, in a secure, confidential and dynamic environment, increasing situational awareness and reducing the impact on UK business.\nCiSP is a forum type platform separated into nodes based on geography, organisation type or threat type. Network Defenders, UK Government, the National Cyber Security Centre, ROCU Protect Network and Law Enforcement can share information promptly and securely\nThe Benefits of CiSP\n- Engagement with industry and government counterparts in a secure environment\n- Early warning of cyber threats\n- Ability to learn from experiences, mistakes, successes of other users and seek advice\n- An improved ability to protect their company network\n- Access to free network monitoring reports tailored to your organisations’ requirements\nTo become a registered CiSP member you must be:\n- A UK registered company or other legal entity which is responsible for the administration of an electronic communications network in the UK\n- Sponsored by either a government department, existing CiSP member or a regional Cyber PROTECT police officer or industry champion.\nOnce you have a sponsor, you can apply here:\nIn the meantime, if you are on Linkedin and Twitter, it’s worth connecting with SEROCU Cyber Protect to assist in this process.\nCyber Security Advice for Organisations\nFor guidance on improving the cyber security of your organisation, work through our summary:\nCyber Security & Privacy for Staff\nFor guidance for staff and individuals on staying secure online and maximising your online privacy, please go to our dedicated pages:']	['<urn:uuid:2d6e3cc2-8002-4248-9e9c-f0009966ba61>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	14	13	1752
69	When cooking with red wine in a sauce, how long does it typically need to be cooked to ensure the alcohol properly evaporates from the mixture?	You need to cook a sauce for at least 20 to 30 seconds after adding wine to it to allow the alcohol to evaporate. Since alcohol evaporates at 172°F (78°C), any sauce or stew that is simmering or boiling is certainly hot enough to evaporate the alcohol.	['Can red wine vinegar replace red wine?\nQuantities. If the recipe calls for less than 1/4 cup of red wine, you can safely substitute red wine vinegar, although it won’t produce the same results. Do not use larger amounts of red wine vinegar, though.\nWhat is a good red cooking wine?\nThe best red wines for cooking are those with moderate tannins: Merlot, Pinot Noir, Sangiovese (the main grape in Chianti), and lighter-style Cabernets. Heat won’t improve the undesirable qualities of bad wine: it will accentuate them.\nCan I use port instead of red wine in a recipe?\nFor red wine you can use port or red vermouth. Dry Madiera is also a good option to have in the storecupboard. It goes with both red and white meats, though will give a darker sauce than say a white wine or white vermouth.\nWhat is the difference between red wine and red cooking wine?\nThe difference between the two wines is the quality of the drink. Regular wine is finer, more flavorful, and will have a stronger taste in your dishes. Cooking wine is a go-to wine that will add the flavor you need, but will not be enjoyable to drink, as the flavors it will bring won’t be as potent.\nIs Merlot or shiraz better for cooking?\n“A lighter red, like pinot noir, is great with pork, mushrooms and salmon,” says Jacobs. “Other reds that go well in lots of cooking, especially with red meat, are the classic shiraz, merlot or cabernet merlot varieties. “You don’t want anything too strong in tannins or acid.\nCan I use sherry instead of red wine in a recipe?\nIf substituting sherry for red wine in the recipe, the additional alcohol will cause the liquid to thicken and reduce faster. Keep a close eye on the skillet and stir often to avoid sticking and burning. You might need to add a small amount of extra chicken broth to compensate for the thickening power of the sherry.\nWhat is the difference between drinking wine and cooking wine?\nHow long do you cook off red wine?\nYou need to cook a sauce for at least 20 to 30 seconds after adding wine to it to allow the alcohol to evaporate. Since alcohol evaporates at 172°F (78°C), any sauce or stew that is simmering or boiling is certainly hot enough to evaporate the alcohol.\nWhat is the best red wine for spaghetti sauce?\nThe best wine to use in spaghetti sauce is Chianti, or any wine made from Sangiovese grapes. Its moderate tannins and full range of flavors, from cherry to leaves to tobacco and sometimes mushroom, add depth of flavor to any red sauce.\nIs red wine the same as cooking sherry?\nRed wine is a great substitute for sherry in meat dishes. Add a few sprigs of rosemary for a match made in heaven. How to use: Like with apple cider vinegar, it is good to dilute red wine vinegar with water. Also, don’t use it in the same amount as the recipe calls for cooking sherry.']	['<urn:uuid:d9187442-7da9-4845-a3ed-0ebb0221dd50>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	26	47	512
70	chemical technique restore disintegrating antique paper materials removal glassine tape	The restoration process involves removing glassine tape, which is water soluble and can be removed in a water bath. After removing the tape and acids from the paper, it is alkalized with a magnesium solution and re-sized with gelatin. For extensive damage, pulpfill technique is used instead of Japanese tissue and wheat starch paste mending. The pulpfill process involves blending various high-quality handmade papers to match the hue and tone of the area being repaired, creating a paper pulp that is applied to the wet object on Hollytex and blotters, then dried under pressure between felts.	['Recently, I had the opportunity to treat a very special item from our Reserve collection as part of our Save America’s Treasures grant. The Protestant Tutor for Children is attributed to Benjamin Harris and was printed by Samuel Green in Boston, 1685. Thought to be a precursor text to the New England Primer, it is the first and only known extant copy printed in New England. AAS has been in ownership of this pamphlet since at least 1885.\nUpon examination, I found this rare pamphlet to be severely mutilated, with paper tears, losses, and stains. Hand-stitched into a dear little cloth wrapper, and with every page backed with glassine tape, it was evident that someone in its almost 330-year life had cared about preserving it. A missing section of the title page had been filled in with new paper and text neatly written in pencil. I imagine they probably felt they were doing a great job of it. There was ink manuscript throughout, declaring ownership (William Giddons and Joseph Ayers, specifically). Overall, charming, but in rough shape!\nWhat is required first in treating such an object is thorough documentation. A detailed description of its condition is written up and captured visually with digital photography. In this case, every page was shot both before and after treatment. The pamphlet was then disbound, and the pages were cleaned of surface grime before washing. All of that glassine had turned yellow and obscured the text. It had certainly kept the pamphlet intact, but it had to go! Fortunately, the adhesive on glassine tape is water soluble, and was easily removed in a water bath. After the tape floated off and the acids were removed from the paper, it was alkalized with a magnesium solution, and when dry, re-sized with gelatin. It was now time to reattach all the little bits that came apart when the glassine came off. Usually we mend tears and fill losses with Japanese tissue and wheat starch paste, but due to the extent of damage this was a better candidate for a pulpfill, which tends to be more seamless. We’d just spent the month doing pulpfills, and I’d saved this one for last.\nMaking fills with pulp is fun, and creative, even kind of artsy – one tears into small pieces a variety of high-quality handmade papers (old and new) in an attempt to match the hue and tone for the area you are repairing. These are whirred up in a blender with water (passersby are unable to resist the smoothie joke) until you have a suitably blended paper pulp. It can take a bit of tweaking, but is highly satisfying when you’ve gotten it right. We then fill a squeeze bottle with the pulp and apply it to the wet object on Hollytex (a synthetic material which allows water to flow through, but prevents sticking) and blotters. (To see what this process looks like, click here.) It is then dried under pressure between felts.\nAfter the paper repairs, the pamphlet was re-sewn into a new paper wrapper because the original wrapper was too fragile for re-use. Both the old wrapper, and the original threads that were cut to release it, have been saved with the pamphlet in an enclosure, along with some notes about the provenance that were found with it. The “after” photos were taken, and it was returned to our cool, dark, secure stacks, where it will remain safe and sound for generations to come.']	['<urn:uuid:ecfceb24-1fd2-425d-851a-e4f4cbb74d9a>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	10	96	579
71	healthcare data sharing benefits drawbacks practice	The benefits of healthcare data sharing are demonstrated by a clinical decision support tool that helped identify high-risk PCI patients and reduced complications from 22.8% to 14.9%, while decreasing hospital stays and costs. However, this data sharing comes with significant privacy concerns under HIPAA. Though patients technically have rights to see, copy and request corrections to their health records, providers are not required to fulfill these requests. Furthermore, certain entities like government agencies, police, and public health organizations cannot be denied access to patient records, creating privacy vulnerabilities despite the potential clinical benefits.	"['A clinical decision support tool helped physicians identify patients at high risk of bleeding complications prior to undergoing a coronary intervention procedure and helped guide the use of bleeding avoidance strategies, leading to less complications and a shorter hospital stay, according to a study being presented March 10 at the American College of Cardiology Scientific Sessions.\nMore than 1.3 million percutaneous coronary interventions (PCIs) are performed each year in the United States. Bleeding during and after these procedures is a common risk (3 – 6%).\n""Bleeding complications after PCI can lead to worse outcomes for the patient, including death, as well as an increase in the length of stay in the hospital, which leads to an increase in hospital costs,"" says the study\'s lead author Craig E. Strauss, MD, MPH, a research cardiologist at the Minneapolis Heart Institute Foundation and physician at the Minneapolis Heart Institute® at Abbott Northwestern Hospital in Minneapolis.\nTherefore, the cardiologists across the Allina Health System used a validated pre-PCI bleeding risk score to accurately identify high-risk patients, allowing them to employ strategies to avoid bleeding and improve patient outcomes. Among those strategies, bivalirudin (Angiomax, The Medicines Company) is a direct thrombin inhibitor that has been shown to reduce bleeding complications in PCI.\n""This risk-stratification tool can be used real-time in the cath lab to identify and mitigate a patient\'s risk for bleeding around the time of the procedure,"" says Strauss. ""The cath lab team uses a web-based calculator, which takes less than 1-2 minutes, and determines the risk category of each patient."" Due to the ease of use, the tool achieved a high adoption rate of 92 percent at three high-volume PCI centers in Minnesota.\nAmong the 2,608 PCI cases performed at the three PCI centers, 24.4 percent of the patients were identified as high risk for bleeding complications. The use of bivalirudin as a bleeding avoidance strategy in high-risk patients increased from 24.2 percent pre-implementation of the risk-stratification protocol to 60.3 percent after the implementation of the protocol.\nAmong high bleeding risk patients, the researchers reported significant reductions in overall complications (22.8% vs. 14.9%), bleeding events within 72 hours (7.7% vs. 2.1%), and median length of hospital stay (2.9 vs. 2.3 days) following protocol implementation. There also was a reduction in the use of blood transfusions and death. Finally, the total variable costs decreased from approximately $15,000 to $14,200.\n""This clinical decision support tool helps cardiologists objectively validate which patients are at high risk for bleeding complications based on existing evidence,"" Strauss says. ""Previously, we could attempt to identify which patients were at risk based on clinical intuition, but this tool gives a much more accurate assessment, as shown through the improved patient outcomes. Furthermore, this tool can help reduce the variability in practice patterns among physicians that can be costly to the healthcare system.""\nAbout the Minneapolis Heart Institute Foundation\nThe Minneapolis Heart Institute Foundation is dedicated to creating a world without heart disease through groundbreaking clinical research and innovative education programs. MHIF\'s mission is to promote and improve cardiovascular health, quality of life and longevity for all.\nAbout the Minneapolis Heart Institute®\nThe Minneapolis Heart Institute® is recognized internationally as one of the world\'s leading providers of heart and vascular care. This state-of-the-art facility combines the finest in personalized patient care with sophisticated technology in a unique, family-oriented environment. The Institute\'s programs, a number of which are conducted in conjunction with Abbott Northwestern Hospital, address the full range of heart and vascular health needs: prevention, diagnosis, treatment and rehabilitation.\nAAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert! system.', '|Healthcare Training Institute - Quality Education since 1979CE for Psychologist, Social Worker, Counselor, & MFT!!\nConfidentiality Issues in the time of HIPAA\nRead content below or listen to audio.\nLeft click audio track to Listen; Right click to ""Save..."" mp3\nIn the last section, we discussed three changes in ethical boundaries in regards to the disclosure of raw test data. These three test data boundary changes include: shift in standards; effects of HIPAA; and protecting test security.\nOn this section, we will discuss three controversies created by HIPAA and its possible breach of ethical boundaries. These three HIPAA privacy controversies include: governmentally accessed information; contradictory language; and employer access.\n3 HIPAA Privacy Controversies\n♦ Controversy #1 - Governmentally Accessed Information\nThe first HIPAA privacy controversy is governmentally accessed information. The branch of the government, Health and Human Services, otherwise known as HHS, in addition to taking on the responsibility of writing the legislation, also takes on the responsibility of monitoring it. However, to do this, the HHS demands unlimited access to clients’ records with or without consent of the client. Upon the inception of HIPAA, the HHS facilitated the federal government’s admittance to the records of millions of mental health clients.\nFor the first time in history, an act of legislation makes it legal for the United States government to retrieve medical information about its citizens, often without the knowledge of the citizens themselves. In addition, HHS has created what HIPAA expert Michael Freeny terms ""a holy trinity of insurers, providers, and claims clearinghouses"" who can exchange information freely between each other without the client’s knowledge.\nEssentially, the information that the client only wanted and thought would be shared just between him or herself and the clinician becomes a free-for-all among these HIPAA approved entities. However, clients are never aware of these transactions and believe their information to be perfectly confidential because, as we discussed in section 3, the Notice of Privacy Policies are so convoluted and obscure that they cannot cipher through it all. Therefore the federal invasion of privacy remains undetected unless a client extensively researches HIPAA for him or herself.\nI have found that many clients who have undertaken the laborious task of sifting through HIPAA legislation feel cheated and betrayed by the government and most significantly by their therapist. This puts an unnecessary strain on the client-therapist relationship so that is why I try and make the language of my NPP as clear as possible to my clients from the beginning so they are not surprised by newly discovered and interpreted legislation.\nThink of your clients. Can you think of any clients who may be shocked by the extent to which the government has access to his or her records?\n♦ Controversy #2 - Contradictory Language\nThe second HIPAA privacy controversy is contradictory language. Although the legislation presents all appearances of attempts to protect a client’s privacy, upon closer scrutiny, it becomes apparent that certain passages may contradict each other.\nMost specifically, Michael Freeny points out a specific section within the legislation which guarantees consumers the right to see and copy their health records and request corrections of mistakes that may be contained in those records. However, providers are obligated by HIPAA to inform clients that they are not bound to fulfill such requests. Therefore, the client’s rights may not be recognized by the provider if the provider deems it unnecessary.\nIn addition, the guidelines stipulate that clients maintain the right to restrict the distribution and transmission of their medical information. However, Freeny points out, ""It’s not true because the next sentence says that the provider is under no obligation to give you that information."" Even further, although the client may retain the right to restrict the distribution of their information and the provider consents, there are certain entities to which the provider cannot deny access, specifically the federal government, police, and public health agencies.\nSo although HIPAA may appear to be more restrictive regarding the distribution of records, this restriction only applies to certain individuals not among the HIPAA approved agencies. Many clients and clinicians are not aware of these contradictions due to the convolution of the regulations themselves. Michael Freeny believes that many therapists prefer to do the bare minimum in regards to HIPAA compliance due to the time it would consume to actually understand the guidelines at a deeper level. Do you agree?\n♦ Controversy #3 - Employer Access\nIn addition to governmentally accessed information and contradictory language, the third HIPAA privacy controversy is employer access. As we discussed in section 4, certain authorities may access information illegitimately and use it to the disadvantage of the client. However, certain employers who operate under the Employee Retirement Income Security Act as insurers for their employees have access to their employees\' health records.\nThis information includes symptoms, medication, and even diagnoses. According to HHS, employers are not allowed to use this information in any business transaction. However, there are no consequences implemented to force the employer to take responsibility for his or her action.\nJohnny, age 32, suffered from alcoholism. He had just obtained a job with a packing company who acted on the Employee Retirement Income Security Act and had access to his mental health records. After about a month at this company, he was fired, even after a positive evaluation. His employer stated that Johnny had been a part of a series of cutbacks, although no other employees, even those with less experience and worse evaluations, had been let go. Johnny believes that his supervisor had been told about Johnny’s drinking problems through this act, but there was no way to prove he had been the victim of this discrimination.\nThink of your Johnny. Could he or she run the risk of losing his or her employment due to his or her disorder?\nIn this section, we discussed three controversies created by HIPAA and its possible breach of privacy boundaries. These three HIPAA privacy controversies included: governmentally accessed information; contradictory language; and employer access.\nPeer-Reviewed Journal Article References:\nBenefield, H., Ashkanazi, G., & Rozensky, R. H. (2006). Communication and records: Hippa issues when working in health care settings. Professional Psychology: Research and Practice, 37(3), 273–277.\nCampbell, L. F., & Norcross, J. C. (2018). Do you see what we see? Psychology\'s response to technology in mental health. Clinical Psychology: Science and Practice, 25(2), Article e12237.\nDouglas, S., Jensen-Doss, A., Ordorica, C., & Comer, J. S. (2020). Strategies to enhance communication with telemental health measurement-based care (tMBC). Practice Innovations, 5(2), 143–149.\nGlueckauf, R. L., Maheu, M. M., Drude, K. P., Wells, B. A., Wang, Y., Gustafson, D. J., & Nelson, E.-L. (2018). Survey of psychologists’ telebehavioral health practices: Technology use, ethical issues, and training needs. Professional Psychology: Research and Practice, 49(3), 205–219.\nRichards, M. M. (2009). Electronic medical records: Confidentiality issues in the time of HIPAA. Professional Psychology: Research and Practice, 40(6), 550–556.\nStiles, P. G., & Petrila, J. (2011). Research and confidentiality: Legal issues and risk management strategies. Psychology, Public Policy, and Law, 17(3), 333–356.\nWhat are three controversies created by HIPAA and its possible breach of privacy boundaries? To select and enter your answer go to .']"	['<urn:uuid:908e40fb-2b1c-4823-857f-8f92d936f200>', '<urn:uuid:e84a5fa8-27a3-4eb0-be19-a8ca48d08943>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	6	93	1810
72	How do various ESD control methods work and what testing limitations exist?	ESD control methods work in four ways: prevention (using materials that cannot hold charge), neutralization (connecting to grounded outlets), dissipation (using static dissipative foam), and shielding (protecting equipment from nearby charges). However, current ESD testing standards have limitations - while IEC 61000-4-2 uses 0.7ns risetimes, it only covers 'personnel ESD' or Human Body Model scenarios, neglecting other sources like furniture discharge and micro-gap discharges. Modern ICs require protection against much faster risetimes (50ps by 2019) due to semiconductor die shrinking, but the standard hasn't been updated to reflect these real-world needs.	"['ESD Training is important as understanding the techniques behind static control in a work environment is a crucial part of many businesses, particularly ones that have access to high voltage and electronics. Electrostatic discharges cause billions of dollars in damages each year. By going through basic ESD training and educating staff members, one can significantly decrease these losses. It only takes a small investment in materials and some ESD training to effectively control static conditions and prevent damages.\nBasics of Static Electricity for ESD Training\nMost people know that static electricity gathers when friction occurs between two conductive materials. The actual discharge, often felt as a short shock, occurs when two objects with different charges come into contact with each other. At 3,000 volts, one can feel static. At 5,000 volts, one can hear it. It takes 10,000 volts or more for static discharge to be visible to the human eye. Discharges at that voltage will damage most electronics. Static charges are more likely to form at higher voltages in dry climates with low temperatures. The most important thing to know for ESD training is that one cannot eliminate static, only reduce it. This is why professionals refer to it as static control, not static elimination. Proper anti static techniques will assess the vulnerability of a workplace and take steps to minimize the likelihood of an ESD event.\nThere are two types of damage that occur after an ESD event. Catastrophic damage comprises 10% of all ESD failures. The static discharge is so strong that it damages the device beyond repair. The device does not work and now needs a replacement before work can proceed. Intermittent damage comprises 90% of ESD failures. The ESD event damages the device, but the device is operational. Without basic ESD training, someone could assume that the device is fine and send it out to a customer. The latent damage will show up later and result in more service calls. Intermittent damage is the most expensive type of ESD failure. Both types of damage can happen at any level, including product assembly and field servicing. Employees should have the ESD training to know when to take extra precautions against ESD. Effective ESD training will also teach them how to use anti static tools correctly.\nThe most common static generators are work surfaces, flooring, clothing, packaging materials, and personnel. Special ESD equipment exists to help control the static for each of these items. Companies should invest in anti static shipping bags, ESD flooring for work environments, and anti static mats with connected wrist straps for direct work with electronics. Those are the minimum tools required for anti static control, but there are other helpful tools available as well. ESD training will help workers decided which tools will benefit each work environment.\nAnti static equipment controls static in one of four ways: prevention, neutralization, dissipation, and shielding. Some materials, such as anti static gloves, prevent charges from forming with material that cannot hold a charge. ESD flooring neutralizes the charge by connecting it to a grounded outlet. Remember, static is a problem when two objects with different charges come into contact with one another. If everything in the system stays at the same charge, the risk decreases. ESD mats slowly dissipate the charge with static dissipative foam. Materials such as anti static bags shield sensitive equipment from any static charges that are nearby. Each of these components works together and controls static with all four methods. This ensures that the environment will remain safe from sudden discharges. Here are four basic rules of static control to review during ESD training: 1) Handle sensitive items at an anti static workstation, making sure to discharge any static electricity that may have built up on hands or clothing. 2) Transport sensitive items in anti static bags 3) Test the static control equipment regularly to ensure that it is functioning correctly. 4) Make sure that members of every part of the supply chain follow the above three rules. Even a small misstep can result in intermittent damage to a component. ESD training can go into much greater depth, but these basics will provide a foundation for preventing damaging ESD events. Call our sales office on 0844 8845 155 for an ESD audit, advice on how to set up an EPA, and a full ESD training presentation.', ""ESD testing needs to change to reflect real-life\n21 Jan 2020\nAuthor: Keith Armstrong\nESD testing needs to change to reflect real-life\nSemiconductor die shrinking means that 0.7ns risetimes are now too slow\nElectrostatic discharges in the real world cover a very wide range: from relatively slow high-voltage events (say, > 1kV peak) to extremely fast low-voltage events (say, < 1kV peak). The figure shown is of the transient field radiated by a micro-gap discharge between two coins – more on this later.\nIn the 1980s, the equivalent of the current IEC 61000-4-2 ESD test used a 5ns risetime, because faster risetimes were not necessary to replicate the real-life ESD problems suffered by the ICs of that time. This was reduced to between 0.7 and 1ns when IEC 61000-4-2 was first published in the 1990s, because by then semiconductor die-shrinking had made ICs susceptible to shorter risetimes.\nSemiconductor die-shrinking every 18 months to two years is a fact of life in the electronics industry, and the resulting ever-smaller transistors operate faster and so can be upset by ever-shorter transient risetimes. Their ever-thinner insulation layers can be damaged by ever-lower peak transient voltages. Their internal I/O protection diodes may be smaller, hence less effective, and might even be omitted because they displace many hundreds of millions of transistors!\nThe result of continual semiconductor die-shrinking is that ESD protection is always ever-more difficult. And because die-shrinking affects all ICs, not just new types, products in serial manufacture can start to suffer ESD-related failures in their operating environments even when their design has not changed. They become more susceptible to real-life ESD every 2 years or so, and I know several manufacturers for whom this fact alone has caused large warranty costs.\nAs long ago as 2004, the ESD test standard IEC 61000-4-2 was criticised in  as follows:\n“In our opinion, the ESD standard should be revised such that ESD generator performance is as similar to the reference events as possible in all their parameters. But manufacturers and users need to be aware that the standard does not cover all possible ESD events. For example, medical equipment might need to be tested using a shorter rise time to cover a larger portion of the real ESDs, notwithstanding furniture ESD or other ESD types.\nThe standard needs to be understood as a minimum requirement, passing it does not protect against ESD related field failures.”\nAlso in 2004, Fischer Custom Communications sold an ESD field simulator called the PEG-1, with these words in its advertising flyer:\n“The PEG-1 simulates forms of ESD in the environment that are not covered by current standards. The electromagnetic fields from these types of ESD events have been shown to cause problems in electronic equipment. Characteristics of these “unusual forms of ESD” include fast risetimes and multiple events over a few seconds. Examples include internal chair discharges and “jingling change”. Descriptions of these events can be obtained from the published papers in the “Technical Information and Downloads” section of http://emcesd.com.\nThe PEG-1 induces voltage in nearby circuits with risetimes of about 200 picoseconds, much faster than present ESD standards. The ringing frequency of the PEG-1 ranges from hundreds of MHz to greater than 1 GHz. In addition, about 3-7 events/second are generated by the PEG-1. All of these PEG-1 characteristics simulate real world events of this type.”\nThe PEG-1 no longer appears on Fischer Custom Communications’ website, but may be available for hire or purchase second-hand, perhaps from ESD Guns or ebay.\nIn January 2017, Doug Smith said in :\n“It seems that ESD is getting even more of a problem lately with strange forms causing new problems. There is no shortage of articles and information on standard tests, but often a custom test is necessary to avoid field problems.\nIssues concerning cable discharge, slow degradation over time in the field due to small to moderate, but very common ESD events, and others are developing. I would be interesting to hear how you are dealing with these problems and I have included some links below.”\nAn issue alluded to in the quotations above, is that IEC 61000-4-2 only concerns ESD caused by people becoming tribocharged and touching things, which is called ‘personnel ESD’ or ‘the Human Body Model’, HBM. It doesn’t cover “Furniture Discharge”, mentioned in the first two quotations, and also mentioned in EMI Story number 138. And “precipitation static” has been a particular problem for aircraft radio-comm’s, see EMI Story number 295.\nThere are many other sources of ESD, as has long been recognised by semiconductor manufacturers, who have a constant battle to prevent ESD in their manufacturing plants from damaging their products even before they are supplied to product and equipment manufacturers. The EOS/ESD Association, Inc., identifies: Human Body Model, HBM; Machine Model (MM); Charged Device Model (CDM); and Field Induced Discharges.\nMany researchers have investigated the so-called “micro-gap discharges” occurring between charged metal objects with less than 100V potential difference across gaps of a few microns, with some evidence that their risetimes can be as low as 10ps – which is very difficult indeed to verify experimentally. In , Doug Smith focusses on the micro-gap discharges that occur between jingling coins in a pocket or bag.\nPeople can only reliably feel sparks from their fingers when they exceed 4kV, and they only start to feel really painful at 8kV and above. But some modern semiconductors can suffer permanent damage from less than 0.7V (which I have found can cause latch-up failures in Si-Ge low-noise microwave amplifiers).\nAnd with modern digital ICs such as DDR4 RAM chips running from power rails as low as 1.35Vdc, data corruption can occur with ESD transients as low as 0.5V peak.\nThe word from the IEC TC77 committee is that they have no plans to reduce the risetime in IEC 61000-4-2 below 0.7ns to better suit modern real life ESD susceptibility. So we are forced to develop our own ESD tests, as Doug Smith says above.\nIn 2013, it was said that typical digital ICs needed protecting against ESD with 100ps risetimes (i.e. up to at least 3GHz) and peak voltages limited to 10V (ideally, kept to within the power rails).\nBecause we have had three die shrinks since 2013, we should probably use 50ps risetimes (i.e. up to at least 6GHz) in 2019. See section 1.18 in my training course Module 1 for how to determine the highest frequency of concern (which I call fMAX) for the ICs used in a product or equipment.\nSo, how can we extend our ESD testing to cover risetimes of less than 0.7ns?\nSome ESD guns can test to different ESD standards by fitting different ‘RC Modules’ (Discharge Networks), which might be available or modifiable to get shorter risetimes (e.g. try 200pF – 0 Ohm ‘Machine Model’ discharge networks).\nThe Thermo Keytek MinZap MZ-15 was measured in  as achieving 150ps risetimes with its discharge network shorted-out. Like the PEG-1 the MZ-15 is not a current model, but there are many of them available second-hand, e.g. from ESD Guns, and Ken Wyatt wrote an article in EDN on refurbishing them. If you want to modify MZ-15 risetimes, make sure to buy the model with a plug-in discharge network module (they also sold a model that had a fixed internal IEC 61000-4-2 discharge network, which is very much more difficult to modify).\nTradeport make a micro-gap contact discharge tip for certain models of Noiseken ESD guns, that they say achieves approximately 200ps risetimes.\nLanger make some ESD coupling products that claim to achieve 200ps risetimes.\nAnd Grund make a “PurePulse” product that claims to test to all of the IEC 61000-4-2 and EOS/ESD Association models, plus a few more such as Transmission Line Pulsing (TLP), Very Fast Transmission Line Pulsing (VF-TLP), and Human Metal Model (HMM), with risetimes as low as 100ps.\nFor even shorter risetimes, we probably have to make our own micro-gap ESD sources similar to Doug Smith’s bag of coins in . Test labs should use transparent bags containing gold coins to impress customers, but the rest of us can make do with low-denomination plated steel or copper coins in any type of bag. It is of course important not to use a static-safe bag!\nIt is not necessarily 'overtesting' when we do more / go further than the requirements in the ‘applicable’ standards.\nDifferent manufacturers make products that are used in different ways, which will experience different electromagnetic threats including ESD – and if they pay attention to their warranty complaints they may notice that by 'overtesting' certain issues (such as ESD) they get fewer field failures and so make more profits. Or they might get more satisfied customers who buy more in the future, reducing the cost of sales and making more profits again. Or both.\n “Characterization of Human Metal ESD Reference Discharge Event and Correlation of Generator Parameters to Failure Levels—Part I: Reference Event”, by Ramachandran Chundru, David Pommerenke, Kai Wang, Tom Van Doren, Federico Pio Centola, and Jiu Sheng Huang, in IEEE Trans. EMC Vol. 46, No. 4, November 2004, pages 498 – 504; https://ieeexplore.ieee.org/document/1362865 and,\n“Characterization of Human Metal ESD Reference Discharge Event and Correlation of Generator Parameters to Failure Levels—Part II: Correlation of Generator Parameters to Failure Levels”, by Kai Wang, David Pommerenke, Ramachandran Chundru, Tom Van Doren, Federico Pio Centola, and Jiu Sheng Huang, in IEEE Trans. EMC Vol. 46, No. 4, November 2004, pages 505 -511, https://ieeexplore.ieee.org/document/1362866\n “HF News: ESD Issues, Issue 2”, January 30, 2017, by Doug Smith, https://myemail.constantcontact.com/HFNews-for-January-20--2017--ESD.html?soid=1127008467999&aid=RmapKkRMm-A\n “Mobile Phone Response to EMI from Small Metal ESD”, by Doug Smith, ‘Technical Tidbit - September 2004’ in his “High Frequency Measurements Web Page”, at: http://emcesd.com/tt2004/tt090104.htm\nGet more from EMC Standards\nEMC Standards is a world-leading resource for all things EMC and EMI related. Our website is packed full of both free and paid-for content, including:\n- Online quiz\n- Training quiz\n- And much more!\nElectromagnetic Engineering (EMgineering) is the basis for proven good design practices for signal integrity (SI), power integrity (PI), and the control of EMI emissions and immunity (EMC).\nOur aim is to help people learn how to more quickly and cost-effectively design and manufacture electronic equipment (products, systems, installations, etc.) to meet functional (i.e. SI/PI) specifications and conform to EMC standards, directives and other requirements.\nSuch equipment should benefit from reduced warranty costs and financial risks, whilst improving uptime, competitiveness and profitability.\nWe also cover basic good electrical safety engineering; and the Risk Management of Electromagnetic Disturbances / EMI, whether for Functional Safety or other types of risk.Join EMC standards TODAY!""]"	['<urn:uuid:3f00ea9d-556f-4284-b9c5-dcc9b594171a>', '<urn:uuid:9dda5bfb-dc6a-4465-98cd-6533655c0288>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	12	91	2486
73	I'm writing an academic paper about medieval trade and I was wondering how far Muslim merchants managed to expand their commercial networks during the 8th century?	By the mid-8th century, Muslim traders had surpassed Roman merchants in reach, establishing trade routes across the Sahara as far south as Ghana, where they traded salt for gold. They also crossed the Gobi desert and established trading posts as far as Canton, China.	"['A Tale of Four Cities\n9th century urbanisation\nran his \'empire\' from a village in Germany. Constantinople\nhad a falling population within its ancient walls, devoted\nto piety and ritual.\nCordova was an up-and-coming \'university\'\ntown in Moorish Spain and Baghdad a thriving metropolis\nof an empire which ran from the Atlantic to central Asia.\n1cm = 5km\nof Muslim Intellectuals\nJust as the\nGreek empire of Alexander, 1500 years earlier, the new world\norder of Islam brought together a diverse heritage – Indian\nmysticism, Persian art, Greek science, Roman pragmatism, Christian\nand Jewish traders. It was a crucible for innovation.\ndarkest of Christian centuries – the 9th and 10th – a\nconfident, expansive Islamic empire nurtured an \'enlightenment\'\nwhich preserved and extended much of classical knowledge. Urbane\ncaliphs patronized art and science and encouraged the translation\nof classical literature. Muslim intellectuals adapted knowledge\nto the needs of Islam, but for the most part they were free to\nwide horizons and made inventions and discoveries unimagined\nsank under vicious Christian despotisms, wrote nonsense-filled\n\'Lives of the Saints\' and wrangled over the dating of\nEaster, scholars in the new cities of Islam established the basis\nfor modern science and mathematics.\nIn this springtime\nof Islam – confined to the later Umayyad and early\nAbbasid dynasties – Muslim scholars began a serious\nstudy and analysis of the science of antiquity.\nhundred years Arabic would remain the major intellectual and\nlanguage of the world.\nModern mathematics was virtually\ninvented by Islamic scholars.\nIt was a useful discipline for\nan empire of merchants.\nmathematician Muhammad ibn-Musa al-Khwarizmi writes \'ilm aljebr\nwa\'Imuqabalah\' (\'the science of reunion and equation\'),\nusing ancient Greek and Indian source books.\nFrom \'al-Khwarizmi\' algorithm is\nderived; from \'al jeb\'r algebra;\nfrom \'jayb\' sine.\nThe Hindu \'sunya\' (\'empty\') becomes the Arabic \'sifr\' which\nin turn becomes the European zero in\nthe 15th century, vital to the further advancement of mathematics.\n\'Sifr\' is also\nof the word cipher.\n9th century Thabit\nbin Qurrah applies algebra to the\nsolution of geometrical problems.\nc 900 Arabs introduce decimal\n(base 10) numbers into Spain (recorded in the Codex Vigilanus,\n1010-1075 Arab Ahmad al-Nasawi (\'al-\nMuqni\'fi al Hisab al-Hindi\') writes about Hindu calculation\nof fractions, square and cubic\nEarly 15th century Al-Kashi writes Key\nIn 1130 Adelard of Bath translated an Arabic copy of Euclid\'s\n\'Elements\'. 300 years later Johannes Campanus – chaplain\nto Pope Urban IV – published a fresh translation\nIn the 13th\ncentury Italy, Leonardo Fibonacci\nintroduced Arabic numerals and 10-based\nnotation (\'Liber abaci\' 1202; \'Liber\nquadratorum\' 1225) from al-Khwarizmi\'s work.\nof Aurillac – later, the \'scientist\npope\' Sylvester II ! – wrote about\n1335 saw the first\noriginal European work on mathematics – Richard\nof Wallingford writes work on trigonometry.\nEuclid\'s Elements (3rd century BC)\nConquest of Hispania – A 5 -Year Wonder\n200,000 Goths ruled an indigenous population of about eight\nmillion Hispano-Romans. And yet the Arab/Berber\narmies overran the peninsular with breathtaking speed.\nMuslim success was due in no small part to the\nbrutality and divisions of the Christian-Visigothic\nkingdom. The Gothic aristocracy was riven by\nrivalry; the indigenous Hispano-Romans were treated\nas second-class citizens; heretics and Jews were\npenalised and persecuted.\nthe population was said to have perished from\nfamine and plague during the reign of Visigothic\nking Erwig (680-686).\nthe Muslim invaders arrived many Roman cities\nwere already disappearing under subsoil, irrigations\nsystems had fallen into disuse and the extensive\nindustry abandoned. The Arab armies followed\nthe Roman roads to reach the major cities and\nas often as not left Jewish garrisons in command.\nIn the decisive battle of the River Guadalete\nin 711 a Berber army of 12,000 defeated a force\nseveral times its size but made up of ill-treated\nslaves led by a treacherous nobility. Visigothic\nKing Roderick was never seen again.\nWhen the Umayyad\ndynasty of Damascus were murdered en masse by the Abbasids in\n750, one Umayyad prince – Abed Al-Rahman I –\nescaped to the distant Muslim province of al Andaluse (Spain).\nHere he re-established\nUmayyad rule in an independent and enlightened\nemirate. His lavish court life nurtured a \'golden age\'\nof Islam in Spain which lasting nearly 300 years\n(756-1031). In 929, in a challenge to the weakening Abbasids\nin Baghdad and rival Fatamids in north Africa, emir\nAbd al-Rahman III proclaimed himself caliph.\nhis capital, was Europe\'s greatest city for more than two hundred\nyears, not only in population\nbut also in the richness of its cultural and intellectual life.\nIn the 10th century the city\'s Jewish Academy and Talmudic\nschool – headed\na wealthy Jew in the diplomatic in the service of the caliph, Hasdai\nShaprut – eclipsed\nthe Babylonian academies of Sura and Pumbeditha. Muslim rule continued\nin a shrinking al Andaluse for\nanother 600 years.\nIn the Spain\nof the Umayyads, Christians and Jews (known as the \'Mozarabes\')\nenjoyed the same liberties as the Muslims, prospered in commerce\nand were admitted into the\nschools and universities.\nthe Bones of Giants\nCorduba/Cordova was the largest city of Roman, Visigothic and Umayyad\nSpain. The Great\nMosque of Cordova (begun in 784 and eventually\nas large as St Peters in Rome) was the wonder\nof the medieval world. Hundreds of marble,\ngranite and onyx columns – spoil\nfrom the local Roman temple and other more\ndistant sites – were mounted by\nstunning double arches and held a ceiling of\nto a cathedral by Ferdinand in 1236 the building\nwas safe – and used by both Christians and Moslems –\nfor 300 years. But then Christian fanaticism\nin the 16th century vandalised the structure.\nA monstrous Gothic \'cathedral\' was raised in\nthe centre of the great prayer hall, a 300\'\nhigh belfry replaced the minaret, and most\nof the many doors were sealed shut, making the structure\ngloomy inside and uninviting from the outside. But\nfor local protest, Christian fanatics would\nhave demolished the whole thing.\nto the left, cathedral to the right!\npeeved Emperor Charles V, expressed regret\nfor the ill-conceived pious vandalism.\n""You have built here what could have been built as well anywhere else;\nand you have destroyed what was unique in the world.""\nThe Mezquita or\nGreat Mosque (showing its best side).\nPredictably, the mosque\nwas built on the site formerly occupied by a Visigothic\nchurch and before that, a Roman temple.\nThe bizarre cathedral-within-a-mosque.\nThe city of Cordova had a special place in Christian triumphalism. When\nConstantine I seized the Roman throne in 312 his\ntheological advisor was Hosius, Bishop of Cordova.\nAbbasid dynasty relocated the centre of Islam from the ancient\ncity of Damascus in Syria to a new city of Baghdad in Iraq. Here\nthe new overlords of Islam, Arabs themselves, became increasingly Persianized\nIn 832 the\nastute caliph al Mamun – a patron of the Mutazilites – established\na \'House of Wisdom\' for the purpose of translating\nGreek and Latin science texts into Arabic. The choice of texts\nfor translation was a privilege of the ruling class, with astrology (astronomy)\nranked highest for its reputed powers of \'prediction\'; alchemy (chemistry)\npromised the creation and control of wealth; and medicine the\nalleviation of suffering. This initial focus on \'useful\' texts\nin time gave way to greater latitude for translators to select\nthe best examples of abstract knowledge. Al Mamun also established\nan observatory at Jundeshapur. The new \'university\' which had\ncome into being served as a model for al-Azhar (\'the\nmosque/seminary of Cairo (969) and the earliest medieval universities\nin the West.\nlater, the barbarian Christian kingdoms of Europe acquired\nthe science and technology developed by the empires of Islam.\nBy the end of the 13th century, universities had been established\nin Paris, Bologna, Padua, Ghent, Oxford, and Cambridge. The\nRenaissance and European imperial conquest became possible.\nIn a tragic\nirony, the Islamic world, trapped in oligarchic theocracy, stultified\nfurther development and began centuries of stagnation and decline.\ncentury Umayyad gold Dinar\nthe mid-8th century Muslim traders had excelled\nRoman merchants, crossing the Sahara and establishing\nthemselves as far south as Ghana where they exchanged\nsalt for gold; and crossing the Gobi and establishing\nthemselves in Canton, China.\nCopy: A gold coin of 8th\ncentury Saxon King Offa of Mercia (757-796)\nEngland\'s Muslim coinage with Arabic inscription: in Kûfic\nscript the coin proclaims \'OFFA REX\' but also \'Muhammad\nis the Apostle of Allah\'.\nShould we be surprised?\nCharlemagne (771-814), Offa\'s illustrious contemporary,\nwas using the Greco-Egyptian deity Serapis on his\nAstronomy: the oldest\nsophisticated instruments which made possible the European\nas the astrolabe, the quadrant, and accurate navigational\nmaps, were all developed by Muslims.\nbuilt on the knowledge known in antiquity and dominated\nastronomy for over a thousand years.\nEarly in the 9th century, an\nobservatory was founded in Jundeshapur, south-west Persia.\nAlso, during the 9th century al-Khwarizmi draws up astronomical tables.\nIn 827 Hajjaj Ibn Mater translated\nPtolemy\'s Geography into Arabic.\nMuslim geographers produced numerous books on the geography of Africa, Asia, India, China and the Indies from\n8th to 15th centuries.\nThese writings included geographical\nencyclopedias, almanacs and road maps. Ibn Battutah\'s 14th\ncentury masterpieces provided a detailed view of the geography\nMuslim geographers and navigators learned of the magnetic\nneedle from contact with China and pioneer\nthe use of magnetic\nneedles in navigation.\nnavigators relied on Muslim pilots and their instruments when exploring unknown\nterritories. From them, the west acquired the compass and the astrolabe.\nDuring the 9th\ncentury Ptolemy\'s \'Syntaxis\nMathematica\', was translated into Arabic\nas the Almagest.\nThe Syntaxis was\nthe first comprehensive mathematical\nstudy of astronomy, and used observations made everywhere\nfrom Ceylon to Britain.\nNadir – from\nArabic \'nazir\' (point of heavens directly under\nobserver); zenith – from \'samt\narras\' (path overhead); Azimuth – the\narc from zenith to horizon – from \'assumut\'.\nAbu Rayhan al-Biruni (973 -1048)wrote\nan elaborate astronomical treatise on the elliptical motion\nof the planets and the movement of the earth around the sun,\ncommenting on the obliquity of the ecliptic, the precession\nof the equinoxes, and the length of the solar\nalso posits the finite nature of matter, centuries before\nin the 12th century the Almagest was translated into Latin by Gerard of Cremona;\nThe Tables of al-Khawarizmi were translated\nby Adelard of Bath, Robert of Chester and Gerard\nby pagan Greeks in the 2nd century BC – preserved\nand improved by Arabs\nArabs acquired the astrolabe from the Christian school\nat Jundishapur in the 8th century and by\nthe 9th century had developed it into a far more sophisticated\ninstrument, used among other things to determine prayer\ntimes and the orientation\nof mosques towards Mecca.\nIn the 8th century Mashallah\nbin Muhammad al Nahavandi writes\ntreatises on the astrolabe, the armillary sphere and the\nmovements of heavenly bodies.\nWilhelm of Hirsaw in \'De astronomia\' described\nIslam\'s astrological instruments.\nSpaniard Pedro Alfonsi brought Islamic knowledge\nof astronomy tothe court of English king Henry I.\nDuring the 12th\ncentury the astrolabe re-entered the Christian west via Muslim Spain.\n858 - 929 The\nSabian scholar Abdallah\nal-Battani at Samara makes a remarkably accurate\ncalculation of the year (356 days, 5 hours, 46 minutes,\n24 seconds). He also developed the\nconcept of cotangent and furnished their table in degrees.\n1087 Al Zarqellu produces astronomic tables in Moorish\nToledo (\'Toledan Tables\') describing positions of\nDuring the 12th\ncentury, \'Toledan Tables\' were translated into\nLatin by Gerard of Cremona and were used in\nEurope for the next 300 years. Users included Nicolaus\nCopernicus, Tycho Brahe and Johannes Kepler\nIn the mid-13th\nRoger Bacon used the Arab almanac in his major work, \'De scienta\nstellerum - De numeris stellerum et motibus.\'\n9th century: Ibn Firnas of Islamic Spain\ninvents eyeglasses and they are manufactured\nand sold throughout Spain for over two hundred years.\n965 - 1038 Al Hassan ibn al Haytham (aka \'Alhazen\')\nwrites work on optics. He makes reference to the camera\nspherical and parabolic mirrors, study of spherical aberration,\nspectrum and refraction.\nIn 1270, Al-Haytham translated into Latin\nand influenced the work of Witelo and Kepler. \'Alhazen\'\nbecomes the most quoted physicist of the Middle Ages.\nRoger Bacon mentioned eyeglasses after studying\nthe work of al-Haytham whose research he frequently\nW. Cook & R. Herzman, The Medieval World View (OUP, 1983)\nB. F. Relly, The Medieval Spains (Cambridge UP, 1993)\nR. McKitterk, The Early Middle Ages (OUP, 2001)\nJohn Gribbin, Science a History (Penguin, 2003)\nWilliam Dalrymple, From the Holy Mountain (Flamingo, 1998)\nN. H. H. Sitwell, Outside the Empire-The World the Romans Knew (Paladin,\nEdward Gibbon, Decline & Fall, Chapters 50-52 The Coming of Islam,\nM. Brett, W. Forman, The Moors, Islam in the West (Orbis, 1980)\nJustin Wintle, History of Islam (Rough Guides, 2003)\nJ. Bloom, S. Blair, Islam - Empire of Faith (BBC Books, 2001)\nJ. J. Norwich, Byzantium, The Early Centuries (Viking, 1988)\nC. McEvedy, The Penguin Atlas of Medieval History (Penguin, 1987)\nRobert Marshall, Storm from the East (BBC Books, 1993)\nSome fifty articles are now available as a book.\nFor your copy order:\nCopyright © 2004\nby Kenneth Humphreys.\nCopying is freely permitted, provided credit is given to the author and no material\nherein is sold for profit.']"	['<urn:uuid:029767d5-b5b3-48f6-8d7b-d631cfabbb12>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	26	44	2186
74	Why does Hispanic Heritage Month start on September 15?	September 15 was chosen because it's the anniversary of independence for five Latin American countries: Costa Rica, El Salvador, Guatemala, Honduras and Nicaragua.	['Are you looking for a way to celebrate Hispanic Heritage Month? The literature, art, food and music from over 20 Spanish-speaking countries provide so much inspiration for special events, lesson plans or activities. Even if your family or students are not Hispanic or Latino, this national observance is an opportunity to learn about this rapidly growing population’s traditions, contributions and history.\nDYK: According to the US Census Bureau, the Hispanic population in the United States was 62 million in July 2021, making people of Hispanic origin the largest racial or ethnic minority at 18.9% of the total population.\nHistory of Hispanic Heritage Month\nBefore planning your celebration, let’s take a moment to review the history. Celebrated from September 15 until October 15, this national observance started in 1968 when President Lyndon B. Johnson designated a week to recognize the contributions of Hispanic Americans. In 1988, President Ronald Reagan expanded the weeklong observance to a month. It was subsequently enacted into law on August 17, 1988. The start date of September 15 was significant because it’s also the anniversary of independence for the Latin American countries of Costa Rica, El Salvador, Guatemala, Honduras and Nicaragua. Mexico, Chile and Belize’s independence days are September 16, 18 and 21. Columbus Day or Día de la Raza, is on October 12. Thus, the national observance in the U.S. extends until October 15.\nHispanic Heritage Month Celebrations at Home, School or Program\nDiscover the arts, crafts, book recommendations, virtual tours, music and more of Hispanic and Latino cultures. The following list of fun and creative ways to celebrate Hispanic Heritage Month will bring vibrant colors, rich history and tradition to life for all ages at your home, school or program.\n- Host a Fabulous Frida Decorating Contest. Featured on U.S. stamps, calendars and postcards, the Mexican artist Frida Kahlo’s self-portrait has also landed in schools and programs nationwide. Make it a program or school-wide contest to create Fabulous Frida bulletin boards or classroom doors for Hispanic Heritage Month. Discover decorating ideas on DIY websites such as Pinterest and Spanish Mama. This downloadable resource is available for $4 on the Teachers Pay Teachers website.\n- Make the Perfect DIY Piñata! Decorating and filling this paper-mâché figure with fruit, candy or toys is only half the fun. The best part of this festive Latin tradition is breaking it open. Get instructions from the Kids Activities website.\n- Read Books Featuring Hispanic and Latino Authors. From picture to poetry, fiction and non-fiction books, celebrate the contributions of Hispanic and Latino authors with a storytime, book club or read-a-thon. Try 11 Picture Books Celebrating Hispanic Heritage Month or Books for Hispanic Heritage Month for the little ones. Also, check out 6 powerful books to read during Hispanic Heritage Month for yourself and your kids, 12 Hispanic Authors Who Target Young Adults and Teens or YA Books by Latino Authors for the older ones.\n- Make and Shake Mini Maracas. Kids Activities will help you create beautiful music with beans, empty film canisters, craft sticks and colored vinyl tap. Kinder Art offers a recycled version using water bottles. A staple of Latin music, a maraca is a rattle usually made from a gourd used as a percussion instrument. Just grab the handle and shake for sound.\n- Savor the Flavors. Host a taste-testing, lunch or dinner featuring the savory flavors of traditional dishes such as flan, pupusas, enchiladas, Arroz con pollo, empanadas and paletas. Celebrate Hispanic Heritage Month with 25 recipes from Mexico, Puerto Rico, Spain and more or 40 Latin-inspired Meals Kids will Love. Let your little chefs help prepare the meals.\n- Paint Ponchos. All it takes is paper, paint, glue and yard to recreate this traditional colorful Mexican sleeveless garment. Deep Space Sparkle says it takes about 40 minutes for children to make the famous Mexican poncho, also called a serape or jorongo.\n- Take a Virtual Tour of the Molina Family Latino Gallery. Take a trip to the Somos Theater or Learning Lounge with this interactive map. Learn about the features in the Molina Family Latino Gallery and content from the ¡Presente! Exhibit right from your classroom or home.\n- Learn Latin American Geography. Young children can color pre-printed maps or flags from Spanish Mama to decorate the classroom. Education.com recommends a map challenge that requires students to research different cities in South America for middle and high-school children.\n- Binge-Watch Spanish Movies. Teachers Pay Teachers recommends the Disney Pixar movie Coco for young children. There are also a variety of Spanish-language films featuring Hispanic and Latino entertainers on Netflix and other streaming services.\n- Make a Mayan Pyramid. The myths surrounding the Maya ruins at Uxmal (pronounced oosh-mawl) have captured the imagination of both young and old for its mystical murals, carvings and sculptures found in the grand architecture of pyramids. Inspired by Family can help you share the legends of the Mayan people with children two and older.\n- Visit the Smithsonian’s Latino Center. In 2020, Congress enacted legislation establishing an American Latino Museum in Washington, D.C. In the meantime, you can visit the Smithsonian’s Latino Center.\n- Step to Salsa. Incorporate a math lesson into your Hispanic Heritage Month celebration as children count the six steps to the salsa. Like the mambo, salsa is a Latin dance with over eight music counts. It’s also an opportunity to shake off stress, get exercise and build motor skills. Music is important for school readiness and social and emotional development. Add other dance moves such as the mambo, merengue and cha-cha. Invite a dance instruction to the classroom or visit YouTube for virtual lessons.\n- Fall for a Fiesta. Conclude Hispanic Heritage Month with a Fall Fiesta. Use the recommendations above to decorate your home, program or classroom. This celebration is an opportunity to showcase your month-long projects, share your knowledge and display your new dance moves. Also, check out the Fiesta Del Sur at William Beans Community Center or the 14th annual Latino Festival of Frederick.\nCheck out the MFE events section of the website that features a variety of local Hispanic Heritage Month events and activities. Share your creative ideas, photos and videos of Hispanic Heritage Month celebrations on Facebook, Twitter and Instagram @MDEngageEarly using the Hashtag #HispanicHeritageMonth.']	['<urn:uuid:e9e4913e-69f1-478d-85b8-211aeb5d85a4>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T00:43:36.053682	9	23	1044
75	compare yield times breidenbach beets golden	The Breidenbach farming operation produces sugarbeets with high yields, averaging 29 tons and 17% sugar last year. They typically plant in early April after a strip-till pass in late February or early March. In contrast, the Golden beet variety, which dates back to the 1820s, has a much shorter growing cycle, with crops ready for harvest after just 55 days (approximately 8 weeks) from planting.	['The Breidenbach sugarbeet legacy in northeastern Colorado goes back a long way — all the way, in fact, to 1905 when Frank Breidenbach began growing the crop for the new beet factory at Sterling.\nSo much has changed since then, of course. But the Breidenbach farming enterprise — now comprised of one of Frank’s grandsons, two great-grandsons and two great-great-grandsons — has continued producing sugarbeets and evolving with the times. Today, the family operation includes Stephen, his sons Dave and Jack, along with Dave’s sons, Don and Luke. Together, they farm 5,000 acres of Logan County ground, including 1,100 acres of sugarbeets, another 2,500 of corn and 1,400 in alfalfa. Ninety percent of their acreage is under center-pivot irrigation systems that utilize ditch and reservoir water.\nOne of the biggest changes in recent years has been the Breidenbachs’ transition to strip-till production of their row crops. They began strip tilling in 2008, and by the following year had converted 100% of their sugarbeet and corn acreage over to the system.\nThey’re hardly alone. As of 2009, about 30% of Colorado sugarbeet acres were under strip till; by 2011, the level was around 35%. In neighboring Nebraska, half of Western Sugar Cooperative shareholders’ acreage was in strip till as of two years ago; now it’s closer to 60%.\nThe region-wide trend has been largely driven by (1) less tillage and hence better conservation of soil moisture, (2) early spring wind erosion protection provided by the inter-row crop residue, (3) significant fuel savings and reduced equipment wear due to fewer passes through the field, and (4) the ability to apply fertilizer with the strip-till pass. It also has gone hand-in-hand with the ability to plant Roundup Ready® sugarbeets.\n“I tell people that strip till, Roundup Ready and Trimble® (GPS) are, together, as big a change in farming as going from a horse to a tractor,” Jack affirms. “It’s a total system.”\nAnnual precipitation in the area averages around 14-15 inches, “so conserving moisture in the spring is huge,” Jack continues. “I think that we’ve probably ‘saved’ two inches of moisture by moving away from conventional till — and, the texture of our soil is so much better.” That in turn contributes to improved water infiltration, not only during the irrigation cycle, but when Mother Nature turns on the spigot. “We tend to get hard rains,” Jack says. “With conventional till, that water just runs off; but with this tillage system, it doesn’t.”\nThe reduced damage from wind erosion has been a huge benefit, he adds. “We get some vicious spring winds, and we’ve [previously] lost up to a third of the beets,” Jack recounts. Replants have averaged less than 5% the past three years, however, while plant stands, as well as final yields, have steadily increased. The Briedenbachs averaged 29 tons and 17% sugar last year.\nThe Breidenbachs follow both corn and alfalfa with strip-tilled beets. They use a 12-row Orthman unit pulled by a JD 8530, tilling to a depth of 8-10 inches. Liquid fertilizer goes down with the Orthman, while starter is added with the planter.\nBecause they graze cattle on much of their ground and likewise are very busy in the fall with row-crop harvesting, the strip-till pass is performed in the spring — typically late February or early March, as weather allows. On corn ground, they’ll typically till at an angle to the old corn rows. Planting usually gets underway in early April.\nWhen farming conventionally, the Breidenbachs were performing up to eight or nine field operations prior to planting the sugarbeets — plowing, mulching twice, two land plane passes, and double bedding. “If it wasn’t dry enough, we’d go till it again to make sure it (the soil moisture) was all gone!” Jack quips, tongue in cheek. Now their entire program is comprised of the single strip-till pass, followed by the planter.\nThe Breidenbachs estimate that the combination of growing Roundup Ready beets under a strip-till system has bolstered their yields by around three tons per acre on average. “We couldn’t have done it (strip tilling) without the Roundup Ready,” Dave observes. “Because we couldn’t get [adequate] control from the previous herbicides, we’d still have had to cultivate.\n“The [strip-tilled] seedbed is so much better than it was under conventional,” he adds. “We have overall better soil health, including more earthworms. And we have much less soil erosion from wind due to having that old-crop residue on top.\n“It’s just a lot easier to get a good stand. I wouldn’t go back.” — Don Lilleboe\nLeft: For the past quarter century, the Breidenbachs have used a custom-built push-tractor system to keep trucks moving in sloping fields and wet conditions during harvest. “It’s safer and easier than hooking up chains,” Jack notes. “You can just hook up, push them through a wet spot, and release.” The disks on the back of the push tractor fill in the center-pivot unit’s deep wheel tracks prior to passage by the beet lifter and trucks.', 'Recommended Heirloom Varieties of Beets\nBotanical Names: Beta vulgaris\nHeirloom is defined by Webster’s Dictionary as “a horticultural variety that has survived for several generations usually due to the efforts of private individuals”. When you are purchasing seeds or plants often it will say heirloom on the package or tag. This simply means that this variety has been preserved for many centuries and not manipulated to any degree. If the seed dates back to the 1800s, then you are growing the same exact plant variety as someone during that time period.\nOften times the people saving the seeds are in the form of private organizations or seed libraries that have popped up across the nation. With seed libraries in place, gardeners can know that seeds dating back to our ancestors are being kept alive and moving forward for generations to come. If you are a seed saver, google “seed libraries” and find one in your local area or join an online group. Articles from the Boston Globe to the The Wall Street Journel have popped up talking about these new-age libraries for gardeners, who share and protect old varieties of seeds.\nHere are some heirloom beet varieties that gardeners clamor for:\n‘Crosby Egyptian’ – Another variety that is not your type beet shape, it is more flatted than round. The root is fabulous, but many gardeners rave about the greens. From seedling to older greens, they are full of flavor and are a must-have for anyone who loves beets. It is said that the “Egyptian” beet varieties were developed in Germany and this variety became popular in the States around 1880. This variety received a glowering endorsement in the American Garden and has been a favorite of kitchen gardeners since 1889.\n‘Crapaudine’ – It is believed that is beet variety is the oldest heirloom seed available dating back 1000 years. In the French book, The Vegetable Garden, published in 1885 it is stated then to be one of the oldest known varieties. This is a very, very rare selection and if you are able to find seeds – buy them right away. Like many older varieties, it is the shape of a carrot, but the flavor is clamored for by top chefs across the world.\n‘Cylindra’ – Want to impress your guests at your next dinner party? Serve them ‘Cylindra’ beets which look more like a carrot than a beet. This Danish heirloom variety dates back to the 1880’s and goes by two other names including ‘Formanova’ and ‘Butter Slicer’. The latter name it due to gets soft and desirable texture. The almost burgundy long, cylindrical shape is where the name obviously stems. At full growth after 60 days, most measure out at being typically 6 to 9 inches long.\n‘Yellow Cylindrical’ - This is a mangel-variety of beets, which means it is more oblong than round. This rare European heirloom was actually first used as animal food, but many foodies are finding this culinary treat and adding it to their gardens. The color is golden-yellow on the skin and the flesh is white. It is typically picked young and used to top salads or roasted as a side.\n‘Golden’ - This variety dates back to the 1820’s or prior. If you are not fond of beets, this may be a variety to start with as the flavor is very mild and unlike reds. The greens are very tasty as well. After planting, crops will be ready for harvest after 55 days.\n‘Chioggia’ or ‘Bassano’ - The striping occurs on the interior of the vegetable and gives it a candy-cane pattern. This heirloom variety was first introduced to the American public in the late 1840. It was named after a fishing village in Northern Italy where it had been first cultivated in the early 1800s. It can also go by the names: Candy Stripe or Bull’s Eye Beet.\n‘Albino’ – Originally from Holland, this gourmet white beets has a very sweet flavor, without the red staining quality of its counterparts. Yield is high and excepted to occur around 50 to 55 days after planting. It super, sweet flavor makes it unusual. Ideal for boiling, pickling, baking, and freezing. Also noted that it can be used for making sugar.\n‘Albina Vereduna’ – This heirloom variety originates from the Netherlands. It is a very worthwhile variety that is valued for its sweet flavor. Its unique white skin and flesh is an added bonus. ‘Albina Vereduna’ is a great side dish when it comes to fish and chicken dishes. Like all beetroot, the leaves are high in vitamins and nutrients. Delicious grated and served raw or paired with other varieties such as ‘Bull’s Blood’ and ‘Golden’.\nWhether it is red, golden, striped or white beets all these varieties have a wonderful history with gardeners and it would be a missed opportunity not to try at least one in the garden this year.\n- How to Grow Sugar Beets to Make Sugar\n- How to Grow Golden Yellow Beets\n- Are Beets a Vegetable?\n- Are There Different Types of Beets?\n- The Best Types of Red Beet Varieties\n- How to Store Beets']	['<urn:uuid:a60aea48-c25e-40b5-8715-182120f4559c>', '<urn:uuid:904c6459-0ab8-4160-900a-dbd945687cfe>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T00:43:36.053682	6	65	1693
76	how vertical farming works history techniques	Vertical farming, first termed by Gilbert Ellis Bailey in 1915, involves growing crops in vertically stacked layers in buildings or structures. The concept was further developed by Ike Olsson in the 1980s and Professor Dickson Despommier in 1999. It uses three main techniques: hydroponics (growing plants in water with mineral solutions), aeroponics (using mist without soil, developed by NASA), and aquaponics (combining fish farming with hydroponics). The system uses both natural and artificial lighting, and employs modern technologies to control humidity, temperature, and airflow, achieving yields ten times higher than traditional farming methods.	['There was a basic idea that was created in a classroom and grew up in a spectacular project that use modern technology and take place in a group of environmentally friendly concepts. Dr. Dickson Despommier introduced vertical farming as one of the possibilities to solve the food crisis problem as a result of the planet overpopulation.\nIt is estimated that by the year 2050 the earth’s population will increase approximately by about 3 billion people. Around 80% of population will be concentrated in urban centers and the rest of the land won’t be enough to produce required amount of food by traditional farming.\nThe Idea of vertical farming is based on producing corps inside multiple floors building directly in the cities. The building has very specific architectonic features and use modern and high efficient technologies that minimize waste and maximize production. As examples can be pointed artificial lighting system, solar panels and water recycle systems.\nThe vertical farm buildings are designed in different shapes and vary from individual projects. Gordon Graff’s sky farm is the name of building proposed for downtown Toronto as can be seen in figure 1. The fifty eight story building is supposed to grow up in the Theatre district and it will cover 2.7 million square feet of floor area and 8 million square feet of growing area. The whole production can feed 35 thousand people per one year.\nVertical farming offers big spectrum of advantages in environmental and also social sphere:\n- Year-round production is higher in vertical farming buildings than outdoor production.\n- Negative weather influence is significantly decreased and corps is protected against failure caused by floods or droughts.\n- There is no necessary use of pesticides, herbicides or fertilizers.\n- The technology allows controlling humidity, temperature, air flow…\n- Black and grey water is recycled and returned to the system by using process called evapotranspiration.\n- Using composting methods via methane generation to reuse energy.\n- Enormous fuel reduction because of eliminating transportation factors and using different harvesting methods.\n- Growth of employment opportunities.\n- Increasing food production in locations poor on farming land.\nThe idea of vertical farming is still young and the projects are developed individually depending on the country and location requirements. This concept might sound like utopia but I still find a huge potential in vertical farming even if I am looking at this concept from a critical prospective. Moreover, vertical farming might be a reasonable solution for countries involved with famine. As the definition says, “EIA is a process of identifying, predicting, evaluating and mitigating the biophysical, social and other relevant effects of development proposals prior to major decisions being taken and commitments made”. Given this definition, this project should follow the EIA principles. I still have some questions and doubts about using the right technologies and their final impact on the environment. However, the potential result makes it worth it to continue the research.\nVideo 1: The father of vertical farming (Source: EconomistMagazine, 2010)\nAlter, L. (2007, June 14). Sky Farm Proposed for Downtown Toronto. Retrieved September 9, 2011, from http://www.treehugger.com/files/2007/06/sky_farm_propos.php\nDespommier, D. (n.d.) The Vertical Farm. Retrieved September 9, 2011, from http://www.verticalfarm.com/\nEconomistMagazine. (2010, December 17). The Father of Vertical Farming. Retrieved September 11, 2011, fromhttp://www.youtube.com/watch?v=2HNSLEcAegw&feature=relmfu\nIAIA. (1999, January). Principals of Environmental Impact Assessment Best Practice. Retrieved from http://www.iaia.org/publicdocuments/special-publications/Principles%20of%20IA_web.pdf\nFigure 1:Proposed designs for vertical farms. From Wikipedia, the free encyclopedia, by Jacobs, Ch., Graff, G., SOA Architectes, Retrieved September 12, 20011, from http://en.wikipedia.org/wiki/File:Vertical_farms.jpg Reprinted with permission.\nSchroeder, K. (2008, November 10). Rewilding Canada through Vertical Farming. Retrieved September 9, 2011, from http://www.worldchanging.com/local/canada/archives/008855.html', 'As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on.']	['<urn:uuid:839fddd8-f1d9-42d0-8816-c24a7736c7f1>', '<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	6	93	1441
77	I'm a structural engineer curious about the difference in earthquake resistance between earthbag and rammed earth walls - can you compare their seismic performance characteristics?	While both systems offer good seismic resistance, rammed earth appears to have superior earthquake performance, particularly in its monolithic form. Research from New Zealand shows that monolithic earth walls perform better under earthquake conditions compared to walls made of separate units. For earthbag construction, it requires specific reinforcement techniques to resist lateral movement, including mesh reinforcement, vertical ties every 24 inches, and corner reinforcements using rebar or other materials to achieve earthquake resistance.	"['Sharing information and promoting earthbag building\n|Wall Bracing: Piers, Buttresses, Corner Reinforcements and more|\nThese general construction techniques are chosen to introduce low-tech and low-cost construction. Combined with good quality control, they can create buildings strong enough to survive hurricanes and resist earthquake damage. Good quality construction includes:\nplumb and level walls\nappropriate building dimensions\nand maintenance of exterior plaster layers and roofs\nEngineers or expert earthbag builders may be able to make specific recommendations better suited to your site and building. This information is intended for small single story houses built of 38 cm (15"") wide walls of 50# bags filled with an earth mix containing some clay.\nEarthbag is very strong in compression (carrying loads) but can benefit from additional reinforcing in key locations, including corners, to resist lateral movement.\nMesh can help earthbag walls resist wind loads and earthquake forces. With a thick plaster coating this can become a reinforced shell.\nMesh can be wrapped from below the lowest bag to over the bond beam on both sides. Use poly fishnet, galvanized chicken wire, or plasterer\'s lathing. Conventional plastic plaster mesh (rust proof) or galvanized plaster mesh can be used if budget allows. Use nylon or poly twine to tie from one side to the other between bag layers.\nAll earthbag walls should be tied together vertically at least every 24 ?. Nylon strapping, the kind used to secure loads on shipping pallets, makes an excellent tie down. Wrap over the bond beam and under the wall and then cinch down with special tool and fasteners. All fasteners must be strong poly or nylon cord or strapping or galvanized metal, and protected with an earth or cement plaster.\nBecause earthbag is also a flexible form rammed earth, walls can be a little bit soft or flexible until they harden and cure. This process usually takes a couple of weeks if a wall is covered from rain. Temporary bracing can be used during construction. Some of these reinforcement techniques will also add stiffness during the building process.\nCorners can be strengthened with metal, by overlapping or extending them, or by curving them. Walls can be stiffened by corrugated metal bracing, piers, buttresses, or by curving or jogging them.\nAll rebar should be at least #4 (1/2"") diameter.\nREBAR CORNER REINFORCEMENT\nBarre/ Rebar: When walls reach 1.5 m (60"") height, hammer a 1.5m (5\') long piece of rebar through the corner bags.\nSac/ Bag: Always alternate bags at corners and stagger joints for strength.\nAs high as possible on the finished wall, hammer a second rebar of the same length in. It will overlap the first.\nThis is the simplest way to strengthen corners of earthbag buildings.\nMESH CORNER REINFORCEMENT\nMaterials: Sturdy mesh of galvanized metal or plastic, bamboo strips or steel angle or pipe or tubular steel, twine or cord\nLatte/ Mesh: Place a 1m (39"") wide strip of sturdy mesh on the outside of the corner from top to bottom.\nCord/ Twine: Fasten the exterior mesh securely to the inside corner at every other course.\nVertical/ Vertical rod: Use metal or bamboo rod or additional mesh placed on the interior corner.\nSac/ Bag: Alternate earth-filled bags at corner.\nBamboo is very strong and flexible for reinforcing cement or earth construction. It can be either well encased in earth or left uncovered for inspection. Termites are very persistent. In hot regions they may be able to tunnel through earth floors. Bamboo (or wood) should be covered with thick earthen plaster that contains something like borax to deter termites if it is not left uncovered inside so that it can be checked and replaced if damaged.\nCORRUGATED METAL CORNER REINFORCEMENT\nMaterials: Corrugated metal roofing, rebar, nails, strapping or twine\nBarre/ Rebar: Drive 60 cm (24"") long rebar through corrugated metal strip at the corner to tie the reinforcement to the wall below. Repeat rebars at ends of metal strip or every 60 cm (24"").\nBande/ Strip: Cut pieces of corrugated metal roofing into strips 20- 30 cm (8- 12"") wide and at least 75 cm (30"") long. Overlap at corners and nail them into the bag below.\nSangle/ Strap: Use strong cord or mesh with wire or strapping every 60 cm (24"") to secure three layers of bags tightly around the metal.\nSac/ Bag: Stagger joints of earth-filled bags and alternate at corners.\nThis corner reinforcement stiffens and strengthens as well as unites walls if repeated every 5 bag courses.\nGalvanized metal lasts well when exposed to rain and damp air. But if metal is well incased in dry earth in a roofed building, and has been firmly tamped, it does not need to be galvanized.\nMaterials: Extra bags and barbed wire\nButtresses strengthen corners without rebar, and stiffen straight walls. Straight walls need a buttress or pier, intersecting interior wall, or a minor corner every 3- 3.5 m (10\'- 11\'). They also make it easier to add on earthbags to extend houses in the future.\nButtresses can be straight, sloping, or stepped. Benches or wider wall bases will also strengthen straight walls if the bags are well woven into the wall.\nA vertical-edged buttress must stick out from the wall at least 60 cm (24""), and a sloping or stepped buttress 75 cm (30"").\nabove: First Course\nFil de fer/ Wire: Lay barbed wire between each course. Loop wire from one wall into buttresses and out the other wall.\nSac/ Bag: Extend one wall out into the buttress.\nLeft: Fourth Course\nMur / Wall: Stagger bag joints on all walls.\nContrefort/ Buttress: Criss-cross bag courses at buttressed corners every course to fasten it into the wall.\nWhen plastering, clay-rich earth can be added to create curves or add decorations.\nMaterials: Extra bags and barbed wire\nA pier is usually a thickened wall section. It only projects out from the wall the width of a single bag.\nCriss-cross the bag courses to tie the pier well into the wall.\nLay barbed wire between layers so that it loops from the wall out into the pier and back into the wall.\nSee Your Ad', 'Rammed earth walls are constructed by ramming a mixture of selected aggregates, including gravel, sand, silt and a small amount of clay, into place between flat panels called formwork. Here is a Performance Summary to help you decide if you want Rammed Earth in your next project.\nThe colour of rammed earth walls is determined by the earth and aggregate used. The ramming process proceeds layer by layer and can introduce the appearance of horizontal stratification to the walls, which can enhance the overall appearance. It can be controlled as a feature or eliminated.\nAggregates can be exposed and special effects created by the addition of different coloured material in some layers, and elements such as feature stones or objects, alcoves or relief mouldings can be incorporated into rammed earth walls, at a price. Unusual finishes can be achieved by including shapes in the formwork that can be released after the wall has been rammed.\nRammed earth is very strong in compression and can be used for multi-storey loadbearing construction. Research in New Zealand indicates that monolithic earth walls perform better under earthquake conditions than walls made of separate bricks or blocks. A five storey hotel in Queensland is built of stabilised rammed earth. Rammed earth can be engineered to achieve reasonably high strengths and be reinforced in a similar manner to concrete, although horizontal reinforcement is not recommended and excessive vertical reinforcement can cause cracking problems.\nInteresting structural features, including leaning walls, have been constructed in rammed earth. Any difficulties associated with placing and ramming around reinforcement can be eased by careful management of the construction process and need not add significantly to the cost.\nRammed earth behaves as heavyweight masonry with a high thermal mass. Thermal mass absorbs or ‘slows down’ the passage of heat through a material and then releases that heat when the surrounding ambient temperature goes down. All other things being equal, a high mass building such as rammed earth remains close to the 24 hour average for the time of year: in many climates this may be too cold or warm for comfort. If heating or cooling is required, the walls need to be insulated to limit energy consumption.\nUsed correctly, and in the right climate, the thermal mass of rammed earth can delay heat flow through the building envelope by as much as 10 to 12 hours and can even out daily temperature variations.\nRammed earth walls become effective when the difference between day and night outdoor temperatures is at least 6°C. Where the diurnal range is greater than 10°C, appropriate design can exploit the high thermal mass of rammed earth to very good effect (see Thermal mass).\nIn cool or cold climates well-located rammed earth walls (e.g. feature walls within a well-insulated envelope) can supply a battery of useful thermal storage. Rammed earth is not recommended for tropical climates where high mass construction can cause a house to hold too much heat and cause thermal discomfort (see Thermal mass).\nBuilding energy efficiency rating tools such as AccuRate include the capability to simulate the combined effects of wall mass and insulation, and also allow comfort in extreme weather conditions to be reviewed, so the building’s performance can be optimised.\nInsulation is about stopping heat passing through a material rather than slowly absorbing or releasing it. As a corollary to its high thermal mass, rammed earth has limited thermal insulating qualities — similar to an uninsulated fibre cement wall.\nInsulation can be added to rammed earth walls with linings but, as a general rule, a 300mm rammed earth wall will not meet Building Code of Australia (BCA) requirements for external wall insulation. Hybrid buildings that use insulated framed external walls with rammed earth internal walls and feature elements can achieve high insulation and high mass.\nUnder certain design criteria (i.e. simple rectangle with north-facing glass) and in moderate (not temperate) climates, it was just possible to meet the NatHERS overall five star performance standard; however, six stars are now the mandatory minimum under the BCA (see Insulation).\nInsulation can also be added within the thickness of a rammed earth wall but this adds to its cost and changes the structural properties of the wall. However, it does provide the benefits of both excellent thermal mass and good thermal insulation in the one wall while retaining the desirable look, texture, feel, acoustics and low maintenance properties of the facing of rammed earth on each side.\nThe best location for insulation when used in conjunction with rammed earth is on the outside face of the wall, so that the thermal mass is within a contained and controllable external envelope (see Passive solar heating; Passive cooling).\nOne of the best ways to insulate against sound is have monolithic mass, which rammed earth provides very well. It has excellent sound reverberation characteristics and does not generate the harsh echoes characteristic of many conventional wall materials (see Noise control).\nFire and vermin resistance\nThere are no flammable components in a rammed earth wall and its fire resistance is thus very good. In tests by the CSIRO a 150mm thick Cinva-rammed earth block wall (similar to rammed earth) achieved a near four hour fire-resistance rating. There is no cavity to harbour vermin and nothing in the material to attract or support them so its resistance to vermin attack is very high.\nDurability and moisture resistance\nThe basic technology has been around for thousands of years and there are many rammed earth buildings still standing that are centuries old. Rammed earth does possess a generally high durability but all types of rammed earth walls are porous by nature and need protection from driving rain and long term exposure to moisture.\nMaintain water protection to the tops and bottoms of walls. Continued exposure to moisture may degrade the internal structure of the earth by reversing the cement stabilisation and allowing the clays to expand. In general, rammed earth does have moderate to good moisture resistance and most modern Australian rammed earth walls do not require additional waterproofing. New water repellent additives that waterproof the walls right through may make rammed earth suitable for very exposed conditions, including retaining walls, but may inhibit the breathability of the material.\nBreathability and toxicity\nProvided it is not sealed with material that is impermeable to air molecules, rammed earth maintains its breathability. Finished walls are inert but take care in the choice of waterproofing or anti-dust finishes to avoid adding toxicity to the surfaces.\nRammed earth has potentially low manufacturing impacts, depending on cement content and degree of local material sourcing. Most rammed earth in Australia uses quarried aggregates, rather than the ‘earth’ it is popularly thought to be made from. On-site materials can often be used but materials need to be tested for their suitability.\nThe embodied energy of rammed earth is low to moderate. Composed of selected aggregates bound with cementitious material, rammed earth can be thought of as a kind of ‘weak concrete’. It may help to understand cement and earth products as being at different points on an energy continuum, with earth at the low end and high strength concrete at the high end. Its cement and aggregate content can be varied to suit engineering and strength requirements.\nAlthough in principle it is a low greenhouse gas emission product, transport and cement manufacture can add significantly to the overall emissions associated with typical modern rammed earth construction. The most basic kind of traditional rammed earth has very low greenhouse gas emissions but the more highly engineered and processed variants may be responsible for significant emissions in their manufacture. For example, a 300mm rammed earth wall with 5% cement content has the equivalent of 15mm thickness of cement, equivalent to over 100mm of concrete (which mainly comprises sand and aggregate).\nBuildability, availability and cost\nRammed earth is an in site construction method. Although its buildability is good, formwork for rammed earth demands good site and logistics planning to ensure that other trades are not adversely affected in the building program. Services should be well planned in advance to minimise difficulties. After walls have been rammed in place, conduits for pipes and wires can be provided much as in other masonry construction, but may impact on surface finishes.\nBasic materials for rammed earth making are readily available across Australia, but cement and formwork may have to be transported long distances, increasing environmental and economic costs. Testing of local aggregates and potential mixes is essential if not using a proprietary system.\nProprietary approaches to rammed earth help guarantee consistency and predictable performance but come at a cost. The cost of a professional rammed earth building is comparable to other more conventional good quality masonry construction, but it can be more than twice as expensive as a rendered 200mm wide AAC block wall (see Autoclaved aerated concrete).\nRammed earth is particularly well established in Western Australia and is thus an economical option in that state. Most states have experienced builders who understand its potential and limitations but because it is not a common construction material outside of WA its relative rarity and specialist nature tends to be reflected in its relatively high cost. It typically requires high levels of control over material sourcing and batching, and expensive formwork. A key element in controlling costs is to design walls as simple panels and to avoid unnecessary complexity. Traditional rammed earth using human power for ramming and simple wooden formwork can be low cost (and low energy) but this is rarely a realistic option.\nThere are good networks in Australia including a broad based national organisation, the Earth Building Association of Australia (EBAA), which is a not for profit organisation ‘formed to promote the use of Unfired Earth as a building medium throughout Australia’.']"	['<urn:uuid:a4746ac1-787e-460d-9790-6103dbea5261>', '<urn:uuid:6cbe5cbd-ee50-4d31-a2dc-f44abda67471>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T00:43:36.053682	25	73	2640
78	contemporary museum entertainment vs contemplation analysis	Contemporary museums are experiencing a shift from pure contemplation towards entertainment and active engagement, as demonstrated through major installations. The traditional museum model emphasized quiet contemplation and prescribed viewing paths, but recent works like Test Site introduce physical participation and pleasure as legitimate artistic experiences. This evolution is further exemplified by exhibitions like 'theanyspacewhatever' at the Guggenheim, where artists create dynamic, participatory environments that expand beyond visual art into social spaces and interactive experiences. However, these entertainment-focused installations still operate within and reinforce institutional frameworks, maintaining the museum's authority while adapting to contemporary audience expectations.	"[""Ten Contemporary Artists Formulate a Group Exhibition\nTEN CONTEMPORARY ARTISTS INVITED BY THE GUGGENHEIM TO COLLECTIVELY FORMULATE A GROUP EXHIBITION OF INDIVIDUAL INSTALLATIONS FOR THE FRANK LLOYD WRIGHT ROTUNDA\nFeaturing Angela Bulloch, Maurizio Cattelan, Liam Gillick, Dominique Gonzalez-Foerster, Douglas Gordon, Carsten Höller, Pierre Huyghe, Jorge Pardo, Philippe Parreno, and Rirkrit Tiravanija\nVenue: Solomon R. Guggenheim Museum,\n1071 Fifth Avenue, New York City\nDates: October 24, 2008 – January 7, 2009\nMedia Preview: Thursday, October 23, 2008, 10 am – 1 pm\n(NEW YORK, NY – October 7, 2008) – During the 1990s a number of artists claimed the exhibition as their medium. Working independently or in various collaborative constellations, they eschewed the individual object in favor of the exhibition environment as a dynamic arena, ever expanding its physical and temporal parameters. For these artists, an exhibition can comprise a film, a novel, a shared meal, a social space, a performance, or a journey. Using the museum as a springboard for work that reaches beyond the visual arts, their practices often commingle with other disciplines such as literature, architecture, design, and theater, engaging directly with the vicissitudes of everyday life to offer subtle moments of transformation.\nThe exhibition brings together ten artists who exemplify this creative impulse: Angela Bulloch, Maurizio Cattelan, Liam Gillick, Dominique Gonzalez-Foerster, Douglas Gordon, Carsten Höller, Pierre Huyghe, Jorge Pardo, Philippe Parreno, and Rirkrit Tiravanija. While these artists all employ markedly different aesthetic strategies and do not constitute a formally affiliated group, their varying practices are conceptually unified by a mutual rethinking of the early modernist impulse to conflate art and life, and, thereby, to resist representation. In the process, the artists attempt to engender a kind of activated spectatorship, often by creating works that absorb and extend the conventions of museum practice.\nWhat is most striking about this loose affiliation of artists, all of whom emerged during the early 1990s and now boast strong, independent careers, is that they periodically and randomly join forces to create a variety of projects ranging from co-directing films, to purchasing the copyright for a Japanese Manga character and franchising her image, to initiating a land reclamation project in rural Thailand. Invited to collectively formulate a scenario for the exhibition, one that would reflect and articulate the unique nature of their practice, the ten artists determined that the presentation should comprise a series of unique projects that would intersect and overlap in the museum’s spiraling rotunda.\nOrganized by the museum’s Chief Curator, Nancy Spector, in close collaboration with the artists, this layered exhibition will thus reflect the dialectic between the group and the individual that informs their shared histories. Ms. Spector was assisted by Joan Young , Associate Curator of Contemporary Art and Manager of Curatorial Affairs, and Katherine Brinson, Assistant Curator, Solomon R. Guggenheim Museum .\nThis exhibition is sponsored by HUGO BOSS.\nAdditional support is provided by the Waldorf=Astoria Collection; The Andy Warhol Foundation for the Visual Arts; Etant donnés: The French-American Fund for Contemporary Art, a program of FACE; and The Grand Marnier Foundation.\nThe Guggenheim Museum gratefully acknowledges the Leadership Committee for theanyspacewhatever. Founding supporters include: Massimo de Carlo Gallery; Gagosian Gallery; Marian Goodman Gallery; Friedrich Petzel Gallery; and Esther Schipper. Supporters include: Casey Kaplan; Yvon Lambert, Paris; neugerriemschneider, Berlin ; Galerie Eva Presenhuber; and José Noé Suro.\nThe planning process began in the fall of 2004 and through a series of regular, open-ended discussions with all of the artists, the conceptual structure of the exhibition was determined. Instead of producing one, jointly created meta-project for the show, the artists have chosen to each produce an individual, site-specific work or selection of works for the museum’s Frank Lloyd Wright rotunda. In some cases, their projects are retrospective in nature, capturing their own individual histories and reflecting on their past collaborations with various members of the group. The exhibition will exist in both space and time; many of the works on view will reveal themselves sequentially and others will change throughout the duration of the project. Performances and film programs will form an integral part of the installation.\nThe Exhibition Title\nSuggested by Liam Gillick, the term “any-space-whatever” is used by French philosopher Gilles Deleuze to describe a cinematic moment of essential heterogeneity—a “singular space” in the film defined by multiple perspectives in which linkages among constituent parts may be made in an infinite number of ways. Therefore, the “any-space-whatever” is a filmic realm that represents a “locus of the possible.” In its application as an exhibition title, the term suggests the idea of a coherent space comprising multiple and shifting views that nevertheless coalesce to invoke the idea of pure potentiality.\ntheanyspacewhatever will be the first large-scale exhibition in the to examine the dynamic interchange among this core group of artists, a many-sided conversation that helped shape the cultural landscape of the past two decades. The artists will each contribute an individual project creating simultaneous, coexisting layers in the museum’s spiraling rotunda. The following is a list of works comprising the exhibition:\nAngela Bulloch (b. 1966, Rainy River, Ontario, Canada. Lives and works in Berlin) will insert a L.E.D. powered “night sky” into the museum’s oculus. Melting away the physical confines of the museum’s architecture, Firmamental Night: Oculus 12 (2008) will create a fiction of time and space that shifts the perceived order of things, so that day becomes night and inside becomes out. In addition, she is creating a new iteration of the “pixel box” sculptures which have formed a key element of her practice since the late 1990s. This sound and light-based sculpture is produced in collaboration with musician David Grubbs.\nMaurizio Cattelan (b. 1960, Padua, Italy. Lives and works in New York City ) will install a new sculpture in the fountain of the museum’s Frank Lloyd Wright rotunda. Cattelan’s life-size effigy of a beloved fairytale character lying facedown in the museum’s fountain will read as a crime scene replete with questions of intent: suicide, homicide, or ill-planned escape?\nLiam Gillick (b. 1964, Aylesbury, England. Lives and works in New York City and London ) will intervene in the Guggenheim’s operational systems such as directions, didactics, and seating, to subtly reorient visitors’ experience of the exhibition itself. His series of hanging aluminum signs will infiltrate the museum, playfully appropriating the conventions of institutional signage.\nDominique Gonzalez-Foerster (b. 1965, Strasbourg, France. Lives and works in Paris and Rio de Janeiro) will present Promenade (2007), an installation that transforms the museum’s third ramp into a lush rain forest with nothing more than eight channels of sound. Using a minimum of means, the artist will “tropicalize” the space, transporting the viewer to another reality altogether. Gonzalez-Foerster’s light-and sound based installation NY. 2022 (2008, created in collaboration with Ari Benjamin Meyers), will regularly animate the Peter B. Lewis Theater as a poetic trace of an orchestral performance commissioned by the museum’s Works & Process series and presented during the opening weekend of the exhibition. This production reconceives the science-fiction film Soylent Green (1973) into an abstract musical narrative about endings and departures.\nDouglas Gordon (b. 1966, Glasgow, Scotland. Lives and works in New York City, Glasgow, and Berlin) will exhibit a compilation of text pieces, providing a veritable archive of his written work. Encountered collectively, the texts reveal the artist’s obsession with opposites and their essential mutability—fact and fiction, good and evil, the base and the sublime, and so on. Gordon’s 24 hour psycho back and forth and to and fro, 2008, which will be shown in its entirety three times during the exhibition, is a new iteration of his landmark 1993 work in which he extended Hitchcock’s 1960 thriller over a 24-hour-period, slowing the film down to near stasis and creating a hypnotic viewing experience. Split onto two screens, the new version shows the film running forward and in reverse, allowing for startling moments of concordance.\nCarsten Höller (b. 1961, Brussels, Belgium. Lives and works in Stockholm) is creating a fully-functioning hotel room that invites visitors to spend the night in the museum’s rotunda on four slow-turning discs equipped with comfortable sleeping, dressing, and working areas. Members of the public can reserve the room for one night each and enjoy a leisurely private viewing of the entire exhibition at any point during their stay. Accompanying the hotel room is Höller’s Krutikow’s Flying City Revolving (2007), a transparent construction of seven rotating towers based on Russian architect Georgii Krutikow’s 1928 utopian vision of an airborne community for living and working, which would liberate the earth for purely recreational purposes. Installed on the roof of a midtown building, the model and the urban skyline behind it are viewed via live transmission from a rotating video camera, creating an ever-changing window to the outside world.\nThe Revolving Hotel Room (2007) has been made possible by The Waldorf=Astoria Collection.\nPierre Huyghe (b. 1962, Paris, France. Lives and works in Paris ) will stage a participatory event three times during the run of the exhibition (October 24 @ 6:30 – 7:30PM; and November 17 and December 8 @ 4:30-5:30PM) that disrupts and disorients the temporal flow of the museum’s presentation. He is also creating a book of iron-on transfers illustrating the Guggenheim’s exterior and interior, incorporating images of the spaces in which the artworks appear as a speculative device – a representation of a possible reality. The transfer book will be on display in the exhibition and available for sale in the museum’s retail stores.\nJorge Pardo (b. 1963, Havana, Cuba. Lives and works in Los Angeles) will transform one of the ramps with a labyrinthine network of intricately patterned and specially illuminated cardboard screens. The screens will provide an elaborate backdrop for silk-screened prints created by the artists participating in the show, which have been produced by a press that Pardo operates in his studio in collaboration with master printer Christian Zickler.\nPhilippe Parreno (b.1964 Oran, Algeria. Lives and works in Paris) will install a site-specific, illuminated movie marquee on the façade of the building, as an enigmatic “label” for the exhibition. Rendered in white Plexiglas and neon, this ghost of a sign announces the show without making any pronouncements about its content or structure. Parreno is also recording a special “guided tour” of the exhibition, which will be available on the museum’s audio guide. Instead of explaining the works on view, the soundtrack identifies earlier, iconic works by each of the exhibiting artists as well as a selection of some of their formative, shared projects. Available at special audio guide stops designed by Liam Gillick, the tour resurrects the histories of the individual artists while underscoring the collaborative impulse that has informed their work since the beginning of the 1990s. In a gesture typical to Parreno’s interest in surrogate voices and a kind of performative distancing from his subject, he invited world memory champion Boris Konrad to recite the audio guide script from memory, attempting to break a world record in the process.\nRirkrit Tiravanija (b. 1961, Buenos Aires, Argentina. Lives and works in New York City, Berlin and Bangkok) is creating a documentary film, CHEW THE FAT (2008) on the occasion of this exhibition. The film features extensive interviews with the artists in this show as well as with other friends and colleagues, thus providing an intimate perspective on the art of the 1990s. The interviews will be viewable on dedicated monitors in the museum’s High Gallery, and an edited-down, feature-length version of the film will be screened regularly in the museum’s theaters.\nIn addition to its core presentation, theanyspacewhatever will include three “micro-exhibitions” featuring the creative endeavors of a number of curators, filmmakers, and designers who have collaborated with many of the ten participating artists over the years to create separate but related projects and enterprises.\n- A series of screenings will showcase the work of Anna Sanders\nFilms, a production company based in Paris. Founded in 1997 by\nPierre Huyghe, Charles de Meaux, Philippe Parreno and the\nassociation of contemporary art distribution (Xavier Douroux and\nFranck Gautherot) in collaboration with Dominique Gonzalez-\nFoerster, Anna Sanders Films brings a new language of imagery to\ncinema, creating a hybrid form between film and the visual arts.\n- The Wrong Gallery, an ongoing curatorial project by Maurizio\nCattelan, Massimiliano Gioni, and Ali Subotnick, will also participate in\nthe exhibition. The gallery, which initially operate in a tiny exhibition\nspace behind a glass door in Chelsea and has since maintained an\nitinerant presence in institutions such as Tate Modern and the\nWhitney Museum of American Art, will present a special reprint of its\nannual newspaper, The Wrong Times, that unites previous issues to\ncreate an exhaustive archive of interviews with artists who have\ncollaborated with the gallery.\n-The graphic design studio M/M (consisting of French designers\nMathias Augustyniak and Michael Amzalag) will present a site-specific\nproject in the museum's Aye Simon Reading Room, in which they will\nrealign this functional space within their distinctive aesthetic style, to\ncreate a library of past projects. M/M has collaborated extensively\nwith many of the artists participating in theanyspacewhatever exhibition.\nThe exhibition will be accompanied by a fully illustrated catalogue featuring over 30 texts by scholars, critics, and curators, most of whom have shared in the artists’ individual and collective histories. The catalogue will include an introductory overview by Nancy Spector, essays devoted to the individual practice of each artist, and a series of concise texts focusing on pivotal group shows, organizations and collaborative projects. These multiple points of view will elucidate the group’s fluid social, intellectual and creative exchange, coalescing into the most comprehensive examination to date of its critical cultural impact. Topics in this section include: No Man’s Time (Villa Arson, Nice, 1991), M/M (founded 1992), Backstage (Kunstverein in Hamburg, 1993), Hiver de l’amour (Musée d’Art Moderne de la Ville de Paris, 1994), Lost Paradise (Kunstraum Wien, 1994), Mobile TV (Le Consortium, Dijon, 1995-98), Association des temps libérés (1995-), Permanent Food (1995-), Vicinato (1995) and Vicinato 2 (1999), Traffic (CAPC, Musée d’Art Contemporain de Bordeaux, 1996), Moment Ginza (Le Magasin, Centre National d’Art Contemporain, Grenoble, 1997), Anna Sanders Films (1997-), The Land (1998-), 6th Biennial of the Caribbean (1999), No Ghost Just a Shell (1999-2003), What If (Moderna Museet, Stockholm, 2000), The Wrong Gallery (2002-), Utopia Station (2003-), All Hawaii Entrées / Lunar Reggae (Irish Museum of Modern Art, 2006-07), and Il Tempo del Postino (2007). The distinguished roster of catalogue authors includes Michael Archer, Jan Avgikos, Daniel Birnbaum, Ina Blom, Stefano Boeri, Francesco Bonami, Nicolas Bourriaud, Xavier Douroux, Patricia Falguières, Hal Foster, Massimiliano Gioni, Michael Govan, Dorothea von Hantelmann, Jens Hoffmann, Chrissie Iles, Branden Joseph, Emily King, Tom Morton, Molly Nesbit, Hans Ulrich Obrist, Beatrix Ruf, Stephan Schmidt-Wulffen, Barbara Steiner, Rachael Thomas, Giorgio Verzotti, and Olivier Zahm. The catalogue is available in the Guggenheim Museum Store and at www.guggenheim.org for $60.00.\nA full schedule of programs will be presented under the auspices of the Sackler Center for Arts Education during the run of the exhibition. For information contact 212 423 3587 or guggenheim.org/education.\nAdmission and Museum Hours\n$18 adults, $15 students/seniors (65+), children under 12 free. Admission includes audioguide. Saturday to Wednesday, 10 AM to 5:45 PM; Friday, 10 AM to 7:45 PM. Closed Thursday. On Friday evenings, beginning at 5:45 PM, the museum hosts Pay What You Wish. For general information call, 212 423 3500, or visit www.guggenheim.org.\nOctober 7, 2008\n(Updated from March 14, 2008)\nFOR ADDITIONAL INFORMATION CONTACT:\nLauren Van Natten, Senior Publicist\nClaire Laporte, Associate, Media Relations\nSolomon R. Guggenheim Museum\n212 423 3840\nFor publicity images go to Guggenheim Images.\nUser ID = photoservice Password = presspass"", 'What Ever Happened to Fun: Carsten Höller’s Test Site\nAs part of the “Unilever” series, Test Site was Carsten Höller’s response to the Tate’s comission to actively use the Turbine Hall space to implement an artwork. The installation consists of five slides descending to the main floor from four different gallery levels. The combination of the playful and the utilitarian is not foreign to Höller’s artistic endeavours: by creating an alternate mode of circulation within the museum, Test Site explores human behaviour in relation to the surrounding environment. Miwon Kwon’s article “One Place After Another: Notes on Site Specificity” provides an illuminating theoretical backdrop to discuss the role of architecture and the physicality of the museum space, as reflected by Höller’s Test Site. By way of entertainment, this pleasure-inducing artwork challenges the historically-accepted purpose of the museum by physically engaging with the viewer within its wall. In analyzing the audience’s involvement with the work and the slide structure itself, the installation can constitute as a critique of the museum space which reminds one of the curatorial decisions concerning the paths that visitors should take when walking through exhibitions. Although fun, Test Site can thus be used to explore deeper concerns about the role of the artist, the participants, and the architectural structure through which a discursive field has been created within the contemporary museum space. As demonstrated by Carol Duncan and Allan Wallach’s institutional critique of the MoMA and the traditional labyrinth motif, Test Site reveals itself not solely as entertainment but as an allegory for museum ritual.\nRe-Thinking Museum Rituals\nDrawing from Pierre Bourdieu studies of the habitus, the installation of the playful Test Site within the Tate Modern’s Turbine Hall, takes on a meaning that transcends the simple pleasures of the audience. The physicality of the work, which modifies the very architecture and structural functioning of the Turbine Hall, as well as the adjacent galleries, reminds one of the significance of the architecture in relation to the formation of social behaviours. In The Silent Complicity of Architecture, Kim Dovey argues: “The more the structures and representations of social practices can be embedded in the framework of everyday life, the less questionable they become and the more effectively they work. This ‘complicitous silence’ of architecture is the source of its deepest power.” Höller’s Test Site clearly uses the architecture of the Turbine Hall to disturb its usual functioning by offering an alternate mode of transportation. As an alternative to traditional stairs, slides are introduced at each gallery level. The participants can thus slide all the way to the ground floor instead of walking. It might be relevant to point out that although the four slides connect the different gallery levels to the ground floor, it cannot be used to slide from one gallery floor to another. This particularity relates to how the traditional museum, as an institution of education and contemplation, arranges its collections according to space. The audience is guided through the institution and led through a certain path. Defined by one’s conscious and counter-intuitive decision, the installation alters and disturbs the museum’s authority, which is constructed by the designed pathways. Test Site can be spatially analyzed through ritualized movement within the museum environment. It can certainly be seen as an allegory of the prescribed path that the museum imposes on the viewer who cares to enter its walls. This allegory reminds one of Bourdieu’s symbolic capital, which involves “the power to establish the legitimacy of a particular symbolic order within a given field,” in addition to Foucault’s notion of the capillarity of power. Thus, the Tate Modern has the power to dictate what is good and what is not for its audience; in the case of Test Site, riding down an aluminum slide is considered appropriate.\nThe combination of the structurally altering slides and the ‘sanctification’ of the slide as appropriate for the Tate’s audience also emphasizes the contemporary issue of the changing purpose of the museum. Mostly because of social, political, and economic change, the purpose of the contemporary museum, now comprised of both culture and entertainment, directs its attention towards fun-inducing practices and the engagement of the audience as integral to the artwork itself. Although this mode of participation is clearly different for Test Site, the placement of the piece within the Tate institution repurposes the museum as a site of entertainment. One can argue that as consequence, it simultaneously diminishes the subversive power of the piece, which as Höller points out, is in fact a test site for the instigation of slide systems as a legitimate mode of transportation. In an article about the “Unilever” Series, it is argued that the series is “the final capitulation of installation art to the demands and logic of the culture industry and the ultimate subsumption of the latter’s critical ambition.” While the argument is not without weight, one can refer to site-specificity or site-responsivity to interpret Test Site in light of how the dispositions of such works function within the institution of the museum.\nA Supersized Labyrinth\nTest Site, with its vertical labyrinthine form, exposed uncanny resemblances to the critique of the architectural and curatorial choices made at the MoMA in New York. The entertainment provided by the ride of Höller’s slides seems undermined by the ritualistic and symbolic value of the museum viewer, turning into an active component within the site-specific work. In both situations, the audience member still absorbs the museum’s ideology, whether it instils discipline or bodily engagement. About the MoMA, Duncan and Wallach discuss:\n“This totality of art and architectural form organizes the visitor’s experience as a script organizes a performance. Individuals respond in different ways […] But the architecture is a given and imposes the same underlying structure on everyone. By following the architectural script, the visitor engages in an activity most accurately described as a ritual.”\nThe labyrinthine path prescribed to the viewer by the MoMA, which can be described as the prototype for the modern museum, is a series of galleries that are arranged in a chain-like pattern. To deviate from the route, it leads one off to cul-de-sacs and secondary paths which contain art that is obviously deemed to be of secondary importance. Assuming one does not deviate from the main route, artworks physically situated outside this conceived path remain unseen. The authority of the museum can never be evacuated because of their curatorial agenda. As discussed by Duncan and Wallach, the labyrinth form is a common image in ceremonial and ritualistic architecture, traditionally associated with enlightenment and rebirth, symbolic of the passage from darkness to light. The physical and conceptual metaphor of the labyrinth is the equivalent to the museum positioning itself as the guardian of art considered worthy of cultural recognition, but also as a provider of historical and artistic truthfulness. The museum is the ultimate provider of secular knowledge. The labyrinth “lead[s] you along a spiritual path that rises to ever higher levels of transcendence. […] Enlightenment in the labyrinth means detachment from the world of common experience and material need.” This experience desired by the viewer at the MoMA, parallels the experience at the Turbine Hall: a descent in one of Höller’s slides. It relates to the experience of the viewer within the slide, the meaning of controlled experience, and to the museum’s wish to provide another form of entertainment that involves the body rather than the intellect. The mere contemplation of artworks is therefore no longer enough. Fun and pleasure, such as in Test Site, is a safe bet in terms of visitor flow.\nThe awareness of the human body and the offering of an experience in a larger than life installation is what characterized Test Site and is what propelled its popularity. The scale of the installation impacts the human experience. Discussing Susan Stewart’s argument about the antagonism between gigantic and miniature, Davits writes: “although oppositional at first, [they] depart from the same distorted relationship to reality.” This distortion of reality is present in the vertigo and the abandonment induced by Test Site. As Dorothea von Hantelmann points out in the exhibition catalogue, the piece messes with the participant’s perception of space, inducing a loss of spatial and moral orientation. Thus, experiencing Test Site through the illusion of losing control, is telling of the abandonment of the viewer to the museum’s prescription. It also relates back to the experience described at the MoMA: “You are in a ‘nowhere’, a pristine blankness, a sunless white womb/tomb, seemingly outside time and history. […] the substance of the ritual is an internal drama.” The experience is lived internally; the aesthetic of the piece has nothing to do with it.\nIntroduced by von Hantelman, Foucaultian theory also finds echo in the human experience: Test Site relates to the modernist ideology of the consistently confident subject. Höller’s piece invites the audience to abandon itself, but unlike the traditional contemplative mode, consciously demands it in order to function as art. Test Site mimics the museum apparatus within the museum, through an entertaining form of engagement.\nThe entertainment provided by Test Site at the Turbine Hall assesses the new role of the museum, the artist, and the viewer in the contemporary art world. The installation of the slides as a site-responsive work, as described by Miwon Kwon in the article “One Place after Another”, disturbs the traditional conception of how to get involved in an artistic experience. This is not without bringing to memory Michael Fried’s argument about the theatricality of minimalism and how the involvement of the viewer is a perverse wish.\nThe modern museum increasingly relies on the engagement of the viewer. The entertainment value of Test Site, felt through the body instead of the eye, is telling of the shifting practices of the art world and the evolution of the criteria applied to the categorization of art. Art today is less about the visual aspect of the work. Thus the museum has to adapt its practices to stay viable while still maintaining its original purpose and intent. In the particular case of Test Site, it can also be said that the work integrates with the institutional framework. Through the experience of the ride, it can allegorically relate to the experience expected in a contemplative setting, as suggested by Duncan and Wallach.\nAs Miwon Kwon argues, “The work no longer seeks to be a noun/object but a verb/process, provoking the viewer’s critical (not just physical) acuity regarding the ideological conditions of that viewing.” Kwon also discusses how Minimal art “voided the traditional standards of aesthetic distinction based on the handiwork of the artist as the signifier of authenticity.” It is evident that the production of works such as Test Site is dependent upon participants, while the museum is dependent upon the viewer for its subsistence. The involvement of the viewer is a primary wish of the artist, because the work, although arguably pleasant to the eye, is not intended to be a strictly aesthetic experience. Höller’s production is perhaps one of the best examples to illustrate this shift towards the enjoyment of the viewer. One only has to look to his work Flying Machine, (1996) or more recent Mirror Carousel (2005). Both engage the viewer in an uncommon experience, whereby the audience is actually in the artwork and is intended to function as operator. The contemplative aspect is nullified; the audience and their experience are now the signifiers of authenticity and thus, the barometer of the museum’s curatorial and financial success.\nIn their essay, Duncan and Wallach write:\n“The everyday world, ostensibly banished from consciousness, nevertheless haunts the labyrinthine way. The labyrinth is, in fact, not a realm of transcendence but of inversions in which the repressed realities of the mundane world return, as it were, disguised as […] overwhelming forces.”\nThose few words obviously don’t refer to Carsten Höller’s Test Site, but to the architectural and conceptual structure of the Museum of Modern Art in New York. The discussion brought forward in the last pages make it easier to juxtapose this critique with Höller’s work. Test Site has the capacity to open a discursive field that relates to both the traditional museum ritual and the contemporary museum’s entertainment premise. The participant’s experience, as enjoyable as it may be, is nevertheless determined by the conceptual and physical structure of the Tate Modern.\nThe euphoric state induced by Test Site, as well as the focus and emphasis put on the audience, testifies to the success of the privately-funded contemporary museum. They evidently achieve their goal: curating an art exhibit that compels the viewer and dismantles the myth of the intellectually elitist museum. The focus seems to have shifted from the intellectual reflection to the bodily engagement, from the silent contemplation of art to the involvement in it.\nThe labyrinthine construction of space at the MoMA and the physicality of Test Site, which is basically a vertical labyrinth, are two different yet similar ways to explore the audience’s control (or perhaps loss of) within the museum wall. The path through MoMA and euphoric ride of Test Site address issues of rituals –whether the intention behind them is education or sheer pleasure. The playfulness of Test Site and the entertainment it provides is seemingly not enough to alter the ritualistic power and discipline advocated both by the traditional and the modern museum. The placement of Test Site within the museum institution proves Bourdieu right when discussing the role of avant-garde art: “[It] change[s] and enliven[s] the field without disturbing its foundations.” Clearly, fun is not sufficient to overturn the impact of institutional forces.\nBourriaud, Nicolas. “Relational Aesthetics.” Participation: Documents of Contemporary Art, (2006): 160-71.\nCarsten Höller Test Site. Edited by Jessica Morgan. London: Tate Modern, 2006. Published in conjuction with the exhibition “Carsten Höller Test Site” shown at the Tate Modern in London. An exhibition catalogue.\nDavidts, Wouter. “The Vast and the Void: On Tate Modern’s Turbine Hall and ‘The Unilever Series’.” Footprint (Autumn 2007): 77-92.\nDovey, Kim. “The Silent Complicity of Architecture.” Habitus: A Sense of Place, (2005): 283-96.\nDuncan, Carol, and Allan Wallach. “The Museum of Modern Art as Late Capitalist Ritual: An Iconographic Analysis.” Modern Art Culture: A Reader (2009): 178-94.\nFried, Michael. “Art and Objecthood.” Artforum (Summer 1967): 12-22.\nKwon, Miwon. “One Place after Another: Notes on Site-Specificity.” October 80 (Spring 1997): 85-110.\nMcClellan, Andrew. “From Boulée to Bilbao: the Museum as Utopian Space.” Art History and Its Institutions: Foundations of a Discipline, (2002): 46-64.\nSchubert, Karsten. The Curator’s Egg:The Evolution of the Museum Concept from the French Revolution to the Present Day. London: Ridinghouse, 2009.\nLIST OF FIGURES\nFigure 1 Installation View. Carsten Höller, Test Site.2006-2007. Turbine Hall, Tate Modern, London. Photos by M. Leith, A. Dunkley, O. Leith, S. Drake, M. Heathcote, J. Fernandes © Tate Photography.\n1 Kim Dovey, “The Silent Complicity of Architecture,” Habitus: A Sense of Place (2005): 291.\n2 Ibid., 288.\n3 Carsten Höller Test Site exhibition catalogue, 28.\n4 Wouter Davidts, “The Vast and the Void: On Tate Modern’s Turbine Hall and ‘The Unilever Series’,” Footprint (Autumn 2007): 79.\n5 Carol Duncan and Allan Wallach, “The Museum of Modern Art as Late Capitalist Ritual: An Iconographic Analysis,” Modern Art Culture: A Reader (2009): 178.\n6 Ibid., 190.\n7 Ibid., 184.\n8 Ibid., 187.\n9 Ibid., 189.\n10 Davidts, 81.\n11 Davidts, 86.\n12 Duncan and Wallach, 187.\n13 Carsten Höller Test Site exhibition catalogue, 27.\n14 Miwon Kwon, “One Place after Another: Notes on Site-Specificity,” October 80 (Spring 1997):103.\n15 Ibid, 91.\n16 Ibid, 99.\n17 Duncan and Wallach, 190.\n18 Dovey, 289.']"	['<urn:uuid:3a1affe6-2532-429e-a1ee-1ae4ce522428>', '<urn:uuid:726fb07d-cf99-4ac2-9049-35a179a5f89f>']	open-ended	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-13T00:43:36.053682	6	95	5192
79	What are premium vinyl features and installation challenges?	Premium vinyl records like UHQR feature hand-selected materials, flat profiles for perpendicular stylus playback, and specialized formulations without carbon black for reduced distortion. Each UHQR undergoes manual pressing and thorough inspection before being deemed market-ready. On the installation side, vinyl applications face significant challenges - cold weather requires pre-warming substrates and equipment, while hot conditions increase adhesive tack causing pre-adhesion issues. Installation temperatures must be carefully controlled, with some vinyl films requiring specific temperature ranges. Solutions include using portable heaters in cold conditions or scheduling installations during cooler times and using water misting in hot weather.	"['Audiophiles remember and still covet the UHQR pressings from JVC Japan from some 30 years ago. They were the pinnacle of high-quality vinyl, and UHQRs remain among the very most collectible and valuable LPs ever pressed. Now, Quality Record Pressings is reviving that long-gone name of excellence and adding to it an even higher standard of perfection. With the UHQR from QRP, we\'ve applied all of the most innovative and ear-approved ideas ever introduced to vinyl LP manufacturing to create an ultimate LP. Never before has one record represented such singular superiority.\nEach UHQR is pressed, using hand-selected vinyl, on a manual Finebilt press with attention paid to every single detail of every single record. All of the innovations introduced by QRP that have been generating such incredible critical acclaim are applied to each UHQR. The 200-gram records feature the same flat profile that helped to make the original UHQR so desirable. From the lead-in groove to the run-out groove, there is no pitch to the profile, allowing your stylus to play truly perpendicular to the grooves from edge to center. Every UHQR is hand-inspected upon pressing completion, and only the truly flawless are allowed to go to market. Your UHQR will be packaged in a deluxe box and will include a booklet detailing the entire process of making a UHQR along with a hand-signed certificate of inspection. This will be a truly deluxe product.\nUHQR: Hand selected and hand inspected. Handmade and worth every dollar paid.\nMuch more information to come. Wait for it...\nClarity Vinyl is the ultimate vinyl formulation because it contains no carbon black additive, common in vinyl formulas for LPs. Carbon black contains trace metals that become magnetized and cause electrical distortions in cartridges during playback. The result is a smearing of the sound. It\'s the same reason that cartridge demagnetizers are effective. By taking the carbon black out of the vinyl, we are able to dramatically reduce the distortion and thus bring more clarity to the playback process, providing a more realistic musical experience for the listener. Our Clarity Vinyl is also made using the highest quality co-polymer available - a key component in the vinyl pellets used for manufacturing LPs.\nMore to come...\nThe Very First Title Pressed at Quality Record Pressings Is…\nCat Stevens’ Tea for the Tillerman\nWhen word got out that we were beginning a pressing plant, customers began to call with questions. And the most common and full-of-anticipation question was inevitably: What will be the first title that you press? Well, we couldn’t possibly imagine hitting a bigger home run than to open Quality Record Pressings (QRP) with one of the all-time most classic audiophile records, Cat Stevens’ Tea for the Tillerman from 1970.\nThere are so many things perfect about this release. First and foremost, it’s a masterpiece of a record. It’s that rare record that couples breathtaking sound with hit after hit after hit. In fact, to list the hits would be to list the entire song list. We couldn’t pick a better vehicle for which to show off what we’re so confident will be the highest quality records ever pressed.\nBut here’s something else that’s cool: We scored the absolute original analog masters, and the tapes were in impeccable condition. It took an unbelievable amount of digging and research – and luck – to get this project done to the standards of Analogue Productions. But, wow, was it ever worth it! The tapes were last used in December 1999 when Ted Jensen at Sterling, along with producer Paul Samwell-Smith, remastered the Cat Stevens catalog for CD.\nIn 1970, Lee Hulko at Sterling Sound cut Tea For The Tillerman for A&M Records in the U.S. and Island Records in the UK using a Telefunken M10 tape machine and a Neumann VMS 66 lathe with a Neumann SX68 cutterhead. Hulko started Sterling in 1968 and was its original mastering engineer. He’s considered among the first engineers to advance mastering from just transferring music from tape to lacquer to an art where attention is paid to all the details that result in better sound. We actually found Hulko’s original mastering notes from more than 40 years ago. It’s incredible, but Sterling still has all of their notes filed away.\nSo, it was originally cut at Sterling – as were all of the early original Cat Stevens albums – and the tapes were last used at Sterling. How appropriate then that we should go back to Sterling for this monumental reissue. Using the original tapes, George Marino handled the mastering this time. He used an Ampex ATR-102 tape machine, another significant point of interest. While Ampex has long been revered for their sound, they had never made a preview version so that a mastering engineer could cut a lacquer from an Ampex machine. Mike Spitz at ATR Services made a unique preview modification for Sterling so that they could cut this record using an Ampex. Marino then used a Neumann VMS 80 lathe with a Neumann SX 74 cutterhead.\n“I think we’ve gotten something quite a bit better than what was originally issued,” Marino says. “I think this version is much more representative of what was on the tape. And that’s not a criticism of what was originally done.”\nMarino points out that since the original issue, there have been advancements in cutting lathe technology that make the improvements of this reissue possible.\n“You didn’t have the same number of options that you have in the new Neumann electronics,” Marino says. “With the new one, they give you more variations to work with. Let’s say there’s a nice kind of present sounding acoustic guitar on the left channel and then all of the sudden there’s a drum peak with cymbal crashes and stuff and that stuff happens to be on the left channel. Being the vocal is down the center, you can drive the high frequency limiter from the right channel. So you can set a threshold on the right channel and grab the vocal without wiping out some of the musical peaks on the left channel. This is what I talk about when I say that we have technical advantages that they didn’t have.”\nMarino also chose to use a wide-track stereo head for this project, which he said allows for better signal-to-noise than the normal stereo head.\nMarino says that he is very pleased with the results.\n“A great record. A classic,” he says. “And those tapes were in excellent, excellent condition. Musically, I think we’ve got something that sounds richer and more natural. It sounds more correct. I had to do very, very little to the tape regarding EQ processing or anything.”\nTo package this reissue, we’ve decided to do a facsimile of the original British Island gatefold jacket rather than the non-gatefold U.S. version. This British jacket also has a textured paper stock on the inside and is glossy on the outside. Additionally, we’re using the original pink Island label.\nSo there you have it. QRP is off and pressing in a big, big way! Finally, we’re ready to unveil the innovations in record pressing that we’ve been working on for more than a year. Among those innovations are the installations of microprocessors on the presses so that all of the presses functions are performed with absolute precision. For example, we’ve developed a dye with an imbedded temperature sensor that we can use to cycle the presses. Rather than having the presses close and open based on time – as it’s been to date – these presses will close and open based on temperature, the far more accurate indicator of when the record is ready. We also have a plating department in QRP, run by the best plating man in the business, Gary Salstrom.\nStill to come from Analogue Productions pressed at QRP will be the Cat Stevens classics Teaser and the Firecat and Catch Bull at Four.\n""This is why I got into this business."" – George Marino, after working with the master tapes for Billie Holiday\'s Body and Soul\nGeorge Marino\'s mastered Don McLean\'s American Pie and all the original Jimi Hendrix and Stevie Wonder\'s Innervisions and AC/DC\'s Back In Black and Guns N\' Roses\' Appetite For Destruction and Jeff Buckley\'s Grace and Coldplay\'s Parachutes and Michael Buble\'s Crazy Love and…yes, the list literally does go on and on. The man has won multiple Grammys. And while after more than 40 years in the business he\'s still passionate about his day-to-day work at Sterling Sound and adamant in his attitude of truly caring for the musicians who entrust him with their art, let\'s be real: quite a lot of what Marino listens to professionally would never make his playlist at home.\nThe point is that it takes something legitimately special to peg George Marino\'s meters.\n""Just the experience of being able to hear these recordings in the studio with all of the original stuff, the way they were intended to be heard, it\'s…it\'s a joy,"" Marino says from Sterling Sound in New York.\nHe\'s talking of course about the 25 cherry-picked Verve titles that are being reissued as part of another milestone Analogue Productions series. And he\'s talking about them with a certain reverence that covers his personal musical tastes as well as his appreciation for breathtaking sound and historic performances.\n""They\'ve all been done very well,"" Marino says. ""They did a great job with the miking. Everything was so controlled and consistent. Basically it\'s nice to hear that type of stuff to begin with, but then to hear a true performance, not something that\'s fabricated…You\'ve really captured an emotion that you don\'t necessarily get nowadays. People still strive for it, but those were special artists.""\nIndeed they were. Verve assembled a monstrous cast of musicians during its heyday run in the 1950s and ‘60s, including Billie Holiday, Ella Fitzgerald, Duke Ellington, Coleman Hawkins, Stan Getz, Oscar Peterson, Ben Webster, Louis Armstrong, Johnny Hodges and Charlie Byrd. The Who\'s Who roster reads as impressively as a George Marino mastering discography.\n""And Ray Brown on the bass,"" Marino reminds us. ""Anytime you hear a great-sounding, tasty bass from that time, you almost know it\'s him automatically.""\nThis Analogue Productions series boils down all of those transcendent Verve titles to the 25 most essential. But here\'s the beauty: You think you\'ve liked them to date? Marino thinks you haven\'t actually heard them yet.\n""I think in almost every case we have made an improvement in what we\'ve been able to get on the disc relative to what they got in those days,"" Marino says. ""It\'s in some cases just getting what was on the tape onto the disc without having to take anything away from it because of the mechanical limitations they had back then. I think we\'re giving people a lot truer representation of what was intended back then.""\nWhile talking about a different Analogue Productions reissue – Cat Stevens\' Tea for the Tillerman – Marino explains one example of ""back in the day"" limitations that could also apply to the Verve catalog.\n""You didn\'t have the same number of options that you have in the new Neumann electronics,"" Marino says referring to advancements in cutting lathe technology. ""With the new one, they give you more variations to work with. Let\'s say there\'s a nice kind of present sounding acoustic guitar on the left channel and then all the sudden there\'s a drum peak with cymbal crashes and stuff, and that stuff happens to be on the left channel. Being the vocal is down the center, you can drive the high frequency limiter from the right channel (to eliminate vocal sibilance). So you can set a threshold on the right channel and grab the vocal without wiping out some of the musical peaks on the left channel. This is what I talk about when I say that we have technical advantages that they didn\'t have.""\nWith more effective high frequency limiting, Marino can avoid the sharp, jagged grooves that are so difficult for playback cartridges to track. Couple that with all of the room he\'s afforded on a double 12-inch 45-RPM, and it becomes easy to understand why already amazing sounding recordings can now shine to their fullest potential.\nAh, but there\'s more. Sterling Sound has the only Ampex ATR-102 tape machine that has ever been customized to include a ""preview"" head. So Sterling can use the ATR-102 to feed a ""preview"" signal from the tape to the disc-cutting computer, which then in turn prepares the Neumann VMS 80 cutting lathe for the width and spacing of the grooves that will be required to properly cut an analog of that specific signal.\nIt\'s an advantage to use the Ampex ATR-102 machine, Marino explains, because that\'s the machine of choice by the engineering world. In other words, if mastering engineers could use an ATR-102 to cut with, they would. In fact, Marino said that most engineers do use the ATR-102 for CD mastering. But because Ampex never made a preview version, it couldn\'t be used to cut a lacquer. Mike Spitz of ATR Services and Sterling technician Barry Wolifson customized Sterling\'s one-of-a-kind model.\n""The ATR seems to have a little magic going for it. I don\'t know if anyone really ever thought of having a machine customized before,"" he says. ""That\'s a big advantage because it\'s such a great sounding machine. I think that helps a lot in the presentation of what is actually on these tapes (the Verves). I think a lot of them – they weren\'t recorded onto this particular model of machine, but they were recorded onto an Ampex machine. To me, a big part of these things is having the best starting point you can have. We really put a lot of emphasis on keeping the machines working to their utmost correctness. We\'re constantly checking the alignment of the machine against the tapes.""\nMarino credits Sterling technicians Wolifson and Phil Sztenderowicz for the indispensable role of keeping all the gear running optimally.\n""Totally invaluable,"" Marino says. ""That\'s the one thing about Sterling Sound. That\'s our history – technical support is key, it really is. These guys take so much care, and they are on top of everything. And kudos to the studio for investing in a maintenance staff. We never have anything break.""\nThe job of working with such historic original master tapes as on this Verve project crosses the threshold from just ordinary work to a level of honor, and Marino says it does come with some level of stress.\n""You definitely feel like you\'re walking on eggs,"" he says. ""You never can put the tape on the machine, hit the rewind button and just turn around.""\nMarino says the Verve masters are in excellent condition, with just one exception.\n""The one problem we had was an Oscar Peterson one – We Get Requests,"" he says. ""We played through it and there was a lot of weirdness going on, and the tape was really deteriorating. But what happened was they got the original three-track on that, and the three-track was great. So it all worked out in the end. That was the one tape that gave us a headache.""\nSo imagine that. We Get Requests, of course, is a timeless audiophile classic. But the stereo master was shot. Given that we know the three-track hadn\'t been used before, this means that all those great reissues of this record to date? They were made from copies of the master. Now we get to hear the original three-track master, cut at 45 RPM!\n""And it\'s the best sounding of all of them,"" Marino says. ""Excellent sounding record.""\nThe same goes with another undisputed classic in this series, Getz/Gilberto. The original master tapes for that title hadn\'t been used since 1980 previous to this reissue. Also, for this Analogue Productions reissue the decision was made to master and present this album as it was originally mixed to master tape. With only one exception – the Speakers Corner reissue – all versions of this title to date have had the channels incorrectly reversed. And again, those reissues you\'ve heard up until now – definitely still breathy, warm and rich – were made from something less than the master. Prepare to hear the veil removed.\nActually, that goes for all 25 titles in this Verve reissue series, according to Marino.\n""I really think that the people who are willing to lay down their money for these reissues are going to get something really outstanding,"" he says.\nQuality Record Pressings • 543 N. 10th St. • Salina, Kansas 67401', 'To avoid problems with vinyl or any other pressure-sensitive material, carefully read the vinyl manufacturer’s technical data sheet. Product bulletins will usually contain all of the information you need, including application temperature range. The application temperature range for most vinyl films is usually between 50⁰ and 90⁰F (10⁰ to 32⁰ C). For some films, the minimum application temperature range is higher.\nTime, temperature and pressure affect the wet-out properties of the adhesive on a pressure-sensitive film. As the adhesive continues to flow naturally into the microscopic pores, ultimate adhesion increases. Rising temperatures accelerate this process; predictably, colder temperatures retard the process.\nAt room temperature, the adhesive of a pressure-sensitive material, such as VinylEfx®, starts to wet out, after pressure is applied. (Remember: it is called pressure-sensitive for a reason. You typically need pressure for the graphic to stick to the substrate). Room-temperature application serves as the ideal environment because the temperature in a sign shop is typically in the range of 65⁰F to 74⁰F (18⁰ to 23⁰C). The temperature in a garage is a totally different story. And when you have to do an outdoor application, you are at the mercy of Mother Nature.\nVehicle graphics applications in the dead of winter are challenging. Sometimes impossible! Tractors and trailers should be washed the day prior to installation. Ideally, the unit should be pulled indoors the night before to sufficiently heat it. This step is absolutely critical if you’re installing graphics on heavy construction equipment, when temperatures are sub-zero. It can take days for such equipment to reach an adequate installation temperature.\nIn the real world, however, fleets are often left outdoors until the time for graphics application. Snow and ice that have accumulated on the equipment should be removed before the units are pulled indoors. This prevents water dripping on you and your work as the vehicle defrosts. If water has accumulated on the trailer’s roof, lower the vehicle’s landing gear at the front of the trailer. This creates a slope on the roof, speeding water runoff.\nOnce indoors, open all the unit’s doors to release cold air. Directing portable heaters inside will speed the warming process. Portable heaters can make graphic applications possible, even when the installation facility is inadequately heated. Remember that the application temperature refers to the substrate’s temperature, not the ambient air temperature.\nSome vinyl films, but not all, can be installed at temperatures as low as 40°F (4⁰C). By misting the adhesive with isopropyl alcohol, you may gain 10°F (6⁰C) of application temperature. At lower temperatures, the best advice is to go home, throw a log on the fire and wait for a warmer day.\nHot-weather installations can be much more difficult than cold conditions. As the temperature rises, the tack, or stickiness, of the adhesive increases. Sometimes, pressure-sensitive vinyl sticks without pressure, resulting in graphic deformation, when you attempt to reposition the graphic. When this happens, the term used (other than the litany of expletives that the frustrated and angry installer utters) is “pre-adhesion” or “pre-tack”.\nWe can’t change the weather, but we can improve installation conditions. Opening vehicle doors allows heat to escape. Scheduling an installation for an evening or an early morning — or simply waiting for the position of the sun to change — can help you beat the heat.\nFor hot weather applications on smooth surfaces, try misting the surface with a garden hose. This cooling effect can reduce the surface temperature and the adhesive tack to allow for repositioning. Application fluid is also an option. While both techniques work usually work well on smooth, flat surfaces, they’re not recommended for rivet applications and not recommended when applying metalized films, such as reflectives or VinylEfx®.']"	['<urn:uuid:3112b393-1600-45dd-b0e5-d5d9e6f6b8b0>', '<urn:uuid:33d72ea5-d5b6-4398-8b43-99dc3a3b48c2>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T00:43:36.053682	8	96	3387
80	What roles do scientific measurements and physical evaluation play in treatment?	In scientific contexts, measurements are crucial for analyzing structures like quasicrystals, where Nuclear Gamma-ray Resonance spectra show golden ratio proportions (1.618:1) in atomic spacing, confirmed through statistical analysis with minimized standard deviations. In physical therapy, evaluations are essential for diagnosing issues - therapists perform detailed assessments to identify physical limitations, understand how they relate to pain or limited independence, and determine appropriate treatments. These evaluations can be done without physician referral in all 50 U.S. states.	"['Phi in Chemistry and Physics\nHydrogen Bond Distances\nIt has been found recently (in 2005, actually), that the golden ratio plays a significant role in atomic physics in that it governs what is known as the Bohr radius (the radius of atoms and ions that enables quantitative discussion of bond lengths between atoms and partial ionic character (D. Yu et al.). Here we will discuss it in a phenomenon known as hydrogen bonding, in which the partially positive hydrogen atom is attracted to an electronegative atom (one that strongly pulls electrons toward it).\nIn a triatomic O-H---O hydrogen bond, D. Yu et al. point out that the total bond strength (the sum of the strengths of the individual bonds, which can be measured thermodynamically or in other ways), is 1 (211). (Just to clarify, the two bonds in question are O-H and H--O.) Now notice that each of these bonds consist of a larger anionic (negatively-charged) radius (from O) and a smaller cationic radius (from H). So we see that the total distance D(H+) is equal to the sum of the smaller distances in both the strong bond (O-H) and weak bond (H---O):\nThe hydrogen radius changed when undergoing bonding, since the atom has become polarized in forming a hydrogen bond. Therefore, we can consider the following image from D. Yu et al. 212 (click to enlarge):\nNote that the ratio of bond distances is 0.618 (1/phi) for both the strong and weak bonds. The validity of these results are further discussed in D. Yu et al 213-214.\nSome years ago it was shown that rapid solidification of an aluminum-manganese alloy can produce a solid that diffracts electrons like a crystal but has no lattice-based translational symmetry (as crystals do). ""A quasiperiodic structure is generated using a pair of polyhedra which are the three-dimensional analogues of the two-dimensional Penrose tiles, which tile the plane aperiodically (for more information on Penrose tiles, see http://mathworld.wolfram.com/PenroseTiles.html). These polyhedra will produce a diffraction pattern when bombarded with gamma rays (this is a method of analyzing crystal structures) that suggests that the two polyhedra occur in the ratio of 1.618:1. The NGR (Nuclear Gamma-ray Resonance) spectrum of this crystal structure appears as follows (click to enlarge):\nThe above spectrum displays two quadripole doublet peaks (this is fancy crystallographer-talk). The least-squares regression fit for these curves saw minimized standard deviations (which is statistically a good thing--the lower the standard deviation, the closer the data is to the mean) when the intensity ratio was constrained to be 1.618, the golden ratio. This suggests that phi is involved in determining the spacing between the atoms in the quasicrystalline lattice structure (Swartzendruber et al. 1384-1385).\nIt is possible to show that one-dimensional quasicrystals are self-similar with a self-similarity factor of phi (Grushina et al. 408). This, then, implies that optical devices constructed in the same manner as one-dimensional quasicrystals must also exhibit phi-based self-similarity. We see this in their diffraction patterns:\nUpon inspection, we notice that Fig. 1a (the Fibonacci grating) generates a fractal-like addition pattern (that is self-similar by definition) rather than anything periodic. Also, the ratios of the lengths of the arrows between peaks is the golden ratio. Also, the self-similar characteristics of these transmission spectra exhibit some amount of stability, as when 5% of the layers of the diffraction grating (through which light is passed) are replaced by layers with a different refractive index (which regulates the amount light bends in a certain material), the locations of the major energy gaps remains the same (see below, specifically Fig. 2b and Fig. 2c):\n""One of the main conclusions to be drawn from our calculations is that the self-similarity in the structures of diffraction gratings and multilayer Fibonacci systems related to the golden ratio is clearly seen in their optical characteristics. Also, the definite consistency in the behavior of optical parameters featuring a unique self-similarity factor is pointed out"" (Grushina et al. 411). The above discussion serves to illuminate just another interesting scientific property of the golden ratio.', 'What is Physical Therapy?\nDr. Adam M Cramer, PT, DPT\nHow can physical therapy improve your quality of life? Physical therapy is the field of medicine that is devoted to restoring and improving human movement. Physical therapists achieve correct posture and spinal alignment by reducing pain and using physical methods such as strength training, massage and using the body’s natural healing processes. Physical therapists are experts in movement, kinesiology (study of human movement) and functional independence. Physical therapists help people of all ages and skills to improve strength, restore mobility and reduce pain. Restoring mobility and pain-free movement is crucial to an individuals ability to participate and perform in recreational and work duties, as well as improving their quality of life and overall health. This leads to reduced risk of falls and reduced risk of stroke and heart attack1.\nIn many cases, physical therapy has been shown to be just as effective as surgery when treating a wide variety of conditions from low back pain, shoulder issues and knee arthritis2,3. How wonderful to be able to participate in several weeks of non-invasive therapy to get the same outcomes as an invasive, costly and risky surgery!\nUnfortunately, there are misconceptions about physical therapy that may discourage people from seeking the skilled guidance of a licensed therapist. Some misconceptions include: insurance coverage, physician involvement, preconceptions about pain, not understanding what physical therapists are capable of treating, clinical indications and therapists’ education.\nIn all 50 states of the U.S., a physical therapist can perform a physical therapy evaluation without a physician referral. By participating in a physical therapist evaluation, you will learn what physical limitations you have and how they relate to your pain or limited independence and how to correct them.\nPhysical therapy is typically a covered service by most insurance policies. For people that have Medicare, services are covered at 80% and the other 20% is covered by a supplemental policy if you have one. Other insurance policies may even cover physical therapy costs at 100% making it a very affordable alternative to costly surgeries3. Insurance companies recognize the significant value physical therapy holds in the medical field.\nPhysical therapy is not only for injuries or accidents. The skilled physical therapist is able to evaluate and diagnose potential issues before they lead to more serious injuries or falls. A physical therapist can improve the joint function in one joint to impact the functional performance of another. For example, if you are having pain in your back related to vertebral dysfunction, the therapist can improve the joint mechanics and muscle action of your low back, and further improve the joint mechanics of your hips and knees, reducing potential complications down the road. All treatments are specific to the individual needs and vary person to person with the ultimate goal of eliminating pain and restoring natural movement.\nPhysical therapist are capable of and license to treat numerous physical disabilities Including orthopedic conditions such as: pre- and post- joint surgery, chronic pain, knee injury including ACL and meniscus tear, joint sprain, arthritis, back pain related to disc herniation, vertebral fractures, spinal stenosis, osteoporosis, frozen shoulder, rotator cuff tear, hand injuries, bursitis, muscle strain, plantar fasciitis/foot pain and Neurologic conditions such as: stroke recovery, peripheral neuropathy, balance impairment, progressive neurologic conditions including Parkinson’s disease, multiple sclerosis and amyotrophic lateral sclerosis, traumatic brain injury, spinal cord injury, headaches, vertigo all utilizing our bodies own natural healing abilities. By incorporating state of the art treatment techniques including laser therapy and trigger point dry needling, manual hands on skills, instrument assisted soft tissue mobilization and private education/training so clients understand exactly what is occurring and how they can alleviate symptoms by themselves.\nA licensed physical therapist or physical therapist assistant can only deliver and provide physical therapy services. Physical therapists are highly educated Doctors of Physical Therapy that possess a thorough understanding of the human body’s anatomy, physiology, biomechanics and health conditions. All therapists participate in yearly continuing education to maintain their state license to practice. Many therapists hold board certifications for specific areas of medicine including: neurology, orthopedics, geriatrics, pediatrics, cardiovascular-pulmonary, sports enhancement, research and women’s health.\nPhysical therapy is not something that can be done on your own or by any other health/medical practitioner. You need the critical and trained eye of a licensed doctor of physical therapy in order to assess and determine appropriate diagnosis, limitations and treatments . So, if you have never sought the guidance and direction from a therapist, what is keeping you away? Call a licensed Doctor of Physical Therapy in order to become pain-free FAST!\nDr. Adam M Cramer, PT, DPT is a licensed physical therapist and the owner of MyoFit Clinic in Middlefield, Ohio.\n- Effects of a Fall Prevention Exercise Program on Muscle Strength and Balance of the Old-old Elderly. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4242952/. Nov. 2014\n- http://www.nbcnews.com/health/health-news/common-knee-surgery-may-not-help-you-study-finds-n376556. June 16, 2015.\n- Physical Therapy or Advanced Imaging as First Management Strategy Following a New Consultation for Low Back Pain in Primary Care: Associations with Future Health Care Utilization and Charges.http://www.ncbi.nlm.nih.gov/m/pubmed/25772625. June 2015.']"	['<urn:uuid:cd645104-8935-4a22-bbec-46ef48b5a804>', '<urn:uuid:2490ca52-69db-4ea9-8255-0f803804897e>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	11	76	1511
81	salt hotel lodging amenities availability internet modern facilities	Several salt hotels have been built in the Bolivia Salt Flats over recent decades to accommodate increasing tourism. Despite their unique construction, these hotels offer modern amenities including hot water, spacious rooms, and even Internet service, providing a comfortable stay while experiencing this unique accommodation.	['Bolivia Salt Flats Q&A\nThe Bolivia Salt Flats make up the largest and highest salt desert in the world. Situated in the Potosi region in southwest Bolivia (South America), the Bolivia Salt Flats, also known as the Uyuni Salt Flats, attract many visitors for their remarkable topography and otherworldly charm. Here are some answers to the most frequent travel questions.\nWHY SHOULD I VISIT THE BOLIVIA SALT FLATS?\nA Paradise for Photography:\nAt nearly 12,000 square kilometers in size, and an extraordinary flatness, the Bolivia Salt Flats provide an endless horizon, which allows photographers to experiment with perspective and proportion. The results yield very creative and fun pictures that will surely go up on your walls.\nThe endless horizon is perfect for playing with perspective & proportion\nIn addition, during the rainy season, the salt flats are covered with a thin layer of water, which creates an amazing mirror effect for truly breathtaking pictures. The effect is one of floating among the clouds!\nAt over 3,650 meters above seal level (nearly 12,000 ft.), the Bolivia salt flats can prove to be a challenging environment. The journey through the flats requires experienced drivers who also play the role of mechanics, as it is not uncommon for vehicles to break down. Only rugged, mechanical 4×4 vehicles are allowed in the salt desert because of the inhospitable environment. Also, it is very easy to get lost in such an expansive landscape, so it is important to hire good quality guides from a reputable tour company who know their way around. Finally, you can satisfy your inner explorer and visit beautiful caves, lakes and islands in the surroundings.\nSpend the night in a Salt Hotel:\nBelieve it or not, the Bolivia Salt Flats are home to several salt hotels that have sprung in the last few decades to accommodate the growing number of tourists. And they are not as rugged as people might think. Many offer great amenities, such as hot water, spacious rooms and even Internet service. The experience of sleeping in a salt hotel is definitely worth putting on your bucket list.\nWHEN SHOULD I VISIT?\nThe region has two marked seasons: wet and dry. The rainy or wet season usually begins at the end of November and lasts through March. The dry season is typically from April to November. Both seasons offer unique experiences of the Uyuni Salt Flats, so you should plan your trip based on what you want to do.\nDuring the rainy season, the Bolivia salt flats are covered in water. As such, travelers can capture amazing pictures of the Salar’s mirror effect but access to certain areas is limited. In addition, excessive rain may cause some tours to cancel. On the other hand, during the dry season, temperatures drop and the salt ground hardens. As such, visitors can explore the iconic Incahuasi Island, and other areas, usually inaccessible during the wet season.\nHOW DO I GET THERE?\nBecause the Uyuni Salt Flats are located in southwestern Bolivia, travelers can cross the border in Chile and Argentina. They can enter Bolivia through the San Pedro de Atacama desert in Chile or through the town of Quiaca in Argentina. From here, it will be two to three days before you reach the flats, however, the voyage is amazing as you’ll explore colorful high-altitude lakes, geysers, rock valleys and more. The scenery of the Bolivian plateaus are spectacular and a final visit to the Salar will be icing on the cake.\nIf you are looking for a quicker trip to the Bolivia salt flats, simply book a flight to either La Paz or Santa Cruz, Bolivia from where you can take a short flight to the town of Uyuni where the flats are located. In addition, other forms of transportation to Uyuni are available within Bolivia, such as buses (from La Paz) and trains (from Oruro). However, these can take long hours, which leaves less time to visit the country.\nTRAVEL RECOMMENDATIONS FOR THE HIGHLANDS: (La Paz, Tiwanaku, Copacabana, Lake Titicaca, Uyuni)\n- If you are flying to La Paz, we recommend you rest for at least 3 hours; do not eat much, as digesting food might be a bit slow during your first day due to the altitude. Also, you can drink coca tea (except those who suffer from high blood pressure and heart problems).\n- Wear comfortable footwear and clothes (dress in layers).\n- Drink water, wear sunglasses, sunscreen and sun cap.\n- Bring moisturizing cream and a small backpack for excursions.\n- Do not forget a camera or videocamera to capture the amazing places you’ll visit.\n- We recommend bringing abundant contact lens solution or an additional pair of glasses.']	['<urn:uuid:a4afe139-8c46-4591-a303-0f503ee9bad2>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	8	45	783
82	dental implant material performance healing time	Modified titanium surfaces show improved early bone cell response and osteogenesis. Specifically, Ultra Fine Grain (UFG) titanium demonstrates higher bone cell colonization compared to traditional Commercially Pure (CP) titanium, leading to faster healing times. While CP titanium typically requires 2-4 months before restoration, UFG titanium implants can be restored after just 1-1.5 months. This faster healing is due to bone cells attaching to boundaries between titanium grains, with UFG titanium having more boundaries due to its nanometer-sized grains.	['Early osteogenic response of osteoprogenitor cells on modified titanium implant surfaces leads to improved osteogenesis\nChakravorty, Nishant, Prasadam, Indira, Ivanovski, Saso, & Xiao, Yin (2011) Early osteogenic response of osteoprogenitor cells on modified titanium implant surfaces leads to improved osteogenesis. In Tissue Engineering and Regenerative Medicine International Society 2011 Asia-Pacific Meeting, 3 - 5 August 2011, Grand Copthorne Waterfront Hotel, Singapore. (Unpublished)\nThis is the latest version of this eprint.\nBackground: Implant surface micro-roughness and hydrophilicity are known to improve the osteogenic differentiation potential of osteoprogenitor cells. This study was aimed to determine whether topographically and chemically modified titanium implant surfaces stimulate an initial osteogenic response in osteoprogenitor cells, which leads to their improved osteogenesis. ----- ----- Methods: Statistical analysis of microarray gene expression profiling data available from studies (at 72 hours) on sand-blasted, large grit acid etched (SLA) titanium surfaces was performed. Subsequently, human osteoprogenitor cells were cultured on SLActive (hydrophilic SLA), SLA and polished titanium surfaces for 24 hours, 3 days and 7 days. The expression of BMP2, BMP6, BMP2K, SP1, ACVR1, FZD6, WNT5A, PDLIM7, ITGB1, ITGA2, OCN, OPN, ALP and RUNX2 were studied using qPCR. ----- ----- Results: Several functional clusters related to osteogenesis were highlighted when genes showing statistically significant differences (from microarray data at 72 hours) in expression on SLA surface (compared with control surface) were analysed using DAVID (online tool). This indicates that differentiation begins very early in response to modified titanium surfaces. At 24 hours, ACVR1 (BMP pathway), FZD6 (Wnt pathway) and SP1 (TGF-β pathway) were significantly up-regulated in cultures on the SLActive surface compared to the other surfaces. WNT5A and ITGB1 also showed higher expression on the modified surfaces. Gene expression patterns on Day 3 and Day 7 did not reveal any significant differences.----- ----- Conclusion: These results suggest that the initial molecular response of osteoprogenitor cells to modified titanium surfaces may be responsible for an improved osteogenic response via the BMP and Wnt signalling pathways.\nImpact and interest:\nCitation countsare sourced monthly fromand citation databases.\nThese databases contain citations from different subsets of available publications and different time periods and thus the citation count from each is usually different. Some works are not in either database and no count is displayed. Scopus includes citations from articles published in 1996 onwards, and Web of Science® generally from 1980 onwards.\nCitations counts from theindexing service can be viewed at the linked Google Scholar™ search.\nFull-text downloadsdisplays the total number of times this work’s files (e.g., a PDF) have been downloaded from QUT ePrints as well as the number of downloads in the previous 365 days. The count includes downloads for all files if a work has more than one.\n|Item Type:||Conference Item (Presentation)|\n|Keywords:||osteogenic differentiation, titanium implant surfaces, osteoprogenitor cells|\n|Subjects:||Australian and New Zealand Standard Research Classification > MEDICAL AND HEALTH SCIENCES (110000) > MEDICAL BIOCHEMISTRY AND METABOLOMICS (110100)|\n|Divisions:||Past > QUT Faculties & Divisions > Faculty of Built Environment and Engineering|\nCurrent > Institutes > Institute of Health and Biomedical Innovation\n|Copyright Owner:||Copyright 2011 The Authors|\n|Deposited On:||15 Jun 2011 07:34|\n|Last Modified:||15 Jun 2011 07:34|\nAvailable Versions of this Item\n- EARLY OSTEOGENIC RESPONSE OF OSTEOPROGENITOR CELLS ON MODIFIED TITANIUM IMPLANT SURFACES LEADS TO IMPROVED OSTEOGENESIS. (deposited UNSPECIFIED)\n- Early osteogenic response of osteoprogenitor cells on modified titanium implant surfaces leads to improved osteogenesis. (deposited 15 Jun 2011 07:34)[Currently Displayed]\nRepository Staff Only: item control page', 'November 6, 2013\nby Americo Fernandes, DMD\nUltra Fine Grain (UFG) titanium is a new improved form of the old classic Commercially Pure (CP) implantable titanium we as dentists are familiar with. It is produced through a process known as Severe Plastic Deformation. This novel patented process, positively impacts the structural strength of titanium by reducing the grain dimensions to nanometer size. As a result of this new process, physical failure of UFG titanium dental implants are expected to decline when compared to CP titanium dental implants used today. Bone cell colonization studies reveal another benefit when compared to CP titanium, higher bone cell numbers per unit area. This improvement in cell colonization can positively impact the healing process. This article describes two years of clinical observations of Ultra Fine Grain titanium, utilized for the very first time as a dental implant material in North America.\nUFG titanium was developed through a joint research effort of American and Russian scientists at Los Alamos National Laboratories and UFA Aeronautical University in Russia.1 By processing traditional Commercially Pure titanium (CP titanium) through a novel patented process known as Severe Plastic Deformation, a new UFG titanium is created.2 The effects of this process on CP titanium result in a new UFG titanium, comprising of desirable nanometer sized grains of titanium. The UFG titanium is chemically the same as CP titanium but is physically different due to the much smaller grain size. This new titanium is inherently much stronger when compared to its traditional predecessor, CP titanium because of the smaller grain size. UFG titanium’s superior properties come from the remarkable gain in strength without increasing undesirable brittleness.3 Subsequent bone cell studies have also shown that bone cells show greater numbers of colonies on the UFG titanium surface when compared to traditional CP titanium, resulting in an attachment increase between bone and implant surface.4 Bone cells attach to boundaries between titanium grains. Since UFG titanium has more boundaries between the smaller grains compared to CP titanium, an increase in bone cell colonization is expected on the UFG titanium surface. This unique characteristic appears to allow the loading of UFG dental implants into function earlier because of the greater number of bone cells. Because of UFG titanium’s superior strength, failures as a result of breakage should also decrease.5 The increase in strength is especially important when using smaller implants such as the very popular one piece minis. Some small diameter dental implants are made from titanium alloy metal in order to overcome implant breakage issues. Titanium alloy dental implants use Aluminum and Vanadium to gain an increase in physical strength. Some clinicians, if given the option, would prefer to not use implantable metals made with Aluminum and Vanadium for variety of reasons not discussed in this paper. UFG titanium does not incorporate Aluminum or Vanadium which is a definite benefit for these clinicians.\nDISCUSSION & RESULTS\nDuring the last two years, I have placed 65 UFG dental implants in a variety of locations in the mouth. I used these new implants to restore single to multiple missing teeth with traditional fixed prosthesis. During this two year period, UFG titanium dental implants and CP titanium dental implants were compared according to their unique clinical performance. I have been using traditional CP titanium dental implants for approximately 20 years and during this time, I have learned a lot about their familiar clinical performance characteristics. From my personal experiences with CP titanium dental implants, I was well prepared to sort out clinical differences between these two types of dental implants. These new UFG dental implants have been manufactured and prepared to the same exact dimensions and designs of CP titanium dental implants that I have been using for the last 20 years. Both UFG and CP titanium dental implants were manufactured by BASIC Dental Implant Systems, Inc., Albuquerque, New Mexico. Based on my clinical experience, dental implants are ready to restore when they meet the following criteria; 1) Lack of infection around the implant 2) The ability of bone to resist clockwise torque force of the implant to 40 Ncm. 3) Intimate bone to implant body surface contact as seen through traditional Periapical Radiographic examination. Depending on the patient and type of bone, an implant can be ready to restore using CP titanium within two to four months. This protocol has been a standard practice in my clinic for many years. Research has shown that a greater number of bone cells tend to colonize UFG titanium surfaces when compared to CP titanium surfaces. Because of these earlier findings, I wanted to know whether it is possible to restore UFG dental implants earlier, compared to the traditional CP dental implants I have been using for many years. An evaluation period of one to two months was chosen to assess UFG titanium implants for restoration loading instead of the typical period of two to four months for classical CP titanium implants.6 All of the patients treated with UFG titanium dental implants returned at 1 to 1.5 months from the time of surgical placement. At recall, it was quite evident that these UFG titanium dental implants are able to meet restoration criteria significantly earlier when compared to traditional CP titanium dental implants. Periapical radiographs of these novel implants generally show a greater degree of bone maturity at the implant surface when comparing to CP dental implants. Radiographic examinations of UFG dental implants typically show greater bone density at the implant body surface, which generally means more bone on the implant’s surface. The increase in bone cell colonization on UFG dental implants was expected because of the earlier research findings and my findings confirm this earlier research. UFG titanium dental implants have proven that they can indeed be loaded earlier compared to CP titanium dental implants. In my experience, torquing CP titanium dental implants to 40 Ncm, at 1 to 1.5 month periods usually results in movement of some implants, requiring additional time for adequate healing, necessary to meet criteria for restoration. Without exception, all 65 UFG dental implants were more resistant to torque forces at recall appointments much earlier compared to CP titanium dental implants. Breakage of dental implants has never been an issue to worry about when utilizing CP dental implants; therefore I have found no difference here. I have encountered very few broken dental implants in my practice during the past 20 years and I cannot report on the clinical benefits to me of UFG titanium dental implants as result of its increased strength properties. My experience utilizing so-called minimal diameter dental implants is only beginning and as such, the strength of these dental implants will most likely play an important role as I increase the use of these implants clinically. My preference is to avoid implanting dental titanium, which incorporates Aluminum and Vanadium to add strength at the expense of biological issues. A nice benefit of UFG titanium is that it significantly surpasses the strength properties of both CP and Alloyed titanium without Aluminum and Vanadium.7 To date, all UFG dental implants are in function and are recalled at six to 12 month intervals. UFG titanium dental implants have also been provided to other dentists in the USA, Taiwan, China and Russia. These clinicians report clinical observations similar to those described in this paper.\nUFG titanium dental implants offer two significant benefits when compared to traditional\nCP and Alloy titanium based dental implants. These two benefits are greater strength, without the Aluminum and Vanadium, and improved healing capability. From a clinician’s perspective, these benefits hold the promise of treating our patients faster with functioning dental implants that are less likely to break under function. This is very desirable to all dentists and patients. From my clinical observations to date, it can be said that traditional CP titanium and Alloy based dental implants can be replaced as the material of choice by UFG titanium. OH\nDr. Americo Fernandes graduated from the University of Manitoba in 1984.\nHe obtains a Private Practice with emphasis on Implant Dentistry, Winnipeg. Placing and restoring dental implants for over 25 years. President, BASIC Dental Implant Systems, Inc.\n1,2,3. Microstructures and Properties of Ultrafine-Grained Pure Titanium. Vladimir V. Stolyarov1; Yuntian T. Zhu2; Terry C. Lowe3; Ruslan Z. Valiev, Journal of Nanoscience and Nanotechnology, June 1, 2001\n4. Accelerated growth of preoblastic cells on ultrafine grain titanium, Y. Estrin, C. Kasper,S. Dietrichs and R. Lapovok. Journal of Biomedical Materials Research, 2008.\n5. Comparison of Mechanical properties of medical grade titanium. Terry Lowe, Accepted for Publication, fall 2012.\n6. Increased osteoblast adhesion on nanophase metals: Ti, Ti6AL4, and CoCrMo. Thomas J. Webster, Jeremiah U. Ejiofor, Biomaterials 25, 2004, 4731-4739.\n7. Progress in material science. M. geetha, A.K. Singh, R. Asokamani, A.K. Gogia. Elsevier Volume 54, Issue 3, May 2009, Pages 397-425']	['<urn:uuid:ff62432b-fc1e-439a-b03e-08ae58aad35a>', '<urn:uuid:b1272181-1b8f-4d00-b743-714f28c2ea72>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	6	78	2029
83	scientific temperature scale preferred astronomy applications advantages compared celsius	In astronomy, scientists use the Kelvin scale (K) rather than Celsius or Fahrenheit. The Kelvin scale starts at absolute zero (0K, which is -273.15°C), making all temperature measurements positive numbers. This shows how much warmer things are than the coldest possible temperature. The scale maintains the same degree intervals as Celsius - for example, a 30°C day equals 303.15K (calculated as 273.15 + 30). Water freezes at 273.15K and boils at 373.15K.	"['|The Solar System|\nWikijunior books welcomes you to the children\'s book ""Solar System"". Outer space is perhaps the final frontier for humanity. Even though the rest of the solar system objects may seem like tiny dots from Earth, our celestial neighbors are still important to learn about. If, when you grow up, you are going to be an astronaut and travel in space, you will need to know quite a bit about the solar system. And even if you don\'t travel to space, the things other people do there will affect you, so you need to know about it. Also, if you meet an astronomer or an astronaut, you do not want to sound ignorant! The importance of learning about the solar system has led many experts here at Wikijunior to donate their time and talents to bring this volume together.\nWikibooks is a project of the Wikimedia Foundation, aimed at providing free, easily available, quality reading for adults and children to promote the global spread of knowledge. Traditional publishing houses make the bulk of their income from re-issues of classic books, new books by authors with long track records, or celebrities who are famous in their own right. The chances of a truly good new work being published solely on the basis of merit skyrocket when the traditional business model is overturned and the wellspring of new talent out there is tapped using the Internet.\nWith this project we have reached a crossroads between the books of yesterday and the encyclopedia of everything for tomorrow. Simply by reading this book and telling others about it, you have advanced the cause of free access to information and of democratizing the field of publishing. Thank you, and once again, welcome.\nStudying the Solar System\nScientists are still exploring the universe. Whether things are very tiny, like the cells of plants and animals, or very big like a solar system or a galaxy, there is still a lot scientists don’t know.\nScientists who study space are called Astronomers or Astrophysicists. They explore the solar system in two different ways. Astronomers do it by observing celestial bodies through telescopes, while astrophysicists (a specialized class of astronomers) try to explain the observed phenomena using physics, as suggested by the name, and theorize about what is still unseen or unknown.\nTelescopes were invented in the early 1600s in Europe and allowed curious scientists like Galileo Galilei to look at very distant things in close-up and see details of the solar system and the universe that nobody had ever seen before. Using his telescope, Galileo was the first person to see the rings around Saturn, and draw a very detailed picture of the moon. He also saw the four largest moons of Jupiter, sometimes called the Galilean moons, and saw spots on the Sun. Telescopes on Earth and in space are still used to explore the Solar System. There are several types of telescopes. The most common ones are optical telescopes, such as Galileo\'s (optical means relating to light, which what these telescopes see), and radio telescopes, that pick up radio waves from outer space (radio waves occur naturally; they don\'t have to be made by humans).\nUntil the 1950s, humans were limited to exploring the Solar System from the ground. However, in 1957 the Soviet Union (now Russia and several other countries) launched the very first satellite, Sputnik 1 (pronounced like spoo-tneek). Since then, humans have been launching vehicles into space to explore the Solar System—some manned (with people) and some unmanned (without people).\nNow, the Solar System is full of human-made probes, exploring the planets and moons of the solar system. The probes send back information to Earth that scientists study to figure out what it means. Every year, scientists are learning more about the Solar System. Sometimes they learn things about other worlds that remind us of Earth. Other times, the things they learn are very strange. Everything they learn helps us to understand more about Earth, Earth\'s history and Earth\'s neighborhood.\nHow is the Solar System measured?\nIt’s important that scientists use measurement to tell how big, how hot or cold, or how far away something is. In science, people use the metric system, which is named after its basic unit the metre. Below is a description of all the types of measurement used in this book.\nDistance or length/width\nFor indication of measurements, how distant something is, or how long or wide it is, scientists use kilometres or metres. Units of the metric system (a kilometre is 1000 metres, a metre is little more than 3 feet in the old imperial unit system still in use in some regions). Kilometres is often shortened to km and metres is often shortened to m. Kilometres and metres can also be spelled as Kilometers and Meters, but the International Bureau of Weights and Measurements uses the -re versions as the official spelling.\nSince distances outside of the Earth get so vast, scientists have also invented new units of measurement to make it easier to measure large distances in space. They invented the Astronomical unit (㍳) which is equivalent to 149 597 871 kilometres. One Astronomical Unit is the approximate distance between the Earth and the Sun. The mean distance between the sun and Neptune (the farthest planet from the sun) is 30.1 ㍳, or 4.503 billion kilometres. This is why it\'s good to use ㍳ for a big distance like that: 30 distances from the Earth to the sun is easier to understand than four and a half billion kilometres. You might not realize there was something wrong if someone told you the distance from the sun to Neptune was 45.03 million kilometres, but if you thought of it as 0.301 ㍳, you would know it couldn\'t be right.\nIn astronomy, they have leveling-up scales similar to the metric (10 mm in 1 cm, 100 cm in 1 m) and customary (12 in. in 1 ft., 3 ft. in 1 yd.) scales. Usually these distances are not used within the solar system, but they are important to know if you want to be an astronomer or astrophysicist.\n- 1 Light-year (ly)=63241.077 ㍳\n- 1 Parsec (pc)=3.26 ly\n- 1 Kiloparsec (kpc)=1000 pc\n- 1 Megaparsec (mpc)=1000 kpc\n- 1 Gigaparsec (gpc)=1000 mpc\nTo help you visualise just how big some of these are,\n- 4.22 ly=The distance from Earth to the nearest star (Proxima Centrari) other than the sun\n- 1.3 pc=The distance from Earth to Proxima Centrari\n- 34 kpc=The length of the Milky Way\n- 0.76 mpc=The distance from Earth to the nearest Galaxy, the Andromeda Galaxy\n- 14 gpc=The radius of the observable universe\nIn order to measure how big something is, scientists measure the mass of an object in kilograms or grams. There are 1000 grams in a kilogram. Scientists do not use weight, because weight is a measurement that means how hard gravity is pulling on an object. An object’s mass is the same wherever you are in the solar system because it measures how much stuff, or matter, a thing is made up of. Your weight will change because the amount of gravity there is varies from place to place.\nOn Earth, mass and weight are the same. If you weigh 30kg (short for kilograms) on Earth, your mass is 30kg. If you are floating around in space, your weight if you try to stand on a set of scales will be 0kg, but your mass is still 30kg. You are still made up of the same amount of matter.\nTemperature is a numeric reference, in a scale of degrees, on how hot or cold something is in relation to a ""constant"" reference. There are several scales. In our everyday lives we measure temperature in degrees Celsius, written °C for short (the little circle ° means ""degrees""); or degrees Fahrenheit, written °F for short. But scientists, especially astronomers, use degrees Kelvin to measure temperature, written K for short (with no °). Don\'t use Celsius or Fahrenheit for temperatures in astronomy!\nSome important temperatures to know in Kelvin:\n- 0K (−273.15°C) is maximum coldness, also called absolute zero. This is a great thing about measuring temperature in Kelvin, that the number is always positive, and tells you how much warmer things are than the coldest they could possibly be.\n- The freezing point of water is 273.15K (0°C), and the boiling point of water is 373.15K (100°C).\n- A sunny day of 30°C, 86°F, would be 303.15K. This is 273.15 + 30, because degrees change equally in Kelvin and Celsius.\nIntroduction for Parents, Guardians, and Educators\nThe Solar System is a Wikijunior book written by a group of volunteers and made freely available to Internet users, printers, and distributors under the terms of its license. It is the result of cooperation between The Beck Foundation, The Wikimedia Foundation, and volunteer writers and editors.\nThe volunteer writers and contributors thank you for obtaining this book. By making it available to a young person you complete the goal of the Wikijunior project to encourage reading and literacy among young people.\nThe original text and graphics are available at http://www.wikibooks.org and printed versions may be available from many different entities under license.\nAgain, thank you, and enjoy.']"	['<urn:uuid:20dacd65-6c92-4d8b-9265-27f23dbbe91f>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T00:43:36.053682	9	72	1545
84	living near highways need help protecting trees from winter salt how to add shade also	To protect trees from salt spray near highways, you can install burlap or canvas screens. Additionally, consider planting salt-tolerant species like blue spruce, Austrian pine and honeylocust. For shade protection, you can construct sun shields on the south and west sides of plants, or use buildings, fences or other plants to provide natural shade during winter.	"['Explore our article library of tips and how-to guides.\nLooking for your next DIY project? Perhaps you need some inspiration on how to make your house a home? Our growing library of how-to guids, DIY tutorials, and home improvement articles are here to guide you through your DIY adventures.\nProtect Perennials and Prepare Your Garden for Winter\nProtect Perennials and Prepare Your Garden for Winter\nPerennials are trusty plants that keep coming back each spring to give you beautiful blooms or foliage all summer long, so it’s only right to give them a little TLC before winter arrives. Rough River Hardware is here to help before you wait until the weather turns cooler and your plants have stopped growing before starting your garden cleanup.\nAlways check the local weather conditions and be aware of your growing zone to choose the best time to start preparing your garden for winter. Keep in mind that growing zones are a guideline only. Open areas that face north and away from the shelter of your house or bushes, for example, will be cooler than the more protected south-facing sections of your garden.\nMany gardening experts recommend leaving the foliage on your healthy plants over the winter as they provide not only interest in your winter garden but seeds and shelter for birds, bees, butterflies and animals.\nLook at your perennials closely to detect damage caused by pests or diseases. It’s advisable to cut plants down to the ground if they appear damaged or diseased to prevent them from carrying the pest on to next spring. You can cut down hostas and perennials in the fall, but it’s best to wait until spring to cut back ornamental grasses, coneflowers, Russian sage, lavender, and roses.\nCompost all your plant cuttings to generate new nutrients for your garden next year.\nKeep Your Plants’ Feet Warm\nSome perennials need a little help to keep their feet warm in the cold winter months. You can pile up soil around roses, chrysanthemums, and other tender plants. When the last snow has melted away and the ground starts to warm up in the spring, break down the mounds and spread the soil evenly in your garden.\nWeed Before Winter\nThis is your last chance to get those weeds out! It’s best to tackle the weeds now to prevent any of the hardier ones from multiplying quickly next spring.\nWater Before the Ground Freezes\nIt’s been a dry summer in many regions so watering your perennials before winter is more important than ever. Use a sprinkler or soaker hose to give your plants a deep soaking before the ground starts to freeze. Don’t forget to turn off the water to your outdoor taps and empty your rain barrels before the winter freeze starts.\nRemove and Compost Annuals\nRemove all annuals including vegetables from your garden, as in most cases they will not survive the winter cold.\nDig up Your Tender Bulbs and Tubers\nBring your cannas, dahlias and gladioli inside for the winter. After the first frost and the flowers are done, cut off the stems and foliage, dig them up and brush off the soil. Let them dry and then store them in a cool, dry place such as your basement or garage.\nPlant Your Spring Bulbs\nWhile you’re out there, don’t forget to plant some tulip, narcissus and daffodil bulbs for a colorful spring display!\nLuckily, robust water plants can spend the winter at the bottom of the pond, but if you have any tropical water lilies, you will need to bring them inside, preferably in a cold, dark basement. Keep them moist throughout the winter so they’ll be healthy when you open up your pond in the spring.\nTrees and Shrubs\nYour trees and shrubs need thorough watering before winter, especially if your region had little rain during the summer and fall.\nTo keep your trees healthy, prune damaged branches and add some mulch around the trunk to help keep moisture in over the winter months. You can prune spring-flowering shrubs in the fall but wait until spring to prune shrubs and trees that are still in bloom.\nCover newly planted evergreens with burlap to protect them from wind and salt.\nFeed your Perennials With Compost\nIf you have any compost left, feed your gardens and perennials to give them a head start for next spring.\nHow to Mulch Perennials\nThere are many benefits to mulching all year round and before the winter snow.\nIf your region’s weather tends to go from extreme cold to sudden thaws and freezing again, then mulch can protect roots from these damaging fluctuations in temperature.\nAre any of your perennials considered tender or half-hardy? Take note of your zone, winter temperatures and the hardiness of each plant to determine how much protection they will need. Tender perennials and young or newly planted perennials will need more protection than established plants, so be sure to cover their roots with mulch.\nTiming of mulching perennials is important. You don’t want to cover plants too soon as this could delay freezing resulting in heaving and thawing. Avoid spreading mulch too late in the fall or the roots may not survive the cold. It is best to mulch young perennials when the first two inches of soil is frozen.\nWhat Type of Mulch is Best?\nThere are so many choices for mulching your garden: pine needles, various types of natural packaged or bulk mulch or bark chips. You can also mulch with what is available in your garden. Instead of putting leaves in bags for yard waste collection, shred them and put them under your trees and on your garden beds around perennials. Mulch will break down and enrich your soil for a healthier and more fertile garden next spring.\nFall is an excellent time to clean your garden tools, empty your planters and urns, compost the plants, and add the soil from the pots to your garden. Clean your terra cotta and other planters, and store them in a warm place to be ready for spring planting.\nFall is such a refreshing time to be out in the garden to take in the beauty of the changing colors and breathe in the fresh, crisp air. Enjoy!\nIf you have any questions or need more information, feel free to visit Rough River Hardware for assistance. We would be happy to help you in any way we can.\nDisclaimer: The information and resources in these articles and on this website are available for informational and educational purposes only. The articles provided on this website are created with every reasonable effort to ensure completeness and accuracy. In doing so, the article writers, publishers, and the business that this website represents assume no responsibility for errors, omissions, or opposed interpretation of the articles and under no circumstance will these parties be held liable for any direct, indirect and/or consequential damages of any kind incurred from undertaking tasks outlined in the articles or on this website. In addition, it is suggested that readers check by-laws, zoning laws and building codes of your local area and country.', 'Trees & Shrubs\nUNL Extension – helping you turn knowledge into ""know how""\nWinter Tough on Landscape Plants\nby Don Janssen, UNL Extension Educator\nWinter can be rough on landscape plants, especially species that aren\'t native to Nebraska. Some types of damage can be prevented, however, and fall\'s the time to do it.\nAmong the most common types of damage are freezing, frost crack, desiccation and physical breakage. Homeowners can prevent or minimize some types of damage by shading plants, shielding them against heavy snow loads and flying salt spray, and wrapping.\nFreezing injury to woody plants most commonly affects species or varieties that are not completely hardy in northern climates. It may kill only the flowers and shoots, the roots or the whole plant. The best way to prevent freezing injury is to plant only species and cultivars known to be hardy here.\nInjury by late spring frost can sometimes be prevented by shading plants. Keeping the warming rays of the sun off the plants can delay bud break by five to 10 days. This may be long enough to avoid damage to tender flowers and shoots. It is practical only with selected small plants, however.\nFrost crack, or ""southwest disease,"" usually affects young, thin-barked trees, particularly young, newly transplanted silver maple trees. The bark splits when a bright, sunny winter day is followed by a rapid drop in temperature. The outer bark cools and contracts faster than the inner tissues and splits open, usually on the southwest side.\nPlanting species that are not prone to the problem and wrapping or shading the trunks or newly transplanted trees can reduce the incidence of frost crack.\nDesiccation is most common in broadleaved evergreens, though it can occur in narrow-leaved evergreens and even on the shoot tips of deciduous species. It occurs on bright, sunny winter days. The sun warms the foliage, which then loses moisture to the surrounding dry air. Because the ground is frozen, the roots cannot take up moisture to replace it and the foliage or shoot dries out.\nPlant sensitive plants where a building, a fence or other plants will shade them in the winter or construct sun shields on the south and west sides of plants to prevent this injury. Making sure plants go into winter well watered also helps.\nDamage caused by drifting salt spray from highway deicing efforts look much like desiccation. Some species -- including white pine, red pine and arborvitae, red oak and crabapples -- are very sensitive to salt injury. Burlap or canvas screens can protect them against salt spray. Planting salt tolerant species such as blue spruce, Austrian pine and honeylocust is another option.\nPhysical damage to plants can occur in the form of branches broken by a heavy load of snow or ice, frost heaving of young plants, and mechanical injury from snow removal equipment.\nWrapping the plants with twine or chicken wire may be all the support that is needed. Others may require more elaborate wood or metal snow shields. Avoid plants known for their tendency to break in storms and prune trees when young to eliminate weak, V-shaped crotches. This will reduce the likelihood of breakage.\nTo prevent heaving of newly transplanted young plants on clay or clay-loam soils, mulch fall planted trees and shrubs after the soil has frozen to prevent its alternately freezing and thawing. This can push the plants\' roots out of the ground.\nTo reduce damage by snow removal equipment, plant trees and shrubs some distance from snow plowing routes. Clearly mark the location of small plants that might be hidden by a blanket of snow.\nThe best time to prevent winter damage to landscape plants is before you plant. At that point you can still choose a hardy plant and a suitable planting site that minimizes the need for winter protection. For plants already in place, prevention is the best approach -- there is no cure for dead or severely damaged plants.\nUniversity of Nebraska-Lincoln Extension in Lancaster County is your on-line yard and garden educational resource. The information on this Web site is valid for residents of southeastern Nebraska. It may or may not apply in your area. If you live outside southeastern Nebraska, visit your local Extension office\nContact Information University of Nebraska-Lincoln in Lancaster County\nWeb site: lancaster.unl.edu\n444 Cherrycreek Road, Suite A, Lincoln, NE 68528 | 402-441-7180']"	['<urn:uuid:5eae5d1b-436e-4b8d-996a-2e49b188569c>', '<urn:uuid:5e4ef9af-f008-4057-989a-491dfef05d05>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T00:43:36.053682	15	56	1914
85	How do height advantages impact fighting strategy in both sports?	In both TKD and boxing, height and reach advantages significantly influence fighting strategy. In TKD, taller fighters can 'snipe' with fast front leg kicks from a safe distance, allowing them to score points early and play defensively. Shorter TKD fighters must work to get inside their opponent's range while avoiding kicks. Similarly in boxing, taller fighters can maintain distance with jabs while staying out of their shorter opponent's range. Shorter boxers need to constantly move their head, weave and slip to get close enough to land hits.	['If you are considering joining either TKD or Boxing and are concerned about which is better, hopefully this guide will give you some insight. For those of you looking at this blog to determine which form of self defense is “better,” this is not what you want, this is also not one of those posts that you should comment and assert that a boxer would beat up a TKDer in a fight or vice versa. This blog will point out some of the interesting similarities and differences I have found between the two sports. As a disclaimer I have been sparring Tae Kwon Do at a collegiate level, I have won the National Collegiate Tae Kwon Do Association championship in 2012 and have been boxing seriously in a gym for about two months. The scope will be mostly what I have personally observed and is in no way even close to represent all the similarities and differences between the two.\nIn Tae Kwon Do, there is more focus on winning matches by points, than by knockout. This is true with boxing; however there are more knockout “kings” in boxing than in TKD, and in general there are more knockouts in boxing than in TKD. TKD scores one point for body shot and significant punches to the body, two points for a spinning body shot, three for a head shot, and four for a spinning head shot. You can win by a point gap of twelve points, or by knockout. Body shots are scored electronically via Daedo socks and Daedo body armor; however, head shots are scored subjectively based on three corner judges. Only light contact is necessary to score the kick to the head and hence excessive force is not necessary for head shots. A “significant” punch is also scored subjectively by three corner judges, for both punches and head shots, 2 of the 3 judges must agree on if it counted. Referees can also call for point deductions in either half point (kyong-gos) for minor infractions like stepping out of the ring or stalling; or full point (gam-jeom) for serious infractions such as punching to the head or throat, kick to the groin, headbutting etc.\nBoxing is scored on a ten point must system, each round there is a winner who will win 10-9 which is decided by three independent ringside judges. There can also be a 10-10 draw in each round, but is normally discouraged as someone usually lands more punches. There are a lot of factors determining who “won” a round, punches thrown, punches landed, punches dodged, aggression, control, and pretty much whatever the judges feels needs to be considered. There can also be deductions off the total for the end of a round such as low blows, not listening to the ref, or getting knocked down which is a guaranteed one point lost. Boxing in the Olympics is scored via 5 judges who press a button for if a punch landed, 3 of the 5 must agree on a landed punch for it to count, the boxer with the most tallied points wins.\nThe two sports are scored differently, however they both have a subjective scoring element that can be sometimes abused and can be frustrating. For example, people may want to throw flurries at the end of a boxing round to show aggression and appear in control. TKDers definitely Kihap (yell) loudly when they throw a kick or go to the head to emphasize the kick so that it’ll most likely be scored. But these are all expected and are normally factored into the experienced judge’s criteria.\nAthleticism and Pain\nTae Kwon Do has three rounds of three semi-continuous minutes. The clock stops for injury time or adjustments or judge review of a kick scoring. There is a thirty second rest between rounds. Amateur boxing typically has four rounds of three continuous minutes, professional boxing bouts can go up to 12 rounds, both have a 30 second rest between rounds. By sparring time structure alone, boxing bouts last longer and I feel are more strenuous. There is a minimum level of flexibility for serious TKD practitioners as kicking to the head is near impossible without proper flexibility. Boxing does not require this flexibility, assuming you can extend your arm out completely and flex it back in, you should not be limited.\nIn sparring, you throw more punches in boxing per round on average than you would throw kicks in a round of TKD. Punching with gloves can get very tiring, and within four rounds you can find yourself having trouble keeping your hands up the whole time. There is a lot of instances of combinations thrown and rapid slipping and dodging in boxing which requires a tremendous lung capacity and muscle stamina to handle the intensity of each round, this explains a very heavy emphasis that we have on cardio for training. Cardio is also important in TKD, but I often find myself winded and exhausted after boxing and more cramped and physically in pain from kicking frequently in TKD.\nIn boxing you will wear headgear and a mouth guard and depending on your match, lower body gear to protect your nether regions. In TKD you get a chest guard (hogu), headgear, mouth guard, arm guards, shin guards, an instep, and optionally small <2 oz. gloves. I more often injure my feet by kicking elbows in TKD, but don’t get injured from getting physically kicked or hit due to ample protection. In boxing, you may get rocked, expect it, expect your face to hurt a bit, to cut your mouth, and maybe have the room spinning after getting hit in the head (most punches aren’t this bad). But nothing hurts more than a solid punch to the body in boxing, with exception to a very strong back kick. I find that wrapping your wrists and knuckles and the glove itself does a good job in protecting your hands, but the tiny foam pad used in TKD to protect your foot does not help much at all. Kicking an elbow is like kicking a concrete pole, it sucks, you’ll also get a lot of blisters on your feet in the process of learning to spar.\nTiming and Range\nBoxing feels much faster than TKD sparring, combinations happen in split seconds and your reactions need to be on point to be able to slip and weave or counter punch. TKD tends to have lulls, as does boxing, but are more dramatic where fighters essentially “lock up”- stand just in each others range and wait to see what the opponent will do to make a move. In boxing we have something similar, clinching, which you do in TKD when you in-fight, but normally people try to put their weight on you and wear you down when you clinch in boxing and also throw punches that can hurt where a chest guard would render a TKD clinch-punch ineffective. Just thinking about it, punches are much faster than kicks, they are lighter and travel less distance, kicks also reveal a lot of motion which gives you more time to react, punches (especially jabs), can have little to no tell.\nTKD has a much farther range than boxing, and many small techniques can make a short person, kick from what seems an unreal distance with fast speeds. Height plays a big factor in both TKD and boxing, reach comparisons between fighters can largely determine styles used and the outcome of a match. A tall TKD sparrer can “snipe” with fast front leg kicks and score points early, easily, and with little risk, setting up a counter-attack defense game for the rest of the match. Shorter TKD sparrer would have to draw out the kicks from the opponent and manage to get inside close enough where he/she could kick, without getting kicked. Both have strategic advantages, but to the inexperienced, being shorter can be a serious crutch without the right technique. I personally have been boxing for about a full month, one of my partners is 6’2″ while I am 5’8″, his reach is significantly more than mine, and I find myself eating too many jabs, meanwhile he can stay at a comfortable distance of hitting without being hit. I must constantly move my head, weave and slip, to even get into distance to have a shot at hitting him. If you want a reference to what it looks like at a professional level, watch Muhammad Ali vs. Joe Frazier, a classic out-boxer in-boxer match up.\nI find that range issues are equally as challenging in between the two, and timing is extremely important, but that boxing timing requires more discipline, you have to know what you’re doing and commit and be able to adjust mid punch very quickly. I feel that you can get away with timing hesitations more in TKD sparring than in boxing.\nThere are many style differences and categories in each sport, I will talk about what I have personally experienced. In TKD you need to have solid technique in attacks, counter attacks, and footwork. Footwork is the foundation. This is similar to boxing as your footwork is also very important. In TKD footwork is a matter or ranging, making quarter steps, half steps, three quarter steps, full steps, all different technique just to get a certain distance to place a desired kick. In boxing, footwork allows you to move freely about the ring, to throw punches without restricting your own power (turning your toe), and keep your balance in order to slip and throw punches. Great footwork is essential to any TKDer or Boxer, I feel great footwork in TKD is more significant than in boxing, just because you can probably get away with bad footwork as a boxer (George Foreman) but not as a TKD sparrer.\nPredictably, hands are much more important in boxing than in TKD. In TKD using a punch is a great way to stop an opponent and tends to be used more defensively than offensively. In boxing, it’s all your hands. It’s about keeping them up, keeping your arms tucked in to protect your ribs and your body, turning your hand when you punch, keeping your hands loose when you aren’t punching to save energy, throwing feints and flicking quick jabs, strong straights, compact hooks and powerful upper cuts.\nTKD has a lot of emphasis on negation, hit without getting hit, or rather score without getting scored on. The best ways of negation being jamming your opponent (coming in too close to get hit or stopping the kick before it’s completely extended) or blocking the kick (via down blocks) or simply avoiding the kick. This puts a lot of emphasis on footwork and dexterity, and moving around the ring. Many matches don’t score beyond double digits and so you can imagine the level of negation and footwork necessary to hold the lead. Often if you have a decent lead, it is to your benefit to go on the defensive and play a counter-attack strategy. In boxing, due to the ten point must system, there is no letting up in each round, as each round is considered a win or lose and you must win more than you lose. Hence boxing matches tend to be a higher intensity throughout rather than a take the lead and play defense method.\nBoxing and Tae Kwon Do are both very different sports that have a lot of commonalities, just different weapons. The hands are very fast and can be very deadly yet lack range. Kicks have much more power and range but are a bit slower. As for what style would win in a fight between two equal height and weight individuals, it’s hard to say. But I do think having both can be helpful. Please let me know if you would like more information as I did a pretty cursory job at comparing and contrasting the two.']	['<urn:uuid:415743d6-c03b-417d-a815-6747ebbc3a48>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	10	87	1992
86	What makes HMS Victory's bronze guns historically significant, and what threats do marine artifacts face?	HMS Victory's bronze guns are historically significant as they were among the last full complement of bronze guns on a Royal Navy warship, with the 42-pounder being extremely rare - the only one ever recovered from a shipwreck. As for threats to marine artifacts, they face both environmental dangers like erosion when protective sand cover is removed, and human threats including vandalism and theft of artifacts which can result in loss of archaeological context and diminished understanding of wreck sites.	['Alan M. Smith\nOn display in the National Museum of the Royal Navy (NMRN) in Portsmouth, UK, is a mighty 42-pounder bronze gun which was retrieved from the wreck of HMS Victory (1737) in 2008. Also recovered at the same time was a 12-pounder bronze gun which remains in the NMRN Conservation hall. These two guns form part of a full complement of 110 bronze guns which served on the flagship of Admiral Sir John Balchen. Captained by Samuel Faulknor, the Victory was the finest and mightiest First-rate warship in the Royal Navy of the time—and one of the last of the large ships to sail with a full complement of bronze guns.\nHMS Victory was lost with her 1,100 crew in a storm on the English Channel on the night of 3–4 October 1744. She was part of a large Anglo-Dutch fleet commanded by Balchen, returning to England having successfully relieved a vital victualling convoy which had been blockaded by a French fleet at the River Tagus, in Portugal, en route to the Mediterranean squadron. The wreck of the Victory was located in 2008 by Florida-based Odyssey Marine Exploration, lying 75 metres below the surface, around 100 km west of the Channel Islands and approximately 80 km south-east of Plymouth.\nWith permission from the UK Ministry of Defence (MoD), the two guns were brought up from the wreck site and handed over to the Receiver of Wreck in 2009 for the positive identification of the wreck. The NMRN then took custody of the guns and commissioned Mary Rose Archaeological Services to conserve them. The work was carried out at the Royal Armouries workshop at Fort Nelson in Fareham, Hampshire. Conservationist experts from West Dean College of Arts and Conservation, near Chichester, were called in to aid in the preparation of the guns for display after extensive chemical desalination treatment. Whilst undergoing conservation work, the excavation of the interior of the guns revealed that they were still fully loaded—complete with hemp rope wadding, gunpowder, and a cannon ball—and ready to fire. In September 2018, the 42-pounder gun went on public display in the ‘Sailing Navy’ gallery of the NMRN at Portsmouth Historic Dockyard, where it remains today.\nWeighing over three tonnes, with a length of 3.4 m (11.5 ft) and a muzzle diameter of 178 mm (7 in), the gun is a beautiful example of period craftsmanship. It includes elegantly ornate dolphin handles and an intricately depicted Royal Crest of King George I (1714–1727). The 12-pounder gun from the ship bears the Royal Arms of King George II (1727-1760). The reigns of these monarchs allow us to date the guns, but also provide relevant contextual information as regards their acquisition for use aboard Victory. The decision to rebuild HMS Victory to the specification of a 100-gun warship—upon the frame of the old ship of the same name—was made in 1726, and her keel was laid the next year in a Portsmouth dry-dock. However, the official warrant to rebuild her was not issued until September 1733 and she was not re-launched until 1737. As such, the manufacture of the larger gun must have preceded the ship’s construction by a number of years.\nVictory was the last major Royal Navy warship to carry a complete complement of guns cast in bronze, as thereafter cheaper iron gun production gradually superseded bronze manufacture. Since bronze guns from this period were typically melted down for re-use, the guns recovered from the Victory are quite rare. The 42-pounder example is extremely rare—indeed, the Victory is the only shipwreck from which a 42-pounder cannon has ever been recovered. Considered the most powerful and prestigious guns used in naval warfare, the Victory carried twenty-eight 42-pounder cannon along her lower decks. Charles Trollope notes that the Victory is “the only wreck site of a First Rate Royal Navy warship with an intact collection of cannon known in the world”. He claims it is possible that the addition of these long, heavy guns may have contributed to the ship’s sinking in rough seas.\nThe Victory’s guns have been described as “extremely rare examples of hybrid guns designed by Colonel John Armstrong based on the former Borgard system and a master template obtained from the French”. Albert Borgard was a Danish mercenary soldier who joined the English Army in 1692, and in 1716 oversaw the standardisation of artillery. Under his method of reform, guns were thereafter known by the weight of the shot being fired (i.e. 12-pounder, 24-pounder, 32-pounder, etc). Armstrong, as the new Surveyor General of Ordnance in 1722, then redesigned the Borgard system to incorporate further modifications.\nThe base ring of the 42-pounder gun displays the name of the maker, “SCHALCH”. Andrew Schalch was born in Schaffhausen, Switzerland in 1692 and was employed at the cannon foundry in Douai, in northern France, before moving to England. In 1716, he was engaged to build furnaces and other material for the new brass works being constructed at Woolwich. In May 1718, he was appointed as the first master founder to the Royal Brass Foundry in Woolwich (later Woolwich Arsenal) where he served until retiring in 1770. The name “SCHALCH”, together with the year “1723”, also appears on a 24-pounder bronze gun currently held in a museum storage facility in the Netherlands. The MoD maintain that the gun was illegally removed from the Victory wreck site by a Dutch salvage vessel using a hydraulic grab in July 2011 and taken back to Holland. Inter-governmental negotiations are currently underway to retrieve the gun.\nThe Victory shipwreck—described by the NMRN as “probably the best example of the early Georgian period that carried bronze cannons”—remains in situ, where it is monitored against any further excavation or interference following a 10-year legal wrangle over the future conservation of the wreck, involving the Maritime Heritage Foundation (MHF), Odyssey Marine Exploration, and the MOD, and subsequent Judicial Reviews sought by the Joint Nautical Archaeology Policy Committee chair and MHF. According to the findings of the most recent Judicial Review in the High Court (September 2019), the wreck contains “at least 41 bronze cannons, ship-borne artefacts, iron ballast, wooden fixtures and fittings, parts of two anchors and a rudder”. Although claims had been made that some guns have been disturbed, damaged, or scattered by trawling over the years, the latest court judgement ruled that all the artefacts should remain with the ship and that the wreck was at “minimal risk” and could be “appropriately monitored”.\nIn addition to the 42-pounder gun on public display at the NMRN in Portsmouth, there are three other related items held in the National Maritime Museum in Greenwich. The first is a magnificent contemporary full-hull, large scale (1:34.3) model of HMS Victory (1737) once used by the Royal Naval Academy at Portsmouth Dockyard to educate young men entering sea service. The second is a portrait of Admiral Sir John Balchen (circa. 1705) by Jean Baptiste de Medina. Finally, there is an oil painting (circa. 1745) entitled The loss of HMS Victory, 4 October 1744 by Peter Monamy, which depcits the ship in extreme difficulty amongst the mighty waves of the storm that took her. Also of related interest is a magnificent memorial to Sir John Balchen in Westminster Abbey, which was erected by his widow in 1746.\nBaugh, Daniel A. 2004. ‘Balchen, Sir John (1670-1744)’ in Oxford Dictionary of National Biography. Oxford: Oxford University Press.\nBBC (British Broadcasting Corporation). 2010. ‘HMS Victory cannon ‘rare example’, says expert’. Digital edition: 11 October. <https://www.bbc.com/news/world-europe-guernsey-11516163>.\nBBC (British Broadcasting Corporation). 2019. ‘HMS Victory: Ship artefacts to remain on wreck’. Digital edition: 27 September. <https://www.bbc.com/news/uk-england-devon-49854546>.\nCharnock, John. 1795. Biographia Navalis: or, impartial Memoirs of the Lives and Characters of Officers of the Navy of Great Britain from the Year 1660 to the present time, Vol. 1. London: R. Faulder. [Available via the National Museum of the Royal Navy archives, Portsmouth].\nHepper, David J. 1994. British warship losses in the age of sail: 1650-1859. Rotherfield: Jean Bardriot Publications.\nMaritime Heritage Foundation. n.d. ‘The Cannon’. HMS Victory – 1744. <http://www.victory1744.org/thecannon.html#>.\nNMRN (National Museum of the Royal Navy). 2018. ‘Incredibly Rare Cannon from Legendary HMS Victory 1744 Wreck Site is to be Displayed for Very First Time’. 8 September. <https://www.nmrn.org.uk/news-events/nmrn-blog/incredibly-rare-cannon-legendary-hms-victory-1744-wreck-site-be-displayed-very>.\nOsborne, John. 1994. ‘Cannon ball sizes: Borgards standardised ordnance’. NZAR A83. New Zealand Arms Register.\nReynolds, Rebecca. 2019. ‘Judicial review undertaken for HMS Victory salvage’. Institute of Art & Law. 10 April. <https://ial.uk.com/judicial-review-undertaken-for-hms-victory-salvage/>.\nWinfield, Rif. 2007. British Warships in the Age of Sail 1714–1792, Vol. II. Barnsely: Seaforth Publishing.\nWinfield, Rif. 2010. First Rate: The Greatest Warships of the Age of Sail. Barnsley: Seaforth Publishing.', 'Opportunities and Threats to the Maritime Environment\nThis section discusses how natural changes to the maritime environment and changes in human exploitation and understanding of the sea, allow greater opportunities to understand and study maritime heritage than ever before, but by the same token also pose greater threats to its integrity.\nThe marine and inter-tidal zones are dynamic and have always undergone natural environmental change and changing patterns of use and exploitation which are nothing new.\nThe coastline has always been subject to change, whether settlements and harbours have been eroded and lost to the sea, as at Dunwich in Suffolk, or have silted up and are now located inland, for example on the Sussex coast.\nThis process is continuous and is also affected by patterns of extreme weather which can reveal new wreck archaeology, result in damage to known wreck sites, or even create new forms of wreck.\nCoastal surveys and the techniques of marine archaeology lead to a greater understanding of the character of the local marine historic environment and how it is affected by these processes.\nResponding to the unexpected\nThe patterns of changing exploitation of the seabed create new opportunities for discovery. For example, the wreck of the London (1665) was located and identified through archaeological assessment in advance of the London Gateway project which deepened the navigational channels in the Thames, resulting in the designation of the wreck and its continuing investigation.\nIn the marine planning system, new forms of construction at sea in the form of offshore renewables (windfarms) have the potential to impact on archaeology, but by the same token, can bring to light new archaeology and increase our understanding of what lies beneath the sea.\nIn a similar vein, individual climate events such as continuing storms of early 2014 can have an impact on local beaches, revealing wrecks that have rarely or never been seen before. These are often reported by members of the public, allowing us to enhance our records and identify wrecks for further research or potential protection.\nSuch wrecks, or other historic seabed or shoreline features such as submerged forests, can also come under threat. These threats may be environmental, for example, the erosion of a protective sand cover also exposes ship timbers on a wreck to the same forces of erosion.\nDespite the positive impacts of public enthusiasm for maritime heritage, there are negative sides to human interest in shipwreck. Human impacts, from vandalism to theft of artefacts or the non-disclosure of removed artefacts from a shipwreck, are also a threat.\nIf objects are removed from shipwrecks without proper archaeological recording their context is lost and this lessens our understanding of these wrecks, while less is left for visiting divers to enjoy. These constitute heritage crimes and Historic England works closely with the Receiver of Wreck in order to educate and inform the public on maritime heritage issues.\nWhat we can do\nWe can care for maritime archaeology in a number of ways. At Historic England we vary our approach according to the needs of each site that are clearly identified in a site- specific Conservation Management Plan.\nBesides coastal survey and marine environmental assessments, photographic and documentary recording allows us to monitor changes to local archaeology and to identify individual sites or forms of archaeology which might need further investigation, research, recording, or protection, including an in situ approach.\nThis includes continual enhancement of the National Record of the Historic Environment (NRHE) database or commissioning strategic reviews that prioritise particular types of vessel for investigation and protection, such as the Ships and Boats pre-1840 review. As a result a number of vessels such as the Daresbury, a rare survival of an 18th century Mersey flat, have been protected.\nArchaeological protocols for commercial activity mean that finds from the dredging, fishing and offshore renewables industries are now routinely reported and recorded in the NRHE database and also reveal new patterns from the past such as clusters of cannonballs which may indicate concentrations of naval or privateering activity and may potentially be matched with particular battles.\nWe are also looking at new ways of engaging the public through developing dive trails, by community engagement and raising awareness through the latest technology, allowing non-divers and non-specialists to access information on maritime heritage.\nWe also welcome reports from members of the public and leisure divers of anything they have seen or researched, which in turn is recorded and made accessible to the public through the NRHE database or through formal archaeological investigation.\nIt becomes a dynamic ‘virtuous circle’ of increasing understanding of the maritime heritage environment.']	['<urn:uuid:903a0196-39e9-46bd-bed7-1d7a77b92f83>', '<urn:uuid:9ba8f652-91fb-4554-96ad-8fda81dc210c>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	15	80	2198
87	latest tech health tracking devices main benefits people	Modern wearable medical devices offer multiple benefits: they can monitor vital signs, provide prompt health alerts to both users and doctors, track fitness activities, manage diseases, dispense medication, and predict potential health conditions. These devices are designed to be small-sized, portable, and user-friendly, helping to extend patient life spans while reducing healthcare costs.	['Big Data: How Smart Medical Devices Can Change the Future of Healthcare\nThe future of healthcare will revolve around big data and smart medical devices. Medical equipment of the past and the present rely on nurses and doctors to interpret results, limiting their flexibility. Big data enabled medical devices; on the other hand, rely on massive sets of data to be able to provide diagnostic services that output results as well as conclusions. That additional functionality stands to revolutionize the whole healthcare industry. Data analytics can also be used to augment everything from current-generation MRIs to X-rays, making it possible to scan images created by such tests for any abnormalities that may exist. Smart imaging systems can be connected to large image libraries, building databases of characteristics that can help with improving diagnostic accuracy. The ability to maintain and scan through large datasets is here, and it will allow for correlations to be made that humans may miss. Here’s a look at some of the ways that big data and next-generation Internet-of-Things (IoT) medical devices are impacting health care right now.\nDeep Learning is Being Tested\nDeep learning using big data is being tested to predict seizures in patients that are suffering from epilepsy. One study showed that through a prediction system powered by deep learning, it was possible to predict seizures 42.3% better than a random predictor system. Those results come despite the fact that medical equipment used in current tests is still in its infancy and still has a long way to go in terms of diagnostic accuracy and the sophistication of the deep learning model. Still, even at this early stage of development, the work proves that predictive analytics and diagnostics for a multitude of diseases are possible. The real promise of deep learning systems as medical diagnostic tools is their ability to scale up quickly. They become both more accurate in their predictions and quicker in producing them with each new data stream that becomes part of the model. In the case of the research mentioned here, as patients have more seizures, these systems will be able to learn from the patient-specific datasets compiled over time. That will train the model to better spot the warning signals that a seizure is imminent so that the patient can receive an alert in advance. The ultimate goal of this research into deep learning will be to create special hardware, in the form of wearables and other devices that will be able to accurately warn patients of seizures in real time and with no intervention from medical staff. Researchers even suggest that through an improvement in their prediction model, it may be possible to build more complex deep learning networks that are able to monitor myriad patient safety factors rather than just hoping to provide early warning for seizure sufferers.\nInternet-of-Things and Big Data\nIoT and big data can come together to offer a variety of new opportunities in healthcare. Connected IoT devices and other smart technology are starting to be adopted for everything from monitoring general fitness to checking email. They’re even being put to use powering a range of apps and services to help people get a good night’s sleep. Now, the healthcare technology industry is starting to introduce wearables to help manage diseases and medical conditions. Wearables and body sensors can provide a higher level of care for patients, allowing for big data to provide greater insight into a person’s health condition. Smart devices have already started to be implemented into the drug dispensary field, allowing for notifications and alerts to be sent to patients so that they know when to take medication. In some instances, the devices themselves are able to administer the proper medication dose to a patient with no input from a doctor using nothing but real-time patient data. Big data has the potential to take this a step further, predicting instances of undiagnosed secondary conditions based on sensor data, diet-tracking and medical records. It may soon be possible for these wearable devices to make real-time recommendations to patients themselves, letting them know when they may need to seek medical care to prevent conditions they may not even be aware of. The benefits of such a revolution in preventative medicine could include extending patient life spans while driving down costs in the healthcare industry. That would, in turn, free up resources that could then go toward researching cures for the diseases that plague mankind, like cancer. The one thing that is certain is that big data is the innovation of the future, and as technology is able to process and learn from this data faster, we will start to see more smart devices enter the field. When that happens it should usher in a whole new, data-driven era for modern healthcare that promises longer, healthier lives for all.', 'Wearable Medical Devices Market By Type (Smart Watches, Activity Monitors, Smart Clothing and, Patches), Distribution Channel (Online Channel, Pharmacies and, Hypermarket), Device Type (Diagnostic & Monitoring Devices and Therapeutic Devices), and Application (Sports & Fitness, Remote, Patient Monitoring and, Home Healthcare) – Global Analysis & Forecast 2019-2030\nReport ID : 249 | Published Date : Jun 2020 | Pages : 180 | Region : Global | Report Code : MD-71\nWearable Medical Devices Market Introduction & Overview\nWearable medical devices are handheld electronic devices which are typically worn over the body. It is used for diagnosing & monitoring several medical conditions & attributes respectively. Wearable medical devices offers better monitoring of vital signs and simultaneously provides prompt alert regarding diverse parameters of health to a user as well as physicians. These devices are small-sized, portable and user-friendly. Various types of wearable devices are available in the market such as pain management devices, diabetic devices, brain devices, sleep tracking devices, and fitness tracking devices among others. These devices are equipped with sensors that monitor and track patient’s health.\nGrowing number of smartphone-based healthcare apps, rising prevalence of lifestyle-associated diseases and rising consciousness regarding one’s health are few of the factors which are playing a pivotal role in augmenting the growth of the market. Moreover, increasing investments from tech giants to develop advanced & innovative devices and their participation in several collaborative initiatives has further propelled the growth of the market. For instance, in 2016, Medtronic and Fitbit collaborated to provide unique activity tracking platform for patients suffering from diabetes.\nWearable medical devices market has primarily been segmented into the following categories, viz. by type, distribution channel, device type, application, and region\nWearable Medical Devices Market by Type\nOn the basis of type, wearable medical devices market is segmented into smart watches, activity monitors, smart clothing and patches. Activity monitors segment is estimated to account for the largest share in the global wearable medical devices market. These devices are used to measure metrics such as total distance covered, heartbeat rate, calories consumption per day and broad range of vital activities.\nWearable Medical Devices Market by Distribution Channel\nOn the basis of distribution channel, the market can be segmented into online channel, pharmacies and, hypermarket. In terms of market share, the segment of online channel is estimated to account for the largest market share during the forecast period owing to easier accessibility and availability of different brands on a single platform and expanding pharmaceutical industries.\nWearable Medical Devices Market, By Device Type\nThe segment of device type can be majorly classified into diagnostic & monitoring devices and therapeutic devices. The diagnostic & monitoring devices are further classified into vital signs monitoring devices, sleep monitoring devices, glucose monitoring devices, fetal monitoring & obstetric devices and, neuromonitoring devices. Therapeutic devices are further classified into Pain management devices, insulin pumps, rehabilitation devices and, respiratory therapy devices. The diagnostic & monitoring devices segment is projected to be the fastest growing segment during the forecast period, owing to rising prevalence of chronic diseases and growing need for tracking & monitoring of several diseases at an early stage.\nWearable Medical Devices Market, By Application\nThe segment of application can be majorly classified into sports & fitness, remote patient monitoring and, home healthcare. The sports & fitness segment is projected to be the fastest growing segment in the global wearable medical devices market during the forecast period owing to changing lifestyle and increasing concern about fitness among consumers.\nWearable Medical Devices Market by Regions\nThe global Wearable Medical Devices market can be segmented into North America, Europe, Asia Pacific, and the Rest of the World (ROW). North America accounts for the largest share in wearable medical devices market owing to early adoption of wearable medical technology, and presence of developed healthcare infrastructure. Asia Pacific is expected to witness the highest CAGR owing to technological advancements, increasing demand for user-friendly & self-monitoring devices by consumers and growing geriatric population among others.\nWearable Medical Devices Market Prominent Players\nSome of the key players operating in the global wearable medical devices market are Fitbit, Inc., Koninklijke Philips N.V., Omron Healthcare, Inc., Drägerwerk AG & Co. KGaA, Polar Electro, VitalConnect., World Global Network, Garmin Ltd., Xiaomi and ten3T Healthcare among others.']	['<urn:uuid:62679386-f007-47d2-abe2-ae45fb675296>', '<urn:uuid:a6d3ce60-793c-46cd-86a1-f9cc3b9eb570>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-13T00:43:36.053682	8	53	1512
88	right angle fuel fitting problems racing	Forged 90-degree elbow fuel fittings should be avoided as they are restrictive and frequently cause fuel flow troubles. Both 90-degree and 45-degree hose ends should also be avoided if possible. If necessary, radiused hose ends with 90-degree bends should be used instead, as they have much better flow rates and are manufactured from aluminum with swivel ends for positive sealing.	"[""Competition Fuel Systems - 15 Common Fuel Delivery Mistakes & How To Correct ThemWritten by on December 1, 2004 Making A Difference In Performance\nFuel lines, filters, pumps, and regulating devices exist for a specific task-to deliver gasoline or alcohol from the fuel cell to the carburetor, and, in a bypassing system, return it to the fuel cell. Pressures and volumes vary depending upon the kind of fuel used (gasoline or alcohol) and the type of fuel system employed. With routine maintenance, a good fuel system will pay dividends. However, many cars are fitted with systems that fall below the necessary standards.\nBG Fuel Systems has encountered some common concerns when handling customers' tech questions. The company put together a list of the most common mistakes in competition fuel systems and a guide for correcting them:1 Incorrect fuel lines Do not use 11/44 to 31/48-inch outside diameter lines on a racing fuel system between the fuel cell and the pump. Racers often joke about their first race cars and how the fuel line was so small it functioned as the main jet. Make sure the fuel is supplied through lines that are the correct size for the application. Remember, the fuel line's size is determined by the system, not the vehicle. Push-Lok, stainless steel braided hoses, and aluminum tubing are the most common fuel lines used on race cars.\n2 Right-angle hose-end fittings from the pump to carburetor Avoid forged 90-degree elbow fuel fittings as much as possible. Although they are inexpensive and readily available, they're restrictive and frequently cause fuel flow troubles. Hose ends with angles of 90 degrees and 45 degrees should also be avoided if possible. Should it become necessary to use one, use radiused hose ends (90-degree bends) which have much better rates of flow. They're manufactured from aluminum, equipped with swivel ends for a positive seal, and are easy to install.\n3 Fuel pumps unsuitable for alcohol An alcohol fuel system differs from the gasoline system in several crucial ways. Fuel pressures in a gasoline system are typically maintained between 7 and 9 psi throughout the rev range. Alcohol carburetors require low pressures of around 4-6 psi at idle and 9-11 psi at fully open throttle. This is necessary to prevent the carburetor from flooding at idle and under light engine load, yet maintain the extra volume necessary for maximum acceleration. Engines producing around 500 hp can be fueled with a 15-psi mechanical pump in conjunction with a throttle bypass valve. However, for engines over 500 hp, a belt-driven system with a diaphragm valve or poppet bypass should be considered. For overall reliability and performance, a belt-driven system is usually the better choice. When using a diaphragm bypass, BG Fuel Systems recommends it be installed close to the carburetor for faster response.\n4 Incompatibility between bypass and pump Throttle bypasses were designed to operate with block-mounted pumps and, similarly, diaphragm bypasses with belt-driven pumps. Never use a throttle bypass with a belt-driven pump. They must function in pairs and are not to be mismatched.\n5 Unsuitable fuel filter Fuel filters with conventional paper elements must not be used with alcohol. Alcohol absorbs water, so the paper and the bonding materials deteriorate quickly. As a consequence, particles can enter the float bowls or get stuck in the needles-and-seats and main jets. Furthermore, it's equally important on alcohol applications to avoid filters with inadequate flow rates and inlet and outlet sizes that are too restrictive.\n6 Neglecting routine maintenance on an alcohol system Although alcohol fuel additives can help prevent corrosion and provide lubrication for pumps and other components, alcohol can severely corrode metal objects, especially aluminum, if allowed to remain in contact too long. There is no substitute for a strict maintenance program. It will prolong the life of the carburetor and fuel system and also keep it trouble-free.\nAlcohol will naturally absorb water from the air. This not only dilutes the fuel, but also adds to the corrosive effects of alcohol. After every race, the alcohol should be drained from the race car and stored in air-tight containers. A vented fuel cell is not considered an appropriate storage container.\nThe fuel system and carburetor should also be thoroughly drained and flushed of any residual alcohol. Some racers will add gasoline to the empty fuel cell and run the engine until they are certain the carburetor is filled with gasoline.\nOther methods include removing the carburetor and flushing it with a cleaning solvent or lubricating aerosol sprays. Removing inlet and outlet fittings from the pumps, bypasses, etc. and lubricating the internals is also an acceptable practice. Whatever the method, maintenance on an alcohol system is crucial. If you ignore it, the system will fail.\n7 Failing to use a high-flow air cleaner Race engines rely on receiving air as well as fuel, but are frequently starved by the use of thin, small-diameter air filters that are detrimental to their performance. If possible, use a filter that is 14 inches in diameter and 4 inches tall. If clearance is at a premium, get one with a recessed pan, which allows for deeper filter elements. Also, get a good-quality air filter. If you're constantly cleaning dirt from the surface of your race car, your air cleaner is struggling to filter the dirt and debris from your engine.\n8 Deterioration of foam-filled fuel cells Modern military-spec foam-filled fuel cells are compatible with conventional fuels, racing fuels, and alcohol. However, alcohol can cause the foam to deteriorate and it must be renewed once a year. To check the condition of the foam, simply remove the cap and pinch it between finger and thumb. If pieces come away, the foam must be replaced.\n9 Lack of proper linkage travel and return springs Make sure that the butterflies of the carburetor are fully open at wide-open throttle. Use a minimum of two good-quality return springs, preferably of stainless steel, and employ them, if possible, in two different places on the linkage. Ensure that the linkage and the return springs operate without interference throughout the full range of throttle travel.\n10 Failure to use an adjustable pedal stop It's amazing to consider the large numbers of race cars that have no form of throttle-pedal stop, and it's inexcusable that so many are permitted to compete. Excessive loadings on the linkage, carburetor shafts, and butterflies can cause the mechanisms to distort and jam, and the consequences are usually grim. To avoid the inevitable, use an adjustable pedal stop. At the fully-open throttle position, synchronize the stop on the carburetor with the stop on the pedal.\n11 Lacking the proper size of carburetor for the application Having the proper venturi sizes for a given application ensures that the carburetor generates sufficient air speed. Air speed creates the necessary depression (low pressure) to draw fuel through the metering systems and booster venturis into the air stream to be atomized. The Race Demon, which is equipped with removable venturi sleeves and boosters, overcomes most of the sizing problems.\n12 Inadequate fuel cell venting If the fuel cell vent is too small, the fuel system can malfunction. In extreme cases, inadequate ventilation can cause permanent damage to the system. As the fuel pump draws fuel from the cell, it needs to be replaced by air. If the vent on the cell is too small, the pump will try to draw the fuel from the cell faster than the air replaces it. This can create a vacuum in the cell, distort its shape, and starve the pump and engine of the fuel it requires.\n13 Filtering of fuel cell vents Just as an undersized vent will adversely affect your fuel system, not having a filter on the vent will cause dirt and debris to enter it. As air replaces the fuel, the vent hose will attract anything that's in the air, including dirt, sand, or debris. These particles will eventually destroy a fuel system as well as an engine.\n14 Fuel filter location To protect the fuel pump and carburetor, use a good filter between the tank and fuel pump, and filtered fittings at the carburetor.\n15 Inspections Steel braided fittings and aluminum fittings have a life cycle. Over a period of time, the rubber bore will deteriorate, causing the line to either collapse and starve the engine of fuel, or disintegrate and possibly block the passage. Radiator hoses and fan belts are routinely replaced. The same thinking holds true for fuel system components. From a safety and performance standpoint, regularly check the lines and fittings. Try to detect soft or weak spots by feeling the outside of the lines, and visually inspect the internals each season. Keep the connections tight.\nThe air quality makes a huge difference in the fuel system's production. A well-made and well-maintained air filter of adequate size will improve performance.\nYou can overcome sizing problems with a removable sleeve model when picking a carburetor. This two-barrel Demon has one sleeve installed for illustration purposes. Several sleeve sizes are available.\nThis is the schematic of a system that uses a throttle bypass.\nThis fuel system uses a belt-driven fuel pump with diaphragm bypass.""]"	['<urn:uuid:47599c38-233b-497f-ac7b-3a3cb08c1958>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T00:43:36.053682	6	60	1528
89	new player golf equipment budget sneakers what footwear affordable beginners	According to market research, new golfers who buy entry-level $400 golf sets typically don't spend $120 on golf shoes - they play in regular sneakers instead.	['Streetwear – long the origin of New York’s hip-hop and California surf culture – has made its way onto the green grass of golf courses.\n“Golf has become cooler and it has become less aloof because there are parts of the sneaker community that have embraced it,” said Jacques Slade, a sneaker YouTuber and golfer who spoke out about the need for more golf shoes that reflect the sneaker. culture.\nHip-hop culture and sneakers have always been close, but the connection between hip-hop and golf may not be too far off, says Ankur Amin, an owner of New York streetwear boutique Extra Butter. He said the ambitious appeal of golf has helped the style of golf bond with its customers.\n“So much that we do in street culture is about pursuing the good life,” he said, “and so much about golf represents that, just like Moët & Chandon or Louis Vuitton do.”\nTiger Woods, a Nike-sponsored golfer, brought many new fans to the sport in the late 1990s, but waning interest in his products in the 2010s paved the way for a streetwear crossover to golf. Nike and a subsidiary, Jordan Brand, began releasing collectible silhouettes as golf shoes, such as the Air Max 1 and the original Air Jordans.\nSneakerheads salivated. “You have people who grew up with the Jordan brand,” said the rapper and golf entrepreneur Macklemore, who has done sneaker collaborations with Jordan. “It makes sense for people to go crazy.”\nAnd sneaker culture’s grip on golf has only grown. While the pandemic has devastated a number of institutions, it has also boosted participation in golf, as well as other activities conducive to social distancing, such as running, walking and cycling, according to the NPD Group, a market research firm.\n“When the golf courses reopened, the business just took off,” said Matt Powell, the NPD Group vice president and sports industry analyst, who said attendance was also slightly higher before the pandemic.\nMany people bought golf sets at entry-level prices in 2020, he said, an indication that newcomers were picking up the sport. “One of the beginners who buy $400 golf sets is not going to drop $120 on golf shoes,” he said. “They’re going to play in sneakers.”\nSneakers have always been an overarching part of millennial fashion choices, but now some adults in their late 20s and 30s have the disposable income to play golf — or at least try to. Top Golf and Five Iron Golf venues, in some ways the equivalent of bowling alleys, have also opened across the country, making elements of the sport more accessible in urban areas where courses are harder to find.\n“Golf is a game that’s very traditional, but when you look at millennials and all the generations that follow them, they’re never afraid to do something different,” said Gentry Humphrey, former vice president of footwear at Jordan. Note that the company’s entry into sports led.\nBefore retiring last fall, Humphrey also spent time running Nike’s golf business. Part of Humphrey’s philosophy was to transform Nike and Jordan sneakers, which collectors love, into shoes that can actually be used on the fairway. “Kids want to go outside,” he said, “and they prefer to go out with something fresh.”\nWhile producing these golf sneakers may seem as simple as adding high-traction soles, there are also other considerations, such as waterproofing and adjusting the cushioning.\n“We didn’t want it to be just a basketball shoe going to the golf course,” said Humphrey, adding that Nike had developed new shoe technologies such as the Integrated Traction Bottom — a rubber outsole without hard spikes that players could wear all day.\nAnother part of Humphrey’s strategy was to provide a broader platform for golf startup brands through product collaborations. For example, Eastside Golf, a brand founded in 2019 by professional golfers Olajuwon Ajanaku and Earl Cooper, who played together at Morehouse College in Atlanta, aims to increase diversity in the sport and introduce young people to the sport.\n“Who said you can’t play golf in a T-shirt?” said Cooper, the first African-American all-state golfer in Delaware. “When they made these rules, minorities weren’t even allowed to play. People are trying to hold on to a tradition that was already broken or flawed.”\nAjanaku, who trademarked the Eastside Golf clothing line, featuring a black man in blue jeans wearing a gold chain and baseball cap while swinging a club, said the prominent placement of a person of color on the company’s products was a milestone.\n“The fact that we have a black golfer’s logo on our clothing appeals to anyone who has not felt welcome in the sport,” he said.\nThe Eastside Golf logo was featured prominently on the tongue of their Air Jordan collaboration, which used the silhouette of the original Air Jordan IV, a retro sneaker highly regarded among sneakerheads. The golf spikes were removable so that the sneakers could also be worn outside the course.\nShoes that are convertible or easily transferable from the green to the clubhouse are one of the most important innovations that have helped shape the sneaker culture within golf. For fashion-conscious individuals, half-inch spikes on the bottom of a sneaker can significantly alter the shoe’s aesthetic. Brands are therefore increasingly opting for subtle traction at the bottom of their golf shoes instead of straight spikes.\n“There were so many people who bought the golf product collaborations but didn’t even play the game,” Humphrey said. “My phone rang more for the Eastside Golf collaboration than for some of the projects we did with Christian Dior. The sport is looking for a new dose of energy and this was a great way to introduce something new to it.”\nOn tour, eagle-eyed golfers or sneaker collectors may have seen these shoes on the feet of Bubba Watson, 43, or Harold Varner III, 31, but even younger pros bring a different swagger to the PGA Tour, Slade, the sneaker YouTuber, said. Many of the players now on tour, he said, “grew up listening to Travis Scott or Tyler the Creator. They come into this world with a completely different perspective.”\nLast summer, Amin’s boutique, Extra Butter, teamed up with Adidas to create a streetwear golf collection inspired by the “Happy Gilmore” movie, which includes golf shoes, sneakers, balls and putter covers. The store is also introducing new golf-based brands to its inventory, such as Radda, Whim and Manors Golf.\n“From the dawn of hip-hop culture, there has always been an air of wanting to represent what you aspire to,” said Bernie Gross, Extra Butter’s creative director. “We have a background that doesn’t represent this, but this is what we hope to achieve one day. Golf is part of that.”\nRappers are also getting into the golf business. Drake teamed up with Nike to launch a 10-piece golf collection worn by four-time major champion Brooks Koepka. And Macklemore, the Seattle-based rapper, launched his own wave line — called Bogey Boys — in February 2021.\nMacklemore only started playing on holiday two and a half years ago and was immediately hooked. But even before hitting his first 5-iron out of the fairway bunker, he was already looking for classic 1970s golf looks. He started his independent golf brand because he saw a market for new players who wanted a unique style on their appearance on the course. to give.\nSince its launch just over a year ago, Bogey Boys, whose look is inspired by the swag of golfers like Arnold Palmer and Lee Trevino, has sold out its first collection of limited-edition products along with Nordstrom and opened its first store location in Seattle in September.\nBut beyond collectability, style and functionality, Eastside Golf’s founders believe there are more possibilities for the conventional sport.\n“Golf can learn from sneaker culture,” Cooper said. “Sneaker culture is all about individuality. That’s what golf was missing.”']	['<urn:uuid:dd28dd4a-a578-487e-855c-210c72d447c3>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	10	26	1312
90	how is mainz important for german wine industry what are classifications of german wine quality	Mainz is a major hub for the German wine industry, serving as a wine trade center and home to the state's wine minister - the only state with such a department. The city is a member of the Great Wine Capitals Global Network and hosts many wine merchants, producers like Kupferberg, and important institutions like the German Wine Institute. The Mainzer Weinmarkt is one of Germany's most significant wine fairs. As for German wine classifications, they are categorized by ripeness at picking: Tafelwein (table wine), Qualitätswein/QbA (quality wine), and Qualitätswein mit Prädikat/QmP (quality wine with distinction). The QmP category includes subcategories in ascending order of ripeness: kabinett, spätlese, auslese, beerenauslese (BA), trockenbeerenauslese (TBA), and eiswein.	"['Mainz – Info Card\n|FOUNDED :||13/12 BC|\n|TIME ZONE :||CET/CEST (UTC+1/+2)|\n|AREA :||97.75 km2 (37.74 sq mi)|\n|COORDINATES :||50°0′N 8°16′E|\n|SEX RATIO :||• Male: 49,1%|\n• Female: 50,9%\n|AREA CODE :|\n|POSTAL CODE :||55001–55131|\n|DIALING CODE :||06131, 06136|\nClimate of Mainz\nThe climate in Mainz is oceanic (Köppen climatic classification Cfb).\nGeography of Mainz\nMainz is situated on the 50th latitude, on the west bank of the Rhine, opposite the Main-Rhine confluence. In early 2012, the population was 200,957; an additional 18,619 individuals live elsewhere but have a second home in Mainz. The city is part of the Rhein Metro region, which has a population of 5.8 million people. Mainz can be reached in 25 minutes by commuter train from Frankfurt International Airport (Line S8).\nMainz is a Rhine river port city that links the Rhine and its major tributaries, such as the Neckar, Main, and, later, the Moselle, and thus continental Europe, with the Port of Rotterdam and therefore the North Sea. Mainz’s history and economics are inextricably linked to its proximity to the Rhine river, which has traditionally handled the majority of the region’s waterborne commerce. Today’s massive container port hub, which allows for trimodal transportation, is situated on the town’s North Side. Another benefit of the river is that it moderates Mainz’s temperature, making riverfront districts somewhat warmer in the winter and cooler in the summer.\nSand dunes were created in the Rhine valley during the last ice age, forming the city’s western outskirts. The Mainz Sand Dunes area is currently a wildlife reserve with a one-of-a-kind scenery and raresteppe vegetation for this region.\nThe houses along the Rhine River stood for the civic settlement of the modern city when the Mainz legion camp was formed in 13/12 BC. Historical sources and archaeological discoveries both demonstrate the military and civilian significance of Mogontiacum as a Rhine port city.\nEconomy of Mainz\nAs a wine trade hub and the site of the state’s wine minister, Mainz is one of the German wine economy’s focal points. Rhineland-Palatinate is the only state with such a department due to the significance and history of the wine business for the federal state.\nThe city has also been a member of the Great Wine Capitals Global Network (GWC) since 2008, an alliance of well-known wineculture-cities throughout the globe. Many wine merchants are also employed in the town. Kupferberg, a sparkling wine maker based in Mainz-Hechtsheim, and Henkell, which is currently based on the opposite side of the Rhine, were both started in Mainz. The famed Blue Nun, one of the earliest branded wines, was promoted by the Sichel family.\nMainz has been a wine producing location since Roman times, and the tourism center promotes the image of Mainz as a wine town. The Haus des Deutschen Weines (English: House of German Wine) is just next to the theater. The German Wine Academy, the German Wine Institute (DWI), and the German Wine Fund are all based there (DWF). The Mainzer Weinmarkt (wine market) is one of Germany’s most important wine fairs.', '|Wine is the healthiest and most health-giving of drinks. - Louis Pasteur|\n|Home Wine Tasting Wine Tasting Terms Food & Wine Wine Glossary Wine Facts Wine Quotes Wine in Our Culture|\nGerman wines are categorized according to ripeness at picking. The minimum levels of ripeness for each category vary by region, but the basic categories are Tafelwein (""table wine""); Qualitätswein, or QbA (""quality wine""); and Qualitätswein mit Prädikat, or QmP (""quality wine with added distinction""). Within the latter category the distinctions are (in ascending order of ripeness) kabinett; spätlese; auslese; beerenauslese, or BA; trockenbeerenauslese, or TBA; and eiswein. Under certain climatic conditions, the grapes may be affected by Botrytis cinerea, a desirable, flavor-enhancing fungus known in Germany as Edelfäule. Although they may contain residual sugar, German wines tend to be richer as one tastes through the categories of distinction; not until beerenauslese is sweetness enough of a dominant factor for a wine to be considered a dessert wine. At all levels German wines are balanced by high acidity, so they do not necessarily taste sweet. Above all, you should look for balance among a wine\'s components.\nGermany produces a compelling variety of mostly white wines from 13 wine-growing regions (including two from the former East Germany). Selecting a German wine appears to be a daunting task, due to the large number of types and styles, and confusion resulting from label nomenclature. By using a few basic guidelines, selecting a German wine is not difficult, given the generally high quality standards most producers uphold. The key factors are finding the appropriate style(s) for your taste and producers you can depend upon.\nUndoubtedly the best German wines are made from Riesling. This white grape is capable of developing intense flavors at lower ripeness levels, making it an ideal cultivar for Germany\'s northern climate. Under the right weather conditions, Riesling will ripen late into autumn, rendering late-harvest styles. When combined with an attack of Edelfäule , these late-harvest grapes produce some of the most stunning and longest-lived wines around. The best values in Rieslings ($12 to $20) are those at lower ripeness levels -- such as QbA, kabinett and spätlese -- and originating from a single vineyard site, such as Piesporter Goldtröpfchen. However, many quality producers are bottling estate Riesling from a number of their sites, simply calling it ""Riesling Pfalz"" to simplify labeling. Yet others are blends of different sites bottled under collective names, or Grosslagen. A couple of examples are Zeller Schwarze Katz or Bernkasteler Kurfurstlay.\nThe bulk of the least expensive German wines are usually either table wine (a category that in Germany includes both tafelwein and landwein) or QbA, and may be blends of two or more grape varieties. Black Tower and Liebfraumilch are two examples. German wine law requires a varietally labeled wine to contain at least 85 percent of the specified grape variety. Varieties commonly blended with Riesling are Sylvaner, Muller-Thurgau and Elbling. These wines often cost $10 or less.\nMuller-Thurgau is the most widely planted white variety in Germany and is capable of producing good value wines if the yields are low enough to coax some character out of the grapes. Riesling has the next largest area under cultivation. Other white grape varieties include Kerner, Scheurebe, Bacchus and Pinot Gris (Ruländer). Minuscule quantities of Gewurztraminer, Pinot Blanc (Weissburgunder), Sylvaner and experimental crossings with exotic names like Huxelrebe and Ortega are also grown. There is some red wine production, mainly from Pinot Noir (Spätburgunder), but the values are more likely to be from varieties such as Blauer Portugieser, Trollinger and Lemberger or blends of the latter two varieties.\nWith German wines, the region often provides an indication of style. Those from the Mosel tend to be the most delicate, often exuding characteristic lime, apple and mineral aromas and flavors. Rheinhessen wines are rounder and fruity, with the best hailing from the steep vineyards overlooking the Rhine river, while Nahe wines fall somewhere between the two. Rheingau produces firmly structured, spicy wines that often need a few years to shed their youthful austerity. The richest, fullest versions of white German wines come from the Pfalz.\nHow To Buy Wine Online\nAbout Fine Wine Auctions\nChampagne Beaujolais Wine Table vs Fortified Vintage Wine Chart\nWine Making Wine Grape Varieties\nStoring Wine Wine Bottles Wine Labels Wine Glasses\nOld vs New World Wine 1855 Classification Wine Countries The Wine Pact More About Wine\nGo To Related Links Find The Site Map Send This To A Friend Send Us Your Comments Link To Our Site\nReport A Broken Link To Us Contact Information\nSite Map |\nPrivacy & Security |\nContact Us |\nPurchase Agreement |']"	['<urn:uuid:164e6205-568c-4553-bda3-a6931f7c436a>', '<urn:uuid:8ece8ca4-e3ee-47b5-b3ff-3b3761cd2659>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	15	115	1282
91	compare control functions elevators vs rudder aircraft control	Elevators and rudders have different control functions in aircraft. Elevators are on the horizontal tailplane and control pitch by moving up/down, affecting the aircraft's vertical orientation. The rudder is the vertical tail component and controls yaw by moving left/right to redirect the aircraft's nose horizontally. Both utilize lift principles but in different directions - elevators work about the lateral y-axis while the rudder operates around the vertical z-axis.	"[""3. CONTROL SURFACES\nAll control surfaces utilize the principle of lift, but they apply lift forces in different directions.\nThese forces act either independently or in conjunction with one another to producevarious maneuvers. Each maneuver is the net resultant force of all individual forces. (A resultant force is the average force that results when two forces are combined. For example, a pure vertical force and a pure horizontal force create an angled force.)\nElevators are flat, hinged surfaces on the tailplane (the horizontal part of the tail assembly). While the entire tailplane surface helps stabilize the aircraft during flight, the elevators apply pitch by angling the trailing (rear) edge of the tailplane up or down.\nTo create pitch, gently pull the flight stick back or push it forward. Take care not to perform pitch maneuvers too quickly. If the angle of attack (angle that the air meets the wing) becomes too steep, the flow of air around the wings can become disrupted. Air no longer flows smoothly over the wing; instead, it buffets in several different directions and disrupts the air pressure around the wing's surface. This situation is called a stall.\nStalls can also occur from lack of airspeed, when not enough air flows over the wings to create lift. This is commonly encountered in propeller-powered aircraft, especially during steep climbs in which gravity reduces airspeed. Note that climbing steeply is not the same thing as pitching up too quickly. The former type of stall is caused by lack of airspeed, while the second type is due to disrupted airflow around the wing.\nThe rudder is the vertical component of the tail assembly. The rear half of the vertical tail section is hinged, allowing it to angle left or right. When you apply rudder, you redirect the aircraft's nose either left or right. Applying left rudder yaws the nose to the left, while applying right rudder veers the nose to the right. Note that applying rudder also produces a very slight rolling movement, which can be negated by pushing the stick in the opposite direction.\nAilerons are thin, hinged surfaces on the outer, trailing edge of each wing. They angle in opposite directions to waggle the wings up and down or roll the aircraft about its nose-tail axis. If you apply stick left or right, one wing's aileron angles down and the other angles up. This rolls one wing up and forces the other wing down, effectively rolling the airplane.\nWhen you apply left stick, the left aileron raises and the right one drops, and the aircraft rolls to the left. The opposite occurs if you push the stick in the opposite direction.\nSimilar to ailerons, flaps are thin, hinged surfaces on the trailing edge of the wing. However, they are located nearer to the wing root than ailerons and operate in tandem. (If one flap is lowered or raised, so is the other.) A raised flap conforms to the wing's natural shape. A lowered flap alters the airflow around the wing, effectively changing the wing's aerodynamic shape and increasing the amount of available lift.\nYou extend flaps during takeoff to gain additional lift, then retract them during flight to maximize your airspeed. While flaps increase your aircraft's angle of attack, they also increase drag. In a pinch, you can use flaps while chopping the throttle to quickly reduce your airspeed.\nOne point to note is that flaps can only be extended at low to medium speeds. If the aircraft is traveling too fast, air flows too fast over the flaps, and they cause drag. In high-speed dives, flaps and other control surfaces may become unusable—air travels so fast over them that you can't move them until you slow down the aircraft.\nCompressibility is a condition that renders an aircraft's control surfaces inoperable. It occurs at very high speeds, such as those attained during a long, steep dive. Air that flows around the airfoil surface separates into two directions at some point in front of each wing. This is called the point of impact.\nAt higher speeds, this point moves further and further in front of the wing and creates pressure disturbances on and around the wing. As an aircraft's speed approaches Mach 1, the speed of the air flowing over the wings reaches the speed of sound before the aircraft does. Remember, air flows faster over the top of the wing and is actually traveling faster than the aircraft at any given point in time.\nPressure waves generated by the movement of wings through the air act much like ripples on a pond. They radiate outward and “warn” the yet undisturbed air molecules in the path of the approaching wing. As the aircraft's speed approaches Mach 1, these pressure waves pile up in front of the wing. (The Mach number is the aircraft's speed divided by the speed of sound for the current altitude and temperature.)\nAt some point, the wing is traveling so fast that the waves no longer radiate ahead of the wing. This creates shock waves and causes the aircraft to buffet. Aileron and elevator controls mounted on the wing and tail surfaces freeze up due to excessive pressure, or act in directions opposite than normal. The phenomenon of compressibility occurs only at very high speeds. The only remedy in WW II aircraft is to chop the throttle and attempt to pull out before it's too late. If you don't react quickly enough, your control surfaces may freeze and you could crash."", 'The aerodynamic design of an air vehicle, apart from the study and analysis of the aerodynamic behavior of its parts (wing, fuselage, tail), singleton or combined, includes the study of its dynamic behavior as well, by sizing the control surfaces. This task is undertaken by the control subsystem of the aerodynamics subteam.\nThe control surfaces are categorized by the rotational axis which they control, and they are parceled out at different parts of the aircraft. The main control surfaces are positioned at the trailing edge of the wing or tail. Τhe ailerons are adjunct to the wing. On a conventional type of empennage, the elevator and the rudder are adjunct to the horizontal and vertical stabilizers, respectively.\nThe ailerons are the “steering wheel” of the aircraft. They pursue its lateral control, which is the rotation about the longitudinal axis. This movement is called “roll”. The elevator, which is the pivotal control surface, controls the aircraft’s vertical orientation. This kind of orientation is called “pitch” and it stands for the angle between the longitudinal axis and the horizontal plane. Lastly, the rudder is responsible for the vehicle yaw control. This way, it can easily bring the aircraft into alignment, even under a crosswind effect.\nFrom the aspect of engineering, aircraft control refers to the moment adjusting during various phases of the flight. To properly define a moment, the following need to be defined first: the point on which the moment exerts, the axis on which it acts, the force which causes it and the distance between the moment point of reference and the force point of reference.\nFor the case of control surfaces the aircraft center of gravity acts as the moment point of reference, the axes are the x, y, z-axes and the distance between the center of gravity and the other points are determined each time by the related force. Every control surface corresponds to a different axis. The ailerons act on the x-axis, which is the aircraft longitudinal or fuselage anterior-posterior axis. The elevator controls the moment about the y-axis or lateral axis. This axis the rudder is responsible for is the z-axis or vertical axis.\nThe control of the aircraft moments is achieved by defining the right deflection of the control surfaces. The deflection position modifies the lift magnitude which is produced by the respective surface. The aim of this control is to achieve the right moment calibration such that it can control the aircraft orientation. In particular, for the aircraft to execute a level cruise flight, the sum of the moments about the three axes must be equal to zero. The control surfaces position where this condition can be achieved is called trim position.\nΤhe aerodynamics engineers of the project, through a devoted study, had to understand the logic behind the aircraft dynamic behavior, which is modified by such additions. Based on purposeful search on bibliography, they created tools that predict the fitting coefficients and the ideal dimensions that will match the new aircraft layout which must serve its target mission.']"	['<urn:uuid:3b955bcc-10a9-4d45-9b9e-e2cfda9e8be5>', '<urn:uuid:ada3ad87-0a2d-4e6c-877b-13d1ae2c3e50>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T00:43:36.053682	8	68	1422
92	What's easier to grow - regular Aloe or Agave plants?	Regular Aloe is easier to grow than Agave. While Agaves are trendy, they require Zone 10 conditions and need protection from both cold and rain. In contrast, Aloe is the hardiest of these desert plants, with some varieties able to survive outdoor winters in Zone 8. Aloes are also more forgiving, as they can thrive even with some neglect and are generally easier to care for.	"['The sweetest spring just might be this one. Breaking out of the covid bubble just as the South Sound undergoes its annual color explosion makes Spring 2021 doubly appreciated.\nIf there was ever any doubt about the importance of gardening in the South Sound…it has been put to rest. Nurseries were considered “essential”and therefore so was gardening. A record number of vegetable gardens were started and a record number of seeds were sown.\nAmid all of the frenzied vegetable growing there has been an undercurrent of interest in the plants usually reserved for the southern most part of North America…plants like…\nAgaves, Aeonium and Aloe\nAgaves are native to the Southwest. You wouldn’t think they would grow in the South Sound but Agave parryi will survive and thrive in our winters. The slow growing gray green succulent leaves initially grow to 2 feet tall and wide. In their native habitat, they can grow to 20 feet. They need to be protected from cold and rain so, if you like a challenge…most require Zone 10\nAeonium resemble a smooth, fleshy petaled daisy and are a few zones hardier than Agave. They are also a little easier to find. They are collected for their maroon, green and yellow colored fleshy leaves. You might find more Aeonium in a houseplant section… Zone 9\nAloe is the hardiest of the three. Everyone knows about the fleshy Aloe vera used as a handy burn remedy. Many more Aloe varieties can make it outside through our winters. Aloe aristata is a dark green, compact, aloe that has made it through the last 3 winters in my yard in a terra cotta pot. As with most of the succulent like plants, it’s our rain that does them in, not our temperature…Zone 8\nThese trendy plants, by nature, are harder to find and more challenging to grow. They are trendy experiments. If the “look” of these desert plants is something you like, you can ease into growing “like” plants by substituting…\nYucca, Soft Succulents and Sempervivums\nYucca filamentosa (Adam’s Needle) have spiky, treacherous leaves like Agave with the added benefit of 6 foot flowers. They do not come in the beautiful blue gray of some of the most striking Agave but they are easy to find and easy to grow…Zone 7\nSoft Succulents are the gentle, un-spiky succulents that come in all of the same colors as Aeonium, They have similar texture, similar colors and unless they’re waterlogged, most will make it through any of our winters. Zone 5\nSemperivivums are “hens and chicks”. They are an easy to grow substitute for the harder to find Aloe species. They can handle our cold as long as they have good drainage. Our rain doesn’t bother them.\nWhichever way you go…trendy or traditional…Break a few rules and succumb to some…\n“Fearless Gardening” by Loree Bohl\nYou may recognize Loree from, Plantlust.com, “a seriously simple search for plants”, her popular blog, “Danger Garden” or her social media presence featuring her unusual desert garden in the middle of Portland, OR. She grows plants fearlessly.\n“Fearless Gardening” is written to motivate and encourage gardeners to break some rules about plant material. Her “ten commandments of gardening” list combines truths with humorous myths. Number 2 is “thou shalt not purchase plants on impulse”. Yeah, right.\nA major takeaway from Loree’s “Fearless Gardening” is to dive right in and try anything…even a desert garden in the Pacific Northwest.\nTo see a local version of what you can do with a desert look; take a trip to the ever-expanding horticultural wonderland at the Point Defiance Zoo. You won’t be disappointed.\nTimber Press, Jan, 2021, 256 p., $24.95,', ""The Aloe Vera plant generally has a lifespan of 12-14 years. They are easy to care for and can thrive in neglect. But, continuous negligence can result in death even before its lifespan ends. You must find the reasons behind it and try to revive them back.\nAloe Vera dies due to poor watering habits, inadequate light, poor drainage, lack of nutrition, and cold damage. To revive a dying aloe vera plant, trim off the damaged parts, repot if required, maintain ideal requirements, and don’t stress them further with any unfavorable conditions.\nBoth beginners and experienced gardeners will have this problem at some point. Today, I will explain the reasons behind a dying Aloe Vera plant and how to revive it. So, let’s start without further delay.\nMy Aloe Vera is dying – what should I know?\nAloes thrive in dry areas with sandy soil, bright sun, and infrequent rainfall.\nThey are effortless to grow and are available in many varieties.\nWith proper care and maintenance, they will grow both indoors and outdoors.\nAloes are hard to kill due to their endurance for dry conditions, sunlight, and warm temperature.\nWith 4-6 hours of sunlight, scarce watering, warm temperature, and low humidity, they can flourish easily.\nBut, overwatering them, not giving them enough sunlight, exposing them to high humidity and cold temperatures, and using ordinary soil can deteriorate the health of Aloes and even kill them.\nWhen such conditions remain in or around the plant for a long time, the plant starts to die.\nIt will also show specific signs indicating that they are dying.\nBy identifying the signs and taking appropriate action, you can revive your plant back.\nSigns of a dying Aloe Vera:\n- Discolored leaves\n- Soft and mushy leaves\n- Plant rotting at the base\n- Black spots all over\n- Dry and crispy leaves with discolored edges\n- Drooping plant and dead leaves at the base\nWhy is my Aloe Vera dying, and how to revive it?\nAloe Vera will die due to critical situations like overwatering, root rot, poor drainage, lack of light, exposure to harsh sunlight and cold temperature, etc.\nSometimes, death is natural when they finish their lifespan.\nHere are 11 steps to revive a dying Aloe Vera plant:\n1. Stop watering if the plant is overwatered.\nAloe Veras enjoy dry conditions.\nThey can even stay without water for some days as they store water in their leaves.\nSo they don’t require frequent watering.\nIf you constantly water them without letting the soil dry, the plant will absorb excess moisture and get overwatered.\nMoreover, overwatering Aloes for extended periods will also lead to root rot, and your plant may die if the situation worsens.\nThis occurs mainly with the potted plants because:\n- We cannot see or feel the soil downwards.\n- The pot sometimes doesn’t have drainage holes, due to which even right watering will lead to overwatering.\n- If your plant is indoors, it won’t get enough light as outdoor plants.\nTo revive the plant from overwatering:\n- If the plant has mild overwatering signs, stop watering for some time until the soil dries.\n- Let them have bright but indirect sunlight. Direct sunlight will stress your Aloe veras. But providing sufficient indirect sunlight will help the soil dry faster.\n- Use gritty ingredients like coarse sand, perlite, or gravel to improve the drainage.\n- Allow the soil to dry out between each watering. At least 3/4th portion of the soil around the roots should be dry. Water them only when 2-3 inches of the topsoil is dry.\n- If you must follow a routine, water them once every 2-3 weeks.\n- Watering will depend on the weather. Aloes need less water during the winters and during summer dormancy when the temperature rises above 80-90°F.\nLooking for gardening supplies? We have tested 100's of products before recommending them to you guys. Check out our best pick below:\n|Raised Garden Bed Kit\n|Check On Amazon\n|XLUX Soil Moisture Meter, Plant Water Monitor, Soil Hygrometer Sensor for Gardening, Farming, Indoor and Outdoor Plants, No Batteries Required\n|82 Pcs Garden Tools Set and Extra Succulent Tools Set\n|Check On Amazon\n|Joeys Garden Expandable Garden Hose with 8 Function Hose Nozzle, Lightweight Anti-Kink Flexible Garden Hoses, Extra Strength Fabric with Double Latex Core, (50 FT, Black)\n|Dual Chamber Compost Tumbler\n|Check On Amazon\n|Sunnyglade Plant Stakes\n|Check On Amazon\n|Organic Cold Pressed Neem Seed Oil\n|Check On Amazon\n|Mighty Mint Gallon :-Insect and Pest Control Peppermint Oil\n|Check On Amazon\n|Scotts DiseaseEx Lawn Fungicide\n|Check On Amazon\n|Jacks Classic 20-20-20 All Purpose Fertilizer\n|Check On Amazon\n|30,000 Seeds Pollinator Attracting Wildflower Mixture\n|Check On Amazon\n|Survival Vegetable Seeds Garden Kit-Over 16,000 Seeds\n|Check On Amazon\n2. Repot if the plant shows dying signs from root rot\nDue to the same reasons, the plant will suffer root rot.\nTo save the plant from root rot, you have to repot the plant urgently.\nHere is a step-by-step guide to repotting:\n- Take the plant out of the container and remove the soil from the roots.\n- Remove the dark and mushy roots. You might have to remove all the roots if they are damaged.\n- Now, prepare a new container and new soil. Though you can use the old container by cleaning it with bleaching powder and soap water, it is always better to use a new one. The soil mix must be new.\n- Fill half of the container with the new soil, place the plant, and cover the base with the remaining soil.\n- Water the plant after a few days and don’t expose them to direct sunlight.\n3. Examine the Aloe leaves to check if they are underwatered\nBelonging to the succulent category, Aloe Veras store water in their leaves as a strategy to survive dry weather and high temperatures.\nBut, if it doesn’t receive enough water, the leaves will become droopy and transparent.\nHowever, looks can be deceiving.\nIt could also be a sign of overwatering or root rot.\nDue to suffocation from excess water, the roots fail to pass the water to other parts.\nEventually, the plant looks dehydrated.\nThe difference lies in the texture.\nIn overwatering, the leaves will feel mushy, whereas they will feel dry and brittle in underwatering.\nTo revive the plant from prolonged dehydration:\n- Give your plant a good drink. But don’t create a pool around them.\n- Water them enough until the water drains out of the drainage holes.\n- If it’s in the ground, soak the soil very well.\n- But, if it’s showing signs of underwatering despite current watering, check the soil. Go through the overwatering and root rot point discussed previously.\n4. Move the Aloe Vera to shade or partial shade if it looks sunburned.\nAloe Vera grows well when they receive sufficient sunlight.\nBut exposure to harsh sunlight will lead to sunburn.\nMoreover, the water stored in the leaves will also heat up, increase the risk of burns and make the leaves squishy.\nYellow leaves or brown patches on the foliage and mushy leaves are signs of sunburns.\nIf the problem prolongs, the plant will die.\nTo revive a dying Aloe Vera plant:\n- Fix shading nets over the soil bed if your plant is in the ground.\n- Shift your potted Aloe to a partially shady area\n- Shift your indoor Aloe a few feet back from the window and fix transparent curtains to filter the light\n- Keep your plant near the north or east-facing window as this direction gives less intense sunlight.\n5. Expose the Aloes to more sunlight if they are not getting enough\nGrowing Aloe Vera indoors may give your interior space a good look, but they won’t receive enough sunlight as they do outdoors.\nSince Aloes thrive in desert areas under the sun, sunlight penetrating from the window won’t be enough for them.\nIf your outdoor plant stays in a damp spot for a long time, the leaves will lose their color, the plant will be small, and the soil won’t dry out quickly.\nIt also creates an obstacle in producing chlorophyll and functioning photosynthesis. If the situation continues, the plant can end up dying.\nTo save the plant from dying due to lack of light:\n- Shift your plant to a bright spot. Since the plant has been under shade for so long, don’t expose them to direct sun straightaway. First, let it stay under filtered light for some days, then partial sun for some more days, and then under the direct sun for 2-3 hours only. Over time, increase the timing.\n- If your plant is indoors, shift them near a sunny spot and maintain acclimatization.\n- A south or west-facing window gives too much sunlight, so don’t put the plant near this window during summers. Keep the plant in this direction in winters since the sun level is low.\n- For summers, try the north or east-facing windows.\n6. Replace the soil if it is not draining well and lacks nutrients\nGenerally, the Aloes don’t require highly nutritious soil.\nThey can easily thrive in poor soil. But lack of some nutrients can make your plant die.\nDue to this deficiency, the plant will become pale.\nOver time, the leaves will wither, and the plant will die.\nSimilarly, soil drainage is essential.\nSince they naturally grow over sandy soil that doesn’t hold moisture, you must mimic the same growing medium.\nIf the soil fails in drainage, the roots will remain wet and result in root rot.\nThis will, in turn, kill the plant.\nTo revive the plant from such conditions, you should replace the soil.\nIf the problem is only drainage, add perlite, coarse sand, and gravel to improve the drainage.\nIf the problem is nutrition, you might have to change the soil.\n7. Drainage holes are important for the potted Aloe Veras\nIf you are growing them in pots, there must be drainage holes.\nIt will help the soil drain the excess water between the watering.\nA container with no drainage holes will keep the water stagnant in the soil.\nAs a result, the soil around the roots will remain wet and begin to rot over time.\nUltimately, the plant will die.\nBesides drainage, also ensure the size.\nBoth under-sized and oversized containers will cause problems.\nThe former will make the roots outgrow the drainage holes, compact, and hold moisture, thus resulting in overwatering.\nAn oversized container requires a lot of soil which will take forever to dry.\nThis, too, will keep the soil around the roots wet for a long time and cause roots to rot.\nTo revive a dying Aloe Vera plant due to the wrong type of container:\n- Immediately change the container\n- While selecting a pot, ensuring its drainage system is your first job.\n- Select a container that depends on the size of the plant. It should be 1/3rd bigger than the plant’s root system.\n- To aid in the draining process, use terracotta pots. As it is made of porous materials, it won’t hold moisture.\n8. You must protect the Aloe Veras from the cold temperatures\nBelonging to the warm, arid regions, Aloes are sensitive to cold temperatures.\nThey will get frost and freeze damage.\nThough the former is less risky, freeze damage will kill the plant.\nHowever, there are still chances of revival if you take quick action.\nBelow are some steps:\n- Bring your plant indoors immediately if they are outside in the frosty weather.\n- Maintain a warm temperature near them, at least around 60-75°F. But don’t put them close to heaters, radiators, or ovens. A warm room with an ideal temperature would work.\n- If only a few leaves have turned brown, let the plant dry out, and then you can remove them from the plant. It will encourage new leaf growth.\n- When your plant is in such crisis, you must scale back watering, especially for the outside Aloes.\n- To prevent such damages, let them have enough sunlight throughout the day and give them protection after the sunsets.\n- You can bring them indoors. You can also fix frost clothes.\n- If your Aloe is an indoor plant, keep them near the south or west-facing window for enough sun.\n- Use well-drained soil to help them dry faster. You cannot completely neglect water because hydration is important. Using well-drained soil helps in increasing the drying and draining process.\n- If your area receives frost, you should grow Aloes in containers for bringing them indoors in winters.\n- Cover the soil surface with small pebbles or stones. It doesn’t let the moisture evaporate faster, so you can skip watering for weeks. Additionally, it traps the warmth in the roots and even prevents the cold from reaching and shocking the roots.\n9. Check for pests and diseases.\nProlonged damp and dry conditions combined with a hot or cold climate can result in infestations. Pests like aphids, mealybugs, aloe mites, and spider mites enjoy staying in extremely dry or wet conditions.\nThey suck the saps of the plant and make the plant discolored, pale, weak, lifeless, and prone to diseases.\nAloe mites cause cancerous growth and kill the plant.\nDiseases like Aloe Rust, Bacterial Soft Rot, Leaf Spot Disease, Anthracnose, Base Rot disease, and Basal Stem Rot are fungal diseases in the Aloes.\nIf ignored, the plant will die eventually.\nInfestation is not the end of the plant unless and until the damage level has crossed 50%.\nYou can always bring the plant back with the right and immediate steps.\nTo save a dying plant from pest infestation:\n- Spray neem oil to the infected area to kill them.\n- Dab rubbing alcohol to the infected area to weaken the hard-shelled pests and kill them.\n- For treating pests that love wet conditions, stop watering and expose them to bright filtered sunlight. Once the soil dries, it will leave your plant.\n- For removing the pests that love dry conditions, shower the plant well to remove them from the plant.\n- Use insecticidal soaps and pesticides to treat them if the infestation is too much.\n- You can also release ladybugs and mantis for letting them feed on the soft-bodied bugs.\nTo get rid of diseases:\n- Isolate the plant to prevent spreading.\n- Remove the damaged parts of the plant and discard them in the dustbin. Never throw them near your garden or use them as compost.\n- Use fungicides to eliminate the spores and prevent the fungi from further occurrence.\n- If the disease is at its initial stage, neem oil can help eliminate it in the beginning.\n- You will also have to repot the plant with new soil and container as the old ones got contaminated.\n- You can also use homemade fungicides like copper-based fungicides or the ones made with baking soda.\n- Maintain low humidity around the Aloes and don’t bother them much with watering or direct sunlight until the problem resolves.\n- Avoid splashing water on the leaves while watering as it can increase the humidity and make the plant prone to diseases.\n10. Remove the dead leaves from the plant.\nYou should remove the dead and damaged leaves from the plants on time.\nThese leaves pull lots of nutrients from the soil and the other healthy parts of the plant.\nMoreover, these leaves can infect the other good leaves and slowly kill them.\nLetting the dead leaves stay can also invite pests and fungus.\nTo save the plant from such conditions:\n- Remove the dead and damaged leaves from the plant. It encourages more leaf growth in the plant and fast recovery.\n- But don’t cut off the broken leaves. Let the cut part seal and heal. Over time, it will turn brown. Remove it at that time.\n11. Cut off the healthy leaves for propagation if it’s impossible to save the plant.\nSometimes, the health of the Aloes deteriorates so much that reviving the whole plant becomes impossible.\nBut, you can revive the Aloe Vera by propagation with leaves or pups.\n- Find out for the pups at the base of the plant. Separate them and plant them in a container with new, well-drained soil. Don’t expose them to sunlight for a few weeks until their establishment. Water them after a few days, but sparingly until they have settled.\n- If there are no pups, search for healthy leaves. Trim them off for propagation. There might not be very high chances of successful propagation, but there is no problem trying. After all, hope is everything.\n- Cut the leaves close to the stem and let the leaves heal for a few days. After some days, when you find the cut portion has hardened, plant them in well-drained soil. Water the soil by misting because this won’t let the soil get wet too much. Keep them under bright indirect light. You will find new roots after a few weeks.\n- Propagating might be the only way left to save your Aloe Vera plant. So, try everything and give your best to get it back.\nUnderstanding the steps to revive a dying Aloe Vera plant is easy if you know the problems, their identification, and ways to solve them. You can easily save the plant with a little bit of effort.\nAloe Veras are easy to take care of as they thrive in neglect. But you must give attention to a few things and avoid situations that can kill the plant.\nBut don’t panic if your plant is impossible to save. You can still get your Aloe Vera back through propagation. Find out the pups or healthy leaves and propagate them properly to get a new Aloe Vera.\nAloes survive for 10-12 years. So, they will die once their lifespan is over. There is nothing to do for revival. But, you will get new Aloes through propagation.""]"	['<urn:uuid:3323ec67-2ac1-4c6b-8764-e94954a561f7>', '<urn:uuid:a6e9bd20-6893-40fc-a993-0f1b064d2f90>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T00:43:36.053682	10	66	3602
93	underwater inspection rov maximum depth versus scuba diving depth limits	The CHINOOK ROV can operate at depths of 300m (1,000 ft) with the standard vehicle or 600m (2,000 ft) with the deep water version, while SCUBA diving is limited by OSHA to depths of 130 feet.	['Homepage / About\nThe CHINOOK is an industrial grade inspection ROV with a wide range of capabilities. Explore depths of 300 m (1,000 ft) with the standard vehicle or 600 m (2,000 ft) with the deep water version.\nA true workhorse, the CHINOOK offers a stable platform yet remains compact. With its small profile, the CHINOOK can operate in tight places and is easily stowed when not in use. Multiple handles and frame cross members allow two people to handle this ROV with ease.\nFor greater versatility, customize your CHINOOK by adding accessories such as extra thrusters, navigational and search- ing aids, manipulators, tool skids and more.\n• Lightweight and portable\n• Runs off of standard mains AC (110 or 240 VAC)\n• Can be operated from the shore, a dock, or a relatively small boat\n• Easy deployment, operation and recovery\n• Open frame and modular components\n• Rugged construction; field-repairable twisted pair tether, or, high-bandwidth fiber-optic tether • Delivered in four portable PelicanTM cases\n• Customizable platform for tooling and sensors\n• UPGRADES AVAILABLE: Scanning sonar, multibeam sonar, and more!\nTARGET INDUSTRY USES FEATURES\n• Forensics/Search and Recovery • Hydro Dam Inspection\n• Marine Security\n• Pipeline Inspection\n• Potable Water\n• Marine Research\n• Structural Analysis\n• Salvage Operations\n• Intuitive controller\n• High-resolution color, zoom camera\n• Variable speed & directional ROV control\n• Integrated controller with LCD monitor & Digital Video Recorder (DVR)\n• Manual and automatic camera focus control\n• Depth-rated to 300 m (1000 ft) or 600 m (2,000 ft) with deep-water model\n• Up to six (6) powerful thrusters\n• Auto depth, auto heading functions • Leak detection warning system\n• Electric wind tether reel\n• Sonar integration\n• 3D positioning and tracking system integration\n• Auxiliary LED lighting\n• Dual-function gripper (open/close and jaw rotation) and/or cutter • Skid-mount instrumentation or tooling platforms\n• CT / UT probe integration\n• Auto altitude\nROV systems are used extensively worldwide by the police, coastguard, military, television companies, and the offshore industry.\nOur ROV and team are ready for immediate mobilization 24 hours per day, 365 days per year.\nIn many areas, engineering inspections and maintenance surveys are dreaded and often put off for lack of funding. Sending divers into structures, pipelines, drinking water tanks, etc has long been the practice for inspecting the structures, but it is dangerous and can pose health risks. Now, more clients are looking into alternative methods to conduct inspections and surveys that can co-exist with divers and make photographic and video inspections safer, more affordable, and more frequent.\n“This mini remote control underwater vehicle is the most effective, safest and economical way to carry out subsea inspection work today”\nWe have the answer...\nHigh-Resolution Side Scan Sonar Surveys\nThe StarFish range has gained worldwide recognition as the most compact, affordable, towed side scan sonars for the professional underwater survey industry. The full-body, three-fin, hydrodynamic design of the StarFish systems challenge the common cylindrical or torpedo-shape of traditional side scan sonars. The unique shape of the StarFish systems dramatically improves the stability of the sonar and consequently the quality of the images it produces. Furthermore, the advanced digital CHIRP (Compressed High Intensity Radar Pulse) acoustic techniques used by StarFish systems offer better range resolution compared to conventional, fixed-frequency side scan sonars. Building on the success of the original StarFish 450F, the StarFish 990F is the ultimate high-resolution side scan sonar for extreme image definition and target detection. The sonar uses a high frequency 1MHz CHIRP transmission with extremely narrow horizontal acoustic beam to provide the sharpest, most defined images. With 35m range capability on each channel (70m total swathe coverage), the sonar is ideal for high-resolution survey in ports & harbours, inland waterways such as rivers and canals and excels when used for SAR (Search And Recovery/Rescue) operations.', 'Underwater Inspection and Evaluation\n“A Critical Component” for Preventative Maintenance, Operation and Relicensing\nWilliam J. Castle, P.E.\nW.J. Castle, P.E. & Associates,\nP.C. 693 Main Street, Building B, Suite 1\nLumberton, New Jersey 08048 USA\nE. Levels of Inspection\nThere are three basic types or levels of inspection used while inspecting dam facilities:\nLevel I – General Visual\nThis type of inspection involves no cleaning of any structural elements and therefore is the most rapid of the three types of inspections. The purpose of the Level 1 inspection is to confirm as-built structural plans, provide initial input for an inspection strategy, and detect obvious major damage or deterioration due to overstress, impacts, severe corrosion, or extensive biological growth and attack.\nLevel II – Close-Up Visual\nThis level is directed toward detecting and identifying damaged/deteriorated area which may be hidden by surface biofouling or deterioration and towards obtaining a limited amount of deterioration measurements. The data obtained should be sufficient to enable the gross estimates of the facilities load capability. Level II examinations will often require cleaning of structural elements. Since cleaning is time-consuming, it is generally restricted to areas that are critical or which may be representative of the entire structure. The amount and thoroughness of cleaning to be performed is governed by the necessity to determine the general condition of the overall facility.\nLevel III – Highly Detailed\nThis level will often require the use of Non-destructive Testing (NDT) techniques. It may also require the use of partially destructive techniques such as core sampling into concrete and wood structures, physical material sampling, or surface hardness testing. The purpose of this type of evaluation is to detect hidden or interior damage, loss in cross-sectional area, and material. A Level III examination will usually require prior cleaning. The use of homogeneity NDT techniques are usually limited to key structural areas, areas that may be suspect, or to structural members which may be a representative of the underwater structure. Level III inspections will require considerably more experience and training than Level I or Level II inspections and should be accomplished by qualified engineering or nondestructive testing personnel. This type of inspection is classified as a specialized inspection within the United States. (U.S. Navy Standards)\nSpecial Testing, Level III Examination\nThe types of testing described in this section are among those which may also be used for Level III examinations. Level III examinations are employed when Level I and Level II examinations cannot conclusively determine the structural condition of the underwater items. Findings of previous inspections or the age of the structure may also dictate the level of examination needed. A Level III examination is not required for all inspections.\nIn steel structures, the inspector is often concerned with measuring the remaining thickness of corroded members. This can be done with a graduated scale, caliper and ultrasonic thickness measuring devices.\nGraduated Scales – For measuring the exposed edges of flanges, a rule, or graduated scale is the most basic tool.\nCalipers – Another simple method of thickness measuring is to use a set of calipers. Calipers are compact and easy to use under most conditions.\nUltrasonic Measuring Devices – Ultrasonic devices are also available for measuring remaining steel thickness. The device sends a sound wave through the member. It then measures the travel time of the sound wave and calculates the thickness of the steel. An advantage to this device is that it only needs a transducer to be placed on one side of the member.\nSeveral nondestructive tests can be performed on concrete; however, the nondestructive testing instruments must be modified for underwater use.\nV-Meter – The V-meter is an ultrasonic testing device. Using ultrasonic to check the condition of materials such as concrete requires two transducers. When taking measurements, the transducers can be arranged in three different positions. The direct transmission method provides the most accurate data. The semi-direct and indirect methods require correction factors to interpret the data. The V-meter measures the time it takes a sound wave to pass through a material. Location of discontinuities in the material, such as cracks and voids are determined by abnormal velocities. Data has to be interpreted by a trained technician or information may not be accurate. The Schmidt Hammer is a mechanical device which measures the compressive strength of in-place concrete. For underwater use, the hammer is placed within a housing and the equipment modified somewhat, including a special scale.\nCoring is a partially destructive test method. It can be used alone or to verify and correlate data from non-destructive test methods. Cores obtained underwater can be tested in a laboratory in accordance with standard procedures. The actual levels of inspection to be used for a particular task must be decided early in the planning phase. This cannot be overemphasized, because the time and effort required to carry out the three different levels of inspection are quite different. The time required will also depend on environmental factors such as visibility, currents, wave action, water depth, severity of marine growth, and the skill and experience of the inspector.\nInspector-divers usually move from dive site to dive site. Often divers must enter waters of unknown quality. Local water quality monitoring agencies should be contacted to determine the degree of hazard the water presents, and appropriate precautions must be taken. These may include additional immunizations and the employment of special diving equipment to provide complete diver encapsulation. In some cases, it may be necessary to obtain water samples and have the samples tested prior to diving. Diving managers must ensure that proper precautions are observed. The long term effects on the diver of these and other occupational hazards may result in future liabilities.\nF. Modes of Diving & Inspection Equipment\nWithin air diving, two principle modes are used: SCUBA, in which the diver carries his air supply with him in a tank, and surface-supplied diving in which the diver’s air source is in a boat or on shore. While some organizations may be predisposed to one mode over the other, both modes are permitted by OSHA standards and both have a place in dam inspections. In some situations, one mode may have significant economic benefits over the other, while providing all the inspection information required without in any way compromising safety. Their appropriateness for any specific diving situation depends on a number of factors including depth, bottom time, and the experience and capability of the diver. Each mode has unique operational advantages and disadvantages.\nScuba is an acronym for Self-Contained Underwater Breathing Apparatus. Scuba is generally recognized today in the open-circuit form: air is inhaled from supply tanks and the exhaust is vented directly to the surrounding water.\nSCUBA is well suited for inspection work because if its portability and ease of maneuverability in the water where there are many dives of short duration at different depths than one sustained dive. SCUBA equipment weighs about 75 pounds and requires no elaborate support operation. It has the advantage that the diver does not have to drag an air hose behind him. The use of SCUBA is limited by OSHA to depths of 130 feet and the bottom time is limited by the amount of air the diver can carry with him.\nSurface to diver communication is possible using SCUBA with either hardwire or wireless systems. Communication may be desirable in deep water or for complicated structures. Wireless systems are also available which allows for greater mobility.\nSurface Supplied Air\nThere are two types of surface-supplied equipment: deep-sea (hard-hat) and lightweight. Deep-sea equipment consists of a helmet and breastplate, diving suit and weighted shoes. The equipment worn by the diver alone can weigh more than 200 pounds. Add to this the air compressor, hoses, lines and possibly a diving launch to work from, and the problems of mobility and transportation become significant. This equipment is cumbersome, and generally not considered economical for most modem diving operations since the development of lightweight equipment. The deep-sea hard-hat has changed little in the last 150 years.\nLightweight equipment usually consists of a full face mask or helmet, safety harness, weight belt, boots or fins, back-up air supply, and an exposure suit (a wet or dry suit). Early helmets were free-flow air hats in which a constant stream of air was supplied to the diver. Today, demand regulators similar to those used in the second stage of SCUBA equipment have been incorporated in helmets and full face masks.\nIn a surface-supplied system, the air is supplied by a high volume, low-pressure compressor or from a bank of high-pressure cylinders equipped with a regulator to reduce the high pressure.\nThe diver’s umbilical (Life-Line) is a combination air hose, safety line, communication cable, and pneumofathometer hose.\nThe primary advantage of surface-supplied air diving is the “unlimited” air supply. Longer bottom times can be obtained on surface-supplied dives if decompression schedules are used. Dives in accordance with OSHA standards can be conducted to a depth of 190 feet or, if bottom times are less than thirty minutes, to depths of 220 feet. The major disadvantage of Surface Supplied diving is the lack of mobility. Inspection work generally requires the diver to constantly change depth or travel around structures or obstacles. In doing so, the diver using Surface Supplied equipment may become entangled in his umbilical. As a minimum, he has the added effort of dragging it after him.\nThis is part 2 of a 4-part series.\nYou can find part 1 here.\nRead part 3 here.\nRead the 4th of this 4-part series here.']	['<urn:uuid:1cfa49b7-e046-4606-8e68-1c50473332df>', '<urn:uuid:92538add-1520-4508-875a-4a2f7a29a361>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T00:43:36.053682	10	36	2229
94	diagnostic vs cohort evaluation key features for understanding patterns	Diagnostic evaluation and cohort evaluation have different approaches to understanding patterns in data. Diagnostic evaluation focuses on investigating why something happened through data exploration and correlational strategies, using drilling down techniques to concentrate on specific aspects and data mining to extract information from unstructured data. On the other hand, cohort evaluation groups data sets together based on shared attributes to analyze historical data, particularly useful in market research to understand market practices and business activities' impact. While diagnostic evaluation seeks to elucidate reasons behind specific outcomes, cohort evaluation organizes visitors or users into specific groups (cohorts) based on criteria like demographics or common characteristics to track behavioral changes over time.	"['The most crucial part of research is to evaluate the result of a study or research. The evaluation process must be rigorous to analyze the findings in the light of research questions. Every researcher should be aware of the methodologies to evaluate the study findings. This article will tell you about the best techniques to evaluate the results of a study for your research.\nWhat Are The 4 Types Of Evaluation?\nFollowing are the four types of evaluation:\nThe descriptive analysis evaluation approach is the foundation of any analytic reflection. It seeks to answer the question, “What did happen?” It accomplishes this by organizing, processing, and analyzing raw data from various sources. Descriptive evaluation transforms the results of a study into valuable insights. Data evaluated through descriptive analysis in market research is valuable for building strategies.\nDescriptive analysis is necessary because it helps us convey our findings in an understandable manner. However, it is important to note that this analysis will not be able to forecast future events. It also does not provide insights into the causes of a happening. But this approach will organize your data and make it suitable for further research. Descriptive evaluation is a common technique in quantitative research to evaluate result of study. Researchers use descriptive evaluation to describe and categorize the responses gathered through surveys. Descriptive evaluation pertains to the following categories:\nThe centre or average values of data sets are the focus of measures of central tendency. In contrast, the dispersion of data is the focus of measures of variability. The usage of charts, graphs, and generalized explanations are common in these two measures. It assists the readers to grasp the significance of the study findings.\nThe central tendency evaluation method describes the central location of data in the dataset. It utilizes mean, median and mode to assess the prevalent patterns in the study findings. Moreover, it also helps the researchers explain the occurrence of every data point in the data distribution. However, most of the students who don’t have any idea about this category prefer to get research proposal help\nthoroughly to evaluate result of study or research.\nMeasures of Variability\nMeasures of variability, often known as spread measures, are beneficial to determine how scattered a collection of data’s distribution is. It is opposite to the central tendency method. Central tendency measurements may provide a researcher with the aggregate of data collection. But it does not inform them about the distribution patterns of data.\nMeasures of Position\nThe descriptive analysis also entails determining the relative location of a particular value or response to others. Statistics such as percentile rank and quartiles are quite valuable in quantitative research.\nThe exploratory evaluation’s principal goal is to investigate, as the name implies. There was still no understanding of the link between the dataset and the variables before it. After a researcher reviews the data, he or she may use exploratory evaluation to make connections. It helps the researcher establish patterns in the data and develop ideas and solutions to specific issues. Data mining is a typical example of exploratory evaluation.\nDiagnostic evaluation is one of the evaluation methods to evaluate result of study. This method enables the researcher to evaluate why did something happen? It’s a thorough investigation into your data in quest of useful information. The first phase in research evaluation is descriptive analysis, which is a simpler procedure. Descriptive analysis evaluates and records the occurrences. Diagnostic evaluation is an advanced step that elucidates the reasons behind specific outcomes.\nDiagnostic evaluation utilizes data exploration and correlational strategies to evaluate data. Analysts pick data sources that will aid them in interpreting the outcomes throughout the discovery phase. Drilling down entails concentrating on a particular aspect of the data or a specific component. Data mining is a computerized method of extracting information from a large amount of unstructured data. Finding regular patterns in the data set can also assist researchers in determining the scope of the inquiry.\nPredictive evaluation of the data set pertains to the predictive quality of the gathered data. It helps the researchers in addressing the question, “What is the likelihood of something happening in the future?”. This form of analysis is a level up from descriptive and diagnostic evaluations. Predictive analysis employs the information researchers gather to develop rational predictions about what will happen next. This evaluation utilizes statistical modelling. It necessitates the use of additional technology and labour to forecast. It’s also crucial to remember that forecasting is merely a guess. The accuracy of forecasts depends on high-quality and thorough data.\nHow Do You Evaluate Result of Study or Research?\nBefore publishing the findings, the researcher should devote some time to a particular phase of research called evaluation. He should evaluate the results by asking the following questions\n- Are the results consistent with the initial objectives of the study?\n- Are there any variations and contradictions in the data?\n- Is the data accurate?\n- Is the data supporting or refuting the proposed hypothesis?\n- Can the researcher improve the overall quality of the findings?\n- Is there any data that should be discarded?\nAssess The Reliability Of The Findings\nThe second step is to evaluate the reliability of the findings. Researchers need to ask the following questions:\n- Are the findings reliable?\n- What is the probability of false findings?\n- What is the margin of error in the research findings?\nEvaluate the Theoretical Models\nThe most important step in evaluating the results of a study is to assess the theoretical models. A research’s findings may turn out to be insufficient if the original theoretical framework is inconsistent and contradictory. You must be thinking that this process comes prior to data collection. But most of the time, a researcher is unaware of such anomalies.\nIn the initial steps, researchers develop tentative research problems, questions and hypotheses. Which are either validated or refuted in the later research process. After gathering much information about the investigated topic, researchers have a better understanding of the theoretical underpinnings of the topic. So, before evaluating the findings, it is essential to ensure that the data set is consistent with the theoretical models.\nWhat Are The 3 Methods Of Evaluation?\nFollowing are three methods of data evaluation:\nThe process of arranging a set of data components in such a way that they are more similar (in a certain sense) to one another. The arranged data is in stark contrast to other groups or clusters. Because clustering has no goal variable, it is frequently used to uncover hidden patterns in data. The technique may also be used to offer perspective to a pattern or statistic.\nCohort evaluation is a common practice in market research. It employs historical data to analyze a particular data set. Researchers often utilize cohort evaluation to compare data sets by grouping them together based on shared attributes. It is a beneficial evaluation method that enables the researchers to gain deep insights into the data set. Cohort evaluation in market research enables the companies to comprehend the market practices. It enables businesses to understand the impact of their activities in the market.\nA collection of statistical procedures for estimating connections between a dependent variable. One or more independent variables is known as regression evaluation. It may be used to determine the correlation between variables and predict how they will interact in the future. Researchers often use this evaluation method when they have to predict and forecast future occurrences.\nWhich Is The Best Evaluation Method?\nStatistical evaluation is the method of evaluating the findings of a research. It reduces the margin of biasness and enhances the reliability of the data. Statistical evaluation encompasses the utilization of statistical tools and methods to interpret the findings. It is a common approach in quantitative research which involves numerical data. It encompasses the survey data and the observational data. There are different tools to perform statistical evaluation. Such as the Statistical analysis System (SAS) and Statistical Package for the Social Sciences (SPSS).\nStatistical evaluation of data involves analyzing the relationship between the variables. Most of the time, the data comprises univariate or multivariate findings. The variation in statistical methods to evaluate the findings depends upon the number of variables a research study has. Statistical evaluation encompasses continuous data and discreet data. Continuous data pertains to a dataset that is not countable. Discreet data encompasses such data that is countable.\nEvaluating the results of a study is essential to ensure that the study’s findings are consistent with the research problem and research questions. Evaluation ensures that the data is reliable and consistent with the initial objectives of the research. Different statistical tools and methodologies, as mentioned above, help evaluate result of study. Researchers also have to analyze the currency and relevance of the collected data. Currency pertains to the timeline of the collected data. It is important to ascertain that the collected data is not redundant. Moreover, the relevance of the data is essential in aligning the data with the research objectives. The accuracy of the data is measurable through comparison of the data and by establishing patterns in the data.', ""The total amount of data generated worldwide increases by 40% every year. Many companies seek to make use of the constantly increasing mountain of data in order to increase their e-commerce business. But making use of such Big Data alone doesn’t add any value — enter data mining. Below you can find a discussion of the various analysis approaches involved in data mining, to give you an idea of how...\nSuccessful marketing campaigns all have one thing in common: they are perfectly tailored to target groups. But finding and reaching these target groups can prove tricky for online marketers. Without intensively studying users’ behaviour as part of a comprehensive web analysis, you can only guess whether your planned marketing steps are creating the desired effect. For example, a complete data set usually acts as a basis where you can find out which devices visitors use to access the website. A different approach to web analysis is known as cohort analysis. Here, instead of collecting different information to analyse at once, different groups (cohorts) are allocated for analysis. The criteria of the cohorts can vary quite considerably, which we discuss below.\nCohort analysis: definition\nFor decades the concept of cohort analysis has played an important role in statistical surveys in social science and demographics. Cohorts (from the Latin 'cohors' meaning 'crowd') are groups of people who share a common demographic. For example, this could be the birth year or the year they started working, or certain historical events such as a president’s inauguration. The term 'generation' is often used. When a cohort analysis (also referred to as a 'cohort study') is carried out, the behavioural changes of the defined groups of people over the time period they are examined. Once you’ve collected the data, you can either:\n- Obtain an accurate picture of the underlying cohorts (intra cohort study), in order to analyse, for example, the development of the birthrate and the change in consumer behaviour (either over a long period, or on a random basis).\n- Make a comparison with at least one other group of people (inter cohort study), in order to obtain useful insights into behavioural differences.\nAt the end of the 19th century, statisticians Karl Becker (1874) and Wilhelm Lexis (1875) laid down the foundation for the analysis of certain population groups. Through advancements made by demographer Pascal Whelpton (1949), these approaches known as cohort analyses finally obtained international notoriety. The aim of Whelpton’s research was to analyse the increase in the US’s birthrate after WWII. Today the process is increasingly used for studies in medicine, politics, and the market economy.\nImplementation and interpretation\nCohort studies can be carried out in two different ways: you can arrange the cohorts together and accompany them in future (prospective cohort study), or you can access data from the past so that you can analyse the present (retrospective cohort study). In order to be able to implement one of these types of cohort analyses, the following steps need to be taken:\n- Define the research question and aim: to obtain relevant information, you have to ask the right questions. Only when you have concrete ideas about the content and purpose of the investigation, can you create the necessary structure of the study.\n- Define cohort events: the second step is to define the events in which cohorts occur, as these can lead to an answer to the research question.\n- Determine relevant cohorts: now you determine which and how many cohorts are to be parts of the study. It is also possible to split or specify the formed cohorts.\n- Perform the cohort study and evaluate it: if the desired cohorts have been found, you can carry out the respective type of study (prospective/retrospective, inter/intra cohort study) and interpret the data received.\nThe changes in behaviour you want to obtain by carrying out the cohort analysis are determined by three factors or effects. The evaluation and weighting of these are the main tasks of interpretation:\n- Cohort effects\n- Age effects\n- Period effects\nCohort effects are the behavioural differences and changes between different cohorts. They can be generally explained by the existence of different social and environmental influences. Age effects, on the other hand, are the changes that can be attributed to the increasing age of people and their related attitudes. Lastly, period effects represent behaviour changes that result from changing environmental conditions – regardless of generational and socio-demographic factors.\nFrom these three effects, you can notice any clear trends regarding the behaviour of individual groups. On the basis of these trends, you can use them to develop future prognoses or solution strategies. The main task is to separate age, cohort, and period effects, which can occur in every result, from each other. If you include these as identification problems in the cohort analysis, you can find a clear reason for the behavioural changes.\nThe benefit of cohort analysis in marketing\nAnalysing the market and the associated target groups is an important part of strategic planning that precedes every marketing campaign. In online marketing, the focus is increasingly becoming more about the behavior of users. The millions of data that have already been collected serve as a strong basis for further planning, but this information first needs to be extensively evaluated. If you want to go a step further than just gaining knowledge about the behaviour of the average user and want to organise the visitors depending on specific criteria, you should definitely take advantage of cohort analysis. For observing the behaviour of new and existing customers or recognizing regional trends, this procedure has been an indispensable tool in e-commerce for a long while.\nExample: cohort analysis in e-commerce\nCohort analyses enable you to check how successful your marketing campaigns are in a very precise way, as the following example shows:\nYou, an online store owner, decide you want a total redesign and layout change. To check how the new design is fairing with customers, you should look at the recorded transactions and categorise your customers into existing customers (cohort 1) and new customers (cohort 2). After two months, you look at the results and notice that the number of transactions has decreased. Without further information, you could say that the new layout wasn’t very well received. A look at the separate figures of both cohorts could reveal two other scenarios:\n- Cohort 1 (existing customers) completed more transactions than before the store redesign. In contras,t there were fewer purchases made by cohort 2 (new customers).\n- There were more purchases made by cohort 2 (new customers) than before. Cohort 1 (existing customers) has carried out fewer transactions.\nCohorts: the more specific, the more meaningful\nThe example above shows the advantages of implementing a cohort analysis, which is that it is much more flexible and specific than a mere analysis of average user behaviour. Thanks to the powerful features of current tools such as Google Analytics with regards to data collection, it’s now possible to differentiate between new and existing customers; the tools help you to check the behavior of more complex cohorts. You can include, for example, the age and location of customers, or the device being used in the categorisation. You can also access the information you need, so that you can respond to the needs of individual customer groups.""]"	['<urn:uuid:d379ccb8-b696-424a-a14e-9d24b9c6873f>', '<urn:uuid:9b3ae5c6-bb39-4a3f-baf4-8ee20111b479>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-13T00:43:36.053682	9	110	2744
95	crime fiction genres impossible crimes supernatural murder legal mysteries describe characteristics main types	Crime fiction encompasses multiple types: impossible crimes involve mysterious deaths with no traceable clues, like John Dickson Carr's works; supernatural murders feature haunted houses and ghostly elements, exemplified by Agatha Christie's The Pale Horse; and legal mysteries are part of modern crime writing forms alongside political thrillers, police procedurals, and psychological suspense novels. These genres have evolved from simple whodunnits to more complex narratives with sophisticated writing styles.	"['An old, legendary house with a room sealed off. Years ago, there was a string of bizarre deaths in that room and legend has it that anyone who spends a night in that room will die. On a dare from his friends, a man enters the room to spend the night. Everyone has a good laugh and promise to let him out in the morning. Morning arrives to see him sprawled dead with his throat slashed and with a blood message that threatens to kill all the others. It is obviously clear that a serial killer is on the loose but who is he? Who will he target next? What is his motive?\nDid that spike your curiosity? Do you wish that it had an ending? Then you, like many others are an avid fan of crime fiction. Curiosity, they say, forms the basis of any crime story. Keeping the reader hooked on to the book is a very hard task and it totally depends on the presence of enough number of page turners to make it a good read. Even inside crime, there is a whole range of plots that run through the entire story.\nTHE IMPOSSIBLE CRIME\nThere are crimes that seem impossible but have happened and there is simply no denying them. But the way the crimes took place leave absolutely no clues behind to be traced and remain an unsolved mystery till the end. The master of this kind of writing was John Dickson Carr who came up with variations of these impossible crimes in his books. One such masterpiece is He Who Whispers that involves a mysterious death that occurred years ago when a man was stabbed to death on top of a tower with no evidence of anyone climbing the tower. In modern day, a woman is almost frightened to death in her room by something outside her window but again, there is no way someone could have climbed the window sill. The creativity of these stories and the apparent lack of rational explanations is what make these stories engaging.\nTHE ROOM THAT KILLS\nThis plot involves an empty room in a deserted house that no one has visited in a long time but suddenly there is a death that shocks everybody. Soon after the incident, anybody who goes to the room ends up dead in the most bizarre ways possible. Carter Dickson’s The Red Widow Murders is the pinnacle of such stories. It involves a bizarre locked room murder and a chase that has adventure lurking in every corner along the way.\nTHE SUPERNATURAL MURDER\nThis is one of those scenarios that involve a ghost of sorts. The typical setting for this kind of plot is a haunted house that is a relic of an unsolved murder blamed on witchcraft or possessions. The set is modified for the present day and when done right, it is one of the best possible mysteries to indulge in. These stories make the reader question the existence of a possible rational explanation behind the events. There are many books in this genre that have left their impressions on readers. Agatha Christie’s The Pale Horse is about a dying woman who gives someone her last confession along with a terrible secret that results in the death of that person, John Sladek’s Black Aura involving levitation, Robert Van Gulik’s The Chinese Gold Murders that involves the ghost of a judge who refuses to lay simply in his grave are all best reads in this genre of crime.\nMURDER IN A THEATRE\nThere is something extremely incredible about a murder in a theatre. The killer lurking in the shadows, trap doors with ropes as props and the actors rehearsing for the performance unaware of the killer all make for an interesting read. It mixes the theatre world with mystery and that gives an edge to the already existing drama. The first book that comes to mind in this genre is Gaston Leroux’s The Phantom of the Opera which is not a complete mystery as such but paved way for future authors to experiment with the style. Some of the other mentions include Full Dark House by Christopher Fowler that is about a bombing case in present-day London and a flashback story to a murderer in a theatre in World War II period of London and Edmund Crispin’s The Case of the Gilded Fly that involves the seemingly impossible murder of a despised actress in a theatre.\nTHE DYING MESSAGE\nIn fiction, dying people seem to say the craziest things that then become the onus of the protagonist to solve and help the person die in peace. Recommendations for this kind of story are The Siamese Twin Mystery by Ellery Queen that involves a forest fire slowly eating its way to the mountain top and it is the responsibility of the detectives to find out the cause behind it and The Tragedy of X by the same author that has a bit of a lecture on dying messages, and tries to explain why anybody would leave one. It’s an appreciated effort and a very good story that failed to create a huge impact on people but is an honourable mention nonetheless.\nTHE SERIAL KILLER\nA serial killer is striking the city randomly and killing people one by one. The police have absolutely no idea of the connection between the victims and the media is having a field day with the story. A mania grips the people and the body count just keeps getting higher and higher in a situation where everybody is treated like a suspect. The serial killer plot sounds more like a script for a good action movie but is extremely tried and tested as far as books go. Ellery Queen’s Cat of Many Tails is a masterpiece in this regard and shows the reader which characters can or cannot be ignored in the book. A serial killer named Cat is on a rampage in the city and strangles his victims to death. After every killing, a picture of a cat is printed in the papers, with one tail more than it had last time. Agatha Christie’s The A.B.C. Murders is another example of a successful serial killer story done right.', ""Writers of crime fiction usually offer fast, intense reads to their fans. The field has expanded. The many forms of crime writing today include political thrillers, police procedurals, psychological suspense novels, detective fiction, and legal mysteries â€“ for starters.\nWriters of crime fiction usually offer fast, intense reads to their fans. The authors bid for your attention with styles that become their own and characters that grow into favorites. The field has expanded. The many forms of crime writing today include political thrillers, police procedurals, psychological suspense novels, detective fiction, and legal mysteries — for starters. The writing is more complicated; we’ve moved on from Raymond Chandler and Dashiell Hammett “whodunnits” with their sparse syntax. Now some crime writers, like James Lee Burke, generate pastoral scenes that are pure literature. Some, like Greg Iles, provide the polish and suspense of Southern writers. Some, like Lee Child, create such powerful characters that Hollywood and Tom Cruise come running. But now, crime writing seems to have added a touch of travel writing.\nI find sometimes the best descriptions of scenes and places lie with my favorite crime writers. With a touch of envy, I ask, why is this? Is it because the best writing advice is write about what you know. That’s often the case, and when you investigate those authors you often find they live in the same cities they are writing about. Indeed, the advice from a website, Creative Writing Now, includes, “Create a three-dimensional world. A mystery novel may be a kind of puzzle, but it's more… Your characters should have lives that extend beyond the particular situation. They have families or lovers or a lack of family and lovers. They live in a particular setting — maybe New York or Los Angeles or a charming small town or a snooty suburb – which you should make real for the reader.”\nPerhaps the emphasis on location, location, location began with the movies’ interest in the James Bond novels. We started seeing faraway places, yes, with strange sounding names and today’s lenses were up to the challenge — and today’s writers and those movies that began with a bang influenced novelists as well. Gone was the Charles Dickens-like convention of slowly developing a theme or place. Instead you were plunged into a Dante-like hell. And the upfront question was, were you going to survive?\nSome authors are so skilled at this beginning this prologue that their readers might not accept any other.\nAmerican novelist Ridley Pearson began his Walt Fleming series in 2007 with Killer Weekend. His prologue was electric. Pearson has written about Seattle and another series about Idaho. Robert Crais takes his readers to Louisiana and, as here, in this The Watchman prologue to Los Angeles. However, good crime writing today needs more than quality prose, it has to have an exciting well-described location, it needs to show readers the place.\nImages by author: I recall how a Ridley Pearson novel The Art of Deception introduced me to the reality that a subterranean Seattle existed. I knew nothing of this. In some ways, Seattle was like Egypt’s Alexandria, a city surrounded by sea with such little room to expand. The new construction had to build on top of the old and the old was put aside to lie forgotten. Within a month of reading Pearson’s novel I was in Seattle’s most unusual tourist attraction, its underground, although the book’s characters were having a more intense experience than I was.\nPearson’s popular character, Lou Boldt, is a Seattle police detective and by the time you have read a few of this series you may be ready to visit Seattle as a tourist.\nTexas publisher, David L. Lindsey, became an author himself in 1980 to create a Houston homicide detective, Stuart Haydon, who has appeared successfully in many of Lindsey’s stories. I really like this author’s style, although I do smile at some of his descriptions which may not exactly make you a Houston tourist; he has shown how his fastidious detective takes a shower on a typically humid Houston day, puts on a clean shirt and goes back to work — the shirt immediately drenched in perspiration!\nImages by author: Top: My Texas memories. Wells Fargo stagecoach made for sale by prisoners in Huntsville Prison; scenes from the Texas Hill Country.\nIf an author creates a successful series, as many writers aspire to, then the sequences take on a life of their own, though loyal readers sometimes feel it takes a lot to beat the earlier episodes. And so Michael Connelly gave us, and finally Netflix, the intense world of Los Angeles and Harry Bosch. James Lee Burke created the gritty spaces of Louisiana and Dave Robicheaux — and if we were troubled when our good guys lost family members to bad guys, we had Jonathan Kellerman’s forensic psychologist, Alex Delaware, to ease our burden.\nA fictitious ex-Seal hero like Mitch Rapp could survive only as long as his creator lived. It was a sad disappointment when author Vince Flynn died in 2013, aged 47, of a long battle with prostate cancer. Readers who love their country but don’t like their politicians took to Rapp’s stories about a wild cannon CIA operative who didn’t hesitate to execute an American congressman who betrayed his country. The publisher of the 13 episodes Flynn wrote has now found a competent and exciting writer to continue the series. “The show does go on.”\nOur capital and Flynn’s political thriller which saw DC invaded.\nWe need heroes. We have to believe right ultimately triumphs over wrong, even if the anti-hero has a place in today’s stories. In real life we seem to be surrounded by such business corruption and government mediocrity that we are desperate for a brief fling in fiction, so it all really does come out right. However, the complicated planning to arrange a plot, a storyline that makes sense, that succeeds in tying up all the loose ends, has never been more difficult for crime writers because the good ones have stretched their stories’ boundaries such that you almost need your passport or at least Google Maps to follow the chapters.\nImages by Gillian Abramson: Daniel Silva’s stories might take you to Russia, to France, to Israel, even to the island of Corsica, to follow the duty of a professional painting restorer who is also (gasp!) a spy. Not only does this series make you want to travel, but also to spend more exciting time exploring art museums.\nSilva’s character’s visit to the island of Corsica sees parallels in the TV camera’s exploration of virgin areas: the rural or island crime scene. Perhaps the Wallender books and the Scandinavian trilogy that began with The Girl with the Dragon Tattoo started the trend — then studios saw the chance to produce streaming video jumped in. Novels by Ann Cleeves have been transferred very successfully by BBC America to a crime series Shetland. A trilogy now starting with a first novel, The Blackhouse, is based on a nearby Scottish archipelago, the Outer Hebrides. Written by Scottish author and film maker, Peter May, it suggests equal promise for a long run on streaming TV.\nImages by Gillian Abramson: The Lewis Trilogy takes you, of course, to the Island of Lewis and Harris and briefly to Scotland’s capital Edinburgh, but this remote island has seen a decline since the 18th century Highland Clearances that evicted settlers for sheep. The middle image shows a black house stands neglected in the Island of Skye in the Gillian Abramson and the bottom image features a window’s look out to sea in the concrete-like bunker boathouse that is the scene of the murder that opens the novel, The Blackhouse.\nYet I would have this caution for author Peter May, whose reader acceptance of his recent series of more than a dozen crime novels has been sensational: The main character in Shetland is a gentle, kind, and compassionate detective inspector, Jimmy Perez, who really cares for those he serves and for the staff he commands. He is most likeable. He is backed up by a competent and congenial female assistant. I would say as an exiled Scott to Peter May, if he hopes to make his police detective, Fin Macleod, an equally strong character on TV, Fin has to be more assured and more likeable.\nThe Andersons, who live in San Diego, are the resident travel & cruise columnists for Physician's Money Digest. Nancy is a former nursing educator, Eric a retired MD. The one-time president of the NH Academy of Family Physicians, Eric is the only physician in the Society of American Travel Writers. He has also written five books, the last called The Man Who Cried Orange: Stories from a Doctor's Life.""]"	['<urn:uuid:03b9aee5-9ec6-4cdc-9b86-3d578da7e3ba>', '<urn:uuid:13ca72e5-9eaf-4a8e-b301-fddbcdaf2000>']	factoid	direct	long-search-query	similar-to-document	three-doc	expert	2025-05-13T00:43:36.053682	13	68	2506
96	I'm researching theatrical censorship patterns - what's the key difference in how censors treated the politically-charged works 'Alasco' versus Shakespeare's 'Coriolanus'?	The tragedy Alasco was heavily censored in 1824 by George Colman the Younger (the Examiner of Plays) to the point that playwright Martin Archer Shee withdrew it from production at Covent Garden, while Coriolanus faced censorship much later and for different reasons - it was banned in France in the 1930s due to its skeptical view of democracy and its portrayal of what would today be considered a fascist hero.	"['In 1819, the former actress Elizabeth Macauley wrote a pamphlet called Theatric Revolution, or Plain Truth Addressed to Common Sense. The pamphlet addressed the mistreatment of actors by the two patent theatres at Drury Lane and Covent Garden. Macauley argued that actors must band together for their common good, and for the common good of society.\nIn his first chapter, Worrall discusses the regulation of the theatres, examining both the patent system that gave Drury Lane and Covent Garden a monopoly on serious spoken drama, and the censorship system that gave an inordinate amount of power to one man, the Examiner of Plays. Beginning in 1824, the playwright (and son of a playwright) George Colman the Younger was the Examiner, which shows just how incestuous the whole system was. His first year in office, Coleman censored so many passages in Martin Archer Shee\'s tragedy Alasco that the playwright withdrew the piece from production at Covent Garden. He subsequently published the play with a preface objecting to Coleman\'s censorship, and by the end of the year Alasco was performed in New York.\nSince the two patent theatres had a monopoly on spoken drama, minor theatres like the Royalty Theatre in the East End had to add music to plays, turning them into ""melo-dramas"" or ""burlettas."" Worrall\'s second chapter deals with attempts to suppress the Royalty, which the actor John Palmer built and opened in 1787. The theatre became the subject of a pamphlet war, with the partisans of the patent theatres arguing for its closure. The Royalty was denied a license in 1802, but then licensed again the following year.\nChapter Four deals with the famous scenic designer Philip de Loutherbourg, who was also a Freemason. His 1773 extravaganza A Christmas Tale contained a great deal of Masonic symbolism. Charles Dibdin provided music for the piece, which had a text by David Garrick. After the outbreak of the French Revolution, some critics of Freemasonry linked the Masons to the radical Jacobins, and by the late 1790s the Masonic imagery in A Christmas Tale ""had become politically dangerous."" The play was not performed for many years, though the Surrey Theatre did stage it in 1817, after anti-Mason (and anti-Jacobin) hysteria had quieted somewhat.\nIn the fifth chapter, Worrall examines the censorship of The Two Farmers by Thomas Dibdin, Charles Dibdin\'s son. Though the play was scheduled for performance at Covent Garden in 1800, it ran afoul of the Larpents. Britain had recently experienced grain shortages, and many people blamed farmers for allegedly hoarding their crops. The Two Farmers introduced a villain named Locust who hoarded grain in order to raise prices and justified his actions as being ""for the good of the nation."" Larpent\'s blue pencil made quite a number of cuts, and ultimately the censor rejected the play in its entirely.\nThe following chapter looks at theatricalized popular British culture. Worrall notes that drama took place well outside even the ""minor"" theatres in London and official theatres of the provinces. For instance, the actor Peter Paterson performed a 12-minute version of Matthew G. Lewis\'s The Castle Spectre at fairgrounds around Birmingham. Audiences could also see spoofs such as W.T. Moncrieff\'s Frank-in-Steam, or, The Modern Promise to pay, which was performed in 1824, only a year after the English Opera House debuted Richard Brinksley Peake\'s Presumption; or, The Fate of Frankenstein, the first adaptation of Mary Shelley\'s novel.\nChapter Eight looks at two plays, the anonymous closet drama Plots and Placemen and James Powell\'s Drury Lane confection Harlequin Negro. Attributed to on ""Zachary Zealoushead,"" Plots and Placemen satirizes the suppression of radicals during the Regency. The powerful foreign secretary Lord Castlereagh becomes ""Callouswretch"" and Lord Sidmouth, the widely reviled home secretary, becomes ""Proudmouth."" This might seem an odd pairing with Harlequin Negro, an 1807 play that Larpent approved without any excisions. However, Powell\'s play contains what Worrall calls ""a vivid moment of empowerment changing the oppressed black Caribbean slave into a gorgeously costumed comic hero."" Both plays, then, were furthering the agenda of Britain\'s radical progressives.\nWorrall also includes a chapter on plays inspired by the infamous 1817 murder of 20-year-old Mary Ashford. The young woman went to a dance in Warwickshire one night, and the next day her body was found in a flooded sandpit, probably raped. In 1818, the anonymous three-act plays The Mysterious Murder and The Murdered Maid appeared in print. Two other plays based on the events appeared on the London stage: William Barrymore\'s Trial by Battle and John Kerr\'s Presumptive Guilt.\nIn the final chapter, Worrall looks at the performance of the radical play Swing at the Rotunda, an improvised theatre space where the well-known radical Richard Carlile staged Robert Taylor\'s five-act tragedy Swing; or, Who Are the Incendiaries? This ""People\'s Tragedy"" seems to have been performed with a cast of two, a man named Harrison and Taylor, who at one point appeared as himself under his nickname ""Robert the Devil."" The shockingly revolutionary play shows the proletarian Swing made king, but he then abdicates so that Britain can become a republic. Carlile hoped to collaborate with Taylor on a second play, but the latter\'s alcoholism rendered that impossible.\nOverall, the picture that emerges of Georgian theatre in Theatric Revolution is one that is far different from the staid Shakespeare revivals people generally associate with that era. Writers like Martin Archer Shee, Eglantine Wallace, Thomas Dibdin, James Powell, and Robert Taylor were constantly pushing the envelope, staging--or attempting to stage--plays that engaged politically with the ever-changing world around them.', 'Dr. Ralph Brief\n1. When was the play first performed?\n2. Where was the play first performed?\nThe first production of which there is a record is Nahum Tate’s adaptation in 1682 at the Drury Lane theatre (where you currently can see Willy Wonka and the Chocolate Factory). At the Third Blackfriars conference, keynoter Peter Holland suggested that Coriolanus might have been the first of Shakespeare’s plays staged at the Blackfriars. Nice to think.\n3. How does this play fit into Shakespeare’s career?\nCoriolanus is probably Shakespeare’s last tragedy. The plays that follow are his tragicomedies, also called the “Romances.”\n4. How is this play like Shakespeare’s other plays?\nLike so many of Shakespeare’s title characters, Coriolanus is multi-faceted, a mixture of things we like and don’t like. And like Julius Caesar, one of Shakespeare’s three other Roman plays, the portrait the play paints of the people – the demo of the word “democracy” – is unflattering. As with Lady Macbeth, Shakespeare has created a woman of ferocious will and strength, Volumnia, Coriolanus’s mother, who challenges received notions of gender roles and, in particular, maternity.\n5. How is this play unlike other Shakespeare plays?\nFrom its hero to its architecture, Coriolanus is one of the most “unlike” of all Shakespeare’s plays. Caius Martius (Coriolanus) is famously an unlovable avatar,a kind of Coach Bill Belichick, endearing only in his predictable single mindedness and contempt for public opinion. As to the play’s architecture, instead of building to large battle scenes as in Julius Caesar, Macbeth, and so many of the history plays, Coriolanus frontloads the action and moves instead toward stillness and a kind of interiority.\n6. What do scholars think about this play?\nOpinion on the play varies greatly, some few scholars rank it among Shakespeare’s best; all admire its excellence as an unusual portrait of a difficult protagonist.\n7. Are there any controversies surrounding the work?\nThe play’s skeptical look at the value of democracy and its obvious interest in what today we would call a fascist hero got it banned in France in the 1930s. Over the last half-century productions of the play have explored the homoerotic obsession that Coriolanus and his enemy Aufidius have for one another, and the remarkable relationship between Coriolanus and his mother Volumnia have made the play a field of play for Freudians.\n8. What scene should I especially look for?\nTwo in particular. The play reaches an exhilarating existential climax in the “banishment” scene in Act III, scene 3, when Coriolanus turns on the Roman mob calling for his banishment and says, “You common cry of curs! … I banish you” and then adds, “Despising, / For you, the city, thus I turn my back: / There is a world elsewhere.” Counterbalancing this scene of defiance is Act V, scene 3, when Coriolanus’s family begs him to relent in his revenge on Rome.\n9. What characters should I especially look for?\nThe two tribunes, Sicinius Velutus and Junius Brutus, who whip up the mob against a public servant, will be familiar to you from the recent presidential campaign.\n10. What is the language like?\nIt ranges from the terse invention of Coriolanus’s insults to the digressive persuasion of his friend Menenius.']"	['<urn:uuid:78099a1e-d752-435e-bcb3-6b7317fa2d62>', '<urn:uuid:86aefdac-17fc-493e-9c15-a9a23da36886>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T00:43:36.053682	21	70	1460
97	napier grass carbon neutral biomass benefits climate	Napier grass is beneficial for climate change mitigation in several ways. It acts as a carbon-neutral biomass source because the CO2 released during burning is reabsorbed by the growing plants, which then release oxygen. The grass has excellent carbon sequestration properties due to its deep root system. It grows very fast, producing high yields of up to 200 tons per hectare per harvest twice a year, and can be used to generate electricity through various processes like combustion and torrefaction. When used as animal feed, high-quality Napier grass can improve cattle productivity while keeping methane emissions relatively lower, as better feed quality leads to more efficient digestion. This makes it an effective solution for both renewable energy production and sustainable livestock management.	"['Now that humanity has realized the long-term dangers of relying on fossil fuels, there is a huge interest in alternative energy sources, which include renewable energy. If run properly, renewable energy companies stand to gain a lot of money. A Filipino company is throwing its hat into the ring, using an unlikely agricultural product as its main source of fuel: napier grass.\nSecura International Corporation President and CEO Danilo Manayaga, Ch.E. said that they have been using napier grass since 2016, when his company was asked to put up a 300 hectare plantation for the Lamsan Power Corporation in Maguindanao. Tagkawayan Power Corporation, a 10 megawatt power plant in Quezon that they are currently setting up, uses napier grass as its feedstock as well. “We are mostly focused on renewable energies, especially the use of biomass,” Manayaga said. “ Another project on the horizon is the production of bio coal from Napier grass biomass. We have two sites in mind, one in expanding the Maguindanao plantation area and also in Morong, Bataan where we have 10,000 hectares for biocoal production.”\nWhy napier grass?\nHow did Manayaga and his partner, Dr. Frank Shotkoski, Secura’s operations manager and Tagkawayan’s president, identified several factors that have helped them determine the variety of napier grass that is optimal for their purposes.\nFast growing. “The holy grail of biomass really is how fast it will grow and how much per hectare it can provide in terms of biomass weight,” Manayaga said. “We selected around 132 different varieties and subjected them to competition [such as] based on fertilizer use, based on water use and the height or the weight of biomass produced per square meter.”\nCarbon sequestration. “The thing about that particular grass is that it grows so fast, it has a very high cellulose content, it does require a lot less water… These plants will grow up to six to seven meters high. The roots are going down and are also sequestering a lot of carbon and holding it down there,” Shotkoski explained. “This particular napier grass, which is a hybrid between a West African sorghum and a native napier grass, has a…very high yield, at least 200 tons per hectare per harvest. You can harvest this thing twice a year…. It has fleshiness to its green leaves, which makes it palatable for livestock and… are [also] really good to be used in a digester to make biogas, so it’s going to be used to run the electricity to run the power plant ….”\nSterile. “It’s sterile–it’s a hybrid, so you don’t have to worry about invasiveness. It does not send out rhizomes, so you don’t have to worry about it invading into other areas as well… and no insect or disease problems as of now.”\nBiodiversity-friendly. “It’s very nice from a biodiversity standpoint. There’s ravines, steep hills, [and it’s] covered with trees and other native plants that makes for very good biodiversity for birds, snakes, lizards, reptiles, even mammals,” Manayaga said.\n“We use the topography of the land…. It really becomes a much better native environment compared to just native grass just growing from the deforested areas. It has a lot of good ergonomic attributes,” Shotkoski added.\nContinuous growth. The grass is cut at the top, leaving its roots intact, so it will keep growing without the need for new plating materials for a long while. “We’ve had some now that’s been planted almost 10 years and it still just comes back, so it’s one of these things where you just don’t have to disturb the soil very often,” Manayaga said.\nOpportunity for idle land owners. To date, the company has 300 hectares planted in Mindanao and around five hectares in Bulacan, all of them distributed to about 90 cooperatives in both provinces. The land is leased for 15 years. “That’s the income to the landowners, which they can use for many others. That’s how this relates to the local agriculture utilization of this unused land,” Manayaga said.\nBy law, farmland cannot be used to grow biofuel, so there is no conflict with food security. “According to the Renewable Energy Law, idle land that is allowed to be used as biomass plantations should have not been planted for 15 years of any food crop, [except for] sugarcane and coconut land areas,” he added.\nFrom grass to gas\nThere are several ways biomass becomes energy. One way is through combustion, which is what the Manguindanao plant does. The biomass is burned in a furnace, which heats up tubes that turn water into the steam which drives a turbine to produce electricity. Another method is by making syngas, which is produced by heating biomass without oxygen so that it is converted to gas. Syngas can also be purified into hydrogen, which has several uses, including being combined with ammonia to produce fertilizer.\nAnother method is through torrefaction, where the biomass is roasted. 30% is turned into heat, and the rest is turned into a kind of coal. “It has higher energy compared to local coal at 24 megajoules or 5700 kilocalories, and it can be used in pellet form,” Manayaga said, adding that the pellets are waterproof and are a good candidate to replace coal. This is the plant that Secura hopes to build in the future.\nThe future is now\nThere are many reasons renewable energy is of interest worldwide, and even more reasons the Philippines should be interested in producing this technology.\nFirst, if produced locally, it can be a source of cheap power. “For the Filipino consumers, we’ll be producing cheaper power and at a stable price because we’re not shipping the coal from over ocean,” Manayaga said. “The only escalation there is the cost of harvesting, which uses diesel at the moment.”\nSecond, it can help mitigate climate change through lessening the amount of travel it will take for fuel to get from its source to its consumers. “Renewable energy is very important because you are not putting more carbon dioxide in the air, it will help mitigate climate change,” Manayaga said.\nShotkoski added that their two current plants are carbon neutral. “You’re going to have the (plant) very near (the fields), so the CO2 that’s going to go into the air is going to be reabsorbed into the plants because the plants need the CO2 in order to grow, and (the plants are) going to give off oxygen. That’s why it’s neutral. If our burning did not give off coal, then it would be carbon negative. The main thing is not to produce or add any more to the environment if we can.”\nThird, it can be important to national security. “For national security, it’s actually a very important component because right now, this country’s energy is dependent on the ability to get coal, mostly from Indonesia,” Shotkoski explained. And if you remember, just a few months ago, Indonesia was going to shut down all exports to China and the Philippines and other parts of Southeast Asia because they were running short themselves. So the Philippines is quite vulnerable because we really don’t have a source of coal. And the coal that we get from Indonesia is not high quality coal.”\nAnd fourth, renewable energy can be converted into many things, including electricity (which is what the Maguindanao plant is doing), hydrogen, biofuel, and gasoline.\nManayaga and Shotkoski are optimistic about the future of renewable energy in the Philippines.\n“Now is the future. We’re ready to expand. The planting materials are available, the market is very conducive, the price of coal is going up, and we are able to help climate change, so we’re now ready,” Manayaga said.\n“Something that’s been making the news a lot lately… the importance of investing in agriculture… What we’re doing is offering agriculture a truly new commodity and a new opportunity, for not just the farmers, but for everybody that’s involved in agriculture, because we can put tens of thousands of hectares of land into production,” Shotkoski added. “This is a huge opportunity for agriculture in general as another commodity.”\nPlanting napier grass for biomass is an interesting way to utilize idle land while providing income through rent to the landowners.\nPhoto courtesy of Secura\nThis article appeared in Agriculture Magazine’s November 2022 issue.', 'Why feeding cows better grass can help fight climate change\nFebruary 24, 2020\nScientists say feeding cows good quality grass would boost their productivity — which is better overall for the climate. Experts explain to DW how that helps tackle global warming — and why it\'s a challenge in Africa.\nDW: How does feeding cattle Napier grass help tackle climate change?\nLutz Merbold: So what we basically do is baseline the environmental footprint of the current diet of animals. In other words, we look at what they\'re eating and then quantify the climate impact in terms of greenhouse gas emissions and then we look at options on how to reduce these greenhouse gas emissions without hampering animal production. And one of the things we\'re testing is Napier grass.\nI think it\'s a bit more complex than just focusing on Napier grass itself because the whole point is that there is no silver bullet on this, or that this is the best grass.\nIt doesn\'t really matter which grass, if it\'s a good palatable grass of good quality and quantity then it\'s going to be fine.\nThe point is how you cultivate this specific grass, because that has an effect on how well it can be digestible for an animal and how much methane it produces.\nSo just feeding Napier grass doesn\'t necessarily reduce methane emissions, as it depends on the quality and quantity of Napier grass or any other grass which is fed.\nSo anything we do, no matter if it\'s Napier grass or whatever, whatever diet you\'ve been giving which meets the demand of an animal properly and consistently will boost productivity. It will also increase methane emissions but not as quickly as it boosts productivity and that\'s the reason why the ratio [of productivity against methane emissions] comes down.\nSo by boosting yields from the animal, be it meat or milk, at a greater rate than the methane the animal is producing, that\'s what makes feeding animals quality forage better for the climate. What have the challenges been in Africa?\nChris Jones: It\'s a system where animals are underfed and malnourished, and they\'re also challenged by the various pests and diseases and therefore the productivity in this part of the world is very low compared to more developed systems.\nMerbold: We see regularly that you have Napier grass in a plot of a farmer who doesn\'t even fertilize it. But then many farmers let it grow until it\'s two meters high.\nBut as soon as it\'s that high it\'s a lot of biomass but it\'s really poor quality, it becomes almost like a ""woody"" vegetation, which is very difficult to eat for a ruminant like a cow.\nThe typical suggestion is to harvest a Napier grass plant every eight to 10 weeks when it\'s approximately a meter high because then you have the best quality, the most energy in the plant.\nThere\'s a lot of sugars etc. which are easily digestible for an animal, so that the animal can actually grow and then, of course, it will also produce methane.\nBut if it just eats the really old stuff, it will chew on it and chew on it and try to digest it. This will actually lead to more methane but less production.\nSo this is partly about increasing awareness among farmers in Africa that they shouldn\'t let their Napier grass grow above a certain height?\nMerbold: Exactly, that\'s one thing. And also if you keep animals, that you should actually have a forage grass plot cultivated so that you can feed your animals, because quite often this is not the case. The animals often just get plant residues from maize or Napier that is grown at the roadside.\nOr then people just send their cattle into a forest, which you also want to avoid because that leads to a certain degradation of forests.\nBut again land sizes are small and very often the land a farmer has is rather used for the cultivation of maize or some vegetables, for example.\nOnce you have certain manure amounts available on your farm, you want to apply it where you get the highest revenue, and that is probably a maize field or a vegetable garden.\nEven if you have a Napier plot but you don\'t fertilize it, your yields go down over time. The crucial point is that you\'re always taking something away. As soon as you harvest, you take nitrogen, for instance, away from your field.\nIf you don\'t bring that back to the field in the form of manure you\'re starting to mine nutrients from your soil and that is what is continuously happening in Africa and leads to low soil health.\nIf you want to have a cow which really produces, you need to feed it properly. For that you need to cultivate the field to forage grasses, you need to fertilize that etc. But that\'s not happening everywhere. There is a lack of manure in some places. So it\'s not as simple a solution as just suggesting a grass, you always have to look at the system.\nWhy is it better that cattle eat Napier grass rather than grazing in the forest?\nJones: So it\'s basically about nutritional quality. The more productive an animal, the less emissions per unit of productivity. So the basic thing is that when they graze in the forests they\'re basically scavenging or going out looking for a low quality forage, which is anything that an animal will eat.\nWhereas what we would say is that if you plant forage and manage it correctly, you have much more productivity per unit area, a much higher quality of feed going into the animal. Is there a suggestion that we then give more land over to Napier grass or similar kinds of grass production and therefore clear forests to make way for better cultivated Napier grass?\nMerbold: You shouldn\'t clear forests. That\'s not an option as these are big carbon storages and also hotspots for biodiversity. But what you can do is basically do a better farm management. In the end an improved farm management allows you to gain more from your land.\nIf you want to keep cattle and you want to produce something with cattle then you need to cultivate the forages and by that you are improving a system and your emission intensities emissions of methane per kilogram of product, such as kilogram of meat, go down. If not, those remain relatively high.\nLutz Merbold is head of the Mazingira Centre at the International Livestock Research Institute (ILRI) in Kenya. Chris Jones is program leader of Feed and Forage Development at ILRI. The interviews were carried out by Melanie Hall.']"	['<urn:uuid:06057c90-f112-41b3-b929-49a48bedc9a3>', '<urn:uuid:4b71ae96-2641-4177-9c67-e7f0992e42dc>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-13T00:43:36.053682	7	122	2498
98	I'm having my wedding next month and I heard some British royals had a special plant in their bouquets - what's the story behind that?	Myrtle has been a royal wedding tradition since the Victorian era. It began when Queen Victoria received a myrtle bouquet from Prince Albert's grandmother and planted the sprigs at Osborne House in 1845. Since then, myrtle has been included in every royal wedding bouquet, including Queen Elizabeth II's in 1947, Princess Diana's in 1981, and Kate Middleton's in 2011.	['True myrtle or common myrtle ( Myrtus communis) is historically one of the most significant plants. It is mentioned a number of times in the bible and throughout the ancient mythology of Greece and Rome. Fragrant myrtle is a useful culinary ingredient and has been valued for its medicinal properties for centuries.\nMyrtle (myrtus communis), also known as true or Roman myrtle, is a hardy, evergreen, highly fragrant shrub with dark green oval shaped leaves and attractive white flowers followed by black berries, during blossoming season (June-July). The exact origin of myrtle is unknown, although generally speaking it is native to the Mediterranean region, North Africa and the Middle East. This attractive, aromatic plant has a prominent place in ancient European mythology and was valued in traditional herbal medicine.\nMyrtle became closely associated with Aphrodite, the Greek goddess of love, beauty and fertility from about the eighth century B.C, when Aphrodite was revered in the gardens of Athens; myrtle at her feet. The famous Venus de Milo statue led to the inspiration of the Roman equivalent of Aphrodite, which was Venus. Venus was described in literature holding a sprig of myrtle as she emerges from the sea, and as with Aphrodite, myrtle was grown around her temples and gardens. Roman ladies wore myrtle wreaths and customarily bathed in myrtle scented water.\nThis led to the tradition of holding a wreath of myrtle over a brides head, during wedding ceremonies in many European countries. In the Victorian era Queen Victoria began a royal tradition after she was gifted a bouquet of flowers, including myrtle, by Prince Albert’s grandmother. The myrtle sprigs were planted at Osborne House on the Isle of White in 1845, and myrtle was included in the bouquet of the 17 year old Princess Victoria, eldest child of Queen Victoria, when she married in 1858. The historic custom would continue to be part of every royal wedding to date, including the 1947 wedding of Queen Elizabeth II; the 1981 wedding of Diana Spencer ( Diana, Princess of Wales) to Prince Charles and myrtle was also included in the bridal bouquet of Kate Middleton for her wedding to Prince William on April 29, 2011.\nIn Judaism (Hebrew) myrtle is known as Hadassah, which is a highly significant name because it was the birth name of Esther, the Jewish queen to a Persian king who saved the Jews from certain annihilation in Persia.\nVarieties of Myrtle: Myrtus communis variegata, better known as variegated common myrtle is an evergreen, perennial shrub that can grow to a height of 3 ft. This plant bears white flowers followed by blue/purple-black berries. The leaves of this plant are the smallest of any Myrtus species and have variegated (white) edges. They can be used in recipes like bay leaf, although the flavor is less concentrated than other species. Myrtle merion, botanical name Myrtus communis merion, is a hardy evergreen, although less hardy that any other myrtus species. This species is lager and more bushy than the aforementioned. Myrtus communis or true myrtle grows wild and is cultivated throughout the Mediterranean region. This species can grow as tall as 16 ft and has the largest leaves and flowers of any species.\nMyrtle and its Culinary Uses: Myrtle leaves are usually dried and used like bay leaf. They have a flavor similar to allspice, with a touch of menthol. The flowers are used as a garnish and myrtle berries are dried, ground and used like a spice, in a similar way to juniper berries. Myrtle leaves and berries are used to season lamb and pork dishes in Middle Eastern cuisine but are far less popular in the west, although they can be found in some western supermarkets. In Italy and on the island of Sardinia, where myrtle grows wild, the herb is a staple spice used in the kitchen and also for wood smoking to impart a distinctive flavor to barbecued food. At the end of a meal Sardinians and also people on the island of Corsica often enjoy a glass of Mirto. Mirto is a liqueur produced from both myrtle berries, known as sweet, (rosso) and myrtle leaves, Mirto bianco.\nThe Nutritional and Medicinal Value of Myrtle: As an herbal folk medicine, myrtle has had numerous application in many cultures from Africa to northern Europe. The essential oil was traditionally used externally to alleviate acne and skin diseases. In Turkey, an herbal tea is made from dried myrtle leaves and berries and is used to help diabetics by lowering blood glucose levels. A similar decoction is used to treat unrinary and bladder infections in the Mediterranean region. This plant also has curative and restorative properties.\nMyrtle and in particular myrtle berries contain many compounds that are thought could be beneficial to human health. One group of flavonols known as myricetin, found in glycosides, are thought to have anti-cancer, anti-inflammatory and antioxidant properties. Myricetin may alleviate the suffering of Alzheimer patients as it is known to inhibit beta-amyloid fibril formation. The flavonoids in myrtle, including myricetin, are heart healthy as they can help reduce the oxidization of LDL cholesterol, which leads to heart disease. Studies using myricetin on diabetic rats have demonstrated that the flavonol reduces glucose plasma levels and might be beneficial to diabetics.\nMyrtus communis: Above image published at wikipedia.org with creative commons licence. Primary image, published at wikipedia.org.']	['<urn:uuid:6e190283-35c5-4dac-9e9f-37f02b055a4c>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	25	59	893
99	who are biological parents new washington apple named wa 2	WA 2 originated from a seed collected from the Splendour cultivar. The male parent is unknown, although the Tenroy cultivar of Gala apple trees surrounded the Splendour tree and may have been the pollen source.	"[""Patent application title: Apple Tree Named 'WA 2'\nBruce H. Barritt (Okanagan Centre, CA, US)\nWASHINGTON STATE UNIVERSITY\nIPC8 Class: AA01H500FI\nPublication date: 2010-09-30\nPatent application number: 20100251440\nA new and distinctive variety of an Malus domestica `apple` tree, named\n`WA 2`, that is distinguished by its attractive blush, outstanding\ntexture being firm, crisp and juicy, and the presence of the homozygous\ngenes for ethylene production ACSI-2/2 and ACO1-2/2 which contribute to\nmaintaining the fruit quality over long periods of storage.\n1. A new and distinct apple tree variety named `WA 2`, as herein shown and\nLATIN NAME OF THE GENUS AND SPECIES OF THE CLAIMED PLANT\nBotanical/commercial classification: Malus domestica/apple tree\nVarietals denomination: `WA 2` seedling designation T19-17-3-9427\nThe invention refers to a new plant variety of apple tree (Malus domestica) named `WA 2`. This new variety is distinguished by the distinctive blush of its fruit, with the texture being firm, crisp and juicy even after 60 days of regular cold storage due to the presence of the homozygous genes for ethylene production, ACS1-2/2 and ACO1-2/2.\nBACKGROUND OF THE INVENTION\n`WA 2` (seedling designation T19-17-3-9427) originated from a seed collected in 1994 from fruit of the `Splendour` cultivar. The male parent is unknown although the `Tenroy` cultivar of `Gala` apple trees surrounded the `Splendour` tree and may have been the pollen source. The germinated seedling was grown in a greenhouse at the Washington State University Tree Fruit Research and Extension Center (WSU TFREC) in Wenatchee, Wash. during the Summer of 1995. In September 1995, a bud from the seedling was budded to M9 rootstock and the resulting `mother` tree was planted in the evaluation orchard at WSU TFREC in the Spring of 1997. Fruit from the `mother` tree were evaluated in 2000 and 2001. Second generation trees were produced by chip-budding to M9 rootstock Fall of 2002 and the resulting trees planted at three locations in Washington State in 2004 near Chelan, Wash., Chelan County; near Wenatchee, Wash., Douglas County; and near Basin City, Wash., Franklin County. Fruit from the second generation trees at all three orchard locations were compared to that of the `mother` tree in 2005, 2006, 2007, and 2008, and found to be essentially similar to that of the `mother` tree.\nSUMMARY OF THE INVENTION\n`WA 2` is an attractive apple that is distinct in appearance from that of other commercially grown apples and has outstanding texture, being very firm, crisp and juicy, both fresh and after 60 days of regular cold storage. Firmness is comparable to the commercial varieties of `Cripps Pink` and `Braeburn` and is firmer than the commercial varieties `Gala` and `Fuji` (Table 1, second column). Fruit of `WA 2` is sweet with percent soluble solids greater than for `Gala` and `Braeburn` and is comparable to `Fuji` and `Cripps Pink` (Table 1, third column). Fruit acidity level of `WA 2` is intermediate between low acidity cultivars `Gala` and `Fuji` and high acidity cultivars `Braeburn` and `Cripps Pink` (Table 1, fourth column). Overall flavor is well balanced between sugar and acid. `WA 2`, like its maternal parent, is homozygous for the ethylene production gene Md-ACS1. In addition it is homozygous for a second ethylene production gene Md-ACO1. The ACS and ACO genotypes were determined by fragment analysis of the results of polymerase chain reactions using the primers published by Harada et al. (2000) and Costa et al. (2005).\nBRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1. The eight year old mother tree of `WA 2` on M9 rootstock near full bloom.\nFIG. 2. `WA 2` mother tree fruit at harvest maturity.\nFIG. 3. View of `WA 2` fruit showing exposed and backsides, and stem and calyx ends.\nFIG. 4. View of typical `Tenroy Gala`, `WA 2` and `Splendour` apples at harvest maturity.\nDETAILED BOTANICAL DESCRIPTION\nThe following detailed description is from the `mother` tree grown at WSU TFREC. All color references are from the R.H.S. colour chart by The Royal Horticultural Society. Tree: Type: Considered spur type with development mostly on 2 yr. old wood, some on current seasons growth; spur length ranges from 1 to 8 cm Vigor.--considered moderate. Overall shape.--upright and spreading. Height.--14 feet. Width.--7 feet. Hardiness.--considered hardy for the regions grown in. Productivity.--considered very productive, exhibits alternate bearing tendencies if over-cropped the previous season. Trunk: Size.--diameter at a height of 45 cm is 11.4 cm. Bark texture.--smooth. Bark color.--from the greyed-orange group (177A). Lenticels.--numerous and horizontal to the plane, size averages 9.9 mm×1.8 mm. Lenticels color.--from the brown group (200C). Branches: First year branches.--considered moderate in number with light spur development; spreading habit with moderate crotch angles of 0° to 35° Color: from the greyed-purple group (187A) Lenticels: round to slightly elongated and oriented parallel to growth with color from the white group (N155D); size average is 0.5 mm×1 mm; density is at 11 per running cm Branch pubescence: present, moderate, with color from the white group (155D) Internodes: spacing averages 4.2 cm. Scaffold branches.--moderate in number; spreading habit with crotch angles of 30°-80° Color: from the grey-brown group (N199D) Lenticels: numerous and horizontal to plane, rough; averages 10.2 mm×2.1 mm in size. Lenticels color.--from the white group (N155C). Leaves: Shape.--considered oval. Texture.--considered leathery. Sheen.--upper surface is glossy. Pubescence.--lower surface is covered with fine pubescence. Length.--ranges from 7.2 to 10.5 cm; average length is 9.2 cm. Width.--ranges from 4.6 to 7.3 cm; average length is 6.0 cm. Margin.--considered double serrate. Tip.--considered cuspidate. Base.--considered obtuse. Stipules.--present on most leaves; average number per leaf is 0.8 with a range of 0 to 2 per leaf; average length is 6.3 mm; width ranges from 0.8 to 1.0 mm. Leaf color.--upper surface from the yellow-green group (147A); lower surface from the yellow-green group (147B). Petiole.--lengths range from 2.2 to 3.9 cm, average is 3.3 cm; average diameter is 1.7 mm; color is from the green group (142D) with high lights from the greyed-purple group (186A). Flowers: Full bloom date May 5, 2008 at Wenatchee, Wash.; number of blossoms per bud ranges from 4 to 6 -- mostly 5 -- with an average of 4.6 blossoms per bud Size.--individual flowers considered medium in size averaging 50.2 mm across. Petal.--width averages 17.9 mm; length averages 21.1 mm. Color.--from the white group (N155B) with highlights from the greyed-purple group (186D); petal vein color from the greyed-purple group (186B). Stamen.--range of 20-22 mm length with an average length of 20.7 mm Filament: color is from the green-yellow group (1D) Anthers: color at maturity from the yellow group 4C; average length is 2.1 mm. Pistil.--average length is 7.9 mm Styles: 4 to 6 in number generally 5, fused at base and pubescent at union; average length is 3.2 mm; color is from the yellow-green group (154C) Stigma: club shaped; color is from the yellow-green group (154B). Sepals.--five per bloom and occasionally curled back towards the peduncle; average length is 7.2 mm and average width is 4.2 mm; colorless pubescence is present on both upper and lower surfaces; sepal color is from the green group (138B) with tips highlighted from the greyed-red group (178B). Fruit: Observations and testing from mother tree fruit Size.--considered medium averaging 7.7 cm in equatorial diameter and 7.0 cm in axis diameter. Form.--considered round-conical. Stem.--medium reaching above stem bowl and averaging 2.3 cm in length; medium stout averaging 3.3 mm in diameter at midpoint; exhibits pronounced clubbed end. Stem cavity.--average width is 2.9 cm; average depth is 1.7 cm. Cavity shape.--acuminate. Lipped.--no. Basin cavity.--average width is 31.2 mm; average depth is 17.6 mm; some light ribbing. Calyx.--eye is generally erect. Skin.--smooth with bloom present. Appearance.--blush over 75-95% of surface. Lenticels.--numerous, distinct, ranging from 6-15 per square cm. Lenticels size.--round and ranging from 0.3-0.7 mm in diameter; color from the white group (157D). Skin color.--blush color is from the red group (47A); undercolor is from the yellow group (4C). Core.--core line attachment is medium. Core position.--is distant. Cell.--Tufted: no Cell Shape: obovate. Tube.--funnel shaped. Sepals.--downy. Stamen position.--medium. Axis.--axile and closed. Seed.--Number: 1-4 per cell with an average of 3 Shape: acute; size averages 4.5 mm in diameter×8.1 mm in length Color: from the brown group (200D). Flesh.--firm, crisp, melting texture, considered mildly acidic with excellent sugar balance; color from the yellow-white group (158D). Aroma.--mild, apple like. Date of harvest maturity.--Aug. 6, 2008; five year average is August 2. Maturity indices.--Starch: Cornel chart is 3.6 Pressure: in Newtons is 82.8 Soluble Solids: in °Brix is 12.3 Titratable Acid: as mg/l malic acid is 0.44. Genotype.--`WA 2` is homozygous for the ethylene production genes Md-ACS1 and Md-ACO1. Keeping quality.--excellent, exceeding 90 days in common storage with little to no loss in firmness and acidity. Pollination: Any diploid apple of the same bloom season Use: Primarily for fresh eating Disease and insect resistance: Considered susceptible to all apple insects and has known susceptibility to apple powdery mildew; susceptibility to other apple diseases is unknown\nTable 1 represents fruit indices of the `WA 2`, `WA 2` second generation, `Gala`, `Braeburn`, `Cripps Pink` and `Fuji` varieties at harvest maturity and following 60 days in common storage. The differences between fruit from the `WA 2` mother tree and from the `WA 2` second generation trees are as expected from an apple cultivar grown in different environments.\nTABLE-US-00001 TABLE 1 Fruit indices at harvest maturity and following 60 days in common storage Sugars Acidity Firmness % Soluble mg/l Newtons solids malic acid Cultivar Fresh Storage Fresh Storage Fresh Storage Year `WA 2` 77 70 13.9 14.3 0.43 0.35 2008 `WA 2` 81 76 13.9 14.0 0.51 0.40 2008 2nd gen. `Gala` 72 73 13.3 14.1 0.58 0.34 2008 `Braeburn` 86 82 14.5 15.4 0.87 0.51 2008 `Cripps Pink` 84 82 15.6 16.0 0.94 0.66 2008 `Fuji` 72 64 14.5 15.7 0.56 0.46 2008\nThe attached figures show the `WA 2` tree and various aspects of its fruit. FIG. 1 is a photo of the eight year old mother tree of `WA 2`. FIG. 2 is a close up of the apples at harvest maturity on the mother tree. FIG. 3 shows representative views of the exposed and backsides of the `WA 2` fruit, in addition to the stem and calyx ends of the fruit. FIG. 4 shows the `WA 2` fruit as compared to the fruit of its two parental varieties at harvest maturity. A typical `Tenroy Gala` apple, suspected of being the male parent cultivar, and a typical `Splendour` apple, are show next to a `WA 2` apple.\nPatent applications by WASHINGTON STATE UNIVERSITY""]"	['<urn:uuid:1a84765c-f71d-49b9-84cd-daf608d8df24>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T00:43:36.053682	10	35	1739
100	quaker meeting for sufferings versus catholic diocese leadership structure comparison	Meeting for Sufferings and Catholic dioceses have different leadership structures. Meeting for Sufferings has representatives appointed from Area Meetings who meet monthly in London using collective decision-making methods. Catholic dioceses are led individually by bishops who have authority over priests and deacons in their jurisdiction, following a clear hierarchical structure.	['Introducing Quakers: Meetings for Business\nLocal Meetings meet approximately monthly to transact whatever business must be undertaken. ‘Concerns’ are brought forward by members, shared with those present at the business meeting, and decisions are made about whether and how to take the matter forwards. Quaker business meetings start in silence. Area Meetings, groupings of Local Meetings, also meet for business every month or two. Britain Yearly Meeting is the grouping of all Area Meetings in Britain, and meets annually. Meeting for Sufferings transacts Yearly Meeting level business during the period between two Yearly Meetings.\nLocal Meetings, Business Methods and Concerns\nWhilst Quakers meet regularly for Meeting for Worship, having no clergy as paid organisational staff (at least, that is the case within Britain Yearly Meeting) means that all the business of running the organisation, other than at a national level, must be carried out by its members. Local meetings meet approximately monthly to transact whatever business must be undertaken. At the most mundane level, this business concerns activities such as buying supplies for hot drinks, ordering books for the library, organising repairs to the Meeting House, and arranging to have the accounts audited. Less prosaically, decisions are made about which charities to support, and, significantly, ‘concerns’ are brought forward by members, shared with those present at the business meeting, and decisions are made about whether and how to take the matter forwards. Example of concerns might include, say, unease about growing financial inequality in British society; outrage that welfare benefit claimants are being penalised for spurious reasons; compassion for people in Canterbury who have no overnight accommodation and are forced to live in the streets. Those present at the business meeting might decide to schedule some action, for example, holding a silent vigil in a public place in order to highlight growing financial inequality. It might be decided to support a local charity for homeless people, such as Catching Lives. It might be decided that the concern should be presented to Area Meeting, which might then decide to present the concern to Meeting for Sufferings. Alternatively, the business meeting might decide that the Meeting does not share the concern, and the matter is not pursued further.\nQuaker business meetings are held in a manner not wholly dissimilar to that of Meeting for Worship. They start in silence. The Clerk reads out the Minutes of the previous business meeting, and addresses any matters arising from these. An agenda will already have been drawn up. The purpose of the meeting is to attempt to discern ‘the will of God’ regarding whatever matter is being addressed. Decisions are made not by voting, nor by consensus, but by agreement that the correct way forward has been found. Sometimes this can be easy, on other occasions it can be very difficult. The process requires, amongst other things, self-discipline, mutual respect, and a willingness to recognise that one’s own preferences may not be the best way forwards. Only one person may speak at a time, and the Clerk may ask that the meeting spends further short periods of time in silence to allow ‘the sense of the meeting’ to become clear. When the Clerk feels able to do so, a draft Minute regarding that agenda item will be written by the Clerk (who may be assisted by the Assistant Clerk), and Friends are asked whether the Minute accurately reflects the will of the meeting, and ‘the will of God’. Amendments to the Minute may be proposed and discussed. This process helps to refine the issue. The amendments may be made incorporated into the Minute until eventually the Clerk asks “Friends, is this Minute acceptable?” to which, if the Minute is acceptable, the answer is “I hope so” meaning ‘I hope that this is the will of God.’ Any member of the Local Meeting may attend the business meeting, and anyone not in membership can ask permission to attend\nWhilst Local Meetings are primarily about religious observance and fellowship, with monthly business meetings to take care of business and to address concerns, Area Meetings on the other hand are primarily about business and addressing concerns. Agenda items may come from a Local Meeting, such as to bring forward a concern; or may be associated with Area Meeting business, such as making appointments to specific roles, and appointing representatives to attend a specific conference; or may originate from Meeting for Sufferings, or some part of the administrative core of the Yearly Meeting based in Friends’ House, London. Concerns that have been brought to Area Meeting from a Local Meeting may subsequently be forwarded to Meeting for Sufferings.\nThe same business methods that are used in the business meetings of Local Meetings are employed. Maybe meeting monthly, Area Meetings also involve both fellowship and Meeting for Worship. A shared lunch or tea is quite common. Any member of a constituent Local Meeting may attend the Area Meeting, and anyone not in membership can ask permission to attend. However, occasionally a person not in membership may be asked to leave the meeting temporarily while confidential matters, particularly regarding membership, are discussed.\nMeeting for Sufferings\nEach Area Meeting appoints one or more Friends to represent the Area Meeting at Meeting for Sufferings. Meeting monthly in London, Meeting for Sufferings addresses the concerns brought to it from the Area Meetings. The same business methods that are used in the business meetings of Local Meetings are employed. With so many Friends present, this requires great discipline, as well as discernment on the part of the Clerks.', 'What is the chain of command in the Catholic Church?\nThe hierarchy of the Catholic Church consists of its bishops, priests, and deacons. Dioceses are divided into individual communities called parishes, each staffed by one or more priests, deacons, or lay ecclesial ministers. Ordinarily, care of a parish is entrusted to a priest, though there are exceptions.\nWhat is the order of seniority in the Catholic Church?\nAs noted above, the first consideration for precedence is always the hierarchy of order : first bishops, then presbyters, next deacons. At earlier times in the Church’s history, deacons were ranked above presbyters, or the two orders considered equal, but the bishop always came first.\nIs a monsignor higher than a bishop?\nNo ecclesiastical rank is higher than a bishop . Monsignor is an honorific title that can be granted to a clergyman of the major orders. No, a Monsignor is an honorary title for a priest. A Bishop is named by the Pope for his diocese.\nWho is above a bishop in the Catholic Church?\nIn the Catholic Church , archbishops and bishops rank below cardinals. Becoming a bishop is the third and fullest level of the Sacrament of Holy Orders. The first level is the ordination of a deacon, the second is the ordination of a priest, and the third is the ordination of a bishop .\nDoes the pope get a salary?\nHow much does one get paid for being pope ? Nothing. In 2001, the Vatican confirmed that the pope “ does not and has never received a salary ”. On his election, the pope -elect’s private property is usually either donated to the church, transferred to a family member, used to endow a foundation, or placed in trust.\nIs a dean higher than a bishop?\nis that bishop is a high ranking official in the catholic church who governs a diocese, or a similar official in other denominations and religions (occasionally abbreviated as when used as a title) while dean is a senior official in a college or university, who may be in charge of a division or faculty (for example,\nWho is the highest person in the Catholic Church?\nIs a canon higher than a priest?\nCanons may be members of the diocesan/bishop’s staff rather than cathedral staff, such as in the Episcopal Church (United States), where a diocese’s ” Canon to the Ordinary” is a senior priest who works directly for the diocesan bishop (Ordinary).\nWhat is the difference between a bishop and a monsignor?\nFor a short while, both bishops and high-ranking priests were called ” monsignor .” Although bishops are still referred to as ” monsignor ” in some European countries, most commonly in Italy, in the rest of the world, ” monsignor ” has come to refer only to priests who have been granted the title.\nDo you call a monsignor father?\nIf a priest has the honorary title of Monsignor , address him as ” Monsignor Last Name” instead of ” Father “, following the same rules concerning verbal and written forms of address for priests. The use of ” Father ” as a verbal title originated in Europe and was only used with priests who were members of a monastic order.\nHow much does a Catholic monsignor make?\nAccording to the Bureau of Labor Statistics, the median annual salary for clergy is $45,740 . Median income is the midpoint, meaning the top half of wage earners made more, and the bottom half made less.\nDo you have to be a monsignor to be a bishop?\nThe 1969 Instruction of the Secretariat of State indicated that the title of ” Monsignor ” may be used for bishops . This is normal practice in Italian, French, Catalan, and Spanish, but not in English.\nWhich church is the richest?\n|Organization||Worth [billion USD]||Country|\n|Catholic Church||Unknown||Vatican City|\n|The Church of Jesus Christ of Latter-Day Saints||100||United States|\n|Catholic Church in Germany||26.0||Germany|\n|Protestant Church in Germany||25.0|\nCan a bishop be married?\nBishops must be unmarried men or widowers; a married man cannot become a bishop . In Latin Church Catholicism and in some Eastern Catholic Churches, most priests are celibate men. Exceptions are admitted and there are over 200 married Catholic priests who converted from the Anglican Communion and Protestant faiths.\nHow does a Catholic priest become a bishop?\nA bishop position will not open until the current bishop reaches 75, becomes very ill, or dies. Once there is a bishop position available, the local bishops will nominate priests to become the new bishop . These names are given to the archbishop who reviews the nominations and then organises a vote to be carried out.']	['<urn:uuid:a3c3c166-2463-4f7b-aa66-80459cda9102>', '<urn:uuid:067629d5-7cc2-4942-abc8-efc59a0713a4>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T00:43:36.053682	10	50	1700
