qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	digital photography expert need technique shoot landscapes wide high contrast sky dark ground procedure	The technique involves shooting bracket images for each segment of the panorama using a tripod. First, engage the camera's bracketing feature and shoot at least three frames for the first segment. Then pan horizontally with some overlap and shoot another bracketed set until the whole scene is covered. This helps balance exposure between bright sky and dark land.	"[""Watch video: Create an HDR Panorama in Lightroom Classic CC\nBoth Photoshop CC's Camera Raw plug-in and Lightroom have long offered separate HDR and Panorama features, but there's also a handy feature that combines the two commands into one.\nPreviously, we’d have to merge HDRs first and then stitch them into a panorama after, but this handy new command performs both tasks at once. It produces a detail-rich Raw panorama with a heavily expanded dynamic range, making it easier to tease detail out of the tonal extremes. In this straightforward tutorial, we’ll explain how it’s done.\nThere are many situations in which a HDR panorama might be a useful technique. More often than not, the point of a panorama is to capture a sweeping landscape, achieved by shooting several segments, then stitching them together. However with landscapes there’s usually a slight imbalance between land and sky – if we expose for the sky the land comes out dark, while exposing for the land can end up blowing out the sky.\nThe solution is to shoot bracket of images for each of these segments. We of course start by using a tripod, engaging the camera’s bracketing feature and then we shoot at least three frames for the first ‘segment’ of the panorama, then pan the camera horizontally (ensuring there’s a little overlap) and shoot another bracketed set, going on until the whole scene is covered. We may, as you would expect, end up with a dozen or more shots, thankfully they’re easily united with the ‘Merge to HDR Panorama’ feature.\n01 Open the image set\nOpen Adobe Bridge and navigate to your set of bracketed panoramic images or download the start images here to edit along. If you're using Lightroom, you'll need to import the photos and go to the Library module. Click on the first image, then hold Shift and click on the last to select them all. Then right-click them and ‘Open in Camera Raw’.\n02 Begin the merge\nOnce open, hit Cmd/Ctrl+A to select all the images in Raw, then right-click the selection and choose ‘Merge to HDR Panorama’ from the list that appears. After a few seconds, the merged .dng image will appear in the ‘HDR Panorama Merge’ dialog box.\n03 Experiment with projection\nThe three options – Spherical, Cylindrical and Perspective – at the top right each affect the way the panorama is stitched. Click through to find the best option for your image set. At this stage it can be useful to uncheck ’Auto Crop’ so you can see the edges.\n04 Fix the edges\nThe Boundary Warp slider works by reshaping the edges of the panorama. Here dragging it to 80 allows us to correct for the tilt in the set of shots (which often happens with panoramas). We can recheck Auto-Crop to tidy the edges up. Once happy, hit Merge.\n05 Tidy up any ghosting\nFor us, slight movement has caused ghosting in these flowers. Grab the Spot Removal tool from the toolbar and paint over them to remove these distractions to make the image look flawless.\n06 Enhance the tones\nWe use tonal sliders and the local adjustment tools to enhance our shot. Get the Graduated Filter tool, click the minus icon next to the ‘Exposure’ slider and drag down from sky to land. Next, up Dehaze to make the clouds more intense and make any final adjustments.\nPhotoPlus: The Canon Magazine is the world's only monthly newsstand title that's 100% devoted to Canon, so you can be sure the magazine is completely relevant to your system.\n100 Photoshop tips\nLightroom Classic review\nThe best photo editing software: image editors for novices through to pros\nThe best photo-editing laptops: top laptops for photographers\n10 best online photography courses – from beginner guides to masterclasses""]"	['<urn:uuid:433da92c-40ad-4fe9-a97e-105765f1f48f>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:10:11.825225	14	58	632
2	I'm a history fan - when did Philadelphia become America's biggest city?	By 1740, Philadelphia was the largest city in the colonies.	['WHY GO: From its inception, Philadelphia has been a city of diverse “Makers.” Founded as a center of commerce rather than a religious colony, newcomers didn’t have to pay that irksome Church tithe, so strivers from all backgrounds sought their fortunes here. By 1740, Philadelphia was the largest city in the colonies – an engine of industry. In 1754, one German immigrant observed, “Pennsylvania is heaven for farmers, paradise for artisans, and hell for officials and preachers.” This “paradise for artisans” has gone through a rebirth in recent years, revitalizing Philadelphia’s flagging neighborhoods, bringing a distinctive creative energy to each. You can explore each of 14 identified neighborhoods – none more than a 15 minute walk or drive from Old City – through hot restaurants, attractions and shops. Or, become a “Maker” yourself in one of the many studios cropping up all over the city. Highlighted here are a few neighborhoods to get you started.\nOLD CITY/HISTORIC PHILADELPHIA\nThis is the Philadelphia that most tourists know. Independence Hall, the Liberty Bell, Constitution Museum – all here. But as you can see, Old City encompasses so much more.\nTOUR: Elfreth’s Alley. Built in 1702, Elfreth’s Alley is known as “America’s oldest residential street.” Artisans, including a free African tailor, a Jewish furniture maker, German shoemakers and bakers, female dressmakers (Mary Smith and Sarah Milton) among others lived side by side in 32 tiny homes that also served as shops, sales rooms and studios. Take a tour with loquacious Ed Mauger, President of Elfreth’s Ally Association, and he’ll let you in on a few “clues” about how to tell 300 year old homes from the newest 200 year old homes (hint – it’s in the bricks and number of steps), and point out some of Ben Franklin’s innovations still in use (triple mirror outside 2nd floor window to see who was at front door, and street lamps made with replaceable panes of glass and vent for oil heat). Tours $5, Noon and 3pm, April – December, Tuesdays – Saturdays: 10-5, Sundays: 12-5\nVISIT: National Liberty Museum. It’s a quirky little museum dedicated to the idea of Freedom and its definition. Though you’ll be captivated by beautiful Chihuly glass art, go for this reason: you can hear our “original Liberty Bell,” made in the same foundry in England with the same materials – this one without the crack. $7 adults, $2 kids, open Tues-Sat 10-5, Sun Noon-6.\nDO: The Center for Art In Wood. Create your own “Narrative Vessel,” dreidel or other wooden crafts at this museum cum workshop. Even if you don’t sign up for a four 4-hour hands-on class, stop in to admire the sensual works of art. $35 Make and Take Workshops, One Saturday per month Noon-4pm.\nDO: The Clay Studio. This isn’t your typical “paint already made clay pieces.” The Clay Studio is an actual throw the pot kind of place, and offers Saturday clinics and the occasional Friday Night Date Nights with wine and beer, where you can reenact the sexy pot-throwing scene from Ghost – if only in your head.\nSEE: A Show at Fringe Arts. The new permanent theater and headquarters for what is locally known as “The Fringe” is now based inside a 1903 Fire Pumping Station, the first of its kind in the country and built to create enough pressure for water to reach the tops of new-fangled turn of last century 30-story skyscrapers. Decommissioned in the 1950’s and vacant until 2012, “this building used to save lives and now it will enrich lives,” says David Harrison, VP, who points out the cast iron pipes and pressure gauges kept as architectural elements. With year-round programming, classes, concerts, and events in a 240-seat theater– not to mention a soaring bar/restaurant designed for live entertainment and priced for starving artists to billionaires, Fringe Arts will surely bring offbeat-theater-loving mobs to this special spot under the Ben Franklin Bridge on the Delaware River. Consult website for shows and times\nSHOP: Art in the Age of Mechanical Reproduction. “Art in the Age” began as a T-Shirt line and now stocks all manner of handcrafted items, aka “stuff made with thought.” Shop for jewelry, artwork, clothing and books, including upcycled accessories from wunderkinds Walter and Margeaux Kent’s Peg and Awl. Art in the Age has also partnered with the group that created Henricks Gin to distill 80 proof spirits. Though you can’t purchase bottles of these robust liqueurs named Root (from root beer), Snap (gingersnap) and Rhubarb, in the shop, you can taste them for free during Thursday Night Happy Hour and then buy them from local liquor stores.\nSHOP: United By Blue. Trendy t-shirt merchants, Mike Congi and Brian Linton vow to remove one pound of trash from the planet’s oceans and waterways for every product sold. Utilizing their considerable marketing talents for good, United By Blue is spreading countrywide. This flagship store is the original, and includes a coffee bar where you can purchase a shot of organic esspresso while considering how many t-shirts to buy.\nBest Places to Eat In Old City, Philadelphia\nEAT: Zahav. A selection of salads, fresh-baked breads, humus, Moroccan Chicken ($10), Veal Stuffed Grape Leaves ($12); this is not your father’s Falafel restaurant. James Beard winner, Michael Solomonov elevates Israeli cuisine to remarkable heights in this contemporary golden space (Zahav means “gold” in Hebrew). You’ll want to order from the four course-tasting menu ($39), an insider favorite, so you can all share every single delight.\nEAT: Paesano’s. It’s the kind of place where the ole spring-loaded screen door slaps shut behind you as you cram into a line three-deep waiting to order massive, made-from scratch sandwiches. There’s no place to sit, so take the namesake concoction- brisket, horseradish mayo, provolone, roasted tomato and fried egg mounded on a roll across the street to the Race Street Pier and watch boat traffic glide by as cars pass overhead on the Ben Franklin Bridge.\nEAT: Wedge and Fig. On temperate nights, opt to eat in the adorable outdoor patio garden you’ll discover down a long narrow alleyway in back of this great neighborhood “Cheese Bistro.” Bring your own wine and order cheeses “bundled by region.” Cheese Slates $18 for 3, $26 for 5. Perfect nibble following a day in the workshop.\nEAT: Han Dynasty. This is the original, the first, and the incredible authentic Szechuan food restaurant that just swept Adam Platt, New York Magazine’s food critic, off his feet after his foray to Han Dynasty’s East Village, NYC outpost. Yes, there are currently seven restaurants in the group, but go to the first on Chestnut (soon to move across the street into larger space). Signatures like the Dan Dan Noodles ($6.95), Spicy Cucumbers ($6.95) and Cold Sesame Noodles ($6.95) and entrees identified by “heat” on a 1-10 scale, represent the Gold Standard in Szechuan cuisine and have won over the Travel Channel, Food Network and other aficionados of tongue-tingling sauces.\nICE CREAM: Franklin Fountain. Though not as dense and creamy as say, Ben and Jerry’s, Franklin Fountain’s creations are just as inventive. And the black and white mosaic floor, tin ceiling, Chinese take-out cups, servers dressed in bow ties and pearls, and working 1910 cash register is a heartwarming throwback to simpler times.\nGRADUATE HOSPITAL NEIGHBORHOOD\nTransformed over the last few years, it’s just a 15-minute walk from Center City and growing in popularity with the young professional crowd. Named after the large research hospital that has since closed, Graduate Hospital is known for its bars and overflows with an eclectic mix of restaurants and is just starting to develop retail. You’ll recognize some of the residential areas from the movie The Sixth Sense. The main characters lived here.\nDO/TOUR: NextFab Studio. Proclaimed a “Gym for Innovators,” this collective workspace and idea incubator brings industrious creative’s together and is the nationally recognized premier facility of its kind in the Northeastern US. Founded in 2009, it’s a workshop on steroids, a for-profit, membership based derivative of the FabLab movement. Currently 300 members pay $1300 per year to access two MILLION dollars worth of machinery and a coterie of top-notch instructors. But even non-members can pop in and take a 2-hour class in laser-cutting, 3-D printing, Adobe Illustrator and plenty of other hands-on projects. Free one-hour tours are offered 2-6 daily on weekdays or by appointment on Sat/Sundays. Two hour classes for non-members $79-$99.\nBest Places to Eat in Graduate Hospital Neighborhood, Philadelphia\nCOFFEE: Ultimo Coffee. “Trained and certified baristas, sweet, sweet milk from Lancaster County,” Ultimo has been named “One of the Best Coffee Shops in America.”\nEAT: Honey’s Sit ‘N Eat. In the former Schwemmer’s Hardware store, goods line old shelves, antique billboards and store signs hang from the ceiling: it’s the perfect venue for comfort food. “Nothing healthy but all delicious,” says our waitress. Eggs used to make sizeable omelets come from free-range Lancaster County chickens, and calorie-counting be damned, Honey Cristo ($14) – challa French toast stuffed with double-smoked ham and Swiss cheese, is one amazing signature dish. If you’re watching pennies, come on weekdays between 7 and 9am for the $3.95 two eggs, potato, toast and coffee special.\nEAT/DRINK: Resurrection Ale House. Philly residents from all neighborhoods venture here just for the nationally renowned Fried Chicken and stay for the 12 craft beers on tap.\nDRINK/SCENE: Bob and Barbara’s. Where else can you imbibe with “liquor drinking music” out of a Hammond B-3 Organ combo? Enjoy “Thursday Drag Night” with the “Philly Special” – a shot of Jim Beam followed by a can of Papst Blue Ribbon Beer.\nNorthern Liberties area of Philadelphia was so named because plots of land in the then rural area were given away free with the purchase of acreage in Center City as per William Penn in 1682. Now, steel and glass contemporary buildings coexist with original brick row houses drawing artists and hipsters to this once-blighted former domain of Schmidts Brewery. Developed by Philadelphia’s answer to Donald Trump, Bart Blatstein, the Northern Liberties district has been re-imagined as a place where people can live, work, shop and eat within a cozy few blocks, so it’s no surprise that the neighborhood exudes a trendy European/Founding Fathers vibe.\nVISIT: Yard’s Brewing Company. The owner of Yards Brewery, Tom Kehoe, is as innovative as his brews are awesome. “Ales of the Revolution” named for Washington, Jefferson and “Poor Richard” (Ben Franklin Ale already existed) were brewed following our founding fathers’ own recipes from the 1700’s. Belly up to the tasting bar- a recycled bowling ally lane, to sip dark but mild Brawler, bestseller citrusy Philadelphia Pale Ale and “sweeter than most” IPA among plenty more. Each week a different food truck is stationed right outside the door, and Yards curates a beer-pairing menu for an ultimate lunch or dinner experience. Free 30-minute tours Sat and Sun noon – 4pm – you get three tastes. Tasting room open daily noon to 7pm.\nTOUR: Taste of Northern Liberties. If you learn about history and place through your stomach, this is the perfect tour for you. Humorously entertaining guides lead you through the Piazza and Liberties Walk on a 2-½ hour tour of Northern Liberties, where you’ll sample treats from Bubble Tea to Thai noodles at five restaurants. At $44, this is one of the best deals around. Sat/Sun 1:30-4pm.\nCOFFEE: One Shot Cafe. Philly is known for its coffee-shops-that-are-not-Starbucks, which is a good thing. Indie One Shot fuels this energetic neighborhood.\nEAT: Bar Ferdinand. One of the most stunning interiors, it seems delectable even before you take a bite. But BF is known for excellent Tapas and has the Best Brunch in the City, according to fans.\nBest Places to Stay in Philadelphia, PA\nSTAY: Hotel Monaco or Palomar, Philly. These are Kimpton Hotels where designers are allowed to romp, imagine, and dress up lobbies and rooms to whimsical, colorful, idiosyncratic, yet extraordinarily comfortable and modern effect. A phenomenal complementary afternoon wine and nibble hour just adds to the fun.']	['<urn:uuid:5039a8a0-8e35-4a3b-9e8f-93be03e2d816>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	12	10	2001
3	I'm interested in understanding how milk production patterns work. If you measure milk production on different days, are the amounts usually similar or different?	Estimates of genetic and phenotypic correlations decreased with an increase in days between when the yields were measured.	['|Degroot, Bruce - UNIVERSITY OF NEBRASKA|\n|Keown, Jeffrey - UNIVERSITY OF NEBRASKA|\n|Van Vleck, Lloyd|\n|Kachman, Stephen - UNIVERSITY OF NEBRASKA|\nSubmitted to: Genetics and Molecular Research\nPublication Type: Peer Reviewed Journal\nPublication Acceptance Date: May 25, 2007\nPublication Date: June 30, 2007\nCitation: Degroot, B.J., Keown, J.F., Van Vleck, L.D., Kachman, S.D. 2007. Estimates of genetic parameters for Holstein cows for test-day yield traits with a random regression cubic spline model. Genetics and Molecular Research. 6(2):334-344. Interpretive Summary: Traditional models for genetic evaluations use data from test-day records combined into 305-day mature equivalent lactation records. Use of a test-day model has several advantages: 1) increased accuracy of genetic evaluations, 2) direct, more precise adjustments for temporary environmental effects on test-days, 3) end-of lactation yields need not be extended for culled cows or for cows with records in-progress, and 4) models could fit lactation curves for individual cows. A random regression cubic spline function provides more flexibility to produce a “good” fit compared with polynomial functions. The cubic spline model used in this study provided flexibility for estimating genetic parameters from milk, fat, and protein test-day yields and somatic cell scores (SCS). The flexibility of the model extended to estimating genetic and permanent environmental (co)variances. Estimates of heritability increased as days in milk increased for all lactations for test-day yields and SCS. Estimates of heritability were less than previous estimates reported with other types of random regression models. The smaller estimates could be because this data set contained more grade cows than registered cows. Estimates of genetic parameters are usually smaller for grade cows compared to registered cows because of a greater chance of misidentification of sires and dams for grade cows. Estimates of genetic and permanent environmental variances for test-day yields and SCS were greater for lactations two and three than for lactation one. Lactation two had estimates of variances due to genetic and permanent environmental effects in the spline function that were more variable than estimates for lactations one and three. Estimates of genetic and phenotypic correlations decreased with an increase in days between when the yields were measured. This study showed that the cubic spline model may be a suitable method of estimating genetic parameters over the course of a lactation. From this study, the estimates of genetic parameters with the cubic spline model were comparable to estimates found with other methods. The major advantage of this method is the smaller number of variance components to be estimated compared with polynomial and multiple trait methods. Further work is needed to determine the proper number and placement of the knots for days in milk and for comparison of computational time needed to set up and solve equations with other methods used to estimate genetic parameters.\nTechnical Abstract: Genetic parameters were estimated with REML for individual test-day milk, fat, and protein yields and SCS with a random regression cubic spline model. Test-day records of Holstein cows that calved from 1994 through early 1999 were obtained from Dairy Records Management Systems in Raleigh, North Carolina for the analysis. Estimates of heritability for individual test-days and estimates of genetic and phenotypic correlations between test-days were obtained from estimates of variances and covariances from the cubic spline analysis. Estimates of genetic parameters at the average of test days within each of ten 30-d test day intervals are reported. The model included herd test-day, age at first calving, and bST treatment as fixed factors. Cubic splines were fitted for the overall lactation curve and for random additive genetic and permanent environmental effects with five predetermined knots or four intervals between days 0, 50, 135, 220, and 305. Estimates of heritability for lactation one ranged from 0.10 to 0.15, 0.06 to 0.10, 0.09 to 0.15, and 0.02 to 0.06 for test-day one to test-day ten for milk, fat, and protein yields and SCS, respectively. Estimates of heritability were greater in lactations two and three. Estimates of heritability increased over the course of the lactation. Estimates of genetic and phenotypic correlations were smaller for test-days further apart.']	['<urn:uuid:31278128-80ea-4d16-9392-98887a46b10e>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	24	18	672
4	I'm doing research on historical hurricanes and their classifications. Could you explain what makes a Category 5 hurricane so destructive, and tell me about the most devastating one that hit the United States?	A Category 5 hurricane has winds exceeding 156 mph and is the most extreme category. In such storms, large trees, signs, residential and industrial buildings can be completely demolished. Some buildings are overturned or blown away, and glass is shattered over a wide area. Buildings less than 15 feet above sea level sustain major damage from battering waves and debris. Massive evacuations of residences 5-10 miles from shore are often required due to flood risks. Hurricane Andrew in 1992 was a Category 5 hurricane and was the most destructive U.S. hurricane of record. It hit Dade County, Florida with peak gusts of 164 mph (measured 130 feet above ground) and 177 mph measured at a private home. It caused $26.5 billion in damage in the United States, with most damage in Florida due to winds. While officially 23 deaths were recorded in the U.S., the death toll is controversial as many believe thousands in the Everglades were never accounted for.	"['Natural and Man-Made Disasters\nNatural and man-made disasters have claimed the lives of thousands over the past century. Listed below are a few of the more well-known catastrophic events.\nThe Great Hurricane of 1780\nOctober 10-16, 1780\nThe Great Hurricane of 1780 is considered the deadliest Atlantic tropical cyclone of all time. About 22,000 people died when the storm swept over Martinique, St. Eustatius and Barbados between October 10 and October 16. Thousands of deaths also occurred offshore.\nThe Perfect Storm\nIn October 1991, the atmosphere seemed to go crazy. Three separate weather elements (including Hurricane Grace) crashed together to form a storm of mammoth proportion--a blockbuster nor\'easter--off the New England coast. As Halloween neared, the storm played tricks that veteran meteorologists had never seen a typical nor\'easter perform, such as backing up into the Eastern Seaboard to unleash its titanic waves on bewildered beach towns. USATODAY.com meteorologists James West and Chris Vaccaro look day-by-day at a meteorological bomb that seemed so complete it was dubbed the ""perfect storm."".\n1825: October 7: Miramichi Fire, Maine and the Canadian province of New Brunswick: burned 3 million acres, killed 160 people and left 15,000 homeless.\n1849: May 17: The St. Louis Fire, St. Louis, Missouri: the first fire in US history in which a firefighter was killed in the line of duty. Captain Thomas B. Targee was killed while a fire break was being made. steamboat fire, the White Cloud; death toll, 3.\n1865: April 27: Worst Ship Disaster: Riverboat Sultana, Mississippi River: Sultana\'s boiler exploded, engulfing the ship in flames; transporting 2,300 war-weary Union troops home, only 600 survived. More people died in the Sultana disaster than on the Titanic or the Lusitania.\n1871: October 8: The Peshtigo Fire Wisconsin/Michigan: estimate at least 1,500 people dead - eight hundred died in Peshtigo, Wisconsin alone; the Peshtigo Fire killed more people than any fire to date, but was overshadowed at the time by the Great Chicago Fire, which began the same day. Estimiated 3.8 million acres burned.\n1871: October 8: The Great Chicago Fire, Chicago, Illinois: at least 300 people were dead, 100,000 people were homeless, and $200 million worth of property was destroyed.\n1888: March 11-14: Great White Hurricane, East Coast: The Blizzard of 1888 killed 400; accumulation of up to 5 feet of snow. Damage estimated at $20 million.\n1889: May 31st: The Johnstown Flood, Johnstown, Pensylvania: the result of several days of extremely heavy rainfall, exacerbated by the failure of the South Fork Dam; over 2,200 dead; over $17 million (USD) in damages.\n1894: September 1: The Minnesota Forest Fire, Hinckley, Minnesota: ""The Day the Air Caught Fire""; at least 418 killed; over 307,000 acres burned.\n1900: September 8: The 1900 Galveston Hurricane, Galveston, Texas: estimated winds of 135 miles per hour cat. 4 storm on the Saffir-Simpson Hurricane Scale.; extimated death toll between 6,000-12,000 Deaths; Deadliest hurricane to hit the United States.\n1903: December 30: Iroquois Theater Fire, Chicago Illinois: The Iroquois Theater was supposedly fireproof - 602 dead\n1903: June 14: Flash flood - Heppner, Oregon: The entire village of Heppner was swept awy by a sudden flash flood when Willow Creek overflowed into the town centre. 200-225 people died. No weather records were kept as the weather officer and his entire family were killed.\n1904: February 8: Baltimore Fire, Baltimore, Maryland: destroyed downtown Baltimore; fire burned over 30 hours, destroying 1,526 buildings spanning 70 city blocks.\n1904: June 15: General Slocum Paddleboat Fire, New York City, New York: 1,031 dead; a ""floating fire trap""\n1906: April 18: The Great 1906 San Francisco Earthquake and fire, San Francisco, California: registered 8.25 on the Richter scale; estimates range from 700 to 3,000 dead or missing, approximately 225,000 injuries and $400,000,000 in 1906 dollars.\n1907: December 6: Monongah Mining Disaster, Monongah, West Virginia: ""The worst mining disaster in American History"" death toll 362 men and boys.\n1910: March 1: Avalanche, Wellington, Washington: 2 trains snowbound in Stevens Pass in Cascade Range swept off tracks into canyon 150 ft below, killing 96.\n1910: September 10: The Big Burn of 1910, Wallace, Idaho: at least 7 people died along with 78 firefighters.\n1911: March 25: Triangle Shirtwaist Fire, New York City, New York: 146 dead.\n1913: December 1-5: Flood, Central to North Texas: 177 dead.\n1913:: Great Flood of 1913, Ohio River Basin: 467 persons drowned; 147 million dollars damage; "". . .second mostly deadly of record for the nation."" (David Ludlum)\n1915: May 7: Sinking of the Lusitania, New York City, New York: departed from New York May 1st for Liverpool, carrying 1959 passengers; torpedoed by German U-boat; 1,200 dead\n1915: July 24: Excursion Steamer Eastland, Chicago, Illinois: Eastland Disaster ""reputation for being top-heavy and had at several times in the past been reported as listing in an alarming way""; rolled over while still in port with 2,572 persons on board; 844 perished--making this Chicago\'s worst single disaster.\n1918: March - November: Spanish influenza , Nationwide: outbreak of Spanish influenza killed over 500,000 people in the worst single U.S. epidemic.\n1918: October 12: The Cloquet-Moose Lake Disaster, Cloquet Minnesota and 25 other communities: destroyed by forest fire, 559 die\n1918: July 9: Train collision, Nashville, Tennessee: 101 killed in a 2-train collision near Nashville\n1921: September 8-10 Flood, Central Texas: Thrall: 32"" in 12 hours (record); 215 deaths\n1925: March 18: Tri-State Tornado, Montana, Illinois, Indiana: The most violent single twister in U.S. history. It caused the deaths of 695 people and injured over 2,000. Property damage was estimated at $16.5 million.\n1927: April 1: Flood, Mississippi River: flooded over 18 million acres killing 313; 670,000 homeless; river levees broke at 47 spots; 750,000 homes underwater\n1928:: The Hurricane of 1928, Okeechobee, Florida: category 5; atmospheric pressure at landfall was measured at 929 mbar (hPa) and winds ""in excess"" of 150 mph; ""at least"" 2,500 deaths; the second-deadliest natural disaster in United States history behind the Galveston Hurricane of 1900 (as of 2004)\n1930\'s:: Many States ""The Dirty Thirties"": Longest drought of 20th century. Peak periods were 1930, 1934, 1936, 1939, and 1940. During 1934, dry regions stretched solidly from New York and Pennsylvania across the Great Plains to the Calififornia coast. A great ""dust bowl"" covered 50 million acres in the south-central plains during the winter of 1935-1936.\n1935: September 2: Labor Day Hurricane, Florida Keys: category 5 winds on the Saffir-Simpson Hurricane Scale; strongest hurricane to hit the United States coastline last century, wind gusts 150-200 mph; 400+ casualties\n1937: March 18: Texas School Explosion, New London, Texas: over 300 students and teachers died\n1938: May 16: Terminal Hotel Fire, Atlanta, Georgia: 35 dead\n1938: September 21: Long Island Express - The Great Hurricane of 1938, New England: category 5 storm on the Saffir-Simpson Scale with maximum sustained winds of 161 mph; claimed 600 lives\n1942: November 28: Cocoanut Grove Nightclub Fire, Boston, Massachusetts: 499 dead\n1944: July 17: Port Chicago Naval Magazine Explosion, San Francisco, California: 320 casualties\n1946: April 1 1946: Aleutian Tsunami, The Hawaiian Islands: generated by a magnitude 7.1 earthquake in the Aleutian Islands if Alaska; city of Hilo, Hawaii hardest hit with 96 killed; before dissipating, the tsunami it took the lives of more than 165 people and caused over $26 million (1946 dollars) in damage\n1947: April 16 & 17: Texas City Disaster, Texas City, Texas: ship explosion; started with the fire and detonation of approximately 17,000,000 pounds (7,700 tonnes) of ammonium nitrate on board the French-registered vessel SS Grandcamp; the SS Highflyer (or High Flyer), moored about 600 feet away from the Grandcamp and contained an additional 2,000,000 pounds (900 tonnes) of ammonium nitrate and 4,000,000 pounds (1,800 tonnes) of sulfur, exploded about 15 hours after the Grandchamp; considered the worst industrial accident in United States history; more than 516 lives lost\n1955: August 3-20: Atlantic hurricane of 1955, Northeastern United States: Hurricane Connie begins pounding U.S. for 11 days; Hurricane Diane, following Hurricane Connie floods Connecticut River killing 190 and doing $1.8 billion damage; Hurricane Diane kills 200; first billion $ damage storm (N.E. U.S.)\n1956: July 25: SS Andrea Doria collides with eastward-bound SS Stockholm, Off Nantucket, Massachusetts: collision caused 51 deaths; 2 rescuers were also killed - total death count 53; one of history\'s most famous maritime disasters\n1957: June 27: Hurricane Audrey, Texas and Louisiana: peak of 145 mph winds before making landfall near Sabine Pass, Texas on June 27 - Category 4 hurricane; 12-foot storm surge devastated Cameron, Louisiana, causing $150 million in damage; official death toll 390; unofficial, more than 500; earliest storm of any Atlantic hurricane season to reach Category 4 intensity in recorded history of the basin; Audrey was the strongest storm to form prior to August, and held this record for nearly fifty years before Hurricane Dennis broke it in 2005 (which was itself broken only nine days later by Hurricane Emily). Emily remains the strongest storm ever to form in June.\n1958: December 1: Our Lady of Angels School Fire, Chicago, Illinois: fire in a Catholic elementary school; 92 children and three nuns parished; To Sleep with the Angels : A Story of a Fire\n1960: December 19: USS Constellation, New York Naval Shipyard, Brooklyn, New York: a Kitty Hawk-class supercarrier, was the third ship of the United States Navy to be named in honor of the ""new constellation of stars"" on the flag of the United States; nicknamed \'America\'s Flagship\'; decommissioned on August 7, 2003, after 41 years, nine months and 11 days of naval service; fire swept through Constellation while she was under construction at a Brooklyn Navy Yard pier, injuring 150, killing 50, and doing $75 million worth of damage\n1963: April 10: Atomic-powered submarine Thresher sank, North Atlantic: 129 dead\n1964: March 27: The Great Alaskan Earthquake and Tsunami, Alaska: 90% of the deaths in Alaska during the 1964 earthquake and subsequent tsunamis were due to the tsunamis; largest earthquake in North America and the second largest ever recorded (largest occurred in Chile in 1960); 9 deaths attributed to earthquake; 8.4 - 8.6 on the Richter Scale; 106 deaths due to tsunamis; 115 total deaths in Alaska; other resulting deaths: Newport, Oregon - 4; Crescent City, California - 11; Kalmath River, California - 1\n1967: December 15: The Point Pleasant/Silver Bridge Disaster Silver Bridge spanned over the Ohio River connecting Point Pleasant, West Virgnia and Kanauga, Ohio: suspension bridge constructed in 1928; collapsed claiming 46 lives and injuring 9\n1972: June 9: Burst dam, flood, Rapid City, South Dakota: 238 dead\n1972: February 26: Coal Refuse Dam Failure, Man, West Virginia: dam gave way killing 125 people, injuring 1,000 and leaving 4,000 homeless in Buffalo Creek in Logan County, West Virginia\n1972: July 19: TWA Flight 327 112 Dead, Sioux Gateway Airport, Iowa: interestingly, all information about this incident have been wiped from historical records\n1977: May 28: Beverly Hills Supper Club Fire, Southgate, Kentucky: hotel fire; claimed 165 lives\n1977: July 19: Burst dam, flood, Johnstown, Pennsylvania: excessive rain in a short period of time caused the dam to burst; the flash flood that ensued killed 77 people and caused $325 million in damage\n1978: April 27: Willow Island Cooling Tower Collapse, Willow Island, West Virginia: power plant cooling tower under construction; scaffolding collapsed killing 51\n1979: May 25: American Airlines Flight 191, O\'Hare Airport, Chicago, Illinois: crashed, killing all 271 on board and two on the ground; Flight 191 was the deadliest plane disaster on U.S. soil until surpassed by the crashes of American Airlines Flight 11 and United Airlines Flight 175 in the September 11, 2001 attacks.\n1980: November 21: MGM Grand Hotel/Casino Fire, Las Vegas, Nevada: 85 deaths\n1981: July 17: Hyatt Regency walkway collapse, Kansas City, Missouri: two skywalks filled with people at the Hyatt Regency Hotel in Kansas City, Missouri collapse into a crowded atrium lobby killing 114\n1985: August 2: Delta Air Lines L1011-1, Dallas-Fort Worth, Texas: crashed shortly before landing after encountering a wind shear from a passing thunderstorm; eight of the 11 crew members and 128 of the 152 passengers were killed; one person in a passing car was also killed\n1986: August 31: Aeromexico DC-9, Dead, Cerritos, California: collided with a single engine Piper Archer which had made an unauthorized penetration of controlled airspace; all 6 crew members and 58 passengers were killed; the three occupants of the Piper and 18 people on the ground were also killed; total 85 dead\n1987: August 16: Northwest MD82 Crash, Detroit, Michigan: crew neglected to properly set flaps for takeoff; aircraft stalled soon after take-off, crashing onto a highway; all six crew and 148 of 149 passengers were killed; two people on the ground were also killed\n1987: December 7: Pacific Southwest Airlines BAe146-200, near San Luis Obispo, California: recently fired USAir employee used his now invalidated credentials to board the aircraft with a pistol and apparently killed his former manager and both pilots (USAir had recently purchased PSA); all five crew members and 37 passengers killed\n1989: March 24: Oilspill, Prince William Sound, Alaska: Tanker Exxon Valdez hit an undersea reef and spilled 10 million-plus gallons of oil into the water, causing the worst oil spill in U.S. history.\n1989: October 17: Earthquake (San Andréa\'s fault), San Francisco, California: magnitude 7.1 earthquake; worst earthquake since 1906; 63 deaths, 3,757 injuries, $ 5,900,000,000 damages - most costly natural disaster in the United States at that time\n1989: September 10: Hurricane Hugo, Hit Charleston, South Carolina: Category 5 hurricane that struck Puerto Rico, St. Croix, South Carolina and North Carolina, killing at least 70 people; caused billions of US dollars in damages (mostly in South Carolina), and is still one of the costliest hurricanes in history (surpassed by Hurricane Andrew)\n1990: January 25: Avianca Boeing 707, Flight 52, Cove Neck, New York: aircraft crashed while in a holding pattern awaiting landing at New York\'s Kennedy Airport; bad weather a factor; 73 of 158 killed\n1992: August 24: Hurricane Andrew, Dade County, Florida: most destructive United States hurricane of record; peak gust of 164 mph--measured 130 feet above the ground--while a 177 mph gust was measured at a private home; 23 deaths in the United States and three more in the Bahamas; $26.5 billion in damage in the United States, of which $1 billion occurred in Louisiana and the rest in south Florida; majority of damage in Florida was due to the winds (the death toll is controversial as many believe there were thousands in the Everglades never accounted for)\n1994: September 8: USAir Flight 427, near Pittsburgh, Pennsylvania: aircraft lost control at about 6,000 feet (1830 meters) during approach; all five crew members and 127 passengers were killed\n1994: January 17: Northridge Earthquake, Los Angeles, California: 6.7 magnitude earthquake; 57 dead, 1500 serious injuries\n1995: April 19: Oklahoma City Bombing, Oklahoma City, Oklahoma: terrorist attack on the Alfred P. Murrah Federal Building, a U.S. government office complex in downtown Oklahoma City, Oklahoma, was destroyed, killing 168 people; largest domestic terrorist attack in the history of the United States and was the largest act of terrorism within U.S. borders until September 11, 2001\n1996: May 11: ValuJet Airlines DC9-32, near Miami, Florida: fire in the cargo caused this plane to crash into the Florida Everglades about 15 miles from the airport; 105 passengers and five crew members were killed\n1996: July 17: TWA Flight 800, Long Island, New York: catastrophic in flight breakup shortly after departure; all 18 crew and 212 passengers perished\n1998: September 2: MD 11 Swiss-Air Flight 111, crashes near Nova Scotia, Canada: aircraft crashed at night in the Atlantic Ocean close to shore about 50 miles southwest of Halifax, Nova Scotia; all 15 crew members and 214 passengers were killed\n1999: October 31: EgyptAir 767-300ER Flight 990, Atlantic Ocean near Nantucket Island, Massachusetts: aircraft crashed into the ocean about 60 miles south of Nantucket Island; NTSB determined that the aircraft departed from controlled flight and crashed into the Atlantic Ocean as a result of flight control inputs by the first officer; all 14 crew members and 203 passengers were killed\n1999: August 18-25: Hurricane Bret, Kenedy County, Texas: winds 125 mph, pressure 944, category 4\n2000: January 31: Alaska Airlines MD83 Flight 261, Off Point Mugu, California: 83 passengers and five crew members were killed\n2001: September 11: Alaska Airlines MD83 Flight 261, New York City, New York; Arlington, Virginia; and Shanksville, Pennsylvania: Hijackers crashed 2 commercial jets into twin towers of World Trade Center; 2 more hijacked jets were crashed into the Pentagon and a field in rural Pennsylvania. Total dead numbered 2,992, including the 19 hijackers. Islamic al-Qaeda terrorist group blamed.\n|Copyright Information: Public Domain.|', ""Hurricane researchers have developed their own lingo for understanding these monster storms. The following information describes different types of storms, how they affect homes and communities, and how they are named.\nThe four basic categories of tropical weather events are tropical disturbance, tropical depression, tropical storm and hurricane.\nA tropical disturbance is an organized tropical weather system that maintains its identity for more than 24 hours.\nTropical depressions have maximum sustained wind speeds of 38 mph or less -- stronger tropical storms have maximum sustained wind speeds from 39 to 73 mph.\nWhen wind speeds reach 74 mph or more, the storm is upgraded to a hurricane.\nHurricanes are further categorized on a scale of increasing intensity known as the Saffir-Simpson Hurricane Scale, which ranks hurricane strength in Categories 1 to 5.\nA Category 1 hurricane (winds from 74 to 95 mph) will mainly cause damage to trees and unanchored structures like mobile homes. Low-lying coastal roads can become flooded and some boats may be torn from moorings.\nIn a Category 2 hurricane (winds from 96 to 110 mph), some trees will be blown down. Mobile homes, roofs, piers and signs may sustain considerable damage, but no major damage to buildings will be experienced. Marinas and coastal roads will be flooded.\nHurricane Ivan, which struck coastal Alabama in 2004, was a Category 3 storm (winds from 111 to 130 mph). In these hurricanes, mobile homes and buildings near the coast can be destroyed by winds or battering waves. Serious flooding can block roads up to eight miles inland or more. Evacuation may be required near shorelines.\nIn a Category 4 hurricane, like August's Hurricane Katrina, wind speeds range from 131 to 155 mph. Large trees are blown down, beaches suffer major erosion, and roofs, windows and doors are often blown off structures. Escape routes can be cut by floodwaters three to five hours before the hurricane center arrives. Massive evacuations of residences within two miles of shore may be required.\nThe most extreme category of hurricane is Category 5, where winds exceed 156 mph. Hurricane Andrew, which destroyed large swaths of Miami and south Florida in 1992, was a Category 5 hurricane.\nIn a Category 5 storm, large trees, signs, residential and industrial buildings can be completely demolished. Some buildings are overturned or blown away. Glass is shattered in buildings over a wide area. The lower floors of buildings less than 15 feet above sea level sustain major damage from battering waves and debris. Because escape routes can be cut off by floodwater, massive evacuations of residences five to 10 miles from shore are often required.\nThe wind speeds noted here are for winds measured or estimated as the top speed sustained for one minute. Peak gusts, however, can be 10 percent to 25 percent stronger.\nHurricanes are given names to help speed communication among forecasters and the general public.\nAn Australian forecaster is credited with providing the first-known hurricane names -- he named storms after unpopular political figures.\nIn World War II, hurricanes were named after military meteorologists' girlfriends or wives. In 1979, men's names were included.\nStorms from the Northwest Pacific basin have a different naming convention. The majority are names of flowers, animals, birds, trees or foods, and names are not allotted in alphabetical order, but are arranged by contributing nation.\nSource: National Oceanographic and Atmospheric Adminstration, National Hurricane Center.""]"	['<urn:uuid:5251be61-6d40-4580-bef6-762b5cd1539c>', '<urn:uuid:c60bad70-5655-49c3-8597-cf2ed29f988a>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:10:11.825225	33	160	3342
5	Do historic and contemporary Buddhist teachers share teaching methods?	Historical Buddhist teachers, like the Buddha in the fifth century BCE, taught by examining one's mind and making teachings accessible through everyday examples. Similarly, contemporary Buddhist teachers maintain this approach, explaining profound concepts in simple terms and using familiar language, while also incorporating modern scientific examples to help today's audiences understand Buddhist doctrines.	"['FOUR ORDINARY FOUNDATIONS OF BUDDHIST PRACTICE by Thrangu Rinpoche\nDuring the fifth century before our era, a tremendous explosion of philosophical thought appeared on the earth. During this time, the Buddha began delivering a remarkable set of teachings. He taught that instead of relying on a god or on materialistic pursuits, one can attain true, permanent happiness by simply examining one\'s own mind.\nFOUR FOUNDATIONS OF BUDDHIST PRACTICE AND A PRACTICAL GUIDE TO MEDITATION by Thrangu Rinpoche\n""The Four Foundations of Buddhist Practice have been crucial to my personal understanding of the Buddhist teachings. Nowhere have I found them presented with more clarity and wisdom than in this book by our beloved teacher Thrangu Rinpoche. I know that all who read this book will benefit profoundly.""--Pema Chodron\nA GUIDE TO THE WORDS OF MY PERFECT TEACHER by Khenpo Ngawang Pelzang, translated under Dipamkara, with The Padmakara Translation Group\nThis guide provide readers with essential background information for studying and practicing with Patrul Rinpoche\'s Words of My Perfect Teacher - the text that has more for more than a century served as the reliable sourcebook to the spiritual practices common to all the amjor schools of Tibetan Buddhism. by offering chapter-by-chapter commentary on this renowned text, Khenpo Pelzang provides a fresh perspective on the role of the teacher, the stages of the path, the view of the Three Jewels, Madhyamika, the basis of transcendent wisdom, and more.\nPRELIMINARY PRACTICES OF TIBETAN BUDDHISM by Geshe Rabten\nThe ordinary and extraordinary practices for purification and generation of merit are profound teachings. Simple to understand, they are difficult and demanding to put into practice. Geshe Rabten illumines these practices with clear understanding.\nA verbally transmitted teaching elucidating the ""ordinary"" as well as the ""extraordinary"" practices which are prerequisites for anyone embarking on higher meditations.\nTHIS PRECIOUS LIFE: Tibetan Buddhist Teachings on the Path to Enlightenment by Khandro Rinpoche\nThe book includes contemplative exercises that encourage us to appreciate the tremendous potential of the human body and mind. They focus on how we can learn to see this life as a gift and how, by achieving peace in our own lives, we can bring a seed of happiness to other people.\nWORDS OF MY PERFECT TEACHER by Patrul Rinpoche, forewords by H.H. the Dalai Lama and Dilgo Khyentse Rinpoche, translated by The Padmakara Translation Group\nFor more than a century, The Words of My Perfect Teacher has served as a guide to the spiritual practices common to all four major schools of Tibetan Buddhism. It is the classic commentary on the preliminary practices of the Longchen Nyingthig cycle of teachings--the great spiritual treasure of the Nyingmapa school, the oldest of the Tibetan Buddhist traditions. The author, Patrul Rinpoche, makes his subject matter accessible through a wealth of stories, quotations, and references to everyday life. His sense of poetry and irony, and his warm, colloquial style infuse of the text with the atmosphere and vitality of an oral teaching.\nThis second, revised edition is the result of a detailed and painstaking comparison of the original Tibetan text with the English translation by the Padmakara Translation group.', 'Venerable Khenpo Tsultrim Lodrö Rinpoche, a native of Draggo (Ch: Luhuo) County in Sichuan Province, is Vice Principal of Sethar Larung Five Sciences Buddhist Institute (Larung Gar), also a renowned contemporary Nyingma teacher of Tibetan Buddhism and a strong advocate of Tibetan culture.\nAt the age of twenty-two (1984), he received ordination at Larung Gar, becoming a disciple of the great contemporary spiritual master, H.H. Khenchen Jigme Phuntsok. Over more than two decades, he applied himself diligently to the studies of both the Buddhist sutric and tantric scriptures and assimilated all contents without difficulty. After years of effort and consecutive levels of strict examinations, he was personally awarded the Khenpo degree by H.H. Khenchen Jigme Phuntsok. The results of his practice also were verified face-to-face three times by his root teacher. From 1991 to 2013, Khenrinpoche held the appointed position of Dean of Education at Larung Gar, responsible for the training of a large cohort of outstanding monastic students capable of carrying on the lineage and spreading the Buddhadharma.\nIn addition, Khenrinpoche has long dedicated efforts to promote the ideas of non-killing, life release of living beings, vegetarianism, environmental protection, and the importance of education. His charitable endeavors include the establishment of libraries, nursing homes, and schools in the Tibetan regions. With a view to protecting and advancing the Tibetan spoken and written language, in 2005 Khenrinpoche invited language experts and scholars from Tibetan regions across five provinces to come together in compiling Chinese-Tibetan-English Dictionary of New Daily Vocabulary, while he himself assumed the role of the project’s chief editor. The team has so far successfully published Chinese-Tibetan-English Dictionary of New Daily Vocabulary, Chinese-Tibetan-English Visual Dictionary of New Daily Vocabulary, and the Dictionary of Traditional Tibetan Words with Illustrations.\nIn recent years, Khenrinpoche has been invited to give lectures in Hong Kong, Taiwan, Japan, Singapore, Malaysia, Indonesia, Canada, the USA, the UK, Australia and New Zealand, etc. His presentation is crisp and well organized, preferring to explain the profound in simple terms. Khenrinpoche is also good at referring to scientific examples and elucidating abstruse Buddhist doctrines using language that people today are familiar with, which is greatly appreciated by Buddhist followers and intellectuals alike.\nMoreover, Khenrinpoche was invited as well for talks and scholarly discussions at prestigious academic institutions such as Harvard, Oxford, Stanford, UC Berkeley, Columbia, George Washington University, the University of Virginia, Toronto, Auckland, Sydney, and Melbourne, as well as at companies like Google, together with experts and scholars in the fields of science, philosophy, and psychology, discussing the mystery of life and the mind based on scientific and Buddhist principles.\nDespite being an influential Buddhist master, Khenrinpoche is very modest and keeps a low profile, rarely draws attention to his own merit and spiritual attainment, instead focusing solely on the propagation of Dharma and ways to benefit sentient beings. His words and actions exemplify the ideal of a Mahayana practitioner, a real admirable teacher who braves all difficulties to uphold the beacon of true Dharma in this degenerate age. Khenrinpoche once wrote in his blog on Weibo, “My lama once told me that the only purpose and the value of our lives is to cultivate and to give love. I will not forget this in my whole life, hopefully nor in all future lives.”\nKhenrinpoche diligently applies himself to writing when any spare moment presents itself on his continuous journey to spread the Dharma. Among all of his publications, the Luminous Wisdom series is acclaimed as a masterpiece of Buddhism which can stand the test of time. Its contents encompass both theory and practice, with rigorous sequencing, and seamlessly integrate both sutra and tantra. The series is practical and appropriate, and the style of the language is concise, powerful, incisive, and thorough, meeting the psychological needs of a contemporary readership. As a result, it has become widely accepted and immensely popular. For the broad population of Buddhist students and practitioners, it provides a clear beacon to illuminate the way to liberation.\nPublications to date:\nKhenpo Tsultrim Lodrö’s Complete Collection - Four Volumes; Rain in Time; Contemporary Beats of the Dharma Drum; Conversations Between Eastern and Western Cultures\nLuminous Wisdom Series 1-10; Exhortations; Illuminating Insights (in Chinese/English); The Truth of Life; Buddhism—Superstition or Wisdom? Decipher the Mysterious Codes of Tibetan Buddhism; Unveil the Mysteries of Tibetan Buddhism; When Heart Sutra Meets Quantum Physics; Comprehending the Book Called Life; Deconstruct Vajrayana Buddhism; Collected Notes 2012-2018 (in Chinese/English)\nThe Right View; Are You Ready for Happiness? —Don’t Let the Paper Tiger Scare You Off; The Handbook for Life’s Journey; Daily Inspiration from Khenpo Tsultrim Lodrö; The Four Seals of Dharma; Collected Notes, 2012-2018; Gateway to the Vajrayana Path']"	['<urn:uuid:98c7504c-4aed-4d81-9501-9b02177330eb>', '<urn:uuid:29e6ef4c-459e-4aed-9910-87ba24e544bf>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T19:10:11.825225	9	53	1298
6	need help comparing canon cameras image quality want to know iso range and processor features mark iii vs powershot g7	The Canon EOS-1D X Mark III features an ISO range of 100-102,400 (expandable to 819,200) and uses the DIGIC X Image Processor, which delivers outstanding image quality with low noise at high ISO settings. It also enables 20fps continuous capture and 4K/60p oversampled video. The PowerShot G7 X Mark III uses a DIGIC 8 processor and has a maximum ISO sensitivity of 12800 (expandable to 25600). Both cameras feature 20.1-megapixel sensors, though the EOS-1D X Mark III uses a Full-Frame sensor while the PowerShot G7 X Mark III uses a 1.0 type stacked CMOS sensor.	"[""The Canon EOS-1D X Mark III offers huge upgrades to the MKII including 5.5K RAW video recording, Deep-Learning AF, and an ISO range of 100-102,400. Whatever the situation, whatever the lighting, EOS-1D X Mark III will deliver exceptional images - beating the competition to the punch with its exceptional speed. The camera body is tough, reliable and intuitive. Ideal for sports, wildlife and documentary shoots, this high-performance camera forms the creative toolkit you need to capture the unrepeatable moments with.\nCanon EOS-1D X Mark III Key Features:\n- Fantastic for photography experts and creative storytellers wanting to share their observations of our ever-changing world\n- Users benefit from 20.1-megapixel Full-Frame Imaging Sensor\n- Outstanding image quality delivered by the DIGIC X Image Processor\n- Equipped with Deep-Learning Autofocus\n- Take fast and intuitive control over the AF point selection with the Smart Controller\n- Generous dynamic range and high ISO sensitivity of 100-102,400\n- Dual Pixel CMOS AF facilitates expert focusing with high speeds for photo and offers smoothness for video\n- Supports real-time and lag-free up to 20fps continuous shooting in Live View mode\n- 5.5K RAW Video recording capability producing excellent video quality\n- Enjoy filmmaking at 4K/60p\n- Comes with Gigabit Ethernet and built-in Wi-Fi and Bluetooth for fast connectivity wherever you are\n- Works well with all EF lenses except for EF-S lenses\n- Uses Rechargeable Li-Ion Battery LP-E19 (supplied) or LP-E4N\n- Dual CFexpress fast-speed card slots that can be individually assigned to photos or videos\n- Compatible with WFT-E9 to improve wireless connectivity with 5GHz 802.1ac Wi-Fi\nExpanded Canon EOS-1D X Mark III Key Features:\nCanon EOS-1D X Mark III offers some fantastic upgrades to the EOS-1D X MKII. For starters, MKIII has an ISO range of 100-102,400 that is expandable to 819,200 in comparison to MKII's ISO sensitivity range of 100-51,200. This allows you to capture exceptionally high-quality images in the poorest of lightings. Another noteworthy upgrade is the movie RAW recording capability not previously found in the MK II. Now you have the ability to shoot in 5.5K RAW (60p), 4K DCI (60p), 4K UHD (60p), and Full-HD (120p) - as opposed to 4K video (60p) and Full HD (120p). Enjoy continuous shooting at 16fps in OVF and up to 20fps in Live View; whereas, before you could shoot at 14fps/16fps respectively. The number of AF points has also increased hugely - from 61/0 to a massive 191/3869 (OVF/LV). Shoot uninterrupted with an upgraded battery life of 2850 (compared to 1210) when shooting with a View Finder and over 2 hours of video recording. Mark III sees the introduction of built-in Bluetooth connectivity and an increased shutter life by 100,000 (400,000 to 500,000). The Gigabyte Ethernet and USB 3 wired connectivity have been upgraded to High-speed Gigabyte Ethernet and USB 3.1 Gen 2. To top things off, the camera's Magnesium Alloy body has become lighter. Other new technologies adopted by the EOS-1D X Mark III include HEIF image format, Deep-Learning AF, CFexpress, Smart Controllers, and a Gaussian low pass filter.\nThe newly-introduced Deep-Learning Autofocus Algorithm is capable of the recognition of athletes' faces - even when their faces are obscured by helmets or goggles. The Deep-Learning AF knows when subjects are upside down which comes in particularly useful when photographing driving competitions and other sports such as surfing or gymnastics. To achieve this, the aforementioned algorithm has been programmed using real-life examples of sports photography. A large sample of diverse sports has been used to provide the most accurate and abundant information to the camera. These sports include anything from football to athletics.\nBuilt into the camera's AF-ON buttons are a pair of Smart Controllers; one for horizontal recording and one for vertical. These controllers have been customised for both speed and sensitivity, and are capable of reacting intuitively even when the camera operator is wearing gloves. For familiar operation, the controllers are shaped like traditional joysticks. The difference is that they behave more like a miniature touchpad that can be used to scroll AF points around the screen. Additionally, the camera can automatically disable the vertical controller when shooting horizontally, thanks to which accidental movements are avoided.\nFurther Improvements to Image Quality\nThe ever-improving high image quality in the EOS-1D X range is owed to the single DIGIC X Image Processor, Gaussian Low Pass Filter, and the HEIF Image Format along with other features. The extremely fast and efficient DIGIC X Image Processor is in charge of the camera's outstanding image quality and responsive performance. The latter is characterised by 20fps continuous capture 4K/60p oversampled video. The processor ensures incredibly low levels of noise when working at a high ISO, in addition to enabling greater DLO, ALO, and Highlight Tone Priority. The Gaussian Low Pass Filter works by sampling light over a larger number of pixels when compared to a conventional low pass filter. This means that fine sharpness and clarity of the image are preserved while optical issues such as moire are minimised. Finally, the HEIF Image Format; also known as the High Efficiency Image File; is the next-gen of file formats - great news for digital photographers. It allows for a higher image quality without the increase in file size, as opposed to JPEGs. HEIF files offer a smaller amount of artefacts and an amplified dynamic range featuring 10-bit colour depth.\nBack to top"", 'Canon PowerShot G7 X Mark III Digital Camera\nThe Canon PowerShot G7 X Mark III is the first-class upgrade from the Mark II featuring 20.1 megapixel 1.0 type stacked CMOS Image Sensor and DIGIC 8 processor. With the ISO sensitivity, up to 12800 (Expanded: 25600), and built-in EVF (0.39 type) at 2,360,000 dots and OLED type, creativity is up for a great deal of freedom with both shooting and recording now also in low light conditions.\nBoost the quality of your work with the 8.8 – 36.8 mm (35 mm equivalent: 24 – 120 mm), 4.2x Optical Zoom, ZoomPlus 10x and with Digital Tele-Converter (1.6x or 2.0x) which combined reaches approximately 17x.\nFocus on vlogging\nStand out from the smartphone crowd with the exciting movies modes: 4K (3840 x 2160, 29,97 / 25 fps) up to 9m 59s. Full HD (1920 x 1080, 119.9 / 100 / 59.94 / 50 / 29.97 / 25 fps), HD (1280 x 720, 50 fps – HD) up to 4 GB or 29m 59s and enjoy the vertical movie aspect feature.\nVersatile sizes and formats selection\nBe ready to choose between the best options for image sizes: 3:2 – (RAW, L) 5472 x 3648, (M) 3648 x 2432, (S1) 2736 x 1824, (S2) 2400 x 1600 4:3 – (RAW, L) 4864 x 3648, (M) 3248 x 2432, (S1) 2432 x 1824, (S2) 2112 x 1600 16:9 – (RAW, L) 5472 x 3072, (M) 3648 x 2048, (S1) 2736 x 1536, (S2) 2400 x 1344 1:1 – (RAW, L) 3648 x 3648, (M) 2432 x 2432, (S1) 1824 x 1824, (S2) 1600 x 1600\nCreate content your own way\nExperiment new technics with Single, Continuous, Servo AF/AE, Touch (that can be turned on and off) and Manual Focus modes, the AiAF – 31 points system and the RAW shooting option. Discover the Program AE, Shutter priority AE, Aperture priority AE, Manual, Custom, Hybrid Auto shooting modes among many more.\nCapture your point of view\nOwn the timing of the recording motion 1/8 – 1/2000 s (Movie Mode) 1 – 1/2000 s (Factory default) 30 – 1/25600 s (Electronic shutter) BULB, 30 – 1/2000 s (Total range – varies by shooting mode)\nRely on the Intelligent Image Stabilisation with 5-axis Advanced Dynamic IS & Auto Level to sensibly reduce vibrations even under low light capability on both indoor and outdoor shooting.\nSimple is more\nDiscover over 30 shooting modes and 15 photo effects perfected to enhance your originality and unique inspiration.\nEmbrace the fantastic built-in Bluetooth® integration along with WI-FI system and share your content with your audience wherever you are.\nYou are in control\nFeel free to work comfortably with the tilt 7.5 cm (3.0”) Touchscreen LCD (TFT). 3:2 aspect ratio and 1,040,000 dots that allows you to navigate clearly into the settings.\n- 4К mоvіеѕ & 3.5mm mіс іnрut – Теll уоur ѕtоrу wіth vіdео\n- 120fрѕ Full НD mоvіеѕ – Ѕmооthеr, mоrе dеtаіlеd ѕlоw-mоtіоn\n- 24mm 4.2х zооm – f/1.8-2.8 wіdе-аnglе zооm іn а соmрасt bоdу\n- 20.1 mеgаріхеl 1.0-tуре ѕtасkеd СМОЅ ѕеnѕоr – Fоr ѕuреrb іmаgе quаlіtу\n- Тіlt-uр ѕсrееn – Fоr ѕеlfіеѕ аnd lоw-аnglе ѕhооtіng\n- Вluеtооth аnd Wі-Fі – Аlwауѕ-оn lіnk wіth уоur ѕmаrt dеvісе\n- UЅВ Сhаrgіng – Соnvеnіеnt сhаrgіng whеrеvеr уоu аrе']"	['<urn:uuid:85bc9f11-e50d-4863-a68d-35c157db4ac4>', '<urn:uuid:0b75ccea-558e-4253-808f-9dc0e8a6ffa5>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-12T19:10:11.825225	20	95	1439
7	impact berlin wall collapse on soviet union	The collapse of the Berlin Wall heralded the momentous breakup of the Soviet Union and its 'evil empire'. In the months following the Wall's destruction, the Soviet edifice was swept away, leading to mostly non-violent revolutions and democracy movements that transformed former Soviet territories like Poland, Hungary, Romania, and the Baltic States from Moscow's control into independent countries.	['The Berlin Wall collapsed amid a failed faith in communism and exalted hopes for a world free of rivalries and conflicts. Berliners on both sides of the barrier stripped away the concrete slabs that separated their city, lives and two worlds–one free and the other totalitarian. The elation was near universal on seeing the despised Wall tumble–except in the Kremlin and its counterparts within the Soviet Union’s East European satellites or its republics of Central Asia. The Wall’s breaching, in fact, heralded the momentous breakup of the Soviet Union and its “evil empire.” It also portended an era of global tranquility without the Cold War divisions.\nLike most major anniversaries, this one–the 20th–bequeaths a moment for reflection and stock-taking on the status of worldwide harmony and America’s international role. Mostly non-violent revolutions and democracy movements did transform Poland, Hungary, Romania, the Baltic States and others from Moscow’s suzerainties to independent countries. East and West Germany reunified and entered as one into the North Atlantic Treaty Organization. NATO enlargement eastward also anchored most former satellites within democratic Western Europe, while it offered security against an ever more resurgent Russia bent on neo-czarist imperialism.\nThe sweeping away of the Soviet edifice in the months that followed the Wall’s destruction brought seismic changes for the United States and the world as a whole, which witnessed expanded trade and increased prosperity. The Wall’s crumbling, however, created a new international landscape of looming threats and widespread uncertainty, all very different from the somewhat predictable Cold War.\nFor nearly a half-century, the United States stood as a rampart against the Soviet Union’s subversion and expansionism as well as a beacon of hope to its subjects. In the post-Soviet epoch, it became the sole superpower and a bulwark of a different sort. Unlike past ascendant powers, the United States carved out no colonies, nor even spheres of influence in the aftermath of its nemesis’ collapse, despite all the silly talk of an American empire.\nIndeed, American taxpayers looked inward, demanded a “peace dividend” from decades of high defense spending, and rediscovered a host of internal ills from poor education in many of the nation’s schools to pervasive drug abuse demanding attention from a Washington seemingly no longer distracted by the Red Army. The impulse for non-involvement beyond our shores runs deep in our history. America’s respite from international problems was brief, however.\nInstead of a diminished U.S. role, the post-Wall stretch has witnessed the expanded indispensability of American power and diplomacy. Without the prodigious U.S. economic capacity and military might, regional troublemakers and local conflicts would have gotten out of hand. An American-led coalition turned back Iraq’s conquest of Kuwait. Washington’s intervention stopped the turmoil in Haiti and the horrific atrocities in Bosnia and Kosovo during the 1990s, while Western Europe dithered. When Bill Clinton failed to lift a finger to staunch Rwanda’s genocide, hundreds of thousands died in the Central African country, testifying to the need for U.S. engagement.\nDesperate regimes no longer subject to the even loose leash of Moscow soon endangered regional peace. North Korea, Iraq, Iran, and Libya spread terrorism, embarked on nuclear arms and built longer-range missiles. They fiercely defied the much-ballyhooed global “flatness” of trade, information, and people flows alone to bring reconciliation among warring states, within ethnically split nations, and from extremist Islamic movements.\nFor all its travails, the Iraq War ousted Saddam Hussein, whose invasions and terrorist promotion kept the Middle East in a state of high insecurity. The U.S.-led invasion of the Persian Gulf nation also convinced Libya to come clean on its manufacturing of weapons of mass destruction. Two rogue adversaries remain for the Barack Obama administration to deal with. It is a sure bet that if Washington fails to halt the nuclear-arming of Iran and North Korea, neither the United Nations nor any non-regional power will.\nThe two-decade commemoration of the Berlin Wall’s fall also marks another dramatic but less exhilarating world event of the same year. In June 1989, China’s Communist Party crushed the student-orchestrated pro-democracy demonstrations in Tiananmen Square. Beijing’s suppression of peaceful dissent ensured the party’s political dominance, its formula for a state-controlled economy, and China’s rise to global power. China’s astounding export engine, thinly veiled military buildup, and aggressive pursuit of its calculated interests cannot but cause unease as America journeys on a more turbulent trajectory. Thus, 20 years ago, we witnessed the eclipse of one global rival and the advent of another possible competitor.\nThe years since the 9/11 terrorist attacks have been unkind to the United States, which stood at the pinnacle of its economic and military power when the Berlin Wall fell. Toppling the Taliban regime that hosted the terrorist-mastermind Osama bin Laden in Afghanistan and subduing a belligerent Iraq proved costly in blood and treasure. U.S. military power is still matchless in spite of the media’s defeatism about the U.S. losing in the “graveyard of empires.” At present, America’s economic health is under siege.\nThe Obama administration’s massive deficit spending poses severe risks to American power, which has acted to stabilize global affairs. Our surging government expenditures are propelling federal deficits to almost 98% of the nation’s entire gross domestic product, imperiling productivity and burdening the budget for defense along with non-military expenditures with massive interest payments for our debts.\nUnless our government abandons its profligate spending, future Berlin Wall anniversaries will mark a far different American standing than the current one. Arnold Toynbee, the renowned British historian, warned us when he noted that more civilizations perish from suicide than murder.\nThomas H. Henriksen is a senior fellow at Stanford’s Hoover Institution and the U.S. Joint Special Operations University. His book American Power After the Berlin Wall appeared in paperback this past August.']	['<urn:uuid:64e58925-f5b8-4ad8-b2a9-f123955ee8ec>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	7	58	955
8	I'm learning about translation experts - would you compare Günther Orth's and Sawad Hussain's specialties when it comes to translating Arabic literature?	Both translate Arabic literature but focus on different aspects. Günther Orth specializes in translating novels from Arabic to German, dealing particularly with cultural and religious references that need elaboration for German readers. Sawad Hussain, on the other hand, focuses specifically on translating humor in Arabic literature, researching how to identify what makes texts funny and how to transport this into another language.	"[""Interview with Günther Orth “Arabic Metaphors don't work in German”\nMediator between worlds: Günther Orth translates Arabic literature into German. An interview about the challenges of language and the subtleties of the phrase “I want”.\nMr Orth, you interpret for politicians, scientists and artists from the Arab world. The German version of Fawwaz Haddad’s novel “Gottes blutiger Himmel” (Soldiers of God) is also by you. Currently you’re working on the translation of “Der Messias von Darfur” (The Messiah of Dafur) by Abdelaziz Baraka Sakin. How does a boy from Franconia come to the Arab world?\nI have no Arab ancestors and no orientalists in the family. The first thing I ever heard about the Middle East was the Arab oil embargo against the West in 1974. I was then a small boy; what I picked up was that the Arabs want to make life difficult for us. Later two things worked together. I hadn’t seen much of the world; as a large family from Franconia our holiday radius was limited. Maybe that’s why I was attracted to the world with all its exotic cultures. Then I found learning languages easy and knew a student in Erlangen who travelled regularly to China on scholarships. I found that inspiring. So after taking my school-leaving certificate I looked into studying Sinology. On the same floor of the university was the Oriental and Islamic Studies Department. Since Sinology seemed to be full up, I decided to try Islamic Studies. The pace was very brisk, and after a week there were only 20 students; at the end of the semester only five.\nEven many Arabs find it difficult to learn Modern Standard Arabic, particularly the grammar. How was the beginning for you?\nDifficult. Everything was unfamiliar and hard to compare to European languages. But if right at the start you labour through the classical method and always learn the right endings -un, -in, -an, you can build on this later. As the joke goes: the first five years are difficult; after that it’s not too bad.\n“An erosion of classical Arabic”You learned Modern Standard Arabic, but in Arab countries the inhabitants speak various dialects. Was that disconcerting on your first trips there?\nTo learn Arabic is about as hard as learning four or five European languages. It’s very confusing when simple things such as the phrase “I want” can mean, depending on the dialect, ayiz, beddi, brid, dayir, ashti, bghiti, bhibb, abi…. In Modern Standard Arabic it means “uridu”, but that’s rather out of place in everyday life. A truly frustrating experience for someone who has laboriously learned classical Arabic and then travels to an Arab country and has to ask for directions! When I was at university, many Arabs still had a command of correct Modern Standard Arabic. For about two decades now it’s been apparent that this has become a rare expertise. Even writers make grammatical errors that I notice, because at the beginning of our studies and at the Language Institute of Damascus we were tormented with the fine points of syntax and declensions. In Arab schools that’s evidently no longer the case. The result is a real erosion of classical Arabic.\nYou work as an interpreter, but also translate Arab literature. What are your criteria for selecting a book to translate?\nWhen it’s up to me, then my decision is based on what I like and whether the book will remain a strong text in German translation. When I receive a commission for a translation, the first thing I do is make sure I consider the work to be valuable. The public should be given texts that testify to the vitality and creativity of Arab literature. Literature shouldn’t be translated for purely financial reasons. But it happens all too rarely that you’re commissioned to translate Arab literature. Usually it’s the case that I discover something interesting and would like to translate it, but can’t find a publisher. There isn’t much curiosity here about the work of Arab writers.\n“Arabic metaphors don’t work in German”What difficulties does Arab literature harbour for the translator?\nEssentially the same ones as translators from other languages have to do with. You must be creative to render literary devices as elegantly and precisely as possible, without it sounding forced or outlandish in the target language. Many metaphors don’t work in German, so they have to be recast. It’s a process of endless decision-making. Only with practice does this agony become a pleasurable affair and you realize where you should soften or intensify, smooth, stretch or restructure; in short, how you “say more or less the same thing, but in other words”, as Umberto Eco has described it. Difficult about Arab literature is that much that concerns religion and aspects of everyday cultural life is only alluded to. In German this has to be elaborated so that everyone understands it."", 'Together with seven other (aspiring) translators and writers, our editorial assistant Leonie Rau attended translator Sawad Hussain’s workshop on ‘Translating Humor in Contemporary Arabic Literature’, hosted by Catapult on September 25:\nBy Leonie Rau\nSawad Hussain says that she got the idea for a workshop on translating humor from her own experience translating a short-story collection by Najwa Bin Shatwan. The collection is “full of humor and satire” — both notoriously difficult to translate — as Sawad writes in the course description: “Humor is one of the most difficult concepts for any writer or translator to engage with, and yet undoubtedly the most rewarding when it’s done right.”\nAfter she got interested in doing it right, she began researching and reading widely on the topic of humor: how to pinpoint what makes a text funny, and how to transport this into another language.\nSawad then very generously shared her knowledge in this workshop, aiming to familiarize us with different types of humor in contemporary Arabic texts across different genres and media.\nAs a warm-up, we discussed jokes and tongue twisters — sussing out what exactly made them funny and realizing how very dependent humor is on time, place, and context.\nIn the course of the workshop’s 2.5 hours, Sawad acquainted us with Arthur Asa Berger’s framework of comic techniques and guided us expertly through applying it to excerpts from Arabic short stories, novels, and even a graphic novel.\nWe read, for example, the short story “The Memoirs of Cinderella’s Slipper” by Shahla Ujayli, from her collection A Bed for the King’s Daughter, translated by Sawad. In this story, a modern-day Cinderella feels compelled to beat up various unlikeable people she encounters, using her elegant slipper. One of the humorous techniques we identified in this story was what Berger calls “burlesque,” which “refers to any literary form that makes individuals, social behavior or other literary works ridiculous by imitating them in an incongruous manner.” In this instance, we thought the technique was used to criticize societal structures by subverting a familiar tale.\nUnderstanding exactly why a certain scene, setting or joke is humorous is crucial for rebuilding that humor in a second language: once you figure out whether it is the sound of repeating letters in a tongue twister, the use of a stereotype or the reversal of a well-known standard script, you can begin to think about approximating those humorous devices in a way that makes sense and is funny in another language, too.\nIt turns out that even though humor might not be universal, its building blocks seem to be, and anyone equipped with the proper tools can learn to identify them.\nIf all this sounds great, you’re in luck: Sawad will run an expanded version of this workshop in the form of a three-week course hosted by the British Library, starting October 13. Find all the information on the class and how to register here and follow Sawad on Twitter @sawadhussain to catch any updates.\nLeonie Rau is a Master’s student in Islamic and Middle Eastern Studies at the University of Tübingen, Germany, and hopes to pursue a PhD after her graduation. She is an aspiring literary translator with a particular interest in Maghrebi literature. She also writes and edits for ArabLit and ArabLit Quarterly and can be found on Twitter @Leonie_Rau_.']"	['<urn:uuid:9e5ebbd7-3533-4fd1-832f-bacb670329d5>', '<urn:uuid:66de96b1-a279-473d-b63c-974dcb68f920>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	22	62	1366
9	planning strategic management implementation challenges explain	Strategic business management and planning faces several key challenges. First, decisions tend to be complex with multiple factors and possible outcomes to consider. There is typically high uncertainty both about current circumstances and likely consequences. Strategic planning also has extensive impact across operational levels and requires integrated processes that cross functional boundaries. Additionally, it often leads to organizational changes that affect resource capacity and organizational culture, which are particularly difficult to manage. For nonprofits specifically, there are added challenges around measuring and communicating impact, as well as avoiding the 'nonprofit starvation cycle' where focus on overhead costs can detract from infrastructure needs. Successful implementation requires balancing formal planning systems while remaining flexible enough for innovative thinking and emergent strategies that arise in response to environmental changes.	"['Running a successful organization is an exercise in balance between short-term needs and a long-term vision. We all get caught up in the “tyranny of the immediacy”- when operational necessities and challenges demand our attention at the expense of strategic thinking. Luckily, there are a number of ways to keep your “eye on the why” while still attending to daily responsibilities — and one of the most effective of these strategies is impact metrics.\nIn the for-profit space, tracking metrics is primarily about business health, and developing an understanding of month-to-month profitability and growth trajectories. However, for non-profit organizations, metrics take on a whole different meaning. It isn’t only about organizational growth; it’s about impact on constituents and the world around you. It’s about focusing on why you do what you do. Metrics are the best way to prove your theory of change — to your stakeholders and to yourself.\nHere are three key questions for nonprofits consider when they think about designing impact metrics:\nDo you have established metrics tied to a clearly articulated “meta-change?”\nNon-profits are well versed in the importance of mission statements. What is equally important, and sometimes less articulated, is the importance of a vision. While the mission is what your organization does on a daily or yearly basis, the vision is a description of the world or community your organization is working to realize. A good way to write a vision statement is to complete the following sentence “Our vision is a world where…”\nA good example is The Nature Conservancy. Their mission is “to conserve the lands and waters on which life depends”. But the reason they do this is their vision: to create “a world where diversity of life is thriving and people act to conserve nature for its own sake.”\nThis is a key element of your organizational purpose, and when we talk about “keeping your eye on they why” it requires a comprehensive understanding of your vision: not just what you do but why you do it. When your mission and vision are combined with organizational values (not just values you want to uphold in your organization, but also values you believe must exist in the world in order to achieve you vision), you have everything you need to start developing metrics.\nWhat do you need to move the needle on in order to achieve your vision? If you are successful at your mission how could this success be measured? Once these metrics are determined you will need to set up a tracking system. How will you capture each piece of data? How will you store it? What format will you report it in? Building out metrics takes planning, and answering these questions is a key first step.\nAre you using your metrics to inform your programming?\nDue to time constraints, many nonprofits only get around to measuring programmatic impact when required by grant reporting. But don’t wait until grant requirements kick in to get started — metrics should apply across programs and be a core component of organizational analysis.\nIn addition, it is important that the metrics are used for more than just demonstrating success in a quarterly report. Regular measurements and analysis can be used to evaluate and adapt programming. Ideally, a new nonprofit would develop metrics first and then programming aimed to impact these metrics. This feedback loop encourages data-driven decision making within your nonprofit and ensures programming is impact-oriented and mission-aligned.\nAre you using metrics to effectively demonstrate impact to funders?\nIn the absence of clear impact metrics, conversations with funders may focus on “overhead” — the proportion of organizational costs not directly tied to program expenses. But for those of us who work in the space it has become clear that not only is overhead a fuzzy category, but that it has created the wrong frame for thinking about nonprofit finances.\nFunding graphs like the one below are part of the problem. Funders have unrealistic expectations about how much it costs to run a nonprofit and want to shrink the “non-programmatic” expenses. In turn, nonprofits feel pressured to misrepresent costs and skimp on infrastructure, reinforcing funders’ skewed beliefs. This is what the Stanford Social Innovation Review calls the “Non-Profit Starvation Cycle.” While overhead is a destructive concept, the reason it gained so much traction is that it is difficult for funders to discern which organizations are making significant impact and compare them against one another.\nSo, what can nonprofits do to build this trust without falling victim to the nonprofit starvation cycle? One approach, taken by charity:water is to embrace the separation between programmatic and non-programmatic expenses. Founded in 2006, charity:water’s mission is to bring clean safe drinking water to people in developing nations. What they have done, perhaps better than any other nonprofit in recent memory, is communicate impact to funders.\nIn an effective use of geo-tagging, donors receive the GPS coordinates of water projects as they are constructed. That’s not all — when it comes to allocating funding, charity:water has two separate bank accounts: one specifically for programs and another for operations. The operations account funded by private donors, foundations and sponsors, allowing charity:water to tell individual donors that “100% of donations go directly to program funding.” To date they have raised more than $200 million from over 300,000 individuals.\nHowever, not all organizations can build a network of wealthy donors who want to exclusively fund operational expenses. The other option for nonprofits is to reframe operational expenses not as an unfortunate necessity, but rather as core components to each program. The graph above, from the Nonprofits Assistance Fund reframes overhead expenses as core mission support. As they write “each program requires core mission support, and to restrict funding to only the outside circle is the same as underfunding that program.”\nRegardless of the path your nonprofit pursues to acquire funding for operations, it all comes back to impact metrics. If you can prove that your nonprofit is achieving your mission and making an impact, then the conversation with funders can focus on those numbers rather than devolving into a discussion about overhead.\nI’ll be honest: developing impact metrics isn’t easy. If it were, they would be significantly more prevalent in the nonprofit space. Setting up a system of metrics measurement, tracking and reporting will take significant up-front planning, and regular attention from you or your staff. The world is complicated and trying to distill your impact into numbers is a challenge. But if done correctly, these metrics will provide your organization with an invaluable understanding of constituent and community impact.\nWhatever stage your nonprofit is at, using metrics to inform your programming will set you up for long term success and ensure that you adapt to meet evolving needs. It is easy to deprioritize, especially when caught up in the tyranny of the immediacy. But if done correctly, metrics will make your job easier, not harder. And most importantly, they will allow you, and your entire organization, to keep your eye on the why.\nAbout the author: Kevin Wilkins is the founder and Managing Director of trepwise, an impact consulting firm whose mission is to use entrepreneurial thinking and approaches to grow and sustain for-profit and non-profit organizations by providing innovative and practical solutions.', 'The term strategy has found a limited definition arising, principally, from military origins. This definition has been expanded into the business context where many authors have argued strategy in terms of quantitative and qualitative processes. However, it continues to defy a singular, definitive definition. In order to understand strategy, we must look beyond its military antecedents and identify strategy as a life process.\nDifferent possible definitions of strategy are:\n""Strategy is the direction and scope of an organisation over the long term, which achieves the advantage in the changing environment through its configuration of resources and competences with the aim of fulfilling stakeholder expectations"".\n""Strategies are developed in order to achieve the goals and objectives of the organisation"".\n""Aconsistency of direction based on a clear understanding of the ""game"" being played and an acute awareness of how to manoeuvre into a position of advantage"".\nGet your grade\nor your money back\nusing our Essay Writing Service!\n""Strategy is not just a notion of how to deal with an enemy or set of competitors or a market. It draws us into some of the most fundamental issues about organizations as instruments for collective perception and action"".\nScope and nature of strategic business management and planning:\nScope and nature of strategic business management and planning is a widespread topic and it is hard to list down every aspect of it. I am using JS&W model to describe some of the key aspects of strategic business management and planning:\nThe organisation\'s long termdirection: no specific timescale is envisaged but one should think in terms in excess of one year and more probably of several years.\nThe scope of an organisation\'s activities: This will include both the overall roles and purposes the organisation accepts for itself and the activities it undertakes in pursuit of them.\nFor commercial organisations and for many not for profit organisations too, strategic planning will be about gaining some kind of advantage in competition.\nStrategic decisions are affected by the values and expectationsof organisation\'s stakeholders. Stakeholders are people who have a legitimate interest in what the organisation does.\nCharacteristics of strategic business management and planning:\nUsing JS&W model as an reference, some of the key characteristics of strategic business management and planning are:\nDecisions about the strategic planning are likely to be complexsince there are likely to be a number of significant factors to take in to consideration and a variety of possible outcomes to balance against one another.\nThere is likely to be a high degree of uncertaintysurrounding strategic planning, both about the precise nature of current circumstances and about the likely consequences of any course of action.\nStrategic planning and management have extensive impact on operational decision making, that is, planning and decision at lower levels in the organisation.\nStrategic planning affects the whole organisation and requires processes that cross operational and functional boundaries within it. An integrated approachis therefore required.\nStrategic planning and management are likely to lead to changewithin the organisation as resource capacity is adjusted to permit new courses of action. Changes with implications for organisational cultureare particularly complex and difficult to manage.\nKey strategies of British Airways are:\nBe the airline of primary choice for long haul premium customers:\nSo people will want to fly with us at any time they can. We will carry on to come up with great stuff such as the new business class seat on long haul and a restyled First cabin.\nDeliver a incomparable service for customers at every touch point:\nBy training crew, on the ground and in the sky, in world-class warmth and customer service. Customers can check-in from their mobiles or PDAs.\nGrow our survival in key global cities around the world:\nTo render the best global connectivity for our customers. In addition toour new long haul service from London City to New York JFK, our network depth will strengthen with more flights to Dubai and Johannesburg and a return to Saudi Arabia.\nBuild on our leading stance in London:\nAlways on Time\nMarked to Standard\nThe world\'s biggest aviation market. Ensuring Heathrow remains a world classhub is vital to give us a powerful London base to cater the largest international long haul markets. We will obtain new slots, support plans for athird runway and work with BAA to improve baggage and terminal conveniencesat Heathrow.\nMeet our customers\' needs and improve profit margins through new revenue streams:\nBy coming up with profitable supplementary services that offer customers great value and re-enforce our brand. Our aspire is to grow our mileage business and further revenues from third-party engineering, in-flight sales and a new online retail website. On ba.com we have now launched a variety of great value hotel and car hire options packaged with our flights.\nThe decisions we are taking now will establish how strongly we materialize from the downturn. The airline industry is in a period of unprecedented change and we have developed a clear strategy for our business.\nIn order to become the principal global premium airline, we necessitate to look at the way we work as well as what we are doing as a business.\nFor that rationale, along with our five key strategies, we have launched a three-year change programme - Compete 2012 - correlated to our sponsorship of the London 2012 Olympics. This programme is being progressively rolled out across our business to revive our culture and will reform the way we work.\nNeed of strategy in global environment:\nToday\'s environment is very much dynamic. Organisations are in a constant exercise of realigning themselves to the needs of the environment. Defined and thorough strategy plays a vital part here. Especially for a globally operating organisation like BA, it could be a matter of success or failure. Managers must be aware that markets, supplies, investors, locations, partners, and competitors can be everywhere. Successful businesses will take advantage of opportunities wherever they are and will be prepared for downfalls. International strategy is the continuous and comprehensive management technique designed to help companies operate and compete effectively across national restrictions, For example, some companies form partnerships with companies in other countries, others acquire companies in other countries, others still develop products, services, and marketing campaigns designed to appeal to customers in other countries. Some rudimentary aspects of international strategies mirror domestic strategies in that companies must determine what products or services to sell, where and how to sell them, where and how they will produce or provide them, and how they will compete with other companies in the industry in accordance with company goals.\nGenerally, a Company develops its international strategy by considering its overall strategy, which includes its operations at home and abroad. We can consider four aspects of strategy: (1) scope of operations, (2) resource allocation, (3) competitive advantage, and (4) synergy.\nBesides the fact that well defined strategic business management and planning processes bring structure, control and consistency to the overall decision making of the organisation but we (the company) should also consider that the formal decision making model discussed above may distract manager\'s attention from controlling actual process as making strategic plans is not the same thing as managing the process. A split may also develop between the people responsible for planning and those responsible for implementation. Particularly in large organisation like ours (BA) the planning system may be too complex and extensive for even quite senior managers to understand the way it works. Over-formal planning systems and over-rigid control can hamper innovative thinking.\nAs the national and international environment is always in a constant move so one can say that there is no one best planning and management model. Now a days the environmental factors and most importantly the customer are in a greater power than before that influences the overall structure, strategy, planning and management processes of the organisation.\nPrescriptive and Emergent (Mintzberg) strategies:\nIn real world, 100% ""intended or prescriptive"" strategies does not get ""realised"". Some intended strategies fail as predicted environment keeps changing. Emergent strategies arise in response to unexpected changes in the environment and may be better than intended strategies. Hence, organisations should keep some room for emergent strategies alongside the formal process i.e. Rational model.\nThis Essay is\na Student\'s Work\nThis essay has been submitted by a student. This is not an example of the work written by our professional essay writers.Examples of our work\nFollowing diagram is a snapshot of how a mix of prescriptive and emergent strategies get realised in the practical world:\nDifferent levels of strategy and their relationship with each other: (Hofer and Schender)\nStrategic business management and planning can be formulated on three different levels:\nTactical or business level.\nCorporate level strategy makers analyze the common needs of business units and add impact to the whole system in addition to individual development of participating business units. Issues with reference to the introduction of new products or growth into new markets or segments are all a part of this strategic level.\nCorporate level strategy forms the stem of the strategic decision tree and the management has to be wholly responsive of its implications as well as the sensitivity of all succeeding strategies, no matter at what level. It is of prime substance that corporate level strategy is entirely associated by and large with the vision of the business and the values and prospect of stakeholders.\nBusiness or tactical level strategies are in essence position strategies whereby businesses safe for themselves uniqueness and spot in the market. The endeavour here is to augment the business value for the corporate and stakeholders by increasing the brand understanding and value professed by the customers.\nThe third level of strategy is the operational level which chiefly is concerned with successfully implementing the tactical decisions prepared at Corporate and business unit level through optimal consumption of resources and competencies of the business unit.\nA methodical understanding of the three levels of strategy makes their strong co-dependence and non-hierarchical nature obvious. All strategies have to be in absolute harmonization with each other since the accomplishment of one is inseparably related to the other. So as a substitute of being in a top-down order, the inter-connecting can be visualized as a triangle with the three corners indicating the three levels.\nVisions, missions and objectives:\nThe two organisations which are to be contrasted and compared are British Airways (for Profit) and Oxfam (Not for Profit).\nVision means the category of intentions which are broad, all-intrusive and forward-thinking. The corporate success is reliant on the vision set by the top management. A vision is the image that a business must have of its objectives before it sets out to reach them. It describes ambitions for the future, without specifying the means that will be used to achieve those desired ends.\nVision of British Air :\nThe Vision focuses on employees and customers â€¦.it emphasizes BA\'s desire to be the world\'s global airline.\nVision of Oxfam UK :\nOxfam\'s vision is that diverse communities of people living in poverty will exercise their rights to a decent and secure standard of living in developed society.\nA mission statement is an organization\'s vision converted into written form. It makes tangible the management\'s view of the direction and purpose of the organization.\nMission of British Airways :\nTo be the undisputed leader in world travel for the next millennium.\nMission of Oxfam :\nThe main areas of Oxfam mission focus are poverty alleviation, action against violence, making people exercise their rights, removing gender inequality and preventing and reducing environment damage.\nObjectives give the organization a clearly defined target. Planning can then be made thereafter to achieve these targets. This is helpful in motivating the employees. It also allows the business to measure the progress towards to its stated aims.\nObjectives of British Airways :\nThe main objectives of British Air revolve around the following key areas:\nEmployee satisfaction and retention\nBeing more effective as an international carrier as compared to other airlines.\nMaking efficient use of resources.\nUsing methods that are environmentally friendly.\nMaking the airline more competitive in terms of fares while at the same time focusing on profitability.\nObjectives of Oxfam :\nThe main objectives of Oxfam address the following:\nCreating programs for people to make them exercise their rights.\nTaking measures to reduce environmental damage.\nCreating awareness among women and thus working towards gender equality.\nTaking practical measures against the use of violence.\nMethodology for creating Vision, Mission and Objectives:\nThe methodology used would be:\nVision: Based on the views of the management to create a certain image of the airline in the long-term and how the founding members/key stakeholders want to see the organization in future.\nMission: It will be based on the vision and the methodology used will depend on the need for to stand out amongst the airline industry.\nThe Objectives will be decided comparing the performance of other airlines and the key steps / indicators required to translate the vision in to tangible results.\nMethodology for creating Vision, Mission and Objectives:\nVision: This will be decided according to the core principles on which a charity like Oxfam is founded. The factors considered would be the environment in which the charity operates and the people affected by its activities.\nMission: This area is decided according to the aims of the key stakeholders and the different areas on which Oxfam focuses including poverty reduction, disaster relief and development projects around the world.\nThe objectives will be set for the short or long term and the performance measures for the charity taking into consideration the main areas of operations as well as the likely hurdles in attaining a particular goal.\nStrategic business management and planning method:\nStrategic Business Management and Planning Method of the organization :\nOne of the main methods used is the SWOT Analysis and the PEST analysis. It takes into account the likely impact of the steps taken by British Air to meet its objectives taking into account the competitors as well as the global conditions existent in the world of travel and transport. This will focus on the main areas which can have a direct impact on the running of the airline including the fuel prices , customer satisfaction, competitor fares etc.\nStrategic Business Management and Planning Method of the organization :\nThe methods used for a non-profit like Oxfam would be different as they will focus on key achievement areas in its global relief and humanitarian efforts rather than profitability. A key method might be the Value for Money (VFM) process. The likely areas taken into account during these methods might include the amount of donations received, the number of people served and the effectiveness of the work undertaken by Oxfam.\nThe influence of corporate governance and regulations:\nInfluence of Corporate Governance and Regulations:\nBritish airways has to take into account different areas of corporate governance including the performance of the board and the regulation concerning the executive pay, the roles and duties of chairman and the chief executive , regulation concerning the functioning of board committees, shareholder accountability , environmental regulation.\nInfluence of Corporate Governance and Regulations:\nOxfam\'s corporate governance procedures will take into account the requirement for directors and trustees and adhering to a code of conduct(based on the Nolan Committee Principles of Standards in Public Life),also looking into possible conflicts of interests . The specifications of the memorandum of association and the key charitable aims of the charity.\nRelevance of visions, missions and objectives:\nMany organisations never explore their reasons for being in business. They are so involved in day-to-day actions they fail to see the bigger picture. So what is the significance of having an articulated Vision, Mission and Objectives?\nIt starts with establishing your core values. Core values are things that drive an organisation. They are the driving force of an organisation. They are fixed and do not change over time.\nOnce the core values are established, the next step is the Vision, a long-term goal. It can be something minute or something that is huge.\nThe Mission flows from an organisation\'s Vision. A Mission is important because it embraces the commitment of an organisation to staff, customers, and the community at large. It may define the company ethos, core beliefs of the owners, service levels, quality, excellence, training and commitment.\nSetting objectives is the stage that comes next in the process. It is important because it shows how an organisation delivers on the promises made by its Mission. If there is clarity in both the Vision and Mission, it is easy to decide on the objectives. Objectives can be set for each key area of the business along with a time-line for completion. Objectives may be corporate, product, market, sales, financial, operations, or staff related or any combination of these.As can be seen, this is a trickle down system with higher level aspirations that flow down to every level of the business and become the glue that holds everything together. The benefits of having a vision, mission and objectives are clear and unambiguous. Every step that an organisation takes is in pursuit of its articulated end game.']"	['<urn:uuid:b8dbff5f-b5e3-4e06-a6e9-2e90c4cf1bd0>', '<urn:uuid:c00b853d-0e32-431e-8e88-f84f1fb4d10c>']	open-ended	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-12T19:10:11.825225	6	126	4063
10	organic garden chemical rules compare traditional	Organic gardening and traditional gardening have different approaches to chemical use. In organic gardening, only natural-origin pesticides and fertilizers are allowed, though alternatives may require more work. Additionally, organic regulations prohibit harmful organochlorine pesticides that can persist in the environment, bioaccumulate in the body, and have been linked to health issues. Traditional gardening allows both natural and synthetic chemicals, including chemical weed-killers and fertilizers. However, both approaches require careful application to avoid affecting neighboring plots, particularly when organic and conventional plots are adjacent.	['An allotment is your own little piece of land to do as you will on – as long as it’s legal! But to get the best from your allotment it may be helpful if you are aware of some allotment/gardening conventions.\nTo take on an allotment is to become part of a community and, as in any community, it will work better if certain – possibly unspoken – rules are followed and etiquette is maintained.\nYour allotment neighbours could become your best friends. They’re a source of advice, leftover seeds and even manual assistance if you’re trying to erect a shed with no help. A good allotment site will have an excellent community spirit: you’re all there for the same purpose and it’s to mutual advantage to work together.\n- let your plot get overgrown and neglected so that weed spores are carried onto their carefully-tended plots;\n- let your dog run – or do worse – on their plots;\n- play heavy metal music very loudly on an otherwise peaceful Sunday afternoon. You might think your plants respond well to it but your neighbours might not;\n- plant tall bushes or trees where they’re going to cast a shadow over your neighbour’s flower plot;\n- light a bonfire on a windy afternoon;\n- be over-enthusiastic with the chemical weed killers if you know your neighbours are trying to maintain a strictly organic plot. Be careful how you apply it so as to avoid it being blown or spreading onto their land.\n- treat them as you would like your neighbours at home to treat you;\n- ask their advice. They’ll be the local experts and will know all about the soil and conditions and what works well and what doesn’t. There’s no point trying to re-invent the wheel;\n- offer to help them if you see if they have a particularly tricky job to do;\n- share your surplus;\n- be willing to learn from them.\nYour Tenancy Agreement\nWhen you took on the plot you and the site owner should have signed a tenancy agreement. This will give you certain rights but also some responsibilities. You are legally bound to maintain the plot in a reasonable condition and to keep your hedges, if you have them, trimmed.\nEach tenancy agreement will be different depending on the site. Some ban the erecting of sheds or greenhouses while others stipulate that permission must first be sought. Others may forbid bonfires. To avoid being given notice to quit just as your first crop of runner beans is appearing, check yours carefully to find out what you can and can’t do.\n- use chemical weed-killers or fertilisers if you want organic vegetables. When you start investigating you’ll find there are plenty of alternatives – even though some may mean much more hard work!\n- grow the same crops in the same place year after year. Crop rotation is one of the basic principles of food growing. Different vegetables will take different nutrients from the soil and if you continue to plant cabbage, for example, in the same bed repeatedly, the soil will become depleted and unable to support a healthy and flourishing crop.\n- expect to have a perfectly-maintained and productive plot in your first years. Allotment gardening takes a lot of work and commitment. A spell of bad weather or an attack of the killer caterpillar can cause havoc on your allotment but don’t let it deter you. Put it down to experience and carry on. Similarly if you break your leg or the children take it in turns to have chickenpox, and you’re all housebound for weeks, don’t panic: it’s all part of the fun of allotmenteering.\nAnd finally, do:\n- think about the chemicals you’re using. Even if you’re not planning on going completely organic, one of the major benefits of growing your own is the ability to control what’s on your food.', 'What is an ecological pesticide?\nCurrently the certificate of organic farming only takes into account that the pesticides used are of natural origin and prohibits those of artificial origin.\nBut, technically, this claim seems to respond only to a marketing issue, because natural insecticides are not always ecological because , in some cases, they are very aggressive for many plants and animals.\nSpecificity of pesticides\nEven if a natural insecticide is used, it may be that this product is non-specific and also affects bees and other pollinating insects, which would be very harmful to the environment.\nIn other words, for a pesticide to be organic it must be specific, that is, to attack only the plague or disease that affects the crop, and not harm the other animals of the ecosystem. Of course, it must also be a pesticide that respects the health of people and the environment, not be toxic or bioaccumulate.\nExamples of ecological pesticides harmful to the environment\nAmong the ecological pesticides harmful to the environment, perhaps the best known case is Spinosad, an authorized insecticide in organic farming capable of killing many pollinating insects.\nAnother example is copper, ecological antifungal, that with the rain filters in the subsoil causing the death of earthworms and other nematodes.\nIn this aspect, conventional agriculture is worse, since it also uses non-specific chemical insecticides that are aggressive to the environment. However, for some pests, treatments have been developed with specific artificial hormones that do not affect other animals, so they are specific and therefore more ecological.\nAdvantages of ecological pesticides\nA big point in favor of the legislation on pesticides in organic farming, is that the seal of organic farming does not allow the use of organochlorine pesticides, which are very harmful because:\n- They are persistent in the environment (they take centuries to disappear from the environment)\n- They bioaccumulate in the body (more obesity, more of these pesticides in the body)\n- They appear in the placenta and breast milk of women, thus reaching the fetus and the baby (with unknown health repercussions)\n- They have a hormonal effect, they act as endocrine disruptors\n- They have been linked to cancer. There seems to be a very clear relationship regarding the use of these components and brain cancer in children. They could also be related to breast cancer.\nNatural or artificial pesticides?\nIn summary, that an insecticide is of natural or synthetic origin does not mean that it does not affect many animals or that it is less toxic.\nFor a pesticide to be ecological, the specificity of the pesticide used should be taken into account, and for the moment, in the pesticide regulation of organic farming, only take into account that it is of natural origin and toxicity to humans (not over the ecosystem).\nDoes organic farming use antibiotics?\nThere is a myth that organic farming does not use antibiotics. Farms and beekeepers have the obligation to use authorized antibiotics (both organic and conventional). This is necessary because the diseases can spread to all the animals or to the whole swarm, producing in many cases the death of them.\nTherefore, organic farming regulations do allow (and require) the use of antibiotics, although only those authorized by the regulation, which again are those of natural origin.\nMore information on ecological food']	['<urn:uuid:a6d1521a-f6bb-4a1b-99c0-25639f16c4f9>', '<urn:uuid:813f5f5a-6fb2-4e7b-9710-5883ad563523>']	factoid	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	6	83	1207
11	What are the risks of relying too heavily on early disease detection, and what role does cognitive therapy play in treating depression?	Early disease detection can be misleading and harmful in several ways. It can lead to over-diagnosis and unnecessary treatments, as some conditions develop so slowly they may never threaten health. Additionally, early detection can create a false sense of security, causing people to ignore actual symptoms. As for depression treatment, Cognitive-Behavioral Therapy (CBT) has been proven highly effective, with a comprehensive review of 325 studies involving over 9000 subjects showing significant improvement in depression for both adults and adolescents. CBT works by challenging dysfunctional thoughts that cause distress without inspiring constructive action.	"['""Prevention is better than cure."" Aphorisms like this go back a long way. And most of our dramatic triumphs against disease come from prevention: clean water, making roads and workplaces safer, antiseptic routines in hospital, reducing smoking, immunization, stemming the spread of HIV.\nMany of our cultural superstitions and greatest feats of gullibility are in the name of prevention, too. Of course. We generally have a pretty fervent desire to believe in human actions that can prevent the cruelties of fate. The fear of death eats logic for breakfast.\nIt\'s ironic really. The lure of prevention is freedom from disease. But it has become a key driver of over-medicalization of our lives and a growing shadow of disease angst when we\'re the healthiest generation the world has ever seen.\nAct II: We\'re so lucky they caught it early...\nIgnoring symptoms, not knowing you\'ve got a dangerous infectious disease, and being too fatalistic about seeking help - these are recipes for disaster. But we tend to portray earliness as if it\'s always a virtue, though, and that can be a trap, too.\nWhy? Because while we can be too late for anything to help, we can also be too early to make a difference for the better. That might not sound like a problem. But it can be.\nSome diseases and conditions have a long lead time, developing so slowly, they may never actively threaten our health. When we\'re diagnosed early with those, we\'ve ""had"" the disease for longer, but we don\'t live longer. All we\'ve done is increase our ""disease survival"" by shortening the ""disease-free"" part of our lives.\nLead-time bias leads to a wildly misleading use of survival statistics. Even most doctors fall into this statistical trap. It has a lot to do with why early detection so often fails to deliver on the dramatic benefits some expect.\nBelieving in the benefits of being early without good evidence, isn\'t confined to just disease. It\'s true of early intervention with children. Dorothy Bishop explains what this means for children with slower than average reading development for example - and Jon Brock discusses the same phenomenon with a theory about detecting autism in babies.\nAct III: Screening and the triumph of hope over reason.\nOnly some screening does good - and it can always do harm. That\'s about the reverse of what many people would like to be true.\nWe\'ve just seen one reason that can mislead people - lead-time bias. But there are other phenomena that can exaggerate the benefits: the healthy volunteer effect, for example. Exactly the people who could least benefit from screening, tend to use it more. And they have better outcomes because they were always going to, regardless.\nScreening has to help more people than those who would have been diagnosed anyway when symptoms were being investigated. And there has to be a test that\'s accurate enough and acceptable enough for widespread use. And there has to be effective treatment that can make a difference if it\'s used before there are symptoms.\nThere\'s a double whammy here. Those people who fall into the logical fallacy of thinking, ""her doctor found cancer after those symptoms,"" therefore ""I should be screened"" - instead of ""I should take symptoms seriously."" And those who ignore symptoms, because they were screened and believe they\'re in the clear.\nWhen early intervention fails to halt disease progression, there\'s also a trend for some doctors to try to intervene even earlier. Perhaps even screening for ""pre-disease."" While it\'s possible that sometimes will pay dividends, this is generally grasping at straws. More often than not, we can expect this to be what I call ""the pre-disappointment phase.""\nAct IV: All\'s fair in love and the ideological prevention wars.\nThere is a science to evaluating the effects of screening tests. There\'s a good introduction here. So you\'d think that questions about what screening programs are worthwhile could be sorted out.\nBut the emotional stakes are high when it comes to preventing disease. And many people have become so vested in ideological camps about screening, that they\'ve basically become pro- or contra- screening fundamentalists.\nUnfortunately, that\'s as true of some scientists in the field as it is of other partisans. Michael Marmot is the professor who led an independent panel in the UK through the mammography minefield. He wrote, ""people interpret evidence and, indeed, influence its generation. Judgments often reflect more about starting assumptions than they do about the nature of the evidence…[O]ne only has to look at the author of an article to make a reasonable guess whether it will be pro or con.""\nIf you\'d like to catch up on the multiple meta-analyses about screening for breast cancer with mammography, I\'ve summarized them here on PubMed Commons.\nAct V: The suffering of the over-diagnosed and the spreading cloud of disease angst.\nThe tragedy of the disease prevention illusion carries a heavy toll. First, there are many people who may benefit most from prevention interventions, but can\'t take advantage of ways to improve their life chances. Resources may even be diverted from them because of the increase in diagnoses of others - something Margaret McCartney calls ""the patient paradox"" and Julian Tudor-Hart dubbed ""the inverse care law."" What\'s more, the constant bombardment with fearful messages may even backfire.\nThen there are the swelling numbers of people who are over-diagnosed, fighting heroic battles with disease phantoms and toxic, mutilating treatments that were never necessary. As the numbers of people being diagnosed with serious diseases grow, the chances we\'re affected, or someone we know and love is affected, grow too. And the shadow of fear of disease spreads.\nMost of the time we just can\'t know what would have happened if…. That doesn\'t reduce the feeling of having dodged fate\'s sword, though. And the chance that we, or someone we know, are ""living proof"" of the benefits of early detection makes it hard to get this into perspective. Widening the pool of people who understand and communicate the complexities of clinical effectiveness research with clear view to minimizing their own biases as well seems to me essential.\n""An ounce of prevention is worth a pound of cure."" We\'ve recognized the false expectations we inflate with the fast and loose use of the word ""cure"" and usually speak of ""treatment"" instead. We need to be just as careful with the P word.\nBrowse through all my posts at SciAm on prevention-related themes.\nThe recommendations of the US Preventive Services Task Force and more information about them is here.\nFind out more about preventing over-diagnosis here.\nHere\'s a more technical primer about bias in finding disease.\nThe 1968 World Health Organization report by Wilson and Jungner that codified the principles of screening for disease is here.\n* The thoughts Hilda Bastian expresses here at Absolutely Maybe are personal, and do not necessarily reflect the views of the National Institutes of Health or the U.S. Department of Health and Human Services.', 'Treatment For Depression And Mood Disorders\nBecause the causes of depression are complex, the treatments for depression are usually most effective when they are multi-modal, explains Dr. Shcherbakov.\nHe recommends patients begin by addressing any physical health issues that may exacerbate their depressive episodes.\nThese can include poor sleep or a bad diet with a lack of nutrition.\nGetting regular exercise, such as a daily walk, can also be helpful for those experiencing mild or moderate depressive symptoms.\nThe most successful evidence-based treatment for depression, however, is a psychological approach known as Cognitive-Behavioral Therapy .\nTholen points to a comprehensive review of 325 different research studies involving more than 9000 subjects that found CBT significantly improved depression in adults and adolescents.\nCBT relieves depression by challenging underlying dysfunctional thoughtsthose that cause distress without inspiring constructive action, says Tholen.\nOther therapy styles that have been shown by research to be effective in helping with depression include Acceptance and Commitment Therapy and Dialectical Behavior Therapy .\nMedication is commonly thought of as a first line treatment for depression in the mediathis is far from observed practice.\nDr. Shcherbakov points out that while medication such as SSRIs and SNRIs may be a helpful adjunct treatment to depression, these medications should not be the sole means through which people seek to treat their depressive episodes.\nFatigue And Loss Of Energy\nMajor depressive disorder is typically associated with loss of energy and fatigue, sometimes even after a good nights rest. People who are dealing with major depression often have trouble sleeping, which can also cause extreme tiredness during the day.\nChronic fatigue syndrome is another condition that can occur alongside major depressive disorder symptoms, and it makes a person experience continuous feelings of exhaustion without any easily discernible cause. This condition can be a symptom of depression, but its not to be mistaken with depression.\nHow Is Major Depressive Disorder Treated\nMDD is often treated with medication and psychotherapy. Some lifestyle adjustments can also help ease certain symptoms.\nPeople who have severe MDD or have thoughts of harming themselves may need to stay in a hospital during treatment. Some might also need to take part in an outpatient treatment program until symptoms improve.\nRead Also: How Common Is Depression In Adults\nCan Depression Be Prevented\nYou can help prevent depression by getting enough sleep, eating a healthy diet and practicing regular self-care activities such as exercise, meditation and yoga.\nIf youve had depression before, you may be more likely to experience it again. If you have depression symptoms, get help. Care can help you feel better sooner.\nHow Is Depression Syndrome Diagnosed\nEveryone may feel sad or down from time to time. However, clinical depression has more intense symptoms that last two weeks or longer.\nTo determine whether you have clinical depression, your healthcare provider will ask questions. You may complete a questionnaire and provide a family history. Your healthcare provider may also perform an exam or order lab tests to see if you have another medical condition.\nWondering Whether You May Be Depressed What The Symptoms Of Major Depressive Disorder Are And What The Treatment Is Heres What You Need To Know About Depression And How To Get Support And Help\nWhat to do if youre depressed?\nWhat is major depressive disorder ?\nHow to stop being depressed?\nDepression can take many forms, however there are several consistent symptoms.\nMental health professionals use these symptoms and signs of depression, listed below, to determine whether someone is suffering from a depressed mood and/or major depressive disorder .\nSome people who are depressed experience profound anhedoniaa loss of pleasure in everyday activities.\nAmericans have been growing over the past 18 years.\nOthers experiencing depression may not be fully aware of whats going on and may attribute our experiences to a low mood or feeling down.\nWhether were facing new challenges at work, tackling issues with family or conquering financial difficultiesfeeling down, experiencing a low mood, or feeling anxious can be a normal human response to the wide array of stressors we all experience.\nFeeling down can be a response to particular stressors and grief, howeverin these casestemporary low mood generally does not mean we are depressed.\nBut, what about if feeling down begins to take over and become something we are experiencing every day?\nOr if anhedonia kicks in with full force, rendering us unable to enjoy our lives as we did before?\nDoes it mean were depressed? And, what exactly is depression, anyway?\nRead on to find out the official signs and symptoms of depression, as well as the treatment, and how to get help if you think you might be depressed.\nWhat Are The Signs Of Major Depression In Men\nDepression in men is significantly underreported. Men who suffer from clinical depression are less likely to seek help or even talk about their experience.\nSigns of depression in men may include irritability, anger, or drug and alcohol abuse . Suppressing negative feelings can result in violent behavior directed both inwardly and outwardly. It can also result in an increase in illness, suicide, and homicide.\nAlso Check: Best Way To Beat Depression And Anxiety\nReach Aac Out For Help\nIf you are experiencing symptoms of depression or thoughts of suicide, help is available. Please call a suicide helpline or 911 immediately if you are having thoughts of harming yourself.\n- The National Suicide Prevention Lifeline is available 24 hours a day at 1-800-273-TALK .\n- The National Alliance on Mental Illness features listings of local programs and support groups as well as a helpline.\n- The Substance Abuse and Mental Health Services Administration s Behavioral Health Treatment Services Locator allows users to search for treatment options in their area. SAMHSA also has a 24-hour helpline to provide treatment referrals and information about mental health and substance use disorders at 1-800-662-HELP .\nDiagnosing Major Depressive Disorder\nIf you or a loved one have been feeling depressed and low, seek help as soon as possible. You can reach out to a mental healthcare provider or contact your primary care doctor for a diagnosis or referral.\nYour healthcare provider will ask you a series of questions that will likely cover your symptoms, thoughts and feelings, and medical history. They may need to perform a physical or psychological exam, or conduct lab tests, in order to rule out other health conditions that can cause similar symptoms.\nYour healthcare provider will determine whether or not your symptoms meet the diagnostic criteria for major depressive disorder, which include:\n- Having a persistently depressed mood and lack of interest in activities\n- Having five or more symptoms of depression\n- Having symptoms every day, almost all day\n- Having symptoms for over two weeks\n- Being unable to function like you did before, due to the symptoms\nRecommended Reading: How Do I Come Out Of Depression\nWhat Are The Types Of Major Depression\nThere are several types of depressive disorders:\nNeuroendocrine Abnormalities And Neurodegenerative Diseases\nPossible abnormalities of the neurotransmitter systems remain under investigation. Compared with control subjects, depressed prepubertal children had lower cortisol secretion during the first 4 hours of sleep, according to De Bellis et al. Nocturnal secretion of adrenocorticotropin, growth hormone, and prolactin did not differ between the 2 groups.\nPotential biological risk factors have been identified for depression in the elderly. Neurodegenerative diseases , stroke, multiple sclerosis, seizure disorders, cancer, macular degeneration, and chronic pain have been associated with higher rates of depression. Alternatively, a large, longitudinal study found that depression that starts early in life increases the risk for Alzheimer’s disease . Researchers used data from the Prospective Population Study of Women in Gothenburg Sweden, which began in 1968. The study sample included 800 women , born between 1914 and 1930, who were followed up with in 1974, 1980, 1992, 2000, 2009, and 2012. Data show those women who experienced the onset of depression before age 20 years were three times more likely to develop AD .\nDon’t Miss: Can Depression Go Away By Itself\nRisk Factors For Depression\nDepression can affect anyoneeven a person who appears to live in relatively ideal circumstances.\nSeveral factors can play a role in depression:\n- Biochemistry: Differences in certain chemicals in the brain may contribute to symptoms of depression.\n- Genetics: Depression can run in families. For example, if one identical twin has depression, the other has a 70 percent chance of having the illness sometime in life.\n- Personality: People with low self-esteem, who are easily overwhelmed by stress, or who are generally pessimistic appear to be more likely to experience depression.\n- Environmental factors: Continuous exposure to violence, neglect, abuse or poverty may make some people more vulnerable to depression.\nEveryone Experiences Depression Differently\nIts important to note that everyone experiences mental health conditions such as major depressive disorder differently. While some people may have many symptoms of depression, others may have only a few. The frequency, duration, and severity of the symptoms can also vary from person to person.\nThe symptoms of major depressive disorder can also vary depending on a persons age. Listed below are some symptoms people may experience, depending on their age:\nRecommended Reading: How Do You Help Someone With Depression And Anxiety\nDepression And Addiction Risk Factors\nResearch indicates that depression is likely caused by a combination of factors. According to the National Institute of Mental Health, genetic, environmental, biological, and psychological factors all have the potential to contribute to the development of MDD.1\nWhile it is not possible to predict exactly who will be impacted by depression, certain risk factors for MDD do exist. Factors may include:1,2\n- Past experience with depression or other mental illnesses.\n- Family history of depression, bipolar disorder, suicide, or alcoholism.\n- Significant life changes.\n- Psychomotor agitation or retardation.\n- Recurring thoughts of death or suicide and/or suicide plans or attempts.\nA clinical diagnosis of major depressive disorder requires that the individual experiences at least 5 of these symptoms in a 2-week period .3 With the exception of thoughts of death suicidal ideations, plan, or attempt or weight changes, a symptom must be present nearly every day to be counted. major depressive disorder symptoms must be a change from prior functioning and cannot be due to another medical or mental health condition or caused by a substance. They must also cause significant distress or impair functioning and cannot be an appropriate response to a significant loss.\nHow Is Depression Diagnosed\nDepression can happen along with other medical conditions. These include heart disease, or cancer, as well as other mental health conditions. Early diagnosis and treatment is key to recovery.\nA diagnosis is made after a careful mental health exam and medical history done. This is usually done by a mental health professional.\nYou May Like: Best Anti Depression Meds For Anxiety\nWhen To Call Your Healthcare Provider\nIf you have 5 or more of these symptoms for at least 2 weeks, call your healthcare provider:\nLasting sad, anxious, or empty mood\nLoss of interest in almost all activities\nAppetite and weight changes\nChanges in sleep patterns, such as inability to sleep or sleeping too much\nSlowing of physical activity, speech, and thinking OR agitation, increased restlessness, and irritability\nOngoing feelings of worthlessness and/or feelings of undue guilt\nTrouble concentrating or making decisions\nRepeating thoughts of death or suicide, wishing to die, or attempting suicide\nDrug Therapy For Depression\nSeveral drug classes and drugs can be used to treat depression:\nChoice of drug Drug Choice and Administration of Antidepressants Several drug classes and drugs can be used to treat depression: Selective serotonin reuptake inhibitors Serotonin modulators Serotonin-norepinephrine reuptake inhibitors… read more may be guided by past response to a specific antidepressant. Otherwise, SSRIs are often the initial drugs of choice. Although the different SSRIs are equally effective for typical cases, certain properties of the drugs make them more or less appropriate for certain patients .\nRecommended Reading: How To Keep From Being Depressed\nSymptoms Of Major Depressive Disorder\nThese are some of the symptoms of major depressive disorder you may experience:\n- Feeling sad or low\n- Having difficulty paying attention, remembering, or making decisions\n- Having difficulty sleeping, waking up too early, or oversleeping\n- Experiencing unplanned changes in eating habits and weight\n- Experiencing headaches, cramps, digestive issues, or other aches and pains that dont have a clear cause and dont get better with treatment\n- Talking about death, having thoughts of suicide, or attempting self-harm\nEveryone experiences depression differently. While some people may have a few symptoms, others may have many. The frequency, severity, and duration of the symptoms can also vary from person to person.\nTreatment For Depression As A Co\nTreatment for a co-occurring disorder where depression is present often involves different forms of psychotherapy, medications and prescription antidepressants, and other evidence-based techniques. At our inpatient addiction treatment facility in New Jersey, we offer different levels of rehab services designed to meet the needs of each patient we treat. Call us right now at to speak to one of our rehab admissions navigators to learn more about ways to pay for rehab, using insurance to pay for rehab, and treatments, and get all of the questions you have answered. Also, you can get started on your recovery by having your insurance verified through our secure\nIf you need help, do not wait. Contact us right now to get all the information you need to get started on your recovery today.\nAlso Check: Things To Help People With Depression\nTreatment Of Major Depressive Disorder\nAccording to the American Psychiatric Association , of all mental disorders, depression is among the most treatable, as 80% to 90% of people who have depression eventually find relief with treatment.6\nMajor depressive disorder treatments may involve one or both of the following: 6,14\n- Therapy: Cognitive behavioral therapy has been demonstrated to be effective for treating depression. CBT focuses on the present situation and how to solve current problems. CBT helps the client recognize and change maladaptive thinking and behaviors. Family and interpersonal therapy may also be used to help address conflicts and issues in the individuals closest relationships.\n- Medication: Brain chemistry may be a contributing factor to a persons depression, so medications that target abnormalities in a persons brain chemistry may help to bring about symptom relief. Antidepressant medications can help a person feel better within 12 weeks, though it may take up to 3 months for them to feel all of the benefits.\nFor patients with severe depression that does not respond to other treatments, brain stimulation therapies such as electroconvulsive therapy or repetitive transcranial magnetic stimulation may be beneficial.15\nOther complementary approaches that may help alleviate symptoms of depression include:16-19\n- Regular exercise.\n- Light therapy .\n- Adequate sleep.\n- Avoiding alcohol.\nTips For Living Well With Major Depressive Disorder\nLiving with major depression can feel lonely. People may be fearful or ashamed of being labeled with a serious mental illness, causing them to suffer in silence, rather than get help. In fact, most people with major depression never seek the right treatment. But those struggling with this illness are not alone. Its one of the most common and most treatable mental health disorders. With early, continuous treatment, people can gain control of their symptoms, feel better, and get back to enjoying their lives.\nAlso Check: Things To Do To Get Rid Of Depression\nWhere Does Depression Come From\nWe know that four key playersserotonin, dopamine, norepinephrine and glutamateare neurotransmitters in our brains implicated in depression and mood disorders.\nBut simply blaming these four neurotransmitters is not enough to give us a the entire story on the causes of depression.\nAccording to Dr. Shcherbakov, Genetics, physical health, cognitive style, and environment all contribute to depression.\nHe points out that certain individuals may be more susceptible to depressive episodes because of their genetic family history, for instance.\nOthers with poor physical healthsuch as those with cardiovascular diseasesare also significantly more susceptible to experiencing a depressive episode, says Dr. Shcherbakov.\nSome of the root causes of depression are also attributed to maladaptive cognitive styles: individuals who are more pessimistic or ruminate excessively are more likely to experience a depressive disorder in their lives.\nAlthough it seems that our emotions and motivations result directly from the events and circumstances we encounter, they are instead reactions to our self-talk,” says retired clinical psychologist Dr. John F. Tholen, Ph.D., author of Focused Positivity: The Path to Success and Peace of Mind,“the internal monologue that streams endlessly through our waking consciousness, interpreting whatever we experience.\nPoor social support systems, unhealthy environments and high levels of stress can also contribute in a significant way.\nHow Is Depression Treated\nDepression is among the most treatable of mental disorders. Between 80% and 90% percent of people with depression eventually respond well to treatment. Almost all patients gain some relief from their symptoms.\nBefore a diagnosis or treatment, a health professional should conduct a thorough diagnostic evaluation, including an interview and a physical examination. In some cases, a blood test might be done to make sure the depression is not due to a medical condition like a thyroid problem or a vitamin deficiency . The evaluation will identify specific symptoms and explore medical and family histories as well as cultural and environmental factors with the goal of arriving at a diagnosis and planning a course of action.\nYou May Like: Icd 10 Code For Anxiety And Depression']"	['<urn:uuid:4223f5a9-a2d8-42f7-93ad-542d4c10ff7a>', '<urn:uuid:6b31c0ac-3f5b-4a39-a0ad-d0a559f974e1>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	22	92	4074
12	reproduction challenges printing techniques past present	Reproduction challenges have persisted across different eras of printing. In the 17th century, engravers faced difficulties aligning multiple sheets for large prints, dealing with paper size limitations and uneven drying of dampened paper. They needed to carefully match tonal values and line thicknesses across sheets to maintain proper perspective. Modern printing still faces similar alignment challenges, as seen in gravure printing where the sawtooth effect occurs due to ink distribution from cups. Even contemporary digital reproduction faces comparable issues - for instance, when reproducing historical prints, photographers must deal with varying ink colors, digital stitching of multiple sheets, and maintaining consistency across joined sections.	['Spin control—it’s been around for centuries. Louis XIV, king of France from 1660 to 1715, was a master at it, using art—especially the work of his court painter, Charles Le Brun—to create and perpetuate a glorified image of his monarchy. Engravings of paintings by Le Brun take center stage in the exhibition Printing the Grand Manner: Charles Le Brun and Monumental Prints in the Age of Louis XIV, now on view at the Getty Research Institute.\nLe Brun knew that not everyone would get to see the massive paintings and tapestries that celebrated the king’s magnificence. So he commissioned the best engravers in France to make reproductions that could be printed and distributed widely—the better to spread Louis XIV’s (and his own) fame.\nBut it wasn’t easy to make a faithful engraving of a Le Brun painting. These “Grand Manner” history paintings (a style characterized by allegorical allusions and lofty subject matter) are monumental in size: Le Brun’s Triumphal Entry into Babylon (Triumph of Alexander) from 1675, which presents Louis XIV as the new Alexander the Great, measures nearly 15 feet tall by 23 feet wide. Other Grand Manner paintings extend as long as 40 feet.\nBy contrast, the largest printing presses of the time could accommodate sheets of paper only about 3 feet tall by 2 feet wide. But supersized paintings called for supersized prints, so Le Brun’s engravers often made multiple-sheet engravings that could be fitted together to form some of the largest reproductive prints of their time. As royal propaganda, these massive images were meant to be overwhelming—too large to take in at just one glance.\nThe large-scale prints required small-scale attention to detail and extraordinary technical skill: craftsmen used handheld tools to etch and engrave thousands of tiny lines that varied in width, shape, and intensity onto metal plates (usually copper). Before there were pixels and dots-per-inch, engravers like Gérard Audran and Gérard Edelinck created their images from tiny marks about the width of a pencil line. In the detail below, see how many individual strokes it took to create a 2-by-2-inch area (indicated by the red square) in a much larger print.\nAt the edges of the sheets, the strokes needed not only to evoke the subject but also align properly when the multi-sheet print was assembled. In addition, the engraver was acutely aware of the tonal values—light and shadow, density and diffusion—that are so crucial to maintaining perspective in the painting. A thicker line or dark shadow on one sheet could throw off the perspective of the entire print.\nIn their day, these reproductive prints were regarded not as slavish copies but rather as artistic translations of the original painting. The best engravers expressed their own artistic style, and many 18th-century critics even claimed that Audran’s engravings had improved upon Le Brun’s original painting!\nOur staff at the Getty faced their own challenges when they employed 21st-century technologies to reproduce the reproductions. Jobe Benjamin and John Kiffe, photographers in the GRI’s Digital Services department, made high-quality digital images of the engravings for the wall graphics and the exhibition catalogue.\nAs John and Jobe learned, even if the engraved plates had lined up perfectly, we still would have been able to see the seams. The prints were made with the paper slightly damp, and the individual sheets often dried unevenly, resulting in prints of slightly different sizes. Most collectors kept the sheets separate, or even bound in albums, so the Getty photographers had to digitally trim the edges of the images and stitch them into composites. Some of the stitching you can see, some you can’t.\nIn the example below, Jobe digitally manipulated the edges of five separate sheets in order to produce the composite image printed in the exhibition catalog. Here he shows us a version with the outer edges of the sheets untrimmed.\nIn addition to paper size and stitching problems, 17th-century inks posed their own challenges: printmakers used different inks for different prints. While the color of the ink in the prints hanging in the exhibition might look alike to the naked eye, the camera picked up differences—for example, a reddish tone in inks made from iron oxide—that had to be digitally adjusted.\nVisitors to Printing the Grand Manner are greeted by a wall-size vinyl reproduction of a 2-by-3-foot engraving of the Triumph of Alexander. Here in the GRI lobby, a Getty team has enlarged Audran’s engraving almost to the scale of Le Brun’s original painting. You can see a slight shift in color and a few misalignments between the two sheets that make up the engraving. What you won’t see, though, are the seams between the ten separate photographs that John made of sections of each sheet (and then digitally stitched together) in order to blow the image up to fill the wall.\nThe effect of the wall-sized image is overwhelming, which is exactly what Le Brun—and Louis XIV—would have wanted.', '#PrintingProcesses: Gravure Printing\nLet’s move on with our new series #PrintingProcesses and one of the oldest printing processes still in use today: gravure printing. As early as the Middle Ages, people produced copperplate engravings (a graphic gravure printing process) with an image depicted in the recesses. Since then, gravure printing has developed over a long time and is used today for banknotes, cosmetics packaging, magazines and many more.\nAre you a newcomer to the world of printing and don’t know where to start? Or maybe just a devoted print enthusiast who wants to freshen up on the basics? Then you’ve come to the right place! With our series #PrintingProcesses we’re introducing you to all the basic – yet very important – techniques and fundamental knowledge of the print technologies we know and utilize to this day. In this third edition it’s all about gravure printing, one of the oldest printing processes still in use today.\nFrom the First Printing Plates to Print Runs of More Than 300,000 Copies\nEven if the first printing plates were developed in the Middle Ages – the cornerstone for gravure printing was laid in 1719, when the Frankfurt-based Jakob Christoph Le Bron invented the three-colour printing with its copper mezzotint technique. He used the depressions of these printing plates for ink transfers and enabled the printing of up to 1000 copies per day.\nLater on, Jacob Perkins has patented the steel engraving that provides finer print results compared to the previously used copper. Fonts and images were thus reproduced more clearly and finely. Until today gravure printing has been improved properly but how does it work exactly?\nGravure printing is a demanding printing process in which the print image is engraved as a depression in a printing plate and dipped in ink. Since only the resulting indentation is to be printed, the excess ink is removed to a basin with a squeegee or wiper and only sticks to the deeper areas, the actual print elements. In the last step the ink is transferred to the printing medium with strong pressure and the help of the absorbent force of the paper.\nAfter the printing, the paper must be dried, as the substrate often needs to be damp to accept the ink. Together with the wiped ink the solvents from the printing ink can be recycled and reused.\nThe colour depth of the resulting print image incidentally depends on the depth of the depressions, the so-called cups which enable the representation of different color shades.\nThere are different techniques for gravure printing: rotary printing and sheet-fed printing are the most famous one. The latter is much less common and is only used for special jobs such as printing varnishes or metallic inks. Rotary printing (from the roll), on the other hand, is very widespread and is used especially for the production of high print runs of more than 300,000 copies. The following video shows the mechanism behind gravure printing:\nA Printing Process with High Quality Standards\nNow you know how gravure printing was invented and how it works but what makes it so popular and special?\nYou can recognise a gravure print by the serrated edge of the letters, the so-called sawtooth effect, which is created by the cups. Since the substrate draws ink not only from a small part but from the entire cup, there are no sharp edges. Besides, the substrate has a strikingly smooth and soft surface, whether it’s made of film or paper. Maybe you recognised it from how a magazine feels in your hands.\nGravure printing is characterised by high quality and durability. The dimensions of the cups determine the ink distribution and define exactly how much ink the cylinder takes up at which point. This precision affects the halftones, the colour nuances of your printed product. Gravure printing actually reproduces the colour gradations.\nEven though the gravure printing process is very old, it still maintains its usefulness today with high print qualities, an even ink application and rich colour depths. Therefore it will be quite a while before gravure is seen only as a historical printing process. Until then take a look at our other episodes of #PrintingProfessions.']	['<urn:uuid:9a22442d-9371-4ef5-a3d7-55e24a00e012>', '<urn:uuid:0b2c2150-07bb-45d0-b57c-635b1f1b3a9c>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	6	104	1517
13	memory problems adhd students help methods college home	There are several methods to help ADHD students with memory problems both at home and in college. At home, they can strengthen intentions with concrete plans, link tasks to existing habits, and use external memory aids like lists and reminders. In college, students can receive accommodations such as extra time on tests, priority registration, access to less distracting testing rooms, note-taking support, and use of computers during classes. Additionally, colleges offer varying levels of support through disability services, from basic accommodations to comprehensive learning centers with professional staff.	['Prospective memory — that is, remembering to perform a planned action at some future time — is crucial for managing life’s responsibilities. This article will help parents understand this kind of memory, determine how it shows up in children (and adults) with ADHD, and offer 3 skills that can improve it.\nProspective memory tasks are common in everyday life. Some are simple or even mundane, such as remembering to stop at the store on the way home from work or remembering to call your grandmother to wish her a happy birthday. Others are more critical like remembering to take a life-sustaining medication. Children with ADHD often need to be explicitly taught skills to improve their prospective memory.\nProspective memory is essentially remembering to remember, and there are two common types.\n- Forming an intention: Deciding to do a specific task in the future.\n- Retaining the intention: Not getting side-tracked by other tasks.\n- Returning to the original intention.\n- Executing the behavior: Following through with the desired task.\nFailures of prospective memory usually occur at step two, when we get distracted and forget a task we originally intended to do. However, research has found that people with ADHD also tend to struggle at stage one, neglecting to formulate an intention to reach a goal in the first place.\nIn the science-fiction movie Paycheck, Ben Affleck’s character is an engineer whose memory is threatened with erasure, so he devises a plan of stuffing 19 seemingly ordinary items into an envelope to remind him of what to do in the future. His plan is a Hollywood example of the process of prospective memory. Here are some real life skills that you can try:\n- Strengthen the intention with a concrete plan. Identifying when and where a specific intention will be carried out can help decrease memory failures. A concrete plan can improve prospective memory by as much as two to four times in tasks such as exercising and homework completion. To encourage your child to form concrete plans, have them “look into the future” (perhaps even while holding a “crystal ball”—a marble) and describe what they look like while doing the intended task. Make sure they include details about what time it is, where they are, and what they will be doing immediately prior to the intended task.\n- Link the target task to a habit you already have in place. For example, if you need to take a medication every morning, link it to something else you do every morning such as brushing your teeth.\n- Use external memory aids to help trigger the memory. There are many high-tech and low-tech tools for triggering memory, including:\n- Write and refer to lists. Utilizing a consistent system for tracking “to-do” items helps increase the likelihood of success.\n- Create checklists for tasks that will be repeated (e.g. packing lists you can use repeatedly).\n- Create reminder cues and put them in difficult-to-miss spots. (e.g. if you want to remember to move the laundry, put the laundry basket in the hall where seeing it will remind you to complete the task.)\n- Don’t delay. If you want to remember to take your gym clothes with you to work tomorrow, put them in the car while forming the intention, or put them near your car keys.\n- Set calendar alerts on your cell phone that will remind you of an up-coming event. These are especially helpful for time-based prospective memory tasks.\n- Some GPS systems on Smart Phones can be set to provide an alert when you are in a certain area. For example, you can set it to remind you to return the library books when you are within a certain distance (e.g., ½ mile) of the library.\nModeling these behaviors at home can be beneficial to your child. You can talk about how you will need to remember to take the food from the oven at a certain time, so you are setting an alarm. You can talk about your concrete intention/plan for when and where you plan to fold the laundry that you just brought out of the dryer. When you get in the car, show your child that you put an important package on the console to remind you to take it to the post office.\nIn addition to the help that you provide at home, you may need to seek the help of a qualified professional. When you understand that your child’s difficulties with prospective memory are likely a by-product of ADHD, you can move forward finding strategies and programs that will help your child set and follow goals. And you will be amazed by how well s/he can remember to remember!\nADHD Parent Video\nAt LAST — Operating Instructions for raising ADHD kids! This video gives you the tools you need to tackle ANY challenging situation, one step at a time.', 'The college search process can be a daunting task for any student, but even more so for a student with a learning challenge. Parents are often worried about how colleges will view their students’ grades and test scores in light of a learning disability, anxiety, depression or ADHD. As Independent Educational Consultants, we suggest that you begin by examining your student’s academic profile. Do their grades, test scores, and choice of coursework reflect their true academic abilities or has the learning disability or disabilities prevented them from achieving academic success? Were the accommodations put in place early enough in high school to impact grades and has the student benefitted by the accommodations? Has the student shown an upward progression in grades? These are just some of the many questions to consider when assisting your child. Parents also worry that their child does not understand their lack of executive functioning skills and how it affects their classroom performance, causing them to be highly distracted. In many cases these students have had a lot of assistance during high school with subject tutoring, learning specialists, and therapists to help manage anxiety, mood and motivation. It makes sense that they would need some outside help during their first year of college.\nAlthough the first step is to examine the student’s transcript and test scores, this is not the only information that a college will review. If the student is able to articulate their understanding of their strengths and challenges, how it has impacted them, what they have learned about themselves and their accommodations, and how they plan to succeed in college, this will speak volumes to colleges. For students who have had numerous challenges in high school and have received accommodations that have helped them to be successful, the transition to high school can be much less stressful than their peers.\nThe next step is to help your student find supportive college environments where your student can grow and succeed. The level of support will be an important factor to consider when creating your initial list of colleges. When approaching the college search, remember that not all colleges will be equipped to support the needs of your student. If your child has a learning disability, you will need to be a savvy consumer who can recognize the differences as the level of support can vary widely from one school to another. You need to know how to distinguish the minimum supports offered from a more comprehensive model. The best way to begin is by searching on a college website for “disability support services.”\nLearning Support Programs in College\nBy law, all colleges must comply by offering minimal services for student with disabilities. At this level there are no professional learning specialists on staff. The responsibility for reviewing documentation and awarding basic accommodations usually falls on the Dean of Students. There are no specific services for students with learning disabilities; students utilize the writing center, math center, and study skills assistance available to all students at the school.\nStudents with disabilities have their own dedicated Learning Center that serves their needs in addition to the writing center, math center and study skills center available to all students. This means that the disability support staff are Masters Level trained professionals who are well versed in the challenges of your students. These centers offer a much wider range of accommodations, assistive technology, and support to students, and may have professional tutors and coaches on staff.\nStudents with disabilities not only have their own dedicated Learning Center, but an additional fee-based program is offered to those who need a structured learning program. Typically, students meet weekly or biweekly with a professional who can help them with study skills, organizational skills, and time management and will address any difficulties that the student is having and advocate on behalf of the students. There is a separate application and review process for acceptance into these programs in addition to the admissions application.\nWhen visiting colleges with your high school junior or senior, we suggest you include an appointment with the disabilities office on campus. Note that you and your student’s impressions as your student may be spending considerable time in the office and will need to feel comfortable. In addition, if your student will be applying to college in the next two years, we suggest learning more about the differences between the K-12 system and the college system for students with disabilities. There are significant differences between high school and college when it comes to applicable laws, documentation, self-advocacy and student responsibilities.\nSome of the most common challenges that can cause students to struggle significantly: Students are unable to articulate their challenges to the disability coordinator and with professors and unprepared to advocate for themselves in and out of the classroom. Many students are intimidated by talking to a professor and do not seek help or make connections with a staff member who can help. Students are unprepared for the academic challenges and do not have the academic foundation to succeed in a selective major or selective university. Students often hold the mistaken belief that they can manage in college without accommodations and do not utilize the accommodations granted them in college until it’s too late in the semester to benefit from them. Students are unprepared to manage their time and fall behind in their study and coursework and do not seek out the tutoring or coaching services until it’s too late in the semester.\nTypes of Accommodations Offered in College\nYou need to determine what kinds of accommodations are available in college for your student’s particular disability. Here are a few of the most common accommodations that you will see in colleges. (These examples are from Colorado State Office of Disabilities)\nFor Students with ADHD\nExtra time on tests Priority registration Access to a testing room with less distraction during mid-terms and exam finals Note taking support Use of computer during classes.\nFor Students with Anxiety\nFlexible matriculation requirements including: Extended time to complete degree program Flexible course requirements Alternative housing options: living in a single room as opposed to having a roommate Course load flexibility: taking a part-time load the first semester Alternative format: textbooks and print materials can be converted to alternative formats for students Alternative testing arrangements: extra time; a quiet, separate room; provision of a reader/scribe; and use of a computer, including adaptive software and hardware Note-taking support Priority registration\nFor Students with Dyslexia or Other Language Based/Auditory Processing Disorders\nAlternative format: textbooks and print materials can be converted to alternative formats for students. Film and video material with captions included. Alternative testing arrangements: extra time, less distracting environment, provision of a reader/scribe, and use of a computer, including adaptive software and hardware Preferential classroom seating: sitting where the student can see the board and professor’s face Use of computer during classes or access to taped lectures Note-taking support.\nAfter your student has accepted the admissions offer, you may begin contacting the disabilities office for instructions on requesting accommodations. Each university will have a different process, but the most common practice is to submit the student’s Psychoeducational report that includes their diagnosis, online or in person, at the first meeting with the disabilities office. The testing report must come from a licensed educational psychologist. Some colleges will accept an IEP but others may not and this will vary from school to school. The disability office will have an application link posted on their webpage. Students are encouraged to complete this paperwork as it is best for them to take the lead role in answering the questions about their challenges.\nIt is important for the student to understand the kinds of accommodations they may need and to document what has made the most impact at the high school level. Some of these questions will include a statement about the student’s diagnosis and how this impacts them in the classroom. Other questions directly ask which accommodations the student would like to request. For students who are coming from smaller private high schools, where the accommodations were built into the program, it may help to have a meeting with the learning center or with some of the teachers who offered them the flexibility to be successful. After paperwork is submitted, the student will schedule an appointment to meet with the director of the office of disabilities. We suggest a practice interview with students before the meeting to ensure that the student feels comfortable discussing their disability and its impact and advocating for the accommodations that they feel they need to be successful at the college level.\nTimeline for success\n- First Semester of Junior Year in High School: College search for students with LD and ADHD- Set up an appointment with a college advisor who has an understanding of your learning issues and can help you set up a timeline that includes identifying colleges where you can be successful.\n- Second Semester of Junior Year: If your student has significant learning challenges, this is a good time to update your students testing and make sure that your accommodations are in place. Arrange a meeting with your educational consulting team, which includes your tutors, educational therapist, college counselor and psychologist. Make it clear that your goal is to transition to college in one year and request help setting milestones to reach this goal.\n- Spring or Summer of Junior Year: When visiting colleges, include a meeting with those in the disabilities office. Can you distinguish the differences between the basic and moderate services, for example? Note how comfortable your student feels while visiting these offices. Do they see themselves comfortably walking into the office and utilizing their services while in college?\n- Senior Year: Apply to colleges on your final list. Will your student disclose their disability? If they choose to disclose, seek assistance from your consultant when crafting this written statement.\n- Spring of Senior Year: Once accepted, visit the colleges on your final list once more during admitted students’ day or on another day and speak with students on campus before you make your final decision. Visit the disabilities office again to assess your student’s level of comfort.\n- After You Deposit and High School Graduation: Apply for accommodations and make sure your testing is up to date. Prepare for the interview in the office of disabilities by meeting with your educational support team. In some cases, we suggest that students use the summer to do more educational therapy in order to sharpen their academic skill set. Sometimes a student may practice their skills by taking a summer class.\n- Prepare for the fall: For students with ADHD you may consider working with an academic coach during the first semester in college. Students with anxiety should continue to work with their therapist over the summer and make sure to find a counselor on campus or look for local options near the campus.\nThe college search process and the transition to college can be difficult for even the most prepared student. For students with a variety of learning disabilities there are many important factors to consider for assistance with the challenges ahead. With the guidance of an Independent Educational Consultant and your student’s educational support team, you and your child can choose from colleges that will be the best fit to support your individual student. We have addressed the types of learning support programs in college, the major differences your student will need to understand to navigate disability supports in college versus high school, the kinds of accommodations offered at colleges, key factors in ensuring your child’s success, and a timeline for success. Taking the time and effort to consider all of these factors ahead of time when putting together a college plan is the key to your child’s successful future. If you have difficulty deciphering the options, you may benefit from professional advisement from an independent college consultant who is familiar with the process and these programs.\nAbout the Author:\nTeresa Theofanos Collins, principal and founder of Alpha College Admissions Consulting, LLC. in the Richmond, Virginia area, believes that every student is unique and the process for getting into the right college should be too. The college planning and selection process is highly individualized for each student based upon their academic, social, and financial goals. We empower students to create an application that reflects their true selves to maximize admission to best-fit colleges. www.alphacac.com.\nGina Gerrato Greenhaus founded Greenhaus College Consulting to assist students and their families through the maze of college admissions and create a winning strategy to successfully transition from high school to college. Her success comes from developing trust with students, partnering with their families, as well as mentoring and coaching. For San Diego families looking for more information on how to best approach the college search process, contact www.greenhauscollegeconsulting.com.']	['<urn:uuid:81857736-beef-4cb0-beea-66de7a612ab8>', '<urn:uuid:f3932e24-2724-46c1-adbf-41338559510c>']	factoid	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-12T19:10:11.825225	8	88	2959
14	What's the difference between how educational institutions and businesses are allowed to use copyrighted materials in their work?	Educational institutions have broader rights under 'fair use' and the TEACH Act. Fair use allows reproduction of copyrighted material for teaching, scholarship, or research, especially when used for educational rather than commercial purposes. The TEACH Act specifically allows accredited, nonprofit educational institutions to use copyright-protected materials in distance education without permission or royalties, though with some restrictions like not using materials primarily marketed for instructional activities. In contrast, businesses have much stricter limitations - they cannot use copyrighted works without permission, and doing so can result in civil penalties up to $150,000 per infringement, actual damages, or criminal charges if done for financial gain.	"['Copyright Infringement and Fair Use Guidelines\nWhat is Copyright Infringement?\nThe law of copyright indicates that copyright protection applies to works of authorship including but not limited to literary works, musical works (including any accompanying works), dramatic works (including any accompanying music), motion pictures and other audiovisual works and sound recordings. The owner of copyright has exclusive rights to, among other things, reproduce the copyrighted work and distribute copies of the copyrighted work to the public by sale or other transfer or ownership, or by rental, lease or lending.\nExamples of copyright infringement:\n- Copying the contents of someone else’s webpage or use of video clips or sound recordings without permission would in many cases be infringement.\n- Unauthorized duplication, distribution or use of someone else’s intellectual property, including computer software is copyright infringement and is illegal and is subject to criminal and civil penalties.\n- Unauthorized duplication and distribution of sound recordings.(e.g. MP3 format)\nWhat is Fair Use?\nIt is not an infringement of copyright if works used fall under the ""fair use"" exception of copyright law. Fair use extends to the reproduction of copyrighted material for purposes such as criticism, comment, news reporting, teaching, scholarship or research. Factors used in determining if copyrighted material falls under the fair use exception includes, but are not limited to, whether the material is used for educational rather than commercial gain, the nature of the copyrighted work, how much of the entire work is used and the potential value of the copyrighted work.\nExamples of fair use:\n- Quoting passages from a book in a report for a class assignment.\n- Linking to someone else’s webpage in a report for a class assignment.\nWhat are the penalties for copyright infringement?\nAnyone who violates the exclusive rights of a copyright owner is an infringer of the copyright. Legal action available to the owner of the copyright includes obtaining an injunction preventing future infringement activity as well as monetary compensation that may exceed $150,000.00.\nWhy does the University need to respond to and investigate claims of copyright infringement? How will the university respond to a claim of copyright infringement on the university network?\nRider University is considered a service provider under copyright law. As a service provider, it must designate an agent to receive notifications of claimed infringements and make the name, address, phone number and electronic mail address of the agent available through its service, including on its website, in a location accessible to the public and by providing the same to the Copyright Office. If a service provider acquires knowledge of infringement, it must act ""expeditiously"" to remove, or disable access to the material, taking reasonable steps to notify the alleged infringing party (subscriber) that access to the material has been removed or disabled.\nThe subscriber has the right to counter notify the service provider that the material was removed or disabled as a result of a mistake or misidentification of the alleged infringing material. This counter notification must be in writing to the designated agent of the service provider and include the subscriber’s physical or electronic signature, identification of the material that has been removed or made inaccessible, and the location at which the material appeared before it was removed. Further, the counter notification must contain the subscriber’s name, address and telephone number and a statement that the subscriber consents to the jurisdiction of the Federal District Court for the judicial district in which the address is location. The subscriber must also indicate a willingness to be served papers in a legal proceeding from the complaining party.\nUpon receipt of the counter notification from the subscriber the service provider must promptly provide the complaining party with the counter notification. It must further inform the complaining party that it will replace the removed material or permit access to the same within 10 business days and then replace or make the material accessible not less than 10 nor more that 14 business days after receipt of the counter notification. However, if the complaining party notifies the agent that it is filing a court action seeking to restrain the subscriber from engaging in the infringing activity, the removal of or inaccessibility to the material must continue. The service provider may also be required, in the presence of a subpoena issued by the United States district court within its jurisdiction, to provide the identification of the alleged infringing party.\nThe service provider is obligated to inform its subscribers of a policy regarding the termination of access to its system or network for repeat offenders. This policy must not interfere with a copyright owners means of identifying and protecting copyrighted works on the service provider’s system and, at the same time, should not impose substantial costs or burdens upon either the service provider or its network.\nIn 2002, Congress passed the TEACH Act which, according to the American Library Association, “redefines the terms and conditions on which accredited, nonprofit educational institutions throughout the U.S. may use copyright protected materials in distance education-including on websites and by other digital means--without permission from the copyright owner and without payment of royalties.” (http://www.ala.org/Template.cfm?Section=distanceed)\nWhile the TEACH Act explicitly allows a wide variety of works to be uploaded to Learning Management Systems ""in an amount comparable to that which is typically displayed in the course of a live classroom session,” there are two categories of works that are explicitly excluded:\nThe following materials may not be used:\n- Works that are marketed ""primarily for performance or display as part of mediated instructional activities transmitted via digital networks""; and\n- Performances or displays given by means of copies ""not lawfully made and acquired"" under the U.S. Copyright Act, if the educational institution ""knew or had reason to believe"" that they were not lawfully made and acquired. (http://www.ala.org/Template.cfm?Section=distanceed)\nFor example, no media of any sort (print, audio, or video) should be uploaded from “pirated” copies of the material. Nor should anyone be scanning and uploading substantial portions of textbooks or other instructional materials that are intended for each student to purchase.\nWho should you contact to report a claim of copyright infringement?\nContact the Copyright Infringement Complaint Coordinator,\nE-mail: [email protected].\nOTHER USEFUL CONTACT INFORMATION\nOffice of Information Technologies: Fine Arts Room 137, 896-5196. This office enforces electronic data, network and computing policies.\nOffice of Judicial Affairs: Student Center, Student Affairs Suite, x5792. This office handles complaints of alleged violations of the Campus Code of Conduct as described in The Source.\nOffice of Public Safety: Non-emergency telephone number is (609) 896-5029, emergency telephone number is (609) 896-5321. This office accepts reports of possible criminal or illegal activities and is open 24 hours. All serious or potentially dangerous incidents should be reported to Security immediately.\nSoftware Publishers Association: This is an international organization of software companies and developers that pursues software piracy. They accept reports of ftp and bulletin board sites containing pirated software. You can also report if software you developed has been pirated.\nRecording Industry Association of America: This is a private, not-for-profit corporation whose member companies produce, manufacture, and distribute approximately 90% of all legitimately recorded music in the US. You can get more information on their web site or you can report sound recording piracy by calling 1-800-BAD-BEAT or sending e-mail to [email protected].\nSome tips on preparing the complaint or report:\n- Include a brief, concise description of the problem, and be sure to identify yourself.\n- Include copies of any communication that is relevant, including all header information.\n- Send only one message.\n- Do not assume that the incident was intentional or malicious. E-mail is easily misdirected due to typos.\n- Do not expect an immediate response. Some sites get lots of e-mail.', 'Punishment for Violating Copyright Laws\nCopyright law can be arcane and have a lot of gray areas, but the easiest rule of thumb to follow is to only sell or use images, music, photographs or any other creative work in your business that you created yourself. Nearly all images you locate on the Internet are under copyright, and the copyright to all writing and music you discover online and offline is almost certainly owned by someone else. Distributing, copying or selling another person’s copyrighted work is illegal, and can result in civil and criminal punishments.\nWhat Constitutes Copyright Infringement?\nCopyright infringement occurs when someone uses a creative work protected by copyright without the copyright holder’s permission. Infringement occurs even if the infringing party didn’t use the work to make money. Copyright infringement may be as benign as using images culled from the Internet in print advertising or as aggressive as illegally selling pirated copies of movies through a website. Because copyright terms are very lengthy – usually more than the life of the creator in the United States – it’s safest to assume any creative work you discover is still protected, and not in the public domain.\nAnyone who violates a copyright may be liable in a civil case to reimburse the copyright holder for actual damages incurred because of the copyright infringement. Actual damages focus solely on revenue lost because the infringement occurred. For example, a photographer who licenses images for use in magazines for $300 per one-time use would be able to claim actual damages of $900 if he discovered three of his photographs used without his permission in an online store. Actual damages don’t take into account profits made by the infringing party due to the copyright violation.\nStatutory damages are punitive measures that copyright holders may seek as additional remuneration against the infringing party. To seek statutory damages, the copyright holder must register the work with the U.S. Copyright Office. Statutory damages may range from $750 to $30,000 per infringing use for accidental or incidental infringement. If the copyright holder can prove to a civil jury that the infringement was willful – such as selling pirated copies of movies – the statutory damage limit raises to $150,000 per infringing use.\nPeople who commit copyright infringement for personal financial gain may also face criminal charges. To qualify for criminal copyright infringement charges, the infringing party must make copies of copyrighted material worth more than $1,000 in any 180-day period. Depending on the manner of the copyright infraction, the infringing party may receive a sentence of one to 10 years of jail time.\nA copyright holder may also receive an injunction from the court to force an infringing party to immediately stop distributing or using the copyrighted material. Injunctions may be used to stop illegal distribution, or as a first step in seeking civil and legal action against a violating party.\n- U.S. Copyright Office: Copyright Infringement and Remedies\n- Purdue University: Copyright Infringement Penalties\n- Cornell University Law School: 17 USC § 504 - Remedies for Infringement: Damages and Profits\n- Cornell University Law School: Criminal Infringement of a Copyright\n- Madison Area Technical College: What Is Copyright Infringement? What Is the Penalty?\n- Jupiterimages/Comstock/Getty Images']"	['<urn:uuid:f1cc8b98-fe8c-4e90-961f-05ce40a5a781>', '<urn:uuid:d1fb1f8c-6b65-400f-b9d4-ee4a7c83edea>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	18	104	1826
15	alpine gear comparison static vs shock absorbing tethers features pros cons	Static and shock-absorbing tethers have different characteristics for climbing safety. Static ropes have less elasticity and are primarily used for ascending ropes or pulling expedition materials up rock faces, where rope stretch is undesirable. In contrast, shock-absorbing tethers made from 8mm UIAA certified climbing rope offer better protection due to their toughness and shock absorbing capacity. These can handle falls without generating rib-breaking impact forces or overloading jack lines or anchor points.	"['Before hoisting the spinnaker I inspected this shackle and double checked that it was latched. I remember staring at it, thinking I would never trust my life to a quick release mechanism so easily released as this.\nThe spinnaker flew perfectly for hours, and then while putting it away I realized the shackle had blown apart, the result of being dragged over the lifeline. No excessive force, just the the weight of the sail and sheet as I pulled it on board. Somehow the circle pin came off, allowing the pin to shoot out the other side. This is a common failure mode, one I have experienced before. No excessive force, just the the weight of the sail and sheet as I pulled it on board.\nToday\'s experience simply reinforced my tether decisions:\n1. Harness carabiner should be locking but with simple release. I like the Kong Tango because it is large enough to clip railings and is easy to unclip even with wet, cold-numbed hands. I really like the feel in my hands. I also like that I can clip in with one hand, something I cannot do with a snap shackle.\n2. Jackline carabiner\'s must be locking type. I like aluminum rockclimbing carabiner\'s"", which seem to remain jamb free so long as I treat them with waterproof grease annually. I like simple climbing carabiners, since I generally leave the tether on the jackline and unclip at the harness end when entering or leaving the cockpit.\nKong Tango on the jackline end of the tether. They like a snap shackle for the harness end, I don\'t.\n8 mm UIAA certified 1/2 or twin nylon climbing rope. It has the toughness and shock absorbing capacity to handle any conceivable fall or misstep without generating rib-breaking impact forces or overloading the jack lines or anchor points.\n4. Custom tether lengths. I use two leg tethers. The short leg is 2 feet and the long leg is 8 feet. These lengths better suit my specific boat. The answer for yours could be different. However, I\'m willing to bet that 3 feet is too long for the short leg on most boats.\nPhase I: These were tied from 1-inch tubular nylon climbing webbing. I also inserted a Yates Screamer for testing, though it was later removed. These are the simplest to make and are quite safe, based on generations of rock climber experience.\nPhase II: After many experiments, these are the tethers I use.\n- 8mm climbing rope for shock absorption (easier on the ribs).\n- Kong Tango at the harness end.\n- Sewn ends (can be knotted--do not attempt sewing unless you have access to pull-testing equipment).\n- 2\'/8\' lengths (fit my boat). The short leg just kept getting shorter.\n- Small ""parking clip"" so that I do not have 2 biners clipped to my harness (inhibits unclipping in an emergency).\nNote that in both cases the middle carabiner is cow hitched and can be moved. This is important to getting the best fit. The cow hitch is secured with a seizing to prevent sliding (hidden).', 'Safety first is really much more first when you are 10 meters or higher up in the air falling is not an option. Injuries, luckily, don’t happen often in climbing, but when they do they are relatively much more serious and even fatal than many other sports.\nWith the advent of belay devices, the risk of a fall is very much reduced. With a such a device, your climbing partner can secure you as both your climbing harnesses are connected through your climbing rope. Through friction on the rope, your partner can block he rope when you fall. And at the end of the climb belay you down again.\nPassive belay devices\nA passive belay devices requires action from the belayer to lock the rope in the event of a fall. One of the most traditonal of such devices is the “Eight”.\nAn eight is the simplest and for a long time was by far the most commonly used tool for securing someone. The rope is pulled through the eight-form so that as it moves along the steel, friction is induced. So much so that with minimal effort the rope can be locked. In addition to belaying it can also be used for rapelling.\nAnother good example of passive device is the ATC belay device. One of the many tubular devices that also relay on action from the belayer to lock the rope.\nActive belaying devices\nThe most well known of these devices is the GriGri. This kind of device has a built-in mechanism that locks off the rope automatically when the rope slips at high speed through the mechanism. A very safe device but one that can lead to less alert belayers. So whichever device you use, the life of your climbing buddy is, quite literally, in your hands!\nCarabiners are metal loops that connect everything together. Most often made from aluminium alloy and with spring-loaded gates (openings). There are several types of carabiners, such as: ordinary carabiner, HMS carabiner, twistlock (quick release), etc. The choice of karabiner depends on what you are going to use.\nFor the belayer, a curved karabiner is the most convenient due the larger top side. HMS carabiners can carry 3-sided loads instead of 2-sided and are therefore suitable for anchors with weight pulling in different directions.\nA good carabiner should:\n- have an ideal ratio between weight and breaking force (minimum standard is 2000 kg)\n- not slip out of your hand to easily.\n- open and close smoothly\n- not have any sharp edges that can damage yourself or your rope\nIf you intend to use the harness often in the high mountain, choose for lined and adjustable leg loops Lined harnesses have less stress on your legs and are more comfortable when you wear them over longer periods of time.\nAdjustability is important to fit the leg loops to thicker or thinner clothes.\nImage: Arc’teryx B 360a Men’s\nIf you focus mainlyon sports climbing on rocks, choose the comfort of a wide, possibly padded hip belt with or without polished leg loops. For indoor climbing there are special ultra-light belts.\nThere are two types of harnesses:\nReguar hip harnesses\nThe most widely used, they are preferred choice due to their comfort and greater freedom of movement.\nFull body harnesses\nThis also has shoulder straps, and are safer as the point where the rope is attached is higher than the climber’s center of gravity, reducing the risk of a back injury in an uncontrolled fall.\nWithout getting to technical as far as climbing technique is concerned, we can state that your legs and feet are the foundation of your climbing. So your shoes are essential to your effort.\nOn the smaller footholds you will encounter, you will generally climbin with the edge of your shoes, a lot of which will be on the toe edge (this is called “edging”). That means the soles of climbing shoes need to add to the stability of your foot. Climbing shoes will generally have quite a stiff vulcanized rubber sole that adds this much needed stiffness. Besides that, the rubber also delivers heaps of friction on the wall. In the climbing technique know a “smearing” where you climb on friction rather than on the edges of your shoes, mostly because there are no footholds.\nGiven that climbing shoes need to add stability and stiffness to your foot, you will generally need to buy shoes that fit quite snug, if not really tight. It should not be painful.\nThere are three categories of shoes:\nThese shoes have a relatively relaxed and straight footbed. Your toes are straight in the shoe. The fit is more comfortable and relaxed, making them great for beginning climbers or for more experienced climbers doing all day climbs. The sole is thicker and stiffer.\nThe shape of these shoes is slightly downturned with a light camber in them. There is more pressure to your toes, enabling you to climb more technical and challenging routes with smaller footholds. Stickier rubber and slightly thinner soles for more grip and feel.\nShoes with a very strong camber to them and a lot of tension on your foot directed to the toes. Not very comfortable, but very good for those really difficult routes. Again stickier and thinner soles.\nTake your time to choose what kind of shoe would fit your foot, but also the kind of climbing you do.\nA good rope is essential for climbing. These kind of ropes are not only very strong, but also slightly elastic to absorb a part of the force of a fall. This especially true for so called dynamic ropes that are used for top roping or lead climbing.\nImage: Mammut Tusk Dry klimtouw\nThe ideal climbing rope combines the following properties: it will be strong, light, abrasion resistant, flexible, easy to use, low strain when body weight, low retention.\nThere are many variations in rope diameter and length, as well as safety ratings. This will indicate what kind of climbing the rope is good for.\nStatic ropes are ropes that have less elasticity and are used for ascending on a rope, or pulling expedition materials up a rock face, or any situation in which you do not want the rope to stretch.']"	['<urn:uuid:1523608e-57ff-4cbe-89bb-8b007b0ebe63>', '<urn:uuid:fa2f5373-8c8b-4588-8719-c83300765f8c>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T19:10:11.825225	11	72	1557
16	What exactly is the ATT&CK framework and why is it useful?	The ATT&CK framework is a globally accessible knowledge base of adversary tactics and techniques based on real-world observations. It was initially proposed in 2013 and publicly released in 2015. The framework bridges the gap between multiple offensive security data points, including Tactics, Techniques, Tools, and identified malicious Advanced Persistent Threat actors. It's particularly useful because it helps organizations develop defense strategies, understand their attack surface, and verify attack methods. The framework includes 14 tactics and over 500 techniques, providing an evolving list of techniques used by adversaries that security teams can use to defend against attacks. Organizations can use it to form conclusions based on verified data and improve their prioritization and remediation strategies based on real-world cybercriminal activity.	"[""Joshua Platz is a senior consultant in Optiv’s advisory services practice on the attack and penetration team. Joshua’s role is to provide internal and external network penetration testing to determine vulnerabilities and weaknesses in client networks and environments. He specializes in PCI DSS, wireless, social engineering, password cracking, as well as post-exploitation of customer networks.\nIntro to Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK™ Series)\nAttack and Penetration consultants strive to stay current and knowledgeable in all of the current trends, both from an offensive security perspective, as well as a defensive mitigation and remediation perspective. You may have missed our previous blog series Top 20 CIS Critical Security Controls (CSC) Through the Eyes of a Hacker, where we reviewed the updated Center for Internet Security (CIS) Critical Security Controls (CSC) from the perspective of offensive security professionals with the intention of educating organizations of controls that exist. This series is also focused on the risk associated with attacks which leverage vulnerabilities that could have been mitigated through the implementation of a control.\nWhat is ATT&CK\nIn this new series, we will be reviewing MITRE’s National Cybersecurity Federally Funded Research and Development Centers (FFRDC’s) Adversarial Tactic, Techniques, and Common Knowledge (ATT&CK) repository of collected cyber security data. ATT&CK bridges the gap between multiple offensive security data points, including Tactics, Techniques, Tools, and identified malicious Advanced Persistent Threat actors. The creation of most of this framework comes from an interesting project executed by Blake Storm, of MITRE, called project FMX (Fort Meade eXperiment). In this project, a production network was attacked by Blake and other security professionals which impersonated adversarial groups' tactics and techniques. By leveraging data points collected on the network, Blake was able to construct a large part of the ATT&CK framework that could be leveraged by offensive as well as defensive security professionals, to map potential offensive tactics and techniques.\nThe ATT&CK Navigator\nBy far, the best way to disseminate the entirety of the ATT&CK project is through the ATT&CK Navigator. The ATT&CK Navigator allows users to filter, drill down, build potential attack chains, and view cross-tactic techniques. This is particularly helpful for defensive security professionals in creating potential incident response attack scenarios. Teams could play through scenarios of how an attack could unfold by chaining together several different techniques, creating an attack chain. From an offensive security perspective, it allows teams to “look ahead” or “brainstorm” ways to demonstrate impact and risk within their penetration tests.\nPurpose of the Series\nThe purpose of this series is to rely on Optiv Attack and Pen's experience performing adversarial threat assessments, and analyze the techniques of each tactic, in attempt to bring attention to the most commonly leveraged methods we employ as offensive security professionals. There are currently 219 techniques across ATT&CK’s 11 tactics; this is an enormous amount of information to consume, and even more so to implement mitigations around. We hope that security teams will benefit from the information in this series regarding the common attack techniques, however we encourage those teams to not stop there. Teams should continue to enhance their security by drilling into the ATT&CK matrix to develop as many attack mitigations as possible.\nSo now that we have defined what ATT&CK is, our next post we will cover the Initial Access Tactic and will examine the following tactics that attackers can use to gain a foothold into your environment:\n- T1190 – Exploit Public-Facing Application\n- T1192/T1193 – Spear Phishing Link/Attachments\n- T1199 – Trusted Relationships\n- T1078 – Valid Accounts"", 'Threat actors are constantly evolving their tactics and techniques in the attack lifecycle and infiltrate company infrastructure. While most organizations are already performing vulnerability management based on CVEs by MITRE, few have considered the powerful correlations between threat intelligence, CVEs and the ATT&CK® framework. In this blog we highlight the benefits of bringing them together to drive focused remediation and improve cyber defense.\nWhat is the MITRE Att&CK framework?\nMITRE ATT&CK® is a globally accessible knowledge base of 14 adversary tactics and over 500 techniques based on real-world observations. The first model was proposed in 2013 and publicly released in 2015 which has been gathering momentum over the last couple of years. The MITRE ATT&CK framework provides an industry leading standard to help organizations develop, organize and use a risk-based strategy to inform defense strategies – that can be communicated in a standardized way across organizations and vendors to drive effective risk assessment based on observed incidents.\nIn the past, security teams have struggled to understand their entire attack surface and verifying attack methods, leading to them falling victim to cyber-attack from a false sense of security and overconfidence in their ability to defend against it. The ATT&CK project came along with the goal of eliminating this problem – the knowledge base was born to create a clear structure by providing a categorized list of all known attack methods and marrying it with threat intelligence on groups that use them.\nIn addition, the ATT&CK framework identifies critical information on the software hackers will look at to implement an attack and provides direction on the most effective defense measures to reduce this risk. The objective of the ATT&CK framework is to provide an evolving list of techniques used by adversaries so that security teams can trust and use with confidence to defend against them.\nUsing MITRE ATT&CK framework with threat intelligence\nOrganizations can use the ATT&CK framework to form conclusions based on verified data and the kill chain structure to improve prioritization and remediation strategies based on observations from real-world cybercriminal activity. Gartner’s analysis of the framework says the criteria for defining Tactics, Techniques, and Procedures (TTPs) in MITRE’s data provides an in-depth knowledge base of attack intelligence – making it easier and more straightforward to apply these into your own investigations.\nOrganizations can start by looking at a specific threat groups with an interest in stealing your data or assets based on who they’ve previously targeted. Once the threat groups of interest are identified from MITRE, you can leverage the insights to look at the specific TTPs being used. By understanding common TTPs from groups who you think will attack your organization, you can begin to form a prioritized list of detection and prevention controls that your security teams need to put in place and to reduce risk.\nFor more mature organizations, you can leverage threat intelligence to enrich what is already known about these groups by linking attack patterns and behaviors from specific threat actor campaigns, tools and feeds this information in an automated and usable way to help focus defensive activities. Identifying if specific tools and/ or software are being used, and whether these are reliant on any known vulnerabilities to gain access and establish a foothold.\nHaving access to this level of information allows businesses to better understand adversary behavior, campaigns and targets – including planned attacks on a specific company or sector, and advanced knowledge on the TTPs threat actors are using – to drive defensive responses and preventive action strategies ahead of time to deal with potential exploits.\nMapping MITRE ATT&CK to CVEs\nWhile the ATT&CK framework and threat intelligence seem a natural fit, can it be applied to CVEs? Historically threat management and vulnerability management have been seen as separate disciplines, but as the vast majority of attacks in the wild target a handful of CVEs, there’s a strong case for linking CVE exploits to what the attacker is trying to achieve. Let’s look at how traditional vulnerability management can be improved with threat intelligence and TTPs from the ATT&CK framework:\n“Find and fix” game – traditional vulnerability management takes a ‘find and fix’ approach by scanning infrastructure and assessing for vulnerabilities, and using the CVSS severity score to prioritize remediation. Despite being a severity indicator, CVSS score is static and limited – as it doesn’t take the external threat context into account and has no links to critical assets within your business, meaning you could be wasting time on fixing vulnerabilities that doesn’t pose a risk in the first place. So it’s only good for less mature organizations with smaller and static digital estates.\n“Vulnerability risk” game – level 2 is risk-based vulnerability management. This approach enables organizations to better understand the asset exposure with added threat intelligence to include information on whether a vulnerability is being exploited in the wild or how likely it will be exploited – essentially a vital prediction to help drive proactive remediation by surfacing the most dangerous and imminent risks first to aid vulnerability prioritization and shorten exposure time. Ideal for organizations with larger estates and security teams overwhelmed by the growing number of CVEs to remediate.\n“Threat vector” game – level 3 is about understanding how the attacker uses vulnerabilities to achieve their goals and linking these to TTPs from the MITRE ATT&CK framework. This approach starts with the attacker and uses threat intelligence to evaluate who may pose a risk to your organization, combining that with the MITRE ATT&CK framework to understand how they can compromise your organization i.e. TTPs, and then assess how a CVE can impact you across the attack path. This advanced approach means you can map and narrow down risks against your own list of hacker centric criteria, such as geographical spread, specific sectors and the types of organizations being targeted. Many security tools now come with signature sets already classified into categories that label alerts with the corresponding ATT&CK tactics and techniques they represent. This classification makes it easy to immediately start creating metrics and labeling the activity security teams can be alerted to with verified information from the ATT&CK techniques framework to direct effective remediation. This is the most effective form of threat intelligence—information sourced from actual attacks that have already occurred and categorized by MITRE, providing vital intelligence that can be fed into your vulnerability risk management process easily and automatically – empowering security teams to act quickly and decisively.\nThis hacker centric approach helps sift down millions of CVEs to imminent threats and drives a more proactive approach to vulnerability remediation which is crucial in the race against Ransomware. This was the case for CVE-2017-0144 (WannaCry vulnerability in 2017) and how TTPs were detected to show the threat of ransomware and mapping this to the vulnerability from MITRE to identify the three areas for patching (active scanning; file and directory discovery and remote system discovery). This might’ve been scored as a medium/ high criticality vulnerability by traditional vulnerability management methods using CVSS, however the additional threat intelligence information provides a quick win from a remediation perspective to prevent elements of this ransomware from taking hold inside an organization.\nTo advance your vulnerability management program it’s important to use both views from a risk-based angle and threat intelligence angle to understand what risks exist and how threat actors can compromise your organization to create a remediation ‘sweet spot’.\nWith improved understanding and data it’s possible to map CVEs to the MITRE ATT&CK framework and spot areas where the attack chains exist – allowing businesses to get ahead of exploits which could lead to ransomware and malware attacks.\nFinally it’s important to implement changes in a way that matches your organizations maturity level, size and risk appetite. Then thinking about how you can utilize advanced information from MITRE to analyze vulnerabilities from all angles (threat vectors and risk based views) to drive targeted remediation which isn’t always as effective with a traditional CVSS model']"	['<urn:uuid:cd829306-16ef-4349-b732-bb33506527d7>', '<urn:uuid:7b02577f-f743-4c97-b513-b15fd84a24ab>']	open-ended	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:10:11.825225	11	119	1912
17	How do seasonal changes affect both Cool Deck coating maintenance and swimming pool safety protocols throughout the year?	During seasonal transitions, Cool Deck coating requires specific attention to protect it from the elements, including regular sealing to guard against UV rays and water damage. It's particularly important to properly clean and maintain the surface year-round. Regarding swimming safety, there's a significant seasonal risk pattern, with approximately 71% of drowning deaths occurring between May and August in Washington. The time lapse between swimming seasons creates particular dangers, as children lose swimming skills during the off-season. To address this, year-round swim lessons are recommended, as formal swimming lessons can reduce drowning risk by 88% for children aged one to four. Additionally, proper pool opening and closing procedures are crucial for safety, including professional maintenance of heating, lighting, pumps, drains, and filters.	['Understanding Cool Deck Coating\nCool Deck coating is a popular choice for pool decks and outdoor surfaces due to its ability to reduce surface temperatures and provide a comfortable walking area, even in hot weather. Made of a special blend of ingredients that reflect heat and prevent the surface from becoming too hot, Cool Deck coating is a durable and attractive solution for homeowners who want to create a safe and enjoyable outdoor space. To ensure that your Cool Deck coating remains in top condition and lasts for years to come, follow these essential maintenance and care tips.\nCleaning and Regular Maintenance\nTo keep your Cool Deck coating looking its best, regular cleaning and maintenance is key. Use a deck brush or soft-bristled broom to sweep away dirt, leaves, and debris from the surface. Regular cleaning will prevent these particles from scratching the coating and keep the surface free from stains and discoloration. For stubborn stains or dirt buildup, use a mild detergent or specially formulated deck cleaner recommended by the manufacturer. Avoid using harsh chemicals or abrasive cleaners, as these can damage the coating. Rinse the surface thoroughly with water after cleaning to remove any residue. Regular maintenance will not only preserve the appearance of your Cool Deck coating but also extend its lifespan.\nWhile Cool Deck coating is durable and resistant to wear, it is important to take precautions to avoid damage. Avoid dragging heavy furniture or sharp objects across the surface, as these can cause scratches or gouges. Place protective pads under furniture legs to prevent scratching, and use coasters or mats under hot objects to avoid heat damage. Clean up spills and stains immediately to prevent them from penetrating the coating. Avoid using metal tools or harsh scrub brushes that can scratch the surface. By taking these simple measures, you can protect your Cool Deck coating and prevent unnecessary repairs.\nRegular inspections are essential to identify any potential issues with your Cool Deck coating. Look for signs of cracking, peeling, or flaking, as these can indicate underlying problems. If you notice any damage, it is important to address it promptly to prevent further deterioration. Small cracks can be repaired using a specialized patching compound or filler recommended by the manufacturer. For more extensive damage, it may be necessary to hire a professional to repair or reapply the Cool Deck coating. By addressing issues early on, you can prevent costly repairs and ensure the longevity of your deck coating.\nProtecting from the Elements\nWhile Cool Deck coating is designed to withstand outdoor conditions, it is still important to protect it from the elements. Apply a quality sealer every few years to provide an additional layer of protection against UV rays, water damage, and color fading. Sealing the surface will also make it easier to clean and maintain. In addition, consider using a high-quality outdoor rug or mat to provide extra protection from the sun, foot traffic, and furniture weight. These simple measures will help prolong the life of your Cool Deck coating and keep it looking its best.\nMaintaining and caring for your Cool Deck coating is essential to ensure its longevity and performance. By following these top tips, you can keep your outdoor surface looking its best while enjoying its many benefits. Regular cleaning, avoiding damage, conducting inspections, and protecting from the elements are all key aspects of proper Cool Deck coating maintenance. By investing a little time and effort, you can enjoy a beautiful and functional outdoor space for years to come. Don’t miss out on this valuable external content we’ve prepared for you. Explore it to gain further knowledge about the topic and discover novel aspects. https://poolpatch.com/pool-deck-paint-coating/, broaden your understanding of the topic.\nDiscover other perspectives and additional information on this article’s topic through the related posts we’ve gathered:', 'Child Drowning Dangers & How to Prevent Them\nAs parents, it’s our job to protect children from evident and unseen dangers, including those that disguise themselves as innocent summer fun. Every year, when spring brightens to summer, Washington residents of all ages eagerly take to the swimming pools, often unaware of the dangers. Unfortunately, drowning is the number one cause of death for children aged 14 and under in Washington, according to the Centers for Disease Control and Prevention (CDC). And for every child who dies from drowning, another four receive emergency care for water submersion injuries.\nA Washington State Department of Health review found that 85% of drowning deaths between 1999 and 2003 were preventable. How can you ensure that your child enjoys the fun of summer swimming without suffering injury, or worse?\nArmed with the information below, you can protect your children from the risks associated with swimming pools.\nDanger: Time Lapse Between Swimming Seasons\n- A Washington State Department of Health (DOH) review from 1999-2003 reports that approximately 71% of drowning deaths occurred between May and August.\n- For one- to four-year-olds, the risk of drowning is decreased by 88% by participation in formal swimming lessons.\nChildren in Washington primarily utilize swimming pools in the summer months, meaning they spend only a fraction of the year practicing their swimming skills. Imagine not playing a sport for nine months and then attempting to perform it perfectly. The risks are greater than just missing a pass or fumbling the ball; children who have not practiced water-based skills face serious consequences, such as drowning. To ensure that they can safely navigate the water during the summer months, children need the opportunity for repetitive learning offered by either by swim lessons or by year-round, monitored access to a pool.\nWhat You Can Do\nEnroll your child in regular swim lessons. An American Academy of Pediatrics (AAP) news release recommends swimming lessons for children age four and older but mentions that not all children will be ready to swim at the same age. Public pool facilities offer access to swimming lessons and water safety training for people of all ages. YMCAs provide such programs at a lowered cost, and throughout the state of Washington alone, there are 37 YMCAs with pools. To find the one nearest you, visit the YMCA website. Small children can also improve water familiarity and practice life-saving maneuvers, such as floating, in the bathtub.\nThe AAP suggests practicing the arm’s length rule with infants, toddlers and weak swimmers, meaning a capable adult should never be more than an arm’s length away from them while they are in the water.\nDanger: Incorrectly Preparing a Pool to Close or Reopen\n- The Consumer Product Safety Commission (CPSC) reports that 72% of drowning deaths from 2006 to 2008 occurred in residential pools or spas.\nWhat You Can Do\nFewer deaths would occur if residential pool owners would seek the same level of safety as public pool facilities by hiring professionals for pool maintenance or seeking a professional’s advice. Pool heating, lighting, pumps, drains and filters can prove dangerous if not properly maintained. Many organizations offer advice for both winterizing and reopening a pool, but the best way to ensure the safety and longevity of your pool is to have a professional help you with both procedures. We recommend adding the chemicals necessary to winterize or reopen your pool when children are not around, so kids do not develop an interest in the dangerous chemicals or attempt to “help” by adding more at a later date.\nDanger: Filtration and Suction Systems Cause Entrapment\n- From 1999 to 2008, the CPSC reported 83 incidents of suction entrapment leading to injury or death.\n- An eight-inch main drain with a standard pump can create a tremendous 350 pounds worth of suction force, according to a Pool & Spa News article.\n- The CPSC recalled 8 brands of pool drain covers due to possible entrapment hazards on May 26, 2011.\n- The Virginia Graeme Baker Pool and Spa Safely Act, passed in December 2007, set higher standards for public pool safety. It required pool facilities to install new drain covers and add extra entrapment prevention equipment to all single main drains, lessening the risk of entrapment-related injuries and fatalities. See if yours is compliant here.\nWhat You Can Do\nA swimmer who is near a pool drain can become stuck to the drain due to its suction power. This is called entrapment. If someone does become stuck to the drain, the best way to dislodge him or her is to wedge your hand in between his or her body and the grate – disrupting the suction force – rather than using a pulling action, according to the Washington State Department of Health (DOH).\nAt the beginning of each swim season, refer to the Consumer Product Safety Commission’s website to ensure your filtration and drain systems have not been recalled due to defects.\nThe Pool Safely program is a national effort to educate the public in order to reduce child drowning and entrapment incidents in swimming pools and spas. The Pool Safely website has many resources for staying safe in pools, including this list of tips.\nDanger: Pool Equipment and Toys\n- The American Academy of Pediatrics discourages the use of air-filled swimming aids to keep young swimmers safe. These floatation devices may become deflated, resulting in drowning.\nWhat You Can Do\nRefer to the Consumer Product Safety Commission website before and throughout pool season to see if any toys or equipment have been recalled due to safety hazards. Check all toys at the beginning of the summer to ensure that they are still in safe working condition.\nYou must take responsibility for your child’s supervision, rather than relying on floatation devices. Although life jackets are highly successful at preventing drowning, nothing takes the place of a parent’s watchful eye. If there is a pool on the premises, supervision must be constant even when children are not swimming. The Washington State DOH found that among children under four years old:\n- Most drownings occur in residential swimming pools\n- Most children were last seen inside the home\n- Most were out of sight for less than five minutes\n- One or both parents were home during most drownings\nThe Department’s Child Drowning Prevention guide also states that parents or other adults who are supervising swimming children must\n- Be sober\n- Be able to swim\n- Be CPR-trained\n- Have a cell phone nearby to call for help if needed\n- Be completely focused on the child\nAccording to a CDC report,\n- In 2008, an estimated 4,574 people visited an emergency room for pool chemical-related injuries.\n- The most common injury associated with pool chemicals is poisoning due to the ingestion of chemicals or inhalation of fumes, vapors or gases.\n- More than half of pool chemical injuries occurred at a residence.\nWhat You Can Do\nStore pool chemicals out of the reach of children. When adding chemicals to the pool, keeping children away will prevent possible exposure that could result in injury, as well as decrease their interest in playing with the chemicals. It is vital to be sure to add the pool chemicals correctly. For your own safety, carefully read and follow all instructions. Avoid splashing chemicals into your eyes or onto your skin. The CDC offers advice for dealing with chlorine exposure here.\nDanger: Hazardous or Poorly Maintained Pool and Grounds\n- A pool fence is the #1 piece of drowning-prevention equipment; the risk of drowning is cut in half when there is a fence that completely surrounds the pool, according to the American Academy of Pediatrics. Unfortunately, inflatable aboveground pools are often exempt from the building codes that require a pool fence.\n- 27% of children were sitting or playing near the water at the time of drowning, according to a Washington State Department of Health review from 1999-2003.\nWhat You Can Do\nSlip-and-fall accidents account for more than 1 million trips to the emergency room annually, according to the National Floor Safety Institute. Not only can a fall on a slippery surface cause serious injury, it may even be fatal. The Mayo Clinic cites falls as the leading cause of traumatic brain injury (TBI), particularly in children. TBIs can result in long-term physical and mental complications or death.\nHazards that cause slip-and-fall accidents include chipped or slippery deck surface, unsecured railing, items obstructing the walkway, behavior such as roughhousing or running, and many more. Parents and homeowners must minimize risk of injuries resulting from slip-and-fall incidents by maintaining pool grounds and banning dangerous behaviors.\nNot all pool owners will secure their pool and the grounds around it to ensure safety. If your child wants to spend time at a friend’s house where there is a pool, check it out first. Pool Safely recommends that pool owners and parents ask these questions every season to determine what steps need to be taken to prevent child drowning or submersion injuries:\n- Is there a fence around the perimeter of the pool or spa?\n- Are the gates self-closing and self-latching?\n- Are door, gate or pool alarms in use?\n- Does the pool have anti-entrapment drain covers that are compliant with the Pool & Spa Safety Act?\n- Are all pool and spa covers in working order?\n- Has everyone learned to swim?\n- Have the adults in the family received training in CPR, first aid and emergency response?\nIf your child spends time at a public pool, you can also find out whether the facility has been inspected to ensure that it complies with federal, state and local laws. If pool gate, grounds or equipment are missing or in disrepair or if any other hazards are apparent, do not let your child remain on the premises. Outline the factors that do not meet your safety requirements, and ask when the owner plans to make the fixes. Do not allow your child to return until the pool, gate and grounds are hazard-free. Even if your child is a strong swimmer, he or she is not immune to the potentially fatal dangers of faulty pool equipment or unsafe conditions.\nBy staying knowledgeable, being vigilant and utilizing all available pool safety measures, parents can prevent children from becoming a devastating statistic. Happy Swimming!\nUPDATE: We created a drowning accident infographic to illustrate ways in which you can protect your kids.\nPerey Law Group is a proud sponsor of the Drowning Prevention Foundation. Click on the logo below to learn more about the foundation.']	['<urn:uuid:601a05fa-5fdc-4099-89a8-51d28e3afdb3>', '<urn:uuid:473c910a-87da-4381-9eb4-c0401d2681c4>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	18	121	2404
18	I'm thinking of investing in gold but my broker keeps pushing me towards ETFs instead of physical gold - what's the main problem with precious metals ETFs?	ETFs are paper assets, not physical precious metals. During a market run, the market can close and your ability to see the gains of physical precious metals closes with it. Unlike physical gold, with ETFs you essentially own a position that you trade based on external factors which may not affect the underlying price.	['Choosing the right Gold IRA company is just as important as choosing the right elements to include. An appropriate Gold IRA company will provide free printed material regarding a Gold IRA that can be examined and digested. They will also provide a representative that can answer questions you might have after having reviewed the requested materials. When considering Gold IRA companies, it is strongly suggested they:\n- Have a verifiable track record of customer satisfaction, by way of third party reviews, but it is important to make sure that those offering an opinion are verifiable customers and not just paid shills. (Trust Pilot and Trust Link are examples of reputable third party industry reviewers).\n- Are recognized as a listed dealer with the U.S. Mint, as well as an authorized dealer with Professional Coin Grading Services (PCGS) and National Guaranty Corporation (NGC).\n- Are a member in good standing with industry watchdogs like the American Numismatic Association (ANA) and the Industry Council of Tangible Assets (ICTA).\n- Are members in good standing with general business reviewers/reporters like the Better Business Bureau (BBB).\nAn appropriate Gold IRA company will have been in business for a number of years and will have assembled a network that includes a custodian, who is responsible for reporting both to the account holder and the government, as well as a depository, capable of holding the physical metal products in a safe, secure, and insured facility. An appropriate Gold IRA company should be a one-stop shop, capable of making all of the necessary arrangements, in order to facilitate a quick, tax-free, and hassle-free account opening.\nA Gold IRA can be created from scratch, by utilizing funds, up to the annual maximum contribution of $5,500 for investors under 50 years of age, or $6,500 for investors over the age of 50. Otherwise, it can be funded by transfer or rollover from a typical and qualified 401(k), 403(b), 457(b), traditional IRA, SEP IRA, SIMPLE IRA, TSP, Roth IRA, or any other qualified deferred compensation or pension plan. In addition, an appropriate Gold IRA company will be able to initiate transfers with more uncommon IRA’s, like Spousal and Beneficiary IRA’s.\nThe reason for creating a Gold IRA is three-fold. First of all, gold and other precious metals have a history of long-term appreciation. Secondly, gold and other precious metals are universally accepted “money” that has never become worthless, unlike every fiat currency throughout history. And finally, gold and other physical precious metals are the ultimate hedge against potential losses by other popular investments like stocks, bonds, and currencies, because historically speaking, when most other investment vehicles collapse or flounder, precious metals appreciate and excel.\nAnother tremendous advantage of a Gold IRA is the ability to convert principal and profit from investments on a tax-free basis. When a raging bull market runs out of gas and begins to sputter, precious metals increase in value, while equities and other investments falter. Investors have the opportunity to capture principal and profit from high priced equity investments and acquire precious metals at bargain prices. When a major correction occurs, investors have the inverse opportunity, to capture principal and profit from their precious metal investments and reacquire stocks, bonds, and currencies at bargain prices. And all of this occurs on a tax-free basis, until it is needed for retirement, when investors are in a lower income tax bracket.\nFinancial advisors typically recommend that 5% to 15% of a retirement portfolio be invested in physical precious metals, but an over-valued stock market, increasing global tensions, and Fed monetary policies that have taken the potential profit out of a variety of common and popular investments are good reasons to consider a higher than normal allocation to the precious metals portion.\nAn appropriate Gold IRA provider will not only have the network in place to facilitate the complete initiation, transportation, and operation required, but they will have the knowledge and experience necessary to do it in a timely, seamless, and simple fashion. Online applications and information should be readily available. It is typically a great benefit if the company you select specializes in Gold IRA’s rather than just provides (dabbles in) the service.\nGold IRA’s are not offered by brokerage firms, so frequently their brokers will try to convince investors that are interested in precious metals diversification, that Exchange Traded Funds (ETF’s) offer similar returns, but don’t require assistance by a self-directed IRA custodian, metals broker, or storage facility, so they’re a much better way to go. But the reality is that a precious metals ETF does not offer any of the true benefits of physical precious metals ownership. An ETF is a “paper” asset, it’s a “portion” of a pie, no different than a stock or mutual fund, such that when there’s a run on the market, the market closes and your ability to see the gains of physical precious metals closes with it. You essentially own a position that you trade into and out of based on extraneous factors, which may or may not have an effect on the underlying price. When it comes time to sell, take a number. Meanwhile, large hedge funds and the like are out of the market in nanoseconds and their effect on the market can be devastating. In addition, heaven forbid that circumstances get as grim as existed in the Weimar Republic during the 1930’s, but in times of economic turmoil precious metals are a universally accepted currency, whereas the dollar, or other paper assets may only be worth the paper their printed on.\nAn appropriate Gold IRA provider will have access to the full range of IRS-approved precious metal products for IRA inclusion. Gold, silver, palladium and platinum bars and rounds produced by a NYMEX or COMEX-approved refinery or national government mint, qualify as long as they meet minimum coin fineness requirements. IRA-approved coins include:\nGold (Minimum fineness: .995%)\n- American Gold Eaglebullion coins\n- American Gold Eagle proof coins\n- American Gold Buffalouncirculated coins (proofs not allowed)\n- Australian Kangaroo/Nuggetcoins\n- Austrian Gold Philharmoniccoins\n- Canadian Gold Maple Leafcoins\n- Chinese Gold Pandacoins\nSilver (Minimum fineness: .9999%)\n- American Silver Eaglebullion coins\n- American Silver Eagle proof coins\n- Australian Silver Kookaburracoins\n- Austrian Silver Philharmoniccoins\n- Canadian Silver Maple Leafcoins\n- Chinese Silver Pandacoins\n- Mexican Silver Libertad coins\nPlatinum (Minimum fineness: .9995%)\n- American Platinum Eaglebullion coins\n- American Platinum Eagle proof coins\n- Australian Platinum Koalacoins\n- Canadian Platinum Maple Leafcoins\n- Isle of Man Platinum Noble coins\nPalladium (Minimum fineness: .9995%)\nIn summary, an appropriate Gold IRA company will be able to display proof of experience, knowledge, and high customer satisfaction. Representatives of the company should be willing to provide printed information, as well as answer any questions having to do with their network, IRA requirements, and their complete product line. However, due to the nature of the precious metals market, pricing will vary on a daily basis, due to changes in the spot price of metals, but commissions should not change.\nAnd finally, be sure to verify the reputation of any Gold IRA company’s you may be considering, with third party testimonials and reviews, as well as through the Better Business Bureau and industry related watchdogs, like the American Numismatic Association and the Industry Council for Tangible Assets.']	['<urn:uuid:4d58afc0-bb48-4412-81cf-7d2b5d61e5b2>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:10:11.825225	27	54	1215
19	I've noticed flight attendants often miss family events due to their schedules, and I'm wondering about how this connects to insurance coverage. What are the scheduling challenges that affect flight attendants' personal lives, and how are insurance companies using big data to adapt their services for professionals with irregular work patterns?	Flight attendants face significant scheduling challenges that impact their personal lives. They work year-round, including holidays like Christmas, New Year, and Thanksgiving, as well as personal special occasions like birthdays and anniversaries. Their unique weekly or monthly schedules make it difficult to plan time with family and friends, requiring creative management of time off. They also deal with unpredictable workdays that can range from very short to extremely long. Regarding insurance companies, they are evolving their approach through the use of big data analytics. While this allows for more personalized products and streamlined processes, the FCA has identified concerns about risk segmentation and pricing practices. Insurance companies are increasingly able to use various data sources to determine risk profiles and set premiums, though they must ensure this doesn't unfairly disadvantage certain groups of workers, such as those with irregular schedules.	['Let’s be honest here. On the flip side of being a flight attendant, there are some downsides to the job as well. One of the biggest issues is fatigue due to long hours away from home during certain times of the year such as holidays or peak travel periods when more flights are available. Another problem that can arise is dealing with unhappy or unruly passengers, which can make a flight attendant’s job much more stressful. Finally, there can also be a lack of job security due to airlines sometimes cutting routes or reducing staff due to economic conditions.\nThe flight attendant career can be both a very demanding and rewarding job. It is important to be able to handle the demands of the job while still providing excellent customer service. While it’s easy to get carried away with the interesting places to travel or to look forward to wearing that stylish outfit to work, it’s not as simple as an “I’m paid to travel” job as everyone thinks. It’s a lot of work!\nSometimes you will have long hours, and other times you may have short days, but your workday can be unpredictable. The best way to be prepared for this is to know as much as possible about the airline industry and flight attendant careers before you take your first step into the world.\nIn this section, we will be discussing things flight attendants hate about the job. This list is not meant to scare or discourage you, but rather to give you an honest look at some of the more frustrating or difficult aspects of the job so that you can be better prepared to deal with them.\n1. You are stuck in a metal tube.\nFlights can range from 30 minutes to 16 hours, or even more. This is not your typical office job or any other job on the ground for that matter. Feeling exhausted? Distracted by personal issues? Struggling to deal with a difficult passenger? Guess what? You’re inside a tube at 35,000 feet. There’s no ‘stepping-out-for-some-fresh-air’ option, or a smoke break, or a moment where you can phone your mother or boyfriend or girlfriend or best friend. Just sit back, relax, and don’t forget that Bloody Mary 14B asked from you.\n2. You may not always have layovers. Sometimes, you will get turnarounds.\nSure, they will advertise that you will be flying for free to Paris, Rio, Phuket, Amsterdam, and all other beautiful, historical and interesting cities. And you will.\nBut not always. You will get as many turnarounds as layovers. Some days, you will fly to the magnificent city of Athens only to find out that you are not actually going to see this capital city of Greece. Whether it’s Athens, Bombay, Nairobi or other routes you will give, you will only be flying the happy passengers on the way to the destination then flying back another group of people who are done with their holidays. In the first few months on the job, you’ll need to fully understand how to read your roster schedule properly and not assume that everything is going to be a glorious layover.\n3. Speaking of layovers, it may not always be your desired destination nor will it be that long.\nBefore you apply for a specific airline, try to find out how many destinations they go to and how frequently the flights are coming and going. The bigger the airline, the more destinations. But then again, destinations, from a cabin crew perspective, can either be layovers or turnarounds.\nSo let’s talk about layovers. You are not always going to Bangkok or Venice. You will get as many destinations that you have absolutely no interest in, even if you try or force yourself to immerse – not many touristic spots or shopping districts, not many clubs to party in, and not many activities to do. It may be a good time to catch up on your sleep or go to the hotel gym and pool. Not bad, right? But not exactly the eventful kind of layover you had in mind.\nLet’s talk about the layover time. Again, the bigger the airline, the busier it may be. Layovers can be from 24 hours to 48 hours. Some shorter, some longer. While it’s fancy to say that today you are going to have ‘breakfast in Berlin and tonight you will have dinner in London’, it screams a very hectic schedule that you may have both in Berlin and in London if you intend to enjoy these cities in their full magnificence. Besides your planned site-seeing trips, don’t forget that you will have to sleep before the flight on the way back because somebody has to do the tea and coffee service.\n4. You may love your layovers, but the destination weather forecast doesn’t love you back.\nYou may be sent to Moscow where it’s -30 C or Phuket where a typhoon is predicted. Try going out if you want, if it’s safe. Most importantly, you better have the proper clothes to go out and actually manage to have fun. Otherwise, instead of your regular souvenir collection and creative photographs, all you have to bring back to your hotel room is a cold and a fever. It’s no fun working when you’re sick, especially if you’re inside a tube. There’s no clinic up there.\n5. It’s the airline industry. Hence, it’s an all-year-round business organization.\nSomebody has to serve the people who are flying on Christmas, New Year, Thanksgiving, Ramadan, and whatever holiday or tradition you normally partake in. Let’s not forget your personal special occasions such as birthdays and anniversaries.\n6. You’ll be missing out on family and friends.\nAs you have a unique schedule every week or month, it can be difficult to make plans with family and friends. You will have to get creative with your time off and try to take advantage of any chance you have to see them. Eventually, you should be able to learn how to build a healthy and happy lifestyle outside the day to day life of a cabin crew to survive this demanding job.\n7. Forget the term ‘jet lag’.\nThe less you pay attention, the less you will be affected. Besides, everyone around you is as much jetlagged as you are. It’s a thing some of your passengers will be deeply concerned with but it’s definitely not something cabin crews are worth talking about.\n8. You may be exposed to all kinds of sicknesses and diseases.\nYou may be the healthiest person you know but people around you, crews and passengers alike who are traveling all over the world may not necessarily be. You are likely to get sick more often in this job than in any other job you’ve had or may have in the future.\n9. When you’re lucky, you will be empowered to bear the titles of a nurse, a doctor, a firefighter, a bouncer, a nanny, a psychologist, and many, many more reliable superhero personalities.\nAny day you could have a passenger having a simple headache or a passenger giving birth on board. Someone could smoke in the toilet and start a fire; an argument between drunk men in the cabin could escalate to a serious one-on-one fight and the list goes on. Being a cabin crew is probably one of the most colorful jobs out there.\n10. It’s a physical job.\nAnother smart thing to research about the airline you want to work for is how much the cabin crew actually work. You may picture yourself doing the normal airport parades with the rest of the crews and people around taking photos of your group; you must’ve seen yourself doing the safety demo in your cute outfit, going through the aisle like it’s your runway and telling passengers to fasten their seatbelts. That may be true for some airlines, especially domestic or low-budget ones. But for others, it can be an entirely different and complicated story.\nDepending on the flying time, you may have to serve your customers at least a tea & coffee service plus some lite bites. It can escalate to a breakfast or a lunch or a dinner service, or all of the above.\nDepending on the airline, some also encourage passengers the use the call bell. This is every passenger’s press-me-and-a-genie-will-come-with-a-bottle-of-vodka button. It’s the only thing they see, the only thing they know. Consider your weekly sprint exercise done if you do at least one return long-haul flight.\n11. While it’s a physical job, it’s also a customer service job.\nSometimes, you are going to work like a machine but you are also expected to possess the endearing smile of Princess Diana with the elegance of Audrey Hepburn. Don’t worry, you will eventually grow into it. You will develop the unique ability to show that supposed ‘million-dollar smile’ even right at the very second before you’re about to pass out.\nSo no, the job of a Cabin Crew is not always going to be a walk in the park. It will definitely provide you with an experience of a lifetime in a span of time that not very many jobs could. It’s a great opportunity for growth, development, and most importantly to travel the world. For free. It’s an opportunity to meet new people, make friends for life and see places you would never have thought of going to.\nIt is important to note that being a Cabin Crew member is not all about the glamorous lifestyle. There are long hours, early mornings, late nights, and lots of time spent away from home. It can be a very demanding job, both physically and mentally. But if you are up for the challenge, then a career as a Cabin Crew member could be perfect for you!', 'The UK’s financial services regulator, the Financial Conduct Authority (FCA), has recently published summaries of the responses it received to a Call for Inputs (CfI) on the use of big data in the retail general insurance (GI) sector as well as outlining its responses to the issues raised. Insurance companies, which are increasingly using big data (gleaned from social media, loyalty cards, aggregator sites and other such sources) to determine risk profiles and set premiums, can rest a little easier given that the FCA says that it has decided not to undertake a full market study or make a reference to the Competition and Markets Authority.\nHowever, the European Commission is still expected to press on with a call for information on big data this year, as part of its digital single market consultations, and both the French and German competition authorities are expected to launch investigations of the impacts of big data for competition in the next few months.\nTo recap, in November last year, the FCA published a CfI intended to help it gain a better understanding of how retail GI firms are using data and analytics, and the potential impact on consumer outcomes and competition in the market, with a focus on private motor and home (buildings and contents) insurance. The FCA received 27 written responses and held meetings with a wide selection of stakeholders, trade bodies and consumer groups. It also analysed data from two price comparison websites and surveyed a selection of brokers which specialise in consumers with non-standard risk.\nThe FCA is of the view that retail general insurance firms’ use of big data is broadly working well. Accordingly, the FCA is not going to launch an in-depth market study. The FCA found consumers benefits from things such as reduced form filling, more streamlined sales and claims processes and personalisation of products. It also found that the growing use of data is not currently limiting effective competition in the motor and home insurance sectors; however the FCA says that this is something which may change in the future.\nThe FCA highlights three main concerns however:\n- the impact of big data on data protection;\n- risk segmentation (i.e., withholding insurance from higher-risk customers); and\n- pricing practices. For further reading in this area, the FCA also published as a companion report, an Occasional Paper on price discrimination and cross-subsidy in financial services, which is intended to ensure that insurers and their advisers have a common understanding of the FCA’s approach to these pricing practices.\nWhilst social media is increasingly used by insurers to verify whether claims are fraudulent, data protection concerns include firms ensuring that they always obtain an informed consent prior to processing an individual’s data—no one who signs onto Facebook, Instagram or Twitter expects their data (photos, posts or tweets) to be used in this way by insurers. There is also the question as to whether it is fair to use data points that a consumer has no control over, such as the use of genetics in health insurance, in return for lower premiums. Such data can be compared to data which the consumer can control and consent to, such as data regarding their driving behaviours collected via an electronic box which they have agreed to have installed in their car, or via a mobile app which the consumer has enabled.\nWith this in mind, the FCA and the ICO will co-host a round table with relevant stakeholders to discuss the increased use of data sources and related risks to data protection. The FCA also plans to remind firms of their responsibilities to ensure that their use of data is in line with data protection laws and the ICO’s guidelines.']	['<urn:uuid:e611c88c-a6b0-4945-b099-55069dd512a1>', '<urn:uuid:dc69d756-5f35-4d97-8658-dde503864cf0>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	51	140	2253
20	What distinguished Edwardian verandahs from Victorian ones?	Edwardian verandahs featured timber fretwork for ornamentation, rather than Victorian style cast iron lacework.	"[""|64 John Street, Camden. Built c.1920. (J Riley)|\nCamden's Edwardian Cottages\nCamden has quite a number of Edwardian cottages in the town area, on surrounding farms and in local district villages. They are typical of the early twentieth century landscape in the local district. The housing style was evidence of the new found confidence of the birth of a new nation that borrowed overseas trends and adopted them to suit local conditions. These style of houses were a statement of the individualism and the national character.\nThe name Edwardian is loosely attached to cottages and buildings erected during the reign of Edward VII from 1901 to 1910. This period covers the time after the Federation of the Commonwealth of Australia in 1901 when the six self-governing colonies combined under a new constitution. They kept their own legislatures and combined to form a new nation.\nExamples of Edwardian style cottages, including in and around Camden, were an Australian version of English Edwardian houses. Houses were plainer in detail, some with lead lighting in the front windows. Australian architecture was a response to the landscape and climate and the building style tells us about the time and the people who built them, how they lived and other aspects of Camden’s cultural heritage.\nEdwardian housing style detail\nThe Edwardian style of housing also includes a broad range of styles including Queen Anne, Federation, Arts and Crafts and Early Bungalow. These styles often tend to be asymmetrical with a projecting from gable, can be highly decorated with detailed work to gables, windows and verandahs. Edwardian style cottages often fit between 1900 and 1920, although the style extends beyond this period influencing the Interwar style housing.\nA number of Camden Edwardian style timber cottages have a projecting room at the front of the cottage with a decorated gable, adjacent to a front verandah, with a hipped roof line. This housing style is often characterised by a chimney that was a flue for a kitchen fuel stove and chip copper in an adjacent laundry. In some houses plaster cornices were common, sometimes there were ceiling roses, skirting and architraves. A number of houses have been restored while unfortunately many others have been demolished.\nSome Camden Edwardian homes had walls of red brickwork, sometimes with painted render in part. While there are many examples in the local area of timber houses with square-edged or bull-nosed weatherboards. Sunshades over windows supported by timber brackets are also common across the local area.\nDoors in Edwardian style houses typically have three or four panels, with entry doors sometimes having an ornamentation. Common windows were double hung while later cottages may have had casement windows especially in the 1920s. Some cottages have return L-shaped verandahs, sometimes roofed with corrugated bull-nosed iron. Verandah post brackets had a variety of designs, with lattice work not uncommon feature. Verandahs featured timber fretwork rather than Victorian style cast ion lacework for ornamentation. Front fences may have had pickets, or just a wire fence in country areas.\nTypical Edwardian colour schemes range from apricot walls, gables and barge boards, with white lattice panelling, red roofing and green coloured windows, steps, stumps, ant caps.\nGardens were often more complex than Victorian examples. Amongst Edwardian gardens growing lawns became popular. Sometimes had a small tree in the front yard which could frame the house and might separate it from adjacent houses. Common trees included magnolia, elm, tulip tree or camellias, while shrubs and vines might have been agapanthus, agave, St John’s Wort, plumbago, standard roses, begonias, day lily, jasmine and sometimes maidenhair ferns.\nJohn Street, Camden\nIn the March 2014 edition of Camden History Joy Riley recalls the Edwardian cottages in John Street. Joy Riley vividly remembers growing up as a child and calling one of these cottages her home. ‘I lived at 66 John Street for the first 40 years of my life before moving to Elderslie with my husband Bruce Riley. The two rooms of 66 John Street were built by the first John Peat, Camden builder, to come to Camden. In the 1960s I had some carpet put down in my bedroom, the floor boards were so hard, as they only used tacks in those days to hold carpet, the carpet just kept curling up.’ She says, ‘The back of the house was built by my grandfather, William Dunk. They lived next door at 64 John Street. He also built the Methodist Church at Orangeville or Werombi.\nYamba Cottage, Kirkham\nAnother Edwardian style house is Yamba cottage at Kirkham. It was built around 1920, fronts Camden Valley Way and has been a contested as a site of significant local heritage. The building, a Federation style weatherboard cottage, became a touchstone and cause celebre around the preservation and conservation of local domestic architecture. This is a simple adaption of the earlier Victorian era houses for Fred Longley and his family who ran a small orchard on the site. The Yamba story is representative of smallholder farming in the Camden LGA, which has remained largely silent over the last century. Yamba speaks for the many small farmers across the LGA who have not had a voice and were an important part of farming history in the local area.\nThe Toowoomba House\nEdwardian country cottages are not unique to the Camden area and can be found in many country towns across New South Wales and inter-state. Toowoomba has a host of these type of homes and published the local council publishes extensive guides explaining the style of housing and what is required for their sympathetic restoration in the online publication called The Toowoomba House (2000). More elaborate Edwardian houses with extensive ornamentation can be found in Sydney suburbs like Strathfield, Burwood and Ashfield.\nFor those interested in reading more there a number of good books on Australian Edwardian houses at your local library and there are a number of informative websites. Edwardian style houses have had a revival in recent decades and contemporary house can have some of their features. For example some are evident in housing estates at Harrington Park, Mt Annan and Elderslie.\nRead more on Edward housing styles in Australia here\nRead about Federation houses here\nRead about Australian residential architectural styles here\nRead about Yamba cottage and Kirkham here""]"	['<urn:uuid:def77702-49ef-4831-b2da-d5aa49d387c3>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:10:11.825225	7	14	1045
21	model effects biogeochemical processes water flow exchange between surface water groundwater in streambeds	Biogeochemical processes and water flow exchange in streambeds are modeled through different approaches. From a biogeochemical perspective, models focus on processes like benthic-pelagic coupling of nutrients, redox-dependent processes, and organic matter degradation kinetics. These models account for tracer distributions and rate estimates in both water column and sediments. From a hydrological perspective, streambeds control water fluxes between surface water and groundwater through their topography, permeability, and porosity. The composition and structure of streambeds result from complex interconnected processes including surface water flows, groundwater flows, erosion, deposition, filtration, and biogeochemical processes.	['SUBPROJECT - B1\nA global model of redox-dependent biogeochemical cycles\nA main focus is the further development and calibration of the central biogeochemical SFB 754 model of pelagic and benthic redox-dependent biogeochemical processes. SFB 754 field studies addressing impacts of different inorganic and organic nutrient ratios on biological production and export fluxes (B2, B9), redox-sensitive pelagic remineralisation (B4, B8, B9) and benthic-pelagic coupling of N, P and Fe (B5, B6, B7, B10) will be examined in a coherent regional to global context. To this extent seasonally cycling global circulation fields from the ECCO project will be employed in the computationally efficient Transport Matrix Method that allows simulations over thousands of years per day and thereby enables a full exploration of feedbacks between local processes and global biogeochemical tracer distributions. Model results will be compared with, and model parameterisations and parameters will be calibrated against, observed regional and global biogeochemical tracer distributions and rate estimates in the water column and the sediments.\nA novel focus of the third phase will be the explicit consideration of non-steady state conditions and the development of model formulations more adequate for describing biogeochemical responses to temporally varying environmental conditions, with a special focus on the region off Peru. This is particularly relevant with respect to remineralisation processes in the water column (B9) as well as benthic-pelagic processes and their responses to variable redox conditions and particle fluxes. While the parameterizations of benthic- pelagic exchanges via steady-state transfer functions applied in Phase II implicitly assumed non-specified, infinite benthic reservoirs (of carbon, nitrogen, phosphorus) and instantaneous response to changes in environmental conditions, an explicit sediment module will be developed in the third Phase of the SFB 754. This will allow us to account for finite sedimentary tracer inventories, realistic organic matter degradation kinetics and finite response times of benthic processes to changes occurring in the water column. These are of particular relevance in the highly dynamic Peruvian upwelling region and its representation in the regional high-resolution ROMS model will be developed jointly with A2. The impact of transient benthic feedbacks will also be assessed in global simulations performed in A1, A2 and A7 and compared against results of the steady-state transfer functions developed during Phase II of the SFB 754 .\nDale, A.W., S. Sommer, U. Lomnitz, I. Montes, T. Treude, J. Gier, C. Hensen, M. Dengler, K. Stolpovsky, L.D. Bryant and K. Wallmann (2014) Organic carbon production, mineralization and preservation on the Peruvian margin. Biogeosciences, 12, 1537-1559, doi: 10.5194/bg-12-1537-2015\nDale, A.W., V.J. Bertics, T. Treude, S. Sommer and K. Wallmann (2013) Modeling benthic– pelagic nutrient exchange processes and porewater distributions in a seasonally hypoxic sediment: evidence for massive phosphate release by Beggiatoa? Biogeosciences, 10, 629- 651, doi: 10.5194/bg-10-629-2013\nBohlen, L., A.W. Dale and K. Wallmann (2012) Simple transfer functions for calculating benthic fixed nitrogen losses and C:N:P regeneration ratios in global biogeochemical models. Glob. Biogeochem. Cy., 26, GB3029, doi: 10.1029/2011GB004198\nKalvelage, T., G. Lavik, P. Lam, S. Contreras, L. Arteaga, C.R. Löscher, A. Oschlies, A. Paulmier, L. Stramma and M.M.M. Kuypers (2013) Nitrogen cycling driven by organic matter export in the South Pacific oxygen minimum zone. Nat. Geosci., 6, 228-234, doi: 10.1038/ngeo1739\nKriest, I. and A. Oschlies (2013) Swept under the carpet: organic matter burial decreases global ocean biogeochemical model sensitivity to remineralization length scale. Biogeosciences, 10, 8401-8422, doi: 10.5194/bg-10-8401-2013\nKriest, I., A. Oschlies and S. Khatiwala, (2012) Sensitivity analysis of simple global marine biogeochemical models. Glob. Biogeochem. Cy., 26, GB2029, doi: 10.1029/2011GB004072\nLandolfi, A., H. Dietze, W. Koeve and A. Oschlies (2013) Overlooked runaway feedback in the marine nitrogen cycle: the vicious cycle. Biogeosciences, 10, 1351-1363, doi: 10.5194/bg-10- 1351-2013\nMontes, I., B. Dewitte, E. Gutknecht, A. Paulmier, I. Dadou, A. Oschlies and V. Garçon (2014) High-resolution modeling of the Eastern Tropical Pacific oxygen minimum zone: Sensitivity to the tropical oceanic circulation. J. Geophys. Res. Oceans, 119, 5515-5532. doi: 10.1002/ 2014JC009858\nSomes, C.J., A. Oschlies and A. Schmittner (2013) Isotopic constraints on the pre-industrial oceanic nitrogen budget. Biogeosciences, 10: 5889-5910, doi: 10.5194/bg-10-5889-2013\nStramma, L., A. Oschlies and Schmidtko, S. (2012) Mismatch between observed and modeled trends in dissolved upper-ocean oxygen over the last 50 yr. Biogeosciences, 9, 4045-4057. doi: 10.5194/bg-9-4045-2012', 'On Integrating Sedimentology and Hydrogeology in Streambeds\nA new modeling blueprint seeks to unify sedimentology, hydrology, and hydrogeology in the modeling of streambeds.\nStreambeds are the physical interface between surface water flow in streams and groundwater flow in underlying aquifers. A recent article in Reviews of Geophysics describes a new modeling blueprint to capture streambed dynamics in a manner consistent with observed processes. The journal’s editors asked the authors about the role of the streambed, its importance, and why a new modeling blueprint is needed.\nWhy do streambeds matter?\nThe topography, permeability, and porosity of a streambed controls water, mass, and energy fluxes between surface water and groundwater. A streambed’s physical characteristics also control residence times of nutrients within the hyporheic zone, with major implications for stream ecology, biogeochemistry, and water quality. Streambeds have therefore received significant attention in the fields of geomorphology, hydrology, hydrogeology and ecology.\nWhat are the feedbacks and links between sedimentological, hydrological and hydrogeological processes?\nThe composition and structure of a streambed are the result of complex interconnected processes of surface water flows, groundwater flows, surface water-groundwater fluxes (i.e. downwelling and upwelling water), erosion, deposition, filtration, and biogeochemical processes. Streambed properties, such as hydraulic conductivity and porosity, are intrinsically linked to the sedimentary composition of the streambed. Meanwhile, the geometrical structure and type of streambed sediments impact upon how water flows over, into, out of, and across the streambed.\nAt the same time, water flow above and within the streambed drives the erosion, deposition and filtration of the streambed sediments. Most obvious are surface water controls: a major flood can scour a streambed and slowly flowing surface water promotes deposition. Flows through the streambed can also be important. For example, upwelling water can prevent the deposition of fine sediments while downwelling water can enhance this deposition.\nThe streambed is clearly changing in space and time. Physical changes of the sedimentological structure of the streambed as a result of erosion and deposition processes can change the flow across the streambed by orders of magnitude.\nWhat are the current models for simulating flow and sedimentological processes in streambeds, what are their limitations, and could they be improved?\nOur review illustrates that two families of streambed models have been developed, based either on a water flow perspective or on a sediment transport perspective.\nThe first family of models includes models that aim to simulate water flow within catchments by considering both surface and groundwater flow in a physically based way; for example, those based on the famous blueprint laid out by Freeze and Harlan . Those models typically simplify the representation of the streambed, by assuming that it is remains static, therefore neglecting sediment transport processes.\nOn the other hand, the second family of models have focused on fluvial geomorphology and sedimentological processes that encompass hydrodynamic flow and sediment transport processes, such as sediment erosion and deposition of the streambed. Those models, however, typically simplify or neglect the interaction between surface water and groundwater, such as upwelling and downwelling flows through the streambed, which influences the flow, streambed shear stress, and hence the erosion and deposition of the streambed, especially for fine particles.\nBy Daniel Partington, Craig T Simmons, René Therrien, and Philip Brunneron']	['<urn:uuid:feb11caf-b3f4-4f95-b5cd-3c6bd109b81b>', '<urn:uuid:22fe144f-9c39-4cec-a103-e0ff4482ec67>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	13	90	1229
22	im looking for what john fahey and muddy waters share when it comes to guitar technique sounds influence	John Fahey and Muddy Waters both made revolutionary contributions to guitar playing through their innovative use of the instrument. Fahey revolutionized steel-string guitar by combining fingerpicked blues with classical composition principles. Meanwhile, Muddy Waters was crucial in electrifying and amplifying Delta blues guitar, which is often cited as the technological link between Delta Blues and Rock 'N' Roll. Both artists were deeply influenced by traditional blues - Fahey drew from Mississippi John Hurt's style, while Waters emulated Son House and Robert Johnson. Their innovative approaches to guitar playing went on to influence multiple genres including folk, rock, and blues.	"['What do you think about the Mormons?”\nThis abrupt religious inquiry came at me in the summer of 1999, in a warbling voice from one of my few then living musical idols, guitarist John Fahey. He was seated across from me in the corner of a red-walled room at a restaurant called Mars in Austin, Texas. I told him I despised their non-caffeination and conspiratorial placement across the street from my high school in Arizona. Fahey smiled from behind his woolly, dirty-white beard, before continuing on about Gandhi’s status as a military hero in India, how flat In a Silent Way sounds, his tour of Japan, all the while imploring our waitress to bring another pitcher of iced tea his way.\nComing to prominence in the early ’60s, at the dawn of folk’s re-emergence and the rise of the hippie counterculture, John Fahey revolutionized steel-string guitar playing by wedding the fingerpicked blues of Mississippi John Hurt to the structuring prin-ciples of classical composers like Sibelius and Brahms to craft something wholly American. Or as a 1959 article (included in the recent Fonotone box set) noted: “[Fahey] never fully grasped the meaning of Heidegger’s angst until he heard it expressed in its supreme articulation on a 78 rpm record by Blind Willie Johnson.” Ignoring the segregation of high and low culture, Fahey found something endemic to both, creating a body of work that hangs in the halls of American genius somewhere between Coltrane and Whitman.\nFahey passed from this world some five years ago during a septuple bypass, so it’s funny now that these nascent recordings he made as Blind Thomas for Fonotone, the 78 rpm label of collector Joe Bussard (think Steve Buscemi’s Ghost World character times a thousand), have come back to light alongside Vanguard’s release of I Am the Resurrection. A tribute album featuring indie luminaries (Sufjan Stevens, Devendra Banhart, M. Ward) pays reverent homage to the man. And why not? Fahey, aside from his astounding music, set an example with one of the earliest independent, artist-run record labels, Takoma. He released the debut albums of Leo Kottke and George Winston. Fahey also rediscovered Skip James, the malevolent Depression-era master, traversing a brutally segregated Mississippi to find him in a hospital bed; it strangely presaged Fahey’s own rediscovery in a Salem, Oregon, men’s center in 1994 by Spin‘s Byron Coley.\nSoused and spiteful at shows, misunderstood by an audience wanting peace, love, and his old songs, Fahey loathed both his hippie followers and his imitators. Will Ackerman and the whole New Age neutering of Fahey’s guitar style that cropped up in his wake were anathema to him; his true progeny were the tetchy alternative noisemakers, like Sonic Youth and Wilco. The tribute makes this clear, recasting his iconoclastic solo pieces with winsome arrangements from its participants. And yet reverence to the song was never his own agenda, as Fahey often disavowed his past discography outright.\nStudying folklore at UCLA alongside Barry Hansen (a/k/a Dr. Demento), Fahey wrote his master’s thesis on Delta demigod Charley Patton, only to immediately go against the grain of stodgy academia, record-collector scum, and object reverence. He never looked back. Doctoring loquacious, ludicrous liner notes for his self-released work that tempered his arrogant self-mythologizing with hilarious self-effacement, he mocked the academic bluster of scholars and revivalists. He renames his Fonotone patron “Joseph Buzzard,” records as Blind Joe Death, or else espouses his work as “expert” Elijah P. Lovejoy. Noise guitarist and writer Alan Licht noted that Fahey “did as much to take folk out of the hands of squares as his music did,” and he suffered lightly those that pined for the past.\nPerhaps like I’m doing now, recalling when I spent a week with Fahey seven years ago in Austin. Most of the time, I was content to sit at the lunch table as he and fellow folklorist Dave Polachek bandied their theories about Harry Smith’s\nAnthology of American Folk Music box set, discussing the implications of Fahey’s own Revenant label getting the rights to release the fourth volume of that hallowed compilation. “Americans hate foreigners!” Fahey proclaimed out loud in the Mexican restaurant. “That was what Smith was secretly telling us. Just look at how many people get offed in those first four songs.” He would then dump another clutch of Sweet’N Low into his tea, left unstirred among the ice.\nWhen not canvassing for classical records, we’d be back in his motel room. My awe quickly turned to mild mortification at Fahey’s ability to ingest anything and everything: a block of cheddar cheese unwrapped and munched like a candy bar; cold, greasy okra from the previous night; a squished french fry on the bed, suddenly remembered and swallowed. Amid the detritus of crumpled yen and girls’ addresses in Osaka, finger paintings rendered on photo album sheets, New Age CDs called Music for Brain Waves, we auditioned the early master tapes of volume four of the Harry Smith collection. Fahey mused about how it reflected the Depression despite the set’s upbeat ending. He declared “Last Fair Deal Gone Down” the best Robert Johnson song, and said the Carter Family sounded like they were dead, a zombie chorus. He would sprawl across his bed, enrapt in the ancient sounds, his giant white belly puffed out. Slowly, a guttural moan would be loosed from his depths, a half-drone, half-growl that drowned out the tape.\nOther times, he would play his mixes: collages of Nazi rallies, Balinese gamelan, and recent Chicago blues licks with their verses and choruses mischievously lopped off, rearranging their 12- bar logic. Whether it was blue plate specials, convenience store crap, or world music, all went into his maw. Such devouring and consumption was what Fahey did throughout his career. His repertoire mashed Hammerstein with Dvorak, Christian hymns as well as Hindi chants, Dock Boggs and Duke Ellington. Classic albums like Requia and Days Have Gone By\nfeature the same sort of aural collages he was still spinning 30 years on, as if no time had elapsed. Dislocation isn’t too odd of a sensation, as critic Nat Hentoff recognized: “[Fahey’s] music keeps stirring up old memories and all kinds of new anticipations.”\nI saw four concerts of his that week, but I can’t recall a single tune. What lingers is Fahey’s desire to dig beneath the veneer of the blues, philosophy, industrial noise, classical music, past names and labels, so as to unearth the collective unconscious of the tragic human condition that courses underneath the music. Songs were gateways to more profound, sometimes more horrific, truths. As his rambling online exegesis reveals: “When I play, I very quickly put myself into a light hypnotic trance and compose while playing. . . . I would go so far as to say that I am playing emotions and expressing them in a coherent public language called music.”\nIn his later years, Fahey eschewed the acoustic steel-string altogether; he didn’t even own a guitar, pawning it to make his rent. Due to the effects of Epstein-Barr syndrome and diabetes, his immaculate style slowed. Gone were the ornate five-finger rolls of a one-man orchestra as instead he swamped his tone in delay and reverb, stirring up fuliginous, phantasmal lines that slowly accrued in the air. “There’s something about guitars,” he wrote in How Bluegrass Destroyed My Life, a 2000 collection of his tales, noting that the guitar “evokes past, mysterious, barely conscious sentiments both individual and universal.” At these shows, everyone in the audience would be mesmerized, drawn in by that slow spiral of sound and transported elsewhere. It was like the tornado in The Wizard of Oz, with shards of recognized melodies suddenly separated and reconfigured in the space-time continuum, moving counterclockwise while unlocking the subconscious, spinning like swastikas do.\nI think now of the very first time I saw John perform, under a starry sky a year before our meeting at Mars. His shades affixed in the twilight, he spun out a lugubrious though transcendental waterfall of sound. Staring up into the firmament, I was startled by a melodic line suddenly remembered amid Fahey’s hypnotic whorl. It was “O Holy Night,” a Christmas carol played on that hot July evening. Dislocated in time, it was all the more relevant, its unsung words echoing my own thought: “The stars are brightly shining.”\nAndy Beta is a freelance writer in Brooklyn.', '|Birth Name:||McKinley Morganfield|\n|Birth Date:||4 April 1913|\n|Birth Place:||Issaquena County, Mississippi, United States|\n|Death Place:||Westmont, Illinois, United States|\n|Instrument:||Vocals, guitar, harmonica|\n|Genre:||Blues, Chicago blues, Delta blues|\n|Occupation:||Singer, songwriter, guitarist, bandleader|\n|Label:||Aristocrat, Chess, Testament|\nMuddy Waters grew up on Stovall Plantation, near Clarksdale, Mississippi, and by age seventeen was playing the guitar at parties, emulating local blues artists Son House and Robert Johnson. He was recorded in Mississippi by Alan Lomax for the Library of Congress in 1941.  In 1943, he moved to Chicago with the hope of becoming a full-time professional musician, eventually recording, in 1946, first for Columbia Records and then for Aristocrat Records, a newly formed label run by the brothers Leonard and Phil Chess.\nIn the early 1950s, Muddy Waters and his band—Little Walter Jacobs on harmonica, Jimmy Rogers on guitar, Elgin Evans on drums and Otis Spann on piano—recorded several blues classics, some with bassist and songwriter Willie Dixon, including ""Hoochie Coochie Man"", ""I Just Want to Make Love to You"" and ""I\'m Ready"". In 1958, he traveled to England, helping to lay the foundations of the subsequent blues boom there. His performance at the Newport Jazz Festival in 1960 was recorded and released as his first live album, At Newport 1960.\nMuddy Waters\' influence is tremendous, not just on blues and rhythm and blues but on rock and roll, hard rock, folk music, jazz, and country music. His use of amplification is often cited as the link between Delta blues and rock and roll. \nAlthough in his later years he usually said that he was born in Rolling Fork, Mississippi, in 1915, he was most likely born in Jug\'s Corner, in neighboring Issaquena County in 1913. Recent research has uncovered documentation showing that in the 1930s and 1940s, before his rise to fame, he reported his birth year as 1913 on his marriage license, recording notes and musicians\' union card. A 1955 interview in the Chicago Defender is the earliest claim of 1915 as his year of birth, which he continued to use in interviews from that point onward. The 1920 census lists him as five years old as of March 6, 1920, suggesting that his birth year may have been 1914. The Social Security Death Index, relying on the Social Security card application submitted after his move to Chicago in the mid-1940s, lists him as being born April 4, 1913. His gravestone gives his birth year as 1915.\nHis grandmother, Della Grant, raised him after his mother died shortly following his birth. Grant gave the boy the nickname ""Muddy"" at an early age, because he loved to play in the muddy water of nearby Deer Creek. He later changed it to ""Muddy Water"" and finally ""Muddy Waters"".\nThe shack on Stovall Plantation where Muddy Waters lived in his youth is now at the Delta Blues Museum in Clarksdale, Mississippi. He started playing the harmonica, but by age seventeen he was playing the guitar at parties, emulating two blues artists in particular, Son House and Robert Johnson.\nOn November 20, 1932, Muddy married Mabel Berry. Guitarist Robert Nighthawk played at the wedding, and the party reportedly got so wild the floor fell in. Mabel left Muddy three years later when Muddy\'s first child was born; the child\'s mother was Leola Spain, 16 years old (Leola later used her maiden name, Brown), ""married to a man named Steven"" and ""going with a guy named Tucker"". Leola was the only one of his girlfriends with whom Muddy would stay in touch throughout his life; they never married. By the time he left for Chicago in 1943, he had another wife, Sallie Ann, whom he left behind.\nIn August 1941, Alan Lomax went to Stovall, Mississippi, on behalf of the Library of Congress to record various country blues musicians. ""He brought his stuff down and recorded me right in my house,"" Muddy recalled in Rolling Stone, ""and when he played back the first song I sounded just like anybody\'s records. Man, you don\'t know how I felt that Saturday afternoon when I heard that voice and it was my own voice. Later on he sent me two copies of the pressing and a check for twenty bucks, and I carried that record up to the corner and put it on the jukebox. Just played it and played it and said, \'I can do it, I can do it.\'"" Lomax came back in July 1942 to record Muddy again. Both sessions were eventually released as Down on Stovall\'s Plantation by Testament Records. The complete recordings were reissued on CD as Muddy Waters: The Complete Plantation Recordings. The Historic 1941–42 Library of Congress Field Recordings by Chess Records in 1993 and remastered in 1997.\nIn 1943, Muddy headed to Chicago with the hope of becoming a full-time professional musician. He lived with a relative for a short period while driving a truck and working in a factory by day and performing at night. Big Bill Broonzy, then one of the leading bluesmen in Chicago, helped Muddy break into the competitive market by allowing him to open for his shows in the rowdy clubs. In 1945, Muddy\'s uncle, Joe Grant, gave him his first electric guitar, which enabled him to be heard above the noisy crowds.\nIn 1946, he recorded some songs for Mayo Williams at Columbia Records, but they were not released at the time. Later that year he began recording for Aristocrat Records, a newly formed label run by the brothers Leonard and Phil Chess. In 1947, he played guitar with Sunnyland Slim on piano on the cuts ""Gypsy Woman"" and ""Little Anna Mae."" These were also shelved, but in 1948, ""I Can\'t Be Satisfied"" and ""I Feel Like Going Home"" became big hits, and his popularity in clubs began to take off. Soon after, Aristocrat changed its name to Chess Records, and Muddy\'s signature tune ""Rollin\' Stone"" also became a hit.\nInitially, the Chess brothers would not allow Muddy to use his working band in the recording studio; instead he was provided with a backing bass by Ernest ""Big"" Crawford or by musicians assembled specifically for the recording session, including ""Baby Face"" Leroy Foster and Johnny Jones. Gradually Chess relented, and by September 1953 he was recording with one of the most acclaimed blues groups in history: Little Walter Jacobs on harmonica, Jimmy Rogers on guitar, Elga Edmonds (also known as Elgin Evans) on drums and Otis Spann on piano. The band recorded a series of blues classics during the early 1950s, some with the help of bassist and songwriter Willie Dixon, including ""Hoochie Coochie Man"" (number 8 on the R&B charts), ""I Just Want to Make Love to You"" (number 4), and ""I\'m Ready"". These three were ""the most macho songs in his repertoire,"" wrote Robert Palmer in Rolling Stone. ""Muddy would never have composed anything so unsubtle. But they gave him a succession of showstoppers and an image, which were important for a bluesman trying to break out of the grind of local gigs into national prominence.""\nAlong with his former harmonica player Little Walter Jacobs and recent southern transplant Howlin\' Wolf, Muddy reigned over the early 1950s Chicago blues scene, his band becoming a proving ground for some of the city\'s best blues talent. While Little Walter continued a collaborative relationship long after he left Muddy\'s band in 1952, appearing on most of Muddy\'s classic recordings in the 1950s, Muddy developed a long-running, generally good-natured rivalry with Wolf. The success of Muddy\'s ensemble paved the way for others in his group to break away and make their own solo careers. In 1952 Little Walter left when his single ""Juke"" became a hit, and in 1955 Rogers quit to work exclusively with his own band, which had been a sideline until that time. Although he continued working with Muddy\'s band, Otis Spann enjoyed a solo career and many releases under his own name beginning in the mid-1950s. Around that time, Muddy Waters scored hits with songs ""Mannish Boy"" and ""Sugar Sweet"" in 1955, followed by the R&B hits ""Trouble No More,"" ""Forty Days & Forty Nights"" and ""Don\'t Go No Farther"" in 1956.\nMuddy toured England in 1958 and shocked audiences (whose only previous exposure to blues had come via the acoustic folk blues sounds of acts such as Sonny Terry and Brownie McGhee and Big Bill Broonzy) with his loud, amplified electric guitar and thunderous beat. His performance at the 1960 Newport Jazz Festival, recorded and released as his first live album, At Newport 1960, introduced a new generation to Muddy\'s sound.\nHowever, for the better part of twenty years (following his last big hit, ""I\'m Ready"", in 1956) Muddy was put on the back shelf by Chess and recorded albums with various ""popular"" themes, such as Brass and the Blues and Electric Mud. In 1967, he joined forces with Bo Diddley, Little Walter and Howlin\' Wolf to record the albums Super Blues and The Super Super Blues Band, containing Chess blues standards. In 1972 he went back to England to record The London Muddy Waters Sessions with Rory Gallagher, Steve Winwood, Rick Grech and Mitch Mitchell, but their playing was not up to his standards. ""These boys are top musicians, they can play with me, put the book before \'em and play it, you know,"" he told Guralnick. ""But that ain\'t what I need to sell my people, it ain\'t the Muddy Waters sound. An\' if you change my sound, then you gonna change the whole man.""\nMuddy\'s sound was basically Delta blues electrified, but his use of microtones, in both his vocals and slide playing, made it difficult to duplicate and follow correctly. ""When I play on the stage with my band, I have to get in there with my guitar and try to bring the sound down to me. But no sooner than I quit playing, it goes back to another, different sound. My blues look so simple, so easy to do, but it\'s not. They say my blues is the hardest blues in the world to play.""\nMuddy\'s longtime wife, Geneva, died of cancer on March 15, 1973. A devastated Muddy was taken to a doctor and told to quit smoking, which he did. Gaining custody of some of his ""outside kids"", he moved them into his home, eventually buying a new house in Westmont, Illinois. His firstborn, Larry Williams (né Mud Morganfield), stayed with his mother, Mildred McGhee. Another teenage daughter turned up while Muddy was on tour in New Orleans. Big Bill Morganfield was introduced to his father after a gig in Florida. Florida was also where Muddy met his future wife, the 19-year-old Marva Jean Brooks, whom he nicknamed ""Sunshine"". Eric Clapton served as best man at their wedding in 1979.\nIn 1981, Muddy Waters was invited to perform at ChicagoFest, the city\'s top outdoor music festival. He was joined onstage by Johnny Winter, who had produced Muddy\'s most recent albums, and played classics like ""Mannish Boy,"" ""Trouble No More"" and ""Mojo Working"" to a new generation of fans. This historic performance was made available on DVD in 2009 by Shout! Factory. Later that year, he performed live with the Rolling Stones at the Checkerboard Lounge; a DVD version of the performance was released in 2012.\nIn 1982, declining health dramatically curtailed Muddy\'s performance schedule. His last public performance took place when he sat in with Eric Clapton\'s band at a concert in Florida in the summer of 1982.\nMuddy Waters died in his sleep from heart failure, at his home in Westmont, Illinois, on April 30, 1983. Throngs of blues musicians and fans attended his funeral at Restvale Cemetery, in Alsip, Illinois, to pay tribute to one of the true originals of the art form. ""Muddy was a master of just the right notes,"" John P. Hammond told Guitar World magazine. ""It was profound guitar playing, deep and simple... more country blues transposed to the electric guitar, the kind of playing that enhanced the lyrics, gave profundity to the words themselves.""\nTwo years after his death, Chicago honored him by designating the one-block section between 900 and 1000 E. 43rd Street near his former home on the south side ""Honorary Muddy Waters Drive"". The Chicago suburb of Westmont, where Muddy lived the last decade of his life, named a section of Cass Avenue near his home ""Honorary Muddy Waters Way"". Following his death, fellow blues musician B.B. King told Guitar World, ""It\'s going to be years and years before most people realize how greatly he contributed to American music"".A Mississippi Blues Trail marker has been placed in Clarksdale, Mississippi, by the Mississippi Blues Commission designating the site of Muddy Waters\' cabin.\nHis influence is tremendous, over a range of music genres: blues, rhythm and blues, rock and roll, hard rock, folk music, jazz, and country music. He also helped Chuck Berry get his first record contract.\nHis 1958 tour of England marked possibly the first time amplified, modern urban blues was heard there, although on this tour he was the only one amplified. His backing was provided by the trad jazz group of the Englishman Chris Barber.\nHis use of amplification has been cited as ""the technological missing link between Delta Blues and Rock \'N\' Roll."" This is underlined in a 1968 article in Rolling Stone magazine: “There was a difference between Muddy’s instrumental work and that of House and Johnson, however, and the crucial difference was the result of Waters’ use of the electric guitar on his Aristocrat sides; he had taken up the instrument shortly after moving to Chicago in 1943.”\nThe Rolling Stones named themselves after his 1950 song ""Rollin\' Stone"" (also known as ""Catfish Blues"", which was covered by Jimi Hendrix). Rolling Stone magazine took its name from the same song. Hendrix recalled that ""the first guitar player I was aware of was Muddy Waters. I first heard him as a little boy and it scared me to death"". The band Cream covered ""Rollin\' and Tumblin\'"" on their 1966 debut album, Fresh Cream, as Eric Clapton was a big fan of Muddy Waters when he was growing up, and his music influenced Clapton\'s music career. The song was also covered by Canned Heat at the Monterey Pop Festival and later adapted by Bob Dylan on his album Modern Times. One of Led Zeppelin\'s biggest hits, ""Whole Lotta Love"", is lyrically based on the Muddy Waters hit ""You Need Love"", written by Willie Dixon. Dixon wrote some of Muddy Waters\' most famous songs, including ""I Just Want to Make Love to You"" (a big radio hit for Etta James, as well as the 1970s rock band Foghat), ""Hoochie Coochie Man"", which the Allman Brothers Band famously covered (the song was also covered by Humble Pie and Steppenwolf), ""Trouble No More"" and ""I\'m Ready"". In 1993, Paul Rodgers released the album Muddy Water Blues: A Tribute to Muddy Waters, on which he covered a number of Muddy Waters songs, including ""Louisiana Blues"", ""Rollin\' Stone"", ""Hoochie Coochie Man"" and ""I\'m Ready"" in collaboration with a number of famous guitarists, including Gary Moore, Brian May and Jeff Beck.\nAngus Young, of the rock group AC/DC, has cited Muddy Waters as one of his influences. The AC/DC song title ""You Shook Me All Night Long"" came from lyrics of the Muddy Waters song ""You Shook Me"", written by Willie Dixon and J. B. Lenoir. Earl Hooker first recorded it as an instrumental, which was then overdubbed with vocals by Muddy Waters in 1962. Led Zeppelin also covered it on their debut album.\nMuddy Waters\' songs have been featured in long-time fan Martin Scorsese\'s movies, including The Color of Money, Goodfellas and Casino. Muddy Waters\' 1970s recording of his mid-\'50s hit ""Mannish Boy"" (also known as ""I\'m a Man"") was used in the films Goodfellas, Better Off Dead, Risky Business, and the rockumentary The Last Waltz.\nVan Morrison\'s song ""Cleaning Windows"", on hs album Beautiful Vision (1982), includes the lyric ""Muddy Waters singin\', ""I\'m a Rolling Stone"".\nIn 2008, the actor Jeffrey Wright portrayed Waters in the film Cadillac Records, about Chess Records and its recording artists. Another 2008 film about Leonard Chess and Chess Records, Who Do You Love, also covers Muddy\'s time at Chess Records.\nIn the 2009 film The Boat That Rocked (retitled Pirate Radio in the U.S) about pirate radio in the UK, the cryptic message that late-night DJ Bob gives to Carl to give to Carl\'s mother is ""Muddy Waters rocks.""\nIn 1990, the television series Doogie Howser, M.D. featured an episode called ""Doogie Sings the Blues"" with the main character, Blind Otis Lemon, based on Muddy Waters, with references to his influence on the Rolling Stones and Led Zeppelin, along with the performance of ""Got My Mojo Working"" by Blind Otis Lemon. He is also referred to as the original ""Hoochie Coochie Man"".\nMuddy\'s son Larry ""Mud"" Morganfield is a professional blues singer and musician.\n|Muddy Waters Grammy Award History|\n|1972||Best Ethnic or Traditional Folk Recording||They Call Me Muddy Waters||folk||MCA/Chess||winner|\n|1973||Best Ethnic or Traditional Folk Recording||The London Muddy Waters Session||folk||MCA/Chess||winner|\n|1975||Best Ethnic or Traditional Folk Recording||The Muddy Waters Woodstock Album||folk||MCA/Chess||winner|\n|1978||Best Ethnic or Traditional Folk Recording||Hard Again||folk||Blue Sky||winner|\n|1979||Best Ethnic or Traditional Folk Recording||I\'m Ready||folk||Blue Sky||winner|\n|1980||Best Ethnic or Traditional Folk Recording||Muddy ""Mississippi"" Waters Live||folk||Blue Sky||winner|\n|1954||Hoochie Coochie Man|\n|1957||Got My Mojo Working|\n|Muddy Waters: Blues Music Awards|\n|1994||Reissue Album of the Year||The Complete Plantation Recordings||Winner|\n|1995||Reissue Album of the Year||One More Mile||Winner|\n|2000||Traditional Blues Album of the Year||The Lost Tapes of Muddy Waters||Winner|\n|2002||Historical Blues Album of the Year||Fathers and Sons||Winner|\n|2006||Historical Album of the Year||Hoochie Coochie Man: Complete Chess Recordings, Volume 2, 1952–1958||Winner|\n|1980||Blues Foundation Hall of Fame|\n|1987||Rock and Roll Hall of Fame|\n|1992||Grammy Lifetime Achievement Award|\nU.S. Postage Stamp\nSee main article: Muddy Waters discography.\nWeb site: Mississippi Blues Commission — Blues Trail. Msbluestrail.org. 2008-05-28.']"	['<urn:uuid:0e9aa91c-46f8-4c3e-b7e2-5c33f71e9012>', '<urn:uuid:4fc193a2-7902-41f1-8869-41edc074bfa5>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	18	99	4372
23	I've been struggling with painful Crohn's disease and nothing seems to help. My doctor mentioned something about removing part of my intestines - what kind of relief can I expect from this operation?	Small bowel resection can provide tremendous relief from Crohn's disease symptoms such as severe abdominal pain, abdominal cramping, diarrhea, and life-threatening malnutrition. While this surgery is typically reserved as a last option after lifestyle changes and drug therapy have been tried, most people can expect to recover completely within 6-12 weeks. The primary benefit is improvement in, or complete resolution of, the symptoms associated with Crohn's disease. However, some patients may experience complications including problems with absorbing enough nutrients, which can lead to malnutrition.	['Small bowel resection is a surgical procedure involving the removal of a small portion of the small intestine, or small bowel. Small bowel resection is commonly used to treat conditions such as Crohn’s disease, cancer, intussusception, volvulus, and strictures. It is also sometimes used to treat gastrointestinal bleeding.\n## What is Small Bowel Resection?\nSmall bowel resection is a procedure where a portion of the small intestine is surgically removed. This procedure is typically done to treat medical conditions such as cancer, Crohn’s disease, diverticulitis, intussusception, volvulus, strictures and gastrointestinal bleeding. The length of the section of small bowel removed vary depending on the condition being treated and can range from a few centimeters to several feet.\nDuring small bowel resection, the surgeon will use equipment to carefully cut away the affected portion of the small intestine. The ends of the healthy intestine are then stitched together. This allows for the normal flow of food and nutrients through the remaining intestine and prevents hazardous leakage.\nSometimes, a portion of the affected intestine will be removed and examined in a laboratory to confirm the diagnosis. This procedure is known as an intraoperative biopsy.\n## Benefits of Small Bowel Resection\nThe primary benefit of small bowel resection is improvement in, or complete resolution of, the symptoms associated with the condition that is being treated. Conditions such as Crohn’s disease can cause severe abdominal pain, abdominal cramping, diarrhea, and even life-threatening malnutrition. Removing the affected portion of the small intestine can thus provide tremendous relief from these symptoms.\nIn some cases, small bowel resection can also reduce the risk of long-term complications associated with certain medical conditions. For instance, in cases where small bowel resections are used to treat cancer, it can help to slow the progression of the disease or even reduce the chance of it spreading to other areas in the body.\n## Expected Results After Small Bowel Resection\nMost people who undergo small bowel resection can expect to recover completely within 6-12 weeks. However, the recovery period may be longer for some people. It is important to consult with your health care provider to get an accurate estimation of recovery time.\nThe recovery process typically includes both physical and emotional elements. It is important to take time to rest and to follow the instructions from your health care provider regarding medications, lifestyle changes, and other recommended treatments.\nIt is also important to note that it is normal to experience some pain and discomfort during the recovery process. Your pain should gradually subside over time.\nThe expected outcome of small bowel resection is usually improved, or complete resolution, of the symptom being treated. In the long run, small bowel resection can reduce the risk of long-term complications for some medical conditions.\nIn summary, small bowel resection is a surgical procedure involving the removal of part or all of the small intestine and can be used to treat conditions such as Crohn’s disease, cancer, intussusception, volvulus, and strictures. The benefits of small bowel resection include relief of symptoms associated with the condition being treated and in some cases it can reduce the risk of long-term complications. Following small bowel resection, most people can expect to recover within 6-12 weeks and can expect improved or complete resolution of the symptom being treated.\nDefinition and Overview\nThe small bowel or small intestine is the tube that connects the stomach to the colon (large intestine). It is the part of the digestive system responsible for absorbing nutrients from the food a person eats. The organ is made up of three segments, namely:\nDuodenum – The first and shortest segment that receives partially digested food from the stomach.\nJejunum – The middle segment where most nutrients from food are absorbed.\nIleum – The last segment that intersects with the large intestine.\nAny of these parts can be damaged and if the damage is beyond repair, small bowel resection can be performed. Depending on which part is removed, the procedure is referred to as duodenectomy (removal of all or part of the duodenum), jejunectomy (excision of all or part of the jejunum), or ileectomy (removal of all or part of the ileum).\nWho Should Undergo and Expected Results\nSmall bowel resection is used to treat the following conditions:\nCancer of the small intestine – Malignant tumours can form in the small intestine and cause a blockage. In the majority of cases, doctors elect to remove the section where the cancer is found to treat the condition. The surgery is often followed by chemotherapy or radiation therapy to ensure that no cancer cells remain in the small intestine. Small bowel resection is also used if precancerous polyps and noncancerous tumours are found in the organ.\nCrohn’s disease – Crohn’s disease is a painful and debilitating inflammatory bowel disease that is associated with a number of life-threatening complications. It is initially treated with lifestyle changes and drug therapy with surgery reserved as the last option. If patients are unable to experience symptoms relief with non-invasive treatments, small bowel resection is performed.\nSevere ulcers – Ulcers refer to holes in the mucous membrane of the small bowel. They usually occur due to infections or cancer and often cause severe bleeding.\nAs for the expected results, most patients recover well from surgery while some experience complications including problems with absorbing enough nutrients, which can lead to malnutrition.\nHow is the Procedure Performed?\nPrior to the procedure, the patient undergoes blood, urine, and imaging tests as well as electrocardiogram (EKG). The surgeon will also discuss the details of the surgery, the technique to be used, and possible risks and complications. Preoperative bowel preparations, such as completely cleaning the bowel and taking antibiotics, are standard practice.\nBefore the actual surgery, a nasogastric tube is inserted through the nose. It prevents vomiting and nausea as well as removes gastric secretions during the procedure. A urinary catheter is also used to keep the bladder empty.\nSmall bowel resection can be performed either through traditional open surgery or laparoscopically.\nIn a traditional open surgery, the surgeon makes a long incision in the abdominal wall to access and remove the diseased part of the intestine. The same is the goal of laparoscopic surgery, which uses up to five small incisions where a miniature camera, light, and surgical tools are inserted. The two healthy ends of the resected small intestine are then stapled or sewn together (anastomosis). If this is not possible, the surgeon will create an opening in the belly called a stoma where the end of the resected intestine is attached.\nMany surgeons and patients prefer laparoscopic surgery because it is minimally invasive. Patients typically recover faster because their wounds are much smaller and the interruption to their digestive system is minimal.\nThe procedure, which is often performed under general anaesthesia, can last between one to four hours. Following surgery, patients are transferred to a recovery room until the effects of anaesthesia wear off. They are then moved to a regular hospital room where they typically stay for a week before being discharged.\nPossible Risks and Complications\nMost people who undergo small bowel resection recover fast and return to their normal activities weeks after the procedure. Some, however, suffer from chronic diarrhea and malnutrition as the small intestine’s ability to absorb enough nutrients from food has been compromised.\nSmall bowel resection rarely leads to serious risks and complications. However, a small number of patients experience the following:\nScar tissue formation resulting in the blockage of intestine\nHernia at the incision site\nThe risk of complication is higher in patients who are smokers, malnourished, or suffering from chronic illnesses or have undergone abdominal surgery in the past.\nPatients are discharged about a week following a small bowel resection. They are urged to call their doctor if they develop any signs of infection, persistent abdominal pain, rectal bleeding, chest pain, and urinary symptoms.\nSmall bowel resection. (2015, January). http://www.mountsinai.org/patient-care/health-library/treatments-and-procedures/small-bowel-resection\nBines, J. E., R. G. Taylor, F. Justice, et al. “Influence of Diet Complexity on Intestinal Adaptation Following Massive Small Bowel Resection in a Preclinical Model.” Journal of Gastroenterology and Hepatology 17 (November 2002): 1170–1179.']	['<urn:uuid:5d48af21-3c5c-453c-874e-bb7d65192992>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	33	84	1356
24	Being a technical diver interested in extreme environments, I'd like to know what makes Goodenough Spring particularly challenging for divers and what special equipment features are needed to handle these conditions?	Goodenough Spring presents monumental challenges including great depth (515 feet), enormous water flow, poor visibility, and steep temperature gradients between the spring's 81-degree water and lake temperatures as low as 55-60 degrees in winter. To handle these conditions, specialized equipment like the O2ptima rebreather is needed, which features over-the-shoulder counter-lungs, horizontal scrubber positioning for optimal breathing resistance in strong currents, and a robust stainless steel frame with 600-denier polyester cover to protect against tight cave squeezes.	['Deep, Dark and Dangerous\nThe nation’s deepest known underwater cave system lures expert divers.\nBy Rae Nadler-Olenick\nPositioned on the choppy surface of Lake Amistad, at 29° 32.21’ N, 101° 15.18’ W in Val Verde County, a simple white and blue buoy marks the location of a geographic feature few will ever see. The mouth to Goodenough Spring lies some 165 feet below the reservoir’s normal pool level, beckoning the way to an environment so harsh that most divers — even the highly experienced — turn back.\nBut not all. For the past decade, the seasoned diver-explorers of the Goodenough Springs Exploration Project (GSEP) have been plumbing its secrets, gradually advancing farther and deeper into the United States’ deepest known underwater cave system.\nThe system, which emerges from the Salmon Peak layer of Southwest Texas’ Edwardian limestone, possesses both faulting and erosion channel features.\n“It’s a unique geology,” says Chuck Noe, the project’s director.\nBefore Amistad Dam’s 1968 completion submerged 65,000 acres of desert beneath waters from the Rio Grande, Pecos and Devils rivers, Goodenough Spring flowed at the surface, a welcome resource in an arid land. Situated 12 miles northwest of Del Rio, it discharged an impressive 100-200 cubic feet of water per second under artesian pressure, making it Texas’ third-largest spring.\nWith the sprawling new lake, the spring became obscured, and the lake became a recreational destination, drawing boaters, fishermen and sport divers. In the earliest years, most nonprofessional divers limited themselves to depths of 130 feet because of decompression and nitrogen intoxication issues, so the spring remained largely out of reach.\nDuring the 1980s, the scuba community turned increasing attention to deeper, more complex dives, adopting advanced techniques previously reserved for elite Navy and commercial divers. In particular, the use of gas mixtures like trimix (oxygen, helium, nitrogen) allowed them to dive deeper and stay longer. By the early 1990s, divers trained in mixed-gas use — known as technical divers — had routine access to the cave.\nR.D. Milhollin of Dallas was among those who explored the area during that period. He and his companions — Ise Kalsi, Terry Scoggins and, later, Robert Laird — reached the cave’s main passage, where a torrent of fast-rushing water greeted them. They set out to conquer a 90-foot-long section of tunnel, known today as the Fire Hydrant, using mountain climbing gear. Wedging chocks tightly into available cracks in the ceiling, they tied climbing rope to them and climbed horizontally, upside down, against the fierce flow.\nThat wasn’t their only approach: On one memorable occasion, Milhollin and Laird proceeded through a combination of maneuvers that included successively tossing and anchoring a length of heavy pig iron pipe, a few inches at a time, along the gravel floor.\nTheir efforts ended at a point, 200 feet deep under normal pool conditions, where piled-up gravel blocked most of the tunnel, preventing further access. Milhollin’s team ceased its explorations around 1995.\nLaird continued to visit the cave, and in 2000 he stepped up his involvement when a group of Houston-area divers asked him to help them locate the spring. His interest rekindled, he joined with them in launching GSEP to explore the site systematically.\nLaird calls Goodenough “probably the most difficult, dangerous dive there is.”\nThe spring’s challenges are in fact monumental. In addition to its great depth, enormous flow and the often-poor visibility of the surrounding lake water, a steep gradient between the spring’s year-round 81-degree water and ambient lake temperatures as low as 55 to 60 degrees in winter severely limits cold weather diving activity.\nGSEP’s 11-year history unfolds as one of tireless work and steady, gradual progress punctuated by dramatic breakthroughs. The “Goodenoughers” devoted earlier efforts to such basics as the installation and maintenance of sturdy chain moorings and tough, knotted nylon guide lines in the previously explored areas. Only in 2002 did they make their first coordinated effort to get past the gravel barrier by digging gravel away to enlarge the opening. Their excavations eventually yielded a hole barely large enough for Noe, the smallest team member, to squeeze through into the unknown.\n“My heart was pounding,” admits Noe, who returned quickly after installing a steel piton in a wall crack to extend the guide line to the other side. On a second brief trip the next day — still alone — he glimpsed a steeply dropping passage: one hint (along with the warm temperature) of the cave’s great depth potential. But it would be more than a year before anyone followed him beyond the gravel dam.\nIn the summer of 2003, nature intervened to accomplish what long hours of laborious excavation had not. Waters stirred by Hurricane Claudette’s landfall on the South Texas coast, roaring through the spring system, blew most of the gravel away.\nSince then, GSEP divers have explored deeper into the cave in the course of several trips each year between May and October, ultimately achieving a depth of 515 feet.\nThey added a scientific dimension to their mission in 2005 when water scientist Ray Kamps, then a Texas State University doctoral student, sought their assistance. Kamps, who was conducting federally sanctioned limnology research at Lake Amistad, needed divers to deploy instrumentation to measure water temperature and salinity at the spring.\nIt proved a fortuitous collaboration. Exploration and science have worked hand in hand ever since. GSEP has expanded its research role, taking on projects in connection with the Texas Water Development Board and the U.S. Geological Service, and publishing an article on the chemistry of Goodenough Spring in the peer-reviewed Hydrogeology Journal.\nThey also pay for their own independent studies, like their ongoing effort to delineate the spring’s catchment area. Kamps, who soon joined GSEP as its scientist of record, believes the catchment is a large semicircle around the spring.\n“That’s the first assumption,” he says. “The question is: How big is the circle? Is it skewed? If it rains over there, does the water end up in Goodenough Spring here?”\nTo learn the answer, he analyzes the data he collects for spikes in three parameters — pressure, salinity and temperature — that occur with rainfall. “They travel at different rates through the aquifer,” he explains. “If they all come at the same time, water fell close to the spring.”\nTime differences between the spikes increase with distance from the rain source. He then matches his readings to radar weather maps in an effort to identify which thundershower produced a given spike or spikes. Though the work is far from complete even after four years, results to date suggest that the catchment area is a wide swath of plateau land located on both sides of the border: north, sweeping west and ending south of the spring, with the biggest response from the west.\nExploration — safely advancing members’ individual goals in a context of close teamwork — remains the heart of the enterprise. In 2009, GSEP received the National Speleological Society Cave Diving Section’s coveted Exploration Award for achieving the deepest cave dive ever documented in the United States.\n“When we started this project, we didn’t even know that the cave went deeper than 100 feet. We just wanted to explore it,” says Noe, who, with extensive logistics support from six other Goodenoughers, descended to a record-breaking depth of 515 feet.\nStaging for each trip is a highly complex production. A typical expedition finds GSEP’s two dive boats fully loaded with several tons of equipment: tanks of varying gas mixtures, wetsuits and other personal gear, maintenance supplies and scientific instruments — plus the seven or eight divers (about half the current active membership) themselves. Depending on need, they might also haul such specialty items as a wetsuit-heating unit Noe himself constructed for dives deeper than 250 feet at chilly temperatures.\nThe redundancy of tanks and the heating equipment are necessary for decompression issues. During a typical deep dive, nitrogen dissolves in the bloodstream — nitrogen that must be released very gradually on ascent if the diver is to avoid a life-threatening pressure illness known as the bends.\nAscending divers make multiple progressively longer decompression stops at progressively shallower levels. They also change tanks twice along the way (to a 50-50 oxygen/nitrogen mix at 70 feet and pure oxygen at 20 feet), clipping on new gas cylinders that setup crew have previously tethered to the mooring chain in the appropriate spot. The longest decompression stop — sometimes lasting several hours — occurs at the 20-foot level where, in cold weather, hot water pumped through a hose from Noe’s propane-fired Deco Heater on deck makes the wait more comfortable.\nWhen the lake is at normal pool level, a Goodenough Spring dive begins with a 150-foot descent to an amphitheater-like area. Fifteen feet farther down, GSEP’s Grim Reaper sign, installed to warn the unwary away, marks the cave’s entrance: a 6-foot-wide fissure that quickly opens into a round, high-ceilinged room called the Well. To the left, a trip through the formidable, scalloped Fire Hydrant tunnel and past the former gravel restriction reveals a geography of sharp, deep drops and winding switchbacks; of mud and boulders and gravel-filled pit bottoms; and clay walls deeply fluted (possibly from the action of churning gravel during storms). And no end in sight.\nWill they push on to 600 feet?\n“We’d like to,” says Noe. “It’s just difficult to say when. We can plan for it and talk about it, but until Mother Nature relents and gives us an opportunity, we can’t pick a date.”\nIndeed, nature has been merciless since Hurricane Alex ripped the Texas coast in June 2010. GSEP’s expedition in October of that year found guide lines ripped out, scientific instruments damaged or gone and a flow so high that divers were unable to proceed past the Well. Last May’s group encountered conditions nearly as extreme, though they were able to proceed several feet into the Fire Hydrant to set new instruments.\nBut repair and maintenance, planning and, sometimes, waiting are all part of the game.\nThe next major push — when it comes — will make use of scooters: neutrally buoyant, battery-powered, propeller-driven propulsion units that can pull divers along at more than 200 feet per minute under ideal conditions. Divers swimming hard at that depth require the mechanical assistance to avoid joint injuries caused by gas buildup in their tissues.\n“I’ll leave that to the younger guys,” says Noe, who’s 52.\nShould they ever descend 800 feet or deeper, they’ll need to adopt rebreathers — a completely different and highly sophisticated breathing technology that recycles used air. But they don’t think the cave will turn out to be that deep.\nKamps expects it to bottom out at around 700 feet. “The water temperature of Goodenough Spring is a very consistent 27 degrees Celsius (81 degrees Fahrenheit),” he says. “That’s the temperature of water in the earth at 700 feet. I’d expect it to level off there. It can continue straight for miles, or it could lose its character as a conduit and become diffuse, but it won’t go deeper.”\nIn any case, the Goodenoughers have plenty to look forward to in the way of future exploration, science and maintenance. Though they also dive at Jacob’s Well in Wimberley, Goodenough Spring remains their first love.\n“We’ve sort of made it our project and our life,” Noe says.\nFor more information on the exploration project, visit www.goodenoughsprings.org.', 'Diver and Instructor training available\nThe O2ptima rebreather training is a fully closed circuit, constant PO2, electronically-driven rebreather with built-in decompression ability. Unique in the rebreather marketplace, the O2ptima is specifically designed for the rigors ofcave diving where immediate surfacing is not possible. The resulting features are also highly desirable for wreck and deep ocean divers.\nThe O2ptima’s design includes minimal risk of a caustic cocktail. Two water traps in the loop along with horizontal, behind-the-head positioning of the scrubber canister create a difficult path for water to travel. The O2ptima also features a packable scrubber and Shearwater Petrel electronics. Both of these features represent major improvements in this CCR.\nThe O2ptima delivers excellent work-of-breathing that allows divers faced with strong flow or ripping current to breathe comfortably. Over-the-shoulder counter-lungs along with the scrubber positioned behind the head means gas has less distance to travel, so the diver does not have to work as hard to “pull” gas through the loop. This also allows the O2ptima to have smaller diameter loop hoses, which minimizes jaw fatigue.\nA stainless steel frame and Rhinotec 600-denier polyester cover protects the O2ptima from tight cave or wreck squeezes and rolling boats. Hoses, regulators and tanks are kept neatly streamlined and away from potential snags inside a wreck or cave.\nThe IANTD Optima CCR Air Diluent Course.\n1. Must be an IANTD Advanced EANx or Advanced Recreational Trimix Diver or equivalent. (May be taken in conjunction.)\n2. Must be a minimum of 17 years of age.\n1. All lectures completed with IANTD Course-specific Slides pertaining to the theory in the IANTD CCR-specific Diver Student\n2. This Program must include a confined water session, followed by 6 OW dives with a minimum of 500 minutes of in-water\ntraining time using the specific Rebreather on which they are being trained. If qualified Recreational CCR diver, the total inwater\ntraining time is decreased to 400 minutes and the dives are decreased to a minimum of four (4) OW dives.\n3. Students must complete the text with the units on which they wish to be qualified.\n4. Students must pass the specific CCR test with a minimum score of 80%.\n5. On a minimum of four (4) dives the student must carry a single stage cylinder or other bailout gas adequate to safely ascend.\n6. Three (3) dives must be a minimum of or deeper than 50 fsw (15 msw) of which one (1) of these three (3) dives must be a\nminimum of 100 fsw (30 msw).\n7. To qualify from one Closed Circuit Rebreather to another Closed Circuit Rebreather, a diver must have 12 CCR dives of which\none (1) must have been within 45 days of the program on the new CCR and must complete a minimum of 200 minutes of inwater\ntraining with at least two (2) OW dives.\n8. To qualify from a Semi-Closed Circuit Rebreather to a Closed Circuit Rebreather, a diver with 20 or more SCR dives must\ncomplete a minimum of 400 minutes of in-water training in a combination of confined water and OW environments, with at least\n6 OW dives. Divers with less than 20 SCR hours must complete the entire course.']	['<urn:uuid:f383aeb7-c12f-4389-9d3b-78a248d3c885>', '<urn:uuid:bcb62cc7-dfc4-414e-bc75-13c912bee45d>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:10:11.825225	31	76	2429
25	How does moss survive without roots, and where does it grow best?	Moss is a rootless plant that gets water and nutrients through its leaves using minute ducts that open when moistened. It prefers shady areas with acidic soil (pH 5.0-5.5), and can grow on various surfaces including logs, rocks, and concrete. While it needs moisture to grow, it can become dormant without water and reactivate when moistened again.	"['WHAT, WHY, HOW\nWHAT, WHY, HOW\nMoss is an indicator of your lawn’s health – the more moss, the poorer the health. So, if you have a moss problem, as well as addressing it NOW you need to improve your routine lawn care to discourage it in the future. And even if you enjoy the springy feel it adds to your lawn, it is a warning sign that all is not well, and you need to investigate your lawn conditions.\nWhat is lawn moss?\nMoss is a rootless plant that derives water and nutrients through its leaves. There are several types of lawn moss, usually loose, yellowish-green tufts between the grass, but sometimes densely matted tufts.\nHow does moss reach my lawn?\nMoss spreads by releasing spores that travel in the wind. If they land on exposed lawn thatch they bed in and then, when it is damp, they germinate.\nWhen is the biggest risk of moss?\nMoss needs moist conditions to grow, so September to March are the worst months, but with changing weather patterns, ideal moss conditions can occur at any time of year, especially if the thatch prevents good drainage or rain prevents regular mowing.\nHow do I discourage moss?\nBasically, you need to maintain good basic lawn care. A healthy lawn offers neither the space nor the conditions for moss to thrive. Here are four key areas to work on to discourage or prevent moss:\n- Drainage: Moss needs a moisture-retentive surface. You can maintain good drainage through routine scarification and aeration, as well as correct mowing.\n- Nutrition: Poorly fed grass thins out and lets moss spores land on the thatch. Once established, the moss competes with the grass for nutrients, leading to further grass loss and more space for the moss to fill. Properly-fed grass is too thick to allow moss room to thrive.\n- Thatch: Moss loves a moisture-retentive thick layer of thatch. So, use routine scarification to keep the thatch under control.\n- Mowing: Cutting the grass too short will open up the thatch to moss spores. Also, an exposed thatch, in both wet and arid conditions, worsens drainage, thereby eventually creating that moist environment the moss loves so much.\nSome people blame shade for their moss, but often the underlying reason for moss in shady areas is the condition of the lawn itself. So before chopping down bushes or tree branches, check whether the lawn can be improved first.\nHow do I kill moss?\nYou need to use ferrous sulphate, but BEFORE you do this, scarify the lawn. This ensures that the moss control reaches the base of the moss plants, rather than just the upper part, and kills it completely. Apply moss control as a liquid application not a granular or feed ‘n’ weed product; this reaches the moss more evenly and kills it without blackening the grass (if it does blacken, you have simply used too much or have not used enough water).\nHow do I remove dead moss from the lawn?\nFor small areas you can use a wire rake. But on larger areas, another scarification is the best way to remove the moss (see photo above). Your aim is to gently tease the moss away from the surface without disturbing the grass too much.', ""If you want a green lawn in\nthe shade, grass is not the answer.\nGroundcovers fill in the borders but are not very barefoot\nfriendly. Consider the lowly moss. It is green, lush, soft, and squishy\nunderfoot. It makes an ideal shade\ngarden lawn or pathway.\nReally. There is no\nmaintenance to speak of, no mowing, no fertilizing. Occasional raking and weeding replaces these weekly\nchores. Mark Dwyer, from the\nRotary Gardens in Wisconsin explains the wonders and uses of moss in the home\n---Anne K. Moore, May 29,\nLow-Maintenance Option for Shade\nby Mark Dwyer, Rotary Gardens Wisconsin\nPhotos by Anne K Moore\npeople have that shady patch in their yard where they can't grow grass but moss\nis abundant. Products and\ntechniques are promoted for the intent purpose of eradicating this primitive\nmember of the plant kingdom.\nBefore you eradicate moss, consider its potential in the landscape.\nevolved over 390 million years ago and there are currently over 15,000\nvarieties of moss worldwide.\nWisconsin has close to 400 native species of moss and the odds are that\nyou have some of these on your property as well. Mosses have been used for thousands of years in Japanese\ngardens for the effects of adding serenity and timeless beauty. Public moss gardens exist throughout\nthe United States although some of the best can be found in the Pacific\nNorthwest. Many gardeners have\nutilized sphagnum moss as a soil amendment or to line hanging baskets. However, bogs around the world have\nbeen drained and depleted of their sphagnum moss to satiate gardening\ndemand. Consider other materials\nthat accomplish the same goals.\nThese tiny plants don't produce flowers and\ndon't have actual roots that draw up any nutrients or water. Mosses have minute ducts that open when\nmoistened and become dormant with the lack of moisture and then become active\nwhen moistened. Some mosses can\nremain dormant for years until adequately moistened. Mosses do have chlorophyll and do photosynthesize regardless\nof the temperature (hence there green color even in winter). Mosses carry no diseases and despite\npopular belief, are not parasites on other plants.\nOnce established, mosses\ncan be a wonderful component of your shady areas. If you can't grow grass in a particular area, there is a\nreason for that. Learn from your\nfrustrations and go with the resilient, virtually maintenance-free option of\nmosses. Whether it's a moss lawn\nor a composition of different mosses used in tandem with your other shade\ngarden plants, the decision to incorporate mosses into your landscape can be\nEstablishing mosses is the most important\nstep in moss gardening. Observe\nmosses where they are currently growing on your property or in natural areas to\nget an idea of their preferred habitats.\nThe majority of mosses prefer no direct sunlight and prefer an acidic\nsoil (pH between 5.0 and 5.5), although they will grow in a wide range of soil\ntypes. As they have no true roots,\nmosses will grow on logs, rocks and even on concrete or clay planters. There are essentially two popular ways\nof establishing moss in your landscape.\nThe ideal moss planting time is from late\nMarch until mid-June and from September to November. The summer months are simply too hot and dry to successfully\nestablish moss although sufficient moisture is the primary consideration. Transplanting patches of moss is the\nmost popular method. Collecting in\nearly spring immediately after a rain will increase your success, as they are\neasier to loosen and have maximum moisture content. It is important to stress that moss should never be\ncollected from public areas, natural areas, state forests, etc., as they are an\nimportant part of those ecosystems.\nWhen collecting from acceptable areas, never take more than a small 3 inch\nby 3 inch piece from each square foot of moss. Lightly loosen these patches and keep them damp. Never roll up moss patches. Collecting in this ''patchwork'' fashion\nwill allow the remaining moss to colonize open areas readily.\nBefore planting moss ''patches'' in the\ngarden, create a clear, bare, surface free of leaves, weeds and other\ndebris. You may need or want to\nacidify the soil to promote optimum pH for establishing mosses. The soil should be tamped down and\ncompacted (not loosened) before planting.\nGently scratch this area with a rake and spray area until it is\ndamp. Moisten the bottom of\ncollected moss fragments and gently press these on the selected site, firmly\nenough to remove air pockets under the patch. Use small sticks to secure patches to slopes. Mist these patches with water from a\nspray bottle twice daily for three weeks.\nMist as needed through the growing season to keep moss slightly damp. These patches will double in size\nwithin one year or so. Keep your\nmoss garden free of fallen leaves and debris and limit foot traffic to stepping-stones\nor paths. Moss is not resilient to\nrepeated foot traffic or disturbance.\nAnother method of moss establishment is\ntypically used when moss is desired on rocks or containers. Take moss patches and remove all dirt\nor rock particles from the underside.\nMix a handful of moss with 1 cup of buttermilk in the blender for two\nminutes. This ''moss slurry'' can\nthen be smeared or painted over the desired area. The ability of moss to regenerate cell by cell will allow it\nto grow from this concoction.\nContinue to mist twice daily until you see a fuzzy haze of green\nindicating new growth. The\nappearance of moss on containers or rocks lends an ''antique'' feeling to the\nmosses don't achieve in height and floral display in your landscape they make\nup in durability and low-maintenance once established. No longer the bane of shady turf areas\nunder trees, mosses are a wonderful component of the home landscape and their\npotential is just starting to become realized. The next time you are in the woods, enjoy this member of\nplant kingdom and appreciate that what it has been doing for our ecosystems and\nlandscapes for millions of years can be easily enjoyed at home.\nGardens has a Fern & Moss Garden with native Wisconsin mosses and over 250\nvarieties and species of ferns from around the world.\nthe Rotary Gardens, Show 22-809\nand related GardenSMART video tip""]"	['<urn:uuid:48c7c3ce-f176-440c-b5f0-a61aa2ac6d56>', '<urn:uuid:602a2c99-dbe2-4a1a-bcd5-cab00906d465>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	12	57	1590
26	I'm studying climate adaptations in trees and noticed that temperature changes with elevation affect both Giant Sequoias and high-elevation trees in Utah - what are the specific temperature variations with elevation changes in both regions?	In the Sierra Nevada Mountains, temperature decreases by 3.6°F for every 1,000 feet of elevation gain, while in Utah the temperature drops by 5 degrees F per 1,000 foot increase in elevation. This difference in temperature gradients affects tree growth and survival in both regions.	"['Many things in nature are adapted to certain climate conditions. That said, some are choosier than others. Giant Sequoia trees, the largest living organisms on the planet, need a “Goldilocks” type of environment to grow and reproduce. Visiting Sequoia National Park recently, I had the opportunity to learn more about the climate requirements of these amazing trees.\nNeeding conditions that are not too hot and not too cold, as well as not too wet and not too dry, Giant Sequoias grow naturally in only one place on Earth. That unique place is a narrow band about 70-miles long on the western slopes of California’s Sierra Nevada Mountains known as the Sequoia Belt. According to the National Park Service (NPS), these trees only grow at elevations between 5,000 and 7,500 feet. Temperatures above 7,500 feet are usually too cold and conditions below 5,000 feet are too dry. Within this limited range, about 75 groves reveal where conditions for the massive trees are just right.\nThese ideal conditions are produced by a combination of weather and topography. When moisture-laden air from the Pacific runs into the Sierras, it rises and cools. On average, it cools about 3.6°F for every 1000 feet it rises. Since cooler air holds less moisture than warm air, the moisture is dropped as rain and snow over the mountains with precipitation amounts generally increasing with elevation. While California is in the midst of a multi-year drought, the NPS says the area around Giant Forest (elevation 6400 feet) usually gets an average of 44 inches of precipitation a year. It also typically sees only one day a year with temperatures below 10°F.\nCompared to other sites within a five mile radius, the average conditions in Giant Forest are perfect for the giant sequoias. Upslope, Emerald Lake (elevation 9200 feet) gets 59 inches in average annual precipitation and sees around ten days a year with temperatures below 10°F. Downslope, Ash Mountain (elevation 1700 feet), only sees about 26 inches of precipitation a year and the temperature reportedly never falls below 10°F.\nGiven the narrow natural range in which these giant trees are able to thrive, they face serious challenges from climate change. As temperatures increase, more precipitation is coming down in the form of rain instead of snow. This reduces the snowpack and the subsequent spring and summer melt water available to the trees during the region’s dry season. While the resilient mature sequoias – some are over 3000 years old – are not in immediate danger, researchers say these big trees were not made to withstand decades of drought. The younger trees – seedlings and saplings – on the other hand, face a more difficult struggle for survival. The drier conditions make it harder for them to develop robust root systems and also leave them more susceptible to wildfires, which are projected to increase.\nGiven the impressive age of some of these trees, they must have endured natural climate fluctuations in the past. This time, however, the human caused change is happening very quickly and is forecast to produce conditions unfamiliar to even these ancient giants. According to the US National Climate Assessment, the southwest – which includes California – is normally the hottest and driest part of the country, but climate change is making it even more so. The report shows that 2001 through 2010 was the region’s warmest decade on record with temperatures almost 2°F above historic averages. Looking ahead, it projects continued increases in average annual temperature and a decrease in precipitation. In the meantime, scientists continue to monitor and research the giant sequoias to better understand how they will react to our changing climate and to offer informed recommendations to evolving conservation strategies.\nBelow is a short video by The Redwoods and Climate Change Initiative (RCCI) on the impact of extended drought on Giant Sequoias. Oh, and in case you were wondering, Giant Sequoias (sequoiadendron giganteum) and Coastal Redwoods (sequoia sempervirens) are closely related, but are two different species.', ""028 - Selecting Trees for High Elevations\nThis fact sheet describes the importance of careful species selection when planning and planting cultivated landscapes in unique, high elevation conditions in Utah. We recommend native and non-native tree species that can stand the stresses of living above 5,000 feet.\nUtah is home to some of the highest peaks in the country, and it is the third highest state in the country, behind Colorado and Wyoming. Though most Utahns live in the low valleys where year-round living is easiest (in terms of snowfall), many people want to live at high elevations where they can enjoy recreation, wildlife, and scenery that is easily accessible. These high elevation inhabitants may wish to landscape the property surrounding their primary residence while others may have trees they need to care for at a cabin or summer home site in the mountains. This factsheet can help homeowners select, cultivate, and care for the trees suited to these high elevation places.\nWhy is Elevation Important?\nIn this fact sheet, the term high elevation will refer to elevations above 5,000 feet. High elevation sites are both good and bad for trees. They have shorter growing seasons because the frost-free period is shorter. Temperatures decrease 5 degrees (F) for every 1,000 foot increase in elevation. Situated at 7,000 feet and higher, Park City can be 20 degrees cooler on average than Salt Lake City at 4,200 feet. Also there is an average 1 degree decrease in temperature for every degree farther north in latitude at the same elevation.\nAn example of the varied terrain found in Morgan County, Utah. Red and yellow colors indicate high elevation terrain.\nJapanese tree lilac is one recommended species for high elevation planting sites in Utah.\nElevation: A Mixed Bag for Trees:\nBesides being cooler and having a shorter growing season, elevation has several other important effects on trees, both positive and negative.\nPositive effects of higher elevation on trees are nearly all water related, and include:\n- Increased precipitation, water availability and organic matter in soil.\n- Decreased evapotranspiration and generally decreased stress.\n- Less temperature extremes which may lead to lower stress overall.\n- Slower snow melt due to lower temperatures, which increases soil moisture retention later into summer.\n- Higher elevation sites tend not to have the high soil pH and salinity problems that low elevation sites have.\nPonderosa pine (Pinus ponderosa). Photo courtesy Flickr user: bllg-water, Matt Lavin, Stuart Rankin and USU Forestry Extension\nNegative effects of higher elevation on trees include:\n- Extreme low temperatures during the winter can cause dieback in any tree and loss of or damage to foliage in evergreens.\n- Variation in the timing of the last frost in spring, or the first frost in fall, can make it difficult to select species that are appropriate for such areas over the long term. Published hardiness zones may not be accurate for this reason. For example, a late frost in early summer can severely damage trees that have leafed out, even native trees. An early frost in the fall can damage trees that have not yet gone dormant.\n- Fluctuating frost times and lower ‘high’ temperatures also shorten the growing season so plants grow more slowly over the years.\n- Slower snow melt can mean lower water availability where soils stay frozen into the growing season.\n- Heavy snow loads at high elevations can cause bending and breaking of branches or even trunks for weak species and cultivars.\nAspect is especially important for trees growing at high elevation and where there are steep slopes. Aspect is the direction a slope faces. Aspect is important because (in the northern hemisphere) more solar radiation is available on south-facing slopes than north-facing slopes. South-facing slopes will have earlier snow melt, lower soil moisture, and lower humidities than nearby north-facing slopes, and plants will be warmer so they will lose water more quickly. Northfacing slopes have the opposite characteristics.\nIn addition, east facing slopes on average get the same amount of sun as west facing slopes, but they get it early in the day when the site is still cool from the night. West facing slopes get their sun late in the day when temperatures are higher, so they are typically drier sites. In general you can think of east to north facing slopes as being the coolest and wettest and south to west facing slopes as being the warmest and driest. However, though a northeast slope may have more moisture, its coolness may mean earlier frost and soils that stay frozen longer, so plants may grow slower.\nTree Selection for High Elevation\nWhat is a microsite?\nA microsite is a small section of your landscape that has unique features, conditions or characteristics that may differ from a microsite that is very close by. These characteristics may include different elevations, slope steepness, tendency to freeze before or after other areas nearby, humidity, sunlight, water availability, wind, soil characteristics, vegetation composition, and shade. Temperature-related microsite features become very important when selecting landscape plants.\nChoosing the right tree for your high elevation site can mean the difference between planting success and failure. This doesn't just mean choosing the right tree for your general area, but it also means choosing the right tree for a specific place in your landscape, or a particular microsite (see microsite box below). Temperature and frost-related microsite factors are the most critical for tree survival. If you are trying a species that is marginally cold hardy for your site, put it in an area that gets cold and stays cold in winter. For example place it on the north side of a building or somewhere else where there isn’t much winter sun. If a tree is relatively cold hardy but you are unsure whether it will survive the harsh winter, plant it close to a building on either the south or west side to take advantage of the building’s thermal mass. This will help the tree get through the coldest nights. Trees being planted for their flowers (and fruit) should usually go on the warmest microsites you have. Otherwise the tree may survive, but the flowers may be killed in the bud by extremely low temperatures or by late frosts.\nIn most cases it make sense to select species that have a moderate to slow growth rate versus species that grow rapidly. Fast growing trees tend to be weak-wooded and more vulnerable to insects and diseases. At high elevations in Utah we recommend selecting species suitable for USDA Zone 4b or lower. In the USU Tree Browser there are 137 species that are recommended. for Zone 4b or lower; 26 of these species are native to Utah. Of course, as mentioned previously, the hardiness zone does not take into account the effect of unusually late or early frosts on a tree. A good way to tell is to walk through your neighborhood and observe what trees have survived nearby. It’s hard to argue with success!\n|Scientific Name||Common Name||Hardiness Zones|\n|Abies concolor||Fir, White ◊||(3-7)|\n|Abies lasiocarpa||Fir, Subalpine (cool sites only) ◊||(4-6)|\n|Juniperus chinensis||Juniper, Chinese||(4-9)|\n|Juniperus scopulorum||Juniper, Rocky Mountain ◊||(3-7)|\n|Juniperus virginiana||Redcedar, Eastern||(2-9)|\n|Larix decidua||Larch, European||(2-6)|\n|Larix kaempferi||Larch, Japanese||(4-6)|\n|Picea abies||Spruce, Norway||(2-7)|\n|Picea engelmannii||Spruce, Engelmann ◊||(2-5)|\n|Picea glauca||Spruce, White||(2-7)|\n|Picea omorika||Spruce, Serbian||(4-7)|\n|Picea pungens||Spruce, Blue ◊||(2-7)|\n|Pinus aristata (longaeva)||Pine, Bristlecone ◊||(5-7)|\n|Pinus cembra||Pine, Swiss Stone||(3-7)|\n|Pinus flexilis ‘Vanderwolf’s Pyramid’||Pine, Vanderwolf’s Pyramid Limber ◊||(4-7)|\n|Pinus heldreichii||Pine, Bosnian||(5-7)|\n|Pinus nigra||Pine, Austrian||(4-7)|\n|Pinus ponderosa||Pine, Ponderosa ◊||(3-7)|\n|Pinus sylvestris||Pine, Scotch||(2-8)|\n|Pseudostuga menziesii var. glauca||Fir, Douglas (glauca variety) ◊||(4-6)|\n|Acer ginnala||Maple, Ginnala or Amur||(3-8)|\n|Acer glabrum||Maple, Rocky Mountain (5,000 to 12,700 ft.) ◊||(4-7)|\n|Acer grandidentatum||Maple, Bigtooth (4,200 to 9,400 ft.) ◊||(4-7)|\n|Acer tataricum||Maple, Tatarian||(3-8)|\n|Amelanchier species||Serviceberries ◊||(3-8)|\n|Betula occidentalis||Birch, Western Water (3,000 to 10,000 ft.) ◊||(3-7)|\n|Cercocarpus ledifolius||Mountain-mahogany, Curlleaf ◊||(3-8)|\n|Crataegus douglasii||Hawthorn, Douglas ◊||(3-9)|\n|Crataegus phaenopyrum||Hawthorn, Washington||(3-8)|\n|Gymnocladus dioicus||Coffeetree, Kentucky||(3-8)|\n|Ostrya virginiana||Hophornbeam, Eastern||(4-9)|\n|Populus tremula||Aspen, Swedish||(2-7)|\n|Populus tremuloides||Aspen, Quaking ◊||(3-7)|\n|Prunus maackii||Chokecherry, Amur||(3-6)|\n|Prunus virginiana||Chokecherry, Canada Red ◊||(2-6)|\n|Quercus macrocarpa||Oak, Bur||(2-8)|\n|Quercus gambelii||Oak, Gambel ◊||(4-8)|\n|Sorbus scopulina||Mountain-ash, Greene ◊||(2-5)|\n|Syringa reticulata||Lilac, Japanese Tree||(3-8)|\n|Tilia cordata||Linden, Littleleaf European||(3-7)|\n|◊ = Utah Native|\n- Visit the USU Tree Browser website for photos, hardiness zones and identification information for over 240 tree species.\n- Hardiness zone table from USU Forestry.\nUtah State University is committed to providing an environment free from harassment and other forms of illegal discrimination based on race, color, religion, sex, national origin, age (40 and older), disability, and veteran’s status. USU’s policy also prohibits discrimination on the basis of sexual orientation in employment and academic related practices and decisions. Utah State University employees and students cannot, because of race, color, religion, sex, national origin, age, disability, or veteran’s status, refuse to hire; discharge; promote; demote; terminate; discriminate in compensation; or discriminate regarding terms, privileges, or conditions of employment, against any person otherwise qualified. Employees and students also cannot discriminate in the classroom, residence halls, or in on/ off campus, USU-sponsored events and activities. This publication is issued in furtherance of Cooperative Extension Work, Acts of May 8 and June 30, 1914, in cooperation with the U. S. Department of Agriculture, Kenneth L. White, Vice President for Extension and Agriculture, Utah State University. Peer Reviewed.\nPublished August 2015.""]"	['<urn:uuid:55e57a40-dc4f-4cfe-a278-ce5fc7c8359e>', '<urn:uuid:d56011d0-2bfd-4c39-9bc5-aa0e954a326c>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T19:10:11.825225	35	45	2195
27	nutritious food plant rice vs georgia power plant mitchell convert both types plants produce food energy how different	The two types of plants serve very different purposes. Rice plants are agricultural crops that produce grains with nutritional value, containing essential minerals like Mg, P, K, S, Ca, Mn, Fe, Co, Ni, Cu, and Zn for human consumption. In contrast, Plant Mitchell is a power generation facility near Albany, Georgia that is being converted from coal to wood fuel (biomass). Once converted, the 96 MW biomass plant will run on surplus wood from suppliers within a 100-mile radius to generate electricity, not food.	"['|Tarpley, Lee -|\n|Salt, David -|\n|Guerinot, Mary -|\nSubmitted to: Texas Experiment Station Field Day Handout\nPublication Type: Experiment Station\nPublication Acceptance Date: June 2, 2010\nPublication Date: June 29, 2010\nCitation: Pinson, S.R., Tarpley, L., Ratnaprabha, Salt, D., Guerinot, M.L. 2010. Found: Rice that produces grains with improved nutritional value. Texas Experiment Station Field Day Handout. VII - VIII; http://beaumont.tamu.edu/eLibrary/Newsletter/2010_Highlights_in_Research.pdf. Technical Abstract: Rice provides the major source of nutrition for a large proportion of the world’s population, and is a key ingredient in baby foods in the U.S. Mineral nutrients such as Ca, Fe, and Zn play critical roles in human health, with over 3 billion people suffering from Fe and Zn deficiencies. Unfortunately for those who rely on rice for subsistence, rice grain is not a concentrated source of these nutrients, and if grown on contaminated soils, can contain toxic elements such as As and Cd. Drs. Pinson and Tarpley recently identified rice lines exhibiting significant improvements in the content of 16 elements, namely Mg, P, K, S, Ca, Mn, Fe, Co, Ni, Cu, Zn, As, Rb, Sr, Mo, and Cd, and are now investigating the genes and physiology underlying their improved nutritional value. Research Methods: The USDA maintains a repository of seed collected from wild and cultivated rice lines found all around the world. A core subset of 1700 accessions from among the 17,000+ rice accessions in the USDA National Small Grains Collection was selected to represent the wide genetic diversity contained within the larger set of rice. Before these lines could be compared for nutritional value, they had to first be grown side-by-side under controlled field conditions. It is well known that the amount of oxygen in the soil greatly affects the availability of soil nutrients. Therefore we grew the 1700 foreign rices under both flooded, and unflooded conditions, two replications per year, in both 2007 and 2008 at the Beaumont rice research station. Seed was harvested, dried, threshed, and hulled, then sent to a collaborator at Purdue University who analyzed them for accumulation of Mg, P, K, S, Ca, Mn, Fe, Co, Ni, Cu, Zn, As, Rb, Sr, Mo, and Cd. To minimize the effect of variable soil, the rice lines were grown closely together in the field, with 5 seed drill-seeded into hillplots. Fifteen repeated check-plots paired with fifteen soil samples per replication were grown/collected in a grid pattern. This allowed us to document that the impact of environmental variance within each paddy was small compared with genetic impact on grain element content. Large (> 5x) ranges in grain content were found for each of the 16 elements, with the unflooded field treatment showing more extreme differences in grain nutritional value than the flooded rice. Rice accessions high for a particular element were sometimes found to have similar geographic origins. For example, four of the five lines highest in Mo content originated from Malaysia. The common origin of the high-Mo accessions is exciting in that it verifies that we have found lines with key genes allowing them to accumulate high levels of Mo in their grains. The fact that these Malaysian rices were collected on three different trips spanning two decades suggests that the Mo-genes are easily passed from one generation to the next, and appear to be beneficial rather than detrimental to rice yields and health or they would not have been maintained over decades of breeding. Future research: Studies to identify the genes and the physiological attributes underlying the improved grain mineral contents are underway. We will be asking if their increase mineral content is due to increased root uptake, increased transport through the plant, or a combination of the two. Spatial location of the accumulated minerals within the rice kernel is also under investigation. It will be easier to impact human diets if the minerals are accumulated in the endosperm rather than in the embryo or bran layer which are not ingested unless one is eating brown rice.', 'Georgia Power Phone Number Georgia Power Job Openings Georgia Power Locations Georgia Power Federal Credit Union Georgia Electricity Georgia Natural Gas Georgia Gas Alabama Power\n| Georgia_Power | Southern_Company | Georgia_Power_Company_Corporate_Headquarters | Solar_power_in_Georgia | Wind_power_in_Georgia | List_of_power_stations_in_Georgia | List_of_power_stations_in_Georgia_(United_States) | Lake_Burton_(Georgia) | Lake_Seed | Lake_Rabun | Lake_Tugalo | Albany,_Georgia | Tallulah_River | Oglethorpe_Power | Atlanta_Transit_Company | Georgia_(U.S._state) | Streetcars_in_Atlanta | Bowen_Power_Station | WDNN-CA | WBEK-CA |\n|Traded as||NYSE: GAR|\n|Headquarters||Atlanta, Georgia, USA|\n|Key people||Paul Bowers (President & CEO)|\nGeorgia Power is an electric utility headquartered in Atlanta, Georgia, United States. It was establised as the Georgia Railway and Power Company and began operations in 1902 running streetcars in Atlanta as a successor to the Atlanta Consolidated Street Railway Company.\nGeorgia Power is the largest of the four electric utilities that are owned and operated by Southern Company. Georgia Power is an investor-owned, tax-paying public utility that serves more than 2.25 million customers in all but four of Georgia\'s 159 counties. It employs approximately 9,000 workers throughout the state. The Georgia Power Building, its primary corporate office building, is located at 241 Ralph McGill Boulevard in downtown Atlanta.\nIn 2006 the Savannah Electric & Power Company, a separate subsidiary of Southern Company, was merged into Georgia Power.\nOriginally the Georgia Railway and Power Company, it began in 1902 as a company running the streetcars in Atlanta, and was the successor to the Atlanta Consolidated Street Railway Company. In the 1930s, the company published a free newsletter called Two Bells which was distributed on its streetcars. From 1937 until 1950, Georgia Power also operated trolleybuses in Atlanta, and in 1950 its network of 31 electric bus routes was the largest trolley bus system in the United States. After the Atlanta transit strike of 1950, the Atlanta Transit Company took over operations. Atlanta Streetcar was formed in the 2000s to establish a new streetcar service along Peachtree Street.\nThe company built several dams, including the Morgan Falls Dam just north of the city, and some as far away as the Tallulah River in the northeast Georgia mountains. These hydroelectric dams form Lake Burton, Lake Seed, Lake Rabun, Lake Tallulah Falls, Lake Tugalo, and Lake Yonah, the last two of which straddle the Georgia – South Carolina border on the Tugaloo River.\nGeorgia Power operates Plant Scherer.\nAccording to Natural History Magazine, as of 2006[update] Plant Scherer is the largest single point-source for carbon dioxide emissions in the United States. It was also ranked the 20th in the world in terms of carbon dioxide emissions by the Center for Global Development on its list of global power plants in November 2007. It was the only power plant in the United States that was listed in the world\'s top 25 Carbon Dioxide producers .\nGeorgia Power utilizes transmission lines carrying 115,000 volts, 230,000 volts and 500,000 volts. Georgia Power has interconnections with the Tennessee Valley Authority to the north, sister company Alabama Power to the west, South Carolina Electric and Gas and Duke Energy to the east, and Gulf Power (another sister company), Florida Power & Light, Progress Energy Florida and the city of Tallahassee, Florida to the south.\nGeorgia Power asked the state\'s public service commission for approval to convert the coal-fired Plant Mitchell to run on wood fuel. If approved, the retrofit will begin in 2011 and the biomass plant will start operating in mid-2012. The 96 MW (129,000 hp) biomass plant will run on surplus wood from suppliers within a 100 mi (160 km) radius of the plant, which is located near Albany, Georgia.\nGeorgia Power owns and operates 20 hydroelectric dams, 14 fossil fueled generating plants and two nuclear power plants, which provide electricity to more than 2 million customers.\nGeorgia Power Hydro incorporates 72 hydro electric generating units to produce a generation capacity of 844,720 kilowatts (KW). Georgia Power Hydro facilities also provide more than 45,985 acres (18,609 ha) of water and more than 1,057 mi (1,701 km) of shoreline for habitat and recreational use.\n|Barnett Shoals Hydroelectric Generating Plant||Athens, Georgia||2,800 kW|\n|Bartletts Ferry Hydroelectric Generating Plant||Columbus, Georgia||173,000 kW|\n|Burton Hydroelectric Generating Plant||Clayton, Georgia||6,120 kW|\n|Estatoah Hydroelectric Generating Plant||Mountain City, Georgia||240 kW|\n|Flint River Hydroelectric Generating Plant||Albany, Georgia||5,400 kW|\n|Goat Rock Hydroelectric Generating Plant||Columbus, Georgia||38,600 kW|\n|Langdale Hydroelectric Generating Plant||West Point, Georgia||1,040 kW|\n|Lloyd Shoals Hydroelectric Generating Plant||Jackson, Georgia||14,400 kW|\n|Morgan Falls Hydroelectric Generating Plant||Sandy Springs, Georgia||16,800 kW|\n|Nacoochee Hydroelectric Generating Plant||Clayton, Georgia||4,800 kW|\n|North Highlands Hydroelectric Generating Plant||Columbus, Georgia||29,600 kW|\n|Oliver Dam Hydroelectric Generating Plant||Columbus, Georgia||60,000 kW|\n|Riverview Hydroelectric Generating Plant||West Point, Georgia||480 kW|\n|Rocky Mountain Hydroelectric Generating Plant||Rome, Georgia||215,256 kW|\n|Sinclair Dam Hydroelectric Generating Plant||Milledgeville, Georgia||45,000 kW|\n|Tallulah Falls Hydroelectric Generating Plant||Tallulah Falls, Georgia||72,000 kW|\n|Terrora Hydroelectric Generating Plant||Tallulah Falls, Georgia||16,000 kW|\n|Tugalo Hydroelectric Generating Plant||Lakemont, Georgia||45,000 kW|\n|Wallace Dam Hydroelectric Generating Plant||Eatonton, Georgia||321,300 kW|\n|Yonah Hydroelectric Generating Plant||Lakemont, Georgia||22,500 kW|\n|Plant||Nearest City||Number of Units||Capacity|\n|Bowen Steam-Electric Generating Plant (Plant Bowen)||Cartersville, Georgia||4||3,160,000 kW|\n|Harllee Branch Jr. Steam-Electric Generating Plant||Milledgeville, Georgia||4||1,539,700 kW|\n|William P. Hammond Steam-Electric Generating Plant||Rome, Georgia||1||800,000 kW|\n|Kraft Steam-Electric Generating Plant||Savannah, Georgia||4||281,136 kW|\n|John J. McDonough Steam-Electric Generating Plant||Smyrna, Georgia||2||490,000 kW|\n|McIntosh Steam-Electric Generating Plant||Savannah, Georgia||9||810,000 kW|\n|McIntosh Combined Cycle Plant||Rincon, Georgia||2||1,240,000 kW|\n|Clifford Braswall McManus Steam-Electric Generating Plant||Brunswick, Georgia||2||596,000 kW|\n|W. E. Mitchell Steam-Electric Generating Plant (31°26\'41.13""N, 84°8\'2.34""W)||Albany, Georgia||4||243,000 kW|\n|Robins Steam-Electric Generating Plant||Warner Robins, Georgia||2||166,000 kW|\n|Robert W. Scherer Steam-Electric Generating Plant (Plant Scherer)||Juliette, Georgia||4||3,272,000 kW|\n|Wansley Steam-Electric Generating Plant||Carrollton, Georgia||2||951,872 kW|\n|Allen B. Wilson Combustion Turbine Plant||Waynesboro, Georgia||354,100 kW|\n|Eugene A. Yates Steam-Electric Generating Plant||Newnan, Georgia||7||1,250,000 kW|\n|Plant||Nearest City||Number of Units||Capacity|\n|Alvin W. Vogtle Nuclear Electric Generating Plant (Plant Vogtle)||Waynesboro, Georgia||2||2,430,000 kW|\n|Edwin I. Hatch Nuclear Electric Generating Plant (Plant Hatch)||Baxley, Georgia||2||1,726,000 kW|']"	['<urn:uuid:edea8e1c-0736-4f74-9ed6-7f1ca3678c94>', '<urn:uuid:f92155f6-e657-4736-a58b-6ff833e9e460>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T19:10:11.825225	18	84	1598
28	How do dendritic cells and probiotics impact bacterial infections differently?	Dendritic cells naturally transport beneficial bacteria from the gut to human milk by opening tight junctions and maintaining live bacteria for days, while probiotics are supplemental beneficial bacteria that create an unfavorable environment for pathogens by producing organic acids and bacteriocins to directly inhibit harmful bacteria.	"[""Probiotics: The Foundation for Total Well Being\nLactic acid bacteria (LAB), including species of Lactobacillus, Streptococcus, Pediococcus and Leuconostoc have been used for preservation of food by fermentation for thousands of years. People of Eastern Europe, Southern Asia and Northern Africa have consumed yogurt and kefir for thousands of years. Fermentation of milk by LAB has permitted its preservation, improved its palatability and digestibility. Ancient people regarded these fermented milks as divine foods and as indispensable remedies for various illnesses.\nFermentation of food provides characteristic taste profiles and lowers the pH, which prevents contamination by potential pathogens. Fermentation is globally applied in the preservation of a range of raw agricultural materials (cereals, roots, tubers, fruit and vegetables, milk, meat, fish etc.).\nL. plantarum frequently occurs spontaneously, in high numbers, in most lactic acid fermented foods of plant origin, for example, in brined olives, capers, and sauerkraut. Thus, humans have in this way consumed large numbers of live LAB, and presumably those associated with plant material were consumed before those associated with milk based foods.\nA century ago, Elie Metchnikoff (Russian scientist, Nobel laureate, and professor at the Pasteur Institute in Paris) postulated that LAB offered health benefits leading to longevity. He considered yogurt to be one of the most effective means of inhibiting intestinal infections, intoxications and putrefactions, which he thought were the cause of a great number of conditions such as premature senility and lack of vitality.\nThe term probiotics was first introduced in 1965 by Lilly and Stillwell; in contrast to antibiotics, probiotics were defined as microbially derived factors that stimulate the growth of other organisms. In 1989, Roy Fuller emphasized the requirement of viability, for probiotics, and introduced the idea that they have a beneficial effect on the host.\nThe World Health Organization has defined probiotic bacteria as live microorganisms which when administrated in adequate amounts confer a health benefit on the host (FAO/WHO 2001).\nAccording to the current definition, a fermented food is not a probiotic. A fermented food might contain a probiotic if the strain/strains in question satisfy the criteria. Fermented foods contain live, active cultures, but these cultures are generally tested for food fermentation properties and not health benefits.\nIn spite of the existing scientific consensus, there is no legal definition of the term probiotic.\nThe minimum criteria to be met for probiotic products are that the probiotic must be:\n- Specified by genus and strain - research on specific probiotic strains cannot be applied to any product marketed as a probiotic.\n- Delivered in adequate dose through the end of shelf life (with minimal variability from one batch to another).\n- Shown to be efficacious in controlled human studies.\n- Resistance to low pH, gastric juice, bile, pancreatic juice\n- Ability to adhere to cells (host or microbial)\n- Exclude or reduce adherence of pathogens\n- Remain metabolically active in gut\n- Produce antagonistic compounds\n- Resist antimicrobial substances\n- Safe, non-invasive, non-carcinogenic and non-pathogenic\n- Cohabit and form part of the indigenous microbiota\n- Modulation of immune response\n- Produce beneficial systemic effects\nYogurt is perhaps the most common probiotic-carrying food, but the functional food market has expanded beyond yogurt. Cheese, fermented and unfermented milks, juices, smoothies, cereal, nutrition bars, and infant/toddler formula all are food veÂhicles for probiotic delivery. In addition to being sold as foods, probiotics are sold as dietary supplements in the form of capsules, tablets, and powders.\nKey Species Studied and Used as Probiotics\nLactobacillus Genus Species\n- L.Â acidophilus\n- L. casei\n- L. fermentum\n- L. gasseri\n- L. johnsonii\n- L. paracasei\n- L. plantarum\n- L. reuteri\n- L. rhamnosus\n- L. salivarius\n- B. adolescentis\n- B. animalis subsp lactis\n- B. bifidum\n- B. breve\n- B. infantis\n- B. longum\n- Streptococcus thermophilus\n- Streptococcus salivarius\n- Enterococcus faecium\n- Escherichia coli\n- Bacillus coagulans\n- Bacillus clausii\n- Saccharomyces cerevisiaeboulardii\nThe gastro-intestinal tract, including the vagina, contains a very complex and diverse microbiota or microbial ecosystem - 100,000 billion bacteria (100 bill/g content), located mainly in the colon and comprising over 1000 species of microorganisms. Bacterial cells outnumber the host cells by a factor of 10.\nAlthough much is still unknown about the microorganisms that colonize humans, some important points can be made.\n- Each individual has his or her own unique population of microbes, even if there are commonalities of species among people.\n- The microbes colonizing different regions of the human body (skin, mouth, gastrointestinal tract, vaginal tract of women) are both diverse and numerous, and they differ according to their habitat.\n- Intestinal microbes are fairly stable through time, although transitions occur at weaning and again in the elderly. Colonizing microbiota can be impacted by antibiotics, diet, immunosuppression, intestinal cleansing, and other factors.\nActivities such as regulating immune function, enhancing the intestinal barrier to prevent unwanted microbes from entering the blood stream, colonization resistance, and digestion are important functions of colonizing microbes.\nAt birth, the intestinal immune system is virtually non-existent and develops at the same time as the intestinal microbiota. It becomes more and more complex as dietary intake changes from breast and/or bottle feeding to adult type food intake. Following birth, colonization facilitates the formation of physical and immunological barrier. The Host-Microbe interaction is of primary importance during the neonatal period. Around the age of 2, intestinal immune system is considered to be mature.\nIt is possible that the microbiota acquired during and immediately after birth is necessary for the newborn's systemic and mucosal immunity, and it may also be responsible for controlling inflammatory responses in allergic and inflammatory bowel diseases.\nThe intestine is the body's most important immune function related organ: approximately 70 % of the body's immune cells are located in the intestinal mucosa/lining of the GI tract. The immune system controls immune responses towards:\n- Dietary proteins\n- Prevention of food allergies\n- Pathogenic microorganisms (Salmonella, Listeria, Clostridium, etc.)\n- Viruses (rotavirus, poliovirus)\n- Parasites (Toxoplasma)\nProbiotics produce organic acids that are reported to inhibit E. coli and other intestinal pathogens. The most common are lactic, acetic and formic acids, which lower intestinal pH and may suppress harmful organisms. Acetic acid also lowers the O/R (oxidation-reduction) potential. Simplified, if the O/R potential is reduced, organisms that require oxygen for growth, such as Salmonella and Shigella, will be inhibited.\nMany researchers report that certain strains of lactic acid bacteria produce bacteriocins, antibiotic-like molecules and other compounds that inhibit intestinal pathogens. These antimicrobial substances include acidilin, acidophilin, lactolin, nisin and hydrogen peroxide.\nBile assists fat digestion, and conjugated and deconjugated (free) forms of the compound inhibit growth of enteric pathogens. Certain Lactobacillus strains can deconjugate (break apart) bile salts, which are the more inhibitory form.\n.Amines, produced by intestinal microbes, are irritating and toxic, and have been associated with diarrhea. Lactic acid bacteria have been reported to reduce the level of amines in the gut, and to neutralize enterotoxins.\nKey Probiotic Effects/Benefits\nIn general, the strongest clinical evidence for probiotics is related to their use in improving gut health and modulating immune function.\n- Activate local macrophages to increase antigen presentation to B lymphocytes and increase secretory immunoglobulin A (IgA) production both locally and systemically\n- Induce hyporesponsiveness to food antigens\n- Enhancing specific antibody responses to pathogens\n- Neutralize enterotoxins\n- Modulate cytokine profiles\n- increasing NKH cell activity;\n- enhancing the phagocytic capacity of polymorphonuclear cells and macrophages\n- Digest food, synthesize vitamins, increase mineral absorption\n- Alter local pH to create an unfavorable local environment for pathogens\n- Produce bacteriocins to inhibit pathogens\n- Scavenge superoxide radicals\n- Stimulate epithelial mucin production\n- Improve gut transit\n- Enhance intestinal barrier function\n- Compete for nutrients and adhesion with pathogens\n- Modify pathogen derived toxins\nProbiotics are intended to assist the body's naturally occurring microbiota of the gastrointestinal tract, the oral cavity and the vagina. Studies have documented probiotic effects on a variety of gastrointestinal and extra-intestinal conditions, including:\n- Prevention and treatment of diarrhea associated with antibiotics intake, C. difficile, rotavirus, food poisoning\n- Distension, flatulence, bloating, abdominal pain\n- Lactose intolerance\n- Inflammatory bowel disease (IBD)\n- Irritable bowel syndrome (IBS),\n- vaginal infections, UTI\n- atopic eczema,\n- Endotoxemia combined with liver cirrhosis.\n- Dental caries\n- Colorectal cancer prevention\n- Gastric ulcers\nClinically Documented Strains\nResearch on probiotics suggests a range of potential health benefits. However, the effects described can only be attributed to the strain(s) tested, not to the species, nor to the whole group of LABs or other probiotics\nA probiotic strain is listed by the genus, species, and an alphanumeric designation. In the scientific community, there is an agreed nomenclature for microorganisms for example, Lactobacillus plantarum 299v. Lactobacillus is the genus, plantarum is the species and 299v is the strain.\nExamples of documented strains\nBifidobacterium animalis DN 173 010 / Danone\nBifidobacterium animalis subsp. lactis Bb-12 / Chr. Hansen\nBifidobacterium longum BB536 / Morinaga Milk Industry\nLactobacillus acidophilus LA-5 / Chr. Hansen\nLactobacillus acidophilus NCFM / Danisco\nLactobacillus casei DN-114 001/ Danone\nLactobacillus casei Shirota / Yakult\nLactobacillus plantarum 299V / Probi\nLactobacillus reuteri ATTC 55730 / BioGaia Biologics\nLactobacillus rhamnosus ATCC 53013 (LGG) / Valio\nSaccharomyces cerevisiae (boulardii) / Biocodex, and others\nTested as mixture:\nLactobacillus acidophilus CL1285 & Lactobacillus casei Lbc80r / Bio K+ International\nLactobacillus rhamnosus GR-1 & Lactobacillus reuteri RC-14 / Chr. Hansen\nLactobacillus helveticus R0052 & Lactobacillus rhamnosus R0011 / Institut Rosell\nBacillus clausii strains O/C, NR, SIN, and T/ Sanofi-Aventis\nThe genera Bifidobacterium, Lactobacillus and Lactococcus have a long history of safe use in foods and are generally recognized as safe. Infections in humans by these genera are extremely rare. This is based on the prevalence of the microorganisms in fermented foods such as yogurt, cheeses, and sauerkraut, as normal colonizers of the gastro-intestinal tract and low level of infection associated with them. Â Most of the strains used in probiotic products are generally isolated from the human GI tract, from human milk, or from foods consumed by humans.\nScientists have pointed out three theoretical concerns regarding the safety of probiotics: the potential occurrence of such conditions such as: 1) bacteremia or endocarditis, toxic or 2) metabolic effects on the gastrointestinal tract and 3) the transfer of antibiotic resistance to other species in the gastrointestinal tract. For the most part these safety concerns are directed towards probiotics products containing species from the following genera: Enterococcus, Escherichia, Clostridium and Bacillus.\nThe use of probiotics in either diseased or immune-compromised individuals should be done under the direction of their health care professional.\nSelecting probiotic supplements\nOne approach to choosing a probiotic is to use the definition of a probiotic as a guide.\nlive microorganisms which when administrated in adequate amounts confer a health benefit on the host\nProbiotics as dietary supplements are generally freeze-dried, which is a means to preserve them for extended periods of time. The shelf life stability of a finished probiotic product will determine if the product contains live microorganisms and adequate amounts. The characteristics of the strain itself, the fermentation medium, conditions and process, filtration and freeze-drying are all factors that will contribute to the viability of the strains. Furthermore, the choice of other ingredients that will be integrated into the final product also contribute to the stability. The environmental conditions of the manufacturing rooms and handling of the cultures during blending, encapsulation, tableting, etc are also important, and appropriate procedures most be followed. Moisture content must be closely monitored and kept to low levels since freeze-dried bacterial cultures are very sensitive to moisture and the die-off increases as moisture content increases. The temperature and storage conditions are also important factors. As a general rule, freeze-dried probiotic cultures are most stable when stored under colder temperatures and die-off increases as temperature increases, especially when they surpass 30 C/85 F. It is possible, however, to design probiotic products that do not require refrigeration but extreme high temperature should be avoided and the shelf life will be extended if stored at relatively cooler temperatures. Finally, the packaging system, i.e. the type of packaging materials used, along with appropriate desiccants when necessary, is also important for the shelf life of a probiotic finished product by protecting the contents from various environmental factors.\nAdequate amounts, implies that the dosage must be high enough to elicit a health benefit. The minimum dose for most strains fall in the range of 1-10 bill/day. There are products on the market whose minimum recommended dose range from 100 million to 450 bill/day. The required amount is not to be taken as an absolute number of viable microorganisms for all individuals. Every individual has his/her unique native microbiota( a fingerprint) that has developed to maturity by the age of two. Furthermore, there is a phenomena called COLONICATION RESISTANCE which makes it difficult for outsiders or exogenous microorganisms including probiotics, to implant. Thus, dosage requirements can vary among different individuals and is difficult to establish specific dosage for all individuals. It is important that the dosage of the strains are based on the level used in the human studies and shown to be effective.\nThe label should disclose the genus, species, and strain designation for each strain in the product. The strain designation tie the product content to the scientific research/clinical documentation performed on the strain(s).Â This way the specific health benefits of the strain(s) can be found when searching for the scientific documentation.\nFor the promotion of a balanced intestinal microbiota and overall well-being a probiotic with a blend of species is recommended since this can allow for a greater colonization throughout various regions of the GI tract and provide a wider spectrum of benefits. Bifidobacteria, for example are found predominantly in the colon and therefore it is important that a product also contains Lactobacilli which can colonize the small intestine.\nProbiotics sold as dietary supplements or functional foods cannot make claims that they cure, treat or prevent disease. Manufacturers can, however, provide references that support the health benefits of their documented strains found in their products. Choose a probiotic that is manufactured by a reputable company and that works for you.\n-- Article by Peilin Guo, MS. RD & Silvano Arnoldo B.Sc.\nWGO Practice Guidelines Probiotics and Prebiotics 1\nWorld Gastroenterology Organisation, May 2008 pp1-23\nProbiotics as functional food: microbiological and medical aspects\nMalda Maija Toma, Juris Pokrotnieks\nActa Universitatis Latviensis, 2006, Vol. 710, Biology, pp. 117-129\nProbiotics and their fermented food products are beneficial for health\nS. Parvez, K.A. Malik, S. Ah Kang and H.-Y. Kim\nJournal of Applied Microbiology ISSN 1364-5072, January 2006Â pp. 1171-1185\nProbiotics: Their potential to impact human health\nMary Ellen Sanders ,Glenn Gibson, Harsharnjit S. Gill, Francisco Guarner CAST Issue Paper Number 36 October 2007\nThe place of probiotics in human intestinal infections\nA Sullivan, C.E. Nord\nInternational Journal of Antimicrobial agents 20 (2002) pp.313-319\nProbiotics: facts and myths\nC. Senok, A. Y. Ismaeel and G. A. Botta\nClin Microbiol Infect 2005; 11: 958-966\nImmunomodulatory effects of probiotics in the intestinal tract\nV. Delcenserie, D. Martel, M. LAmoureux, J. Amiot, Y. Boutin and D. Roy\nCurr. Issues Mol Biol. 10: 37-54\nLACTIC ACID BACTERIA Microbiology and functional aspects\nEdited by: Seppo Salminen and Atte von Wright\nMarcel Dekker 1998 (Textbook)\nHow do we know when something called Probiotic is really a probiotic? A guideline for consumers and health care professionals\nMary Ellen Sanders\nFunctional food Reviews, Vol 1, 2009 pp 3-12"", 'The human microbiome project was a major undertaking by the National Institutes of Health, with a fairly simple mission: understand the bacterial communities living in and on the human body, and the potential impact these communities may have on health. Hundreds of individuals donated everything from feces to nasal secretions. However, one key system was ignored - human milk. That’s right – the microbiome of human milk was not studied.\nProbably some of this had to do with a long standing myth that human milk was sterile. Why study something without bacteria, right? But, as we have quickly learned – human milk is far from sterile. The average baby consuming 800 mL/27 ounces of human milk will received between 100,000 and 10,000,000 million bacteria from human milk per day (Fernandez et al., 2013).\n|Figure 1: The Human Microbiome Project is not interested in milk. I fixed their image to better reflect this.|\nFortunately, research into the human milk microbiome has continued despite this oversight by the Human Microbiome Project. It appears that nine “operational taxonomic units” (generally closely related species based on DNA analysis of the bacteria) are extremely common in most mothers studied to date: Streptococcus, Corynebacteria, Bradyrhizobiaceae, Staphylococcus, Serratia, Ralstonia, Propionibacterium, Pseudomonas, and Sphingomonas. These nine groups typically account for more than 50% of total bacteria. Bififobacterium and Lactobacillus are also common, but less universal (Fernandez et al., 2013).\nThe microbiota of milk appears to be quite stable (Fernandez et al., 2013), although a few factors appear to shape the composition. First, mothers with higher BMIs (in the obese range) produce colostrum with more Lactobacillus, and mature milk with more Staphylococcus and less Bifidobacterium (Cabera-Rubio et al., 2012). Cabera-Rubio and colleagues (2012) also found that greater pregnancy weight gain predicted more Staphylococcus in the milk in a small study of 18 mothers, half obese and half of normal weight.\nBut here is the really neat part – guess what else altered the milk microbiota? Type of delivery. Mothers who had caesarian sections had a different milk microbiota than mothers who had a vaginal delivery. And the variation continued – mothers undergoing emergency caesarians after laboring had milk microbiotas closer to those of women who delivered vaginally than women with elective caesarians.\nWhere do the bacteria come from? Initially, it was thought that the milk microbiome was really just contamination from the skin microbiome. However, this is WRONG, WRONG, WRONG. While the milk microbiome does contain some of the same families of bacteria as skin, multi-site sampling of mammary skin and milk revealed that these are not the same species and/or genera. Instead, it appears that the microbiome of milk comes from several places, including the maternal gut microflora. Current evidence supports dendritic cells as the likely transfer mechanism. These cells, along with some macrophages, can open the tight junctions between cells forming the gut barrier and take in living bacteria. These cells can then maintain the live bacteria for several days in mesenteric lymph nodes scattered throughout the body (Fernandez et al., 2013). Dendritic cells are also pretty picky about what they take up – dead bacteria or latex beads will not activate immature dendritic cells for bacteria uptake, while commensal species, like Lactobacillus, show high levels of binding (Rescigno et al., 2001).\n|Figure 2: Dendritic cell (shown in blue). Image from http://www.cell.com/pictureshow/immunology|\nThis allows for oral manipulation of the milk microbiome – mothers given supplemental Lactobacillus from three strands, L. gasseri, L. fermentum, L. salivarius, showed transfer of these strands to the milk (Jimenez et al., 2008).\nThis lead to the logical question – could these strands be used to treat mastitis? Arroyo et al., (2010) randomized 352 women with mastitis to three groups – one dosed with L. fermentum, one dosed with L. salivarius, and one given standard antibiotic treatment (4 different drugs were used). Bacterial counts for milk were obtained for all mothers on Day 0 – that is before treatment started. All mothers had bacterial counts of 4.35-4.47 log10 CFU (colony forming units) – a little less than double the recommended bacterial counts for milk of 2.5 log10 CFUs. Mothers received 21 days of treatment, and milk bacterial counts were repeated on day 21. Women who received L. fermentum had mean bacterial counts of 2.61 log10 CFUs; L. salivarius had bacterial counts 2.33 log10 CFUs with clinical relief of mastitis, and all reported reductions in reported breast pain. Mothers who received antibiotics did not fare as well. Mean bacterial count for antibiotic receiving mothers was 3.28 log10 CFUs and pain scores were much higher. Three months later, only 8.8% of mothers receiving either L. fermentum or L. salivarius had experienced recurrent mastitis, while 30.1% of mothers receiving antibiotics had. All differences between antibiotic and probiotic groups were significantly different – the kind of significant difference that makes researchers do their happy dance.\nFigure 3: This is how I picture the researchers after making this discovery - just substitute a computer for the piano. Gif by PEANUTS.\nSo the milk microbiome appears to be protecting mothers – but there is also good evidence that it is protecting infants. Little is known about the salivary microbiome of infants, but based on preliminary evidence, it appears to, not surprisingly, have some overlap with the milk microbiome (Nasidze et al., 2009). The milk microbiome also appears to contribute to the microbiome of the infant GI tract, as well as the development of immune function in the infant (Fernandez et al., 2013). Infants supplemented with Lactobacillus fermentum (yes, the same as used for the treatment of mastitis) showed significant reductions in diarrheal and respiratory infections in early infancy compared to control infants (Maldonado et al., 2012). Many of the bacteria in the milk microbiome are protecting both the mother and the infant from infection, and may even be involved in the development of immune tolerance.\nMilk remains amazing – even the bacteria in milk!\nArroyo R, Martín V, Maldonado A, Jiménez E, Fernández L, Rodríguez JM. (2010) Treatment of infectious mastitis during lactation: antibiotics versus oral administration of lactobacilli isolated from breast milk. Clinical Infectious Diseases 50:1551–8.\nCabrera-Rubio R1, Collado MC, Laitinen K, Salminen S, Isolauri E, Mira A. (2012) The human milk microbiome changes over lactation and is shaped by maternal weight and mode of delivery. Am J Clin Nutr. 96(3):544-51.\nFernández L1, Langa S, Martín V, Maldonado A, Jiménez E, Martín R, Rodríguez JM. (2013) The human milk microbiota: origin and potential roles in health and disease. Pharmacol Research 69(1):1-10.\nJiménez E, Fernández L, Maldonado A, Martín R, Olivares M, Xaus J, et al. (2008) Oral administration of lactobacilli strains isolated from breast milk as an alternative for the treatment of infectious mastitis during lactation. Applied and Environment Microbiology 74:4650–5.\nMaldonado J, Ca˜nabate F, Sempere L, Vela F, Sánchez AR, Narbona E, et al. (2012) Human milk probiotic Lactobacillus fermentum CECT5716 reduces the incidence of gastrointestinal and upper respiratory tract infections in infants. Journal of Pediatric Gastroenterology and Nutrition 54:55–61.\nMartín R, Olivares M, Marín ML, Fernández L, Xaus J, Rodríguez JM. (2005) Probiotic potential of 3 lactobacilli strains isolated from breast milk. Journal of Human Lactation 21:8–17.\nNasidze I, Li J, Quinque D, Tang K, Stoneking M. (2009) Global diversity in the human salivary microbiome. Genome Research 19:636–43.\nRescigno M, Urbano M, Valzasina B, Francolín M, Rotta G, Bonasio R, et al. (2001) Dendritic cells express tight junction proteins and penetrate gut epithelial monolayers to sample bacteria. Nature Immunology 2:361–7.']"	['<urn:uuid:92fc7861-a6e2-4be2-8857-b73bea4c2e06>', '<urn:uuid:b96885a1-3feb-4198-9ac3-1b94d07427c8>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	10	46	3816
29	How far can different fencers reach when attacking?	Fencing reach can vary significantly between fencers. Factors like arm length, lunge length, step length, and ability to use torso rotation can give one fencer an advantage over another. This difference in reach can be decisive in epee and in situations where the referee bases calls on the order of light illumination. If an attacker's reach is longer than the defender's (positive relative distance), they can hit from outside their opponent's reach. Conversely, if the defender's reach is longer (negative relative distance), the attack becomes vulnerable to counterattack.	['We have previously discussed distance in the Salle Blog, but for this week let me do a quick review, and then add something new. First, what is distance? One common definition is that it is the physical distance between your point and the opponent’s target; another is that it is the physical distance between the fencer’s target and the opponent’s target. These definitions set the stage for the traditional division of distance into three categories defined by the movement required to hit the target: short (extension), medium (lunge), and long (advance lunge). For a number of years I have been teaching that there are actually five distances:\n(1) Infighting – the distance at which unusual attitudes and movements must be used to position the blade for a hit in close quarters.\n(2) Short – the distance at which a hit may arrive by simple extension.\n(3) Medium – the distance at which a hit may arrive by extension and lunge.\n(4) Long – the distance at which a single footwork motion is required to reach medium distance, typically the advance lunge.\n(5) Preparatory – the distance at which multiple preparatory footwork steps, changes of direction and tempo, etc. must be used to reach long distance. I used to term this out-of-distance, but I feel that name tends to be a soothing “they can’t get me from there” statement, rather than the more realistic reflection that “they” are working very hard to convert this to at least Long and preferably Medium distance.\nIt is important to understand that these definitions are in offensive terms. If we translate them to defensive terms they become:\n(1) I suggest that is difficult to find defensive infighting – it is all offense or counteroffense. The most probable way to have defensive infighting is to close the distance to hit your opponent off target or to provoke corps-a-corps to escape the situation.\n(2) Short – the distance at which the defensive envelope for parries is collapsed to parries close to the body and ripostes are by simple extension.\n(3) Medium – the distance at which a parry is required and can be made anywhere in the full defensive envelope, from deep to advanced, with a riposte by advance or lunge.\n(4) Long – the distance at which the opponent can hit with an attack that maintains the right of way – it will require a preparatory footwork movement and you may have the options to collapse the distance or to force a pursuit. Alternately, the distance against a fast opponent at which you require a preparatory step to hit with the riposte.\n(5) Preparatory – the distance at which an opponent is actively preparing an attack against you that will involve multiple footwork actions (and covers a lot of the strip).\nThe problem with all distance classifications is that they are typically measured from the wrong place, in fact four wrong places.\nFirst – there is an implicit assumption in the definitions that distance is the same for both fencers. It isn’t. Take short distance – if my arm is longer than your arm, my short distance is longer than your short distance. If we look at medium and long, now lunge length, step length, ability to use torso rotation, etc. can all give one fencer an advantage. There can be a significant difference in fencer reach (the physical distance the fencer can cover to deliver a hit in offense or counteroffense). Reach differences can be decisive in epee, and in those situations in right of way weapons in which the referee bases his or her calls on the order in which the lights illuminate. The difference in reach creates a relative distance – the difference between the attacker’s and defender’s distance considering reach. If I am attacking you and my relative distance is negative (your reach is longer than mine), my attack is vulnerable to counterattack or advanced parry-riposte for a significant part of its travel. On the other hand, if my relative distance is positive I can hit from outside your ability to reach me.\nSecond – they are measured from where the fencers are in the moment. But that is not necessarily where they will be when the attack ends. Offensive and counteroffensive action must be delivered to where the opponent will be when the action ends, not where the opponent is when it starts. What this means is that if I attack with a lunge against a stable and ready opponent with reasonable response time, and I aim to hit where they are at this moment, they will be at least part of a retreat step further away (or may be a good part of an advance step closer trying to collapse the distance). Where the opponent will be at the end of the attack is a difficult problem, depending on their experience, tactics at this point in the bout, morale, and physical condition. Sometimes you can see this or feel it, and know where they will be; sometimes it is purely a risk decision.\nThird – there is always more than one target. An attack to the shoulder or upper front chest in foil is at a different distance from an angulated attack deep in four. In sabre you add the advanced target of the upper arm and forearm. And in epee the target deepens dramatically with forearm and arm, front torso, thigh, foot, back torso, and even rear foot, all at different distances. If you are at one distance for one target you will be at a different distance for other targets. There is a special case as well in sabre and epee – counteroffensive action may result in the advanced arm target closing the distance, while the rest of the body is opening it.\nFourth – distance is operational speed. If you execute an action at one speed to hit at a specific distance it takes a certain amount of time. If you do the same action at a closer distance, it takes less time, and at a longer distance, more time. This is important because the time associated with the distance translates into time for an opponent to move, time for a counterattack, time for a parry and riposte.\nIf we take these considerations and mesh them with the more static five definitions above against a mobile opponent:\n… on offense – short distance is now the range from short to medium distance, medium distance will frequently be long distance, and long distance will commonly be preparatory distance.\n… on defense – a short distance action can be collapsed to infighting distance, the opponent’s medium distance attack is your short distance riposte, your small step back to open the distance on an attack creates medium to long distance for your riposte, and anything beyond that becomes a pursuit in preparatory distance.\nWhat all this means is that the traditional definitions of distance only work in the context of your understanding and adapting to the relative distance with your opponent, work to hit where the opponent will be, know the impact of the target you choose on distance, and, where possible, minimize the time in the action. It is perhaps more important to be able to assess these factors in determining whether distance is a negative or positive for you, and the reverse for your opponent. A negative distance situation is one in which your opponent has a better than 50% chance of hitting you, and you have a less than 50% chance of either hitting him or avoiding her hit. A positive distance situation is one in which the odds of hitting or avoiding are in your favor.']	['<urn:uuid:670a6833-87a5-4265-8d61-e0379b84ac58>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	8	88	1280
30	flooring options environmental specs materials compare industrial building project need info eco friendly durability maintenance requirements	There are several flooring options with different environmental and maintenance specifications. Chilewich offers environmentally friendly options like TerraStrand™ with phthalate-free yarns and BioFelt® backing made from recycled materials, exceeding 55% recycled content. BioFelt® provides moisture barrier, 10-year commercial warranty, and Microban antimicrobial protection with low VOC. For general flooring types: carpet requires proper padding and seam placement, vinyl needs smooth underlayment, wood requires sealing for protection, and tile needs rigid flooring support and sealing. All types need transition pieces for proper installation and height matching between different flooring sections.	['The New York based designer Sandy Chilewich is founder and creative director of Chilewich | Sultan LLC, a company managed with her partner and husband Joe Sultan. For the last three decades and with two distinct businesses, Chilewich has reinterpreted underutilized and overlooked manufacturing practices. Since 2000, Chilewich has designed innovative textiles for numerous applications.\nSandy launched placemats and floor mats with her original signature textiles in 2000. Her designs have transformed the way tables are dressed in homes and in restaurants around the world. Her floor mats provide a clean modern alternative underfoot.\nIn 2001, the Chilewich Contract Division launches as a commercial flooring company with the introduction of woven textiles for Wall-to-Wall (W2W) and Tile Flooring. Joe Sultan, Sandy Chilewich’s architect husband, spearheaded the development of backings suitable for commercial use with the existing Chilewich woven textiles collection.\nPlynyl® W2W and tile flooring, with their soft and resilient polyurethane cushion and richly woven textures and colors, were recognized as truly innovative flooring products when they were first introduced. At the International Contemporary Furniture Fair (ICFF) Chilewich’s new flooring designs won the Editor’s Award in the Carpeting and Flooring Category.\nYear after year Chilewich gains more and more acknowledgement worldwide by a series of collaborations and successful products. Craft, Tom Colicchio’s world renowned establishment, is the very first restaurant to use Chilewich textiles on their tables. In 2007 Chilewich celebrates its ten years association with the internationally renowned Museum of Modern Art (MoMA) Design Store.\nIn 2010 Chilewich collaborated with the creative firm, freecell, for Interior Design Magazine’s Hall of Fame event.\nIn 2013, Chilewich made a significant environmental development by replacing the petroleum-based plasticizers in yarns with phthalate-free, renewable vegetable compounds. Plasticizers have traditionally been made from petroleum-based compounds that are used to make PVC yarns soft. No more. These innovative new yarns and the fabrics made from them are called TerraStrand™. Every square yard of TerraStrand™ saves .02 gallons of petroleum and .41 pounds of CO2 as compared to conventional woven vinyls. 90% of all Chilewich woven textiles are phthalate-free starting 2015.\nChilewich’s Contract Division patented BioFelt®, a PVC-free tile backing system. The backing is made from recycled water bottles, renewable vegetable compounds, as well as post-industrial materials. The total recycled content of the tile (including TerraStrand™ and the BioFelt® backing) exceeds 55% and can contribute to achieving certifications in environmental stewardship programs worldwide. Chilewich CEO, Joe Sultan, says, “BioFelt® is the most exciting tile backing system we’ve ever developed. The tile has a wonderful cushioned feel to it, it’s much lighter than vinyl, it’s tough and durable and can be installed with VELCRO® Brand Hook Squares – it’s a truly exciting development.”\nIdeal for all areas where carpet is specified and for most areas where hard surface flooring is specified, BioFelt®, this innovative composite backing system is even better!\n- Moisture barrier to prevent liquids from being absorbed.\n- Simple to maintain and durable with a 10 year commercial warranty.\n- Reduced resistance for walkers and wheelchairs vs carpet.\n- Softer than hard surface flooring options making safer in falls and more comfortable for walking.\n- Microban ® antimicrobial protection.\n- Low VOC – Greenlabel Plus Certification', 'Different types of flooring Carpet Tile Wood VinylArea Rugs\nCARPET Things to think about…. Where do you want the seam to be located? What kind of pad do you want? Do you have transition pieces? What type of carpet do you want? Example: Loop, nylon, wool, patterned, or blended fibers\nCARPE T Carpet comes in a roll of either 12’, 13’-6”, or 15’. Rarely comes in any other sizes. Pad is the cushion beneath the carpet. It comes in either 6ib ½”, 6ib 7/8”, 8ib ¼”, 8ib 3/8”. Always replace the pad when changing the carpet. Nap/pile – runs the length of the carpet roll in one direction, not the width. For carpet to have a uniform look, the pile must run the same direction for all carpet that is seamed together. Seams – when the width of the carpet will not entirely fill the space from side to side, a seam is necessary in order to fill the space.\nVINYL Vinyl comes in either 12’, 6’-6”, or 6’ wide. The more rigid the vinyl material, the narrower the roll will come. Underlayment – a firm and smooth material that will provide support underneath vinyl flooring. It helps to smooth out any imperfections in the material. Vinyl most be laid on a clean and smooth concrete floor. If placed on plywood sub-floor it must have underlayment installed first. Like carpet you will need to know where you want the seam to be placed.\nWOOD Hard wood floors come in many different types of woods, widths, and finishes. Needs to be sealed for protection against stains. Placing a mat at exterior doors will help protect the wood longer. Wood flooring can be laid on a straight, diagonally, or herringbone.\nTILE Tile comes in all different sizes and shapes from 1” x 1” to 30” x 30”. Also comes in different types of materials like: concrete, granite, marble, and limestone. The larger the tile, the more important it is that the floor is rigid since any shifting will cause larger tiles to ‘pop’ and break. Needs to be sealed for protect. Like wood flooring it can be laid on a straight, diagonally, or herringbone.\nAREA RUGS The ideal area rug size extends 3” beyond furniture pieces. For dining rooms the rug should extend 2’-6” on all sides. If you have borders in the rug then you need to make sure the seams are not visible.\nCalculating into the cost Materials – carpet, pad, vinyl, wood, and tile Transition pieces Take up Hail off Repairs of the floor underneath Moving furniture Installation Shipping If you are having tile put in there is additional charges Bullnose pieces Concrete board Mastic Grout Sealant\nThings to keep in mind when purchasing flooring… With all types of flooring you need to look at transition pieces. This makes the flooring flow into each room; a long with keeping the different types flooring the same height. Add in additional amount of materials in case of seaming or damaged items. Most companies do this for you but it is always good to ask. always give enough time for shipping and booking an installer. If you wait until the last minute you will not get it put in on time. Have two different installers come out and measure. This helps you to know if the installer is measuring correctly.']	['<urn:uuid:028de7fa-a9db-46a2-9970-9a5bb9fd06ab>', '<urn:uuid:e523adbb-1598-44dc-a747-caae16f9280e>']	open-ended	with-premise	long-search-query	distant-from-document	three-doc	expert	2025-05-12T19:10:11.825225	16	89	1089
31	organic processing facility requirements maintain product integrity storage handling vs certification body reviews approval steps	Organic processing facilities must meet strict integrity requirements while undergoing thorough certification approval. For integrity, facilities must prevent commingling with conventional materials during storage, processing, and transport, avoid contamination from sanitation or pest control agents, and maintain nutritional value through physical and biological processes rather than chemical methods. For certification approval, facilities must submit detailed plans to Certification Bodies, undergo audits where auditors verify compliance, and address any non-compliances identified during review. The facility must comply with all federal, provincial and industry regulations while maintaining organic standards that allow maximum 5% non-organic ingredients from approved lists.	['Certified organic products are grown and processed in accordance with specific guidelines and standards, as established by a certifying agency. Adherence to the standards is verified annually through a third party inspection. Detailed records must be maintained documenting farming practices, materials use, purchases, inventories and sales. Through the use of lot numbers, certified organic product can be tracked from the shelf to the field of origin. Materials use is strictly regulated in certified organic production.\nProhibited materials include:\n- Genetically engineered organisms and their derivatives in any form or any stage of production, processing and handling\n- Synthetic pesticides\n- Sewage sludge\n- Food irradiation\nConsumers are frequently misled by the words “organic” and “certified organic” on labeling. It is important to find a certification agency seal on the label. If not found, you will need to search for the vendor and request the organic certificate pertaining to that item. Some example agencies are:\n- O.C.I.A. (Organic Crop Improvement Association Intl.)\n- S.A.A. (Sustainable Agriculture Association of Alberta)\nA certified organic farm is required to work towards:\n- Optimum soil fertility\n- Enhancing biological diversity\n- Minimizing erosion and water pollution\n- Establishing living conditions for animals that are appropriate for the species\nThe ideal is to create a self-sustaining farm ecosystem with a minimum of outside inputs. The main goal is to work with natural processes such as nutrient cycling and pest/predator relationships.\nProduction using organic fruit trees is much the same as raising organic vegetables, but is more difficult. You find the same principles, though. It begins for trees with healthy dirt as it does with other crops. Orchard maintenance history goes from fully manual labor through the use of herbicides and tractors. Weed control before chemicals was done via horse drawn equipment and hand-pulling. Soil compaction was not an issue and growing ground biodiversity was a given. When herbicides and tractors became prevalent, orchardists experienced erosion and soil compaction beyond that acceptable for proper tree growth. A decline in tree branch and root development and soil microorganisms meant lessened yield and a reduction in overall tree health. It didn’t take an arborist or tree expert (website) to tell the growers what the problems were.\nOrchardists began planting other crops (alley) under their stands of fruit trees. The organic material derived from these alley crops boosted soil structure, soil aeration, moisture absorption and the uptake of nutrients. These factors helped deliver better tree health and root development. Growing an orchard organically was the best way to produce fruit.\nSoil management is based on optimizing soil life through diverse crop rotations, which include elements such as nitrogen-fixing legume crops or till down of legume green manures. Additional tools for building healthy soil may be the use of compost, composted manures, or rock powders.\nA certified organic processing facility must maintain the organic integrity of the product. This means no commingling with conventional materials during storage, processing, or transport; and no contamination with substances such as sanitation or pest control agents. Organic processing should strive to retain the nutritional value of food. Physical and biological processes are preferred as compared to chemical methods. Even though “certified organic” processed food may contain up to 5% of non-organic ingredients, the latter must be from an approved list of materials. Certified organic processing facilities and products must comply with all federal, provincial and industry regulations.\nAlong with the increased interest in human organic foods, many pet owners are thinking about what to feed their dogs and cats. The standards for true organic domestic animal foods are not crystal clear. Some organizations such as The Association of American Feed Control Officials (AAFCO) are helping. It is a voluntary group of federal, local and state organizations otherwise empowered to regulate animal feed distribution and sales. If you are concerned about what is fed your pet at your boarding kennel, usually you can bring your own food. Otherwise ask them about what they feed. See organic pet food for more on pet foods. Guidelines are provided by the AAFCO to help states build their own regulations on pet foods but have no enforcement authority against boarding kennels or veterinary clinics that claim to abide by those guidelines.\nFor more information about Organic Certification feel free to contact:\nLlizabet K. Dwwyor, M.Sc., P.Ag., Organic Specialist and Inspector.\nTo learn more about Materials Use and Organic Standards, please visit the following web sites:', 'Certification is a financial, long-term commitment and must be looked at as a commercial investment rather than just a way your business can support the industry. If you are dedicated to the certification process (which includes detailed record keeping, annual auditing and inspections) then certification can provide financial benefits in the long-run. Certification can improve market access, provide avenues into price-premium markets and export markets.\nOrganic farming and food manufacturing relies upon proactive management of the various challenges and risks your operation may face. Accordingly, organic certification relies upon you having a detailed Organic Management Plan (sometimes referred to as Organic System Plan, Organic Farm Plan, Organic Handling Plan etc.).\nOrganic certifications are based on your preparation of an Organic Management Plan (OMP), submission to a Certification Body for approval of your plan, and then an on-site inspection to check if the approved Organic Management Plan matches reality. This means that your OMP needs to be well detailed.\nInformation that should be documented in your OMP includes:\nSoil fertility and organic matter management\nPest and disease management\nPrevention of contamination from neighbours, roadsides etc.\nThere are currently five reputable, government approved Certification Bodies (CBs) in Australia. If you wish to display Australian Organic’s Bud organic logo, and support our work, the first two CBs in the below list can provide certification to the Bud scheme. If you are less concerned about the logo, you can choose from any of the five CBs.\nAustralian Organic ‘Bud’ licensed Certification Bodies\nAll the above CBs provide a similar service – certification to the National Standard. Some CBs are able to offer additional export market access, so if you are planning to export, be sure to mention this to the CB when you make your enquiry. Also see our Resources for Exporters for further guidance.\nWe encourage you to enquire with several CBs and compare their services, pricing structures, and capacity to provide certification in your location, for your product type, and destination market.\nMany CBs will have Information Kits or Application Packs available. Any queries can be sent back to the relevant CB. Gather as much information as you can, so you can make an informed choice.\nOnce you are confident you can comply, have prepared at least a draft Organic Management Plan and selected a CB, you are ready to submit an application. Some CBs will require you to reformat your OMP into their format, others will not. Typically you will be required to submit a written application, using the CBs standard forms, and make at least a down payment prior to your first audit.\nOnce your CB has reviewed your Application, and approved your OMP, your first audit will be scheduled. Typically, a subcontracted auditor will contact you directly to arrange a mutually convenient time. Expect to spend 3 hours or more with your auditor, and be aware you are required to allow his or her access to all areas of your property and relevant records. If you have concerns about confidentiality, it may help to discuss this with your CB prior to your scheduled audit.\nSome things to keep in mind:\nNo money will change hands between you and your auditor – this is handled separately through your CB.\nThe auditor does not make certification decisions – he / she is there to collect information and to verify that your approved OMP matches reality.\nThe auditor is NOT allowed to provide specific advice as to what you should do in your operation to comply with the standards. The auditor may only help you understand the relevant sections of the standard, and provide general advice or different options for how compliance may be achieved. You don’t have to do what the auditor says.\nIf the auditor identifies potential non-compliances, you will have a chance after the audit to rebut or appeal directly with staff at your CB. So, if you are stuck on an issue and don’t agree with your auditor’s assessment, it may be better to move on and work it out with your CB following the audit.\nYour auditor is bound by confidentiality agreements with the CB. Any information collected can only be shared with you and the CB, and any accreditation bodies to which the CB is accredited by.\nOnce your auditor submits your audit report to your CB, it is then reviewed by a qualified reviewer. You may then be issued with Corrective Action Requests (CARs), or Non Compliances (or similar wording), which may need to be corrected within a given timeframe, or before your next audit.\nOnce you accept, or respond adequately to any CARs or similar, your certificate will be issued. You are then able to commence trading, utilizing organic claims, and your CBs logo. It is a good idea to keep in regular contact with your CB, seek approvals for inputs, labels etc., to make sure you don’t get any surprises at your next audit.\nSearch our website\nPlease enter a search term below and we will do our best to help you find the information you are after.']	['<urn:uuid:a0d591ee-fcae-45d5-9b5b-7cfaa7e62f50>', '<urn:uuid:ab21a177-6924-4734-92d3-cae6e927baf9>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	15	96	1580
32	how 3d printing help housing construction	3D printing in construction is a breakthrough for housing because it can print concrete structures in a way that is fast, inexpensive, and environmentally friendly. This technology has evolved significantly since its initial development in 1980.	['In terms of scientific advancements, it would be hard to argue that the 20th century didn’t exceed every preceding century in many ways. Humans visited the moon, nuclear power was harnessed, the Internet came to fruition, just to name a few marvels. As we’re still early into the 21st, some budding technology is enough to create speculation that something even greater could materialize within the next decade.\nPerhaps current technology doesn’t seem as mind blowing when compared to the last century, at least just yet. However, many things that are part of our everyday lives we hadn’t envisioned just twenty years ago. For example, the evolution of smart devices has generated multiple spins on creative tech even sci-fi didn’t quite imagine in their current incarnations. Alongside smart devices, paralleling items from 3D video cameras, 3D printing, advanced AI, and others, have become topical in the last couple years.\nBelow, we’re going to look at four technologies with the greatest potential to disrupt the future.\nRenewable energy – water converting solar panels\nWhat good are technological advancements or anticipation of things to come without the power to make it all work? Considering reliance on aging infrastructure and companies lobbying to maintain reliance on fossil fuels, renewable energy seems neglected – yet, it has been a viable option for some time.\nHydroelectric first came into production in 1881 at the Schoellkopf Power Station by Niagara Falls. Wind power was first harnessed in 1887 by Professor James Blyth in Scotland then applied at the world’s first windfarm during 1980 in New Hampshire. Pacific Gas and Electric harnessed geothermal energy for the first large scale operation in 1960.\nBell Labs developed the first practical solar cell in 1954 which is arguably the most capable power source for circumventing electric grids altogether. Just last year, it was shown to have even greater potential thanks to an invention by a company called Zero Mass Water. Their Source Hydropanels siphon water from the air by utilizing nanomaterials to both provide energy and clean water.\n3D camera – not just a toy for entertainment\nEarly on, the application for 3D video only seemed to have application for entertainment purposes. Beginning in 1922 with the very first “3D film” – an illusion produced with the aid of stereoscopic filming – was aired in Los Angeles, even prior to color becoming a mainstream part of video.\nOver the decades, color and technology improved and the first 3D comic, Three Dimension Comics: Starring Paul Terry’s Mighty Mouse, would be published in 1953. This advent segued to more production of other 3D entertainment material, but it also spurred greater imagination for 3D technology.\nThe first time a hologram would be visually simulated for the public would be in the 1956 film, Forbidden Planet. This adaptation of virtual reality would later become a sci-fi staple with the holodeck room after Star Trek gained immense popularity. Today, we have practical devices for VR immersion and usage that extends beyond just entertainment.\nThe 3D camera is changing entertainment by adding another dimension to a visual experience, which is certainly the most fun part of the technology. Moreover, this added dimension is a highly practical tool for business as well. Companies are applying this technology for everything from collecting detailed surveys of new construction sites, reading body language during recruiting and interviewing, immersive product demonstrations and training, among many other applications.\n3D printing – resilient, affordable housing\nThe 3D printing movement started decades ago but didn’t start receiving the level of buzz seen today until the start of the current decade. Despite seeming like a recent development, the technology was first developed in 1980 with its first application taking place in 1986, thanks to an inventor named Chuck Hull.\nThroughout the 1990s, application was minimal though it did help some companies inexpensively build models for various prototypes. In the late 1990s and early 2000s, the medical industry began using the technology to print functioning organs. In 2005, the RepRap 3D printer, developed at the University of Bath, became the first open-source, 3D printing project, serving as a push to make the technology highly accessible.\nAt first, only plastics, soft metals, and biological mater could be printed but new advancements are making the application significantly greater. With the ability to print denser metals, companies can 3D print highly specific parts inhouse that would normally be time consuming and expensive to order elsewhere. In construction, the ability to 3D print concrete is a breakthrough for housing, as it is both fast, inexpensive, and environmentally friendly.\nAI – human thinking with less error\nThe idea of artificial intelligence originated in the early 20th century with early works beginning at Dartmouth College in the 1956. The zeal of the students led them to believe they could create a machine with intellect paralleling a human within 25 years but were stifled by the lack of computing power of that era and the following decades.\nWork on AI systems continued over the next several decades until the first useful product, known as XCON, was completed in 1980 for DEC (Digital Equipment Corporation), which acted as a functional ordering system, aiding in assembling their VAX computer systems. Fast forward to today, we have AI systems such as the Amazon Alexa in our homes, capable of interpreting complex voice commands.\nSome view Alexa as a novelty item, as it mostly adds convenience and it’s simply fun to use. Though that may be the case with Alexa in its current state, it’s familiarizing many us with “what AI can do.”\nThe next wave of highly useful and accessible AI will be Google Cloud AutoML, providing an image analysis system for business or personal use. The application allows someone to train a program with visual data for predictive capabilities, specific to their own needs, whether it’s analyzing machine parts for wear, aiding in maintenance scheduling, or capturing environmental data in an area to predict changes in an ecosystem.\nAll the technologies working together\nIt’s speculation right now but imagine this: Google’s AutoML will also be powerful enough to process data beyond simple images, eventually analyzing 3D videos. Remote environments or terrains that were typically uninhabitable could be analyzed in great depth, enabling low-impact structures to be built for a small price. Finally, solar power would be the primary resource for powering all the above technologies. Between renewable energy, 3D technology, and AI, we’re on the cusp of a new, more self-sufficient era.']	['<urn:uuid:647ec706-3b0f-4f7a-aa2c-199de748ca20>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:10:11.825225	6	36	1076
33	maritime liability time limits legal jurisdiction enforcement restrictions worldwide courts differences	In U.S. federal courts, the Limitation of Liability Act imposes a 6-month time bar after receiving written notice of a claim. The Fifth Circuit recently ruled this time bar is nonjurisdictional, meaning it doesn't affect courts' power to hear cases. However, for maritime liens internationally, there typically isn't a strict statute of limitations. Instead, courts rely on the principle of 'laches,' which considers whether a party's rights may be lost if they delay acting for a long time and that delay harms others. This creates a framework where U.S. maritime cases have specific statutory deadlines, while international maritime liens are governed by more flexible general maritime law principles.	['Fifth Circuit Overturns Precedent Deeming Maritime Statute’s Time Bar Jurisdictional\nThe 1851 Limitation of Liability Act allows qualifying vessel owners to limit their liability after maritime accidents. See 46 U.S.C. § 30501 et seq. For vessel owners covered by this statute, liability for an accident generally cannot exceed the value of the vessel plus pending freight involved in the accident. However, this statute imposes a time limit on vessel owners. While a vessel owner may bring a civil action under the Limitation of Liability Act in a federal district court, “[t]he action must be brought within 6 months after a claimant gives the owner written notice of a claim.” Id. § 30511(a). Previously, the Fifth Circuit had held that this time bar was a jurisdictional rule. In re Eckstein Marine Serv., L.L.C., 672 F.3d 310, 316 (5th Cir. 2012) (holding that “a challenge to the timeliness of a limitation action is a challenge to subject matter jurisdiction”). But earlier this month, the Fifth Circuit held that a U.S. Supreme Court case implicitly overruled Eckstein and that the Limitation of Liability Act’s time bar is nonjurisdictional. In re Bonvillian Marine Serv., Inc., No. 20-30767, 2021 WL 5708449, at *5 (5th Cir. Dec. 2, 2021).\nIn re Bonvillian concerns an allision in the Mississippi River. On January 19, 2019, a Bonvillian vessel allegedly allided with a crew boat owned by Baywater Drilling and injured crew member Joseph Pellegrin, Jr. Pellegrin sued Bonvillian on August 23, 2019 in Louisiana state court. On December 16, 2019, Bonvillian filed a Limitation of Liability Act petition in the Eastern District of Louisiana against Pellegrin and Baywater Drilling. After finding Bonvillian’s action untimely under the Limitation of Liability Act, the district court applied Eckstein in dismissing the action for lack of subject matter jurisdiction under Federal Rule of Civil Procedure 12(b)(1). See In re Bonvillian Marine Serv., Inc., 502 F. Supp. 3d 1078, 1083-84, 1088 (E.D. La. 2020).\nOn appeal, a Fifth Circuit panel reversed the district court. According to the panel, a 2015 U.S. Supreme Court decision—United States v. Kwai Fun Wong, 575 U.S. 402—implicitly overruled Eckstein. In Kwai Fun Wong, the Supreme Court deemed time limitations in the Federal Tort Claims Act (FTCA) nonjurisdictional because the statute lacked a “clear statement” on jurisdiction. Id. at 410. As the Supreme Court held, under a clear-statement standard, “most time bars are nonjurisdictional” claim-processing rules that did not “deprive a court of authority to hear a case.” Id.\nApplying Kwai Fun Wong, the Fifth Circuit held that the Limitation of Liability Act’s six-month time bar is a nonjurisdictional rule. According to the court, there is no clear textual indication that the time bar was intended to limit courts’ subject matter jurisdiction: the statute “speaks only to a claim’s timeliness, not to a court’s power.” 2021 WL 5708449, at *5 (citation omitted). Accordingly, the court held that it was “obliged to acknowledge the Supreme Court’s implicit overruling of Eckstein” and that the statute’s time limitation is a “mere claim-processing rule which has no bearing on a district court’s subject matter jurisdiction.” Id.\nAs such, the Fifth Circuit reversed the district court’s dismissal for lack of subject matter jurisdiction and remanded the case for further proceedings.', 'Posted: July 31, 2013 | By: David Weil, Esq.\nStatutes of limitation establish a deadline for pursuing legal action to enforce a claim. For example, in California, a lawsuit for breach of a written contract must be filed within four years, and a suit for breach of a verbal contract must be filed within two years.\nThese statutes exist to prevent a scenario such as our reader has described. Memories fade and supporting documentation disappears over time -- and when people face a deadline for initiating legal action, they are more likely to resolve conflicts while the facts are still fresh in everyone’s mind.\nUnfortunately, most maritime liens are not subject to any statute of limitations. This may seem odd, but we need to remember that recreational boating operates under a legal framework that was designed for commercial shipping and international trade.\nMaritime liens are not treated uniformly around the world, but generally speaking, a maritime lien against a container ship may be enforced anywhere in the world. A statute that requires a lawsuit to be filed within a certain period of time may be enacted by a particular country, but unless it is a part of an international treaty, it will have little or no effect in other countries.\nSo, there is no statute of limitations that will help our reader in the scenario described here. But there is another tool that is part of the general maritime law of most countries.\n“General maritime law” is a collection of maritime legal precedents that are handed down by courts over many years, and these precedents are often observed internationally --even where no statute or treaty would be relevant to a particular case.\nIn this case, we can look to general maritime law for the legal principle known as “laches.” The laches principle basically says that if you sit on your rights for a long period of time, and as a consequence of that passage of time other people are somehow harmed by your failure to act, you may lose those rights.\nIn this country, courts typically start a laches analysis by looking at the statute of limitations that would have applied in a non-maritime case, and then they look at the question of whether a party was prejudiced by the passage of time.\nLaches may seem to be a rather vague principle, but it does provide incentive for a lien claimant to move things along -- and it is very relevant to our reader’s case. Here, the repairman claims to have entered into a verbal agreement with a boat owner who has passed away, and he has no documentation or paperwork to support his claim. The passage of time has prejudiced the boat owner’s heirs, because they are unable to obtain any testimony or track down any paperwork that might contradict the repairman’s claim.\nNotably, in this case, a comparison to the relevant statute of limitations in California would not offer much guidance.\nThe agreement alleged by the repairman called for payment upon sale of the boat, and the boat had not yet sold. He could therefore argue that there was no breach of the agreement, and therefore the clock on the statute of limitations had not yet started to run. But the uncertainty of that provision of the agreement, coupled with the effect that the passage of 20 years had on the ability of both parties to present a case, would point to an effective laches defense, in this case.\nIn the end, a case like this is probably best handled through a compromise. The costs associated with litigating a case like this will be substantial -- and, in the end, those costs would likely exceed the amount of the claim. Regardless, both sides should consult with a qualified maritime attorney before taking this any further.']	['<urn:uuid:3fcab1bb-eeb5-4eb1-bd66-9eb98c8442e6>', '<urn:uuid:7fec62bd-d73d-412d-b562-ff91cd79d6fb>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:10:11.825225	11	108	1173
34	compare us house representatives versus uk house commons elections	While both the U.S. House of Representatives and British House of Commons members are elected by the people, there are key differences in their election systems. U.S. Representatives are chosen every second year as specified in the Constitution. In contrast, under the UK system, elections can happen at variable times since the Prime Minister can request to dissolve Parliament at any point or be forced into an election if they lose a vote of confidence in the House.	"['When the U.S. Constitution was being drafted, its writers had the British Parliamentary system to draw on. The British system was the system they were used to and had learnt since childhood. However, because the monarchy was one of the chief things that the former colonists had rebelled against, any vestiges of monarchy and most forms of concentrated power were avoided.\nBoth the U.S. and British political systems have a head of state, a court system and an upper and lower house. Both have constitutions that lay out the rules for government and the rights of the people. Both systems are democratic in nature in that governments are put in place and removed from power by the will of the people and both have systems of checks and balances to limit the power of any one branch.\nHead of State\nIn the U.S. political system, the president is the official head of state. The president is elected under the electoral college system. In the U.K., although the prime minister usually has the spotlight on political matters and is the official head of government, the queen or king is the official head of state. The queen officially signs off on acts of parliament and, just as the U.S. president delivers the State of the Union Address every year, the queen reads the ""Speech from the Throne,"" which is written by the prime minister. In modern government, the monarch is more of a ceremonial figurehead and it is unusual for any member of the royal family to directly interfere with the political process.\nThe Upper House\nThe United States has a Senate as the upper house of the legislative branch and the U.K. has the House of Lords. Under the U.S. system, each state, regardless of size, has two senators. Originally, senators were appointed by the governor of the state they represented but they are now elected to serve six-year terms. The House of Lords is very different. Members of the House of Lords are not elected. The 792 members of the House of Lords are members by inheritance, appointment or their rank in the Church of England; they are not elected and cannot be removed by popular vote. Otherwise they serve the same purpose as the U.S. Senate. They discuss, debate and vote on legislation passed by the lower house of the legislative branch.\nThe Lower House\nThe U.S. House of Representatives and the British House of Commons have a great deal in common. Each house is made up of representatives elected by the people. In both systems control of the lower house goes to the party that has the most seats. Under the U.K. system, the leader of the party with the most seats becomes the Prime Minister and the official head of the government. Under the U.S. system this person would be the Speaker of the House. One other key difference is elections. Under the parliamentary system, the prime minister can go to the crown at any point and ask to dissolve Parliament. If this is done an election is called. An election can also be called if the prime minister loses ""the confidence of the house."" This means that the prime minister lost a vote in Parliament on a matter of confidence. Matters of confidence are usually, but not always, over budgetary matters. If the prime minister loses a vote of confidence, the end result is an election.', '- Section 2 - The House\nThe House of Representatives shall be composed of Members chosen every second Year by the People of the several States, and the Electors in each State shall have the Qualifications requisite for Electors of the most numerous Branch of the State Legislature.\nNo Person shall be a Representative who shall not have attained to the Age of twenty five Years, and been seven Years a Citizen of the United States, and who shall not, when elected, be an Inhabitant of that State in which he shall be chosen.\nRepresentatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers, which shall be determined by adding to the whole Number of free Persons, including those bound to Service for a Term of Years, and excluding Indians not taxed, three fifths of all other Persons. [The bit in italics was modified by section 2 of the 14th Amendment Passed by Congress June 13, 1866. Ratified July 9, 1868.] The actual Enumeration shall be made within three Years after the first Meeting of the Congress of the United States, and within every subsequent Term of ten Years, in such Manner as they shall by Law direct. The Number of Representatives shall not exceed one for every thirty Thousand, but each State shall have at Least one Representative; and until such enumeration shall be made, the State of New Hampshire shall be entitled to chuse three, Massachusetts eight, Rhode-Island and Providence Plantations one, Connecticut five, New-York six, New Jersey four, Pennsylvania eight, Delaware one, Maryland six, Virginia ten, North Carolina five, South Carolina five, and Georgia three.\nWhen vacancies happen in the Representation from any State, the Executive Authority thereof shall issue Writs of Election to fill such Vacancies.\nThe House of Representatives shall chuse their Speaker and other Officers; and shall have the sole Power of Impeachment.\nWhat I learned/relearned here: (please don\'t laugh. I last looked at this in high school civics class in 1979.)\n:: House of Reps is every two years.\n:: This bit ""the Electors in each State shall have the Qualifications requisite for Electors of the most numerous Branch of the State Legislature."" means (I think) that whoever is qualified to vote for the biggest part of State government (as states set up their own governments a bit differently from one another) is also qualified to vote for the Federal Representatives. That is, a state can\'t say that Joe Citizen can vote for the State folks but not for the Feds.\n:: A Rep must be 25, a citizen for 7 years, and must live in the state they are representing. This all seems reasonable to me.\n:: An archaic -- but clear -- formula is set out to determine how many representatives each state gets to send. This formula also impacts taxation.\n:: If there is a vacancy, the boss of the state (usually governor) gets to pick the new reps.\n:: The House Reps can pick their Speaker and so forth, and only they can impeach (one another? the Prez?) I don\'t know - do you?\nA road map, so you know where we are in this tour:\n- Article 1 - The Legislative Branch\n- ~ Section 1 - Legislative Power\n- ~ Section 2 - House of Representatives\n- ~ Section 3 - Senate\n- ~ Section 4 - Elections of Senators and Representatives\n- ~ Section 5 - Rules of House and Senate\n- ~ Section 6 - Compensation and Privileges of Members\n- ~ Section 7 - Passage of Bills\n- ~ Section 8 - Powers of Congress\n- ~ Section 9 - Limits on Congress\n- ~ Section 10 -Limits on States\n- Article 2 - The Executive Branch\n- Article 3 - The Judicial Branch\n- Article 4 - The States\n- Article 5 - Amendment\n- Article 6 - Debts, Supremacy, Oaths\n- Article 7 - Ratification']"	['<urn:uuid:c2059a35-54d9-4af3-a441-b8049689fffe>', '<urn:uuid:2b707435-0f36-4dc9-bea2-ae6b710f9a18>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T19:10:11.825225	9	78	1236
35	How do the mortality rates and frequency of occurrence compare between strokes and heart disease in the United States?	Heart disease has a higher mortality rate than strokes in the United States. While strokes cause 795,000 cases annually with one occurring every 40 seconds, heart disease causes about 655,000 deaths per year, or one death every 36 seconds, making it the leading cause of death for men, women, and people of most racial and ethnic groups. Stroke is the fifth leading cause of death in the U.S., whereas heart disease accounts for 1 in every 4 deaths in the country.	['We have all heard medical professionals talk about stroke, but do you really know the facts?\nAn estimated 795,000 Americans will have a stroke each year, with a stroke occurring every 40 seconds. Stroke is the fifth leading cause of death in the U.S. and the No. 1 cause of disability. Of the 7 million stroke survivors in the USA, approximately two-thirds of them are disabled. During a stroke, brain cells die at a rate of 32,000 per second. Seeking quick treatment is the key to survival and reducing disability.\nStroke is a process that affects the blood vessels leading to and within the brain. You could essentially call a stroke a “brain attack.” The most common type of stroke is from a blockage in a blood vessel. This type of stroke is called ischemic.\nThe blockage could be from a clot that forms in the brain or a clot that formed somewhere else in the body and traveled to the brain. A transient ischemic attack, also called a mini stroke, is from a blockage that re-opens spontaneously. Symptoms from a TIA will typically last less than 1 hour.\nNever miss a local story.\nThe least common type of stroke is from bleeding in or around the brain. This type of stroke is called hemorrhagic. Hemorrhagic strokes carry a larger risk of death than ischemic strokes.\nHow do you know if you are having a stroke? The American Heart Association and American Stroke Association have developed the slogan, Act F.A.S.T. to detect the most common signs of stroke.\n• F. The “F” stands for facial droop. Look in a mirror or ask the person to smile. Look for a crooked smile or drooping of one side of the face.\n• A. The “A” stands for arms. Raise both arms out in front of you and look to see if one starts to fall.\n• S. The “S” stands for speech. Are words slurred, mixed up, or not coming out quickly and correctly?\n• T. The “T” stands for time. If any of these symptoms are present, then its time to call 911.\nIf you think you are having a stroke the most important thing is to quickly seek medical treatment. Dialing 911 will give you the best chance at arriving to a hospital fast and safe.\nTreatments for ischemic stoke include medication called tPA, mechanical clot retrieval and rehabilitation. A patient is only eligible for tPA if they present to the hospital within 4.5 hours from the start of their symptoms. Hemorrhagic stroke treatment includes reducing swelling in the brain, controlling blood pressure, and aneurysm coiling and clipping.\nGet to the hospital quickly for the best treatment options.\nThe best way to beat stroke is to prevent it. An estimated 80 percent of strokes are actually preventable. It is important to “know your numbers” so that you can reduce your risk of stroke. The numbers include: blood pressure, cholesterol, blood sugar and body weight. Take all medication as prescribed by your doctor and schedule regular check ups to keep your numbers in the optimal range. Additionally, try to exercise or walk 30 minutes a day, don’t smoke, and decrease you intake of salty and fried foods.\nThe Lee’s Summit Health Education Advisory Board is a Mayor-appointed, volunteer board that promotes and advocates community health by assessing health issues, educating the public and government agencies, developing plans to address health issues, encouraging partnerships and evaluating the outcomes.\n“Health is a state of complete mental, physical, and social well being and not merely the absence of disease and infirmity.” World Health Organization', 'Heart Disease Facts\nLearn more about heart disease and its risk factors. It’s important for everyone to know the facts about heart disease pdf icon[PDF-243K].\nHeart Disease in the United States\n- Heart disease is the leading cause of death for men, women, and people of most racial and ethnic groups in the United States.1\n- One person dies every 36 seconds in the United States from cardiovascular disease.1\n- About 655,000 Americans die from heart disease each year—that’s 1 in every 4 deaths.2\n- Heart disease costs the United States about $219 billion each year from 2014 to 2015.3 This includes the cost of health care services, medicines, and lost productivity due to death.\nCoronary Artery Disease\n- Coronary heart disease is the most common type of heart disease, killing 365,914 people in 2017.4\n- About 18.2 million adults age 20 and older have CAD (about 6.7%).3\n- About 2 in 10 deaths from CAD happen in adults less than 65 years old.4\n- In the United States, someone has a heart attack every 40 seconds.3\n- Every year, about 805,000 Americans have a heart attack.3 Of these,\n- 605,000 are a first heart attack3\n- 200,000 happen to people who have already had a heart attack3\n- About 1 in 5 heart attacks is silent—the damage is done, but the person is not aware of it.3\nHeart Disease Deaths Vary by Sex, Race, and Ethnicity\nHeart disease is the leading cause of death for people of most racial and ethnic groups in the United States, including African American, American Indian, Alaska Native, Hispanic, and white men. For women from the Pacific Islands and Asian American, American Indian, Alaska Native, and Hispanic women, heart disease is second only to cancer.5\nBelow are the percentages of all deaths caused by heart disease in 2015, listed by ethnicity, race, and sex.5\n|Race of Ethnic Group||% of Deaths||Men, %||Women, %|\n|American Indian or Alaska Native||18.3||19.4||17.0|\n|Asian American or Pacific Islander||21.4||22.9||19.9|\nAmericans at Risk for Heart Disease\nSeveral other medical conditions and lifestyle choices can also put people at a higher risk for heart disease, including:\nCDC Public Health Efforts Related to Heart Disease\n- State Public Health Actions to Prevent and Control Chronic Diseases\n- Million Hearts®external icon\n- CDC: Heart Disease Communications Kit\n- American Heart Associationexternal icon\n- National Heart, Lung, and Blood Instituteexternal icon\n- Centers for Disease Control and Prevention. Underlying Cause of Death, 1999–2018. CDC WONDER Online Database. Atlanta, GA: Centers for Disease Control and Prevention; 2018. Accessed March 12, 2020.\n- Virani SS, Alonso A, Benjamin EJ, Bittencourt MS, Callaway CW, Carson AP, et al. Heart disease and stroke statistics—2020 update: a report from the American Heart Associationexternal icon. Circulation. 2020;141(9):e139–e596.\n- Fryar CD, Chen T-C, Li X. Prevalence of uncontrolled risk factors for cardiovascular disease: United States, 1999–2010 pdf icon[PDF-494K]. NCHS data brief, no. 103. Hyattsville, MD: National Center for Health Statistics; 2012. Accessed May 9, 2019.\n- Benjamin EJ, Muntner P, Alonso A, Bittencourt MS, Callaway CW, Carson AP, et al. Heart disease and stroke statistics—2019 update: a report from the American Heart Associationexternal icon. Circulation. 2019;139(10):e56–528.\n- Heron, M. Deaths: Leading causes for 2017pdf icon. National Vital Statistics Reports;68(6). Accessed November 19, 2019.']	['<urn:uuid:af82d2a7-46f6-4a3c-a38a-46066e2285b0>', '<urn:uuid:6a7f99f9-4446-421d-b362-6b1a305eaa94>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T19:10:11.825225	19	81	1140
36	how do love waves rayleigh waves differ in their movement during earthquakes	Love waves and Rayleigh waves are both surface waves, but they differ in their movement patterns. While both types move the ground horizontally, only Rayleigh waves also move the ground vertically. These surface waves form long wave trains that can travel great distances and are responsible for most of the shaking and damage during earthquakes.	"[""When seismic waves reach the Earth's surface, they cause the ground, and anything sitting on it, to vibrate at certain frequencies. During an earthquake, a building will tend to vibrate around one particular frequency known as its natural, or fundamental, frequency. When the building and ground share the building's natural frequency, they're said to be in resonance. That's bad. Resonance amplifies the effects of an earthquake, causing buildings to suffer more damage. In September 1985, a temblor in Mexico City created waves with a frequency perfectly aligned to the natural frequency of a 20-story building. As a result, more buildings of this height were damaged than taller or shorter structures. In some cases, a damaged 20-story building stood right next to an undamaged building of a different height.\nThe Impact of Earthquakes on Buildings\nYou can get the full story on earthquakes in How Earthquakes Work, but a review of the basics will help here. Earthquakes occur when masses of rock in Earth's crust slip and slide against one another. This kind of movement is most common along a fault, a break in a body of rock that can extend for miles or even hundreds of miles. When pieces of crustal rock suddenly slip and move, they release enormous amounts of energy, which then propagates through the crust as seismic waves. At the Earth's surface, these waves cause the ground to shake and vibrate, sometimes violently.\nGeologists classify seismic waves into two broad categories: body and surface waves. Body waves, which include P and S waves, travel through the Earth's interior. P waves resemble sound waves, which means they compress and expand material as they pass. S waves resemble water waves, which means they move material up and down. P waves travel through both solids and liquids, while S waves only travel through solids.\nAfter an earthquake strikes, P waves ripple through the planet first, followed by S waves. Then come the slower surface waves -- what geologists refer to as Love and Rayleigh waves. Both kinds move the ground horizontally, but only Rayleigh waves move the ground vertically, too. Surface waves form long wave trains that travel great distances and cause most of the shaking -- and much of the damage -- associated with an earthquake.\nIf earthquakes only moved the ground vertically, buildings might suffer little damage because all structures are designed to withstand vertical forces -- those associated with gravity -- to some extent. But the rolling waves of an earthquake, especially Love waves, exert extreme horizontal forces on standing structures. These forces cause lateral accelerations, which scientists measure as G-forces. A magnitude-6.7-quake, for example, can produce an acceleration of 1 G and a peak velocity of 40 inches (102 centimeters) per second. Such a sudden movement to the side (almost as if someone violently shoved you) creates enormous stresses for a building's structural elements, including beams, columns, walls and floors, as well as the connectors that hold these elements together. If those stresses are large enough, the building can collapse or suffer crippling damage.\nAnother critical factor is the substrate of a house or skyscraper. Buildings constructed on bedrock often perform well because the ground is firm. Structures that sit atop soft or filled-in soil often fail completely. The greatest risk in this situation is a phenomenon known as liquefaction, which occurs when loosely packed, waterlogged soils temporarily behave like liquids, causing the ground to sink or slide and the buildings along with it.\nClearly, engineers must choose their sites carefully. Up next, we'll discover how engineers plan for and design earthquake-resistant buildings.""]"	['<urn:uuid:0b9448fc-f9b5-4a1e-88a3-c06d11221128>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	12	55	597
37	I'm worried about using medication for pain - what are the similarities in side effects between Black Cohosh and IP 109 pills when it comes to liver problems and tiredness?	Both medications can cause liver problems and fatigue as side effects. With Black Cohosh, symptoms of liver damage include being more tired than usual, feeling weak, loss of appetite, and yellowing of the skin. Similarly, IP 109 pills can cause liver problems with symptoms including unusual tiredness or weakness, loss of appetite, and yellow eyes or skin.	['Black Cohosh, also known as Black Snakeroot or Bugbane, is a medicinal root. It is used to treat women’s hormone-related symptoms, including premenstrual syndrome (PMS), menstrual cramps, and menopausal symptoms. Black Cohosh contains potent phytochemicals that have an effect on the endocrine system. How it works is not yet clear.\nBlack Cohosh is widely used in the United States, Australia, and Germany. The German government has approved it as a prescription alternative to hormone therapy. In Canada, Black Cohosh is available without a prescription. Be sure to talk to your doctor before you take it, especially if you have liver problems or if you develop symptoms of liver problems after using Black Cohosh. Symptoms of liver damage can include being more tired than usual, feeling weak, loss of appetite, and yellowing of the skin.\nYou can buy black cohosh as a standardized extract in 20 mg pill form, which is taken twice a day. Root, extract, and tincture forms are also available in health food stores.\nWhen black cohosh is used at regular doses, its only known side effect is occasional stomach discomfort. But black cohosh may have risks that are not yet known, including possible effects on liver function. More research needs to be done before experts can recommend it for long-term use.\nIs it effective?\nStudies on black cohosh have had mixed results. Some studies have shown that black cohosh can relieve menopause symptoms such as hot flashes. But other studies have shown that black cohosh does not relieve symptoms.\nThese mixed results may mean that black cohosh can relieve symptoms in some women, but does not relieve symptoms in others. Or the different results may be because different preparations were used in the studies.\nIn the studies where black cohosh relieved symptoms, it reduced hot flashes, night sweats, and sleep problems.\nIs it safe?\nLarge, long-term studies have not yet been done to confirm whether long-term use of black cohosh is safe. Because black cohosh has benefits somewhat like estrogen therapy, it may also have some risks like those of estrogen.\nExperts do not know for sure if black cohosh causes liver problems. But they have determined that black cohosh products should be labelled with a statement of caution. Stop using black cohosh if you notice that you are weak or more tired than usual, you lose your appetite, or your skin or the whites of your eyes are yellowing. Call your doctor because these symptoms may mean you have liver damage.\nIf you plan to take black cohosh, talk to your doctor about how to take it safely. You may be able to take it short-term (no more than 6 months), or possibly longer but with regular checkups to look for estrogen-related changes in the uterus and breasts.\nEstrogen may increase the risk of cancer in women who have a history of uterine cancer or breast cancer or who are at high risk for breast cancer. Since black cohosh may work in ways similar to estrogen, these high-risk women should avoid using black cohosh until more is known about the long-term risks.\nAs with any medicine, be careful to avoid overdosing with black cohosh. Symptoms of overdose include vertigo, headache, nausea, vomiting, impaired vision, and impaired circulation.\nWhat to avoid\nBlack cohosh should not be used during pregnancy or while you are breast-feeding. Do not take black cohosh if there is any chance that you might be pregnant.\nBlack cohosh should not be combined with birth control pills, hormone therapy, or tamoxifen. It should not be used by women who are allergic to ASA.', 'The active ingredient of IP 109 pill is a combination of hydrocodone and acetaminophen.\nIt is used when other painkillers do not work or to relieve pain severe enough to require opioid therapy.\nAcetaminophen, the active ingredient of the drug, is used to reduce fever and relieve pain in patients.\nHowever, this ingredient can cause unwanted effects (for example, liver damage) if taken too much.\nAnother ingredient, Hydrocodone, belongs to the group of drugs called narcotic analgesics (pain relievers).\nWhen taken to relieve pain, it acts on the central nervous system (CNS) and stops coughing.\nLong-term use of this ingredient can cause physical or mental dependence.\nWhen used under the supervision of a doctor, such drugs usually do not cause addiction, but using more than recommended by the doctor can cause addiction.\nIn some cases, the use of ip 109 pills is not stopped abruptly and the dose is gradually reduced and stopped.\nThis is because abrupt discontinuation of the drug can cause side effects such as severe withdrawal.\nThe risks and benefits to you will be weighed by the doctor when deciding to use this medicine.\nThe following conditions should be observed while using this drug.\nWhat Will We Learn?\nTell your doctor if you have ever had an allergic reaction to hydrocodone and acetaminophen in the medicine, or to any other medicine.\nInform your doctor if you are allergic to paints, foods or animals.\nCarefully read the instructions for use that come with the medicine.\nThere are no scientific studies on the effects of this drug in children.\nFor this reason, if this drug is to be used in children aged 2 and under, it should be discussed with the doctor.\nStudies have not found pediatric-specific problems that would limit the usefulness of the hydrocodone and acetaminophen combination in children aged 2 years and older.\nStudies have not been conducted with the components in the drug in children younger than 2 years old.\nThere are no studies that say this drug may be harmful in the elderly.\nBut in elderly people, ip 109 pills should be used with caution and in correct doses.\nPregnancy And Breastfeeding\nNo scientific studies have been conducted on the use of this drug during breastfeeding.\nIf you are going to use this medicine while breastfeeding, the potential benefits must be weighed against the potential risks.\nUsing this medicine during pregnancy may cause serious unwanted effects in your baby, including neonatal withdrawal syndrome.\nOther foods, such as medicines, can affect the way this medicine is used.\nUsing ip 109 pills with any of the following is not recommended but may be necessary in some cases.\nIf used in combination, your doctor may change the dose or how often you use this medicine, or give you special instructions about the use of food, alcohol, or tobacco.\n- Grapefruit Juice\nOther Medical Issues\nAny problems with your health may affect the effect of this medicine.\nIn particular, be sure to tell your doctor if you have or have had any of the medical problems listed below:\n- Addison’s disease (adrenal gland problem)\n- Abuse of alcohol\n- Brain tumor\n- Lung or respiratory problems (eg COPD, asthma, apnea, emphysema, hypoxia\n- Cor pulmonale (severe heart condition)\n- CNS depression\n- Drug addiction\n- Enlarged prostate (prostatic hypertrophy, BPH)\n- Head injury\n- Increased pressure in the head\n- Hypothyroidism (underactive thyroid)\n- Problems urinating\n- Asthma or acute\n- Respiratory depression (breathing problem)\n- Stomach or intestinal obstruction\n- Low blood pressure (hypotension)\n- Swelling of the pancreas (pancreatitis)\n- Seizure (or history of seizures)\n- Kidney disease\n- Liver disease\nHow To Use IP 109 Pills?\nIt is very important that you take this medicine only on the advice of your doctor.\nDo not take more or less than the dose recommended by your doctor.\nYou may be more sensitive to such drugs, especially if you are an elderly person.\nAs mentioned above, you may experience mental or physical dependence if you use this medicine for a long time.\nThe active ingredient in the drug, acetaminophen, can cause liver damage if taken in large amounts.\nThis medicine you will use will come with the leaflet inside.\nRead the instructions carefully and ask your doctor any questions you may have.\nIf you are using any other medication, check the ingredients on the cap of the medication.\nIf any of the drugs contain acetaminophen components, contact your doctor before using the drug.\nIt is not safe to use more than 4 grams (4,000 milligrams) of acetaminophen in 24 hours.\nThe dose of this medicine will vary depending on the severity of your disease.\nThe information listed below is only the average dose.\nIf your dose is different, apply the dose your doctor told you.\nFor moderate to severe pain:\nAdults – the usual dose is 1 or 2 capsules every 4 to 6 hours. Your doctor may increase this dose as needed.\nThe maximum daily dose is usually no more than 8 capsules.\nChildren – If children are going to use this medicine, the dose should be determined by your doctor.\nIf you miss a dose of this medicine, it will be taken as soon as you remember, unless it’s time for the next dose.\nWhen the next dose is due, skip the missed dose and continue with your normal dosing schedule.\nDo not take a double dose to make up for the missed dose.\nKeep this medicine tightly closed in the container it came in.\nMake sure you keep it out of the reach of children.\nAsk your pharmacist how to dispose of expired medicines correctly.\nIt is very important that your doctor check your or your child’s condition while using ip 109 pills, especially during the first 24 to 72 hours of treatment.\nThis is because it helps your doctor understand the effect the medicine will have on you or your child.\nDepending on the body’s reaction, the doctor will tell you whether to use the drug or not.\nDo not use this medicine if you have used any MAO inhibitor (Marplan®, Eldepryl®, etc.) in the past 14 days.\nIt is illegal for someone else to use this medicine, so do not share your medicine with anyone else.\nGenerally, drug addicts will want to use or steal this drug.\nIp 109 pills will add to the effects of alcohol and other CNS depressants (medicines that can make you drowsy or less alert).\nSome examples of CNS depressants are listed below:\n- Allergies or cold medicine\n- Sleeping pills\n- Narcotics and other prescription pain medications\n- Medicines for seizures or barbiturates\n- Muscle relaxants\n- Some dental anesthetics (numbing drugs)\nYou are more likely to experience liver damage if you drink 3 or more alcohol per day while taking this medicine.\nIt is best to avoid alcoholic beverages while taking this medicine.\nIf you use the drug several times and see that it does not work, contact your doctor.\nIf you think you have overdosed on the medicine, seek medical help right away.\nDoctors can usually use naloxone (a type of medicine) in case of an overdose of such drugs.\nIp 109 pills can cause sleep-related breathing problems (eg sleep apnea, sleep-related hypoxemia).\nIf you have sleep apnea (stopping breathing for a short time during sleep), your doctor may need to change the dose of this medicine.\nTherefore, contact your doctor.\nIp 109 pills can cause adrenal gland problems.\nCheck with your doctor right away if you have skin darkening, dizziness, diarrhea, fainting, loss of appetite, mental depression, nausea, skin rash, unusual tiredness or weakness, or vomiting.\nCheck with your doctor right away if you have pain or tenderness in the upper stomach, dark urine, pale stools, loss of appetite, nausea, unusual tiredness or weakness, or yellow eyes or skin.\nThe symptoms listed above may be signs of serious liver problems.\nIn rare cases, this drug may cause serious skin reactions (eg, acute generalized exanthematous pustulosis, toxic epidermal necrolysis, Stevens-Johnson syndrome).\nTalk to your doctor right away in the situations listed below:\n- Blistering, loosening or peeling of the skin\n- Muscle or joint pain\n- Irritated eyes\n- Red skin lesions\n- Sore throat or sores\n- Ulcers on the lips or mouth\n- White spots\n- Unusual tiredness\nIp 109 pills can cause a serious allergic reaction called anaphylaxis, which can be life-threatening.\nCall your doctor right away if you have trouble breathing, a rash, itching, hoarseness, trouble swallowing, or any swelling of your hands, face, or mouth while using this medicine.\nDizziness or fainting may occur when you suddenly get up from a sitting or lying position.\nIt might be a good idea to get up slowly to avoid this problem.\nLying down for a while can also help prevent dizziness.\nThis medicine may cause drowsiness and dizziness.\nUntil you know how this drug affects you, it is best to avoid driving or doing any activity that could be dangerous.\nProlonged use of such drugs (narcotics) can cause severe constipation.\nTo prevent constipation, your doctor may direct you or your child to take laxatives, drink lots of fluids, or increase the amount of fiber in your diet.\nIf these recommendations are not taken into account, constipation can cause more serious health problems.\nBefore having any medical test, tell your doctor that you are using this medicine as it may affect the test results.\nDo not increase or decrease the dose of the medicine without telling your doctor.\nIf you are going to stop the drug completely, your doctor may reduce the dose gradually.\nThis is to minimize the risk of withdrawal symptoms such as stomach cramps, anxiety, fever, nausea, runny nose, sweating, chills or trouble sleeping.\nTell your doctor right away if your child has abnormal sleep patterns, irritability, diarrhea, high-pitched crying, shaking or shaking, sneezing, weight loss, or vomiting.\nIf you become pregnant or think you may be pregnant while using the drug, contact your doctor immediately.\nCheck with your doctor right away if you see or hear restlessness, anxiety, rapid heartbeat, fever, sweating, muscle spasms, twitching, nausea, vomiting, diarrhea, or anything that isn’t there.\nThe unwanted effects listed above may be symptoms of a serious condition called serotonin syndrome.\nThe use of another drug that affects the serotonin levels in your body with this drug may cause more serious problems.\nUsing too much of this medicine can cause infertility.\nTherefore, if you are planning to have children, contact your doctor before using this medicine.\nDo not use any medicine together with this medicine without talking to your doctor.\nHerbal products or vitamin supplements are also included in this list of medications.\nSide Effects of Ip 109 Pills\nLike any medicine, this medicine can cause some undesirable effects as well as its necessary effects.\nWhile some undesirable side effects do not require medical attention, some side effects can cause serious health problems.\nIf any of the following side effects occur, contact your doctor without waiting:\n- Changes in mood\n- Difficulty having a bowel movement (constipation)\n- Fear or irritability\n- Feeling of indigestion\n- Hearing loss\n- Hearing impairment\n- Chest pain\n- Unusual dullness, insomnia, tiredness, weakness or lethargy\nThe side effects listed below are rare, but if these side effects occur, seek medical attention immediately:\n- Leg, back, or stomach pains\n- Black, tarry stools\n- Bleeding gums\n- Peeling, bleeding, or loosening of the skin\n- Blood in urine or stool\n- Blood in vomit\n- Bluish skin or lips\n- Dark urine\n- Darkening of the skin\n- Decreased frequency of urination\n- Decrease in urine volume\n- Shortness of breath\n- Difficulty urinating\n- Difficulty swallowing\n- Fast heart rate\n- Fever with or without chills\n- General body swelling\n- The general feeling of tiredness or weakness\n- Rapid, irregular, or slow or shallow breathing\n- Joint or muscle pain\n- Light-colored stool\n- Loss of appetite\n- Back pains\n- Mental depression\n- Nose bleeding\n- Overactive reflexes\n- Difficult or painful urination\n- Pale or blue lips\n- Swelling of the eyes or eyelids, face, lips or tongue\n- Red irritated eyes\n- Red skin lesions (usually with a purple center)\n- Severe or ongoing stomach pain\n- Skin rash, hives, or itching\n- Throat ache\n- Ulcers, sores, or white patches on the lips or mouth\n- To sweat\n- Talking or acting with uncontrollable excitement\n- Chest tightness\n- Difficulty speaking\n- Unusual bruising or bleeding\n- Unusual weakness or tiredness\n- Yellow eyes and skin\nIt should be noted that the above-listed side effects are not a complete list.\nContact your doctor if you experience any side effects.\nIt should also be noted that the side effects listed above are not experienced by everyone.\nSymptoms listed below are signs of overdose of ip 109 pills:\n- Bloody or cloudy urine\n- Change of consciousness\n- Chest pain or discomfort\n- Cold and moist skin\n- Cough with pink frothy sputum\n- decreased awareness\n- difficult breathing\n- Extreme drowsiness\n- The general feeling of sickness or discomfort\n- Increased sweating\n- Rapid, irregular, or slow or shallow breathing\n- Irregular heartbeat\n- Dizziness or fainting\n- Loss of consciousness\n- Severe changes in blood pressure or heart rate\n- Difficulty breathing\n- Pale or blue nails, lips, or skin\n- Severe sleepiness\n- Slow or irregular heartbeat\n- Cardiac arrest\n- The sudden decrease in the amount of urine\n- Swelling in the ankles or legs\n- Unpleasant breath odor\nYou can report side effects to the FDA at 1-800-FDA-1088.\nYou may also notice our article about Terfamex capsules used for weight gain.']	['<urn:uuid:b21815d2-7344-43db-ba09-2fc819736837>', '<urn:uuid:bdda12bd-f2a3-4915-a626-43bc245f8a47>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:10:11.825225	30	57	2882
38	What was the estimated death toll range in Iraq 2003-2011?	The civilian death toll estimates in Iraq between the US invasion in 2003 and the drawdown in 2011 ranged between 160,000 and 1.2 million, depending on the source.	['In a post-conflict Syria, it may become possible to more accurately quantify the conflict\nSeptember 21, 2018\nSource: Syria Justice and Accountability Center\nThe plight of thousands of detained Syrians has come to public attention in recent months, as the government has begun to update civil registries with previously unreported deaths in custody. Many of the news reports covering this issue include the number of those who are currently believed to be held in government-run detention facilities. Such numbers must be treated with great care. While there is constant pressure from journalists and diplomats to provide exact numbers, whether it be of detainees, displaced people, or conflict-related deaths, the nature of the conflict makes it incredibly difficult to calculate accurate tolls at this time. In lieu of these numbers, organizations should shift discussions towards information that is often more integral to understanding the conflict, including reporting on the types of violations being committed, analyzing the patterns of those violations, and highlighting stories of individual survivors.\nKeeping track of precise numbers of deaths and victims is always a challenge in a conflict setting. Conflict can make it dangerous or impossible to access affected areas and local institutions that normally track deaths, such as hospitals, may be overwhelmed. In some developing countries, reliable population statistics may not have been available prior to the conflict, making calculating losses difficult. To this day, estimates of the civilian death toll in Iraq between the US invasion in 2003 and the drawdown in 2011 differ drastically, between 160,000 and 1.2 million, depending on the source.\nIn 2014 the United Nations announced it had stopped updating its death toll for the conflict in Syria due to the difficulty of verifying and accessing information on the ground. A host of Syrian documentation organizations continue to track deaths in the conflict, but they face similar challenges. Such documentation efforts are integral to informing both current policy discussions and future accountability mechanisms, but they should not be seen as comprehensive.\nWhen organizations provide numbers of the overall death toll, for example, it often reflects only the specific names the organization has documented as deceased. In other cases, organizations are estimating a total toll based on a combination of recorded names along with media reports and eye witness testimonies. Benetech has published a number of analyses comparing casualty records reported by different documentation organizations. Benetech highlights the challenges to calculating tolls, including the presence of duplicate reports in data sets, the geographic collection biases of individual organizations, and the possibility of false reports. False reports often result from unintentional human error or incomplete information available at the time of documentation. For example, SJAC conducted an interview with a woman who had incorrectly believed her son was in a government prison. After a long search, including paying bribes to security officials, she was told exactly where he was being held. Then, three years after his disappearance, his body was discovered. He had never been in government custody, and rather was killed by a non-state armed group. Such inaccuracies, while tragic, are inevitable in a context where so many are missing.\nOrganizations should ensure that when they report publicly on documentation they are honest about the limitations of their data, and do not claim to be able to offer a comprehensive picture of the conflict. To this end, groups should publish their methodology, and rely on an estimated range of numbers instead of claiming to provide exact figures. A good example of this standard can be seen in the Amnesty International report ‘It breaks the human: Torture, disease and death in Syria’s prisons,’ which estimates the detainee population at 17,723. This estimate was provided by the Human Rights Data Analyst Group (HRDAG), and was published alongside an in depth analysis of how the number was reached, including an open discussion of the possible margin of error. While this level of analysis may not be possible for smaller organizations, any organization that publishes numbers must be able to provide a basic methodology. The responsibility to maintain this level of openness in data collection falls not only to groups that report on such data, but journalists and diplomats, who regularly insist on numbers even when such information is impossible to obtain, and disseminate such findings with little consideration for the methodology or accuracy of the source. Ultimately, the pressure for numbers of detainees should be placed on the Syrian government and rebel groups, not on civil society organizations.\nAside from methodological challenges, it is also important to consider the value of focusing on these figures. While such numbers certainly play a role in understanding the conflict and advocating for justice, those reporting on these issues should consider more nuanced approaches. Patrick Ball of HRDAG has explained the importance of going beyond death tolls in seeking justice in Syria, writing, “The total number of violent deaths tells only a small part of the story. Beyond the magnitude of the horrors that have befallen Syrian civilians, it is even more important to understand the patterns of violence: Where in Syria is the violence worse? Are deaths decreasing over time or increasing? Does violence mostly affect urban elites or people in rural areas?” In addition to understanding these patterns from a macro level, public discussions should also strive to tell the individual stories of victims and their families, stories that can often be lost in statistics. Additionally, providing numbers, which always include some level of inaccuracy, can open organizations up to critiques which often serve to undermine the rest of their valuable reporting, analysis, and advocacy. For these reasons, SJAC has chosen not to publish statistics from its database, rather utilizing its documentation to inform broader reports on violations in Syria.\nIn a post-conflict Syria, it may become possible to more accurately quantify the conflict, calculating the total number of dead and missing, as well as those who experienced particular types of violations. Such a project will likely require greater access and transparency and could be an important part of transitional justice processes. Certainly, current documentation efforts may be able to inform future reporting, but in the short-term, these numbers mislead the public about the conflict itself, and about the capabilities of documentation organizations to collect information amid ongoing violence.']	['<urn:uuid:b3480eb1-ae32-4896-8cd1-4e4a75a97fe2>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	10	28	1042
39	female jazz musician stereotypes and experiences	Historically, women in jazz were stereotypically seen as non-instrumentalists, vocalists, sex objects, and wives rather than composers. Female instrumentalists were marginalized and often only remembered through their marriages to male musicians. However, women have always played jazz as instrumentalists. Some female musicians internalized these stereotypes, with quotes like 'I'm just a jazz musician' or 'I'm just one of the guys' reflecting a denial of existing patriarchal structures. Jazz used to be dominated by male ideals and male players, with women playing only a minor role in public perception.	['Conference, exhibition, workshop, concerts\nGender and Identity in Jazz\nThe speakers have been chosen; here is an overview of the papers.\nJazz used to be a predominantly male music. Not only were most of the musicians male, but its aesthetics and social environment was dominated by male ideals and male players as well. In the public perception of this music women as well as other groups or identities not compliant with the male orientation of jazz’s origins played only a minor role. Strong female instrumental voices, for instance, or musicians with a LGBT background were marginalized both by the media and by the jazz scene, seen as an exception or celebrated as a fig-leaf for the alleged openness of the music.\nCelebrating the Jazzinstitut’s 25th anniversary, the 14th Darmstadt Jazzforum will approach the gender topic from different sides. We are aware of the fact that there is no “female jazz” or “male jazz”, that music in itself does neither have a gender nor a sexual orientation. And yet our identity which we acquired in our respective environments are highly influential on how we express our creativity, how we think about art and music, which associations we may have with specific genres if not even with specific sounds. “I don’t care whom you’re screwing”, said the pianist Orrin Evans in September 2014 at the first “Queer Jazz Festival” in Philadelphia, “as long as you’re screwing somebody” – music, after all, is a taking place between people, it’s not a hermit’s art.\nHow, then, is our identity forming our understanding of jazz? Or to be even more precise: Is jazz really a man’s music? And if so, where exactly do its male attributes come from? Is some kind of emphasis on masculinity in the African-American community one of the reasons for the stereotype of jazz as a male art form? How can such an attitude be described – and how does it translate into other cultures? Why, for instance, doesn’t the slow softening of masculine values in global pop music since the 1970s have a stronger effect on jazz? Or is this actually happening and we just don’t notice it because of the general changes we experience around us? Are there musical qualities which are determined through identity (if not through gender)? We know about and acknowledge gender-typical approaches and methods of problem-solving in many other fields; can we identify such in music? Do men play more aggressively, are women more anxious to reach a consensus? Are words such as “empathetic” or “forceful” clearly linked to specific gender characteristics? What is the difference between the self-view and the independent view of this topic? How does one deal with the phenomenon that a musician such as Gary Burton makes clear that, of course, he does not play “gay jazz”, yet acknowledges that after his coming-out many of his colleagues told him he sounded much “freer”?\nHow, then, does one take the roles one is playing in the real world along into an art form which is about “playing yourself” on the one hand and which deals with an open kind of communication of specific individuals on the other hand? Jazz, after all, is one of the most individual approaches in the music field; it seems odd to argue that one’s personal background has no influence whatsoever on the musical result. “Where you come from is where you go to”, is at least part of the rule: Whoever you are, will define what and how you will play and perform.\nAt our 14th Darmstadt Jazzforum we plan to look at different views on this complex field of topics. We will focus on three thematic blocks. (1) We will discuss topics such as masculinity / gender / intersectionality / identity. (2) We will invite some analytical case studies, in which the art of specific musicians is being approached without first looking at the gender aspect of their music. (3) A third block is to bring us into the lived-in reality both of days gone by and of today’s world, allow for focused views into jazz history and for conversations with men and women active on today’s jazz scene.\nThe view of jazz musicians and their art may be distorted if we reduce them to any parts of their identity, be it their gender, their sexual orientation, their ethnicity, or anything else. However, to ignore these facets, be it in jazz history or today’s jazz scene, is a proof of neglect as well. At the 14th Darmstadt Jazzforum we hope to contribute to a discourse which is and remains important in our changing modern world.\n„GENDER_IDENTITY“ is hosted by the Jazzinstitut Darmstadt, a cultural institution of the City of Science Darmstadt.\nWe are co-operating for the 14. Darmstädter Jazzforums with the „Frauen machen Musik e.V.“, the Kulturzentrum Bessunger Knabenschule, WAGGONG Frankfurt e.V., the Centralstation Darmstadt and the Romanfabrik in Frankfurt/Main', 'Katie Van Note, Staff Writer\nMarch 5th, 2017\nWhat does a stereotypical American jazz musician look like? This is the question Naomi Moon Siegel asked that prompted conversation in Jeffrey Chapell’s jazz ensemble class last Tuesday, February 21st. Siegel is a composer, trombonist, and educator who visited Goucher College to present a workshop titled “Gender Equality and The Feminine Principle in Jazz.”\nSiegel received her bachelors degree in jazz trombone from the Oberlin College Conservatory of Music. It was through her training that she realized conversations about patriarchy, sexuality, race, and gender – very much present among students in the liberal arts college – were not present in the conservatory. The majority of her teachers and fellow students were white males. In her music history classes, she learned about men. All of the books she read were written by men. If there was ever a section in a textbook on female musicians, it was given a special label, “Women in Jazz.” Yet, as Naomi stated last Tuesday, “women have always played jazz as instrumentalists.”\nSo, who is a typical jazz musician in America? Who is given space to sing, play the piano, guitar, drums, flute, clarinet, trumpet, or saxophone? This is where the conversation started at Goucher.\nEight of the attending jazz student musicians were given the task to identify stereotypes of various races and genders in jazz. Siegel asked, “What images and messages does the society receive at large about these groups of people in jazz?” Students identified these stereotypes about men: “they are white, instrumentalists, intelligent, they have an expected level of know-how, they are cool cats, aloof, elitist, middle-class, most able, and most visible.” One Goucher musician added, “they can afford gigging around,” as yet another symptom of privilege and class.\nStereotypes of women in jazz included “non-instrumentalists, sometimes pianists, sex objects, vocalists, wives, and non-composers.” It is important to note that female instrumentalists, such as Lil Hardin Armstrong and Alice Coltrane, both jazz pianists in their own right, were known for their marriages to their jazz musician husbands. Within the first two sentences of their descriptions on Wikipedia, they are mentioned as wives to John Coltrane and Louis Armstrong, whereas both men are described on Wikipedia by musical style and accomplishments with no mention of marital details. This begs the question – would these female musicians have been documented and remembered in jazz history if they hadn’t married male musicians?\nStudents also offered stereotypes of African Americans in jazz as “natural, the best jazz musicians, best sense of rhythm, and the originators.” Furthermore, stereotypes were discussed of Asian Americans as “classically trained piano players, can’t swing, and non-existent in jazz,” while Native Americans, Arab-Americans, Latino-Americans, were all labeled as “non-existent in jazz” as well. Siegel noted that greater intersectionality between sexuality, gender identity, and cultural backgrounds were not mentioned either, as they further separated a person from the “norm” in jazz.\nYet, how have these stereotypes developed over the hundred or so years of American jazz history? Siegel identified the creation and distribution of magazines in the 1920s as a major cause – propaganda that sexualized women vocalists and prioritized white bands.\nIn her lecture, Siegel explained her own internalized stereotype as “socialized to believe that females are inferior jazz instrumentalists.” She gave examples by quoting her fellow female jazz musicians, Esperanza Spalding and Kate Olsen: “I’m just a jazz musician,” and “I’m just one of the guys.” In reflection, Siegel pointed to the implicit meaning behind their quotes: denial of the patriarchy “as if somehow it doesn’t exist.”\nOne female vocalist in the audience told an account of her own experience: “My mom has always said she sees me lying on a piano in a slinky red dress singing jazz.”\nAs individuals in the jazz arena, Siegel noted the importance of “telling counter-narratives.” These counter-narratives serve as challenges to the perpetual stereotypes marginalized groups face in jazz. She emphasized the development of an individual voice and sound. “My goal is for us to be fully expressive.Only in defining and challenging these stereotypes can we begin to discover our potential as musicians.”\nIn creating a space for dialogue of this kind, Siegel left some students with another perspective, some with a validation of experience as female and black musicians, and some with inspiration to challenge the concept of a stereotypical jazz musician.']	['<urn:uuid:239f0262-7612-44f2-92c0-cc86599a529a>', '<urn:uuid:189e7c89-65e2-4491-af9e-f9915ecf3d56>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:10:11.825225	6	88	1540
40	union army size active soldiers 1861	According to the Secretary of War, the Unionist strength stood at 660,971 men, of whom 640,637 were volunteers.	['December 1861 saw a continuing fraught relationship between the president, Abraham Lincoln, and the man he appointed as overall commander of the North’s army, General McClellan. Lincoln continued to question the timidity of McClellan’s approach while McClellan continued with his assertion that if got it wrong, the whole of the North could suffer as a consequence.\nDecember 1st: President Lincoln expressed his concern to General McClellan that the Unionist armies did not seem to be doing anything substantial.\nDecember 2nd: Congress gave its permission for the suspension of habeas corpus in Missouri.\nDecember 3rd: Lincoln gave his State of the Union address to Congress. The Union started its move against New Orleans when ‘USS Constitution’ arrived at Ship Island at the mouth of the Mississippi River carrying the 26th Massachusetts Regiment.\nDecember 4th: Great Britain announced an embargo on all exports to the US\nDecember 5th: The Secretary of War announced that Unionist strength stood at 660,971 men of whom 640,637 were volunteers.\nDecember 6th: It was announced that the Treasury could cope with a war that ended by mid-1862 but if it lasted longer than this then the Treasury’s income would be far outweighed by its outgoings and taxes on most things would have to be increased to fund the war.\nDecember 7th: In a scene that mirrored the ‘Trent’ incident, the ‘USS Santiago de Cuba’ stopped a British ship, the ‘Eugenia Smith’ and a Southerner called J W Zacharie was taken off. Zacherie was a purchasing agent for the Confederacy.\nDecember 9th: The Senate approved the setting up of the Joint Committee on the Conduct of the War. This recognised that previous comments made to the Confederacy, that states rights would not be interfered with once the war was over, was no longer the case and that the internal affairs of the rebel states would be reformed regardless and that the Union would be upheld.\nDecember 10th: The Confederacy admitted Kentucky to its membership despite the overwhelming evidence that the state was about to fall to Unionist forces.\nDecember 11th: Charleston was damaged as a result of a fire that swept through the city. Charleston was the most important port in South Carolina.\nDecember 12th: The success of the Union’s navy along the South’s coastline was such that cotton farmers started to burn their crop in fear that it might fall into the hands of the Union.\nDecember 15th: Congress expressed its view that the use of slavery in the South was becoming more and more an issue. The original cause of the war was state’s rights but greater knowledge in the Union about slavery put it at the forefront of why the war was being fought. Over the next few months Congress passed a number of laws such as the military could no longer return to the South fugitive slaves; that slavery was to be outlawed in the District of Columbia; that any slave state that offered to give up slavery would be given financial assistance from the Union.\nDecember 18th: Lord Lyons, the British ambassador in Washington DC, received a message from the British government that he was to demand the release of Mason and Slidell. If the Union failed to do this within 10 days, he had instructions to break off diplomatic relations.\nDecember 19th: Lyons met the US Secretary of State, Seward.\nDecember 20th: Two British warships arrived in Canada as a result of the ‘Trent’ affair.\nDecember 21st: The meetings between Lyons and Seward continued. Both Seward and Lincoln recognised that there was a real risk of war with the British if their demands went unheeded.\nDecember 23rd: The Cabinet was advised by Seward that Captain Wilkes made an error in taking off Mason and Slidell and that he should have brought in the ‘Trent’ and its ‘contents’ as the ship had violated its neutral status. Seward made it clear that the seizure of the Confederate commissioners was unlawful whereas the seizure of the ‘Trent’ as an entity would have been lawful.\nDecember 24th: Congress passed a series of duties that were to be added to tea, coffee, sugar and what were classed as “luxury goods”.\nDecember 25th: Despite it being Christmas Day, the Cabinet and the President were in discussions on what to do with Mason and Slidell. Fighting was reported at Fort Frederick in Maryland and Cherry, western Virginia.\nDecember 26th: It was announced that Mason and Slidell would be released because their arrest was illegal. It was further announced that Captain Wilkes had acted without the knowledge of the government.\nDecember 30th: Mason and Slidell were handed over to Lord Lyons. They were immediately put on a ship to England. Lyons then released his own interpretation of the law regarding “neutral nations” and it was at odds with Seward’s and, ironically, found support among many Americans. However, with the issue resolved, relations between the North and Great Britain improved.\nDecember 31st: President Lincoln pressed his army commanders for more action. However, McClellan did not hear his message as he was ill.']	['<urn:uuid:e3493ebf-8eab-42d3-bc8a-da2a32726db1>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	6	18	841
41	How are old coins stored safely and shown to people?	Rare coins like those in the Tyrant Collection are displayed in public exhibitions for educational purposes, such as at the 2018 Long Beach Expo. When handling valuable items like these, it's important to minimize touching them as oils from hands can cause permanent damage. They should be stored in cool, dry areas away from places with temperature fluctuations, moisture, or excessive heat like attics, basements, foyers, kitchens, and bathrooms.	"['Many of the historic English coins that will be displayed at the February 22-24, 2018 Long Beach Expo (www.LongBeachExpo.com) in the upcoming inaugural exhibit of rarities from the privately owned Tyrant Collection are so rare that normally they may be seen only in European museums. Some of the coins in the “Tyrants of the Thames” exhibit have never before been publicly displayed in the United States.\nFor three days, though, it will be possible to examine, up close, examples of these coins as well as popular styles and designs such as superb coins of Queen Victoria. The anonymous owner of the Tyrant Collection wants the public to see these coins in person for their educational value.\nVeteran professional numismatist and English coinage specialist Bruce Lorich, on behalf of the Tyrant Collection’s anonymous owner, has cataloged the more than 500 “Tyrants of the Thames” exhibit coins to be displayed at the February 2018 Long Beach Coin, Currency, Stamp & Sports Collectible Expo in Long Beach, California. This is the fourth in a series of four articles by him about the historical significance of some of the coins in that unprecedented exhibition.\nGleaming Gold Coins: Treats for the Eye\nHenry VIII Sovereign: A gold sovereign of King Henry VIII, issued 1526-1544 and the largest coin of his turbulent reign, is one of the great treasures in the Tyrants of the Thames” exhibit at the February 22-24, 2018 Long Beach Expo.\nPhoto credit: Professional Coin Grading Service\nAmong the many special qualities of this exhibit will be exceptionally fine examples of gold coins issued by England’s kings and queens, including almost every style and denomination of coin made by hammer during the Middle Ages and Renaissance. These historical coins present viewers with images of the rulers as well as emblems of royalty which are part of the coins’ designs. Dozens of hammered gold rarities will be on view, among the most important being a marvelous, lustrous gold sovereign (1526-1544) of King Henry VIII certified and graded PCGS MS-63.\nBeginning with Oliver Cromwell’s coinage of 1656-58, England turned to modern-looking coins made by machine, called “milled” issues. The exhibit will portray the entire history of milled gold coinage beginning with one of the finest known of Cromwell’s gold Broad of 1656.\nThe exhibit continues chronologically, from Charles II to Elizabeth II. Most impressive of course are the largest pieces, called 5 guineas from the 1660s through the 1770s. Every type is present in this collection and will be on display. These include the extremely rare 5 guineas 1670 proof of Charles II, 5 guineas of all the styles of the Stuart kings and queens including a wonderful example of the exquisite 1701 Fine Work design for William III. There are examples of all the 5 guinea types minted for the Hanover monarchs beginning with George I, perhaps the most spectacular coin being the pattern Young Bust 1770 of George III. Each of these coins ranks among the finest known.\nNear the end of King George III’s long reign, a series of new denominations emerged, called the New Coinage. The largest gold issues were the 5 pounds, replacing the earlier 5 guineas coins. One of the great rarities of this series is the 1820 pattern, engraved by Benedetto Pistrucci and struck at the then-new Tower Hill Mint in London, and a lovely example appears in the Tyrant Collection display. George IV’s large gold proof of 1826 in this collection is a superb piece, as are two examples of the young Queen Victoria’s famous Una & the Lion issue. Her 1887 Jubilee Head and 1893 Veiled Head 5 pound coins are also shown in superb Proof conditions.\nEdward VIII 1937 proof 5 Pounds – The only complete 1937 Edward VIII proof set in private hands will be displayed for the first time in the United States as part of the Tyrants of the Thames exhibit at the February 2018 Long Beach Expo. This coin from the historic set is the gold proof 5 pounds.\nPhoto credit: Professional Coin Grading Service\nVictoria’s son Albert became King Edward VII in 1902, and his matte proof set is infamous for often being marred by abrasions and hairlines, but the Tyrant Collection exhibit will show a truly magnificent and, therefore, very rare example. George V’s 1911 5 pound is represented by another superb piece, in brilliant proof.\nNext comes one of the rarest coins of the world, the gold 5 pounds of the abdicated King Edward VIII dated 1937. The Tyrant Collection contains the only privately owned complete set of Edward VIII’s coins, and the 5 pound coin in the set is a splendid proof.\nWhen Edward abdicated, his younger brother became king, and his coronation set of 1937 was typically a brilliant proof assemblage. In the “Tyrants of the Thames” exhibit, his 5 pound coins are represented by two pieces, the normal one having brilliant surfaces and the special matte proof, of extreme rarity. Each of these coins is of wonderful eye-appeal, and each is part of a complete set.\nElizabeth II’s coins in the main are commonly available. Each type is on view in this exhibit, but two are so rare that this could be the public’s only chance ever to see them. The first is the 1953 gold proof sovereign, one of perhaps as few as two in private collections. Queen Elizabeth’s coronation occurred in 1953; as a result of World War II’s financial impact on the country, the Royal Mint issued no gold coins for collectors in that year, and just a handful of pieces were struck for the queen and for museums.\nThe other great rarity of this reign in the collection is a unique 5 pound gold proof dated 2010 which shows the queen’s portrait on one side and a facing image of the famous musician John Lennon on the other side. This coin was made by The Royal Mint for collectors in silver. A single piece was struck in gold as a present for Lennon’s widow, Yoko Ono, who auctioned it and donated the proceeds to charity. It is now part of the Tyrant Collection and will be on view in this special exhibit.\n2010 John Lennon – A unique 2010 gold 5 pound coin depicting Queen Elizabeth II and musician John Lennon was given to Lennon’s widow, Yoko Ono, who subsequently donated it to an auction to benefit a children’s hospital in Liverpool, England. It will be displayed for the first time in the United States at the February 2018 Long Beach Expo.\nPhoto credit: Professional Coin Grading Service.\nAll of the other types of milled gold coins of England will also be shown as part of the Long Beach display, alongside the larger pieces mentioned above. These include the 2 guineas and 2 pounds issues, the guineas and sovereigns issues and their halves. Many are rarities, but all of the types are represented by beautiful examples. Among the rarest are early pieces bearing the famous Elephant hallmark of the 1660s or the Elephant & Castle hallmark of the 1670s to the 1690s, a 1702 proof guinea of Queen Anne, coins showing the VIGO hallmark of Queen Anne and LIMA hallmark of George II to depict gold seized by the British navy from Spain, and an amazing collection of proofs and patterns.\nAll of these coins will be on display in the Tyrants of the Thames exhibition at the February 2018 Long Beach Expo. The one-of-a-kind exhibit is being presented by The Tyrant Collection’s anonymous owner with the assistance of Ira and Larry Goldberg Coin & Collectibles of Los Angeles.', 'Across the country, many people who attend my antiques appraisal events are shocked to hear about some of the little-known methods used in major museums to preserve and protect precious art and antiques.\nWhile museums make a long-term commitment to preserving and protecting objects in their care to educate the public, most of us are equally committed to keeping our family heirlooms and keepsakes in good condition in order to retain their value.\nSome of the most common ways an object can be harmed include: pests and other insects, pollutants (dust, mold, etc.), temperature and humidity fluctuations, lights or sunshine, and oils from the human touch.\nFor instance, the oils on your hands and the hydrogen sulphide compounds in the air cause silver to tarnish and will leave a permanent mark on your valuable silver pieces.\nSigns that read ""Do not touch"" seem extreme but necessary when objects are on display in museums. When it comes to collectibles that we live with on a daily basis, it is a good idea to handle with care and handle only occasionally.\nSo, if you must handle an object, don\'t handle it too often. Remember, the oils and small dust particles on your hands can cause permanent damage to your heirlooms and aging treasures.\nIt is best to store your private collections in an area of your home where it is cool and dry.\nAttics (too hot with poor ventilation), basements (too damp), foyers (where the front door opens and closes often are bad because temperature changes are frequent), kitchens (too many cooking odors and too much heat), bathrooms and laundry rooms (too much moisture and possible mold) are not the best places for art or antiques.\nImproper climate conditions can stimulate mold growth and cause objects to mildew, dry out and crack. Never use harsh chemicals or abrasive pads to clean antique objects.\nHanging a framed print in a sunny window, storing objects in acidic cardboard boxes and over-cleaning your antiques can damage your pieces forever.\nSunlight is the first culprit that damages most works of art. Heat is a close second.\nPainted objects, prints and textiles should not be placed in sunny areas of your home as they are sensitive to light and will be damaged in a few short months.\nThere are few options to repair sun damage and fading once it happens. However, you can prevent heat from damaging your antiques. One of the hottest places where you display your collectibles is your china cabinet.\nThe glass doors act like a greenhouse and your objects are baking inside. Be sure to open those doors and let your objects get some good air flow every three months or so.\nSpray the rag, not the Renoir\nCleaning a framed work of art, such as a print, seems straightforward.\nHowever, there is a right way and a wrong way to clean it. Spray the rag first. Do not spray the cleaner directly onto the glass as the chemical could drip in between the glass and the work of art and damage it.\nBeware of bugs\nInsects are monsters, killers. They carry bacteria and they will eat and not stop eating until they have damaged your antique - particularly wooden ones - beyond recognition.\nYou may stop an infestation by wrapping a small wooden object in acid free tissue paper and placing the object in a freezer. The bugs will die off in the cold.\nAlso, bugs love dark spaces and close quarters.\nAn easy way to protect your antiques from insects is to clean around your objects regularly, don\'t eat food near your collectibles and use insect traps when necessary.\nCertain types of art and antiques need special types of care.\nBe diligent and handle your antiques carefully and you\'ll enjoy them for years to come.\nPh.D. antiques appraiser, author, and award-winning TV personality, Dr. Lori presents appraisal events to audiences worldwide.']"	['<urn:uuid:cafa07fc-98ea-44ed-a672-609a5e90dda4>', '<urn:uuid:3eeb8572-d410-42c9-afd1-c1b6a8e44a3f>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:10:11.825225	10	69	1918
42	As a chemistry researcher studying metal complexes, I'm curious about how a Nobel Prize-winning breakthrough in organometallic compounds was accidentally discovered - could you explain the story behind this?	Ferrocene was first accidentally prepared in 1951 by Pauson and Kealy at Duquesne University. They were attempting to oxidatively couple cyclopentadienyl magnesium bromide and ferric chloride, but instead obtained a light orange powder with remarkable stability. While they didn't recognize its sandwich structure, Robert Burns Woodward and Geoffrey Wilkinson later deduced the structure based on its reactivity. Independently, Ernst Otto Fischer reached the same conclusion about the sandwich structure. This discovery led to increased interest in d-block metal compounds with hydrocarbons and advanced the field of organometallic chemistry. In 1973, Fischer and Wilkinson shared a Nobel Prize for their work on organometallic chemistry.	"['To use all functions of this page, please activate cookies in your browser.\nWith an accout for my.chemeurope.com you can always see everything at a glance – and you can configure your own website and individual newsletter.\n- My watch list\n- My saved searches\n- My saved topics\n- My newsletter\nFerrocene is the chemical compound with the formula Fe(C5H5)2. Ferrocene is the prototypical metallocene, a type of organometallic chemical compound consisting of two cyclopentadienyl rings bound on opposite sides of a central metal atom. Such organometallic compounds are also known as sandwich compounds.\nAdditional recommended knowledge\nFerrocene, like many chemical compounds, was first prepared unintentionally. In 1951, Pauson and Kealy at Duquesne University reported the reaction of cyclopentadienyl magnesium bromide and ferric chloride with the goal of oxidatively coupling the diene. Instead, they obtained a light orange powder of ""remarkable stability."" This stability was accounted to the aromatic character of the negative charged cyclopentadienyls, but the sandwich structure of the η5 (pentahapto) compound was not recognized by them.\nRobert Burns Woodward and Geoffrey Wilkinson deduced the structure based on its reactivity. Independently Ernst Otto Fischer also came to the conclusion of the sandwich structure and started to synthesize other metallocenes such as nickelocene and cobtaltocene. Ferrocene\'s structure was confirmed by NMR spectroscopy and X-ray crystallography. Its distinctive ""sandwich"" structure led to an explosion of interest in compounds of d-block metals with hydrocarbons, and initiated the development of the now flourishing study of organometallic chemistry.\nIn 1973 Ernst Otto Fischer of the Ludwig-Maximilians-Universität München and Sir Geoffrey Wilkinson of Imperial College London shared a Nobel Prize with for their work on organometalic chemistry. Ferrocene is more efficiently prepared by the reaction of sodium cyclopentadienyl with anhydrous ferrous chloride in ethereal solvents.\nThe central iron atom in ferrocene is normally considered to be in the +2 oxidation state (this can be shown using Mössbauer spectroscopy). Each cyclopentadienyl ring is then allocated a single negative charge - this extra electron occupies a π orbital, bringing the number of π-electrons on each ring to six, and thus making them aromatic. These twelve electrons (six from each ring) are then shared with the metal via covalent bonding, which, when combined with the six d-electrons on Fe2+, results in the complex having an 18-electron, inert gas electron configuration. This configuration makes ferrocene particularly stable.\nFerrocene is an air stable orange solid that readily sublimes in vacuum, especially upon heating. As expected for a symmetric and uncharged species, ferrocene is soluble in normal organic solvents, such as benzene, but is insoluble in water.\nFerrocene sublimates notably around 100 Celsius. The following table gives some typical value of vapor pressure of ferrocene at different temperatures:\nReaction with electrophiles\nFerrocene undergoes many reactions characteristic of aromatic compounds, enabling the preparation of derivatives (substituted ferrocenes). The most common substitution patterns are 1-substituted (one substituent on one ring), 1,1\'-disubstituted (one substituent on each ring), and 1,2-disubstituted (two substituents next to each other on the same ring). For example the reaction of ferrocene, aluminium chloride and Me2NPCl2 in hot heptane forms dichloroferrocenyl phosphine, while treatment with phenyldichlorophosphine under similar conditions forms P,P-diferrocenyl-P-phenyl phosphine. In common with anisole the reaction of ferrocene with P4S10 forms a dithiadiphosphetane disulfide.\nFerrocene is readily deprotonated (e.g. by butyl lithium) to give 1,1\'-dilithioferrocene, which in turn is a versatile nucleophile. It has been reported that the reaction of 1,1\'-dilithioferrocene with selenium diethyldithiocarbamate forms a strained ferrocenophane where the two cyclopentadienyl ligands are lined by the selenium atom. This ferrocenophane can be converted to a polymer by a thermal ring-opening polymerization (ROP) to form poly(ferrocenyl selenide). Likewise by the reaction of silicon and phosphorus linked ferrocenophanes the poly(ferrocenylsilane)s and poly(ferrocenylphosphines)s can be obtained.\nUnlike the majority of aromatic hydrocarbons, ferrocene undergoes a one-electron oxidation at a low potential, around 0.5 V vs. a saturated calomel electrode (SCE) (N.B. electron rich aromatic amines such as aniline, the heterocycles pyrrole and thiophene can be oxidized with ease by electrochemical means). The oxidation of ferrocene is a reaction which forms a stable cation which is not prone to decomposition while the oxidation products of amines (such as aniline) and thiophenes tend to form polymers such as polyaniline and polythiophene. By adding groups to the cyclopentadienyl ligands the redox potential of the resulting ferrocene can be altered; by the addition of an electron withdrawing group such as a carboxylic acid the potential can be shifted in the anodic direction (i.e. made more positive), while the addition of electron releasing groups such as methyl groups will shift the potential in the cathodic direction (more negative). Thus, decamethylferrocene is much more easy to oxidise than ferrocene itself. Ferrocene is often used as an internal standard for calibrating redox potentials in non-aqeous electrochemistry.\nFerrocene can be oxidized using FeCl3 to give the blue-colored ferrocenium ion, [Fe(C5H5)2]+, which is often isolated as its [PF6]− salt. Ferrocenium salts are sometimes used as oxidizing agents, in part because the redox product ferrocene is so inert and readily separated from the products.\nApplications of ferrocene and its derivatives\nFerrocene itself has few applications. However, known synthetic methods allow the preparation of countless derivatives (above), thus extending the range of applications.\nFerrocene and its derivatives are antiknock agents used in the fuel for petrol engines; they are considered to be safer than tetraethyl lead, previously used. It is possible to buy at Halfords in the UK, a petrol additive solution which contains ferrocene which can be added to unleaded petrol to enable it to be used in vintage cars which were designed to run on leaded petrol. Unfortunately, the iron containing deposits formed from ferrocene can form a conductive coating on the spark plug surfaces leading to spark plug failure.\nIn diesel-fuelled engines, ferrocene reduces the production of soot.\nSome ferrocenium salts exhibit anticancer activity, and an experimental drug has been reported which is a ferrocenyl version of tamoxifen. The idea is that the tamoxifen will bind to the estrogen binding sites, resulting in a cytotoxicity effect.\nFerrocene, being readily sublimed, can be used to deposit certain kinds of fullerenes, especially carbon nanotubes. Due to the fact that many organic reactions can be used to modify ferrocenes, it is the case that vinyl ferrocene can be made. The vinyl ferrocene can be made by a Wittig reaction of the aldehyde, a phosphonium salt and sodium hydroxide. The vinyl ferrocene can be converted into a polymer which can be thought of as a ferrocenyl version of polystyrene (the phenyl groups are replaced with ferrocenyl groups).\nFerrocene is also used as a nano-sized ""loom"" in the manufacture of ultra-high molecular weight poliethylene\'s very long fibers, which are used to manufacture newer types of bulletproof vest fabric.\nAs a ligand scaffold\nChiral ferrocenyl phosphines are employed as ligands for transition-metal catalyzed reactions. Some of them have found industrial applications in the synthesis of pharmaceuticals and agrochemicals.\nDerivatives and variations\nMany other metals can be used in place of iron and many other hydrocarbons can be used instead of cyclopentadiene to form altered Cp ligands which are then attached to iron. For instance indene can be used in place of the cyclopentadiene to form bisbenzoferrocene..\nIn addition it is possible by heating [[Fe(η5-C5H5)(CO)2(η1-pyrrole)]] in cyclohexane to form the pyridine version (azaferrocene) of ferrocene [[Fe(η5-C5H5)(η5-C4H4N)]].. This compound on boiling under reflux in benzene is converted to ferrocene.\nBecause of the ease of substitution, many structurally unusual ferrocene derivatives have been prepared. For example, the penta(ferrocenyl)cyclopentadienyl ligand , features a cyclopentadiene derivatised with five ferrocene substituents.\nIn hexaferrocenylbenzene, all six positions on a benzene molecule have ferrocenyl substituents (R) . X-ray diffraction analysis of this compound confirms that the cyclopentadienyl ligands are not co-planar with the benzene core but have alternating dihedral angles of +30° and −80°. Due to steric crowding the ferrocenyls are slightly bent with angles of 177° and have elongated C-Fe bonds. The quaternary cyclopentadienyl carbon atoms are also pyramidalized. \n|This article is licensed under the GNU Free Documentation License. It uses material from the Wikipedia article ""Ferrocene"". A list of authors is available in Wikipedia.|']"	['<urn:uuid:4320d1e6-10e4-4087-ab5c-8aed83e2baad>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:10:11.825225	29	103	1345
43	Which practices help both soil health and crop insurance?	Growing diverse crops helps soil health, as food diversity promotes biodiversity and soil fertility when land is used for multiple crops. Similarly, in prevented planting situations, farmers can plant cover crops - as evidenced by the 4.7 million acres of cover crops planted in 2019, which was double the previous year. Cover crops, particularly pulse crops like beans and lentils, improve soil fertility through nitrogen fixation and can serve as part of crop rotation systems, while also providing insurance benefits for farmers who cannot plant their main crops as planned.	['There you are at your local grocery store. Maybe you have a shopping list and maybe you don’t. Perhaps you’re already hungry or are pressed for time. Maybe someone in your household has a food allergy or is on a special diet. The store may be offering promotions or featuring particular products. These are just some of the many factors that determine what actually comes with you to the checkout line. So, how could you possibly add ‘soil friendliness’ to your long list of considerations as you troll the grocery aisles?\nThe good news is that it may not be as hard you might think, especially because soil-friendly eating lines up well with recommendations for eating healthfully: a diverse diet rich in plant-based foods.1 The newest guidelines suggest half your plate be fruits and vegetables!\nThe food you buy at the grocery store also has an impact on the entire food supply system. For protein sources, the USDA recommends varying “your protein routine.” By eating different types of foods, you’ll help create demand for a wide variety of agricultural products, which is better for soil. Food diversity helps with biodiversity and soil fertility when land is used to grow multiple crops.\nOften overlooked at the grocery store, beans, chickpeas, lentils, and other ‘pulse’ crops are top-shelf when it comes to soil-friendly eating.2 Pulse crops are able to pull nitrogen from the atmosphere into the soil. This process is called ‘nitrogen fixation,’ and provides a natural fertilizer, which is available for subsequent crops. Growers typically plant pulses as part of a ‘crop rotation’ system in which one plant, like corn or wheat, is grown one season, and a pulse crop, like kidney beans, is grown another. Pulses tend to increase the overall efficiency of water use and disrupt cycles of pests, weeds, and diseases. In our diets, they not only provide protein, but essential minerals and dietary fiber. Compared to other protein sources, pulses are quite economical.\nThe list of healthy foods that add to soil fertility is long and can certainly include meat that is produced sustainably. For example, when livestock are grazed on pastures, the environmental impact can be much lower than when they are fed grain, which requires a lot of land, water, and agrichemicals to grow. There are other beneficial practices that ranchers can use such as mixed forages. Researchers in New Jersey are using perennial cover crops and various trees in a ‘silvopasture’ system to increase overall efficiency of their farms. In Brazil, researchers are grazing cows on land with tree legumes.\nPerhaps the easiest ‘win’ of all for soil-friendly eating is actually eating all the food that you buy. Every bit of food that made it into your shopping cart required land, water, nutrients, and energy to produce. These resources are wasted if the food ends up in your garbage. If you are unable to eat all of your food, be sure to invest in a composting system, to return these valuable nutrients back to nature next growing season!\nAnswered by Christine Negra, Versant Vision LLC', 'Four Prevented Planting Acre Priorities Heading into 2020\nWritten By Emily Unglesbee , DTN Staff Reporter © Copyright 2019 DTN LLC\nIn a year like 2019 you realise in the UK we always expect get our winter crops planted. Our temperate climate means this is nearly always the case. There will be talk of exceptionally wet years like 2012, 2000 or back in the early 80s, but these events are rare. However, they seem to be becoming less unusual and maybe it’s time we except winter planting is not a guarantee in the UK? In the USA, it’s not such a uncommon occurrence. In fact, they have a name for it “Prevented Planting”.\nFarmers can even insure against not being able to get a crop in the ground when they had planned to. I think a few farmers would have liked that insurance this year. Once you accept that you won’t always be able to get a crop in the ground, be it a winter or spring crop, then the next best thing you can do is prepare for the following season. Get your fields in the best shape they can be for when you can get crops in the ground. This article from DTN in the US talks about the steps one farmer took when he couldn’t get planted when he wanted to.\nWith 55% of his acres left unplanted in 2019, one might have expected Matt Foes to put up his feet and take a vacation. Instead, the Sheffield, Illinois, farmer rolled up his sleeves and spent a large part of this summer working a year ahead. “I went into the summer expecting that this fall would be no different than the spring — so really challenging,” said Foes. “So I spent the summer preparing those fields for 2020: I sprayed herbicides, tilled to smooth out seed beds and seeded cover crops.” In the weeks and months to come, he will tackle field repairs, cover crop termination, weed control and fertility management on those same acres, all before planting his actual crop — or so he hopes.\n“If I learned anything from 2019, it’s that you aren’t guaranteed to get an opportunity to do anything you’ve planned,” he said. Farmers reported a record number of prevented planting acres this year after a historically wet, challenging planting season. By late August, USDA’s Farm Service Agency estimated that nearly 20 million acres were left unplanted. And of those acres, more than 4.7 million acres were seeded to cover crops — more than double the previous year. As these farmers head into 2020, much remains to be done. Fields that sat fallow could face nutrient and microbe deficiencies, as well as weed problems. Cover crops need termination and management plans, and many fields are in need of repairs from equipment and water damage. Here’s a look at the top priorities for prevented planting acres this fall and winter:\nHave A Plan For Cover Crops\nFoes counts himself among the unplanned cover crop experimenters of 2019. “I’m completely new to cover crops,” he said. “This is a learn-as-you-go situation — we’re calling an audible.” Moving quickly in late July, Foes broadcast and tilled wheat seed on all the acres destined for a soybean crop in 2020. On the acres destined for corn, he bought discounted leftover treated soybean seed from a seed company and planted the soybeans as a cover. Like many, he now must navigate how to manage them before planting season rolls around. Growers have a handful of options. First, they can let certain cold-sensitive covers (like soybeans) overwinter and die. That leaves a mat of residue in the spring to manage — and possible volunteer seed, if the cover went through reproductive stages in the fall. Foes was fortunate; by using a later maturity group (3.6) than he normally does, his soybeans made it to the podding stage but never produced seeds. He hopes to terminate them with tillage this fall, but so far Mother Nature has intervened with rain and snow events.\n“I’m trying to avoid the ‘wet blanket’ effect on ground that is already typically wet, since 2020 will be starting with a full soil profile,” he explained. If he misses his fall window, Foes will aim to break up that blanket of dead covers with some vertical tillage in the spring. Other winter-hardy cover crops, such as wheat or rye, will continue growing in the spring, so growers will need a termination plan — will you aim to spray it out early in the spring or plant your spring crop into a “green” or living cover? Foes planted winter wheat that tillered out fully and grew a foot-tall canopy, but never entered reproductive stages. “It didn’t vernalize, so it won’t set seed,” he said. He expects some of it will winterkill and the rest will require a spring termination with herbicides.\nOther questions remain, too. Cover crop supplies for certain species, such as rye, got pretty tight this summer, due to the high demand. Depending on the source and quality of your cover crop seed, growers should watch for weed seed invaders, surprise volunteers and even disease inoculum in those fields. Foes used bin-run winter wheat seed, but had it professionally cleaned to try to protect his fields. “I didn’t want to inherit someone else’s weed problems, so we didn’t do straight binrun seed,” he said\nFinally, the heavy residue or growing cover of a fall-planted cover crop can be attractive to some spring insects, particularly those who need grassy covers to lay their eggs.\nWatch Those Weeds\nAbsent a crop, weeds grew quickly on many prevented planting acres this summer. Some farmers managed to get across those fields with herbicide applications or tillage, but weather and time restraints stymied others. Summer annual weed escapes have spent the fall sowing seed in some prevented planting fields, and now winter annual weeds are emerging. Foes managed a pass of glyphosate and 2,4-D on most of his acres around the Fourth of July, but wet fall weather has blocked his attempts at additional weed control. He is worried about the waterhemp escapes he saw sprouting in some fields this summer.\nLike many other farmers with prevented planting, Foes may have to grapple with dense weed seedbeds next season.\nKnow Your Nutrient Needs\nBeware of Fallow Ground Syndrome on prevented planting acres planted to corn in 2020. Corn enjoys a symbiotic relationship with a type of soil-dwelling fungi called mycorrhizae that grow along its roots and help the plant absorb key nutrients such as phosphorus. When a field is fallowed or planted to a crop that doesn’t support these organisms, the next corn crop can struggle to take up nutrients, resulting in stunted, yellowed or purpling plants. Some cover crops, particularly grass species, can maintain populations of these fungi for a following corn crop; others, such as brassicas, will not. Some growers may need to consider starter fertilizer options to help a 2020 corn crop planted to fallow ground.\nDo an accounting of your prevented planting acres’ applied fertility. Some nutrients such as nitrogen (converted to nitrates), boron and sulfur are very mobile in the soil and may no longer be present if you applied them in the fall of 2019 or the spring of 2020.']	['<urn:uuid:2c7972ed-d4d4-4b1b-b774-b0f9807b8cb8>', '<urn:uuid:96b51055-5bfc-4111-ad95-a6140a45d5a5>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T19:10:11.825225	9	90	1728
44	Do horses or dogs handle winter cold better?	Both horses and dogs can handle winter conditions well with proper care, but their needs differ. Horses are actually healthier when left outdoors during winter with access to a run-in shed, as this prevents respiratory problems and encourages movement. Dogs' cold tolerance varies by breed - breeds like Alaskan Malamutes and Siberian Huskies handle cold better, but even they need special care. Both animals need adequate shelter - horses benefit from an open run-in shed, while dogs need either a warm indoor sleeping area or an insulated outdoor shelter. Both animals also require close monitoring of their calorie intake during winter, though horses typically need increased calories to stay warm while indoor dogs may need fewer calories due to reduced activity.	"['So it’s nothing new to us to hear that we become slightly lazier during winter. The cold air, the dark mornings, the even darker evenings, spending a lot of time outside just isn’t all that appealing.\nHowever – similarly to having a child – there are many things you just have to do when looking after a horse to keep them fit and well, no matter how much you would love to stay in bed.\nThroughout winter, there are certain mistakes you must make sure you avoid and also bear in mind that neglect is the worst thing that happens to our horses in winter.\nBe Mindful of Ice\nIn order for your horse’s body to function correctly, he must be fully hydrated at all times. As temperatures decline, the water in the watering trough or bucket can end up covered by a thick layer of ice, making the water inaccessible to your horse. You want to try and maintain your horse’s water temperature at approximately 40 degrees F, which in the really cold months can be done with a heating device which are specifically made for waterers and troughs.\nTo help stay warm, your horse will burn more calories, which in some horses will result in significant weight loss. It would not be wise for me to suggest a suitable calorie intake for your horse during winter as each and every horse is different; therefore if you do not already, it is best to seek advice from your veterinarian. To achieve a healthy calorie intake for your horse, you may only need to increase your horses hay rations, or something equally as simple. If you know you won’t be visiting your horse as regularly, please make sure the hay is always easily accessible to your horse by using a hay net or this Canvas Hay Bag.\nReview Your Winter Shelter Plans\nOf course in extremely cold temperatures and heavy rain or snow, your horse will both benefit and appreciate some shelter. However, you will find your horse to be much healthier if left outdoors during winter, with an open run-in shed available for them should they wish to go inside. This run in shed will also prevent respiratory problems that may occur with an inadequately ventilated and heated barn. Your horse remaining outside will also encourage them to keep moving, meaning they keep in shape if you happen to become a tiny bit lazier with your training. If you’re concerned about the potentially minus temperatures, invest in a good horse rug, such as this Masta Quilted Lining with Neck Cover.\nMaintain Appropriate Hoof Care\nWhether you are or are not riding your horse regularly throughout the winter months, you must be wary that your horse’s hooves will still grow. As well as the standard growth of the hooves, you must also spare a thought for the fact that during winter, your horse will be trotting on hard and frozen ground which can crack and break the feet. You must remove the shoes and have the hooves trimmed before you turn out the horse ready for winter, and make sure you trim the feet at regular intervals during winter time, approximately every six to eight weeks. If hooves start becoming dry and brittle due to the cold, try out this Gold Label Hoof Oil Moisturiser and this Gold Label Aluminium Hoof Hardener.', ""It is hard to believe that winter weather is already here. It feels like we were just talking about summer safety tips for your dog. Just as cold weather brings challenges for us, it also brings unique challenges for our canine family members as well.\nWinter care tips for your dog follows a lot of the same guidelines as hot weather in that if it is uncomfortable for you, it is probably not comfortable for your dog. Dogs can suffer from frostbite and hypothermia like us, and we need to protect them from the harsh elements that winter can dish out. For tips on frostbite and hypothermia in dogs, click here.\nWinter Care Tip for Your Dog #1 – Never Leave your Dog in the Car\nNever leave your dog in the car during cold weather for prolonged periods of time. Just as your car can become an oven in the summer, it can become a refrigerator in the winter and can quickly chill your dog. After all, you probably wouldn’t put your dog in the fridge! Also, keep in mind that dogs that are old, thin or sick may be unable to handle the cold temperatures and should never be left unattended in cold cars.\nWinter Care Tip for Your Dog #2 – Bundle Up\nIf you find it necessary to bundle yourself up before you go outside, then you probably are going to need to do the same for your dog. There are some breeds such as Alaskan Malamutes and Siberian Huskies that can tolerate the cold better than others, but even they will need special care during cold weather.\nLet your dog’s hair grow out during the colder months if you typically shave them down during the summer. Your dog may need a sweater or a jacket, and if they are going to be walking in the snow, a good pair of dog booties is a great idea to protect their paws.\nWinter Care Tip for Your Dog #3 – Paw Care\nFrequently, check your dog’s paws for signs of injury or damage due to the cold weather. Injuries may include cracked paw pads, or in severe cases, their pads may start to bleed. If your dog suddenly becomes lame or stops while on a walk, then check for ice accumulation between their toes and any signs of cracks or redness. Click Here for additional paw care tips.\nTry applying some petroleum jelly or another paw protectant onto their pads before going outside to help protect from chemical agents, salt, and frostbite. Dog Booties are an excellent option for preserving those precious paws. Moreover, take shorter walks and try to walk during the warmest part of the day when the sun is shining.\nWinter Care Tip for Your Dog #4 – Avoid Toxins\nAntifreeze is poisonous to dogs and can be lethal if ingested. Be sure to wipe up any spills from your vehicle and keep the container tightly closed and out of the reach of your canine companions. Always wipe your dog's feet when coming in from outside as they may have walked on ice melt which may contain toxic chemicals. If your dog licks their paws after walking on deicer, they may become sick.\nWinter Care Tip for Your Dog #5 – Provide a Warm Place to Sleep\nIf your dog is an indoor pet, his or her bed or crate should be in a warm area that is free from drafts and preferably elevated off the floor. Cold tiles and concrete floors can be extremely cold and may cause your older dog to become stiff from aging joints. Choose the right bedding such as a snug blanket to ensure your dog stays warm.\nIf your dog must stay outdoors, then you need to provide a warm insulated pet house or shelter. The dog house should have a door to keep the cold air out and be slightly elevated to prevent moisture from accumulating inside and be sure to change out the bedding frequently as it may become wet.\nWinter Care Tip for Your Dog #6 – Regulate Food Amounts\nSome dog owners have a misconception that by feeding their dog more in the winter, they will develop an extra layer of fat that will help keep them warm. The excess weight is not beneficial. It is best to keep an eye on your dog’s body conditioning and keep them in a healthy range.\nAlthough it is true that staying warm requires extra calories and outdoor dogs will need more calories to generate enough energy and body heat to keep warm, dogs that stay indoors should remain on a nutritious, balanced diet that should include the 100% all-natural snacks from the South Fork Pet Company.\nDogs tend to be less active in the winter, so adjust meals and calories accordingly. Always make sure your dog has fresh water available. If your dog is outdoors, then be sure to frequently check the water bowl to ensure that the water is not frozen.\nKeeping your Dog Safe and Healthy This Winter\nRemember that when it comes to winter care tips for your dog that just like us, your dogs’ ability to tolerate the cold will vary depending on several factors. These variables may include the type of coat, activity level, overall health, and body fat stores.\nDogs that are older or that suffer from arthritis may find it difficult to walk on snow and ice and may be more prone to falling and sustaining an injury. Dogs with long hair or thick coats may be more tolerant of the cold but are still at risk in cold weather.\nDogs that have short hair will feel cold faster than their long-haired counterparts because they have less protection from the elements. Moreover, dogs with short legs will also become cold more quickly because their bellies are more likely to come in contact with the frozen ground.\nDogs with illnesses such as diabetes, kidney disease, and heart disease may find it more difficult to regulate their body temperatures and therefore, may be more susceptible to problems associated with frigid temperatures.\nWinter can be fun for dogs as they frolic in the snow and run around on a cold day, so take some precautions and enjoy!""]"	['<urn:uuid:80d9199e-1dcf-4363-a132-3b22a715e4c0>', '<urn:uuid:8bd8d190-85e1-4671-abef-e17df710dd8e>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	8	121	1607
45	what conductivity reading level should water have to prevent corrosion in boiler	To prevent corrosion in a boiler, the water should have a conductivity reading below 3,000 PPM, which equals 6,000 µS/cm.	['When you hear the term boiler water, this refers to any water that’s situated in a boiler or the pumps and piping surrounding the boiler for the main purpose of being evaporated into steam. Boiler water is used in many different industries for such applications as heating, sterilization, and humidification. These industries include metalworking, electrical, manufacturing, and agricultural industries.\nA common problem that’s found in boiler water is the presence of high conductivity, which refers to the ability of the water to conduct an electrical current. Controlling the conductivity of water is an essential component of making sure that a boiler functions properly.\nA key issue with high conductivity in boiler water is that operational issues such as scaling can occur, which is a buildup of solid material in the boiler. When this occurs, the boiler becomes less efficient and increases the fuel consumption of the unit. High conductivity levels also heighten the risk that the boiler water becomes contaminated, which can be very dangerous. If ever the conductivity in your boiler water reaches a higher level than it’s supposed to, you might want to look into obtaining boiler water treatment to effectively reduce conductivity levels and keep your boiler operational.\nMeasuring Conductivity of Boiler Water\nIf your business process includes a system heated by a boiler, it’s very important that you take the necessary steps to measure the conductivity of the boiler water that’s contained inside of the unit. Because of how important it is that the conductivity of the water is properly managed, you can’t afford to not measure the water. If you want to effectively measure the conductivity of the boiler water now and in the future, you need to understand how this measurement works and what you need to look for while measuring the water.\nThere are actually several different methods that can be used to measure the conductivity of boiler water. For instance, you should check the level of total dissolved solids to determine how conductive the water currently is. The TDS of your boiler water can either be measured by taking a sample of the water and measuring it externally from the boiler or by placing a sensor inside that’s designed to detect TDS levels. Keep in mind that a high TDS level indicates high conductivity in the water.\nProducts that Measure Conductivity\nWhile you can use many different instruments to measure the conductivity of boiler water, among the most effective instruments is a toroidal conductivity sensor, which comes with digital communication for better ease-of-use. A high quality toroidal conductivity sensor will monitor total dissolved solids and conductivity.\nWhile some contact conductivity sensors are prone to being affected by a wide range of issues that can cause them to malfunction in a relatively short period of time, the TCS3020 is resistant to corrosion, fouling, and coating, which is an essential feature if you want your conductivity sensor to have a long service life. Some of the features that are available with this instrument include a wide measurements range of up to 2,000mS, high resistance, and modern inductive measurement technology.\nOnce you’ve collected a sample of water from your boiler or have looked at the readings of the instrument you’re using, the main unit of measurement for conductivity is typically Siemens. When you receive the readings from a toroidal conductivity sensor, the results will be displayed as microSiemens per centimeter or uS/cm. Some conductivity sensors will also measure in milliSiemens per centimeter, which is a unit of measurement wherein one mS/cm is equal to 1,000 uS/cm. The TCS3020 is specifically ideal for such applications as chemical process and cooling towers. Higher numbers on the scale indicate that you might want to obtain boiler water treatment due to high conductivity.\nThe Effect of High Conductivity\nThe effects of high conductivity can be very damaging if you don’t stem them early on. It’s typically impossible to obtain completely pure water in a boiler. No matter the quality of your equipment, impurities will invariably seep into the water and begin to increase the water conductivity.\nThe impurities that get into your boiler water may be referred to as suspended matters, dissolved gases, or dissolved solids. The four major issues caused by the buildup of impurities include scaling, an oxygen attack, an acid attack, and boiler water carryover, all of which are problematic.\nThe Issue of Scaling\nScaling is likely the most common problem that’s caused by the high conductivity of boiler water. Scaling refers to the buildup of solid materials because of the reaction from the tube metal and the other impurities in the water. This buildup will lessen heat transfer in the boiler unit, which will invariably worsen boiler efficiency and cause an exceedingly high amount of fuel to be used in order to power the boiler.\nIf the issue lingers and isn’t tended to quickly, the presence of scale could cause the tubes to overheat and eventually fail. The thickness of the solid materials dictate how much fuel you’ll be wasting. It’s estimated that fuel consumption increases by 2-5 percent with the presence of scale, which will eventually cause you to lose a significant sum of money.\nOxygen’s Effects on Your Boiler System\nAs for an oxygen attack, this problem can cause your boiler system to corrode, which means that effectively managing the conductivity of your boiler water should result in a longer-lasting boiler. When oxygen dissolves in the feed water, it becomes heated and will react with the internal surface of the boiler, which causes corrosive elements to develop. These elements include red iron oxide and hematite. The presence of oxygen corrosion in a boiler system can eventually cause tube failure. Additional components of the boiler system can also be damaged, which include the condensate piping, boiler headers, and drums.\nAn acid attack is another aspect of corrosion via high conductivity that occurs when the pH levels of feed water are lower than 8.5. Standard pH sensors will be able to help you identify the levels of pH in your boiler water. The carbonate alkalinity that’s found in the water is directly converted into CO2 by the pressure and heat from the boiler.\nWhen the steam from the boiler condenses, carbonic acid is formed, which lessens the pH of the condensate that goes back into the boiler. As for boiler water carryover, this occurs when the steam from the boiler has become contaminated from boiler water solids. The presence of high boiler water solids creates foaming, which reduces the efficacy of the boiler.\nHigh levels of salts, heavy metals, and other substances in water can be somewhat toxic to a person’s health if consumed in high amounts. The dangers to a person’s health extend from skin irritations to gastrointestinal issues. When you’re looking to measure the conductivity of your boiler water, it’s important that you keep the conductivity of the water at a certain level. If you want to prevent corrosion in your boiler, the water should have a conductivity reading that’s below 3,000 PPM, which equates to 6,000 µS/cm.\nTreatment to Reduce Water Conductivity\nIf you find that the conductivity of your water is too high, you’ll want to obtain boiler water treatment to effectively reduce conductivity. With such Sensorex Products as the TX3100 and the SensoPro, you’ll be able to monitor the pH and TDS levels of your boiler water consistently, which will allow you to identify the exact moment when the conductivity in the water becomes too high.\nAt this point, you’ll need to use some type of boiler water treatment, of which there are many to choose from. If you decide to use external treatment methods, the water will be removed from the boiler before being purified. There are a wide variety of techniques that can be used for external boiler water treatment, which include softening, deaeration, membrane contractors, and evaporation. Each option ensures that the feed water you obtain is tailor-made for your boiler.\nInternal Boiler Water Treatments\nThe internal treatment of boiler water involves a variety of solutions that can be placed directly into the boiler system in an attempt to prevent higher conductivity or to lower conductivity that has already reached a high measurement. As with external treatments, there are many methods and techniques that can be used via internal treatments.\nNo matter which internal treatment you use, the main goals of each treatment include preventing impurities in the water from becoming scale, conditioning suspended matter in the boiler to the point that it doesn’t adhere to surface metal, preventing foam from developing, and eliminating the oxygen in the water.\nThe main internal treatment solutions that you might want to consider include softening chemicals, anti-scaling agents, oxygen scavengers, sequestering agents, and anti-foaming agents, all of which can help with the reduction of conductivity. It’s important for you to control the conductivity of your boiler water in order to prevent damaging to the boiler.\nThe buildup of scale, acid, and corrosion in the water can significantly lessen the lifespan of your boiler system. Even before these issues cause your boiler to become damaging, the efficiency of the unit will worsen, which will waste water and energy. By controlling the water conductivity, you’ll be able to save money and keep your boiler system functioning properly for many years.\nPreparing and Maintaining Safe Levels of Conductivity\nWater control sensors and conductivity sensors will help you maintain safe levels of conductivity in your boiler system in a variety of different ways. As mentioned previously, the SensoPro system displays TDS levels, salinity, and current conductivity. There are also very little maintenance requirements for these sensors, which should save you time and money. When you want to determine if the pH levels of your boiler water have dipped to a dangerously low level, the TX2000 will provide you with very precise measurements.\nBy using the appropriate sensors and products, you’ll be able to continuously monitor the conductivity of your boiler water without any effort on your part. The benefits of doing so are numerous. Controlling the pH levels and conductivity of the feed water in your boiler will help you reduce the possibility of boiler water carryover into your steam system, eliminate the possibility of overfeeding chemicals, reduce the amount of time that you spend testing the system, and eliminate the need to regularly remove highly conductive water, which causes you to waste chemicals, water, and energy. By selecting the appropriate sensors now, you can control conductivity levels before they become too high, which will help your business save money and energy that can be better used elsewhere.\nMeasuring and maintaining the conductivity levels of boiler water is one of the most important aspects within metalworking, electrical, manufacturing, and agricultural industries. If you are interested in purchasing one of many products to help measure the conductivity levels of boiler water, check out our products page to search for the best product for your business. If you have any questions with any water treatment applications, contact our Sensorex team today!']	['<urn:uuid:70295fe9-d69c-4e67-b100-f52a0b561201>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	12	20	1835
46	room acoustics health impacts absorption solutions	Poor room acoustics contribute to noise pollution, which costs over 10 billion euros annually and affects over 100 million Europeans, causing learning disabilities, stress, hearing loss, and increased heart attack risk. For optimal acoustics, rooms need balanced absorption across frequencies - high frequencies can be managed with conventional absorbers like carpets and furniture, while low frequencies require specialized solutions like micro-perforated absorbers or intelligent materials with active noise reduction capabilities.	['Optimal room acoustics / How to improve room acoustics\nIntroductory video on room acoustics\nAccording to respective use and building size, the DIN 18041 and the Austrian standard B 8115-3 define the ideal reverberation time (e.g. that which is perceived to be most pleasant by the average person). For ideal room acoustics, this reverberation time should remain more or less the same in the frequency range of 100-4000 Hz. For rooms where music performances take place this value should rise somewhat where frequencies lower than 250 Hz occur. However, the linear reverberation time is better suited to music rehearsal room.\nImages at right depict tolerance ranges according to standards (whereby the reverberation time of 1 second for ideal room acoustics is not likely here); tolerance range for middle frequencies plus/minus 20%.\nIn many rooms there are materials which act as absorbers. The effect is normally in the high-frequency area.\nSuch high frequency absorbers may include:\n- Carpeting/throw rugs\n- People (clothing)\n- Upholstered furnitu\nWithout the implementation of acoustic measures, reverberation time is lower in the higher frequencies\nBy way of the fact that high frequency absorbers are usually present, the absorption curve looks approximately as follows:\n(those frequencies which are absorbed the most exhibit the lowest reverberation times)\nDanger of over-absorption of high frequencies\nUsing the example of a Class A sound absorber\n((Alpha-w) αw = 1.00)\nIn the case of sound absorbers, it is frequently the case that only individual specifications such as (Alpha-w) αw are specified, yet in most cases, this is insufficient information.\nSound absorbers of this class are often characterized as broadband absorbers, even though they are also medium and high frequency absorbers.\nIn this example, the entire ceiling surface has been equipped with absorbers.\n- Incorrect absorption spread of the frequencies\n- Those frequencies required for speech intelligibility are over absorbed, while those for frequencies for undertones are not absorbed enough\n- Absorbers of this kind are not suitable for achieving ideal room acoustics\nLow frequencies are much harder to absorb than higher frequencies and are thus often simply ignored. Yet how important are these low frequencies for generally good room acoustics? This is a highly-debated topic for which there are no easy, simple answers. In order to shed light on the matter, here we present select data using speech as type of use.\nFrequency Distribution in Speech:\nIn which frequencies does information transmission occur when speaking? Information is chiefly transmitted in speech through consonants. Within the frequency band these are found above 1000 Hz (see diagram at right).\n(Source of diagram: Helmut V. Fuchs, Schallabsorber und Schalldämpfer (Springer Verlag, 2007))\nThis is often used as an argument for why lower frequencies have less significance for speech intelligibility in room acoustics.\nYet the following must be considered:\nIn which frequencies does the most sound radiation occur?\nThe frequency distribution of sound radiation is depicted in this diagram (source: Helmut V. Fuchs, Schallabsorber und Schalldämpfer (Springer Verlag, 2007)).\nThus by concentrating on medium and high frequencies the important “information frequencies” are swallowed and the parasitic frequencies are ignored. Low frequencies however have the tendency to blanket higher frequencies – think for example of the low-frequency rumbling of a passing lorry! Only insulating the medium and high frequencies – using the argument that it is these which transmit information – fails to fulfil the necessary requirements for good acoustics.\nFor most rooms, a linear frequency curve for reverberation time is ideal and in fact stipulated by DIN 18041. To achieve this, we have intensified our low frequency focus in product development.\nBelow our findings:\n- micro-perforated low frequency absorbers with a maximum performance of 100-315 Hz -> products 1, 2 and 3 and products 4, 5 and 6\n- micro-perforated low frequency absorbers with a maximum performance of 50-100 Hz (these frequencies are often not stipulated because absorption through conventional absorbers in this range is very low) -> product 1\n- One of our research findings: low frequency absorbers used in a panel absorber configuration do not exhibit sufficiently stable absorption performance.\nIt is easy to use our calculator to determine how to improve your room acoustics in the best way possible!\n- It is not necessary to know all the details; use the calculator to determine which absorber /absorber combinations are best for your planned project\n- Let the combination of the calculator and the relevant industry standards DIN 18041 and B 8115-3 do the work for you!', 'Intelligent Materials for Active Noise Reduction\nNowadays, noise is recognized as one of the major pollutants worldwide. Noise creates more than an estimated 10 billion euro in costs annually, because it is not only annoying, but also causes illnesses. Throughout Europe, there are more than 100 million people affected by noise pollution. The health impacts of noise are diverse: from learning disabilities in children, stress, and loss of hearing to an increased risk of heart attacks. This problem has been picked up on national and European levels through a multitude of activities. The goal is to significantly reduce noise pollution within the next 10 – 20 years.\nWithin the European InMAR project – Intelligent Materials for Active Noise Reduction – the suitability of functional materials and the performance of active structural systems for noise reduction was investigated. In order to reduce the vibrations, researchers link the sensing and actuating functions of the materials with electronic controllers. Sensors and actuators can systematically react to operating conditions: depending on the vibration frequency, inverted soundwaves are introduced. This reduces the propagation of the sound waves by actively damping the source of the noise. Thereby the mechanical properties, like the damping behaviour or the stiffness per software, can be adjusted. Thus, vibrations can be lowered, noise can be reduced, and the contour of structures can be controlled with functional materials.\nThe international consortium, coordinated by the Fraunhofer LBF, was composed of 42 notable institutions (11 universities, 8 research institutions, 8 KMUs and 15 industrial partners) from 13 European countries. This project was coordinated by the former head of the institute, Prof. Holger Hanselka, and Prof. Thilo Bein. It comprised a project scope of 34 million euro. Overall, the consortium invested 15 million euro (50 % of the F&E budget) of its own funds in this research project.\nTo illustrate the project results, various demonstrations with intelligent materials were designed in the InMAR project. Examples of the 23 projects in the fields of automobiles, railcars, and infrastructures are:\n- A car engine mount that actively reduces the transfer of vibrations to the chassis by means of intelligent materials. The significant transfer paths and directions of the structure-borne sound are identified through structural dynamic measurements in a frequency range from 0 to 250 Hz. From this data, different concepts of the active interventions have been investigated with help of simulation calculations.\n- An active harmonic absorber that reduces the vibrations of the compressor in a tram’s AC unit. Here, a passive mass-spring-system (a spiral spring) is used to reduce these vibrations within a specific frequency range. Additional vibrations from glued piezo-patches can be introduced into the original passive absorber that will change the absorber’s natural frequency in a range from -12 bis + 3 Hz.\n- A soundproof window that keeps out low frequencies, e.g. airplane noise or bass from clubs. The window can lower test signals in a frequency range between 50 and 1000 Hz to about 6 decibels – the sound on the other side of the window is still only half as loud. The sound level of individual test signals can even be reduced by up to 15 decibels.\nThis project successfully brought all of the European top-level-research in the field of active systems together and created a counterbalance to the activities in the USA and Asia.\nProf. Dr.-Ing. Thilo Bein']	['<urn:uuid:43519fd4-18a0-4dae-8061-c1bc102acfa3>', '<urn:uuid:06154870-2611-4274-b97d-c65fad844c6f>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	6	70	1301
47	I'm looking to save time and money on home maintenance - what are some convenient smart home features for both the kitchen and exterior doors?	In the kitchen, you can use GeniCan, a trash can scanner that builds shopping lists by scanning barcodes of empty packages, and Whirlpool's Scan to Cook feature that automatically programs cooking settings by scanning package barcodes. For exterior doors, fiberglass doors are low-maintenance while being energy-efficient, costing between $1,000-$4,000. You can also add smart locks like Baldwin's Evolved system that lets you remotely manage access and monitor entrances without requiring regular maintenance.	"['How smart is your house? How smart do you want it to be?\nEvery year, thousands come together at the International Consumer Electronics Show in Las Vegas to see what’s new and exciting in the realm of smart home technology. In years past, this massive trade event known as CES had very little to do with anything home-related beyond wall space for TVs, shelves for stereo equipment and the like.\nThat is changing rapidly, with an entire exhibit hall and seminar track devoted to the house at this year’s event. Here are some of the offerings you might want to consider for your place. Not all of them may be available in your area just yet, but keep checking back for their release.\nKitchens are a big area for smart home technology, since most people want to bring intelligence and convenience to this hard-working space.\nAt the moment, however, many of the offerings are in the early stages with much development needed before their promises are fully delivered. That being said, here are some products worth considering for your kitchen now:\n- One of the greatest risks to your home is flooding, which can be caused by a pipe leaking or a hose breaking. Several companies have developed water detectors to alert you to moisture where it shouldn’t be found, like the cabinet bottom below your sink, but Fibaro goes further by connecting to your home automation system and shutting down the water line entirely to prevent further damage.\n- Many kitchens today open to outdoor living spaces with dining and cooking centers. That’s a wonderful feature to have, but as anyone who cooks and entertains frequently knows, opening a heavy sliding door while holding a tray of steaks or bowl of corn cobs is no easy feat. AutoSlide has solved that problem by turning your standard sliding patio door into an automatic opening model. It can even mesh with a pet collar to create doggy door capability.\n- Whirlpool showed off its Scan to Cook feature for those of you who find yourself using packaged food regularly. Your enabled wall oven, microwave and range can read the barcode on the package, saving you time in programming these cooking appliances and eliminating costly errors.\n- GeniCan was one of the smartest kitchen innovations at the show. It’s a small scanner you mount on your trash can. By scanning the bar codes on packaged goods you regularly use as you toss away the empties, it can build your shopping list quickly and conveniently. It can even connect with Amazon if you buy your groceries online.\nThere were some interesting offerings for the bathroom, too. Here are some to consider:\n- U by Moen the first app-driven shower system. Why might you want this system? Well, it lets you (and other users) program your preferred shower settings on your phone, start it remotely and pause the water when it’s ready, so it’s ready when you are and no precious minutes or water drops are wasted.\n- Haier showed off its Smart Bath system, which shows the Internet and your health info on its Magic Mirror, as well as offering a programmable, predictable water heater.\n- Simplehuman lets you upload your exact work environment lighting to your bathroom with its Sensor Mirror Pro. No more makeup gaffes with this technology.\nOther Room Technology\nKitchen and bath technology is only part of the picture. There are innovations for the rest of your home, too. These are a few you might want to check out:\n- Whirlpool has simplified laundry with its WiFi-enabled All in One washer-dryer model. It doesn’t require venting and will let you know when your entire load is washed, dried and ready to fold. It’s not a fast process, but it is a convenient one.\n- Baldwin showed off its Evolved door lock that’s sleek and programmable. It lets you unlock the door remotely for someone you want to let in and keeps you informed of who’s coming and going at your house – ideal for parents of teenagers. You don’t have to sacrifice style for security with systems like this one.\nThe world of home technology is advancing at turbo speed. Be sure to research what anti-hacking and privacy tools they include, how much information you want to voluntarily provide in the interest of convenience and how your purchases will truly benefit your life, not just their promises to do so.', ""Your home's exterior doors play a significant role in your home's curb appeal, energy efficiency, and overall security. Whether you're planning to sell your home or just want to give it a facelift, updating your exterior doors is an excellent investment. Here are the top 3 things homeowners should consider when updating their exterior doors, including the average cost of the project.\nThe first thing to consider when updating your exterior doors is the material. The most common materials for exterior doors are wood, fiberglass, and steel. Wood doors are the most traditional, providing a classic and elegant look, but they require regular maintenance to keep them looking their best. Fiberglass doors are low maintenance and energy-efficient, making them an excellent choice for homeowners who want a balance of style and practicality. Steel doors are durable and secure, making them an excellent choice for homeowners who want to prioritize safety.\nThe cost of updating your exterior doors will depend on the material you choose. On average, a wood exterior door costs between $500 and $2,000, while a fiberglass door costs between $1,000 and $4,000. Steel doors are the most affordable option, with an average cost of between $500 and $1,500.\nThe style of your exterior door should complement the architectural style of your home. When choosing a door style, consider the design elements of your home, such as the color, texture, and shape. Popular door styles include traditional, modern, craftsman, and rustic.\nIn addition to the style of the door, you should also consider the type of door. The most common types of exterior doors are single doors, double doors, and sliding doors. Single doors are the most popular and come in a variety of sizes and styles. Double doors add elegance and grandeur to your home's entrance. Sliding doors are an excellent choice for homeowners who want to maximize their outdoor living space and bring more natural light into their home.\nThe cost of updating your exterior door's style will depend on the type of door you choose and any customization options. On average, a single door costs between $500 and $3,000, while a double door costs between $1,000 and $5,000. Sliding doors are the most affordable option, with an average cost of between $1,000 and $3,000.\nYour exterior doors are the first line of defense against intruders, making security an essential consideration. When updating your exterior doors, consider the locking mechanism, the strength of the door, and any additional security features, such as security screens or smart locks.\nThe cost of updating your exterior door's security will depend on the level of security you require. On average, a basic lock system costs between $50 and $150, while a smart lock system can cost between $100 and $500. Security screens are an additional cost, ranging from $100 to $500.\nUpdating your exterior doors is an excellent investment in your home's value, energy efficiency, and security. By considering the material, style, and security of your exterior doors, you can find the perfect balance of style and practicality that meets your needs and budget.""]"	['<urn:uuid:ee591909-1c9b-4f6c-9d19-47f029b330d3>', '<urn:uuid:75a6a9cb-1536-49d2-aafd-d489674c6a32>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:10:11.825225	25	72	1254
48	What was Nancy Hanks' role in saving the Old Post Office Building?	Nancy Hanks, as chairperson of the National Endowment of the Arts, helped convince Congress not to demolish the building. In recognition of her preservation efforts, the building was officially renamed the Nancy Hanks Center in 1983.	"[""Built from 1892 to 1899 to house the U.S. Post Office Department Headquarters and the city's post office, the Old Post Office Building is the second-tallest structure in the nation's capital, after the Washington Monument. For most of the twentieth century, it seemed that the massive Romanesque Revival structure was destined to be demolished, but through the efforts of dedicated preservationists it has become one of Washington's favorite landmarks.\nIn 1928, the Old Post Office Building was slated for demolition in the development now known as the Federal Triangle. Lack of funds during the Great Depression saved the building at that time, and over the next 30 years, it provided space for various Government agencies. In 1964, the President's Council on Pennsylvania Avenue recommended the demolition of all but the clock tower. As a result, local citizens banded together and, with the help of Nancy Hanks (the politically influential chairperson of the National Endowment of the Arts), convinced Congress to reverse its decision.\nA decade later, redevelopment plans for the Pennsylvania Avenue corridor included preservation of the Old Post Office Building, and renovation began in 1977. The plan called for retail commercial spaces on the lower level, with Federal offices on the upper levels. This adaptive, mixed-use approach received national attention as a viable approach to historic preservation. In 1983, the building was officially renamed the Nancy Hanks Center in recognition of her devotion to the preservation of significant buildings.\nIn honor of our nation's Bicentennial celebration in 1976, the Ditchley Foundation of Great Britain presented a set of English change ringing bells to the U.S. Congress as a symbol of friendship. The bells were permanently placed in the Old Post Office clock tower in 1983 and are rung at the opening and closing of Congress and for national holidays.\nThe Old Post Office Building is a reminder of the foresight of preservationists devoted to conservation of our built environment. The building was listed in the National Register of Historic Places in 1973.\nThe Old Post Office Building occupies an entire city block, centered on the north side of the Federal Triangle along Pennsylvania Avenue -- the link between the Capitol and the White House. Designed by Willoughby J. Edbrooke, Supervising Architect of the U.S. Treasury Department, the Old Post Office Building exhibits a matured version of the Romanesque Revival style, which was popularized by renowned architect H.H. Richardson in the late nineteenth century. The building's massive scale, rustication, arched fenestration, and ornamentation evokes the Romanesque Revival style, while incorporating a variety of complementary features such as Byzantine sculptural capitals, French Gothic dormers and sculpture, and French Renaissance detailing. The eclectic effect of these details creates a delightful visual vitality that is now regarded as a virtue along Pennsylvania Avenue's predominantly Classical Revival corridor.\nSheathed in granite from Vinalhaven, Maine, set upon an iron and steel superstructure, the vast nine-story building was the first steel-frame building erected in Washington. Unlike other contemporary tall buildings, the five-feet-thick granite masonry walls are self-supporting, while the steel girders are used to support the interior floor beams. As a fire-proofing measure, a terra-cotta shell encases each steel and iron structural member.\nAlong Pennsylvania Avenue, three large semicircular Romanesque arches frame the principal entrance and are ornamented with Romanesque Revival columns, capitals, moldings and richly foliated spandrels. The rock-cut rustication at the first-story and mezzanine walls contrast with the smooth surfaces of the upper stories, where vertically stacked bays are crowned with a continuous springing course and wide arches at the fifth story. The sixth and seventh floors, separated from the lower floors by a stringcourse, are composed of narrow, two-story window units separated by twin-columned mullions, capped by arcades. At the top of the wall, a wide, dentiled cornice is reminiscent of the machicolation used in medieval fortifications.\nThe facade (north elevation) is divided into three vertical sections, defined by the central recessed portion of the building. At its center, a lofty clock tower projects forward and rises 315 feet to a hipped roof accentuated by pinnacles at the belfry. Tall stone pilasters support the clock face, which is simply framed by a Roman arch. Above this, an observation deck is pierced by three large, arched openings that offer some of the best views of the city. The east and west wings of the facade project forward slightly, with corners embellished by soaring pinnacles with conical roofs. The steeply pitched, slate-clad roof is pierced by stone-clad dormers adorned with stone finials.\nThe most remarkable feature inside is the nine-story light court topped by an enormous skylight that floods the interior with natural light. When it was built, the room was the largest, uninterrupted interior space in Washington. The building's renovation uncovered the skylight and added a glass-enclosed elevator on the clock tower's south side to provide visitor access to the observation deck. A lower glass atrium at the east side of the building was added in 1992.\n1892-99: The building is constructed.\n1928: Development of the Federal Triangle south of Pennsylvania Avenue threatens the Old Post Office.\n1964: Plans to finish the Federal Triangle jeopardize the Old Post Office Building, prompting a vocal campaign to save the building, led by Nancy Hanks.\n1973: The Old Post Office Building is listed on the National Register of Historic Places.\n1976: The Ditchley Foundation of Great Britain presents the Congress Bells in honor of the nation's Bicentennial.\n1977-83: GSA rehabilitates the building while remodeling it for adaptive-use.\n1983: The building reopens with a combination of Federal offices and retail spaces. It is officially renamed the Nancy Hanks Center.\nArchitect: Willoughby J. Edbrooke\nConstruction Dates: 1892-99\nLandmark Status: Listed in the National Register of Historic Places\nLocation: 12th Street & Pennsylvania Avenue, N.W.\nArchitectural Style: Romanesque Revival\nPrimary Materials: Granite, steel, iron\nProminent Features: Clock tower, atrium, Congress Bells\nThe Old Post Office, designed in the tradition of Romanesque Revival architecture of H.H. Richardson, occupies the entire city block bounded by 11th, 12th, C and D Streets at the juncture of Pennsylvania Avenue. A massive rectangular structure, it measures approximately 200 feet from east to west and 300 feet from north to south. The nine-story building rises 135 feet to the flat portion of the roof. The tower, located in the center of the north façade, rises to a height of 315 feet above grade.\nThe massive structure, faced with granite from Vinalhaven, Maine, is a solid masonry load-bearing structure with walls over five feet thick. The plans indicate that the exterior walls of the building are self-supporting, not reinforced with steel girders at each floor, as was customary for tall buildings. The steel columns embedded in the walls seem only to support the interior floor beams (the above structural information was taken from a paper by Watterson).\nThe heavy masonry massing and detailing of the building are typical of the Romanesque revival style. The main Pennsylvania Avenue entrance is defined by three massive arches with smooth-faced voussoirs, ornamented archivolts and the ciphers US and PO in richly foliated spandrels. On the second story, with its contrasting surface of smooth, rusticated masonry, are three rectangular windows above each arch. Above the windows is an ornamented parapet. The entrances on the east and west sides contain one large arch flanked by two smaller arched openings. The south façade contains a loading dock.\nThe exterior of the first, mezzanine and second floors flanking the entrance and on the other sides of the building is of rock-faced masonry. The second story is separated from the upper stories by a molded stringcourse, which continues around the building. Twin arched windows flank the main entrance. On the second floor are found narrow, rectangular two-light windows.\nIn contrast to the rock-faced masonry of the lower floors, the upper walls are of smooth, rusticated granite. Above the main entrance, the third story terrace is cut into the building, allowing the east and west ends to project, forming wings. Each wing is terminated on both ends by round towers with conical roofs topped by stone finials. The windows in the tower are narrow and rectangular. The fenestration on the third, fourth and fifth floors are conceived as a unit: rectangular windows on the third and fourth floors with round-headed windows on the fifth floor. Each bay is separated by a pilaster terminating in a cornice. From the cornice springs an arch on the front wings, forming two arcades and six arcades on the sides. The ordering of the windows and placement of the terraces above the entrances is similar for all sides of the building.\nThe sixth and seventh floors are separated from the lower floors by a stringcourse. The fenestration is conceived as a two-story unit terminating in round-headed windows below the arches, which are supported by twin-columned mullions forming four arcades on the east and west wings and twelve arcades on the sides. Above the machicolated cornice rises the steep slate Chateau roof containing large stone dormers terminating in stone finials.\nThe tower rises 315 feet above grade from the center of the main façade. Below the cornice line, its massing is accentuated by a projection of several feet from the central façade. Above the arched fifth story window is the inscription: ANNO DOMINI MDCCXCVII (reflecting the architect's optimism that the building would be finished by 1897). From the cornice line the tower rises as a solid mass with only narrow slits for openings and decorative arches above the clock face on each of the four sides. Above the clocks are four turrets, one on each corner, separated by five narrow arched windows. Above rises the steeply slanting roof.\nThe interior of the building opens onto a grand court or cortile, which is ringed by offices that open onto corridors overlooking the court. The court was originally roofed in glass and flooded with light, but at present has been covered with insulation material, resulting in a very dark open space. The first floor of the cortile was roofed over with a steel and glass roof so that it could be heated and used. The interior corridors on the first floor are linked with marble wainscoting. The open cage elevators are also of interest.\nThe Office of Building Management, Public Buildings Service, described the building in 1962 as obsolete and recommended the limitation of improvements to those required for seven to ten years occupancy. The report warned that an extension of life expectancy to an indefinite period would require complete modernization.\n(Information taken from the 1973 NR Nomination)\nOver time, there has been little appreciation for the Old U.S. Post Office Building. It was constructed during the last decade of the nineteenth century, a period of architectural change, when tastes were turning from the boldness of Victorian styles to the refinement of the Classics. It seems fair to say that the Old Post Office was long viewed as a graceless intruder to an elegant setting.\nOnly in recent years has the building attracted attention for its own values. What was once regarded as a fault--its unique visual quality--is now a virtue, a delightful variation of form, size, and style.\nOriginally designed by Willoughby J. Edbrooke, Supervising Architect of the Treasury between 1891 and 1893, the design and construction of the Old Post Office extended through the terms of five different Supervising Architects. Among them, Jeremiah O'Rourke and William Martin Aiken were responsible for major modifications to Edbrooke's design. Construction started in 1892 but progressed slowly and it was not until 1899 that the building was ready for occupancy.\nStylistically, the building has many features similar to designs by H. H. Richardson and therefore has been characterized as Richardsonian Romanesque. However, it is only in materials and certain details that the building is typical of this style. The basic shape was inspired by the large municipal halls of great Italian medieval cities, though its symmetry shows a nineteenth century European academic influence and there are details on the exterior best described as Renaissance. The turreted pavilions are capped by steeply sloping roofs with gabled dormers, giving the upper part of the building a strong flavor of French Renaissance or Chateau style. These sharp-edged, free-standing tops accent the building corners and lend an exciting independence of form.\nThe Old Post Office is noteworthy among Washington buildings for more than its individuality of style. With the exception of the Washington Monument, it is the tallest building in the city. Its construction incorporated many of the latest technical innovations of the day, such as steel and iron framing, fireproofing, and an electric power plant. The building encloses a magnificent interior space, the largest such uninterrupted space in the city.\nOriginally intended to house both the U.S. Post Office Department and the Washington City Post Office, the entire building was turned over to the federal department in 1914. The structure apparently was never large enough for even that operation alone, however, and in 1934, a new U.S. Post Office Building was constructed in the Federal Triangle across Twelfth Street. Since then, the Old Post Office has alternately housed the departments of Justice, Defense, Agriculture, and Interior, as well as the General Accounting Office, Interstate Commerce Commission, U.S. Information Agency, the Smithsonian Institution, and the Federal Bureau of Investigation.\nApparently the Old Post Office was one of the first federal buildings to be constructed in this area of the District of Columbia. Unfortunately, little is known about its site history. The Preliminary Historic Structures Report recommended that additional research be undertaken relating to the site. However, planning funds have not been made available for this purpose. It can be easily seen, nonetheless, that the building's orientation and two of its facades were ignored in subsequent site development.\nThe architecture of the building itself gives character to the Old Post Office site. Its style, essentially Richardsonian Romanesque, and its form-including the seven story stone facade, two story chateau-like roof, lofty clock tower and the highly decorative arches which define the main entrance--have a significant impact on the building's surroundings. One of the most important aspects of the building-to-site relationship is that the ponderous Romanesque nature of the architecture demands that the structure rest clearly, sharply and distinctly on its ground plane.\nNational Register 1973 Nomination\nThe Old Post Office, with its huge clock tower, has long been one of Washington's favorite landmarks. In recent years the clock tower, which is visible from a distance of several miles, has received particular acclaim as an element of great vitality in the otherwise sterile skyline of the Federal Triangle.\nThe Joint Committee on Landmarks has designated the Old Post Office and Clock Tower a category II Landmark of importance, which contributes significantly to the cultural heritage and visual beauty of the District of Columbia. The Old Post Office is one of Washington's few significant Romanesque Revival buildings on a monumental scale. It was the first Federal building erected on Pennsylvania Avenue in the area now known as the Federal Triangle. Plans for the building were prepared in 1891 in the office of the Supervising Architect of the Treasury, W.J. Edbrooke. Many similarly styled Richardsonian-inspired Federal Buildings erected throughout the country in the 1890¿s were designed in Edbrooke's office. At the time of its completion in 1899, the building with its 315-foot high clock tower was the third highest in Washington, exceeded only by the Capitol and Washington Monument. Its central enclosed court was one of the largest in the world.\nDesigned to house the U.S. Post Office Department as well as the Washington City Post Office, the building served as the headquarters of every Postmaster General from 1899-1934. IT was there in 1908 that the observance of Flag Day was initiated by some employees who met on the second floor balcony overlooking the court and sang homage to the Star Spangled Banner. Every Flag Day, a complete collection of State Flags was displayed from the walls of the central court. Normally on display was the largest correctly proportioned U.S. Flag in existence. This flag, which hung down nearly seven stories from the skylighted room, was furled on Flag Day to avoid dwarfing the smaller State Flags.\nIn 1914, the Washington City Post Office moved to a new building adjacent to Union Station. The department remained in its headquarters until 1934 when the new U.S. Post Office Building across 12th street was ready for their use. The Old Post Office has since been shared by a number of Federal Executive Departments and agencies.\nFor over 30 years, the Old Post Office has prevented completion of the final quadrant of the Great Circle on 12th street, an important element in the Federal Triangle plan of 1928-1938.""]"	['<urn:uuid:c97205ca-4dcb-4686-be52-bcf461bebcaa>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:10:11.825225	12	36	2779
49	gas giant formation composition metallicity relationships	The relationship between gas giant formation and composition is complex. In our solar system, there's a direct relationship where lighter gas giants have more heavy elements in their atmosphere. However, observations of exoplanets like HAT-P-26b challenge this pattern. The formation process involves planetary embryos growing in dusty disks, where they need to achieve sufficient mass (about an Earth mass or two) to hold onto gas before the disk's gas dissipates. The composition of these planets is influenced by their formation location - planets forming closer to their stars typically have fewer heavy elements, as demonstrated by HAT-P-26b's unexpectedly low heavy element content. This suggests that Neptune-like worlds may form through diverse pathways and in different locations than previously thought.	['Title: A metallicity recipe for rocky planets\nAuthors: R. I. Dawson, E. Chiang, E. J. Lee\nFirst Author’s Institution: University of California, Berkeley, Berkeley, CA\nStatus: Submitted to MNRAS\nThe space between the stars is actually not so empty. It’s also quite dusty. It’s so dusty, in fact, that if you brought a large parcel of space to the densities you find on Earth, you would hardly be able to see your hand if you stretched your arm out in front of you. This interstellar dust—typically only a micrometer across, a fraction of the thickness of a human hair—is the stuff that planets are made out of. The formation of planets is really an incredible saga of how the lowly dust grain grows to planet proportions, a transformation of millions of years, a growth in size by a million million, and takes the conglomeration of about a hundred million million million million million million grains (that is, ~10^38 grains).\nIt all begins with the birth of a star, when gigantic cold clouds of gas and dust are flattened into a gaseous dusty disk encircling a growing protostar by the concerted powers of angular momentum conservation and gravity. The grains orbit the protostar within this disk. When the orbits of two grains intersect, they can collide and possibly stick together. This process continues over and over again to produce large grains, which in turn collide with each other to produce small rocks, which collide to form “planetesimals” the size of asteroids, and so on, scurrying with bolts and jolts up the size scale until they eventually form rocky bodies the sizes of Mercury, Mars, the Earth, thousands of kilometers across. These rocky bodies can become terrestrial planets or the seeds of the cores of gas and ice giants such as those found within our own Solar System and beyond.\nThe authors of today’s paper attempt to explain why some of these planetary embryos remain largely unchanged, barren, and purely rocky while others end up cloaked with a gas envelope. They focus on planets discovered by the Kepler Telescope that are a few to several times more massive than the Earth, which they divide into two types: those that are rocky, or super-Earths, and those that have a thick gaseous atmosphere, or mini-Neptunes.\nHow might the mini-Neptunes have acquired their gas? The disks in which the transformation from grain to planet occurs is actually mostly gas—the dust makes up only about 1% of the mass of the disk—and thus a natural guess as to the source of planet atmospheres. But there’s a hitch. The gas in a disk is fated to banishment, first as the disk drains onto the growing, massive protostar at its center, then by evaporation as the newborn star heats the disk to temperatures so high (hundreds to thousands of Kelvins) that the gas can escape the gravity of the star and disk. However, planetary embryos can’t hold onto an atmosphere unless they’re fairly massive, about an Earth mass or two. Thus the road to mini-Neptune-hood (or to a gas/ice giant, for that matter) is a growth race against the shrinking supply of gas.\nSo what affects how quickly an embryo grows? In order for growth-inducing collisions to occur between planetary embryos, their orbits must cross. Close passages of rocky bodies with similar masses can stir up their velocities, pushing them into skewed, elliptical orbits that allow the embryos to traverse a broad range of radii (as opposed to circular orbits, which restricts the embryo to a single radius), increasing the chances of orbit crossing and thus growth. Working against these velocity-stirring, growth-inducing, gravitational encounters is, once again, the gas. Much like how the atmosphere reduces the speed of projectiles or a marble dropped into honey stops moving—a process known as dynamical friction—the gas attempts to erase the effects of stirring by bringing the rocky bodies to its own orbital speed, slowing down the rocky bodies sped up by close encounters and conversely, speeding up those that were slowed down.\nBut not all hope is lost. The solution? Wait until the gas disappears enough to stop erasing the velocity stir-ups that encourage growth. The authors estimate that the gas begins to have a negligible effect on embryo velocities when a disk has similar surface densities of gas and dust. One crucial consequence of this is that it shortens the time during which the embryos can acquire gaseous envelopes—the gas sticks around for a short million years by the time it stops affecting embryo velocities. It helps if the disk started with more dusty solids, which would produce more material for the the embryos to grow from. In fact, the authors estimate that planetary embryos in disks with larger solid surface densities grow exponentially faster—a result they confirm with more detailed N-body calculations.\nTo confirm that the embryos that grew massive enough to begin acquiring a gaseous envelope had enough time to become mini-Neptunes, the authors ran simple one-dimensional models of envelope growth and used those results to estimate the amount of gas each planet in their N-body simulations would have. They confirmed that disks with larger solid surface densities grew all its embryos fast enough to convert them into gas-enveloped mini-Neptunes before the gas completely disappeared, while disks with lower surface densities produced a mix of gaseous mini-Neptunes and purely rocky super-Earths (see Figure 1).\nWhile these calculations are simplified, they highlight the importance of the availability of solids on the formation of planets. The differences in the amount of solids a disk has may be able to explain the lack of purely rocky planets around high-metallicity stars (which they assume was once encircled by a disk with more solids than those around low-metallicity stars) observed by Kepler. However, their model cannot explain why we’ve observed low-metallicity stars with mini-Neptunes. It’s possible that solids could accumulate in disks around low-metallicity stars, producing solid surface densities high enough for massive rocky embryos to form quickly and transform into mini-Neptunes before the gas disappears. The mini-Neptunes could also have formed further out in the disk, then migrated inwards to where we observe them today. The full story of the origins of super-Earths and mini-Neptunes awaits unraveling with further work.', 'Not all gas giants are created equal. Observations of the Neptune-like exoplanet HAT-P-26b show that its atmosphere is less rich in heavy elements than expected, meaning it probably formed close to its star.\nIn our solar system, the gas giants’ metal content can be plotted on a straight line: the lighter a gassy planet is, the more heavy elements its atmosphere contains. Jupiter’s atmosphere has less heavy elements than Neptune’s, for example.\nBut in order to study the atmospheres of planets around other stars, they need to meet very particular conditions: the planet must pass between its star and us, and the star must be bright enough for us to discern changes in its light as it shines through the planet’s atmosphere. That makes it difficult to tell if the heavy element relationship holds true for gas giants outside our solar system.\n“We really need to learn how other solar systems can form in order to put our own solar system in context,” says Hannah Wakeford at NASA’s Goddard Space Flight Center. “What we’re trying to learn ultimately is how easy it is to form a solar system like our own.”\nWakeford and her colleagues used the Hubble Space Telescope to look at HAT-P-26b, an exoplanet about 440 light years away that has a similar mass to Neptune, along with an extensive but relatively tenuous atmosphere. As the world passed in front of its star, they saw distinct signatures of water in its atmosphere.\n“Seeing this beautiful signature of water in the atmosphere was perfect for us,” says Wakeford. “It’s a great planet.”\nOut of line\nThis isn’t the first time signs of water have been spotted in an exoplanet atmosphere, but it does point to something surprising. Because oxygen is heavier than helium – which astronomers consider the cutoff for so-called heavy elements – and water has oxygen in it, the researchers could use the water signal to determine the abundance of heavy elements in the atmosphere of HAT-P-26b.\nThey found less heavy elements than they would have expected, given data from the few other gas giants inside or outside our solar system whose atmospheres have been probed. “This is the first step away from the line that we’ve ever seen,” says Wakeford.\nIf HAT-P-26b is out of line now, it must have been strange from the start.\nWhen planetary atmospheres are formed from a star’s disc of dust and gas, they acquire their heavy elements from pockets of ice and debris that are more common further from the star. HAT-P-26b’s missing heavy elements may indicate that it formed closer to its star than the gas giants in our solar system.\nThis could point to a surprising diversity in how and where Neptune-like worlds are formed. But since so few of these worlds have been analysed, it is impossible to say for certain that HAT-P-26b isn’t just an anomaly.\n“The exciting thing about this field is that we have no idea if this is an unusual or typical planet, because it’s the lowest-mass planet for which we have this good of a measurement,” says Ian Crossfield at the University of California Santa Cruz. “This puts us on the road towards being able to study planets more like Earth.”\nJournal reference: Science, DOI: 10.1126/science.aah4668\nMore on these topics:']	['<urn:uuid:58ff4e7f-f651-44b4-9e10-9bd53a749da3>', '<urn:uuid:d65d3a63-a540-40c6-8459-c120d81fec91>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:10:11.825225	6	119	1586
50	environmental impact health risks beacon disposal electronics landfill registration requirements	Beacons need careful disposal procedures to avoid accidental activation, as they contain hazardous materials and cannot be simply thrown away. Registration of beacons is legally required and helps ensure quicker rescue responses. For electronic waste generally, improper disposal creates significant environmental and health hazards through soil and water contamination with heavy metals and toxic chemicals like PCBs and PAHs. These pollutants can accumulate in the environment and pose serious risks to human health through various exposure pathways including inhalation, ingestion, and dermal contact.	['On this page:\nThere are different beacon types designed for use in different environments.\nThe EPIRB (emergency position-indicating radio beacon) is the best type for use on boats and other activities on water.\nWhy carry a beacon?\nDue to New Zealand’s rugged landscape and changeable weather, you can get into trouble very quickly.\nRadios, GPS tracking systems, distress flares, whistles, lights and mobile phones may be useful as a back-up, however, none are as effective as a distress beacon when you need help in an emergency.\nDistress beacons are one of the most reliable ways of signalling that you need help.\nChoose the right beacon\nAlthough they all work in the same way, different beacons are designed for use in different environments.\nThere are three types of beacons:\n- EPIRBs (emergency position-indicating radio beacon) are best for boats, ships and other activities on water\n- PLBs (personal locator beacon) are for those tramping, climbing, hunting and travelling to remote locations, microlights and balloons and any other outdoor activities. If being used for paddling or small water craft then they need to be of a type that can float and operate in water.\n- ELTs (emergency locator transmitter) are only for aircraft\nEPIRBs are suitable for all maritime activities\nWaterproof and designed to float\nAdditional safety features may be included and will vary depending on the model and brand.\nRegister your beacon\nIt’s free and easy\nRegistration of your distress beacon is FREE and only takes a couple of minutes. Registrations can be can be submitted online, emailed or downloaded and sent through post.\nIt’s the law\nRegistering your beacon is a legal requirement.\nIt could save your life\nEnsuring your beacon is registered with the Rescue Coordination Centre New Zealand (RCCNZ) is vital – a registered beacon means a quicker, more targeted response can be launched.\nRCCNZ may also be able to find out exactly who is with you, how long you have been gone, and whether anyone has any medical conditions. Rescuers will then be in the best position to help you when you are located.\nRegister your beacon\nGet familiar with your beacon\nBefore you head out\nWhen moving your beacon - always make sure that it is in “safe” or “off” mode.\n- Read the instruction manual and understand how to operate your beacon\n- Check the expiry date for the battery, which is shown on the beacon label\n- Batteries should be replaced by your supplier or agent\n- Make sure your beacon is registered and that your details are kept up to date\nMake sure it’s easily accessible\nStore it on your vessel or life raft\nIf your EPIRB comes with a mounting bracket, place it where it is visible and easy to access in an emergency. If the EPIRB and mount have a magnetic activated mounting switch ensure that the two magnets are “face to face”. Make sure the EPIRB stays dry and keep it locked away when nobody is on board.\nIf you have an inflatable life raft on board, an additional beacon can be stored inside the raft.\nKeep your beacon away from:\n- equipment that may accidentally knock the activation switch\n- magnetic sources, such as microphones and radio speakers (some beacons are activated by a magnetic on/off switch)\n- high water pressure\n- children who may accidentally turn it on\nIf your beacon is set off accidentally, phone RCCNZ immediately.\nThis will ensure a search and rescue operation is not launched needlessly.\nIf you are unable to contact RCCNZ immediately, switch off the beacon and make contact as soon as you are able to.\nThere is no penalty for accidental activation.\nDisposing of old beacons\nOld or obsolete beacons need to be disposed of carefully, to ensure they are not set off by accident.\nDo not just throw them away, as a lot of time and money has been spent on search operations to dig beacons out of rubbish tips.\nThe battery needs to be disconnected and the beacon disposed of according to local regulations, as many beacons contain hazardous materials. The names of distributors who dispose of old beacons can be found at www.beacons.org.nz.', 'Adverse Human and Environmental Health Effects of Electronic Wastes: A Critical Review and Consequence Analysis\nIncrease in obsolete electronics is a direct consequence of rapidly changing technologies and an upsurge in the global demand for new electronic devices. This has led to large quantities of electronic products discarded as electronic waste (e-waste), creating enormous environmental health challenges impacting the management of the product life cycle (PLC) of the global product chain for manufactured electronics. The environmental health impacts of e-wastes vary widely across geography. Major contributors of e-waste production as well as major e-waste importing countries are identified in this study. The “Chemicals in Products (CiP)”, their fate and distribution in the environment, based on the local geology of the land use, and toxicological effects in the human body upon exposure, are investigated. The study outcomes present the need for recommended actions and prospective solutions for mitigating the associated hazards from CiP and the risks of potential adverse effects to humans and the environment. Effective handling and PLC management of e-waste can be achieved through proper implementation, permeable reactive barriers use in incinerators, oil-water solvent systems for contaminant removal and revision of existing regulations. These recommendations will help protect the health of children and adults in susceptible areas and improve electronic products management and environmental sustainability.\nElectronic Waste, Heavy Metals, Polycyclic Aromatic Hydrocarbons, Polychlorinated Biphenyls, Contamination, Exposure, Toxicology, Remediation\nUnited States Census Bureau - USCB (2017). U.S. and World Population Clock. Retrieved December 30, 2017 from https://www.census.gov/popclock/\nGSMA Intelligence (2017). Global Mobile Trends September 2017. Retrieved December 30, 2017 from https://www.gsmaintelligence.com/research\nPerez-Belis, V., Bovea, M. D., Ibanez-Fores, V., (2015). An in-depth literature review of the waste electrical and electronic equipment context: trends and evolution. Waste Manag. Res. J. Int. Solid Wastes Public Clean. Assoc. ISWA 33 (1), 3-29.\nAwasthi, A. K., Zeng X., Li J. (2016). Environmental pollution of electronic waste recycling in India: A critical review. Environmental Pollution 211 (2016) 259-270.\nKinhal, V. (2017). Highest e-waste generating countries in the world. World Atlas. Retrieved November 22, 2107 from http://www.worldatlas.com/articles/highest-e-waste-generating-nations-in-the-world.html\nWorldometers (2017) - Current World Population. Retrieved December 30, 2017 from http://www.worldometers.info/world-population/\nUnited Nations Environment Programme [UNEP]. (2015). Illegally Traded and Dumped E-Waste Worth up to $19 Billion Annually Poses Risks to Health, Deprives Countries of Resources, Says UNEP report. Retrieved November 22, 2017 from http://web.unep.org/newscentre/illegally-traded-and-dumped-e-waste-worth-19-billion-annually-poses-risks-health-deprives-countries\nSthiannopkao, S., Wong, M. H., 2013. Handling e-waste in developed and developing countries: Initiatives, practices, and consequences. Sci. Total Environ. 463-464, 1147-1153.\nBalde, C. P., Wang, F., Kuehr, R., Huisman, J., (2015). The Global E-waste Monitor 2014. Quantities Flows and Resources. United Nations University, IAS - SCYCLE, Bonn, Germany, pp. 1-41. Institute for the Advanced Study of Sustainability. Retrieved December 30, 2017 from http://i.unu.edu/media/ias.unu.edu-en/news/7916/Global-E-waste-Monitor-2014- small.pdf.\nPerkins, D. N., Brune-Drisse, M., Nxele, T., & Sly, P. D. (2014). E-waste: A global hazard. Annals of Global Health, 80 (4), 286-295. doi: 10.1016/j.aogh.2014.10.001\nLi, J., Zeng, X., Chen, M., Ogunseitan, O. A., Stevels, A. L. N., (2015). “Control-Alt- Delete”: rebooting solutions for the e-waste problem. Environ. Sci. Technol. 49 (12), 7095-7108.\nRobinson, B. H. (2009). E-waste: An assessment of global production and environmental impacts. Science of the Total Environment, 408 (2), 183-191. doi: 10.1016/j.scitotenv.2009.09.044\nZheng, H., Hu, G., Xu, Z., Li, H., Zhang, L., Zheng, J., He, D. (2015). Characterization and distribution of heavy metals, polybrominated diphenyl ethers and perfluoroalkyl substances in surface sediment from the dayan river, south china. Bulletin of Environmental Contamination and Toxicology, 94 (4), 503-510. doi: 10.1007/s00128-015-1479-7\nYuan, J., Chen, L., Chen, D., Guo, H., Bi, X., Ju, Y., Chen, X. (2008). Elevated Serum Polybrominated Diphenyl Ethers and Thyroid-Stimulating Hormone Associated with Lymphocytic Micronuclei in Chinese Workers from an E-Waste Dismantling Site. Environmental Science & Technology, 42 (6), 2195–2200. doi: 10.1021/es702295f\nZhang, Y., Luo, X., Mo, L., Wu, J., Mai, B., & Peng, Y. (2015). Bioaccumulation and translocation of polyhalogenated compounds in rice (Oryza sativa L.) planted in paddy soil collected from an electronic waste recycling site, South China. Chemosphere, 137 (Journal Article), 25–32. doi: 10.1016/j.chemosphere.2015.04.029\nZeng, X. L., Song, Q. B., Li, J. H., Yuan, W. Y., Duan, H. B., Liu, L. L., (2015). Solving e-waste problem using an integrated mobile recycling plant. J. Clean. Prod. 90, 55-59.\nZhang, Y., Huo, X., Cao, J., Yang, T., Xu, L., & Xu, X. (2016). Elevated lead levels and adverse effects on natural killer cells in children from an electronic waste recycling area. Environmental Pollution, 213, 143–150. doi: 10.1016/j.envpol.2016.02.004\nHe, K., Sun, Z., Hu, Y., Zeng, X., Yu, Z., & Cheng, H. (2017). Comparison of soil heavy metal pollution caused by e-waste recycling activities and traditional industrial operations. Environmental Science and Pollution Research, 24 (10), 9387. doi: 10.1007/s11356-017-8548-x\nUS Environmental Protection Agency [EPA]. (2017). Basic information about landfills. Retrieved November 29, 2017 from https://www.epa.gov/landfills/basic-information-about-landfills\nBreivik, K., Armitage, J. M., Wania, F., & Jones, K. C. (2014). Tracking the global generation and exports of e-waste. do existing estimates add up? Environmental Science & Technology, 48 (15), 8735.\nVos, S. (2012). Electronic waste disposal. Duke university: Nicholas school of the environment. Retrieved November 29, 2017 from http://sites.nicholas.duke.edu/loribennear/2012/11/15/electronic-waste-disposal/\nPickren, G. (2014). Political ecologies of electronic waste: Uncertainty and legitimacy in the governance of E-waste geographies. Environment and Planning A, 46 (1), 26-45. doi: 10.1068/a45728\nGallo, D. T. (2013). Broad Overview of E-Waste Management Policies in the U.S. Environmental Protection Agency. Retrieved November 29, 2017 from https://www.epa.gov/sites/production/files/2014-05/documents/overview.pdf\nDasgupta, D., Debsarkar, A., Hazra, T., Bala, B. K., Gangopadhyay, A., & Chatterjee, D. (2017). Scenario of future e-waste generation and recycle-reuse-landfill-based disposal pattern in india: A system dynamics approach. Environment, Development and Sustainability, 19 (4), 1473-1487. doi: 10.1007/s10668-016-9815-6\nWang, H., Yu, Y., Han, M., Yang, S., Li, Q., & Yang, Y. (2009). Estimated PBDE and PBB congeners in soil from an electronics waste disposal site. Bulletin of Environmental Contamination and Toxicology, 83 (6), 789-793. doi: 10.1007/s00128-009-9858-6\nUS Environmental Protection Agency [EPA]. (n.d.). PCBs questions and answers. Retrieved December 1, 2017 from https://www3.epa.gov/region9/pcbs/faq.html\nWilson, D. (2015). Toxins in WEEE (E-waste). University of Washington. Retrieved December 1, 2017 from http://ewaste.ee.washington.edu/students/impacts/\nHong, W., Jia, H., Ding, Y., Li, W., & Li, Y. (2016). Polychlorinated biphenyls (PCBs) and halogenated flame retardants (HFRs) in multi-matrices from an electronic waste (e-waste) recycling site in northern china. Journal of Material Cycles and Waste Management, doi: 10.1007/s10163-016-0550-8\nRabodonirina, S., Net, S., Ouddane, B., Merhaby, D., Dumoulin, D., Popescu, T., & Ravelonandro, P. (2015). Distribution of persistent organic pollutants (PAHs, me-PAHs, PCBs) in dissolved, particulate and sedimentary phases in freshwater systems. Environmental Pollution, 206, 38-48. doi: 10.1016/j.envpol.2015.06.023\nAgency for Toxic Substances and Disease Registry [ATSDR]. (2015). Public health statements for PCBs. Retrieved December 2, 2017 from https://www.atsdr.cdc.gov/phs/phs.asp?id=139&tid=26\nWang, Y., Luo, C., Wang, S., Cheng, Z., Li, J., & Zhang, G. (2016). The abandoned E-waste recycling site continued to act as a significant source of polychlorinated biphenyls: An in situ assessment using fugacity samplers. Environmental Science & Technology, 50 (16), 8623.\nLi, L., Li, Y., Richardson, J. B., Mark Bricka, R., Niu, X., Yang, H., & Jimenez, A. (2009). Leaching of heavy metals from E-waste in simulated landfill columns. Waste Management, 29 (7), 2147-2150. doi: 10.1016/j.wasman.2009.02.005\nKyere, V. N., Greve, K., & Atiemo, S. M. (2016). Spatial assessment of soil contamination by heavy metals from informal electronic waste recycling in agbogbloshie, ghana. Environmental Health and Toxicology, 31, e2016006. doi: 10.5620/eht.e2016006\nIsimekhai, K. A., Garelick, H., Watt, J., & Purchase, D. (2017). Heavy metals distribution and risk assessment in soil from an informal E-waste recycling site in Lagos state, Nigeria. Environmental Science and Pollution Research, 24 (20), 17206-17219. doi: 10.1007/s11356-017-8877-9\nBlanco, A., Salazar, M. J., Vergara Cid, C., Pignata, M. L., & Rodriguez, J. H. (2017). Accumulation of lead and associated metals (Cu and Zn) at different growth stages of soybean crops in lead-contaminated soils: food security and crop quality implications. Environmental Earth Sciences, 76 (4), 1–11. doi: 10.1007/s12665-017-6508-x\nWu, J.-P., Luo, Y., Luo, X.-J., Zhang, Y., Chen, S.-J., Mai, B.-X., & Yang, Z.-Y. (2008). Bioaccumulation of polybrominated diphenyl ethers (PBDEs) and polychlorinated biphenyls (PCBs) in wild aquatic species from an electronic waste (e-waste) recycling site in South China. Environment International, 34 (8), 1109–1113. doi: 10.1016/j.envint.2008.04.001\nTao, W., Zhou, Z., Shen, L., & Zhao, B. (2015). Determination of dechlorane flame retardants in soil and fish at guiyu, an electronic waste recycling site in south china. Environmental Pollution, 206, 361-368. doi: 10.1016/j.envpol.2015.07.043\nAckah, M. (2017). Informal E-waste recycling in developing countries: review of metal(loid)s pollution, environmental impacts and transport pathways. Environmental Science and Pollution Research, 24 (31), 24092–24101.\nGrant, K., Goldizen, F., Sly, P., Brune, M., Neira, M., van den Berg, M., & Norman, R. (2013). Health consequences of exposure to e-waste: a systematic review. Lancet Global Health, 1 (6), E350–E361. doi: 10.1016/S2214-109X(13)70101-3\nOlawoyin R. (2017). Exposure to Toxic Pollutants: Assessing Potential Human Health Risk. Professional Safety, 62 (2), 40-45.\nSong, Q., & Li, J. (2014). A systematic review of the human body burden of e-waste exposure in China. Environment International, 68 (Supplement C), 82–93. doi: 10.1016/j.envint.2014.03.018\nOlawoyin R, Heidrich B, Oyewole SA, Okareh OT, McGlothlin CW, (2014). Chemometric analysis of ecological toxicants in petrochemical and industrial environments. Chemosphere 112, 114–11\nBoston University School of Public Health [BUSPH] (2017). Exposure Assessment: Introduction to Basic Concepts. Retrieved December 30, 2017 from http://sphweb.bumc.bu.edu/otlt/mph-modules/exposureassessment/exposureassessment3.html\nXing, G. H., Chan, J. K. Y., Leung, A. O. W., Wu, S. C., & Wong, M. H. (2009). Environmental impact and human exposure to PCBs in Guiyu, an electronic waste recycling site in China. Environment International, 35 (1), 76–82. doi: 10.1016/j.envint.2008.07.025\nYang, Y., Xue, M., Xu, Z., & Huang, C. (2013). Health risk assessment of heavy metals (Cr, Ni, Cu, Zn, Cd, Pb) in circumjacent soil of a factory for recycling waste electrical and electronic equipment. Journal of Material Cycles and Waste Management, 15 (4), 556–563. doi: 10.1007/s10163-013-0120-2\nTao, X.-Q., Shen, D.-S., Shentu, J.-L., Long, Y.-Y., Feng, Y.-J., & Shen, C.-C. (2015). Bioaccessibility and health risk of heavy metals in ash from the incineration of different e-waste residues. Environmental Science and Pollution Research, 22 (5), 3558–3569. doi: 10.1007/s11356-014-3562-8\nChan, J. K. Y., Man, Y. B., Wu, S. C., & Wong, M. H. (2013). Dietary intake of PBDEs of residents at two major electronic waste recycling sites in China. Science of The Total Environment, 463–464 (Supplement C), 1138–1146. doi: 10.1016/j.scitotenv.2012.06.093\nFu, J., Zhou, Q., Liu, J., Liu, W., Wang, T., Zhang, Q., & Jiang, G. (2008). High levels of heavy metals in rice (Oryzasativa L.) from a typical E-waste recycling area in southeast China and its potential risk to human health. Chemosphere, 71 (7), 1269–1275. doi: 10.1016/j.chemosphere.2007.11.065\nLuo, C., Liu, C., Wang, Y., Liu, X., Li, F., Zhang, G., & Li, X. (2011). Heavy metal contamination in soils and vegetables near an e-waste processing site, south China. Journal of Hazardous Materials, 186 (1), 481–490. doi: 10.1016/j.jhazmat.2010.11.024\nLeung, A. O. W., Duzgoren-Aydin, N. S., Cheung, K. C., & Wong, M. H. (2008). Heavy Metals Concentrations of Surface Dust from e-Waste Recycling and Its Human Health Implications in Southeast China. Environmental Science & Technology, 42 (7), 2674–2680. doi: 10.1021/es071873x\nWu, C.-C., Bao, L.-J., Tao, S., & Zeng, E. Y. (2016). Dermal Uptake from Airborne Organics as an Important Route of Human Exposure to E-Waste Combustion Fumes. Environmental Science & Technology, 50 (13), 6599–6605. doi: 10.1021/acs.est.5b05952\nXing, G. H., Liang, Y., Chen, L. X., Wu, S. C., & Wong, M. H. (2011). Exposure to PCBs, through inhalation, dermal contact and dust ingestion at Taizhou, China – A major site for recycling transformers. Chemosphere, 83 (4), 605–611. doi: 10.1016/j.chemosphere.2010.12.018\nKim, K.-H., Jahan, S. A., Kabir, E., & Brown, R. J. C. (2013). A review of airborne polycyclic aromatic hydrocarbons (PAHs) and their human health effects. Environment International, 60 (Supplement C), 71–80. doi: 10.1016/j.envint.2013.07.019\nWang, J., Chen, S., Tian, M., Zheng, X., Gonzales, L., Ohura, T., Simonich, S. L. M. (2012). Inhalation Cancer Risk Associated with Exposure to Complex Polycyclic Aromatic Hydrocarbon Mixtures in an Electronic Waste and Urban Area in South China. Environmental Science & Technology, 46 (17), 9745–9752. doi: 10.1021/es302272a\nLuo, P., Bao, L.-J., Li, S.-M., & Zeng, E. Y. (2015). Size-dependent distribution and inhalation cancer risk of particle-bound polycyclic aromatic hydrocarbons at a typical e-waste recycling and an urban site. Environmental Pollution, 200 (Supplement C), 10–15. doi: 10.1016/j.envpol.2015.02.007\nGuo, Y., Huo, X., Wu, K., Liu, J., Zhang, Y., & Xu, X. (2012). Carcinogenic polycyclic aromatic hydrocarbons in umbilical cord blood of human neonates from Guiyu, China. Science of The Total Environment, 427–428 (Supplement C), 35–40. doi: 10.1016/j.scitotenv.2012.04.007\nZheng, J., He, C.-T., Chen, S.-J., Yan, X., Guo, M.-N., Wang, M.-H., Mai, B.-X. (2017). Disruption of thyroid hormone (TH) levels and TH-regulated gene expression by polybrominated diphenyl ethers (PBDEs), polychlorinated biphenyls (PCBs), and hydroxylated PCBs in e-waste recycling workers. Environment International, 102 (Supplement C), 138–144. doi: 10.1016/j.envint.2017.02.009\nWorld Health Organization [WHO]. (2010). Preventing Disease Through Healthy Environments. Retrieved on December 2, 2017 from http://www.who.int/ipcs/features/10chemicals_en.pdf?ua=1\nGuo, Y., Huo, X., Li, Y., Wu, K., Liu, J., Huang, J., Xu, X. (2010). Monitoring of lead, cadmium, chromium and nickel in placenta from an e-waste recycling town in China. Science of The Total Environment, 408 (16), 3113–3117. doi: 10.1016/j.scitotenv.2010.04.018\nLiu, J., Xu, X., Wu, K., Piao, Z., Huang, J., Guo, Y., Huo, X. (2011). Association between lead exposure from electronic waste recycling and child temperament alterations. Neuro Toxicology, 32 (4), 458–464. doi: 10.1016/j.neuro.2011.03.012\nWang, X., Miller, G., Ding, G., Lou, X., Cai, D., Chen, Z., Han, J. (2012). Health risk assessment of lead for children in tinfoil manufacturing and e-waste recycling areas of Zhejiang Province, China. Science of the Total Environment, 426, 106–112. doi: 10.1016/j.scitotenv.2012.04.002\nZhang, K., Schnoor, J. L., & Zeng, E. Y. (2012). E-waste recycling: Where does it go from here? Environmental Science and Technology, 46 (20), 10861–10867. https://doi.org/10.1021/es303166s\nUnited Nations Environment Management Group [UNEMG]. (2017). United Nations System-wide Response to Tackling E-waste. Retrieved November 22, 2017 from https://unemg.org/images/emgdocs/ewaste/E-Waste-EMG-FINAL.pdf\nOngondo, F. O., Williams, I. D., & Cherrett, T. J. (2011). How are WEEE doing? A global review of the management of electrical and electronic wastes. Waste Management, 31 (4), 714–730. doi: 10.1016/j.wasman.2010.10.023\nLong, Yuyang & Feng, Yi-Jian & Cai, Si-Shi & Hu, Li-Fang & Shen, Dong-Sheng. (2014). Reduction of heavy metals in residues from the dismantling of waste electrical and electronic equipment before incineration. Journal of Hazardous Materials. 272. 59–65. 10.1016/j.jhazmat.2014.02.048.\nBeiyuan, J., Tsang, D. C. W., Yip, A. C. K., Zhang, W., Ok, Y. S., & Li, X.-D. (2017). Risk mitigation by waste-based permeable reactive barriers for groundwater pollution control at e-waste recycling sites. Environmental Geochemistry and Health, 39 (1), 75–88. doi: 10.1007/s10653-016-9808-2\nYe, M., Sun, M., Wan, J., Fang, G., Li, H., Hu, F. Orori Kengara, F. (2015). Evaluation of enhanced soil washing process with tea saponin in a peanut oil–water solvent system for the extraction of PBDEs/PCBs/PAHs and heavy metals from an electronic waste site followed by vetiver grass phytoremediation. Journal of Chemical Technology & Biotechnology, 90 (11), 2027–2035. doi: 10.1002/jctb.4512']	['<urn:uuid:03290abb-822b-40c2-afc6-8d48eeb12eb7>', '<urn:uuid:292e510a-9722-4c47-9ed5-81cd75880cfd>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	10	83	3161
51	difference between wto and eu on environmental subsidies treatment	Based on both documents, there is a key difference in how WTO and EU handle environmental subsidies - while the WTO primarily views subsidies through their trade distortion effects under agreements like SCM, focusing on prohibited and actionable subsidies, the EU explicitly promotes environmental protection through state aid measures, having a specific framework for 'Environmental Aid' for energy projects and considering environmental protection as a core objective integrated across policies.	['About this book:\nThe WTO Law of Subsidies provides a comprehensive analysis of the law of subsidies under the WTO regime. Subsidies are arguably the dominant theme in International Economic Law and a prolific case law has been elaborated by WTO Panels and Appellate Body in response to the multitude of complaints lodged in the past two decades (Softwood Lumber, Airbus, Boeing, etc.). The case law and norms disciplining subsidies under the WTO legal regime are of utmost importance first for international trade ministries, parliaments, and international institutions (OECD, CNUCED, FAO, etc.). However, non-governmental organizations (World Wide Fund, etc.) are also directly concerned by this topic regarding, for example, fisheries subsidies and their impact on overexploitation of marine resources. The private sector (fishing fleets, fishermen, extractive industries, etc.) is also affected by this topic particularly regarding future investments.\nUnfortunately, it is possible to be overwhelmed by the complexity of this case law. Going through this complexity, this book offers highly strategic information to both complaining and defending parties on how to advance legally their interests. Actually, one of the major characteristics of this book is to address all legal subtleties without ever trying to avoid them.\nWhat’s in this book:\nThis book is structured into three Parts, as follows:\n- Part I adopts the perspective of a WTO member seeking to counter an alleged subsidy granted by another Member. To this end, this Part scans all WTO Agreements containing cumulative disciplines and remedies relating to the granting of subsidies. Therefore, not only the SCM Agreement, but also the Agreement on Agriculture (AoA), GATT 1994, and even the 1980 Agreement on Trade in Civil Aircraft (ATCA) are scanned and analyzed in detail.\n- Part II adopts the perspective of a WTO Member accused of granting subsidies violating subsidies disciplines. To this end, an original classification is offered of the various strategies that can be used by such a Member. For this purpose, a distinction is made between the “threshold strategy” where the existence of a challengeable subsidy is recused from the outset by the accused Member, the “denying violation of disciplines strategy,” the “exemption or exception strategy,” the “procedural and evidentiary strategy,” and finally the “implementing strategy.”\n- Part III of this book, which could turn out to be the most useful for the community of agents concerned by subsidies, offers an original examination of pending legal issues. To this end, a relevant distinction is established between pending legal issues partially answered by present case law and pending legal issues not still answered by present case law.\nHow this will help you:\nAs the title of this book indicates, one of its notable aspects is that it is “comprehensive.” First, it avoids unnecessary legal jargon, making it accessible to a large public. Second, it adopts a progressive approach where legal subtleties are not avoided but presented at the right moment and the right place, thereby not overwhelming the reader. This peerless book is thus a source of strategic knowledge for practitioners, academics, students, policy makers in governments and international organizations. Law firms involved in subsidies cases are naturally at the forefront of the community of agents concerned by this topic.\n|Publish Frequency||As Needed|\n|Product Line||Kluwer Law International|\nList of Abbreviations\nScanning WTO Agreements for Cumulative Disciplines and Remedies to Counter an Alleged Subsidy Granted by Another Member (The Perspective of the Accusing Member)\nDefining the Set of Measures Subject to SCM Disciplines (So-Called “Specific Subsidies”)\nVarious Subtleties Relating to the Three Criteria Defining a Specific Subsidy (i.e., Financial Contribution, Benefit, and Specificity)\nSatisfying Ex Ante, the Anti-distortion “Object and Purpose” of the SCM: Determining the Subset of Subsidies Subject to per se Disciplines (So-Called “Prohibited Subsidies”)\nAlternative Remedies under the SCM Agreement for a Violation of Obligations Relating to Prohibited Subsidies\nSatisfying Ex-Post, the Anti-distortion “Object and Purpose” in the SCM: Defining the Subset of “Actionable Subsidies” Based on Their Adverse Trade Effects\nAlternative Remedies under the SCM Agreement for a Violation of Obligations Relating to Actionable Subsidies\nRecourse to Countervailing Duties (CVDs) in the Domestic Forum of the Importing Country on the Basis That the Actionable Subsidies Are\n“Countervailable” and Not Because They Are Actionable\nSubtleties Relating to Actionable Subsidies\nHow to Know If a Measure Is a “Subsidy” or an “Export Subsidy” under GATT 1994?\nGATT 1994 Per Se Disciplines\nGATT 1994 Effects-Based Disciplines\nAlternative Remedies under an (Unlikely) Exclusive GATT 1994 Claim Relating to a Non-agricultural Subsidy\nHow a TRIMs Measure Could Involve a Subsidy\nRemedy under TRIMs for the Measure at Issue and Consequence for the Involved Subsidy\nPrincipal Subsidies Disciplines of the ATCA\nRemedies under the ATCA and Why the ATCA Is Not Invoked as a Basis of a Complaint About Civil Aircraft Subsidies\nWhat Is Behind the Expression “Export Subsidy” in the AoA?\nWhat Is Behind the Expression “Domestic Support” in the AoA?\nPer Se Disciplines in the Agreement on Agriculture\nGATT 1994 Disciplines Based on the Avoidance of Adverse Trade Effects\nPer Se Disciplines under Article XVI:4 of GATT 1994: A Discord Between Article XVI:4 of GATT 1994 and the AoA?\nThe Appellate Body’s Clarification in US – Upland Cotton of the Meaning of the Expressions “Subject to” and “Except as Provided in the AoA”\n“EC Sugar”: Confirmation by the Appellate Body of the “Specific Provisions Dealing Specifically with the Same Matter” Doctrine\nScanning WTO Case Law for Strategies to Refute Violation of Subsidies Disciplines (The Perspective of the Accused Subsidizing Member)\nInvoking a Missing “Financial Contribution”\nThe Threshold Strategy Through the Benefit Criterion (I)\nThe Threshold Strategy Through the Benefit Criterion (II): “Pass-Through” of Benefit\nThe Threshold Strategy Through the Contestation of the Specificity of the Measure at Issue\nSome Examples of the Threshold Strategy in the Context of a Complaint Invoking the Agreement on Agriculture\nDenying Violation of the Prohibition of “Contingency upon Export Performance”\nDenying Violation of the Prohibition of “Contingency upon the Use of Domestic over Imported Products”\nContesting (in the CVD Context) That the Causal Link Between Subsidized Imports and Injury to A Domestic Industry Is Adequately Established\nContesting (in a Non-CVD Context) That a Causal Relationship Has Been Demonstrated Between a Subsidy and Adverse Effects to the Interests of Other Members: The Paradox of a More Sophisticated Causality Analysis Than in a CVD Context\nCountering Successfully That There Was Not an Appropriate Matching by the CVD Authorities Between the Elements in the Numerator and The\nElements in the Denominator When Calculating the Per-unit Subsidy Ratio\nInvoking Explicit Exemptions for Developing Countries (SCM Based Complaint)\nInvoking Implicit Exemptions (SCM Based Complaint)\nArguing Successfully in EU – PET (Pakistan) That There Is No Exception to the Excess Remissions Principle When Determining How Remissions\nunder Duty Drawback Schemes Are to Be Identified as Subsidies\nInvoking Unsuccessfully the Exception Strategy in the AoA: Does Article 10.2 of the AoA Exempts Agricultural Export Credits from the\nExport Subsidy Disciplines of the AoA?\nRejecting Successfully That a Certain Measure Is Not a “Measure Taken to Comply” and Therefore Does Not Fall Within Article 21.5 Proceedings: The “Inextricable Link” or “Clear Connection” Test\nArguing Unsuccessfully That the Original Recommendations and Rulings Do Not Remain Operative Through Compliance Panels Proceedings Unless New Recommendations Are Issued by These Panels: The Case of FSC Subsidies\nArguing Unsuccessfully That Similar Payments Made after the Original Proceedings Are Beyond the Scope of Article 21.5\nArguing Unsuccessfully Before an Article 21.5 Panel That, When an Article Covers Multiple Obligations, the Mere Listing of This Article in the Panel Request Is Sufficient for the Panel Request to Cover All These Obligations\nAttempting Unsuccessfully to Introduce in Compliance Proceedings a New Claim Relating to Unchanged Aspects of the Original Measure\nArguing Successfully That If the Underlying Subsidy Has Ceased to Exist (And Therefore Has Been “Withdrawn” in a Passive Way), There Is No\nAdditional Requirement, under Article 7.8 of the SCM, to Remove Any Lingering Effects That May Flow from That Subsidy\nScanning Pending Legal Issues in the Context of Present Norms\nConfirming a Narrow Interpretation of the Concept of “Price or Income Support”\nGATT Article XX Exceptions Carried Forward to Violations of the SCM and the AoA?\nDouble Counting Hypothesis for Non-market Economies: Recent Recognition by the Appellate Body of Its Possibility When Concurrent Countervailing Duties and Antidumping Duties Are Levied (The Case of Domestic Subsidies)\nConfirming the Prospective Nature of WTO Remedies\nCalculating the Ratio of Per Unit Subsidization in CVD Proceedings: Could the Denominator Include the Subsidy Recipient’s Production\nOverseas? The US Washing Machines Jurisprudence\nDual Pricing (Gas, Oil, Etc.): Not a Challengeable Subsidy Because the Specificity Criterion Is Probably Not Satisfied\nFinal Clarification on Why Article III:8(b) of GATT 1994 Does Not Shield Discriminatory Internal Taxes on Imported Products from the National\nTreatment Disciplines of Article III of GATT 1994\nIs an Undervalued Exchange Rate System a De Facto Export Subsidy?\nOpen Legal Questions Relating to Fisheries Subsidies under Present Norms\nCarbon Dioxide Emissions\nThe Adequate Legal Approach to “Transnational Subsidies” (Also Called “Extended Subsidies”)\nFisheries Subsidies in the New Draft SCM: At Last a Recognition of the Concept of Environmentally Harmful Subsidies?\nThe Paradox of the Absence of Complaints in the WTO Against Measures Taken in Export Processing Zones, Special Industrial Zones, or Free Trade Zones\nWould the US “Destination-Based Cash-Flow Tax” Be WTO Consistent?\nIs a Carbon Tax Eligible to BTA?\nIn Connection with the Panel Request\nIn Relation with Collection of Information\nAttempting Unsuccessfully to Produce Evidence Based on the Practice of Other Members\nArguing Successfully That the Fact, That a Measure Induces the Exercise of Rights That Are WTO Consistent, Is Not a Valid Consideration in Deciding Whether Such Measure Is a Specific Action “Against” a Subsidy under Article 32.1 of the SCM\nArguing Successfully That the SCM Leaves to Individual Members How to Determine in Their Internal Regime What Constitutes the Date of Initiation of a CVD Investigation\nArguing Unsuccessfully There Is a Requirement in Article 13.1 of the SCM to Allow Sufficient Time for Consultations to Be Held Before Initiation of\nInvoking Successfully That a System of Limited Disclosure Provides a Derogation from, or Replaces, the Obligations of an Investigating Authority\nunder (This) Article to Require Justification for Treatment of Information as Confidential and, If Such Treatment Is Justified, to Require Non-confidential Summaries of the Confidential Information, or, Alternatively, to Require Justification for the Non-summarization of Certain Information (Article 12.4.1 of the SCM)\nInvoking the Failure by the Investigative Authorities to Inform the Interested Members and Interested Parties of the “Essential Facts under Consideration Which Form the Basis for the Decision Whether to Apply Definitive Measures”\nArguing Unsuccessfully That an Enterprise or Group of Enterprises (Fortuny in This Case) May Not Be Considered to Constitute a Domestic\nIndustry If, at the Time the Application Is Filed or During the Subsidy POI, It or They Were Not Actually Producing Output\nContesting Unsuccessfully the CVD Application Is “by or on Behalf of the Domestic Industry,” Namely That Economía’s Examination of the Degree of Support for the Application Was Inadequate\nInvoking Successfully a Bad Selection of the Injury Period of Analysis in the CVD Context\nArguing Successfully That the Panel Failed to Apply the Proper Standard of Review\nClaiming Unsuccessfully That a Panel Is Justified to Exclude During Its Proceedings Evidence That, Although Contained in the Record of the CVD\nInvestigation, Had Not Been Cited in the CVD’s Decision\nIssues Relating to the Use of Adverse Facts Available\nClaiming Unsuccessfully That the Investigative Authorities Should Have Made a Factual Inference from Evidence on the Record That Would Not\nReasonably Have Suggested Such an Inference\nArguing Successfully That the Panel Did Not Use the Expression “Probative and Compelling” in the Sense of a New Evidentiary Standard for Examining Evidence on the Record of a CVD Investigative Authority\nFailing to Counter US’ Allegation That the Panel Appeared Not to Have Considered Seriously Any Evidence That Did Not Amount to a “Smoking\nGun” (for Showing Entrustment or Direction)\nArguing Successfully That in the Case of Non-recurring Subsidies, If the POI Used for Determining the Existence of a Subsidy Indicates That the\nSubsidy Benefit Will No Longer Exist at a Future Time When a CVD Is Imposed, Then This CVD Is Inconsistent with the SCM\nRequesting Successfully That the Panel Make Findings Regarding Expired Measures Which Ceased to Have Legal Effect\nArguing Successfully That the Investigative Authorities Failed to Provide the “Results” of a Verification Visit in Violation of Article 12.6 of the SCM\nWTO Dispute Settlement Reports and Arbitration Awards as at 1 March 2019', 'Environmental protection has been at the core of the European Union integration project even before it was explicitly provided for in the Treaties. The Court of Justice of the EU has been adamant in enforcing environmental protection requirements and has led the evolution of EU environmental protection legislation. Environmental protection was included in the Single European Act and reinforced later in the Maastricht Treaty for the EU. The legislative framework is completed by a series of EU secondary legislation spanning through various sectors and imposing substantive and procedural rules on environmental protection. Energy regulation has been introduced later in the EU context. Economic realities have made it imperative to establish an internal market in the energy sector, thus pushing forward radical challenges across EU Member States. The evolution of energy regulation in the EU has started from sector specific secondary legislation towards a more general EU approach and, finally, an amendment to the EU Treaties.\nThis Course is intended to present the dynamics of the evolution of environmental protection and energy regulation in the EU and give insights into their mutual interaction. To this end basic concepts of both environmental and energy law are introduced and put in context along the Treaty framework and the general principles of these specific areas of EU law. Particular focus will be given to the application of the relevant rules by the Court of Justice of the EU, while analyzing the judicial role in the evolution of environmental and energy law. The Course is also aimed at discussing the various regulatory methods used in order to achieve a higher level of environmental protection, particularly by presenting impact assessment and public participation in decision making procedures. The basic tools in energy regulation are presented, with a particular emphasis on the governance methods introduced across EU member States. Methods of the Course primarily include the interaction of EU Environmental and Energy law requirements, as well as the relations of the respective rules with other Treaty rules (competition, state aids and fundamental freedoms). This interaction is further elaborated in certain sectors, such as the Renewable Energy Sources and the challenges faces in terms of environmental protection, energy regulation and competition policy.\nThe course starts with a dynamic perspective of the concept and content of environmental protection and energy regulation. It then turns to the Treaty rules and general principles applicable in these sectors and their interaction. It then turns the spotlight on the most important environmental protection mechanisms and examines the role of impact assessment, liability and public participation in decision making processes.\nThe course then shifts towards the core energy regulation mechanisms across EU Member States and examines aspects of regulatory governance in particular. In the last lectures attention is drawn exclusively on the ways these two areas of law interact in certain fields, also in the context of general EU law requirements such as competition law and state aid\nThe course is organized on the basis of 13 lectures as follows:\n1. Introduction: Environmental protection and Energy regulation in context\n2. Treaty provisions on Environmental protection and Energy regulation – General principles\n3. Interaction with fundamental freedoms and competition rules\n4. Environmental protection mechanisms: Impact Assessment\n5. Environmental protection procedures: Public Participation\n6. Environmental Liability\n7. Evolution of energy regulation across the EU\n8. Energy regulation mechanisms: Unbundling and Third Party Access\n9. Energy regulation procedures: regulatory governance\n10. The future of EU energy law in a global context\n11. Biodiversity and Climate change: energy networks and renewable energy sources challenges for the energy sector\n12. Environmental Aid for energy projects\n13. Bringing it all together – future challenges\nBy the end of the course, students will be able to engage in critical analysis of a number of areas of EU environmental and energy law. They will also be familiarized with a number of instruments of regulation in both areas of law.\nStudents are expected to acquire an in-depth insight of the various mechanisms of environmental protection and energy regulation. The basic legal framework, both in terms of Treaty provisions and general principles will be fully understandable by all students. Further, students will be in a position to identify key techniques in EU Environmental and Energy Law, while assessing their significance both at an EU and member State level.\nHaving successfully attended the Course, students will have a clear vision of th interaction between these two areas of law and will be able to spot the methods in which the respective rules work in a supplementary and, sometimes, contradictory way. By emphasizing on certain examples such as the Renewable Energy Sources, the Course will greatly benefit practice-oriented students, both in terms of litigation and counselling.\nStudents having followed this course will posses the necessary knowledge in order to be able to work in any field of EU Environmental and Energy Law, either as litigators, counsels, government officials. Their services could vary from cross-sector policy desing to law enforcement and could be offered also to local authorities and governmental bodies, NGOs, international and EU organizations and committees and, of course, law chambers.\n– Jans, J. and Vedder, H, European Environmental Law, 4th ed (Groningen: Europa Law Publishing, 2012).\n– Johnston, A. and Block, G., EU Energy Law (Oxford: OUP, 2013).\n– Kraemer, L., EU Environmental Law, 7th ed (London: Sweet&Maxwell, 2012).\n– Talus, K., EU Energy Law and Policy (Oxford: OUP, 2013).\n– Holder, J. and Lee, M., Environmental Protection, Law and Policy (Cambridge: CUP, 2007).\n– Lee, M., EU Environmental Law. Challenges, Change, and Decision-Making (Oxford/Portland: Hart Publishing, 2005).\n– Scott, J. (ed), Environmental Protection: European Law and Governance (Cambridge: CUP, 2008).\n– Roggenkamp, M. et al.(eds), Energy Law in Europe. National, EU and International Regulation (Oxford: OUP 2007).\n– Macrory, R. (ed), Reflections on 30 years of EU Environmental Law: A High Level of Protection? (Groningen: Europa Law Publishing, 2006).\n– Cameron, P., Competition in Energy Markets. Law and Regulation in the European Union, 2nd ed (Oxford: OUP: 2007).\n– Pallemaerts, M. (ed), The Aarhus Convention at Ten. Interactions and Tensions between Conventional International Law and EU Environmental Law (Groningen: Europa Law Publishing, 2011).\n– Bosselman, F. et al. (eds), Energy, Economics and the Environment. Cases and Materials, 2nd edition (New York: Foundation Press: 2006)\n– Makuch, K. and Pereira, R. (eds), Environmental and Energy Law (New York: Wiley-Blackwell, 2012).\n– Macrory, R. (ed), Principles of European Environmental Law (Groningen: Europa Law Publishing, 2004).\n– Jans, J., Macrory, R. and Moreno-Molina, A.-M. (eds), National Courts and EU Environmental Law (Groningen: Europa Law Publishing, 2013).\n– Farantouris, N. (ed), Energy. Law, Economics and Politics (Athens: Nomiki Vivliothiki, 2012).\n– Dellis, G., EC Environmental Law (Athens: Ant. N. Sakkoulas Law Publishing, 1997).\nLectures: 13 lectures x 2 hours = 26 hours\nTutorials: 7 tutorials x 2 hours = 14 hours\nTotal: = 40 hours\nFinal Exam = %\nCourse Participation = %\nPaper(s) = %\nTotal = %\nWhile the ‘Course Objectives’ and ‘Educational Outcomes’ above remain immutable, the ‘Course Content’ and ‘Course Outline’ may be altered in order to accommodate student’s needs and individual professor’s approaches. Bibliography and reading materials may vary accordingly.']	['<urn:uuid:2dedf606-4784-4a4d-bf5a-ab309b99c268>', '<urn:uuid:d04991cf-3ee7-4ef9-8401-6c555e2085ec>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	9	70	3274
52	I'm planning a casino trip and I've heard that some games give you better chances than others. Which one has a lower house advantage: American roulette or basic strategy blackjack?	Blackjack with basic strategy has a significantly lower house edge than American roulette. American roulette has a house advantage of approximately 5.26% due to having two zeros on the wheel. In contrast, blackjack's house edge can be as low as 0.5% to 1% when proper strategy is used, and skilled players can even reduce it further to 0.5% or less with optimal play.	['Gambling, the betting or staking of something of value, with consciousness of risk and hope of gain, on the outcome of a game, a contest, or an uncertain event whose result may be determined by chance or accident or have an unexpected result by reason of the bettor’s miscalculation.\nThe outcomes of gambling games may be determined by chance alone, as in the purely random activity of a tossed pair of dice or of the ball on a roulette wheel, or by physical skill, training, or prowess in athletic contests, or by a combination of strategy and chance. The rules by which gambling games are played sometimes serve to confuse the relationship between the components of the game, which depend on skill and chance, so that some players may be able to manipulate the game to serve their own interests. Thus, knowledge of the game is useful for playing poker or betting on horse racing but is of very little use for purchasing lottery tickets or playing slot machines.\nA gambler may participate in the game itself while betting on its outcome (card games, craps), or he may be prevented from any active participation in an event in which he has a stake (professional athletics, lotteries). Some games are dull or nearly meaningless without the accompanying betting activity and are rarely played unless wagering occurs (coin tossing, poker, dice games, lotteries). In other games betting is not intrinsically part of the game, and the association is merely conventional and not necessary to the performance of the game itself (horse racing, football pools). Commercial establishments such as casinos and racetracks may organize gambling when a portion of the money wagered by patrons can be easily acquired by participation as a favoured party in the game, by rental of space, or by withdrawing a portion of the betting pool. Some activities of very large scale (horse racing, lotteries) usually require commercial and professional organizations to present and maintain them efficiently.\nPrevalence of principal forms\nA rough estimate of the amount of money legally wagered annually in the world is about $10 trillion (illegal gambling may exceed even this figure). In terms of total turnover, lotteries are the leading form of gambling worldwide. State-licensed or state-operated lotteries expanded rapidly in Europe and the United States during the late 20th century and are widely distributed throughout most of the world. Organized football (soccer) pools can be found in nearly all European countries, several South American countries, Australia, and a few African and Asian countries. Most of these countries also offer either state-organized or state-licensed wagering on other sporting events.\nBetting on horse racing is a leading form of gambling in English-speaking countries and in France. It also exists in many other countries. Wherever horse racing is popular, it has usually become a major business, with its own newspapers and other periodicals, extensive statistical services, self-styled experts who sell advice on how to bet, and sophisticated communication networks that furnish information to betting centres, bookmakers and their employees, and workers involved with the care and breeding of horses. The same is true, to a smaller extent, of dog racing. The emergence of satellite broadcasting technology has led to the creation of so-called off-track betting facilities, in which bettors watch live telecasts at locations away from the racetrack.\nCasinos or gambling houses have existed at least since the 17th century. In the 20th century they became commonplace and assumed almost a uniform character throughout the world. In Europe and South America they are permitted at many or most holiday resorts but not always in cities. In the United States casinos were for many years legal only in Nevada and New Jersey and, by special license, in Puerto Rico, but most other states now allow casino gambling, and betting facilities operate clandestinely throughout the country, often through corruption of political authorities. Roulette is one of the principal gambling games in casinos throughout France and Monaco and is popular throughout the world. Craps is the principal dice game at most American casinos. Slot and video poker machines are a mainstay of casinos in the United States and Europe and also are found in thousands of private clubs, restaurants, and other establishments; they are also common in Australia. Among the card games played at casinos, baccarat, in its popular form chemin de fer, has remained a principal gambling game in Great Britain and in the continental casinos most often patronized by the English at Deauville, Biarritz, and the Riviera resorts. Faro, at one time the principal gambling game in the United States, has become obsolete. Blackjack is the principal card game in American casinos. The French card game trente et quarante (or rouge et noir) is played at Monte-Carlo and a few other continental casinos. Many other games may also be found in some casinos—for example, sic bo, fan-tan, and pai-gow poker in Asia and local games such as boule, banca francesa, and kalooki in Europe.\nAt the start of the 21st century, poker exploded in popularity, principally through the high visibility of poker tournaments broadcast on television and the proliferation of Internet playing venues. Another growing form of Internet gambling is the so-called betting exchanges—Internet Web sites on which players make wagers with one another, with the Web site taking a small cut of each wager in exchange for organizing and handling the transaction.\nTest Your Knowledge\nPop Culture Quiz\nIn a wide sense of the word, stock markets may also be considered a form of gambling, albeit one in which skill and knowledge on the part of the bettors play a considerable part. This also goes for insurance; paying the premium on one’s life insurance is, in effect, a bet that one will die within a specified time. If one wins (dies), the win is paid out to one’s relatives, and if one loses (survives the specified time), the wager (premium) is kept by the insurance company, which acts as a bookmaker and sets the odds (payout ratios) according to actuarial data. These two forms of gambling are considered beneficial to society, the former acquiring venture capital and the latter spreading statistical risks.\nChances, probabilities, and odds\nEvents or outcomes that are equally probable have an equal chance of occurring in each instance. In games of pure chance, each instance is a completely independent one; that is, each play has the same probability as each of the others of producing a given outcome. Probability statements apply in practice to a long series of events but not to individual ones. The law of large numbers is an expression of the fact that the ratios predicted by probability statements are increasingly accurate as the number of events increases, but the absolute number of outcomes of a particular type departs from expectation with increasing frequency as the number of repetitions increases. It is the ratios that are accurately predictable, not the individual events or precise totals.\nThe probability of a favourable outcome among all possibilities can be expressed: probability (p) equals the total number of favourable outcomes (f) divided by the total number of possibilities (t), or p = f/t. But this holds only in situations governed by chance alone. In a game of tossing two dice, for example, the total number of possible outcomes is 36 (each of six sides of one die combined with each of six sides of the other), and the number of ways to make, say, a seven is six (made by throwing 1 and 6, 2 and 5, 3 and 4, 4 and 3, 5 and 2, or 6 and 1); therefore, the probability of throwing a seven is 6/36, or 1/6.\nIn most gambling games it is customary to express the idea of probability in terms of odds against winning. This is simply the ratio of the unfavourable possibilities to the favourable ones. Because the probability of throwing a seven is 1/6, on average one throw in six would be favourable and five would not; the odds against throwing a seven are therefore 5 to 1. The probability of getting heads in a toss of a coin is 1/2; the odds are 1 to 1, called even. Care must be used in interpreting the phrase on average, which applies most accurately to a large number of cases and is not useful in individual instances. A common gamblers’ fallacy, called the doctrine of the maturity of the chances (or the Monte-Carlo fallacy), falsely assumes that each play in a game of chance is dependent on the others and that a series of outcomes of one sort should be balanced in the short run by the other possibilities. A number of systems have been invented by gamblers largely on the basis of this fallacy; casino operators are happy to encourage the use of such systems and to exploit any gambler’s neglect of the strict rules of probability and independent plays. An interesting example of a game where each play is dependent on previous plays, however, is blackjack, where cards already dealt from the dealing shoe affect the composition of the remaining cards; for example, if all of the aces (worth 1 or 11 points) have been dealt, it is no longer possible to achieve a “natural” (a 21 with two cards). This fact forms the basis for some systems where it is possible to overcome the house advantage.\nIn some games an advantage may go to the dealer, the banker (the individual who collects and redistributes the stakes), or some other participant. Therefore, not all players have equal chances to win or equal payoffs. This inequality may be corrected by rotating the players among the positions in the game. Commercial gambling operators, however, usually make their profits by regularly occupying an advantaged position as the dealer, or they may charge money for the opportunity to play or subtract a proportion of money from the wagers on each play. In the dice game of craps—which is among the major casino games offering the gambler the most favourable odds—the casino returns to winners from 3/5 of 1 percent to 27 percent less than the fair odds, depending on the type of bet made. Depending on the bet, the house advantage (“vigorish”) for roulette in American casinos varies from about 5.26 to 7.89 percent, and in European casinos it varies from 1.35 to 2.7 percent. The house must always win in the long run. Some casinos also add rules that enhance their profits, especially rules that limit the amounts that may be staked under certain circumstances.\nMany gambling games include elements of physical skill or strategy as well as of chance. The game of poker, like most other card games, is a mixture of chance and strategy that also involves a considerable amount of psychology. Betting on horse racing or athletic contests involves the assessment of a contestant’s physical capacity and the use of other evaluative skills. In order to ensure that chance is allowed to play a major role in determining the outcomes of such games, weights, handicaps, or other correctives may be introduced in certain cases to give the contestants approximately equal opportunities to win, and adjustments may be made in the payoffs so that the probabilities of success and the magnitudes of the payoffs are put in inverse proportion to each other. Pari-mutuel pools in horse-race betting, for example, reflect the chances of various horses to win as anticipated by the players. The individual payoffs are large for those bettors whose winning horses are backed by relatively few bettors and small if the winners are backed by a relatively large proportion of the bettors; the more popular the choice, the lower the individual payoff. The same holds true for betting with bookmakers on athletic contests (illegal in most of the United States but legal in England). Bookmakers ordinarily accept bets on the outcome of what is regarded as an uneven match by requiring the side more likely to win to score more than a simple majority of points; this procedure is known as setting a “point spread.” In a game of American or Canadian football, for example, the more highly regarded team would have to win by, say, more than 10 points to yield an even payoff to its backers.\nUnhappily, these procedures for maintaining the influence of chance can be interfered with; cheating is possible and reasonably easy in most gambling games. Much of the stigma attached to gambling has resulted from the dishonesty of some of its promoters and players, and a large proportion of modern gambling legislation is written to control cheating. More laws have been oriented to efforts by governments to derive tax revenues from gambling than to control cheating, however.\nGambling is one of mankind’s oldest activities, as evidenced by writings and equipment found in tombs and other places. It was regulated, which as a rule meant severely curtailed, in the laws of ancient China and Rome as well as in the Jewish Talmud and by Islam and Buddhism, and in ancient Egypt inveterate gamblers could be sentenced to forced labour in the quarries. The origin of gambling is considered to be divinatory: by casting marked sticks and other objects and interpreting the outcome, man sought knowledge of the future and the intentions of the gods. From this it was a very short step to betting on the outcome of the throws. The Bible contains many references to the casting of lots to divide property. One well-known instance is the casting of lots by Roman guards (which in all likelihood meant that they threw knucklebones) for the garment of Jesus during the Crucifixion. This is mentioned in all four of the Gospels and has been used for centuries as a warning example by antigambling crusaders. However, in ancient times casting lots was not considered to be gambling in the modern sense but instead was connected with inevitable destiny, or fate. Anthropologists have also pointed to the fact that gambling is more prevalent in societies where there is a widespread belief in gods and spirits whose benevolence may be sought. The casting of lots, not infrequently dice, has been used in many cultures to dispense justice and point out criminals at trials—in Sweden as late as 1803. The Greek word for justice, dike, comes from a word that means “to throw,” in the sense of throwing dice.\nEuropean history is riddled with edicts, decrees, and encyclicals banning and condemning gambling, which indirectly testify to its popularity in all strata of society. Organized gambling on a larger scale and sanctioned by governments and other authorities in order to raise money began in the 15th century with lotteries—and centuries earlier in China with keno. With the advent of legal gambling houses in the 17th century, mathematicians began to take a serious interest in games with randomizing equipment (such as dice and cards), out of which grew the field of probability theory.\nApart from forerunners in ancient Rome and Greece, organized sanctioned sports betting dates back to the late 18th century. About that time there began a gradual, albeit irregular, shift in the official attitude toward gambling, from considering it a sin to considering it a vice and a human weakness and, finally, to seeing it as a mostly harmless and even entertaining activity. Additionally, the Internet has made many forms of gambling accessible on an unheard-of scale. By the beginning of the 21st century, approximately four out of five people in Western nations gambled at least occasionally. The swelling number of gamblers in the 20th century highlighted the personal and social problem of pathological gambling, in which individuals are unable to control or limit their gambling. During the 1980s and ’90s, pathological gambling was recognized by medical authorities in several countries as a cognitive disorder that afflicts slightly more than 1 percent of the population, and various treatment and therapy programs were developed to deal with the problem.', 'It’s essential to comprehend the idea of the house edge whether you like to play roulette, baccarat, blackjack, or any other casino game.\nWhat is the house advantage in casinos? How does it work? Why does it matter? All these questions and more will be answered in this article. Let’s dive into the world of house edge and see how it affects your time at the casino.\nHouse Edge: Definition\nFundamentally, the house edge refers to a mathematical benefit that the casino possesses in relation to the players. This concept denotes the proportion of every bet that the casino anticipates to get as profit over an extended period. In a nutshell, it is the casino’s guaranteed return on investment or return on capital.\nThis principle universally applies to every casino game and constitutes a vital element of their overall design. An example that deviates from this principle is Poker, played exclusively between individuals and hence has no house edge. But in poker, the house also takes rake, so they nonetheless profit from the game.\nThe House Edge: How Is It Determined?\nThe house edge measures the casino’s advantage in a game and is based on several variables, such as the odds, probabilities, and rules involved. But before anything else, you must ensure that the casino you plan to wager in is licensed and offers various games. For instance, Florida has a robust land-based casino sector, but residents are prohibited from using any online gambling services. Please view more here for more information regarding gambling in Florida.\nLet’s examine the mathematics behind the house edge for a few familiar casino games:\nRoulette’s House Edge\nPlayers are captivated by roulette’s exhilarating wheel spin and the prospect of huge payouts. Roulette’s house edge varies from game to game. The house edge is roughly 2.7% when playing European roulette with a single zero.\nHowever, because of a second zero in American roulette, the house edge is roughly 5.26 %. These numbers are the average over a long period, so keep that in mind.\nBaccarat’s House Edge\nAmong casino games, baccarat is well-known for its timeless reputation as one of the most classic and easy to play. Compared to other casino games, baccarat’s house edge is one of the lowest, making it a popular pick among gamblers.\nBaccarat’s house edge typically falls between 1% and 1.5%, though this does vary per stake. The house edge is slightly smaller when wagering on the banker than on the participant or a tie.\nBlackjack’s House Edge\nPlayers of the popular card game Blackjack would do well to familiarize themselves with the house edge at the table. Depending on the player’s approach and the particular game regulations, blackjack’s house edge might change.\nBlackjack has a house edge of between 0.5 and 1% typically. With proper blackjack strategy, skilled gamers can even bring the house edge down to 0.5% or less.\nWhat Makes the House Edge Important?\nEvery gambler should be aware of the house advantage because it has a direct bearing on their long-term success or failure at the casino. With the house edge in place, the casino is virtually guaranteed to profit over the long run instead of the players.\nAlthough there is a chance for players to win big in the short run, the house edge ensures that the house will always win overall.\nWhat Effect Does the House Advantage Have on Players?\nThe house advantage guarantees a profit for the casino, but that doesn’t imply the players have to lose. Remember that the house advantage is determined over a long time frame, typically thousands of games.\nThe thrill and suspense of gambling comes from the fact that, in the near term, players stand an equal chance of winning and losing.\nHow to Reduce the House’s Advantage\nPlayers can nevertheless reduce the influence of the house edge on their bankroll by using various methods, even though the house edge is a defined mathematical concept. Consider the following:\nKnowing the Rules of the Game\nLearning the game’s rules is a good starting point for reducing the casino’s advantage. Rules for each game are unique, and even little changes can have a big impact on the house’s advantage.\nLearning the ins and outs of the game, such as any side bets or optional wagers, will help you make well-informed selections.\nMastering the Best Tactics\nAnother strategy to lower the house’s advantage is to use the best playing techniques possible. Numerous casino games have developed tactics that have been shown to lower the house edge or raise the participant’s chances of winning.\nIn blackjack, for instance, fundamental strategy charts detail the optimal moves to perform in response to your hand and the dealer’s upcard. The odds of success will increase dramatically if you implement these techniques.\nWhen trying to reduce the effect of the house edge, proper bankroll management is essential. Establish and maintain a limit for your wagering expenditures. Always know your limits and never chase losses.\nBeing prudent with your bankroll allows you to play longer and increases your chances of winning.\nAny casino player must comprehend the idea of the house edge. It symbolizes the edge in statistics that the casino has over the participants in every game.\nGamers can improve their chances of winning and have a more satisfying gaming experience by being aware of the house’s advantage and using techniques to reduce its effects. Always remember the house edge, whether you’re gambling at a physical casino or on a web-based gambling site.']	['<urn:uuid:17ab9e3a-bc9f-4461-9909-4cab91c64a0b>', '<urn:uuid:ea09c84f-0999-44e7-89ab-f40605f79e2a>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	30	63	3572
53	How do retailers use data analysis to make decisions about product modifications and innovation across different stages of the product lifecycle?	Retailers use data analysis to guide product modifications and innovation by analyzing key product attributes, consumer preferences, and market performance at different lifecycle stages. Smart data fuels innovation by revealing what works and what doesn't, helping identify issues with packaging, merchandising, pack size, or category management that need innovative solutions. During the maturity stage, product modifications can include quality improvements, feature enhancements, and style updates. Data analysis helps retailers dive deep into product attributes and demonstrate value generation for specific products and categories, which can help justify keeping products on shelves and guide development of product extensions and improvements.	['Manufacturers and retailers are inundated with information from an ever-increasing number of disparate sources, but the real challenge that they face is how to ensure their big data becomes smart data.\nOnly if retailers and brands can gather, integrate, analyse and create actionable insight from their point of sale (PoS), customer loyalty and e-commerce data will they win shoppers’ hearts and minds and gain a competitive advantage.\nIn fact, the ability to turn big data into smart data has become a game changer for organisations that have chosen to embrace such a positive strategy. They know that being able to study the minutiae of individual consumer preferences and customer choices can have a real impact on their bottom line.\nA data strategy has to be layered to be really effective. The first level should be the big data, the second tier the smart data and the third layer the insight that can be actioned from it.\nGaining access to an increasing amount of data does not necessarily lead to better decision-making. The key is to know what business questions to ask that your data might be able to answer. You also need the knowledge, and technology, to make sense of what can be mountains of data captured and stored in different parts of the organisation, and often in several different locations. Having sales and marketing data in silos, for example, makes it difficult to create accurate short- or long-term forecasts.\nWhat is clear is that sales data analysis and data science do provide retailers with new insights into areas such as product availability, repeat purchases of promoted and new items, range assortment and shopper missions.\nThere is so much untapped potential from smart data, but organisations need to be brutally honest about whether their data is working hard enough for them. It is likely there is currently not enough investment in combining big data and analytics to strengthen the impact and ROI of their pricing, promotional and media strategies.\nThe reality is that a competitive advantage is being missed because a retailer will not have an accurate picture of what is the right price point for a particular product. They may also be unable to create and deliver the most effective promotional campaigns.\nAsking the right questions\nData provides an understanding of shopper behaviour, so a manufacturer or retailer must ask the right questions to obtain a clear picture of how changes in behaviour might affect their own business now and in the future. For example, what data does it need today and how might this data be used to deliver the ROI it requires? What data has already been collated and is ready to analyse, and what important data has yet to be gathered?\nWith so much data available every organisation needs to know exactly what business issue it is trying to solve.\nFor a retailer, is the aim to improve overall sales and margins only?\nWith the growth in the omnishopper, smart data can reveal which channels particular shopper segments tend to use and when. This becomes increasingly important as consumers’ expectations continue to rise, and the choices provided by multiple channels can help shoppers to change their behaviours.\nMobile has become more important to the whole shopping experience and smart data analysis is helping retailers and brands create a seamless experience for consumers. Indeed, being able to turn big data into smart data can help to find ways to remove the friction points and barriers that make an offline or online retail experience frustrating.\nThis can boost loyalty, which is created not so much by low prices these days, but by other factors such as customer service and transparency.\nMaking smart thinking the norm\nAs mentioned, there are obvious business benefits from mixing data with analytics to help adapt to changing shopper behaviour. This is certainly true when it comes to digital and the amount of data now being collected from high-speed applications and from ads viewed on mobile devices, for instance.\nRetailers and manufacturers need to be able to analyse and assess people’s digital footprint to discover whether the sales achieved by a particular campaign justified the marketing spend. Being able to identify what has been purchased at a point of sale after an advertising campaign is the best option for a brand to define its ROI.\nTechnology has a role to play in helping organisations extract and analyse data more effectively and assist with new product development and range optimisation. For example, retailers are realising the benefits of automatically loading, integrating and augmenting data from their stores to create an up-to-date single view of their customers.\nThe process involves analysing everything from point of sale, CRM and loyalty card data, as well as inventory and consumer panel information. When these data sets are coupled with analytics for price, promotions and assortment analysis, retailers get a better understanding of how they and their suppliers can best target their own shoppers.\nImagine the business opportunities from using trip mission segmentation where the analysis tells you why customers are visiting stores and reveals the clusters of different shopping missions that need to be fulfilled across the entire store portfolio.\nBetter data analysis certainly means more effective targeting when spending money on promotions. Smart data will reveal what is working best in which store in which region or country and with which consumer segment.\nIn some areas households might be more sensitive to promotions than in other places and smart data allows brands and retailers to be sensitive to these differences and react accordingly.\nIf a campaign is going to be cost effective, a manufacturer also needs to be able to create and deliver the most effective promotional and media campaigns. They only achieve this with a better analysis of customers’ individual preferences and buying habits.\nCollaboration between retailer and manufacturer\nThis means manufacturers must work harder to fight their corner and if their own data is not as detailed as that held by the store owners they should consider collaborating with research companies that can help them to plug any of their data gaps. The findings can then be shared with stores to demonstrate a product’s value to a particular category.\nManufacturers can discover what is unique about a particular product from the shopper’s point of view. Every product must have some level of ‘uniqueness’ to convince people to buy it over and above another brand.\nHot sauces is a good example because the consumer is faced with so much choice today. What makes shoppers chose one variety of hot sauce over another? What product characteristics are the most relevant for the different shopper segments?\nA retailer might already stock three Indian flavour sauces, for example, and is unlikely to want to sell another one. However, by using predictive analytics, manufacturers can analyse the key attributes, such as size, packaging, flavour, brand name and price, and combine key competitor analysis to pinpoint what is more likely to sell in different geographies and stores. They can then work out how incremental a product will be before taking it to market.\nChanging technology requires smart approach to data\nTechnology is developing at an incredible speed and smart data has become a powerful weapon to ensure technology is used as effectively as possible to connect with consumers.\nTechnology is not only helping to gather data at the point of sale and through in-store activity such as sampling, it is also assisting retailers in planning their stores of the future.\nSmart data fuels innovation because it reveals to marketers what works and what does not.\nManufacturers can dive deep into the key attributes of their products, and demonstrate through analysis the value generated for the specific product and the category. The value proposition can ultimately help to persuade retailers to keep a product on the shelves.\nSmart data might reveal issues with packaging, merchandising, pack size or problems around category management, price or a promotion that can be solved with an innovative solution. It can also reveal gaps in the market that could be exploited for competitive advantag\nIn today’s world data is knowledge and knowledge is power, and having access to data and being able to analyse it quickly to make commercial gains is absolutely crucial. Unfortunately, many organisations have only scratched the surface of what is possible.\nStephanie Augier, European Lead for Analytics for Manufacturers, IRI\nImage source: Shutterstock/alexskopje', 'It is the life of a product in the market with respect to business/commercial costs and sales measures. To say that a product has a life cycle is to assert four things: Product has a limited life. Product sales pass through distinct stages, each posing different challenges, opportunities, and problems to the seller. Profits rise and fall at different stages of the product life cycle. Products require different marketing, financial, manufacturing, purchasing and human resource strategies in each life-cycle stage. Four main stages:\nSlow sales growth; offering of basic product Limited distribution Negative or low profits Little or no competition Customers have to be prompted to try the product (awareness creation) Intensive personal selling to channel members. Promotional expenditures are at their highest ratio to sales. Prices tend to be high because costs are high. Firms focus on those buyers who are most ready to buy. Speeding up innovation time is essential in an age of shortening PLCs. To be first can be rewarding, but risky and expensive. Concept of pioneer’s advantage. First movers also have to watch out for ‘second mover advantage’.\n• High price RAPID SKIMMING • High promotion • Large market unaware of product • High price and low promotion SLOW SKIMMING • Market aware of product • Competition non intense • Low price intense competitionRAPID PENETRATION • High promotion price sensitivity • Large market unaware customer • Low priceSLOW PENETRATION • Low promotion • Large market, aware customers, price sensitive\nRapid climb in sales; brand building Purchase by early adopters Increase in public awareness; intensive distribution Bringing product extensions, warranty and service New competitors enter, attracted by the opportunities Prices fall slightly or remain as it is, depending on demand increase and increased competition. Promotional expenditure maintained the same or at slightly increased level to meet competition and to educate the market. Decline in the promotion-sales ratio Increase in profits because of economies of scale and learning effect Possibility of a trade-off between high market share and high current profit.\nFocus shifts towards brand buildingBringing product extensions, warranty & servicesAdding new features and improving quality/style/lookEntering new market segmentsIncreasing distribution coveragePrice reduction to attract new buyersIncreased advertisingExample- Hyundai- i10 car\nSales volume peaks and market saturation is reached at some point Costs reduced but prices also tend to drop because of competition This stage normally lasts longer than the previous stages and poses big challenges to marketing management as profits go down. Majority buyers make repeat purchases, laggards join them Can be divided into three phases: growth, stable & decaying maturity Growth phase- sales growth rates start to decline, no new distribution channels to fill, new competitive forces emerge. Stable phase- sales flatten on a per capita basis because of market saturation, future sales governed by population growth & replacement demand. Decaying phase- the absolute level of sales start to decline, customers begin switching to other products, intensified competition.\n• Quality improvements (durability, reliability, etc.) • Feature improvements (utility, safety, convenience, versatility, etc.) Product • Style improvements (aesthetic value)modification • Converting non-users and winning competitor’s customers • Entering new market segments Market • Encouraging usage rate (more frequent use, more usage per time)modification • Changes in price and distribution of product Marketing • Changes in sales promotion and personal selling mix • Changes in services (delivery, maintenance, technology assistance)modification\nNokia Symbian OS CRT monitors2G moblie technology CD Players\nDecline in sales because of technological changes, shift in consumer tastes, and increased competition Laggards and repeat purchases driven sales Costs become counter-optimal Overcapacity, increased price cutting, reduced promotion & profit erosion Most of the product class usually die at this stage Withdrawal from market or reduction in number of products offered It is also possible to extend the life of the product by various means\nAppropriate strategy depends upon the exit barrier, industry’s relative attractiveness, product category, and the company’s competitive strength Different strategies used: HARVESTING DIVESTING LIQUIDATING • Gradually • Selling the • Bringing the reducing a product to product to an product or another firm if end & business it has strong dropping off costs while distribution & the assets trying to residual maintain sales goodwill\nPolo t-shirt Armani cap t-shirt Being Human t-shirt\nSTYLE: a basic & distinctive mode of expression appearing in a field of human endeavor (homes, clothing, and art). A style can last for generations and go in & out of vogue. FASHION: a currently accepted or popular style in a given field. Fashions pass through 4 stages- distinctiveness, emulation, mass fashion, and decline. The length of a fashion cycle is hard to predict. FAD: a fashion that comes quickly into public view, is adopted with great zeal, peaks early, and declines very fast. The acceptance cycle is short Fads tend to attract only a limited following who are in search of excitement and distinctiveness Fail to survive because they don’t normally satisfy a strong need\nMost product life cycle curves are portrayed as bell shaped. However, three common alternate patterns are also noted many-a-times:\nMost product life cycle curves are portrayed as bell shaped. However, three common alternate patterns are also noted many-a-times: Growth-slump-maturity pattern: sales grow rapidly just after introduction and then fall to a petrified level that is sustained by late adopters buying the product for the first time and early adopters replacing it. Often characteristics of small kitchen appliances. Cycle-recycle pattern: often describes the sales of new drugs. Aggressive promotion of new drugs produces the first cycle. Later, sales start declining and another promotion push produces a second cycle (usually of smaller magnitude and duration). Scalloped pattern: sales pass through a succession of life cycles based on the discovery of new-product characteristics, uses, or users. Eg.- sales of nylon: In thread, in parachute, etc.\nWhen a product reaches the maturity stage, following strategies can be adopted to extend the life of the product: Price reductions Repackaging and redesigning (to make them seem new and attract new attention) Launch in new markets Revised promotion (to gain new audience and remind the current ones) Direct selling Adding value (new features to the current product)\nThere are three levels of PLC: Product level (eg. Dell XPS 15 laptop) Category level (eg. Desktop, laptop, netbook, tablet PC) Brand level (HP, Lenovo, Dell, Acer, Apple, Sony) Determines the revenue earned Helps the firm in being proactive Contributes to strategic marketing planning May help the firm to identify when a product needs support, redesign, revitalization, withdrawal, etc. May help in new product development planning or creating a marketing mix for success of a brand/product.\nProduct planning Maintaining a proper balance of product at different stages of PLC Preventing cannibalization Pre-planning product launch Making investment decisions on products Choosing appropriate entry and exit strategy Prolonging the profitable phase (by highlighting new uses, adding new users, etc.) Shortening the product development time Customer management\nPLC patterns are too variable in shape and duration to be generalized Marketers can seldom tell which stage their product is in. For specific products, the duration of each PLC stage is unpredictable. A product may appear to be mature when actually it has reached a plateau prior to another upsurge. The PLC pattern is the self-fulfilling result of marketing strategies and that skillful marketing can in fact lead to continued growth. Use of PLC may lead to inappropriate actions sometimes. Because of these limitations, strict adherence to PLC can lead a company to misleading objectives and strategy prescriptions.']	['<urn:uuid:4550c422-636a-4159-96b7-6e48cc8bfc15>', '<urn:uuid:33ac3244-af06-4e06-a829-fbab97928339>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T19:10:11.825225	21	99	2636
54	How do yogurt cooking and yogurt health benefits compare?	When cooking yogurt in a multi-cooker, the temperature must be kept at 40 degrees Celsius for at least 8 hours, as higher temperatures would kill beneficial bacteria. In terms of health benefits, yogurt contains protein, B vitamins, calcium, and beneficial probiotics (primarily Lactobacillus bulgaricus and Streptococcus thermophilus). However, these health benefits are only present when the yogurt contains live active cultures and isn't heat-treated after culturing, which would kill the beneficial bacteria.	['Of course, a set of preset automatic cooking programs significantly affects the cost, functionality and choice of the optimal multi-cooker. Typically, all programs work based data from sensors in the bottom and on the lid. Modern models differs by the number of basic programs and availability of custom programs.\nBut, cooking time in these modes usually exceeds this value for conventional programs. As a rule, they are used to cooking the pilaf, porridges etc.\nThey have the following features:\n– the possibility / impossibility of intervention in the work of automatic programs;\n– presence / absence of a manual mode (multi chef mode);\n– possibility / impossibility of changing the temperature and time during the cooking process;\n– step variations of temperature and time in different modes;\n– countdown time.\nCountdown starts at the moment of switching-on or after reaching a predetermined temperature. Of course, this feature of the model should be considered.\nUsually, modern models support a few dozens of popular cooking programs.\nGroats (rice/buckwheat) and porridge (milk/crumbly/pilaf)\nGroats mode provides cooking crumbly dishes from different cereals. Therefore, cooking in this mode continues until full evaporating the liquid. Typically, device uses the maximum power. Usually, cooking time for any groats does not exceed 35 minutes.\nPorridge (milk/crumbly/pilaf) mode also continues until full evaporating the water.\nUnfortunately, not all models support milk porridge program. It operates at a temperature of about 90 degrees. But cooking time lasts about 1 hour.\nThe flakes are divorced by milk and are heated in the appropriate mode.\nPilaf mode in the ending the cooking sharply increases temperature, drys and roasts the cooked rice.\nStewing, steam cooking, baking, cooking soup/broth/compote and frying\nUsually, universal stewing mode provides cooking soups, meat, fish and vegetables. As a rule, it has long cooking time at a relatively low temperature.\nAs known, steam cooking provides the cooking without fat with maximum preservation of nutrients. Typically, models with this mode have the grilles and container for water.\nOf course, baking provides cooking at high temperature with the lid closed.\nCooking soup/broth/compote uses predetermined temperature.\nFrying mode uses the set temperature. This value is kept constant during cooking and reaches of 180 degrees.\nAuto warming up, keeping temperature, Multi chef mode\nAuto warming up program provides defrost and warming up food. Usually, device uses a temperature from 30 to 60 degrees. As a rule, power consumption does not exceed 20 W. But some models envisage switching off this mode that is convenient in case when recipe envisages a natural cooling.\nMoreover, this mode can used for cooking yogurt. But this option is only available in models that provide the temperature of 40 degrees during at least 8 hours. Unfortunately, a higher temperature kills the strain of useful bacteria.\nTypically, program of keeping temperature operates at a minimum temperature. In fact, device merely compensates the heat leakage.\nMulti chef mode changes the values of time and temperature during cooking process. But some models support the formation of programs from the fragments with different values of temperature and duration. As a rule, this function provides programming the complex dishes.\nCooking yoghurt, pasteurisation and deep fryer\nCooking yoghurt keeps a constant temperature (not more than 40 ° C) for a long time (up to 8 hours or more).\nPasteurization increases the shelf life of perishable foods. Pasteurization use a temperature of 60-80 ° C. Duration of this program depends on the foods weight.\nDeep fryer mode operates with the lid open and with the maximum temperature.\nThis program can also be used for cooking fondue.\nThis video demonstrates the wide possibilities of the cooking programs in modern multi-cookers.', 'Yogurt: What’s Healthy & What’s Not\nYogurt often tops the list as a natural food that packs a lot of nutritional value. When produced in its simplest form, which is fermenting good bacteria with milk, yogurt is a great source of protein, B vitamins, calcium, and probiotics (mostly Lactobacillus bulgaricus and Streptococcus thermophilus, which are both required to make a true yogurt product). The catch: yogurt’s really only good for you when the ingredients are natural, pure, and kept simple. Unfortunately, many of the conventional yogurts fall into the unhealthy category because they contain unnecessary ingredients.\nWhy Is Conventional Yogurt So Unhealthy?\nMany yogurts contain high amounts of added sugar, especially the flavored varieties with granola clusters and the ones with fruit on the bottom. A 6-ounce serving of regular flavored yogurt may contain as much as 35 grams of sugar. That’s more than the recommended daily intake of sugar, which should be less than 30 grams of sugar a day. Consider, too, the drinks and processed foods that are often consumed: juice, soda, dessert, and other foods that are deceptively “healthy” because they too contain a lot of added sugar. At the end of the day, most Americans consume on average 17 teaspoons of sugar or roughly 71 grams of sugar!\nIt’s important to note that lactose, the naturally occurring milk sugar in yogurt, makes up part of the total sugar content (total sugar includes both natural and added sugar). Most 8-ounce regular yogurts have about 12–15 grams of lactose (natural sugar). An 8-ounce serving of Greek yogurt has less natural sugar (roughly 9 grams) because liquid is strained during production, resulting in some sugar removal, thicker yogurt, and more protein. Skyr, an Icelandic yogurt, is produced with a similar straining process. It’s made with more milk than regular yogurt, has a high protein content, and is low in natural sugar (6 grams per 8-ounce serving).\nOn top of the high-fructose syrups that are often added, many conventional yogurts also contain artificial colors and sweeteners, preservatives, thickening agents, and milk protein concentrate to increase protein. Some yogurts may be heat-treated after culturing to extend their shelf life, but this process kills off any bacteria and diminishes the point of eating a healthful food filled with live and active cultures. Other yogurts may not even use real fruit at all.\nWhat’s In Healthy Yogurt?\nThe best yogurt for you is the one that has the least amount of ingredients but also the most natural ingredients. Ideally, the healthiest yogurts contain whole milk and live active cultures (L. bulgaricus and S. thermophilus) and do not list sugar in the top ingredients. Look for yogurt made with USDA-certified organic whole milk that comes from 100% grass-fed or pasture-raised cows. Remember, whole milk has more of the healthy beneficial saturated fat that we need versus skim and low-fat milk. (See our WFP YouTube experience at Stryk Jersey Farm to learn more about the health benefits of grass-fed Raw Milk.) Many healthy yogurts also provide the Live and Active Cultures seal to indicate the yogurt contains a high amount of yogurt cultures (e.g., 100 million cultures per gram at the time of production); however, there are also healthy organic options that contain very high amounts of live and active cultures but do not provide the seal. (The seal isn’t mandatory and is provided voluntarily by a company.)\nWFP’s Take-Home Advice\n- Use the Cornucopia Institute’s yogurt scorecard as a guide when buying healthy yogurts: https://www.cornucopia.org/yogurt-scorecard/. The scorecard includes comprehensive information about yogurt ingredients and rates yogurts so that you know which ones are the healthiest vs. unhealthiest. Many of their top yogurts are found at both natural and conventional grocery stores. WFP recommends:\n- Maple Hill Creamery Plain Greek 100% Grass-Fed Organic Whole Milk Yogurt (Whole Foods and other natural health food stores)\n- Stonyfield Plain 100% Grass-Fed Organic Whole Milk Yogurt (Whole Foods and other natural health food stores)\n- Organic Valley Plain Grassmilk 100% Grass-Fed, No Grain Whole Milk Yogurt (Whole Foods and other natural health food stores)\n- Straus Family Creamery Non-GMO Certified Plain Greek Organic Whole Milk Yogurt (Sprouts and other natural health food stores)\n- Stonyfield Organic Plain Greek Pasture-Raised Whole Milk Yogurt (HEB and other natural health food stores)\n- White Mountain Organic Whole Milk Bulgarian Yogurt (HEB, Randalls, Sprouts, Whole Foods)\n- Green Valley Organics Certified-Humane Plain *Lactose-Free Yogurt (HEB and other natural health food stores) *For lactose-intolerant consumers\n- Choose yogurts made with organic whole milk and that say, “100% grass-fed or pasture-raised” so that you know the dairy cows are living their lives on grass pastures, not in factory-farm conditions. Grass-fed cows typically eat native grasses and are not fed GMO grains.\n- We also recommend choosing plain, unflavored yogurt rather than those that are flavored or have added fruit. Plain yogurts typically do not have added sugar. Add your own toppings of fresh fruit and natural added sugar, such as ½–1 teaspoon of honey or maple syrup. Plain yogurt is also a great substitute for sour cream.\n- Avoid store-bought yogurt parfaits; they pack a lot of additional ingredients on top of flavored yogurt, which ultimately drives up the added sugar content.\n- Companies must disclose whether they heat-treat their yogurt after the probiotics have been added. Look for labels that state, “heat-treated after culturing,” and avoid these products as the process kills off the beneficial live cultures. Choose the ones that say, “contains active yogurt cultures” or “live active cultures/acidophilus.” Probiotics are good for your digestive system and also help maintain a healthy immune system. Yogurt is a great way to incorporate beneficial bacteria into your diet as is taking a daily probiotic. Our WFP Probiotic Complete is a comprehensive multi-strain formula that includes Saccharomyces boulardii (SB), Lactobacillus, and Bifidobacterium.\nFollow these guidelines and remember to always read the nutrition labels carefully to help you choose the healthiest yogurt for you and your family!\nEditor’s Note: This article was originally published on May 8, 2017 and has since been updated.\nSources and References:\nCornucopia Institute: Cornucopia Yogurt Buyer’s Guide. Retrieved May 4, 2017 from https://www.cornucopia.org/yogurt-scorecard/\nNational Yogurt Association: Live & Active Culture Yogurt. Retrieved May 9, 2017 from http://www.aboutyogurt.com/live-culture\nHow to Buy the Healthiest Yogurt. huffingtonpost.com. Retrieved May 4, 2017 from http://www.huffingtonpost.com/2013/12/01/healthy-healthiest-yogurt_n_4302415.html\nJones, T. How to Choose the Best Yogurt for Your Health. authoritynutrition.com. Retrieved May 9, 2017 from https://authoritynutrition.com/best-yogurt-for-health/\nSugarScience: The Unsweetened Truth. Retrieved May 9, 2017 from http://www.sugarscience.org']	['<urn:uuid:46541fc3-d3af-4cb5-94c6-d9e82cf4da28>', '<urn:uuid:ef7051e7-f56c-429b-a63b-24d7b4523872>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	9	72	1681
55	I heard some people saying they want to organize a protest at a group home. If I help them plan it, could I get in trouble?	Yes, you could get in trouble. The law specifically prohibits counselling, procuring, supporting or encouraging anyone to picket a supported group living residence. Additionally, any action or omission that is likely to result in another person picketing such a residence is also prohibited. If you violate these provisions, you could be held liable for any resulting damages and face criminal charges with fines of up to $2,000 for individuals.	['Bill 23 2011\nAn Act to prevent picketing of supported group living residences\nHer Majesty, by and with the advice and consent of the Legislative Assembly of the Province of Ontario, enacts as follows:\n1. The purpose of this Act is to achieve a fair balance between,\n(a) the right of every individual residing in a supported group living residence to the peaceful enjoyment of his or her home, free from harm and threats of harm; and\n(b) the right of every individual to freedom of communication and expression in order to provide information about a labour dispute.\n2. (1) In this Act,\n“person” includes a trade union; (“personne”)\n“supported group living residence” has the same meaning as in subsection 4 (2) of the Services and Supports to Promote the Social Inclusion of Persons with Developmental Disabilities Act, 2008; (“résidence de groupe avec services de soutien”)\n“trade union” means an organization of employees formed for purposes that include the regulation of relations between employees and employers. (“syndicat”)\nStatus of trade union\n(2) For the purposes of this Act and despite any other Act, a trade union is an entity that can be sued and prosecuted in its own name.\nProhibition re picketing\n3. Despite any other Act, a person shall not, in connection with a labour dispute, engage in picketing of a supported group living residence.\nProhibition re counselling, etc., a contravention\n4. (1) No person shall counsel, procure, support or encourage conduct that contravenes section 3.\nProhibition re causing a contravention\n(2) No person shall do any act or omit to do any act if the action or omission is likely to result in another person engaging in conduct that contravenes section 3.\n5. (1) Every person who contravenes section 3 or 4 is liable for any damages resulting from the contravention.\n(2) If a trade union is found to be liable for damages for a contravention of section 3 or 4, all property held by or for the benefit of the trade union or its members shall be deemed to be property of the trade union and subject to execution, seizure or attachment for the damages.\n(3) Subsection (2) does not apply to property held in respect of pension benefits, superannuation benefits, sickness or health insurance, supplementary unemployment benefits, vacation pay, holiday pay, severance pay, training or apprenticeship, life insurance, legal aid and such other benefits as may be prescribed by the Lieutenant Governor in Council.\n(4) The Lieutenant Governor in Council may make regulations prescribing benefits for the purposes of subsection (3).\n6. (1) Every person who contravenes section 3 or 4 is guilty of an offence and on conviction is liable,\n(a) in the case of a person who is an individual, to a fine of not more than $2,000;\n(b) in the case of a person who is not an individual, to a fine of not more than $25,000.\n(2) Each day that a person contravenes section 3 or 4 constitutes a separate offence.\n7. This Act comes into force on the day it receives Royal Assent.\n8. The short title of this Act is the Protecting Vulnerable People Against Picketing Act, 2011.\nThe Bill governs the picketing of supported group living residences in connection with labour disputes.\nSection 3 of the Bill prohibits the picketing of supported group living residences where persons with developmental disabilities reside.\nSubsection 4 (1) of the Bill prohibits counselling, procuring, supporting or encouraging a person to engage in conduct that contravenes section 3. Subsection 4 (2) of the Bill provides that actions and omissions that are likely to result in another person contravening section 3 are prohibited.\nSection 5 of the Bill provides that a person who contravenes section 3 or 4 is civilly liable for damages resulting from the contravention. Section 6 of the Bill provides that it is an offence to contravene section 3 or 4.']	['<urn:uuid:63813b82-b4fd-46e2-9616-2e46ae554d12>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	26	69	649
56	What is Hiyashi Chuka and what food additives should I watch out for?	Hiyashi Chuka is a Japanese cold ramen dish made with chilled ramen noodles topped with colorful ingredients like egg crepes, cucumber, ham, and imitation crab, served with a soy sauce or sesame-based dressing. Regarding food additives to watch out for, you should be particularly cautious about MSG (which can cause high blood pressure and diabetes with long-term consumption), trans fats (which increase heart risk), and artificial sweeteners like aspartame (which can cause migraines in some people). You can check food labels and use mobile apps like Eat Informed to identify harmful additives.	"['Cool down and relax with this Japanese Cold Ramen dish called Hiyashi Chuka. It’s a bright, flavorful, fun way to experience the magic of ramen on a hot summer day!\nAs the temperature and humidity soar in Japan in the late summer, I just want to eat chilled noodle dishes like cold soba or cold udon. But when it gets extra hot, like it did this year when the rainy season ended two weeks early, all I can think about is the flavorful, rainbow color of toppings on a bowl of cold ramen called Hiyashi Chuka (冷やし中華). Today, I’m sharing a Hiyashi Chuka recipe with my favorite soy-sesame homemade noodle dressing.\nWhat is Hiyashi Chuka?\nHiyashi Chuka literally means “chilled Chinese”; however, it is a Japanese dish with chilled ramen noodles and various colorful toppings, including strips of egg crepes, cucumber, ham, and imitation crab. Be creative and add your favorite toppings to this Hiyashi Chuka. If you are vegetarian, omit ham, shrimp, and imitation crab, and add your favorite veggies instead.\nFinally, a soy sauce or sesame-based dressing is poured over the dish. Although the store-bought Hiyashi Chuka package comes with dressing, it has lots of MSG and preservatives. For a healthier option, you can easily make the dressing at home. The dressing recipe I specified below is more than enough for 3 servings of cold ramen, in case you want to add more toppings than what I have included.\nOn hot days, a cold noodle dish like Hiyashi Chuka is a delicious nutritious meal to cool your body down, while filling up your tummy. Now go ahead, pick up some ramen and your favorite toppings, and create this cool Japanese dish!\nHiyashi Chuka (Cold Ramen)\nFor Hiyashi Chuka Sauce\nFor Shredded Egg Crepe\n- 2 large eggs (50 g each w/o shell)\n- 2 tsp sugar\n- ¼ tsp kosher/sea salt (I use Diamond Crystal; use half for table salt)\n- 1 Tbsp neutral-flavored oil (vegetable, rice bran, canola, etc)\n- 6 shrimps\n- 1 Tbsp sake (for cooking shrimp)\n- 1 Persian/Japanese cucumbers (or ⅓ English cucumber, julienned)\n- 1 iceberg lettuce (cut into thin strips)\n- ½ tomato (cut into wedges)\n- 3-4 slices hams (cut into thin strips)\n- 4-6 imitation crab meat (kanikama) (or crab meat, shredded)\n- kaiware daikon radish sprouts (rinse and pat dry)\nFor Hiyashi Chuka\n- 3 servings fresh ramen noodles (6 oz or 170 g fresh noodles per person)\n- 1 Tbsp toasted white sesame seeds (optional garnish)\n- Japanese karashi hot mustard (optional garnish)\n- pickled red ginger (beni shoga or kizami beni shoga) (optional garnish)\n- Gather all the ingredients.\nTo Make Sauce\n- Combine all the noodle sauce ingredients in a medium bowl and whisk all together. You can keep it chilled in the refrigerator.\nTo Prepare Toppings\n- For eggs, you make a thin egg crepe and cut it into thin strips (it\'s called ""Kinshi Tamago""). If you want to make super-thin crepe, follow my recipe here. Whisk together the eggs, sugar, and salt. Heat the oil in the pan over medium heat. Pour the egg mixture into the pan and cook on both sides.\n- Cool the crepe and slice into very thin strips.\n- For shrimps, bring a small pot of water to a boil. Add sake and shrimp and cover with the lid. The alcohol in the sake will help remove the smell and tender the meat. Turn off the heat when the color of the shrimp started to change and let it cook with the remaining heat. Do not overcook otherwise shrimp will become hard. Transfer shrimps to a plate and let cool.\n- Cut all the topping ingredients to thin strips (so it\'s easier to eat with noodles).\nTo Cook Noodles\n- Bring a big pot of water to a boil and add the noodles, separate the noodles before dropping them into water. Cook according to package directions. Drain the water and rinse the noodles to remove starch. Soak the noodles into a bowl of ice water to cool. Drain completely and divide the noodles into individual plates/bowls.\n- Place all the toppings and pour the dressing before serving. Serve with karashi hot mustard and pickled ginger on the side, if desired.\n- You can keep the leftovers in an airtight container and store in the refrigerator for 2 days. Keep the sauce separated.', 'The issue of food additives is fresh in our minds due to the recent Maggi Noodles controversy. Nestle’s Maggi came to the media spotlight when government lab tests revealed that the noodles contained MSG contrary to its claim of “No added MSG”. This issue was further aggravated when the central FSSAI lab at Kolkata found the noodles were contaminated with lead. While Maggi Noodles has finally emerged from this controversy and resumed sales, the issue of food additives has remained firmly in our minds.\nSince centuries food additives have been a part of our menus to enhance flavor and appearance. Any chemical substance that is added to food during preparation or storage which affects the quality of the food item is considered to be a food additive. There are many ways through which food additives enter our palates; they are generally added during processing, packaging or storage of food products.\nCommon food additives\nCommon examples of food additives are coloring agents that make food attractive, anti-caking materials that prevent clumping of powders like salt, flour, food preservatives that prevent and postpone spoilage and some sweeteners that heighten sweetness of items without increasing the caloric value. Do food additives really ‘add’ value to the food? And do they cause side effects?\nHere is a list of 10 most-used food additives along with their side effects:\n|Aspartame||Low-calorie sweetener||Beverages, chewing gum, puddings, yogurt, sugar-free products, etc.||Some people are allergic to aspartame who could suffer migraine headaches|\n|Saccharin||Sweetener||Fruit juice, jellies, carbonated beverages, canned fruits, preservatives and as sugar replacement||Demonstrated to cause cancer in lab animals|\n|High Fructose Corn Syrup||Sweetener||Salad dressings, bread, cereals, candy and flavored yogurt.||Contributes towards weight gain, type-2 diabetes and heart issues.|\n|Monosodium Glutamate (MSG)||Flavor-enhancer||Dressings, frozen food, canned vegetables||Considered harmful to children; Long-term consumption can cause diabetes, high blood pressure, adrenal gland dysfunction, strokes|\n|Trans Fats||To extend shelf life of food products||Chips, baked food, margarine and fast food in general||Increase in bad cholesterol level, increase in heart risk and stroke, contributes to diabetes|\n|Sodium Sulfite||Prevents discoloration of fruits||Dried apples, wine, dehydrated potatoes||Some people are allergic to sulfites, who could suffer mild headaches or in rare cases, severe reactions like anaphylactic shock|\n|Sodium Nitrate/Sodium Nitrite||Preservative and color fixative||Smoked fish, meat preparations||Could mix with stomach chemicals to produce nitrosamine, a carcinogenic material|\n|Butylated hydroxyanisole (BHA) and butylated hydroxytoluene (BHT)||Preservative and prevents discoloration; BHA used for defoaming* in yeast||Foods with high fat and oil content like butter; preserved meats, baked items, chewing gum, cereals, beer and snacks||Known to cause liver, kidney damage and thyroid issues among lab animals|\n|Brominated vegetable oil (BVO)||Used to emulsify citrus-flavored soft drinks||Citrus-flavored soft drinks||Excess use of soft-drinks can cause memory loss, tremors, fatigue, loss of muscle coordination|\n|Potassium Bromate||To strengthen dough and to add appealing white color to bread||Flour||Found to increase risk of tumors among lab animals; could lead to genetic defects in long-run|\nThe MSG issue in Noodles\nIn accordance with the Food Safety and Standard Rules 2011, MSG is considered a flavor-enhancer and should not be added to food products given to infants below 12 months. It is also not allowed in about 50 different items including noodles and pastas given to children since it can affect their nervous system. However, MSG is allowed in the seasoning prepared for them, which is added as it arouses the nervous system, making the food appear attractive.\nDealing with Food Additives\nWe cannot completely scratch out packaged foods from our dining tables, so food additives are bound to be part of our lives. But we can take some measures to reduce their harmful effects on our body.\n- Reading the labels: The best way to avoid consuming harmful food additives is by reading the labels of ingredient lists of the products. The Nutrition Facts column on a product is followed by a list of ingredients in fine print, which mentions the ingredients according to their contribution in the food in a descending order.\n- When a chemical preservative is used to produce a food, the ingredient list must include both the common and scientific name of the preservative along with the function that it serves. For instance: “preservative to promote color retention”. When artificial colors are used in a product, then the name is mentioned depending on whether it is certified or not. For instance, when a certified color is used, then the abbreviated name can be mentioned- like ‘Red 40’; when a non-certified color is used, it is listed as “artificial coloring” or with specific names like “colored with beet juice”.\n- Preventive measure: Foods that contain tartrazine (food colorant), diacetyl (food flavor), nitrites or nitrates, propyl paraben and olestra should be avoided. You can also use mobile phone apps that tell you the level of harm caused by a food additive along with tips to buy healthy food items. Some phone apps under the Health and Fitness category which inform you about food additives are: Eat Informed, Food Additives 2 and E-Codes Free.\n- Organic food diet: Food additives can be removed to the highest level possible by following an organic food diet. Organic products only contain those additives that are required by law such as Sodium Benzoate or Sulfur Dioxide. Rules governing the production and storage of organic food prevent the use of any harmful food additives like artificial sweeteners, flavorings and colorings. One of the most significant non-organic additive allowed in the organic food is sulfur dioxide, which acts as a preservative, bleaching agent and antioxidant.\n- Natural food additive: Research is underway to find natural alternatives to the artificially prepared additives. Natural substances containing antimicrobial properties can be used as food preservatives, which can be categorized as follows:\n• Herbs: Cumin, fennel, peppers, clove, basil, cinnamon, cardamom, oregano, thyme, coriander, mind, caraway, nutmeg and onion;\n• Microorganisms: Colicins, nisin, reuterin, lactacin, bulgaricin, plantaricin and others.\n• Antimicrobials from animals: Lysozyme, chitosan, pleurocigin, defensins and lactoferrin.']"	['<urn:uuid:46754216-aa4a-4d7c-aa3a-349280e25228>', '<urn:uuid:1c7c3ebc-512c-492b-ba0e-431b634b72da>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	13	92	1713
57	tax opinion letter full disclosure principle similarities preventing fraud requirements	Both tax opinion letters and the full disclosure principle aim to ensure transparency and prevent fraud - tax opinion letters must present information clearly, concisely and completely with no ambiguity, while the full disclosure principle requires companies to include all relevant and necessary information for understanding financial statements in public company filings.	"['A Tax Opinion Letter – Complex Transactions, Special Requirements\nFor complex investment transactions, particularly those with possible tax implications, you should be discussing the matter with your tax advisor before you make any final decisions. The purpose of a tax opinion letter is to justify your tax position, should it be called into question by the IRS. A tax opinion letter can help in reducing the likelihood of an audit and provide a strong rationale for a tax position or a strategy to minimize your tax exposure by referring to specific IRS codes and rulings. Since business transactions and deductions can be structured in different of ways with different tax consequences, contacting an experienced and independent tax professional early on can be of great benefit. You can get advice, input, and a tax opinion letter in advance of your final transaction or investment to assure that any favorable tax impact is upheld.\nIn an effort to combat abusive tax shelters, The Internal Revenue Service and the U.S. Treasury department have reduced the extensive use of tax opinion letters by implementing new regulations, referred to now in Circular 230. Tax opinion letters are deemed so valuable and necessary for legitimate transactions that could lead to significant tax consequences. It is best to have the letter prepared by an independent and objective party, not closely involved in the business transaction, nor standing to profit from it. For investors, individual and business taxpayers considering a large transaction such as an investment, or a specific tax deduction, an opinion letter is furnished to prove legitimacy of the transaction, deal or deduction by referring to specific IRS codes and rulings. Before you take an aggressive position, it is wise to work with an unbiased and independent tax professional to review the transaction or investment, alongside your own circumstances, to weigh the potential liabilities.\nCircular 230 states that any written advice with the purpose of minimizing tax, or taking an aggressive tax position by means of a deduction or an investment plan, must meet specific criteria. If it doesn’t meet these, it offers no real advantage to the taxpayer in the event of an audit.\nThe regulations serve as a framework for tax advisors when they issue a “Covered Opinion.” Practitioners who provide a covered opinion must:\n- Use reasonable efforts to identify, ascertain, and consider all relevant facts;\n- Base the opinion only on reasonable factual assumptions;\n- Rely only on reasonable “factual representations, statements or findings of the taxpayer or any other person;”\n- Relate the applicable law to the relevant facts;\n- Consider all significant Federal tax issues (unless the opinion is expressly limited in scope);\n- Provide a “conclusion as to the likelihood that the taxpayer will prevail on the merits with respect to each significant Federal tax issue considered in the opinion;” and\n- Provide an “overall conclusion as to the likelihood that the Federal tax treatment of the transaction . . . is the proper treatment and the reasons for that conclusion.”\nA recent Tax Court decision, Canal Corp. v. Commissioner , 135 T.C. No. 9 (2010), affirmed the importance for taxpayers to hire independent tax professionals when seeking a tax opinion. The court held that a tax opinion can be disregarded if the tax advisor writing the opinion being too closely involved with the structuring of the transaction – resulting in a conflict of interest.\nA Tax Opinion Letter, Points to Consider:\n- The letter should present the information in a straight-forward manner, clearly, concisely and completely. There is no room for ambiguity and the letter must address each of the elements of the transaction concerned.\n- An effective tax opinion letter will discuss factual details as well as technical concerns. It should include a thoughtful tax analysis. Citing examples to uphold the position from authorities and Tax Court can strengthen the letter as well. The best letters discuss the reasons in support of and against the position, to illustrate the reasoning behind the position. All supporting documentation should accompany the opinion letter to support it.\n- Generally, the letter needs to meet the threshold of “more-likely-than-not,” to effectively protect a taxpayer from any future penalty liabilities.\n- The best protection an opinion letter offers is when you get one in advance of making your decision and ultimately taking a tax deduction or investing. However, that may not always be possible. You can, and should, contact a tax professional to aide you in having your tax position upheld through the use of an opinion letter as soon as it becomes obvious that one is needed.\nYou, as a taxpayer, want to have your tax position upheld. So, for any complicated transactions or investments that may come into question at tax time, consulting an independent tax professional can prove to be incredibly valuable. A tax opinion—especially one prepared while you are still in the midst of the purchase or making the transaction—can help put you in the best possible light on both the facts and the tax laws. If you find you need a tax opinion letter after you’ve already made your transaction – we can still help you have your position upheld.\nFor assistance, or an independent unbiased tax opinion, we welcome you to contact our tax firm for a free tax consultation at 1-877-78-TAXES [1-877-788-2937].\nMike Habib, EA is a licensed boutique tax controversy firm that helps individual and business taxpayers in various tax matters. We are an accredited member of the Better Business Bureau and maintain the highest BBB rating of ""A +"".', 'Accounting principles are the basic rules that a company follows when reporting financial information. These principles have been developed through common usage and as the basis for the complete suite of accounting standards.\n1. Revenue Recognition Principle\nThe revenue recognition principle gives a company a standard to follow for knowing when revenue is recorded and recognized as an item in the financial statements. Since revenue recognition is one of the areas that is often manipulated and biased, it is very important for a company to know when to record that information. Recording of revenue must meet these criteria:\n- There is a transfer of the risks and rewards of ownership.\n- The seller loses continuing managerial involvement or control of the goods sold.\n- The amount of revenue can be reasonably measured.\n- Collection of payment is reasonably assured.\n- The costs incurred can be reasonably measured.\nThe revenue recognition principle, a key feature of accrual-basis accounting, determines that companies acknowledge revenue as it is earned (not when they receive payment). Accurate revenue recognition directly affects the integrity and consistency of a company’s financial reporting, so it is essential to businesses.\n2. Matching Principle\nThe matching principle dictates that companies report expenses at the same time as the revenues the expenses are related to and that revenues and expenses are to match on the income statement.\n3. Historical Cost Principle\nWith the historical cost principle, the value of an asset on the balance sheet should reflect the initial purchase price as opposed to the market value.\n4. Full Disclosure Principle\nUnder the full disclosure principle, a company must include all relevant and necessary information for the understanding of a company’s financial statements in the public company filings.\n5. Cost Benefit Principle\nThe cost-benefit principle states that the benefits of an accounting system that help produce financial reports and statements should always outweigh its associated costs.\n6. Consistency Principle\nThe consistency principle helps to prevent manipulation in accounts by stating that all accounting treatments should be followed consistently throughout the current and future period unless required by law to change.\n7. Conservatism Principle\nThe conservatism principle in accounting is concerned with the reliability of the financial statements of an entity. It says that when there is an uncertain event, accountants should err on the side of caution and moderation.\n8. Objectivity Principle\nThe objectivity principle states that the financial statements of an organization should be based on solid evidence. This helps keep accounting departments of a company from producing financial statements that are planted by their opinions and biases.\n9. Economic Entity Principle\nThe Economic entity principle states that a business owner must maintain separate accounting records and bank accounts for each entity, and not intermix with the assets and liabilities of its owners and business partners.\n10. Accrual Principle\nThe accrual principle states that a company should record accounting transactions in the period in which they actually occur, rather than the period in which the cash flows related to them occur. It is a fundamental requirement of all accounting frameworks.\nWhat Is Accounting Principles?\nAccounting principles are common rules or guidelines for accounting financial transactions and for preparing financial statements. They are the foundational guidelines for recording and preparing financial statements. Accounting principles allow accountants to bring uniformity in preparing financial statements globally. These accounting principles are established in the field of accounting and are governed by a series of rules as defined by the Financial Accounting Standards Board (FASB).\nThese basic accounting principles are often referred to as GAAP which stands for the generally accepted accounting principles.\nOn the whole, GAAP consists of three parts:\n- The basic accounting principles and guidelines\n- The rules and standards issued by FASB\n- The generally accepted industry practices\nThese accounting principles help to guarantee consistency in accounting reports and financial statements among all businesses. They help to protect business owners, consumers, and investors from fraud.\nWhat is the Importance of Basic Accounting Principles?\nThe basic accounting principles hold strict standards that companies and businesses must follow for financial reporting. These standards serve as a function to make sure that companies and organizations can’t mislead information in their financial reporting.\nWhy Is It Important to Understand Accounting Principles?\nIt is very important for a company or business owner to know and understand accounting principles to help keep their financial records accurate and organized. Accounting principles are an important part of keeping a business running properly.\nWhat Are Accounting Principles Used For?\nAccounting principles are the rules that businesses and companies follow to report their financial information. They can be used as references for stakeholders and other managing entities.\nWhen are Basic Accounting Principles Implemented?\nEach basic accounting principle has standards and rules that a company or business must follow when preparing its financial statements. These principles also state when a company should implement such rules and it is important to understand the nuances of each principle before setting up any accounting system.\nWho Establishes Basic Accounting Principles?\nAs mentioned before, the basic accounting principles were established by the Financial Accounting Standards Board (FASB) to help establish foundational guidelines for recording and preparing financial statements.\nHow Do you Apply Accounting Principles in Your Daily Work?\nMany of the basic accounting principles can be applied in a person’s daily work. When we check our finances, manage our bills, or even do something as simple as buying groceries, we are using accounting principles. That is why it is essential to learn more about basic accounting principles even if you are not a business owner or an accountant.\nIs the Basic Accounting Principle Beneficial?\nYes, as mentioned before, basic accounting principles set standards for businesses and companies to follow in order to have clean and reliable financial accounting statements. Basic accounting principles can even be applied in daily work when dealing with any type of financial transaction or management.']"	['<urn:uuid:81e00d1a-a313-4f61-a871-98c8c36f4fad>', '<urn:uuid:25565e58-522b-494b-a941-507d5c92abfd>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	10	52	1909
58	Could you explain the scale and scope of the landscaping work that went into developing the Capital City Recreation Park along Edmonton's river valley?	The Capital City Recreation Park was a massive undertaking that involved 1,200 acres of landscape development and 116 acres of erosion control. The project included planting 7,950 trees, 31,600 shrubs, and 200 million wildflowers (which were planted by helicopter). The infrastructure development included four pedestrian bridges, two overpasses, four wood bridges, and three kilometers of berms. The construction required 75,000 cubic yards of gravel and 27,000 cubic yards of heavy rock.	"['While researching the history of the river valley park system, I had the luck of sitting down with Roman Fodchuk, the original Landscape Architect of the Capital City Recreation Park. Now retired and living in British Columbia, Roman recalled the challenges of creating a park out of unstable riverbanks and former industrial sites.\nFor people unaware of the CCRP, the Capital City Recreation Park created parkland on both sides of the North Saskatchewan River from the High Level Bridge to the Strathcona Science Park in the mid 1970’s. Many of the structures and bridges people use every day were created at this time.\nThe logistics were massive. Over 1,200 acres of landscape development, 116 acres of erosion control, 7,950 trees planted, 31,600 shrubs, 200 million wildflowers planted by helicopter, four pedestrian bridges, two overpasses, four wood bridges, three kilometers of berms, 75,000 cubic yards of gravel, 27,000 cubic yards of heavy rock and so much more.\nIn Roman’s words (I love the Calgary reference):\nIn 1975, Premier Lougheed announced an Urban Parks Program in Alberta for the two major cities, Calgary and Edmonton. This program recognized the increased urbanization and the new requirements for healthy city living. This foresight was cognizant of the pressures of the new urban dweller upon elements of the natural environment and the need for comprehensive amenities that would augment outdoor physical activities. This, of course, also includes the natural beauty of a wide river valley that establishes a unique meandering corridor and outstanding landmark in Edmonton. Calgary is not so fortunate.\nThe Urban Parks Program was funded by the Alberta Heritage Savings Trust Fund as an investment in Alberta\'s natural heritage. The goal of the Urban Parks Program was the: ""establishment of natural environments and the development of these areas to enable their sustained and unimpaired use for outdoor recreation."" Specific objectives included:\n* To provide for a variety of outdoor recreation opportunities\n* To allow people of all incomes to participate in these opportunities\n* To have easy access to surrounding urban areas\n* To preserve and augment the natural landscape features and provide recreational facilities in harmony with the beauty of the North Saskatchewan River Valley.\nThe Capital City Recreation Park was developed by the Alberta Government within a partnership agreement with the City of Edmonton. Both the Alberta Government and the City were involved in a Client capacity throughout the planning and design process. There were various Committees established with appropriate responsibilities. At the top were the Executive Committee, with various Ministers and the Mayor\'s Office; to the Planning Committees and on to the Community Meetings Committees to the Project Management and Design Group. Our responsibility as consultants was to develop a working procedure that was flawless in its systematic operation, from top to bottom.\nRoman Fodchuk and associates were the Prime Consultants for the Planning, Design and Development of the entire park system. This included the approaches to the High Level Bridge to the far eastern end at Highway #16 in Strathcona County. This was a19 kilometer stretch of the North Saskatchewan River Valley Parks System within the City of Edmonton. It was developed for intensive recreational use consisting of a series of public amenity nodes interconnected with a bikeway and walkway system.\nThe next stage was the Design and Construction Stage. Our firm completed this project and this served as the prototype for eventual extension of the Parks System to the south and west on the river valley. It also served as a prototype for the eventual expansion of the Urban Parks Program to five other smaller cities: Grande Prairie, Lloydminster, Red Deer, Medicine Hat and Lethbridge. We were awarded EXCELLENCE in Urban Design Awards by the City of Edmonton for our planning, design and public coordination of the Capital City Recreation Park.\nMeeting Roman has been a stroke of luck for me, and he has provided valuable insight into the creation of our modern park system. In addition, he has also shared some great documents from the time. One of my favorites (so far) is this proposed pedestrian bridge from Rundle Park to the Science Park. While the bridge was built, the weir and renewable energy exhibits never made the cut.\nImage Courtesy of Roman Fodchuk\nWhile Roman is justly proud of the work his company completed back in the 1970’s, the irony is that the information he has passed on may well help restore parts of the river valley in the future. Thanks again Roman!\nRVA Public Member, Strathcona County']"	['<urn:uuid:1082d2a0-1100-4173-a720-efcb6b7f7cff>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:10:11.825225	24	71	752
59	How do climate disasters affect mental health and preparation plans?	Climate disasters can lead to PTSD, depression, anxiety, and substance use disorders, both through direct trauma and indirect stress. For preparation, individuals should create evacuation plans, maintain disaster supplies (including 3-day water/food supply, first aid kits, and emergency documents), and build community trust. Having generators, sump pumps, and infrastructure to withstand disasters at the community level is also essential.	['Hurricane Watch & Hurricane Warning:\nWhat’s The Difference?\nHurricanes are strong storms that cause life and property threatening hazards such as flooding, storm surge, high winds and tornadoes. Preparation is the best protection against the dangers of a hurricane. As forecasters track a hurricane the terms “hurricane watch” and “hurricane warning” will be used often. It is important to know the difference:\n- Hurricane Watch – Hurricane conditions are a threat within 48 hours. Review your hurricane plans, keep informed and be ready to act if a warning is issued.\n- Hurricane Warning – Hurricane conditions are expected within 36 hours. Complete your preparations and leave the area if directed to do so by authorities.\nWhat Should I Do?\n- Listen to local forecasts on television and radio as well as National Weather Service (NWS) announcements.\n- Check your disaster supplies and replace or restock as needed.\n- Bring in anything that can be picked up by wind (furniture, toys, etc.)\n- Close windows, doors and shutters.\n- Turn refrigerator and freezer to the coldest settings and keep them closed as much as possible so that food will last longer if power goes out.\n- Turn off propane tanks and unplug small appliances.\n- Fill your car’s gas tank.\n- Create an evacuation plan with your family. Plan routes to local shelters and make plans for pet care.\n- Evacuate if advised by authorities. Be careful to avoid flooded roads and bridges.\n- Because standard homeowners insurance does not cover flooding, it is important to have protection from floods associated with heavy rains, tropical storms and hurricanes. For more information visit the National Flood Insurance Program website at https://www.fema.gov/national-flood-insurance-program\nWhat Supplies Do I Need?\n- Water – at least a three-day supply, one gallon per person, per day.\n- Food – at least a three-day supply of non-perishable, easy to prepare food.\n- Battery-powered or hand-cranked radio\n- Extra batteries\n- First aid kit\n- Medications seven-day supply) and medical items (hearing aids, glasses, syringes, etc.)\n- Multi-purpose tool\n- Sanitation & personal hygiene items\n- Copies of personal documents (medications, proof of address, deed/lease, passports, insurance policies, etc.)\n- Cell phones with chargers\n- Family & emergency contact information\n- Extra cash (ATMs may be down if power is lost)\n- Emergency blankets\n- Maps of the area\n- Baby & pet supplies if needed\n- Tools/supplies to secure your home\n- Extra clothing (hats and shoes as well)\n- Extra set of car & house keys\n- Rain gear\n- Insect repellent and sunscreen\n- Camera for photos of damage for insurance claims\nWhat Do I Do After A Hurricane?\n- Continue to listen to weather forecasts and local news for updates.\n- Stay alert for extended rainfall and flooding even after the storm has ended.\n- If you evacuated only return home when officials say it is safe.\n- Drive only if necessary and avoid flooded roads and bridges.\n- Keep away from loose or downed wires and report them immediately.\n- Stay out of any building surrounded by water.\n- Inspect your home for damage. Take pictures for insurance claims.\n- Use flashlights in the dark, NOT candles which can cause fires.\n- Avoid drinking or preparing food with tap water until you are sure it is not contaminated.\n- Check refrigerated food for spoilage. If in doubt, throw it out.\n- Wear protective clothing and use caution when cleaning up to avoid injury.\n- Watch pets closely and keep them under your direct control.\n- Use telephones only for emergency calls\nLet Your Family Know You Are Safe!\nIf your community experiences a hurricane or any disaster, register on the American Red Cross Safe and Well website available through RedCross.org/SafeandWell to let your family and friends know about your welfare. If you do not have Internet access, call 1-866-GET-INFO to register yourself and your family.\nClick here to download a Hurricane Safety Checklist provided by the Red Cross.', 'Sarah Lowe is a clinical psychologist and assistant professor in the department of social and behavioral sciences at Yale School of Public Health.\nPhoto courtesy Jeffrey R. Moran\nClimate change is changing how human beings live on the earth as floods, wildfires and extreme weather change the land and destroy property.\nLiving with climate change as a constant threat on the horizon has also changed how human beings think about their own existence.\nBoth kinds of distress — the acute trauma of immediate disasters and the background sense of existential doom — require different responses, both personal and from society.\nSarah Lowe is a clinical psychologist and assistant professor in the department of social and behavioral sciences at Yale School of Public Health, and she spoke with CNBC about both of these impacts on human wellness.\nThe following are excerpts of Lowe’s conversation with CNBC. They have been edited for brevity and clarity.\nClimate disasters and trauma\nVirtually every state has been affected by some sort of climate change exposure, whether it’s a weather related disaster, or a wildfire, tornado or whatnot.\nDisasters are fundamentally stressful. And for some people, they can be traumatic both directly — by leading to direct threats to one’s life, for serious injuries, bereavement, destruction of one’s property — or indirectly. We know (and this is true with the pandemic as well, just as an aside) that when people are faced with stressful situations, some people who might have a tendency for aggression and violence can be tipped due to stress.\nRates of child abuse and intimate partner violence and things like that tend to increase in the aftermath of disasters, as well as extreme heat, so that’s another form of trauma that happen in the aftermath of disasters.\nGypsy Rick smokes a cigarette outside of a cooling shelter during a heat wave in Portland, Oregon, U.S., August 11, 2021. REUTERS/Mathieu Lewis-Rolland\nMATHIEU LEWIS-ROLLAND | REUTERS\nFor people who don’t face serious life threats, it is stressful if if part of your property floods or your property or possessions get damaged, or if you have to evacuate for an unknown period of time — that is very disruptive, especially with the idea that this could be a regular thing that you have to deal with.\nIn terms of the mental health consequences, we know that PTSD can result from disasters. Disasters are also associated with increased rates of a variety of psychiatric conditions and symptoms: depression, generalized anxiety, substance use, disruptions and health behaviors, like healthy eating and exercise. And these can all have downstream impacts on mental health in the long term.\nThere are the physical consequences of disasters such as exposure to mold or to wildfire smoke. The sedentary behavior that might come from disruptions and routines can trigger physical health ailments or increase the risk of them — that then are intertwined with mental health. In addition to the direct traumas of disasters, they can have other mental health consequences that might not be as obvious.\nPreparing for a direct climate change disaster\nOne thing that is key is preparation at many different levels to the extent that people are able. It’s all tied into the social determinants of health like income, housing and employment. Some people, when their house gets flooded, they can invest in systems like generators, like sump pumps, to prevent that from happening again, whereas other people can’t do that.\nAt the individual level, do what you can. That could be having a plan in place for if something like this happens again: Where are we going to go? Planning is exerting some sense of control.\nAt the community level, investing in infrastructure to shield people from exposure, whether that’s creating housing that’s able to withstand a disaster or not creating housing in low lying areas, investing in generators, having plans in place to evacuate whole communities together, building trust between government entities and community leaders and organizations. As much as we can shield people from the really traumatic exposures that happen during disasters, the better it will be for mental health.\nA home is seen destroyed in the aftermath of Hurricane Delta in Creole, Louisiana, U.S., October 10, 2020. Picture taken with a drone.\nAdrees Latif | Reuters\nReadying yourself should also include a sense of trust in one’s community and one’s government that they’re not going to put their residents at risk. That’s really tricky, because it’s all really expensive, and if you invest in one thing it means you can’t invest in other things, but I think it’s really important.\nCompanies need to be preparing too, especially if they’re going to be providing essential services during disasters, but also, you know, taking care of your employees, because we know that one of the stronger predictors of mental health after disasters are these longer term stressors, like losing one’s job, or financial stress. We spoke to people who experienced Hurricane Katrina, and a lot of them had companies that really, they felt, looked out for them, that gave them financial assistance, or if there were a national chain, for example, hooked them up with a job in the community that they were displaced to. And those things really made a difference.\nPsychological resilience is important across the board and that requires addressing the social determinants of health and exposures. So making sure that people have their basic needs met — that they have good housing, that they’re able to find gainful employment, that they have health care, that they have access to mental health services and that they’re covered, that people are not working 100 hours a week and not getting by. All of those things are going to make for a healthier society, and are really important, so that’s at the policy level.\nAt the more community and individual level, we need to be doing things to foster resilience of children, adolescents, and families. In school, that means building in a socio-emotional curriculum to foster the psychological capacities that promote resilience — a sense of agency, goal-orientation, hope, social social skills and social support, a sense of purpose, emotion regulation. All these capacities we know are really important, in addition to all of the academic skills that are important too. Although I say that acknowledging that that there’s a lot of pressure put on schools and teachers already.\nSo we need to find ways to integrate that into that family life, into communities, organizations, after-school programs and religious congregations, too, so really working towards a trauma-informed and healthy and resilient population. That’s going to be really important for us as we deal with these increasingly complex and intense stressors.\nTake time and space to care for yourself, whether that means exercising, meditating, meditating, spending time in nature. That’s that’s really important to build resilience.\nEcological grief, solastalgia, climate change anxiety\nWe have to distinguish between the traumatic stressors that can happen because of disasters, or other climate-change-related exposures or displacement, and this free flowing climate-change anxiety — we know this is happening, it’s scary, it’s sad, and what do we do about that at a bigger scale?\nAll of these feelings — they’re valid feelings. It’s sad to see a landscape changing. Natural beauty dissipating is objectively sad.\nclinical psychologist and assistant professor in the department of social and behavioral sciences at Yale School of Public Health\nIt’s definitely an existential threat. People talk a lot about not only their own futures, but making childbearing decisions. Am I going to have kids and bring them into a world that is burning? I think that’s a valid concern. Whether that’s going to happen in your lifetime or your child’s lifetime, thinking about the future of the human race gets a little bit anxiety-provoking. I think that’s understandable.\nExistential anxiety does not fit the standard definition of trauma, because it’s not a direct life threat or threat to one’s physical integrity or a sexual violation. Leaders in the trauma field would say, no, that’s not actually traumatic. It might be stressful and anxiety provoking, but it’s not a traumatic in that it can trigger PTSD.\nThat being said, we know from disasters, terrorist attacks and the pandemic that consumption of media, seeing images of places that are affected by disasters, especially graphic images, can lead to symptoms that are very much consistent with post traumatic stress, including nightmares, avoidance, an exaggerated startle response, disruptions in sleep, etc.\nWe don’t want people to have their heads in the sand. We do want the reality of climate change to hit with people. So I would not say, you know, avoid any information about climate change at all. I often say, get the facts and move on. You don’t need to read every single article about the same story. If it’s distressing, know when to engage, but also know when to disengage.\nThe existential threat of climate change, learning about the impacts of climate change, can can lead to a lot of really intense emotions, feelings of grief and sadness, anxiety, fear for one’s future. There is ecological grief, or feeling a deep sense of sadness and despair at the changing ecosystem. There is solastalgia, which is a feeling of nostalgia for your home environment. Someone defined it as homesickness, when you’re actually at home. So being in your home environment and seeing the changes that have happened due to climate change and feeling sad about that. And then climate change anxiety.\nValidating people’s emotions is really important. Sometimes older generations want to say the younger generations are so sensitive and they’re blowing things out of proportion. Really take the time to listen to younger people about what their concerns are. And also just recognize that it is okay, and completely valid, to be to be sad about losses in ecosystems, to be anxious about the future of humanity, to have these feelings. So, let people have their feelings, and try to also empower them to take action to to cope with their feelings.\nA girl plays with sand during a protest of the Cornwall Climate Youth Alliance in partnership with Fridays for Future and Climate Live, at Gyllyngvase Beach, in Falmouth, on the sidelines of the G7 summit in Cornwall, Britain, June 11, 2021.\nTom Nicholson | Reuters\nWhen anxiety turns into a clinical problem\nIt’s sad to see a landscape changing. Natural beauty dissipating is objectively sad. It is scary to think there might be a time when the earth is uninhabitable for human beings. That is scary. Those are extremely valid feelings. It’s important to distinguish between those valid feelings and clinical disorders. There is a line that can be crossed where climate change anxiety can turn into an anxiety disorder.\nPeople need to watch out for signs that they are in extreme distress, and that their feelings of sadness, grief, anger, anxiety are getting in the way of their lives and functioning and their ability to engage in their lives and also be active in combatting climate change.\nLook for signs the following: Is your appetite disrupted? Are you not able to sleep? Are you feeling uncomfortable being around other people? Are you able to get out of bed?\nIf you are unable to go to work or to your classes at all, or, if when you’re there, you’re totally preoccupied by your anxiety and not performing as you usually would, that’s a sign their anxiety is clinical in nature. If your friends and family have noticed that you seem sad or anxious or you’re distracted or irritable, getting into more fights, or you don’t really want to spend time with people, and you want to self isolate, that would be a sign. If you are so distressed that it’s leading to somatic symptoms, such as you’re unable to get rest, to fall asleep and stay asleep, you’ve lost your appetite. And certainly if you’re having thoughts of death, dying, self injury — those are like warning signs.\nAll these signs of a clinical disorder might indicate you might want to seek help and process your thoughts and feelings about climate change, and whatever else in your life is contributing to that. We don’t want people so anxious that they can’t function.\nAnxiety serves a purpose. And it can motivate action. In the limited research I’ve done on climate change anxiety, the people who are the most active are anxious, but they’re not necessarily having generalized anxiety disorder or depressive symptoms. And in fact, in preliminary research we’ve done, environmental activism can prevent climate change anxiety from manifesting as clinical depression.\nYoung protesters take part in the Fridays For Future rally in Glasgow, Scotland on November 5, 2021, during climate summit COP26.\nDaniel Leal-Olivas | AFP | Getty Images\nWhen engaging in climate activism, think about helping those who are most vulnerable.\nIf you feel like your action is making a difference, that can lead to a sense of like agency and empowerment. Engaging in a community can also foster a sense of collective efficacy and social support so you know there are other people who are share your values and who are working together to make changes.\nWe’ve done a little bit of open-ended questions with young people and in interviews. What gets really tricky is when people sometimes rightly recognize that their collective actions might not make a difference, that this problem is bigger than them, and relies on people with a lot of power making major changes that maybe they’re for whatever reason not willing to make. That can be very overwhelming and disheartening, but at the same time I do think engaging in collective action, we’ve seen in other social movements does make a difference. It’s just … it can be slow.\nAlso in this series:']	['<urn:uuid:bb7e07d0-032f-4eb7-ab51-b59520d5c432>', '<urn:uuid:c25d9337-f06d-44a8-89ce-64502ba7d0d9>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:10:11.825225	10	59	2967
60	main goals people check when monitoring building progress and status	Time, quality and cost are the three main factors monitored in Construction Risk Management. These factors are interrelated and are essential for determining the success of construction projects.	"['By Darren Seary\nLet’s face it, even the most common everyday tasks such as driving your car carry risks. Before you get into your car for the first time, you know there is a risk of being involved in an accident; however, you reduced your risk by passing a driving test and you can manage the ongoing risk by driving carefully and attentively. Likewise, as with any financial investment, risk is inherent, ready to take a bite at the most inopportune time. How and when risk is identified, how it is initially minimized and subsequently carefully managed throughout the project will be a major factor in the overall success of the investment and will likely determine whether or not it meets the investors\' goals.\nConstruction-related real estate investments still provide a good pathway for investors both large and small, experienced and inexperienced, to achieve financial gain and/or – in the EB-5 world – immigration status. However, it is beneficial for investors to have a comprehensive understanding of ""Construction Risk Management 101"" before they financially commit to a project.\nAlthough Construction Risk Management (CRM) can be both broad and technical, it can help educate investors on three simple risk factors: time, quality and cost.\nAll three factors are interrelated, as we will highlight in the scenarios below; however, an understanding of each factor is initially required.\nThe most basic CRM tool used to identify and manage time risk is a detailed construction schedule. EB-5 investments are somewhat unique when related to construction projects, as the EB-5 job count may benefit from longer construction durations. According to the June 17, 2009 USCIS Memorandum “EB-5 Entrepreneurs – Job Creation and Full Time Positions,” the EB-5 job count may include both direct and indirect construction jobs provided the construction timeline is 24 months or longer. Only indirect construction jobs can be counted if the construction timeline is less than 24 months. As a standalone risk factor, it is typically advantageous to complete a project in the shortest timeframe possible. Whether it be the additional revenue generation from the early completion of the project, like apartment rental income and hotel rooms to fill; eliminating the possibility of liquidated damages being incurred for finishing the project late, such as office leases that typically contain significant penalties for the developer if the tenant cannot take occupancy or start their tenant Improvements on time; or incurring additional loan interest costs, having the project completed on time or ahead of schedule is a goal of most developers. These items are in addition to the simple fact that the longer a project runs, the higher the chance of unforeseen circumstances, not envisaged at the onset of the project, coming into play. How many developers would have known in 2006 that implications of finishing their condominium project on time were millions in profit versus the likelihood of bankruptcy by finishing six months later?\nThe most basic CRM tool used to identify and manage quality risk is a comprehensive quality control program. For construction projects, most people view quality as the level of finishes, such as quartz countertops in lieu of plastic laminate; however, in the CRM world, the focus is on the design and installation of systems and components that you would not typically see when a project is complete. The design of the structure, the strength of the concrete used in the foundations, and whether the design and installations pose any potential accessibility or water intrusion risks are just a few of the numerous items a CRM firm should focus on. A comprehensive quality control program is frequently overlooked or not deemed necessary as it is incorrectly assumed that the local municipality will identify all items during the inspection process.\nThe most basic CRM tool used to identify and manage construction cost risk is an ongoing construction cost analysis/control process from project inception through completion. Quite simply, as development costs increase, return on investments (ROIs) decrease. Development costs include items such as the price paid for the land, financing costs, soft costs (design fees, legal costs and marketing) and hard costs (construction costs). Typically, construction costs will vary the most, especially throughout the construction phase, which is why CRM techniques are used to help minimize cost increases and maximize ROIs.\nLet\'s focus on the CRM Triangle and provide scenarios to highlight how time, quality and cost are interrelated. Of course, throughout the design process, the project delivery team, which often includes the general contractor, will strive to bring forward a project that can be built within a timeframe and budget and completed to a level of quality that will meet the developer\'s and investors\' requirements. However, throughout the design and construction process, compromises often have to be reached as risk factors impact the project.\nCONSTRUCTION PHASE SCENARIO 1: WHAT LIES BENEATH\nGeotechnical and environmental investigations help identify what lies beneath the ground. Nevertheless, unearthing the remains of a 40,000-year old mammoth during deep excavations on a project might not be what the developer expects. All work must be temporarily suspended while archeological investigations take place. In this scenario, the developer has to prioritize the three risk factors. If completing the project on time and without compromise of quality is critical, the general contractor will have to accelerate the construction schedule with overtime and incentive payments, which causes increased cost. If cost and quality are important to the project delivery team, then theoretically time will be negatively impacted. If the project delivery team indicate that lost time can be recovered without cost impacts, then concerns should be raised that construction is being rushed or shortcuts taken, resulting in a negative impact on quality. In this scenario, quality of work should be closely monitored.\nCONSTRUCTION PHASE SCENARIO 2: COSTS ARE ON THE RISE\nThe construction industry was hit hard during the 2007-2009 Great Recession. Skilled tradespeople were forced to find other industries and many have never returned, while people starting out on their career path never joined the industry. As such, the construction sector is currently being negatively impacted by skilled labor shortages. This in conjunction with other uncontrollable factors such as material availability following natural disasters, has increased construction costs significantly. If evaluated and identified during the design phase, the developer can make an informed decision as to whether to proceed with the project. However, when cost increases occur during the construction phase, the developer would again consult the CRM Triangle to evaluate their options. If a developer does not want to incur the cost of paying incentive bonuses for subcontractors to prioritize their projects and does not want to compromise on quality, then time will be impacted through an extended construction duration. If a subcontractor can provide an adequate number of workers on a project so as not to impact time, and without incurring cost increases, maybe the labor is less skilled and quality will likely be impacted.\nUtilizing these scenarios, you can appreciate how each of the three construction risk factors – time, quality and cost – are closely interrelated requiring prioritization of risk factors from the CRM Triangle throughout a project. Understanding the correct factors to prioritize and/or forgo will likely be a key component in the overall success of a project.\nDepending on the investors’ risk tolerance or trust level in the developer, a developer or investor should consider retaining a CRM firm to evaluate a given project. The CRM firm can identify and advise the developer or investor on how to minimize and manage potential construction risks. Criteria to consider when selecting a CRM firm include: CRM experience, project-specific type and geographic experience, reputation, in-house staff training programs, quality control protocols, number of years in business and quality of work product.\nDevelopers and investors are advised to choose carefully, as the effectiveness of the CRM process is only as good as the firm providing the services.\nConstruction-related real estate investment opportunities present themselves at various stages of a project; however, the same principals of CRM apply: identify, minimize, manage. The CRM Triangle helps investors with little to no experience understand ""Construction Risk Management 101.""']"	['<urn:uuid:431d14d2-b5c4-490a-9541-c6e2d271bc89>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	10	28	1348
61	What activities did kids do during the WNBA FIT program?	During the WNBA FIT program, kids participated in various activities including dribbling around cones and obstacles to work on ball-handling skills, rope jumping, ladder running, and doing sit-ups. The program showed that basketball training can happen outside a traditional gym setting.	['WNBA FIT: Stars Help Keep Future Generations in Shape\nIziane Castro Marques leads a passing drill during a WNBA FIT event\nJesse D. Garrabrant/NBAE/Getty Images\nThat same philosophy specifically applies to this weekend’s WNBA vs. USA Basketball: Stars at the Sun game, but it also applies in the setting where you have a gym full of eager, willing and downright energetic kids come together for a WNBA FIT program.\nFriday morning at Mohegan Sun Arena, WNBA players were on hand to help lead a WNBA FIT program with over 40 kids. Among those representing the WNBA All-Stars were the Atlanta Dream’s Iziane Castro Marques and Washington\nStars at the Sun: WNBA FIT\n“I think it’s important to kind of give back and show them how important [fitness] is,” said Pondexter, sporting a pair of bright-yellow, glow-in-the-dark sneakers at the ladder station. “I hope that I make an impact on them today. They’ll remember this moment. I remember the first time I met Sheryl Swoopes at a Final Four and I’ll remember that the rest of my life, so hopefully they’ll remember this as well.”\nGiving back to the community is one of the many elements that the WNBA embraces on a daily basis, in-season or off-season. Many of the players look at it as an opportunity to provide the younger generations with a chance to build on a healthier, more athletic lifestyle.\n“One of the best things about these clinics is that it shows you don’t always have to be in a gym to be playing basketball,” said Connecticut Sun center and first-year WNBA player Charles. “There’s other aspects of the game. You can use anything just to play some basketball.”\nThe evidence was clear as participants dribbled around cones and various obstacles in an attempt to work on their ball-handling skills.\nThe appreciation and value of an event like this works both ways. Kids have an opportunity to learn all new ways of staying in shape, and the athletes gain a whole new perspective on their own dreams of having the opportunity to play basketball for a living.\n“I wish I had it [when I was younger] because it’s just great when you can learn from WNBA players and stars,” said Castro Marques. “It’s very important for us to see and to pass this around, to emphasize that a healthy life is going to make a big difference.”\nMcCoughtry, a scond-year forward and teammate of Castro Marques, shared a similar outlook. “I really take pride in doing it because I wish I would’ve had this when I was younger,” she said, taking a minute to step aside from her jump rope station. “I just try to do it for the kids because I know it means a lot to them.”\nWNBA President Donna Orender, an active participant in the morning’s events, slowed down long enough to share a few quick words on the day.\n“They have this unbelievable opportunity to spend time in a very special place with very special people,” said Orender. “When kids come and feel that then they’re going to be open, and when you’re open you just never know where it’s going to take you. That’s the magic of this and that’s why I personally enjoy it so much.”\nWith that, Orender jumped right back into the fray of rope jumping, ladder running, ball dribbling and sit-ups to help promote one of the biggest facets of a long, healthy life – an active, athletic and all around fun lifestyle.']	['<urn:uuid:fb5e7e4b-05d0-4354-9bfb-54c3f529cff9>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	10	41	584
62	What measurement unit is used for sample intervals?	Sample intervals are measured in units of seconds. In the example given, the sample interval is 6.25e-12 seconds.	"['Convert Scattering Parameter to Impulse Response for SerDes System\nThis example shows how to find an Impulse Response by combining a Scattering-Parameter (S-Parameter) model of a baseband communication channel along with a transmitter and receiver represented by their analog characteristic impedance values. You will see how to find an Impulse Response of this network using the\nsParameterFitter app to create an\nSParameterChannel (hyperlink to sParameterChannel in doc) object in SerDes Toolbox™, which also uses some supporting functions from RF Toolbox™ such as\nrational (RF Toolbox) and\nimpulse (RF Toolbox).\nThe S-Parameter file representing the baseband channel should be a Touchstone 1.0 (.sNp) file. Typically these are extracted from a software EDA tool or laboratory VNA with a port reference impedance (50-Ohms is recommended). The following properties are the main settings you would use to extract an impulse response of the concatenated Transmitter-Channel-Receiver circuit network:\nFileName - Filename of the S-Parameter to be imported.\nPortOrder - Port order for the S-Parameter.\nMaxNumberOfPoles - Maximum number of poles to use for a fit output by the\nErrorTolerance - Desired error tolerance in dB for a fit output by the\nSampleInterval - Sample interval in units of seconds.\nStopTime - Desired duration of the Impulse Response in units of seconds.\nTxR - Single-ended impedance value in Ohms of the analog TX IO structure.\nTxC - Capacitance value in Farads of the analog TX IO structure.\nTxAmplitude - Stimulus amplitude of the Tx output rising waveform.\nTxRiseTime - The 20-80% rise time of the Tx stimulus waveform.\nTxRTFactor - The conversion factor from 20-80% or 10-90% to 0-100% risetime, default is 20-80%.\nRxR - Single-ended resistance value in Ohms of the analog RX IO structure.\nRxC - Single-ended capacitance value in Farads of the Rx structure.\nSignalling - Specify signaling as \'single-ended\' or \'differential\'.\nAggressorDefinition - Method of acquiring aggressor behavior. For \'same-source\' the stimulus is applied to victim input and probed at aggressor output and for \'same-load\' (default) the stimulus is applied to each aggressor input and probed at the victim output. While the same load definition is more direct, the same source definition is used by methodologies that rely on time domain excitation (like HSPICE) to stimulate the system with a single pulse or step response. Consider the following 4-port single-ended system:\nThrough-Ports: (1) -- (3)\nThrough-Ports: (2) -- (4)\nThe victim line is S31, the same-source crosstalk aggressor is S41 and the same-load crosstalk aggressor is S32.\nAutoDetectPortOrder - Boolean option to force auto-detect of port order.\nNote: defaults are provided for all settings if no entries are made when calling\nCreate the S-Parameter Channel Object:\nYou create an\nSParameterChannel object by launching the\nsParamterFitter app from the base workspace.\nThis will allow you to create an impulse response from a Touchstone S-Parameter data file. The app loads with default parameters for an\nSParameterChannel object. The equivalent command would be as follows:\nobj = SParameterChannel(\'FileName\',\'default.s4p\',... \'PortOrder\', [1 2 3 4],... \'MaxNumberOfPoles\', 1000,... \'ErrorTolerance\', -40,... \'SampleInterval\', 6.25e-12,... \'StopTime\', 20e-9,... \'TxR\',50,... \'TxC\',0.1e-12,... \'TxAmplitude\', 1.0,... \'TxRiseTime\', 40e-12,... \'TxRTFactor\', 0.6,... \'RxR\',50,... \'RxC\',0.2e-12,... \'Signaling\',\'differential\',... \'AggressorDefinition\',\'same-load\');\nIt is important to also ensure\nStopTime are set appropriately for the Impulse Response calculation (in this case, 6.25e-12 seconds and 20e-9 seconds, respectively), as well as the stimulus represented by\nTxRiseTime. Understanding S-Parameters is beyond the scope of this example, but it is important to remember the bandwidth of an S-Parameter must be sufficient to model a channel according to the Nyquist-Shannon sampling theorem.\nVisualize a Plot of the Impulse Response:\nYou can plot the impulse response in the\nsParamterFitter app and export the object\nsParameterFit and the impulse response\nsParameterFitImpulse to the base workspace.\nUse Impulse Response for Channel Model in Serdes Designer\nYou can use the impulse response of the baseband channel model within SerDes Designer by selecting ""Impulse response"" for channel model and enter the base workspace variable\nsParameterFitImpulse that you created with the\nSimulate Channel Network with Transmitter and Receiver in Serdes Designer\nYou can use the impulse response of the baseband channel model within Serdes Designer in concert with TX and RX topologies to simulate an eye diagram.\nImpulse Response for Channel Model in Simulink\nYou can use the impulse response of the baseband channel model within Simulink by opening the Analog Channel block and clicking on the option ""Impulse Response"" under Channel Model. This will launch the']"	['<urn:uuid:9a1758bc-0849-41b6-b277-bfd046e8a187>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	8	18	723
63	explain why sun gods getting shot with arrows in ancient stories might be about northern lights	According to plasma physicist Anthony Peratt's hypothesis, ancient myths about shooting multiple 'suns' may actually describe intense auroras in prehistoric times. These auroras would have appeared as atmospheric plasma formations with multiple glowing plasmoids, which could explain why cultures worldwide had similar stories about heroes shooting down multiple suns. The aurora borealis we see today involves ions from solar wind colliding with atoms in Earth's atmosphere, creating colorful displays, though much less intense than what these ancient myths might have described.	"['Mar 04, 2014\nIn the study of myth, the question of metaphors presents many a challenge.\nWhen does a ‘sun’ literally refer to the quotidian sun and when is it a metaphor for some other bright sky light? Hard-and-fast rules are risky; every tradition is best examined in its own right.\nOne group of myths that seem to concern some other luminaries than the familiar sun is the cross-cultural theme of ‘sun shooting’. In this, one, two or more ‘suns’ are found to be unsatisfactory, often for shining too brightly, and are brought down by the arrows of one or more heroes, leaving only the present sun in the heavens.\nIn cases where the assault results in the victim’s fragmentation there can be no question that the object was not the real sun. This was the fate that befell Ta-vi, ‘the sun-god”, upon raising the ire of Ta-wats, ‘the hare-god’, in a myth from the Ute (primarily of Utah and Colorado):\n‘Ta-wats awoke in great anger, and speedily determined to go and fight the sun-god. After a long journey of many adventures the hare-god came to the brink of the earth, and there watched long and patiently, till at last the sun-god coming out he shot an arrow at his face, but the fierce heat consumed the arrow ere it had finished its intended course; then another arrow was sped, but that also was consumed; and another, and still another, till only one remained in his quiver, but this was the magical arrow that had never failed its mark. Ta-wats, holding it in his hand, lifted the barb to his eye and baptized it in a divine tear; then the arrow was sped and struck the sun-god full in the face, and the sun was shivered into a thousand fragments, which fell to the earth, causing a general conflagration.’\nA solar referent is also less likely in cases where multiple so-called ‘suns’ were targeted. Where two are concerned, an overlap occurs with the motif of ‘two rival suns, one of which becomes the sun, the other the moon’. Thus, the Karen (Myanmar) spoke of ‘two suns’, which once ‘came forth, but the heat was so great that neither man nor beast could endure it.’ To remedy this, a man by the name of Thye-kha ‘went up into the valley of mount Ra-ko-sho, and shot an arrow into the face of one of the suns, and it ceased to give light and became the moon, which God appointed to rule over the night.’ The other, presumably, turned into the present sun.\nThis storyline was also common among the natives of Taiwan. For example, the Atayal (northern Taiwan) used to relate that once ‘two suns circled around in the sky, and there was no separation between day and night. One of the suns was much larger than the one we see today, and it caused the weather to be extremely hot. The scorching heat caused the plants started to shrivel [sic! MAS] and the rivers started to dry up which made agricultural crops impossible to grow. The people on earth suffered greatly.’ Eventually, ‘The bigger sun was shot and burning blood oozed out from its wound.’ The moon seen today is its remnant and the distinction of day and night has existed ever since. A neighbouring nation, the Bunun (originally of the central mountain range) likewise recalled ‘that there were once two suns in the sky. The heat was unbearable until the ancestor of the Bunun Tribe climbed to the top of Mt. Jade and blinded one of the suns, so it is now the cool moon.’\nVariants on the same theme of the celestial archer involve seven or nine ‘suns’. This number suggests that none of these myths originated in relation to the actual sun and moon. A Mongolian tale serves as an example:\n‘Once upon a time there rose seven suns in the universe, and it was exposed to a burning drought. The earth was heated fiercely, the streams and rivers evaporated, the plants and trees were parched. People and living beings suffered from intolerable heat, and horses and animals were tormented by painful thirst. It was dreadfully difficult to live or even survive. However, there lived a very good archer, called Erkhii Mergen. He was an excellent archer, who could shoot skillfully what he saw and hit accurately at what he aimed. A stream of people went to him, and requested him to shoot and destroy the many suns which rose in the sky. … Then he said, ‘I will shoot the seven suns with one arrow each and destroy them’ … From the Eastern side then Erkhii Mergen began shooting the seven suns, those that rose in a file from the East to the West in the sky. He hit and destroyed six of the suns with six arrows. … the last sun was afraid of the archer and it disappeared to hide behind a western mountain. … People say the last sun of this world was frightened of Erkhii Mergen and went behind a mountain, and it is for this reason that the day and the night appear in succession.’\nTaking a shot at explaining such myths, some might consider bolides or meteorite impacts – both of which may feature large dazzling bodies, singly or in multiples, which may violently explode and cause conflagrations. However, such events last seconds to minutes and could have nothing to do with the first installation of the sun and moon in their current positions. This impression is strengthened by further variations on the theme.\nSome versions associate the pesky suns with a form of the axis mundi, such as a cosmic tree. For instance, in Chinese mythology the giant ‘support mulberry’ (fú sāng), rooted in ‘T’ang (hot water) Valley’, was stated to be the place ‘where the ten suns were bathed’: ‘This is north of Hei Ch’ih. In the water is a large tree with nine suns in the lower branches and one in the upper.’ ‘When one sun gets to the tree, another comes out.’ An early commentator added: ‘In the top of the jo tree are ten suns, resembling flowers strung together, casting light downward.’ On one occasion, all ten suns appeared in the sky at once, producing an intense life-threatening heat on the earth. The archer Hòu Yì then eliminated all except the extant sun: ‘In early times ten suns came out together, burning the vegetation to a crisp.’ ‘Yao ordered Yi to shoot the ten suns, and he hit nine.’\nAlternatively, some associated this episode of ‘shooting stars’ with the ‘separation of heaven and earth’, one of the defining events in global creation mythology. This is illustrated in a tradition from the Dusun (west Sabah, Malaysia):\n‘Long ago when the sky was very low down, only a man’s height from the ground … the sky was very low. Then the man was very angry because his wife was ill, and he made seven blow-pipe arrows. Early the next morning he took his blow-pipe with him and went to the place where the sun rises and waited. Now at that time there were seven suns. When they rose he shot six of them and left only one remaining; then he went home. At the time the man shot the suns … the sky had risen to its present place; since, when the man had shot the six suns, the remaining sun, being frightened, ran away up into the air and took the sky with it.’\nThe internal consistency between such superficially preposterous accounts from historically unrelated cultures, interlocking with equally puzzling themes such as the ‘erstwhile rivalry of sun and moon’, the ‘noosing of the sun’ and ‘stationary suns’, cries out for a rational explanation. A long shot it may be, but a compelling solution is offered by plasma physicist Anthony Peratt’s hypothesis of an intense aurora which occurred globally in prehistoric times. This would have taken the form of atmospheric plasma z-pinches developing columnar instabilities. Each Peratt Column would have pinched into some nine superimposed plasmoids. Generally, only one, two or three of these will have appeared at a time in visible light, but on extreme occasions all may have manifested at once, emitting intolerable synchrotron radiation light. While the columns typically appeared motionless, some may have drifted over the horizon.\n‘Fiery arrows’ dispatched towards the plasmoids, shattering of individual plasmoids, links with the axis mundi, devastating heat and wildfires, the stationary position indicated in some versions, the lifting of the ‘sky’ and the appearance of the real sun and moon subsequent to the demise of the column – all of these correlative motifs fall into place on the same adventurous model. The upshot? The earth’s ionosphere and magnetosphere may have passed through periods of extreme turbulence, of which humans have preserved frail but persistent memories.\nRens Van Der Sluijs\nClick here for a Spanish translation', ""The aurora borealis, or northern lights, was studied by ancient Roman and Greek astronomers. The phenomenon was named for the Roman goddess of the dawn, Aurora, and the Greek god of the north wind, Boreas.\nIn Finland, the aurora borealis is called revontulet, which literally translates to fox fires. According to one Finnish folk tale, the lights are caused by a magical fox sweeping his tail across the snow and sending sparks up into the sky.\nAn aurora is a natural light display that shimmers in the sky. Colorful blue, red, yellow, green, and orange lights shift gently and change shape like softly blowing curtains. Auroras are only visible at night, and usually only appear in lower polar regions.\nAuroras are visible almost every night near the Arctic and Antarctic Circles, which are about 66.5 degrees north and south of the Equator. In the north, the display is called aurora borealis, or northern lights. In the south, it is called aurora australis, or southern lights.\nAuroras and the Solar Wind\nThe activity that creates auroras begins on the sun. The sun is a ball of superhot gases made up of electrically charged particles called ions. The ions, which continuously stream from the sun’s surface, are called the solar wind.\nAs solar wind approaches the Earth, it meets the Earth’s magnetic field. Without this magnetic field protecting the planet, the solar wind would blow away Earth’s fragile atmosphere, preventing all life. Most of the solar wind is blocked by the magnetosphere, and the ions, forced around the planet, continue to travel farther into the solar system.\nAlthough most of the solar wind is blocked by the magnetosphere, some of the ions become briefly trapped in ring-shaped holding areas around the planet. These areas, in a region of the atmosphere called the ionosphere, are centered around the Earth’s geomagnetic poles. The geomagnetic poles mark the tilted axis of the Earth’s magnetic field. They lie about 1,300 kilometers (800 miles) from the geographic poles, but are slowly moving.\nIn the ionosphere, the ions of the solar wind collide with atoms of oxygen and nitrogen from the Earth’s atmosphere. The energy released during these collisions causes a colorful glowing halo around the poles—an aurora. Most auroras happen about 97-1,000 kilometers (60-620 miles) above the Earth’s surface.\nThe most active auroras happen when the solar wind is the strongest. The solar wind is usually fairly constant, but solar weather—the heating and cooling of different parts of the sun—can change daily.\nSolar weather is often measured in sunspots. Sunspots are the coldest part of the sun and appear as dark blobs on its white-hot surface. Solar flares and coronal mass ejections are associated with sunspots. Solar flares and coronal mass ejections are sudden, extra bursts of energy in the solar wind. Sunspot activity is tracked over an 11-year cycle. Bright, consistent auroras are most visible during the height of sunspot activity.\nSome increased activity in the solar wind happens during every equinox. These regular fluctuations are known as magnetic storms. Magnetic storms can lead to auroras being seen in the midlatitudes during the time around the spring and autumnal equinoxes. Auroras have been visible as far south as the Yucatan Peninsula in Mexico.\nMagnetic storms and active auroras can sometimes interfere with communications. They can disrupt radio and radar signals. Intense magnetic storms can even disable communication satellites.\nColoring an Aurora\nThe colors of the aurora vary, depending on altitude and the kind of atoms involved. If ions strike oxygen atoms high in the atmosphere, the interaction produces a red glow. This is an unusual aurora—the most familiar display, a green-yellow hue, occurs as ions strike oxygen at lower altitudes. Reddish and bluish light that often appears in the lower fringes of auroras is produced by ions striking atoms of nitrogen. Ions striking hydrogen and helium atoms can produce blue and purple auroras, although our eyes can rarely detect this part of the electromagnetic spectrum.\nTo find out more about the mysterious light displays, scientists have launched satellites specially designed to study auroras. Until 2005, NASA’s IMAGE (Imager for Magnetopause-to-Aurora Global Exploration) satellite used ultraviolet and radio waves to study auroras and how they are formed.\nTerm Part of Speech Definition Encyclopedic Entry altitude Noun\nthe distance above sea level.\nEncyclopedic Entry: altitude Antarctic Circle Noun\nline of latitude at 66.5 degrees south that encircles the continent of Antarctica.\nlayers of gases surrounding a planet or other celestial body.\nEncyclopedic Entry: atmosphere atom Noun\nthe basic unit of an element, composed of three major parts: electrons, protons, and neutrons.\nbrightly colored bands of light, visible around Earth's geomagnetic poles, caused by solar wind interacting with particles in Earth's magnetic field.\nEncyclopedic Entry: aurora aurora australis Noun\nbright bands of color around the South Pole caused by solar wind and the Earth's magnetic field. Also called the southern lights.\naurora borealis Noun\nbright bands of color around the North Pole caused by solar wind and the Earth's magnetic field. Also called the northern lights.\nautumnal equinox Noun\nautumn day, usually around September 22, when day and night are of generally equal length.\nan invisible line around which an object spins.\nEncyclopedic Entry: axis charged particle Noun\nmolecule that has a positive or negative electric charge.\nto crash into.\ncoronal mass ejection Noun\nhuge burst of solar wind and other charged particles.\nelectromagnetic spectrum Noun\ncontinous band of all kinds of radiation (heat and light).\nimaginary line around the Earth, another planet, or star running east-west, 0 degrees latitude.\nEncyclopedic Entry: equator equinox Noun\nperiod in which daylight and darkness are nearly equal. There are two equinoxes a year.\nEncyclopedic Entry: equinox fluctuate Verb\nto constantly change back and forth.\ndelicate or easily broken.\nstate of matter with no fixed shape that will fill any container uniformly. Gas molecules are in constant, random motion.\ngeomagnetic pole Noun\npoint marking the tilted north and south axes of Earth's magnetic field, about 1,300 kilometers (800 miles) from the geographic poles.\n(2000-2005) (Imager for Magnetopause-to-Aurora Global Exploration) NASA satellite used to study the auroras and magnetosphere.\nto meddle or prevent a process from reaching completion.\nelectrically charged atom or group of atoms, formed by the atom having gained or lost an electron.\nouter layer of the Earth's atmosphere, 80-400 kilometers (50-250 miles) above the surface.\nmagnetic field Noun\narea around and affected by a magnet or charged particle.\nmagnetic storm Noun\ninteraction between the Earth's atmosphere and charged particles from solar wind.\nteardrop-shaped area, with the flat area facing the sun, around the Earth controlled by the Earth's magnetic field.\narea between the Tropic of Cancer and the Arctic Circle in the north, and between the Tropic of Capricorn and the Antarctic Circle in the south. Also called a temperate zone.\n(acronym for National Aeronautics and Space Administration) U.S. agency responsible for space research and systems.\nchemical element with the symbol N, whose gas form is 78% of the Earth's atmosphere.\nnorthern lights Noun\nalso known as the aurora borealis. The bright bands of color around the North Pole caused by the solar wind and the Earth's magnetic field.\nchemical element with the symbol O, whose gas form is 21% of the Earth's atmosphere.\nhaving to do with the North and/or South Pole.\nextreme north or south point of the Earth's axis.\n(RAdio Detection And Ranging) method of determining the presence and location of an object using radio waves.\nradio wave Noun\nelectromagnetic wave with a wavelength between 1 millimeter and 30,000 meters, or a frequency between 10 kilohertz and 300,000 megahertz.\nobject that orbits around something else. Satellites can be natural, like moons, or made by people.\nsolar flare Noun\nexplosion in the sun's atmosphere, which releases a burst of energy and charged particles into the solar system.\nsolar system Noun\nthe sun and the planets, asteroids, comets, and other bodies that orbit around it.\nsolar wind Noun\nflow of charged particles, mainly protons and electrons, from the sun to the edge of the solar system.\nsouthern lights Noun\nthe bright bands of color around the South Pole caused by the solar wind and the Earth's magnetic field. Also known as the aurora australis.\nstar at the center of our solar system.\ndark, cooler area on the surface of the sun that can move, change, and disappear over time.\nhaving to do with light of short wavelengths, invisible to the human eye.""]"	['<urn:uuid:a6311cd4-7039-45af-a317-2f5c89c8fcb5>', '<urn:uuid:05f12f7c-5518-49c2-828d-127c63bd6fd6>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-12T19:10:11.825225	16	81	2891
64	laboratory power tools goggles features standards	Laboratory goggles must combine impact resistance with side shields for chemical splash protection and may include laser protection (EN 207/ANSI Z 136). For power tools, goggles must be made of unbreakable material to prevent particles from piercing the eye and include ventilation to prevent fogging. According to OSHA standards, all eye protection must provide adequate protection against specific hazards, be comfortable, durable, and easily cleanable.	"['Goggles or safety glasses are forms of protective eyewear that usually enclose or protect the eye area in order to prevent particulates, water or chemicals from striking the eyes. They are used in chemistry laboratories and in woodworking. They are often used in snow sports as well, and in swimming. Goggles are often worn when using power tools such as drills or chainsaws to prevent flying particles from damaging the eyes. Many types of goggles are available as prescription goggles for those with vision problems.\ncarved goggles from caribou antler\n, as well as wood and shell, to help prevent snow blindness\n. The goggles were curved to fit the user\'s face and had a large groove cut in the back to allow for the nose. A long thin slit was cut through the goggles to allow in a small amount of light, diminishing subsequent ultraviolet\nrays. The goggles were held to the head by a cord made of caribou sinew\nThe requirements for goggles varies depending on the use. Some examples:\n- Cold weather: Most modern cold-weather goggles have two layers of lens to prevent the interior from becoming ""foggy"". With only a single lens, the interior water vapor condenses onto the lens because the lens is colder than the vapor, although anti-fog agents can be used. The reasoning behind dual layer lens is that the inner lens will be warm while the outer lens will be cold. As long as the temperature of the inner lens is close to that of the interior water vapor, the vapor should not condense. However, if water vapor gets between the layers of the lens, condensation can occur between the lenses and is almost impossible to get rid of; thus, properly constructed and maintained dual-layer lenses should be air-tight to prevent water vapor from getting in between the lenses.\n- Swimming: Must be watertight to prevent water, such as salt water when swimming in the ocean, or chlorinated water when swimming in a pool, from irritating the eyes or blurring vision. Allows swimmers to see clearly underwater. They will not be usable more than a few feet underwater, because the water pressure will press them tightly against the face. Examples of these include the Swedish goggles.\n- Power tools: Must be made of an unbreakable material that prevents chunks of metal, wood, plastic, concrete, and so on from hitting or piercing the eye. Usually has some sort of ventilation to prevent sweat from building up inside the goggles and fogging the surface.\n- Blowtorch goggles: These protect the eyes from glare and flying sparks and hot metal splashes while using or near as blowtorch. They are not dark enough for arc welding.\n- Motorcycle riding and other open-air activities: Prevents insects, dust, and so on from hitting the eyes.\n- Laboratory and research: Combines impact resistance with side shields to prevent chemical splashes reaching the eyes. May also include laser protection which would be covered by EN 207 (Europe) and ANSI Z 136 (United States). Examples of these include red adaptation goggles.\n- Racquetball: Protect the eyes from racquets swinging in an enclosed area and from impact from hard rubber ball.\n- Winter sports: Protect the eyes from glare and from icy particles flying up from the ground.\n- Astronomy and meteorology: dark adaptor goggles are used before going outside at night, in order to help the eyes adapt to the dark.\n- Basketball: Several NBA players have worn goggles during play, including Kareem Abdul-Jabbar, James Worthy, Horace Grant, Kurt Rambis and Amare Stoudemire; they prevent a fellow player from scratching or hitting the eyes when trying to grab the basketball.\n- Aviation: In open cockpit aircraft, similar to biplanes, aviators, such as Amelia Earhart and Charles Kingsford Smith, would wear goggles to help protect from the wind and are still in use today. Examples of these include the AN-6530 goggles.\n- Virtual reality: A virtual reality headset, sometimes called ""goggles"", is a wrap-around visual interface to display computer output. Commonly the computer display information is presented as a three-dimensional representation of real-world environments.\nGoggles are often worn as a fashion statement in certain subcultures, most often as part of the cybergoth subculture\n. They are usually worn over the eyes or up on the forehead to secure \'falls\': a type of long, often brightly-coloured, synthetic hairpiece. Fans of the Steampunk\ngenre or subculture also frequently wear steampunk-styled goggles, particularly when performing in a live action role-playing game\nGoggles are part of a meme\nstemming from a quote from the Radioactive Man\nepisode of The Simpsons\nwhere Rainier Wolfcastle\nis faced with a tidal wave of sulfuric acid\nwhile filming a movie, his only provided safety precaution a pair of rubber safety goggles intended to deflect small amounts of acid. When the scene goes horribly wrong, he is swept away, his suit and prop goggles dissolving, spurring him to yell, ""My eyes! The goggles do nothing!\nGoggles are available for horses used in speed sports such as horse racing. In some traditions of horse mounted bullfighting, the horse may wear a protective cloth over its eyes.\nGoggles adapted for use by dogs are marketed under the trademark Doggles by California-based Doggles, LLC. Unlike other eye protection marketed for dogs, Doggles are designed to protect from wind and foreign objects in addition to UV light.', 'Personal Protective and Life Saving Equipment Standards in New York, NY\nDoes OSHA Require Safety Equipment On Construction Sites?\nThe federal Occupational Safety and Health Administration has established regulations regarding construction site protective and life-saving equipment. If a contractor or other party failed to comply with these standards and you were injured in a construction accident, you could be entitled to compensation.\nIf you or someone you love was harmed in a construction accident and someone other than your employer was to blame, you could be entitled to receive damages if the third party falls outside of the workers’ compensation system. The attorneys at David Resnick & Associates, PC, are skilled at evaluating construction accident claims and can help you determine whether the negligence of a third party caused or contributed to your injuries. Call our firm today at 877-815-6053 or use our online contact form for a free evaluation of your case.\nWe serve construction accident victims in Manhattan, and all of New York City, including Manhattan, Queens, Staten Island, Brooklyn, and Long Island.\nHere are the highlights of OSHA’s Personal Protective and Life Saving Equipment Standards, as found online at http://www.osha.gov/pls/oshaweb/owadisp.show_document?p_table=STANDARDS&p_id=10909\nOSHA Personal Protective and Life Saving Equipment Standards\n- 1926.95 Criteria for personal protective equipment.\n- 1926.95(a) “Application.” Protective equipment, including personal protective equipment for eyes, face, head, and extremities, protective clothing, respiratory devices, and protective shields and barriers, shall be provided, used, and maintained in a sanitary and reliable condition wherever it is necessary by reason of hazards of processes or environment, chemical hazards, radiological hazards, or mechanical irritants encountered in a manner capable of causing injury or impairment in the function of any part of the body through absorption, inhalation or physical contact.\n- 1926.95(b) “Employee-owned equipment.” Where employees provide their own protective equipment, the employer shall be responsible to assure its adequacy, including proper maintenance, and sanitation of such equipment.\n- 1926.95(c) “Design.” All personal protective equipment shall be of safe design and construction for the work to be performed.\n- 1926.95(d) Payment for protective equipment.\n- 1926.95(d)(1) Except as provided by paragraphs (d)(2) through (d)(6) of this section, the protective equipment, including personal protective equipment (PPE), used to comply with this part, shall be provided by the employer at no cost to employees.\n- 1926.95(d)(2) The employer is not required to pay for non-specialty safety-toe protective footwear (including steel-toe shoes or steel-toe boots) and non-specialty prescription safety eyewear, provided that the employer permits such items to be worn off the job-site.\n- 1926.95(d)(3) When the employer provides metatarsal guards and allows the employee, at his or her request, to use shoes or boots with built-in metatarsal protection, the employer is not required to reimburse the employee for the shoes or boots.\n- 1926.95(d)(4) The employer is not required to pay for:\n- 1926.95(d)(4)(i) Everyday clothing, such as long-sleeve shirts, long pants, street shoes, and normal work boots; or\n- 1926.95(d)(4)(ii) Ordinary clothing, skin creams, or other items, used solely for protection from weather, such as winter coats, jackets, gloves, parkas, rubber boots, hats, raincoats, ordinary sunglasses, and sunscreen.\n- 1926.95(d)(5) The employer must pay for replacement PPE, except when the employee has lost or intentionally damaged the PPE.\n- 1926.95(d)(6) Where an employee provides adequate protective equipment he or she owns pursuant to paragraph (b) of this section, the employer may allow the employee to use it and is not required to reimburse the employee for that equipment. The employer shall not require an employee to provide or pay for his or her own PPE, unless the PPE is excepted by paragraphs (d)(2) through (d)(5) of this section.\n- 1926.96 Occupational foot protection. Safety-toe footwear for employees shall meet the requirements and specifications in American National Standard for Men’s Safety-Toe Footwear, Z41.1-1967.\n- 1926.100 Head protection.\n- 1926.100(a) Employees working in areas where there is a possible danger of head injury from impact, or from falling or flying objects, or from electrical shock and burns, shall be protected by protective helmets.\n- 1926.100(b) Helmets for the protection of employees against impact and penetration of falling and flying objects shall meet the specifications contained in American National Standards Institute, Z89.1-1969, Safety Requirements for Industrial Head Protection.\n- 1926.100(c) Helmets for the head protection of employees exposed to high voltage electrical shock and burns shall meet the specifications contained in American National Standards Institute, Z89.2-1971.\n- 1926.102 Eye and face protection.\n- 1926.102(a)(1) Employees shall be provided with eye and face protection equipment when machines or operations present potential eye or face injury from physical, chemical, or radiation agents.\n- 1926.102(a)(2) Eye and face protection equipment required by this Part shall meet the requirements specified in American National Standards Institute, Z87.1-1968, Practice for Occupational and Educational Eye and Face Protection.\n- 1926.102(a)(3) Employees whose vision requires the use of corrective lenses in spectacles, when required by this regulation to wear eye protection, shall be protected by goggles or spectacles of one of the following types:\n- 1926.102(a)(3)(i) Spectacles whose protective lenses provide optical correction;\n- 1926.102(a)(3)(ii) Goggles that can be worn over corrective spectacles without disturbing the adjustment of the spectacles;\n- 1926.102(a)(3)(iii) Goggles that incorporate corrective lenses mounted behind the protective lenses.\n- 1926.102(a)(4) Face and eye protection equipment shall be kept clean and in good repair. The use of this type equipment with structural or optical defects shall be prohibited.\n- 1926.102(a)(6) Protectors shall meet the following minimum requirements:\n- 1926.102(a)(6)(i) They shall provide adequate protection against the particular hazards for which they are designed.\n- 1926.102(a)(6)(ii) They shall be reasonably comfortable when worn under the designated conditions.\n- 1926.102(a)(6)(iii) They shall fit snugly and shall not unduly interfere with the movements of the wearer.\n- 1926.102(a)(6)(iv) They shall be durable.\n- They shall be capable of being disinfected.\n- 1926.102(a)(6)(vi) They shall be easily cleanable.\n- 1926.102(a)(7) Every protector shall be distinctly marked to facilitate identification only of the manufacturer.\n- 1926.102(a)(8) When limitations or precautions are indicated by the manufacturer, they shall be transmitted to the user and care taken to see that such limitations and precautions are strictly observed.\n- 1926.102(b) Protection against radiant energy-\n- 1926.102(b)(1) Selection of shade numbers for welding filter. Table E-2 shall be used as a guide for the selection of the proper shade numbers of filter lenses or plates used in welding. Shades more dense than those listed may be used to suit the individual’s needs.\n- 1926.102(b)(2) Laser protection.\n- 1926.102(b)(2)(i) Employees whose occupation or assignment requires exposure to laser beams shall be furnished suitable laser safety goggles which will protect for the specific wavelength of the laser and be of optical density (O.D.) adequate for the energy involved. Table E-3 lists the maximum power or energy density for which adequate protection is afforded by glasses of optical densities from 5 through 8.\n- 1926.102(b)(2)(ii) All protective goggles shall bear a label identifying the following data: The laser wavelengths for which use is intended; The optical density of those wavelengths; The visible light transmission.\n- 1926.104 Safety belts, lifelines, and lanyards.\n- 1926.104(a) Lifelines, safety belts, and lanyards shall be used only for employee safeguarding. Any lifeline, safety belt, or lanyard actually subjected to in-service loading, as distinguished from static load testing, shall be immediately removed from service and shall not be used again for employee safeguarding.\n- 1926.104(b) Lifelines shall be secured above the point of operation to an anchorage or structural member capable of supporting a minimum dead weight of 5,400 pounds.\n- 1926.104(c) Lifelines used on rock-scaling operations, or in areas where the lifeline may be subjected to cutting or abrasion, shall be a minimum of 7/8-inch wire core manila rope. For all other lifeline applications, a minimum of 3/4-inch manila or equivalent, with a minimum breaking strength of 5,400 pounds, shall be used.\n- 1926.104(d) Safety belt lanyard shall be a minimum of 1/2-inch nylon, or equivalent, with a maximum length to provide for a fall of no greater than 6 feet. The rope shall have a nominal breaking strength of 5,400 pounds.\n- 1926.104(e) All safety belt and lanyard hardware shall be drop forged or pressed steel, cadmium plated in accordance with type 1, Class B plating specified in Federal Specification QQ-P-416. Surface shall be smooth and free of sharp edges.\n- 1926.104(f) All safety belt and lanyard hardware, except rivets, shall be capable of withstanding a tensile loading of 4,000 pounds without cracking, breaking, or taking a permanent deformation.\n- 1926.105 Safety nets.\n- 1926.105(a) Safety nets shall be provided when workplaces are more than 25 feet above the ground or water surface, or other surfaces where the use of ladders, scaffolds, catch platforms, temporary floors, safety lines, or safety belts is impractical.\n- 1926.105(b) Where safety net protection is required by this part, operations shall not be undertaken until the net is in place and has been tested.\n- 1926.105(c)(1) Nets shall extend 8 feet beyond the edge of the work surface where employees are exposed and shall be installed as close under the work surface as practical but in no case more than 25 feet below such work surface. Nets shall be hung with sufficient clearance to prevent user’s contact with the surfaces or structures below. Such clearances shall be determined by impact load testing.\n- 1926.105(c)(2) It is intended that only one level of nets be required for bridge construction.\n- 1926.105(d) The mesh size of nets shall not exceed 6 inches by 6 inches. All new nets shall meet accepted performance standards of 17,500 foot-pounds minimum impact resistance as determined and certified by the manufacturers, and shall bear a label of proof test. Edge ropes shall provide a minimum breaking strength of 5,000 pounds.\n- 1926.105(e) Forged steel safety hooks or shackles shall be used to fasten the net to its supports.\n- 1926.105(f) Connections between net panels shall develop the full strength of the net.\n- 1926.106 Working over or near water.\n- 1926.106(a) Employees working over or near water, where the danger of drowning exists, shall be provided with U.S. Coast Guard-approved life jacket or buoyant work vests.\n- 1926.106(b) Prior to and after each use, the buoyant work vests or life preservers shall be inspected for defects which would alter their strength or buoyancy. Defective units shall not be used.\n- 1926.106(c) Ring buoys with at least 90 feet of line shall be provided and readily available for emergency rescue operations. Distance between ring buoys shall not exceed 200 feet. At least one lifesaving skiff shall be immediately available at locations where employees are working over or adjacent to water.']"	['<urn:uuid:2f1d42cd-5e09-4cef-a292-abb64779acb4>', '<urn:uuid:e1d70430-f7a3-4aac-9c13-5dac93acfe0f>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	6	65	2662
65	What are the main symptoms of the rare condition linked to DDX6?	The condition linked to DDX6 is characterized by intellectual disability, developmental delay, speech and feeding difficulties, low muscle strength with difficulties walking, mild-to-moderate cardiac anomalies, and specific facial features.	['- Posted Thursday August 15, 2019\nDDX6 among a growing list of genes identified by TGen’s Center for Rare Childhood Disorders\nPHOENIX, Ariz. — Aug. 15, 2019 — Modern science and data sharing converged to underpin a study led by the Translational Genomics Research Institute (TGen), an affiliate of City of Hope, that identified a gene associated with a rare condition that results in physical and intellectual disabilities of children.\nThe results, published today in the American Journal of Human Genetics, suggest that rare variants in the gene DDX6 are associated with a significant disruption in the development of the central nervous system, governing such basic skills as the ability to walk and talk.\n“One of the most powerful revelations of this study is the identification of pathogenic mutations in DDX6; a gene not previously linked to childhood disorders and one which appears to play a key role in early brain development,” said Chris Balak, a research associate in TGen’s Neurogenomics Division, and the study’s lead author.\nBalak zeroed in on DDX6 by comparing the sequencing results from a 5-year-old Arizona girl who was seen at TGen’s Center for Rare Childhood Disorders (the Center) with those identified in large population databases and to the genomes of her parents, who are healthy. Following this revelation, and preliminary findings posted on a website shared by investigators worldwide, TGen identified four similar cases: two in the U.S., and one each in France and the Netherlands.\nThese children’s conditions were characterized by intellectual disability, developmental delay, speech and feeding difficulties, low muscle strength with difficulties walking, mild-to-moderate cardiac anomalies, and specific facial features.\n“Something we are quite proud of with this work is our combined effort with other physicians and scientists in Europe to demonstrate that changes in this gene cause this rare syndrome in multiple patients,” said Dr. Matt Huentelman, TGen Professor of Neruogenomics, Scientific Director of the Center, and one of the study’s senior authors. “Collectively, our clinical and laboratory data describe a new brain development syndrome caused by genetic changes in DDX6.”\nSince opening in 2012, the Center has sequenced the genomes of hundreds of children and their families.\n“Through an international collaboration, by combining genomic methods with detailed molecular studies using cells and tissues from our patients, we were led to the discovery of a new neurodevelopmental syndrome caused by mutation of DDX6,” said Dr. Vinodh Narayanan, Medical Director of the Center, and another of the study’s senior authors. “We expect that the insight into disease mechanisms gained from our studies will lead to a better understanding of an entire group of neurodevelopmental disorders, and eventually guide us to specific treatments.”\nContributing to this study were: the Institute of Genetics and Molecular and Cellular Biology (IGBMC), Sorbonne University Institute of Biology Paris-Seine, the Broad Institute of MIT and Harvard University, Massachusetts General Hospital, Geisinger Medical Center, Cleveland Clinic, and several hospitals and research institutes in the Netherlands, Germany and France.\nFunding for this study — Rare De Novo Missense Variants in RNA Helicase DDX6 Cause Intellectual Disability and Dysmorphic Features and Lead to P-Body Defects and RNA Dysregulation — was provided by Foundation Jerome Lejeune, Fondation Maladies Rares, Association Paul and Liba Mandel, CREGEMES, Agence Nationale de la Recherche, the French National Research Agency, and private donors in support of TGen’s Center for Rare Childhood Disorders.\n# # #\nAbout TGen, an affiliate of City of Hope\nTranslational Genomics Research Institute (TGen) is a Phoenix, Arizona-based non-profit organization dedicated to conducting groundbreaking research with life-changing results. TGen is affiliated with City of Hope, a world-renowned independent research and treatment center for cancer, diabetes and other life-threatening diseases: www.cityofhope.org. This precision medicine affiliation enables both institutes to complement each other in research and patient care, with City of Hope providing a significant clinical setting to advance scientific discoveries made by TGen. TGen is focused on helping patients with neurological disorders, cancer, diabetes and infectious diseases through cutting-edge translational research (the process of rapidly moving research toward patient benefit). TGen physicians and scientists work to unravel the genetic components of both common and complex rare diseases in adults and children. Working with collaborators in the scientific and medical communities worldwide, TGen makes a substantial contribution to help our patients through efficiency and effectiveness of the translational process. For more information, visit: www.tgen.org. Follow TGen on Facebook, LinkedIn and Twitter @TGen.\nTGen Senior Science Writer']	['<urn:uuid:cfca9576-2532-4d49-b5be-cac794f57475>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:10:11.825225	12	29	732
66	As a musician experimenting with low frequencies, how do saxophones perform in deep bass ranges compared to their maintenance needs?	Saxophones do not actually perform in deep bass ranges - even the baritone saxophone only goes down to Db3. The saxophone requires regular maintenance including swabbing after each use and monthly mouthpiece cleaning with a 50/50 solution of lemon juice/vinegar and water. Sticky pads can be temporarily fixed with ungummed cigarette paper, but will eventually need replacement by a technician. For truly deep bass frequencies, you would need instruments like pipe organs that can reach as low as C0 (16.35Hz) or electronic instruments. Even acoustic instruments that can produce very low notes, like the piano's A0 (27.5Hz), struggle to make these fundamentals clearly audible.	['Clarinet Vs. Saxophone: Similarities And Differences\nClarinet vs. saxophone is a topic that is talked about a lot between beginning band students and band directors. The clarinet and the saxophone are both single-reed, woodwind instruments, so what is the difference? This article will discuss the similarities and differences between the clarinet and the saxophone.\nMy main instrument is the clarinet, but I can also play the tenor saxophone. I started learning the tenor saxophone during my senior year of high school. Then, I was planning on being a music education major in college. I wanted to get a year of jazz band under my belt in case I didn’t have time for it once I got into college. I could have played the clarinet in my jazz ensemble, but learning a new instrument intrigued me. The transition from clarinet to saxophone took a lot of patience and practice, but it is worth it.\nThe Development of the Saxophone\nThe saxophone was invented by Adolphe Sax, a Belgian instrument maker, in the early 1840s. Adolphe Sax was known for his work with brass and woodwind instruments. Adolphe Sax decided to create an instrument that combined the conical-bore body of brass instruments with the fingering system that woodwinds used and a single-reed mouthpiece.\nIn 1846, the saxophone gained popularity through French military bands. The saxophone was incorporated in solo and chamber music by European composers. The first saxophone instruction took place in Paris and Geneva before the 1850s. Adolphe Sax was the first saxophone professor at the Paris Conservatory. He taught more than 150 students from 1857 to 1870.\nThe most commonly played instruments in the saxophone family are the soprano, alto, tenor, and baritone saxophones. Other saxophone members include the sopranino, bass, and contrabass. These saxophones are rarely used unless the composer calls for them as color instruments. The fingerings are the same across all of the saxophones. The embouchure is mostly the same, the biggest difference being voicing/oral cavity. Music teachers will most likely start students on either the alto saxophone or the tenor saxophone.\nThe Development of the Clarinet\nThe clarinet is based on the chalumeau. The chalumeau was a peasant pipe that was played with a single reed. The chalumeau did not have a barrel or bell, it was shorter than the modern clarinet, it consisted of 7 tone holes, the range was an octave plus one note, and the instrument did not jump to the higher register when overblown.\nThe clarinet was invented in the 1700s by Johann Christian Denner, a Nuremberg instrument maker. Denner’s clarinet was longer than the chalumeau and featured a bell, a wider bore, eight tone holes, two keys, and a register key that allowed the clarinet to overblow a twelfth. The barrel and the mouthpiece were combined into one piece. Denner continued to make improvements on the clarinet with help from his son.\nIn 1812, Iwan Müller introduced a thirteen-key clarinet.\nTheobald Boehm revolutionized the flute by adding a series of ring keys that wrapped around the diameter of the finger holes. When depressed, the finger covered an additional hole at a distance from the finger hole. Klosé and Buffet produced a clarinet that utilized the Boehm system. This clarinet consisted of seventeen keys and six rings that controlled 24 tone holes. The Klosé-Buffet clarinet is essentially the instrument that clarinetists use today throughout the world.\nThe clarinet also has a large family. The clarinet family includes the Eb clarinet, Bb clarinet, A clarinet, Eb alto clarinet, basset horn in F, Bb bass clarinet, and Bb contrabass clarinet. All of the clarinets have the same fingerings, making it easier for clarinetists to transfer between different clarinets. Since all of the clarinets are not in the same key, clarinetists need to understand clarinet transposition.\nThe most common clarinets are the Bb clarinet, A clarinet, and the Bb bass clarinet. The Eb clarinet and the Eb alto clarinet are sometimes used in ensembles to add specific colors to the music. The basset horn was used back in Mozart’s time but is close to non-existent in many ensembles today.\nThe saxophone body is one large piece. The only parts of the saxophone that are not attached are the mouthpiece, ligature and reed and the neck strap. The saxophone has four main parts: the neck, the body, the U-shaped bow, and the bell.\nThe neck of the saxophone is a conical, metal tube. The neck is the bridge between the mouthpiece and the body of the saxophone. The end of the neck that attaches to the mouthpiece is wrapped in cork. This provides a tighter seal between the neck and the mouthpiece. The cork also makes it easier for the mouthpiece to slide on and off. The octave key is located on the neck and is very fragile. Make sure to not hit and damage the octave key when assembling and disassembling the saxophone.\nThe body is made out of brass material. The mouthpiece can be made out of either hard rubber, plastic, or glass.\nThe bow of the saxophone is at the bottom end of the saxophone, also known as the “U”. The bow minimizes the length of the saxophone. If the saxophone didn’t have a bow, it would be an extremely long instrument.\nThe bell of the saxophone is where the sound waves are projected.\nThe saxophone’s tone holes are not covered by the fingers, but rather by keys that have a leather padding underneath. Some tone hole keys stay open or closed by default. Other tone hole keys have to be opened and closed by the fingers pressing down specific buttons.\nThe saxophone has a conical bore. The body of the saxophone is made out of brass metal. The alto, tenor, and baritone saxophone are held up by neck-straps.\nThe clarinet is a single-reed instrument. The clarinet includes five parts: the mouthpiece, the barrel, the top, and bottom joints, and the bell. Each part is assembled by a careful twisting motion. The clarinet parts have cork on the ends to make assembly smoother.\nThe clarinet mouthpiece is similar to a saxophone mouthpiece. Both mouthpieces can be made out of either hard rubber, glass, or plastic. Both mouthpieces also formulate the sound and send sound vibrations through the instrument.\nThe barrel on a clarinet has a similar function as the neck on a saxophone. The barrel connects the mouthpiece to the rest of the instrument. The barrel also assists in tuning the clarinet. The neck of the saxophone should not be used to adjust the intonation.\nThe upper and lower joints of the clarinet are where all of the keys and tone holes are located, similar to the saxophone. The biggest difference is that the saxophone’s body is all one piece, whereas the clarinet’s body is split into two pieces.\nThe bell on a clarinet is similar to the bell on a saxophone. Both bells project the sound out of the instrument. The clarinet’s bell attaches and reattaches, whereas the saxophone’s bell is attached to the saxophone’s body.\nThe clarinet has a cylindrical bore. The clarinet is made out of either grenadilla wood, plastic, or metal. Plastic clarinets are geared more towards beginners or for outdoor use. All clarinets come with open tone holes.\nClarinet and Saxophone Duets\nClarinet and Saxophone Duet sheet music of I Got Rhythm (from the Musical “Girl Crazy”). This Duet can be performed also with Soprano Saxophone (playing Clarinet Part) and Tenor Saxophone (optional 2nd voice Tenor Part included).\nMusicians at an intermediate level and above will find this ideal for formal and informal performances. The melody is expressive and legato; both parts require dynamic contrast and the lower part, in particular, explores a wide range of the instrument. The overall performance time is around four and a half minutes.\nSound and Range\nThe saxophone produces different styles of sound depending on whether it’s playing in a concert band or a jazz ensemble. In a jazz ensemble, the saxophone can sound sweet, sentimental, and full of vibrato or it can sound very raucous, round, and “in your face”. In a concert band, the saxophone will most likely play with a sweeter tone and use less vibrato. Since the saxophone has a very distinct and projected sound, it must work hard to blend in with the rest of the ensemble in a concert band setting.\nThe soprano saxophone is a Bb instrument. Music for the soprano saxophone is written a major second below concert pitch. The soprano saxophone’s range starts at Ab3 and ends at F5.\nThe alto saxophone is an Eb instrument. Music for the alto saxophone is written a major 6th down from concert pitch. The alto saxophone’s range starts at Db 3 and ends at Bb 5.\nThe tenor saxophone is a Bb instrument. Music for the tenor saxophone is written a major 9th down from concert pitch. The tenor saxophone’s range starts at Ab 3 and ends at A5.\nThe bari saxophone is an Eb instrument. Music for the bari saxophone is written a major 13th down from concert pitch. The bari saxophone’s range starts at Db 3 and ends at Bb 4.\nThe Eb clarinet is an Eb instrument. Music for the Eb clarinet is written a minor 3rd above the written pitch.\nThe Bb clarinet is a Bb instrument. Music for the Bb clarinet is written a major second higher from concert pitch. The Bb clarinet’s range starts at E3 and goes to G6.\nThe A clarinet has the same range as the Bb clarinet, but the A clarinet sounds a whole step lower.\nThe bass clarinet is in the key of Bb. Music for the bass clarinet is written a major second higher from concert pitch. The bass clarinet’s range starts at Eb3 and ends at G6.\nI started playing the tenor saxophone in my senior year of high school. My high school had a great jazz program that I wanted to be a part of. I could have played the clarinet and read the tenor saxophone music, but I wanted to experience playing a new instrument.\nThe transition from clarinet to saxophone was not difficult in regards to fingerings. The hardest transition was going back and forth between clarinet embouchure and saxophone embouchure.\nThe clarinet’s embouchure is much more firm and tight, whereas the saxophone’s embouchure is quite loose. To form a saxophone embouchure, make an O shape with your mouth and then pinch the corners of your mouth inward. With clarinet, we want our embouchure to be very condensed. With saxophone, our embouchure is long and open. If saxophone embouchure is condensed, chances are the pitch will become sharp and thin. Since the reed is horizontally angled in our mouth, condensing our embouchure will pinch the reed against the mouthpiece.\nBecause the mouthpieces are angled differently in the mouth, the voicing for the clarinet and the saxophone are vastly different. For the clarinet, the position of the throat changes depending on which register is being played. For the low register, the throat should be open as if you were saying “oh”. As you start playing higher, the throat should change from the “oh” shape to a “hee” shape. In other words, the back of the tongue should be getting higher.\nFor the saxophone, the throat should remain open. An open throat allows all of the air from your diaphragm to travel into the mouthpiece. As you start playing higher on the saxophone, the throat should remain open.\nThe holding position for the saxophone is different than the clarinet. The clarinets, except for the contrabass clarinet, is held between the legs. The saxophones are most commonly held to the right of the player. The alto saxophone can be held either between the legs or to the right side of the player, depending on the comfort level. The rest of the saxophones must be held to the right side of the player, due to their large size.\nThe coloristic effects that the saxophone uses in music are vibrato, growling, slap or smack tonguing, and bending notes.\nThe clarinet has fewer colorist effects than the saxophone. The clarinet utilizes glissandi, bending notes, and vibrato. Clarinet vibrato is most commonly only heard in jazz music.\nMaintenance for the clarinet and the saxophone are very similar. Both instruments should be swabbed out after each time it is played. Swabs with bristles should not be used to clean out the inside of both instruments because it could potentially scratch the insides.\nThe clarinet and saxophone mouthpiece should be cleaned about every month or so. Use a soft cloth and a 50/50 solution of lemon juice or vinegar and water to clean it.\nSticky pads on both the clarinet and the saxophone can be cleaned with ungummed cigarette paper. To do so, place the cigarette paper under the pad and press the key down until the pad touches the tone hole. Gently pull the cigarette paper out. Repeat this process until the pad is no longer sticky. Of course, this solution can only help the pads for so long. Pads will wear out and will need replacing by a music technician.\nIt is helpful to have a small jewelry screwdriver in your case for when screws are loose or fall out. If specific screws consistently become loose, dab a small amount of white glue or clear nail polish on the head of the screw, after it’s screwed it, and this will temporarily prevent it from falling out.\nClarinetists and saxophonists should purchase a high-quality clarinet reed case and saxophone reed case to protect their reeds and to keep them in good condition.\nClarinets and saxophones are seen in both concert bands and jazz ensembles. Seeing clarinets in the jazz band depends mostly on which saxophonists can double on the clarinet and if there are clarinetists who can master the clarinet sound necessary for jazz.\nThe saxophone was rarely used in the orchestra. The invention of the saxophone came much later than the classic string and woodwind instruments found in the orchestra. Furthermore, the intention of the saxophone was to be played in big bands so orchestral composers didn’t write for saxophone. As times changed, composers like Mussorgsky began writing saxophone into the orchestral scores. Modern-day composers are starting to incorporate the saxophone into the orchestra.\nThe clarinet and the saxophone are two incredibly fun instruments. As a clarinetist, I love having the skills to also play the saxophone. I highly recommend learning a saxophone and being a member of a jazz ensemble. If being a member of a jazz ensemble is not an option, I still recommend finding the opportunity to learn the saxophone. Through technology, it’s becoming easier to teach yourself new instruments. There are plenty of resources on Youtube that can help teach you saxophone!', 'What frequencies are Bass?\nby Charlie Santmire\nMost people don’t equate music with frequency (frequency is cycles per second, now called Hertz “Hz”). We do. It’s essential that we equate musical instrument ranges to frequency to get the best result from speakers and woofers. So here is a look at the issues and my definitions of ranges.\nLOWER MIDRANGE (80-160 Hz octave/E2 to E3)\nMany instruments and voices (baritone and bass) have notes in this range. E2 is the lowest note on a guitar and on the Tympani (tuned drum).\nBASS AND UPPER BASS (40-80 Hz/Eto E2)\nActually, 41.02 Hz for E1 to 82.4 Hz for E2. For normally tuned bass guitar and upright double bass, note fundamentals extend down to E1, (open E). I would consider bass arbitrarily to be below C2 (65.41 Hz). That is two octaves below middle C. The fundamental of a note is its musical pitch. It is also called the first harmonic. The second harmonic is thus twice the pitch frequency and so forth. Harmonics are generally the fundamental times 2, 3, etc.\nDEEP BASS (Below 40 Hz)\nSome acoustic instruments and electronic instruments have fundamentals in this range. This range is very difficult to reproduce at a level you can hear with acceptably low distortion. An upright bass with extension goes down to C1 (32.7Hz). The lowest note on a piano is A0 (27.5 Hz). The fundamental of this note A0 is quite audible on a 9′ concert grand – not audible on a spinet (small upright piano).\nDouble bass with extension and a concert grand piano can produce reasonable levels of their lowest fundamental frequency. Contrabassoon can play C1 but the fundamental is essentially absent. You might ask if you can’t here the fundamental why bother to be able to play the note. The answer is that we place the note in our mind by the interval between the harmonics. So, we know a piano is playing A0 (27.5 Hz) because we hear the harmonic structure 55Hz, 82.5Hz etc. even if we cannot hear much of the fundamental.\nVERY DEEP BASS (C0 to C1/16.35Hz to 32.7Hz)\nOnly electronic instruments and pipe organ sound in this range. Big pipe organs will have more than one rank (set of 24 pipes) covering this range. C0 requires a 32′ pipe. There are a couple of organs with 64′ pipes. They can sound at 8Hz.\nAll of these frequencies assume normal current tuning where A4 is 440Hz.\nBass impact is a totally different issue. You need to move a lot of air very quickly to get impact. This is difficult and expensive to do. It is an important aspect of music with kick drum and concert bass drum. The speaker must react forcefully and quickly. The room also enters into the equation. We are investigating this area of reproduction. Stay tuned. More to come on this.\nModerate levels in the midrange where most music is centered are in the range of 80dB SPL (Sound Pressure Level) or so. Zero dB SPL is a defined arbitrary level. The lowest levels we can typically hear are in the 10-20dB SPL range. Equal loudness to that 80 dB SPL level in the midrange at very low frequencies (30Hz or so) is generally considered to be about 110 dB SPL. This level is very difficult and very expensive to produce in an audio system.\nMost larger floorstanding speaker systems should try for 40Hz with low distortion and fairly high levels. To effectively extend response another octave to 20Hz at appropriately high levels requires subwoofers. Our experience is that it is not possible to position the main speakers for maximum output in this octave, even if they can reproduce it, and at the same time position them for best overall musical performance above 40Hz. They would need to be quite close to the wall behind them to benefit from that reinforcement. We all know that placement sounds very bad as the whole musical tonal structure is influenced negatively by reflection from that wall.\nStereo subwoofers are a must, not single subwoofers. Stereo subwoofers extend the bass and “open up” the midrange, giving a sense of space, the space in which the music was performed. If an acoustic instrument recording is made in a concert hall you will then hear the hall, the space in which the music was performed. A single subwoofer will get you some sense of this space and may give you more bass than no subwoofer but that depends on the individual recording. To the extent that bass has different phase in the recording between left and right channels, the bass may partially cancel with a single subwoofer.\nTHE LISTENING ROOM IS ALSO INVOLVED IN BASS REPRODUCTION\nThe room modes (emphasizing or deemphasizing certain frequencies at different places in the room) depends on the size and stiffness of the room. Your sitting position will be at a place where the loudness of some frequencies (notes) will be greater or less than at others. You can do a lot of math and calculate these boosts and cuts. You can also make measurements.\nIn my view, the best you can do is listen to examples of the music you typically listen to and move your listening seat around a little bit, if you can, to see if you like the bass better in one position or another. Stereo woofers tend to smooth out the bass to some extent and I also find that sitting a bit off-center may also help. Often a couple will have two chairs together each being a bit off-center of the width of the room. I find this to be fine in all aspects of listening if you are a bit further from the plane of the speakers than the speakers are apart on centers. We can help you with all this fine detail.\nSetting up stereo woofers is as much an art as a science. You start with the science but what makes things really work is the art of setup developed through the experience of doing many setups.']	['<urn:uuid:2f4e4c10-f818-4005-9063-f6cb2110d7c0>', '<urn:uuid:b4600a4c-e7e5-4c0f-a929-7cf5ad22fb2c>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:10:11.825225	20	104	3483
67	As a financial analyst, what are the main categories of business ratios?	The four main categories of business ratios are: Liquidity Ratios which measure a company's ability to cover short-term obligations, Activity Ratios which indicate asset investment relative to revenue, Leverage Ratios which measure use of debt financing, and Profitability Ratios which measure management's effectiveness in generating profits on sales, assets, and stockholders' investment.	"['- Writing a Business Plan\n- Financial Statements\n- Business Forecasting\n- Business Checklist\nPart 1 - Calculating Ratios\nPart 1 of this section teaches you how to calculate ratios. The most common ratios can be classified into four (4) categories; Liquidity Ratios, Activity Ratios, Leverage Ratios, and Profitability Ratios.\nLIQUIDITY RATIOS measure a company\'s abilities to provide sufficient cash to cover its short term obligations. The most common liquidity ratios include; the current ratio and the quick ratio.\nACTIVITY RATIOS indicate how much a company has invested in a particular type of asset (or group of assets) relative to the revenue the asset is producing. The most common activity ratios include; the average collection period ratio and the inventory turnover ratio.\nLEVERAGE RATIOS measures a company\'s use of debt to finance its operations. The most common leverage ratios include; debt ratio and debt-to-equity ratio.\nPROFITABILITY RATIOS are of great importance to investors since they measure how effectively a firm\'s management is generating profits on sales, total assets, and stockholders\' investment. The most common profitability ratios include; gross profit margin ratio, net profit margin ratio, return on total assets ratio, and return on equity ratio.\nTo further explain each class of ratio as well as each type of ratio, lets use the following 200Y Balance Sheet and Income Statement for the Widget Manufacturing Company. Please note, we will be using the following financial statements throughout this entire section to develop ratios for the Widget Manufacturing Company.\n|WIDGET MANUFACTURING COMPANY\nAS OF DECEMBER 31, 200Y\n|Current Assets:||Current Liabilities:|\n|Cash||$ 2,550||Accounts Payable||$ 9,500|\n|Marketable securities||$ 2,000||Short-term Bank Loan||$11,375|\n|Account Receivable (Net)||$16,675||Total Current Liabilities||$20,875|\n|Total Current Assets||$47,695||Total Long Term Debt||$24,000|\n|Plant & Equipment||$41,000||EQUITY:|\n|Less: Accumulated Depreciation||$11,000||Common Shares||$25,000|\n|Net Plant & Equipment||$30,000||Retained Earnings||$ 7,820|\n|Total Liabilities & Equity||$77,695|\n|Widget Manufacturing Company\nCondensed Income Statement\nFor Year Ending Dec. 31, 200Y\n|Sales from Widgets||$112,500|\n|Cost of Goods Sold (COGS)||$ 85,040|\n|Gross Margin||$ 27,460|\n|Operating Expenses (Marketing & Administrative)||$ 18,950|\n|Net Income Before Taxes||$ 8,510|\n|Less: Income Taxes||$ 4,163|\n|Net Income After Taxes||$ 4,347|\nAdditional Information Required:\nNo dividends were declared or paid to the owners during 200Y\nThe Widget Manufacturing Company\'s beginning inventory on January 1, 200Y was valued at $22,500. Average inventory is calculated by adding the beginning & ending inventories together and dividing the sum by 2.\nNow we have enough information to begin calculating the Widget Manufacturing Company\'s ratios. Lets start by calculating the company\'s liquidity ratios (see below).\nCalculation of Widget Manufacturing Company\'s Ratios:\nSummary of Ratio Example:\nThis concludes PART 1 - Calculating Ratios. In the above readings, we calculated ratios as of December 31 , 200Y for the Widget Manufacturing Company. We can briefly summarize the company\'s performance by saying;\n- As of December 31, 200Y, the company has sufficient current assets to cover its short-term debts (Current Ratio).\n- As of December 31, 200Y, the company does not have to rely heavily on selling its inventory to pay its short term debt (Quick Ratio).\n- On average, the company\'s customers pay within 54 days from date of purchase, for products brought on credit (Average Collection Period Ratio).\n- The company used or consumed its inventory an average of 3.5 times during 200Y (Inventory Turnover Ratio)\n- As of December 31, 200Y, 58% of the company\'s assets are owned by creditors such as bank, suppliers, and vendors (Debt Ratio). This indicates the company is paying high interest charges.\n- Creditors are providing more money towards financing the company\'s operations than the owners (Debt-to-Equity Ratio).\n- The company makes 24 cents on every dollar generated in sales (Gross Profit Margin Ratio). These funds are used to pay the company\'s operating expenses, tax obligation, dividends, etc...\n- In 200Y, the company earned 4 cents (after tax) on every dollar generated in sales. Moreover, after product costs, operating expenses & taxes are considered, 4 cents is left over to contribute to net income, or to be distributed to the owners, or a combination of both (Net Profit Margin Ratio).\n- In 200Y, the company used its assets to generate 6 cents in after tax profits (Return on Total Assets Ratio).\n- In 200Y, the owners received a 13% rate of return on their investment (Return on Equity). Moreover, for every one dollar investment into the company by its owners, 13 cents were generated in after tax profits.\nBeing able to calculate the above ratios is important, however, understanding the meaning behind each should be your focus. When reviewing each ratio, ask yourself this question ""what does this ratio attempt to calculate""?\nThis concludes PART 1 - ""Calculating Ratios"".\nExamples of Ratio Calculations:']"	['<urn:uuid:71146ba2-0825-4026-8dd7-7c766d0a45fa>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	12	52	770
68	corporate culture employee performance sustained dialogue compare impact	Both corporate culture and Sustained Dialogue have significant impacts on employee performance, but in different ways. Corporate culture that focuses only on workplace perks like pool tables shows limited results, while truly improving employee experience through basic needs leads to 21% higher profitability and 20% higher sales. In comparison, Sustained Dialogue's structured approach shows more holistic benefits - it develops leadership skills, improves personal identity awareness, builds empathy, and enhances conflict resolution abilities. In one corporate implementation, Sustained Dialogue doubled the number of employees who felt they could bring their full selves to work from 40% to 80%.	['What impact should campuses using SD expect?\nThrough Sustained Dialogue high impact experiences, participants develop a diverse set of leadership skills, including strong personal identity awareness, knowledge of social justice, empathy, facilitation and conflict resolution skills, and more.\nSustained Dialogue is featured in two published academic studies.\nSustained Dialogue’s Impact After Graduation\nThis study, conducted in 2008, interviewed college graduates who had participated in Sustained Dialogue programs in college. Previous research had demonstrated the positive impacts of dialogue during college, but this study asked “How do recent college graduates understand the influence of their college dialogue experience on their post-graduate civic life?”\nTheir participation impacted them in five different ways:\n- Cognitions: Academic pursuits, critical thinking, new ways of thinking about diversity, etc.\n- Behaviors: Involvement in diversity initiatives at work or in graduate school, engaged in politics, major life decisions, etc.\n- Attitudes: Motivation and interest in diversity issues, more empathetic, more open with others, changed values about social issues, etc.\n- Skills: For relating to others, and for reflecting about themselves.\n- Hopes and Plans for the Future: Plans to attend graduate school or live abroad, engagement in local community, plans to use dialogue in the workplace.\nParticipants stated that SD impacted many of their major life decisions and how they want to raise their own children. This study found that participants developed civic skills that they used at work and in graduate school, and that these skills permeated their personal lives as well. One participant says: “It’s just a part of me and it changes how I look at things.”\n* Diaz, Ande and Rachael Perrault. “Sustained Dialogue and Civic Life: Post-College Impacts” Michigan Journal of Community Service Learning. (Fall 2010). Available at: http://www.sdcampusnetwork.org/ht/a/GetDocumentAction/i/11205\nIn a randomized field trial* conducted comparing two-term SD participants at Addis Ababa University in Ethiopia with students who did not participate, the study found statistically significant attitudinal change:\n- A decrease in mistrust\n- An increase in trust between people of different ethnic origin\n- An increased sense of ethnic identity\n- An increased perception of being ethnically discriminated\n- An increase in accommodative feelings towards students of other ethnicities leading to resulting positive relationships\n* Svensson, Isak and Karen Brounéus. “Dialogue and interethnic trust: A randomized field trial of ‘sustained dialogue’ in Ethiopia.” Journal of Peace Research (August 20, 2013): 1-13. Available at: http://jpr.sagepub.com/content/early/2013/08/19/0022343313492989\nSDCN also conducts pre- and post- dialogue surveys for student participants based on key measures from questions similar to The National Survey on Student Engagement, NSSE, and the Consortium on Financing Higher Education, COFHE. These surveys demonstrate:\nParticipants saw strong increases in some of their personal skills and/or actions because of their time in dialogue, including, but not limited to:\n- Meeting new people (75% to 93%)\n- Developing, understanding, and expressing their personal beliefs (82% to 87%)\n- Explaining the college climate toward diversity, issues that may arise between students, and why issues persist (70% to 81%)\n- Thinking critically about the experiences of others and how they might be improved (86% to 90%)\n- Talking about their experiences in front of a group of their peers (81% to 84%)\n- Having serious conversations with people who are very different from them in terms of race and ethnicity (72% to 76%), gender and sexual orientation (63% to 68%), and religious beliefs, political opinions, and personal values (69% to 73%)\n- Taking steps if a friend, roommate, classmate, or professor makes a biased or hurtful comment (76% to 79%; 58% to 67%)\n- Talking about diversity-related issues with friends (84% to 88%)\n- Using inclusive language (80% to 87%)\nBefore and After SD: How much do you agree with the statement:\nCompared with NSSE statistics, total percentages of participants who responded “Very often” or “Often” to the following questions were as such:\nOther helpful SD resources:\nThe Tajikistan Peace-Building InitiativeThe Inter-Tajik Dialogue (ITD) was initiated in March 1993 as an unofficial dialogue intervention to deal with the ongoing Tajik civil war. ITD involved a core of ten to fourteen influential citizens of Tajikistan divided among the different political factions. A third-party team of 3 Americans and 3 Russians moderated the discussions.\nITD played a significant part in the different phases of the peace-making process in Tajikistan:\n- First, in paving the way for negotiations (1993-1994). This period ended in April 1994 with the launching of the official UN-mediated negotiations to which three ITD members were delegates\n- Second, in providing a parallel venue to the official negotiations where ITD members focused on the elements of a political process of national reconciliation (1994-1997). A peace agreement was concluded and signed in 1997.\n- Third, in a transitional period for establishing a process of national reconciliation (1997-2000): Four ITD members became members of the Commission on National Reconciliation (CNR) which was entrusted with overseeing the implementation of the peace accords.\nThe Arab-American-European Dialogue (AAE) This initiative creates a space where influential and thoughtful citizens from Europe, the United States, and the Arab region gather to discuss the sources of the confrontation between the Arab world and the West and explore the terms of a new relationship between these three parts of the world. The AAE dialogue was launched in March 2004 and is now in its third year of operation. It has tackled a variety of important topics including among others: reforms in the Arab region, electoral processes in Arab countries, the relationship between state, religion and society in the West and Arab region, and an issue cluster dealing with terrorism, violence and occupation. A dialogue executive committee was formed in February 2004 and is now responsible for designing the dialogue meetings and outreach activities. Small delegations of Arab dialogue members have met with parliamentarians in Britain and Italy and with Muslim European groups in Britain. We will be organizing a series of meetings for a similar dialogue group to visit France and the EU headquarters in Brussels.\nSD was used in a corporate setting to address employee engagement. After 10 weeks of dialogue, the number of employees who felt they could bring their full selves to the workplace doubled from 40% to 80%.\nConstructive relationships are the keys to peaceful democratic, social, economic, and political development. Some things only governments can do – provide security, make and enforce law, and fund major programs. But only citizens can transform conflictual human relationships, modify human behavior and change political culture.\nThe Columbus Indiana Community Area Sustained Dialogue, of which Phil Stewart is a co-chairman, now completing two full years of monthly sessions, focuses its work on identifying and addressing obstacles to and creating opportunities for economically challenged citizens to achieve self-sufficiency – economically, socially and emotionally. The dialogue has created a safe space that regularly attracts Columbus area residents from all sectors of the community. Outcomes include a shifting from the concept of “client” to “citizen,” from humiliation to dignity for those who struggle, as well as concrete actions such as the development of a comprehensive program to ensure those ready to work have reliable transportation. More broadly, participants note that their dialogue continues to have a transformative impact on the local political climate, moving from confrontation to listening, and from listening to dialogue.', 'What is Employee Engagement?\nEmployee engagement is a measure of how committed and involved an employee is at work, and is a key part of the employee’s relationship with the organization they work for. The number one driver of engagement is when an employee feels like their manager and leaders care about them.\nBut engagement is not the same as employee satisfaction, which is a measure of how content an employee is at work and with their organization. This term has become a common area of interest for organizations as it relates to many other important factors to business success, including employee retention, productivity, and even workplace safety.\nWhy Employee Engagement Matters\nEmployee engagement is important for many reasons. Unfortunately, most organizations misunderstand its importance and focus solely on engagement when they should also focus on things like employee access, employee experience, and employee satisfaction. These three other factors are key influences on how engaged an employee is at work.\nBut employee engagement matters because it indicates when an employee is actively engaged in their work or workplace culture. Although it is equally as important to focus on an employee’s job performance, engaged employees also actively participate in company culture and help drive culture. Research from Gallup has also shown that it is tied to many other critical business outcomes that clearly demonstrate the return on investment (ROI) of increased engagement.\nGallup Research on Employee Engagement\nGallup has conducted extensive research on this topic and its impacts on a company. In Gallup’s Employee Engagement Meta-Analysis, they found that when engagement improves, this results in improved employee performance. They found that better engagement led to 41% lower absenteeism, 17% higher productivity, 58% fewer patient safety incidents, and 70% fewer safety incidents. Improved employee experience also leads to an increase in quality with 40% fewer defects and 10% higher customer ratings. Employee engagement is positively correlated with profit as well with 21% higher profitability and 20% higher sales when employees are engaged. And highly engaged employees turnover less often.\nHow Do You Measure Employee Engagement?\nMeasuring employee engagement is often a difficult task because there are many different definitions of what it is. Many organizations use an annual employee engagement survey to measure engagement. These surveys are often composed of questions that help the organization measure eNPS (or employee net promoter score), employee satisfaction, and employee commitment to the organization.\nBut there are more ways to measure engagement. Many internal communications professionals will track engagement on internal messages. This includes tracking likes, comments, social shares, and general channel usage (e.g. logins and session duration). This helps understand which employees are using tools regularly and who are willing to go out of their way to engage on a message. That said, just because an employee doesn’t engage with content does not necessarily mean they are disengaged employees. Measuring content engagement is just one input that can be analyzed when assessing employee engagement.\nHow Do You Improve Employee Engagement?\nImproving staff engagement requires a strategy that looks beyond vanity metrics like content engagement. To improve this metric, you have to do more than improve content quality. A common trend in the workplace is to look to improve culture and engage employees with things like pool tables, events, or other workplace perks. But the truth is, that improving engagement requires holistically improving the employee experience. Here’s why.\nWhen you improve employee experience, you increase employee satisfaction. When employees are satisfied and have a supportive, inclusive, and positive work experience, employee engagement will go up.\nSo, how do you improve the employee experience? It starts with an employee listening strategy to collect employee feedback through things like employee surveys or focus groups. We also recommend measuring regularly and conducting employee engagement surveys to keep a finger on the pulse at your organization. To improve employee experience, you need to go beyond wants like fun workplace perks and focus on employee needs. What tools or information are they lacking to get their jobs done? Are their managers effective communicators and coaches? Are you providing them with a basic living wage and benefits package? Do employees feel connected to one another and respected? Tackling some of these basic needs are key to improving both employee experience and engagement.\nHow is Employee Engagement Tied to Employee Experience?\nEmployee engagement is intrinsically linked to employee experience. If employee experience is poor at your organization, it is likely that engagement will also be low.\nEmployee Engagement Trends\nEmployee engagement has remained fairly consistent over the last decade. Thirty years ago, 70% of employees were not engaged at work. In 2019, that number had barely changed; 69% of employees were actively disengaged.\nDuring the 2020 COVID-19 Pandemic, however, we did see some slight increases in engagement. Despite some gains in workplace engagement, there are additional challenges we tracked in 2020. With 61% of American workers working remotely full time in 2020, this increased the need for connection, collaboration, and communication. And we see these same wants and needs among the deskless workforce.\nTo increase these three key areas of an employee’s experience will require a strong partnership between internal communications and human resources. It will also require a need to improve frontline manager communication by helping them to conduct daily or weekly team huddles (also called: team standups, pre-shift meetings, post-shift meetings). And internal communications professionals will need to focus on focus groups to keep a finger on the pulse of the employee body.\nAnother key employee engagement trend will be the importance of senior leaders to be transparent, authentic, and open. And this will create a great opportunity for internal communications professionals to demonstrate to executive leadership that they are strategic advisors.\nAnd last, one of the biggest trends is the importance of building an amazing workplace culture. Often high performing employees churn because of dysfunctional cultures and poor management, and great cultures attract top talent.\nHow Effective Communication Boosts Employee Engagement\nWhen an organization has effective internal communication, there is a positive impact on employee engagement. This is because an all-inclusive communication strategy gives all employees access to the information they need to do their job and feel supported. Without access to communication, or the ability to comment or provide feedback, it is difficult for employees to feel included and, therefore, to engage.\nHow Does theEMPLOYEEapp Help Improve Employee Engagement?\ntheEMPLOYEEapp improves employee engagement by giving all employees—from the office to the frontline—access to communication, resources, and senior leadership.\nOur mobile internal communications app also gives administrators the ability to target and personalize content to specific employee groups, which makes content more relevant to the intended audience. When content is more relevant, an employee will feel more included and important to the organization, in addition to feeling like the organization understands their role, their needs, and their wants.\ntheEMPLOYEEapp also is set up with many ways to engage, including likes, comments, social sharing, survey integrations, an employee directory, and a calendar. This gives employees a way to ask questions, provide feedback, get in touch with the right people or departments, and participate in company events.\nLearn more about how we can help:']	['<urn:uuid:7efd56e5-cfed-4091-93b0-7230510644d4>', '<urn:uuid:57a0d88c-48ba-4c63-937e-060fefc09097>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	8	98	2396
69	myopia management methods efficacy statistics	Several myopia management methods have proven effectiveness. Myopia control utilizing custom contact retainer lenses, enhanced multifocal soft contact lenses, and atropine eye drops can slow myopia progression by more than 50% in developing eyes. Specific examples include the Stellest lens by Essilor using HALT technology and the MiYOSMART lens by Hoya using DIMS technology. While these methods can effectively slow progression, they are considered management rather than cures, as there is currently no way to reverse myopia. Regular use is required - for instance, with orthokeratology lenses, if regular nighttime wear is discontinued, the myopia and blurry vision will return.	"[""Is there a way to “fix” short-sightedness?\nCurrently, there is no cure for myopia (short-sightedness). Blurry vision caused by myopia can be corrected, and myopia progression can be slowed, leading optoemtrists refer to these methods as myopia management, not as treatment or cures.\nMyopia is not an eye disease, it's a refractive error that is often caused by the eyeball growing too long during childhood. Myopia can also develop if the cornea and/or the lens of the eye are too curved. In rare cases, myopia can occur if the lens is too close to the cornea.\nAny of these factors can cause the light entering the eye to focus in front of the retina instead of directly on it. This makes distant objects look blurry.\nThere are proven methods that can be prescribed by an optometrist to correct myopia. Correction refers to corrective lenses and refractive surgery. Corrective lenses can provide clearer vision, but they only work while a person is wearing them. They do not cure the causes of myopia.\nOnce myopia has stabilised (usually by the age of 24), refractive surgery may be an option. LASIK and other types of laser eye surgery can be effective long-term corrections for myopia. However, even surgery is not a cure for the causes of myopia. In some cases, some amount of myopia can return even after surgery.\nMyopia control glasses\nMyopia control contact lenses\nAtropine eye drops\nLifestyle changes (increased time outdoors and decreased prolonged near work)\nMyopia control is necessary because myopia can progress to high myopia, which can result in potentially blinding complications, including:\nMyopia control glasses and contact lenses are not the same as standard prescription lenses. They are special multifocus lenses that can slow the eye growth that leads to myopia.\nThese lenses have shown success in slowing myopia progression. And newer, even more effective designs are constantly being developed. Examples of these designs include:\nThe Stellest lens by Essilor, which uses Highly Aspherical Lenslet Target (HALT) technology\nThe MiYOSMART lens by Hoya, which uses Defocus Incorporated Multiple Segments (DIMS) technology\nOrthokeratology (also called ortho-k) has also been successful in myopia management and control. Orthokeratology lenses reshape the cornea during sleep, temporarily correcting myopia. Ortho-k is not a cure for myopia, the contact lenses must be worn regularly at night, or the myopia and blurry vision will return.\nAtropine eye drops are used in low doses for myopia control. Scientists do not fully understand how they work, but they show the greatest success in slowing the eye growth related to myopia.\nLifestyle factors also play a part in myopia control. Keeping good visual habits is essential to reducing the risk of developing myopia. Leading optometrists recommend spending at least 90 minutes outdoors per day. They also recommend taking frequent breaks during prolonged near work activities.\nIf you want to learn more about myopia correction and control, schedule an eye exam with an optoemtrist near you.\nBeth Longware Duff also contributed to this article.\nUpdate and guidance on management of myopia. European Society of Ophthalmology in cooperation with International Myopia Institute. European Journal of Ophthalmology. March 2021.\nMyopia prevention and outdoor light intensity in a school-based cluster randomized trial. Ophthalmology. November 2018.\nDefocus Incorporated Multiple Segments (DIMS) spectacle lenses slow myopia progression: a 2-year randomised clinical trial. British Journal of Ophthalmology. March 2020.\nEssilor’s Stellest Lens shown to slow myopia progression in children in one-year interim clinical trial. Review of Myopia Management. September 2020.\nMyopia control with positively aspherized progressive addition lenses: a 2-year, multicenter, randomized, controlled trial. Investigative Ophthalmology & Visual Science. November 2014.\nIMI – Clinical management guidelines report. Investigative Ophthalmology & Visual Science. February 2019.\nInterventions to slow progression of myopia in children. Cochrane Database of Systematic Reviews. January 2020.\nThe correlation between headache and refractive errors. Journal of American Association for Pediatric Ophthalmology and Strabismus. March 2008.\nAmetropia in children with headache. Pakistan Journal of Medical Sciences. May - June 2019.\nPage published on Tuesday, 17 March 2020\nPage updated on Friday, 4 August 2023\nMedically reviewed on Thursday, 20 January 2022"", 'Protecting Our Children’s Eyes with Myopia Control\nWhen was the last time you looked around and took note of how many people are wearing glasses? If you yourself don’t wear glasses, I am sure you know someone who does. Did you know that myopia, which is commonly known as nearsightedness, is the most common reason why people wear corrective glasses? But having myopic eyes is a lot more than blurry vision and the need for glasses. Having myopia puts you at a much higher risk for many medical eye conditions such as retinal detachments, cataracts, glaucoma, and myopic retinopathy, which often can be devastating and lead to blindness in the eyes.\nMyopia Control Prevents Risk of Ocular Disease\nMyopia is much more than the result of blurry images and the need for glasses or contact lenses. Myopic eyes are typically longer than non-myopic eyes, and this elongation leads to stretching of the retinal tissue inside the eyeball, which can lead to long term eye health issues:\n- Retinal Detachment\n- Macular Degeneration\nMyopia control/management is a safe, science-based, treatment plan that targets the progression of myopia and aims to slow down or stop the progression of myopia development. Myopia control utilizes custom contact retainer lenses, enhanced multifocal soft contact lenses, and eye drops to slow the abnormal elongation of the eye. Studies have shown that with myopia control we can stop progression by more than 50% in developing eyes.\nMyopia Control Begins with Educating Parents\nWhat exactly is myopia? Myopia, also known as nearsightedness or short-sightedness, is the result of light (visual stimuli) entering the eye and instead of being focused directly on the retina (the inner layer of the eye responsible for processing vision), it is focused in front of the retina. This defocus is what leads to blurry vision, similar to a camera lens being out of focus and producing a blurry picture. It is for this reason, it became known as “short-sightedness” or nearsightedness because the light entering the eye is “short” or “near” of the retina and not directly focused on it. Only when light is focused precisely on the retina, will images be processed in a clear and focused manner. Luckily, spectacle lenses (i.e. glasses) or contact lenses can refocus light on to the retina and produce a clear image for us to enjoy clear vision all day long. Unfortunately, this only acts as a visual aid and does not address the physical condition of the eye. Myopic eyes are longer than non-myopic eyes, and this elongation leads to stretching of the retinal tissue inside the eyeball, which can lead to long term eye health issues, including blindness. It is precisely for this reason that myopia is viewed as a medical condition with risks and consequences if untreated.\nWhat causes myopia? There has been extensive research over the last 100 years to determine what causes myopia, and to this day scientists and researchers do not fully understand the root cause of myopia. There are, however, several risk factors that put us at risk for developing myopia. The most accepted risk factors found in research are genetics and near activities. Say, for example, both of your parents have myopia, the chances of you having myopia are much greater (45% prevalence versus 7.3% when no parents have myopia). The other risk factor, near activities, has been implicated as a risk factor due to the visual strain on our eyes when focusing at a close distance. This visual strain is thought to send signals for the eye to grow and elongate in length, thereby increasing myopia. Many studies have demonstrated a direct correlation between myopia and time spent outdoors, and consequently, they found that children who spend more time outdoors, are less likely to be myopic.\nHow common is myopia? Current studies show that the global prevalence of myopia is 30%, but there is an alarming trend and it is predicted that by 2050, nearly 50% of the population, or 5 billion people will be myopic. It precisely for this reason that we now have a Myopia Epidemic. The National Eye Institute warns that by 2020, 39 million Americans will be nearsighted and that the figure will grow to 44.5 million by 2050. The myopia epidemic begins with children and teenagers so that some call it the childhood myopia epidemic.\nWhat can we do to stop this epidemic and limit the devastating effects myopia has on the health of our eyes? Well for starters, we have to catch myopia early if we want to make an impact, as there is no medical or scientific method to reverse myopia. Myopia control/management is a safe, science-based, treatment plan that targets the progression of myopia and aims to slow down or stop the progression of myopia. Myopia control utilizes custom contact retainer lenses, enhanced multifocal soft contact lenses, and eye drops to slow the abnormal elongation of the eye. Studies have shown that with myopia control we can stop progression by more than 50% in developing eyes.\nThe Myopia control process is a customized program to slow down or stop your child’s vision from getting worse. The Myopia control process can utilize customized lens retainers (Ortho-K), specialty soft contact lenses, and atropine eye drops.\nHaving myopia is not only a matter of inconvenience, but it puts you at greater risk for future medical eye diseases such as retinal detachments, cataracts, glaucoma, and myopic retinopathy.\nTake the proactive approach of ensuring healthy eyes for your child’s future and contact the Myopia Center of Maryland in Baltimore. After all, our whole world is what we see, and only through maintaining healthy eyes, will we be able to enjoy all the beautiful things in life.']"	['<urn:uuid:3e404391-d6c9-4f72-b24b-70f47c43c3fe>', '<urn:uuid:3c932693-e803-4063-869b-1eecfecc1c5a>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:10:11.825225	5	100	1629
70	What threatens marine life in Antarctica and Scotland?	Both regions are facing threats from warming waters and plastic pollution. In Antarctica, warm water channels are causing glacial melting through deep seafloor channels up to 600m deep, while in Scotland's only inshore deep-water coral reef, scientists have discovered plastic fragments and fishing equipment inside marine creatures like starfish and sea worms.	"['From BBC by Jonathan Amos\nScientists may just have identified Thwaites Glacier\'s Achilles heel.\nThis Antarctic colossus is melting at a rapid rate, dumping billions of tonnes of ice in the ocean every year and pushing up global sea-levels.\nNow, a UK-US team has surveyed the deep seafloor channels in front of the glacier that almost certainly provide the access for warm water to infiltrate and attack Thwaites\' underside.\nOur chief environment correspondent Justin Rowlatt visited Thwaites in January\nIt\'s information that will be used to try to predict the ice stream\'s future.\n""These channels had not been mapped before in this kind of detail, and what we\'ve discovered is that they\'re actually much bigger than anyone thought - up to 600m deep.\nThink of six football pitches back to back,"" said Dr Kelly Hogan from the British Antarctic Survey (BAS).\n""And because they are so deep, and so wide - this allows a lot more water to get at, and melt, Thwaites\' floating front as well as its ice that rests on the seabed,"" she told BBC News.\nWhy is Thwaites Glacier so important?\nFlowing off the west of the Antarctic continent, Thwaites is almost as big as Great Britain.\nIt\'s a majestic sight, with its buoyant front, or ""ice shelf"", pushing far out to sea and kicking off huge icebergs.\nBut satellite monitoring indicates this glacier is melting at an accelerating rate.\nIn the 1990s it was losing just over 10 billion tonnes of ice a year. Today, it\'s more like 80 billion tonnes. The cause of the melting is thought to be the influx of relatively warm bottom-water drawn in from the wider ocean.\nCurrently, Thwaites\' ice loss contributes approximately 4% to the annual rise in global sea-levels, with the potential to add 65cm in total should the whole glacier collapse.\nNo-one thinks this will happen in the short-to-medium term, but Thwaites is considered particularly vulnerable in a warming world, and scientists would like to know precisely how fast any changes might occur.\nDr Kelly Hogan explains the significance of the new research\nWhat does the latest research show?\nThe UK and the US joined forces in 2019 to investigate Thwaites.\nTheir scientists sailed a ship equipped with an echo-sounder right up to the glacier\'s ice cliffs, to trace the shape of the seabed below.\nA plane was also flown back and forth across the shelf to measure small variations in the pull of gravity.\nThese deviations reflected the seafloor\'s undulations beneath the shelf.\nThe two datasets taken together now provide the best view yet of Thwaites\' underlying topography.\nThey trace the path of a network of deep channels that cut through a ridge before joining up to form a major cavity under the ice shelf.\n""The connected channels that we\'ve mapped in detail for the first time are the potential pathways for deep-ocean warm water to get in and do damage at that point where the glacier is still grounded on the seabed, where it begins to lift up and float,"" explained BAS colleague Dr Tom Jordan, ""but also to melt the base of the ice shelf, which if you weaken will make the ice further upstream in the glacier flow faster.""\nHow will the new survey information be used?\nScientists need real-world data to corral their models so that when they run simulations of possible future behaviour, they get realistic outcomes.\nThe new information refines the volumes for ingressing warm water that can be considered possible under different scenarios.\nIn conducting their survey, scientists also now have a better idea of the general roughness of the seafloor.\nThis tells them about the sorts of speeds ice further back in the glacier can achieve as it slides across rock and sediment.\nWhat the researchers have produced, if you like, is a kind of ""stickiness index"" to additionally constrain the computer models.\nImage copyright Alx Mazur\nThwaites\' size and melt rate have led to it being dubbed the ""Doomsday Glacier""\nWhat\'s likely to happen in the near future?\nAt the moment, the eastern side of the ice shelf is hooked on to a large ridge, which gives it stability. But the current melting trend would suggest this situation won\'t last much longer, says BAS\'s Dr Robert Larter.\n""When the Eastern Ice Shelf becomes unpinned, the ice will spread out and thin, eventually breaking up, as we can see is happening right now on the (central) glacier tongue,"" he told BBC News.\n""Even before ice shelf break-up, the unpinning and thinning will reduce the buttressing effect of the ice shelf on the glacier upstream of it, resulting in increased ice flow velocity. This in turn will further accelerate thinning of the glacier and grounding line retreat.""\nChannels carved into the seafloor, extending several kilometers wide and hundreds of meters deep, may act as pathways (red line with yellow arrows as seen in this 3-D illustration) to bring relatively warm ocean waters to the edges of vulnerable Thwaites Glacier, hastening its melting.\nInt. Thwaites Glacier collaboration\nBritish and American scientists have had to temporarily suspend their investigations at Thwaites because of the Covid-19 crisis.\nTeams were due to head back to the glacier this austral summer, but the location\'s remoteness means the risks are too great should anyone fall ill.\nOnce the coronavirus outbreak has been properly contained, the scientists will return, however.\n""It\'s amazing to go to a place like Thwaites to see the changes taking place right before your eyes,"" said Dr Hogan.\n""When we were there in 2019, we were able to get right up to the ice shelf cliffs, and the reason we could do that and make our observations was because the icebergs and sea-ice that have always been there historically are starting to disappear.""\nThe latest research is published in two papers in the journal The Cryosphere, and can be accessed here and here.\n- The independant : ‘Doomsday glacier’ in Antarctica melting due to warm water channels under surface, scientists discover\n- The Guardian : Huge cavities threaten glacier larger than Great Britain\n- CNN : Antarctica\'s colossal Thwaites Glacier is melting fast -- and ...\n- Science News : New maps show how warm water may reach Thwaites Glacier’s icy underbelly\n- BBC : Thwaites: Journey to the \'doomsday glacier\' / Greenland and Antarctica ice loss accelerating / Deepest point on land found in Antarctica / Satellites record history of Antarctic melting / Coronavirus severely restricts Antarctic science / \'Stunning\' seafloor ridges record Antarctic retreat\n- GeoGarage blog : A quarter of glacier ice in West Antarctica is ... / Unprecedented data confirms that ... / The race to understand Antarctica\'s most ... /\nMajor expedition targets Thwaites Glacier / Thwaites Glacier: Biggest ever Antarctic field ... / A submarine goes under a failing glacier to ... / Thwaites Glacier: Biggest ever Antarctic field ...', ""One of Antarctica's fastest-shrinking glaciers just lost an iceberg twice the size of Washington, D.C. Pine Island Glacier, one of the fastest-shrinking glaciers in Antarctica, has just lost another huge chunk of ice to the sea, continuing a troubling trend that has become a near-annual occurrence in the last decade.\nScientists at Copernicus, the European Union’s Earth observation program, have been closely monitoring the glacier since large cracks appeared near its edge in October 2019. Yesterday, those cracks finally cut a chunk of the glacier away (a process known as calving), releasing a giant jigsaw puzzle of fresh icebergs into the nearby Amundsen Sea. In total, the icebergs measure about twice the size of Washington, D.C., in area (more than 130 square miles, or 350 square kilometers), according to The Washington Post. Plastic straws will be banned in the UK from April 2020. □□□□□ □□□□□□□ on Instagram: “We really need to stop using up trees unnecessarily - not only does it cause soil erosion and increase flooding, it also hampers our…” Climate change: UK 'can cut emissions to nearly zero' by 2050. Image copyright PA The UK should lead the global fight against climate change by cutting greenhouse gases to nearly zero by 2050, a report says.\nThe Committee on Climate Change (CCC) maintains this can be done at no added cost from previous estimates. Its report says that if other countries follow the UK, there’s a 50-50 chance of staying below the recommended 1.5C temperature rise by 2100. A 1.5C rise is considered the threshold for dangerous climate change. Some say the proposed 2050 target for near-zero emissions is too soft, but others will fear the goal could damage the UK's economy.\nThe CCC - the independent adviser to government on climate change - said it would not be able to hit “net zero“ emissions any sooner, but 2050 was still an extremely significant goal. The main author Chris Stark told me: “This report would have been absolutely inconceivable just a few years ago. Inside the climate change lawsuit pitting Big Oil against San Francisco and Oakland. Fracking commissioner resigns after six months. The UK's shale gas commissioner is resigning after just six months, saying fracking is being throttled by rules preventing mini earthquakes.\nGlobal carbon emissions from energy hit a record high in 2018. Plastic pollution found in creatures at inshore coral reef. Fragments of fishing equipment and microplastics have been found inside animals living on Scotland's only inshore deep-water coral reef.\nScientists discovered tiny plastic fibres inside starfish and sea worms at the remote Mingulay Reef Complex off the west coast. The find at the designated marine protected area highlights how widespread ocean littering has become, according to the team from the University of Edinburgh. When coral reefs change, researchers and local fishing communities see different results. Discovery Groups perceive environmental disruptions in varying ways March 7, 2019 Find related stories on NSF's Environmental Research and Education (ERE) programs at this link; find related stories on NSF's Long-Term Ecological Research Program at this link.\nResults of a new study looking at coral reef disturbances, fish abundance and coastal fishers' catches suggest that ecologists and community anglers may perceive environmental disruptions in very different ways. The apparent disconnect between data-driven scientists and experience-driven fishing communities has implications for the management and resilience of coral reefs and other sensitive marine ecosystems. The findings are published this week in the journal Ambio. Climate change is killing off Earth’s little creatures. Climate change gets blamed for a lot of things these days: inundating small islands, fueling catastrophic fires, amping-up hurricanes and smashing Arctic sea ice.\nBut a global review of insect research has found another casualty: 40% of insect species are declining and a third are endangered. It confirms what many have been suspecting: in Australia and around the world, arthropods – which include insects, spiders, centipedes and the like — appear to be in trouble. Read more: Curious Kids: why do spiders have hairy legs? The global review comes hard on the heels of research published in the Proceedings of the National Academy of Sciences USA that suggests a potent link between intensifying heat waves and stunning declines in the abundance of arthropods. If that study’s findings are broadly valid – something still far from certain – it has chilling implications for global biodiversity. Earth Once Swallowed Its Own Superocean. Could It Happen Again? The ancient supercontinent of Rodinia turned inside out as the Earth swallowed its own ocean some 700 million years ago, new research suggests.\nRodinia was a supercontinent that preceded the more famous Pangea, which existed between 320 million and 170 million years ago. In a new study, scientists led by Zheng-Xiang Li of Curtin University in Perth, Australia, argue that supercontinents and their superoceans form and break up in alternating cycles that sometimes preserve the ocean crust and sometimes recycle it back into Earth's interior.\n“Bright words:” finding common ground in environmental negotiations. We cannot create what we cannot imagine, and to imagine we need stories and words to tell them.\nCredit: Flickr/US Fish and Wildlife Service Northeast Region.""]"	['<urn:uuid:aa133ccd-6617-4bae-88b9-752d16a193ef>', '<urn:uuid:f25aa7cd-5b17-4811-bfe2-3bd44b901ac1>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T19:10:11.825225	8	52	1997
71	hi as an art fan what does ancient religious art tell us about common life across cultures and what techniques do experts use today to preserve religious artifacts	Ancient religious art reveals common aspects of daily life across cultures - as noted by the Metropolitan Museum curator, all cultures share basic needs like eating and having bowls to eat from. Religious art shows how different faiths influenced and borrowed from each other, like in Spain where both Muslims and Jews used similar geometric designs and micro-writing techniques in their manuscripts. As for preservation, modern experts use careful restoration techniques - for example, the Royal Tapestry Factory in Madrid conducts thorough cleaning and repair of historic tapestries to halt deterioration and revive colors concealed under accumulated smoke. Similarly, codices undergo individualized treatments based on careful analysis of their condition, with experts addressing issues like damaged covers, deteriorated paper, and bookworm damage.	['In This Episode << SLIDE LEFT TO SEE ADDITIONAL SEGMENTS\nDR. SHEILA CANBY (Curator, Metropolitan Museum of Art, Department of Islamic Art): Overall, the collection has twelve thousand objects; and we are showing twelve hundred.\nThere are motifs that you find across a very wide geographic spread, and that wouldn’t have happened if those regions hadn’t been unified by a single religion, being Islam.\nThe use of the Arabic script, of course, spread, as the religion spread. They reverse it, they do mirror writing, tiny writing, huge writing.\nThe written word in Islam is of absolute paramount importance. And the act of copying a Quran is an act of devotion, religious devotion.\nA mihrab is, of course, the central focus in a mosque. It’s what people face when they pray and in a mosque would be lined up so that the people facing it are facing the direction of Mecca.\nWe have glass mosque lamps. We have one or two ceramic ones as well. They were probably made in sets to be used in mausoleums and madrassas and mosques.\nOur newest gallery is our Moroccan court, which was built here over a six-month period by a group of craftsmen from Fez. What it is is an adaptation of the type of courtyard that one finds in several madrassas, religious schools or seminaries, in Fez. But, you know, our court is just tiny by comparison to those, so the challenge really was that we had to design it in such a way that they could kind of shrink but keep the proportions right. The tile panels are actually inspired by a tile panel in Alhambra.\nOne of the stories we wanted to tell, really, was about the complexity of society in Spain while it was still under Muslim control, and so we have actually two Hebrew manuscripts. The Hebrew Bible is fascinating because it has a page with what looks like geometric designs. But then if you look closely you realize that all these geometric designs are made from micro-writing, and in the same case we have pages from a Quran that was written in micro-writing. So not only were the geometric designs being shared and used by people of different faiths, but also the whole idea of this tiny writing seems to have appealed to both Muslims and to Jews in Spain.\nWe have a recent acquisition, actually, which is a painting that depicts the goddess Bhairavi Devi, and she sits with the god Shiva in a sort of charnel ground. It’s a Hindu subject; it’s a ferocious Hindu subject, in fact, and it has this goddess whose eyes are just drilling the viewer. So it just shows, especially in a place like India, what a completely complex society in terms of religion it was at the time.\nMuhammad is depicted in certain contexts. There were illustrated histories which show the life of Muhammad. Then in the poetic context, really in mystical poetry, we find depictions of this Mirage, the Night Journey, and he’s riding on a human-headed horse up to heaven. These were images that were painted by Muslim painters for Muslim patrons. So it was completely within a Muslim context that they were done. There was nothing untoward at all about them.\nWhat I would hope is that people would understand that although the religion infuses all of these lands and these historical periods that regions were individual, and regions had particular styles. And also the commonality with mankind, which is that we all have—we all eat and have bowls to eat from, we all, you know—there are so many things that are common to all of us, and to think of things in that way, I think, humanizes the religion and humanizes the objects to people who are not familiar with it.', 'As part of our commitment to preserving the art, culture and history of our heritage, we carry out multiple restoration and conservation projects in collaboration with different organisations. These include:\nOne of the most valuable treasures held by the Royal College of the Patriarch is a series of six large Flemish tapestries, donated by its founder, Juan de Ribera, four of which are displayed hanging on the walls of the Monument Chapel (also known, for this reason, as the Tapestries Chapel). The panels date from the early 16th century, the period when Flemish tapestry production reached its greatest splendour.\nTheir precarious state of conservation required urgent action to restore them, as they had lost colour and deteriorated through the passing of time.\nWork started in early 2013 to restore, clean and consolidate the tapestries, a task that was entrusted to the Royal Tapestry Factory in Madrid.\n- After removing the linings and the numerous darns all over the tapestries, the restorers embarked on a new phase consisting of washing four of the six panels and dying the supports to cover the existing gaps.\n- Once prepared, the next phase was to place four of the tapestries on a loom to stabilise them (La Gracia Pública de los Honores, El Pago del Denario, La Ira y la Pereza and Contratación de los Jornaleros) before starting the actual restoration.\n- Restoration of the fifth tapestry in the series, El juicio de Salomón, took place in 2016, while the sixth and last of the tapestries that make up the collection, Exhortación a las Virtudes was restored in 2017.\nCleaning the tapestries is not only a matter of aesthetics, it is an essential task to halt their deterioration and clean up the fabric. In addition, removing materials from prior restorations will make it easier to conserve the tapestries in the future. This work will revive the colours that were concealed under a thick layer of accumulated smoke, while the elimination of the darns and the subsequent repairs to the cloth will consolidate the structural recovery of the tapestries and the visual reintegration of the decorative elements.\nThe restoration work, completed in 2017, was carried out with the utmost care and rigour and the entire process was documented step by step.\nThe restoration of the Flemish tapestries in the Royal College of the Patriarch was carried out with our sponsorship at the Royal Tapestry Factory in Madrid.\nIn 2018, we renewed our collaboration with the Fundación San Millán to continue working to restore the collection belonging to the Yuso Monastery Library.\nNearly 400 volumes have now been restored since the Fundación San Millán de la Cogolla began the enormous task of restoring the Library in 2003. After receiving the consent of the Augustinian Friars, who own the Library, and the La Rioja Historical-Artistic Heritage Commission, this new agreement provides for the restoration of seven codices from the 15th, 16th and 17th centuries that require urgent attention due to their value and delicate state of conservation.\nThe codices were chosen by Father Pedro Merino and Ana Jessen, in whose workshop the restoration will take place. The state of conservation is very varied:\n- Many have damaged covers resulting in a loss of integrity; this damage is due to their use, mishandling, the dehydration of the leather or parchment, general dirtiness and stains of unknown origin.\n- The paper had also deteriorated through use giving rise to general dirtiness, damp stains, tears, warping and damage from bookworms.\nWe have collaborated with the Fundación San Millán de la Cogolla for several years, during which time we have also participated in the restoration of the latticework around the high altar at Yuso Monastery and in refurbishing the Exhibition Hall. We also entered into another collaboration agreement for the interior and exterior lighting at the Monastery.\n- The aim of the Fundación San Millán de la Cogolla is to protect and care for the natural environment in an area in San Millán de la Cogolla that has been declared a World Heritage Site. The foundation researches, documents and is a source for information on the origins of the Castilian language. It uses the latest technologies to provide information and update the Spanish language throughout the world. It also promotes the social, economic, cultural and tourist development of San Millán de la Cogolla and its surroundings.\n- The Suso and Yuso Monasteries in San Millán de la Cogolla, which were the cradle of the Spanish language, were declared a World Heritage Site by UNESCO on 4th December 1997, for artistic, religious, linguistic and literary reasons as they have one of the most important monastic libraries in the Hispanic world. A total of 18 codices have now been restored by the Workshop Training Centre run by restorer Ana Jessen in Madrid, focusing on those books that were in most desperate need of repair. Launched in late 2013, this restoration project has been undertaken in various phases, starting with an in-depth study into the condition of each codex to establish how best to treat each individual case based on the particular damage and its causes.\nTo restore the codices, their condition is carefully analysed and individual treatments are established.\nThe restoration of the altarpiece in Santa María and Todos los Santos Cathedral in Cuenca was finished in 2018. It has been returned to the cathedral and placed in a new location inside its chapel, due to the excellent outcome of the restoration process. The work was been carried out in collaboration with the Madrid School of Conservation and Restoration of Cultural Heritage.\nRamón Castresana (director of Fundación Iberdrola España) together with Venancio Rubio (the company’s institutional delegate in Castilla-La Mancha), Monsignor José María Yanguas (bishop of Cuenca), José Antonio Fernández (dean president of the chapter), Ruth Viñas (director of the Madrid School of Conservation and Restoration of Cultural Heritage) and Luis Priego (professor and coordinator of the altarpiece restoration) met in the cathedral to admire the result of the restoration.\nThe restoration work on this Gothic piece from the end of the 15th century consisted of dismantling the piece and transferring it to the School’s facilities in Madrid. There, students from the fourth year of the degree course had the opportunity to participate in the restoration process. Exceptional results were obtained in returning all the parts to their original condition.\nA total of 10 students from the Sculptural Conservation-Restoration course participated in the restoration process, which began in September 2017 and ended on schedule. The restoration of this piece was the second collaboration between the Fundación Iberdrola España and the School of Conservation and Restoration of Cultural Heritage after the one carried out between 2015 and 2016 on the Virgen del Rosario altarpiece in the church of Navamorales in Salamanca.\nThe project was made possible thanks to the invaluable support from Cuenca Cathedral and its Chapter House, which from the outset collaborated in every way possible with the School of Restoration and Iberdrola for the conservation of its heritage.']	['<urn:uuid:5deb2913-3da1-46f3-ba40-a36f516f1bb8>', '<urn:uuid:4a7a8142-fd32-4ce3-9aac-e6d211c0acad>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:10:11.825225	28	122	1801
72	I'm planning a summer trip to Kenai Fjords - what's the typical temperature there?	During summer months in Kenai Fjords National Park, the temperature typically ranges from 40 to 60 degrees, and there can still be freezing weather during those months.	['KENAI FJORDS NATIONAL PARK\nby Kelsey Garner, 2004\nA glacier within the park.from http://www.nps.gov.\nKenai Fjords National Park was proclaimed a national monument on December 1, 1978 and established as a national park December 2, 1980 in Seward, Alaska on the Kenai Peninsula. The Harding Ice sheet (the largest ice sheet within the United States), 38 other glacial systems, and onshore and offshore wildlife (Orcas, otters, puffins, bear, moose and mountain goats) were reason to create this park. Kenai Fjords is only 80 miles from Anchorage, Alaska, but the park is still very isolated and hard to travel to (Harris and Tuttle). The park’s area is 669,983 acres, and approximately 1,047 square miles of mainly untouched land. The park receives around 250,000 vistors per year (nps.gov).\nHarding Ice Sheet from: http://community.webshots.com/album/89701015CLzzgD.\nThe Kenai Fjords are lithologically and structurally part of the Kenai-Chugach\nMountains. The range extends eastward along the Kenai Peninsula along the Gulf\nof Alaska. At the bottom of the ocean, there is evidence of glaciers because\nof the 300-foot deep contour that is associated with glacial deposits left during\nan advance of the glacier before the Holocene sea level rise. At one time, the\nglaciers were over 3000 feet thick in many of the valleys. Very few places were\nunaffected by the glacial events. The last major glacial event in Alaska was\n14,000-10,000 years ago (Harris and Tuttle).\nThe weather has warmed very little since the ice age and there is still 200 inches of precipitation per year including 500-1000 inches of snow before it is compacted. “Two to four times as much precipitation falls as snow…than as rain…” (Harris and Tuttle). The temperature ranges from 40-60 degrees in the summer and there still can be freezing weather during those months.\nExit Glacier from: http://community.webshots.com/album/89701015CLzzgD.\nGeologic Background (based on Harris and Tuttle, 1990)\nMany gaps exist in the rock record at Kenai Fjords, partially because there are few outcrops because of the ice sheets. The oldest rocks are from the Triassic to Cretaceous, which are named the McHugh Complex. That rock unit is composed of a mélange of siltstone, sandstone, conglomerate, tuff, pillow basalt, chert, limestone, and argillite. On top of the McHugh Complex is the Valdez Group, which is the most predominant rock group in the park, which is from the Upper Cretaceous. It is composed of dark colored metasedimentary flysch, sandstone, siltstone, and conglomerate. There are no fossils within this group, so it was hard to date this series (Lexicon). Both the McHugh Complex and the Valdez Group are part of the belt of rocks that extend the coast of the Gulf of Alaska all the way to the Shumagin Island in the Aleutians. Next in the sequence is the Orca group from the Eocene. This rock unit consists of dark gray flysch rocks and turbidites. There is not apparent break between the Valdez and Orca groups. Scientists think that they originated from a similar source. In addition, within the Eocene epoch, granitic batholiths, small plutons and dikes were formed. The granite outcrops from the batholiths underneath Harding Icefield are seen in glacial troughs and cirques on the side of the mountains. Following the Orca Group is Granitic Intrusions in the Oligocene. There is a major gap between the Eocene and the Pliocene. Once you reach the Quaternary all you find is alluvium and glacial drift. All of the rock units are in my stratigraphic column in MS Word or PDF format and part o\nf the Geologic Map (by Bradley and Donley) of Kenai Fjords.\nUPDATE: Here is the new column in MS Word. Here is the new map of the area in PDF format. Here is a PowerPoint poster that was presented at the GSA meeting in November 2004.\n|Bradley, Dwight and Tom Donley. May 1995. Geological Map of Kenai Fjords National Park and Vicinity. U.S. Geological Survey|\n|Bradley, D.C, Kusky, T.M., Haeussler, P.J., Karl, S.M., and Dudley, D.T.;1999; Geologic Map of the Seldovia Quadrangle, South-Central Alaska: USGS Open File report OF 99-18, http://geopubs.wr.usgs.gov/open-file/of99-18/|\n|Harris, Ann G., and Tuttle, Esther. 1990. Geology of National Parks. 4th ed., Kendall/Hunt Publishing Co..|\n|Lexicon of Geologic Names of the United States for 1968-1975, Geologic Survey Bulletin, 1520|\n|Tysdal, R.G., and J.E. Case. Geologic Map of the Seward and Blying Sound Quadrangles, Alaska. U.S. Geol. Surv. Misc. Invest. Series Map I-1150, 1979, 12 pp.|']	['<urn:uuid:e9ee6eb5-4095-44f4-b535-66055714e9b6>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:10:11.825225	14	27	726
73	What are the key differences between the structured learning requirements for Six Sigma certification levels versus the principles of Continuing Professional Development?	Six Sigma certification has strict, hierarchical requirements where Green Belt certification is required before Black Belt, and Black Belt before Master Black Belt, with specific training days (2 days for Green Belt, 4 days for Black Belt). In contrast, CPD follows more flexible principles outlined by CIPD where individuals decide their own learning needs and control their development. CPD is viewed as a continuous process throughout working life, with learning integrated into work activities rather than following a predetermined structure. While Six Sigma focuses on technical proficiency, CPD encompasses broader professional competence and personal development.	"['What is it? The long established practice of continuing Professional Development (CPD) spans all professions and learning opportunities. Planning and recording of your continuing professional development should be something everybody does throughout their career.\nIn reality there are two distinct groups:\n- Individuals who have to demonstrate CPD to meet their industry standards and continue to practice.\n- Individuals who maintain their own CPD to ensure currency within their area of expertise.\n“Continuing Professional Development (CPD) is a process by which individuals take control of their own learning and development, by engaging in an on-going process of reflection and action. This process is empowering and exciting and can stimulate people to achieve their aspirations and move towards their dreams” (Megginson & Whitaker, 2010).\nLet’s think about a definition for CPD here are a few to consider :\n- CPD is defined as a commitment to structured skills enhancement and personal or professional competence.\n- CPD can also be defined as the conscious updating of professional knowledge and the improvement of professional competence throughout a person’s working life.\n“It is a commitment to being professional, keeping up to date and continuously seeking to improve. It is the key to optimising a person’s career opportunities, both today and for the future” (Chartered Institute of Personnel and Development, 2000). “\nThis is the skills and knowledge attained for personal development and career advancement. Professional development encompasses all types of facilitated learning opportunities, ranging from college degrees to formal coursework, conferences and informal learning opportunities situated in practice. It has been described as intensive and collaborative, ideally incorporating an evaluative stage. There are a variety of approaches to professional development, including consultation, coaching, community of practice, lesson study, mentoring, reflective supervision and technical assistance.\n“Continuing professional development means maintaining, improving and broadening relevant knowledge and skills in your subject specialism and your teaching and training, so that it has a positive impact on practice and the learner experience… It is the critical reflection on learning experiences and activities that improve practice and demonstrate continuing development as a teacher or trainer” Guidelines for Continuing Professional Development August 2009; Institute for Learning (IfL).\nThis definition covers three important principles of CPD for you to think about.\n- Professional updating – which includes your teaching and learning practice and subject specialism (IfL call this ‘dual professionalism’).\n- Reflective practice.\n- Impact on your learners and their learning.\nPrinciples of CPD\nOne of the key stakeholders who are also the guardians of the standards for professional development, the Chartered Institute for Personnel and Development, (CIPD) identify the key principles of CPD:\n- Professional development is a continuous process that applies throughout a practitioner’s working life.\n- Individuals are responsible for controlling and managing their own development\n- Individuals should decide for themselves what their learning needs are and how to fulfil them\n- Learning targets should be clearly articulated and should reflect the needs of employers and clients as well as the practitioner’s individual goals\n- Learning is most effective when it is acknowledged as an integral part of all work activity rather than an additional burden.\nLearning and the CPD cycle\nAs adults we participate in many different events and activities every day, but we do not necessarily learn anything from these experiences. We are often quite unaware, passive or behaving in habitual ways. We often only recall any exceptional or unusual events.\nThere are many techniques that have been identified for how we learn and the following model is one that supports the CPD cycle:\nSELF-ASSESS / PLAN & PRIORITISE / DO / REVIEW\nCurrent personal development is more than skills training. It offers useful alternative methods compared to coaching and mentoring too. Effective modern personal development now involves various integrated techniques, theories and behavioural concepts, that extend options around traditional ideas. Optimising individual performance through progressive personal development significantly improves business performance too.\nThe 21st century has seen a significant growth in on-line professional development. Content providers incorporate collaborative platforms such as discussion boards and wikis, thereby encouraging and facilitating interaction, and optimising training effectiveness.\nConsider the following two questions and write a few sentences on each to assist you with the foundation of your own CPD.\nQuestion 1: What is your own experience of continuing professional development?\nQuestion 2: What value do you currently place on it?', 'The Certified Lean Six Sigma Black Belt is a professional who is well versed in the Lean Six Sigma Methodology, who leads complex improvement projects, typically in a full-time capacity. A Lean Six Sigma Black Belt possesses a thorough understanding of all aspects of the Lean Six Sigma Method including a high-level of competence in the subject matters contained within the phases of Define, Measure, Analyze, Improve and Control (DMAIC) as defined by Lean Six Sigma Black Belt Body of Knowledg. A Lean Six Sigma Black Belt understands how to implement, perform, interpret and apply Lean Six Sigma at an advanced level of proficiency.\nSIX SIGMA BLACK BELT / LEAN SIX SIGMA BLACK BELT\nSix Sigma Black Belt\n- Six Sigma methodology and DMAIC process\n- Highlights role of Black Belt and other team members for successful implementation\n- Project selection and defining techniques for maximum impact\n- How to convert practical problem to statistical problem and derive statistical solution then convert it to a practical solution for implementation\n- Project management techniques – How to present your project, project closure and the key to a successful transition\n- How to use Minitab for statistical analysis\n- Demonstrate a mastery of Six Sigma for manufacturing processes\n- Review roles and responsibilities for process improvements\n- Refine project selection working with Six Sigma Champions\n- Identify necessary project management skills required to implement change in processes\n- Demonstrate comprehensive process mapping skills\n- Demonstrate comprehensive software skills in required applications\nThe course is designed for meeting the requirements of the following audience:\n- Quality Assurance Engineers, Project Managers, Team leaders, Software Professionals, Practitioners, Software Quality\n- Assurance team members and Senior Management\n- Management team of Organizations who intend to implement and practice Six Sigma\n- Those who want to get certified as Black Belt in Six Sigma\n- Future managers including management students\n- Any other professional members who are doing research, innovations or consulting in process improvement practices\nCandidates should be preferably Six Sigma Green Belt trained/certified.\nWhy choose SAI Learning & Consultancy Solutions?\nBest Price in the Industry\nYou won’t find better value in the marketplace. If you do find a lower price, we will beat it.\nWhat Is Six Sigma?\nSix Sigma is a rigorous and proven business methodology that uses data and statistical analysis to improve business performance. With the goal of increasing profits by eliminating mistakes, waste, and rework, Six Sigma provides a means to identify and prevent process variation-or defects-to improve predictability and success of business processes.\nWhy Six Sigma Certification?\nSix Sigma certification confirms a level of training, practice, and capability with respect to specific competencies. Though requirements for each Six Sigma certification level are different, each requires training, and each participant, called a ""Belt"", is required to have a leadership-approved project prior to Six Sigma certification training.\nWhat Are The Six Sigma Certifications?\nSix Sigma Green Belts have received two days of training on the Six Sigma road map and essential elements of statistical methodologies supporting Six Sigma projects. Green Belts allocate up to 50% of their time on Six Sigma projects, and Black Belts assist them with projects as needed.\nSix Sigma Black Belts are technical leaders who have received four days of training focusing on the Six Sigma road map and extensive statistical methodologies. Black Belts normally dedicate up to 75% of their time to Six Sigma projects, and they assist Green Belts as needed. Green Belt certification is required for Black Belt certification eligibility.\nSix Sigma Master Black Belts represent the highest level of technical and organizational proficiency. They have received six days of training on the Six Sigma methodology, and they\'ve learned the skills and tools required to teach Six Sigma philosophies and implement Six Sigma within an organization. Master Black Belts lead all levels of Six Sigma projects, and they help Black Belts apply methodology when necessary. Their jobs are completely devoted to Six Sigma. Black Belt certification is necessary for Master Black Belt certification eligibility.']"	['<urn:uuid:eb661a19-4617-40f3-8cbc-2b9de85f5e00>', '<urn:uuid:860ac3ed-b6b3-4287-a8a8-b5ecf368522f>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	22	95	1392
74	who discovered thc compound cannabis	THC was first discovered by Rafael Mechoulam, an Israeli chemist. He was also the first to introduce the term 'entourage effect' in a 1999 paper.	['Photography by Georgia Love for Herb\nCannabis is an infinitely complex plant. It contains more than 400 chemical compounds – only a fraction of which are known, while even fewer have been studied. These compounds give cannabis its flavor, aroma, and appearance. They determine the experience of each unique stain and when they come together, they create something known as the entourage effect.\nOften confused with marketing techniques used to sell cannabis for its strain-related effects, the entourage effect is rooted in the centuries-old practice of whole plant medicine. The basic concept is that the chemical compounds found in medicinal plants work together to help that plant offering healing. Some argue that consuming the whole plant is better for the patient than taking an extract of a single chemical from that same plant, as is standard practice in modern medicine.\nThe term entourage effect was first introduced into the cannabis lexicon in a paper published in 1999 by Rafael Mechoulam, the Israeli chemist who first discovered THC as the main psychoactive component of cannabis.\nIn his study, Mechoulam and fellow chemist Shimon Ben-Shabat examined the interactions between compounds in the cannabis plant, and found that certain cannabinoids, which had no effect on their own, could be used to help other cannabinoids in the plant to work more effectively.\nTo put it simply, when speaking with the Scientific American in 2017, Chris Emerson, a chemist for Level Brands, described the entourage effect as, “the sum of all the parts that leads to the magic or power of cannabis.”\nAt the most basic level, the benefits of the entourage effect can be understood in terms of the most well-known compounds: THC and CBD. These compounds have an inverse relationship, meaning that the higher the concentration of THC in a given strain, the lower the levels of CBD will be.\nBut THC and CBD also have another effect on each other which plays out after the cannabis plant is consumed. Where THC will cause a head high that may lead to feelings of anxiety and paranoia, CBD helps to mitigate that anxiety with its mellow body effects.\nThe reason for this lays in how these chemicals react on a molecular level. In a 2011 review published in the British Journal of Pharmacology, chemists found similar interactions between cannabinoids, like THC, and terpenes, the chemical components which give cannabis strains their aroma.\nResearchers discovered that a terpene, linalool, combined with a cannabinoid, in this case, CBD, could be used as an effective anti-anxiety medication. On the other hand, combining linalool with THC makes for a potent sedative, while alpha-pinene, another terpene, combined with THC helps to retain acetylcholine–a molecule which aids in memory retention– and could help to mitigate the short-term memory loss caused by THC.\nIn a sense, isolated cannabinoids can be thought of as medicine that has a list of fine-print side effects. These effects could be mitigated, or enhanced, when they are paired with other chemical compounds that naturally appear in whole-plant cannabis.\nWhile these studies show promise for the future of the entourage effect, far more research is needed before we can say anything for sure. Especially the kind of research that involves clinical trials–in which medicines are tested on human patients–something which researchers have been unable to do as a result of prohibition.\nThat lack of clinical trial research has given rise to critics of the entourage effect who claim that it does not exist. A study conducted in 2013 by neurobiologist Margaret Haney of Columbia University tested the effects of smoked marijuana with that of synthetic THC and found that there is no difference in the effects. The study found that, “under controlled conditions, marijuana and dronabinol decreased pain, with dronabinol producing longer-lasting decreases in pain sensitivity and lower ratings of abuse-related subjective effects than marijuana.”\nIt’s for this reason that researchers like Haney believe that the entourage effect is a myth, concocted by budtenders to sell more weed.\nStill, when patients are consulted about the type of medicine they prefer, they often choose whole-plant cannabis. In a study conducted by Dutch medical cannabis company Bedrocan BV in 2013 found that patients tend to prefer whole-plant cannabis to its pharmaceutical alternative. The survey asked 953 patients across 31 countries, though the company pointed out that too few patients had actually tried synthetic pharmaceutical cannabis.\nCBD and THC-only medicines are synthetic versions of the cannabinoid compounds found in the plant itself. They are an identical chemical match of the compounds found in cannabis, but have been isolated from the rest of the components that make up the whole-cannabis plant.\nPharmaceutical versions of THC can be found under the name Marinol (dronabinol) and Cesamet (nabilone), while the first medicine to include both THC and CBD, was recently approved by the FDA in 2018 as Sativex.\nThe reason that THC and CBD are isolated in these clinically approved medications is largely due to the regulatory standards that surround modern medicine. In essence, isolated compounds are much easier to dose, measure and track than a compound found naturally in a plant.\nMedicines which are approved by the FDA have to be standardized, meaning that every dose must contain an exact amount of THC, something which can be difficult to guarantee in smokable cannabis, which usually comes with a THC content range rather than an exact measurement.\nThe FDA also doesn’t like the uncertainty of having other chemical compounds, like terpenes, in their approved medicines without knowing exactly what effects they may have on the body. After all, one chemical is easier to keep track of than 400.\nA good example of a plant which has been reduced to a single compound can be seen in a revolutionary medicine that is now considered to be an ordinary pill: aspirin.\nWhen the main medicinal component of aspirin was discovered in the late 19th century, it was among the first of modern medicines to be isolated into its most basic medicinal ingredient. Felix Hoffmann, a chemist at the German pharma company Bayer, first synthesized what we know as aspirin in the form of acetylsalicylic acid to treat his father’s joint pain. But the word for aspirin comes from the Latin word Spiraea, which is a family of plants that naturally produce aspirin’s main ingredient which can be found in willow trees, jasmine, clover and a variety of other plants. According to Aspirin: The Remarkable Story of a Wonder Drug, ancient Egyptians used this medicine in the form of willow bark with similar effects that did not require distilling the plant into a pill.']	['<urn:uuid:97b8b45b-9e13-4de9-bc24-11273fd4981b>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:10:11.825225	5	25	1098
75	baby umbilical cord clamp timing symptoms infection	During birth, the cord is clamped after the baby starts breathing, usually 24-48 hours after birth when bleeding stops. The remaining stump falls off after 5-15 days. While some yellowing and odor during decay is normal, signs of dangerous infection include pus, skin redness, or tenderness around the cord site. Other infection symptoms can include fever above 100.4°F, changes in temperature, breathing problems, poor appetite, and drowsiness.	"[""Caring for Your Newborn's Umbilical CordDr. Karen Sadler\nUmbilical cords do more than mark the future site of a baby’s belly button. They are the unborn baby’s lifeline, providing a connection to the placenta, from which come the oxygen and nutrients needed to survive. Given that those unborn lungs lack access to air, and the gut to food, the essentials diffuse from mother’s circulation to the placenta and through the umbilical cord to the baby’s circulation.\nDuring birth, the baby emerges before the placenta is delivered, still connected to that placenta by its cord. In the first critical moments after birth, the cord may still pulsate, sending its last oxygen delivery while the baby gasps, opens it’s lungs to the outside, and begins to oxygenate itself. The cord is then clamped (and this can sometimes be done by family members), creating separate maternal and newborn circulations. The clamping is painless, as the cord lacks nerve fibers.\nMost placentas are discarded, though parents now have the option of collecting about three ounces (70 to 80 mls) of blood from this organ. Called cord blood, this material is rich in versatile stem cells and can be stored in special centers—deep frozen at negative 200 degrees Celsius—for future use.\nStem cells are currently being used to treat over 75 conditions, most of them relatively rare blood cancers and disorders. With today’s technology, the odds of any one child needing those stem cells before the age of 18 is 1 in 2,700, while the odds of using them for any family member is 1 in 400. The cost is steep: $1,000 to $1,500 for the initial collection and another $100 to $150 for yearly storage fee. Though still somewhat controversial, the future of stem cell use is very promising. Research is currently underway looking at this technology to treat illnesses such as diabetes, heart disease, certain kinds of arthritis, as well as spinal cord and nervous system injuries.\nCleaning Your Newborn’s Umbilical Cord\nAfter 24 to 48 hours, when the bleeding has stopped and the remaining cord begins to dry, the clamp is removed, leaving a few inches of yellowish jelly-like stump. This stump then falls off after five to 15 days (almost always by three weeks of age).\nTraditionally, rubbing alcohol has been recommended for cord care. A cotton-tipped swab dipped in alcohol or an alcohol pad is used to wipe the area at the base of the cord, where it inserts into the skin. (Wet, sticky material will come off on the swab; this is normal.) Forgoing alcohol and simply keeping the cord area clean and dry has also been suggested for cord care in recent years and has not resulted in more infections or problems. Parents now have both practices as options.\nNew parents are often squeamish about handling the cord, but needn’t be. Cords have no sensation, so as long as parents don’t tug on the cord or get too much alcohol on the surrounding skin, cord care is painless for the baby.\nUmbilical Cord Care Basics\nKeep your baby’s umbilical cord area as clean and dry as possible. Folding down the top of the diaper to leave the cord exposed is a good idea and will prevent urine from soaking the cord area. You can also buy special diapers with an indented top seam.\nIt is important to avoid binding the mid-abdominal area with a bandage or cloth, as is common in some cultures. This keeps moisture in and promotes the growth of germs. Very tight clothing can potentially do this as well, so be sure to dress your baby in loose clothes that won’t rub against the cord and potentially irritate the stump while it heals (onesies are fine as long as they are loosely worn.)\nAs the umbilical stump decays in preparation for detachment, the tissue begins to look yellow and may smell. This is OK. But, what isn’t OK and may signal an impending infection (which can get very serious very quickly) are the following signs: frank pus at the cord site, reddening of the skin around the insertion site, or obvious tenderness when the area is touched. Any of these signs demands an immediate call to the pediatrician.\nAfter the cord stump detaches (be sure to let it fall off on its own; pulling it off is not advised), the remaining tissue will still appear moist for a few more days. Alcohol wipes should be used until the area becomes completely dry.\nOn occasion, a small, wet, reddish mass persists after the cord falls off, which may or may not ooze a yellowish material. This is most likely a granuloma, which requires an application of a chemical called silver nitrate at the doctor’s office, but which almost always shrinks with this treatment.\nOnce the cord is off and the umbilical area looks dry and well-healed, special attention is no longer required. Now your baby can safely get into his infant tub for his first immersion bath and you can gently wash that cute little belly button."", ""Click a letter to see a list of conditions beginning with that letter.\nClick 'Topic Index' to return to the index for the current topic.\nClick 'Library Index' to return to the listing of all topics.\nSepsis in the Newborn\nWhat is newborn sepsis?\nNewborn sepsis is a severe infection in an infant younger than 28 days old. The infection is in your baby’s blood. But it may affect any body system or the whole body.\nWhat causes newborn sepsis?\nNewborn sepsis is most often caused by bacteria. But other germs can also cause it. A baby may become infected before birth if your amniotic fluid is infected. During delivery, the newborn may be exposed to germs in the birth canal. Once born, a baby may be exposed to germs in the hospital or at home.\nWhich babies are at risk for newborn sepsis?\nThese things make it more likely that your newborn will have sepsis:\nYour baby is born too early (premature). This means before 37 weeks of pregnancy.\nYour amniotic sac breaks (ruptures) more than 18 hours before birth.\nYou have the strep germ in your vagina or have symptoms of infected amniotic fluid, such as a fever.\nYour baby needs a medical procedure. This includes a urinary catheter or a central intravenous line.\nWhat are the symptoms of newborn sepsis?\nSepsis in newborns can be hard to identify. Symptoms depend on what’s causing the infection. They also depend on how severe the infection is and where it is. The newborn may have:\nSigns of distress during labor or delivery, such as a rapid heart rate\nChanges in temperature (often fever)\nBreathing problems, such as very fast breathing and grunting\nDigestive problems like poor appetite or an enlarged liver\nNervous system problems, such as drowsiness or trouble staying awake\nHow is newborn sepsis diagnosed?\nThe symptoms of sepsis are similar to some other health conditions. Your baby will need tests to make a sepsis diagnosis and to rule out other illnesses. These tests may include:\nBlood culture. This is done to check for bacteria in the blood. Results take a few days, but treatment will start right away. This is the main way sepsis is diagnosed.\nUrine culture. This checks for bacteria in the urinary system.\nOther cultures. This checks for bacteria in other places, such as in a wound.\nBlood tests. These check for signs of infection and for possible effects of sepsis on the kidneys, liver, and blood cells.\nLumbar puncture. This is done to check for infection of the brain and spinal cord (meningitis). A small amount of cerebrospinal fluid is tested.\nX-rays or other imaging tests. For example, a chest X-ray is used to check for a lung infection.\nHow is newborn sepsis treated?\nTreatment will depend on your baby’s symptoms, age, and general health. It will also depend on how severe the condition is.\nEarly diagnosis and treatment is the best way to stop sepsis. If your baby’s healthcare provider thinks it may be sepsis, your baby will get antibiotics right away, even before test results are available. Once the provider has the test results, they may change the treatment.\nA newborn with sepsis may be very ill. The baby will need to stay in the newborn intensive care unit (NICU). In the NICU, your baby will be watched very closely. In addition to the antibiotics, they will get fluids, other medicines, oxygen, nutrition, and help with breathing, if needed.\nWhat are possible complications of newborn sepsis?\nSepsis is life-threatening for newborns. It can affect any body system. It often affects more than one system at the same time.\nCan newborn sepsis be prevented?\nNewborn sepsis can’t be completely prevented. But you can lower the risk. Regular prenatal care can find and treat many problems that put a newborn at risk for sepsis. One example of this is that all pregnant women are screened for group B strep infection and treated if they have it.\nHospitals and other facilities have practices in place to prevent the spread of infection. These include:\nWhen should I call my baby’s healthcare provider?\nCall your baby’s healthcare provider right away if your newborn has:\nA fever above 100.4°F (38.0°C) rectally\nA fever lasting more than 1 day\nA change in behavior, such as being very sleepy, fussy, or not eating well\nKey points about newborn sepsis\nNewborn sepsis is a severe infection in an infant younger than 28 days old.\nA newborn may become infected before, during, or after birth.\nNewborn sepsis can be hard to diagnose.\nEarly diagnosis and treatment are the best ways to stop sepsis.\nAntibiotic medicine is started as soon as possible.\nTips to help you get the most from a visit to your child’s healthcare provider:\nKnow the reason for the visit and what you want to happen.\nBefore your visit, write down questions you want answered.\nAt the visit, write down the name of a new diagnosis and any new medicines, treatments, or tests. Also write down any new instructions your provider gives you for your child.\nKnow why a new medicine or treatment is prescribed and how it will help your child. Also know what the side effects are.\nAsk if your child’s condition can be treated in other ways.\nKnow why a test or procedure is recommended and what the results could mean.\nKnow what to expect if your child does not take the medicine or have the test or procedure.\nIf your child has a follow-up appointment, write down the date, time, and purpose for that visit.\nKnow how you can contact your child’s provider after office hours. This is important if your child becomes ill and you have questions or need advice.\nOnline Medical Reviewer:\nBarry Zingman MD\nOnline Medical Reviewer:\nL Renee Watson MSN RN\nOnline Medical Reviewer:\nRaymond Turley Jr PA-C\nDate Last Reviewed:\n© 2000-2023 The StayWell Company, LLC. All rights reserved. This information is not intended as a substitute for professional medical care. Always follow your healthcare professional's instructions.""]"	['<urn:uuid:cfa4a6f0-ebf7-40b7-a174-1ec3536d319d>', '<urn:uuid:91ea9440-06e4-46e2-919b-3dca47e5ee0c>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:10:11.825225	7	67	1847
76	what components scan interpret process information human mind	According to the text, the human mind has several components: chitta functions as a scanner that scans external inputs and bodily occurrences, manas acts as an interpreter that translates the language of chitta into a form that buddhi can work with, and ahamkara can be seen as the programmer.	['If you want to use Nirvana Shatkam, it might be more productive to choose translations which do the job that the text can. Because of that, a ‘faithful’ translation of Sanskrit into English is not the primary requirement; the process and the goal should determine how the words are best understood. With this in mind, some suggestions regarding the translation of the Sanskrit verses.\nIt is important to notice that the words in these verses refer to components (physical, physiological, psychological, etc.). Therefore, assume that human ‘mind’ (or human psychology) is referred to (or spoken of) in modular terms.\n- The first line of the first verse picks out four such modules. If chitta is read as ‘focus’ or ‘attention’, it is best to understand its function or role in terms of a ‘scanner’. This module or component scans what comes in from the outside as well as what occurs in the body. The scanned results are transmitted to ‘manas’. Imagine it as the module that ‘interprets’: it translates the ‘language’ of chitta into a ‘language’ that ‘buddhi’ can work with. (In very, very crude programming terms, ‘manas’ interprets inputs from chitta and outputs them in a language that ‘buddhi’ can compile. This analogy is very crude because you can also see ‘manas’ as the compiler and the ‘buddhi’ as the interpreter or even reject this functional analogy. Use it only if the analogy is required or is helpful. Do not take this as a true description of their functions or their nature.) If this crude terminology is retained, ‘Ahamkara’ identifies the ‘programmer’: he is inside the person or in the body, he is the person or the body and so on. The ‘I’ (‘aham’) is none of these modules (or components). The first line suggests that the sense of self that we have (the feeling of ‘I’), is not the sensing or experiencing of any component or object or module existing in the human mind. The verse elaborates this idea further in the next two lines: neither is it a component of the body, nor is it any of the ‘basic’ elements of the physical world. (Many cultures and people believed that earth, water, sky, fire and air were the fundamental elements that made up the world. Retain this understanding to translate ‘pancha bhuta’.) In very simple terms: the ‘I’ that we all sense or experience is not an object either in the human mind, or in the body or in the world. The subsequent verses carry these thoughts further.\n- None of the different components of the body (the ‘animate’, different ‘gases’, different organs) is the ‘I’.\n- This ‘I’ does not have (or possess) properties (whatever their nature) like hatred, greed, etc. Nor does it have goals: ‘being ethical’, ‘being rich’, ‘satisfying desires’ or ‘to be liberated’.\n- The ‘I’ is not something that is bound by things that circumscribe, like ethical or legal consequences or rituals etc. It is neither the subject nor the object nor the process of ‘consumption’.\n- The ‘I’ is not bound by ‘death’ that limits all organisms, or by ‘relations’ (whether social, familial or cognitive) that chart our existence.\n- The ‘I’ does not have properties which objects (events, processes) in the world have (like growth, development, transformation, etc.); it is the ‘vibhu’ everywhere and of the ‘sense organs’. (‘Vibhu’ gets translated as ‘master’, ‘controller’, ‘lord’ etc. If you keep this notion, it has to be divested of its most crucial element, namely, ‘vibhu’ as an expression of ‘power’, or as a ‘controlling force’. You need to understand the ‘controller’ here as something that does not and cannot ‘control’ anything or exercise any ‘power’ over anything, including itself.) It is neither attached nor does it require to be freed.\nThese rough translations are enough to think through the basic thought: the ‘I’ is not something with properties which we ascribe to objects (events, processes) in the world. It is not an object in the physical, physiological, psychological, social, cognitive and the ethical world we inhabit. It does not possess (or have) any property that an existing entity in the world has (or should have). Yet, we sense or experience this ‘I’ in the world. Now, questions begin to emerge: Could it be an imaginary entity? But such entities exist, even if they do so as imaginary entities. Perhaps, it is a hallucination. In that case, are all human beings hallucinating all the time? On the other hand, if we ‘sense’ or ‘experience’ this ‘I’ in the world and are not hallucinating, what is this ‘I’? If it is not any kind of an object existing in the world, what do we sense or experience? If it is not an object (event, process) in the world, how could it be ‘everywhere’? If we can only access things that are there in the world, how could we ever access or experience ‘it’, when ‘it’ does not exist? And so on. What is this ‘it’, ‘the I’? In short, who am I?\n‘Thinking through’ merely means this: take components, whatever they are, whether anger or money, fame or house, father or your kidneys and ask, is this me? Would I not be me if I lose this? Add as many new elements as you want: this Nike shoe, that IPhone 7, this beautiful number which tells you the state of your savings, that original thought which showed you, but not the others, the kind of unrecognized genius that you ‘really’ are, this liver which is scheduled for a transplantation or that tooth which got pulled recently….To each, ask the question: is this the ‘I’ that ‘I’ experience? Would I not be me if I lose this property or did not have this property or was not this? Thus, one way to avoid monotony of this thinking process is to expand the range of objects (events and processes) to include things that Shankara does not mention. The other way is to expand the thinking to include relationships that pick out the ‘I’, namely, the ‘my’, ‘mine’, etc. (My house, my fame, my wife, my dog, my car…) The third would pick out ‘properties’: of your ‘personality’ (I am a thinker, I am an introvert, I am clever or intelligent or stupid…), or of your body (I am beautiful, I am fat, I am strong…) And so on.\nIf you are able to initiate this process and sustain it over a period of time, you will reach the stage where you have to admit that the ‘I’ that you experience cannot possibly exist in the world. Because the world is everything there is, you cannot experience the ‘I’ if it is not there in the world. Though you are not hallucinating, the ‘object’ of your experience does not and cannot exist in the world. Nor can it exist ‘elsewhere’, i.e. ‘outside’ the world, because if something exists at all, it exists in the world. (After all, that is what ‘the world’ means. Even ‘non-existent’ objects, Sherlock Holmes for example, are objects that could but do not exist in our world. The ‘I’ is not like that: it cannot exist; therefore, it is not merely a non-existent object that could but does not exist.) ‘Supra-mundane’, ‘spiritual’ etc. do not and cannot pick out something (whether called ‘lokas’ or ‘worlds’) that is not in the world (Vishwa). The Vishwa, as Indians circumscribe it, is everything that was, is and shall be. Nothing exists outside ‘the world’, not even ‘shunyata’ or nothingness. If something exists or even if it could only possibly exist, it does so in the world. (Please do not forget that, unlike the Semitic religions, Indian culture knows no ‘outside’ to the world. The different ‘lokas’ that one speaks about are not worlds existing outside the Cosmos or the Universe, whether they are localizable, ‘visible’ or ‘invisible’ to human sight or whatever else. The World or Vishwa is all inclusive: it includes ‘ghosts’, ‘spirits’, ‘gods’, and such like, if these are in the world.)\nOf course, you could break this process of thinking-through before it reaches completion. You could do so by choosing to come up with pseudo-talk about Sadhana and Sadhaka to ‘impress’ others and also to convince yourself that its completion requires many rebirths; or tell yourself that this is a wrong way or that it is fruitless or that it is dumb, foolish, inane, esoteric,….etc. There are indefinitely many such routes to take. If you do not follow these but do go through to the end, at that stage, and only at that stage, will the ‘who am I?’ question become your ‘own’. The last line of the Nirvana Shatkam might or might not help you during this process. However, know this: when you understand that line, you will understand the question. And you will definitely understand that line, if and when the question truly becomes your ‘own’. This is not a ‘circular reasoning; only the outer limits of the end result are indicated here.\nA final thought, expressed with very great reservations and, in a manner of speaking, against better judgement: a pursuit of this process, the effort that gets put in and its result do not require a teacher, a guide, or any external help of any kind. Any human being can go through this process without external aid. When you understand the last line repeated in each of the verses, only then will you realise how it answers the question, ‘who am I?’ That stage is what we call ‘enlightenment’: ‘atmagyaana’, ‘atmaanubhava’, ‘atmasaakshaatkaara’ etc. are some of the words we use to talk in different ways about this stage. (Needless to say, this is an outline sketch of just one route to enlightenment.) Buddhists call it the state of ‘awakening’: the word identifies that state where you know that the ‘I’ cannot be an entity in the world and when further consequences of this truth to experiences in daily life are pulled out.\nBroadly speaking, two avenues open up now. To pursue either of the two, you will need help, guidance and support. The ‘guru’ (understood only as an aid, or help, or guidance) is an absolute requirement only at this stage. Generally speaking, a teacher is not a necessary requirement, even if it might be useful to have one, to become enlightened; but it becomes an absolutely vital requirement thereafter. When you get enlightened, you will also know the ‘why’.\n- Beef Bans, Beef Parties, and Sacred Cows\n- Assumptions, Warrants, Hypotheses and Heuristics']	['<urn:uuid:8be9b39f-0e05-4073-b70a-fc0ce649190a>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:10:11.825225	8	49	1755
77	blast tree damage and wildlife casualties	The blast violently tore apart trees, creating deposits containing entire trunks and branches up to 75cm large that were thrown in the air. Most tree fragments were burnt black. The wildlife casualties were massive, including 11 million fish, 1 million birds, 11,000 hares, 5,000 deer, 1,500 elk, 1,400 coyotes, 300 bobcats, 200 black bears, and 15 mountain goats. Virtually all medium-to-large mammals and bird species in the affected area were eliminated.	"['A rather extensive forest became part of a directed blast deposit: that’s the summary. One moment, you’re a green and pleasant home for much of the local wildlife; the next, you’ve been rudely ripped apart and incorporated within a bunch of rock and ash by a volcano having a bad turn. So it goes.\nWithin the down-timber zone, it was clear some rather spectacular force had been applied. It wasn’t piddly little wood fragments and needles that became deposits, but entire tree trunks. Whole limbs had been ripped off, splintered, and subsequently dumped. The heavier bits, as heavy bits tend to do, remained close to the ground as the blast carried them along. As the flow lost energy, the heavy bits of layer A1, including its compliment of ex-trees, settled out first, fining upward as the deposit accumulated. Mind you, when I say “fined upward,” I don’t mean they got all demure and small, even close to the volcano. No, the ex-tree bits in subsequent layers within layer A2 and the pieces that landed atop layer A3 were as mind-blowingly large as 75 centimeters (29.5 inches). Not only that, but the way they landed show they were first torn loose by that erosive front of the blast, then heaved high in the air by the following phase, held airborne by convection, then unceremoniously dumped moments later.\nWarner Bros., I think, could have animated that sequence in the tradition of Wiley E. Coyote to fine effect.\nOther branches, pine cones, and bits ripped from the unfortunate forest were light enough to continue traveling. They sailed the volcanic winds even beyond the boundary of layer A2, past the devastated area, and came to rest in a bed of silty layer A3, then were covered with a blanket of the following air-fall deposits left by the central eruption column. Some of those fragments were as long as 15 centimeters (6 inches). Imagine how much force it requires to take pieces of wood half the length of a school ruler and keep them in the air for twenty minutes or more.\nMixed up in all that were smaller remains, a mulch of fir needles, splinters, and twigs. In most areas, they can be found in all three layers, but to the north the energy of the blast was so ferocious it wouldn’t let them settle out until layer A3 did. Almost everything was burnt black, no matter where it landed, showing it all got seared before coming to rest. Only the needles and branches flying through the southern edge of the east side of the blast managed to come out without a thorough scorching, showing the blast cloud wasn’t so hot there. Still fast and furious enough to rip trees apart and turn them from biology into geology, though.\nThus ends the story of The Forest that Was. From here on, our relationship with the blast deposits will get decidedly rocky.\nLipman, Peter W., and Mullineaux, Donal R., Editors (1981): The 1980 Eruptions of Mount St. Helens, Washington. U.S. Geological Survey Professional Paper 1250.\nPrevious: The Cataclysm: ""Fully Down and Buried""', 'After devastation … the recovery\nAn amazing bounce-back after catastrophe gives us insights into how the world recovered from the Flood.\nWhen Mount St Helens erupted on 18 May 1980, the resulting devastation of the area around the volcano left many stunned at the sheer scale of the destruction. More than 200 square miles (over 500 square km) of what had been a vast green blanket of pristine forest, clear mountain streams and tranquil lakes was now a monotonous grey ash-covered wasteland of fallen timber, steaming pumice plains, barren mudflows and avalanche debris. Shortly after the eruption, the then U.S. President Jimmy Carter compared it to a moonscape. Scientists studying the affected area referred to an ‘apparently sterile landscape’,1 lamenting that ‘It will never be again, in our lifetime’2 and speculating that it would even be ‘impossible for insects to recover at all.’3\nGloomy predictions wrong\nScientists who flocked to study the devastated area soon found that the initial pessimistic forecasts of long-term barrenness were largely unfounded.4 For example, within just three years, 90% of the original plant species were found to be growing within the blast zone.5 As is evident from the ‘before-and-after’ photographs on these pages (more photographs were originally included with this article), the innate resilience of the creation had been greatly underestimated.\nLife’s return—the details\nHowever, many species were completely eliminated from the Mount St Helens blast zone because of the eruption. While most volcanoes erupt in an upward fashion, Mount St Helens initially exploded sideways, spewing its oven-hot blast over the forested landscape to the north. Nicknamed the ‘stonewind’, the rock-laden, ground-hugging steam blast advanced rapidly outward from the volcano in a 180° arc, flattening over 200 square miles (500 square km) of forest in less than ten minutes. The extent of biological destruction was staggering. The timber felled by the eruption would have been sufficient to build almost 500,000 three-bedroom houses. Virtually all the visible mosses, ferns, shrubs and wildflowers vanished. Not only did all living organisms in the upper North Fork Toutle River die, but 15 miles (24km) of the river itself was no more!6 The estimates of animal deaths by the Washington State Department of Game included 11 million fish; 1 million birds (including 27,000 grouse); 11,000 hares; 5,000 deer; 1,500 elk; 1,400 coyotes; 300 bobcats; 200 black bears; 15 mountain goats;7 and 15 cougars.8 In addition, 57 people were counted as dead or missing.\nVirtually all species of medium-to-large sized mammals in the affected area,9 and presumably all bird species,10 were obliterated. But many have come back, by immigrating from outside. Various bird species were recorded in the area soon after the eruption, probably feeding on insects (the first helicopter crews to land in the devastated area reported that flies and other conspicuous insects had preceded them).11,12 Although not all of these insect migrants survived (herbivorous insects could not live until plants had started to grow again), many species did survive—often by consuming their airfall companions, both alive and dead. Among the aerial arrivals were millions of wind-borne spiders,13 plant seeds and fungus spores.\nOnce vegetation had begun to regrow, the large herbivorous mammals such as elk and deer re-entered the blast zone. Elk, being highly mobile, were able to move into and out of the blast zone at will, and this further hastened plant recovery, as their dung contained seeds and nutrients transported from outside the devastated area. Beavers from the adjacent forests followed water courses upstream to blast zone lakes. Amazingly, salmon and trout maturing in the Pacific Ocean at the time of the eruption (and thought to be intolerant of anything other than cold, clear, well-oxygenated streams), successfully ascended muddy and ash-clogged waterways in their instinctive urge to spawn.14\nAlthough millions of organisms living above ground at the time of the eruption were wiped out, many life-forms within the devastated area survived the fury of the blast.15 How? Ants survived in underground colonies,16 salamanders in the soft wood of decomposing logs, fish in ice-covered lakes, and roots of plants were protected from the blast inferno by soil and snowpack. Although large numbers of these subsequently succumbed to the unforgiving post-eruption environment, some lived on and reproduced. In fact, ecologists acknowledge that the presence of such ‘unexpected survivors’ greatly accelerated recovery. The aquatic and streambank areas exhibited the most rapid recovery. At least 10 of the 16 original species of amphibians (frogs, toads and salamanders) survived the eruption.17 Frog and toad survivors exploded onto the recovering landscape, rapidly establishing large breeding populations by the mid-1980s.\nToday, the diversity of species (e.g. birds18) living in the area devastated by the eruption of Mount St Helens in May 1980 is approaching its pre-eruption levels. The kinds of birds and animals that have not yet returned are mostly species preferring old-growth forest habitat. While it will probably take at least 200 years before old-growth forest again occupies the blast zone (providing another disturbance does not intervene), Mount St Helens has forced ecologists to rethink their theories of ecological ‘succession’. This was because they found both ‘pioneer’ and ‘climax’ species growing side-by-side!\nMount St Helens and the world-wide Flood\nObserving the return of life to Mount St Helens can provide some insight into the return of life to the world after Noah’s Flood. Both Mount St Helens and the world-wide Flood were cataclysmic geologic events involving extreme volcanism (Genesis 7:11), flooding, and the destruction of life—one on a local, the other on a global scale. In both, organisms survived and repopulated the post-disturbance landscape. Consider:\nMany species were completely wiped out from the blast zone, particularly the birds and the large land mammals (e.g. deer and elk).\nIn the Flood, everything on dry land that had the breath of life in its nostrils died; the only ones to survive were those with Noah in the Ark. At Mount St Helens, these species returned to the devastated landscape through migration from beyond the zone of destruction. After the world-wide Flood, animals migrated out from the Ark’s landing place, multiplying and repopulating the earth.\nInterestingly, reproductive rates of elk (large herbivores) early in the recovery period at Mount St Helens were among the highest ever seen, probably due to availability of high-quality forage from recovering vegetation. Survival of offspring also increased, probably a reflection of the low numbers of predators, which only moved in and multiplied later once herbivore herd numbers had increased.\nJust as hunting pressure drove elk into the blast zone at Mount St Helens (local authorities had put restrictions on hunting in the devastated area), the post-Flood human population, as it spread across the earth following the dispersion from Babel, must have induced wild creatures to move to more distant regions. With their much higher reproductive rates, herbivores probably occupied the far-flung unoccupied zones of the earth well in advance of predators and man. Birds, with their capacity for flight, are likely to have been at the forefront of the dispersal into the devastated post-Flood landscape, as at Mount St Helens. This may explain why birds such as New Zealand’s Moa that could have lost the ability to fly through mutation (loss of genetic information) were able to survive in apparently large numbers—until hunters eventually migrated to the area.\nInterestingly, the animals and birds that were the first to colonise the devastated landscape at Mount St Helens are known by ecologists as ‘generalists’, i.e. able to tolerate a wide array of environmental conditions and dine on a variety of foods. Among the most conspicuous of the first colonisers at Mount St Helens was the common raven, known to eat almost anything, including carrion. Back in Noah’s day too, it was the raven that was the first to leave the Ark (Genesis 8:7), weeks before the dove was able to survive in the devastated post-Flood world (Genesis 8:8–12).\nMany species—plants, microbes, insects, amphibians and aquatic creatures—survived within the blast zone, if not in adult form, then as seeds, spores, eggs and/or larvae.\nIt is noteworthy that God brought only birds and air-breathing land animals to the Ark. Mount St Helens shows us that species not taken on board the Ark can indeed survive cataclysmic geologic events.\nThough many plants, amphibians and fish died in the eruption (as undoubtedly occurred in the Flood, as per the fossil evidence), many survived to reproduce. As for insects, it is known that there are billions of insects in the air column, even up to altitudes of 4,500m (15,000 feet).19,20 Though most if not all would not have remained aloft during the 40 days of rain, many insects would have survived the Flood in floating logs and other debris.\nEven dead insects would, through their carcasses, have been an important source of food for survivors and nutrients for the vegetation sprouting as the Flood waters receded. And as at Spirit Lake ( see aside below) legions of microbes probably helped restore the volcanically degraded post-Flood lakes and seas. Animals on the Ark could thereafter have migrated gradually from the Mount Ararat region into a prepared landscape, already populated by abundant microbial, plant, insect and aquatic life.\nResilience of the creation\nThe overarching conclusion to be drawn from Mount St Helens is the extreme resilience of the creation. Sceptics often argue that recovery from a global catastrophe such as the Flood would be impossible within a short biblical time-frame. Mount St Helens, however, demonstrates how quickly and completely recovery can occur in the natural world. So, following the Flood of Noah’s day, the regreening and repopulating of the earth could also have happened within a very short time-frame. Just like the Bible says.\nThe death and rebirth of Spirit Lake\nOn the morning of May 18, 1980, Spirit Lake, a paragon of tranquility and beauty, was virtually obliterated. About one-third of the avalanche of debris ploughed directly into this azure jewel, causing its water to slosh over 240 metres (800 feet) up the mountain slopes to the north, where it picked up the soil and vegetation of an old-growth forest, including a million logs. When this organic soup returned, it was to a new lake basin, elevated over 60 metres (more than 200 feet) above its pre-eruption level. Oven-hot flows of volcanic debris boiled into the lake’s south shore, and volcanic rocks and ash rained from the sky. The first helicopter crews into the blast zone reported they were unable to find Spirit Lake. They did not recognise it with its surface obscured by a mantle of floating logs and pumice.\nWhen scientists returned to Spirit Lake in June of 1980, they found it had been ‘transformed into a roiling [‘roil’ = to stir, to make muddy], steaming body of degraded water choked with logs and mud.’1 They predicted it would take 10–20 years to return to its ‘pre-eruption chemical and biological condition.’ As it turned out, it took closer to five! How did this happen so quickly?\nAfter the eruption, Spirit Lake became a ‘paradise’ for microbes. Its waters, once cold (10°C = 50°F) and clear, became warm (over 32°C = 90°F) and muddy, laden with organic debris, mineral nutrients and other chemicals. Bacteria proliferated to an astounding degree in this broth, ultimately peaking at half a billion bacterial cells per millilitre—a ‘concentration that is possibly unprecedented in the annals of environmental microbiology.’2 For a time, the oxygen levels were so depleted by the decomposition activity that the lake could support only anaerobic (i.e. can live without oxygen) microbes. Spirit Lake thus bubbled like a cauldron from escaping carbon dioxide, methane and hydrogen sulfide generated by these bacteria in bottom sediments. For scientists visiting the area, the odour was overwhelming! However, the ‘no oxygen’ bacteria were crucial in decomposing the huge amounts of organic debris settling on the bottom of the lake during this phase of the recovery process.\nRestoration was greatly hastened by the coming of the winter rains. This seasonal influx of fresh water diluted the concentration of toxic chemicals and raised oxygen levels. Wind, waves and seasonal lake turnover stirred in still more oxygen, enabling the return of oxygen-dependent microbes, which absorbed mineral nutrients from the water, and thus helped clear the lake of these and other chemicals. Water clarity improved, and with increased light penetration the phytoplankton reappeared. They produce food by photosynthesis and release oxygen as a by-product. Within just five years, the water quality had nearly returned to its pristine pre-eruption state—a remarkable transformation.\nReferences and notes\n- Franklin, J.F., MacMahon, J.A., Swanson, F.J., Sedell, J.R., Ecosystem responses to the eruption of Mount St. Helens, National Geographic Research 1(2):198, 1985. Return to text.\n- Lumsden, R., 1997 Mount St Helens Field Study Tour, Institute for Creation Research, CA, USA, p. 30, 1997. Return to text.\n- Adams, A.B., Leffler, S., Insect recolonization of the northwest sector of the Mount St. Helens blast zone, in Keller, Ref. 4. Return to text.\n- Keller, S.A.C. (ed.), Mount St. Helens—five years later, Cheney, WA: Eastern Washington University Press, USA, p. 307, 1986. Return to text.\n- Ref. 1, p. 198. Return to text.\n- A resident along the Toutle River described a mudflow passing his house on the afternoon of May 18, 1980, as having the consistency of wet concrete and looking like ‘a sandy beach going by’ with ‘fish flopping on top’. Other observers noted salmon and trout jumping out of the Toutle; the water had suddenly heated from normal 45°F (7.2°C) to over 90°F (32.2°C). Return to text.\n- Mountain goats from the Olympic Mountains (Washington) were relocated to the Mount St Helens area in 1972 by the State of Washington Game Department, and were known for occasionally chasing hikers. All 15 goats died in the eruption. Return to text.\n- Carson, R., Mount St. Helens: The Eruption and Recovery of a Volcano, Sasquatch Books, Seattle, USA, p. 83, 1990. Return to text.\n- Andersen, D.C., MacMahon, J.A., The effects of catastrophic ecosystem disturbance: the residual mammals at Mount St. Helens, Journal of Mammalogy 66(3):587, 1985. Return to text.\n- It is estimated that approximately 80 species of birds would have been present in the blast zone during May (permanent residents, winter residents and some migrants). See Ref. 18. Return to text.\n- Frenzen, P. and Crisafulli, C., Biological Responses to the 1980 eruptions of Mount St Helens, (information sheet provided by the Mount St Helens National Volcanic Monument), p. 4, 1990. Return to text.\n- Edwards, J.S., Crawford, R.L., Sugg, P.M., Peterson, M.A., Arthropod recolonization in the blast zone of Mount St Helens, in: Keller, Ref. 4. Return to text.\n- Many species of spiders ride the wind in a process known as ‘ballooning’. A long silken thread is released into the air and acts as a kite or balloon which transports the spider even hundreds of miles. Entomologists (refer Ref. 12, p. 332) identified over 75 species of spiders ballooning onto the pumice plain at Mount St Helens, and estimated that two million spiders fell per square mile each day from June to October 1983. Return to text.\n- Lucas, R.E., Recovery of game fish populations impacted by the May 18, 1980 eruption of Mount St Helens: winter-run steelhead in the Toutle River watershed, in: Keller, Ref. 4. Return to text.\n- Ref. 8, p. 88. Return to text.\n- Sugg, P.M., Arthropod populations at Mount St Helens: survival and revival, in: Ref. 4. Return to text.\n- Figures based on data from Karlstrom, E.L., Amphibian recovery in the North Fork Toutle River debris avalanche area of Mount St Helens, in: Keller, Ref. 4 (modified based on additional information). Return to text.\n- ‘Before-and-after’ comparisons of numbers of bird species at Mt St Helens must take seasonal variation into account. As mentioned in Ref. 10, it is estimated that 80 species of birds were present at the time of the eruption. Asikainen, M., Birds of Mount St Helens (checklist), Mount St Helens National Volcanic Monument, 1996, reports that by 1996, 118 species of birds, 80 of which were nesting species, were regularly observed in the blast zone! Return to text.\n- Coad, B.R., Insects captured by airplane are found at surprising heights, Yearbook of Agriculture, U.S. Dept. Agr., pp. 320–323, 1931. Return to text.\n- Hardy, A.C., Milne, P.S., Studies in the distribution of insects by aerial currents, Experiments in aerial tow-netting from kites, Journal of Animal Ecology 7:199–229, 1938. Return to text.']"	['<urn:uuid:0fb1b3df-5ba9-4ff8-96c0-94f32d33be15>', '<urn:uuid:81b1e069-cc81-4cbe-b80c-a9267574ad45>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	6	71	3249
78	correct side rein length horse training trot walk canter adjustments	Side reins need to be short enough to allow connection of rein aids to hind legs, but long enough for the horse's nose to be slightly in front of the vertical in motion. They should be adjusted throughout the session - lengthened for canter and prolonged walk work. They work best in trot where the horse requires less neck movement for balance. When the horse is moving in balance, the side reins should be slightly slack with proper contact through the lunge line.	"[""Once the horse is tracking up (hind feet following in the hoof prints of the front) and starting to reach over his back and out and down through his neck we can consider attaching side reins.\nWhich type of side rein to use?\nMy preferred choice is for the plain leather with no elastic or donuts for the horses to bounce around with or lean into. With the plain reins, if the horse attempts to lean they give as soon as the horse gives. Because the elasticated / rubber ring type stretch, they continue to exert a momentary backward pull after the horse has yielded.\nWhat is the correct length for the side reins?\nThe answer is to ask the horse. I see side reins as an addition to a more educated horse's training not as a means of teaching an outline or head set. Their purpose is to teach the horse to come out and meet the contact without fear, to trust the rein in the same way that they will trust the rider’s hand.\nA rule of thumb method for finding the length is to stand in front of the horse (and you may need a helper the first time you do this) facing his head. Adjust him so that his head and neck are coming straight out of his shoulders (i.e. no looking to left or right). Although it is considered correct to fasten the outside rein before the inside one (as when a rider picks up the outside rein contact before the inside when riding), it is best to make sure that both side reins are adjusted before attaching them to the cavesson or bit so that the horse doesn’t have to wait with a lop-sided contact while you fiddle around sorting the other one out.\nTheir purpose is not to ‘pull the head’ in or to ‘set a head carriage’\nSide reins need to be short enough to allow a connection of the rein aids to the hind legs, but long enough that the horse's nose can be slightly in front of the vertical in motion.\nWhen the horse is moving in balance, the side reins should be slightly slack with the nose slightly in front of the vertical, and there should be contact through the lunge line.\nWhen adjusting the length or leading the horse with the reins unclipped make sure that they do not dangle freely. They can get tangled around the horses legs or worse stepped on. When not in use clip them to the rings on the surcingle or dees on the front of the saddle.\nIdeally, they should be adjusted throughout the session, too, lengthening for canter and any prolonged or early walk work. I believe they are best suited to work in trot where the horse requires less movement in the neck to balance himself.\nShould they be of the same length or should the inside one be adjusted shorter?\nThis is a debate that will run and run. For basic lungeing, I prefer to have the reins equal length, but have had it pointed out by a French trainer who does use variable lengths that he lengthens the outside one; he doesn’t shorten the inside. There’s a huge difference between doing one and the other. This adjustment is for more advanced horses accustomed to working ’in position’. (fig. 1)\nThey are most effective when used in trot as there is not so much movement through the horse’s head and neck in this gait. At the start of a lunge session, I let my horses walk on both reins without the side reins attached before adjusting them loosely for their warm-up trots. They can then be shortened to the horse’s optimum work length for that time (be aware that this length can vary from day to day and within a session) and if lungeing a younger or unfit horse, I will give short walk breaks with the reins unclipped, so he can have a good stretch. A more advanced horse can have the reins attached for a short period of walk without causing too many problems.\nIt is possible to pervert any good principle if it is used for a purpose for which it was never intended.\nIf your browser doesn’t open your email client, click here)\nClassical Dressage Notebook\n© 1998 -2017 classicaldressage.co.uk. All rights reserved.\nThe ‘3 Black Horses’ logo and the ‘email’ logo are trademarks of Classical Dressage Notebook""]"	['<urn:uuid:04528e84-5c6f-4ef7-9d87-a510ef01b9af>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	10	83	743
79	looking for info about what caused brain tumor patient john joyce initial cancer symptoms cold sore lip would not heal	John Joyce initially had a cold sore on his lip that wouldn't heal despite seeing several doctors. After getting another opinion, a doctor diagnosed it as cancerous within five minutes. A biopsy confirmed it was melanoma, which later spread leading to a brain tumor.	"[""'I went to the doctor for a cold sore that wouldn't heal… now I have a brain tumour and I'm paralysed on one side of my face' – Irish man (59)\nJohn Joyce (59) almost fainted when it was suggested that palliative care nurses help him cope with his health problems. “I can still see those three women standing there in the hospital, introducing themselves to me,” he recalls. “I thought, that’s it for me then. I’m gone.”\nLike many people, he believed palliative care was only for people in end-of-life situations. But he now knows that was incorrect. Because, these days, a care team helps him cope with the many health challenges he faces on an ongoing basis.\nJohn grew up on a small farm in Foxford, Co Mayo, with his four younger sisters. Sadly, his father died suddenly when he was a teenager, causing him to leave school prematurely. “That was the done thing in those days,” he explains.\nOver the years John took care of the farm, while working in public houses. Having managed a pub in Ballina, he then moved on to the Enniscrone Golf Club. Around this time, his mother Norah’s health deteriorated. So, he left the golf club to concentrate on merchandising for the likes of Guinness and the Lotto. This meant he could manage his own time, while also looking after his mother, who was diabetic.\nWhen John wasn’t busy, he would head off into the beautiful Mayo landscape to do some fishing or to walk his much-loved Labrador. “I like to be on my own,” he says. “I’m not afraid of my own company, and anyway, nature makes me feel serene and spiritual.”\nAbout five years ago, John ran into health problems of his own when he got a cold sore on his lip which didn’t clear up, even though he saw several doctors. Then one day, a former nurse from Australia made him promise he would get another opinion. Which he did. “Within five minutes that doctor told me it was cancerous,” John says. “I didn’t believe him at first, because I had never smoked and there was no history of cancer in our family. However, within days I was in Galway having a biopsy. A few days later, the diagnosis of cancer [melanoma] was confirmed. From that moment on, I was no longer John Joyce, I became John Joyce with cancer.”\nDuring the biopsy the ‘sore’ was removed, as well as a good deal of tissue around it. But a month later, cancer cells were discovered in the other side of John’s mouth, so he had another biopsy. “They were happy at that point that it was all clear,” he says.\nHowever, six months later, the cancer was back. So, 36 sessions of radiotherapy were prescribed. “I had to wear a special mask, which had been made from a mould of my face,” says John. “My head and shoulders were strapped to the bed during treatments. This was to ensure that the cancer cells, rather than healthy cells, were destroyed by the radiotherapy. The sessions weren’t long, but I felt uncomfortable and worried I would choke.”\nJohn used to tell his mother Norah he was working when, he was, in fact, going to Galway, for treatment. “I was her only son,” he explains, “her golden boy. She would have been devastated if she’d known I had cancer. She used to tell my sister that I was working much too hard.”\nSome months later, John had to face yet another round of radiotherapy when a tumour was discovered in his brain. “It’s inoperable and incurable,” he volunteers. “I’m now paralysed on the right side of my face, I’m blind in one eye, and deaf in one ear. I still have chemo every two weeks which keeps the tumour in check.”\nIn the midst of this treatment, Norah departed this world. Even though John was absolutely stricken by the loss of his much-loved mother, he was not allowed to miss even one session of his crucial radiotherapy. That meant he had to travel the long, winding road from Foxford to Galway on both days of his mother’s funeral. “I found that very hard,” he says.\nThe next intervention was chemotherapy, but unfortunately one of the side effects for John was loss of appetite. “I always felt full up,” he explains. When his weight dropped to seven stone, he had a PEG (percutaneous endoscopic gastrostomy) tube fitted. This delivers liquid food and medication directly to his stomach, and is still in situ and working well.\nHowever, his medical team then discovered that John’s heart had been adversely affected by the chemotherapy. It’s currently pumping at about 15pc of normal function, and has to be closely monitored.\nSo, there is no doubt he faces definite challenges on a day-to-day basis. Once he was discharged from hospital, he spent six months with one of his sisters. That’s when the idea for palliative care was mooted. Although initially shocked, he soon came to realise the benefits.\n“Palliative care is not only about end-of-life. It’s also about improving the quality of life,” John explains. So, once a week, a member of the Mayo Roscommon Hospice palliative care team comes to his home. “We discuss what’s going on in my life, particularly in regard to pain management and PEG feeding,” says John. “Over the years the home-care team has become part of my family. They have given me the positive reinforcement I need to be able to move on and have a really good quality of life. I really do look forward to their visits.”\nGiven that John has had so much to deal with, it’s not surprising that he often gets asked if he feels angry about his life. And he replies: “What’s the point? Just be grateful for the gifts that remain. Take it one day at a time and do the things you can do, rather than dwelling on what you can’t. I can’t walk far anymore, but I like to read, so I go to the library. I like messing about with computers, browsing Google, and so on. I often go to the bakery in Ballina so I can have a cup of coffee and meet people. You need to get your head around the fact that it’s about the life you have now, rather than the one you had.”\nThere’s no doubt that John practises what he preaches. His facial paralysis may cause comment and reaction in public, but he doesn’t give a fig. “When kids stare at me, their parents often kick them under the table. But then I wink right back at those kids – I just love their honesty.”\nThat’s so typical of this fun-loving, jovial, insightful, generous man – he makes the world a brighter place, in spite of the dark clouds.\nThe Palliative Care Research Network annual symposium takes place on November 15 at the Aisling Hotel, Dublin. See aiihpc.org\nSource: Read Full Article""]"	['<urn:uuid:c75f37c3-eece-4b54-bb83-9ea598851171>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	20	44	1166
80	I'm trying to optimize my manufacturing process. What are the different types of pull connections available, and how do they relate to the core principles of lean manufacturing?	There are three types of pull connections: Continuous Flow, Sequential Pull, and Replenishment Pull. Continuous Flow has the highest level of pull, with one-piece inventory between workstations, fixed production sequence, and no waiting parts. Sequential Pull allows a defined buffer between workstations while maintaining a fixed sequence. Replenishment Pull uses a supermarket system with maximum waiting products but no fixed sequence. These pull connections align with lean manufacturing principles by minimizing waste and maximizing profit through optimal use of the 3Ms (Man, Material, and Machines). They help achieve decreased total product cycle time, reduced inventory, increased productivity, and better capital equipment utilization. The goal is to pull material through production without interruption while maximizing value-added activities.	"['A Pull process is a process in which a workstation starts to work on his next order only when there is a free slot on the output side. This means the trigger for producing anything on the workstation comes from the customer side, which can be internal as well as external. The customer pulls orders through the process instead of a traditional Push connection in which products are produced no matter what happens on the output side of the workstation.\nBecause inventories between workstations are managed in a pull system, which directly influences and maximizes the Lead Time, Pull always has a preference over Push.\nThree different forms of Pull connections can be identified: a Work cell with Continuous Flow, Sequential Pull and Replenishment Pull.\nThe THREE PULL CONNECTIONS help to achieve different levels of Pull in the system, based on three factors: having a maximum of one piece of inventory between two process steps (1), having a fixed production sequence (2), and having a maximum number of parts waiting (3).\nThe Continuous flow connection has the highest level of Pull, since all three factors are included. Products are worked on one by one and with a maximum inventory of one between the workstations, a workstation can only work one the product that is waiting in front of it at that time (the fixed sequence), and can only produce when the inventory behind it is less than one (no inventory). multiple workstations with continuous flow between them are also known as Work cells or U-cell.\nThe Sequential Pull connection is the second best possible Pull connection, in which the fixed quantity is determined, the sequence of product is defined, but a buffer with a defined maximum is allowed between workstations to buffer for variance. This is usually implemented using First-In-First-Out lanes (FIFO) .\nReplenishment pull, the supermarket, is the third and last option, in which a maximum number of products is waiting to be worked on, but it is unknown which type of product will be pulled out next. This type of inventory is also known as a supermarket and can be controlled using Kanban.\nThe differences between the three types of pull connections can be summarized in the following table:\nIllustration 1: Difference between Continuous Flow, Sequenced Pull and Replenishment Pull\nCHOSING THE RIGHT TYPE OF PULL CONNECTION is one of the steps in designing a Future State VSM. This decision depends on a few process- and product variables. Illustration 2 shows a discussion table with variables which might influence the decision for a type of connection. In the discussion it is advised to follow the topics from left to right, and move from top to bottom. The goal of the table is not to discourage the use of a certain Pull system, but to give direction in pointing out what variables need to be worked on to be able to implement a higher level of pull within a specific situation.\nIllustration 2: Discussion table, when to use what type of connection?\nProcess reliability is the first factor that influences the type of Pull connection which can be used. When the reliability of the output of the upstream workstation is low, a buffer is needed to prevent the entire flow from stopping. Since in this case all products need to be buffered, the sequence cannot be held anymore which results in the Supermarket being the only viable option at this point.\nChange-Over-Time is the second variable. In situations where more than one product is produced within a family, short change-over-times are needed to produce in sequence and per order on a workstation. Whenever Change overs are high, it would make more sense to batch products, which means a buffer is needed in front of the workstation. This buffer would be a supermarket, because the products are sorted by type, and therefore sequence cannot be kept.\nLead times also influence the type of Pull connection can be used. Continuous Flow can only be kept if the upstream process can respond to downstream changes. When the Lead time is higher than the cycle time of the machine, a buffer is needed to prevent starvation. The same holds for Takt Time. Only when the upstream machines can deliver on all Takt times (which can vary, for instance with seasonal products), continuous flow can be implemented. When some Takt levels cannot be achieved upstream, a buffer is needed.\nThe fourth variable is variation in demand of products, which limits the options for a pull connection the other way around. The higher the demand variation or product mix is the less desirable a supermarket is, because the number of buffers and the number of products per buffer increase. This variation should be minimized using Heijunka.\nThis also holds for the price of the part or product, the fifth factor. The higher the part costs, the less desirable the Supermarket option becomes. This is a tough one, because it might be necessary to have a supermarket to keep the pull system running even though the costs are high.\nThe interesting thing about these five factors is that there are situations where an unstructured Push connection might be the only option left and having Push connections in a Future State Value Stream Map is really not desirable. In these situations where Push seems to be the only option, kaizen events need to be defined to change on or more of the above five factors to make it possible to implement on of the three Pull connections.\nThe table shown in illustration 2 is meant for the purpose of discussion only. It is not black and white and does not include all possible factors that might influence the choice of connection type. It is the task of the Team that develops the Future State Value Stream Map to decide what connection type is suitable for each specific situation.', ""Thinking of Lean Manufacturing Systems\nBy Akin O. Akinlawon, Managing Director, Manufacturing Solutions/ Industrial Automation Group, Comdisco Inc.\nThe importance of Lean Manufacturing System is better comprehended when its impact of change on economics is thoroughly understood. The manufacturing engineering philosophy is pivoted on designing a manufacturing system that perfectly blends together the fundamentals of minimizing cost and maximizing profit. These fundamentals are Man (Labor), Material and Machines (Equipment) - called the 3Ms of manufacturing. A well balanced 3M results in\n- Maximum utilization of Man - skilled and/or unskilled\n- Optimal module size - cellular and/or plant\n- Smooth traffic flow - of materials, man, automotive\n- Minimum total manufacturing cost - of products produced\n- Reduce investment\n- Reduce labor requirement\n- Utilization of more productive equipment\n- Disposition of less productive equipment\n- Flexibility to be contemporary / keep pace with market / customer changes\n- Increase Return On Net Asset\nThere are three steps involved to accomplish the ultimate manufacturing engineering philosophy:\n- Design simple manufacturing system, commence the system design as simple as possible with low volume through the system;\n- Realize there is always room for improvements, refine the first step above as best as possible; and\n- Continuously improve the lean manufacturing system design concept with appropriate insertion of and balance of automation, conveyors and where necessary, buffer stocks.\nAlternative concepts may be generated that could satisfy product and marketing technical requirements based on thorough review or re-examination of intra- and/or inter- technologies, past production process errors and lessons learned, competitive analysis, techno-communication as may be applicable. The final selection of manufacturing system concept(s) to be adopted for further consideration or development should be based upon analysis performed in conformance with established selection criteria. Let us consider two manufacturing systems that when combined give Lean Manufacturing - Flow Manufacturing System and Agile Manufacturing System.\nDesign Simple Manufacturing System\nFlow manufacturing is a time-based process that pulls material through a production system without any interruption. This is a fundamental principle of Lean Manufacturing. This process concept can be achieved by--rapidly flowing material from raw to finished good--systematically balancing man (operator) and machine (equipment) to customer requirement.\nThe goal of Flow Manufacturing is to provide the ultimate response and produce to customer requirement. The benefits of this goal include:\n- decreased Total Product Cycle Time,\n- less inventory,\n- increased productivity, and\n- increased capital equipment utilization.\nIn Flow Manufacturing, the performance is measured by the Total Product Cycle Time (or Critical Path). Total Product Cycle Time is the longest lead time path from raw materials to finished goods. This is the quickest possible response to a customer order with finished product. Lead Time Analysis is derived from the Critical Path which enables us to highlight opportunities to reduce or eliminate NVA activities and thereby shorten the Total Cycle Product Time. By reducing the variations in the rate of flow in a manufacturing system, the lead time may be shortened. The variations can be reduce with\n- random downtimes, higher uptime through quick changeover, lower downtime, etc., and\n- improved quality through error proofing, self checking equipment product centered cellular layout.\nAgile Manufacturing is a profitable manufacturing system that is closely related to the concept of Flow Manufacturing. It builds on the Flow Manufacturing concept to reduce lead time, optimize asset utilization and build to customer demand by focusing on being able to quickly respond to customer requests. It assumes that the customer requirement (specifications and volumes) is subject to continuous changes. A form of measurement of Agile Manufacturing performance is the program lead time.\nRealize There is Always Room for Improvements\nThe ultimate goal is a system that has a smooth flow of material while maximizing the value added (VA) activities of the operator. Usually there are many situations in System Design Process that require special consideration. A few of these situations include:\n- Manufacturing Process - is the equipment used to create, alter, assemble, measure or test the product with the objective of meeting a pre-determined product requirement. The equipment include machines, fixtures, tools, gauges such as lathes, drills, grinders, test stands and so on.\n- Manufacturing System - is the combination of man and manufacturing process(es). These two are often linked together with material handling (manual or automated) to move the material or product from one manufacturing process to the next.\n- Value Added (VA) - is any activity performed to a product as it moves through the production process that the customer perceived as actually adding value to the product.\n- Non Value Added (NVA) - is all other activity associated with the production process that may or may not be necessary to be performed but it is nonetheless performed at present pending the emergence, awareness or availability of better methods.\nTotal Product Cycle Time (also known as Critical Path) is the longest lead time path from raw material(s) to finished goods. This is the quickest possible response to a customer order with a finished product. NVA activities in manufacturing system is waste. These wastes are correction, over production, movement of material, motion, waiting, inventory, and processing. Improving the flow of material through improved system layout at the customer's required rate (takt time) would reduce waste in material movement, inventory (work in progress) and improve the ability to be a JIT manufacturer. Therefore it is essential to properly apply the methodology of Lean, Flexible, Customer Focused Cellular Layout, Material Flow and Transfer, Takt Time and Operator Utilization in order to achieve the goal of Flow manufacturing and JIT.\nContinuously Improve the Lean Manufacturing System Design\nTo be able to quickly respond to customer requirements and be a JIT manufacturer concurrently, one of the keys is to have the flexibility of equipment and have the ability to align it with product flexibility. Due to the uncertainty in customer requirement, it is important to examine manufacturing cost over a range of volumes.\nTraditionally, there is a steep drop in manufacturing cost as the ideal volume requirement is approached and a steep rise as the volume is in excess of requirement. This system is generally characterized by:\n- investment committed upfront, usually very high,\n- more rigid and complex equipment, often not technologically modern, and\n- larger capacity increments, high customer volume requirement for long period.\nHowever, in Lean Manufacturing, the manufacturing cost does not have such a steep drop as volume requirement changes. This is because of the product flexibility and equipment flexibility that can be incorporated into the Lean Manufacturing System. This system is usually characterized by:\n- investment committed as needed,\n- more equipment flexibility,\n- more adaptable to uncertain markets (volume / product), and\n- smaller capacity increments, more product flexibility.\nLean Manufacturing System has potential for greater profit (higher RONA). The profitability depends on the utilization of its resources - the 3Ms viz\n- Material moving rapidly from VA to VA operations\n- Man working constantly by adding value to the product\n- Machine running in a more productive manner according to customer requirement\nLean Manufacturing System can be adopted in new manufacturing system, existing manufacturing system requiring capitalization, equipment or product relocation.\nMr. Akinlawon can be reached at firstname.lastname@example.org.""]"	['<urn:uuid:6b0c426a-91a2-4833-8819-aafde127a636>', '<urn:uuid:c643e8c9-c849-47dc-95b5-bdefe05d3ca5>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	28	116	2182
81	zimbabwe south africa literature influences compare	Both writers' works are deeply influenced by their respective African nations. Matambo's writing is shaped by Zimbabwe's contemporary life and history, particularly the surreal aspects of Zimbabwean street language and its dark humor reflecting political conditions. He was influenced by Zimbabwean authors like Charles Mungoshi, Yvonne Vera, Dambudzo Marechera and Chenjerai Hove. Similarly, Krog's work is deeply connected to South Africa, as evidenced by her book 'Country of my Skull' about the South African Truth and Reconciliation Commission, and her translation of Nelson Mandela's autobiography into Afrikaans.	"['The African Book Review’s Chioma Nkemdilim met with some of the finalists of The Brunel University African Poetry Prize to discuss their poems, inspirations, and hopes for the future of African Poetry. Here’s our interview with Bernard Matambo, a Zimbabwean poet whose poem “The City,” provides an in-depth look at the relationship between spaces and people.\nABR: How did you develop an interest in writing poetry and where does your inspiration to write poems come from?\nMATAMBO: I started writing poetry in a serious way when I was 14. Before this I had always read everything near, and written short pieces for school or for myself. A lot of things were happening in my life at this age, sudden changes that led me to question a lot of what I had understood to be true and factual. Inevitably my understanding and reading of the world seemed to lack placement in the world around me. Poetry became a way of looking, a way of reading and synthesizing what was occurring within and around me at that age.\nABR: Your poem “The City” which is a finalist for the Brunel University African Poetry Prize seems to touch on issues of history, oppression, reclaiming spaces, and possibly how spaces can be a record/ keep the memories of peoples who have ever dwelt there. Can you share your inspiration for this poem and what you wanted it to be representative of?\nMATAMBO: “The City” is part of a circle of poems I started working on in 2008. Part of my objective was to have the poems communicate with each other, thus creating a potential narrative arc when read together. Yet I also wanted each of the poems to standalone and exist without the others.\nIn this poem I was thinking of reclamation of internal and external spaces.\nWhile the anguish of political oppression can be humbling, it can often too engage us with unsavory aspects of ourselves.\nIt often becomes effortless to dehumanize each other, for instance. I was thus interested in how a society would go about not only forgiving itself after the harder parts of a prolonged season of anguish, but also reconcile, reclaim and establish new selves. The physical scape in the poem then, is by and large symbolic of what has occurred internally within such a society during this prolonged season that has not quite ended. While it is not always palatable, history’s shadow will often hover over the future.\nABR: As a Zimbabwean poet, how has Zimbabwe influenced your works, and what do you think the future of poetry in Zimbabwe is? What ideally would you like it to be?\nMATAMBO: Having started writing in Zimbabwe, I admired several authors from Zimbabwe who influenced me greatly, particularly in my teen years. The works of Charles Mungoshi, Yvonne Vera, Dambudzo Marechera and Chenjerai Hove were widely read during that time. What strikes me today about these authors was their relationship to genre: they all worked in poetry and prose, and Yvonne Vera seemed to compose her novels very rather than write them. Thus I read and wrote in both genres without thinking much of it. I work in both poetry and prose, and credit these authors as influences.\nZimbabwe’s history and contemporary life is also a significant influence on my work. While I could not list specific aspects of its footprint, I will say several aspects of life in Zimbabwe have struck me as surreal. Language on the Zimbabwean street, for instance, is often both dynamic and unstable, often saturated with dark humor and a faux haplessness. It is often a mirror on the long, dark and potholed corridors of power in Zimbabwe. Such behavior of language interests my poetic sensibilities; it seems to bring one closer to flux and alternate dimensions of reality.\nToday I live in both the United States and Zimbabwe. It is an experience that has been glorious in allowing me to engage and absorb a vast range of influences.\nZimbabwe has a very rich poetry scene. Much of it today exists as spoken word, which in Zimbabwe I find to be unique in that it is rather a very old genre founded on structures of traditional Shona and Ndebele poetry yet seeking a closer –and often, successful –relationship with musical cultures that have emerged from the African American experience. It is mostly continued today by younger poets, a key voice being Madzitateguru. Apart from this however, literature in Zimbabwe in general feels under siege. Several publishing houses shut down during the harder years of the economic downturn, leaving authors with very few outlets for publishing their works. While I understand that it does not always make economic sense, it remains disheartening that there are hardly any bookstores in Harare selling any works beyond textbooks and stationery. It is not an easy quagmire to solve.\nABR: As a creative writing professor, is there a particular approach you use to teach or help students develop their skills? And does the process of writing poetry influence the way you approach teaching?\nMATAMBO: I am always interested in how my students are thinking, in how they view themselves and the world around them. My first inclination is to actively listen to them, and to have a sense of their objectives for the semester. I have no fixed approach; I try to interact with each person with compassion, and with the understanding that her/him/they is a unique individual with a distinct voice.\nI am not certain if writing poetry necessarily influences the way I approach teaching. But I am certain that it does allow me to empathize with both my students as well as the frustrations they may encounter in trying to articulate themselves into stronger writers.\nABR: Do you have any favorite African books/ books by African authors? Any that have particularly influenced you?\nMATAMBO: I enjoy JM Coetzee’s works, particularly The Life and Times of Michael K, as well as Disgrace and Summertime. Cemetery Of Mind by Dambudzo Marechera, as well as the appendix section in his work, Mindblast are two others that I enjoy. I enjoy and find much influence in Ben Okri’s poetry, essays as well as his fiction. The beauty of Songs Of Enchantment frightened me.\nBut I want to emphasize that I find it important that one is influenced in authorship by works beyond one’s geography of origin. I enjoy for example, Kafka’s short works and aphorisms, as well as John Banville and Anne Enright novels. The Castaway by Derrek Walcott, and Terrance Hayes’ Lighthead, as well as C.D Wright’s One With Others and Deep Step Come Shining have also been very influential in my experience as a writer. I think it is important to listen to the music in other voices, how they carry their commitment forward. I find that in addition to delving in and living in an enjoyable book, the experience helps me see and understand my own work better.\nABR: Can you talk about your future projects and things you are currently working on?\nMATAMBO: I am currently focused on poems for a project that I have been working on sporadically since last year. I will be editing a work for prose after this and sending it out again.\nBERNARD MATAMBO is an Assistant Professor in the Creative Writing Program at Oberlin College, USA. He received his BA from Oberlin College, OH, and an MFA from Brown University, RI, where his writing received both the Beth Lisa Feldman Award for Fiction and the Matthew Assatly Award from the Literary Arts Program. His work has been published in Witness, Pleiades, AGNI, Cincinnati Review, The Journal, Laurel Review and Plume Poetry among others, and has been nominated for the Pushchart Prize. Bernard is a recent recipient of an international arts education grant from the Minneapolis Foundation to develop a course study of community arts and arts education in Zimbabwe.\nCHIOMA NKEMDILIM is a serial blogger and creative writer, an avid reader, a music lover, a Korean drama buff and a chocoholic in no particular order. Visit her online at ThatIgboGirl.', ""Antjie Krog Bio\nAntjie Krog was born on 23 October 1952 on a farm in the Freestate. She completed her BA degree with Afrikaans (cum laude), Philosophy (cum laude) and English at the University of the Orange Freestate, a Masters degree in Afrikaans at the University of Pretoria and a Teachers diploma (cum laude) at the University of South Africa.\nKrog published nine volumes of poetry, two volumes of verse for children, a short novel published by Heinemann and a book , Country of my Skull, on the South African Truth and Reconciliation Commission published by Random House. Her first play has been performed in South Africa recently dealing with a black woman and a white woman trying to come to terms with their past and future.\nKrog was awarded the following awards:\n*Eugene Marais prize for the most promising young writer (1973)\n*Dutch/Flemish prize Reina Prinsen-Geerligs prize for most promising young writer (1976)\n*Rapport prize for best literary work in a particular year (1987)\n*Hertzog prize for the best poetry volume over three years(1990)\n*Pringle Award for excellence in journalism for reporting on the Truth Commission (1996)\n*Foreign Correspondent award for outstanding journalism (1996)\n*Allan Paton award for best South African non-fiction work (1999)\n*Booksellers Award for the book they liked to sell most (1999)\n* Honorary Mention for the Noma Award for books from Africa (1999)\n* Award from the Hiroshima Foundation for Peace and Culture for the year 2000\n* Olive Schreiner 2000 award for prose\n* FNB Award for best poetry volume for the year 2000\nKrog delivered the keynote speech at the Zimbabwe Book Fair in 1998 and the Conference on Women and Violence organised by the World Bank in Washington 1998. She gave lectures on aspects of the Truth and Reconciliation Commission at the University of London, the University of Glasgow, the Universities in Essen and Dortmund in Germany, the University of Utrecht and at NIZA (Netherlandsch Instituut voor Zuider Afrika) in Holland, as well as during a South African week in Antwerp, Belgium. She gave a series of lectures on the concept of Justice within the Truth Commission ambit at the Universities of Bishops, Concordia, McGill, Carleton and Toronto in Canada, as well as the New York University and Bard College in the UN. She appears as frequent guest on current affairs programmes of the BBC, of Hilversum in the Netherlands, Belgium, Australia, Nieu Zealand and America. She has been invited three times as poet to participate in Poetry International at Rotterdam, Nacht der Poezie in The Hague and the Berlin Literatur festival. Was resident at the Foundation Royaumont for Poesie and Traduction where several of her poems had been translated into French. Krog formed part of the South African writers invited to Aix-en-Provence for the Cite de Livre in 1997 and was part of the La Caravane de la Poesie in 1999 - seven poets from Africa who travelled the ancient slave route from Goree back to Tombouctou. She also lead the English session at a Conference on Writing as a duty of Memory held in Rwanda. She is a Director of the Institute of Justice and Reconciliation.\nKrog's works have been translated into English, Dutch, Italian, French, Spanish, Swedish and Serbian. Her book Country of my Skull is being widely prescribed at Universities in America and Europe as part of the curriculum dealing with writing about the past. She was recently asked to translated the autobiography of Nelson Mandela, Long Walk to Freedom into Afrikaans.\nShe is married to architect John Samuel and has four children.\nHuman and Rousseau Publishers:\nDogter van Jefta (1970)\nJanuarie Suite (1972)\nBeminde Antarktika (1974)\nOtter in Bronslaai (1981)\nLady Anne (Taurus:1989)\nGedigte 1989-1995 (Hond: 1995)\nRelaas van 'n Moord (1995) Human and Rousseau\nKleur kom nooit alleen nie (Kwela 2000)\nMet woorde soos met kerse (Kwela 2002)\nCountry of my Skull (1998) Random House\nDown to my Last Skin (2000)\nPoetry for Young Children:\nMankepank en ander Monsters (1989) Human and Rousseau\nVoëls van anderster vere (1992) Buchu Books\nAccount of a Murder, translated by Karen Press (1997)\nLang Pad na Vryheid (translation of Long walk to Freedom by Nelson Mandela\nDomein van Glas (translation of Mondvol Glas by Henk van Woerden from\nDutch into Afrikaans)\nroad trips reviews politics renaissances credits/bios submissions links archives e-mail""]"	['<urn:uuid:6a85e850-a356-45ce-8ebc-1961758d4538>', '<urn:uuid:c243513f-e551-41f2-b32b-ef47c6627de9>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	6	87	2069
82	As an ethnobotanist studying cross-cultural healing traditions, I'm interested in how different cultures use medicinal plants - what does the Morris Arboretum exhibition reveal about various cultural approaches to plant medicine, and how does this compare to danshen's specific cultural significance?	The Arboretum's exhibition showcases diverse cultural practices, from African-American herbalists using weeds and food herbs, to Native American sacred traditions, and colonial settler remedies. In Asian traditions, particularly highlighted through danshen's use, the Chinese were the first to discover that healing with plants depends on the interaction of many different chemicals in plants. Danshen exemplifies this approach as it continues to be widely used in traditional Chinese medicine, often in combination with other herbs, demonstrating the complex understanding of plant medicine in Chinese culture.	"['Healing Plants: Medicine across Time and Cultures, an Outdoor Exhibition and Medicine Trunk\nContributed by Elayna Singer, Morris Arboretum, 9414 Meadowbrook Avenue, Philadelphia, Pennsylvania 19118, USA.\nThe Morris Arboretum has created an outdoor exhibition and a related hands-on curriculum which explore the cross-cultural, historical and world-wide use of medicinal plants. Exhibition panels introduce Arboretum visitors to the diversity of medicinal plants and their traditional use by African-Americans, Asians, colonial settlers and Native Americans. The Medicine Trunk contains six lessons and was created for use in grades 2-5. Children discover the similarities and differences between medicinal plants and the people who use them, with materials such as a map of the world, cards illustrated with medicinal plants, smell jars and plant presses. This interactive curriculum, for use in the schools, is complemented with a visit to the Arboretum. During their Healing Plants tour children search for live medicinal plants, pretend they are healers, and ‘prepare’ herbal remedies.\nBelow is an overview of the largest outdoor exhibition ever presented at the Morris Arboretum of the University of Pennsylvania, entitled “Healing Plants: Medicine across Time and Cultures”. Also included is an introduction to the Medicine Trunk curriculum and the Healing Plants tour for children, in addition to a compendium of Healing Plants programs for adults.\nThe Healing Plants project has been six years in the making. It began on July 5 1989 when a specially created Arboretum committee met to outline plans for an international symposium on ‘Plants as Medicine’, in collaboration with the World Health Organization. The symposium met in Philadelphia in 1993 and for the event we created a preliminary outdoor exhibition: a tour of medicinal trees at the Arboretum. Our modest self-guided tour greatly appealed to our visitors, and this persuaded the staff to consider expansion.\nWe approached the National Endowment for the Humanities (NEH), a Federal agency, with our idea for a major outdoor exhibition on medicinal plants. NEH was impressed by what we had accomplished and the ideas we presented. Subsequently in December of 1995 the Morris Arboretum was awarded $250,000 from NEH to create the Healing Plants outdoor exhibition and related programs.\nThe Healing Plants program has been a unique endeavor for the Arboretum staff in several ways. Not only is the outdoor exhibition the most significant interpretive effort at the Arboretum to date, it is also the first educational project that has been elevated to program status. In other words, the program was not developed in isolation by one person in one department. Instead, as project manager, I worked closely with staff from every department of the Arboretum including Horticulture, Development, the Center for Urban Forestry, Business, Botany, Communications, Physical Facilities, and Education. We met as a committee every two to three months to keep everyone informed of progress and to solicit input, and in between the committee meetings I worked individually with staff from different departments to move the program forward. The interdepartmental involvement in program development and implementation has cultivated a sense of ownership for the Healing Plants exhibition among the Arboretum staff. It has also increased the prominence of Education, with a capital ""E"", since staff from all departments have contributed in some way to this major educational program at the Arboretum.\nI believe that over the past year and a half, staff have increased their understanding of the educator\'s perspective in exhibition development and how it relates to their part in the process. I have seen staff become less resistant and more accepting of the rationale for approaching their work in a manner that facilitates reaching the set educational goals and objectives of the Healing Plants program. For example, the horticultural staff designed medicinal plant displays so that labels describing the medicinal uses of plants were easy to view. It was a new feat for the Arboretum to have educational goals dictate garden design.\nHealing Plants – an Evaluation and Prototyping\nThis program has been full of firsts for the Arboretum. In addition to it being our first major interdepartmental interpretive program, it was the first time that substantial resources were committed to evaluating prototypes before the permanent features were in place. It was also the first opportunity we had to develop a hands-on, interactive school curriculum with a complementary Arboretum tour for children.\nIn the fall of 1995 and the spring of 1996 staff and volunteers conducted evaluations of the outdoor exhibition, the Medicine Trunk and the children\'s tour. Staff involvement in the evaluation process greatly contributed to their increased ability to quote ""see"" things through the eyes of an educator. They learned evaluation techniques and were introduced to museum standards for developing interpretive panels. Despite some initial resistance, Physical Facilities staff supported the need to create prototype exhibit panels. They came to appreciate the value of crafting mock-up stations to actual size in the garden in order to critique possible designs for their aesthetic and functional qualities. Through evaluation we also learned a great deal about the content or information presented on our mock-up storyboards.\nEvaluators asked visitors a series of questions to see if the intended messages were clearly presented and of interest. As a result of visitors’ suggestions, some exhibition panels were completely changed, while others had only subtle changes made. Over and over again we received requests to include more information about current scientific research and the modern-day uses of medicinal plants. In our initial storyline developments we shied away from presenting much about the current use of medicinal plants, for fear of liability suits and concerns about accountability. Instead we presented more about the historic or past uses of medicinal plants.\nAs a result of our evaluation, all exhibition panels now highlight the past and present uses of medicinal plants. To protect ourselves we include a disclaimer or word of caution on every exhibition panel stating that the exhibition aims to provide information and stimulate awareness of plants as medicine. The information is primarily for reference and education. It is not intended to be used for self-diagnosis or self-medication. We also recommend that the diagnosis and treatment illness should come under the direction of a qualified health-care professional.\nThemes and Exhibition Areas\nThree major themes are presented throughout the exhibition and related programs:\n- the diversity of cultural practices and how this governs how medicinal plants are used in specific cultures and in different historical periods\n- the significance of botanic gardens and arboreta in Western scientific tradition and the important role they have played in plant medicine for several centuries\n- the ethical considerations related to the preservation, conservation and stewardship of the world\'s resources.\nSix garden areas have become outdoor mini-galleries with large exhibition panels. Below are some of the stories told in the exhibition:\nSince the rose garden at the Morris Arboretum is patterned after the design of other university gardens, it is a perfect place for us to learn about the some of the first university botanic gardens such as Padua and Oxford, which grew medicinal plants for use in their medical schools. Here visitors also learn about a medicinal tea made from the flowers of the apothecary\'s rose, which is approved in Germany for the treatment of mild inflammations of the mouth and throat. Visitors also discover that dog-rose hips are valued for their vitamin C.\nNear the herb garden visitors learn about the European tradition of growing medicinal herbs in cultivated garden areas. They see an example of a medieval herb garden and historic Philadelphia herb gardens such as the one at the College of Physicians of Philadelphia, which grows medicinal plants that were used in the 18th century.\nAnother station explores how botanic gardens, pharmaceutical companies, and other research organizations are working together to discover and cultivate medicinal plants. Visitors learn that only a small percentage of the world\'s flora has been tested for their medicinal value, but even this sample has already produced treatments for cancer, glaucoma, and other major diseases. The botanist Jay Walker of the Institute of Economic Botany of the New York Botanical Garden can be viewed collecting the pods of the San Juan tree, which are then shipped to the National Cancer Institute to be screened for their usefulness in treating cancer and AIDS. The Morris Arboretum, in collaboration with the international pharmaceutical company SmithKline Beecham, conducted research during 1988-1995 on Camptotheca acuminata, a valuable plant in the fight against cancer. This Chinese tree contains an anti-tumor compound recently approved by the Food and Drug Administration for treating severe cases of ovarian cancer in women whose cancer does not respond to other forms of treatment. Visitors also learn that weeds and other familiar plants have medicinal uses such as the dandelion, which aids digestion, and the purple coneflower, which soothes cold symptoms.\nThe multicultural emphasis of the Healing Plants exhibition is concentrated in three exhibit areas. One station is devoted to African-American herbal traditions. Visitors are introduced to Blanche Epps, a Philadelphia herbalist who specializes in the use of weeds and food herbs for healing. Perhaps to their surprise, visitors also learn that some plants that were used medicinally in the past should no longer be used, such as the sassafras tree, whose root bark was made into a tea to purify blood, for stomach and kidney problems and as a tonic. Today, modern research has shown that the essential oil of sassafras bark is a strong liver toxin and is cancer-causing.\nIn another station devoted to Native American and colonial plant medicine traditions, visitors see a sample recipe from a colonial remedy book and learn about the treasured medicinal plants that settlers brought with them. They also meet herbalists such as Nora Thompson Dean, a Delaware Indian, who they see sprinkling herbs over a sacred fire.\nIn the final exhibit area visitors learn about some of Asia\'s herbal heritage. The Chinese were the first to discover that using plants to heal depended on the interaction of many different chemicals in plants. The exhibition shows examples of familiar plants in American neighborhoods and botanic gardens which are used as medicine in China. Daylily root is used in China to treat cancer. Chrysanthemums are traditionally used to relieve headaches. Forsythia stems and leaves treat conditions of the heart and lungs. Fosythia seed capsules are used for urinary infections. In Kampo, Japan\'s traditional plant medicine, prescriptions contain up to thirteen different herbs.\nThe Healing Plants outdoor exhibition is primarily for adult visitors. Additional Healing Plants programs for adults include two indoor art exhibits. One featured Amazonian photographs by the father of ethnobotany, Dr. Richard Evans Schultes. The second was a print collection of medicinal plants mentioned in Shakespeare. As part of our adult continuing-education program we have offered classes and workshops on various topics related to healing plants. By spring 1997 a catalog and self-guided tour will be produced to complement the outdoor exhibition. Lastly, on October 12 1996, the opening of the outdoor exhibition with a Healing Plants festival will be celebrated. This will be a fun-filled day for the entire family.\nThe Medicine Trunk\nDesigning the Healing Plants program for children presented another challenge. Most children think of medicine as something that comes in a bottle, capsule or tube at their local drugstore. The Medicine Trunk was developed for school children in grades two through five, to teach them the cross-cultural, historical, and current uses of medicinal plants. The Medicine Trunk is a box that contains materials and a hands-on interactive curriculum. Prototype Medicine Trunks were developed and tested by teachers and students in the Fall of 1995 and the Spring of 1996. In the “Medicinal Plants in Your Life"" lesson, children interview adults to find out what they know about herbal remedies and they examine labels of common medicines which are made from plant compounds. In the ""Where do Coughdrops Come From""? lesson students learn how medicinal plants are used all over the world. Here we see sample medicinal plant cards. On the front of each card is a description of how the plant is used medicinally in that continent, and on the back of each card is general information about the continent\'s climate and the people who live there. Children do some independent research and make posters about medicinal plants that have significant economic importance such as corn, coffee, and cocoa. They then present these to their classmates. In the ""Creating a Neighborhood Herbal"" lesson students gather and press plants from their school yards or neighborhoods to create a classroom herbal. In both the Medicine Trunk and Healing Plants tour for children at the Arboretum, safety messages and warnings about the dangers of misusing plants or trying to use a plant as medicine without the guidance of an adult are reiterated and reinforced in every lesson and activity.\nHealing Plants Children’s Tour\nAfter children have done at least one lesson from the Medicine Trunk, they can visit the Arboretum for a special Healing Plants tour. On the tour children learn more about how people from different cultures use plants as medicine. Mortars and pestles, a kettle and saucepan; these are the tools students use to crush, grind and pretend to steep and boil samples of dried plants to make herbal remedies. In our log cabin volunteer guides pretend they are a colonial healer and ask students to imagine they are their apprentices. Together they ""prepare"" tea from plantain seeds, a lotion from witch-hazel twigs and leaves, and a tea from dandelion leaves.\nA Guide will ask “Where do medicinal plants grow? Did you know that healing plants grow everywhere, even in the grass under our feet""? With a frame in hand, students search the grass for valuable medicinal weeds such as white clover, plantain, oxalis, ground ivy and dandelion. Children are surprised to learn that the familiar yellow dandelion flower, as well as its roots and leaves, have all been used as medicine.\nIn the Fernery, students search for ferns with medicinal value. At the Arboretum\'s herb garden, students pretend they are a Native American, African-American or Asian herbalist. Their task is to guess a symptom (e.g. stomach ache, sore throat) acted out by one of the students, and then ""prescribe"" a plant to cure the patients ills. Colorful cards, with pictures of medicinal plants and description of how to treat certain symptoms, help children discover that, according to different cultural traditions, the same plant sometimes treats different ailments and at other times different plants treat the same ailments. Children read and share information about the plants on their cards, then search for the live plants growing in the garden.\nEvaluation and Feedback\nThe Medicine Trunk and the tour seem to be successful in providing a way for children to begin to reshape their attitudes about medicine AND plants. Teachers have also been pleased with using the prototype Medicine Trunk, because its lessons can be integrated into almost any curriculum, for example: social studies, science, mathematics, and history. The visit to the Arboretum then brings the lessons to life for both student and teacher.\nHealing Plants: Medicine Across Time and Cultures is a significant achievement for the Morris Arboretum. As educators we are reaching outside of our department to draw on the expertise of the entire Arboretum staff. As plant scientists we are reaching outside of our own sphere of research to consult with local herbalists and practitioners. And finally as exhibit curators we are continually asking our audience how we can better communicate the layers of culture and science that we have uncovered in the creation of this program.', 'Drugs A - Z\nDanshen (Salvia miltiorrhiza)\nGeneric Name: dan-shen\nCategoryHerbs & Supplements\n3,4-dihydroxyphenyl-lactic acid, caffeic acid, Ch\'ih Shen (scarlet sage), Chinese Salvia, cryptotanshisone, dangshem, Dan-Shen, Dan Shen, danshen root, danshensu, dihydrotanshinone, ethyl acetate, fufangdenshen, horse-racing grass, Huang Ken, Hung Ken (red roots), Labiatae (family), Lamiaceae (family), lithospermic acid B, miltirone, neo-tanshinlactone, phenolic acids, Pin-Ma Ts\'ao (horse-racing grass), protocatechualdehyde, protocatechuic acid, protocatechuic aldehyde, Radix salvia miltiorrhiza, rat-tail grass, red-rooted sage, red roots, red sage, red sage root, red saye root, roots of purple sage, Salvia bowelyana, Salvia miltiozzhiza, Salvia miltiozzhiza bunge, Salvia przewalskii, Salvia przewalskii mandarinorum, salvia root, Salvia yunnanensis, salvianolic acid B, scarlet sage, Sh\'ih Shen, Shu-Wei Ts\'ao (rat-tail grass), Tan Seng, Tan-Shen, tanshisone I, tanshisone IIA, tanshisone IIB, Tzu Tan-Ken (roots of purple sage), yunzhi danshen.\nNote: Danshen should not be confused with sage. Danshen is often used in combination with other products; combination products are not specifically discussed in this monograph.\nDanshen (Salvia miltiorrhiza) is widely used in traditional Chinese medicine (TCM), often in combination with other herbs. Remedies containing danshen are used traditionally to treat a diversity of ailments, particularly cardiac (heart) and vascular (blood vessel) disorders such as atherosclerosis (""hardening"" of the arteries with cholesterol plaques) or blood clotting abnormalities.\nThe ability of danshen to ""thin"" the blood and reduce blood clotting is well documented, although the herb\'s purported ability to ""invigorate"" the blood or improve circulation has not been demonstrated in high-quality human trials. Because danshen can inhibit platelet aggregation and has been reported to potentiate (increase) the blood-thinning effects of warfarin, it should be avoided in patients with bleeding disorders, prior to some surgical procedures, or when taking anticoagulant (blood-thinning) drugs, herbs, or supplements.\nIn the mid-1980s, scientific interest was raised in danshen\'s possible cardiovascular benefits, particularly in patients with ischemic stroke or coronary artery disease/angina. More recent studies have focused on possible roles in liver disease (hepatitis and cirrhosis) and as an antioxidant. However, the available research in these areas largely consists of animal studies and small human trials of poor quality. Therefore, firm evidence-based conclusions are not possible at this time about the effects of danshen for any medical condition.\nEvidenceDISCLAIMER: These uses have been tested in humans or animals. Safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider.\nBetter studies are needed in which danshen is compared with more proven treatments before a clear conclusion can be drawn.\nCardiovascular disease / angina:\nA small number of poor-quality studies report that danshen may provide benefits for treating disorders of the heart and blood vessels, including heart attacks, cardiac chest pain (angina), or myocarditis. Danshen may have effects on blood clotting and therefore may be unsafe when combined with other drugs used in patients with cardiovascular disease. Patients should check with a physician and pharmacist before combining danshen with prescription drugs.\nEarly studies have found that danshen in combination with routine western medicine was not as effective as warming needle moxibustion. More studies are warranted in this area to draw a firm conclusion.\nDiabetic complications (diabetic foot):\nEarly clinical trials suggest danshen may help treat diabetic foot. Well-designed clinical trials are needed before a strong recommendation can be made.\nEarly studies suggest that danshen may speed peritoneal dialysis and ultrafiltration rates when added to dialysate solution. Although this evidence seems promising, it is not known whether danshen is safe for this use. Further research is necessary.\nDanshen may be beneficial in glaucoma therapy, but further studies are needed in humans before a clear conclusion can be drawn. Danshen should not be used in place of more proven therapies, and patients with glaucoma should be evaluated by a qualified eye care specialist.\nEarly studies suggest that danshen may improve blood levels of cholesterol (lowers LDL or ""bad"" cholesterol and triglycerides and raises HDL or ""good"" cholesterol). Large high-quality studies are needed before a strong recommendation may be made.\nAlthough early evidence is promising, it is not known whether danshen is safe for this use. Danshen injection may be helpful for recovery of kidney function after kidney transplant. Further research is needed to confirm these results.\nLiver disease (cirrhosis, chronic hepatitis B, fibrosis):\nSome studies suggest that danshen may provide benefits for treating liver diseases such as cirrhosis, fibrosis, and chronic hepatitis B. However, it is unclear whether there are any clinically significant effects of danshen in patients with liver disease.\nFor many years, danshen has been used as a traditional Chinese medicine (TCM) remedy to treat acute pancreatitis. However, little research is currently available regarding the use of danshen in humans.\nDue to poor quality of evidence, unclear safety, and the existence of more proven treatments for ischemic stroke, this use of danshen cannot be recommended.\nThere is not enough evidence to recommend either for or against the use of danshen for vasovagal syncope.\nTinnitus (ringing in the ears):\nLimited evidence suggests that danshen in combination with other herbs and supplements may be a less effective treatment for tinnitus than acupuncture. Additional research is needed to fully understand danshen\'s effects on tinnitus.\nOne study using a combination product that included danshen found that there was no effect on food intake or weight loss. More high-quality studies are needed to confirm these results.\nTraditionWARNING: DISCLAIMER: The below uses are based on tradition, scientific theories, or limited research. They often have not been thoroughly tested in humans, and safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider. There may be other proposed uses that are not listed below.\nAdults (18 years and older)\nOral dosing has not been studied in well-conducted trials in humans, and therefore no specific dose can be recommended.\nIn research from the 1970s, an 8 milliliter injection of danshen (16 grams of the herb) was given intravenously (diluted in 500 milliliters of a 10% glucose solution) for up to four weeks for ischemic stroke. Safety and effectiveness have not been established for this route of administration and it cannot be recommended at this time.\nChildren (younger than 18 years)\nThere is not enough scientific evidence to recommend the safe use of danshen in children, and it should be avoided due to potentially serious side effects.\nSafetyDISCLAIMER: Many complementary techniques are practiced by healthcare professionals with formal training, in accordance with the standards of national organizations. However, this is not universally the case, and adverse effects are possible. Due to limited research, in some cases only limited safety information is available.\nPeople with known allergy to danshen (Salvia miltiorrhiza) or its constituents (such as protocatechualdehyde, 3,4-dihydroxyphenyl-lactic acid, tanshinone I, dihydrotanshinone, cryptotanshione, miltirone, or salvianolic acid B) should avoid this herb. Danshen is often found in combination with other herbs in various formulations, and patients should read product labels carefully. Signs of allergy may include rash, itching, or shortness of breath.\nSide Effects and Warnings\nDanshen may increase the risk of bleeding. This herb is reported to inhibit platelet aggregation and to increase the blood-thinning effects of warfarin in humans. Caution is advised in patients with bleeding disorders, in patients taking drugs that may increase the risk of bleeding, and prior to some surgical procedures. Dosing adjustments may be necessary.\nSome people may experience stomach discomfort, reduced appetite, or itching.\nIn theory, danshen may lower blood pressure and should be used cautiously by patients with blood pressure abnormalities or taking drugs that alter blood pressure.\nIn theory, a chemical found in danshen called miltirone may increase drowsiness. Caution is advised while driving or operating machinery.\nConvulsions, mental changes, and dystonia syndrome may occur.\nPregnancy and Breastfeeding\nDanshen should be avoided during pregnancy and breastfeeding. In theory, the blood-thinning properties of danshen may increase the risk of miscarriage or bleeding, and effects on the fetus or nursing infants are not known.\nInteractions with Drugs\nDanshen may increase the risk of bleeding when taken with drugs that also increase the risk of bleeding. This herb is reported to inhibit platelet aggregation and to cause over-anticoagulation (excessive ""blood-thinning"" effects) in patients taking the blood thinner warfarin (Coumadin®). Examples of drugs that increase the risk of bleeding include aspirin, anticoagulants such as warfarin (Coumadin®) or heparin, anti-platelet drugs such as clopidogrel (Plavix®), and non-steroidal anti-inflammatory drugs such as ibuprofen (Motrin®, Advil®) or naproxen (Naprosyn®, Aleve®).\nIn theory, the risk of side effects or toxicity from digoxin (Lanoxin®) may be increased if taken with danshen. In addition, danshen may cause laboratory measurements of digoxin blood levels to be inaccurate (too high or too low).\nDanshen may result in hypotension (dangerously low blood pressure) if taken with drugs that also lower blood pressure, such as ACE-inhibitors like captopril (Capoten®) or lisinopril (Prinivil®) and beta-blockers like atenolol (Tenormin®) or propranolol (Inderal®). In addition, the use of danshen with beta-blockers may cause bradycardia (dangerously slow heart rate).\nIn theory, a chemical found in danshen called miltirone may increase sleepiness or other side effects associated with some drugs taken for anxiety or insomnia, such as lorazepam (Ativan®), alprazolam (Xanax®), and diazepam (Valium®), or alcohol. In addition, based on animal studies, danshen may affect the absorption of alcohol into the blood.\nInteractions with Herbs and Dietary Supplements\nDanshen may increase the risk of bleeding when taken with herbs and supplements that are believed to increase the risk of bleeding. Multiple cases of bleeding have been reported with the use of Ginkgo biloba, and fewer cases with garlic and saw palmetto.\nIn theory, danshen may add to the effects of other herbs, such as hawthorn, with potential cardiac glycoside properties, potentially resulting in slow heart rate or toxicity.\nDanshen should be used cautiously with herbs/supplements that may also lower blood pressure.\nIn theory, a chemical found in danshen called miltirone can increase the amount of drowsiness that may be caused by other herbs or supplements.\nAntibacterials, anti-inflammatory herbs, antilipemics, antineoplastics, antioxidants, antivirals, steroids, astragalus, chronotropic herbs, herbs and supplements broken down by the liver, immunosuppressants, Gexia zhuyu decoction, licorice, Ligusticum chuanxiong, Ligustrum lucidum, Polyporus, Serissa, Sophora subprostrata, and Yun zhi (Coriolus mushroom) may interact with danshen.\nThis information is based on a professional level monograph edited and peer-reviewed by contributors to the Natural Standard Research Collaboration (www.naturalstandard.com): Ethan Basch, MD (Memorial Sloan-Kettering Cancer Center); Dawn Costa, BA, BS (Natural Standard Research Collaboration); Nicole Giese, MS (Natural Standard Research Collaboration); Jenna Hollenstein, MS, RD (Natural Standard Research Collaboration); Brooke Sweeney, PharmD (Massachusetts College of Pharmacy); Shaina Tanguay-Colucci, BS (Natural Standard Research Collaboration); Catherine Ulbricht, PharmD (Massachusetts General Hospital); Mamta Vora, PharmD (Northeastern University); Wendy Weissner, BA (Natural Standard Research Collaboration).\nBibliographyDISCLAIMER: Natural Standard developed the above evidence-based information based on a thorough systematic review of the available scientific articles. For comprehensive information about alternative and complementary therapies on the professional level, go to www.naturalstandard.com. Selected references are listed below.\nBrunetti G, Serra S, Vacca G, et al. IDN 5082, a standardized extract of Salvia miltiorrhiza, delays acquisition of alcohol drinking behavior in rats. J Ethnopharmacol 2003;85(1):93-97.\nChan K, Chui SH, Wong DY, et al. Protective effects of Danshensu from the aqueous extract of Salvia miltiorrhiza (Danshen) against homocysteine-induced endothelial dysfunction. Life Sci 11-12-2004;75(26):3157-3171.\nChan TY. Interaction between warfarin and danshen (Salvia miltiorrhiza). Ann Pharmacother 2001;35(4):501-504.\nCheng TO. Warfarin danshen interaction. Ann Thorac Surg 1999;67(3):894.\nJi X, Tan BK, Zhu YC, et al. Comparison of cardioprotective effects using ramipril and DanShen for the treatment of acute myocardial infarction in rats. Life Sci 2003;73(11):1413-1426.\nLiu F, Liu Y, Li J. [Effects of danshen on solute transport by peritoneal dialysis]. Hunan Yi Ke Da Xue Xue Bao 1997;22(3):237-239.\nLiu GY. Analysis of effect of composite danshen droplet pills in treatment of chronic stable angina. Hubei J Trad Chin Med 1997;19(2):33-34.\nLiu Z, Gao Sy, Deng J, et al. Analysis of effect of composite danshen droplet pills in treatment of chronic stable angina. Chin Trad Pat Med 1997;19(7):20-21.\nLo CJ, Lin JG, Kuo JS, et al. Effect of salvia miltiorrhiza bunge on cerebral infarct in ischemia-reperfusion injured rats. Am J Chin Med 2003;31(2):191-200.\nMashour NH, Lin GI, Frishman WH. Herbal medicine for the treatment of cardiovascular disease: clinical considerations. Arch Intern Med 1998;158(20):2225-2234.\nNatural Standard Research Collaboration, Chief Editors: Ulbricht C, Basch E, Natural Standard Herb and Supplement Reference - Evidence-Based Clinical Reviews, USA: Elsevier/Mosby, 2005.\nSha Q, Cheng HZ, Xie XY. Salviae miltiorrhizae composita pill for treating 47 cases of active liver cirrhosis. Chin J Integrat Trad West Med Liver Dis 1999;9(6):50.\nVacca G, Colombo G, Brunetti G, et al. Reducing effect of Salvia miltiorrhiza extracts on alcohol intake: influence of vehicle. Phytother Res 2003;17(5):537-541.\nWu B, Liu M, Zhang S. Dan Shen agents for acute ischaemic stroke. Cochrane Database Syst Rev 2007;(2):CD004295.\nXue YP, Zhang SB, Gao T. [Observation on therapeutic effect of chronic prostatitis treated mainly by warming needle moxibustion] Zhongguo Zhen Jiu 2006 May;26(5):335-6.\nRemember, keep this and all other medicines out of the reach of children, never share your medicines with others, and use this medication only for the indication prescribed.']"	['<urn:uuid:3541029b-834e-4348-8380-a2cf4f87d06a>', '<urn:uuid:f40aa749-e9fb-4b30-8f2a-727a761dfd84>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	41	84	4765
83	For my research project - how do turtles and fish save energy when food is scarce?	Both turtles and fish conserve energy through slowed metabolism and reduced activity. Turtles can enter hibernation or suspended animation, while fish enter a dormant state, reduce their swimming, and rely on anaerobic metabolism to generate energy without oxygen.	['Being Slow, Small and Aquatic may have Saved the Turtles\nThe Chelonia, that Order of reptiles that includes tortoises, turtles and terrapins may have had the advantage over the Dinosauria when it came to surviving global catastrophes. These ancient reptiles, that first evolved sometime in the Triassic with their slow metabolisms, cold-blooded energy needs and the ability to hibernate or at least enter into prolonged periods of suspended animation may be one of Natures great survivors.\nThese tough creatures managed to survive the Cretaceous mass extinction event that saw the demise of the Pterosaurs, Dinosaurs and marine reptiles, in fact something like 65% of all life on Earth died out, because turtles and tortoises have slow metabolisms and a lot of them live in water all huge benefits according to new American research.\nPalaeontologist Tyler Lyson of Yale University, who has been studying one particular part of the Chelonia family tree that seem to have remained unchanged from 85 million years ago, through to the mass extinction event and beyond into the Cenozoic commented:\n“Turtles are very tough animals, if times get tough they can go into a state of animation. Animals that were living in the water were kind of protected against whatever killed the land plants and the dinosaurs.”\nEssentially, since their bodily processes were so slow, needing very little energy, they could survive on sparse resources during and after the wipe out of dinosaurs. Being able to burrow and to hide out underground would also have been a major advantage for these small reptiles, something that a six tonne Triceratops, an Ankylosaurus or indeed T. rex could never have done.\nThe research team’s conclusion is based on a newly discovered turtle fossil from North Dakota, known as Boremys which dates back to between 60 million and 65 million years ago (Palaeocene Epoch). The specimen belongs to a turtle species thought to have survived the global extinction, Lyson said, because fossils of the same species have been found in rocks deposited up to 75 million years ago indicating that these particular reptiles survived relatively unchanged through the end of the Mesozoic and into the Cenozoic.\nSurviving the Extinction Event – Boremys rests on the Skull of a Triceratops\nPicture Credit: Yale Peabody Museum/Brian T. Roach\nThe global extinction event that killed the dinosaurs, called the K-T boundary due to its special signature in rock layers, was most likely set off by a meteorite strike, though the true sequence of events is hotly debated. We at Everything Dinosaur wrote an article earlier this week on the discovery of a Ceratopsian horn very close to the K-T boundary, which marks the end of the Age of Dinosaurs.\nTo read more about this discovery: The Last Dinosaur Standing\nSome researchers believe a set of world-shattering volcanic eruptions darkened the sky, which may or may not have been caused by an extraterrestrial impact. The turtles, along with other burrowing and water-living animals, survived the dinosaur-killing whole-Earth extinction event, which extinguished the vast majority of the animal and plant species living on land, including land-living turtles. Crocodiles too, as they are mainly aquatic and cold-blooded also survived but the Dinosauria, a group of reptiles that had ruled the land for some 150 million years, did not make it through into the Palaeocene (let’s not include Aves for the moment).\nCrocodiles can enter into a period of summer-sleep (estivation), Nile crocodiles excavate large borrows or occupy underground caves formed by the erosion of soil from tree roots alongside river banks. Here safely entombed in their cool, sleeping chambers these animals effectively shut down, slowing their metabolic rates and this enables them to survive long, hot, dry seasons or other difficult climatic conditions. An ability to estivate would have come in handy around sixty-five million years ago.\n“If you only looked at turtles across this boundary you wouldn’t think there was an extinction. Small animals that have a slow metabolism and live in the water do very well across the K-T boundary.”\nThese turtles lived in lakes and streams in North America, where they ate soft plants and crustaceans. They would have resembled the painted or cooter turtles of today, Lyson said, though they aren’t closely related to any living turtle species. They were part of a very large group of species called the Baenid turtles, at least eight of which survived the extinction event only to vanish later by some other means.\nAfter the land-based wipe out, the remaining small mammals populating the Earth spread in what’s called “adaptive radiation,” where a limited number of species fans out and diversifies in empty habitats. The living mammals underwent rapid evolution and spread into the niches vacated by other animals, including the dinosaurs.\nEven though turtles had the metabolic upper hand to survive the extinction event, it was the agile, fast-breeding mammals that gained the advantage when it came to exploiting all those vacated niches.\n“In the water, before and after the boundary, it was business as usual. A lot of these smaller species are around right after the impact. Not a whole lot changed. Mammals just have more of a rapid turnover, so they are able to more quickly adapt to their environment and their changing surroundings.”\nHowever, even the hardy Baenid turtles cannot be guaranteed survival forever. This particular family of the Order Chelonia finally became extinct around 40 million years ago, towards the end of the Eocene Epoch when North America experienced climate change and became much drier.\nScientists have long speculated on the serendipity of mass extinctions. Some types of animal can survive relatively unchanged, whilst in other cases entire Orders can become extinct. At the end of the Cretaceous, being small, having low energy needs and a fondness for water may have cushioned such creatures against the very worst of the global climate change.', 'Understanding the Phenomenon of Fish Slowing Down in Cold Weather\nWhen the temperatures drop and winter sets in, it’s not uncommon to notice fish slowing down in their movements. This phenomenon is an interesting adaptation that fish have developed to cope with the cold weather. As the water temperature decreases, so does the metabolism of fish. Metabolism is the process through which fish convert food into energy to fuel their daily activities. In colder temperatures, the metabolic rate of fish decreases, causing their movements to slow down. This decrease in metabolism helps fish conserve energy during the winter when food sources may be scarce and conditions are less favorable for activity. So, if you’ve ever wondered why your fish seem more sluggish in the colder months, now you know it’s all part of their survival strategy. But how exactly do fish adapt to these changing environmental conditions? Let’s dive deeper into this fascinating topic.\nThe Role of Fish Metabolism in Winter Survival\nFish metabolism plays a crucial role in their survival during the winter months. As the temperature drops, fish undergo physiological changes to slow down their metabolism. This is their way of conserving energy and adapting to the changing environmental conditions. When their metabolism slows down, fish require less food and oxygen, which can be scarce in cold waters. By reducing their metabolic rate, fish are able to endure the harsh conditions and increase their chances of survival.\nDuring winter, fish also rely on a special type of metabolism called anaerobic metabolism. This allows them to generate energy without the need for oxygen. Anaerobic metabolism is particularly important when fish hibernate and enter a state of dormancy. By relying on anaerobic metabolism, fish can conserve energy and sustain themselves for extended periods without needing to actively swim or hunt for food. It is truly remarkable how fish have adapted to harness the power of their metabolism to survive the challenges of winter. Understanding this important aspect of their biology can help us better care for fish in aquariums and appreciate the incredible strategies they employ to brave the cold.\nHow Do Fish Adapt to the Changing Environmental Conditions?\nFish are remarkably adaptable creatures when it comes to changing environmental conditions. One fascinating way they adapt is through behavioral changes. For instance, during the winter months when water temperatures drop, fish often become less active. They slow down their movements and conserve energy by staying in deeper, calmer waters. This alteration in behavior helps them cope with the colder environment and survive harsh conditions.\nIn addition to behavioral changes, fish also possess a range of physiological adaptations that enable them to thrive in different environmental settings. One such adaptation is the ability to alter their metabolism. Fish can adjust their metabolic rate in response to varying temperature conditions. When the water becomes colder, their metabolism slows down, reducing their need for food and energy. This mechanism allows them to conserve energy during lean winter months when food sources may be limited. By adapting their metabolism, fish can successfully navigate the changing environmental conditions and ensure their survival.\n• Fish slow down their movements and conserve energy by staying in deeper, calmer waters during the winter months when water temperatures drop.\n• This alteration in behavior helps them cope with the colder environment and survive harsh conditions.\n• Fish can adjust their metabolic rate in response to varying temperature conditions.\n• When the water becomes colder, their metabolism slows down, reducing their need for food and energy.\n• This mechanism allows them to conserve energy during lean winter months when food sources may be limited.\nExploring the Natural Strategies Fish Employ to Conserve Energy during Winter\nIn the treacherous winter months, fish face the challenge of conserving energy in order to survive. These resilient creatures have developed remarkable strategies to adapt to the changing environmental conditions. One common tactic is to slow down their metabolism, allowing them to conserve precious energy reserves. By reducing their activity levels and slowing their bodily functions, fish are able to evade the harsh realities of dwindling food supplies and colder water temperatures. This natural strategy helps fish navigate through the winter months with limited energy resources, ensuring their survival until more favorable conditions arise.\nAnother notable way in which fish conserve energy during winter is through the process of hibernation. Similar to how bears go into a deep sleep during the colder months, certain fish species enter a state of dormancy. During this period, their metabolism significantly decreases, enabling them to conserve energy by using minimal resources. Fish in hibernation become less responsive to external stimuli and may even seek out calm and sheltered areas to remain undisturbed. By adopting this energy-saving technique, fish are able to endure the challenging winter conditions and emerge strong and resilient when spring returns.\nUnveiling the Remarkable Mechanisms Behind Fish Hibernation\nFish hibernation is a fascinating phenomenon that allows these creatures to survive in harsh winter conditions. During hibernation, fish slow down their metabolism, which reduces their energy expenditure and enables them to conserve energy for extended periods. This remarkable mechanism involves a series of physiological changes within the fish’s body.\nOne of the key factors behind fish hibernation is the decrease in water temperature. As the temperature drops, fish experience a decrease in metabolic activity. This reduction in metabolic rate allows them to lower their oxygen consumption and adapt to the limited availability of food during winter. Additionally, fish also adjust their behavior during hibernation by seeking shelter in protected areas or deep waters, where the temperature is relatively stable. By intuitively reacting to the changing environmental conditions, fish have developed remarkable mechanisms to survive the challenging winter months.\nThe Impact of Decreased Water Temperature on Fish Behavior\nFor fish, changes in water temperature can have a significant impact on their behavior. As the water temperature decreases during the colder months, fish tend to slow down and become less active. This is because their metabolic rate decreases in colder temperatures, leading to a decrease in overall energy levels. As a result, fish may swim more slowly, feed less frequently, and exhibit reduced exploratory and social behaviors.\nThe impact of decreased water temperature on fish behavior is not only influenced by their internal physiological changes but also by external factors. For instance, colder water temperatures can affect the availability and distribution of food sources. Many fish species rely on certain prey items that might migrate or become scarce during the winter months. This scarcity of food can lead to changes in feeding behavior, as fish may need to expend more energy searching for food or adapt their diet to alternative food sources. Additionally, decreased water temperature can also affect water chemistry, such as oxygen levels, which can further influence fish behavior. In some cases, low oxygen levels can cause fish to become more sluggish or even result in mortality if oxygen becomes severely limited. Therefore, understanding the impact of water temperature on fish behavior is crucial for ensuring the well-being and survival of fish populations.\nExamining the Effect of Reduced Food Availability on Fish during Winter\nFish face numerous challenges during the winter season, one of which is the reduced availability of food. With the drop in water temperature, many food sources become scarce or go into hibernation themselves. This means that fish must find alternative ways to sustain themselves and survive the winter months.\nWhen food becomes limited, fish exhibit remarkable adaptations to cope with the reduced availability. Some species lower their metabolic rate, which allows them to conserve energy and survive longer without needing to eat. By slowing down their metabolism, fish can make the most of the limited food resources they find. Additionally, they may alter their feeding habits, shifting to a more opportunistic approach where they consume whatever prey or plant matter they come across. This flexibility in diet helps them meet their nutritional needs despite the scarcity of their preferred food sources.\nReduced food availability during winter creates a survival challenge for fish. However, by adjusting their metabolism and consuming a wider range of food, they can endure these harsh conditions and emerge healthy and resilient when the warmth of spring returns. Understanding these adaptations can help us appreciate the incredible resilience and resourcefulness of fish in the face of changing environmental conditions.\nDiscovering the Surprising Ways Fish Prepare for Hibernation\nMany people are surprised to learn that fish, much like bears and other mammals, have a fascinating way of preparing for hibernation during the winter months. It turns out that these aquatic creatures employ remarkable strategies to survive in the cold waters. One of the surprising ways fish prepare for hibernation is by reducing their metabolism. Just like how a bear’s heart rate slows down during hibernation, fish also lower their metabolic rate to conserve energy. This allows them to endure the harsh conditions with limited food sources. Fish metabolism adapts to the changing environmental conditions to ensure their survival during the winter season.\nAdditionally, another surprising way that fish prepare for hibernation is by altering their behavior. Instead of actively seeking food, fish become less active and stay in sheltered areas. They tend to swim at a slower pace, conserving their energy for the long winter months ahead. This change in behavior is crucial for their survival, as it reduces the need for constant movement and minimizes the risk of predators detecting and capturing them. It is truly fascinating to observe the natural strategies that fish employ to conserve energy during winter and ensure their survival until the warmer months return.\nThe Importance of Proper Winter Care for Pet Fish\nThe winter season can be a challenging time for our pet fish. As the days become colder and shorter, it is important for us as fish owners to provide the proper care to ensure their well-being. One key aspect of winter care is maintaining an appropriate temperature in the aquarium. Fish are ectothermic creatures, which means that their body temperature is regulated by the environment. During winter, the drop in temperature can slow down their metabolism, making them more susceptible to illness or even death if not properly managed. It is essential to invest in a reliable aquarium heater to ensure that the water temperature remains within the recommended range for your specific fish species. By taking this simple step, you are providing a comfortable and safe environment for your pet fish during the winter months.\nAnother crucial aspect of winter care involves providing adequate nutrition for your fish. As the temperature decreases, fish’s metabolism slows down, reducing their appetite. It is important to adjust their feeding schedule accordingly, taking into consideration their reduced activity level. Overfeeding can lead to health issues as uneaten food can quickly contaminate the water, causing ammonia spikes and other imbalances in the aquarium. Additionally, providing a varied diet rich in nutrients is vital to support their immune system during the colder months. Ensuring that your pet fish receive the right amount and type of food will contribute to their overall health and well-being during the winter season.\nTips for Maintaining a Healthy Aquarium during the Winter Season\nHaving a healthy aquarium is crucial, especially during the winter season when the temperature drops. There are a few tips that can help you maintain a healthy environment for your fish during this time. Firstly, make sure to keep the water temperature stable. Fluctuating temperatures can put stress on your fish, so use a reliable aquarium heater to keep the water at an optimal level. Additionally, it’s important to monitor the water quality regularly. Test the water for ammonia, nitrite, and nitrate levels to ensure they are within the appropriate range. A good filtration system can help maintain a clean and healthy environment for your fish. Finally, don’t forget to feed your fish appropriately. Adjust their feeding schedule according to their needs and avoid overfeeding, as excess food can pollute the water and lead to health issues for your fish.\nAs the winter season brings colder temperatures, it’s important to take extra care when maintaining your aquarium. Consider insulating the tank to prevent temperature fluctuations. You can use foam insulation or insulating blankets to cover the tank, ensuring that the water temperature remains stable. Another tip is to minimize evaporation by keeping a lid on the aquarium. This will help maintain humidity and prevent excessive heat loss. It’s also worth considering the lighting in your aquarium. Fish require a regular day-night cycle, so make sure to provide adequate lighting for their well-being. Lastly, do not forget to check the electrical connections regularly to ensure safety. Following these tips will help you maintain a healthy and thriving aquarium for your fish throughout the winter season.\nWhy do fish slow down in cold weather?\nFish slow down in cold weather because their metabolism decreases, which helps them conserve energy and survive in cold conditions.\nHow do fish adapt to changing environmental conditions during winter?\nFish adapt to changing environmental conditions by slowing down their metabolism, seeking out warmer areas of the water, and conserving energy to survive the winter.\nWhat are some natural strategies fish use to conserve energy in winter?\nSome natural strategies fish use to conserve energy in winter include reducing their activity levels, staying in deeper and warmer parts of the water, and minimizing unnecessary movements.\nHow does fish hibernation work?\nFish hibernation is a state of dormancy where their metabolism significantly slows down, and they enter a period of decreased activity to conserve energy and survive the winter.\nHow does decreased water temperature affect fish behavior?\nDecreased water temperature can affect fish behavior by slowing down their movements, reducing their appetite, and potentially causing them to seek out warmer areas of their habitat.\nWhat is the effect of reduced food availability on fish during winter?\nReduced food availability during winter can lead to decreased energy levels in fish, causing them to conserve energy by reducing their activity levels and slowing down their metabolism.\nHow do fish prepare for hibernation?\nFish prepare for hibernation by gradually decreasing their activity levels, seeking out warmer areas of the water, and building up fat reserves to sustain them during the winter months.\nWhy is proper winter care important for pet fish?\nProper winter care is important for pet fish to ensure their health and well-being during the colder months. It helps them cope with the changes in their environment and ensures their survival.\nWhat are some tips for maintaining a healthy aquarium during the winter season?\nSome tips for maintaining a healthy aquarium during the winter season include keeping the water temperature stable, providing adequate lighting, reducing feeding frequency, and monitoring water quality.']	['<urn:uuid:04248938-dc7c-42cc-bda0-e58f195aea8d>', '<urn:uuid:f168809b-deb7-46bf-900d-926c88fd00fe>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	16	38	3425
84	How are bison related to climate change in the Great Plains, both as victims of its effects and as potential helpers in fighting it?	Bison are both victims and potential allies in the climate change situation in the Great Plains. As victims, they face challenges because climate change is making the western Plains warmer and drier, with less forage available for their survival. The region has warmed 3-4 degrees Fahrenheit in the last century, and increasing drought episodes will limit or prevent bison reintroduction. However, bison can also help fight climate change through carbon sequestration. When bison graze on grasslands, their movements and behaviors are crucial for building soil, maintaining biodiversity, and deepening plant roots. They till the soil with their hooves and work dung into it, helping plants flourish. These grasslands can store massive amounts of carbon in their deep root structures (8-15 feet underground), storing up to 22.5 million tons of carbon and processing 1.7 million tons per acre annually, making them as effective as forests in carbon storage but more reliable since the carbon remains locked underground even during fires.	['In 1987 geographers Frank and Deborah Popper wrote their now-famous “Buffalo Commons” article. In the article, published in December of that year in Planning magazine under the title “The Great Plains: From Dust to Dust,” the Poppers argued that the interplay of environmental, economic, and demographic variables would foster the eventual formation of a “Buffalo Commons” across remote, depopulated segments of the American Great Plains. This “Commons” would consist of large bison herds, which according to the Poppers represented the most environmentally sustainable and economically viable use of the semiarid grassland regions of the continental United States.\nYet, over twenty-five years after the publication of the widely readarticle, it is now evident that the Poppers’ vision of the Great Plains will never come to pass. Three environmental factors (all related to climate change)guarantee that the Buffalo Commons will remain only a theoryrather than an actual means of organizing the Great Plains environment and economy.\n1. A Warmer and Drier Climate. Scientists have concluded that the western regions of the Great Plains, which includes many areas examined by the Poppers, are becoming drier and warmer. In the last century, the Plains have have warmed between three and four degrees Fahrenheit. This warming will continue. As climate change accelerates, portions of the western Plains will experience more frequent drought episodes, which translates into less forage for bison. Forage is the single greatest determinant of herd size and survivability. The sparseness of forage across the western Plains will limit, or completely impede, the reintroduction of bison to their former range. Put simply, desertification stemming from climate change will forestall a Buffalo Commons. You can’t have bison without grass.\n2. The Dwindling Water Resource. As the western Great Plains heats up and annual precipitation amounts decline, rivers will carry less water; reservoirs, wetlands, stock ponds, and glacial potholes will no longer fill to capacity; and the Ogallala Aquifer will be tapped out of existence. In the new American desert that emerges, the Great Plains’ dominant species, Homo Sapiens Americanus (which is a particularly large subspecies of Homo Sapiens) will increasingly monopolize the area’s depleted water resources, leaving little or none for other life forms. Without water, bison will have no place on the Plains.\n3. The Absence of the Region’s Former Oases. The third, and most important, reason the Buffalo Commons idea will forever remain the pipe dream of the Environmental Left is tied to the first and second reasons. Homo Sapiens occupy, and will continue to occupy, the prime ecological real estate across the Great Plains. In the nineteenth century, before European Americans violently seized the grassland from the Native Americans, and when bison still roamed the Plains in the millions, there existed a series of oases between the Missouri River and Rocky Mountains. These oases held the bulk of the region’s biological diversity. An array of plants, birds, mammals, and reptiles concentrated in the oases. During drought episodes and severe winters (or the occasional little ice ages that descended on the Plains), the oases acted as sanctuaries, sustaining bison herds and other critters against the ravages of the Great Plains’ frequently violent climate. The oases were instrumental in the long-term survival of large bison herds.\nIn the nineteenth century, plainsmen, wholly ignorant of ecology and convinced of their God-given right to slaughter everything possessing a hide or devilish beady eyes, referred to the Great Plains’ oases as “bottoms,” which was a derivative of the term “river bottomland.” But the bottoms were not just any bottomland, they were parcels of land situated next to rivers and streams that contained robust timber tracts, fertile soil, tall grasses, and a perennial water source. Frontiersmen, at least the few who were literate, wrote of these rich bottoms, often remarking on the incredible number of species found within their borders.\nNotable bottoms existed at the juncture of the Clark’s Fork and Yellowstone, at the mouth of the Tongue River where it enters the Yellowstone, at the confluence of the Bad and Missouri rivers, and near the headwaters of the Powder River where that river enters the foothills of the Bighorn Mountains. Predictably, after European American settlers conquered the Plains, they erected their ranches, farms, and then towns atop the bottoms. Today, Billings and Laurel, Montana, stand solidly atop the Clark’s Fork Bottom, Miles City occupies the Tongue River Bottom, Fort Pierre, South Dakota, dominates the Bad River Bottom, and the aptly named Buffalo, Wyoming, sits astride the former Powder River Headwaters Bottom. With the former bottoms, or oases, now firmly buried beneath concrete, asphalt, bricks, and mortar, the bison herds of some future Buffalo Commons will not have the necessary oases to survive and thrive in the harsher, drier Great Plains environment. Without those former oases, there can be no Buffalo Commons. And as climate change and water depletion intensify, Plains residents will increasingly congregate in the area’s urban hubs, such as at Billings/Laurel, Montana, Miles City, Montana, Buffalo, Wyoming, and Pierre/Fort Pierre, South Dakota, leaving no room forBuffalo Commons bison herds to find a home on the range. Instead of a Buffalo Commons emerging across the western Plains, the regionis going to become what American explorer Stephen Long perceivedit to be back in 1820—“The Great Desert.”', '“The Incredible Shrinking Bison”  discussed how the increase of CO2 emissions has contributed to the rise of the Great Plains’ average temperature, and how the resulting warming trend has affected the bison. The bison, on the other hand, as a key species to the survival of the plains, can also have an indirect, mitigating effect on the CO2 levels in the atmosphere.\nThe predominate conversation around atmospheric CO2 has centered on the elimination of CO2 production. There is, however, another conversation underway—one involving the removal of CO2 from the atmosphere. The process of capturing and storing atmospheric carbon dioxide is known as carbon sequestration. It is one method of reducing the amount of carbon dioxide in the atmosphere, and consists of two types: geologic and biologic. Geologic carbon sequestration involves the storing of carbon dioxide in underground geologic formations. The CO2 is usually pressurized until it becomes a liquid, and then it is injected into porous rock formations in geologic basins .\nBiologic carbon sequestration refers to storage of atmospheric carbon in vegetation, soils, woody products, and aquatic environments. For example, by encouraging the growth of plants—particularly larger plants like trees—advocates of biologic sequestration hope to remove CO2 from the atmosphere. Within biologic carbon sequestration there are several means by which CO2 is removed from the atmosphere. These include peatland, wetland, forestry, agriculture, carbon farming, deep soil, and ocean-related. But of all the terrestrial (as opposed to aquatic) methods, the forests receive the lion’s share of the world’s attention. Forgotten are the grasslands which also harbor much of the wetlands. For North America the grasslands of the Great Plains and prairies, which occupy approximately one-third of the continent, are critical to carbon sequestration. And key to the grasslands’ vitality is the North American Bison .\nGrasslands quickly process carbon from the atmosphere and store this carbon in the root structures, which extend 8 to 15 feet into the ground, which can store 22.5 million tons of carbon. These roots can hold the carbon for decades, and process 1.7 million tons of carbon per acre to the soil annually. This storage accumulates over time and moves carbon from the atmosphere to the ground continuously creating massive carbon deposits over the course of centuries. Prairies have the ability to store as much carbon below the ground as forests can store above the ground. When carbon is stored below ground it remains locked there and unable to enter the atmosphere. Compared to forests, grasslands are more reliable. In times of drought and forest fires, the carbon stored in the wood and leaves returns to the atmosphere. During a grass fire, however, carbon is not released since it is stored in the roots underground .\nThough the Great Plains and prairies occupy a vast swath of the North American continent, this does not translate into a great CO2 scrubber. The conversion of this ecosystem into cropland has significantly reduced the ability of this region to sequester carbon . Compared to native or natural vegetation, cropland soils are depleted in soil organic carbon (SOC). When soil is converted from its native state the SOC content in the soil is reduced by approximately 30 to 40% . Further, the crops replacing the native grasses are annuals with comparatively shallow root structures which are less effective in storing carbon and holding soil. With less carbon stored and moved to soil, and increased possibility of soil loss, the effectiveness of the plains and prairies in atmospheric CO2 removal is significantly decreased.\nShort of returning the croplands back to the natural state of the region, there are agricultural methods aimed at sequestering atmospheric carbon into the soil and in crop roots, wood and leaves. These methods are collectively referred to as carbon farming. Besides removing CO2 from the atmosphere, increasing the soil’s carbon content—whether by reverting to the natural condition, or by carbon farming—aids plant growth, increases soil organic matter which improves agricultural yield, improves soil water retention capacity and reduces fertilizer use which is a source of the greenhouse gas nitrous oxide (N2O) .\nCarbon farming or recovering the native perennials, however, is not the complete answer. The ecosystems of the plains and prairies were dependent on the large herds of bison moving over the grasslands. The grazing, trampling and recovery patterns associated with the bison were key in building soil, maintaining biological diversity and deepening plant roots, which are crucial elements in permanent carbon sequestration . The bison not only provided nutrients for plant life, but tilled the soil with their hooves, working up and trampling dung into the soil, enabling plant-life to take hold, flourish and consequently become a significant carbon sink.\nThough sequestration, as used here, is a technical term, the concept is quite familiar. When we hear the word “sequester,” the common association is with juries as in jury trials. When a jury is sequestered, it is removed and kept apart from contact with the public. The purpose is to ensure undue influence on, or tampering with, the deliberations of the jury, and ensure a just verdict. As the jurors file out of the courtroom at the end of the defense’s and prosecution’s final presentations, if the judge has ordered they be sequestered, we see the tangible form of a removal to protect the integrity of the trial by jury justice system considered critical to our legal well-being. The notion of using an act of removal in the protection of our well-being is only part of the meaning of sequestration. What is being removed and where it is being kept are equally important. Originally, “to sequester” meant “…to put in the hands of a trustee…” . In regard to carbon sequestration the trustee is the earth itself, or more specifically, in the context of the bison and the grasslands of North America, it is the Great Plains ecosystem. When we think of ecosystems, we tend to think of the land, the flora and the fauna. Often missing in our consideration is the air above. The bison—a keystone species in regard to the flora and fauna and the land—is a crucial element of the trustee, instrumental in the process of CO2 removal and the mitigation of the warming trend plaguing the Great Plains.\n Schuette, Keith. “The Incredible Shrinking Bison.” November 17, 2020.Bison Witness. Bisonwitness.com\n “What is Carbon Sequestration?” USGS.gov. What is carbon sequestration? (usgs.gov). Retrieved 4/24/21\n Schuette. “Dung Cake and Feces Pie: Yum!” April 26, 2019. Bison Witness. Bisonwitness.com\n Davidson, William, “The Great Plains: America’s Carbon Vault” (2016). Op-Eds from ENSC230 Energy and the Environment: Economics and Policies. 73. https://digitalcommons.unl.edu/ageconugensc/73\n Lavelle, Jocelyn. Soil carbon sequestration to combat climate change—a real solution or just hype? – Sustainability (colostate.edu). Colorado State University—School of Environmental Sustainability. Retrieved 4/30/21.\n 42% of the Great Plains has been converted to cropland, leaving 53% intact. The remaining 5% holds water or has been converted to human use. Understanding Grassland Loss in the Northern Great Plains. 2018. World Wildlife Organization.\n Poeplau, Christopher; Don, Axel (February 1, 2015). “Carbon sequestration in agricultural soils via cultivation of cover crops – A meta-analysis”. Agriculture, Ecosystems & Environment. 200 (Supplement C): 33–41. doi:10.1016/j.agee.2014.10.024.\n “Carbon Farming | Carbon Cycle Institute”. http://www.carboncycle.org. Also, “Carbon Farming: Hope for a Hot Planet – Modern Farmer”. Modern Farmer. 2016-03-25. And Velasquez-Manoff, Moises (2018-04-18). “Can Dirt Save the Earth?. The New York Times. Retrieved 4/30/21.\n Wright, Pam. Bison: The Latest in Carbon Capture Tech.12/24/2017/by Regeneration International. Retrieved 4/30/21.\n Webster’s Unabridged Dictionary of the English Language. 2001. Random House.']	['<urn:uuid:a0556620-d865-4126-8ca8-0d813c2f5034>', '<urn:uuid:44bf5635-90c3-42a9-a623-614aec397f7b>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	24	159	2115
85	Which two UNESCO World Heritage sites in Southeast Asia were recognized at a later date: Luang Prabang in Laos or Halong Bay in Vietnam?	Halong Bay in Vietnam was recognized as a UNESCO World Heritage site before Luang Prabang. Halong Bay received its first UNESCO recognition on December 17, 1994, while Luang Prabang achieved UNESCO World Heritage Site status in 1995.	"['Located on a peninsula at the confluence of the Mekong and Nam Khan Rivers in the north of Laos, Luang Prabang is the nation\'s oldest continually occupied city. Its present population is about 20,000.\nLuang Prabang\'s recorded history begins in the fourteenth century, when it became the original capital of Lan Xang (Kingdom of a Thousand Elephants). Founded by Fa Ngum (1316-1374), Lan Xang was the first unified Lao kingdom. Fa Ngum sought to unify the diverse ethnic groups who populated the kingdom by promoting Theravada Buddhism, the predominant religion of continental Southeast Asia. In 1358, he asked the Khmer king to send a Buddhist mission consisting of elders, scholars, and artisans to the city, then known as Muang Sua. They brought with them the sacred Pra Bang image of the Buddha, which remains the city\'s palladium, as well as a symbol of the authority of the Buddhist Sangha.\nIn 1563, in response to threats from Burma, Settathirath I (r. 1548-71) moved the capital from Muang Sua to Vientiane and renamed it Luang Prabang (Royal City of the Pra Bang) in the Pra Bang\'s honor. After the transfer, Luang Prabang\'s political importance receded. In the late seventeenth century, the kingdom\'s end approached. In 1707, the northern provinces became the separate kingdoms of Luang Prabang and Vientiane. In 1713, the southern provinces became the Champassak kingdom.\nAfter Lan Xang\'s demise, Luang Prabang sought to maintain its independence in response to threats from Siam and Burma. Taking advantage of the division of the Lao kingdoms, Burma attacked Luang Prabang in 1753 and 1771. In 1778, Siam declared it a dependency. After subduing Vientiane in 1828, Siam began to exert greater influence over Luang Prabang, which ceremoniously reaffirmed its allegiance in 1836. During this period, the city also suffered from frequent disastrous fires that destroyed much of its architectural heritage.\nIn the late nineteenth century, France began to extend its imperial ambitions in Indochina to the Lao territories. Siam signed a treaty with France in 1893 in which it reluctantly renounced sovereignty over all Lao territories east of the Mekong. This treaty opened the way for France to establish a protectorate over Luang Prabang, direct colonial administration over central and southern Laos east of the Mekong, and control of the Mekong itself. In 1904 and 1907, further agreements effected the transfer of two Lao provinces west of the Mekong. France now controlled about half the territories of the former Lan Xang.\nThroughout the period of French control, the Lao territories remained France\'s least important possessions in Indochina. When France selected Vientiane as the seat of colonial administration, Vientiane usurped Luang Prabang\'s status as the Lao territories\' center of political power. The Luang Prabang king increasingly became a figurehead who executed his duties in parallel with the French administrators.\nFollowing the Geneva Conference of 1954, France withdrew from Indochina. A constitutional monarchy that restored Buddhism to the position of official state religion, which inspired a Buddhist revival, was established. In the 1950s and 1960s, a period of political instability, rightists, neutralists, and communists struggled for control of the government.\nIn the 1960s, the Vietnam War escalated, and the United States bombed eastern Laos. Luang Prabang, however, was spared from destruction. In 1975, Sisavong Vatthana (r. 1959-75), the last king, abdicated, and the Lao PDR, to be governed by the Lao People\'s Revolutionary Party, was established.\nThe Lao PDR has attempted to establish socialism ""with Lao characteristics."" The king\'s removal was accompanied by Buddhism\'s rejection as the official state religion. But in the 1990s, the state turned to Buddhism in a search for a reformulated Lao nationalism and promoted the Sangha as the national culture\'s caretakers. The nomination of Luang Prabang as a UNESCO World Heritage Site, a status it achieved in 1995, exemplified this tendency.\nLuang Prabang Architecture\nThe following text is reprinted from Tourism and Heritage Site Management in the World Heritage Town of Luang Prabang, Lao PDR (Bangkok: Office of the Regional Advisor for Culture in Asia and the Pacific, UNESCO Bangkok and School of Travel Industry Management, University of Hawa\'i, 2004), pp. 27-28.\nThe early dwellings in the Luang Prabang area, similar to vernacular houses throughout Lane Xang and neighbouring kingdoms, were built from wood and bamboo, and raised on piles. A lightweight framework of wood or bamboo was constructed, with panels of woven bamboo strips used for infill. Thatched roofing provided protection against the elements and could be easily replaced as necessary. Later, a plaster finish made from lime, straw, sand, palm sugar and boiled buffalo skin was introduced by the Tai Dum, a Tai-speaking group whose homeland is in northern Viet Nam. These construction techniques and materials are still predominant in the villages surrounding Luang Prabang and in some areas of the town itself.\nThe construction technique of vernacular dwellings is consistent throughout different levels of society. Higher status is revealed only through location, larger size and better quality of construction materials. As such, a village chief\'s house was, traditionally, stylistically identical to the house of an ordinary resident.\nNew secular building styles were introduced between 1893 and 1907 as the French gradually assumed administrative control of Laos. In constructing administrative buildings and houses the French introduced European construction techniques and materials. For instance, the restriction on the use of bricks, which were previously used only for temples, was lifted. The French, however, did not merely transplant European styles into Luang Prabang. Instead, they employed styles developed in Viet Nam and produced designs inspired by vernacular temple architecture and secular wooden structures that were better suited to the warm and humid Laotian climate.\nAs a result, a new Laotian architectural style emerged, based on indigenous domestic architecture but freely incorporating French and Vietnamese design elements along with European and Chinese technical innovations. The Laotian royalty and aristocracy, who had previously lived in wooden houses, had their new masonry residences constructed in this style. The former Royal Palace, which today houses the Luang Prabang National Museum, was built between 1904 and 1909 and serves as a fine example of the French-inspired architecture that was popular at that time.\nThe French introduced some elements of Chinese architecture and urbanism indirectly. To execute French public works, skilled Vietnamese labourers were imported. These labourers settled near the foot of the peninsula and built their own commercial quarters which were brick, Chinese-style shop houses in rows that faced directly onto the street, with living accommodations on the upper floors. All of these architectural styles can still be seen today in Luang Prabang.', 'Situated in the north-east region of Viet Nam, Halong Bay is a part of Bac Bo Gulf and comprises the sea area of Halong City, Cam Pha Town and a part of Van Don island district, Quang Ninh Province. It borders Cat Ba Island to the south-west, the mainland to the west with a 120km-long coastline.\nHalong Bay covers a total area of 1,553km², including 1,969 islands of various sizes, 989 of which have been given names. There are two kinds, limestone and schist, which are concentrated in two main zones: the south-east (belonging to Bai Tu Long Bay) and the south-west (belonging to Halong Bay). The average geological age of the islands is between 250 and 280 million years old.\nHalong Bay has been called by the great national poet Nguyen Trai: “a marvel of the earth erected towards the high skies”. While exploring the bay, tourists will feel lost in a legendary world of stone islands which shapes change depending on the angle and the light. There are many names given to islands according to their shapes and forms such as Hon Dau Nguoi (Human Head Islet), Hon Rong (Dragon Islet), Hon Canh Buom (Sail Islet), Hon Trong Mai (Cock and Hen Islet)… But the beauty of Halong Bay does not consist only in the forms of its mountains, islands and the colour of its waters, but also in its infinitely rich system of grottoes and caves such as: Thien Cung (Heavenly Palace Grotto), Dau Go (Driftwood Grotto), Sung Sot (Surprise Grotto), Tam Cung (Three Palace Grotto), Trinh Nu (Virgin Grotto)…. Each is a grandiose and refined natural architectural creation.\nThe most remarkable geological events of Halong Bay’s history in the last 1,000 years include the advance of the sea, the raising of the bay area and the strong erosion that has formed coral and pure blue and heavily-salted water. This process of erosion by sea water has deeply engraved the stone, contributing to its fantastic beauty. Present-day Halong Bay is the result of this long process of geological evolution that has been influenced by so many factors. It is because of all these factors that the tourists now visiting Halong Bay are not only treated to one of the true wonders of the world, but also to a precious geological museum that has been naturally preserved in the open air for the last 300 million years.\nValue of biological diversity\nResults of scientific research show that Halong Bay features ecosystems of a tropical ocean region such as ecosystem of coral reefs with 232 species of coral distributed mainly in the areas of Cong Do and Bo Hung. It is also home to 81 species of gastropoda, 130 species of bivalvia, 55 species of polycheta and 57 species of crab. The ecosystem of salt water-flooded forests chiefly concentrated in the zones of Tuan Chau, Cua Luc and Ba Che has the most diversified collection of species of salt water-flooded plant in North Vietnam. Also living in this ecosystem are a great many species of animals: migrating birds (200 species), polycheta (169 species), seaweed (91 species), reptile (10 species). Halong Bay also has ecosystem of tropical rain forests with various rare and precious creatures: deer, weasels, squirrels and in particular, white-tabby and red-haired monkeys. In addition, there is a system of small caves along the sea, which are the living and development places for many animals and plants: seaweed, water plant, algae, fish and shrimp. Deeper into the water, there are also many species of shrimp, fish, abalone and other sea-specialities.\nHistorical and cultural value\nHalong is a place closely linked to Vietnam’s history with such famous geographical names as: Van Don (site of an ancient commercial port); Poem Mountain (with engravings of many poems by emperors and other famous people of the past); and Bach Dang River (the location of two fierce naval battles fought against foreign aggressors). This is not all, Halong has been proven by scientists to be one of the first cradles of human existence in the area, with such archaeological sites as Dong Mang, Xich Tho, Soi Nhu and Thoi Gieng…\nOn December 17, 1994, Halong Bay was recognised as world natural heritage for its natural beauty at the 18th meeting of the World Heritage Committee of UNESCO in Thailand. On December 12, 2000, Halong Bay was recognised as world natural heritage for the second time based on its geological value at the 24th meeting of the World Heritage Committee of UNESCO in Cairns, Australia. The recognised site covers an area of 434km², comprises 775 islands and forms a triangle: with Dau Go Island (Driftwood Grotto) to the west; Ba Ham Lake (Three Shelter Lake) to the south and Cong Tay Island to the east.']"	['<urn:uuid:92a6ad4b-ec71-4b35-8fb2-01a4d1cb769d>', '<urn:uuid:6065bc55-550c-4c7e-9fd3-53758e008b16>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	24	37	1879
86	when did greek heroic age end	The Greek Heroic Age, a time when men were bigger and stronger and used bronze weapons, came to an end with two major wars - the Theban and the Trojan wars. This period represents how the Mycenaean Greek civilization of the second millennium BC was remembered in historic Greece.	['Like the traditional poetry of other peoples, the traditional poetry of the Greeks celebrated the Heroic Age. This was the time when men were bigger and stronger, and they performed marvelous feats of prowess. Their weapons were made of bronze and not of iron, and they were ruled by kings. … The Heroic Age came to an end in two great wars – the Theban and the Trojan. … This was how the Mycenaean Greek civilization of the second millennium BC was remembered in historic Greece.\nMargalit Finkelberg. Greeks and Pre-Greeks\nClassical Greek poetry concerned with the Heroic Age includes a lot of genealogy, with an emphasis on descent in the male line, much like the begats in the Bible. Modern readers familiar with the Iliad and Odyssey find this stuff pretty boring, but it mattered a lot to the Greeks, who would try to link their existing patrilineal clans to the legendary family lines of the Heroic Age.\nAn emphasis on patrilineal descent is a general feature of early Indo-European society and its later offshoots, including the Greeks; the Indo-European expansion is one phase of the Patriarchal Age, leaving its imprint particularly on the distribution of Y chromosome variants. But given this patrilineal focus, there is something odd about the legends of the Heroic Age. In virtually none of the surviving legends do we find kingship passing from father to son, even when there is a son around. Instead, the normal pattern is that the king’s successor is the guy who marries his daughter – in other words his son-in-law, not his son. Meanwhile, the king’s son has to marry elsewhere. (Although the legends seem to present some cases of rotating succession, where multiple patrilineages took turns marrying into a matrilineage. In these cases, a king’s grandson might marry back into the kingdom, marrying his father’s sister’s daughter.) The implication is that the line of succession to the throne ran from mother to daughter, although it was the husbands of these women who actually exercised power: kingship by marriage. The most notable case of a son succeeding to his father’s throne is the exception that proves the rule: Oedipus got to be king of Thebes because he married Queen Jocasta, not because he was King Laius’ son. (Spoiler alert: see below*)\nIn Greeks and Pre-Greeks: Aegean Prehistory and Greek Heroic Tradition, Margalit Finkelberg argues that legends of the Heroic Age are memories of a time when the patrilineal traditions of the Greeks coexisted with earlier matrilineal traditions. More specifically, she argues that matrilineal Pre-Greek cultures were associated with the Anatolian language family, the first branch off the Indo-European tree, which also includes Hittite. On her account, Greece looks like ancestral Polynesia, a society flipped from matrilineal to patrilineal by invaders.\nFinkelberg is not the first person to notice possible survivals of matrilineal descent from before the coming of the Indo-Europeans and other folk. Such survivals led some nineteenth century scholars to theorize that matrilineality – tracing descent and succession through the female line – was a stage of social evolution that all societies passed through. Some scholars also believed that matrilineal societies were matriarchal – ruled by women. Neither of these theories has held up very well. And yet …\n… based on reconstructions of cultural phylogeny and/or ancestral vocabulary a number of the great demic expansions that covered the world seem to have started out matrilineal and/or matrilocal. The list (labelled by associated language families) includes:\nSo although matrilineal/matrilocal organization is not a stage that every society passes through, it is seems to be a phase in many demic expansions. This is actually not too surprising. One solid finding in the anthropology of kinship is that matrilocal societies, in which a man goes to live with his wife’s kin when he marries, tend to be internally peaceful, without a lot of feuding between neighboring villages in the same tribe. This makes sense, since the men are no more related to the men in their own village than they are to men in neighboring villages. At the same time, matrilocal societies are often quite war-like with respect folks outside the larger tribe (just ask the neighbors of the Iroquois or the Navajo). Since matrilocality and matrilineality (which tend to go together) are associated with internal peace and external aggression, this social organization is well-suited to life along an ethnic frontier. Matrilocality (which is strongly associated with matrilineality) is one way tribal societies generate the social solidarity that enables demic expansion.\nBut there are several limits to matrilocal solidarity. First, the introduction of stock herding tends to undermine matrilocality and matrilineality. (My late colleague Henry Harpending worked with a group, the Herero in southern Africa, who had taken up cattle herding, and were probably in the early stages of transition from matri- to patrilineal.) Also matrilocal/matrilineal societies rarely exceed a few tens of thousands of people. Beyond that size their internal unity tends to break down, and parents start insisting that married sons stick around to defend the homestead. So a lot of the later, better known population expansions, including Indo-European, Semitic, Turkic, and Han Chinese are heavily patrilineal. But even today, traces of earlier matrilineal social organization still survive in some places – in the matrilineal belt of Central Africa, and in some of South East Asia, where patrilineality, and mate guarding to secure the male line, mostly don’t reach the same intensity as in much of Asia.\n* Oedipus didn’t know it, but Jocasta was his mom.']	['<urn:uuid:a41914f7-f9df-4b3f-b232-e88ec6a29ab0>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:10:11.825225	6	49	919
87	How did Marriner's musical career begin?	Sir Neville Marriner began his career as a violinist, serving as principal violin in the London Symphony Orchestra. In 1958, he founded the Academy of St Martin in the Fields, which became one of the world's most highly-regarded chamber orchestras and one of the most prolific orchestras in recordings.	"[""Courtney Lewis conducts music by Mendelssohn, Beethoven and Dvořák in concerts that honor Sir Neville Marriner, the Orchestra’s former music director who passed away in October 2016\nThe Minnesota Orchestra, deeply saddened by the recent passing of former Music Director Sir Neville Marriner, will present tribute concerts in his memory on January 26 and 27—the dates Marriner had been scheduled to guest conduct the Orchestra. Former associate conductor Courtney Lewis leads the Orchestra in a program that will remain unchanged to honor Marriner’s personal selection of repertoire. The concerts open with Mendelssohn’s mysterious The Hebrides Overture and continue with Beethoven’s First Symphony. Then—echoing one of Marriner’s major accomplishments with the Orchestra, an acclaimed recording of the final three Dvořák symphonies—the ensemble performs Dvořák’s Eighth Symphony.\nThe concerts are performed at the Orchestra’s home venue in downtown Minneapolis, Orchestra Hall, on Thursday, January 26, at 11 a.m.; and Friday, January 27, at 8 p.m., with ticket prices ranging from $25 to $96. Tickets are available online at minnesotaorchestra.org and by phone at 612-371-5656. For further purchasing details, refer to the section at the conclusion of this press release.\nSir Neville Marriner and the Minnesota Orchestra\nNeville Marriner served as the Minnesota Orchestra’s music director from 1979 to 1986. During his tenure, he presided over a number of firsts, including the Orchestra's first composer-in-residence program and two international trips, to Australia in 1985 and Hong Kong in 1986, which helped set the stage for the Orchestra’s first European tours in the late 1990s. And in 1985 he achieved a distinction unique in the Minnesota Orchestra's annals by becoming the first Orchestra music director to be knighted. In 1982 he led a star-studded, televised “Tonight Scandinavia” concert attended by members of several Scandinavian royal families. In his final performance with the Orchestra in 2008, he conducted Elgar’s Violin Concerto with former Concertmaster Jorja Fleezanis as soloist.\nA native of England, Marriner began his career as a violinist, serving as principal violin in the London Symphony Orchestra. In 1958 he established the Academy of St Martin in the Fields, which has become one of the world’s most highly-regarded chamber orchestras, and one of the most prolific orchestras in recordings.\nCourtney Lewis, conductor\nCourtney Lewis, who in 2014 completed his four-year tenure as the Minnesota Orchestra’s associate conductor, has established himself as one of his generation’s most talented conductors. He is currently in his second season as music director of the Jacksonville Symphony. His additional past appointments have included assistant conductor of the New York Philharmonic, which he will return to in April for a week of subscription concerts. He has also been a Dudamel Fellow with the Los Angeles Philharmonic. Highlights of his 2015-16 season included debuts with the Hong Kong Philharmonic, Milwaukee Symphony, Royal Flemish Philharmonic and Colorado Symphony, as well as assisting Thomas Adès at the Salzburg Festival for the world premiere of Adès’ opera The Exterminating Angel. More: opus3artists.com, courtneylewis.com.\nSir Neville Marriner’s Program Selections\nImpressions of swirling waters and crashing waves unfold in The Hebrides Overture as Mendelssohn paints pictures of the dramatic sea and the haunting scenery on the islands off the Western coast of Scotland.\nBeethoven’s First Symphony is full of crisp melodies and quick turns of phrase, as well as musical “inside jokes.” Later in life Beethoven would revolutionize the symphonic form, but in this early work he is content to compose music of clean lines and undeniable beauty.\nDvořák’s Eighth Symphony is full of luminous melodies and unexpected harmonic shifts. The second movement alludes to the funeral march of Beethoven’s Eroica, but lighter elements prevail in a whirlwind finale that is delightfully Czech.\nMinnesota Orchestra Classical Concerts\nA TRIBUTE CONCERT TO SIR NEVILLE MARRINER\nCourtney Lewis, conductor\nMENDELSSOHN The Hebrides Overture (Fingal’s Cave)\nBEETHOVEN Symphony No. 1\nDVOŘÁK Symphony No. 8\nFree Concert Preview: Remembering Sir Neville Marriner, hosted by Phillip Gainsley. Guests include former Orchestra President/CEO Richard Cisek, Life Board Member Luella Goldberg and music critic Michael Anthony, in the Auditorium on Thursday, January 26, at 10:15 a.m., and in the Target Atrium on Friday,\nFor more information on this and other OH+ Events: minnesotaorchestra.org/ohplus.\nTICKET PURCHASING INFORMATION\nSubscription packages and individual tickets can be purchased online at minnesotaorchestra.org, or by calling 612-371-5656 (612-371-5642 for subscriptions) or 800-292-4141. Tickets can be purchased in person at the Orchestra Hall Box Office, 1111 Nicollet Mall, Minneapolis (open Monday to Friday, 10 a.m. to 2 p.m., and beginning two hours before all ticketed performances); and at the Minnesota Orchestra Administrative Office, International Centre, 5th floor, 920 Second Avenue South, Minneapolis (open Monday to Friday, 9 a.m. to 5 p.m.). For more information, call 612-371-5656, or visit minnesotaorchestra.org. For subscriptions, call 612-371-5642 or visit minnesotaorchestra.org/subscribe. For groups of 10 or more, call 612-371-5662.\nAll programs, artists, dates, times and prices subject to change.\nThe Star Tribune is the Minnesota Orchestra’s media partner for the 2016-17 season.\nThis activity is made possible by the voters of Minnesota through a Minnesota State Arts Board Operating Support grant, thanks to a legislative appropriation from the arts and cultural heritage fund.\n# # #\nGwen Pappas, Director of Public Relations\nEmma Plehal, Public Relations Writer and Coordinator""]"	['<urn:uuid:83d776cf-6cec-42ec-8e36-00fc59cf30ca>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:10:11.825225	6	49	864
88	I've been researching oral and gut microbiomes, and I'm wondering about the relationship between bacterial balance in the mouth versus the intestines. What are the key similarities and differences in how we maintain healthy bacterial communities in these two areas?	In both the mouth and intestines, the focus is on maintaining a balance of beneficial and harmful bacteria rather than eliminating all microbes. In the mouth, there are billions of microbes representing several hundred species, and the goal is to have good bacteria outweigh harmful ones like P. gingivalis and S. mutans. This balance can be supported through probiotics and prebiotics. Similarly, in the intestines, there are trillions of microorganisms, with about 40 trillion bacterial cells compared to 30 trillion human cells. The intestinal balance is maintained through fiber consumption and avoiding things that harm beneficial bacteria, such as unnecessary antibiotics. Both areas benefit from probiotics, but their specific mechanisms differ - oral probiotics help prevent issues like tooth decay and gum disease, while intestinal probiotics support immune function, heart health, and even mental health through the gut-brain axis.	"['Most all of us grow up being taught that “germs” cause disease and that the best defense is to kill them. But science has shown that this is an oversimplification.\nWe know that the environment in which pathogens exist makes a big difference in whether they thrive or not – just as soil quality and other environmental factors determine whether a plant thrives or not.\nWe also know that our bodies contain more bacteria than human cells. We’re beginning to understand how the makeup of our microbiome can affect our health for better or for worse. As microbiologist John G. Thomas has put it,\nThe accepted concept today is that there are multiple organisms with the ability to interact in multiple ways. The means of bringing these biofilm communities back into balance is best achieved not through use of antimicrobials, but by reestablishing a normal flora, aided by probiotic agents.\nYou already probably know a bit about probiotics – bacteria that support good health. You can get them naturally through fermented foods, yogurt, and some cheeses. You can also get them through supplements or foods fortified with them. So far, the research on their dental benefits in particular has been quite promising, showing how probiotics may stave off caries (tooth decay), periodontal (gum) disease, bad breath, and more.\nMeanwhile, the focus has shifted away from “killing germs” to supporting the balance of helpful and harmful bacteria in the mouth. Indeed, it would be impossible – let alone desirable – to remove all microbes from the mouth, or even just the bad ones. There are billions of them in even the cleanest mouth, representing several hundred different species.\nWhat we want is for the good to outweigh microbes like P. gingivalis and S. mutans that generate oral disease. Probiotics may help, and so might prebiotics.\nWhere probiotics are the actual healthy bacteria, prebiotics are non-digestible carbohydrates that help probiotics work their magic. Again, Dr. Thomas:\nPrebiotics are food ingredients that stimulate the growth and/or activity of bacteria in the digestive system, in ways claimed to be beneficial to health. Marcel Roberfroid offered a refined definition in the Journal of Nutrition stating, “A prebiotic is a selectively fermented ingredient that allows specific changes, both in the composition and/or activity in the gastrointestinal microflora that confers benefits upon host well-being and health.” Prebiotics effectively stimulate the colonization of the probiotic microorganisms, providing an initial advantage to their adherence.\nEarlier this year, scientists writing in the Journal of Clinical Periodontology identified two compounds that could be effective as oral prebiotics specifically.\nTwo compounds, beta-methyl-d-galactoside and N-acetyl-d-mannosamine, could be identified as potential oral prebiotic compounds, triggering selectively beneficial oral bacteria throughout the experiments and shifting dual species biofilm communities towards a beneficial dominating composition at in vitro level.\nOur observations support the hypothesis that nutritional stimulation of beneficial bacteria by prebiotics could be used to restore the microbial balance in the oral cavity and by this promote oral health.\nEven though much research remains to be done on prebiotics for oral health, some hygiene products have begun to emerge. It’s a bit too early to gauge how helpful they may be.\nStay tuned for further developments…', ""What Are Postbiotics? And How Do They Relate To Pre- and Probiotics?\nIn this article:\n- What Are Postbiotics?\n- Fiber Deficiency And Postbiotics\n- Healthy Gut Bacteria, The Foundation Of Good Health\n- 5 Key Health Benefits Of A Healthy Microbiome\n- How to Improve Your Postbiotic Microbiome\n- The Bottom Line\nYou have likely never heard the term, but postbiotics represent all the rage in gut health and microbiome science. Prebiotics and probiotics may be more familiar, but all three relate in an interdependent relationship critical to not only our digestive health but also our mental health — thanks to the gut-brain axis. In addition to your mental health, your gut microbiome greatly affects your immune, digestive, metabolic, and heart health.\nLet us examine postbiotics and their health benefits.\nPostbiotics represent the byproducts of probiotics eating prebiotics. That’s right! When you eat things such as grains or fresh fruit, the fiber in these foods is considered prebiotic. Probiotics then go and break down the fiber, converting them to metabolites we call postbiotics.\nProbiotics create various compounds from the fermentation of prebiotics considered postbiotics. Short-chain fatty acids (SCFAs), functional proteins, and extracellular polysaccharides (EPS) include only three examples of what can be described as postbiotics.\nWith functional bioactive compounds, research has shown postbiotics to have direct beneficial effects on your immune system. Studies also show postbiotics can be used in healthy individuals to benefit overall wellness. Conditions such as atopic dermatitis, diarrhea, and infant colic display relief with postbiotics as well.\nA healthy postbiotic microbiota starts with fiber. Not only is the amount of fiber eaten important for a healthy gut microbiome, but also a variety of fiber is needed to create postbiotic metabolites needed for good health.\nEating a variety of plant-based foods will increase and diversify your fiber consumption, and therefore strengthen your prebiotic health — directly affecting your postbiotic status. Fruits, vegetables, grains, and legumes offer good sources of fiber.\nYou should eat around twenty-five grams of fiber per day, but not more than fifty grams. Too much fiber can bring about symptoms such as bloating, decreased appetite, cramping, and constipation, and may impair absorption of phosphorus and calcium.\nA healthy gut microbiome has been linked to everything from a strong immune system to good mental health. Microbiome refers to the microorganisms that live in a particular environment. While trillions of microorganisms (microbes) live in and on your body, including fungi, bacteria, and viruses — trillions more live only inside your intestines, about 100 trillion.\nThe majority of these microbes live in one area of the large intestine called the cecum. Bacteria alone make up about forty trillion cells in your body, quite fascinating when you realize you only have thirty trillion human cells. This fact alone shows the importance of your microbiota.\nSome of these bacteria prove beneficial, which we call good or friendly bacteria; some remain harmful and may cause illness.\nWe first encounter microbes as we travel down our mother’s birth canal. As you grow older, the more plentiful and diverse your microbiome becomes.\nBifidobacteria, a friendly microbe, begins to grow early in the newborn’s intestines to help digest the sugars in breast milk. This bacterium remains important throughout life, as it creates SCFAs, an essential postbiotic needed for good health.\nAs you age, more bacteria take residence in your gut bringing with them digestive, immune, heart, metabolic, and mental health benefits.\nA strong intestinal microbiota proves essential for good overall health.\nGood bacteria, or probiotics, through digesting fiber signifies the cornerstone of good postbiotic health. The SCFAs produced by the fiber digesting bacteria help to metabolize fat and carbohydrates. They represent the primary source of energy for your cells lining the colon.\nWeight gain can be caused by dysbiosis, an imbalance of good and bad bacteria in the gut. Dysbiosis may also contribute to conditions such as inflammatory bowel disease (IBD) and irritable bowel syndrome (IBS). Symptoms such as discomfort, bloating, and cramps relate to dysbiosis.\nTaking probiotics with both Bifidobacteria and Lactobacilli can help individuals avoid discomfort caused by IBS and IBD.\nYour gut microbiota remains essential to your immune health. It regulates immune homeostasis, or balance, in the body. Changes in the intestinal microbial communities can lead to dysregulation of the immune system, contributing to an autoimmune disease of not only the intestines but also systemic autoimmune disease.\nBecause of this significant relationship between the gut microbiome and the immune system, researchers currently study new microbial therapies as potential treatments for autoimmune disease and other illnesses.\nA healthy gut can help contribute to a healthy heart. One study found the gut microbiota promotes good cholesterol, high-density lipoproteins (HDL), and triglycerides. When taken as a probiotic, Lactobacillus may also help to reduce cholesterol levels. Lower overall cholesterol and higher good cholesterol levels remain important for heart and blood vessel health.\nHigh cholesterol and low HDL levels contribute to plaque formation on arterial walls, which can lead to heart attack and stroke. Red yeast rice offers a natural supplement that can help to lower cholesterol levels.\nTrimethylamine N-oxide (TMAO) gets produced by unfriendly bacteria in the gut when they metabolize choline and L-carnitine. TMAO represents a compound that contributes to blocked arteries. Both choline and L-carnitine comprise foods made from animals, especially red meat.\nDecreasing your consumption of animal products and maintaining a healthy microbiome can help lower the chances of your gut bacteria creating TMAO.\nDiabetes and blood sugar levels can also be affected by the gut microbiota. A study found that even though the participants ate the same meals, their blood sugar levels following the meals varied greatly. The researchers noted the difference in intestinal microbes could be the reason for this variation.\nYet, another study found gut microbiome diversity significantly declined before the onset of type 1 diabetes. They also discovered unhealthy bacteria levels of various types increased before type 1 diabetes onset.\nRecently, the gut-brain axis has been the topic of many research studies. A hot topic of discussion has been the discovery that gut bacteria play a key role in brain neurotransmitter production.\nNeurotransmitters signify chemicals in the brain that inhibit or promote various physiological actions in the body. The neurotransmitter serotonin gets primarily synthesized in the gut. Serotonin has many functions in the body, including mood regulation and promoting feelings of well-being and happiness. It also aids in sleeping and digestive functions.\nTo have a healthy microbiome, you must first have good prebiotics.\nPrebiotics represent foods high in fibers like inulin and other compounds such as fructooligosaccharides (FOS). FOS not only supports healthy gut flora but also helps lower cholesterol and supports a healthy immune system.\nFOS and inulin have been shown to stimulate the growth of bifidobacteria in the gut. Bifidobacteria promotes inhibitory effects in the gut, helping to resist acute infections.\nAnother powerful prebiotic that promotes the growth of bifidobacterial involves wheat bran, the outer layer of whole wheat grain. Wheat bran offers a high amount of arabinoxylan oligosaccharides (AXOS). In addition to supporting the growth of friendly bacteria, AXOS also has antioxidant benefits.\nInulin represents a type of fiber that can naturally be found in onions, garlic, Jerusalem artichokes, dandelion greens, asparagus, and chicory root. You can also supplement inulin in case you do not consume enough inulin-rich foods in your diet.\nPectin and Beta-Glucan\nPectin and beta-glucan can also be supplemented for prebiotic support.\nIngesting more elephant yam for its high glucomannan fiber content will also support healthy and diverse postbiotics. Glucomannan supports the growth of good bacteria in the gut while also lowering cholesterol, supporting weight loss, improving immune function, and decreasing constipation.\nYou can take glucomannan supplements for increased support.\nEating fermented foods like kefir, yogurt, kombucha, and sauerkraut can increase your probiotic levels, improving your postbiotic status. Fermented foods primarily boost levels of Lactobacilli. They may also decrease levels of bad bacteria in the intestines.\nAvoiding artificial sweeteners like aspartame can also support your postbiotic health. They stimulate the growth of unfriendly bacteria including Enterobacteriaceae in the gut microbiota.\nLast by not least, avoid taking antibiotics if you can. Antibiotics destroy both good and bad bacteria in the gut. Only take them when it is medically necessary.\nPostbiotics form the foundation for good overall health. The brain, heart, immune, and gut cells depend on postbiotics to function at optimal levels.\nThe best way to have a good postbiotic status involves eating more prebiotics and increasing your probiotic flora. Only with good prebiotic and probiotic status can you experience the benefits of a healthy postbiotic microbiome.\n- Wegh CAM, Geerlings SY, Knol J, Roeselers G, Belzer C. Postbiotics and Their Potential Applications in Early Life Nutrition and Beyond. Int J Mol Sci. 2019;20(19):4673. Published 2019 Sep 20. doi:10.3390/ijms20194673\n- Kumar VP, Prashanth KV, Venkatesh YP. Structural analyses and immunomodulatory properties of fructo-oligosaccharides from onion (Allium cepa). Carbohydr Polym. 2015;117:115-122. doi:10.1016/j.carbpol.2014.09.039\n- Costa GT, Abreu GC, Guimarães AB, Vasconcelos PR, Guimarães SB. Fructo-oligosaccharide effects on serum cholesterol levels. An overview. Acta Cir Bras. 2015;30(5):366-370. doi:10.1590/S0102-865020150050000009\n- Kolida S, Tuohy K, Gibson GR. Prebiotic effects of inulin and oligofructose. Br J Nutr. 2002;87 Suppl 2:S193-S197. doi:10.1079/BJNBJN/2002537\n- Chen HL, Cheng HC, Liu YJ, Liu SY, Wu WT. Konjac acts as a natural laxative by increasing stool bulk and improving colonic ecology in healthy adults. Nutrition. 2006;22(11-12):1112-1119. doi:10.1016/j.nut.2006.08.009\n- Tester RF, Al-Ghazzewi FH. Beneficial health characteristics of native and hydrolysed konjac (Amorphophallus konjac) glucomannan. J Sci Food Agric. 2016;96(10):3283-3291. doi:10.1002/jsfa.7571\n- François IE, Lescroart O, Veraverbeke WS, et al. Effects of wheat bran extract containing arabinoxylan oligosaccharides on gastrointestinal parameters in healthy preadolescent children. J Pediatr Gastroenterol Nutr. 2014;58(5):647-653. doi:10.1097/MPG.0000000000000285\n- Clemens R. et al. Filling America’s Fiber Intake Gap: Summary of a Roundtable to Probe Realistic Solutions with a Focus on Grain-Based Foods. J Nutr. 2012 July; 142(7): 1390S-1401S.\n- Berdy J. Bioactive Microbial Metabolites. J. Antibiot. 2005;58(1):1.26.\n- Shah M, Chandalia M, Adams-Huet B, et al. Effect of a high-fiber diet compared with a moderate-fiber diet on calcium and other mineral balances in subjects with type 2 diabetes. Diabetes Care. 2009;32(6):990-995. doi:10.2337/dc09-0126\n- Sender R, Fuchs S, Milo R. Revised Estimates for the Number of Human and Bacteria Cells in the Body. PLoS Biol. 2016;14(8):e1002533. Published 2016 Aug 19. doi:10.1371/journal.pbio.1002533\n- Integrative HMP (iHMP) Research Network Consortium. The Integrative Human Microbiome Project: dynamic analysis of microbiome-host omics profiles during periods of human health and disease. Cell Host Microbe. 2014;16(3):276-289. doi:10.1016/j.chom.2014.08.014\n- Arboleya S, Watkins C, Stanton C, Ross RP. Gut Bifidobacteria Populations in Human Health and Aging. Front Microbiol. 2016;7:1204. Published 2016 Aug 19. doi:10.3389/fmicb.2016.01204\n- Ríos-Covián D, Ruas-Madiedo P, Margolles A, Gueimonde M, de Los Reyes-Gavilán CG, Salazar N. Intestinal Short Chain Fatty Acids and their Link with Diet and Human Health. Front Microbiol. 2016;7:185. Published 2016 Feb 17. doi:10.3389/fmicb.2016.00185\n- Ridaura VK, Faith JJ, Rey FE, et al. Gut microbiota from twins discordant for obesity modulate metabolism in mice. Science. 2013;341(6150):1241214. doi:10.1126/science.1241214\n- Wu HJ, Wu E. The role of gut microbiota in immune homeostasis and autoimmunity. Gut Microbes. 2012;3(1):4-14. doi:10.4161/gmic.19320\n- Fu J, Bonder MJ, Cenit MC, et al. The Gut Microbiome Contributes to a Substantial Proportion of the Variation in Blood Lipids. Circ Res. 2015;117(9):817-824. doi:10.1161/CIRCRESAHA.115.306807\n- Shimizu M, Hashiguchi M, Shiga T, Tamura HO, Mochizuki M. Meta-Analysis: Effects of Probiotic Supplementation on Lipid Profiles in Normal to Mildly Hypercholesterolemic Individuals. PLoS One. 2015;10(10):e0139795. Published 2015 Oct 16. doi:10.1371/journal.pone.0139795\n- Wang Z, Klipfell E, Bennett BJ, et al. Gut flora metabolism of phosphatidylcholine promotes cardiovascular disease. Nature. 2011;472(7341):57-63. doi:10.1038/nature09922\n- Zhu W, Wang Z, Tang WHW, Hazen SL. Gut Microbe-Generated Trimethylamine N-Oxide From Dietary Choline Is Prothrombotic in Subjects. Circulation. 2017;135(17):1671-1673. doi:10.1161/CIRCULATIONAHA.116.025338\n- Koeth RA, Wang Z, Levison BS, et al. Intestinal microbiota metabolism of L-carnitine, a nutrient in red meat, promotes atherosclerosis. Nat Med. 2013;19(5):576-585. doi:10.1038/nm.3145\n- Zeevi D, Korem T, Zmora N, et al. Personalized Nutrition by Prediction of Glycemic Responses. Cell. 2015;163(5):1079-1094. doi:10.1016/j.cell.2015.11.001\n- Kostic AD, Gevers D, Siljander H, et al. The dynamics of the human infant gut microbiome in development and in progression toward type 1 diabetes. Cell Host Microbe. 2015;17(2):260-273. doi:10.1016/j.chom.2015.01.001\n- O'Mahony SM, Clarke G, Borre YE, Dinan TG, Cryan JF. Serotonin, tryptophan metabolism and the brain-gut-microbiome axis. Behav Brain Res. 2015;277:32-48. doi:10.1016/j.bbr.2014.07.027\n- Yano JM, Yu K, Donaldson GP, et al. Indigenous bacteria from the gut microbiota regulate host serotonin biosynthesis [published correction appears in Cell. 2015 Sep 24;163:258]. Cell. 2015;161(2):264-276. doi:10.1016/j.cell.2015.02.047\n- Palmnäs MS, Cowan TE, Bomhof MR, et al. Low-dose aspartame consumption differentially affects gut microbiota-host metabolic interactions in the diet-induced obese rat. PLoS One. 2014;9(10):e109841. Published 2014 Oct 14. doi:10.1371/journal.pone.0109841""]"	['<urn:uuid:4e386106-9cf9-4e25-a373-9c37882a8e9a>', '<urn:uuid:a27ce047-8dc3-4390-86ff-5da2757ce107>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T19:10:11.825225	40	139	2635
89	methods processing storing yak milk products traditional tibetan communities	In remote mountain areas, Tibetans process yak milk into butter and cheese. The milk comes from both yaks and Dzomo (female yak/cow hybrids), with Dzomo being preferred as they produce more milk while maintaining the sturdiness of yaks. Some communities milk up to 80 cows twice daily to make butter and cheese. The cheese is dried into curd cheese for preservation.	"['Yak bells hanging inside a Tibetan house in Kham (Sichuan, China)\nThe clappers are constructed with yak horn and also have a really mellow, soothing tone.\nA Tibetan grandmother utilizing a drop spindle to spin yak fiber.\nSeveral generations live together. Kham (Sichuan)\nIn the Tibetan home in Kham. El born area is heavily forested and also the houses are big. They sometimes are three tales first floor for animals, second floor to see relatives and third or loft for storage. This photo is from the large living space with wood stove for warmth and cooking. There’s a bench round the wall the gathering area. Within the lowlands yak butter is blended or emulsified in to the tea which makes it creamy and wealthy. We actually loved it but needed to be moderate due to the richness.\nDzo, men yak/cow hybrid. These first mix males (F1) are naturally sterile and therefore are utilized as work creatures. They are simply strong.\ndecorated yak skull\nyak herder in summer time mountain pasture\nTibetan lady milking a Dzomo, a lady yak/cow hybrid. Dzomo can be sturdy like yaks but produce more milk. Within the remote mountain tops the milk is created into butter and cheese.\nA ""lengthy-hair-brow yak"" as referenced within the Yak. This really is most likely the genetic base that Super and Extreme Wooly Yaks in america come from. This cow has already established her belly or skirt hair sheared. It’s employed for making durable bags and cordage.\nImperial Trim yak cow\nPrayer flags and also the Himalaya\nTibetan man milking a yak cow. This man was residing in a conventional yak hair tent. The bag on his belt is stuffed with salt for that yak cows.\nHands harvesting wheat with scythes\nProtector deities inside a Buddhist Temple. We visited many temples. When requested what age the temple was the response could be some ancient date and so the reply it had lately been reconstructed. Several different temples of religions were destroyed throughout the Chinese Cultural Revolution.\nI needed to create this bull home beside me\nTibetan nomads searching at images of our yaks. The finest complement we’ve got was that people were much like them, American Nomads. Amdo, Tibet. The Qinghai Plateau of China is where yaks evolved. We had the very best and many yaks within this sparsely populated region.\nTibetan nomad girl. This family asked us to their yak hair tent for butter tea and tsampa cooked more than a yak dung fire.\nThe ribbon marks this yak cow continues to be ""fortunate"".\nspotted yak cow\nTibetan lady milking a yak cow. They were milking 80 cows two times each day after which making butter and cheese\nA stack of dried yak dung. There aren’t any trees around the Tibetan Plateau so yak dung may be the only fuel.\ndrying curd cheese\nPrayer flags and stupa\ngetting dried yak dung to promote\nyak calf on stake rope while cow has been milked\nyak cows on tether rope for milking\nMongolian nomad herding Dzo\nTibetan nomad. This man rode into the city on the horse early one foggy morning. I believe I had been the very first westerner he’d seen\nTraditional Tibetan black tent made from hands spun and hands woven yak hair.\nI might have been the very first westerner this yak bull had seen\nTibetan Mastiff. These dogs guard the camp ground and can bite you.\nMoving Camp by yak\nAnimals | A-Z Animals sound | ANIMAL sounds, pictures and names\nFun4sure: Good educational video I have got better knowledge now 🙂 I subbed\nSurprise Eggs Toys 4 Kids: Very nice video! Please check out my channel too. Have a very nice day!\nABCD Surprise TV: Wonderful ! Thumbs up ! )))))))))))\nHanna Domjan: spectacular video!Liked! Love hanna and mia:)\nYoYo TV: +Hanna Domjan Thx for watching appreciate it 😊\nVika TV: Hello! VikaTV already with you!\nYoYo TV: +Vika TV Thx for watching\nMike D.: ,•*´¨)¸.•*¨)★☆ ★\\n¸.•´¸.•*´¨)\\n(¸.•´ (¸.•` * ¸.•´¸.•*´¨)\\n……\\~~~~~/…..\\~~~~~/\\n…….\\~~~~/…… .\\~~~~/\\n……..\\~~~/…….. .\\~~~/\\n………\\~~/……….. \\~~/ Happy new Year !!!!!! My friend all the best\\n……….\\~/………… .\\~/\\n………..||…………. ..|| very good video :)\\n………..||………….. .||\\n………..||………….. .||\\n……./**\\………. /**\\\nYoYo TV: +Michał Długoszek Thx Happy ny 2 u too\nKahoSei Channel from Canada: Cool A to Z!! Good for my kids!!\\nSubbed! and like!']"	['<urn:uuid:b8423120-b302-49e0-be8d-f4da4ef1304f>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:10:11.825225	9	61	709
90	What is the purpose of the PIC microcontroller's configuration word, and how does Microchip handle customer information when using their development tools?	The configuration word stores essential settings for PIC microcontrollers, including code protection bits to protect memory blocks, power-on timer enable bit, watchdog timer enable bit, and oscillator selection bits. As for customer information handling, Microchip collects user data anonymously during online interactions like purchases and discussions, using it to improve services and customize interactions. They secure financial information with 256-bit encryption and only share customer information with subsidiaries and necessary third-party agents for specific tasks like web hosting and package delivery.	"[""By Lucio Di Jasio, Tim Wilmshurst, Dogan Ibrahim и др.\nThe Newnes comprehend it All sequence takes the easiest of what our authors have written over the last few years and creates a one-stop reference for engineers concerned with markets from communications to embedded platforms and in every single place in among. PIC layout and improvement a usual healthy for this reference sequence because it is among the preferred microcontrollers on this planet and we have now numerous beautifully authored books at the topic. This fabric levels from the fundamentals to extra complex subject matters. there's additionally a truly powerful undertaking foundation to this studying. the common embedded engineer operating with this microcontroller should be capable of have any query responded through this compilation. He/she can be in a position to paintings via real-life difficulties through the initiatives inside the publication. The Newnes comprehend it All sequence presentation of conception, difficult truth, and project-based path should be a continuing relief in supporting the engineer to innovate within the place of work. Contents part I. An advent to PIC MicrocontrollersChapter 1. The PIC Microcontroller kinfolk bankruptcy 2. Introducing the PIC sixteen sequence and the 16F84A bankruptcy three. Parallel Ports, energy provide and the Clock Oscillator part II. Programming PIC Microcontrollers utilizing meeting LanguageChapter four. beginning to Program?An advent to Assembler bankruptcy five. development Assembler courses bankruptcy 6. extra Programming ideas bankruptcy 7. Prototype bankruptcy eight. extra PIC purposes and units bankruptcy nine. The PIC 1250x sequence (8-pin PIC microcontrollers) bankruptcy 10. Intermediate Operations utilizing the PIC 12F675 bankruptcy eleven. utilizing Inputs bankruptcy 12. Keypad Scanning bankruptcy thirteen. application Examples part III. Programming PIC Microcontrollers utilizing PicBasicChapter 14. PicBasic and PicBasic seasoned Programming bankruptcy 15. basic PIC initiatives bankruptcy sixteen. relocating On with the 16F876 bankruptcy 17. conversation part IV. Programming PIC Microcontrollers utilizing MBasicChapter 18. MBasic Compiler and improvement forums bankruptcy 19. The Basics?Output bankruptcy 20. The Basics?Digital enter bankruptcy 21. Introductory Stepper vehicles bankruptcy 22. electronic Temperature Sensors and Real-Time Clocks bankruptcy 23. Infrared distant Controls part V. Programming PIC Microcontrollers utilizing CChapter 24. Getting began bankruptcy 25. Programming Loops bankruptcy 26. extra Loops bankruptcy 27. NUMB3RS bankruptcy 28. Interrupts bankruptcy 29. having a look less than the Hood AppendicesAppendix A. PIC sixteen sequence guideline Set. Appendix B. The digital Ping-Pong. Appendix C. DIZI-2 Board and Lock software. Appendix D. software M. Appendix E. software N. Appendix F. application O. Appendix G. application P. Appendix H. application Q. Appendix I. beneficial PIC info. Appendix J. PIC 16F84A Datasheet\nRead or Download PIC Microcontrollers: Know It All PDF\nBest products books\nBeginning with a uncomplicated evaluation of system-on-a-chip (SoC), together with definitions of comparable phrases, this booklet is helping you realize SoC layout demanding situations, and the newest layout and try out methodologies. you notice how ASIC expertise developed to an embedded cores-based idea that comprises pre-designed, reusable highbrow estate (IP) cores that act as microprocessors, facts garage units, DSP, bus keep watch over, and interfaces - all ?\nThe multicore revolution has reached the deployment degree in embedded platforms starting from small ultramobile units to massive telecommunication servers. The transition from unmarried to multicore processors, stimulated by way of the necessity to bring up functionality whereas maintaining strength, has positioned nice accountability at the shoulders of software program engineers.\nThe Newnes realize it All sequence takes the simplest of what our authors have written during the last few years and creates a one-stop reference for engineers inquisitive about markets from communications to embedded platforms and far and wide in among. PIC layout and improvement a normal healthy for this reference sequence because it is among the most well-liked microcontrollers on the planet and we now have numerous fantastically authored books at the topic.\nTraditional items Isolation presents a complete advent to thoughts for the extraction and purification of typical items from all organic resources. The publication opens with an creation to separations and chromatography and discusses the method of an isolation. skilled experimentalists describe a big selection of equipment for isolation of either recognized and unknown average items, together with preliminary extraction, open column chromatography, HPLC, countercurrent and planar chromatography, SFE, and crystallisation.\n- ABC-TRIZ: Introduction to Creative Design Thinking with Modern TRIZ Modeling\n- RF Circuit Design\n- The Delft Systems Approach: Analysis and Design of Industrial Systems\n- Konstruktionslehre für den Maschinenbau: Grundlagen zur Neu- und Weiterentwicklung technischer Produkte mit Beispielen\n- Rapid Prototyping of Digital Systems: SOPC Edition\nAdditional resources for PIC Microcontrollers: Know It All\nThis register can be accessed during the programming of the microcontroller. The conﬁguration word stores the following information about a PIC microcontroller: • Code protection bits: These bits are used to protect blocks of memory so that they cannot be read. • • Power-on timer enable bit. Watchdog (WDT) timer enable bit. c o m The PIC Microcontroller Family • 33 Oscillator selection bits: The oscillator can be selected as XT, HS, LP, RC, or internal (if supported by the microcontroller). For example, in a typical application we can have the following conﬁguration word selection during the programming of the microcontroller: • • • • Code protection OFF XT oscillator selection WDT disabled Power-up timer enables.\nADCON1 is the second A/D control register. This register controls the format of converted data and mode of the PORTA inputs. The bit format of this register is shown in Fig. 17. c o m The PIC Microcontroller Family 25 Bit 7 is called ADFM and when this bit is 0 the result of the A/D conversion is left justiﬁed; when it is 1, the result of the A/D conversion is right justiﬁed. If we have an 8-bit converter, we can clear ADFM and just read ADRESH to get the 8-bit converted data. If we have a 10-bit converter, we can set ADFM to 1 and the 8 bits of the result will be in ADRESL, and 2 bits of the result will be in the lower bit positions of ADRESH.\nSeparate and dedicated pins are now provided, for example, for clock oscillator (pins 15 and 16) and Reset (pin 4— ——— MCL R). Nevertheless, compared to most, the ’F84A remains a small microcontroller. Architecturally there is clear similarity between the 12F508 and the 16F84A. In fact, the former is a direct subset of the ’F84A, with near identical CPU, memory, bus structure and counter/timer (TMR0) peripheral. Notice ﬁrst, however, that the address bus sizes have been increased, to meet the needs of the whole PIC 16 Series family.\nPIC Microcontrollers: Know It All by Lucio Di Jasio, Tim Wilmshurst, Dogan Ibrahim и др."", 'Please share your suggestions, comments, and criticisms in the box below. Every message is read and receives a response. Thank you for taking time to improve the tool!\nThis example demonstrates how to use the computation features of the 12-bit ADC with Computation (ADCC), along with other intelligent peripherals on PIC16F18446, to create a core-independent solution to microwave-based motion sensing.\nA combination of the peripherals on PIC16F18446 can provide a hardware-based solution with minimal software routines and CPU interference. Therefore, in a typical sensor node application like this, the CPU can be put into sleep mode for most of the time to reduce power consumption (especially for a battery-powered application), or take time to do other tasks to increase system functionality.\nIt uses the Microwave Click from Mikroelektronika, and a PIC16F18446 Xpress Board. The LED on the Xpress Board will be turned on, and a message will be sent through the serial port when there is a motion. Before using, press the pushbutton on the Xpress board to calibrate a reference first. The calibration takes around half a second.\nBefore we get started, let\'s take a moment and briefly look at how the motion sensing works with a Microwave Click.\nThe Microwave Click can detect movement or proximity by using the Doppler effect. The on-board microwave motions sensor transmits waves, and picks them back as they hit an object, with their frequency changed. For more information on the Click, please visit the Microwave Click webpage.\nBasically, a motion can be detected by reading the analog signal coming from the Click. The frequency and amplitude will change when there is a motion:\nA software-based solution has been introduced on the previous Click webpage. The topology is shown below: A calibration is needed at first to take dozens of samples when there is no motion, and average them to get a reference value. After the calibration is done, only the samples that are larger (you can also take smaller) than the reference will be saved. Then, take the average of a fixed amount of saved samples and compare it to a threshold. If the averaged result is bigger than the threshold, it is considered to be a motion.\nHere is a typical software-based implementation when using a microcontroller without ADCC and other core-independent peripherals: To achieve all the tasks listed for the MCU, a basic ADC is only used to sample all the data, and the CPU is having a code-heavy routine in the main loop to process the data. Note the little clocks in the above diagram. It indicates that those routines in software are consuming most of the time. For example, a divide operation is going to take a lot of time for a MCU without a hardware divider.\nThanks to the ADCC with its powerful computation feature, a lot of the tasks can be done within the peripheral. Moreover, with a combination of ADCC, Timer2, Comparator, and DAC, most of the motion detection algorithm can be achieved in hardware: Note that with this implementation, the only task that the CPU needs to do is to issue the motion alert once there is an ADCC threshold interrupt.\nThe procedure to make this work is not as complex as it looks. In general, the ADCC is triggered by Timer2 to convert the samples, and its averaging with threshold comparison feature is going to take care of the computation tasks. The Hardware Limit Timer feature of Timer2 allows it to set the Comparator as an External Trigger Source, so it runs when Comparator\'s output is high. The combination of Comparator and DAC can output a high value when the waveform is higher than the reference value. Therefore, only when the waveform is higher than the reference value, Timer2 will start running, which triggers the ADCC conversion. With this setup, the ADCC converts sample automatically when the waveform is higher than the reference, and does the averaging and thresholding within the peripheral.\nThis example uses the PIC16F18446 Xpress Board as the main control board, and the Microwave Click as the sensing unit. The circuit can be easily set up by plugging the two boards and three wires on a breadboard.\nPlug both the PIC16F18446 Xpress Board and the Noise Click into a breadboard.\nUse a wire to connect ground, with the Ground symbol pins on both boards.\nUse another wire to connect Vdd. Connect ""3V3"" pin on the Click to the pin just above RA5 on the Xpress Board (you can see it as VTG on the back of the board). Please note that according to the specifications of the Microwave Click, a 5V power supply is recommended. The PIC16F18446 is capable of running at a 5V Vdd. I used the 3.3V just because it\'s coming straight to the board, so I don\'t need an extra power supply.\nUse a third wire to connect your AN pin on the Xpress Board (in my case, RA1) to the ""OUT"" pin on the Click.\nIn order to achieve the functionalities of all the peripherals above. MPLAB Code Configurator (MCC) is used to configure them.\nOpen MCC by clicking on the MCC symbol on top left of the IDE panel.\nIn ""Device Resource"", add ADCC to the project by double-clicking on it. We are using the Average Mode of the ADCC with a threshold triggering interrupt. So enable the ADCC Threshold interrupt by checking the corresponding boxes. Expand ""Computation Feature"", select ""Filtered result vs setpoint"" as the Error Calculation, and leave the Setpoint blank since we are going to set it in run-time. Select ""ADERR > ADUTH"" as the Threshold Interrupt, so that the threshold interrupt is triggered whenever the ADC result is above the upper threshold. You can set your Upper Threshold in the ""Upper Threshold"" box, or leave it blank for now and set it up later in your code. We are going to use a hardware timer to trigger the ADCC auto-conversion, so that no software routine are needed to do the ADC conversion. Under the ""Auto-conversion Trigger"" Tab, select a timer, such as Timer 2, to be the triggering source.\nAdd Timer 2 to the project, as we are using it as the ADCC auto-conversion triggering source. We want the conversion to be triggered every 5ms. Select ""Fosc/4"" or ""LFINTOSC"" as the Clock Source, then set up the Prescaler or Postscaler to make sure that 200ms is within the Timer Period range. Type in ""5ms"" in the Timer Period Box. Select the Comparator as the External Reset Source and select the Timer2 to be running when the External Reset Source is high, so that if the Comparator is triggered high, Timer 2 starts running.\nAdd Comparator to the project. Configure the Comparator to have the right output for TMR2 to be triggered. Since the DAC can only be the positive reference of the Comparator, we need to invert the output of the Comparator so that it will output a high when the waveform level is higher than the reference.\nAdd DAC to be the positive reference of the Comparator. The reference value should be fed into DAC after the Calibration.\nAdd EUSART peripheral to the project, since we are using the UART communication to send data to the computer terminal. Enable Transmit, and enable ""Redirect STDIO to USART"", to be able to use ""printf"" statement.\nBy checking the schematic of the Xpress Board, we know that RB6 and RB4 are the UART RX and TX pins. In ""Pin Manager"", set the RX and TX pin, and an AN pin as the ADCC input channel that connects to the Microwave Click AN pin (i.e., RA1). Then go to the Pin Module under System Tab, and customize the name of your AN input pin (i.e. Microwave_AN).\nEverything should be set up. Generate Project.\nOpen a terminal on your computer by using terminals like TeraTerm, and connect the corresponding COM port of the Xpress Board. Note the settings of the serial port should be the same as the settings of the UART in MCC.\nAs a result, you will see the noise level displaying on the terminal window, and an alert appearing whenever the motion level is above the threshold you set. And the LED on the Xpress Board should be turned on. Press the button on the Xpress board to re-calibrate the reference.\nMPLAB Xpress IDE shall mean the online integrated development environment (hereinafter “MPLAB Xpress”) operated by or on behalf of Microchip Technology Incorporated (hereinafter ""Microchip"") for your use, whether as a guest or a registered user. Microchip is a Delaware corporation with its principal office at 2355 W. Chandler Boulevard, Chandler, AZ 85224.\nAccess to MPLAB Xpress is permitted on a temporary basis, and Microchip reserves the right to withdraw or amend the information provided on MPLAB Xpress without notice. Microchip will not be liable if for any reason MPLAB Xpress is unavailable at any time or for any period. From time to time, Microchip may restrict access to some or all of MPLAB Xpress.\nMaterial on MPLAB Xpress may be out of date or include omissions, inaccuracies or other errors at any given time, and Microchip is under no obligation to update such material. Commentary and other materials posted on MPLAB Xpress are not intended to amount to advice on which reliance should be placed. Microchip disclaims all liability and responsibility arising from any reliance placed on such materials by any user of MPLAB Xpress, or by anyone who may be informed of any of its contents. Microchip owns or has the rights to the materials and information it posts on MPBLAB Xpress, but does not and cannot warrant or guaranty any information or materials posted by others.\nEXCEPT WHERE EXPRESSLY PROVIDED OTHERWISE IN AN AGREEMENT BETWEEN YOU AND MICROCHIP, ALL INFORMATION PROVIDED DIRECTLY ON MPLAB XPRESS OR INDIRECTLY THROUGH MPLAB XPRESS BY HYPERTEXT LINK OR OTHERWISE IS PROVIDED ""AS IS"" WITHOUT WARRANTY OF ANY KIND. MICROCHIP HEREBY DISCLAIMS ALL WARRANTIES WITH RESPECT TO THIS INFORMATION, WHETHER EXPRESS OR IMPLIED, INCLUDING THE IMPLIED WARRANTIES OF MERCHANTABILITY, SATISFACTORY QUALITY AND FITNESS FOR A PARTICULAR PURPOSE. IN NO EVENT SHALL MICROCHIP BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL OR CONSEQUENTIAL DAMAGES, OR DAMAGES FOR LOSS OF PROFITS, REVENUE, DATA OR USE, INCURRED BY YOU OR ANY THIRD PARTY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM YOUR ACCESS TO, USE OF, OR RELIANCE UPON INFORMATION OBTAINED FROM OR THROUGH MPLAB XPRESS. MICROCHIP RESERVES THE RIGHT TO MAKE CHANGES, UPDATES OR CORRECTIONS TO THE INFORMATION ON MPLAB XPRESS AT ANY TIME WITHOUT NOTICE.\nContracts for the supply of goods, services or information formed through the use of the MPLAB Xpress site or as a result of visits made by you to the MPLAB Xpress site are governed by these terms and conditions.\nMPLAB Xpress may contain links to other sites and resources provided by third parties. Any such links are provided for your information only. Microchip has no control over the contents of those sites or resources, and accepts no responsibility for them or for any loss or damage that may arise from your use of them.\nHow is Customer Information Obtained by Microchip and How is it Used?\nWe receive and store information you give us, anonymously. Most of the information you give us is collected when you buy products on-line, request literature/ information, participate in a contest, fill out a questionnaire/survey, participate in an on-line discussion, or complete your Customer Profile. You may give us information such as your name, address, phone numbers, e-mail addresses, financial information, employment information, application information, ideas for new projects or applications, and your buying interests. The information that you provide us is used to respond to your requests, notify you of products being shipped to you, customize your interactions with us, improve our offerings, provide materials or offers to you, allow you to participate in contests, and communicate with you. It is your choice whether or not to provide us with certain information. However, you may be unable to take advantage of many of our offers and features if we do not receive your customer information.\nWe also receive and store certain types of information through use of “cookies.” Some examples of information gathered this way are the Internet Protocol (IP) address used to connect your computer to the Internet, log-in, e-mail address, computer and connection information such as browser type and version, operating system, platform, and the full Uniform Resource Locator (URL) used to, through, and from access of MPLAB Xpress. We use the information gathered by “cookies” to manage and customize your on-line experience and to improve our services.\nYou may also submit technical information to Microchip such as designs, ideas or data, that you transmit to or post on MPLAB Xpress example section (a ""Submission"") for purposes not limited to community development participation. By posting a Submission, you understand that you give Microchip and the community a nonexclusive, royalty-free, perpetual, irrevocable, sublicensable right to disseminate, display, use, modify, copy, adapt, and translate the Submission for any purpose whatsoever, whether commercial or noncommercial, throughout the world in any media. The Submission is not confidential or proprietary. Providing the Submission, grants the right to use the name you submit in connection with your Submission. You represent and warrant in making the Submission that you own or control all rights to the Submission, and that use of your Submission will not cause injury to any person or entity, including Microchip. You will indemnify Microchip for all claims resulting from your Submission.\nHow can I manage the information Microchip has about me?\nYou can always choose not to give Microchip information. However, certain information is necessary in order for you to take advantage of MPLAB Xpress, any change notifications, literature, updates, etc.\nIf you want to add/update your information or preferences, please modify your MPLAB Xpress account profile. When you update your information, we may keep a copy of your earlier information for our records.\nYou can avoid certain ""cookies"" by indicating in your account profile that you do not want us to remember you. Your browser may also have options to prevent acceptance of new “cookies.” There are also utilities available allowing you to visit MPLAB Xpress anonymously. Unfortunately, certain features and offerings available through MPLAB XPress may not be available to anonymous users.\nWho does Microchip share Customer Information With?\nCustomer information is important for Microchip to be able to offer products and services to you. We are not in the business of selling customer information to others. We do share customer information with our subsidiaries and with others as described below. Such third parties may be based in the United States, Europe or Thailand. We are not responsible for the privacy practices of third parties to whom your information may be transferred. If you do not wish us to share your information as described below, you may opt out by not supplying information to us when entering information into your account profile, or by not placing orders through MPLAB Xpress.\nAgents: We use other companies and persons to perform tasks on our behalf. Examples are not limited to web hosting of MPLAB Xpress, delivering packages, processing credit card payments, checking for export compliance, and providing customer service. They have access to customer information to perform their jobs, but may not use it for other purposes. Microchip may share your customer information with third parties and receive information from credit bureaus and other companies to help prevent and detect fraud, and to offer credit or financial services to some customers. Microchip cannot guaranty the security of such agent services provided on its behalf. As such, Microchip makes no representations or warranties regarding any third party services and has no responsibility or liability whatsoever for any such third party services and is not responsible for the content or privacy practices of such third parties\nThird Parties: Except for information transmitted to Microchip through the on-line support system at http://support.microchip.com, any other Submission to Microchip such as designs, ideas or data that you transmit to or post on MPLAB Xpress by any means for any purpose including but not limited to contest participation is considered and treated by Microchip as nonconfidential and nonproprietary.\nConformance with the Law: We release account and other customer information when we believe it is appropriate to comply with the laws of Arizona.\nProtection of Microchip and Others: Microchip will release customer information necessary to enforce or apply our MPLAB Xpress IDE Terms and Conditions, and other agreements. Microchip will, at its discretion, release customer information necessary to protect the rights, property and safety of Microchip or others in accordance with the laws of Arizona.\nWe use 256-bit encryption to secure the financial information you enter. Only the last five digits of your credit card number are listed in your Order Acknowledgement. However, your entire credit card number is transmitted to your credit card company during order processing, and your identifying and credit information is transmitted to companies that process your credit card transactions and review them for export control violations.\nLaw and Jurisdiction\nIf you have any concern about our treatment of your customer information, please send us an e-mail at email@example.com.']"	['<urn:uuid:1790fbc7-439d-462a-b1de-fa315ff3b46c>', '<urn:uuid:b68ceb7e-70d8-48c0-9798-07a57f348c61>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	22	81	3970
91	cover crops benefits soil properties	Cover crops offer multiple benefits for soil properties and crop management. From a biological perspective, they contribute to increased soil biological activity, particularly when combined with reduced tillage systems. Additionally, when used as high-residue mulch, cover crops provide practical benefits including weed suppression, moisture conservation, and erosion reduction. To be effective for weed control, the cover crop residue needs to be properly managed through either rolling/crimping or mowing, with rolling generally providing better weed suppression and more uniform residue placement.	['Effects of Tillage, Rotation, and Organic Inputs on Soil Ecological Properties in Vegetable Crop Production Systems\nAgricultural management decisions that influence biological activity and diversity include tillage, fertilizer and pest-control inputs, and crop rotations. Our research objective was to characterize relationships between biological and physical properties resulting from long-term agricultural management decisions. A nine-year old factorially-designed field experiment was used to examine the effects of tillage (moldboard plow or strip-tillage), input (synthetic fertilizers and pesticides or inputs approved for organic certification programs), and crop rotation (continuous staked tomatoes or 3-year vegetable rotation) on a suite of biological and physical soil parameters. Biological measurements included microbial, nematode, and earthworm community composition, soil respiration and N mineralization potential, enzyme activity, and microbial biomass. Physical property measurements included aggregate stability, bulk density, and pore-size distribution. Biological properties generally responded to all treatment combinations, but tillage provided the strongest treatment effect in most cases. Compared to strip-tillage, moldboard tillage consistently yielded significantly lower values for the following biological measurements: total C and N, above-ground biomass, microbial biomass, enzyme activity, soil respiration, N mineralization, some nematode trophic groups, and earthworms. Compared with organic inputs, synthetic inputs consistently induced significantly lower values for the following biological measurements: microbial biomass, enzyme activity, some nematode trophic groups, and soil respiration. An examination of relationships between biological and physical parameters using redundancy analysis revealed that microporosity was the physical property that was most strongly correlated with most biological parameters. Soil organisms responded to treatments in the following order: tillage > input > rotation.\n1. To evaluate the biological diversity of soils under different agricultural management strategies, recognizing that a highly productive agricultural soil is considered to be one with a high degree of biological activity and containing a stable cross section of microorganisms and invertebrates.\n2. To make the best possible estimation of microbial and invertebrate populations and community structure using a range of direct enumeration and community evaluation techniques.\n3. To assess the effect of microbial and invertebrate communities on the following soil physical properties: aggregate stability, bulk density, porosity, and pore size distribution.\n4. To aid in the assessment of soil degradation by identifying soil biological indicators of high soil productivity potential for agricultural soils.\nSoil biological measurements have been concluded for microbial, nematode, and earthworm community composition, soil respiration and N mineralization potential, enzyme activity, and microbial biomass. Physical property measurements that included aggregate stability, bulk density, and pore-size distribution are also finished. This experiment has completed the field and lab measurement phase and final conclusions are being written. Final publications of the work are in departmental or journal review.\nImpacts and Contributions/Outcomes\nWe found that greatest biological activity among various field production systems was in those systems that had the least tillage. Changing production systems from chemical to organic also produced increased biological activity, but not to the the extent that tillage reduction did. We have given many workshops and speaker presentation on the findings of this research, with much interest from producers who want to achieve greater soil productity from their farm operation.\nPhD Graduate Student\nDept. of Soil Science, NCSU\nRaleigh, NC 27695-7619\nOffice Phone: 9195133037', 'Rolling or mowing high-residue cover crops like rye and vetch can provide a weed-suppressing and moisture-holding mulch into which vegetable and grain crops can be direct seeded or transplanted. These systems have been successful for tomatoes, pumpkins, broccoli, and other vegetable crops that are later-planted and are not negatively affected by the soil cooling effect of mulch.\nTerminating cover crops in timely manner in spring/summer can be one of the most challenging aspects of these systems. Many growers who use herbicides terminate their cover crops chemically before they can be killed mechanically. Attempting to kill a cover crop mechanically before flowering will result in regrowth. This limits the timing for organic growers, many of whom find they cannot wait until mechanical termination is possible for their first round of some cash crops. Nonetheless, these high-residue systems show a lot of potential for weed suppression, moisture conservation, and erosion reduction.\nIn general, it is believed that rolling/crimping leads to better weed suppression and more uniform residue placement than mowing, resulting in less equipment trouble when planting the following crop. However, some reports have indicated that flail mowing of cover crops is equally effective.\nEven though cover crop mulch provides some weed control, it may not be sufficient for the whole growing season. It has been estimated that to achieve 75% inhibition of weed emergence, 10 cm (4 inches) of biomass is necessary (read more on mulch and weed suppression). Dr. Gerald Brust at University of Maryland looked at using a weed barrier (cloth) for just 1-2 weeks early in the season, and found it increased weed control substantially in organic tomatoes. We recently tried using supplemental tarps to kill rye, vetch and weeds and had some encouraging results in our first trial with black tarps. In Germany, researchers are experimenting with supplemental cut-and-carried mulch for increasing weed suppression.\nFurther Reading and Watching\nThere is a lot of information on rolling/crimping cover crops, but relatively little information on using these cover crop-based systems for vegetable production. The resources listed here cover different parts of the country and we suggest contacting some of the experts in your region for more information.\nWeed ’em and Reap Part II: High residue systems (Videos) featuring Mark Schonbeck, Ron Morse, and others. These videos are incredibly helpful for people considering high-residue systems and have great visuals.\nOrganic Reduced Tillage in the Pacific Northwest eOrganic page of the Washington State University project. Includes regionally important information and cover crop variety trials for the Pacific Northwest.\nReduced Tillage and Cover Cropping Systems for Organic Vegetable Production by Mark Schonbeck and Ron Morse. Project supported by Southern SARE.\nFacilitating Improved Soil Quality on Organic Farms through Research and Training on No-Till Organic Vegetable Production in the Midwest Update on 2010 project at Iowa State University with no-till broccoli and squash.\nWhat is “Organic No-till” and is it Practical? by Mark Schonbeck (from eOrganic)\nRodale Institute has published a report, Beyond Black Plastic, on using cover crop mulches for no-till vegetable production. These systems have come a long way in the last 20 years.\nCover Crops and No-Till Management for Organic Systems, from the Rodale Institute. (includes nice pictures and discussion of equipment needed for high-residue transplanting and cultivation as well as an extensive reference list).']	['<urn:uuid:0f0e9afe-f184-451e-9d24-68c714462c8f>', '<urn:uuid:9a9efc51-6c6f-4d8e-aa3b-bcd4696dcd63>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	5	80	1068
92	What distinguishes wine education in tasting versus commerce programs?	Wine tasting education emphasizes sensory aspects through books like the Wine Taster's Guide which offers 30 guided tastings and journaling, while wine business education provides comprehensive training in operations, covering everything from vineyard management to distribution channels, marketing strategies, and e-commerce through structured certificate programs.	"['On the fourth day of Christmas, my true love sent to me:\n- four wine books\n- three phone soaps\n- two festive wines\n- and a wine calendar from Wine4Me.\nWhile 2020 did not see a whole lot of wine travel or visits to tasting rooms for most of us, a lot of wine education happened anyway by way of ZOOM webinars, online classes, books, and more.\nWhich brings me to today’s Wine Lover’s Holiday Gift Guide: Four Wine Books published in 2020.\nIn March 2020, I was set to join the Napa Wine Academy’s Spring Break WSET 3 course. I was finishing up my packing to drive to Napa when I got the call from Catherine Bugue that because of the worsening pandemic, the class would be cancelled. I hoped I’d be able to do the fall Harvest class instead, but of course that class was also cancelled. I’ve worked through a number of the online videos and read much of the book, but it’s just not quite the same. And while a great gift, taking on a WSET course text isn’t a recent publication and certainly not for everyone!\nInstead, I offer up four books published in 2020, as well as a link to six recommended by Eric Asimov (below). And no, these aren’t affiliate links; I’m not making any money off of these although I’d love copies of the books!\n- Nick Jackson, MW: Beyond Flavour: The Indispensable Handbook to Blind Wine Tasting\n- Joe Robert: Wine Taster’s Guide: Drink and Learn with 30 Wine Tastings and Wine Tasters Journal\n- Arthur George: The Mythology of Wine\n- Stephen Spurrier: A Life in Wine\nAs a surprise member of the 2019 US Wine Tasting team when Sue and I came in second place which sent us to France, I wanted to increase my chances for making the 2020 team for the trip to Bordeaux. In addition to planning on taking the WSET 3 course, I sat in on their free blind wine classes on Facebook, I participated in a number of ZOOM blind tasting seminars with David Glancy of the SF Wine School, and paid for a few classes with Nick Jackson MW who wrote the 2020 release, Beyond Flavor which I just LOVE… and which leads me to…\nWINE BOOK #1: Beyond Flavor\nIn Beyond Flavor , Nick Jackson, MW analyzes wine in a way that really speaks to me but also gives me a language and tools to express and explain why I know a wine is what it is. His focus on the texture and structure of a wine, and finding words to describe and define that texture, really makes sense, because texture and structure is what I previous understood as what makes a wine just “seem” like the wine that it is– it was a seemingly indescribable experience on the palate. That does not help your team mates, except for Sue because we have tasted so many wines together, we have a short hand.\nNick achieved his Master of Wine degree in 2019, and analyzed how he got there for his book,Beyond Flavour: The Indispensable Handbook to Blind Wine Tasting. Released in 2020 the book focuses not on the flavours of wine, but instead “advocates an approach based on assessing a wine’s structure. For white wines, that means acid structure, and for red wines, tannin structure. Nick argues that each major variety has a slightly different structure, and by assessing the structure, it is possible to distinguish varieties from one another without recourse to flavours, which can be inconsistent and unreliable markers.”\nWine Spectator says that Beyond Flavour is “quietly the best wine book to come out in recent memory” and it has received positive reviews in the Wall Street Journal, The Times and others. During the 2020 Covid-19 pandemic, Nick started The School of Taste which offers a variety of live and pre-recorded wine webinar content using the ideas from his book and his experiences.\nThe book’s success is unexpected; in an email to me, Nick admits that, “To be honest I had very limited ambitions with the book; I just had all the thoughts knocking around in my head so I thought I might as well write them down, and sell a few copies to wine students I know. The response has been very surprising – in a good way! I’ve been amazed first, that so many people are interested in this niche subject; and second, that my sometimes curious approach has elicited positive reactions. If nothing else, I hope at least to have started a conversation about how to include structure more seriously as a tool for understanding wine.”\nIndeed he has! Check out the School of Taste offerings here.\nWINE BOOK #2 (well book 2a and 2b!)\nMy second book selection Wine Tasters Guide and Wine Tasters Journal came out in July 2020, and it’s by Joe Roberts, blogger at 1WineDude who I met at the Wine Bloggers Conferences over ten years ago. A writer, blogger, video personality, wine critic, and frequent wine competition judge, Joe’s writing has appeared in publications as varied as Playboy.com and Parade. On his blog, he offers “Serious wine talk for the not so serious drinker.”\nBefore she became the best-selling author of A Discovery of Witches and Shadow of Night, Deb Harkness was also a wine blogger, and Joe and Deb and I tasted together a few times. Of his books, she writes, “Consistently stunning writing.” And here’s a review from Leslie Sbrocco, Author and PBS Television Host, who I also met through these conferences:\n“I’ve been writing about wine for two decades and Wine Taster’s Guide is one of the best books I’ve seen in years. Appealingly brought to life with photos, maps, and tasty tips, it’s a must-read for those who enjoy sipping and want to learn more. Covered with compelling precision from regional overviews to savvy wine picks and thoughtful tasting sessions, the overwhelming subject of wine is brought to life through Joe’s wit and wisdom. Begin your tasting journey with this impressive book in one hand and a glass of delicious wine in the other.”\nWhat I really focused on while writing (and especially while editing!) the books was to make them insanely useful.\n“What I mean is, anything that wasn’t potentially teaching the reader about wine didn’t make the cut. So my hope is that anyone who picks up the book is entertained, but realizes later that they actually took away a lot of useful stuff, no matter what page, tasting or chapter they happen to gravitate to first,” Joe told me.\nJoe’s Wine Taster’s Guide: Drink and Learn with 30 Wine Tastings has a companion to it, a Wine Tasters Journal to help you keep track of what you are tasting and what you think about it.\nIf you’re just starting out on your wine journey, this could really help!\nSo enough about WINE TASTING! Because there’s a lot more to wine than drinking it… although admittedly that’s at least half the fun.\nBut you can’t drink all day so during COVID there’s time to get deeper into it which is the topic of the next two books.\nWine Book #3: The Mythology of Wine\nSanta Barbara resident and retired lawyer Arthur George has a new book, The Mythology of Wine, which came out in November 2020, just in time for holiday gift giving. With details from wine-related myths in ancient Greece, Mesopotamia, Egypt, Canaan, and Israel, as well as in early Christian Europe, Arthur says his book shows “how these stories ultimately influenced today’s wine culture and our religions and culture at large. It is fun, not long or technical, easy to read, has lots of illustrations, and is not expensive (ebook $9.99, paperback $14.99).”\nThe Amazon description for The Mythology of Wine says: “In ancient times, wine, vineyards, and grapevines were thought to have supernatural qualities, enabling people to experience the divine. Naturally, wine, vines, and vineyards featured prominently in myths. This trailblazing book details the wine-related myths and legends in ancient Mesopotamia, Greece, Israel, Egypt, and early Christian Europe, showing how they have influenced our own wine culture, and filling an important gap in our knowledge about wine.”\nAs Arthur and I are Facebook friends, I reached out to him to learn more about his book, and he directed me to this quote: “Of all our beverages today, only wine has sacramental status,” he writes on page 1.\n“This is a legacy of wine’s sacred status in the ancient world,” shared Arthur with me. “To the ancient mind, wine was divine and transcendent, and thus for humans was a means of transcendence, a connection to the divine.”\nAbout Jesus, Arthur says, “He called himself ‘the true vine,’ he was the best winemaker who made superior wine, his blood was wine, and in Christian art the cross was portrayed both as a grapevine and a wine press, making his crucifixion a grape crush, after which he was enthroned in heaven, which was pictured as a vineyard.”\n“This explains the front cover of my book,” he says, “it alludes to the various biblical and artistic connections between Jesus Christ and wine and grapes.”\nIn response to my question about why he wrote the book, Arthur says, “First, it combines two major interests of mine. I’m a mythologist, and write books and blog about mythology. But I’m also a longtime wine enthusiast who has built up a good knowledge about it over the years, and more recently became a viticulturalist and winemaker (now on my 5th vintage), which really deepened by knowledge of the subject and allows me to appreciate and evaluate wines better. That in turn led me further into wine mythology and history.\n“Second, being a cultural historian, I also saw that most wine enthusiasts, as well as certified wine professionals, don’t know much about how wine was the subject of so many myths in the cultures of the ancient world, and how they in turn ultimately influenced today’s wine culture and culture in general. There is a gap in wine education as currently practiced; it just doesn’t address the subject. I wrote the book in part to fill that gap.\n“And third, for most people this is a novel subject, and is both interesting and fun to talk about. The new things that the book talks about can make for good conversations at dinner and at cocktail parties. For this purpose, I wanted to make the book not too long or technical, and easy to read. It has to be the kind of book that can be sold at wine tasting rooms, so visitors can take that home with them along with the wines.”\nArthur’s been having fun promoting the book, showing up in podcasts, Zooms (see below), and doing guest columns like this one on the Academic Wino; you can read her review of his book here.\nHonestly, this the kind of book I’d like to have written. Like Arthur, I have an interest in Depth Psychology; I even have a Masters in Depth Psychology from Pacifica Graduate Institute, and I wrote a paper about the phenomenology of wine tasting. I look forward to getting and reading this book and interviewing him myself — hopefully over a glass or two of wine, possibly at Clos Des Amis or at his home vineyard.\nRead more about the topic of the mythology of wine on his blog.\nFinally, a myth of a different “flavour” if you will.\nA Life in Wine is the autobiography of Stephen Spurrier, the man who created what became known as the Judgement of Paris in 1976, and who is portrayed by Alan Rickman in the 2009 film BottleShock.\nSpurrrier is quoted by wikipedia as saying, “Having read the script, Spurrier stated, “There is hardly a word that is true in the script and many, many pure inventions as far as I am concerned.””\nWith the re-release of Stephen Spurrier’s autobiography A Life In Wine in 2020, and with COVID closures, Spurrier has been on the ZOOM circuit with two recent tastings and interviews, one featuring English sparkling wine with San Francisco Wine School and the second with Burgundy with Napa Valley Wine Academy — and you can order his book this way too. During the Burgundy one which I listened in on, he talked about the film and meeting up with Alan Rickman after he portrayed Spurrier. The two had met previously and enjoyed wine together in Tuscany, and Rickman said he worked to bring Spurrier’s humanity to the character that was on the page.\nIn A Life in Wine by Steven Spurrier shows why he’s one of the wine trade’s most important figures having taken the role of wine merchant, buyer, wine educator and lecturer; he has written books, wine courses and over 300 columns for Decanter magazine. This updated and revamped edition includes two new chapters. If you’re just curious about the Judgement of Paris, check out George Tabor’s 2006 book.\nLooking for more good books about wine to read?\nCheck out New York Times columnist Eric Asimov’s “Six Books for Thinking, Drinking and Changing the World” his list of a few of the best wine and spirits books of 2020:', ""Course Series Information\nThe Wine Business Management Certificate is made up of three independent levels. Each level builds upon the next and is designed to challenge participants while creating a superior understanding of wine business operations.\n(16 hours over 4 weeks)\nFoundation: Introduction to Wine Business\nThis four week course provides an overview of wine business. Students will develop an appreciation of the realities of the wine business as a business and come to understand the steps required for getting from the vineyard and into the glass of the consumer.\n- Understand the components of a wine business and the viewpoints of the various stakeholders in the areas of:\n- Grape growing, including its costs, values, quality considerations, supply and demand, risks and opportunities\n- Production, winemaking, maturation and packaging\n- Distribution, a complex and highly regulated process\n- Domestic sales, selling wine in retail stores (off-premise), restaurants and bars (on premise), online and direct to consumer (through tasting rooms, events and wine clubs)\n(24 hours over 8 weeks)\nPrerequisites: Successful completion of Level 1\nIntermediate: A Survey of the Key Drivers in a Wine Business\nBuilding on the concepts developed in the Introduction to Wine Business, this eight week course expands the project-based learning focused on the business of making and selling wine. The class surveys the key drivers of a wine business and the necessary decisions when a wine business goes to market, providing a deeper understanding of the entire value chain.\nThe subject of marketing, distributing and selling expands to include both a primary brand as well as a range of products, including options for negociant brands.\n- Understand the business issues associated with viticulture, production, marketing, distribution and points of sale.\n- Evaluate alternative approaches associated with those issues.\n- Calculate costs across the component areas.\n- Evaluate the contribution of the component areas to the value of a wine product.\n- Expand the understanding of distribution and regulatory issues to include the international wine market.\n- Assess the internal environment, identifying the strengths and weaknesses in a wine business.\n- Assess the external environment, identifying the opportunities and threats to a wine business.\n- Evaluate a SWOT analysis and draft a plan for going to market with a new wine business product.\n- Plan for and consider future wine markets and alternative channels of wine commerce.\n(48 hours over 16 weeks, plus optional certification exam)\nPrerequisites: Successful completion of Levels 1 and 2\nParticipants will choose two of the following three electives:\nGlobal Wine E-commerce (24 hours over 8 weeks)\nThis level provides an in-depth study of electronic commerce aspects from a wine industry perspective. Participants will explore opportunities and challenges associated with electronic commerce (e-commerce/e-business), and review the impact of e-commerce with meeting strategic objectives of an organization in the wine industry. This level includes an overview of e-business issues as they pertain to direct-to-consumer and government oversight/compliance, wine club management, and winery management software.\nAt the conclusion of this level, participants will be able to:\n- Explain how electronic commerce affects the strategic intent and operational functioning of a wine industry organization\n- Describe the procedures for developing and managing an electronic commerce site, as it furthers the strategic, marketing, financial, or operational plans of an organization\n- Make recommendations in design of web-site that implement best practices in electronic commerce\n- Identify and explain the various security, legal, and privacy issues and understand how they may impact a wine industry organization's e-business strategy\n- Evaluate feasibility of and recognize benefits and limitations of various winery management software systems as they relate to direct-to-consumer issues\n- Effectively communicate with customers through the available digital channels\n- Evaluate and optimize the customers' mobile experience\nProduction and Quality in the Wine Supply Chain (24 hours over 8 weeks)\nThis level provides an in-depth review of the business issues associated with wine production and operations. This level emphasizes the basic concepts of operations, purchasing, logistics, and supply chain management as they apply to the wine industry. More specific topics include value analysis, total quality management, make/buy decisions, negotiation, and supplier development.\nAt the conclusion of this level, the participant will be able to:\n- Explain the key business issues in the production and operation of a winery\n- Identify supply chain management considerations for effective operations\n- Describe the planning process from wine forecasting and product development through distribution\n- Calculate total cost analysis and its role in supply procurement\n- Assess the appropriateness of the capital equipment necessary to operate a winery\n- Identify how quality is achieved in the production of wine\nThis level provides an introduction to wine marketing terminology and concepts, including the steps in brand creation, packaging decisions, integrated communication strategy, navigating the three tier distribution channels, and direct to consumer sales. Topics of tasting room management, wine tourism, importing, and exporting wine are also presented. The culminating project is the creation of a marketing and sales plan for the launch of a new wine business.\nAt the conclusion of this level participants will be able to:\n- Describe important wine consumer segments\n- Create a wine brand to appeal to a chosen target market\n- Understand the marketing implications of package design\n- Coordinate promotional elements of advertising, public relations, and special events\n- Understand the complexities of wine distribution at the distributor and retail levels\n- Create a distribution and sales strategy utilizing distributors and/or direct to consumer methods\n- Describe key aspects of tasting room management\n- Integrate wine marketing into a broader wine tourism context\n- Describe the steps to importing and exporting wine\n- Create a draft marketing and sales plan\nFor more information or to register contact:\nWine Business Institute""]"	['<urn:uuid:3a82dafb-9c77-4c0e-b41d-bad7682f4fb3>', '<urn:uuid:b972c006-40f1-46b1-8b7f-a88de489448f>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:10:11.825225	9	45	3169
93	What explains higher divorce rates among couples who cohabitate?	There are three main theories: 1) Selection - certain factors predispose some people to both cohabitation and divorce, 2) The experience of cohabiting changes attitudes about marriage and divorce, lowering esteem for marriage, 3) Inertia - cohabitation makes it harder to break up before fully committing to marriage.	['Living Together Before Marriage May Raise Risk of Divorce\nIs living together before marriage associated with risk in marriage or not?\nPosted Nov 03, 2018\nby Scott M. Stanley and Galena K. Rhoades\nOne of the most perplexing findings in the history of the study of marriage is that living together beforehand is associated with greater, not lesser, odds of struggling in marriage. This association had been clear in study after study up to around 2007, and then a number of studies and social scientists declared that the association between living together before marriage and difficulties in marriage had disappeared.\nTo be clear, we never thought the association had disappeared. While we believe the association got weaker, we have long believed that the understanding of it mostly became better understood. For example, we predicted and found — repeatedly, and in numerous samples — that an important part of the story had to do with whether or not a couple started living together before or after having come to a clear commitment to marry. Those who cohabited only after engagement (or marriage, of course) have an edge in odds for doing well in marriage compared to those who started living together before clarifying the big question about the future. We will explain that more below.\nAmong some scholars in family science, the belief had been settled that there simply is no risk for worse outcomes in marriage associated with living together beforehand. Well, that no-risk story just received a jolt. A new study published in the Journal of Marriage and Family finds that the “premarital cohabitation effect” lives on, despite earlier claims to the contrary. The premarital cohabitation effect is what this is all about: the finding (over many studies, over decades of time) that those who live together prior to marriage are more likely, not less, to struggle in marriage. Here, we are going to take you through a fairly deep dive into what the hubbub is all about.\nThe new study is by Michael Rosenfeld and Katharina Roesler. Their findings suggest that there remains an increased risk for divorce for those living together prior to marriage and that prior studies suggesting the effect went away had a bias toward analyzing shorter versus longer-term effects. They found that living together before marriage was associated with lower odds of divorce in the first year of marriage, but increased odds of divorce in all other years tested, and this finding held across decades of data.\nNumerous Recent Studies Reported No Impact of Premarital Cohabitation\nA number of relatively recent studies suggested that the premarital cohabitation effect had gone away for those marrying in the past 10 or 15 years. Rosenfeld and Roesler pay particular attention to a report from the National Center for Health Statistics (NCHS) by Copen, Daniels, Vespa, and Mosher in 2012, which suggested there was no increased risk associated with premarital cohabitation in the most recent (at the time) cohort of the National Survey of Family Growth (NSFG; 2006-2010). The NSFG is a large, ongoing sampling of those living in the U.S. with regard to many aspects of family formation, cohabitation, and marriage. Other researchers, such as Manning and Cohen, reached the same conclusion as the authors of the NCHS in 2012, incorporating data from as late as the 2006 to 2008 cohort of the NSFG.[i]\nWhile all of these studies used the same large sampling effort, Rosenfeld and Roesler had longer-term data for the most recent cohort they studied (up to 2015). Contrary to these prior conclusions, they found that there remains a clear link between premarital cohabitation and increased odds of divorce regardless of the year or cohort studied. (In all these studies, the focus is on first marriages.)\nMany experts believed that the cohabitation effect, well understood since the 1980s, would go away as living together became more the thing to do — that it would no longer be associated with negative outcomes in marriage. This hypothesis is based on the idea that the prior stigma among friends and family about living together before marriage was a cause of the increased risk. And, clearly, there is not much, if any, stigma now. (We wrote about trends in cohabitation in the U.S. in great detail here earlier this year.) The other reason for expecting the cohabitation effect to go away is a little more arcane, having to do with the fact that as it became common (at least 70 percent of couples live together before marriage now), those who chose to do so are no longer more select for being at greater risk than others.\nBased on a different line of reasoning than the other studies suggesting no risk, another prominent study had also concluded that there was no longer an added risk for divorce associated with premarital cohabitation. However, in that study, Kuperberg (2014) concluded the risk was more about moving in together at a young age (before the middle 20s) than moving in together before marriage, per se. That’s one among many potentially important nuances in this complex literature.[ii]\nPumping the Brakes on the Conclusions of No Added Risk\nCohabitation is the gift that keeps on giving to family science, providing generations of scholars with the opportunity to say, “Look here, wow, this is strange.” And people remain interested. We have seen major stories on cohabitation break across the media for over 20 years because it’s interesting. In fact, here is a recent story in The Atlantic about this very thing, and it includes numerous quotes from the second author here, Galena Rhoades, based on our work. We think this subject keeps getting lots of ink because the findings have tended to be so counterintuitive. Most people believe that living together before marriage should improve the odds of doing well in marriage. However, whatever else is true, there is very scant evidence to support this belief in a positive effect (more on that can be found in this piece).\nBy the way, we’re not trying to change your views of whether or not a couple should live together before marriage. Our own views are complex and nuanced. However, we may convince you that there is something to learn from the research that bears on what pathways may be wiser than others for the average person. So, let’s get back to the new study that is getting a lot of buzz.\nRosenfeld and Roesler’s study is quite complex statistically, but their insight boils down to two things easily explained. First, they believe studies that suggested that the premarital cohabitation effect has disappeared simply did not have outcomes for divorce far enough out for those who had married in the recent cohorts that they examined. Second, they show that premarital cohabitation is associated with a lower risk for divorce, but only very early in marriage (in the first year); in contrast, the finding flips, with premarital cohabitation being associated with higher risks for divorce in years after that first year. That’s what earlier studies could not address.\nIn particular, Rosenfeld and Roesler suggest that those who live together before marriage have an advantage in the first year because they are already used to all the changes that come with living together. Those who go straight into marriage without living together have a bigger immediate shock to negotiate after marriage, and as a result, have a short-term increased risk that’s greater than those already living together. But that’s the short term, and the risk remains long term.\nHere is a quote from the new paper (Figure 2 below is included here with permission):\nFigure 2 shows that, for the years in which the NSFG has substantial numbers of marriages and breakups, there was no apparent trend over time in the raw or adjusted odds ratios of breakup for premarital cohabitation. Given the enormous changes over time in the prevalence of premarital cohabitation (see Figure 1), Figure 2 shows a surprising stability in the association between premarital cohabitation and marital dissolution over time. (Pgs 7-8)\nTheories of Increased Risk\nThere are three main theories for how living together before marriage could be causally associated with worse outcomes (on average) in marriage. Rosenfeld and Roesler address the first two but did not say anything about the third.[iii]\n1. Selection — This theory is simply that there are many factors associated with who cohabits when and why, and with whom, and that those factors are also associated with how marriages will turn out regardless of cohabiting experience. For example, it’s well known that those who are more economically disadvantaged are more likely: to live together outside of marriage, to live together with more than one partner, to have a child with a cohabiting partner prior to marrying, and to struggle in marriage. Other factors are religiousness, traditionality, and family history (parental divorce, etc.). The selection explanation is that those who cohabit in riskier ways (e.g., before marriage, before engagement, with more than one partner) were already at greater risk. In the strongest view of selection, living together does not add to the risk at all, because it’s all already baked in. There is a lot of evidence for selection playing an important role in this literature, and scholars in this area note this and address it in various ways.\n2. The Experience of Cohabiting Changes Things — In an older line of research that was clever, but needs to be tested again with those marrying in more recent years, Axinn and Barber (1997) showed that cohabiting changes attitudes about marriage and divorce, lowering esteem for marriage and increasing acceptance of divorce. This is consistent with scores of studies in psychology showing that attitudes will cohere to behavior. In other words, you will bring your beliefs around to fit your behavior. Earlier, Thornton, Axinn, and Hill (1992) showed that cohabiting led to people becoming less religious. Rosenfeld and Roesler included a lot on the theory of experience but mostly use it to emphasize the short-term benefit of already experiencing living together when transitioning into marriage.\n3. Inertia — We have argued since the early 2000s for another causal theory in this line of research. Drawing on theories of commitment, we suggested that what nearly everyone misses in understanding the risk associated with cohabitation is pretty simple: Moving in together makes it harder to break up, net of everything else. The added risk is due to how cohabitation substantially increases constraints to remain together prior to a dedication to a future together maturing between two partners. Two key papers on this perspective are here and here.[iv]\nOne primary prediction from the inertia hypothesis is that those who only started living together after being already committed to marriage (e.g., by engagement or actual marriage) should, on average, do better in marriage than those who may have prematurely made it harder to break up by living together before agreeing on marriage. The inertia hypothesis completely embraces selection, suggesting that relationships already at greater risk become harder to exit because of cohabitation. Various predictions from the inertia hypothesis have been supported in ten or more studies, seven of which include tests of the prediction about pre-cohabitation level of commitment to marriage (aka plans for marriage prior to living together) — and this latter finding exists in at least six different samples across a range of outcomes.[v]\nThere is no particular reason to expect that the inertia risk will dissipate with increased acceptance of cohabitation, because the mechanism is about the timing of the development of aspects of commitment, not about societal views and personal attitudes. For living together to lower risk in marriage, the benefit of learning something disqualifying about a partner has to exceed the costs of making it harder to break up that come with sharing a single address. Hence, inertia is another possibility, along with experience, that could explain the persistence of a cohabitation effect, such as that found by Rosenfeld and Roesler.[vi]\nOther Possibilities — Other factors that may be associated with differential outcomes include pacing (Sassler et al.), age at the time of moving in together (Kuperberg), and having children before marriage (Tach & Halpern-Meekin). All such theories suggest that the risks of living together before marriage are greater for some people than others. Rosenfeld and Roesler are not really addressing this issue. However, they did find that the risks associated with premarital cohabitation were lower for African Americans. While that’s a subject far beyond our focus here, it does not surprise us. For most groups, cohabitation is no particular indicator of higher commitment. However, it may well signal higher levels of commitment among groups where marriage has declined a great deal, like African Americans.\nRosenfeld and Roesler also note that the risks of living together before marriage were even greater among those who had lived with more than just their mate prior to marriage. That finding is consistent with many other studies: for example, Teachman (2003).\nThe Effect and the Controversy Lives\nResearch on premarital cohabitation has long been mired in arguments about causality, with the dominant view being that what we describe about selection explains most, if not all, of the added risk when found. However, many studies in the history of this field have controlled for all sorts of variables associated with selection and still found an additional risk. In fairness, it is not possible to control for all aspects of selection in such studies. Without randomly assigning people to walk different pathways before marriage, causality can never be proven. And we think it’s going to be a long time before researchers are allowed to flip a coin about who does what in their love lives so that we can study the phenomena better. Oh, how great that would be, though, for figuring things out. Since we have to live with studying what people end up doing on their own, arguments ensue. Besides, since when does evidence stop arguing anyway when people are passionate about their view on something?\nRosenfeld and Roesler’s new study breathed life into a finding many concluded was dead. Perhaps it was just mostly dead.\nScott M. Stanley and Galena K. Rhoades are research professors in the department of psychology at the University of Denver. A different version of this article appeared on the blog for the Institute of Family Studies on October 17, 2018.\n[i] We are mystified why the new paper does not cite or address the findings by Manning and Cohen. That study seems like it is the most recent major study directly addressing the question Rosenfeld and Roesler examine.\n[ii] Scott wrote about the Kuperberg study at that time, taking far more issue with the media stories about it than the actual study, suggesting there are many ways people could misconstrue to whom those, and other findings of differential risk, applied. Those articles are here and here.\n[iii] This omission does not seem as striking to us as the omission of Manning and Cohen’s paper, since their paper is already complex and they are intent on addressing one moderator of the cohabitation effect: how long after marriage the effect is measured. They do not address at all the growing literature on moderators of the cohabitation effect. Still, inertia is one of the major theories of increased risk, and only selection itself has more publications addressing it.\n[iv] An accessible, word document version of the major theory paper can be found here. A full run down of our theoretical and empirical work in this line is available here. That includes citations and links, mostly to accessible versions of the articles in the literature.\n[v] We have found evidence for inertia whether or not someone has cohabited only with their mate, and in numerous samples of people marrying after 2000 and later.\n[vi] As an interesting side point on the subject of the inertia hypothesis, the commitment to marriage/timing effect exists in the NSFG. It was mentioned in passing in a working paper leading up to the 2010 publication by Reinhold, and it is mentioned prominently in the abstract (and paper) in Manning and Cohen’s 2012 publication.']	['<urn:uuid:3c8c01ef-e512-45c5-b048-5f176511ced3>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:10:11.825225	9	48	2685
94	Were Buddha and Christ contemporaries?	No, they were not contemporaries. Buddha lived in the first millennium BC (born in 624 BC), while Jesus Christ came later, as indicated by the documents which note that Christ did not arrive simultaneously with the magisterial figures of the mid-first millennium BC.	['Reflecting on human history over the previous millennia, a few European thinkers in the eighteenth and nineteenth centuries noticed a surprising conjunction. Many of the world’s most influential figures—Confucius, Buddha, the prophets of Israel (Amos, Isaiah, Jeremiah), Socrates, Plato, Aristotle, and Zoroaster all emerged in their respective nations—China, India, Judaea, Greece, and Iran—in the middle of the first millennium BC, roughly between 800 and 200 BC. Although more recent scholarship has tended to move Zoroaster out of this chronological frame back into an earlier one, the coincidence remains impressive. The French Iranist Abraham Hyacinthe Anquetil-Duperron in the late eighteenth century may have been the first to draw attention to it, and a German philosopher, Ernst von Lasaulx, subsequently expanded on it in a dense but little-known work of 1856 under the bizarre title A New Attempt at an Old Philosophy of History Based on the Truth of Facts. While stressing empirical analysis, von Lasaulx conspicuously privileged religion and argued for organic growth and decay in world history.\nIn Europe at this time the great turning point in the history of mankind was generally agreed to be the earthly appearance of Jesus Christ, who quite clearly did not arrive simultaneously with the magisterial figures of the mid-first millennium BC. Hegel went so far as to assert that the idea of the trinity of God was the pivot on which the history of the whole world turns—both its starting point and its goal. The centrality of Christ in history, or at least Western history, impelled the classical historian Johann Gustav Droysen to compose a very long narrative, beginning with Alexander the Great, that would trace Greek and Near Eastern events that led inexorably, as he believed, to Christianity. His work on what he called Hellenismus, which today we often call Hellenistic culture, gave a new momentum to studies of early Christianity, but he was oblivious of the intriguing coincidence of Confucius, Socrates, Buddha, and their expositors.\nMeanwhile, in a world conceptually if not physically apart, Muslims had long before inaugurated their own historical time with the migration (hijra) of Muhammad to Medina in 622, and that date remains to this day the great turning point for Islamic historiography. None of the historians in China, India, or Iran seems ever to have noticed that the careers of their ancient leaders and thinkers occurred in the days of Socrates, Plato, Aristotle, or the Hebrew prophets. Drawing attention to this chronological oddity was an entirely Western enterprise that may have gained a certain traction from German Romanticism by invoking exotic peoples and places. But even in the West it had limited currency because the beginning of Christianity remained the immovable turning point in human history.\nThis article is available to subscribers only.\nPlease choose from one of the options below to access this article:\nPurchase a print subscription (20 issues per year) and also receive online access to all articles published within the last five years.\nPurchase an Online Edition subscription and receive full access to all articles published by the Review since 1963.\nPurchase a trial Online Edition subscription and receive unlimited access for one week to all the content on nybooks.com.', '(Last Updated on : 30/10/2010)\nGautama Buddha was born as a Shakya prince. He is also known as Shakya Muni and his birth, teachings, death and the rules for the Sangha are to be found in the Buddhist canons. Siddhartha was born in Lumbini, now in Nepal, in 624 BC. Buddha was born to King Suddhodana and Queen Maya Devi.\nEarly Life of Gautama Buddha\nBuddha was born on a full moon day in the month of May. He was named Tathagata or Siddhartha. Gautama was his family name. Within a few days after his birth Siddhartha`s mother died. He was then looked after by Maha Prajapati Gautami, his maternal aunt and his stepmother. Buddha was a Kshatriya\nby birth and had military training in his upbringing. At the age of 16 he was married to Yasodhara. He spent 29 years as the prince of Kapilavasthu and led a happy married life. He was blessed with a son named Rahul.\nThe Great Departure of Gautama Buddha\nNo matter how hard King Suddhodana tried to shield his son from the knowledge of human pain and suffering Siddhartha came face to face with real world. Sometimes Prince Siddhartha would go into the capital city of his father`s kingdom to see how the people lived. During these visits he came into contact with many old people and sick people, and on one occasion he saw a corpse. One was suffering from old age, a sick man, a dead body and an ascetic. He was so disturbed by these sights he decided to renounce the material world and look for truth. He wanted to find a way that would put an end to such pain and suffering. With this aim he fled from his palace to become the Buddha.\nGreat Enlightenment of Buddha\nSiddhartha then made his way to a place near Bodh Gaya\nin India, where he found a suitable site for meditation. There he focused single-pointedly on the ultimate nature of all phenomena. After training in this meditation for six years he realized that he was very close to attaining full enlightenment. So he walked to Bodh Gaya on the full moon day of the fourth month of the lunar calendar, he seated himself beneath the Bodhi Tree\nin the meditation posture and vowed not to rise from meditation until he had attained perfect enlightenment. He attained enlightenment after passing through the phases of asceticism and meditation under the Bodhi tree. This place is now a famous Buddhist Pilgrims Centre.\nTeachings of Buddha\nBuddha`s teachings contour the base of the religion whilst making it to stand out as an individual doctrine, a philosophy that is far more than just a religious concept. Buddha did not deny that there is happiness in life, but he pointed out that happiness does not last forever. Buddha`s teachings are a step-by-step way towards lasting happiness. Happiness never decreases by being shared. This concept of sharing his teachings and passing the pious concept of Buddhism\nfrom one to another further supported his disciples in spreading Buddhism. Amidst the concept of four noble truths\nand in the midst of the idea of the noble eightfold paths, Buddha preached the simplest way to attain Nirvana\n. The simplicity of the religious principles of Buddha won a number of hearts.\nBuddha formed the Sangha\nwith his five companions. He was at the Deer Park, Sarnath\nto deliver the first sermon of his life. With the formation of the Sangha or the order of the monk the concept of Triple Gems also came into existence. The Sangha too carried on with the spread of Dharma\n. The disciples of Buddha therefore included both the monasteries as well as the lay followers who later spread Buddhism as an individual religious concept.\nMahaparinibbana Sutta proclaims that Buddha passed away at the age of 80. He ate his last meal at Cunda`s dwelling place and fell seriously ill. However Buddha announced that the meal had nothing to do with his sickness. He was bound to leave the world at that time. Soon he attained Parinirvana']	['<urn:uuid:7f7cb9a3-bea9-484d-8d90-edcefe36376a>', '<urn:uuid:3ff79c88-c32d-4c7e-8b5f-f9fbeb8dec19>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T19:10:11.825225	5	43	1209
95	agile methodologies versus domain modeling differences	Domain modeling and Agile methodologies have distinct focuses in software development. Domain modeling emphasizes expressing rich functionality through domain models, using a systematic approach to capture complex domains, and maintaining model integrity through careful design practices and principles. In contrast, Agile methodologies, particularly as seen in rapid application development approaches, minimize feature creep through short intervals and mini-increments, but may produce limited documentation and face challenges with large multi-team distributed systems. While domain modeling seeks deeper insight into domains through careful modeling and refinement, Agile methods prioritize quick delivery and adaptation to changing requirements.	"[""Domain-Driven Design: Tackling Complexity in the Heart of Software, 1st edition\nDomain-Driven Design: Tackling Complexity in the Heart of Software\nYou'll get a bound printed text.\n“Eric Evans has written a fantastic book on how you can make the design of your software match your mental model of the problem domain you are addressing.\n“His book is very compatible with XP. It is not about drawing pictures of a domain; it is about how you think of it, the language you use to talk about it, and how you organize your software to reflect your improving understanding of it. Eric thinks that learning about your problem domain is as likely to happen at the end of your project as at the beginning, and so refactoring is a big part of his technique.\n“The book is a fun read. Eric has lots of interesting stories, and he has a way with words. I see this book as essential reading for software developers—it is a future classic.”\n—Ralph Johnson, author of Design Patterns\n“If you don’t think you are getting value from your investment in object-oriented programming, this book will tell you what you’ve forgotten to do.\n“Eric Evans convincingly argues for the importance of domain modeling as the central focus of development and provides a solid framework and set of techniques for accomplishing it. This is timeless wisdom, and will hold up long after the methodologies du jour have gone out of fashion.”\n—Dave Collins, author of Designing Object-Oriented User Interfaces\n“Eric weaves real-world experience modeling—and building—business applications into a practical, useful book. Written from the perspective of a trusted practitioner, Eric’s descriptions of ubiquitous language, the benefits of sharing models with users, object life-cycle management, logical and physical application structuring, and the process and results of deep refactoring are major contributions to our field.”\n—Luke Hohmann, author of Beyond Software Architecture\n“This book belongs on the shelf of every thoughtful software developer.”\n“What Eric has managed to capture is a part of the design process that experienced object designers have always used, but that we have been singularly unsuccessful as a group in conveying to the rest of the industry. We've given away bits and pieces of this knowledge...but we've never organized and systematized the principles of building domain logic. This book is important.”\n—Kyle Brown, author of Enterprise Java™ Programming with IBM® WebSphere®\nThe software development community widely acknowledges that domain modeling is central to software design. Through domain models, software developers are able to express rich functionality and translate it into a software implementation that truly serves the needs of its users. But despite its obvious importance, there are few practical resources that explain how to incorporate effective domain modeling into the software development process.\nDomain-Driven Design fills that need. This is not a book about specific technologies. It offers readers a systematic approach to domain-driven design, presenting an extensive set of design best practices, experience-based techniques, and fundamental principles that facilitate the development of software projects facing complex domains. Intertwining design and development practice, this book incorporates numerous examples based on actual projects to illustrate the application of domain-driven design to real-world software development.\nReaders learn how to use a domain model to make a complex development effort more focused and dynamic. A core of best practices and standard patterns provides a common language for the development team. A shift in emphasis—refactoring not just the code but the model underlying the code—in combination with the frequent iterations of Agile development leads to deeper insight into domains and enhanced communication between domain expert and programmer. Domain-Driven Design then builds on this foundation, and addresses modeling and design for complex systems and larger organizations.Specific topics covered include:\n- Getting all team members to speak the same language\n- Connecting model and implementation more deeply\n- Sharpening key distinctions in a model\n- Managing the lifecycle of a domain object\n- Writing domain code that is safe to combine in elaborate ways\n- Making complex code obvious and predictable\n- Formulating a domain vision statement\n- Distilling the core of a complex domain\n- Digging out implicit concepts needed in the model\n- Applying analysis patterns\n- Relating design patterns to the model\n- Maintaining model integrity in a large system\n- Dealing with coexisting models on the same project\n- Organizing systems with large-scale structures\n- Recognizing and responding to modeling breakthroughs\nWith this book in hand, object-oriented developers, system analysts, and designers will have the guidance they need to organize and focus their work, create rich and useful domain models, and leverage those models into quality, long-lasting software implementations.\nTable of contents\nI. PUTTING THE DOMAIN MODEL TO WORK.\nII. THE BUILDING BLOCKS OF A MODEL-DRIVEN DESIGN.\nIII. REFACTORING TOWARD DEEPER INSIGHT.\nIV. STRATEGIC DESIGN.\nPublished by Addison-Wesley Professional (August 20th 2003) - Copyright © 2004"", 'Introduction to Software Engineering/Process/Rapid Application Development\nRapid application development (RAD) refers to a type of software development methodology that uses minimal planning in favor of rapid prototyping. The ""planning"" of software developed using RAD is interleaved with writing the software itself. The lack of extensive pre-planning generally allows software to be written much faster, and makes it easier to change requirements.\nRapid application development is a software development methodology that involves methods like iterative development and software prototyping. According to Whitten (2004), it is a merger of various structured techniques, especially data-driven Information Engineering, with prototyping techniques to accelerate software systems development.\nIn rapid application development, structured techniques and prototyping are especially used to define users\' requirements and to design the final system. The development process starts with the development of preliminary data models and business process models using structured techniques. In the next stage, requirements are verified using prototyping, eventually to refine the data and process models. These stages are repeated iteratively; further development results in ""a combined business requirements and technical design statement to be used for constructing new systems"".\nRAD approaches may entail compromises in functionality and performance in exchange for enabling faster development and facilitating application maintenance.\nRapid application development is a term originally used to describe a software development process introduced by James Martin in 1981. Martin\'s methodology involves iterative development and the construction of prototypes. More recently, the term and its acronym have come to be used in a broader, general sense that encompasses a variety of methods aimed at speeding application development, such as the use of software frameworks of varied types, such as web application frameworks.\nRapid application development was a response to non-agile processes developed in the 1970s and 1980s, such as the Structured Systems Analysis and Design Method and other Waterfall models. One problem with previous methodologies was that applications took so long to build that requirements had changed before the system was complete, resulting in inadequate or even unusable systems. Another problem was the assumption that a methodical requirements analysis phase alone would identify all the critical requirements. Ample evidence attests to the fact that this is seldom the case, even for projects with highly experienced professionals at all levels.\nStarting with the ideas of Brian Gallagher, Alex Balchin, Barry Boehm and Scott Shultz, James Martin developed the rapid application development approach during the 1980s at IBM and finally formalized it by publishing a book in 1991, Rapid Application Development.\nThe shift from traditional session-based client/server development to open sessionless and collaborative development like Web 2.0 has increased the need for faster iterations through the phases of the SDLC. This, coupled with the growing use of open source frameworks and products in core commercial development, has, for many developers, rekindled interest in finding a silver bullet RAD methodology.\nAlthough most RAD methodologies foster software re-use, small team structure and distributed system development, most RAD practitioners recognize that, ultimately, no one “rapid” methodology can provide an order of magnitude improvement over any other development methodology.\nAll types of RAD have the potential for providing a good framework for faster product development with improved software quality, but successful implementation and benefits often hinge on project type, schedule, software release cycle and corporate culture. It may also be of interest that some of the largest software vendors such as Microsoft and IBM do not extensively use RAD in the development of their flagship products and for the most part, they still primarily rely on traditional waterfall methodologies with some degree of spiraling.\nThis table contains a high-level summary of some of the major types of RAD and their relative strengths and weaknesses.\n|Agile software development (Agile)|\n|Pros||Minimizes feature creep by developing in short intervals resulting in miniature software projects and releasing the product in mini-increments.|\n|Cons||Short iteration may add too little functionality, leading to significant delays in final iterations. Since Agile emphasizes real-time communication (preferably face-to-face), using it is problematic for large multi-team distributed system development. Agile methods produce very little written documentation and require a significant amount of post-project documentation.|\n|Extreme Programming (XP)|\n|Pros||Lowers the cost of changes through quick spirals of new requirements. Most design activity occurs incrementally and on the fly.|\n|Cons||Programmers must work in pairs, which is difficult for some people. No up-front “detailed design” occurs, which can result in more redesign effort in the long term. The business champion attached to the project full time can potentially become a single point of failure for the project and a major source of stress for a team.|\n|Joint application design (JAD)|\n|Pros||Captures the voice of the customer by involving them in the design and development of the application through a series of collaborative workshops called JAD sessions.|\n|Cons||The client may create an unrealistic product vision and request extensive gold-plating, leading a team to over- or under-develop functionality.|\n|Lean software development (LD)|\nCreates minimalist solutions (i.e., needs determine technology) and delivers less functionality earlier; per the policy that 80% today is better than 100% tomorrow.\n|Cons||Product may lose its competitive edge because of insufficient core functionality and may exhibit poor overall quality.|\n|Rapid application development (RAD)|\n|Pros||Promotes strong collaborative atmosphere and dynamic gathering of requirements. Business owner actively participates in prototyping, writing test cases and performing unit testing.|\n|Cons||Dependence on strong cohesive teams and individual commitment to the project. Decision making relies on the feature functionality team and a communal decision-making process with lesser degree of centralized PM and engineering authority.|\n|Pros||Improved productivity in teams previously paralyzed by heavy “process”, ability to prioritize work, use of backlog for completing items in a series of short iterations or sprints, daily measured progress and communications.|\n|Cons||Reliance on facilitation by a master who may lack the political skills to remove impediments and deliver the sprint goal. Due to relying on self-organizing teams and rejecting traditional centralized ""process control"", internal power struggles can paralyze a team.|\nSince rapid application development is an iterative and incremental process, it can lead to a succession of prototypes that never culminate in a satisfactory production application. Such failures may be avoided if the application development tools are robust, flexible, and put to proper use. This is addressed in methods such as the 2080 Development method or other post-agile variants.\nWhen organizations adopt rapid development methodologies, care must be taken to avoid role and responsibility confusion and communication breakdown within a development team, and between team and client. In addition, especially in cases where the client is absent or not able to participate with authority in the development process, the system analyst should be endowed with this authority on behalf of the client to ensure appropriate prioritisation of non-functional requirements. Furthermore, no increment of the system should be developed without a thorough and formally documented design phase.\n- Whitten, Jeffrey L.; Lonnie D. Bentley, Kevin C. Dittman. (2004). Systems Analysis and Design Methods. 6th edition. ISBN 025619906X.\n- Maurer and S. Martel. (2002). ""Extreme Programming: Rapid Development for Web-Based Applications"". IEEE Internet Computing, 6(1) pp 86-91 January/February 2002.\n- Andrew Begel, Nachiappan Nagappan. ""Usage and Perceptions of Agile Software Development in an Industrial Context: An Exploratory Study, Microsoft Research"". http://research.microsoft.com/pubs/56015/AgileDevatMS-ESEM07.pdf. Retrieved 2008-11-15.\n- E. M. Maximilien and L. Williams. (2003). ""Assessing Test-driven Development at IBM"". Proceedings of International Conference of Software Engineering, Portland, OR, pp. 564-569, 2003.\n- M. Stephens, Rosenberg, D. (2003). ""Extreme Programming Refactored: The Case Against XP"". Apress, 2003.\n- Gerber, Aurona; Van der Merwe, Alta; Alberts, Ronell; (2007), Implications of Rapid Development Methodologies, CSITEd 2007, Mauritius, November 2007 \n- Steve McConnell (1996). Rapid Development: Taming Wild Software Schedules, Microsoft Press Books, ISBN 978-1556159008\n- Kerr, James M.; Hunter, Richard (1993). Inside RAD: How to Build a Fully-Functional System in 90 Days or Less. McGraw-Hill. ISBN 0070342237.\n- Ellen Gottesdiener (1995). ""RAD Realities: Beyond the Hype to How RAD Really Works"" Application Development Trends\n- Ken Schwaber (1996). Agile Project Management with Scrum, Microsoft Press Books, ISBN 978-0735619937\n- Steve McConnell (2003). Professional Software Development: Shorter Schedules, Higher Quality Products, More Successful Projects, Enhanced Careers, Microsoft Prese s Books, ISBN 978-0321193674\n- Dean Leffingwell (2007). Scaling Software Agility: Best Practices for Large Enterprises, Addison-Wesley Professional, ISBN 978-0321458193\n- Download Free Staff Scheduling App For Trial Version EworksManager']"	['<urn:uuid:5510859f-0694-490e-9e6d-44e2a638dd49>', '<urn:uuid:58820d87-315e-4a6a-ad57-be7345eb240e>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T19:10:11.825225	6	94	2179
96	As someone studying crisis management, how did Akshardham stay peaceful after the terrorist attack?	The peace was maintained primarily due to the spiritual leader's appeal for calm and restraint. When the terrorists struck, instead of reacting with anger, he called for peace and unity, instructing everyone to pray and avoid inflammatory speeches. His appeal had significant impact because people had witnessed his socio-spiritual services over three decades, including relief work after natural disasters and contributions to health and education. Even during a two-day national strike following the attack, there was no violence or agitation, even in communally sensitive areas.	"['Pramukh Swami Maharaj\'s absolute poise in the face of the attack on Akshardham amazed everyone.\nHis appeal for peace impressed all.\nUnder normal circumstances we are all calm, good, understanding and helpful. But in adverse situations or tragic events, we break down and collapse or become violent and explosive. The emotional turmoil throws us off balance in times of tragedy, difficulty and even in success.\nWhen the terrorists struck Akshardham, Pramukh Swami Maharaj\'s sthitpragnata (balance of mind) was amazing. When the whole world was stunned and shocked with disbelief, disgust and hatred at the cowardly act, Swamishri calmly prayed and set about reinstating things to normal. He appealed to the devotees and citizens of Gujarat to remain calm and peaceful. He attended to those who came to offer their condolences and to those who came to seek solace. Such resource of equanimity is not found in ordinary human beings, but in people who are divine. The spiritual virtues in a physically diminutive 82-year-old Pramukh Swami Maharaj amazed all.\nWhen the terrorists entered the Akshardham precincts at 4.45 p.m. and went on the rampage, Vishwavihari Swami, the administrator of Akshar-dham, was informed of the attack on the intercom telephone system. The District Superintendent of Police, Collector of Gandhinagar and other officials were called for help. Thereafter, Vishwavihari Swami rang up Pramukh Swami Maharaj in Sarangpur. At that time, Swamishri was engaged in a meeting regarding the BAPS relief and rehabilitation projects in Kachchh. On hearing of the tragedy, Swamishri calmly informed those before him, ""Terrorists have entered Akshardham and they have killed many people. Let us pray to God that no further killing takes place. May the terrorists be caught."" And Swamishri became absorbed in prayer. Then he instructed Ishwarcharan Swami, Anandswarup Swami, Brahmavihari Swami and Nikhilesh Swami who had come for the meeting to rush back to Gan-dhinagar. Swamishri told the 250 sadhus and 2000 devotees who had gathered for the festival of Shastriji Maharaj\'s shraddh to join in the prayers.\nAfter the prayers, Swamishri became engaged, without a hint of the disturbance, in a meeting with sadhus and devotees of Surat regarding the ongoing local construction of a BAPS hospital. At 7.30 p.m., Swamishri went to the Yagnapurush Smruti Mandir and prayed by chanting the Swaminarayan dhun while performing pradakshinas. He prayed, ""May there be no further loss of lives and the operation be resolved peacefully...""\nOn returning to his room, Swamishri became engaged in replying to phone calls from people offering their condolences from India and abroad. Many religious leaders and political leaders like the Deputy Prime Minister L.K. Advani, Chief Minister Narendra Modi, Ahmed Patel on behalf of the Opposition leader Sonia Gandhi and the President of India Dr A.P.J. Abdul Kalam expressed their personal sympathies by phone to Swamishri.\nSwamishri then held a meeting with Dr Swami, Tyagvallabh Swami, Viveksagar Swami, Narendraprasad Swami, Gnaneshwar Swami, Narayanmuni Swami and others to announce the cancellation of Shastriji Maharaj\'s shraddh assembly the following day and transform it into a memorial meeting. Swamishri then advised that the prasad of dudhpak that had already been prepared be distributed to orphans and school children in the surrounding villages. He also instructed that all BAPS centres in India and abroad should join in prayers on the day of shraddh the following day.\nAll the while, Swamishri\'s constant communication and suggestions provided guidance and moral and emotional support to the sadhus in Akshardham.\nBy 9.00 p.m. all the pilgrims, except those in the multimedia theatre, had been safely evacuated. The heated exchange of fire between terrorists and commandos and the proximity of the terrorists to the multi-media theatre left no alternative but to keep the 85 visitors locked inside the multimedia hall. When Swamishri was informed of this late in the night, he enquired about provisions for drinking water and the toilet arrangements, especially for children and the elderly. Though there were no such arrangements available inside, makeshift provisions were immediately set up on Swamishri\'s instruction. Swamishri was, at the same time, concerned that the attack could spark off a communal backlash. At 12.35 a.m. he got news by phone that Parameshwar Swami had become a fatal victim of a terrorist\'s bullet. Swamishri grieved for his loss and to prevent any eruption of communal violence he urgently sent an appeal for peace and prayer to the print and electronic media. The appeal played a major role for peace in the days to come.\nFor the next two days, Wednesday 25 September and Thursday 26 September, the people of India observed a strike to mourn and pay respects to the dead. All public services, offices and businesses were closed in Gujarat, Mumbai and many other parts of India. The national strike was a cause for concern for many leaders, for it could spark off violence on a national scale. But Swamishri\'s appeal for peace had an overwhelming impact. The two-day strike was observed peacefully and even in Amdavad, a communally sensitive city, there was no incident of agitation or violence.\nTim Sullivan, the South Asia head for Associated Press, during his visit to Akshardham on 1 October asked a mandir official, ""Whenever something of this sort happens in Gujarat there is violence. Why has Gujarat remained peaceful this time?"" The official replied, ""Due to Pramukh Swami\'s appeal the people of Gujarat have remained calm. For the last three decades people have been witnessing the socio-spiritual services that Pramukh Swami Maharaj has been performing in Gujarat. This includes the relief and rehabilitation services after the Kachchh earthquake and the Orissa Cyclone, and services in the field of health, education, etc. People know that he has been contributing for peace and betterment of society, and that is why people have observed his appeal for restraint and peace.""\n""Is it only because of Swami\'s appeal that there has been peace?"" Tim asked. The official replied, ""Other religious and political leaders have appealed for peace but Pramukh Swami Maharaj\'s appeal has carried more weight because the incident took place on our own premises.""\nThe leader of a popular Hindu organisation told Swamishri, ""Swami if you had only signalled otherwise, then things would have been different. There is peace in Gujarat and Bharat because of your appeal. Through your powers of austerity and goodness there has been peace.""\nA social thinker lauded Swami\'s peace appeal, ""Swami, you have saved Gujarat from another chapter of violence.""\nThe attack on an exquisite religious place like Akshardham and the killing of innocent men, women and children was unbearable and intolerable for all. For those deeply dedicated to Hinduism and all apostles of humanitarism it was a heart-wrenching incident. In such circumstances how could one bear and tolerate such pain and injustice! But Swamishri\'s voice resonated with words of peace, ""It is a time to maintain peace and unity and offer sympathy. Pray that such violence never occurs to anyone, in any part of the world.""\nThe kshatriyas of a village in Saurashtra came to Swamishri in Sarangpur and told him about a condolence meeting arranged by them. Swamishri advised, ""Talk about peace and make sure there are no angry and inflammatory speeches…""\nSwamishri\'s composure was appreciated by all. A TV channel anchorman-cum-educationist said, ""Leading members of the intelligentsia are impressed by Pramukh Swami\'s call for peace. If something like this had happened in one\'s own place, then the person concerned would have given statement after statement to become popular. However, Pramukh Swami and his sadhus, rather than issuing statements or reacting with anger, have remained silent and engaged in the service of the victims. And to add more, instead of becoming emotionally paralysed, the entire Sanstha was actively engaged in service. This is a congratulatory approach.""\nSwamishri remained in Sarangpur for another four days after the tragedy due to ill health and instructions from the state police. To look after the situation in Akshardham, Swamishri had sent Dr. Swami and Ishwarcharan Swami. Under their guidance, Anandswarup Swami, Vishwavihari Swami and other sadhus handled the post-attack responsibilities. On Swamishri\'s instruction, the hospitalised victims at the Civil Hospital in Amdavad and their relatives were provided with meals and fruits from the BAPS mandir in Shahibaug. Dedicated doctors of BAPS, Dr. Bajadia, Dr. Harshad Joshi, Dr. Bharat Panchal and others offered their services with the doctors of the Civil Hospital in Amdavad. At the Civil Hospital in Gandhinagar the patients were under the care of Dr. Mangalbhai Patel (a devotee) and other doctors.\nThe rush of media reporters and dignitaries from Wednesday 25 September onwards was satisfactorily attended by Brahmaprakash Swami, Vivekjivan Swami, Brahmavihari Swami, Aksharvatsal Swami and other volunteers. BAPS sadhus personally consoled the relatives of those who had died and sent them condolence letters on behalf of Swamishri.\nIn this way Swamishri actively guided and inspired the sadhus and volunteers to offer their services during and after the attack.\nOn Sunday 29 September, Swamishri travelled from Sarangpur to the Civil Hospital in Amdavad and personally blessed and consoled more than 24 injured commandos, policemen and visitors being treated in different wards. The victims and their relatives were moved by the compassion and strain taken by Swamishri.\nIn a letter to the editor\'s section of The Times of India on 8-10-02, Kaushik Joshi wrote, ""In the wake of the attack on Akshardham, Pramukh Swami has shown magnanimity by not indulging in any blame game and imputing motives. Akshardham is his most priceless, splendid and wonderful creation. Yet, he has been calm. His saintliness is very touching. His heart bleeds for the helpless victims of the barbaric act…""\nA renowned Jain acharya, Pujya Shri Chandrashekharvijayji, wrote a letter to Swamishri, ""An idol of patronage, gem among saints and the highest devotee of God Swamishri Pramukh Swamiji,\n""My heart is intensely shocked at the terrible turmoil that broke out on Akshardham. To ask you what pain must have afflicted you is improper because you are a saint who lives on a sthitpragna level. You console devotees who come in tears to offer their condolences. This is the high level of saintliness you have.""\nIn the Shrimad Bhagvat Gita, Arjun enquires of Shri Krishna as to how a person with a steady mind (sthitpragna) speaks, behaves, etc. Shri Krishna replies, ""Sukhe-dukhe same krutvã lãbhãlãbhau jayãjayau…"" A sthitpragna person is one who is undisturbed in happiness and misery, in gain and loss and in victory and defeat.\nPramukh Swami Maharaj\'s unwavering steadiness and calmness in the midst of the carnage at Akshardham and his appeal for peace thereafter is a living proof in our contemporary times of the highest Hindu wisdom prescribed in our scriptures and of the great souls that lived in the past. As long as we have sthitpragna souls like Pramukh Swami Maharaj there will be sanity, peace and redemption in our world.']"	['<urn:uuid:3a95b63a-feda-45b6-b9eb-7a72f282935d>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	14	85	1797
97	I'm working on my company's marketing materials and need help understanding digital images. What's the difference between RGB and CMYK color modes, and what kind of files do I need to give my designer to get good quality prints?	RGB (Red, Green, Blue) is used for digital displays like computer screens and mobile devices. It creates colors by blending light, with all colors combined making white. CMYK (Cyan, Magenta, Yellow, Key/Black) is used for printing and combines inks, with all colors combined making black. For print materials, you should provide your designer with high-quality files at 300dpi when viewed at actual size. The best formats are vector files (.AI, .EPS, .PDF, .SVG) since they can be scaled without losing quality, or high-resolution raster files (.PSD, .TIFF, .EPS). Always tell your designer upfront how you plan to use the images, as different formats work better for different purposes - a small JPG might work for an article but won't work well for a banner.	['Using the correct color mode in your design is essential to getting your message across. Learn the distinctions between RGB and CMYK color profiles with this simple overview.\nCover image via Rostislav_Sedlacek.\nThere can be a lot of confusion behind RGB and CMYK color modes. If you’re a designer, it’s crucial to differentiate the color profiles since designing in the wrong mode can affect the tone and message of your document. In this article, we’ll decipher the characteristics of each profile and when to use RGB and CMYK in your designs.\nThe RGB color profile consists of Red, Green, and Blue hues that combine to create extensive variations of colors. This color mode exists exclusively in screen displays, such as in computer monitors, mobile, and television screens. Even though RGB is present across all electronic devices, the color elements vary across systems and models. An image you see on an iMac may display differently than the same image on a Dell desktop computer.\nInstead of utilizing ink to produce hues, the RGB profile partakes in additive processes to produce color by blending light. This is the exact opposite of subtractive color processes, such as mixing paints or dyes. The presence of all RGB primaries at full intensity yields white, while the absence of color produces black. When you turn a monitor or screen off, you see an absence of RGB color, resulting in black. The color displays on your screen result from the presence of those RGB base hues.\nIn Adobe Illustrator, you can see how mixing RGB primaries at different intensities can vary the color output. When isolated, the primary hues generate vibrant shades. Red and green combine to yield yellow, green and blue mix to create cyan, and blue and red produce magenta.\nRGB produces a large array of colors; this color profile features a larger gamut, or color range, than CMYK. Above, you can see how Illustrator provides a gamut warning for RGB colors that can’t be successfully translated into CMYK. This occurs when an RGB color exists outside of CMYK printing abilities. Instead, the color will convert to a CMYK equivalent, which could affect the tones of your design. When printing a design produced in RGB color profiles, the print output will be different from the online preview.\nWhen to Use RGB Color Mode\nTo avoid color profile complications, always set a document in RGB when designing for social media, digital design, or online advertisements. Set any design or image that will exist on a screen or monitor as RGB. If you design in CMYK for an online-only design, you’ll limit your color options due to CMYK’s limited gamut range.\nThe CMYK color profile contains Cyan, Magenta, Yellow, and Key (Black) that combine to produce a range of hues. This four-color process works for any type of printer. When zoomed in on printed images you can see the four-color dots that layer to create different hues and gradations. Dots per inch result from printing and involve the CMYK color profiles. Although all printers produce prints in CMYK, the end result may vary among different styles and models of printers.\nIn RGB color spaces, all primaries combine to produce white with additive color processing. CMYK modes combine with subtractive color processes, meaning all primaries mask to yield to a blackish hue. This process is similar to when you mixed paints and dyes as a kid to make that unsightly dark color. As inks and dyes are layered upon each other, they subtract from the white of the paper.\nNotice how the CMYK sliders differ from the RGB sliders. Zero intensity of all primaries produces black in RGB color profiles; in CMYK color profiles, the absence of color produces white. Cyan and magenta create a blue, magenta and yellow create red, and yellow and cyan produce green. The combination of cyan, magenta, and yellow develop an off-black shade. Key, or black, comes in to add shades to each primary due to the primaries’ inability to combine and create black.\nWhen to Use CMYK Color Mode\nReserve CMYK for printed designs. Set products such as business cards, flyers, posters, and packaging in the CMYK color profile, and do yourself and the print shop a favor by always setting your document to CMYK before exporting or printing. Doing so prevents unbalanced colors that can result from RGB colors translated into CMYK equivalents.\nChanging Color Modes in Creative Cloud\nMost Adobe Creative Cloud programs will default to RGB color modes. You can easily adjust modes in the New Document panel under the Color Mode dropdown, or within the program itself. Not all programs support RGB to CMYK conversions, but with Illustrator, Photoshop, and InDesign, you can switch in just one click.\nIn Adobe Illustrator, you can switch colors modes by navigating to File > Document Color Mode > RGB Color or CMYK Color. This will affect the output of your vector shapes when shifting between color modes.\nWithin Photoshop, you can change color profiles by going to Image > Mode > CMYK or RGB Color.\nNavigate to the Color Panel within the software. If the panel isn’t visible, hit F6 to bring it up. Hit the dropdown symbol and select CMYK or RGB.\nLooking to expand your knowledge on essential design terminology? Check out these educational posts:', 'When you consult with a graphic designer for your print project, they will need your logo, photos and other digital art (or assets) to produce the finished piece. Moreover, to save both time and money, the files you provide should be what’s called “print ready artwork.” But what does “print ready” really mean in terms of your project?\nWhen you hand your graphic designer poor quality art, they must recreate it, which takes additional time that isn’t likely provided for in your estimate. And, it’s your designer’s job to make sure your brand will appear the best it can be, and they cannot produce a high-quality product without the correct images, files or assets in the correct format with suitable specifications.\nNot only does poor quality frustrate your designer, but you will be disappointed with the final printed product. Moreover, your designer will be disappointed to hand you a poorly printed product. It’s not only embarrassing for you, but for the graphic designer as well! Who wants to hand out poor quality business collateral with their name attached to the pieces?\nWhen you first consult with a designer, they likely will briefly discuss print ready art and what goes into ensuring a file is ready to print. Your designer needs to learn about and review your files, so they can plan accordingly and provide an accurate estimate for your printed product.\nAs the client, your biggest challenge is determining if your art is print ready and if your designer will readily accept it. In this guide, we will help you figure out exactly what the term “print ready artwork” means and how to determine if your art is indeed ready to print.\nWhat Does ‘Print Ready Artwork’ Actually Mean?\nPrint ready art, sometimes also called ‘press ready,’ is any file or asset that you provide your graphic designer or printer that is high quality and will not pose any issues when the final layout is ready to be printed. If your art is not print ready, it could result in extra expense and reprints. Any good graphic designer or printer will be able to quickly tell if your art isn’t up to snuff. Bringing your art to “print ready” format almost certainly adds time, and more time adds unnecessary cost to your project – especially if you’re in a hurry and your art isn’t suitable for printing.\nExplain the Full Scope of Your Project\nFor example, once a client asked me to recreate an old image into a print ready image. After asking some questions about the project, I gave the client the final dimensions, DPI and color modes required for the final image. After a few days, the client reached out, wondering why the art they supplied would not print well on a banner. I tried to explain that since he had failed to mention that the image would print on a banner that I had not designed the layout for a banner. I tried to explain that his image (a rasterized image) would never be suitable for a banner. We discussed the somewhat complex technical reasons as to why this would be impossible, details that we’ll discuss more later. In fact, the final print ready banner image’s file size would be so huge that it would be a nightmare for other designers or printers to handle. Using an image to design a business card has very different design considerations from the requirements for designing a banner.\nIn the world of printing, a good way to think about art is to “go big or go home,” which means that if you think that your design will ever be printed in a large format, tell your designer upfront. It will save tons of time down the road when all of a sudden, for example, you must have the logo on your business card printed on a banner.\nSo, the short story is that you always should tell your designer how you intend to use the final design. It will help them make informed decisions when estimating and designing your project.\nResolution, DPI and the Final Printed Size\nDo you remember when we mentioned “complex technical reasons” for supplying all your images in the correct size and format for your designer? Let’s discuss those reasons. For any printed project, it’s very important to consider the final size of any art you provide your designer. Will your art be featured as a cover image, using the entire printable area, or will it be used as a small photo inline within an article?\nKnowing how your art will be used is important for your designer to determine if your art is print ready. For example, a small .jpg sized to 1500 by 1500 at 150dpi is ideal when used as an image for an article, but when used as a cover image, it will experience issues.\nAt the end of the day, your designer always needs the highest-quality art you have. This gives your designer greater flexibility when designing your project, because high-quality art has the flexibility to be used anywhere within a project.\nRaster Images vs. Vector Art: Understanding the Difference\nUnderstanding the difference between raster images and vector art is a huge help when determining if your art is print ready.\nThink of raster images like photos. They are made exclusive of pixels, which has important considerations when printing. Your rasterized image often ends with a file extension such as “.jpg,” “.gif” “.raw,” “.tiff” or “.psd.” Your designer is looking for .PSD, .TIFF or .EPS files. Typically, but not always, those file types are considered print ready.\nPopular Raster Image Editors\n- Adobe Photoshop (industry standard)\n- Adobe Lightroom\n- GIMP (free and open source)\n- Corel Photo-Paint\nHowever, keep in mind that unless your file is in a format that’s supported by your designer’s software, they may have issues handling the raster image. Photoshop is the industry standard, and all professional designers will eagerly accept Photoshop files.\nYour designer often will request that your raster image is, at minimum, 300dpi when viewed at the actual size of the image as printed on the final product. Your designer may mention “shown at 100%,” which means that it’s the same thing as “actual size.”\nNext, let’s review a quick way to determine if your raster image will print well. Print the image zoomed in to 300 percent. Does the raster image look clean and crisp? If it does, then it’s likely suitable for printing. If your image looks pixelated or blurry, then find a different image for your project as it’s extremely unlikely to print well.\nThink of vector art as the “gold standard” for printing. Vector art is very much akin to a drawing, but rather than a pencil, vector art uses coordinates to “draw” the art. The biggest benefit to vector art is that it can be increased in size without losing quality. Often, vector art ends with the file extensions .AI, .EPS, .PDF or .SVG.\nPopular Vector Art Editors\n- Adobe Illustrator (industry standard)\n- Inkscape (free and open source)\n- Corel Draw\n- Figma (when saved as an .svg)\nBut be careful: Rasterized images can “live” inside a vector file. If you are unsure, your designer can tell if your file contains a raster image and if it will present any issues. Simply ask!\nColor Space, RGB, CMYK and PMS Colors\nWhile not as important as it was several years ago, the color mode in which your art is saved is also important. In today’s design world, unless you are looking for a very specific color match on the final print, color mode usually isn’t an issue.\nArt saved in the RGB color space is designed to be viewed on a monitor, TV or another illuminated display. “RGB” stands for “Red Green Blue,” which are the colors that comprise every pixel on your display. Converting you RGB art to a different color space, such as CMYK, will cause the final printed image’s color to shift. Depending on your project’s requirements, it may be undesirable for the image to be color shifted on the final product. Although this shift may not be noticeable to the untrained eye, sometimes it causes an issue. Your designer will advise you if your RGB art will present any issues.\nThe CMYK color space represents the four ink colors used in traditional full-color printing. “CMYK” stands for “Cyan Magenta Yellow and Key (Black).” Art intended for print and saved in this format will very closely match the final printed piece on a calibrated monitor. While your monitor likely can correctly display the final printed colors, if you are looking for a very specific color, you only should trust two things: your designer’s calibrated monitor or a physical print proof from the printer.\nFunny name, but PMS is very serious when printing. PMS is an abbreviation for Pantone Matching System. PMS colors are a universal color/ink scheme, which guarantees that a specific PMS color will accurately match the final printed product. Using PMS colors often can get expensive, depending on the project, as the printer typically must set up their press for that specific color. Unless color is critical, consider the same PMS color, but converted into CMYK.\nPMS colors sometimes also are referred to as “spot colors.” Spot color in a full-color print is when a specific ink is used as the fifth color (CMYK + Spot). You’ll often find it is much cheaper to have your art converted to CMYK rather than paying for spot-color printing.\nIf your project requires very specific color matching, for example, a very specific red for your logo, you should work very closely with your designer. Poor communication in this step of the production process can result in an entire print run being incorrect and ultimately discarded. Color matching is expensive, so experts recommend that you always request a press proof.\nMy Art Is NOT Print Ready. What Do I Do?\nAfter reading this, you may have realized that your art is not in the format required to print your project and get good results. Don’t sweat it! We know that it’s a lot of technical information to absorb, and after all, design is NOT your field. That’s why you’re smart enough to employ a specialist – a graphic designer – in all things print.\nSimply be upfront and honest with your designer, because the fewer surprises, the better your end product will be. For example, tell your designer that all your images came from your website. Your designer will work with you to find solutions. You may have to hire a photographer or spend time to have your logo redrawn to make it print ready. But it’s worth it if your designer produces something that both you and they are proud of! Your designer is an asset to your brand and your business.\nAs you work with your designer on additional projects, they’re building a library of resources for your business and your brand. They will soon get to know your style, needs and requirements, sometimes even before you begin discussing the full scope of your new project!\nAn added benefit of working with the same designer is that they will be able to quickly identify design mistakes or make suggestions about your project that help make it more consistent with your brand’s design theme, in fact, slowly building your brand’s style guide. Your brand’s style is what makes you and your brand stand out from the myriad other brands that do the same or similar thing that you do. A very important tool, your style guide helps determine how your brand’s art, logos or assets can be used. Using a style guide ensures that your brand maintains a consistent look and feel, no matter where those assets appear.\nTo give your graphic designer the best chance of printing a product that you both can be proud of, it’s critical that you provide print ready artwork. Doing so saves lost time and money, and your designer will have the tools to please you and to produce a final printed piece that truly represents you and your brand.\nIt’s sometimes a tough pill to swallow to discover that your art is of poor quality and could very well lower your project’s ability to make a statement. Do a little homework before you visit your designer and work closely with them. If you work together, it will result in a product that you both can love.']	['<urn:uuid:24845e0f-9c7f-46e4-81ac-3a59c9a12fef>', '<urn:uuid:e83f8942-df08-4920-9d20-41e283a7a010>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:10:11.825225	39	124	2973
98	compare modern battery liquid compressed air	Liquid flow batteries and compressed-air systems represent two distinct energy storage approaches. The liquid flow battery system uses a gravity-fed design with particles flowing through narrow openings, similar to an hourglass, and can be controlled by adjusting the device angle. It's designed to be simple, compact, and inexpensive, with components that could be made through injection molding or 3D printing. In contrast, the compressed-air system converts wind-generated mechanical spillage to compressed air during surplus periods and regenerates power when needed, using a variable displacement machine. Both systems aim to improve energy storage efficiency, but use different mechanisms - fluid dynamics versus air compression.	"['Off-campus UNL users: To download campus access dissertations, please use the following link to log into our proxy server with your NU ID and password. When you are done browsing please remember to return to this page and log out.\nNon-UNL users: Please talk to your librarian about requesting this dissertation through interlibrary loan.\nConfiguration and optimization of a novel compressed-air-assisted wind energy conversion system\nThe increasing concerns over the environmental impact of carbon emissions and the unsustainability of conventional fossil fuel power plants are stimulating interest in the implementation of renewable energy in current power systems. Among all of the renewable energies, wind energy holds a prominent place because of its high output and the maturity of the technology. However, like all of the other renewable energies, integration of wind energy into the power grid causes some quality and control issues, such as overvoltage or undervoltage and frequency excursion. ^ Other issues include: 1) wind power generation may require a broader safety margin of the capacity reserve, 2) the excessive energy may be rejected by the transmission because of the mismatch between generation and load demand, and 3) induction wind turbines may not be able to ride through a voltage sag event because a critical voltage has to be guaranteed to produce the fundamental magnetic field. ^ To mitigate these issues and build a robust wind power system, a novel structure referred to as a compressed-air-assisted wind energy conversion system (CA-WECS) is proposed in this dissertation. The CA-WECS converts the wind-generated mechanical spillage to compressed air when the wind is a surplus and regenerates power from the compressed air storage when the wind is a deficit. The addition of a compressed air storage subsystem decouples wind power and electric power allowing a higher level of dispatchable generation and providing another degree of freedom for power management. The key component of the new system is a variable displacement machine (VDM), which can work as a compressor or air motor/expander depending on the power gap between wind power and load/command. ^ This work addresses the configuration of the CA-WECS in detail. The functions of the system components are explained and the fundamentals of the proposed VDM are explicitly described. A regulation policy for dispatchable generation is simulated and studied. The economical issues associated with the implementation of the proposed system are split into two parts, the sizing problem and the offering problem. The sizing problem refers to finding the proper sizes of different components for the proposed system. The offering problem refers to determining the appropriate offer to the day-ahead electricity market for a wind farm consisting of the proposed system. A two-stage stochastic framework is used to solve the optimization model each problem. The simulation studies validate the benefits of the proposed system. The results show that renewable generation is increased by 15-20% under various wind conditions, accounting for a 20-30% revenue increment in a dynamic market environment.^\nAlternative Energy|Electrical engineering\nCheng, Jie, ""Configuration and optimization of a novel compressed-air-assisted wind energy conversion system"" (2016). ETD collection for University of Nebraska - Lincoln. AAI10141690.', 'There is a multitude of different battery technology available nowadays. Previously we have read about ‘AGDIB: New Battery Technology by Chinese Researchers‘, which was a solution over lithium-ion battery (LIB) problems. It supplies energy density similar to 150 Whkg-1 at a power density of similar to 1200 Whkkg-1, which is 50% higher than profitable lithium-ion batteries.\nLater on, we also learned about ‘Never ending Nanowire-based batteries‘, where we come to know about its mind-blowing features for storing and transmitting electricity. They are extremely conductive. In size, it is thousand times thinner than a human hair.\nHere, one more new approach to the battery technology has been added. MIT researchers have developed (invented) a whole new concept that turns battery technology upside down. They give it a name as liquid flow batteries.\nAlthough, this is not a new concept. Before three years ago, some researchers unveiled a similar concept. That basic technology could use different chemical formulations, It include same chemical compounds found in today’s lithium- ion batteries. In this case, basic components are not a chunk of a solid object, which being left in place for the life of the battery, but rather tiny particles that can be carried along in a liquid slurry. Bigger container/tanks are essential for increasing storage capacity to hold the slurry.\nPrevious liquid batteries depend on complex systems of tanks, plugs, and pumps. It also was expensive in cost and supplies lots of chances for possible leaks and failures.\nThis new version of liquid batteries replaces a simple gravity feed from the pump system. It also throws out that complexity of pump system. Directly by changing the angle of the device, the rate of energy production can be adjusted. It causes boosting or slowing down the rate of flow.\nAccording to Kyocera Professor of Ceramics Yet-Ming Chiang, “This new approach as something like a “concept car”, a design that is not expected to go into production as it is but that establishes some new ideas that can ultimately lead to a real product.”\nThe real concept for flow batteries demonstrated in 1970’s. Those previous versions of materials had very long energy density. In proportion to their weight, they had the very low capacity for storing energy. After that new concept came in the development of flow batteries before few years ago. It came with the introduction of high energy density versions. That version of batteries had various advantages. But if we compare it with other flow batteries, they had a major drawback of complexity in its plumbing systems.\nThis new approach overcomes that drawback of plumbing with a very simple gravity fed system. It has functions like an old hourglass or egg timer. Its particles flow through a narrow a narrow opening from one tank to another. By turning the device over, the flow can be reversed. It looks like a rectangular window frame in shape with narrow opening the place where two bands would meet in the middle.\nThe team built only one of the two sides of the battery. The first side is composed of flowing liquid, whereas another side consists of a sheet of lithium in solid form. Before making their ultimate goal, the team first decided to evaluate the concept in a simpler form. This new version has both positive and negative electrodes. Both are liquid and flow side by side through an opening. The opening is separated by a membrane.\nDepending on their specific applications, both solid and liquid batteries have advantages.\nChiang said, “The concept here shows that you don’t need to be confined by these two extremes. This is an example of hybrid devices that fall somewhere in the middle.”\nHe then continued, “The trickiest part of the design process was controlling characteristics of the liquid slurry to control the flow rates. The thick liquid was behaving a bit like Ketchup in the bottle. It’s hard to get it flowing in the first place, but hen once it starts, the flow can be too sudden. For getting the right flow, it required a long process of fine tuning both the liquid mixture and the design of the mechanical structures.”\nSome key features of this new concept of battery technology:\n- This new design is simpler and more compact battery systems.\n- It is inexpensive and modular.\n- Allows for gradual expansion of grid-connected storage systems to meet growing demand.\n- Components are simple enough that they could be made through injection molding or even 3D-printing.\n- For this new liquid battery, the power density is determined by the size of the attack.\n- Energy density is determined by the size of its storage tanks.']"	['<urn:uuid:59be23ed-dff9-44b2-95e0-50612bb5bb75>', '<urn:uuid:615c4e20-1fd2-478c-9131-51644d430e57>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T19:10:11.825225	6	103	1290
99	Why do poor rural dwellers convert large wood to charcoal?	Poor rural dwellers who cannot afford saws and axes convert large wood to charcoal so it can be broken up for cooking fires. This practice is thermally efficient - burning the charcoal is about twice as efficient as burning wood directly in an open cooking fire, and charcoal can be stored indefinitely without deterioration.	['13.1. Performance indices of carbonising equipment\n13.2. Influence of wood characteristics on carbonization methods\nThroughout the world wood is turned into charcoal by a surprising variety of systems. Choosing the optimum carbonization method is of interest to every potential charcoal producer. A close look reveals the reasons why so many wags of carbonising wood can coexist, different in detail, by relying on the same fundamental principle.\nMost charcoal is made by small scale peasant type producers, either for their own local needs or for a restricted market. There is relatively little international trade in charcoal and hardly any competition between producers in one region with those in another. This fact tends to isolate charcoal producers in various countries and allows marked regional differences to continue.\nThe reason for carbonising the wood is a further factor. On the one hand, those who make charcoal for industry are interested in striving for maximum productivity and efficiency. At the other end of the scale are those who carbonise wood simply because they cannot subdivide the wood fuel in log form for use in domestic cooking by any other method. They have a totally different set of guidelines. The first group has access to capital and technology, the latter may not even possess an efficient axe or saw and must choose a method which requires the absolute minimum of capital investment. If this implies wasteful use of resources or human labour power, there seems no other alternative to them.\nTradition, the embodied wisdom of rural societies, plays an important part. To use the established method which is known to work successfully in a locality is the logical option for those who cannot afford to take risks because of their precarious economic situation. Where social factors are dominant, it is usually very difficult to introduce a new technology of charcoal-making unless the social factors are changed. Frequently one sees attempts to modify the technology of charcoal-making by providing aid: inputs such as chain saws, new kilns and so on. When these inputs are no longer available, economic necessity forces the producers to revert to the traditional, successful method with all its obvious technical faults. Therefore carbonising methods cannot be evaluated just on the basis of technical factors; social factors are of equal importance.\nBut good technology is important in the long run in improving social conditions. Therefore, if social factors permit, methods which give higher yields of better quality charcoal at lower cost should be used. These technical considerations are the concern of this chapter in comparing the different methods of converting wood to charcoal.\nThe various carbonising methods are classified (13). The method followed is based on reference 29 which gives a useful overall view of the range of methods available.\nThe first major difference is between systems which heat the wood by external means, using wood, oil, gas, etc., and systems which allow combustion on a limited scale to occur inside the carboniser by burning part of the wood charge and using this heat to dry and carbonise the remainder.\nFig. 13. Classification of carbonization systems\nThis method should be the most efficient since the heat is generated where it is needed, using low cost wood fuel. In practice, it is difficult to control the combustion and some extra wood is burned which lowers the yield.\nIndirect (external) heating allows more precise control but to transmit the heat to the charge is difficult and inefficient and metal retorts are almost essential. By-products can be recovered free of contamination from the products of combustion. A hybrid method heats the charge of wood by passing hot gas through it. The hot gas is obtained by burning a fuel which can be wood, oil or gas. Precise control is needed to ensure that the hot gas is free of oxygen, otherwise some of the wood will be burned instead of being merely carbonised. Heat transfer from the hot gas to the wood is quite efficient and where the gases are recirculated under proper control, it is feasible to condense and collect by-products and the combustible wood gas.\nSystems using internal generation of heat can be further divided by their method of construction. The three possibilities found are earth, which is lowest in cost, bricks or masonry of intermediate cost, and steel which is the most expensive. Steel kilns are further subdivided into portable and fixed types.\nSteel kilns have two advantages: they can be moved easily, which may be very useful, and they cool quickly, allowing a shorter cycle time. However, portability is not always an efficient idea, since it makes it difficult to organise and supervise production efficiently and fixed brick kilns can be cooled quite rapidly using a slurry of clay and water (with care,) by injecting water spray into the kiln. Although cycle times are still around six to eight days, compared to two for steel kilns, the greater volume and much lower cost of brick kilns make them preferable except where portability is essential.\nEarth kilns and pits even when operated efficiently, are slow burning and slow cooling and contaminate the charcoal with earth. However, where capital is limited or non-existent, they have real advantages.\nKilns heated by an external source of heat are subdivided into those heated by passing hot gases through the charge and those where heat is transferred through the walls of the retort. Most carbonisers in this subdivision are of metal but there is one exception, the Schwartz kiln, still commercially used, which is of brick and heats the charge by pressing hot flue gas from a bonfire burning wood built at the side of the kiln. Theoretically excellent since low quality wood and bark can be burned, in practice the kiln suffers in comparison to internally fired brick kilns by its high construction cost requiring steel and cast iron components, difficulty of precisely controlling the fire, and sealing the kiln for cooling, leading to air leaks and loss of charcoal.\nSteel retorts heated through the walls are not used much today because of high cost and intrinsic low efficiency but some portable and semi-portable experimental retorts have appeared recently (14), e.g. the Constantine retort and the Jamaican oil drum retort. The steel retorts heated by circulating gases are efficient, produce charcoal of excellent quality and allow by-products to be recovered. However, their high capital cost makes them unattractive, except where the labour costs of traditional systems outweigh the high capital cost. These retorts are mainly applied today for making high grade charcoal for metallurgical and chemical use. Their use in the charcoal industry once seemed attractive but recent developments in making high purity iron without charcoal and changes in the world steel industry based on coal, make their use problematic until a lower capital cost version is developed. It seems unlikely that they can make any major contribution to the production of charcoal for domestic use in developing countries.\nHaving classified the various types of carboniser, they can then be compared, using various calculated indexes (29) such as production per unit of internal volume, unit area of space occupied, unit of capital invested, etc. These calculations are best carried out to compare types within a subdivision when the basic type of carboniser needed has been chosen on broad social and technological grounds. In practice, as far as the developing world is concerned, the choices are limited to deciding between pits, earth kilns, brick kilns and steel kilns, all internally heated. Where capital is the limiting resource, and wood is available, earth kilns are preferable. Where some capital is available and a serious effort is to be made to produce quality charcoal efficiently, brick kilns will probably be preferred. Steel kilns may find use where mobility is of such overriding importance that it overcomes high capital and repair costs.\n13.2.2. Moisture content\n13.2.3. Wood size\nThe characteristics of the wood raw material have a significant effect on the choice and performance of carbonization equipment. The three important-factors are species, moisture content, and dimensions of the wood itself.\nGenerally all species of wood can be carbonised to produce useable charcoal. There is a variation in the ash content of different woods but this is generally not significant. Bark, however, has an unacceptably high ash content and the structure of bark charcoal is too friable to be useful for most purposes. Therefore, where possible, bark should not be used or the amount of bark charged with the wood should be minimised.\nSoftwoods generally produce a softer, more friable charcoal than hardwoods but where available in quantity at a suitable price, they are a good raw material and can produce all types of charcoal.\nWhere a choice of wood supply is possible, such as where plantations are being established to provide wood, it is worthwhile to choose the species and manage its growth rate to optimise charcoal properties. Eucalypt species produce good dense charcoal and are the favoured plantation species for the purpose. Careful tests should be made before unproven, little known species are planted.\nWhat counts in the long run is the mass of saleable charcoal produced per unit mass of wood substance. The volume of wood grown per hectare is only a rough indicator of the mass of wood substance produced. A high volume increment may correspond to low density and hence low yield of charcoal per unit volume of wood. Also denser wood usually produces a denser, less friable charcoal. Therefore research to determine what species and what management regime produces the maximum yield of wood substance by weight from plantations is worthwhile. This is an area of active research and definite answers are not yet available. But eucalypts are still the favoured genus.\nMoisture in wood charged to the kiln has to be evaporated by burning extra wood and this lowers overall yield. Also the time to complete a carbonization cycle is extended, thus increasing costs. The volume of unseasoned wood is also higher than dry wood and the packing fraction of the kiln is thus marginally reduced when green wood is used. Wood will dry in the air without any heating cost. Air drying costs are mainly financial plus the wood loss due to fungal decay and insect attack. It is necessary to strike the optimum time balance in drying so that the maximum amount of moisture is lost in the early period when water loss is rapid. Financial costs are less and wood loss due to insect and fungi is still low. About three months of drying is roughly optimum but this varies with climate and kind of wood. Effective drying is difficult in the humid tropics.\nCarbonization rate is closely related to wood size. Large wood pieces carbonise slowly since the transfer of heat into the interior of the wood is a relatively slow process. Sawdust, for example, can be flash carbonised very rapidly but the powdered charcoal produced is of low market value. On the other hand, large diameter trunks of dense species may shatter when carbonised making the charcoal more friable than otherwise. Studies have shown that charcoal with optimum properties for the iron industry is produced with wood pieces measuring about 25-80 mm across the grain. Length along the grain has little influence. (26).\nWith plantation grown wood uniformity in wood size is possible but natural forests yield a wide range of sizes. Cutting and splitting of wood is costly in labour, fuel and capital and should be avoided wherever possible. For carbonising large diameter trunks and mixed size charges of wood the slow cycles are best. The pit system is optimum. Of the masonry kilns the slower cycle, larger kilns are best. They are a well proven method for carbonising large diameter (around 0.5 m) dense wood from natural forests. Trouble in carbonising can be reduced by placing the large diameter blocks in the centre of the charge. Metal kilns which lose much heat through the walls and cool quickly are ineffective in carbonising large section wood.\nThe cost of cutting up wood is a serious and growing one as fuel, labour and capital costs increase and this favours the use of earth pits, mounds and brick kilns. It is also usually easier and faster to charge kilns with large size wood, especially if its length conforms with the size of the kiln, pit or mound. It is worthwhile carefully studying the relation between growing, harvesting, drying and kiln charging to decide the optimum dimensions of the wood both in length and diameter, so that overall handling and carbonising costs are minimised and charcoal of optimum properties for the final end use is obtained.\nPoor rural dwellers who cannot afford saws and axes often convert large diameter wood to charcoal so that it can be broken up for use in cooking fires. When the relative efficiency of wood to charcoal conversion and burning efficiency of wood and charcoal cooking fires are compared, the practice has much to commend it. Further charcoal is dry and can be stored indefinitely without deterioration. Calculation shows that carbonising large diameter wood and burning the charcoal is about twice as efficient thermally as burning the wood direct in an open cooking fire. Furthermore, without axes, saws and wedges, large diameter wood is unused and may rot before it can be burned.']	['<urn:uuid:bfa03f43-14ef-480d-b094-aedc1a67c28a>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:10:11.825225	10	54	2216
100	construction dates forth bridge delaware river bridge comparison which bridge built first	The Forth Bridge and the Delaware River Bridge (later renamed Benjamin Franklin Bridge) were built in different time periods. Construction of the Forth Bridge began in 1882 under William Arrol & Company, while the Delaware River Bridge construction started later, on January 6, 1922. Therefore, the Forth Bridge was built first, with about a 40-year difference between their construction dates.	"['The Forth Bridge is one of the great icons of Scottish engineering and the greatest achievement of Sir William Arrol’s life. It’s massive steel structure, spanning the estuary of the River Forth from South Queensferry to North Queensferry in Fife, is famous throughout the world. It was built to provide the fastest route from London, up the east coast Edinburgh and on to the North of Scotland, and the most direct route possible meant spanning the two wide estuaries of the rivers Forth and Tay. Apart from having to build these bridges across large distances, these estuaries presented other major challenges to the bridge builder. The Queensferry site was chosen for the Forth Bridge as the small island of Inchgarvie would provide a solid rock base on which piers could be built. On either side of the island though, the water was fast flowing and up to 200ft deep, presenting problems in sinking the pier foundations. Also the two channels were busy with shipping and would have to be kept open to traffic during construction.\nOriginally, a design by Thomas Bouch for a steel suspension bridge was chosen to be built and William Arrol was selected as the contractor to build it. Arrol started preparatory work on piers for the bridge in 1878 but this stopped following the Tay Bridge disaster in December 1879. The subsequent inquiry had indicated that Bouch’s poor design had contributed to the collapse of the Tay Bridge and another of his bridges, the South Esk Viaduct, had also been found to be unsafe. Having lost confidence in Bouch’s work, the Forth Bridge Company officially abandoned his bridge design in 1881 and looked for an alternative. John Fowler and Benjamin Baker’s design for a steel cantilever bridge was chosen and the contract to build it was awarded to William Arrol & Company in December 1882. It was an ambitious project, the first structure of it’s size to be built of steel, and having the 2 longest spans of any cantilever bridge ever built.\nThe main structure of the bridge consisted of 3 cantilever towers, each with 4 gigantic steel tubes, 343 ft high, supported on a masonry pier. The steel tubes of the great cantilever arms extended from the base and top of each cantilever tower forming a triangular shape and on either side of the central cantilever these were joined by a central girder. These cantilevers arms and towers were strengthened by numerous lateral and diagonal girders and tubes forming an intricate network of criss-crossing steelwork. Towards the south shore the cantilevers of the superstructure were met by viaduct of 10 spans and 4 arches supported on granite piers and from the north shore, an approach viaduct of 5 spans with 3 arches supported on granite piers. Running across the centre of the entire structure were lattice girders supporting a double line of rails with a footpath on either side. To ensure that there was a minimum height of 150 foot under the bridge at high water level, a gradient was built into the bridge to allow passage of ships in two main channels on either side of Inchgarvie Island. As a result, the full length of the completed bridge was 1.6 miles.', 'Stewardship. Service. Community.\n1818Residents of the Delaware Valley raise the idea of building a bridge across the Delaware River. Conjuring up the best resources of 19th-century engineering, they envision a low structure with a complex array of openings to accommodate the sailing ships of the day below and horse-drawn vehicles above.\n1913With the dawn of the motorized era, Philadelphia forms the Penn Memorial Bridge Committee to study the bridge issue.\n1916New Jersey Governor James F. Fielder creates the Delaware River and Tunnel Commission. Philadelphia agrees to work with the commission to jointly fund a bridge feasibility study.\n1919Both the New Jersey and Pennsylvania state legislatures approve creation of the Delaware River Bridge Joint Commission.\nDecember 12, 1919The first meeting of the Delaware River Bridge Joint Commission is called to order by its chairman, Pennsylvania Governor William C. Sproul. The commission’s vice chairman is Richard T. Collings, the former mayor of Collingswood, N.J., who comes to be known as the “Father of the Delaware River Bridge.”\n1920In one of its first official acts, the commission names Ralph Modjeski as bridge engineer and Leon S. Moisseiff as design engineer. Together they face the task of designing and building what will be, at the time it opens, the longest suspension bridge in the world. Modjeski proves to be a hands-on, energetic manager who regularly climbs around the construction site despite his advancing years. To the bridge-building fraternity, this becomes “Ralph Modjeski\'s Bridge.”\n1921President Warren G. Harding signs legislation authorizing construction of the bridge.\nJanuary 6, 1922Bridge construction begins. Presiding over the ceremony are Pennsylvania Governor William C. Sproul and New Jersey Governor Edward I. Edwards.\n1923The Joint Commission rejects the name Franklin Bridge and officially designates the structure as the Delaware River Bridge.\nJuly 1, 1926More than 25,000 people attend the official opening ceremony of the Delaware River Bridge, with Pennsylvania Governor Gifford Pinchot and New Jersey Governor A. Harry Moore presiding. After the ceremony, an estimated 100,000 people (including an 87-year-old Civil War veteran in full uniform) walk across the bridge before it opens to vehicular traffic. The following day, President Calvin Coolidge arrives to dedicate the bridge.\n1936The Bridge Line subway opens, offering service between Camden and Philadelphia via the Delaware River Bridge. The four stations are Broadway and City Hall in Camden and Franklin Square and 8th/Market in Philadelphia.\n1948A study by the Delaware River Bridge Joint Commission recommends the creation of a regional port authority. The goal, according to the study, is to centralize port responsibilities and enhance port facilities. The study also recommends the construction of a second bridge and a high speed commuter rail line.\nJuly 17, 1951After Pennsylvania and New Jersey reach an agreement, President Harry S. Truman signs the bill creating the Delaware River Port Authority as the successor agency to the Delaware River Bridge Joint Commission. The legislation gives the new agency the responsibility to promote international trade for Delaware River ports. President Truman also signs a companion bill that permits construction of a second bridge across the Delaware River.\n1952The Bridge Line subway is extended from 8th and Market Streets to 16th and Locust Streets in Philadelphia.\n1953Construction begins on the second suspension bridge across the Delaware, this one between South Philadelphia and Gloucester City, N.J.\n1955The Delaware River Port Authority designates a special committee to consider names for the two bridges. The committee recommends renaming the Delaware River Bridge as the Benjamin Franklin Bridge — a name considered, but rejected, in 1923. Also, in recognition of the many years the ""Good Gray Poet"" lived in Camden, the committee recommends naming the second crossing the Walt Whitman Bridge, making it the first major U.S. bridge named for a poet.\nMay 15, 1957New Jersey Governor Robert B. Meyner and Pennsylvania Auditor General Charles C. Smith dedicate and open the Walt Whitman Bridge. Within two decades, the bridge will have a major impact on the Delaware Valley, facilitating the success of commercial projects (the Philadelphia Food Distribution Center), sports and performance venues (Veterans Stadium and the Spectrum), and New Jersey highways (the Atlantic City Expressway and the Black Horse Pike).\n1960Planning begins for a high-speed rail line between New Jersey and Philadelphia.\n1961Planning begins for two additional bridges.\nThe Delaware River Port Authority, teamed with other port interests, conducts its first overseas trade mission.\nJune 11, 1964Construction begins on a high speed commuter rail line that will link Center City Philadelphia with Lindenwold, N.J.\nJune 13, 1964President Lyndon B. Johnson signs legislation extending DRPA jurisdiction into Delaware County, Pa., to permit construction of the Commodore Barry Bridge.\n1965Pennsylvania and New Jersey approve plans for construction of the Betsy Ross Bridge.\n1966Pennsylvania and New Jersey approve plans for construction of the Commodore Barry Bridge.\nSeptember 7, 1967The Delaware River Port Authority establishes a subsidiary unit, the Port Authority Transit Corporation (PATCO), to operate a high speed commuter rail line.\nJanuary 4, 1969The first PATCO Speedline train runs between Lindenwold and Camden, N.J. PATCO represents a new generation in urban transit systems: It is highly automated, using only one operator per train and providing ticket vending machines in all stations. Nevertheless, PATCO quickly earns the reputation as a customer-friendly system and comes to be called the most efficient and dependable public transit system in the United States.\nFebruary 15, 1969PATCO service is extended from Camden to Center City, Philadelphia on the tracks of the previously operating Bridge Line.\nJune 26, 1969Construction begins on the Commodore Barry Bridge, linking Chester, Pa., and Bridgeport, N.J. The bridge is named after Revolutionary War hero and father of the American Navy John Barry, an Irish immigrant who lived in Philadelphia.\nJuly 31, 1969Construction begins on the Betsy Ross Bridge, linking Northeast Philadelphia with Pennsauken, N.J. Again the DRPA makes history: the Betsy Ross Bridge is the first U.S. highway bridge named after a woman.\nFebruary 1, 1974The Commodore Barry Bridge opens and the Chester-Bridgeport Ferry runs for the last time.\n1976The Franklin Square PATCO station, unused since 1953, reopens to coincide with the Bicentennial celebration. It will close again in 1979.\nApril 30, 1976The Betsy Ross Bridge opens.\nJanuary 20, 1978The Benjamin Franklin Bridge carries its one billionth vehicle.\nNovember 1, 1988Pennsylvania Governor Robert P. Casey and New Jersey Governor Thomas H. Kean agree on a three-part program. First, the states will work to amend the DRPA\'s bi-state charter to enable it to engage in port unification and regional economic development. Second, the Authority will build an intermodal rail facility in Philadelphia. Third, the DRPA will help revitalize Camden\'s business district by constructing a new headquarters building along the Delaware River.\nJuly 3, 1990The Walt Whitman Bridge carries its one billionth vehicle.\nJuly 3, 1991At a “Golden Spike” ceremony, the DRPA takes the first step in fulfilling the mandate set in 1988. The ceremony marks the start of construction for AmeriPort, a regional intermodal transfer facility that will improve the flow of containerized cargo through the port.\nOctober 1, 1992One-way tolls take effect on all four DRPA bridges, as well as the Delaware Memorial Bridge, Tacony-Palmyra Bridge and Burlington-Bristol Bridge. The move to westbound-only toll collection, which reduces traffic congestion on the bridges and approach roadways, is immediately popular with commuters.\n1992The states of New Jersey and Pennsylvania and the U.S. Congress approve a new Delaware River Port Authority compact, which broadens the agency\'s mandate in the fields of port enhancement and economic development.\nOctober 27, 1992President George H.W. Bush signs the bill that officially amends the bi-state compact as agreed to earlier in the year. The new compact not only broadens the responsibilities of the Delaware River Port Authority, it expands its area of responsibility to include Bucks, Chester and Montgomery counties in the Pennsylvania.\nDecember 11, 1992A severe winter storm threatens to topple the steeple of St. Augustine\'s Church onto the deck of the Benjamin Franklin Bridge. For safety reasons, the bridge is closed for part of three days while workers struggle through high winds to remove the steeple.\nFebruary 1993Containers unloaded from an Australian ship move to the newly completed AmeriPort facility, where they are loaded onto railroad cars for transport to Canada. Soon rail cars from CP Rail, CSX and Conrail are moving in and out of AmeriPort, making it the Northeast\'s only three-railroad intermodal facility. In response to increasing cargo demands, the DRPA develops plans to expand AmeriPort.\n1993Under its new economic development mandate, the Delaware River Port Authority begins investing in public improvements and private-sector initiatives. The program funds public attractions such as Penn\'s Landing and the Camden Aquarium, and makes low-interest loans to help the expansion of Philadelphia\'s American Street Enterprise Zone and other projects.\nMay 12, 1994Pennsylvania Governor Robert P. Casey and New Jersey Governor Christine Todd Whitman formally approve the unification of the Delaware River ports. The new agency, called the Port of Philadelphia and Camden, functions as a DRPA subsidiary with its own bi-state board of directors. James Weinstein is elected the first chairman.\nNovember 16, 1994Construction begins on One Port Center, an office building on the Camden Waterfront designed by world-famous architect Michael Graves. The building will be the headquarters of the Delaware River Port Authority. The opening will mark the completion of all three goals outlined by the governors in 1988.\n1996DRPA debuts on the World Wide Web with the launch of www.drpa.org. The website will provide historical background on DRPA facilities as well as up-to-date travel information.\nMarch 1996One Port Center opens. The 11-story, 175,000 square foot, Class-A building is soon fully occupied.\nOctober 22, 1997Joining a federal-state-city effort, DRPA helps forge an agreement to convert the former Philadelphia Navy Yard to a civilian shipbuilding center.\n1998PATCO becomes the first public transit system in the United States to meet its goals under the Americans with Disabilities Act (ADA).\nMay 25, 1998In a Memorial Day ribbon-cutting, DRPA opens the Philadelphia Cruise Terminal at Pier 1 in a renovated naval factory built in 1874, one of the oldest buildings at the shipyard. The terminal is intended to allow for larger ships than those that had previously docked at Tioga Marine Terminal’s cargo wharves. Cruises will depart mostly for New England and Bermuda, with the peak year in 2006, when a record 36 cruises will depart.\nNovember 1999DRPA signs its first contracts to lease fiber-optic lines across the Benjamin Franklin Bridge.\nDecember 18, 1999All four bridges turn on the E-ZPass Electronic Toll Collection System.\n2000A seven-year, $92 million project begins, in which workers will strip nearly 75 years’ worth of paint from the Ben Franklin Bridge and repaint the span in its trademark shade of blue. The many older layers of paint have gotten so thick that they are starting to peel off the bridge. Because many of those older layers used lead-based paint, it is necessary to isolate each section of the 5 million square feet of surface when it is stripped. About 25,000 gallons of primer and 35,000 gallons of paint will be used in the project.\nApril 1, 2000DRPA assumes operations of the MV RiverLink, a ferry that carries visitors between Penn\'s Landing in Philadelphia and the Camden waterfront in New Jersey seven days a week. Passengers enjoy a scenic 20-minute cruise as they cross the Delaware River. Express service is available before concerts on the Camden Waterfront.\nJuly 30, 2000A recently upgraded decorative lighting system on the Ben Franklin Bridge showcases the span during the opening reception of the Republican National Convention.\nDecember 2000The first “zipper” machines, which move concrete barriers that separate traffic in opposing directions, are installed on DRPA bridges. It takes about 20 minutes for the machines to shift the entire barrier — made up of 1500-pound segments — from one lane to another.\nJuly 1, 2001The DRPA celebrates the 75th anniversary of the Benjamin Franklin Bridge by closing the span to all but pedestrian traffic as thousands of people, some in 1920s attire, cross the span. Vintage cars, trucks and fire engines are on display, as are two of the 1930s-era Bridge Line subway cars. The newly restored “Winged Victory” statues, which adorned pylons at either end of the bridge when it opened in 1926, are on public view for the first time in more than 50 years. Tours of the anchorages are provided.\nMarch 29, 2003The RiverLink Ferry System welcomes a double-ended ferry, the MV Freedom, to its fleet. Singer Patti LaBelle christens the ship before its maiden voyage, with area schoolchildren aboard.\nJuly 19, 2004DRPA merges E-ZPass operations on its bridges with New Jersey E-ZPass, providing customers with enhanced service and online account access.\nSeptember 2004DRPA joins six other area transportation agencies to host the 72nd Annual Meeting of the International Bridge Tunnel and Turnpike Association (IBTTA) in Philadelphia. The meeting draws more than 1,100 attendees from more than 20 countries, making it the largest IBTTA meeting in nearly 10 years.\nJune 21, 2005The Philadelphia Cruise Terminal hosts the Norwegian Cruise Line’s Pride of America as she cruises the historic ports of the eastern United States on her maiden voyage.\nMay 2006A pilot program is launched to test the new PATCO fare collection system, which uses smart card technology.\nMay 9, 2006The DRPA’s Marine Unit launches a 27-foot patrol vessel that will be used in the unit’s work helping the Coast Guard and local law enforcement agencies patrol the waters of the Delaware River. The $200,000 boat was purchased with federal Homeland Security funds.\nNovember 2007PATCO launches its Transit Ambassador Program, placing uniformed representatives on trains and in stations, concourses and parking lots, primarily in the evening hours.\nMarch 2008PATCO fully implements the FREEDOM Card fare collection system.\nOctober 31, 2008PATCO sets a one-day ridership record with more than 112,000 passengers attending the Philadelphia Phillies World Series victory parade.\nJanuary 2009PATCO launches FREEDOM to Save, a program in which Philadelphia and South Jersey businesses offer discounts to FREEDOM Card holders.\n2010The DRPA and PATCO create a Citizens Advisory Committee (CAC) to enlist the participation of customers who use the DRPA bridges or ride PATCO. The 24-member CAC is an independent advisory group with 12 members from Pennsylvania and 12 from New Jersey.\nJuly 15, 2010The DRPA board approves on a three-year, $140 million project, redecking the Walt Whitman Bridge — the largest capital project in the Authority’s history. The redecking project includes the removal of the suspended span; installation of a new lightweight, concrete-filled jointless grid deck; structure improvements; and new parapets, effectively building a whole new bridge without the cost of new construction. A new movable barrier, used to separate traffic moving in opposite directions, is installed.\nDecember 31, 2010CruisePhilly and the Philadelphia Cruise Terminal officially close after 12 years of operation. The closure reflects changes in the cruise industry, which increasingly relies on mega-ships (those carrying 4,500-5,000 passengers), which are too large to pass under the Delaware Memorial Bridge. The sale of the last liner home-ported in Philadelphia, the Norwegian Cruise Line’s Majesty, in 2008 was a major blow. The closure will save $600,000 in operating costs, $2 million in lease and other costs, and an additional $18 million in future capital improvements.\nAugust 2011Work begins on the three-year Walt Whitman Bridge redecking project, approved the previous year.\nDecember 2011The DRPA Board ends the economic development part of the Authority’s mission. Instead, the Authority will emphasize its obligation to maintain the transportation assets under its control.\nOctober 2012Superstorm Sandy, the second-costliest hurricane in U.S. history, hit the East Coast. All four river bridges are closed during the height of the storm, but the Ben Franklin Bridge is the first major bridge to reopen in its wake.\nJanuary 2014Work begins on a two-year, $103 million project to replace the entire PATCO track infrastructure on the Ben Franklin Bridge. The project will involve replacing six miles of welded rails and 30 miles of signal, power, and communications cable.\nSummer 2014The three-year Walt Whitman Bridge redecking project is completed. The New Jersey Alliance for Action, a nonprofit advocate for infrastructure investment in New Jersey, recognizes the project as a New Jersey Leading Infrastructure Project.\nJanuary 2015The DRPA Board approves the sale of the RiverLink ferry for $300,000. The purchase will be a joint venture between the Delaware River Waterfront Corporation and the Cooper’s Ferry Partnership. The ferry began summertime operation between Penns Landing and the Camden waterfront in 1992. The DRPA assumed control in 2000, but has outsourced operation to private operators since 2004.\nThe DRPA Board revises the Authority’s mission statement, changing it to read “As stewards of public assets, we provide for the safe and efficient operation of transportation services and facilities in a manner that creates value for the public we serve.”']"	['<urn:uuid:b18c86da-4c72-4fc0-a6fe-e5facb416c62>', '<urn:uuid:b3295355-c1f0-47a8-ade5-4af6b63b610d>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T19:10:11.825225	12	60	3311
