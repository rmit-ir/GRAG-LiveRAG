qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	Which issues do institutions face when archiving research data?	Common issues include finding places for long-term data access, making data discoverable, reusable, and reproducible, determining appropriate metadata and identifiers, establishing archiving and preservation policies, and securing funding for data management.	['Managing Research Data: Some Ins and Outs\nJoyce Ray, Johns Hopkins University\nGeneva Henry, George Washington University\nMichele Kimpton, DuraSpace\nMelissa Levine, University of Michigan\nBased on a book: Research Data Management – Practical Strategies for Information Professionals.\nOverview of the volume:\n– Policy context\n– Managing active data\n– archiving and managing data long-term\n– measuring success\n– case studies\n– what’s next\n– planning is essential and ongoing\n– essential infrastructure goes beyond software and your own institution – it includes tools, services, policies, and communities of practices\n– collaboration internally and externally helps to maximize insittutional investment\n– value of managing research data is not yet proven\nData Curation for the Humanities (based on work done at Rice)\n– Where’s the data in humanities research?\nDigital content enables structure to be added to otherwise unstructured resources (metadata, OCR, text markup, georeferencing, 3d recreations of space)\nData enables new research never before possible (social network analysis to discover historic relationships, time and space anlysis, closer inspection of historic sites, content analysis for frequency of terms across multiple works)\n– Creating sustainabile digital content and what it takes to curate it\nDigital content is powerful but enhancements must be maintained and reusable. Balance in extreme markup/annotation vs. minimal additions of interpretations; managing elements associated with a work as separate objects. Example of migrating content across newer versions of TEI.\n– teams and infrastructure\nDomain expertise and technical expertise needed for success – partnering between academic faculty and librarians and technologists is powerful. Don’t go in with the attitude that this is just a service provided – scholars should be involved in considering markup. Opportunities for learning new skills. Platforms that can handle varied content.\n– project case studies – Our Americas Archive partnership; travelers in the middle east archive; shepherd school of music collection; Rice ephemera archive; Houston Aiian American Archives Oral Histories collection.\nMichele Kimpton – Archiving research data in the cloud or in a local repository\nDid a survey of people in the Duraspace community around practices and use cases.\nCommon issues: Where can I put my data for long term access; How do I make it discoverable, reuseable, reproducible?; What metadata, provenance, and identifiers should I use? (very much an emerging set of practices); What policies should be in place for archiving and preserving data? (multiple locations? cost associated); How do I fund this?\nData management in DSpace – new features in DSpace 5.0 related to data management and archiving (coming out end of this month). DOI support – EZID, ORCID integration, linked open data support, integrated with DuraCloud.\nData management in Fedora – last week Fedora 4 became available. Supports linked open data; content modeling; versioning; large files; fixity checking; external, asynchronous storage.\nDataOne project -humanities, social sciences, earth science. 80% of files are excel or comma-delimited – the long tail of data.\nCommercial based cloud solutions – Attract end users because solves immediate need without adding a ton of work to end user. Share, collaborate,or meet mandate by publisher or funding agency; Little to no preservation practices in place; No stated or unstated longerm data management practices; long term at risk reliant on investors interest and success in the market; lack of trust and control in academic community.\nPublishers are paying for storage of data in Figshare.\nCommunity based cloud solutions: duracloud (in partnership with DPN and chronopolis); center for open science; dryad (UNC, based on DSpace); zenodo.\nQuestions: Is it open source? Are the policies transparent? What is the governance? Are there policies to preserve the data?\nPOWRR study from IMLS – comprehensive study of archiving tools.\nAvailability-Usability Gap – Copyright, open data and the availability-usability gap: challenges opportunities, and approaches for libraries.\nThe big ideals: “data as the new gold” – we are building the mines, can we make them safe and stable?\nPrinciples: Denton Declaration 2012. Research data when repurposed has accretive value; publicly funded research should be publicly available for public good (issues with commercial providers); transparency in research is essential to sustain the public trust; validation of research data by peer community essential to the function of responsible research; managing research data is the responsibility of a broad community of stakeholders including researchers, funders, institutions, libraries, archivists, and the public.\nStill early times:open access/data: Open access funding mandates (NIH 2008, NSF 2010, OSTP 2013); Simple things complex: making sure authors have the rights they need to deposit, can only pass on what they have to give; trending toward proactive planning in rquired data plans (data considered at the front end, well before publication).\nIssues: technology outpaces law; law is harder than technology because it’s about people; different countries differ; different disciplines differ; hiding data (squeezing the last bit of publication before sharing; disincentives to share); data citation – chain of title; public versus private interest; cost – good metadata is expensive.\nProgress: Research Data Aliiance- CODATA; DataCite; DataONE – best practicies primer; databib; ORCID; new roles for libraries as hubs of expertise – even if it’s to other parts of the enterprise.']	['<urn:uuid:339eb7ac-a0f1-43af-9e3c-d458605245c2>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	9	31	843
2	long term effects viral hemorrhagic fever vs hepatitis liver damage death rate	Viral hemorrhagic fevers can lead to life-threatening conditions including nervous system and kidney failure, though blood loss is rarely the cause of death. For hepatitis, the long-term effects vary by type: Hepatitis C leads to chronic infection in 75-85% of cases with 1-5% potentially dying from liver disease, while Hepatitis B can cause chronic infection in 90% of infants infected at birth, with 15-25% of chronically infected people dying from liver failure.	"['Viral Hemorrhagic Fevers\nWhat are viral hemorrhagic fevers?Viral hemorrhagic fevers (VHFs) are illnesses caused by several types of viruses. Some of these viruses cause mild illness. Many others lead to life-threatening diseases with no known cures. One of the best known of this group is the Ebola virus.\nViral hemorrhagic fevers are rare in the U.S. They generally occur in Africa, South American, and Asia. However, health officials are concerned that these viruses could be used in biological terrorism.\nWhat causes viral hemorrhagic fevers?\nThese illnesses are caused by viruses from 4 groups:\nWhat are the symptoms of viral hemorrhagic fevers?\nThe symptoms vary by the specific disease. Each person may also have slightly different symptoms. Various organs in the body can be affected. Symptoms often include:\n- Muscle aches\n- Loss of strength\nPeople with severe cases often show signs of bleeding. This may be under the skin, in internal organs, or from body openings such as the mouth, eyes, or ears. But blood loss is rarely the cause of death. These people may also have:\n- Nervous system failure\n- Kidney failure\nThe symptoms may look like other health problems. Always see your healthcare provider for a diagnosis.\nHow are viral hemorrhagic fevers diagnosed?Your healthcare provider will ask about your past health and travel history. You will also need a physical exam. Blood tests can also help in the diagnosis.\nHow are viral hemorrhagic fevers treated?Generally there is no known cure or treatment for these diseases. People with these illnesses may get supportive treatment. This may include getting fluids or assistance with breathing and pain relievers. An antiviral medicine may help some people with Lassa fever.\nCan viral hemorrhagic fever diseases be prevented?\nNo vaccines are available to prevent these diseases. Two exceptions are for yellow fever and Argentine hemorrhagic fever. As a result, the focus is on staying away from the rodents or insects that carry the virus.For rodents:\n- Control the number of rodents\n- Prevent them from entering or living in homes or workplace\n- Learn how to safely clean up nests and droppings\nFor viruses spread by ticks or mosquitoes, prevention focuses on:\n- Control mosquitoes and ticks in your environment\n- Use insect repellent, proper clothing, bed nets, window screens, and other insect barriers to avoid being bitten\nIf you are traveling to an area where there is risk for viral hemorrhagic fevers:\n- Wear long sleeves and long pants treated with permethrin\n- Use insect repellent\n- Use bed nets in areas where outbreaks are occurring\n- Avoid contact with livestock or rodents in areas where outbreaks are occurring\n- Viral hemorrhagic fevers are illnesses caused by several groups of viruses. They can be mild or life-threatening. Many have no known cure.\n- These viruses live in animals, often in rodents. People can become infected if they come into contact with an infected animal or person. They may also get the disease if they are bitten by a mosquito or tick carrying the virus.\n- No cure or treatment is available for these illnesses.\n- Prevention includes avoiding insect bites and staying away from infected rodents.\nTips to help you get the most from a visit to your healthcare provider:\n- Know the reason for your visit and what you want to happen.\n- Before your visit, write down questions you want answered.\n- Bring someone with you to help you ask questions and remember what your provider tells you.\n- At the visit, write down the name of a new diagnosis, and any new medicines, treatments, or tests. Also write down any new instructions your provider gives you.\n- Know why a new medicine or treatment is prescribed, and how it will help you. Also know what the side effects are.\n- Ask if your condition can be treated in other ways.\n- Know why a test or procedure is recommended and what the results could mean.\n- Know what to expect if you do not take the medicine or have the test or procedure.\n- If you have a follow-up appointment, write down the date, time, and purpose for that visit.\n- Know how you can contact your provider if you have questions.', 'VIRAL HEPATITIS A\nA liver disease caused by the hepatitis A virus (HAV).\nSymptoms: Jaundice, fatigue, abdominal pain, loss of appetite, nausea, diarrhea, fever (adults will display symptoms more often than children)\nLong-term effects: There is no chronic, or long-term, infection. Once someone has had hepatitis A, he or she cannot get it again. About 15 percent of people infected with HAV will have prolonged or relapsing symptoms over a six- to nine-month period.\nTransmission: Found in the stool of people with hepatitis A, HAV usually is spread from person to person by putting something in the mouth (even though it looks clean) that has been contaminated with the stool of a person who has hepatitis A.\nPeople at risk: Household contacts of infected people; sex contacts of infected people; travelers to countries where hepatitis A is common; men who have sex with men; users of injection and noninjection drugs\nVaccine available? Yes\nVIRAL HEPATITIS B\nA disease caused by a virus that attacks the liver. The hepatitis B virus (HBV) can cause lifelong infection, cirrhosis -- or scarring -- of the liver, liver cancer, liver failure and death.\nSymptoms: Jaundice, fatigue, abdominal pain, loss of appetite, nausea or vomiting, joint pain (about 30 percent of people have no signs or symptoms, and signs and symptoms are less common in children than adults)\nLong-term effects without vaccination: Chronic infection occurs in 90 percent of infants infected at birth, 30 percent of children infected at age 1 to 5, and 6 percent of people infected after 5 years of age. Death from chronic liver failure occurs in 15 percent to 25 percent of chronically infected people.\nTransmission: Occurs when blood from an infected person enters the body of a noninfected person. HBV is spread by having unsafe sex with an infected person; by sharing drugs, needles or ""works"" when injecting drugs; through needle sticks or sharps exposure on the job (as with health care workers); or via transmission from an infected mother to her baby during birth.\nPeople at risk: Those who have multiple sex partners or a diagnosis of a sexually transmitted disease; men who have sex with men; sexual contacts of infected people; injection drug users; household contacts of chronically infected people; infants born to infected mothers; infants or children of immigrants from areas with high rates of HBV infection; health care and public health care workers who are exposed to blood; hemodialysis patients.\nVaccine available? Yes. Other preventive measures include not sharing personal items (such as razors or toothbrushes) that may have blood on them and weighing the risk of tattooing or body piercing (HBV can be transmitted through tools that have someone else\'s blood on them or by unhygienic piercing or tattooing practices).\nVIRAL HEPATITIS C\nA liver disease caused by the hepatitis C virus (HCV).\nSymptoms: Jaundice, fatigue, dark urine, abdominal pain, loss of appetite, nausea.\nLong-term effects: Chronic infection occurs in 75 percent to 85 percent of infected people; cirrhosis occurs in 20 percent of chronically infected people; and 1 percent to 5 percent of infected people may die from chronic liver disease. HCV is the leading indication for liver transplants.\nTransmission: Occurs when blood from an infected person enters the body of a person who is not infected. HCV can be spread through sharing needles or ""works"" when shooting drugs and through needle sticks or sharps exposures on the job (for example, health care workers), and can be caused by an infected mother passing on the virus to her baby during birth. (Locally, it\'s suspected that possible new HCV cases may be linked to the improper use of syringes and single-use vials of anesthetic medications during colonoscopies.)\nPeople at risk: Users of illegal injectable drugs and recipients of blood clotting factors made before 1987 (both high risk of infection); hemodialyis patients, recipients of blood and/or solid organs before 1992, people with undiagnosed liver problems and infants born to infected mothers (all intermediate risk of infection); and health care/public safety workers, people who have sex with multiple partners, and people having sex with an infected steady partner.\nVaccine available? No. Prevention includes not shooting drugs; not sharing personal items that may have blood on them; for health care workers, following routine barrier precautions and handling needles safely; being aware of the risks of body piercing or tattooing; using safe sex practices (while HCV can be spread by sex, it\'s rare); and, for those who are HIV positive, not donating blood, organs or tissue.\nVIRAL HEPATITIS D\nA liver disease caused by the hepatitis D virus (HDV), which needs the hepatitis B virus to exist. It can be acquired either as a co-infection with the hepatitis B virus, or as a superinfection (a reinfection or second infection with another virus) in people with an existing chronic hepatitis B infection alone.\nSymptoms: Jaundice, fatigue, abdominal pain, loss of appetite, nausea or vomiting, joint pain or dark (tea-colored) urine.\nLong-term effects without vaccination: People who have an HBV/HDV co-infection may have more severe acute disease and a higher (2 percent to 20 percent) risk of developing acute liver failure compared with those infected with HBV alone. Progression to liver cirrhosis is believed to be more common with HBV/HDV chronic infections.\nTransmission: Occurs when the blood from an infected person enters the body of a person who is not immune. HDV can be spread by having unsafe sex with an infected person; by sharing drugs, needles or ""works"" when shooting illegal drugs; through needle sticks or sharps exposures in health care jobs; or be passed from an infected mother to her baby during birth.\nPeople at risk include: Users of illegal injectable drugs; men who have sex with men; hemodialysis patients; sex contacts of infected people; health care and public safety workers; and infants born to infected mothers (although that\'s very rare)\nVaccine available? Hepatitis B vaccine should be given to prevent HBV/HDV co-infection.\nVIRAL HEPATITIS E\nA liver disease caused by the hepatitis E virus (HEV). Hepatitis E is uncommon in the United States\nSymptoms: Jaundice, fatigue, abdominal pain, loss of appetite, nausea, vomiting, dark (tea-colored) urine\nLong-term effects: No chronic, or long-term, infection (but note that hepatitis E is more severe among pregnant women, especially in the third trimester).\nTransmission: Found in the stool of people and animals with hepatitis E, HEV is spread by eating or drinking contaminated food or water. Transmission from person to person occurs less commonly than with the hepatitis A virus. Most outbreaks in developing countries have been associated with contaminated drinking water.\nPeople at risk: Travelers to developing countries, particularly in South Asia and North Africa. Rare cases have occurred in the United States among people with no history of travel to endemic countries.\nVaccine available? No. Prevention includes washing hands with soap and water after using the bathroom, changing a diaper, and before eating or preparing food; avoiding drinking water (and beverages with ice) of unknown purity, uncooked shellfish, and uncooked fruits or vegetables that are not peeled or prepared by the traveler.']"	['<urn:uuid:16c62ba9-eef3-486a-9b6e-72cc8066d7c4>', '<urn:uuid:2414aa04-3099-4d37-b508-64540c92d994>']	factoid	with-premise	long-search-query	similar-to-document	three-doc	novice	2025-05-13T04:56:11.774636	12	72	1876
3	What is the difference between determinate and indeterminate tomatoes?	Determinate tomatoes are bushy plants that grow to 3-5 feet, bearing fruit for 4-6 weeks before declining. Indeterminate varieties produce vines that continue growing and setting fruit all summer until killed by cold or disease.	"[""You Can Grow Tomatoes in a Shady or Foggy Garden\nA Shady or Foggy Garden? You Can Grow Tomatoes!\nA Shady or Foggy Garden? – You Can Grow Tomatoes!\nWell, it’s that time of year when we are happily preparing our garden beds and pots for our vegetable planting! We are either gathering our purchased vegetable seedlings or transplanting the seedlings we have grown – or some of both. And among those are tomatoes - unless perhaps you are one of the many living in the cool coastal or wooded areas of Alameda County. If you fit in that category, you may be thinking “There’s no way I can grow tomatoes!” or “Tomatoes have never worked in my yard!” If that sounds like you, there is likely more hope than you think, and this article is for you!\nNow it may be true that if you live in a very heavily wooded area, with no sunny corner in any part of your yard or patio or porch, you may not be able to grow sun-loving vegetables like tomatoes. But if you have sun anywhere outside your home, even if for only part of the day, please read on! There are tomato varieties that have been developed just for your situation.\nThe key to growing tomatoes in cooler climates is to grow varieties that have a relatively short number of days to maturity or that produce a harvest in cooler temperatures. Fortunately, there are now a number of these tomato varieties, and with some experimentation, you will likely find one that works for you. For best results, plant your tomatoes in the area of your yard that received the most hours of sun. This may be an area in your yard or even in containers on your patio.\nDays to Maturity\nDays to maturity (DTM) means the number of days from planting a seed to when the seedling grows into a mature plant and begins to produce a harvest. The range for “days to maturity” for tomatoes is about 55 to 100 days, and of course, depends upon the variety grown. For best results in cooler climates, try tomato varieties with 55-75 days to maturity. Although there are exceptions, many of these varieties have been found to do best in cooler locations. In addition, tomatoes often take longer to mature in cooler climates. Using varieties with shorter warm weather maturities will give your harvest a longer time to ripen in your cooler climate.\nVarieties that Produce in Cooler Climates\nUniversity of California researchers have evaluated several tomato varieties for growth and production in three different climatic zones in our state, designated as A, B, and C. They have recommended varieties for use in the zones in which they proved successful. (Many other popular tomato varieties have yet to be evaluated in this way).\nThe coolest of these zones is “Zone C”. This Zone includes the central and northern coastal areas, the cool coastal valleys from Santa Maria north to the Oregon border, the San Francisco Peninsula, and areas with direct exposure to San Francisco Bay, the northern coastal foothills, and other areas with cool to moderate summers and evening temperatures frequently in the 45° to 55° F range. Alameda County cities located in Zone C include Alameda, Oakland, and Berkeley.\nListed below are some of the tomato varieties the researchers found did well in Zone C, sorted by three types: small cherry tomatoes, standard size, and container tomatoes. Of course, your own results will depend upon the microclimate in your own neighborhood, and even within your own yard. Some tomato varieties produce well in large pots, so if that is your warmest spot try the container varieties below.\nSweet 100: Gardener's Delight: Sweet Chelsea\nImages and information courtesy of San Mateo/San Francisco Master Gardener at https://www.notion.so/Tomatoes-344ebbc109e64203b233d7f7eae2acb0\nStandard Size Tomatoes\nEarly Girl: Stupice: Early Wonder\nImages and text boxes reprinted from San Mateo/San Francisco Master Gardener at https://www.notion.so/Tomatoes-344ebbc109e64203b233d7f7eae2acb0\nImage courtesy of Mississippi State Extension http://extension.msstate.edu/news/southern-gardening/2008/interest-patio-gardens-spreads\nBetter Bush VFN\nVery early, Small to medium size cherry\nSmall, red cherry tomato\nVery early. Small red, cherry tomato\n|All Zone: A, B, C|\nDeterminate vs Indeterminate Tomatoes\nYou’ll notice that each of the varieties above state whether they are “determinate” or “indeterminate”. Tomatoes are described as determinate or indeterminate based on the plant’s growth habit.\nDeterminate, or “bush,” tomato plants have a somewhat bushy appearance, grow to a given size, (about 3 to 5 feet), bear most of their fruit within about 4 to 6 weeks, and then decline in vigor. Most early-ripening and canning tomato varieties are of the determinate type. All the above recommended container tomatoes are determinate.\nIndeterminate varieties produce vines that continue to grow and set fruit all summer until they die from cold weather or disease. Many typical standard-sized tomatoes for the home garden are indeterminate varieties. The cherry and standard tomatoes recommended above for cool weather growing are all indeterminate.\nLeft to grow without support, indeterminate plants tend to sprawl on the ground, leaving the fruit susceptible to rot where it comes in contact with the soil. So support these plants with sturdy trellises, cages, or stakes.\nGrowing Your Tomato Plants\nTry two or more varieties; experimentation is fun and the best way to find the tomato that grows best in your yard! Once you planted them, whether from seed or seedling transplants, growing tomatoes is fairly easy. The links below have much more information on the care, feeding, watering and pest management for your tomatoes:\nFrom the San Mateo/San Francisco Master Gardener webpages\nStill need help?\nWould like more information? Email us at email@example.com. Or contact us through our website http://acmg.ucanr.edu/Contact_Us/ .""]"	['<urn:uuid:ffd1078b-a302-48fa-8304-39b55c5fe5a5>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	9	35	944
4	compare health benefits blueberries cranberries antioxidants cancer prevention research findings	Both blueberries and cranberries have significant health benefits related to antioxidants and cancer prevention. Cranberries contain polyphenolics including anthocyanins that reduce oxidative stress, and compounds that can inhibit various cancers including colon, breast and prostate cancer. Blueberries contain phytochemicals and nutrients with potential anti-cancer effects, increase antioxidant activity in blood, show potential to prevent DNA damage, and may help prevent skin cancer development. Additionally, both berries offer cognitive benefits - cranberries may help protect against conditions like Alzheimer's, while blueberries can reduce age-related cognitive decline.	"['Cranberry fruit and products are the third most important export from the state of Massachusetts with a significant impact on the economy of Southeastern MA.\nConsumption of cranberries and cranberry products such as juice, sauces, sweetened-dried fruits and supplements has grown substantially in recent years due to the established urinary health benefits and many other potential health-promoting effects including oral health, antioxidant, anti-aging, anti-microbial and protection against cancers and cardiovascular diseases.\nCranberries are closely related to other healthy fruits including lowbush blueberry (Vaccinium angustifolium), lingonberry (Vaccinium vitis-ideae) and bilberry (Vaccinium myrtillus). Consumption of these berries has been linked to improved cognitive function, cardiovascular health and night vision.\nPotential health benefits\nConsumer interest in cranberries has increased over the past decade due to its proposed health-promoting effects, giving cranberry the status of a ""functional food"". The most well-known application of cranberry and cranberry products is in the maintenance of urinary tract health. Compounds in cranberries and their metabolites can prevent the adhesion of E. coli bacteria, associated with urinary tract infections to the urinary tract wall. These properties may also prevent infection by bacteria associated with oral and periodontal diseases and stomach ulcers. Other potential health benefits include protection against certain cancers, cardiovascular diseases, pathogenic fungal infection, diabetes, and other disease states related to oxidative stress and inflammation such as Alzheimer’s disease, Parkinson’s disease and stroke. The health benefits of cranberry and other fruits have been a priority for major federal funding agencies including NIH and USDA.\nWhat’s good for you in cranberries?\nThe health benefits of cranberry are associated with a diverse group of phytochemicals. Cranberries are rich in polyphenolics, including the deep red anthocyanins that give the berries their color and may play a role in reducing oxidative stress, and oligomeric proanthocyanidins (PACs) that have been associated with urinary tract and oral health benefits. The fruit also contains the natural anti-inflammatory ursolic acid, carotenoids such as lutein which may promote eye health, and the antioxidant quercetin which may be produced by the plant in defense against stress. These compounds reduce cell proliferation associated with certain cancers, scavenge free radicals that cause damage to biomolecules and tissues, and may fight microbial infections. Other potentially beneficial compounds in the cranberry plant are under investigation.\nResearch on Cranberries and Health at UMass Dartmouth\nNeto Research Group:\nThe research program of Catherine Neto focuses on the chemistry of biologically active compounds from the cranberry plant (Vaccinium macrocarpon), their anti-cancer, antioxidant and antimicrobial properties, and their potential roles as functional food factors in disease prevention and nutrition. A closely related topic is the development of cranberry secondary metabolites in response to stress. The Neto group has developed effective MS, HPLC and NMR analysis methods for cranberry extracts, and chromatography methods to isolate and characterize flavonoid and other phenolic constituents including oligomeric proanthocyanidins, and isoprenoid metabolites. Together with our collaborators, we investigate the ability of these compounds to inhibit human bacterial and fungal pathogens; their mechanisms of action against colon, breast and prostate cancer; neuroprotective and cardioprotective properties; and the dual roles of these metabolites in protection of both the consumer and the plant itself.\nThe Neto research group has active collaborations with scientists at UMass Dartmouth (Tracie Ferreira, bioengineering, mechanisms of cancer prevention; Maolin Guo, biochemistry, role of iron in antioxidant action; Frank Scarano, microbiology, inhibition of oral bacteria and pathogenic yeasts); UMass Amherst (Hang Xiao, food science, colon inflammation and cancer); the University of Prince Edward Island in Canada (Robert Hurta, cancer biology, prostate cancer; Marva Sweeney, biology, cardiovascular disease); Worcester Polytechnic Institute (Terri Camesano, chemical engineering, bacterial biofilm inhibition); Bruker-Biospin (Kim Colson, NMR-based metabolic fingerprinting); the UMass Cranberry Experiment Station (Frank Caruso, Erika Saalau-Rojas, plant pathology) and Cornell University (Lori Bystrom, cancer biology; Justine Vanden Heuvel, plant physiology). These projects have been funded by the USDA, NCCAM, the Cranberry Institute, the National Cancer Institute of Canada, Atlantic Canada Opportunities Agency, Bruker Biospin and UMass Dartmouth.\nGuo Research Group:\nThe research program of Maolin Guo is in the interface between chemistry, biology and medicine. The Guo lab uses a multidisciplinary approach to investigate the molecular basis of cranberry (Vaccinium macrocarpon) in modulating metal ion homeostasis, oxidative stress and its related neurological disorders, cancer, diabetes, and infectious diseases. The research goal is not only to study the mechanisms of action, but also to develop novel technology for research, diagnosis or therapy. Guo lab research involves chemical synthesis of functional molecules, specific tagging of proteins and natural products, cloning, expression and engineering of novel target proteins and enzymes, enzymatic assays, spectroscopy, x-ray crystallography, bacterial and mammalian cell cultures and fluorescent confocal imaging of live cells.\nThe Guo research group has proposed that modulating of iron homeostasis plays a vital role in the bioactivities of cranberry polyphenols and related compounds in other herbal medicine. The Guo group has developed molecular probes capable of real-time imaging metal ions, free radicals and polyphenols in live cells. By using these novel tools, Guo lab investigates how these compounds entering cells and the mechanisms of their anti-bacterial, anti-oxidant, anti-diabetic, anti-aging, anti-cancer, neuroprotective and cardioprotective properties at cellular and molecular levels.\nThe Guo research group collaborates with researchers from both academia and industry (including but not limited to): Dr. Cathy Neto (cranberry phytochemistry, UMass Dartmouth), Dr. John Keaney, cardiovascular medicine, UMass Medical School), Dr. Gang Han (pharmacology and bioimaging, UMass Medical School), Dr. Wei Chao (cardiovascular diseases, Harvard Medical School/Mass General Hospital), Dr. David Lee (natural products, Harvard Medical School/McLean Hospital), Dr. Terri Camesano (bacterial biofilm inhibition, Worcester Polytechnic Institute), Dr. Boxin Ou (natural products and oxidative stress, Brunswick Laboratories Inc.), Dr. Lori Bystrom (natural products and cancer, Weil Cornell Medical College), Dr. Michael Brownlee (diabetes, Albert Einstein College of Medicine), and Dr. John Lemasters (ischemia reperfusion, Medical University of South Carolina). Guo lab research has been funded by the NIH/NCCAM, National Parkinson Foundation, the American Parkinson Disease Association, UMass Science & Technology Initiative, UMass Cranberry Research Program and UMass Dartmouth.', 'Cancer-fighting foods | & nbsp Image source: & nbspiStock Images\nthe main points\n- Food must be given its sanctity. Food can make or break our health.\n- Cancer has become a major cause for concern as fast-paced lifestyles have left little or no time for people to focus on what they should eat.\n- Researchers are looking for fruits, vegetables, and other plant foods to help prevent and treat a variety of cancers.\nThe modern era of cancer treatment is constantly evolving, with new breakthroughs and discoveries changing the course of care all the time. Researchers are working hard, trying to come up with modern cancer treatments.\nBut there is no denying that the C word is a huge shock to anyone and their loved ones. What does no one do to ensure that this disease is not infected at all? With the disease a major cause of death worldwide, scientists are looking for ways to find out why it developed, how it spreads, and what could prevent it from taking root in the first place?\nThe attention of clinicians and researchers is now focused on food. Is there something in the food that causes or promotes it? Even better, are there any foods one can consume regularly to keep cancer away?\nBioactive compounds found in vegetables, fruits, and other plant foods have received great attention for their potential anti-cancer properties.\nAccording to the American Institute for Cancer Research (AICR), there is no single plant or plant that is likely a magic bullet for preventing cancer. But the good news is that you can eat some foods that show anti-cancer potential. And of course, instead of eating only one type of food, it is better to eat a variety of foods, colors, and foods that are rich in nutrients. Research shows that a diet full of a variety of vegetables, fruits, whole grains, beans and other plant foods helps reduce the risk of many types of cancer. In laboratory studies, several minerals, vitamins, and phytochemicals have shown anti-cancer effects,” Dr. Naveed Saleh, MD, MSc AICR quotes in piece for MDLinx.\nAccording to research conducted by the Department of Nutrition and Food Sciences, Texas A&M University, College Station, Texas, USA, dark sweet cherry phenols (Prunus avium) enriched in anthocyanins show enhanced activity against more aggressive breast cancer subtypes without toxicity to normal breast cells. MDLinx states that the anti-cancer effects of foods may be particularly relevant in preventing pancreatic cancer — otherwise called the silent killer. Pancreatic cancer is an aggressive and fatal disease that kills nearly half of those diagnosed with it within 6 months. Due to its asymptomatic nature in the initial stages, early diagnosis and treatment are challenging. Cherries and sour cherries, as well as berries and apples, are rich in polyphenols. According to the authors of the review published in Antioxidants, “A large number of polyphenols have been shown to have potent antitumor, anti-inflammatory, antioxidant, and proapoptotic effects on cell cultures of different human cancers.” (https://www.mdpi.com/2076-3921/9/6/547/htm) But the flip side of the findings is also that polyphenols are not well absorbed in the human digestive system and are rapidly metabolized and excreted. Therefore, future research could focus on a form of the drug that would allow for better absorption and bioavailability of the polyphenols. The authors concluded that the application of nanoparticles is one approach to improving the bioavailability of polyphenols. Research also indicates that cherries not only have anti-cancer properties, but also contain anti-aging elements.\nAccording to the American Institute for Cancer Research (AICR), blueberries contain several phytochemicals and nutrients that show potential anti-cancer effects in laboratory studies. Several studies have found that eating blueberries increases the activity of antioxidants in the blood and also shows the potential to prevent DNA damage. Oxidative stress from prolonged exposure to UV rays plays a role in the development of skin cancer. Antioxidants derived from natural products including blueberries (https://www.mdlinx.com/article/you-d-never-guess-these-things-can-fight-cancer/lfc-2910) have been researched for their roles. preventive measures in the development of this disease, reports MDLinx. According to a review published in Cell.com, “These biological responses suggest that a chemopreventive option against skin cancer could be including these substances in the daily diet.” Besides fighting cancer, blueberries are among some of the fruits believed to be beneficial in reducing age-related cognitive decline.\nThe MDLinx report says that the bioactive compound bergamottin is found in the pulp of grapefruit and citrus pomelo. This type of furanocoumarin is a widely known inhibitor of cytochrome P450. “Recent studies have demonstrated the powerful antioxidant, anti-inflammatory, and anti-carcinogenic properties of grapefruit furanocoumarin both in vitro and in vivo,” according to the authors of the review published in International Journal of Molecular Sciences. The authors specifically cited in vitro research indicating anticancer effects of bergamot in relation to glioma, breast cancer, multiple myeloma, prostate cancer, liver cancer, as well as skin and lung cancer. But the Mayo Clinic cautions that you shouldn’t self-medicate or start a grapefruit treatment if you’re already taking any type of medication. Grapefruit and some other citrus fruits, such as Seville oranges, can interfere with several types of prescription medications. Problems arise because the chemicals in the fruit can interfere with enzymes that break down (metabolize) the drug in the digestive system.\nHarvard doctors call it the jewel among the fruit. The Cleveland Clinic quoted nutritionist Julia Zumpano, who says that pomegranate is a fruit full of healing elements and immune-boosting properties. Pomegranate contains antioxidants that help protect cells from environmental toxins such as pollution and cigarette smoke, and prevent and repair DNA damage that can lead to cancer. Zompano adds that pomegranate juice alone won’t prevent cancer, but studies suggest it may be a nutritious addition to a healthy vegetarian diet like the Mediterranean diet. The Harvard Report notes that studies suggest that pomegranate juice may help lower levels of bad LDL cholesterol and reduce blood pressure. The authors of a systematic review published in Molecules Note that the polyphenols from pomegranate exhibit strong anti-cancer activity.\nThe cruciferous vegetable is named for its four-petaled flowers that resemble a cross or “cross,” and are hardy and cancer-resistant. According to a study published by the Department of Surgical Oncology, University of Illinois College of Medicine, Chicago – “The results clearly indicate a positive relationship between cancer prevention for several target organs and consumption of cruciferous vegetables or their active ingredients. However, we are still far from fully understanding the effects of combinations of substances The chemically protective phytochemicals present in these cruciferous vegetables and their overall mechanisms (mechanisms of action) in providing the protective effects.” Brussels sprouts, cabbage, and cauliflower are all forms of cruciferous vegetables and are rich in a class of phytochemicals referred to as isothiocyanates. UNC Lineberger (UNC University School of Medicine) says cruciferous vegetables are known for their many health benefits, including their anti-cancer properties. Broccoli is the most popular cruciferous vegetable, although there are many others. Brussels sprouts, cabbage, broccoli, kale, and dark leafy greens like cabbage and collard greens are also cruciferous. Nearly all are excellent sources of vitamins C and K, folate (a B vitamin), potassium, fiber, and magnesium. Despite the cold weather, you can still have vegetables in season. He attributes the cancer-fighting properties of cruciferous vegetables to compounds called glucosinolates, which are found in all cruciferous vegetables — which, when ingested, degrade into isothiocyanates and indoles, which are associated with reduced inflammation, reducing the risk of cancer. The National Cancer Institute (USA) claims that in vitro studies show that they:\n- They help in inactivating carcinogens.\n- Helps protect cells from DNA damage.\n- It has antiviral and antibacterial effects.\n- It has anti-inflammatory effects.\n- They induce cell death (programmed cell death).\n- They prevent the formation of tumor blood vessels (angiogenesis) and the migration of tumor cells (necessary for metastasis).\nDisclaimer: The advice and suggestions mentioned in the article are for general information purpose only and should not be construed as professional medical advice. Always consult your doctor or dietitian before starting any fitness program or making any changes to your diet.']"	['<urn:uuid:fbdf7da9-e855-453e-84fa-b1a621126c7c>', '<urn:uuid:ee24db76-52d9-4a10-a29f-b471a0325707>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T04:56:11.774636	10	85	2332
5	what religious significance ancient olympic games zeus honor sacrifices	The Olympic Games were fundamentally a religious festival to honor Zeus, the King of the Greek Gods. The games were held in the sanctuary of Olympia, which was a sacred spot featuring an altar to Zeus and olive trees from which victory wreaths were cut. On the middle day of the festival, a vast number of cows were slaughtered in honor of Zeus. According to the religious custom, Zeus was given a small taste of the sacrificed cows, and the rest was distributed to the people for a feast.	"[""Origins[ edit ] To the Greeks, it was important to root the Olympic Games in mythology. According to the story, the dactyl Heracles not to be confused with the son of Zeus and the Roman god Hercules and four of his brothers, PaeonaeusEpimedesIasius and Idasraced at Olympia to entertain the newborn Zeus.\nHere, we give you the essential lowdown, highlight our favourite facts.\nFull of blood, passion and extraordinary feats of athletic endeavour, the Olympic Games were the sporting, social and cultural highlight of the Ancient Greek calendar for almost 12 centuries.\nSo, they actually had to delay putting the army together to defend the country against the Persians. All free Greek males were allowed to take part, from farmhands to royal heirs, although the majority of Olympians were soldiers. Women could not compete or even attend.\n|E, about three thousand years ago, the first Olympic Games took place. Originally, the games were part of a religious festival to honor Zeus.|\n|Ancient Greek Olympics - the first Olympic Games in Greece||Origins[ edit ] To the Greeks, it was important to root the Olympic Games in mythology.|\nThere was, however, a loophole to this misogynistic rule — chariot owners, not riders, were declared Olympic champions and anyone could own a chariot.\nKyniska, daughter of a Spartan king, took advantage of this, claiming victory wreaths in BC and BC. At their heart, the Games were a religious festival and a good excuse for Greeks from all over the Mediterranean basin to gather for a riotous barbeque.\nOn the middle day of the festival a vast number of cows were slaughtered in honour of Zeus, King of the Greek Gods — once he had been given a small taste, the rest was for the people.\nFor the first plus years all the action took place in the sanctuary of Olympia, situated in the north-western Peloponnese. Pock-marked by olive trees, from which the victory wreaths were cut, and featuring an altar to Zeus, it was a hugely scared spot.\nThe Games lasted a full five days by the fifth century BC and saw running, jumping and throwing events plus boxing, wrestling, pankration and chariot racing.\nBut first, here are our 10 favourite facts to get you started: All athletes competed naked Wrestlers and pankration a sort of mixed martial art which combined boxing and wrestling competitors fought covered in oil Corporal punishment awaited those guilty of a false start on the track There were only two rules in the pankration — no biting and no gouging Boxers were urged to avoid attacking the on-display male genitals There were no points, no time limits and no weight classifications in the boxing Athletes in the combat sports had to indicate their surrender by raising their index fingers — at times they died before they could do this Boxers who could not be separated could opt for klimax, a system whereby one fighter was granted a free hit and then vice-versa — a toss of a coin decided who went first.The Ancient Olympic Games were religious and athletic festivals held every four years at the sanctuary of Zeus in Olympia, webkandii.comition was among representatives of several city-states and kingdoms of Ancient webkandii.com Games featured mainly athletic but also combat sports such as wrestling and the pankration, horse and chariot .\nFROM ANCIENT TO MODERN Although the ancient Games were staged in Olympia, Greece, from BC through AD, it took years for the Olympics to return.\nThe first modern Olympics were held in Athens, Greece, in The ancient Olympics seem to have begun in the early BC, in honour of Zeus. No women were allowed to watch the games and only Greek nationals could participate.\nOne of the ancient wonders was a statue of Zeus at Olympia, made of gold and ivory by a Greek sculptor Pheidias. The first ancient Olympic Games can be traced back to Olympia in BC. Full of blood, passion and extraordinary feats of athletic endeavour, the Olympic Games were the sporting, social and cultural highlight of the Ancient Greek .\nThe Ancient Olympics [Nigel Spivey] on webkandii.com *FREE* shipping on qualifying offers.\nThe word 'athletics' is derived from the Greek 5/5(2). The first ancient Olympic Games can be traced back to Olympia in BC. Full of blood, passion and extraordinary feats of athletic endeavour, the Olympic Games were the sporting, social and cultural highlight of the Ancient Greek calendar for almost 12 centuries.""]"	['<urn:uuid:14fee884-fdca-4211-9e88-c3392027dc7e>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	9	89	740
6	Which works differently: the grace period for credit card purchases or balance transfers?	Credit card purchases and balance transfers work differently regarding grace periods. Regular purchases typically have a grace period (usually around 21 days) where no interest is charged if you pay the balance in full before it ends. In contrast, balance transfers are not eligible for a grace period - interest starts accruing as soon as the balance transfer posts to your account, unless you have a special 0% introductory APR offer.	"['Learn Your 6 Debt Consolidation Options\nLooking for your best debt consolidation loan options? You are not alone. Where to find a debt consolidation loan and how to qualify for a debt consolidation loan are frequent questions for the Bills.com advice columnist, Bill. You have at least six options for a debt consolidation loan, which we outline in the table below.\nThe lenders we mention are not your only options. We included these lenders to show you examples of the rates and terms available today. You might be able to find better rates locally by shopping around. We discuss some important points about qualifying for a debt consolidation loan, and some alternatives to a debt consolidation loan below.\n|Debt Consolidation Loans & Rates From Sample Lenders|\n|Loan||Lender||Amount Available||Interest Rate||Term (months)|\n|Bank Personal / Signature Loan||Citi||$500 - $50,000||10.49% - 25.49%||24 - 60|\n|Peer-to-Peer Loan||Prosper||$2,000 - $35,000||6.73% - 35.36%||36 - 60|\n|Cash-Out Mortgage Refinance||Chase||$215,000+||2.5% - 4.125%||60 - 360|\n|Home Equity Loan||Patelco Credit Union||$5,000 - $250,000||6% - 7.817%||84 - 240|\n|Cash-Out Auto Refinance||Wells Fargo||$5,000+||4.42%+||12 - 72|\n|Credit Card Cash Advance||Discover||Depends on available balance||10.99% - 19.99%||Variable|\nDebt Consolidation Loans\nWe display a range of loans in the table above. Let\'s look at each:\nBank Signature Loan\nA signature consolidation loan does not require any collateral, which is also called ""security."" Because the loan is not tied to a vehicle or a house, the interest rate on a signature loan is higher than a secured loan. Citi, a bank we chose at random, charges from 10.49% to 25.49% interest on a personal loan. Why such a wide range? Different borrowers present different levels of risk, which we discuss below in the section ""Debt Consolidation Loan Qualifications.""\nA peer-to-peer loan is exactly what the name suggests. Two companies, Prosper or Lending Club, broker peer-to-peer loans today. The interest rates vary widely, and are based largely on the borrower’s credit score.\nCredit Card Cash Advance\nLike a signature loan or peer-to-peer loan, a credit card cash advance is not secured. Therefore, like these other loans, the interest rate you pay on a credit card cash advance is higher than secured loans — up to 20% in the case of Discover.\nHome Equity Loan\nA home equity loan is really just a fancy name for a second mortgage. Because the borrower gives the lender the right to foreclose on their home if the borrower fails to repay the loan, the interest rate for a home equity loan is low — the 6% to 7% range. Borrowers need two key elements:\n- A home or other real property\n- Equity in the home or property\nA quick and easy way to learn if you have equity in your home is to go to Web site like Zillow.com and learn the estimated market value of your home. Then look at recent statement on your mortgage, if you do not own your property outright, and find your current loan balance. Let us say your loan balance in $100,000 and your property is worth $250,000. Your loan-to-value ratio would be 100 divided by 250, which equals .4. In the mortgage business, your LTV would be 40% on this property. An LTV of less than 80% makes you a prime candidate for a home equity loan.\nCash-Out Mortgage Refinance\nA cash-out mortgage refinance is similar to a home equity loan with one exception. Instead of creating a new loan, you are asking a lender to refinance your existing mortgage for one that is larger than the current balance. The difference is paid to you in cash, which you can use however you see fit, including remodeling, paying off a vehicle loan, or paying for a college education.\nCash-Out Auto Refinance\nA cash-out auto refinance is exactly like a cash-out mortgage refinance, except that a vehicle is involved. If you own a high-value vehicle outright, consider a cash-out auto refinance.\nNow that you understand your options, let\'s see if you qualify.\nDebt Consolidation Loan Qualifications\nThe big question a potential lender asks a person seeking a debt consolidation loan is, ""Do you qualify for a loan?"" Qualification standards vary by lender and type of loan. In general, lenders look for these four qualities in a borrower:\n- Steady income history: Have you worked or received income for the last two years? If you are self-employed, is your income steady or are there vast time gaps between jobs?\n- Clean credit history: You do not need an 800 credit score to qualify for debt consolidation loan. However, the higher your score the higher your chance will be of a lender accepting your loan application. Put another way, the lower your score, the lower your chances are of qualifying for a loan.\n- Low debt-to-income ratio: Some readers write to Bill and say, ""I make $100,000 per year and have a 750 credit score, but do not qualify for a loan. Why?"" After a brief e-mail conversation, the full facts emerge and we learn that even though these readers may have excellent income and high credit scores, their existing debts frighten potential lenders.\nFor example, to qualify for an FHA mortgage, the FHA recommends a debt-to-income ratio of no more than about 40%. If you are spending half of your paycheck on student loans, car loans, and credit card payments, do not expect to qualify for a debt consolidation loan because your existing debt-to-income ratio is already very high.\n- Reserves: Do you have access to other funds if you suffer a temporary loss in income?\nThere is one other qualification if you want a cash-out refinance or home equity loan: equity. As we discussed in the section above, your LTV is a critical qualification for any refinance or a home equity loan. Without a low LTV, do not expect to qualify for a home equity loan or a cash-out refinance.\nA Very Important Question\nLet us say you offer a lender a perfect package: You have steady income, a high credit score, and low debt. Is a debt consolidation loan your best option to solve your problem? A debt consolidation loan is not a good idea if you:\n- Do not have a household budget, and\n- Need a loan to pay-off debts relating to out-of-control spending, and\n- A loan will not change your underlying problem\nLook at your situation objectively. Is the loan a fix to a temporary setback? Or is the loan a Band-Aid that pushes your problems down the road? If a debt consolidation loan is a quick fix, then look at other solutions, which we discuss in the next section.\nBest Debt Consolidation Loan Alternatives\nIf you do not qualify for a debt consolidation loan, or if a debt consolidation loan is not your best solution, consider your alternatives. Two options to examine are credit counseling and debt settlement. It is beyond the scope of this article to explain these in detail. Click on the links just mentioned to learn more about each.', 'If you’re looking for a way to save money on your credit card, it’s important to understand how APR works. APR, or annual percentage rate, is the amount of interest that you will be charged on your outstanding balance each year. It’s important to know what your APR is so that you can make sure you’re getting the best deal possible. In this article, we will teach you how to check your credit card’s APR and find the card with the lowest rates.\nHow to Check APR on Credit Card Table of Contents\nWhat is APR on a Credit Card?\nThe Annual Percentage Rate (APR) on a credit card is the interest rate charged by the issuer on outstanding balances. It can also be described as the cost of borrowing money from the credit card company. APR is expressed as a percentage and is calculated by taking into account the interest rate, any fees that may be charged, and how often interest is applied to the account.\nMost credit cards have variable APRs, which means that the rate can change over time. The APR will generally fluctuate with changes in the prime rate, which is set by banks based on current economic conditions. Many issuers will increase or decrease their rates in line with movements in the prime rate. However, some issuers may not change their rates immediately or may only change them by small amounts.\nThere are a few things to keep in mind when checking the APR on your credit card:\n- Look for both the introductory APR and the ongoing APR. The introductory APR is the rate that applies during the initial period after you open your account, while the ongoing APR is the rate that applies after that period.\n- Make sure to check how long the introductory APR period lasts. This will give you an idea of how long you have to take advantage of any intro offers before reverted back to the standard, ongoing rate.\n- Check if there is a balance transfer fee. Some credit cards charge a fee (usually around $50) when you transfer a balance from another card. This fee is in addition to any interest you’ll pay on the balance.\n- Check if there is an annual fee. Some credit cards charge an annual fee just for having the card, even if you don’t use it.\nBy taking the time to check the APR on your credit card, you can make sure that you are getting the best deal possible and avoid paying more in interest than you need to.\nWhat Are The Different Types of APR on a Credit Card?\nThere are four different types of APR on a credit card: purchase, balance transfer, cash advance, and default.\n- Purchase APR: This is the interest rate that is applied to purchases made with your credit card.\n- Balance transfer APR: This is the interest rate that is applied to any balance transfers you make with your credit card.\n- Cash advance APR: This is the interest rate that is applied to any cash advances you make with your credit card.\n- Default APR: This is the interest rate that is applied if you default on your payments or go over your credit limit.\nHow to Check APR on Credit Card\nIt’s important to know what your credit card’s APR is before you agree to open an account. This way, you can be sure that you’re getting the best deal possible and avoid any unwanted surprises down the road.\nFortunately, checking your credit card’s APR is a relatively simple process. In most cases, you can find this information online or by contacting customer service. Here’s a step-by-step guide on how to check APR on a credit card:\nIf you have an online account with your credit card issuer, log in and navigate to the account summary page. The APR should be listed here, along with other important account details. If you don’t see it, try looking for a tab labeled “Rates and Fees” or something similar.\nIf you don’t have an online account or can’t find the APR information on your account summary page, give customer service a call. They should be able to tell you what the current APR is for your credit card.\nKeep in mind that credit card issuers reserve the right to change APRs at any time, so it’s important to stay up-to-date on this information. If you see a significant jump in your APR, make sure to contact customer service and ask why this is happening. In some cases, they may be able to offer a lower rate.\nWhat Is The Average APR on a Credit Card?\nThe average APR on a credit card can vary depending on the type of card you have. For example, the average APR for a cash advance is usually higher than the APR for a purchase.\n- Purchase APR: The average purchase APR is around 15%.\n- Balance transfer APR: The average balance transfer APR is around 15%.\n- Cash advance APR: The average cash advance APR is around 25%.\n- Default APR: The average default APR is around 30%.\nWhat Is The Difference Between Fixed and Variable APR?\nVariable APR means that the interest rate on your credit card can change over time. Fixed APR means that the interest rate on your credit card will not change over time.\n- Purchase APR: The average purchase APR is usually fixed.\n- Balance transfer APR: The average balance transfer APR is usually fixed.\n- Cash advance APR: The average cash advance APR is usually variable.\n- Default APR: The average default APR is usually variable.\nHow Can I Avoid Paying Interest on my Credit Card?\nThere are a few ways that you can avoid paying interest on your credit card: by paying your balance in full every month, by using a 0% intro APR credit card, or by transferring your balance to a 0% intro APR credit card.\n- Purchase APR: You can avoid paying interest on purchases by paying your balance in full every month.\n- Balance transfer APR: You can avoid paying interest on balance transfers by using a 0% intro APR credit card.\n- Cash advance APR: You can avoid paying interest on cash advances by transferring your balance to a 0% intro APR credit card.\n- Default APR: You can avoid paying interest on defaults by making timely payments and staying within your credit limit.\nHow To Avoid Paying APR on a Credit Card\nNo one wants to pay more for their credit card than they have to. However, many people don’t know how to avoid paying APR on a credit card. Here are a few tips:\n- Know when your grace period ends. Your grace period is the time between when your bill is due and when the finance charges will start accruing. For example, if your grace period is 21 days and you make a purchase on day 20, you will not be charged interest on that purchase as long as you pay it off in full before the 21st day.\n- Understand how balance transfers work. If you transfer a balance from one credit card to another, you may be able to take advantage of a 0% introductory APR offer. However, you will typically have to pay a balance transfer fee, which is usually around three percent of the balance.\n- Pay your bill in full every month. If you can’t do this, try to at least pay more than the minimum payment due. By doing this, you’ll avoid paying interest and finance charges on your credit card balance.\n- Know what purchases are not eligible for a grace period. Cash advances and certain types of transactions, such as balance transfers and convenience checks, are not eligible for a grace period. This means that interest will start accruing on these balances as soon as they are posted to your account.\nBy following these tips, you can avoid paying APR on your credit card.\nHow Do Credit Card Companies Decide On How Much APR to Charge?\nThe answer to how credit card companies decide how much APR to charge is actually quite simple: They base it on how much they think they can get away with. In other words, the amount of APR charged by a credit card company is largely dependent on the current market conditions and what the competition is doing. For example, if interest rates are high, credit card companies will likely charge higher APRs. Conversely, if interest rates are low, credit card companies may choose to lower their APRs in order to remain competitive.\nIt’s also important to keep in mind that different types of cards will often have different APRs. For instance, cash back and rewards cards typically have higher APRs than regular cards because the issuer is essentially offering you a discount on your purchases in the form of rewards points. So, if you’re trying to figure out how much APR you’ll be paying on a particular card, it’s important to take the type of card into account.\nUltimately, the best way to avoid paying high APR is to simply use your credit card wisely and pay off your balance in full each month. By doing so, you’ll never have to worry about interest charges eating into your hard-earned cash.\nWhat is The Difference Between APR and Interest Rate?\nThe difference between APR and interest rate is that the former includes fees and other costs associated with borrowing money, while the latter is simply the percentage of the principal loan amount that you will be charged for borrowing. In order to get an accurate idea of how much your credit card will cost you in interest, it is important to check the APR. Here’s a step-by-step guide on how to do just that:\nFirst, find your credit card statement. This should be easy to locate – it’s usually the document that comes in the mail every month detailing your account activity and balance. Once you have your statement in hand, look for the section that lists all of the charges associated with your account. This will likely include things like the annual fee, late fees, and cash advance fees.\nNext, find the APR for your credit card. This should be listed as a percentage next to the heading “Annual Percentage Rate.” Once you have located the APR, multiply it by the total amount of credit you have available. This will give you an estimate of how much interest you will be charged over the course of a year.\nFinally, divide that number by 12 to get your monthly interest charge. This is how much extra you can expect to pay each month on top of your minimum payment due – so make sure to factor it into your budget!\nBy following these simple steps, you can easily check the APR on your credit card and get a better understanding of how much it will cost you to carry a balance. Just remember to keep an eye on those fees – they can add up quickly!\nWhat is a Good APR for a Credit Card?\nThe answer to this question depends on a few factors, including your credit score and how you plan to use the credit card. For example, if you have excellent credit and only plan to use the card for emergency expenses, you may be able to get away with a higher APR. However, if you have average credit and plan to use the card for everyday purchases, you’ll want to keep your APR as low as possible.\nHere are a few things to keep in mind when considering APR:\n- Your credit score: The better your credit score, the lower your APR will be. If you have good or excellent credit, you should be able to qualify for a card with a 0% intro APR period.\n- The type of card: Some cards, like balance transfer cards, have higher APRs because they offer special features that come with a cost.\n- Your spending habits: If you only use your credit card for occasional purchases, you can afford to pay a higher APR. However, if you’re someone who relies on your credit card for day-to-day expenses, you’ll want to keep your APR low so you don’t have to pay too much in interest.\n- Your repayment habits: If you always pay off your balance in full and on time, you can afford to carry a higher APR. However, if you frequently carry a balance or make late payments, you’ll want to keep your APR low so you don’t end up paying a lot in interest.\nWhat is a Bad APR for a Credit Card?\nThere’s no hard and fast rule for what is considered a “good” or “bad” APR for a credit card. However, according to CreditCards.com, the average APR for all credit cards is currently 17.73%. So if your credit card has an APR below this level, it could be considered a good deal. Of course, the best APR is 0% – but these offers are usually only available for a limited time and often come with restrictions, such as balance transfer fees.\nHow Can I Lower The APR on My Credit Card?\nIf you have a credit card with a high APR, there are a few things you can do to lower your rate. One option is to call your credit card issuer and ask for a lower rate. If you have a good payment history and credit score, they may be willing to give you a lower APR.\nAnother option is to transfer your balance to a different credit card with a lower APR. Many issuers offer promotional rates for balance transfers, so this could be a great way to save on interest. Just be sure to read the terms and conditions carefully before you make the transfer, as there may be fees involved.\nFinally, if you’re struggling to keep up with high monthly payments, you may want to consider a debt consolidation loan. This can help you pay off your debt more quickly and at a lower interest rate, making it easier to get out of credit card debt for good.']"	['<urn:uuid:1f5e597e-4bbc-4935-a181-b02665781243>', '<urn:uuid:f72e3f51-9c9f-4100-a477-5f9a0c7038a8>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:56:11.774636	13	71	3542
7	nanoscale disease testing environmental applications	Nanoscale devices enable quick and efficient disease testing, with technologies like the University of Alberta's device that can test bacterial drug resistance in real-time using samples millions of times smaller than a raindrop, and DNA nanotechnology that can detect disease-related proteins in just 10 minutes. On the environmental side, nanotechnology applications include water treatment and purification, with nanoporous membranes and magnetic nanoparticles being used for effective filtration and removal of heavy metal contaminants from waste water, offering more cost-effective solutions compared to traditional methods.	"['Digital Devices to Enhance Nanotechnology\nAs technology advances, so does the need for more powerful and efficient power sources that can keep up with computing demands while remaining scalable and inexpensive. New nanotechnology innovations are opening the door to the technology of the future.\nArtificial neural networks are vital to developing computing abilities such as pattern recognition at levels on par with humans. Nanoscale devices called memristors (rhymes with ancestors) might be the answer to creating truly functional artificial brains. Memristors control power flow, remember charge, and are tiny and inexpensive. Scientists at the University of Southampton have shown that memristors can “learn” information without assistance and process data in real time, making them a potential foundation for the next generation of the Internet of Things.\nThe silicon carbon transistors found in conventional computer chips aren’t efficient enough to keep up with the performance requirements demanded by such chips. Nearly 20 years ago, nanotubes—minuscule rolls of carbon sheets—were discovered to be much more efficient. However, there have been several roadblocks to manufacturing functional nanotubes. To connect the nanotubes with the metal contacts needed to conduct energy, he created a new way to fuse them together at the nanotubes’ ends. IBM plans to replace silicon carbon transistors with nanotube transistors within the next decade, banking on much better performance at a fraction of the power use.\nMicrocable Power Textile\nThe next wave of electronics will require lightweight, efficient, and inexpensive power sources. Researchers at the Georgia Institute of Technology have created a potential solution: a textile that can produce power using nothing but the sun and human body motion.\nThe researchers used polymer fibers to make solar cells, and then wove the cells with fiber-based triboelectric nanogenerators (materials that become electrically charged when in contact with each other), which create energy from motion. The resulting fabric is only 320 micrometers thick—approximately one-third of a millimeter—and could be integrated into items such as tents and clothing to power devices like phones and wearables.\nTesting for disease is often a race against time, especially when trying to prevent potential outbreaks, or when quick treatment is crucial. These nanotechnology devices enable testing that’s efficient, inexpensive, and quick, producing real-time results that could help to stop epidemics in their tracks.\nModern medicine has made many once-endemic diseases, like polio and tuberculosis, rare. But sometimes bacteria and microorganisms mutate into “superbugs,” rendering conventional treatments ineffective. The World Health Organization calls antibiotic resistance one of the biggest threats to humans.\nQuickly testing bacteria for drug resistance is crucial, but the usual ways to grow and test bacteria samples in a lab take time. Researchers at the University of Alberta have created a nanoscale device that can test samples on site, in real time.\nThe device captures bacteria from a sample using a minuscule cantilever, which sends the sample through a channel where receptors identify the bacteria type. The bacteria are then exposed to antibiotics, and the reaction indicates whether they are treatable or antibiotic-resistant. Another advantage, given that sometimes only minuscule samples are available: the device can be used to test samples millions of times smaller than a raindrop.\nTesting for cancer and infectious diseases can currently mean waiting for hours before lab results are available. But researchers at the Henry Samueli School of Engineering and Applied Science, the California NanoSystems Institute, and the David Geffen School of Medicine collaborated to discover a faster method to test for the presence of proteins in body fluids that indicate cancer or other diseases.\nThe new test uses DNA nanotechnology—which exploits DNA’s chemical and physical attributes rather than its genes—to trigger a molecular chain reaction if disease-related proteins are present. The results appear in about 10 minutes, and the test can be done in a doctor’s office, removing the need for a separate hospital visit. The researchers have successfully tested for flu, with plans to test for diseases characterized by more complex protein structures. Eventually, they want to integrate the technology into a handheld reader, which could become the go-to device in every doctor’s office.', 'Environmental impact of nanotechnology\n|Part of a series of articles on the|\nThe environmental impact of nanotechnology is the possible effects that the use of nanotechnological materials and devices will have on the environment. As nanotechnology is an emerging field, there is great debate regarding to what extent industrial and commercial use of nanomaterials will affect organisms and ecosystems.\nNanotechnology\'s environmental impact can be split into two aspects: the potential for nanotechnological innovations to help improve the environment, and the possibly novel type of pollution that nanotechnological materials might cause if released into the environment.\nNanopollution is a generic name for waste generated by nanodevices or during the nanomaterials manufacturing process. Ecotoxicological impacts of nanoparticles and the potential for bioaccumulation in plants and microorganisms is a subject of current research, as nanoparticles are considered to present novel environmental impacts. Of the US$710 million spent in 2002 by the U.S. government on nanotechnology research, $500,000 was spent on environmental impact assessments.\nThe capacity for nanoparticles to function as a transport mechanism also raises concern about the transport of heavy metals and other environmental contaminants. Two areas of concern can be identified. First, in their free form nanoparticles can be released into the air or water during production, or production accidents, or as waste by-product of production, and ultimately accumulate in the soil, water, or plant life. Second, in fixed form, where they are part of a manufactured substance or product, they will ultimately have to be recycled or disposed of as waste.\nScrinis raises concerns about nano-pollution, and argues that it is not currently possible to “precisely predict or control the ecological impacts of the release of these nano-products into the environment.” A May 2007 Report to the UK Department for Environment, Food and Rural Affairs noted concerns about the toxicological impacts of nanoparticles in relation to both hazard and exposure. The report recommended comprehensive toxicological testing and independent performance tests of fuel additives. Risks have been identified by Uskokovic in 2007. Concerns have also been raised about Silver Nano technology used by Samsung in a range of appliances such as washing machines and air purifiers.\nLife cycle responsibility \nTo properly assess the health hazards of engineered nanoparticles the whole life cycle of these particles needs to be evaluated, including their fabrication, storage and distribution, application and potential abuse, and disposal. The impact on humans or the environment may vary at different stages of the life cycle.\nThe Royal Society report identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that “manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure” (p.xiii). Reflecting the challenges for ensuring responsible life cycle regulation, the Institute for Food and Agricultural Standards has proposed standards for nanotechnology research and development should be integrated across consumer, worker and environmental standards. They also propose that NGOs and other citizen groups play a meaningful role in the development of these standards.\nEnvironmental benefits of nanotechnology \nNanotechnology could potentially have a great impact on clean energy production. Research is underway to use nanomaterials for purposes including more efficient solar cells, practical fuel cells, and environmentally friendly batteries. The most advanced nanotechnology projects related to energy are: storage, conversion, manufacturing improvements by reducing materials and process rates, energy saving (by better thermal insulation for example), and enhanced renewable energy sources.\nCurrent commercially available solar cells have low efficiencies of 15-20%. Research is ongoing to use nanowires and other nanostructured materials with the hope of to create cheaper and more efficient solar cells than are possible with conventional planar silicon solar cells. It is believed that these nanoelectronics-based devices will enable more efficient solar cells, and would have a great effect on satisfying global energy needs.\nAnother example for an environmentally friendly form of energy is the use of fuel cells powered by hydrogen. Probably the most prominent nanostructured material in fuel cells is the catalyst consisting of carbon supported noble metal particles with diameters of 1-5 nm. Suitable materials for hydrogen storage contain a large number of small nanosized pores.\nNanotechnology may also find applications in batteries. Because of the relatively low energy density of conventional batteries the operating time is limited and a replacement or recharging is needed, and the huge number of spent batteries represent a disposal problem. The use of nanomaterials may enable batteries with higher energy content or supercapacitors with a higher rate of recharging, which could be helpful for the battery disposal problem.\nWater filtration and remediation \nA strong influence of nanochemistry on waste-water treatment, air purification and energy storage devices is to be expected.\nMechanical or chemical methods can be used for effective filtration techniques. One class of filtration techniques is based on the use of membranes with suitable hole sizes, whereby the liquid is pressed through the membrane. Nanoporous membranes are suitable for a mechanical filtration with extremely small pores smaller than 10 nm (“nanofiltration”) and may be composed of nanotubes. Nanofiltration is mainly used for the removal of ions or the separation of different fluids.\nMagnetic nanoparticles offer an effective and reliable method to remove heavy metal contaminants from waste water by making use of magnetic separation techniques. Using nanoscale particles increases the efficiency to absorb the contaminants and is comparatively inexpensive compared to traditional precipitation and filtration methods.\nSome water-treatment devices incorporating nanotechnology are already on the market, with more in development. Low-cost nanostructured separation membranes methods have been shown to be effective in producing potable water in a recent study.\nSee also \n- Gyorgy Scrinis (2007). ""Nanotechnology and the Environment: The Nano-Atomic reconstruction of Nature"". Chain Reaction 97: 23–26.\n- Vuk Uskokovic (2007). ""Nanotechnologies: What we do not know"". Technology in Society 29: 43–61. doi:10.1016/j.techsoc.2006.10.005.\n- Royal Society and Royal Academy of Engineering (2004). Nanoscience and nanotechnologies: opportunities and uncertainties. Retrieved 2008-05-18.\n- Tian, Bozhi; Zheng, Xiaolin; Kempa, Thomas J.; Fang, Ying;Yu, Nanfang; Yu, Guihua; Huang, Jinlin & Lieber, Charles M. (2007). ""Coaxial silicon nanowires as solar cells and nanoelectronic power sources"". Nature 449 (7164): 885–889. Bibcode:2007Natur.449..885T. doi:10.1038/nature06181. PMID 17943126.\n- Hillie, Thembela; Hlophe, Mbhuti (2007). ""Nanotechnology and the challenge of clean water"". Nature Nanotechnology 2 (11): 663–664. Bibcode:2007NatNa...2..663H. doi:10.1038/nnano.2007.350. PMID 18654395.\n- Zhang, Wei-xian (2003). ""Nanoscale iron particles for environmental remediation: an overview."". Journal of Nanoparticle Research 5: 323–332. doi:10.1023/A:1025520116015.\nFurther reading \n- Colvin VL (Oct 2003). ""The potential environmental impact of engineered nanomaterials"". Nat Biotechnol. 21 (10): 1166–70. doi:10.1038/nbt875. PMID 14520401.\n- University of California Center for Environmental Implications of Nanotechnology\n- Duke University Center for the Environmental Implications of NanoTechnology']"	['<urn:uuid:2dd36a51-1467-4963-bd0e-4d6c1e4e72bd>', '<urn:uuid:070cf2a9-bbb6-4bca-8984-96222ad21c86>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:56:11.774636	5	84	1785
8	I'm interested in both environmental impact and disaster preparedness - how do green buildings help reduce waste and emissions, and what building features make them more resilient during power outages?	Green buildings significantly reduce environmental impact - LEED-certified buildings have 34% lower CO2 emissions and have diverted over 80 million tons of waste from landfills. For power outage resilience, buildings can incorporate several features: backup power generators for critical systems, emergency lighting systems that last longer than the standard 90-minute requirement, high-performance building envelopes that maintain stable indoor temperatures, and distributed cooling and heating systems throughout the building. Access to potable water can be maintained through rooftop water tanks and pressure-booster pumps.	['Benefits of green building\nBuildings are responsible for an enormous amount of global energy use, resource consumption and greenhouse gas emissions. As the demand for more sustainable building options increases, green construction is becoming increasingly profitable and desirable within the international construction market.\nGreen building is profitable & cost-effective\n- Upfront investment in green building makes properties more valuable, with an average expected increase in value of 4 percent. By virtue of lowered maintenance and energy costs the return on investment from green building is rapid: green retrofit projects are generally expected to pay for itself in just seven years.1\n- Green buildings reduce day-to-day costs year-over-year. LEED buildings report almost 20 percent lower maintenance costs than typical commercial buildings, and green building retrofit projects typically decrease operation costs by almost 10 percent in just one year.2, 3\n- Between 2015 and 2018, LEED-certified buildings in the United States are estimated to have $1.2 billion in energy savings, $149.5 million in water savings, $715.2 million in maintenance savings and $54.2 million in waste savings.4\n- By 2021, in the U.S., green activity is expected to grow, with those doing the majority of their projects green increasing from 32 percent to 45 percent. Client demands remains the top trigger driving the market and encouraging occupant health and well-being is the top social reason for building green.\nGreen buildings lower utility bills and benefit the environment\n- Buildings are positioned to have an enormous impact on the environment and climate change. At 41 percent of total U.S. energy consumption, buildings out-consume the industrial (30 percent) and transportation (29 percent) sectors.5\n- Buildings use about 14 percent of all potable water (15 trillion gallons per year), but water-efficiency efforts in green buildings are expected to reduce water use by 15 percent and save more than 10 percent in operating costs.6, 7 Retrofitting one out of every 100 American homes with water-efficient fixtures could avoid about 80,000 tons of greenhouse gas emissions, which is the equivalent of removing 15,000 cars from the road for one year.8\n- Standard building practices use and waste millions of tons of materials each year; green building uses fewer resources and minimizes waste. LEED projects are responsible for diverting more than 80 million tons of waste from landfills, and by 2030 that number is expected to grow to 540 million tons.9\n- In the United States alone, buildings account for almost 40 percent of national CO2 emissions and out-consume both the industrial and transportation sectors, but LEED-certified buildings have 34 percent lower CO2 emissions, consume 25 percent less energy and 11 percent less water, and have diverted more than 80 million tons of waste from landfills.10\n- The market is responding to these cost savings and environmental benefits at a dramatic rate. According to a Dodge Data & Analytics World Green Building Trends 2018 SmartMarket Report, global green building activity continues to rise and nearly half of survey respondents expect the majority of their projects in the next three years to be green buildings.11\n1 McGraw Hill Construction (2012). World Green Buildings Trends: Business Benefits Driving New and Retrofit Market Opportunities In Over 60 Countries.\n2 U.S. Department of Energy (2011). Re-Assessing Green Building Performance: A Post Occupancy Evaluation of 22 Buildings.\n3 McGraw-Hill Construction (2012). World Green Buildings Study. Accessed Nov. 29, 2012.\n4 Booz Allen Hamilton and the U.S. Green Building Council (2015). 2015 Green Building Economic Impact Study.\n5 National Trust for Historic Preservation (2011). The Greenest Building: Quantifying the Environmental Value of Building Reuse.\n6 U.S. Geological Survey (2000). 2000 data.\n7 McGraw Hill Construction (2010). Green Outlook 2011: Green Trends Driving Growth.\n8 U.S. Environmental Protection Agency. Green Building, Green Homes, Conserving Water. Water Use and Energy.\n9 Watson, Rob. Greenbiz Group (2011). Green Building and Market Impact Report.\n10 U.S. Department of Energy (2011). Re-Assessing Green Building Performance: A Post Occupancy Evaluation of 22 Buildings.\n11 Dodge Research and Analytics (2018). World Green Building Trends 2018 SmartMarket Report.', 'As areas affected by recent hurricanes and earthquakes struggle to recover from the damage, demand for resilient multifamily housing capable of resisting natural and man-made disasters is expected to climb in coming years.\nGiven the power and magnitude of the quakes and storms, boosting multifamily building resilience in the Caribbean, Mexico and along the U.S. Gulf and Atlantic coasts is not an extravagance, but a necessity.\nToward a More Resilient Latin America and Caribbean\nFrom 1990 to 2011, natural disasters cost the housing sector in 16 Latin American counties and the Caribbean at least $53 billion, according to a global assessment report published in 2013 by the United Nations Office for Disaster Risk Reduction.\nLack of access to safer, affordable housing alternatives has forced 25 percent of the region’s population to live in substandard housing and informally built homes that are much more vulnerable to natural hazards. Public and private housing efforts traditionally have focused on quantity rather than quality, endorsing new housing construction and purchase subsidies while neglecting improvements to existing housing.\nIn March last year, The World Bank gathered international experts and housing authorities from 14 countries—among them the U.S., Chile, Argentina, Colombia, Peru, Honduras, Mexico and Jamaica—in Washington, D.C., to discuss how to improve housing affordability and resilience in order to mitigate increasing climate and disaster risks. They came up with several recommendations:\n- Align structural retrofitting efforts with better incentives for home/building owners.\n- Proactively engage finance institutions to increase affordability and improve building resilience.\n- Link housing policies and improvement efforts to both affordability and energy efficiency.\nDesigning for Multifamily Building Resilience\nResilient multifamily housing calls for innovative design approaches and construction methods that can tackle both acute shocks (hazard events) and the chronic stresses that stem from the gradual and perpetual disruptions that reduce a community’s ability to recover from disaster.\nAffordable housing and community development advocate Enterprise Community Partners recently published a collection of strategies for making affordable multifamily housing more resilient against the effects of extreme weather events. Based on the advice of 50 technical experts, the manual—“Ready to Respond: Strategies for Multifamily Building Resilience”—is a guide for building owners on retrofit strategies to help adapt and protect their properties against a variety of climate hazards.\nImproving multifamily building resilience poses significant design, cost and safety challenges, but the costs associated with disaster recovery—response, repairs, reconstruction, etc.—can be as disastrous as the natural hazards themselves. The old adage stands true: better safe than sorry.\nTHE COSTS ASSOCIATED WITH DISASTER RECOVERY CAN BE AS DISASTROUS AS THE NATURAL HAZARDS THEMSELVES. THE OLD ADAGE STANDS TRUE: BETTER SAFE THAN SORRY.\nStrategies for Resilient Multifamily Housing\nNot every housing unit or development is the same nor does it requires the same design and construction approach. Following are some the strategies designers and builders can use to improve multifamily building resilience.\nProtection strategies to reduce a building’s vulnerability to extreme weather.\n1. Wet Floodproofing\nAllows unoccupied portions of a building to be flooded during a storm by letting water flow through the building in a controlled way.\n2. Dry Floodproofing\nSeals buildings to keep water out and prevent damage to critical systems and mechanical equipment, reduce recovery time and deter mold growth. There are two types of dry floodproofing: active and permanent. Active measures require removable elements to be put into place before an anticipated flood. Permanent measures are fixtures and systems integrated into the structure itself that do not need to be manually deployed in the event of an emergency.\n3. Site Perimeter Floodproofing\nDeploying temporary (sandbags, water-inflated tube systems) or permanent (floodwalls, berms) physical barriers to prevent floodwaters from reaching a building. Does not require modifications to the building structure.\n4. Resilient Elevators\nElevators are often the only way residents have to reach higher floors, making them a critical building system. They are vulnerable to flood damage because elevator pits typically extend well below the lowest floor. Several techniques can make them more resilient.\n5. Backwater Valves\nSewage backflow occurs when storm water backs up into a building basement because of sewer line blockage or storm drain overflow due to flooding. A backwater valve is a relatively inexpensive retrofit that can prevent significant problems from sewer line failure by blocking reverse flow from entering the building through wastewater pipes.\n6. Sump Pumps\nThese remove water that accumulates in the low points in a building, typically the basement. They are an effective and affordable way to reduce costly flood damages.\nAdaptation strategies to improve a building’s ability to adapt to changing climate conditions.\n1. Envelope efficiency\nA high-performance envelope is very valuable during a power outage because indoor temperatures change more slowly, increasing a building’s “passive survivability.” The thermal performance of a building envelope depends on its insulation value, air leakage and thermal bridges.\n2. Elevated Equipment\nWith more extreme weather events causing coastal flooding, elevating or relocating mechanical and electrical equipment out of flood-prone basements and lower floors increases building resilience. Critical equipment can be elevated in place or moved to higher floors, the roof or an outdoor platform.\n3. Elevated Living Space\nAbandoning floors below the design flood elevation or using them for a non-residential purpose can limit damage to critical equipment or living space during a flood. By eliminating living spaces and mechanical systems below the base flood elevation and incorporating wet floodproofing measures, buildings may become eligible for lower insurance rates.\n4. Surface Stormwater Management\nStormwater is a major cause of urban flooding, especially in cities with combined sewer and stormwater systems. Most water treatment systems cannot handle additional volume during a large storm. Infiltrating water into the ground on-site reduces the need for large infrastructure projects and can ease flooding, speed recovery after a storm and reduce sewer backups.\n5. Window Shading\nExterior or interior window treatments that shade rooms can lessen solar heating in the summer and insulate against heat loss in winter.\n6. Distributed Cooling and Heating\nDecentralized and high-efficiency heating and cooling systems distributed throughout a building can help avoid flood damage while lowering operating costs. Distributed systems are most effective in buildings with high-performance windows and well-insulated, airtight envelopes.\nBackup strategies that provide critical needs for when a building loses power or other services.\n1. Maintaining Backup Power to Critical Systems\nDuring a power outage, backup power is vital to continue building operations and make repairs during and after a storm event. Consider the needs of your site when selecting a backup power generator, including the location where it will be installed, fuel storage issues, the amount of emissions it will produce and your budget.\n2. Emergency Lighting\nBuilding codes generally require only 90 minutes of emergency illumination. Different lighting strategies—such as natural light, battery- and solar-powered emergency lighting—are necessary to keep buildings operating safely during and after emergencies.\n3. Access to Potable Water\nA reliable source of potable water is essential if a building is to remain livable during a power loss. Taller multifamily buildings typically use rooftop water tanks, rainwater storage tanks, pressure-booster pumps, critical load circuits (for the water pump and power generator) and water efficiency measures to keep the water flowing.\nOther strategies that can improve multifamily building resilience include sea walls, elevated post-tension concrete design, missile-impact glass and subterranean concrete pilings.\nSources: The World Bank, Enterprise Green Communities, Resilient Design Institute.']	['<urn:uuid:06b7e19a-ee3d-47e0-84e4-00e787b471a5>', '<urn:uuid:ab67aee7-00d9-4d77-9232-0672226623f7>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	30	82	1885
9	As someone who regularly exercises and does strength training, I've heard chocolate milk might be beneficial - is there any truth to this for muscle recovery?	Yes, chocolate milk can be a good post-workout recovery drink because flavored milks have the ideal 4:1 ratio of carbohydrates to protein that the body needs for muscle recovery and rebuilding after exercise. These drinks contain the same nutrients as regular milk, though they do contain added sugar.	['The milk aisle is packed with options. Which is best for you? We’ve got the lowdown on 15 varieties of dairy, soy, nut, and seed milks.\nCow’s Milk: Skim\nGrocery stores have more milk options than ever, including plant, nut, and seed versions you may not have heard of. Classic cow’s milk is a good source of three nutrients most Americans don’t get enough of: calcium, vitamin D, and potassium. Nonfat, or skim, is made by skimming the cream from whole milk. At about 90 calories and zero fat per cup, it’s a good option for milk drinkers trying to lose weight.\nCow’s Milk: 1% and 2%\nAll four types of cow’s milk (skim, 1%, 2%, and whole) have the same amount of protein, calcium, and vitamins. The difference is in calories and fat. A cup of 1% has 102 calories and 1.5 grams of saturated fat. A cup of 2% has 3.1 grams of saturated fat. Both have some of the creaminess that skim milk lacks. If you’re trying to switch to non-fat milk, you might start with these to adjust to the taste difference. They can also be good options for kids who need to drink less juice or soda.\nCow’s Milk: Whole\nWhole milk, which is 3.5% milk fat, is closest to the liquid that comes out of the cow before processing. Who should drink it: Babies between 1 and 2 years old, whose growing brains need a higher-fat diet, and adults who could use more calories. If you just love the taste of whole milk, it’s OK to indulge. Pair it with a high-fiber cereal and then watch your saturated fats for the rest of the day.\nBesides being a kids’ favorite, chocolate milk may be a good post-workout recovery drink. There’s nothing special about the chocolate, though; flavored milks (strawberry and vanilla, too) have the ideal 4:1 ratio of carbohydrates to protein the body needs for muscle recovery and rebuilding after exercise. These drinks have the same nutrients as regular milk, but they have added sugar.\nSome people have trouble digesting lactose, the natural sugar in dairy foods. The problem, known as lactose intolerance, can cause bloating, gas, and diarrhea. But lactose-free milk has been treated, so it doesn’t cause those problems. It has the same nutrition as cow’s milk. But it tastes slightly sweeter because the lactose has been broken down into simple sugars.\nIn the plus column, a cup of this drink has tons of protein — 9 grams per cup, the most of any milk. It’s also richer in calcium than skim, with 35% of what you need each day. The cons: It has 170 calories and 7 grams of saturated fat per cup, far more than whole cow’s milk. One study found that it had no clear nutritional advantage over cow’s milk, and if you’re allergic to cow’s milk, you’ll probably be allergic to this as well\nMade from ground soybeans soaked in water, it’s a nutrition powerhouse: All the benefits of cow’s milk from a nonanimal source. A cup of calcium-fortified nonsweetened soy milk has about 6 grams of protein, 6 grams of carbohydrates, and 450 milligrams of calcium. (Flavored versions can be high in added sugars.) It’s a good option for vegans, people who are lactose intolerant, or those with a dairy allergy. But if you go for a milk substitute, make sure your choice is fortified with calcium and vitamin D.\nMade from ground almonds and filtered water, one cup of almond milk can have fewer than 30 calories while packing 450 milligrams of calcium. That’s more than most cheese, yogurt, and soy milk. Keep in mind that the benefits don’t come close to what you get from an ounce of raw almonds, which have far more protein, fiber, and healthy fats. But it can be a good choice for people on a low-sugar diet, those with a dairy or soy allergy, and vegans who don’t like soy milk.\nRice milk has earned its place on the shelf as an easy-to-digest soy alternative that’s also nut- and gluten-free. It’s made from ground brown rice and water, and nutritionists recommend it for people with multiple food allergies. On the downside, a cup of unsweetened rice milk has no protein, and because it’s made from rice, it’s naturally high in carbohydrates.\nWith a sweet and creamy flavor, cashew milk is a popular option for nondairy cream soups, dressings, and sauces. Close in texture to whole milk, it has fewer calories than skim. One cup of fortified unsweetened cashew milk has 25 calories per serving, zero sugar, and also has 50% more calcium than dairy milk.\nMilk made from macadamia nuts is one more option for people on a plant-based diet. Like almond milk, a cup of it gives you 45% of your recommended daily calcium, but with about as much fat as 2% cow’s milk (5 grams per cup). Who should try it: Milk drinkers looking to avoid dairy, soy, and gluten.\nIt’s not just for pina coladas and curries — coconut milk is everywhere these days. But know the differences: Canned coconut milk is the liquid expressed from coconut meat. With 12 grams of fat per cup, it’s the nondairy equal of heavy cream. It’s a great substitute in soups, puddings, and vegan ice cream. “Coconut milk beverage,” found in the refrigerated section, is watered down to lower the calories and fat. One cup has 45 calories and 4 grams of saturated fat.\nHemp Seed Milk\nLovers of this drink say it’s thicker and creamier than soy milk, and not as grainy. Made from hemp seeds, a cup of unsweetened hemp milk is rich in omega-3 and omega-6 fats, and other nutrients. On the other hand, it has 80 calories per cup, which is higher than rice and almond milks. Another option for people who have multiple food allergies.\nFlax Seed Milk\nIt’s one of the newer milk substitutes to hit the shelves. It’s simply cold-pressed flax oil mixed with filtered water. At 25 calories per unsweetened cup, it’s much lower in calories than all the other milks on the shelf. Besides that, it boasts 1,200 milligrams of heart-healthy omega-3 fatty acids. Who should try it: Those looking for a dairy-free milk without the taste of soy or nuts.\nBuy or Make Your Own?\nBasic nut and seed milks are fairly easy to make: Simply soak, blend, and drain. Commercially manufactured nut milks are typically fortified with vitamin D, calcium, and other nutrients to put them on par with dairy milk. On the other hand, manufacturers also add sweeteners, preservatives, and stabilizers you may not want.\nSource: Are Milk and Milk Substitutes Healthy? A Look at Your Options']	['<urn:uuid:057d90c9-b55f-4195-8112-d01f674b5f52>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	26	48	1119
10	What are the benefits of having adjustable ISO settings in digital cameras compared to film, and what are the downsides of using high ISO in terms of image quality?	The main benefit of digital cameras is that they allow you to change ISO on the fly, while with film you would have to physically change the roll to change ISO. However, the major trade-off is that higher ISO introduces more noise. In film, noise appears as larger grain, which can be desirable especially in Black & White photography. In digital cameras, the noise manifests as higher grain with color shifts and artifacts, particularly affecting large areas of single color and shadow areas, potentially making high ISO barely usable in some cameras.	"['When we speak about creating the perfect exposure, we often allude to the Exposure Triangle. The Exposure Triangle is comprised of three components: ISO, Aperture and Shutter Speed. We often talk about Apertures and Shutter Speeds, because they are the most variable of the lot. But ISO is really the cornerstone of the Exposure Triangle, and it’s one of the reasons why I often list it first when speaking of the three. Today we’re going to speak about ISO, how to use it and how it affects your photographs.\nWhat You Need To Know\nI could write a dissertation about ISO; the physics behind it, the mechanics and design constraints and of course how it has improved over the years. But I don’t need to do that as I think it would really confuse the issue (and most of you would probably fall asleep, not\nsomething I’d like to have a reputation for). Besides, that’s not the purpose of this article. In simple terms, ISO is a measurement of the sensor’s sensitivity to light. The higher the ISO, the more sensitive to light it is. In other words, you’ll need a higher ISO to take photographs under lower light conditions. With film, you would have to physically change the roll of film to change your ISO. But in what many see as an advantage over film, digital cameras allow you to change the ISO on the fly.\nNot all cameras are created equal when it comes to ISO. And one cannot assume that every camera will perform the same under different lighting Some cameras handle it better than others. There are many factors that affect ISO performance, but the main contributors are the type of sensor (size, resolution and type) and the camera’s on-board processor (which often provides a preliminary noise reduction). Even closely related cameras from the same manufacturer may have different ISO performance. You must experiment with different ISOs – much in the same way you experiment with different apertures or shutter speeds – to find the limitations of your camera.\nHow ISO Affects Your Work\nBeing able to change the sensitivity to light may seem like a great thing, but there is a major trade-off: The higher the ISO, the more\nnoise gets introduced. With film, the noise is simply in the form of a larger grain, which is not always undesirable, especially with Black & White photography. But on digital, the noise can come in the form of a higher grain with color shifts and artifacts. With some digital\ncameras, high ISO is barely usable; just because a camera can support high ISO doesn’t mean you’d want to use it. The good news is that many of the latest cameras offer significantly improved high ISO performance. I’ve seen some very impressive results from some of the new full-framed cameras on the market where even at ISO 2400 and above, the noise is rather insignificant. There are even some cropped sensors on the market that have impressive results as well. So maybe one day in the future, ISO won’t even be a major concern.\nEven if you had the camera with the best ISO performance on the market, you still couldn’t completely ignore ISO. The noise may be minimal, but it will still be there, impacting the overall quality of your images. For quality purposes, you would still want to go with the lowest ISO feasible for the shot. The needs will differ between shots. If you’re shooting stationary subjects with the camera locked down on a tripod, there would be little reason to use a higher ISO. A low ISO would mean a longer shutter speed, but with such a shot, that might be of little concern. The same would be true in a studio environment where you have absolute control over all of the lighting. The higher quality of the low ISO would give you a much cleaner image to start with when you needed to do some touching up. However, on the sidelines of an evening football game in the fall, it may look like the stadium is pretty bright, but may still not be very bright from your camera’s perspective. You need fast shutter speeds to capture action under those conditions, so pushing up the ISO is going to get you the edge. But the purpose of that type of photograph is vastly different. It’s about capturing the scene, maybe for the purpose of the newspaper, so you have a little bit more flexibility in the finished quality. The inverse is true as well. Suppose you’re shooting a stream or waterfall in broad daylight and you want to slow the shutter as much as possible. Lens filters are the ultimate solution, but a great place to start would be to drive the ISO as low as possible. Lower ISOs will result in longer shutter speeds. So a combination of a low ISO and a Neutral Density Filter (or more) would help to make that water silky smooth.\nAs with all technical aspects of photography, you can’t let the ISO aid in your laziness. Just because you can capture photographs in\nrelatively low doesn’t mean you always should. It’s one thing when you’re at a party and you’re just documenting some good times. But if you’re setting up for a fine art photo, bumping up the ISO is a last resort. Instead, you should consider using longer exposures (on a\ntripod, of course), off-camera flash or other ways to introduce light into the shot. Light is your paintbrush, and there is no reason to opt for higher ISO instead of painting with light. Bottom line, ISO is simply one more tool, one more setting to tweak, and it should be\ntreated as such. So again I repeat: Keep the ISO as low as you can so as to guarantee the best quality image you can create.', ""It is perhaps the bane of the photographers life, the dreaded image noise that has the ability to ruin the look of a well composed picture. Whilst modern cameras have much improved sensors and algorithms, there is still the distinct possibility of noise creeping in, not only in low light but also in incorrect exposures. So what is image noise?\nEven the best cameras will have image noise. Here a D3 is using ISO 6400 – Photo by The Odessa Files\nWell its not a new phenomenon, those of us who dabbled in film will know that as you use a higher ISO film, the more grain you saw in the image. The digital equivalent of grain is noise and it is caused by the same increase in ISO. In its simplest terms, noise is caused when a sensor’s sensitivity is “turned up” or in other word increasing it’s ISO.\nIt happens because to increase the sensitivity of the sensor, the camera boosts the power going to that sensor, this in turn increases the electro-magnetic fields around each pixel, causing interference with the associated image quality reduction. This effect can also happen when you make long exposures. Noise most often will manifest itself in large areas of a single color, for example a blue sky or in shadow areas of an image.\nSo lets take a look at how to reduce noise.\nIn Camera – Your camera will have a built in image noise reduction system. Generally this only works on jpeg files and can be adjusted in terms of how aggressively it reduces noise. Aggressive noise reduction may be effective at removing noise but it can introduce softness and a smearing effect to the image.\nIn camera noise reduction only – Photo by The Odessa Files\nA better option is to shoot RAW. RAW files do not apply noise reduction, leaving you free to use your RAW convertor to reduce the noise. Software noise reduction is more often than not a much more effective way of reducing the problem compared to in camera techniques and can also be applied to jpg files.\nIf you prefer to shoot jpg, another technique is to shoot multiple shots, all at the same exposure. This is because noise does not manifest itself in the same place in each image, so by merging three separate shots in Photoshop, you can dramatically reduce noise. If you are using this technique, remember to switch your in-camera noise reduction to low or off. This technique is also perfectly useable when shooting RAW as well.\nShooting Habits That Can Reduce Noise: Although it may seem obvious, the best way to reduce noise is not to introduce it in the first place. The main reason for noise is bumping up the ISO to maintain an acceptable shutter speed for hand holding. If you are shooting static objects, then the use of a tripod will allow you to set the camera to it’s lowest ISO (and highest image quality) to shoot virtually noise free longer exposures.\nLastly, always aim for the correct exposure, look at your histogram and check that all the light is falling inside the graph, any outside the graph, particularly at the shadow end (the left side) is going to increase the possibility of noise.\nIn Post Production – Of course, you will not be able to reduce noise in absolutely every shot in camera. Wildlife and sports photographers for example will often need to shoot jpg to keep the camera’s shooting speed up and the use of a tripod may not be feasible. In this situation, it’s time to reduce the noise in post production.\nNearly all image processing and management software has noise reduction tools built in and there are also a number of stand alone software or plugins that offer increased capabilities. Most of them work in a similar fashion allowing you to define how aggressively you apply the noise filter, usually on a scale of 0-100% and a pixel radius, allowing you to set how far from each pixel to apply the filtering.\nMore advance tools or software will allow you to differentiate between chroma noise (noise that appears as mis-colored pixels in your image) and luminance noise which is more akin to the static noise you see in an untuned TV.\nThe advantage of post production noise reduction is that you can see the correction live on your screen. You should examine your image at at least 100% image size checking any blocks of color and shadow areas, and you should apply the noise reduction after you have finalised any exposure any color corrections to the image. Watch for image softness when using the software, most noise reduction tools have an image sharpen facility as well, the disadvantage of this is that if you are too aggressive with it, you can re-introduce noise.\nUsing Apple Aperture's default noise reduction – Photo by The Odessa Files\nOver reduction introduces softness – Photo by The Odessa Files\nLastly image noise can be quite a subjective matter. In the days of film, grainy black and white images often conveyed a sense of atmosphere and mood. To this end you should not look as sensor noise as something that should always be eliminated. Applied well and in the right situation noise can actually add something to your images.\nSome noise can be good – Photo by The Odessa Files""]"	['<urn:uuid:c51f5a44-d3e8-4d14-a889-a7b9abc8ab7c>', '<urn:uuid:8f362aeb-908d-474e-9826-5caeb84f84db>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	29	92	1885
11	What's the best position for important KPIs on a dashboard?	The most important KPIs, metrics, and indicators should be placed at the top left of the dashboard in Left to Right (LTR) countries.	['This is a collection of tips that will help you create powerful Excel dashboards. This ranked list started by collecting the top Excel dashboard tips from multiple search engine pages. After aggregated the tips I ranked them for usefulness based on 30 years of experience developing Excel dashboards and strategic performance projects.\nIdentify the dashboard’s primary user and interview them for their needs. Here are a few categories and the ends of a range of responses you might want to touch on in your interviews. The general idea for these comes from a Juice Analytics whitepaper, A Guide to Creating Dashboards People Love to Use.\n|Time Period||Historic, static||Real-time, predictive, dynamic|\n|Depth||High level, Across||Detailed, Drill down|\n|Use||Read, decide||Query, analyze|\n|Ease with Data||Business-level||Operational data lover|\nBefore you spend hours of fun brainstorming KPIs and metrics check with your industry organization and in some of these great books for industry standard KPIs and metrics.\nLean Analytics: Use Data to Build a Better Startup Faster (Lean Series)\nby Alistair Croll and Benjamin Yoskovitz\nMarketing Metrics: The Manager’s Guide to Measuring Marketing Performance (3rd Edition)\nby Paul Farris, Neil Bendle, Phillip E. Pfeifer, David J. Reibstein\nThe HR Scorecard: Linking People, Strategy, and Performance 1st Edition\nby Dave Ulrich, Mark A. Huselid, Brian E. Becker\nThe Workforce Scorecard: Managing Human Capital To Execute Strategy 1st Edition\nby Mark A. Huselid, Brian E. Becker, Richard W. Beatty\nA KPI is a Key Performance Indicator. It is a metric that drives the success of a key strategic or operational objective. Selecting the right KPI(s) for each objective is absolutely critical. Obviously, if you don’t have the right KPIs you may drive the organization’s efforts in the wrong direction.\nOften a strategic or operational objective will need two KPIs, one to balance the other. For example, an objective that demands the revenue KPI go up could be counter-productive if that happens because margins go down, so a margin KPI might be needed as balance. For every KPI you should have two or three driver metrics that are the primary drivers of the KPI. Rather than pulling metrics out of the air or using “what we’ve always used”, draw a map.\nFor Balanced Scorecards start with a Strategy Map. Start at the top and work down identifying KPIs and metrics that drive each objective. Some KPIs, especially customer perception and branding, are extremely difficult or expensive to measure.\nFor operational KPIs and metrics use one of the many business flow diagraming methods to create a flow map, then identify the cause and effects in the operation. From the diagram you should be able to identify the most important KPIs and their drivers. Best way to do it? Grab two or three SMEs (subject matter experts) and a stack of Post-It notes and start mapping on a wall. (For Strategy Maps and simple business maps I use inserted objects in PowerPoint.)\nThis reminds me of my favorite story about the Texas Rangers. In the old West in the United States (think cowboys) before the West had states and governments, there was only one law officer per territory, a Texas Ranger. When a riot of immense proportions was feared to break out in the town of Dallas, Texas, the mayor telegraphed for a squad of Rangers. The story goes that the next morning a single Texas Ranger stepped off the train. The mayor, anguished, asked, “Where are the other rangers?” The Ranger slowly answered, “One riot. One ranger.”\nDashboards at the executive and top-management level are usually designed to monitor one performance domain, for example, Regional Sales. Put on the dashboard the KPIs and metrics needed to make a decision about the one performance domain. No more, no less.\nSometimes two or three counter-balancing KPIs are needed, like Revenue, Average Deal Size, and Margin. Put those KPIs on the top of the dashboard or down the left side. The other metrics on the dashboard should be drivers of those top KPIs.\nThe most important data should be at the top left of the dashboard in Left to Right (LTR) countries. So, put your most important KPIs, metrics, and indicators at the top left. Put driver metrics and related analytics down and right.\nThis clean example is out of a tutorial and sample files using PivotTables in the Journal of Accountancy, Jan 31, 2011.\nDashboard Your Scorecard\nBy Mark W. Lehman, CPA, Carol M. Lehman and Jim Feazell\nThis tutorial has a nice use of a drop-down in Unit Styles to retrieve data from a PivotTable using the GETPIVOTDATA function. (To learn how to do this see Chapter 21, Working with PivotTables, in “Balanced Scorecards & Operational Dashboards with Microsoft Excel” by Ron Person, the author of Critical to Success.) Data, calculations, and display are also kept in separate worksheets as tip 11\nKeep your dashboards clean and simple. Use lots of whitespace so readers can focus on important data and relationships. If an element is not adding to the understanding or insight, delete it.\nFrom Debra Dalgleish at Contextures\nWhen management first begins using dashboards it is best to start with dashboards, perhaps using a grid layout with three (above) or four rectangles. As they use the dashboard more they become more familiar with how to read it and the relationship between sections. Once they are familiar you can can begin including additional data; however, it is still important to put “just enough” and “not too much.” They will let you know what they want. I usually have a few backup dashboards ready to support the main dashboards.\nMake it easy for your readers to decide what to focus on. Use icons to give a quick assessment of good, neutral, or bad results. Conditional icons use conditional formatting and are found on the Home ribbon, in the Styles group, in Conditional Styles. Select an icon set that will indicate whether the metric is good, neutral, or bad.\nFrom the Excel Easy site\nYou don’t have to become an artist. But, a few design principles will make you feel more confident and make your dashboards more pleasing.\nSeriously lift your design credentials with “The Non-Designer’s Design Book (4th Edition)” by Robin Williams. You’ll learn to design with CRAP – Contract, Repetition, Alignment, and Proximity.\nIt isn’t specific to dashboards, but it will change how you see text and designs from the moment you open the book.\nKeep the look clean. Use lots of white space.\nAs soon as Excel creates its default chart, get rid of the chart clutter by clicking on the interior horizontal and vertical lines and pressing Delete. If the chart shows only one data set you don’t need a legend, so click it and press Delete. If the context is obvious you don’t need the title, click it and press Delete.\nOne way to make this easier is to use chart templates. See the following tip on templates.\nTemplates give a professional look that matches your organizational brand. It also maintains consistency in layout and style between different operational presentations so the user can work on making a decision rather than searching for meaning in the dashboard. Just like the templates you create in PowerPoint and Word, you can create templates and styles in Excel.\nGrid Layouts for Excel Dashboards\nAlmost every dashboard solution you need will fit one of three or four general grid layouts. Choose three or four grid layouts for your templates. This series of dashboard layout grids are from Microsoft’s Operations Manager 2012 Dashboard Widget.\nCustom Reusable Cell Styles for Excel Dashboards\nCreate custom cell styles that include fonts and font size, number formats, cell borders, and cell colors and shading. You can create, apply, modify, and remove custom cell styles.\nCreate, save, and apply chart templates for chart designs you use frequently.\nMake it easier to maintain and protect larger Excel projects by putting reports and charts, calculations, menus and items, and data on separate worksheets. Keeping each type in a separate worksheet makes it easier for you to architect the layout for each sheet. For example, unless it is very simple, putting data on the same sheet as a report and charts will restrict the freedom of your row and column layout. You may also want multiple report and chart sheets feed from one sheet of calculations.\nThis figure shows a worksheet I’ve used to layout the lists for menus, menu items, drop-down lists, and all the control variables used to control a very robust ROI calculator. With it all on one well-documented sheet it makes it easy for another developer to find and make changes (and for me to remember). Notice the arrows that point between tables to show the logic flow of menu selections and calculations.\nDon’t keep your data in the Excel workbook, if possible. Keeping data in the workbook, from a CSV file for example, is asking for “One Thousand Versions of the Truth.” You have difficulty knowing the currency and validity of the data.\nInstead, link the workbook to data in a repository. For example, link a PivotTable to an external data source.\nWell documented range names make formulas much easier to understand. For example, instead of a formula reading,\nIt’s much more understandable when you use,\nUsing a naming syntax will make it easier for you to know where the range name is and what it does. See the links below for more.\nLearn how to use the Name Manager to make range names easy to change. To get to the Name Manager on the Home ribbon, in the Defined Name group, click Name Manager.\nExecutive- and upper-management dashboards usually show just the top strategic KPIs and their direct drivers. However, operational level managers and analysts want to interact with data and get at the details.\nUse the dynamic charting methods described in Critical to Success to filter and query data. Use drop-downs, sliders, Data Validation, and more so that dashboard users can retrieve and analyze the data they need.\nPivotTables are the demi-god of data analyses. You can use them to slice and dice large databases. Slicers are clickable widgets that filter the results from a PivotTable.\nA good place to start learning PivotTables is at Excel Easy.\nMake it easy for managers and analysts to see the difference between budget (expected) and actual (reality). This blog will show you five different ways to chart budget vs. actual (forecast vs. actual).\nTrends smooth erratic and show the overall direction of movement. I recommend using one of Excel’s trend functions rather than the automatic trend lines you can add to charts. Functions give you more control.\nIn a chart, if you add data to the end of the chart’s data sequence or add a new data series, the chart does not update to include the added data. There are two ways to correct this. One is to select the range of headings and data for the chart and then choose Home, Styles, Format as Table. The data becomes an Excel table which will auto-expand to include new data for the chart. The second method is to use dynamic range names.\nUse the OFFSET and COUNT or COUNTA functions to create dynamic charts in Excel that resize automatically as new data is added. Dynamic range names are MUCH more work than making the chart data into a table (see tip 18), but once you know this technique you will have amazing super powers to create interactive data and charts.\nSome executives love to see pie charts, but I try to steer people clear of them. It’s much better to use column charts (not stacked columns) so viewers can easily compare the ratios of heights.\nNever use 3D charts!\nNever use 3D charts or did I write that already? 3D charts look flashy, but they make visual analysis and comparison difficult.\nUse the Insert Comment and Edit Comment commands to insert pop-up help throughout your data-entry area, dashboard, or report. Comments pop-up a text box when the mouse hovers over the cell containing the comment. You can format comments with font size, bold, colors, etc. by selecting the cell they are in, right-click Edit Comment, then move over the comment box and right-click Format Comment.\nPut a version number and date of refresh on your dashboards. People can quickly compare whether they are using the same dashboard version and whether they have the current data.\nWe’ve all seen tens of minutes wasted trying to understand which charts and reports have the most current numbers.\nIn addition to user comments, have empathy with yourself or other dashboard developers. If you have separated your calculations and menus from the presentation you have room to add comments and explanations. I even put in arrows to show the flow of data or logic. (Yes, you can use Home>Formulas>Trace Dependents/Precedents, but arrows on the worksheet give a better idea of the holistic process.)']	['<urn:uuid:bdbab22e-b6b3-4f5d-86ac-14ed97c26de6>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	10	23	2150
12	recycled clothes new garments percentage	Currently, less than 1% of post-consumer textile is recycled into fibers and used in new clothing, according to Mariska Zandvliet, president of the Textile Branch of the European Recycling Industries Confederation (EuRIC).	"[""Ahead of Global Recycling Day on 18 March, the designers who showcased their designs made from recycled textiles at London Fashion Week were greeted by the Global Recycling Foundation. As sustainability is gaining importance hence to minimize the impact on the environment, the designers are embracing calls for more brands to re-think their designs.\nAs per the estimation made by the Ellen MacArthur Foundation, new textile production is a major contributor to climate change and the fashion industry accounts for 10% of global carbon emissions – more than the aviation and maritime industries combined.\nThe forward-thinking designers are incorporating recycled textiles into their collections. In this year’s digital London Fashion week, one of such brands which showcased clothing made from recycled textiles is Bethany Williams. Working along with local manufacturers and social projects, each garment from their collection is manufactured in the UK and is made using recycled and organic materials.\nDesigner Bethany Williams said that within the UK, the waste stream researching and exploring to discover how different materials can be innovatively recycled into something that fills them with joy. Designing is a part of problem-solving which enables one to find how a product, material, or garment which has been used to serve its original purpose, can be given a second life and reused.\nThe same belief can be seen in collections of N Palmer which is a brand launched in summer 2020, which uses innovative textile techniques and contemporary design to explore repurpose vintage fabrics. Owner Nicholas Palmer said that 2021 is all about the waste and pollution which the fashion industry has already created. In the present time, it would have been too self-indulgent to create things without taking into consideration the product’s entire life cycle, hence he couldn't stop himself from incorporating recycling/upcycling solutions into his brand.\nOne of the oldest and most established sustainable industries is textile reuse and recycling which consists of a higher percentage of re-use i.e. 50% to 60% than most other waste streams.\nThe president of the Textile Branch of the European Recycling Industries Confederation (EuRIC) Mariska Zandvliet said that considering the point of view of the textile industry along with keeping in mind the waste hierarchy, one of the essential and important things which should come before recycling is re-use. She points out that in the current time the amount of post-consumer textile which is recycled into fibers and used in new clothing is less than 1%. At the end of life, the designs which contain multiple layers and mixed materials make it very difficult while separating the fibers for recycling. Because of which mechanical recycling is used which shortens the length of the fibers and makes them unfit to be used again without adding virgin material.\nShe adds that new business models will rise by creating the demand for recycled fibers, co-operation, innovative recycling techniques, and eco-design which will allow the textile chain to become circular. One of such innovations is using polyester made from recycled plastic water bottles which are possible because both polyester and plastic bottles are made from PET.\nFounding President of the Global Recycling Foundation Ranjit Baxi said that they applaud their heroes' innovative recycling practices and report them to their supporters all over the world every year. He hopes that designers will be nominated for its #RecyclingHeroes competition to mark Global Recycling Day. The nominations for the award should be sent to [email protected] – by 8 March.\nA weekly report covering market and price information on the entire chain of polyester along with online access to daily polyester chain prices.\nOne-time reports that are issued annually cover the demand and supply trends in individual products including polyester, nylon, acrylic, viscose, and cotton.\nOne-time reports that are issued annually cover the demand and supply trends in the individual country's natural and manmade fiber/filament industries.\nCountries Served Worldwide""]"	['<urn:uuid:07850db7-f209-4a9a-be0a-cf0842e7c23f>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	5	32	643
13	why dangerous store alcohol racing fuel maintenance needed	Alcohol racing fuel is dangerous to store because it is hydroscopic, meaning it absorbs water from the air, which can lead to contamination and corrosion. When alcohol fuel dries, it progresses through three harmful stages: first turning to gel, then forming clumps, and finally becoming sand-like, which can damage engine components with tight metal-to-metal clearances. Additionally, alcohol can break down standard fuel cell foam within weeks, causing it to become gummy and then brittle, resulting in foam particles potentially clogging the fuel system. Therefore, proper storage requires draining fuel cells after races and using specially designed alcohol-resistant fuel cells.	"['The carburetor alone is not the fuel system. Instead, the carb is a component within the w\nThe Saturday night racer faces some very real issues with the fuel systems in their cars. The demands on the fuel system can make or break a racing program depending on the level of preparation that the racer places on the system.\nFrom a systemic perspective, the fuel system isn\'t that complex. The whole system will usually include a fuel cell, some fuel line, a filter or two, either a mechanical or electric pump, and the fuel delivery device, either a carburetor or possibly fuel injection. Depending on the complexity of the fuel delivery device there may also be a fuel pressure regulator, a fuel pressure gauge, and a return line to the fuel tank or fuel cell. The higher the power level of the engine, the more complex and involved the system requirements will be.\nIt\'s a simple equation: As the power levels climb, the fuel system will get more and more complex.\nFor Saturday night racer applications, the fuel system doesn\'t start in the car, but at the point we as racers start to control the fuel. In this case, that\'s the point of sale for the fuel. Racers may bring their own fuel to the track or they may purchase fuel from a vendor at the track.\nWhile it doesn\'t look like the filter is easy to work on, in reality it\'s fairly accessibl\nThe fuel may be transferred several times prior to it ever reaching the fuel cell or tank in the car. It\'s in this manner that we can introduce variation into the fuel system through contamination by dirt, water or by mixing different grades or ages of fuel. Any way you look at it, we have the opportunity to maintain or degrade the quality of the fuel. So for the Saturday night racer to more precisely control the fuel system, we need to really understand the process of how we move and use the fuel.\nOnce the fuel is in the tank or the fuel cell, we still need to be concerned about the cleanliness of the fuel. Fuel cells don\'t last forever and the foam inside the bladder can break down over the course of a season or sooner, depending on the fuel you are running. If this happens, the fuel will be contaminated by the foam and can cause system problems. The foam should be replaced at the intervals suggested by the fuel cell manufacturer. Racing fuels can be very caustic depending on the chemicals used as additives in the manufacturing process.\nBy the same token, if you\'re still using the OEM tank, you\'re not home free just yet. It\'s entirely possible that the tank has seen a good number of over-the-road miles and is possibly or more than likely contaminated with rust and scale after many years of use as a grocery getter. You may want to remove the tank and have it cleaned. The truth of the matter is that once the OEM tank is removed, you might as well think about installing a fuel cell just for the safety that this type of tank offers.\nThis Dwarf Car has a very clean setup for the fuel cell. It\'s mounted solidly to the chass\nSome gasoline blended for the racing industry is very hard on the rubber components of the fuel system. This can be the lines, gaskets and any diaphragms. These ""rubber goods"" are especially sensitive. They\'re synthetic rubber and plastics, but they\'re not completely impervious to the chemicals in fuel. Depending on the type of fuel you run, it can cause some rubber goods to degrade and the performance of the car will suffer.\nYou also need to be concerned if you\'re running alcohol, as alcohol is hydroscopic, meaning that the alcohol will absorb water out of the air. So if you live in an area that has high-ambient humidity and you\'re not exercising great care to prevent your fuel from undergoing exposure to the air, you will get water in your fuel. Alcohol is also very corrosive to aluminum parts. If you allow fuel to sit in the lines or in the carburetor for extended periods, you will get corrosion. Even the steel portions of the fuel system are subject to rusting from alcohol fuels due to the higher instances of water in this fuel. The use of alcohol fuels will require greater fuel system maintenance just to keep the system clean.\nThe fuel line started out as a flexible neoprene line, then it was joined to a hard line a\nA big funnel with a filter in it and a translucent fuel jug. This Modified uses alcohol so\nIn this installation, the line exited the tank and was routed to a fuel filter, then to an', 'Alcohol Race Engines - Alcohol MaintenanceWritten by on August 14, 2007 We\'re Not Talking About Restocking The Liquor Cabinet, But How To Properly Take Care Of That High-Maintenance Alcohol Race Engine\nWe already know the main selling point for racing with alcohol fuel (methanol) in classes where it is allowed: In an engine tuned correctly it can make more power than gasoline. That\'s all you need to know, right? But alcohol is also a tricky fuel that is notoriously hard on engines, especially wherever aluminum is involved.\nThe problem with alcohol is the fuel evaporates quickly, attacks aluminum, and offers virtually no lubrication, unlike gasoline. These drawbacks don\'t offset the fact that you can use alcohol to make gobs of power, but care must be taken with your weekly and off-season maintenance programs. Some of these precautions may seem a bit unusual if you are switching from a gasoline class to one that allows alcohol, but they really aren\'t major once they become part of your routine.\n""The physical properties of the fuel are unique,"" explains Brad Cauzillo of Kinsler Fuel Injection, which manufactures some of the most respected mechanical fuel injection systems for Sprint Car racing. ""Methanol dries relatively quickly, and it attracts moisture which can contaminate the fuel. When it dries it first turns to a gel. In the next phase it starts to clump up, and in the last phase it turns into something a lot like sand."" In mechanical fuel pumps, the barrel valves, and other components used in alcohol systems, there are very tight metal-to-metal clearances, so getting sand into the system is detrimental.\n""Another problem is methanol causes scaling,"" continues Cauzillo. ""There is a chemical reaction that occurs between the aluminum and the methanol. That\'s why raw aluminum doesn\'t survive a long time if you don\'t maintain it. [Anodizing forms] a helpful protective coating, but the methanol will still eat through it.""\nCauzillo recommends flushing the fuel system after every race. It should be done the next day if possible. Believe it or not, one of the best ways to do this is with race gas. With a mechanical injection system, you can plumb the fuel pump to an external fuel tank, install a very lean main jet, and crank the engine. It should crank, sputter, and die, but that will be enough to adequately flush the fuel pump, clean out the main jet, and wash down the combustion chamber and cylinder walls. But Cauzillo says the backside of the high-speed and secondary jets will not see enough fuel pressure to operate, so they still need to be removed and cleaned by hand. A lubricant/dispersant such as Marvel Mystery Oil seems to work well for this purpose. Drain the fuel tank and cap off any fuel lines that may have alcohol in them. Since moisture in the air contaminates the methanol, the system must be sealed off from the atmosphere as much as possible.\nOf course, now that all the alcohol has been flushed out with gasoline (it needs to be race gas to avoid detonation problems), you need to have a way to get the gasoline out when the new season begins. It may take a little work to get the gasoline flushed out and the engine running on tune with the alcohol again, but it\'s worth it.\nIf you don\'t have that kind of time every week, Cauzillo says there is another option. A few fuel and oil manufacturers offer an alcohol additive that adds lubricity to the fuel. ""Lubricants can help reduce corrosion,"" he explains. ""There are a lot of products out there, and there are a handful we\'ve tested that work well. The M2 brand that VP Racing Fuels manufactures is an upper-lube additive and works well. It\'s translucent and doesn\'t change the weight of the fuel. All you do is add it to the fuel and go race. A quart mixes with 35 to 55 gallons. It helps reduce the corrosion and adds lubricity to the methanol. That works really well for the guys who might lose a crew member for a week whose job was to flush the fuel system.""\nIf you are running a carbureted engine, you can do the same thing. Many racers running carbureted alcohol systems keep a separate carburetor jetted for gasoline to flush the engine. Simply pull the alcohol carb, cap off the fuel feed lines that contain alcohol, bolt on the gasoline carburetor, and plumb a line from the fuel pump to an external fuel can. When you crank the engine and run it for a few seconds, it will flush everything from the fuel pump to the exhaust valves.\nNext, the alcohol carburetor needs to be cleaned. This is as simple as draining the float bowls, pulling the metering blocks, and blowing out the jets, the float bowls, and the main block. Follow that up by giving everything a nice coat of Marvel Mystery Oil before reassembly.\nAfter you have completed the final event of the race season and you are preparing your race car for winter storage, there are a few extra measures you should take. If you are racing a mechanically injected Sprint Car, Cauzillo recommends removing the mechanical pump and barrel valve, oiling them with Marvel, and capping them off. Next, disassemble your valves, clean them, and inspect for excessive wear on the poppet heads and on the diameter of the poppet. ""Also,"" he says, ""check the O-rings to see if they are swollen-maybe you have run some additives that have not worked well with that O-ring compound. If so, and you notice the O-rings are swollen, hard, or cracked, go ahead and replace them.\n""Disassemble and clean out the fuel filter. Inspect it very closely for debris. If you find any, you need to find out what it is-especially if it is steel or aluminum-and determine where it came from so you can resolve any problem. Finally, I recommend taking advantage of the off-season and sending the barrel valve and nozzles back to the manufacturer. We can flow-test them, make any corrections if necessary, and send them back to you. That way you will know that they will be ready for the start of the season.""\nThe last piece of the puzzle before packing the car away for the season is to clean the fuel cell. This is actually something you should do at the end of the racing season no matter if you are racing gasoline or alcohol.\nThe first thing to note is that all fuel cells are not created equal. If you run alcohol in a standard cell constructed for holding racing gasoline, mechanical troubles will be in your future. The methanol actually breaks down the foam used in a standard container. It is also damaging to the rubber bladder. Craig Mitchell of fuel cell manufacturer Aero Tech Laboratories (ATL) says that alcohol will cause standard foam to become gummy within a week or two. Shortly after that it will dry the foam, making it brittle. When this happens, parts of the foam will flake off and can become sucked into the fuel system. The result is a clogged fuel filter and potentially clogged jets in the carburetor. If you run alcohol, you should use a fuel cell constructed for alcohol. Those cells use special materials that are more resistant to alcohol racing fuel. It\'s always a good idea to drain all the alcohol left in the fuel cell after a night of racing instead of allowing it to sit in the fuel cell all week long.\nBesides draining the alcohol from the fuel cell during the week, after every race you should visually inspect the exterior of the fuel cell for dents, rips, or holes. Any of these, or a leak, will necessitate disassembly of the cell for further inspection. Otherwise, you are good to go.\nIf you have fuel cell damage, it is a good idea to break your fuel cell down into individual components and inspect everything thoroughly. This should be done at the end of the season, too. ""First, drain the fuel out,"" Mitchell says of ATL\'s recommended end-of-season procedures. ""Then rinse the cell thoroughly with water. That will dissipate any fumes that can cause an explosion. Now the fuel cell should be safe, so you don\'t have to worry about a spark. Disassemble the components and pull the foam out. The foam will be soaking wet with water. Just allow it to air dry. You don\'t want to try to wring it out or apply heat. Pull the bladder from the container and wipe it out so it is nice and dry. Wash out the metal container with soap and water to get out any dirt or rust. Wipe it dry, too.\n""After everything has dried out, go ahead and reassemble the fuel cell. Give your foam the pinch test. If you can pinch it with your fingers and some of it comes off into your hands, you need to have it replaced. If it springs back to shape, it\'s still OK. Give the bladder a visual inspection. If it\'s a rubber bladder, look for any dry cracks, like in a tire. Also check to see if there are any scrape marks from installing and removing the bladder into the metal container. If the bladder is fabric, see if there are any loose fibers or threads hanging out. If you have any dents in the metal container, go ahead and flatten them back out now. You don\'t want to have anything that can rub the bladder and wear it down.\n""If you have any doubt about anything, send the cell back to the manufacturer to have it further inspected. If not, you are ready to begin reassembly. We recommend installing the bladder and foam into the container and then positioning the pickup tube where you want it. Obviously, in circle track racing that\'s going to be the right-rear corner. Before attaching the line to the fill plate and bolting the fill plate to the container, take a moment to check the sealing washers. Sealing washers are the nylon washers that seal the bolts that hold down the fill plate. Make sure they aren\'t cracked or squashed out of shape. Also, we recommend you change the cork gasket that seals the fill plate against the container every off-season. You don\'t find out the fuel cell leaks at the gasket until you are upside down, and that\'s not what you want.""\nAfter your fuel cell has passed all the checks, continue bolting up the fill plate. Cap off all the lines so that everything is sealed and place the fuel cell in a protected, dry area of your shop where it will be out of the way until you are ready to reinstall it at the beginning of the next season.\nMore Photos From This Article\nAt the end of the season, pull the fuel cell from your race car and perform a complete disassembly and inspection. Begin by draining the cell and flushing out any remaining fuel and fumes with clean water. Here, the bolts holding the top of the cell\'s steel container are being removed. Courtesy of ATLPull the bladder from the container and check for cracks, which is a result of the bladder rubbing against a portion of the steel container. Courtesy of ATLPull the foam out of the bladder, dry the interior of the bladder with a clean cloth, and inspect the interior in the same manner the exterior was inspected. Courtesy of ATLDon\'t take a chance on an old gasket. Use a new cork fill-plate gasket every time you disassemble the fuel cell. Courtesy of ATL']"	['<urn:uuid:5566067d-e9fb-4d82-a611-7424d2979057>', '<urn:uuid:a66804bc-2a1d-45ea-b086-7e38958f5102>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	8	99	2757
14	strymon ojai power supply output voltage and current specifications	The Strymon Ojai has five outputs, all providing 9V and 500mA each. All outputs are DC, center negative and are double isolated.	['Building your first pedal board? Looking for clean, silent power for all of your effects pedals?\nThen you need a good pedal power supply.\nNo matter how many pedals you have in your chain or what level of musician you are, you’re going to need electricity fed to your effects pedals in order for them to operate properly.\nBut with all these terms like Voltage, Currency, and Isolation, how are you supposed to pick the right power supply for your needs and budget?\nThese are our top picks for the best pedal power supply in 2021.\nSnapshot: Best Pedal Power Supplies in 2021\n- Voodoo Lab Pedal Power 2 Plus – Best Overall\n- JOYO JP02 – Best Budget Option\n- Friedman Power Grid 10 – Best High End Option\n- Strymon Ojai – Best Isolated/Best Mini\n- JOYO JP05 – Best Rechargeable\nPicking a Great Pedal Power Supply That’s Right for Your Board\nWhen picking out the best pedalboard power supply, I kept in mind the following four essential pieces of information:\n- Voltage – Power requirement. Usually 9V, but ranges from 12V, 15V, 18V, even 24V. Minimum power must be met to power your pedal. NEVER add too much voltage – will damage your pedal.\n- Current – Flow of electrical charge, measured in milliamps (mA). Minimum current is a requirement and exceeding current minimum is ok.\n- Required Current – Rule of Thumb: add up mA requirements of all your pedals and double it. This is what your power supply needs to be rated.\n- Polarity – AC and DC. Most pedals are 9V, DC, Center-Negative\n- Isolation – means each pedal is individually powered, as opposed to daisy chain power. Isolated power is more reliable and reduces unwanted noise due to ground loops.\nThat Pedal Show refers to these requirements using the acronym VCPI. To learn more, check out their video on power supplies.\nMy list of the best pedalboard power supplies includes options that addressed the VCPI essentials in a safe and dynamic manner so that a wide array of pedals could be properly powered.\nMost notably, every option on this list features isolated power, as this is the best way to power your pedals.\nI also kept in mind that doing this can get expensive, so I’ve included some models that power for less, but not at the expense of quality. I also made sure to pick some smaller and rechargeable (battery powered) options for traveling/touring guitarists.\nThe Best Pedal Power Supply Options in 2021\n1. Voodoo Lab Pedal Power 2 Plus Review – Best Overall\n- A universal power supply for all battery-operated guitar pedal effects\nThere is no better place to start a list of the pedalboard power supplies than a tried and true staple of the industry. The Power Plus 2 from Voodoo Lab has been the go-to pedal power supply for professional guitarists for over a decade.\nThe Voodoo Lab Pedal Power 2 Plus is a compact power supply that is built like a tank and utilizes toroidal transformers for low noise. It requires a standard 120V power, so it can be used anywhere in the US. It has eight outputs, each with short circuit protection.\nEven if one of your pedals were to go down due to a short circuit, the rest would still be powered. The box comes with all the required connections, including a detachable AC chord.\nEach of the Power 2 Plus’s eight outputs are switchable between 9 and 12 volts, giving you greater flexibility for analog pedals. You will need a separate voltage-doubler if you want to power 18V pedals. Two of the outputs are designed to handle 250 mA of current with the included red cables.\nAll of the outputs are set for DC, Center Negative polarity and are 100% isolated.\n- Ultra Low Noise\n- Sturdy build ready for the road\n- Can Sag 4V to 9V on last two outputs\n- 250mA max current\n- Bulky, won’t fit under mini pedal boards.\n- Dimensions: 6 x 3.4 x 1.8”\n- Voltage: 9-12V Switchable\n- Current: 100/250mA (Rated 1100mA)\n- Isolated: Yes\n- Features: Sag, Low Noise, Highest Build quality,\nFinal Thoughts on the Voodoo Labs Pedal Power 2 Plus\nThough not the most versatile pedalboard power supply on the market, the Power 2 Plus is the perfect companion for analog pedal heavy boards. It has a solid construction that will last you a lifetime and is dead quiet. If you need other power requirements that this model doesn’t offer, I highly recommend looking at the rest of the Voodoo Labs power supply line.\n2. JOYO JP02 Review – Best Budget Option\nI get it – you want to spend your money on new pedals instead of a power supply. Thankfully, the JOYO JP02 is a highly rated and flexible power supply with a small price tag.\nThe JP02 from JOYO is the perfect pedalboard power supply for small pedal boards or those who are on a budget. Each of the ten outputs features independent short circuit prevention technology, so even if one pedal goes down the rest of your chain receives reliable and steady power.\nThe power supply has bright blue LED’s above each of the outputs, so you know that they are functioning. This is great for troubleshooting purposes. It features high performance, peripheral-simplified switching for an extremely low noise performance.\nThe first eight inputs are set for 9V, 100mA power that is ideal for overdrives and other analog pedals. The eight input is switchable up to 500mA and can power large Strymon or Line 6 digital modeling pedals. Output 9 is set for 12V and output 10 is set for 18V.\nThe supply can be powered by a DC18V input and is rated for 1.5 amps of current out. The entire supply is DC out and features all isolated power for each.\n- Easy to use and safe\n- Low noise\n- Low price point\n- Small build\n- Only one 500mA output\n- Dimensions: 5.75 x 2.56 x 1.38”\n- Voltage: 9/12/18V\n- Current: Rated 100/500 (Rated 1500mA)\n- Isolated: Yes\n- Features: LED’s\nFinal Thoughts on the JOYO JP02\nThis is a great option if you are purchasing your first power supply for your pedals, you have a small board, or if you are on a budget. It will power just about any analog pedal but may not be the best option if you have a wide array of high current digital pedals.\n3. Friedman Power Grid 10 Review (9V) – Best High End Option\n- Universal power from 100-240Vac meaning you can plug it in anywhere in the world\nOn the surface, the Power Grid 10 is like a Voodoo Lab Power 2 Plus on steroids. It has more inputs, more current potential, and also a larger price tag. In this case, the increase in price is very worth it once you realize what the Power Grid has to offer on the inside.\nThe Power Grid 10 is a lightweight power supply for your pedalboard built without traditional transformers in it. While it isn’t the smallest option around, it is built to fit perfectly under all Friedman pedal boards using just two screws.\nIf you aren’t using Friedman products, the Power Grid 10 can still be mounted on top of any board and is designed to be stable enough to have two standard sized pedals fit on top of it. It has universal power (100-240vac) so it can be plugged in anywhere around the world.\nThe box also comes with all the necessary cables and performs with zero hum, even with pedals on top of it trying to cause interference.\nWhile all ten outputs power 9V, the Power Grid 10 can power 18V pedals using the two slot cable included in the box. Because all then outputs allow for 350mA of current, this box can power just about any pedal on the market and many of them. As the front panel shows, all outputs are DC, Center Negative and are fully isolated for perfect power.\n- Universal Power (100-240vac)\n- 10 isolated, high current outputs\n- Zero hum\n- Dimensions: 4.5″ x 6.75″ x 1.75″\n- Voltage: 9V\n- Current: 350mA\n- Isolated: Yes\n- Features: Universal Power, Includes all cables, Riser with zero hum\nFinal Thoughts on the Friedman Power Grid 10\nFriedman makes some of the best pedalboard hardware around, including one of the best buffer pedals on the market. So it is no surprise that they will also make one of the best pedalboard power supply options ever. This will work for just about any pedal and it is the ultimate defense against noisy pedal boards.\n4. Strymon Ojai Pedal Power Supply Review – Best Mini/Best Isolated\nIf you have a small pedal board, real-estate is at a premium. You can’t afford to waste any space on your board for a pedalboard power supply, but you also need a power supply that can power the most demanding pedals. The Strymon Ojai (pronounced “Oh Hi”) is the most powerful supply in its class.\nThe Strymon Ojai is a small, powerful wonder of technology. Compared to all the other products on this list, the Ojai boasts big numbers in the currency department, while maintaining a manageable price point and a tiny footprint.\nIt is fully compatible with the rest of the Strymon power supply family using the 24V thru jack, so you can chain two or more Ojai’s together for larger pedalboards and it will remain low noise and high power. It also has LED’s to indicate which outputs are working.\nAll five of the outputs on the Strymon Ojai are 9V and a whopping 500mA, meaning that any pedal can be powered by this supply. It can even be used to power modelers like the Line 6 HX Stomp with the right connections. All of the outputs are DC, center negative and are double isolated. This makes this the best isolated pedal power supply on the market.\n- Incredibly small\n- High current\n- Low noise\n- Limited to 9V\n- Dimensions: 3.2″ x 2.3″ x 1.3”\n- Voltage: 9V\n- Current: 500mA\n- Isolated: Yes\n- Features: Expandable with 24V Thru, Dual Isolated outputs, LED’s\nFinal Thoughts on the Strymon Ojai\nFinding a powerful, reliable, and quiet power supply for your pedals was a challenge for a long time. Now, the Strymon Ojai has changed everything. This pedalboard power supply will fit under just about any pedalboard. While not the most dynamic in terms of voltage, it’s pretty crazy to think that power supplies twice as big as this can’t power as much as the Ojai.\n5. JOYO JP05 Review – Best Rechargeable Pedal Power Supply\n- With a built-in 7.4V/4400mAh rechargeable lithium battery pack, charging time is about 2.5hrs.\nSometimes a small pedalboard power supply isn’t good enough. Sometimes you need a small pedalboard that doesn’t need to plug into a wall to power all of your pedals. Whether you’re on tour or playing a small show outside, the JOYO JP05 is a high quality, battery powered supply that every traveling musician could benefit from using.\nThe JP05 steps things up a notch from the JP02 by incorporating a lithium battery to deliver its power. With 2.5 hours of charging time, you are rewarded with 10 hours of power time. If that isn’t enough, the JP05 is still capable of being powered through a DC 9V wall cable, and even if it comes unplugged the board continues to power your pedals through the battery.\nThe modern features don’t stop there. The JP05 also has a 5V USB output to power your phone or tablet. This will come in hand for all you working musicians out there that keep your charts on your phone. It is also small enough to fit under even the smallest of boards.\nSimilar to the JPO2, the JP05 has 4 DC 9V 100mA outputs, 3 DC 9V 500mA output, 1 DC 9V/12V/18V adjustable output, maximum output current is 100mA. All the outputs are DC, center negative, but the polarity can be reversed. Of course, all the outputs are fully isolated, making this a reliable, quiet, and extremely versatile power supply.\n- Low price point\n- Optional battery power\n- Wide range of current choices\n- USB output for phones and tablets\n- Only one jack for 12/18V\n- Dimensions: 7.83 x 6.77 x 2.76 inches\n- Voltage: 9/12/18V\n- Current: 100/500\n- Isolated: Yes\n- Features: USB output, LED, Rechargeable battery\nFinal Thoughts on the JOYO JP05\nOnce again, JOYO has created a game changing piece of gear at an unbeatable price. This might be the best pedalboard power supply for anyone looking to put together a small rig to take out on the road, or even for gigging musicians that need versatility when it comes to their power supply without breaking the bank. This is about as exciting as it gets when it comes to powering your pedals.\nFind The Right Pedalboard Supply For Your Needs\nDoing just a little bit of research about what kind of power your pedals need can go a long way in building your confidence around gear, as well as give you peace of mind knowing that your pedals are operating at their best.\nChoosing the best quality pedalboard power supply not only gives you a quieter signal, but it also protects your pedals and ensures that they will work year after year, not matter where you take them.\nI’ll be the first to admit that spending money on a power supply for your pedal board isn’t as exciting as buying a great tremolo pedal or a great delay pedal, but you will only be able to get the most out of your pedals if you give them the right power. Thankfully, doing this doesn’t have to break the bank.\nIt is my hope that this list has helped explain how power supplies work and how you can use that info to make an informed decision about what kind of pedal power supply you need.\nOnce your pedals are powered, all that’s left to do is play the guitar!\n- 6 Best Phaser Pedals In 2021 (Mini, Budget & High-End)\n- What Does A Wah Pedal Do?\n- What Does A Compressor Pedal Do & How Do They Work?\n- Guitar Effects Pedals Explained (How They Work & Our Favorites)\n- What Does A Chorus Pedal Do?\nDavis Wilton Bader is a professional guitarist/writer based out of St. Louis, MO. He plays in the bands Lumet and The Outskirts.']	['<urn:uuid:1d13adc4-bb58-48ec-a280-e60f182c7799>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	9	22	2436
15	compare early payoff costs penalties regular mortgage	For regular mortgages, early repayment charges apply if you repay before the agreed term. These include an Early Repayment Charge as mentioned in the closing costs. For closed mortgages specifically, the prepayment penalty is calculated as the higher of either three months of interest on the remaining mortgage amount or the interest rate differential (IRD). For example, on a $200,000 mortgage at 6% interest, the three-month penalty would be $3,000. However, if using the IRD method with 36 months remaining and a current rate of 4%, the penalty could be as high as $12,000. These costs can make early payoff significantly expensive in both types of mortgages.	['Surprises are not always good! When it comes to mortgage, you have to make sure you understand all the fees and charges involved before making a choice.\nAmidst the thick pile of paperwork filled with the technical jargons, one gets tempted to skip the details and make the buying decision in haste. But since home is the most precious investment you are going to make in your life, every detail is worth a read! Keeping a track of the fine print makes sure you understand the real cost of the mortgage with all its ins and outs.\nOne should realize that there’s more to mortgage than just the principal and interest amount. Closing cost is the sum total of various fees payable at the time of closing a real estate transaction (the moment when property is actually transferred to the buyer). These costs are in addition to the down payment on the home. Closing costs are of two types, recurring and non-recurring.\n• Recurring Costs primarily consist of real estate taxes and homeowner’s insurance.\n• Non-Recurring Costs consist of various one-time fees such as\norigination fee, application fee, appraisal fee, underwriting fee, document preparation fee, etcetera\nClosing costs are either paid entirely by the buyer or partially by the buyer-and-seller in various proportions. The usual closing cost amount to 2-5% of the home price. So on a home that costs you $100,000, you will be liable to pay $2000 – $5000 as closing costs.\nThe closing costs involve may vary from plan to plan and consist of the following types of recurring and non-recurring costs –\n1. Booking Fees – Some mortgage plans come with a booking fee that is payable upfront. Other plans do not involve such fee.\n2. Mortgage Broker Fee – The fee a broker charges for advising and arranging a mortgage that meets your specific loan needs. If the broker charges no fees, the chances are that he’ll earn his commission from the lender.\n3. Mortgage Arrangement Fee – The fee charged by the lender for arranging your mortgage. This fee is inversely proportional to the interest rate. The lower the rate, the higher the fee.\n4. Early Repayment Charge – If you intend to repay your mortgage earlier than the agreed term, you will be liable to pay this charge.\n5. Legal Fee – It consists of the costs incurred on fulfilling paperwork and legal obligations involved in buying/selling of property. This fee varies from one jurisdiction to other.\n6. Origination Fee – This is relevant when you approach a lender through a broker. The broker paves the way to get the loan approved for you. As a high risk borrower, you will be liable to pay higher origination fee and vice-versa.\n7. Underwriting Fee – This is the fee that the lender charges for underwriting the loan. This fee is not payable when you apply for a mortgage through a broker.\n8. Application Fee – Lenders and brokers can pull out your credit reports virtually at no cost for the simple reason that they do it very frequently. But they charge a fee for it from the borrower anyway. This fee is known as Application fee.\n9. Appraisal Fee – Many lenders make appraisal a precondition for approving a mortgage loan. An appraisal involves a licensed appraiser verifying the property’s sale price against its current market value. Appraisal fee is usually paid by the buyer.\n10. Homeowner’s Insurance – While closing the deal, the borrower needs to produce a receipt of a full year advance payment of homeowner’s insurance premium. If the property comes under a flood hazard area, you need to include the flood insurance clause in your insurance plan.\n11. Mortgage Insurance – The premium of mortgage insurance depends on the type of loan and the down payment made on it.\n12. Property Taxes – Before the property is actually transferred to the buyer, one has to pay the due taxes on the property, if any.\n13. Survey Fee – Sometimes the property (the land as well as structures on it) needs to be surveyed, especially in case of commercial property. Survey fee needs to be paid to the surveying company.\n14. Inspection Fee – Some lenders are very specific about making sure that the property is in good condition when the deal is closed. Inspection fee is paid by either party to licensed home inspectors.\nA Word of Caution\nHave you been offered a ‘no-fee mortgage’ of late? You might land yourself in a trap if you are taken in by such loans without having a proper understanding of them. In a typical no fee mortgage, the borrower is not charged of the usual fees associated with mortgage including processing fee, appraisal fee, underwriting fee and so forth. Rather, your lender pays all the fees involved and in return you agree to pay back the mortgage at a higher interest rate. Though the high interest you pay on your mortgage is tax deductible, but you eventually end up paying more than what you would have paid on a regular mortgage. Somewhat similar are ‘super cheap mortgages’ that are offered at a surprisingly low interest rate but involve a huge fees.\nIf experts are to be believed, the borrowers should avoid both these luring deals and go for plain mortgages.', 'Table of Contents\n- What Is A Closed Mortgage?\n- Costs Involved In Paying Off A Closed Mortgage Early\n- Pros And Cons Of Breaking A Closed Mortgage Contract\n- How To Reduce Or Avoid Prepayment Penalties?\nPaying off an open mortgage early is certainly possible. But what about a closed mortgage contract? Is it possible to exit from a closed mortgage contract early? The answer is YES!\nBut paying it off early is a little bit trickier than you might have hoped. Be it an open or closed mortgage, paying early in both cases tags along with some costs.\nIn the case of a closed mortgage, the prepayment penalty, breakage costs, and other charges could be higher–sometimes to the extent that paying off early becomes a costly affair. Therefore, a thorough calculation before deciding to end a closed mortgage contract is a must! Visit this site to know the differences between an open and closed mortgage.\nWhat Is A Closed Mortgage?\nA closed mortgage, also known as a closed-end mortgage, is a limiting mortgage that prohibits borrowers from paying off, renegotiating, or refinancing the mortgage early. Specific prepayment penalties, breakage costs, and other charges are involved in renegotiating or paying off the mortgage early.\nA closed mortgage is preferred over an open-end mortgage due to lower interest rates. But it limits the borrower’s ability to use the house’s equity as collateral to refinance before its term ends.\nGenerally, you should opt for a closed mortgage if:\n- You have a long-term vision\n- You’re not going to move or sell early\n- You’re searching for attractive interest rates\nPros Of A Closed Mortgage\n- It offers lower interest rates to homebuyers.\n- Borrowers have the assurance that the interest rates will not vary till the end of the term.\n- It offers substantial time to repay the amount.\n- It adds a sense of security to people with fixed, regular incomes.\nCons Of A Closed Mortgage\n- It limits homebuyers to pay off, negotiate, and refinance the mortgage before the term ends.\n- Repayment penalties, breakage costs, and other charges make it too costly.\n- It is challenging to refinance a closed mortgage.\n- It is a less flexible model.\nReasons To Break A Closed Mortgage Contract\nSeveral factors influence a homebuyer to pay off a closed mortgage early. The most popular driving factors are:\n- A homebuyer wants to sell the home before the mortgage term ends.\n- Market stability and growth have cut down the interest rates.\n- A borrower’s income has increased, so they want to pay off the mortgage early.\n- Refinancing to find a better rate.\n- Fulfilling the purpose of buying the home.\n- Moving out of the property for some reason.\nCosts Involved In Paying Off A Closed Mortgage Early\nDeviating from the closed mortgage contract could impose a prepayment penalty, charge, or breakage cost. The lender will charge a prepayment penalty if a borrower:\n- Wants to pay more than what is in agreement.\n- Opts to break a closed mortgage contract.\n- Transfers the mortgage to another lender.\n- Wants to renegotiate the terms of the contract.\nIf not planned or in case of an uncalculated decision, a borrower may end up paying more on breaking the close mortgage contract than they’d have otherwise paid on abiding by the contract.\nTo safeguard the interests of borrowers, an option called prepayment privilege is available. Prepayment privilege is a powerful tool that eliminates prepayment penalties for breaking a closed mortgage contract. It offers:\n- Flexibility to increase the regular payments up to a certain limit\n- Make lump-sum payments of up to a certain percentage\nCalculation Of Prepayment Penalties\nThe calculation of prepayment penalties depends upon several factors like repayment amount, remaining term, and lender. Most established lenders, financial institutions, and banks provide a prepayment penalty calculator on their official websites.\nGenerally, lenders charge a penalty equal to three months of interest on the remaining mortgage amount or the interest rate differential (IRD), whichever is higher. They calculate IRD when:\n- The current mortgage rate is lower than your current mortgage interest rate.\n- Your mortgage is not older than five years.\nExample Of Prepayment Penalty Calculation\nLet’s assume the outstanding amount is $200,000 and the current interest rate is 6 percent. So, the prepayment penalty will be three months’ interest.\n200,000 x 0.06 = 12,000\n12,000 / 12 = 1,000\nSo, 1,000 x 3 = $3,000 will be the prepayment penalty.\nNote that the IRD calculation comes into effect if 36 months are left from the entire term of five years.\nSuppose the current interest rate is 4 percent. Here, the difference between rates becomes 2 percent.\n$200,000 x 0.02 = $4,000\nNext, divide the term left with 12:\n36/12 = 3.\nFinally, $4,000 x 3= $12,000 will be the prepayment penalty as per the IRD method.\nLenders will charge $12,000 since it’s the highest of the two figures.\nPros And Cons Of Breaking A Closed Mortgage Contract\nGenerally, a borrower will break a closed mortgage contract if he comes across a better interest rate. Therefore, it’s advisable to weigh its pros and cons before going through with it.\n- Better mortgage rate with an extended-term.\n- You get peace of mind if you pay off the mortgage early.\n- Homeowners can use their house’s equipment to refinance their other needs.\n- You get an option to renegotiate the mortgage rate and term.\n- The idea of paying off early or renegotiating is to put off the burden and save the remaining interest. But the hidden charges and prepayment penalty associated with breaking the contract could cost you more in the long run.\n- There’s a possibility that a homebuyer fails to secure a new mortgage or refinance under stringent new economic conditions.\nHow To Reduce Or Avoid Prepayment Penalties?\nThe first step should be to understand the terms and conditions of the contract before inking it. Some useful tips to avoid prepayment penalties are:\n- Include the prepayment privilege option in the contract.\n- Utilize the prepayment privilege option fully to repay the mortgage amount early–this helps reduce the mortgage amount significantly. Keep in mind that lenders calculate a prepayment penalty on the remaining balance.\n- You can port your mortgage or blend and extend the mortgage to avoid prepayment penalties.\n- Wait till the term ends to reduce the prepayment penalty.\nAlways calculate the potential costs associated with breaking a closed mortgage contract. If ignored, you end up paying more in the form of prepayment penalties and other charges. Lenders are advised to research, compare, and seek expert advice before signing or breaking a closed mortgage contract.']	['<urn:uuid:e903f197-b19d-4a34-b0eb-f9b3bf9c0320>', '<urn:uuid:888eb12d-9fdc-43f2-8aed-866d42378502>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:56:11.774636	7	107	2006
16	I'm interested in sword design - between the Senlac and Baron swords, which one has a fuller blade?	The Senlac sword has a fuller (a groove in the blade) that is not quite as wide as a Type X sword, while the Baron sword has a lenticular cross-section without mention of a fuller, indicating the Senlac has the fuller blade between the two.	"[""A Resource for Historic Arms and Armour Collectors\nAlbion Armorers Next Generation Senlac Sword\nA hands-on review by Bryan Heff\nIn the year 1066, during the twilight of the Viking Age, a desperate struggle for control of England came to a head. Duke William of Normandy, with claims to the throne of England, landed his Norman army consisting of mixed troops that included a large contingent of heavy cavalry on the Southeastern coast of England. King Harold II of England and his Saxon army, fresh from a hard fought battle and difficult march from the north, had quickly returned south in an effort to stop this Norman challenge to his throne. The Saxon took up a defensive formation with a traditional shield wall, high atop a hill along the London road in an effort to block the Normans' march north and bring them to decisive battle. Several weeks before, this same Saxon army with its Huscarl core of heavy infantry had successfully smashed an invading Viking army at Stamford Bridge. The fate of the island now lay at the feet of these two armies. By the end of battle a victor would emerge and history would be set. The Battle of Hastings was about to unfold and its outcome would help set the stage for European battle tactics for years to come. The battle was fought on a hill called Senlac, and swords similar to the subject of this review would have been used on both sides. Whether being swung from horseback or from behind large shields, these slashing cutting blades designed to oppose mail and textile armour were the order of the day.\nAlbion Armorers Next Generation Senlac is a long bladed single-hand medieval sword best described using Ewart Oakeshott's typology as a Type Xa, a cousin to the broad cutting blades of Type X. Being slightly longer, having a fuller that is not quite as wide and with a more pronounced profile taper is what essentially differentiates the Type Xa subclass. While potentially more suited to a thrust than a Type X, its primary purpose would be in the cut. Albion produces three swords all using the same blade. Those swords are the Senlac, Norman and Knud. The Norman and Senlac are very similar, differing most significantly in the cross-guard only. The riser patterns on the grip are also different between the two but not greatly so. The Senlac is a no-nonsense sword with little adornment. It is a sword that soldiers could have worn at their hip from Hastings all the way to the gates of Jerusalem during the First Crusade. This is a sword a man could rely on with confidence.\nMeasurements and Specifications:\nReplica created by Albion Armorers of Wisconsin.\nThe Senlac is a long-bladed weapon with a fairly short grip intended solely for single-hand use. These combined factors could create a sword that is sluggish. This is not the case here. The handling is really quite nice. No cutting was done with the sword prior to this review, just basic dry handling movements to gauge how it feels; smooth would be the word most adept at describing its overall characteristics. To borrow from boxing terminology, the blade could be described as a middle-weight. The point of balance is not particularly close to the hilt so it displays nice blade presence. It is a weapon that allows quick attacks and recovery but still has enough heft to deliver vicious slashing and hewing cuts as well.\nOften swords like this would have been used as side arms, not necessarily the primary weapon. The handling speaks volumes to this kind of usage. It is a sword agile enough to be used for a long period of time if need be while being light enough to wear on long campaign marches; it has enough punch to get its wielder out of a jam.\nFit and Finish\nOne aspect of the blade that really stands out is the slight convex curvature of the profile the final 1/3 of the blade. Many production swords available today have very straight lines along the blade's profile until the final tip is formed. The ever so slight convex curve of the Senlac's profile really gives it a beautiful blade presence and is one of the subtle details that helps make what seems like a simple sword much more.\nThe pommel is a disc-shaped Oakeshott Type G. It is simple in form, not overly complex, and could be easily described as quite plain. It does however have some rounding of the faces from the center out to the edges which gives it an element of complexity that differs from a flat disc. It is also not a perfectly round circle in shape but comes out of round ever so slightly where the pommel meets the grip. The peen is blended into the pommel so well that you simply cannot see it unless the light hits it in just the right way. The convex sides of the pommel are not perfect in terms of symmetry, but come very close to it and I think provide a good example of the hand work on the sword. This is not a criticism so much as an observation. It adds a warm handmade look and feel.\nThe guard is a straight, very basic Oakeshott Style 1a. The guard slot is expertly cut and fitted to the blade with extremely tight tolerances. The guard is rectangular in cross-section where the blade slot is located and then tapers down to a thinner square as it moves away from the blade. What struck me right away is how thin the cross-guard is compared to heavier/bulkier cross-guards more common on lower-priced swords. It's almost elegant in its utilitarian and lean design. There is not a lot of extra unneeded steel in the guard, just enough to accomplish its purpose.\nThe Senlac is unquestionably a simple and plain sword at first glance. This may be a deterrent to some sword enthusiasts especially at its price point. I think the argument could be made that along with their Bayeux and Hospitaller models, it is the least adorned medieval sword Albion Armorers produces in terms of hilt components. With all that in mind this reviewer has come to the conclusion that the Senlac is simply and clearly a classic medieval cruciform sword. It is perhaps the classic sword type for this post-Viking Age time period and it makes perfect sense that Albion would want such an iconic design as part of its Next Generation line. The straight guard and disc pommel are function over form in its purest sense. There is beauty in that simplicity. Its clean and straight lines are pleasing to the eye. It has an overall balance in both looks as well as handling that hit the mark on stylistic sensibilities that subscribes to the less is more ideal and it delivers on that extremely well. The Senlac is a beautifully understated but classic sword design.\nAbout the Author\nBryan Heff is a Systems Analyst living in the Philadelphia suburbs with his wife and 2 sons. He has always been intrigued by the European sword as a work of art, symbol and weapon of war. His main area of interest is rooted in the 8th to 14th centuries but he has interests in other time periods as well. In addition to adding to his small but growing collection of swords, he enjoys modifying and customizing swords in his compact basement workshop, hiking and exploring historic sites and trying to get his boys to do their homework.\nPhotographer: Bryan Heff"", 'Albion Armorers Next Generation Baron Sword\nA hands-on review by Patrick Kelly, with comments from Peter Johnsson and Chad Arnow\nThe world is an ever changing place. Nowhere is this more evident than in the field of arms development. Ever since the first caveman slew his neighbor with a rock, mankind has been looking for more efficient ways to bring harm to his fellows. The period we know as the Middle Ages was no different. (I wonder if people knew that they were living in the ""middle"" back then.) This period saw a constant change and evolution in arms and armor. The standard form of armor during this age was mail (erroneously known today as ""chain mail""). Mail had already been in use for centuries and was well established. By the mid-13th century the medieval knight\'s body had become fully encased in mail. Full length sleeves with mittens had been added to the Hauberk, as well as an integral hood known as a Coif. Mail hose called Chausses now protected the knight\'s legs.\nFor several centuries the single handed sword had served the warrior as his standard sidearm, and would continue to do so throughout the Middle Ages. However, by the mid-13th century it was felt that a larger and more powerful sword was needed to deal with the improving body defenses then in use. The result was a sword that would become a battlefield mainstay, in one form or another, and would remain so throughout the Middle Ages, and into the early renaissance. In its early form this sword was referred to as Espee de Guerrel (""Grete war sword""), as well as many other names which ably described both its size and purpose. These swords were what we today call a ""Hand and a Half"" sword, or rather swords that can be utilized for either one-handed or two-handed use.\nWhile similar to their single-handed brothers in overall design, they featured a much longer two-handed grip, as well as a correspondingly longer blade. Average weights appear to have ranged from 3.5 to 4 pounds with blade lengths of 35 to 39 inches. These were not swords designed for riding to the county faire, or for collecting the local taxes. These medieval war swords were meant to deliver large cleaving blows in the heat of battle. The late Ewart Oakeshott gave these swords two different classifications in his well-known typology; those of XIIa and XIIIa. While both shared the previously mentioned overall dimensions, the XIIIa sported a blade which featured very little taper in the blade\'s profile, as well as a spatulate point. The type XIIa featured a blade that, while still broad at the base, tapered to a much more acute point. An example of the later, as manufactured by Albion Armorers, is the subject of this review.\nIn mid-2003 a decision was made to take the company\'s line in a new direction. While the then extant line (now known as the First Generation) had been generally well received by the arms collecting community, it was felt that the line didn\'t quite fit with Albion\'s ultimate goal. With noted Swedish swordsmith Peter Johnsson working as the company\'s foremost designer and researcher it was decided to offer a line of swords with more historical authenticity. Details such as blade geometry, overall proportion, and hilt construction were addressed. Exhaustive hands-on research was conducted by Peter Johnsson and Albion\'s Master Cutler Eric McHugh. Many existing original swords were examined and measured, with the intent of incorporating the information into a new product line. Shortly thereafter, Albion introduced this Next Generation line of swords. To say that this new line has been well received would be an understatement.\nDetails have been incorporated into blade design that have not been commonly seen in the production field. Gone is the method of using one generic blade for a broad range of swords, with mix and match hilt components. Individual blade types are designed from the ground up in terms of geometry, mass, proportion, and strength. A longsword type from the high Middle Ages will have a dramatically different blade than one from the late Middle Ages.\nMeasurements and Specifications:\nReplica created by Albion Armorers of Wisconsin.\nOne of the key differences in the Next Generation line concerns hilt construction. Many, if not most, modern production replica swords use a means of hilt securement which utilizes a threaded tang, a cold peened tang, or a combination of both to secure the entire hilt. The hilt components are slipped onto the blade\'s tang, and are then held in place by a screw-on pommel, pommel nut, or a combination of pommel nut and a cold peened tang. These methods can produce a serviceable sword but are far from ideal, or historically accurate for many of the types of swords being manufactured. This method places a more than ideal level of stress on the end of the tang, and the hilt components as well.\nIt can be said that Albion has gone backward instead of forward on this issue. Gone are the slip-on components and threaded pommels. Hilts are now assembled in the time proven historic method. Guard and pommel are tightly wedged onto the tang. The underside of both components is then peened against the tang, thereby forming a permanent assembly. The tang end is then heated and hot peened against the top of the pommel or a separate rivet block. Once this has been accomplished, the grip, in the form of two hollowed out sections, is then mounted onto the tang. The grip is then covered with cord and leather. The result is a hilt comprised of components that are self supporting. Every component bears its own share of the stresses involved in maintaining the assembly. This process results in a hilt assembly that should remain tight throughout the life of the sword. You may be able to loosen it but you\'d destroy the sword in the process. When these historic methods are combined with the modern manufacturing methods of CNC machining and investment casting the result is a sword of excellent quality and construction, yet is still affordable.\nThe Baron\'s designer Peter Johnsson has this to say concerning the sword:\nWhen starting to work on the design on this one there were a few things that were in focus. The big war-sword of Type XIIa is an absolutely classic type. I wanted the Baron to make justice to the best of those XIIa blades that I\'ve experienced.The Baron was one of the first swords completed and produced for the Next Generation line. It\'s proven to be a very good way to begin.\nThe goal for the Baron was to represent a large-sized weapon with powerful and quick handling and an immediate and compelling visual impact. I have seen and handled quite a few of these in museum storerooms and they have left strong impressions with me. Perhaps the most important aspect is how they combine imposing size with very pleasant handling characteristics.\nThe dimensions of the blade changes in a very dynamic way from the base to the point: from thick and sturdy to crisp and relatively thin, while still retaining a working stiffness. When seeing well preserved examples up close it is possible to see how economical and effective the material is used to achieve an effective combination of blade and edge geometry, strength, and agile balance. These were important factors I wanted the Baron to exhibit.\nA more subtle but still crucial aspect is the use of harmonic proportions in the design. The practical application of harmonic proportions is a design tool that has been used by artisans, artists, and craftsmen since the dawn of history. It is fascinating to see how these principles are implemented in the craft of sword-smiths and cutlers as well. The most striking and beautiful examples of the craft through the ages often follow these principles in many aspects: linear dimensions and volumes as well as dynamic properties. The Baron is a study of the use of harmonic proportions considering both the physical and visual properties of the sword.\nThe big war-sword of the high Middle Ages is something that I have always found fascinating. The originals are close to that archetypical sword we all imagine when we hear the word ""sword"". I was happy to get the opportunity to start working with one of these weapons early on in the new Next Generation line.\nFit and Finish\nChad Arnow, an owner of the Albion Baron sword, makes these observations:\nThe small fit and finish and small detail work on this sword were unlike any other production-level sword I\'ve ever owned. The blade is shaped very smoothly, showing no ripples or waves in its surface. The fuller is very well-executed, and its end transitions smoothly into the rest of the blade. The blade\'s edges have no discernable extra bevel. The octagonal cross-section of the guard is very crisply done, and shows the kind of three dimensional properties that are commonly seen on period originals. The grip is quite comfortable and secure, with the cord and leather wrapping. The pommel and cross are very cleanly cast, showing far fewer casting flaws than many other production swords. The pommel tapers in thickness, being thicker toward the hand. This is a nice aesthetic touch, and also should help in terms of mass distribution. The pommel on mine is not quite symmetrical, giving a cast hilt piece a very hand-made look. This adds to the overall feel of a sword much older than it is.Handling Characteristics\nWhen first examining the Baron one is struck by the overall size of the piece. This is a large sword. This impression of size leads to quite a surprise when the sword is first placed into hand. The sword\'s nearly four pound weight is belied by its balance. The Baron exhibits outstanding handling qualities. The blade features a rather radical distal taper. The blade\'s base is almost a quarter of an inch thick yet dramatically tapers to the point. The last quarter of the blade seems like a large heavy-duty carving knife. However, this radical taper does not result in a blade that is by any means weak. The rather broad blade still maintains enough mass in its cross-section so that a good strong cutting edge is maintained. When combined with a pronounced profile taper, this distal taper results in a distribution of mass that makes for a very swift handling sword. While the Baron handles well when used one-handed it really excels when used in a two-handed fashion. Given that this type of sword was designed to deliver large cleaving and slashing blows, this two-handed method was obviously preferred. While the blade does possess a serviceable point its lenticular cross-section does not allow for the kind of rigidity necessary for a dedicated thrusting blade. Cutting is the obvious priority for the Baron. Cutting tatami mats revealed no surprises with the Baron. All cuts were cleanly and easily executed. The sword tracks well in the cut, and doesn\'t seem to be too demanding in strike placement. Cuts delivered on either side of the blade\'s center of percussion were easily executed with no discernable vibration. While the Baron could be used in the later styles of longsword play that are currently very popular, it really won\'t give best service in this regard, a later period design should be chosen for these pursuits. It will, however, give outstanding service in the earlier style of swordplay for which it was designed.\nChad Arnow adds these comments regarding his Baron:\nThis sword, while very large, does not handle sluggishly at all. In fact, it is much livelier and quicker in the hands than some swords of similar dimension that weigh less. Gripped with two hands, it flows well through short chops, as well as long sweeping cuts. The grip is very comfortable for gloved or bare hands, and the sword handles equally well whether the cross is ""fingered"" or not. There is plenty of room on the grip for my two rather large hands. This sword would definitely be quite a cleaver when used in battle. This sword could be used one-handed, but for this reviewer and his limited cutting experience, the sword seemed much slower when used this way.Conclusion\nAlbion\'s Baron is a sword which possesses an austere beauty which, when combined with the subtle details of shape, mass, and proportion, results in the recreation of an absolutely classic design from the high Middle Ages. This type of early war-sword has always been one of my favorites. Consequently, I have owned many production samples over the last two and a half decades. Every one of those previously owned swords lacked something in aesthetics, construction, performance, or all of the above. Not so with the Baron. This sword combines the qualities of appearance and performance that have already caused many to take notice of Albion\'s Next Generation line. Whether a collector, or martial artist who desires their very own ""Grete war sword"", one need look no further than the Baron by Albion Armorers.\nAbout the Author\nPatrick is a State Trooper serving with the Kansas Highway Patrol. He has been fascinated with edged weapons, particularly the medieval sword, since early childhood. Not only is Patrick thankful for any opportunity to indulge in his favorite hobby, he is also blessed with a wife who tolerates a house full of sharp pointy things.\nI want to extend my special thanks to Peter Johnsson and Chad Arnow for their comments and observations.\nPhotographer: Chad Arnow']"	['<urn:uuid:9a7aa7bd-203d-45b3-bab1-63a32baf3d37>', '<urn:uuid:d3dc587a-be40-456a-a678-cc14d44899ce>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T04:56:11.774636	18	45	3527
17	As an AI researcher, I'm curious how algorithms and AI applications in healthcare intersect. What's their role in diagnosis?	AI algorithms and healthcare applications work together in diagnosis through pattern recognition and data processing. AI-based algorithms can spot patterns more efficiently than humans and analyze patient data for diagnosis. They process real-time health data like blood pressure and heartbeat, and help pathologists analyze genes and tissues for more accurate disease diagnosis. These algorithms follow precise, non-ambiguous steps and must finish in finite time, ensuring correctness for all inputs.	"['Introduction to Algorithms\nIntroduction to Algorithms\nWhat\'s an Algorithm?\nDefinition - Algorithm: a set of instructions, or rules that, starting with initial data, solves a class of problems using precise operations, executed mechanically, without creative human intervention.\nCooking recipes could be thought of as algorithms. They enumerate a step-by-step procedure which achieves a task (the final dish) starting with initial data (ingredients) and asking you to execute relatively straight forward instructions. Yet, any of us who tried to use a cookbook know that \'straight forward\' is such a relative term. Recipes tend to be ambiguous and could potentially be a nightmare for the beginner chef. So, then, what should we ask of a good recipe?\nTo qualify as an algorithm, the set of instructions should have some properties:\n- Preciseness - Or non-ambiguity. Each step must be non-ambiguous and executable without creative intervention.\n- Generality - It should solve a class of problems, not just one particular problem.\n- Finiteness - The algorithm must finish in a finite amount of time. For all practical purposes it should finish in reasonable time (e.g. before we die).\n- Correctness - The end result should be correct for all inputs.\nDo recipes have the above properties? Supposedly they are correct, though some of your dinner guests might disagree (if they feed it to your dog while nobody\'s watching that might be a sign). They are also finite. What they lack is generality, since they cook one dish using fixed input. And what most of them lack is preciseness, which, on one hand makes them bad examples of algorithms, but on the other hand leaves way for creativity and uniqueness.\nThe word algorithm has interesting roots. Some say it comes from al-Khwārizmī, a Persian mathematician who lived at around 800 and wrote books on linear and quadratic equations. Other people say it is an anagram of logarithm, a mathematical operation which is an inverse of the exponential. In the old days there seemed to be little distinction between algorithms and \'math recipes\'. For a long time the word was linked with Euclid\'s algorithm for computing the greatest common divisor. See The timeline of algorithms for more details. Also, for a more detailed introduction to the concept visit Algorithm. We\'ll move on to see how we can create and describe algorithms.\nFlow charts are one way to describe algorithms (another way is pseudocode, but we won\'t be dealing with that one here). They consist of the following symbols:\nAs the name suggests, these boxes are used to compute things. They usually comprise mathematical computations. Examples:\n- computes the expression\n2 + 3and stores the result (in this case\n- takes the old value of\n1to it and stores the result in variable\nx, overwriting the old value. If\n5, the final value will be\n6, after executing this block.\n- negates the value stored in\nxand assigns it to\ny. In our case\nThese boxes deal with the flow of data in and out of our algorithm. We can read (or input) initial values, as well as write (or output) the results. Examples:\n- asks for a value to be entered and stores that value in\nx. The \'R\' is for \'read\'.\n- asks for two values, one to be stored in\nxand the other in\n- writes (or outputs, displays) the value currently stored in\ny. The \'W\' is for \'write\'.\nA decision symbol is used to create a \'fork on the road\'. It branches the execution depending on the result of the condition inside the box. If the expression is true, the execution of the algorithm continues on the right branch. If the expression is false the execution continues on the left branch. Examples:\n- tests if\nxis less than zero, when reaching this box in the execution of the algorithm. If\nxis less than zero, we move on to the boxes on the right branch. If\nxis greater than zero, or equal to zero, we continue on the left branch.\n- tests if\nx\'s value is at least\ny\'s value plus\n3. If so, we continue on the right branch. If not, we continue on the left branch.\nWe\'ll get a better idea about decision boxes when we design our first flow chart.\nI wouldn\'t even call this a proper symbol, we could probably do without it just fine. Its only role is to set where the algorithm starts and where it ends. Here are, for now, the two terminators we will use:\nExample one: Absolute value\nWe will now plunge into the hot waters of flow charts by designing our first one. And no, it\'s not the \'hello world\' example, it\'s boring and so classical. Let us try to compute the following math function:\nWhat function is this? Indeed, it\'s the absolute value, or modulus function. Let\'s build a flow chart that computes it:\nTo execute a flow chart, we start at the START symbol and follow the arrows. First thing we read\nx, the number for which we want to compute the absolute value. Then we test if it\'s less than zero. If so, we follow the right branch, the\nyes branch, where we compute\ny, the absolute value, as negative\nx is greater or equal to zero than we follow the\nno branch, making\nx the value for\ny holds the value of the function\nf(x). In the end we display\ny and we stop at the STOP symbol.\nThe flow chart implements the definition function exactly. One alternative is to display the result on each branch, without storing it in\ny (which means we don\'t really need\ny, we used it for extra clarity).\nExample two: Sum of the first n integers\nHaving successfully survived the encounter with our first algorithm, let\'s go on with a second one, shall we? Let\'s compute the following summation:\nIn human language that is the sum of the first integers:\nNumber is the input of our algorithm, while number is the output.\nOne way to do this is to have a counter, a variable that counts from to ; let\'s name it . More precisely, will hold all values from to , one after another. For that, we create a loop in which is increased by on each iteration:\nk ← k + 1 is interpreted as taking the old value of\nk, then adding\n1 to it, and then storing it as the new value of\nk. Therefore, this flow chart section will add one, to\nk, in a loop. However, it will never end (and we know that is not quite an algorithm). To fix it, we have to add a condition to exit the loop when\nk exceeds :\nThere is one more thing we need to do: our counter has to start somewhere. Let\'s initialize it with\n1, the first number we need to add:\nNow, that we have a counter which cycles through the numbers we need to add, all that\'s left to do is add it to our sum\nS. The sum needs to be initialized, as well. What should its initial value be? Math teaches us that zero is probably a good candidate, since it is the neutral value for addition.\nk has dual purpose, in our algorithm. First, it makes sure that the loop is executed\nn times. Second, it is used in computing the sum. In general, counters control the number of times the loop gets executed, but they don\'t always take part in the computation, as in our case.\nThe summation variable,\nS is an accumulator. It accumulates the result incrementally.\nBy this time I\'d expect math geeks to jump up and down crying there\'s a simpler and more efficient way to do this. Indeed, there is a formula that computes the sum of the first n integers (derived from the general case of arithmetic progressions):\nThe flow chart for computing the sum becomes much simpler:\nThis is clearly a simpler and faster solution. There are two morals to this. First, we started with a more complicated solution to get our feet wet with a more complex example. Second, math is a powerful tool when building algorithms. Don\'t ignore it. Gauss certainly didn\'t. In elementary school his teacher, J.G. Büttner tried to occupy pupils by making them add up the integers from 1 to 100. The young Gauss produced the correct answer within seconds by a flash of mathematical insight, to the astonishment of all. Gauss had realized that pairwise addition of terms from opposite ends of the list yielded identical intermediate sums: 1 + 100 = 101, 2 + 99 = 101, 3 + 98 = 101, and so on, for a total sum of 50 × 101 = 5050. For more information, see Gauss\' Biography and Gauss\'s Day of Reckoning.\nTo make everyone happy, including folks who might think ""if he wanted a more complex example, why didn\'t he pick one that didn\'t have a trivial solution"", imagine the problem of computing the product of the first integers, which mathematicians call the factorial:\nDefinition - Factorial:\nNot surprisingly, the algorithm looks the same as our first solution, except the\n+ sign in\nS ← S + k becomes a\n* is the symbol for multiplication) and\nS has to be initialized to\n1. There is no simple formula for computing\nThis example introduces two common tools in algorithms: counters and accumulators. Let\'s try and define them.\nDefinition - Counter: a variable that keeps track of the number of times an event occurs. It is usually incremented by one at a time, and it is used frequently to count (and decide) the number of times a loop is executed.\nDefinition - Accumulator: a variable that holds a partial result and that aggregates values obtained incrementally along the computation process. The values are usually computed in a loop and they are usually added to the accumulator (though they could contribute in a more complex way). At the end of the process the accumulator contains the final result.\nIn other words, a counter counts, and an accumulator accumulates. What a surprise!', 'What are the Applications of Artificial Intelligence?\nToday, AI is used in a variety of ways. It is becoming increasingly important in this era of advanced tech as it can efficiently handle complex issues in a wide array of areas including numerous industries such as robotics, defense, transport, healthcare, marketing, automotive, business, gaming, banking, chatbots, etc. Our daily lives are becoming more and more efficient and convenient as a result of the various applications of AI.\nBefore moving forward, let us look at the agenda of this blog on the applications of Artificial Intelligence:\nNow, let us start with the applications of AI in robotics.\nAI Applications in Robotics\nThe field of robotics involves the designing and creation of automated machines or robots in such a way that they possess the ability to perform tasks on their own. Nowadays, robots are becoming more and more advanced and efficient in accomplishing tasks without human intervention. This is because AI tools and techniques are specially designed for the field of robotics. Advanced robots consist of sensors, high-definition cameras, voice recognition devices, etc.\nThese robots are capable of learning from their past mistakes and experiences and adjusting the algorithms according to the environment. AI is an extremely useful tool for robotic applications. When it is combined with advanced devices, it can help in optimizations. It is helpful in enhancing the complex manufacturing process in industries such as aerospace. The packaging process in industries also uses AI to enhance productivity and lower the overall cost.\nWatch this video on Artificial Intelligence Tutorial for Beginners:\nAI Applications in Defense\nDefense is one of the most crucial sectors for AI’s contributions. Defense security systems can be vulnerable to attacks from hackers to steal confidential government or defense data. This can prove to be detrimental to any country. The manual identification and processing of unusual activities may not detect potential threats and can be time-consuming; this is where the involvement of AI proves to be of great use.\nThe analysis of large amounts of data becomes easy with the help of AI. AI tools help in finding suspicious activity. AI algorithms are designed in such a way that they try to learn from the data and distinguish between authorized access and unauthorized access. They also keep a track of the safety of military databases. Any alteration(s) in the same by an unknown source is immediately tracked down for action.\nInterested in learning Artificial Intelligence? Learn more from this Artificial Intelligence Course in New York!\nAI Applications in Transport\nAI has completely transformed the transport industry. As the competition in the transport industry is high, there is a need to analyze all the factors that influence the various facets of the business. These may be price, seasons, festivals, number of passengers, etc.\nYou must have noticed that the price of flights on Google fluctuates often. Seasons, festivals, routes, halts, and number of travelers on a route can all be reasons for these fluctuations. With the help of predictive analytics, data related to these factors, which impact the cost of transport, can be analyzed. AI tools help perform predictive analytics on data efficiently. With AI tools, the software or application can also predict the best prices for specific routes. This helps passengers find cheaper options. Further, AI-based applications notify passengers about the rise and drop in prices and about the available offers while booking tickets.\nAnother real-life application of AI in the field of transport is in online ride-sharing services such as Uber, Lyft, and Ola. These applications use AI, machine learning (ML), and data science to deliver customized services. AI helps in showing optimized paths and prices for the rides.\nBecome a master of Artificial Intelligence by going through this online Artificial Intelligence Course in London!\nAI Applications in Healthcare\nThese days, most healthcare organizations are relying on AI-based software for their day-to-day tasks. These tasks vary from patient diagnosis to hospital data management. The amount of data generated by the healthcare industry is 44 trillion gigabytes per day; so, there is a need for AI-based advanced processors that can extract, manipulate, analyze, and draw meaningful insights from this data.\nAI and ML technologies are doing a fabulous job in the healthcare industry. The AI-based algorithms that are fed into the systems are capable enough to spot patterns much more efficiently than humans. These algorithms also help in the analysis of patient data, thus helping with the diagnosis. AI-based devices help measure real-time data such as blood pressure, heartbeat, body temperature, and much more. They send the collected data to doctors to analyze the patients’ health conditions from remote locations. This can save the time of both doctors and patients. AI-based bots and assistants also enable doctors to save and manage time for patients by streamlining the processes. Further, the applications of AI in Healthcare help pathologists for the analysis of genes and tissues, which makes the diagnosis of certain diseases more accurate.\nAI Applications in Marketing\nOne of the key factors in running a successful business is marketing. Proper marketing strategies lead to generating high profits. According to Forbes, the revenue generated by the marketing industry in 2019 was more than US$300 million.\nLearn about AI in our in-depth blog on Artificial Intelligence tutorial!\nToday, the marketing industry is revolutionized by the applications of AI. Various industries, such as e-commerce, e-learning, advertising, media, and entertainment, use AI to boost their profitability. Suppose, you are searching for a product on Amazon. Along with the product, it will also show you the best sellers, similar products, varieties of the same product, and recommended-for-you list of products.\nYou must have also observed that you get advertisements for similar products in your searches. Suppose, if you are searching for a specific Netflix series, then you will get advertisements and recommendations for other similar shows. This is achieved when AI-based algorithms for advertisements try to understand customer interests. Moreover, they learn and improve their performance over time. This type of automated marketing helps in targeted advertising, giving a personalized experience to customers, and boosting the businesses’ revenue.\nAI Applications in Automotive Industry\nThe emergence of AI has revolutionized the automotive industry as well. The invention of self-driving cars has completely changed the world of automobiles. There are various companies, such as Tesla, Nissan, Audi, Volvo, etc., developing self-driving cars. Self-driving cars are built by using a combination of various technologies, and one of the majorly used technologies is AI.\nA self-driving car uses sensors, cameras, voice detectors, and many other devices. It analyzes the surroundings by collecting data; the software used in the car gives commands such that the car can make a safe drive. You just have to enter the location in the route map, and the AI-enabled advanced systems will find an optimized path to the destination.\nWith the help of AI, you can address problems such as traffic accidents, respond to natural disasters, etc. The applications of AI have proved to be one of the greatest resources created by humans. The various applications of Artificial Intelligence are helping change the user experience in different ways.\nIf you are preparing for an Artificial Intelligence job interview, go through this blog on Top Artificial Intelligence Interview Questions and Answers!\nAI Applications in Business\nBusinesses are adopting AI to improve their customer relationships and to handle activities that would normally be completed by people but can be completed faster via robotic process automation. Businesses are also employing ML algorithms to determine the best methods to offer clients. Chatbots are integrated into businesses to provide rapid services to clients, which is one of the many uses of AI in the commercial sector. AI has the ability to increase sales, do predictive analysis, improve customer relationships by improving their entire experience, and create efficient and productive work procedures.\nAI Applications in Gaming\nAI has been an important aspect of the gaming industry in recent years.\nThe gaming industry is the finest illustration of clever AI applications since it is on this platform where there are many changes in the end goal. AI is employed in a variety of ways including game creation, character development, etc. In gaming, AI implies interactive and adaptable video game experiences. Non-player characters, or NPCs, are typically used to create this type of AI-powered unique entertainment; NPCs act intellectually or artistically as if they were directed by a human player. The AI engine governs how an NPC behaves in the game environment.\nAI Applications in Banking\nBanking is another application where AI is used extensively. Automation, chatbots, adaptive intelligence, algorithm trading, and ML are all used in financial activities. Several banks already use AI-based systems or software to provide customer service and identify abnormalities and fraud. AI is significantly valuable for the banking industry when it comes to fraud detection. Pattern recognition may be difficult for humans, but robots excel at it. By using AI and ML algorithms, security experts can stop fraud by tracing card usage and endpoint access.\nAI Applications in Chatbots\nAI chatbots can understand natural language and reply to users who utilize the live chat option that many businesses offer for customer care. ML enables AI chatbots, and they may be incorporated into a variety of websites and apps. In contrast to gathering data from an existing choice of inclusive replies, AI chatbots can construct a database of responses. These chatbots can successfully address customer concerns, reply to basic inquiries, enhance customer service, and provide 24/7 help as AI continues to advance. Overall, AI chatbots assist in increasing customer satisfaction.\nWilling to master AI? Enroll for the Artificial Intelligence Course in Chennai Now!\nAI has a variety of applications in a wide range of industries and domains, all of which have the ability to reduce human effort by automating and running everything through machine algorithms. Predictive analytics and AI will undoubtedly play a larger role in content production and software development in the future. AI is already changing sectors and assisting in the solution of complicated issues, and this trend will continue. Global technical equivalence and AI have the potential to become the future in the areas of health, environment, public safety, and security.']"	['<urn:uuid:37a425c8-82be-45d2-b5f8-bef6ba87a7d8>', '<urn:uuid:f2f017bc-ca43-4add-b39f-a932cbbee36e>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T04:56:11.774636	19	69	3386
18	With my background in environmental remediation, I'm interested in understanding how coal tar sealant contamination is being addressed both legally and practically - what are the financial implications of cleanup efforts, and how are courts handling the liability aspects through tools like Lone Pine orders?	The cleanup of PAH-contaminated water from coal tar sealants is extremely costly, with projected expenses in the Twin Cities area alone approaching several billion dollars. Each contaminated pond requires $75,000 to $125,000 for proper disposal at authorized landfills. The legal system is addressing these cases through established case management tools like Lone Pine orders, which courts have consistently upheld as appropriate mechanisms to handle complex environmental litigation. These orders require plaintiffs to provide preliminary evidence of exposure, injury, and causation before proceeding with full discovery, helping to manage these complex cases efficiently while ensuring valid claims move forward. This approach balances the need for justice with judicial efficiency in cases like the Minnesota cities' lawsuit against coal tar refiners, where the municipalities are seeking to shift cleanup costs from taxpayers to the manufacturers who allegedly knew about the environmental risks.	['by Richard O. Faulk, Hollingsworth LLP*\nTo listen to the plaintiffs’ bar, you’d think that “Lone Pine” orders were a novelty recently conjured out of “thin air” by creative defense lawyers—or a device unsupported by any significant precedents. But although those orders may seem new to the uninitiated, they have deep roots in the history of active case management.\nMany lawyers know—or have learned the hard way—why these case management tools are called “Lone Pine” orders, and what they are intended to accomplish. In Lore v. Lone Pine Corporation, No. L-03306-85, 1986 WL 637507 (N.J. Sup.Ct. Nov. 18, 1986), the plaintiffs claimed injuries resulting from contamination allegedly coming from a landfill. When the defendants presented a government investigation that found no offsite contamination, the court required the plaintiffs to make a preliminary showing of exposure, injury, and causation before allowing full discovery to proceed. This ruling led to other cases which recognized the propriety of “Lone Pine” orders when doubt existed “over what medical condition or disease, if any, can be causally related to the toxic agent exposure alleged by each plaintiff.”2 Lawrence G. Cetrulo, Toxic Torts Litigation Guide § 13:49 (2013). Since then, “Lone Pine” order have proliferated, not only in toxic tort litigation, but also in other types of cases.See generally, David B. Weinstein and Christopher Torres, Managing the Complex: A Brief Survey of Lone Pine Orders, 34 Westlaw Envt’l J. 1 (Aug. 21, 2013) (providing extensive list of categorized cases).\nLawyers who oppose these orders seek to “roll back the clock” to a time when the pace and progress of litigation was largely deferred to counsel, rather than courts—when judicial intervention was eschewed, instead of actively utilized to promote fair, prompt and cost-effective resolutions. Fortunately, in most courts throughout the nation, “the way we were” is no longer the “the way we are.” Instead of passively entrusting parties with broad power to manage their own disputes, new rules and principles have expanded and enlarged the scope of trial court discretion. As a result, judges are empowered and encouraged to manage cases creatively in the interests of justice. By these rules, the delay and costs formerly associated with the administration of justice are reduced, access to justice is enhanced, and the resources of the parties and the public are conserved.\n“Lone Pine” orders are rooted deeply in the history of “active case management,” a concept that revolutionized the manner in which disputes were administered in state and federal courts over the past decades. By the 1970s, serious concerns regarding costs and delays focused attention on how cases actually progressed from filing to disposition—and how the overall process might be improved to hasten fair administration of justice. Critical studies of federal and state courts ultimately agreed that the most effective way to improve the judicial process was for judges to control the progress of litigation. See, e.g., Maureen Solomon, Caseflow Management in the Trial Court (American Bar Association, 1973), at 29-30; Steven Flanders, Case Management and Court Management in United States District Courts (Federal Judicial Center, 1977); Thomas Church et al., Justice Delayed: The Pace of Litigation in Urban Trial Courts 54 (National Center for State Courts, 1978). Based on these studies, Federal Rules 16 and 23 were ultimately amended to facilitate, promote, and require greater judicial involvement—not only to reduce costs and delay, but to ensure that the expectations of the parties in efficient case administration were addressed.\nThe triumph of managerial judging was marked by its recognition by the Manual for Complex Litigation which, by the 1985 edition, provided:\n[T]he propriety, if not the necessity of judicial control to promote the efficient conduct of the litigation . . . stems from an awareness that the tensions between an attorney’s responsibilities as an advocate and as an officer of the court frequently are aggravated in complex litigation and that the tactics of counsel may waste time and expense if the judge passively waits until problems have arisen.\nSee Manual for Complex Litigation, Second, Sec. 20.1 (1985). By 1989, the process was fully underway in the nation’s courts, and the Supreme Court of the United States announced that “[o]ne of the most significant insights that skilled trial judges have gained in recent years is the wisdom and necessity for early judicial intervention in the management of litigation.” Hoffmann-La Roche v. Sperling, 493 U.S. 165, 172 (1989) (Per Kennedy, J.).\nThe evolution of most states’ programs, and the rules amendments that authorized and empowered them, closely resemble the experience of the federal judiciary, a system whose appellate courts have approved “Lone Pine” motions in every instance where they have been challenged. The Federal Rules and most state rules share a common purpose, namely, the “just, speedy and inexpensive” resolution of disputes. Compare, e.g., Fed. R. Civ. P. 1 with C.R.C.P. 1(a). The Rules are also similar because many state courts accomplish this purpose through provisions similar to Federal Rules 16 and 26. See e.g., Richard P. Holme, Colorado’s New Rules of Civil Procedure, Part I: Case Management and Disclosure, The Colorado Lawyer, Vol. 23, No. 11 (Nov. 1994); Richard P. Holme, Colorado’s New Rules of Civil Procedure, Part II: Rediscovering Discovery, The Colorado Lawyer, Vol. 23, No. 11 (Dec. 1994). Both state and federal rules also “reflect an evolving effort to require active judicial management of pretrial matters to curb discovery abuses, reduce delay, and decrease litigation costs.” See, e.g., DCP Midstream, LP v. Anadarko Petroleum Corp., 303 P.2d 1187, 1190 (Colo. 2013). With these commonalities, it is not surprising that federal appellate decisions have consistently accepted “Lone Pine” orders as appropriate case management tools.\nIn the most recent example, the U.S. Court of Appeals for the District of Columbia Circuit affirmed the dismissal of claims filed by a group of Ecuadorian provinces and individual farmers. Arias v. DynCorp, ___ F.3d ___, 2014 WL 2219109 (D.C. Cir. May 30, 2014). In Arias, the plaintiffs filed a putative class action on behalf of all Ecuadorians who lived within ten miles of the Colombian border. They alleged that they were injured by an anti-drug herbicide spraying operation in Colombia that was conducted by an American company.\nUltimately, the class action allegations were dropped and, to move the case forward, the district court ordered the remaining individual plaintiffs to submit answers to questionnaires regarding their injuries, which, according to the D.C. Circuit, is a “common trial management technique in toxic tort cases with multiple plaintiffs.” 2014 WL 2219109 at *4. The court explained that “[s]uch an order is sometimes called a Lone Pine order . . . It generally requires plaintiffs in a toxic tort case to produce affidavits setting forth some basic information regarding their alleged exposure and injury.” Id. According to the court, “such orders are issued under the wide discretion afforded district judges over the management of discovery under Fed. R. Civ. P. 16.” Id.\nEven after the response deadlines were extended in Arias, many plaintiffs still submitted incomplete responses. After the judge warned that failure to fully complete the forms would lead to dismissal with prejudice, he extended the deadline yet again. After plaintiffs’ repeated failures to adequately complete the responses—and three deadline extensions—the district court “ultimately exercised its Rule 37(b) prerogative to sanction the plaintiffs by dismissing the case.” Id. at *5.\nOn appeal, the dismissed plaintiffs argued that dismissal was “too harsh of a sanction” and that the trial court should have considered “less dire alternatives.” Id. But the court of appeals disagreed:\nThe court gave the plaintiffs every opportunity to complete their responses. Indeed, the court appears to have been, if anything, too patient, applying no sanctions at all for the plaintiffs’ earlier failures. Only when further extensions were obviously futile did the court dismiss these cases.\n2014 WL 2219109 at *6. Based on this record of non-compliance, the appellate court ruled that it was “impossible to conclude that the judge abused his discretion” by dismissing the claims with prejudice. Id. This ruling was consistent with other federal appellate decisions that previously addressed “Lone Pine” situations. See Avilla v. Willets Environmental Remediation Trust, 633 F.3d 828, 833-834 (9th Cir. 2011), cert. denied, 132 S. Ct. 120 (2011); Acuna v. Brown & Root Inc., 200 F.3d 335, 340 (5th Cir. 2000). Under the circumstances, these courts’ decision to dismiss plaintiffs’ claims was consistent with not only the Rules of Civil Procedure, but also the tradition of active case management upon which they were founded.\nViewed in this light, “Lone Pine” orders are hardly a novelty. They are a natural product of the principles of active case management. They rely upon the critical importance of early and carefully considered judicial intervention to achieve the most salutary goal of any judicial system—a just and prompt resolution at the lowest cost necessary to reach that end. They are not dispositive in themselves, but when issued, they offer an opportunity to identify, isolate, and develop particular issues which may lead to that end. “Lone Pine” orders are entirely consistent with the history of active case management, the procedural underpinnings of federal and state procedural law, and a host of decisions from jurisdictions that have embraced similar goals.\n*Partner, Hollingsworth LLP, Washington, DC. Senior Director, Initiative for Energy and the Environment, Law & Economics Center, George Mason University School of Law, Arlington, Virginia. Mr. Faulk is counsel for a number of amici curiae in Antero Resources Corp., et al, v. Strudley, No. 2013SC576 (Supreme Court of Colorado), which involves the propriety of Lone Pine orders under Colorado law.', 'A fresh coat of coal tar-based sealant can make dingy driveways, parking lots and other paved surfaces look brand new. But the thick, black liquid is also full of polycyclic aromatic hydrocarbons, or PAHs, which can kill aquatic life and cause cancer in humans.\nCommunities that have filed lawsuits say the makers of coal tar sealants knew their products were dangerous and would contaminate the environment but sold them anyway. The cities are suing to have the coal tar refiners cover the costs of monitoring and cleaning up local waterways.\nCleaning up PAH-contaminated water is intensive and expensive. In the Twin Cities area alone, projected cleanup costs could approach several billion dollars.\nMinnesota Cities Sue Coal Tar Refiners\nThe state of Minnesota passed a bill banning the use of coal tar-based sealants in 2013, and it’s been illegal to sell or use the controversial product in the state since 2014. But years of prior use have caused extensive water contamination.\nA 2014 study in the Archives of Environmental Contamination and Toxicology found high concentrations of PAHs in more than a dozen stormwater ponds in the Minneapolis-St. Paul metropolitan area. The study, which was conducted by the Minnesota Pollution Control Agency, reported that coal tar-based sealant runoff and dust contributed to more than 67 percent of the pollution.\nThe PAH levels were high enough in nine ponds to pose a risk to humans, according to the study. Because of the potential risk to humans, the Minnesota Pollution Control Agency concluded that the contaminated sediment in those ponds would have to be dug up and hauled off to landfills.\nIn January 2019, eight cities in the Twin Cities area filed suit in U.S. District Court in Minneapolis against several large chemical refiners over the pollution. The cities are seeking to recover the costs of cleaning up the ponds and disposing of the hazardous waste. The group of cities that are suing include: Bloomington, Burnsville, Eagan, Eden Prairie, Golden Valley, Maple Grove, Minnetonka and White Bear Lake.\nAmong the defendants named in the case is Koppers Inc., a Pittsburgh-based company that manufactures pavement sealants from coal tar. The company told Minnesota Public Radio that the claims in the lawsuits are without merit and it will fight them. Other defendants named in the lawsuits include: Coopers Creed Chemical Corporation, Lone Star Specialty Products, Rain Carbon Holdings LLC, Rain Carbon Inc., Ruetgers Canada Inc. and Stella-Jones Corp.\nCleaning up PAH-contaminated water is not cheap. After the toxic material is dredged from a pond, it requires a special type of disposal. It must be hauled away to authorized landfills where it won’t seep into soil and water.\nHauling away heavily contaminated sediment can cost an extra $75,000 to $125,000 per body of water, and cleaning up all the ponds in the Minneapolis region could cost upwards of $1 billion, according to the Pioneer Press.\nThe lawsuits contend that the companies who manufactured the toxic materials should bear the expense, not taxpayers.\nCosts could run even higher, however, because no one knows exactly how many ponds are affected. According to the City of Bloomington’s website, study and sampling have been limited, but anywhere from one-fifth to one half of all ponds may be contaminated with PAHs.\nWhile the lawsuits filed by the cities in Minnesota are the first of their kind, they may not be the last. Sealcoating with coal-based products is a widespread practice east of the Mississippi and more cities may take legal action as contamination is uncovered and cleanup costs pile up.\n17 Cited Research Articles\nConsumernotice.org adheres to the highest ethical standards for content production and references only credible sources of information, including government reports, interviews with experts, highly regarded nonprofit organizations, peer-reviewed journals, court records and academic organizations. You can learn more about our dedication to relevance, accuracy and transparency by reading our editorial policy.\n- Associated Press. (2019, January 16). Cities Sue Companies over Sealant Pollution in Minnesota Ponds. Retrieved from https://www.insurancejournal.com/news/midwest/2019/01/16/514874.htm\n- City of Bloomington. (2019, January 2). Stormwater pond contaminant litigation. Retrieved from https://www.bloomingtonmn.gov/com/news/stormwater-pond-contaminant-litigation-2019-01-02\n- City of Eagan, Minnesota v. Koppers Inc. et al. (2019, January 9). Complaint. Retrieved from https://www.bloomberglaw.com/public/desktop/document/CityofEaganvKoppersIncetalDocketNo019cv00058DMinnJan092019CourtDo?1561748656\n- City of Eagan, Minnesota v. Koppers Inc. et al. (2019, January 9). Complaint. Retrieved from https://www.bloomberglaw.com/public/desktop/document/CityofEaganvKoppersIncetalDocketNo019cv00058DMinnJan092019CourtDo?1562009593\n- Crane, J.L. (2014, February). Source Apportionment and Distribution of Polycyclic Aromatic Hydrocarbons, Risk Considerations, and Management Implications for Urban Stormwater Pond Sediments in Minnesota, USA. Retrieved from https://link.springer.com/article/10.1007/s00244-013-9963-8\n- Farrell, P.A. (2017, September 15). More Michigan communities ban popular driveway sealer over health hazards. Retrieved from https://www.freep.com/story/news/local/michigan/2017/09/15/michigan-ban-coal-tar-driveway-sealer/669338001/\n- Jones, H. (2019, January 14). Lawsuit: Driveway sealant leached carcinogens into Minnesota Ponds. Retrieved from http://www.citypages.com/news/lawsuit-driveway-sealant-leached-carcinogens-into-minnesota-ponds/504226591\n- Marohn, K. (2019, January 8). These driveway sealants polluted Minnesota ponds. Who should pay? Retrieved from https://www.mprnews.org/story/2019/01/08/driveway-sealants-polluted-minnesota-ponds-who-should-pay\n- McLean, L. & Rajkowski, F. (2019, January 2). Suburbs file lawsuit coal tar products have contaminated stormwater ponds. Retrieved from https://kstp.com/news/suburbs-file-lawsuit-alleging-coal-tar-products-have-contaminated-stormwater-ponds/5194868/\n- Minnesota Pollution Control Agency. (n.d.). Coal tar-based sealants. Retrieved from https://www.pca.state.mn.us/water/coal-tar-based-sealants\n- National Park Service. (2013). State of the River Report: Water Quality and River Health in the Metro Mississippi River. Retrieved from https://www.nps.gov/miss/learn/nature/upload/SOTR-2nd-Edition-v2-022213.pdf\n- Needleman, H. (2016). The Use, Impact, and Ban of Coal Tar-Based Sealants. Retrieved from https://law.wm.edu/academics/programs/jd/electives/clinics/vacoastal/documents/Needlleman%20White%20Paper_Web.pdf\n- Neutkens, D. (2019, January 9). Coal tar refiners sued for polluting ponds. Retrieved from https://www.presspubs.com/white_bear/news/coal-tar-refiners-sued-for-polluting-ponds/article_9f894164-1441-11e9-9f36-d3dc28165078.html\n- Serres, C. (2019, January 2). Minnesota cities sue refiners over cost of cleaning up polluted stormwater ponds. Retrieved from https://www.startribune.com/minnesota-cities-sue-refiners-over-cost-of-cleaning-up-polluted-stormwater-ponds/503826042/\n- Teller, E. (2019, January 4). Eden Prairie, Minnetonka among cities suing over coal tar pollution. Retrieved from https://www.swnewsmedia.com/eden_prairie_news/news/local/eden-prairie-minnetonka-among-cities-suing-over-coal-tar-pollution/article_da86f8ab-3ab9-549f-8dcb-ac317798571b.html\n- U.S. Geological Survey. (n.d.). Coal-Tar-Based Pavement Sealcoat, PAHs, and Environmental Health. Retrieved from https://www.usgs.gov/mission-areas/water-resources/science/coal-tar-based-pavement-sealcoat-pahs-and-environmental-health?qt-science_center_objects=0#qt-science_center_objects\n- Verges, J. (2019, January 3). White Bear Lake, other cities sue makers of banned driveway sealant alleging $1 billion pond problem. Retrieved from https://www.twincities.com/2019/01/02/minnesota-cities-sue-makers-of-banned-driveway-sealant-alleging-1-billion-pond-problem/']	['<urn:uuid:ef768d0c-67bd-4923-ab7c-9991e98f464d>', '<urn:uuid:a101937e-190b-4229-a25e-2a8774dc562b>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:56:11.774636	45	140	2547
19	How do outdoor support structures compare in durability and maintenance between wire mesh and wood?	Concrete wire mesh easily gets very rusty, which can leave marks on plants' leaves and fruits. In contrast, wood fences require considerable maintenance including annual sanding and painting/staining, and can rot in moist climates or face termite problems. Wood fences typically last around 15 years before needing replacement.	['USING CONCRETE WIRE MESH TO TRAIN YOUR PLANTS: HOW-TO, PROS AND CONS\nDoes using remesh to train your plants really work as you expect?\nConcrete wire mesh (remesh) is a very strong material which is usually used for construction, but gardeners have seemed to give it another use. Surprisingly, wire mesh has become widely adopted in gardening, due to its low cost, high resistance and durability. It can easily be found at hardware or construction material stores and generally comes in 48×84 inches panels, or 60 inches tall per 150 feet wide rolls. When rolled, concrete wire mesh is difficult to flatten, so they are commonly used to build tomato cages.\nIt is true that prefabricated cages for tomato can be found at gardening stores, but they are made of thin wire and because of this can only be used to train bush tomatoes. For bigger tomato varieties it is need a sturdier structure; that is why many gardeners go for building concrete wire mesh cages by themselves. Cages need to be at least 6 feet tall (1, 82 m) and 18 inches (45 cm) in diameter, but that will depend on your necessities and particular preferences. Tomato remesh cages are also ideal for containing berries plants, which if not limited, will occupy all the space they find in their path. It is better to place cages when berries are still young or if you have pruned them recently. There are gardeners that assure they have used cages for training zucchini, cucumber and eggplants as well, but this would require constant pruning and would make of picking up crops a very difficult job.\nPanels, unlike rolled wire, are perfect for trellising and can be supported with deeply nailed in the ground posts, to guarantee the structure will withdraw strong winds and stormy weather. Concrete mesh panels are not only ideal for tutoring tomatoes, but heavy vegetables like squashes and cucumbers as well. However, if you place them like a wall or a fence, crops will be more likely to rip off vines when they become too heavy. For this reason it is better using two panels, well secured to the ground, leaning on each other in an A-shape. This way it will not be so easy for crops to pull off the plant from the support structure. This disposition works amazingly with any type of pole beans as well, and in addition to the great support it provides, it looks very clean and aesthetically pleasing.\nAlternatives to concrete wire mesh for trellises.\nAlthough using remesh for trellises seems like a very trustworthy method, it may bring you complications in the future. For example, if it is used in very hot zones, you cucumber plants would run the risk of getting burned when the wire reaches high temperatures, causing severe damage to the plant. Another clear disadvantage of wire mesh is that it easily gets very rusty. This may not seem like a problem, but it will leave rust marks on your plant’s leaves and fruits. Other con would be that these structures have to be made by you, which takes a lot of time and hard work, since this wire is very difficult to mold and cut.\nFortunately, there are other clever methods out there that can make you have more improvements in your yield, and one of them would be using plastic trellising. If given an appropriate support, plastic nets structures can hold vegetables plants, prevent diseases, they are reusable and end up being cheaper.', 'Loading, please wait...\n(4-ft high, 209-yd fence, installed)\nGet free advice and estimates from fence contractors in your city.\nIf you’re thinking about securing your property with a fence so that you can safely let your children play or easily let your pet outside, there are important considerations to keep in mind. Several options are used to build fencing, and this guide will explore wood and chain link. In the sections below, we will explain the differences between these options to help you make an informed decision.\nWood fences provide the most versatility regarding design and finishing options. These fences can be painted to match your home’s exterior and decorative features, and lattice work or metal fixtures can be added for a custom look. Wood fences can be built from different kinds of wood, with the most common being pressure-treated pine. Spruce and cedar are other possible options. These fences range in height and include ranch/split rail, picket, and privacy styles.\nChain link fences have fewer design options and are usually limited to black, green, brown, or silver colors. Generally, chain link fences are used where privacy is less of a concern, and their utilitarian design is ideal for enclosing areas for pets, around HVAC units, or in industrial settings. The fence material is made from galvanized aluminum and can be coated in vinyl 1. Chain link fences are typically only 4 feet in height but can be built to 5- or 6-feet high.\nBoth wood and chain link fences can be installed as a DIY project. However, there is considerable labor involved. Digging holes for fence posts 2 takes a lot of time and effort and typically requires specialized equipment. Fence posts 2 should be cemented into the ground to prevent shifting or leaning over time. Using a professional is recommended because of necessary permits, site inspection, and grading 3.\nWood fences require each fence board to be nailed individually to rail 4 posts. Depending on the size of the fence, this can take many hours to complete. Chain link fences are sold in large rolls, so once the posts are installed the fence material is unrolled and attached to each post. This makes for a quicker installation.\nThe cost of a wood fence is dependant on the style, height, and type of lumber selected. For a 6-foot high, 209-yard privacy fence 5 with a gate, concrete posts, and removal of the old fencing, the cost for materials is between $8 and $18 per linear foot. Labor costs, if charged hourly, are between $25 and $50 per laborer. The total cost for this project ranges between $6,500 and $10,000. Many companies, however, will offer a flat rate quote so that homeowners are not surprised by additional costs.\nChain link fences are a more inexpensive option that can make them attractive for covering large areas in need of security, like around a business or industrial area. For a 4-foot high, 209-yard fence, materials range in price between $7 and $12 per linear foot. Labor costs range between $8 and $15 per linear foot, making the total cost for this project between $3,000 and $5,200 on average. Additional items like gates, vinyl 1 slats, or fabric screens will increase the total cost.\nWood fencing can provide considerable privacy depending on the design used and are ideal for securing a space used for playing, relaxing, or entertaining. A wood fence can be built to various heights to provide additional privacy, like around a pool area.\nChain link fences don’t provide a lot of privacy because the mesh material has large spaces that make these fences virtually see through. Slats can be installed in the chain link material, which provides some privacy. Chain link slats are usually sold by the box. They come in a variety of colors to help your fence blend in with the rest of your property. A box of slats costs around $45, and professional installation adds $2 to $3 per linear foot to the installation cost. Because the slat material is visually unappealing and not very durable, this option is not commonly used.\nWood fencing can completely block light and might not be the best choice if you have plants or a garden nearby. Chain link fences allow a lot of light through and may be ideal if you’re simply looking to limit access to your yard or protect a specific area, like a vegetable garden.\nThe maintenance requirements of wood fences are considerable in order to keep them in good repair and looking great. Depending on the paint used and climate you live in, your wood fence may require annual sanding 6 and painting or staining. Moist or wet climates may cause the wood to rot, and termites can also be a problem. If not properly installed and cemented into the ground, posts can lean or fall and will likely cause damage to the portions of the fence close to the damaged post. Depending on the amount of damage, repairs can be costly as sections of the fence may need to be rebuilt using new materials.\nChain link fences require little maintenance because the metal and other materials used are very durable in a variety of climates. The galvanized metal has been treated using a special process that makes it less likely to rust. Metal is also stronger than wood and won’t sag over time like a wood fence.\nTypically a wood fence lasts around 15 years before it needs replacement. A chain link fence will last approximately 20 years, and if you’ve selected a mesh material coated with vinyl, you can expect an even longer lifespan.\nWood fences may initially seem like a sustainable option, but this is dependent on where and how the wood was harvested. Clear cutting of old-growth forests for lumber can cause significant damage to the surrounding ecosystem. Additionally, many lumber products are treated with toxic chemicals to prohibit insects and mold growth. Chain link fences are made from easily-accessed raw materials, but they still require a significant amount of energy in the manufacturing process.']	['<urn:uuid:b3763314-d110-4d60-bdf1-d6266659d2cb>', '<urn:uuid:d4ec7909-8116-40de-a544-0b94e381e295>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T04:56:11.774636	15	48	1599
20	How do Liang's hyphenation algorithm and spelling correction methods compare?	Liang's hyphenation algorithm uses a pre-computed pattern-based approach with multiple levels (hyphenation, inhibition, hyphenation, etc.) to find 95% of correct hyphenation points, requiring words to be at least 5 letters long and using numbered patterns to determine hyphen positions. In comparison, spelling correction methods like FAROO's Symmetric Delete approach pre-compute possible error variants of dictionary terms and use hash tables for fast lookups. While Liang's algorithm focuses on never inserting incorrect hyphens but may miss some valid ones, spelling correction algorithms like FAROO aim for complete coverage of possible variations within a specified edit distance while maintaining efficiency through pre-computation and constant-time lookups.	['April 28, 2009\nYesterday was the 218th anniversary of the birth of Samuel F. B. Morse, who invented the telegraph. We present this simple exercise in his honor.\nMorse Code is an encoding of alphanumeric characters using short and long pulses of sound. Originally developed in the 1840s for the use of Morse’s telegraph, it is still in use today, primarily in the fields of amateur radio and aviation. Letters and digits consist of dots and dashes as shown in the chart at right; a single space is inserted between characters, and an extra space is inserted between words. For instance, “Programming Praxis” is rendered in Morse code as the string “\n• — — • • — • — — — — — • • — • • — — — — — • • — • — — • • — — • • — • • — — • • — • • • • •“.\n|A • —||N — •||1 • — — — —|\n|B — • • •||O — — —||2 • • — — —|\n|C — • — •||P • — — •||3 • • • — —|\n|D — • •||Q — — • —||4 • • • • —|\n|E •||R • — •||5 • • • • •|\n|F • • — •||S • • •||6 — • • • •|\n|G — — •||T —||7 — — • • •|\n|H • • • •||U • • —||8 — — — • •|\n|I • •||V • • • —||9 — — — — •|\n|J • — — —||W • — —||0 — — — — —|\n|K — • —||X — • • —|\n|L • — • •||Y — • — —|\n|M — —||Z — — • •|\nYour task is to write functions that convert back and forth between character strings and Morse code. When you are finished, you are welcome to read or run a suggested solution, or to post your solution or discuss the exercise in the comments below.\nApril 24, 2009\nIn 1982, Frank Liang published in his doctoral dissertation a description of the hyphenation algorithm that he invented for Donald Knuth’s TeX typesetting program; his algorithm is the basis of most of the hyphenation programs used today. Liang’s algorithm is described in Appendix H of The TeXbook by Knuth. Liang’s algorithm never inserts a hyphen where one does not belong, but does miss some opportunities to insert hyphens where they do belong; Liang claims to find 95% of the hyphens in most texts. Liang’s algorithm is quick, and reasonably stingy with space.\nLiang’s method works by pre-computing a list of hyphenating and inhibiting patterns based on a given hyphenation dictionary. First, a list of hyphenating patterns is established; for instance,\nc-c are good hyphenating patterns. But all the patterns have exceptions; for instance, the\n-tion pattern improperly hyphenates the word\ncat-ion. Thus, a set of inhibiting patterns prevents hypenations; in this case, the inhibition pattern\n.ca=t (the dot indicates the beginning or end of a word, the equal-sign inhibits a hyphen) overrides the hyphenation pattern and prevents the hyphen at\nca-tion. Of course, there are exceptions to the inhibition patterns; Liang’s algorithm goes five levels deep — hyphenation, inhibition, hyphenation, inhibition, hyphenation — to get a good set of patterns, and even then requires an exception listing to fix a few words that would otherwise be hyphenated incorrectly. It is also required that a word must have a least five letters to be hyphenated, no hyphens can be inserted after the first letter or before the last letter of a word, and any word that includes non-alphabetic characters is never hyphenated.\nConsider this example of hyphenating the word hyphenation: there are two hyphenating patterns\n1tio, then three inhibiting patterns\nhe2n, then another hyphenating pattern\nhy3ph, another inhibiting pattern\nhena4, and finally another hyphenating pattern\n. h y p h e n a t i o n .\n1t i o\nh y3p h\nh e n a4\nh e n5a t\nh y-p h e n-a t i o n\nAfter all the patterns present in the word are identified, the highest number at any inter-letter position wins (we’ll call that number a rule), and hyphens are inserted at all the odd-numbered rules. In this case, the\n1na pattern originally placed a hyphen at\nhyphe-nation, but the\nhe2n pattern inhibited the hyphen, because rule 2 trumps rule 1; likewise,\n1tio. Note that three patterns start at the fourth letter of hyphenation:\nLiang’s hyphenation algorithm iterates through the letters of the input word, identifying all the patterns that are present in the word, and takes the maximum rule at each position. The task of identifying the patterns calls for some sort of search algorithm; Liang used a trie that he cleverly packed into an array.\nFinally, Liang’s dissertation described a program patgen for pre-computing the hyphenating and inhibiting rules, for English or any other language, given a hyphenating dictionary. See his dissertation for details. The original lists of patterns and exceptions used by Liang for TeX 82 are shown on the next page.\nYour task is to write a function that takes an input word and returns a list of suitable hyphenation points. When you are finished, you can read or run a suggested solution, or post your solution or discuss the exercise in the comments below.\nApril 21, 2009\nIn a previous exercise we built a spell checker based on storing words in a trie. That spell checker was exact: the spell checker reported success if and only if the checked word was in the dictionary. Today we will build a spell checker that is probabilistic: it correctly reports failure if the checked word is not in the dictionary, and correctly reports success if the checked word is in the dictionary, but may also incorrectly report success even if the checked word is not in the dictionary. The probability of error can be made arbitrarily small, as determined by the programmer.\nWe will use a bloom filter, a data structure invented by Burton Bloom in 1970 to test membership is a set. A bloom filter consists of an array of m bits, plus k different hash functions that map set elements to the range 0 to m-1. All the bits are initially 0. To add an element, calculate the k hash values of the element, and set each kth bit to 1. To test if an element is in the set, calculate the k hash values of the element, return true if all of the kth bits are 1, and false if any is 0. In this way, it is certain that the element is not in the set if any hash returns 0, but it is possible that an element not in the set may be incorrectly reported as being in the set if all of the hashes return 1, but one of the hashes was set by some other element.\nThe easiest way to build a large number of hash functions is to use a single hash function and “salt” the dictionary words with an additional letter. For instance, to hash the word “hello” three times, use “ahelloa”, “bhellob”, and “chelloc” and hash with a standard string-hashing function.\nThere is some considerable math involved in determining the appropriate values of m and k. For a set of n elements, the probability p of a false positive is given by the formula:\nTo give this a sense of scale, storing a fifty thousand word dictionary in a bloom filter of a million bits using seven hash functions will result in a false positive every 5102 words, on average.\nYour task is to build a probabilistic spell checker as described above. When you are finished, you can read or run a suggested solution, or post your solution or discuss the exercise in the comments below.\nApril 17, 2009\nSpell checkers are ubiquitous. Word processors have spell checkers, as do browser-based e-mail clients. They all work the same way: a dictionary is stored in some data structure, then each word of input is submitted to a search in the data structure, and those that fail are flagged as spelling errors. There is a certain art to building the word list on which a spell checker is based; Exxon isn’t a word in anybody’s dictionary, but it is likely included in the word list, but cwm (a geological structure), which is certainly a word, is most likely omitted from the word list, on the grounds it is more likely an error than a legitimate spelling (unless you are a geologist).\nThere are many appropriate data structures to store the word list, including a sorted array accessed via binary search, a hash table, or a bloom filter. In this exercise you are challenged to store the word list character-by-character in a trie. Consider this sample trie that stores the words CAR, CART, CAT and DOG:\nTo see that CAT is a valid spelling, follow the C branch, then the A branch, then the T branch, where you find a filled circle, indicating a word. To see that CAB is not a valid spelling, follow the C branch, then the A branch, and fail when there is no B branch.\nTries can be very fast; for instance, to see that QARTER (a misspelling of QUARTER) is not a word, follow the Q branch, then fail when there is no A branch. This is even faster than hashing, which must read all six letters of QARTER just to compute the hash value. And tries can also be space-efficient, since space is shared between common prefixes.\nApril 14, 2009\nThis puzzle comes from the 2008 Google Treasure Hunt:\nFind the smallest number that can be expressed as the sum of 7 consecutive prime numbers, the sum of 17 consecutive prime numbers, the sum of 41 consecutive prime numbers, the sum of 541 consecutive prime numbers, and is itself a prime number.\nFor example, 41 is the smallest prime number that can be expressed as the sum of 3 consecutive primes (11 + 13 + 17 = 41) and the sum of 6 consecutive primes (2 + 3 + 5 + 7 + 11 + 13 = 41).\nApril 10, 2009\nWords that are formed from the same set of letters are anagrams of each other. For instance, pots, post, stop, spot, opts, and tops are anagrams.\nYour task is to write a program that, given a dictionary and an input word, prints all the anagrams of the input word. You are also to determine the largest anagram class in your dictionary. When you are finished, you can read or run a suggested solution, or post your own solution or discuss the exercise in the comments below.\nApril 7, 2009\nPity the poor waiter:\nThe chef in our place is sloppy, and when he prepares a stack of pancakes they come out all different sizes. Therefore, when I deliver them to a customer, on the way to the table I rearrange them (so that the smallest winds up on top, and so on, down to the largest at the bottom) by grabbing several from the top and flipping them over, repeating this (varying the number I flip) as many times as necessary.\nYour task is to write a function that sorts a list of unique positive integers into ascending order using the pancake-flipping algorithm. When you are finished, you can read or run a suggested solution, or post your solution or discuss the exercise in the comments below.\nApril 3, 2009\nIn a previous exercise you wrote a turing machine simulator. In this exercise, you are challenged to write a program for the turing machine that performs multiplication. Write a program that takes an input like\n_ _ _  1 1 * 1 1 1 1 _ _ _, which indicates the multiplication 3 × 4, and writes the output\n_ _ _ 1 1 1 1 1 1 1 1 1 1 1  _ _ _.', 'Update: We released a C# implementation as Open Source.\nUpdate2: We are 100,000 times faster for edit distance=3.\nUpdate3: Spelling correction is now also part of FAROO search.\nUpdate4: The source code is now also on GitHub.\nUpdate5: Improved implementation now 1,000,000 times faster for edit distance=3.\nBoth algorithms are based on Edit distance (Damerau-Levenshtein distance).\nBoth try to find the dictionary entries with smallest edit distance from the query term.\nIf the edit distance is 0 the term is spelled correctly, if the edit distance is <=2 the dictionary term is used as spelling suggestion. But our way to search the dictionary is different, resulting in a significant performance gain and language independence. Three ways to search for minimum edit distance in a dictionary: 1. Naive approach\nThe obvious way of doing this is to compute the edit distance from the query term to each dictionary term, before selecting the string(s) of minimum edit distance as spelling suggestion. This exhaustive search is inordinately expensive.\nSource: Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze: Introduction to Information Retrieval.\nThe performance can be significantly improved by terminating the edit distance calculation as soon as a treshold of 2 or 3 has been reached.\n2. Peter Norvig\nGenerate all possible terms with an edit distance <=2 (deletes + transposes + replaces + inserts) from the query term and search them in the dictionary.\nFor a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time.\nSource: Peter Norvig: How to Write a Spelling Corrector.\nThis is much better than the naive approach, but still expensive at search time (114,324 terms for n=9, a=36, d=2) and language dependent (because the alphabet is used to generate the terms, which is different in many languages and huge in Chinese: a=70,000 Unicode Han characters)\n3. Symmetric Delete Spelling Correction (FAROO)\nGenerate terms with an edit distance <=2 (deletes only) from each dictionary term and add them together with the original term to the dictionary. This has to be done only once during a pre-calculation step.\nGenerate terms with an edit distance <=2 (deletes only) from the input term and search them in the dictionary.\nFor a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, for a total of n terms at search time.\nThis is three orders of magnitude less expensive (36 terms for n=9 and d=2) and language independent (the alphabet is not required to generate deletes).\nThe cost of this approach is the pre-calculation time and storage space of x deletes for every original dictionary entry, which is acceptable in most cases.\nThe number x of deletes for a single dictionary entry depends on the maximum edit distance: x=n for edit distance=1, x=n*(n-1)/2 for edit distance=2, x=n!/d!/(n-d)! for edit distance=d (combinatorics: k out of n combinations without repetitions, and k=n-d),\nE.g. for a maximum edit distance of 2 and an average word length of 5 and 100,000 dictionary entries we need to additionally store 1,500,000 deletes.\nRemark 1: During the precalculation, different words in the dictionary might lead to same delete term: delete(sun,1)==delete(sin,1)==sn.\nWhile we generate only one new dictionary entry (sn), inside we need to store both original terms as spelling correction suggestion (sun,sin)\nRemark 2: There are four different comparison pair types:\n- dictionary entry==input entry,\n- delete(dictionary entry,p1)==input entry\n- dictionary entry==delete(input entry,p2)\n- delete(dictionary entry,p1)==delete(input entry,p2)\nThe last comparison type is required for replaces and transposes only. But we need to check whether the suggested dictionary term is really a replace or an adjacent transpose of the input term to prevent false positives of higher edit distance (bank==bnak and bank==bink, but bank!=kanb and bank!=xban and bank!=baxn).\nRemark 3: Instead of a dedicated spelling dictionary we are using the search engine index itself. This has several benefits:\n- It is dynamically updated. Every newly indexed word, whose frequency is over a certain threshold, is automatically used for spelling correction as well.\n- As we need to search the index anyway the spelling correction comes at almost no extra cost.\n- When indexing misspelled terms (i.e. not marked as a correct in the index) we do a spelling correction on the fly and index the page for the correct term as well.\nRemark 4: We have implemented query suggestions/completion in a similar fashion. This is a good way to prevent spelling errors in the first place. Every newly indexed word, whose frequency is over a certain threshold, is stored as a suggestion to all of its prefixes (they are created in the index if they do not yet exist). As we anyway provide an instant search feature the lookup for suggestions comes also at almost no extra cost. Multiple terms are sorted by the number of results stored in the index.\nIn our algorithm we are exploiting the fact that the edit distance between two terms is symmetrical:\n- We can generate all terms with an edit distance <2 from the query term (trying to reverse the query term error) and checking them against all dictionary terms,\n- We can generate all terms with an edit distance <2 from each dictionary term (trying to create the query term error) and check the query term against them.\n- We can combine both and meet in the middle, by transforming the correct dictionary terms to erroneous strings, and transforming the erroneous input term to the correct strings.\nBecause adding a char on the dictionary is equivalent to removing a char from the input string and vice versa, we can on both sides restrict our transformation to deletes only.\nWe are using variant 3, because the delete-only-transformation is language independent and three orders of magnitude less expensive.\nWhere does the speed come from?\n- Pre-calculation, i.e. the generation of possible spelling error variants (deletes only) and storing them at index time is the first precondition.\n- A fast index access at search time by using a hash table with an average search time complexity of O(1) is the second precondition.\n- But only our Symmetric Delete Spelling Correction on top of this allows to bring this O(1) speed to spell checking, because it allows a tremendous reduction of the number of spelling error candidates to be pre-calculated (generated and indexed).\n- Applying pre-calculation to Norvig’s approach would not be feasible because pre-calculating all possible delete + transpose + replace + insert candidates of all terms would result in a huge time and space consumption.\nOur algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size (but depending on the average term length and maximum edit distance), because our index is based on a Hash Table which has an average search time complexity of O(1).\nComparison to other approaches\nBK-Trees have a search time of O(log dictionary_size), whereas our algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size.\nTries have a comparable search performance to our approach. But a Trie is a prefix tree, which requires a common prefix. This makes it suitable for autocomplete or search suggestions, but not applicable for spell checking. If your typing error is e.g. in the first letter, than you have no common prefix, hence the Trie will not work for spelling correction.\nPossible application fields of our algorithm are those of fast approximate dictionary string matching: spell checkers for word processors and search engines, correction systems for optical character recognition, natural language translation based on translation memory, record linkage, de-duplication, matching DNA sequences, fuzzy string searching and fraud detection.\nBTW, by using a similar principle our web search is three orders of magnitude more efficient as well. While Google touches 1000 servers for every query, we need to query just one (server/peer).\nThat’s not because of DHT! Vice versa, because even for a complex query in a web scale index only one of the servers needs to be queried, it enables the use of DHT for web search.\nOur algorithm improves the efficiency of central servers in a data center to the same extent.']	['<urn:uuid:fce94be6-fb93-4ef9-ab55-443dfbac3896>', '<urn:uuid:09eca620-2671-4864-9023-be971693bcc1>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:56:11.774636	10	103	3390
21	how did british theater and cinema represent germans during world war 1 different perspectives	British theater during WWI portrayed Germans as villains, specifically depicting them through themes of 'torture, murder and bestial lust' as evidenced in 1915 stage productions. In contrast, the influential 1930 film All Quiet on the Western Front offered a humanizing perspective by following German schoolboys through their war experience, though this sympathetic portrayal was controversial and led to Nazi riots in Berlin theaters and eventual banning of the film in Nazi Germany.	['Great War Theatre is a collaborative community research project investigating plays about the war written and performed between 1914 and 1918 across England, Scotland and Wales. These plays, their production histories, and contemporary responses to them, offer a unique and hitherto unexplored insight into contemporary experiences of, and attitudes towards the war.\nBetween the end of August 1914 and the end of December 1918, 2,996 new plays were written and licensed for performance. Over a quarter of these dealt directly with the war. Most however have been long forgotten and are only available as card-catalogued manuscripts at the British Library, searchable solely by title.\nIn 2016 Brooks launched ‘Recovering First World War Theatre’, a public engagement project in which she worked alongside 31 volunteers in the British Library. The project recorded key information on all new wartime plays and identified scripts that dealt with the war, providing a glimpse of the diverse ways in which contemporary theatre engaged with the conflict. Out of this, a group of volunteers emerged who wanted to continue their research. The questions and discussion led to the Great War Theatre project, which draws on and significantly develops those findings by undertaking detailed production and content analysis of selected war-plays. It also develops a model of collaborative research.\nThe Great War Theatre project was co-designed by Brooks and 17 of the original 2016 volunteers. It aims to generate new insights into the Great War through analysing the content, production, and reception of war-plays and considering this in relation to the wider context of the conflict. With a national spread of plays being examined, the project aims to shed new light on local, regional and temporal, as well as national, experiences of the war. Two central and interlinking questions, reflecting the interests of the community members and Brooks, underpin this research:\n- How did new war-plays respond to and represent the conflict?\n- In what ways did the production and performance of war-plays reflect regional and temporal experiences of the war?\nAs an academic-community collaboration the project also has methodological aims. These are to:\n- Provide a space where diverse members of the wider community who share common interests in the research themes and/or methodologies of the group can come together to develop projects, learn new skills and undertake historical inquiry.\n- Encourage, support, and develop models of co-production and co-inquiry between public and academic researchers, in particular around communities formed through thematic interest rather than geographic locality.\n- Undertake rigorous, collaborative research which draws on both academic and public knowledge, skills and experience.\n- Advance the skills and knowledge of a diverse group of new and existing volunteers (including in archival and online historical research),\n- Support members in developing, designing, and realising project ideas and/or outputs around the group’s research themes.\nThe research is undertaken by a team of original and new volunteers with diverse expertise, knowledge, and approaches. Each volunteer is allocated key plays and playwrights and undertake archival research into their production and reception using a combination of online resources, archives and manuscripts in the British Library. This data is then input into a bespoke online repository alongside a copy of the script where possible. The repository allows patterns of production and content to be digitally mapped within the wider temporal and geographic context of the war. The project is supported by a Steering Group and external expertise.\nThe group seeks to encourage diversity in its membership and recruits nationally through the National Council for Volunteering Organisations and local volunteer bureaus. It also recruits through organisational mailing lists and networks including those of The National Archives, the Association of Performing Arts Archives, the Western Front Association, U3A, and the Society for Theatre Research.\nThe project has a number of outcomes.\nFor participants these include: the archival skills and historical knowledgeof the volunteers and the PI, as well as providing volunteers with the social benefits of being part of a community group. By bringing together individuals from diverse backgrounds the project provides the opportunity for peer-led learning, support, and friendship. For some volunteers it may provide skills and/or experiences which will aid them in seeking employment.\nMethodologically, the project provides a model for undertaking co-designed and co-produced historical research with a diverse community connected by thematic interest rather than geographic locality.\nThe open-access, searchable repository database makes available a substantive body of open-access research data and knowledge for future scholars working on society and culture in the Great War. The database contains comprehensive data on every new play licensed for performance in England, Scotland and Wales during the Great War. To date this data has been hidden in individual manuscripts and newspaper records. It has taken over three years in archives to identify and record data on each of these plays. By bringing this information together in a searchable database for the first time, the project makes it possible to analyse and map out performance histories and gain new understandings of popular culture of the period. Design input from the Steering Group and volunteers seeks to ensure the database is accessible to a diversity of users. The repository incorporates the ability to add extra and new forms of data, as well as enabling new modules for visualising the data to be added, beyond the life of the project. CSV files of the data-set are also available on request, enabling scholars to manipulate and develop further research using the project data.\nProject findings have also been made available in performances which raise awareness of the value of drama and performance history as a lens through which to understand and examine the war. These include performances at Theatre Royal, Brighton (2017); by Threadbare Theatre Company (2014-2017); and at symposia such as ‘Women and Work’ (Inverness, 1018)\nThe findings from the project also form the core of Brooks’ next monograph Staging War, and will shape the forthcoming Cambridge Companion to British Theatre and the First World War (under contract). They have also been shared at a number of conferences and invited talks to both academic and public audiences including most recently:\n- Brooks, H. (2017). ‘It was just like that out there’: Wartime Representation of the Trenches. in: TaPRA Conference 2014. View in KAR\n- Brooks, H. (2017). Theatres of War: New insights into wartime theatre production through the Recovering First World War Theatre project. in: TaPRA Conference, 2017. View in KAR\n- Brooks, H. (2017). Staging the Great War in the British Theatre between 1914 and 1918. in: War Through Other Stuff.View in KAR\n- Brooks, H. (2017). Spies and Brutes: The Representation of Germans as Villains on the British Stage Between 1914 and 1918. in: A Symposium on Villains. View in KAR\n- Brooks, H. (2017). Collaboration and Co-production: an example of a partnership between the University of Kent and Theatre Royal, Brighton. in: In Good Company: Working with Partners (APAC Annual Conference).View in KAR\n- Brooks, H. (2017). Recovering First World War Theatre: A Community Research Project. View in KAR\n- Brooks, H. (2017). Mobilizing the Home Front: British Theatre in 1916. in: The Fictional First World War. View in KAR\n- Brooks, H. (2017). Not just ‘A Little Bit of Fluff’: British Theatre during the Great War. in: Civilian Experiences of War Symposium.View in KAR\n- Brooks, H. (2017). ‘Torture, murder and bestial lust’: German Villains on the British stage in 1915. in: ‘Pack up your Troubles’: Performance Cultures of the First World War.View in KAR\n- Brooks, H. (2017). Staging Jutland. in: Jutland and After: The First World War at Sea from War to Peace 1916-1919. View in KAR', 'Why you should care\nBecause one story above all the rest has shaped how we picture World War I, and it’s still causing controversy.\nThe “war to end all wars” has faded from living memory, but it’s clearly still on our minds. If you’ve opened a newspaper, gone to the movies or turned on the TV this year, chances are you got a glimpse of century-old horrors — muddy trenches, bombed-out landscapes and young soldiers with thousand-yard stares.\nA hundred years on, it’s worth examining why these images strike such powerful chords. After all, World War I was fought on many fronts — what is it about the trenches that haunts our modern sensibilities? The truth is, we’ve all been shaped by one story in particular without realizing it.\nPeople were still scrambling to come to terms with what World War I meant, and the film had an answer, albeit a grim one.\nAll Quiet on the Western Front is nearly as old as the Great War itself — and since 1929, it’s been stirring up emotion, controversy and inconvenient truths.\nTo gauge the impact this war story made in its heyday, imagine a bizarre cross between the Harry Potter films and Saving Private Ryan. When the novel by German war veteran Erich Maria Remarque was released in 1929, it leapt to the top of best-seller lists. Cinemas in European and American cities treated the 1930 Hollywood version with unprecedented respect: It played at two leading London theaters simultaneously, an unheard-of feat at the time.\nHow to explain the popularity of a film about the most devastating war the world had ever seen? The truth was, in 1930, people were still struggling to come to terms with what World War I meant, and the film had an answer, albeit a grim one.\nAll Quiet on the Western Front follows a band of German school friends who journey from the innocence of youth to the despair of war. One boy, Paul Bäumer (portrayed by Lew Ayres), undergoes rigorous basic training and, to his excitement, finally gets to see combat. But when he and his friends arrive on the Western front, they quickly learn that war is not the adventure they’d planned; in fact, it’s hellish and destined to destroy them indiscriminately.\nThe dangers, savageries, the madness of war, and the appalling waste and destruction of youth … depicted with relentless veracity.All Quiet features some of the most agonizingly real war scenes of all time. Characters die brutally; soldiers slog and stumble through mud and otherworldly craters on fruitless charges. As one of the first blockbuster “talkies,” All Quiet’s sound mix is an unsettling series of explosions, gunfire and tense silences — especially, film historian Dorothy Jones notes, when accompanied by devastating human close-ups.\nPaul’s eventual take on war? “You still think it’s beautiful and sweet to die for your country, don’t you?” he sneers incredulously while on leave, visiting his gung-ho old schoolmaster who convinced him to enlist. “It’s dirty and painful to die for your country. When it comes to dying for your country, it’s better not to die at all.”\nBut of course, he does die — spectacularly and needlessly, hit by a sniper as he reaches for a butterfly just over the edge of his trench. It’s an iconic, sobering image that captured a World War I people could understand and process psychologically.\n“I hate it,” one London reviewer and veteran exclaimed in 1930. “It made me shudder with horror. It brought the war back to me as nothing has ever done before since 1918. The dangers, savageries, the madness of war, and the appalling waste and destruction of youth … depicted with relentless veracity.”\nHe wasn’t the only one who disliked it. During the film’s controversial Berlin premiere, the then-unelected Nazis rioted in movie theaters. Goebbels strongly objected to the film’s pacifist message and later banned the film in Nazi Germany. When Universal reissued the film in 1939, it added a damning epilogue showing Nazis burning Remarque’s novel.\nDespite its basis in truth, All Quiet on the Western Front is fiction — its war, however historically accurate, is an imaginary one. But it’s what viewers took away from the film that matters. Saving Private Ryan, arguably the closest thing to D-Day we’ll ever see, compares in that it follows a group of young men through the horror of combat — but Steven Spielberg’s World War II drama is undeniably about a “good war” and doesn’t summon the same weighty despair as All Quiet’s trenches.\nA century after the Great War, All Quiet on the Western Front still conveys the human side of war more viscerally than any memorial service.\nAll Quiet on the Western Front']	['<urn:uuid:5ab90622-c3f9-45f5-b9a0-4489e0aa5323>', '<urn:uuid:d0104a8b-9867-42ac-8e0b-7a071ed9ac24>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:56:11.774636	14	72	2055
22	I'm interested in studying art in college but worried about online classes. What were the biggest problems students faced when learning art subjects remotely?	The main difficulties in distance learning were technical issues (use of too many different learning platforms, lack of good internet connection), organizational challenges (time management difficulties, easy distraction), and psychological factors (overwhelming written information, high teacher demands, and lack of social exchange).	"['INFLUENCE OF DISTANCE LEARNING ON THE ACADEMIC MOTIVATION OF STUDENTS FROM SPECIALTIES IN THE FIELD OF ARTS\nKeywords:distance learning, motivation, student, art\nThis article discusses the problem of the impact of distance learning on the motivation to learn in students studying arts. Learning arts such as music, dance, and fine arts via distance learning is proving to be a real challenge for students and teachers. The aim of the present study is to determine how distance learning affects the learning motivation of students studying art at the university.\nMethod: a questionnaire for measuring academic motivation and a modified scale for measuring students\' engagement in online learning, and a survey.\nSample: 109 students from first to fourth-year in university, studying specialties related to music, fine arts and dance\nResults: The results showed that distance learning did not prove to be a major obstacle in establishing the learning material, but students who are internally motivated to learn prefer traditional learning. The main difficulties in distance learning turned out to be technical - the use of too many and different learning platforms, lack of a good internet connection, organizational - time management difficulties, easy distraction and psychological factors such – overwhelming written information, the high demands of the teachers, and a lack of social exchange. Correlation analysis found that students who prefer traditional education have a high intrinsic motivation to study art-related disciplines. ANOVA did not establish statistically significant differences in the factors, course and specialty.\nBolliger, D. U., and Martindale, T. (2004). Key factors for determining student satisfaction in online courses. Int. J. E-Learn. 6, 61–67.\nCarini, R.M., Kuh, G.D. & Klein, S.P. (2006). Student Engagement and Student Learning: Testing the Linkages. Res High Educ 47, 1–3. https://doi.org/10.1007/s11162-005-8150-9\nChin Choo Robinson & Hallett Hullinger (2008) New Benchmarks in Higher Education: Student Engagement in Online Learning, Journal of Education for Business, 84:2, 101-109, DOI: 10.3200/JOEB.84.2.101-109\nJeongju Lee & Hae-Deok Song & Ah Jeong Hong, (2019). ""Exploring Factors, and Indicators for Measuring Students’ Sustainable Engagement in e-Learning,"" Sustainability, MDPI, Open Access Journal, vol. 11(4), pages 1-12, February.\nLee, Y., and Choi, J. (2011). A review of online course dropout research: Implications for practice and future research. Educ. Technol. Res. Dev. 59, 593–618. doi:10.1007/s11423-010-9177-y\nMaina-Okori N.M. (2019). Sustainability Domains in Higher Education. In: Leal Filho W. (eds) Encyclopedia of Sustainability in Higher Education. Springer, Cham. https://doi.org/10.1007/978-3-319-63951-2_489-1\nMcPhee, I., and Söderström, T. (2012). Distance, online and campus higher education: reflections on learning outcomes. Campus Wide Inf. Syst. 29, 144–155. doi:10.1108/10650741211243166\nRadoslavova, M., Velichkov, A. (2005). Metodi za psihodiagnostika, Sofiya, izd. Pandora –prim\nRoddy, C., Amiet, D. L., Chung, J., Holt, C., Shaw, L., Mckenzie, S., Garivaldis, F., Lodge, J. M., & Mundy, M. E. (2017). Applying best practice online learning, teaching, and support to intensive online environments: An integrative review. Frontiers in Education, 2, . https://doi.org/10.3389/feduc.2017.00059\nHow to Cite\nCopyright (c) 2021 Author\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nThe author is the copyright holder. Distribution license: CC Attribution 4.0.']"	['<urn:uuid:8da937ad-74b1-4bfb-99c2-ba384a3425dd>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	24	42	500
23	What are the key benefits of forward-swept wings in aircraft?	The swept-forward wing design provides several advantages compared to swept-back wings of the same area: it offers a higher lift to drag ratio, superior dogfight maneuverability, extended range at subsonic speed, better stall resistance and anti-spin characteristics, improved stability at high angles of attack, lower minimum flight speed, and shorter take-off and landing distance.	"[""Su-47 (S-37 Berkut) Golden Eagle Fighter, Russia\nThe Sukhoi Design Bureau of Moscow, Russia has developed the Su-47 (previously called the S-37 Berkut or Golden Eagle) fighter aircraft, which first flew in September 1997. Su-47 is in a forward-swept wing configuration and uses a highly unstable triplane (with three main lifting surfaces) aerodynamic configuration. The Su-47 was introduced in January 2000 and completed the first stage of flight trials in December 2001. The aircraft is operated by the Russian Air Force\nIn May 2002, Sukhoi was selected as prime contractor for the next-generation Russian PAK FA fighter programme. The PAK FA fighter aircraft is a development of the Su-47 but without the forward swept wings. The first flight test of the PAK FA fighter aircraft was completed on 29 January 2010.\nThe design of the very high manoeuvrability prototype is based on the avionics and aerodynamics technologies developed for the Su-27 upgrade programme.\nSome of the systems and component designs from the Su-27, (the all weather supersonic fighter aircraft with Nato reporting name Flanker), have been used in the Su-47, for example the design of the canopy, landing gear, some of the avionics and the near-vertical tails.\nThe Su-47 has extremely high agility at subsonic speeds enabling the aircraft to alter its angle of attack and its flight path very quickly, and it also retains manoeuvrability in supersonic flight.The Su-47 aircraft has very high levels of manoeuvrability with maintained stability and controllability at all angles of attack.\nMaximum turn rates and the upper and lower limits on air speed for weapon launch are important criteria in terms of combat superiority in close combat and also at medium and long range, when the mission may involve engaging consecutive targets in different sectors of the airspace. A high turn rate of the Su-47 allows the pilot to turn the fighter aircraft quickly towards the next target to initiate the weapon launch.\nThe swept-forward wing, compared to a swept-back wing of the same area, provides a number of advantages: higher lift to drag ratio; higher capacity in dogfight manoeuvres; higher range at subsonic speed; improved stall resistance and anti-spin characteristics; improved stability at high angles of attack; a lower minimum flight speed; and a shorter take-off and landing distance.\nThe Su-47 fuselage is oval in cross section and the airframe is constructed mainly of aluminium and titanium alloys and 13% by weight of composite materials.\nThe nose radome is slightly flattened at the fore section and has a horizontal edge to optimise the aircraft's anti-spin characteristics.\nThe forward swept midwing gives the unusual and characteristic appearance of the Su-47. A substantial part of the lift generated by the forward-swept wing occurs at the inner portion of the wingspan. The lift is not restricted by wingtip stall. The ailerons - the wing's control surfaces - remain effective at the highest angles of attack, and controllability of the aircraft is retained even in the event of airflow separating from the remainder of the wings' surface.\nThe wing panels of the Su-47 are constructed of nearly 90% composites. The forward-swept midwing has a high aspect ratio, which contributes to long-range performance. The leading-edge root extensions blend smoothly to the wing panels, which are fitted with deflectable slats on the leading edge; flaps and ailerons on the trailing edge.\nThe all-moving and small-area trapezoidal canards are connected to the leading-edge root extensions.\nThe Su-47 experimental fighter aircraft features 14 hardpoints (2 wingtip, 6–8 underwing, 6-4 conformal underfuselage). The hardpoints are equipped with R-77, R-77PD, R-73, K-74 air to air missiles.\nIt is also fitted with air to surface missiles X-29T, X-29L, X-59M, X-31P, X-31A, KAB-500, KAB-1500.\nThe cockpit's design has focused on maintaining a high degree of comfort for the pilot and also on the pilot being able to control the aircraft in extremely high g-load manoeuvres.\nThe aircraft is equipped with a new ejection seat and life support system. The variable geometry adaptive ejection seat is inclined at an angle of 60°, which reduces the impact of high G forces on the pilot. The seat allows dogfight manoeuvres with significantly higher g loadings than can normally be tolerated by the pilot.\nThe pilot uses a side-mounted, low-travel control stick and a tensometric throttle control.\nThe aircraft uses a retractable tricycle-type landing gear with a single wheel at each unit. The smaller nose wheel retracts towards the rear and the two mainwheels retract forward into the wing roots.\nThe Su-47 fighter aircraft is powered by two Perm Aviadvigatel D-30F6 turboshaft engines. Around 83.4kN of dry thurst can be produced by each engine. The engine is principally used in short-haul airplanes for passenger transport.\nThe length and fantip diameter of the engine are 3.98m and 1.05m respectively, while the dry weight and delivery weight of the engine are 1,550kg and 1,712kg respectively. The engine also features a thrust reverser and a low pressure compressor.\nThe Su-47 fighter aircraft can climb at a rate of 233m/s. The cruise speed is 1,800km/h. The range and service ceiling of the aircraft are 1,782nm (3,300km) and 18,000m respectively.\nThe maximum take-off weight of the aircraft is 34,000kg. The wing loading and maximum g-force of the Su-47 are 360kg/m² and 9g respectively.""]"	['<urn:uuid:8266ca6a-22ca-45ca-8cbc-0a858f3f68a0>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	10	54	869
24	What makes younger and older folks want to live together nowadays?	Several modern factors are driving intergenerational living. The 2008 housing crisis led many millennials to live with parents and grandparents, while urbanization and an aging population (expected to reach 98 million seniors in the U.S. by 2060) are creating new housing needs. In successful mixed-age communities, younger residents benefit from affordable housing options - some even getting free rent in exchange for helping seniors - while older residents gain opportunities for meaningful social interactions and purposeful engagement. This arrangement helps both groups, as approximately 17% of Americans already live with other adult generations. The trend is supported by evidence showing that seniors with close intergenerational connections report better physical health and life satisfaction.	['Long Beach Development Could Redefine Mixed-Income, Senior Living\nContemporary. Hip. Accessible. Vibrant. Artistic.\nThese are usually not the words associated with affordable housing, let alone senior affordable housing. But that is precisely what Studio One Eleven and Meta Housing Corporation aimed for when they were handed the design and development keys to a collection of land parcels sitting at Anaheim and Long Beach Boulevard. And it just might be the future of not just affordable housing, but housing development in urban spaces as a whole.\nLet’s go into what is simple about the Long Beach Senior Arts Colony and its attached two buildings, one another affordable housing complex and the other a soon-to-be built 12-story market rate apartment complex.\nOriginally a redevelopment project, Meta Housing Corporation, the developer of the project, acquired the parcels following redevelopment’s dissolution. Originally, Studio One Eleven was just on board for architectural peer review—until it was discovered that they could corral various city agencies to entitle the project in three months to meet a critical funding deadline.\n“We didn’t initially have the job,” said Studio One Eleven principal Michael Bohn, laughing. “We weren’t even invited; just hired by the City to do the peer review.”\nThankfully, their relationship with the City led to One Eleven scoring the job and creating what is its most simple aspect: intelligently uses cheaper materials to make a beautifully well-made product.\nTake, for example, its use of concrete blocks to simulate wood, even down to the reflection and varnish. Or, how the six-story building towers over the central, south facing courtyard. A developer building for market rate would typically set the top story or two back, providing dimension that would make the courtyard feel more open and less encapsulated. However, Studio One Eleven opted for a much more nimble approach by simply painting the top floor a differing coloring, giving the illusion that the sixth floor is actually set back.\nDetails like this are the reason the building is garnering accolades—so far, four awards in a single swoop: the Multifamily Executive Affordable Housing Project of the Year, the Multi-Housing News Award of Excellence Gold Award for Best New Senior Development, PCBC’s 2013 Gold Nugget Grand Award-Winner for best Senior Housing Community, and a design award from the American Institute of Architects.\nBut what makes the building more than just well-done in design is its more complex philosophical angle and larger scope for its future phases.\nBohn is unabashed when discussing what the urban landscape should be: vibrant, dramatic, and attractive. And the original architect with their multiple buildings that exceeded eight stories, ignored cost and how to engage multiple backgrounds.\n“There should be a place to live for everyone of all cultures and backgrounds in the urban landscape,” Bohn said. “If we were to just make one tower the centerpiece, that is, the market rate tower, we could make these other buildings senior and affordable housing areas. And then we have not just mixed income, but an intergenerational space—and that is something that hasn’t been done before in Long Beach.”\nBohn is no stranger to the benefits of mixed-income living; after all, the policy of integrating subsidized housing with market rate units rose in popularity in the 90s. The main hope in this style of development was that the resources that middle-income people had would be trickled down to their lower-income neighbors through interaction and living proximity.\nHowever, practice was not as bright in theory within the ongoing battle to bridge the gap between poverty and affluence: many of the developments didn’t provide paths for interaction between low-income and middle-income folk; subsidized housing provided little route to investment in one’s neighborhood for those who couldn’t afford it; and accessibility to transit and other services were widely ignored in planning. And in fact, these precise points were addressed in one of the most recent reports released by the U.S. Department of Housing and Urban Development through the work of Vanderbilt University and the University of Chicago.\nBohn and developer Meta Housing Corporation saw an opportunity—both financially and developmentally—to address these issues head on.\nFirstly, they applied for Prop 1C funds, which provides funding to developments which are geared around public transit as well as a one parking space per unit. And given that Anaheim and Long Beach Boulevard are Long Beach’s most used and traveled arterials in regard to public transit, Bohn and Meta Housing knew they were onto something when they created a space at what Bohn called the “Main & Main” of Long Beach. People have access to the entire city along with the Blue Line all within a hundred feet.\nMuch to the point the altering mindset of transportation, it is not just younger or less affluent folk who would like to have access to it: at full capacity, the Senior Arts Colony parking garage—with just one space per unit—is only one half full.\nSecondly, they aimed for mixed-use that not only provides commercial space below the units—a common theme amongst today’s developers—but also provide community and recreational space that span between all buildings through interconnected pathways. Take, for example, the alignment of art studios that line the sidewalk below the Senior Arts Colony; this will not only encourage activity, but people on the sidewalk will pass by and seen said activity, increasing their curiosity and engagement.\nThirdly, the units are own-able—and even more, with accessibility to transit and parks around them, seniors and other residents feel and act as true stakeholders rather than stand-bys. Bohn noted the residential interest in upgrading and up keeping the 14th Street park nearby.\n“The endgame is this,” Bohn said. “To have mixed-use, mixed-income, intergenerational facilities that people can cross-pollinate ideas, lives, and interactions. Imagine when you have a space where people are, for the most part, required to interact because you have a 99-seat theatre over here in the senior space where a young couple wants to go see something or the recreational space over in The Annex [the other affordable housing building]. This young couple then meets seniors and people from different backgrounds, begins to know them, empathizes with them, and maybe even perhaps seeks them out as babysitters or friends. One on hand, the senior is provided with a higher purpose and on the other, a younger generation who understands their elderly community rather than seeing them shipped to pasture.”\nWhat lies in the brilliance of all this is not just the consideration of the issues that fell upon mixed-income housing initially, but the inclusion of going beyond with the concept of intergenerational—after all, as Bohn noted, 75% of the current U.S. population falls into the category of senior citizen or Millennial, so the two should work together. Rather than castigating seniors to a they-can’t-do-much-for-themselves role, they are encouraged to be active and engage both mind and body.\nBohn even noted an original idea to build senior housing above medical facilities, with developers thinking this would be a great idea.\n“The response from seniors was astoundingly negative,” Bohn said, “and something that only hindsight proved to be obvious: Why would any senior want to live above a constant reminder of unhealthiness?”\nThe next phase, which is to build the 12-story market rate unit tower, will begin when the real estate market fits value.', 'Could Architecture Keep You Young?\nWHY YOU SHOULD CARE\nBecause growing old doesn’t have to be isolating.\nBy Tracy Moran\nOver the past 22 years, retiree Bruce Coldham has watched three cohorts of children grow up in his beloved Pine Street Cohousing Community in western Massachusetts, and he has every intention of living beside his young, middle-aged and older neighbors for as long as possible.\nWith a host of bold new plans for multigenerational living, you too might spend your golden years cavorting with tykes you’re not related to. Thanks to the housing crisis of 2008, increasing urbanization and an aging population that will see roughly 98 million golden oldies in the U.S. by 2060, the debate about how and where we age is taking center stage. Designers are increasingly looking at how intergenerational housing and retirement facilities can be combined in healthy, interactive and transparent ways. Meanwhile, older ideas like cohousing — a Scandinavian innovation that features generations growing up side by side and sharing common space — are getting a new life.\nH.O.M.E., a low-income senior housing provider in Chicago, offers free rent to young adults who agree to help the senior residents at the organization’s three sites on the North Side.\nStudies by the Pew Research Center show that approximately 51 million Americans — nearly 17 percent of the population — already live with other adult generations, often within their own families. Since the most recent housing crisis, millennials have struggled to break out on their own, and many have ended up living with parents and grandparents. Some families are retrofitting their homes to accommodate these adult children; many new builds connect single-family homes with granny-style apartments that cater to aging loved ones. After 25 years of slow and steady growth, the cohousing movement also has seen a recent uptick in interest, according to Alice Alexander, executive director of the Cohousing Association of the United States. An additional 130 projects soon will come online to join the existing 162 communities, a trend Alexander expects to net huge growth over the next decade.\nMore ambitious projects are in the works. German-born architect Matthias Hollwich, of Hollwich Kushner in New York, spent years designing his approach to a new way of aging: the sculptural Skyler Tower, a 1,000-person mixed-use skyscraper with health care, nurseries, offices, studios, duplexes and hubs for retirees. The concept is that a person could live well at any age in Skyler, which Kushner hopes will be a model for how cities can encourage intergenerational living from birth to death.\nDanish planners, meanwhile, hope to put their futuristic retirement plan into action by 2022 at a Copenhagen-based facility called Future Sølund. The project aims to tear down barriers between those in senior living and the community around them, offering a nursery, a kindergarten, 150 dormitory-like apartments for young adults, 35 ground-floor apartments with private gardens for independent senior living and 360 homes in a full-care senior facility. They’re throwing out the isolated, gated-community retirement model for a whole new generation of facilities, says Mads Mandrup, a partner at the design firm C.F. Møller, which is in charge of the project. It will be a rethink, he says, of “how we integrate [retirement living] into the city, not just as a building, as architecture, but on a programmatic level.”\nNo young families will live on site, but Future Sølund will offer social interaction for residents and nonresidents alike via services, shops and child care. That way the “city’s not moving around the building, but in and through it,” Mandrup explains, adding that this will serve as a model for how northern Europeans will cater to their aging populations for decades to come. Similar but smaller projects are being planned in the Netherlands and Norway.\nMixing generations within retirement homes, of course, isn’t a new concept. Since the 1990s, Providence Mount St. Vincent in Seattle has housed both a retirement-living community and day care for 125 children who mingle with the retirees. For more than three decades, Housing Opportunities & Maintenance for the Elderly (H.O.M.E.), a low-income senior housing provider in Chicago, has offered free rent to young adults who agree to help the senior residents at the organization’s three sites on the North Side. The combination of younger and older generations, says program director Janet Takehara, is intentionally intergenerational, “but through the lens of primarily housing seniors.” In the next five years, H.O.M.E. plans to expand to the South Side and West Side of Chicago. In a similar setup at the Humanitas retirement home in Deventer, Netherlands, university students live rent-free in exchange for interacting with and helping the facility’s seniors.\nFor seniors, the potential advantages are huge. The Cornell Legacy Project has found that active, involved older adults with close intergenerational connections consistently report much less depression, better physical health and higher degrees of life satisfaction. They tend to be happier with their present life and more hopeful for the future.\nThe biggest challenge may be combining openness and security. Future Sølund will attempt it with different access levels — public, semipublic and private — to ensure residents feel safe but not confined, Mandrup says. Security aside, not all seniors want to hear noisy children every day or 20-somethings with a penchant for partying. Even within the normally intergenerational cohousing movement, there has been a trend in recent years to create “senior cohousing” for 50-plus adults only — no children.\nMandrup and his colleagues, however, hope that by creating a community center–style retirement facility that’s woven into the fabric of an existing urban site, they can make intergenerational living and retirement attractive for all ages.']	['<urn:uuid:17554267-c3a0-4ddd-a421-8faafa921663>', '<urn:uuid:a37f7ef2-6367-40f9-9784-4f6796ce2087>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	11	113	2153
25	How do traditional and modern Indigenous leadership approaches differ?	Traditional Indigenous leadership emphasizes spiritual and environmental stewardship, with leaders like Bobby C. Billie serving as spiritual and clan leaders who uphold Natural Law and sacred traditions. In contrast, modern governance is structured through federal recognition and the Indian Act, which provides for government oversight and provincial jurisdiction over aboriginal matters, subject to treaty terms and federal law.	"['While the United Nations has been holding a climate change summit in Paris (November 30 to December 11), several indigenous leaders from Turtle Island (what you might call North America) voiced their frustration with the destruction that has brought us to this point.\n""We cannot live on the Moon or Mars,"" they wrote plainly, ""so people must change their behavior.""\nHere in what we now call South Florida, thousands of indigenous people were murdered or forced westward in the 1800s during three Seminole Wars. White men stopped fighting only when they felt they had completely decimated the local population, with a mere 200 or so natives remaining scattered on tree islands in the Everglades. Because of subsequent environmental destruction, those who remained eventually were forced to modernize. Some went on to form the Seminole Tribe and sought federal recognition (which brought certain protections and negotiating power) from the U.S. government in 1957. Some instead formed the alternative Miccosukee Tribe, which was federally recognized in 1962. But a few holdouts never joined either tribe.\nIn English, this group calls itself the Original Miccosukee Simanolee Nation Aboriginal Peoples (sometimes called the ""independents"" ) and insists that an agreement from the 1840s and signed by President James Polk has never been superseded and so remains in effect, and thus about 5 million acres — essentially, all of Southwest Florida, including Everglades National Park — rightly belongs to them. They scoff at the notion that the U.S. government should have any role in defining them. (""Newcomers separate themselves by imaginary lines,"" the group once wrote, seemingly bewildered at such a dumb concept.)\nStill, because this group is not ""federally recognized,"" it often gets ignored politically.\nToo bad, because they have some beautiful and important things to say. Every so often, when circumstances have dictated, they have composed and released powerful letters — often multipage screeds, written meticulously because the leaders still speak their native Mikasuki language and always fear that their words will never be accurately translated and always taken out of context. They have spoken out against the National Park Service disturbing burial grounds and the state\'s celebration of Europeans coming to America.\nNow, spiritual leader Bobby C. Billie is one of four signatories to a statement to the United Nations on climate change. Others include Sioux chief Arvol Looking Horse; Leland Grass, a Dine’ traditionalist; and Faith Spotted Eagle of the Ihanktonwan Dakota Oyate. (Billie is skeptical of reporters and did not respond to a request for comment through a rep.)\nThey implore world leaders – and every individual – to heed these leaders and create a new, sustainable path forward. Though it doesn\'t offer specifics, it is quite powerful in its simplicity and urgency.\nIf you like this story, consider signing up for our email newsletters.\nSHOW ME HOW\nYou have successfully signed up for your selected newsletter(s) - please keep an eye on your mailbox, we\'re movin\' in!\nHere is the letter:\nIndigenous Elders And Medicine Peoples Council Statement United Nations Convention on Climate Change COP21 Paris, FranceNovember 30, 2015 - December 11, 2015\nThis statement reflects the wisdom of the Spiritual People of the Earth, of the four directions, working in unity to restore peace, harmony and balance for our collective future and for all living beings. This statement is written in black and white with a foreign language that is not our own and does not convey the full depth of our concerns.\nAll Creation has a right to live and survive on this Sacred Earth and raise their Families where the Creator placed them to be.\nThe Creator created the People of the Earth out of the Land and gave us a Way of Life. This Sacred Way of Life has been passed down generation-to-generation since the beginning of the Creation of Life. The sanctity of this Way of Life has been violated and abused by people who are living without regard for the well being of Mother Earth and our collective future.\nWe, the Original Caretakers of Mother Earth, have no choice but to follow and uphold the Natural Law, which sustains the continuity of Life. We recognize our umbilical connection to Mother Earth and understand that she is the source of life, not a resource to be exploited. We speak on behalf of all Creation today, to communicate an urgent message that man has gone too far, placing us in the state of survival.\nThere is no more time for discussion on preventing “Climate Change”. That opportunity has passed. “Climate Change” is here. The Air is not the same anymore. The Water is not the same anymore. The Earth is not the same anymore. The Clouds are not the same anymore. The Rain is not the same anymore. The Trees, the Plants, the Animals, Birds, Fish, Insects and all the others are not the same anymore. All that is Sacred in Life is vanishing because of our actions. The truth is we have moved beyond climate change to survival.\nWe’ve entered the state of survival because of the magnitude of the desecration, damage, and destruction to the Creator’s Sacred Creation.\nThe COP21 Climate Summit in Paris, France is being held to contemplate strategies to prevent more than two degrees Celsius of warming. Though this is important, it is not nearly enough. To truly heal Mother Earth and ensure our survival, we must recognize that the entire natural system is one life system rather than fragmented parts. Our concern is with the acceleration of the cumulative and compounding devastation that is being wrought by the actions of people around the world. Modern living and all that it encompasses does not respect the Sacredness of Life and has ruptured the sacred seal around the Earth. This has contributed to extreme weather patterns and the melting of the Ice – the sacred cooling system of Mother Earth. Man-made creations of vehicles, trains, railroads, airplanes, bullets, guns, weapons, spaceships, building of high rise towers, shopping malls, pipelines, roads, slaughtering of animals for recreation, the genetic alteration and pollution of our food and water sources, introduction of invasive species, drilling and digging deep into Mother Earth, and into the mountains for oil, gas, gold, silver, minerals, precious stones, coal and uranium, and all the others. Changing whole ecosystems forever by draining wetlands, changing the course of rivers, clear cutting of forests, damming rivers and flooding lands to cool nuclear reactors and natural gas plants, damming rivers for hydroelectric power, using rivers and lakes for dumping toxic chemicals and sewage, polluting the Earth and Oceans with toxic chemicals and waste and piling up and burying garbage has contaminated and polluted our Earth. All these actions and more created Climate Change.\nAboriginal Indigenous Peoples have seen first-hand the impacts these destructive actions have had on our sacred places and the natural world. We have sent out messages to the people of this world warning that “this dark time” or “this day” would come if the people did not immediately stop their destructive activities and re-align themselves in harmony and balance with the Creator’s Natural Law. Our messages of prophecy fell on deaf ears and remained unseen by eyes blinded by money, greed and power.\nToday, we lack leadership. We have misplaced our trust in governmental leaders and the leaders of industry. They failed us by trying to maintain their profits, economies and their power over the people. Their lack of action to adequately and seriously work to prevent “climate change” has brought us to the state of survival, threatening the collective future of All Life. Leaders meeting in Paris have a responsibility to create real solutions and do something right for the future of all life. We strongly urge all leaders to work and consult with us, the spiritual people of the Earth, to solve the world’s problems, without war. We, the Original Caretakers of Mother Earth, offer our spiritual insight, wisdom and vision to the global community to help guide the actions needed to overcome the current threats to all life and shift from the path of self-destruction to the path of peace, harmony and balance with All Creation.\nThe people of the world cannot continue to ignore Aboriginal Indigenous Peoples, the Natural System of Life, the Natural Law and our connection with All Life.\nWe cannot live on the Moon or Mars so people must change their behavior. We must sacrifice and move beyond our own comforts and pleasures. We must stop the damaging activities and begin working on healing the natural environment for the future of All Life. To date, the Sacred has been excluded from all discussions and decisions.\nTO SURVIVE CLIMATE CHANGE AND SEE THE FUTURE WE MUST HEAL THE SACRED IN OURSELVES AND INCLUDE THE SACREDNESS OF ALL LIFE IN OUR DISCUSSIONS, DECISIONS AND ACTIONS.\nWe must heal the Sacredness within ourselves, within our families, within our communities and within our Nations. We must respect, follow and uphold the Creator’s Natural Law as a foundation for all decision-making, from this point forward. This begins with a deep respect and understanding of the human life cycle. To bring sacredness back into this cycle, human beings must bring forth life in a conscious way, honoring our sacred obligation to care for that life. With the creation of life comes a responsibility to ensure the care and survival of that life throughout the lifespan of the child. When we bring forth life haphazardly and without sacredness, with no concern for how that life will be sustained, we violate our sacred responsibility and bring imbalance to Mother Earth. We must restore our balance with Creation and respect the sacredness of our sexuality, and our ability to bring new life into this world.\nWorld leaders in Paris must lead us away from the commodification of Mother Earth, which places our lives and our future on an unstable foundation based on money, greed and power. We must open our hearts to Love, Care and have Respect for one another and All Creation to create peace. No one can survive without clean Air, clean Water, and a clean Earth. We cannot breathe money, we cannot eat or drink money. The people of the world must recognize that man made laws have failed us and will continue to fail us, because those laws attempt to place profit and power over our sacred obligation to All Life and our shared future. Those seeking profit and power have created a business of war and destruction that now threatens the lives of billions around the world. It is time to address this spiritual illness and begin the process of healing, together.\nAboriginal Indigenous Elders and Medicine Peoples are not scientists, but we do have a connection to the source of Truth and Life, we have our “Prophecies“ and the “Signs” of disharmony. All of these sources of knowledge tell us clearly what will happen if we do not change our behavior. All of the things that are happening today are the very same things that were spoken of in the earlier days, the very things that the Creator warned would happen, if we failed to respect and follow His Natural Law. There are scientists and experts that know the scientific reality of what is going on. They know that life will vanish if we continue in the direction we are going. These scientist and experts must speak the truth to the people and not be controlled by those that provide their paychecks.\nNeither world leaders nor modern institutions have the tools to adequately address climate change. Indigenous peoples must be included.\nWe are all responsible and we are all capable of creating a new path forward with new sources of energy that do not harm the people or the Earth. We are obligated to all take action now to protect what is left of the Sacredness of Water and Life. We can no longer wait for solutions from governmental and corporate leaders. We must all take action and responsibility to restore a healthy relationship with each other and Mother Earth. Each of us is put here in this time and this place to personally decide the future of humankind. Did you think the Creator would create unnecessary people in a time of such terrible danger? Know that you yourself are essential to this World. Believe that! Understand both the blessing and the burden of that. You yourself are desperately needed to save the soul of this World. Do you think you were put here for something less?\nWater is Life\nWe are the People of the Earth united under the Creator’s Law, with a sacred covenant to follow and uphold and a sacred responsibility to extend Life for all future generations. We are from various Nations and we are spiritually related. We will not be divided by any terminology that defines us as “recognized” or “unrecognized”, by the Government of the United States or any other body. We are expressing deep concern for our shared future and urge everyone to awaken spiritually and take action. It is critical that we come together with good minds and prayer as a global community of all faiths, to honor the Creator and the Creator’s Gift. We must work in unity to help Mother Earth heal so that she can bring back balance and harmony for all her children.\nRepresentatives of the Council\n**Chief Arvol Looking Horse\n19th Generation Keeper of the Sacred White Buffalo Calf Pipe\nSpiritual Leader The Great Sioux Nation Peoples\nLeland Grass, Dine’ Traditionalist\nBobby C. Billie\nClan Leader and Spiritual Leader Council of the Original Miccosukee Simanolee Nation Aboriginal\nFaith Spotted Eagle\nTunkan Inajin WinBrave Heart Society, Grandmother/Headswoman & Ihanktonwan Treaty Council Ihanktonwan Dakota from the Oceti Sakowin 7 Council Fires\n**-ADDITIONAL SIGNATURES TO FOLLOW-\nHere\'s video of Billie from March:', 'Canadian Constitutional Law/Aboriginal peoples\nBy virtue of section 91(24) of the Constitution Act, 1867 the federal government has exclusive jurisdiction over matters relating to “Indians and lands reserved for the Indians”. On this basis the federal government has sole jurisdiction to negotiate treaties with the aboriginal peoples. Any attempt on the part of the provincial governments to interfere with any of the treaties would be in violation of the Constitution.\nThe Indian Act is the sole legislation regulating the government oversight of aboriginals. Much of the Act deals with the rights of band members living on reserves, and has little constitutional significance.\nSection 88 of the Act provides that the provinces can have jurisdiction over aboriginal matters so long as it does not conflict with the laws of the Act.\n- 88. Subject to the terms of any treaty and any other Act of Parliament, all laws of general application from time to time in force in any province are applicable to and in respect of Indians in the province, except to the extent that those laws are inconsistent with this Act or any order, rule, regulation or by-law made thereunder, and except to the extent that those laws make provision for any matter for which provision is made by or under this Act.\nThe extent of provincial authority over Indian matters apply only with respect to matters that fall within the provincial jurisdiction. However, in practice five exceptions have been developed.\nAboriginal and Treaty rights\nSection 35 of the Constitution Act, 1982 acknowledges the existence of aboriginal and treaty rights, it states:\n35. (1) The existing aboriginal and treaty rights of the aboriginal peoples of Canada are hereby recognized and affirmed.\n(2) In this Act, ""aboriginal peoples of Canada"" includes the Indian, Inuit and Métis peoples of Canada.\n(3) For greater certainty, in subsection (1) ""treaty rights"" includes rights that now exist by way of land claims agreements or may be so acquired.\n(4) Notwithstanding any other provision of this Act, the aboriginal and treaty rights referred to in subsection (1) are guaranteed equally to male and female persons.\nOften this section is confused as being part of the Charter which ends at section 34. Thus, this section provides an absolute right to aboriginals and is not subject to section 1 or 33.\nThe word ""existing"" is key to the understanding of section 35. The Supreme Court acknowledged in R. v. Sparrow 1 S.C.R. 1075 that, prior to 1982, aboriginal rights existed by virtue of the common law, and therefore could extinguished by an Act of Parliament. However with the constitutional entrenchment of those rights in section 35, aboriginal rights cannot be extinguished unilaterally by the government.\nBroadly speaking, section 35 provides for two types of aboriginal rights: Aboriginal Treaty Rights and Aboriginal Rights. The first is a sui generis right gained through the enactment of treaties between the government and the aboriginal peoples. Proving the existence of these rights can often prove difficult as they are frequently based on oral tradition passed on through the aboriginal peoples. The second type is a right based in the common law.']"	['<urn:uuid:d43650d9-7a47-4cad-a9b1-a55ce02184b9>', '<urn:uuid:203ae99c-0121-49c9-abe0-007bc509de1b>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:56:11.774636	9	58	2825
26	Can both foam brushes and squirrel hair brushes be used with varnish?	No, while foam brushes work well with oil-based varnish since it's a cold finish that won't melt them, squirrel hair brushes are specifically designed for watercolor painting and wouldn't be suitable for varnishing.	"['Brushing On Varnish\nBy Sal Marino\nWhile not very easy to apply by spray application, oil based varnish is one of the easiest finishes to apply by brush. Because varnish sets-up slowly it gives the user plenty of time to brush and spread it out evenly on to the surface. It\'s hard to spray because it has a tendency to run if applied too heavy. I firmly believe that any film finish can be sprayed successfully if thinned out enough, but varnish is one of the last finishes I would want to spray. Over many years of testing, I have come to realize that brushing is the best way to apply oil based varnish.\nBefore applying varnish by brush, you should know a little more about how long it takes for each coat to set-up and how long before you can apply the next coat along with how it reacts to temperature and humidity and some other facts. Oil based varnish is much higher in solids than some other film finishes like lacquer. Therefore, it should only take a few coats of varnish to build a film significant enough to protect the surface of what you are finishing. After the surface has been sealed, it usually only takes about three coats to give you enough protection.\nOne very important factor when applying varnish is how the temperature effects the speed at which it cures. You should not apply varnish in temperatures lower than 65 degrees. If you apply varnish in lower temperatures it may take several days, even weeks for it to cure. Room Temp. (approx. 70 to 75 degrees) is good for applying varnish. Hotter temps. will make the varnish cure quicker, but the solvent in the varnish will evaporate quicker, making the varnish set-up quickly and you may have a problem getting the varnish to flow out properly. This could result in brush marks, bubbles and an uneven film. When working in temperatures higher than 75 degrees, try not to work on large surfaces.\nSome Tips For Preparation\nTry to set aside a room or part of your work shop to apply your varnish. This room should be as dust free as possible. Do not do any other woodworking, (especially sanding) in this area. If you are going to set aside an area of your shop instead of using a different room, it would be a good idea to also surround this area with heavy plastic sheeting. Before applying the varnish, wet mop the floor, this prevents you from kicking up any dust when you walk around. I always place clean craft (brown) paper under the piece I will be varnishing. Once the surface has been prepared properly you are ready to brush on your varnish.\nChoosing A Brush\nThere are a number of high quality brushes that can be used for brushing on clear topcoats. The best for shellac and lacquers are natural hair (like badger) or china bristle brushes. While any of these brushes will do a great job when applying varnish, there is a much less expensive alternative. A polyfoam brush. That\'s right, the disposable type. Oil based varnish is classified as a cold finish. This means the solvent use is not as strong as evaporative finishes like shellac and lacquer. Alcohol and lacquer thinner will melt a foam brush but the mineral spirits, solvents or turpentine used in most oil based varnishes will not harm a foam brush. Foam brushes are especially useful for novices who have a hard time getting brush marks out when applying a finish. If used properly, you can get excellent results. I always have a good supply of 1"",2"" and 3"" foam brushes in my shop. They are very inexpensive, so I use one for each coat and then throw it away.\nApplying The Varnish\nYou don\'t need a special sealer to seal the wood. Special sealers like sanding sealers will not do any better of a job of sealing the wood than the finish itself. Sealers only make the first coat easier to sand, thus speeding up production time. Also, if you use the wrong type of sealer, you may have adhesion problems. The best sealer for your first few coats should be the varnish itself. Take some of the same varnish you are planning to use as your finish and thin it down 50 percent ( this is a 1 to 1 ratio) with mineral spirits or gum turpentine. This will be your sealer. It will do a good job of sealing the wood and you won\'t have to worry about contamination problems. Pour some varnish through a paper paint strainer or stocking into another can or jar, then add the same amount of mineral spirits into the varnish. Stir well and strain a second time into a deep dish or bowl. It\'s best to work out of an open bowl or dish so you can easily dip your brush into it. Now, dip the foam brush into the mixture until the brush has been loaded slightly past the bevel on the foam brush. Lift the brush up and let the excess drip back into the dish. Next, brush on the first coat with the grain making sure not to leave any puddles or drips. Allow the sealer coat to dry overnight and then sand with 320 grit paper. Remove the dust with a vacuum, or tack cloth. If you are working on very porous woods, apply a second sealer coat following the previous steps.\nIt\'s a good idea to also thin out your coats of varnish a little. You can reduce your varnish 20 to 25 percent 4 parts varnish to 1 part mineral spirits or gum turpentine or 3 parts varnish to 1 mineral spirits or gum turpentine. This will not effect the strength of the varnish, it will only make it flow better and allow time for air bubbles that form when brushing to pop. The only drawback is that you will have to add a few more coats because less will remain on the surface once the varnish has dried. Prepare the varnish by mixing and straining in the same way you prepared the sealer. Use a foam brush and load it in the same manner as the sealer. Apply the varnish to the surface by brushing either with or against the grain initially. The main idea is to get it on the surface doing as little brushing as possible. Once on the surface take one light pass with the tip of the brush moving with the grain. Overlap each pass slightly, then leave the varnish alone, do not do a lot of brushing, this will make the solvent evaporate quicker and the varnish will set up too quickly and not have enough time to flow out. Let the varnish dry overnight, and then sand with 320 grit sandpaper and remove dust using vacuum or tack cloth. When sanding, if the varnish starts to clog the paper, it has not dried enough. If the varnish turns to powder, it is dry enough to sand and apply the next coat. Continue to apply 2 to 3 more coats of varnish using the same process. If you are going to rub out the finish (by wet sanding) after it has cured, you may want to apply at least a total of 6 coats ( not including sealer coats). This is because if there is not enough varnish left on the surface, you may cut through the finish into the raw wood in some spots.\nOnce you have applied the last coat, let the finish cure for several weeks before you are ready to use it or rub it out. Varnish does not need much maintenance. If you wish, you may apply a coat of paste wax or liquid polish from time to time.\nIf you want to rub out the varnish finish and are not familiar with the process, I am planning to put this info up on my homepage soon. However, if you need this information sooner, or have any other questions, please contact me by e-mail.', 'Classification of paint brushesThe artist brush is the most basic tool that the artist uses to realize his concept on paper, canvas, cloth and many other surfaces.\nIt is structurally divided into three main parts: the handle(for holding), ferrule (or capsule)that holds the hairs and the hair tuft, which is the part of the tool that interacts both with paint/ink and the surface.\nThe handle is generally made of solid wood from deciduous trees (oak, birch, beech, etc.), sometimes synthetic materials (e.g., plastic) are used. There are exclusive brushes with ivory handles, some minerals, precious stones etc.\nThe hair tuft of the brush is formed in certain size and a predetermined shape, with natural hair (tiger, pony, goat, badger, columns, bristles, etc.) or synthetics.\nThere is a division of the hands and the exhibition of the hair (ie the length of the hair in the hair bundle). Typically, the major manufacturers of brushes produce three main types of hair in the hands of the exhibition of the same size: an elongated, standard and shortened. This differentiation allows different work during a session of painting. Brush with a short exhibition conveniently perform short strokes or lighten the exact right place, washing off the ink particles from the texture of the paper. Brush standard averaged exhibitions usually performed most of the work. This brush is universal. And finally, the elongate brush insert is intended for drawing the long thin lines of constant width.\nThe most universal brush type is the ""cartrige"". The ferrule, or ""cartrige"" is a round metal part that holds the bristles together and attached to the handle of the brush. The base coat is fixed with adhesive and sealant is further applied. The ferrule must adhere to the handle, they are bonded with a special machine. When some hairs are constantly falling off, this usually means that there was not enough glue, or it was badly applied.\nThe shape of the cartridge brushes can be divided into two main groups: round (the cross section of the cartridge at its end iscircular) and flat (the cartridge is where it holds the hairs).\nround bristle brushRound brushes - perhaps the most versatile of all the brushes. Depending on the tilt and pressure it is possible to apply paint and draw lines as constant and variable width in a particular area, to carry out broad, thick strokes or exactly the opposite - the thinnest line with its very tip.\nThe round brush is ideal for drawing as it can be used for fine detailing.\nA specific example of round brushes are the font and line brushes. gives a definite shape, ensures the safety of the hair bundle and connects it with a pen. It is usually metal but may be made of other materials. Most are easy to use, practical, reliable and attractive\n|flat brush, parts of the brush|\nThey are very convenient for representing the pictorial elements in nature such as land, grass, leaves, as well as architectural compositions of urban landscapes and anything that requires a relatively precise outlines. This brush is characterized by that the capsule is flattened at the end, allowing the hair to form the desired flat plane - hence the name. Thick, long hair flat brush allows you to keep a sufficiently large amount of paint.\nType of flat brushes, that have a shorter hair tuft, giving extra firmness to the stroke. This ensures high quality flat strokes. Thanks to the short hair, with the help of contour brush it is very convenient to make the outlines of drawing or, for example, graphic works and can also be used to create smooth color transitions.\nThis brush type, a variety of the flat brush, got it\'s name due to the special shape of the hair bundle - a dome or oval shaped. It is versatile and it allows, depending on the position and inclination towards the working surface (canvas etc.)to perform strokes, characteristic for both circular (at a perpendicular position to the surface), and for the flat brush.\nThis is a kind of round brush, that has\nvery long hair. This form and the length of the hair bundle the main purpose of the brush (hence the name) is the ability of thin straight lines without reloading. Very handy brush for graphic works, but using to its full potential in the process of painting, it takes much practice and experience.\nFan brushFan brushes are a kind of flat brushes that got its name due to the special shape of the beam, that looks like a fan. Due to this they are very convenient to perform sophisticated color mixing, soften the contours and create contrasting textures . Just one swab allows you to put a lot of fine lines, for example, when drawing grass, etc. Working with it requires a certain skill.\nThere are three main characteristics that must be considered when assessing the quality of a brush. You can easily separate the quality brushes from cheap low-quality fakes. There are three main characteristics that make a good artist brush:\n1. The ability of the hair bundle to form a sharp tip. This indicator is very important, because in the creative process one often needs to apply very precise strokes, fine lines, very fine detail etc. If your brush will is not able to perform these tasks, because edge of the beam is too thick or the hairs are scattered among themselves, then it simply hinders your work.\nWhen pulled up from the painting surface, its hairs should retake their original form and be ready for the next stroke.\n2. The elasticity of the hair bundle. For quality brush strokes, it is important that the hairs have certain elasticity. This allows various effects to be achieved, that cannot be easily done in another way, e.g. scrubbing in etc.\n3. Control of the fluidity of paint. When working with dilluted paint, it should flow from the hair brush with the desired consistency, in a controlled manner.\nFew tips for choosing a brush.\nFirst determine which technique you are going to work in, meaning what paint are you going to use. The thicker the paint, the harder the hair of the brush should be. For watercolor, the brush from squirrel hair is suitable. For gouache - synthetic. For acrylic - synthetic, Kolinsky brushes, for tempera - synthetic, bristle, badger, oil - bristle, and other synthetics. Of course, your choice should mostly be based on personal experience and preference.\nEach brush has different size and mark-making properties. Typically, the size is denoted by a number indicating the diameter of the ferrule. However, this marking is not always universal, especially if the brush manufacturers are from different countries. Therefore, choosing a brush pay attention not only to the number, but gauge the size ""by eye"" to see if the brush suits your needs.\nSo, now that you have decided on the material and the size of the brush, you should check its quality. Start with the ferrule. It should be evenly and firmly fixed to the handle, not hanging out and not sliping or moving on it. Such a brush will be uncomfortable for work, it will be very difficult to perform the desired shape and stroke with a predetermined pressure. In addition, during painting will the brush will shed hair that are left on your canvas/paper.\nIt is also important to check the elasticity of the brush. Press it against your palm, to see how hard/soft it the hair is, and if the brush retains its shape after applying pressure on it.\nMoisten the brush with water ( wait for a while until the hair is fully saturated) and lightly tap your finger on the handle. A good round brush tip is to sharpen. Try now without departing from the surface to draw Arabic numeral ""eight"" (if there is paper,try on it, if not - on your palm). If the line has a smooth, uniform thickness without breaks and spray, that means you have a good quality brush. Additionally, note the individual hairs in the bundle. The hair should have natural ends and not be cropped.\nCare for your brushes.\nBrushes require careful attitude. Rough handling (e.g., roughly pressing a watercolor brush in the bottom of the water tank) can easily lead to damaged brush, sometimes even making it unusable.\nDo not leave the brush in the water or medium, this will bend the hairs and change the shape of the hair tuft. After finishing work, the brushes should be thoroughly cleaned of paint and put in a container (jar, vase, round box, etc.) for storing brushes, with the hair side up. The accumulation of dry paint on the hair of the beam near the ferrule and inside it, significantly reduces the life of the brushes, so after painting session always thoroughly clean the brush.\nAfter working with watercolor and gouache, rinse the brushes well with lukewarm water, making sure that there is no paint residue left.\nIf you are working in acrylic or oils, keep in mind that poorly washed solvent will accumulate inside the ferrule, and it can dissolve the glue inside, which leads to hair loss and a worthless brush.\nKeep the brush in a dry place protected from moths to avoid damage to the hair, and in order for the hair tuft to retain its shape.\nAll brushes differ, depending on the type of their hair. Here are some examples of the most common hair types.\nNatural fiber brusehs\nBest group of brushes for watercolor. Squirrel hair retains its shape, does not break, perfectly absorbs and releases moisture, and are easy to clean under running water. Squirrel brush allows you to draw smoothly without uneven strokes and overlay blurry spots. This brush is indispensable when working in glazes. The very soft hair allows you to overlay one layer of paint on top of the dried one, without damaging it. However, this brush will not work if you want to leave a strong brushstroke by pressing the paint in the texture of paper/canvas, as squirrel hair is relatively soft.\nThe best squirrel hair is obtained from the Siberian squirrel.\nThose use natural hair pig. • Top quality bristle brushes are those that are made from the hair of Chinese angkingskogo hog. These hair bristles are hard and resilient. It is perfect for use with thick paint (eg, acrylic, oil).\nHowever, if you decide to work with a bristle brush in watercolor, be advised that this hair is hard enough to not only destroy the paint layer (operating glaze), but also damage the basic structure of the paper! So please be careful and cautious.\nBrushes made of synthetic hair\nThanks to the innovation in technologies a unique synthetic fiber has been created - polyester.Special technology allows it to have the necessary form, identical to the hairs from natural origin. Synthetic hair show excellent quality -fluidity, elasticity, rigidity, all of the qualities needed for artist brushes.\nIt is important to note that the synthetic brushes, unlike natural, are of different stiffness (designation: stiffness icons 1 to 4) and the elasticity depending on the purpose. Very hard (4, exceeds the elastic stiffness of the hair bristles), hard (3 is similar to bristles) and average (2, similar in stiffness with column) synthetics superbly suits working with acrylic and oil, and the soft synthetics (1) are suitable for gouache.\nNote that the synthetic hair brushes are readily available for purchase and are much cheaper than their natural counterparts, although still inferior to them in some ways.']"	['<urn:uuid:a3176c52-91ea-4851-8621-5ca4c07cfb8a>', '<urn:uuid:14a2444f-ebfe-4e5d-ae1d-9908c73c42e9>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T04:56:11.774636	12	33	3276
27	how does malware wait to avoid antivirus emulator detection	Malware can evade antivirus detection by adopting a waiting period. When the emulation system records the behavior of the malware to check for bad patterns, it won't detect any suspicious activity because the malware remains dormant during inspection. Since the program appears to do nothing during the emulation period, the antivirus system concludes there is no danger present.	['Cyber attackers use sneaky tricks to evade detection. This post reviews the classes of attacks that bypass detection.\nWe often interact with customers that are a bit puzzled by what we do. They already have a cybersecurity software on their endpoint, an AV and a firewall. Why would they need anything more?\nPreventive vs Detective Security Controls\nWhile AV detects malicious code, it really should be viewed as a preventive measure.\nMost modern AV software will run any unknown code in an emulator to see what it does before letting it run on the system. If the AV finds any malicious pattern in the code, it will block execution rather than let it through.\nAV is much better at finding malware before it runs on the computer than after.\nOnce malware has run, it can hide from the AV. That’s why you often have to unhook the persistence mechanism of a running malware before it can be successfully detected after the fact by AV.\nIsn’t preventing the infection much better than detecting it and having to clean it after the fact?\nWell, yeah. However, that preventive power is not free. The main drawback to preventive measures is that the cost of false positives is much greater.\nThe Cost of False Positives\nHow would you like it if a Windows patch essentially made your computer unusable because your AV thought it was malicious software? Or you couldn’t update to the latest version of a product because your AV thinks it’s malicious?\nMost preventive measures go to great lengths to avoid false positives. As discussed in the post about alert fatigue, the trade-off for less false positives is more missed malicious events. In other words, to avoid blocking legitimate users from doing legitimate stuff, AV lets malware though by design.\nA second layer, one that can be less strict with false positives because it doesn’t proactively block anything, needs to be added.\nLayered Detective Controls\nThe use of layering detective controls to address the risks that cannot be prevented is a tried-and-true approach.\nIn fact, in every heist movie you have ever seen, the item that they have to steal is not only protected by locked doors and biometric access controls (preventive controls), but also by security cameras and alarm systems (detective controls). Nobody in the physical realms thinks they shouldn’t install security cameras because they don’t stop robbers. Nor do they think that the security cameras should have guns mounted on them to “remediate” incidents immediately (although, I’m sure we could find advocates for that one…)\nIn that sense, EDR should be seen as a 2nd layer of detection in the same vein as a security camera.\nModern AV Detection\nNow that we’ve established why AV lets things through, we can look at the techniques AV uses to detect threats.\nThe common point of all of these techniques is that they can be tuned to produce a very small number of false positives. There are three main techniques for detection:\n- Signature-based detection\n- Heuristic-based detection\n- Threat intelligence/reputation engines\nLet’s go over the first two detection techniques and their bypass methods. The third technique will be covered in a future post where we will talk about operational security for malware operators (and how defenders can use it to their advantage).\nThe signature-based detection is how AV used to work. People still think they work this way.\nIn this paradigm, the detection is based on a unique property of the malware. For example, its cryptographic hash (which is unique for each program) or a special string of characters inside the code. This method of detection is very fast and extremely reliable when detecting known samples (given that suitable care is made in generating the detection string).\nUnfortunately, this method is easy to bypass by creating unique variants. In fact, creating variants is so easy that it is the single cause behind the explosion in the malware statistics.\nPolymorphism, as applied to software, is the ability to take many forms. In software, there is usually many paths that lead to the same destination. For example, let’s say you have an inventory of apples and oranges and you want to know to the total number of fruits. You could take a straightforward approach like this:\nFruits = apples + oranges\nHowever, you could also get the same result with the following:\nFruits = 2*apples + oranges – apples\nWhile both of these equations are structurally different, they are functionally the same. That is why, when using polymorphism, you can easily create two distinct programs with the same functionality. Now two signatures are needed for the same malware.\nObfuscation is a subset of polymorphism that, in addition to keeping the same functionality, makes the decoded program harder to read. For a good example of obfuscation in action, refer to the Powershell example in our Emotet blog post.\nHeuristic-based detection is the AV answer to the exploding volume of malware that came with polymorphism.\nIn the general sense, a heuristic is a rule of thumb that is not guaranteed to give the correct answer, but usually gives one that is close enough to the true solution.\nWhat does that have to do with detecting malware? To answer the question, we must make an unfortunate detour in the realm of computational complexity.\nDetection Lessons from Computer Science\nLet’s say you wanted to buy a car and want to make sure you bought the best car available on the market for the best price. You can take into account, car price, fuel economy, reliability, safety, comfort, number of seats compounded by the number of children you are statistically expected to have, etc.\nThe number of combination of criteria can quickly become overwhelming but would be the only method where you could be absolutely certain to get the best car for the price.\nYou could also make a spreadsheet with all the car reviews and the price and easily get the result for the best star to dollar ratio and buy that car. While the choice is not guaranteed to be the best, it would be close enough (heuristic).\nIn computing science, problems are evaluated based on the complexity of the program that can be used to find the exact solution in relation to the possibility space of answers. Some problems are “easy” to solve while other are “hard”.\nWhen you hear that someone cannot crack their encryption solution within the lifetime of the universe with current computing software, you can guess that this is a “hard” problem.\nFinding if two pieces of non-identical code have the same functionality (eg. does this software look a lot like this known malware sample) is one of those “hard” problems. It is not possible to know for sure, hence the use of heuristics to find “pretty good” solutions in a short period of time.\nStrategy 1: Wait it Out\nThe first strategy to beat a heuristic detection is usually to just wait for a while.\nImagine you launch your new accounting software, the AV launches it in an emulator, and then you need to wait for three days for the AV to tell you if it’s safe to run or not.\nThat is obviously ridiculous.\nOne of the most frequently used techniques to evade detection is for malware to adopt a waiting period. Then, when the emulation system recording the behavior of the malware asks itself if it saw any behavior that look like bad patterns yet, the answer will be no.\nIn fact, it seems like the program under inspection didn’t do anything! Clearly, there is no danger present.\nStrategy 2: Alter Behavior\nThe second trick is similar to the first. Malicious software will use tricks to detect that it is under inspection, and alter its behavior to just perform innocuous actions. Then, when the malware is allowed to run on the real machine, it will be free to be its malicious self.\nStrategy 3: Abuse Known Good Software\nFinally, one of the best methods to avoid detection is to abuse known good software that is already present on the system to perform your malicious actions. Your antivirus software will never flag the Powershell program, it is a critical component of the Windows operating system. However, it can be leveraged to perform malicious actions as was demonstrated in previous post about Emotet. This is the so-called living-off-the-land attacks and the explanation for why these attacks are becoming increasingly popular for the attackers.\nWhen Threats Evade AV…CYDEF Protects Your Operation\nPreventive security controls, like AV software, are incredible tools to defend your business. However, much like locked doors, savvy attackers know how to bypass these tools. Additional security controls are needed to address the residual risk.']	['<urn:uuid:122a0f2b-7789-4eb2-96d5-6a4f807ac9f7>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	9	58	1455
28	What are the physical space limitations for multiple fetuses in the uterus, and how do early ultrasound limitations affect the accuracy of twin detection?	Regarding physical space, while there's no strict scientific limit to how many fetuses can fit in a uterus (with 15 being the largest reported number), the main limitation is the total volume and weight of the babies. Each fetus normally has its own amniotic sac with fluid volume about two-thirds that of the fetus, allowing movement like water balloons. As for detection accuracy, ultrasound results are less reliable during the first trimester because one embryo may be overshadowed by another or occupy a tricky position. The embryo's small size in early pregnancy can lead to misinterpretation or missed detection, which is why multiple ultrasound scans throughout pregnancy are recommended for accurate assessment.	"[""A woman gave birth to octuplets in California Monday after 30 weeks of pregnancy. The six boys and two girls ranged in weight from 1 pound, 8 ounces, to 3 pounds, 4 ounces. How many babies can fit inside a pregnant woman?\nThere's no scientific limit, but the largest reported number of fetuses in one womb was 15. In 1971, Dr. Gennaro Montanino of Rome claimed to have removed 15 fetuses from the womb of a 35-year-old woman. The largest number of babies ever delivered was in Australia in 1971, when a 29-year-old women gave birth to nonuplets. They all died. (Decaplets were supposedly born in Brazil in 1946, but that report is unconfirmed.) The largest set of children to be born together and survive—not including this week's octuplets, who are all still alive—is seven. A set of septuplets born in Iowa in 1997 became the first to survive infancy. At least two more sets of septuplets have since been born and survived.\nHow do so many babies fit in one woman? The limit isn't so much the number of babies as their volume and weight. In general, once the total weight of the babies inside reaches about 12 pounds, the uterus goes into labor. The greater the number of fetuses, the earlier the labor will occur. (The rough formula for due dates is to start at 40 weeks of gestation for one baby and subtract three weeks for each additional child: 37 for twins, 34 for triplets, etc. It's remarkable, then, that eight fetuses were able to gestate for 30 weeks, as in the California case.) Labor can also occur if a sac ruptures and introduces bacteria into the uterus or if one of the fetuses isn't growing correctly and signals the uterus to induce labor. (Scientists still don't know exactly what causes labor to start.) But for safety reasons, multiple births are often carried out before labor naturally occurs, and almost always by cesarean section. If the uterus gets too big and the fetuses too heavy, it can be difficult for the woman to breathe. The amount of placental tissue can also cause the mother to develop pre-eclampsia, or pregnancy-related hypertension.\nDon't the babies get tangled up? Usually not. * Normally, each baby is enclosed in its own amniotic sac, which keeps the various umbilical cords and limbs from getting intertwined. Picture a bucketful of water balloons. In each sac, there's plenty of fluid—about two-thirds the volume of the fetus—for the baby to move around in. After the sacs break, the babies can get tangled up during birth—so-called interlocking twins—which is why doctors usually opt for cesarean section. It is possible to have monoamniotic twins, or fetuses that share the same sac, with dangerous results: Cords can get compressed or entangled and cut off the babys' oxygen and food supply.\nGot a question about today's news? Ask the Explainer.\nExplainer thanks Joshua A. Copel of Yale University Medical School and Thomas Moore of University of California, San Diego.\nCorrection, Jan. 30, 2009: This article originally stated that fetuses in multiple births always have their own amniotic sacs. ( Return to the corrected sentence.)"", 'Have you ever come across the term “hidden twin”? It is the surprise discovery of being pregnant with multiple fetuses at a very late stage, sometimes even right on the day of childbirth. Think of it as children starting their game of hide-and-seek in the womb even before they are born! It often comes as a great surprise to parents who had undergone tests and ultrasound and still the presence of the twin fetus remained undetected.\nAdvances in ultrasound and related technologies have made the phenomenon of a twin fetus remaining undetected a rare one. However, every once in a while, the image obtained from the ultrasound might be misleading or subject to misinterpretation. This can especially be the case during the first two months of pregnancy. However, the reasons for not being able to detect a second baby are not limited to just these. Let’s dive in and get to know about the curious case of “hidden twins’ in greater depth:\nDuring the screenings in the first trimester of pregnancy, the presence of twins may be hard to detect. What happens here is that one of the embryos gets overshadowed, right behind the visible one, unseen to the ultrasound scanner. It is also sometimes described as one child being “eclipsed” by the other. Here are a few reasons why the presence of the twin child might go undetected:\n- The Embryos Occupy A Single Amniotic Sac:\nEmbryos are surrounded by a fluid cover known as an amniotic sac. In the case of normal twin babies, each of the embryos will have separate amniotic sacs from where they meet their nutrition needs. However, in certain cases, both the embryos end up in just one such sac. This is not an ideal situation and in fact, deemed dangerous for the survival of the babies (1). A single amniotic sac also makes it difficult to detect the presence of a second baby.\n- Ultrasound Done In Early Days Of Pregnancy\nUltrasound results would be less accurate when performed during the first trimester of pregnancy. As such, if you’re ruling out the possibility of having a twin inside you with ultrasound reports of the initial days you might stand corrected. Better to have follow-up ultrasound tests during the mid and later months of pregnancy as well.\n- Human Error\nIt’s totally possible that the lab technician misread or misinterpreted the pictures of the test. Medical professionals are human after all and once in a while, it’s possible that they fail to notice certain small aberrations. But having said that, it would be a rare case for such errors to occur multiple times.\n- Tricky Position Occupied By The Embryo\nIn certain rare cases, nature plays its own tricks by placing the embryo in a position that is sneaky and hard to detect. The twin may lie behind the sibling perfectly covered and undetected. This can be a very effective trick in evading scans especially in the early stages of pregnancy.\nUltrasounds are great, but they aren’t without imperfections. An early scan can prevent you from hearing your baby’s heartbeat. But by the time you get to the third trimester, it is pretty much impossible for a developing baby to remain hidden.\nThe doctor commonly uses a handheld fetal doppler on the expectant mother’s belly to detect the baby’s heartbeat. This procedure is called abdominal ultrasonography (2). One can hear their baby’s heartbeat for the first time during the first prenatal check-up. It is usually scheduled between the 12th and 14th week of pregnancy, or maybe later in some cases (3). In most women, a combination of several methods could be needed to accurately assess the baby’s heartbeat instead of a single process (4). However, a fetal heartbeat also stands the odds of being inaudible. It is because fetal heart monitors can fail to observe fetal heart position accurately (5) .\n3. Can A Twin Hide Out In Your Uterus?\nThe 20-week ultrasound will check for multiple births and congenital disabilities in your baby (6). Generally speaking, the chances of a full-grown baby hiding in your uterus forever is pretty slim. A scan after 20 weeks will give a much more accurate result and confirm the presence of the twin if present.\n4. What Are The Common Signs Of Twin Pregnancy?\nJust in case an ultrasound fails to detect a twin in your womb, here are a few other signs to keep a watch on that suggest you could have a twin pregnancy:\n- Heavy Morning Sickness Or Nausea: Hyperemesis Gravidarum is more common during twin pregnancies. Severe dehydration, fatigue, and poor weight gain could last for months (7).\n- High Blood Pressure: If you are carrying multiples, you will be at a greater risk of developing high blood pressure-induced complications or disorders (6)..\n- Gestational Diabetes: Moms with twin pregnancies are at a greater risk of suffering from increased blood sugar levels than singleton pregnancies (8).\n- Large Fundal Height: When the baby measures bigger than expected for its gestational age, it could indicate a twin pregnancy (9) .\n- Higher levels of hCG: During pregnancy, the level of the hormone Human Chorionic Gonadotropin (hCG) typically increases. This is helpful in stopping menstruation during pregnancy, protecting the uterine linings. The level of hCG is usually seen to be higher in the case of twin pregnancy (10). Once you get your hCG levels tested, consult your doctor and see if it falls within normal limits or whether it is higher than normal.\nAs seen from above, the chance of twin fetuses going unnoticed by medical experts is a rare occurrence. Ultrasound is pretty effective at accurately detecting the presence of twins. It’s usually the early weeks of pregnancy when the size of the embryo is too small, that the presence of a twin child is missed. As such, the logical action would be to undergo ultrasound scans more than once, spread across different periods of the pregnancy. As you near your delivery date and go in for more scans, all your confusion will disappear. Are you concerned that you may be carrying undetected fetuses? Speak to your doctor and verify your tests with an experienced medical practitioner before you come to any conclusion. Do tell us if you had a similar experience or know of someone who shares a similar story!\n- Monoamniotic Twins (Sharing The Same Amniotic Sac): Do They Do Better If Born Early?\n- Abdominal Ultrasound\n- Pregnancy Week By Week\n- Assessment Of The Fetal Heart At 12-14 Weeks Of Pregnancy Using B-mode Color Doppler And Spatiotemporal Image Correlation Via Abdominal And Vaginal Ultrasonography\n- Paris: Passive And Continuous Fetal Heart Monitoring System\n- Twin Pregnancy: What Twins Or Multiples Mean For Mom\n- Nausea and Vomiting of Pregnancy and Hyperemesis gravidarum\n- Gestational Diabetes And Fetal Growth In Twin Compared With Singleton Pregnancies\n- What\\’s The Significance Of A Fundal Height Measurement?\n- The Thyrotrophic Role Of Human Chorionic Gonadotrophin (Hcg) In The Early Stages Of Twin (Versus Single) Pregnancies']"	['<urn:uuid:f7222aec-848e-4877-a3b9-9ad5be62c258>', '<urn:uuid:644c52a7-07fb-43ea-95c6-fc9d0cbab541>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:56:11.774636	24	112	1696
29	What are autism's key symptoms and severity levels?	The core symptoms of autism include repetitive behaviors (like hand-flapping and rocking), extreme sensitivity to stimuli, and persistent deficits in social communication. These symptoms exist on a spectrum requiring different support levels - from 'very substantial support' for those with little verbal language to 'support' for those who can speak but struggle with conversations.	['Researchers have found that some baby sibs have distinctive brain-wave patterns and a loss of interest in faces beginning as early as 6 months of age. The new study uncovers changes in brain development even earlier.\nCan you detect autism in newborns?\nAlthough autism is hard to diagnose before 24 months, symptoms often surface between 12 and 18 months. If signs are detected by 18 months of age, intensive treatment may help to rewire the brain and reverse the symptoms.\nCan you tell if a baby has autism at 2 months?\nEarly signs of autism or other developmental delays include the following: 2 months: Doesn’t respond to loud sounds, watch things as they move, smile at people, or bring hands to mouth. Can’t hold head up when pushing up while on tummy.\nHow early do babies show signs of autism?\nThe behavioral symptoms of autism spectrum disorder (ASD) often appear early in development. Many children show symptoms of autism by 12 months to 18 months of age or earlier.\nWhat are the 3 main symptoms of autism?\nPatterns of Behavior\n- Repetitive behaviors like hand-flapping, rocking, jumping, or twirling.\n- Constant moving (pacing) and “hyper” behavior.\n- Fixations on certain activities or objects.\n- Specific routines or rituals (and getting upset when a routine is changed, even slightly)\n- Extreme sensitivity to touch, light, and sound.\n6 дек. 2020 г.\nDo autistic infants smile?\nSmiling frequency also increased with age, but by 12 months the infants with autism smiled less often than the other children in the study. At 18 months, the babies later diagnosed with autism continued to smile less than the other baby sibs.\nDo autistic babies sleep more?\nChildren with autism are more likely than typical children to have had problems falling asleep as infants, according to a new study1. These infants also have more growth in the hippocampus, the brain’s memory hub, from age 6 to 24 months.\nWhat are autistic newborns like?\n“Kids with autism like looking at moving things in front of their eyes, such as flapping hands or following contours.” At 7 to 10 months, babies later diagnosed with autism also showed a decline in their thinking and motor skills, perhaps as a consequence of the very early sensory and motor signs.\nDo autistic babies watch TV?\nBabies who watched television or video screens when they were 12 months old showed more autism-like symptoms when they reached age 2, a prospective study showed.\nAt what age is hand flapping a concern?\nIf the child grows out of these behaviors, generally around 3 years of age, then it is not much worrisome. But if a child hand flaps everyday then there is cause for concern. This is an example of self–stimulation.\nCan autism go away?\nThere is no cure for autism, but early intervention using skills-training and behavior modification can yield excellent results. This type of educational and behavioral treatment tackles autism symptoms — impaired social interaction, communication problems, and repetitive behaviors.\nCan you be slightly autistic?\nNo, there is no such thing as being a little autistic. Many people may show some characteristics of autism from time to time. This may include avoiding bright lights and noises, preferring to be alone and being rigid about rules. This does not make them autistic.\nCan autism develop later in life?\nThe consensus is no, autism cannot develop in adolescence or adulthood. It is, however, common for autism to be missed among girls and people with high-functioning autism when they are young. Because they aren’t accurately diagnosed, it might lead to some people believing they developed autism as they matured.', 'Autism Spectrum Disorder (ASD) is a broad term used to categorize a group of neurodevelopmental disorders that are present from early childhood and affect many aspects of day-to-day functioning. The core features of ASD include\n- Persistent deficits in social communication and social interaction\n- Restrictive and repetitive patterns of behavior, interests or activities\nThese symptoms exist on a spectrum, and are diagnosed based on severity and intensity of support required (very substantial support, substantial support, and support). Currently, ASD is estimated to occur in approximately 1 in 68 individuals. ASD disproportionately affects boys; however, the expression of the disorder is often more severe in girls. Despite advances indicating a genetic component, the causes of ASD remain largely unknown at this time.\nWhat is the role of the word spectrum in ASD?\nThere is a saying that if you’ve met one individual with ASD, you’ve only met one individual with ASD. This is because each individual diagnosed with ASD may present differently and require a different level of intervention.\nIn the category of persistent deficits in social communication and interaction, some individuals with ASD “requiring very substantial support” may have little or no verbal language. These individuals rarely initiate social interaction, and may not respond to social overtures from others. On the other side of the spectrum, individuals in need of “support” may be able to speak in full sentences, but struggle to engage in reciprocal conversations or to initiate and maintain friendships.\nIn terms of patterns of behavior, individuals requiring “very substantial support” may engage in self-stimulatory behavior (i.e., hand flapping) that severely limit functioning throughout the day. Behavior that exists on the other end of this spectrum may include difficulty transitioning between activities, as well as organizing and planning.\nOverall, it is important to remember that the behaviors in each domain that make up ASD may exist with varying severity and impact on day to day functioning.\nWhat are the most effective and evidence based treatments for ASD?\nRegardless of the severity of ASD symptoms, early and intensive intervention is universally considered to be best practice in treatment. Research on ASD intervention is primarily focused in two areas: Applied Behavior Analysis (ABA) and Developmental Social Pragmatic (DSP).\n- ABA interventions are based primarily on the concept that difficulties associated with ASD can be addressed through the identification and reinforcement of specific target behaviors\n- DSP interventions are focused on improving the ability to jointly engage in activities with adults and peers.\nThe following is a more in depth look at evidence-based interventions for the treatment of ASD\n- The most notable treatment for ASD is Early Intensive Behavioral Intervention (EIBI). EIBI consists of 20-40 hours per week of treatment for 2-3 years. As previously indicated, earlier intervention is critical, and EIBI is recommended prior to 5 years of age. Comprehensive ABA programs are adult led, and focused on all areas of functional deficit. Most comprehensive programs begin with an assessment of age appropriate behavior milestones, which are then addressed by an individualized curriculum. Depending on level of support needed, therapy may take the form of Discrete Trial Training (DTT) or Natural Environment Teaching (NET). DTT is a highly structured teaching method in which skills are broken down and individually taught. Each trial is typically reinforced with a tangible reinforce (i.e., candy, toy). NET is primarily delivered in the natural environment, capitalizes on opportunities for incidental learning, and makes use of natural reinforcers in addition to tangibles.\n- Pivotal Response Training (PRT) – PRT is based on the principles of ABA, and primarily targets “pivotal” responses rather than individual target behaviors. Pivotal responses include: motivation, response to multiple stimuli, social interaction, as well as self-management. The overarching philosophy is that improvement of these pivotal responses will lead to improvement in a variety of other functional behaviors. As PRT is play based and child initiated, this intervention may be more effective for children who possess already established behaviors including: increased social initiation and toy play.\n- Picture Exchange Communication System (PECS) – PECS is an augmentative/alternative communication (AAC) intervention based on the principles of ABA. PECS encourages the initiation of communication. That is, the child must initiate a request by presenting a picture of a desired item to a communicative partner who then fulfills the request. As the child masters various stages, the communicative repertoire is expanded to include sentences, attributes, commenting, as well as the answering questions. PECS and other AAC devices are primarily used with minimally verbal children.\n- Early Start Denver Model (ESDM) – ESDM is a blend of both ABA and DSP approaches to treatment. ESDM integrates elements of a relationship-based approach with behaviorally based teaching strategies. Specifically, a therapist or parent is instructed to match their child’s affect and allow the child to lead the activity. The program is based on a developmental curriculum and outlines the skills that need to be taught at any given time. ESDM has been shown to be effective across a wide range of abilities.\nYou can click to learn more about our Applied Behavior Analysis services.\nIn addition to these treatments, there are a variety of treatments that are currently being thoroughly investigated. These interventions include various parent training techniques, DIR floortime, comprehensive classroom based approaches, as well as PCIT. It is important to keep an eye on the literature, as treatments are constantly evolving, and new randomized controlled studies are being completed.\nThe following article was utilized in the development of the above:\nTristram Smith & Suzannah Iadarola (2015) Evidence Base Update for Autism Spectrum Disorder, Journal of Clinical Child & Adolescent Psychology, 44:6, 897-922\nAs the variety of intervention strategies for ASD has increased significantly over time, this article was chosen, as it is the most recent evaluation of what is currently offered in the field. The future directions for research stated by the authors place value on treatments that are emerging as evidence based, supporting the notion that any review of evidence based treatment must be as recent as possible.\nWritten by Ariel Kornblum, PsyD, BCBA, LBA']	['<urn:uuid:eaddea7f-e770-4c4d-baeb-4279f47ee43e>', '<urn:uuid:c430fd78-133a-47f8-8ded-888fbe4029e9>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T04:56:11.774636	8	54	1615
30	traveler with medical background comparing health risks cusco titicaca la paz elevation differences disease prevention measures	The major cities have significant elevation differences: Cusco is at 11,152 feet, Lake Titicaca/Puno at 12,549 feet, and La Paz at 13,250 feet. Health risks at these elevations include altitude sickness due to lower oxygen levels. Disease prevention measures include getting yellow fever vaccination (required in some regions), malaria prophylaxis in tropical areas, and hepatitis A/B immunizations. Additional precautions include using purified water, ensuring food is thoroughly cooked, and protecting against mosquito-borne diseases like dengue fever.	['Cusco seen from Sacsayhuaman fortress. Photo by Martin St Amant.\nWhy visit Cusco? (aka Cuzco)\nCusco, previously capital of the Inca empire is now capital of Peruvian tourism, a UNESCO World Heritage Site and primary destination for most tourists in the country, getting close to a million visitors a year. The official Quechua (Andean Indian language) name for Cusco is now Qosquo.\nAt an altitude of 3, 400m (11, 200ft) Cuzco (as the name is also spelled) is a chilly but charming town, half Inca, half Spanish. Built in the shape of a Puma with the hilltop fortress of Sacsayhuaman as the head, Cusco is one of the world’s prettiest towns and certainly Peru’s.\nWhite walls, three storey colonnaded buildings with terracotta tiled roofs and lots of woodwork make up the private sector, while the top section of many public buildings is Spanish colonial but the lower half is solid Inca stonework.\nIn one case there is a stone that is estimated to weigh 30 tons, has twelve sides, and fits perfectly into the middle of a wall of similar stones. Or was it 12 tons and 30 sides? Whatever, cement was never used and it is still impossible to get a knife blade into the cracks between the stones.\nPlaza de Armas, the heart of Centro Historico de Cusco, ringed by colonnaded walkways, magnificent architecture and several competing churches. Photo by Martin Lang.\nFood in Cusco is passable, especially if you are fond of Italian cooking as most of restaurants seem to be pizzerias or bistros. Pisco Sour is the traditional tourist drink and very tasty it is too, though usually made with a raw egg so beware if you have a delicate stomach.\nHotels range from quite expensive to very cheap while celebrations both big and small regularly offer bizarre spectacles.\nThe bugcrew stumbled into a firework show where men were holding up bamboo frames shaped – among other things – like ships and planes, loaded with rockets, which they fired at each other from a range of two or three metres. The encore was to put on paper mache bulls heads with variously fused rockets in place of horns and lunge repeatedly at the crowd, firing rockets at random. Cue for much shrieking and laughing and smell of burning hair.\nCuzco is not an good place to arrive at by plane direct from sea-level Lima, particularly for older, less fit travellers who may not enjoy altitude sickness.\nTry to arrange a gradual ascent such as taking a land route via Nazca, Arequipa (2, 400m) and possibly Lake Titicaca (3, 800m) if time permits. By the time you arrive in Cusco you will then be fitter, more comfortable in your environment and well acclimatised, ready to enjoy the holiday of a lifetime.\nNote that Cuzco has few attractions aimed at kids.\nPlaza de Armas and a closer look at the colonnades and Jesus’ Company Church. Photo by Jorge Lascar.\nCusco’s main attractions\nn. b. No photos are allowed in Cusco churches, even if you have paid to enter. The authorities mistakenly believe you will buy shoddy old, low-rez postcards if you can’t take your own pictures.\n• Sacsayhuaman, a huge Inca fortress on the hill above the town with wonderful views and amazing stonework.\n• Plaza de Armas (Huacaypata): Hang out in the graceful, calm and colonnaded central square, get your shoes shined, see off the hawkers, book tours, chat to other travelers, have a drink, a meal, people-watch, run into a festive procession, visit the churches and so on. This is the heart of Cusco. The Inti Raymi festival takes place here in June.\n• Cusco Cathedral, otherwise known as the Cathedral Basilica of the Assumption of the Virgin. Built in 1654 it’s situated in Plaza de Armas, on the left in the photo above. Unlike most churches in the rest of the world tourists have to pay to enter but the interior is magnificent, loaded with gold (stolen from the Incas), fine woodwork and encompasses 13 varied chapels. A guide is useful. The cathedral is connected to the first Christian church built in Cusco, Iglesia del Triunfo.\nSanto Domingo Convent\nSanto Domingo Convent built over the Inca Sun Temple, Coricancha. Photo by David Stanley.\n• Santo Domingo Convent, Qoriqancha/Coricancha. A classic case of an attractive Spanish church built over the ‘Golden Courtyard’ Inca temple (walls paneled with gold before the conquistadors arrived), now simply loaded with superb masonry.\nStep One: enter a lovely 17thC church through a Moorish doorway.\nStep Two: enter the cloisters and find an Inca temple, mostly revealed after an earthquake! Brilliant.\n• Iglesia de Santo Domingo, Plaza de Armas, dazzling displays of artifacts such as a solid silver altar, fine wood carvings and a Peruvian version of ‘The Last Supper’. Baroque and Soul! No photos allowed tho’.\n• Iglesia de la Compania de Jesus (Church of the Society of Jesus), Plaza de Armas, lots of classy fixtures and fittings and and wonderful views from the front towers.\nA 12-sided stone slotted into a Cusco wall with no gaps and no mortar. How?\nSpanish conquistadors arrived in Cusco in 1533, destroying many Inca structures but using Inca stone bases for adobe Spanish buildings that still stand today.\n• Wandering the streets to stumble upon stuff like The Twelve Sided Stone. An amazing bit of showmanship, this large and superbly cut is embedded in a wall in Calle Hatum Rumiyoc.\n• Barrio San Blas is the arty-farty neighbourhood uphill and north of Plaza de Armas, writhing with artists-manqué, neo-hippies, temporary settled backpackers. A great place to pick up unique souvenirs, eat alternative, smoke alternative and rhapsodise with bohemians.\n• Inca Museum, Unsaac Museo Inca, displays a fantastic collection of items and information from pre-Inca civilisations through to the Spanish conquest, including the largest trove of Inca objects in the world. All this is located in one of Cusco’s best, half-Inca, half-Spanish Colonial buildings. A bit crude in places but essential for anyone interested in the Inca.\nYou may consider paying a modest sum for one of the private tour guides standing outside Coricancha or similar locations, they are well worth the money and can explain many interesting facts about Inca construction methods such as how they made their structures earthquake-proof, how they fitted the stones together without cement, about the traditional ceremonies that happened in these temples and so on.\nDon’t imagine you’ll be avoiding the ubiquitous llamas just because you’re in an urban area. Actually the disgruntled one on the left (not the one with the hat) is an Alpaca. Of course this kind of photo costs a little, but be sure to negotiate beforehand to avoid absurd demands!\nThe Inti Raymi, ‘Festival of the Sun’, is a dazzling religious ceremony held in Sacsayhuaman in honour of the Inca sun god, Inti, marking the summer solstice. With a week of festivities, the actual day of Inti Raymi is June 24.\nCusco’s best out-of-town attractions\n• Machu Picchu. The biggest Wow factor ever. Take a train to Aguas Calientes + bus and preferably stay at least one night, or even better walk for three days over the mountains on the camino.\n• Inca Trail, a totally awesome 3/4 day hike through wonderfully diverse and beautiful scenery on a kind-of garden path. Best walk ever, but hard, mainly due to the altitude of up to 4, 200m.\n• The Sacred Valley of the Incas (Urubamba Valley) near Cusco and stretching from Pisac to Ollantaytambo:\n• Pisac (Sacred Valley)\n• Ollantaytambo. 60 kms NW of Cusco, loaded with Inca history and structures. Also near the start of the Inca Trail.\n• Tipon. About 25kms and an hour bus ride from Cusco plus 45 minutes walk uphill, so take a taxi if you’re feeling flush! This ruined Inca retreat for royalty (possibly) offers excellent views, fountains, terraces and irrigation systems.\nApril-October is the dry season with warm, sunny days, highs around 20C/68F but chilly, near zero nights, it is winter after all!\nNovember-March is the wet season, with especially heavy rain December-February, daily highs also around 20C but warmer nights around 6C/42F.\nNote that the 3, 400m elevation here not only offers the average tourist flying in from Lima the chance to experience altitude sickness but also to freeze at night and burn during the daytime. UV measurements in 2006 indicated that Cusco received the highest Ultraviolet light on Earth.\nOur advice is\na) avoid sunstroke by wearing a hat during the middle of the day at least.\nb) avoid altitude problems by traveling by road from Lima up to mid-altitude Arequipa via Nazca, then on to high, cold and less interesting Puno (3, 800m) on the edge of Lake Titicaca, before catching the train to Cusco (lower and warmer).\nc) bringing some compact but seriously warm clothing, whatever the season in Peru.\nGetting to Cusco\n• Fly in via Alejandro Velasco Astete International Airport, but really beware altitude sickness, from Lima at sea level to Cusco’s 3, 400 metres (11, 200 ft) in one hour! You may well be in for several days of extreme unpleasantness, migraine/nausea/lighthead/weakness which could disrupt holiday plans.\n• Train (recommended) from/to Puno, but only three times a week so book ahead. This popular but expensive 12 hour train journey crosses the altiplano with foreigners locked in their own railcar by guards, past dozens of white Andean peaks and hundreds of brown llamas.\nAn excellent alternative is the first class bus ride with Cruz del Sur listed below – fast, comfortable, entertaining and one fifth of the train cost!\n• Buses to/from La Paz, Bolivia, ca n be difficult, especially since guide book advice and bus company websites seem to be always out of date and various scams are perpetrated such as selling bed-seats that don’t exist.\n• Buses from Lima take a minimum of 21 hours to reach Cusco on very hairy, windy, partially unmade roads.\n• Car Hire/Self drive is not recommended for Andean roads as they are narrow, endlessly serpentine and spectacularly dangerous.', 'Health Tips for Travel in South America\nSouth America presents unique health challenges to travelers from North America and Europe. In addition to unfamiliar diseases travelers also have to deal with a number of other health concerns such as food and water safety, different climate conditions and altitude sickness. While it is easy to become overly concerned about your health when traveling in South America, it is accurate information and not exaggerated precautions that will help you stay healthy on your trip.\nPrepare Before You Go\nThe more you learn about the health risks at your destination, the better you will be prepared to stay healthy. Find out about illnesses and health threats in the South American countries you plan to visit. Most health risks for travelers come from a number of diseases we are unfamiliar with and therefore have no experience of how to prevent them.\nBring an updated international immunization record that also lists health conditions and allergies, which can help you get the right medical treatment, if necessary. If you take prescription medication, bring a supply large enough to last you for your trip. If you wear glasses or contact lenses, bring a spare pair, or write down the prescription to be able to get a replacement easily. You should also consider bringing your favorite brands of non-prescription medications, since you might not be able to find them in South America.\nThe best way to prepare medically for a trip is to arrive in good health. Seeing a doctor in a foreign country can be a challenge because of the language barrier and the uncertainty that comes with seeking medical help abroad. See a doctor or a visit a travel clinic at least one month before your departure in order to determine what boosters or immunizations you will need.\nCommon Health Problems\nIt usually takes some time for your body to adapt to the different bacteria in food and initial digestive problems such as diarrhea or constipation are fairly common. These symptoms should go away after a few days. Infections caused by insect bites, small cuts and wounds are also quite common. It is best to treat such small injuries with much more care than you would at home. If your wound does get infected most are easily treated with antibiotics. Below I have listed the most common diseases travelers should beware of in South America:\nTraveler’s Diarrhea is common among foreigners who are just getting used to the different bacteria contained in food. However travelers’ diarrhea usually subsides by itself after a few days. If the condition persists longer than a few days or worsens, you should see a doctor.\nMalaria is a parasitic disease that affects the liver. It is transmitted by the bite of the infected female anopheles mosquito. For short-term stays in areas with a malaria threat it is recommended that you take prophylactic drugs. It is best to bring a supply large enough to last for the period of potential exposure. But keep in mind that malaria is only present in tropical South America and is very rare in urban areas. The best malaria protection is to prevent mosquito bites. Use mosquito netting and wear long-sleeve clothing in the evenings and early mornings, in addition to mosquito repellent.\nDengue Fever is a viral disease, common in tropical areas across South America, and it is transmitted by the Aedes aegypti mosquito. Dengue occurs most commonly in urban areas, especially during the rainy season. Since there is no vaccine, the only way to lower your risk is to avoid being bitten by mosquitoes during the day.\nYellow Fever is a viral disease that can cause liver problems. It is transmitted by mosquitoes and occurs mainly in the Amazon region. Since a vaccine is available, you should not take any risks and get an immunization. You will be issued an international certificate of vaccination, which is valid for 10 years. Make sure you take it with you. Free yellow fever immunization is available in Brazil at local vaccination posts throughout the Amazon, at vaccination posts along bus routes to the Amazon, and at airports with flights to the Amazon region. Some countries require a Yellow Fever vaccination when entering from a neighboring country where Yellow Fever is know to frequently occur.\nChagas Disease is a serious disease present in most of South America. It is caused by a parasite transmitted by contact with the feces of a reduviid beetle (also known as kissing bug), a brown oval-shaped beetle. The beetle occurs mostly in rural areas. Bed netting can help prevent infection.\nRabies is a viral disease transmitted by bites from infected animals. Rabies cases from dog bites are decreasing in South America, but there is growing rabies risk from the bites of vampire bats. If you plan on staying in remote rural areas you might want to consider getting a pre-exposure rabies vaccination.\nHepatitis is a viral disease that affects the liver. Hepatitis A is transmitted by consuming contaminated food and water, or by person-to person contact. Be especially aware of unsanitary conditions, where the transmission of the disease is much more likely. An active vaccine is now available. Hepatitis B is transmitted by bodily fluids, and an immunization is also available, Hepatitis C, which is transmitted by blood and through sexual contact, is the most serious type of hepatitis. It is often only diagnosed in the advanced stage, when it causes liver cancer and cirrhosis.\nTuberculosis is a bacterial infection that most frequently affects the lungs. It is transmitted through the air by coughing, or through unpasteurized milk or milk products. Tuberculosis is on the rise worldwide and treatment-resistant strains have recently emerged. If you suspect that you might have been exposed you should get a tuberculin skin test to determine if you have been infected.\nTyphoid Fever is a serious bacterial disease transmitted through contaminated food and water. Although typhoid vaccination is not completely effective, you should add it to your immunization list, especially if you intend to travel in rural and remote areas.\nAIDS is caused by the HIV virus and is transmitted through blood and sexual intercourse. It is estimated that nearly two million people in South America are currently infected with HIV. The highest number of HIV infections is among men who have sex with other men, but heterosexual women are also increasingly affected. Female sex workers also have a higher risk of contracting the virus, and since prostitution and sex tourism are present in South America, foreign travelers should take special precautions.\nWhile this list of common diseases may sound discouraging, keep in mind that most visitors to South America have few health problems. If you take common precautions and are aware of the most common health risks, you will most likely not have any serious health problems.\nSanitary conditions vary drastically from country to country, but it is a good idea to be concerned about water and food safety throughout South America. Numerous diseases and germs are food-borne, and it is important to adapt your sanitary practices to the tropics where germs are more abundant. Wash your hands often and always before you eat. Hygiene standards at restaurants vary a lot, and a more expensive restaurant is not automatically a guarantee for food safety. Pick a restaurant with a high turnover, which is the best assurance that the food is safe. You should be especially careful with street vendors and roadside food stalls. If you eat raw fruits or vegetables, make sure they are peeled or washed in purified water. Be careful with seafood, especially shellfish, and with lightly cooked meat dishes. Food should be cooked thoroughly and served hot. Avoid unpasteurized dairy products.\nThe most common health problem I have had in South America is food poisoning. At one time in La Paz, Bolivia, I was in bed for a week, and I was ready to call a doctor when my condition finally improved. It is mostly in rural areas, where sanitation is not always up to standard, and the chances of catching a food-borne disease are greater. If you drink unfiltered water in rural areas you should get tested for intestinal parasites after you return home. You might want to bring water purification tablets, or a portable water filter, in case you travel to areas where no purified water is available. Tap water in cities and towns is usually treated, but you should avoid drinking unfiltered tap water. Avoid ice cubes as well, unless you know that they are made from purified water.\nThere are also several environmental factors that can impact travelers’ health and it is important to correctly assess your own health. Are you sensitive to intense sunlight, humidity, or certain allergens? Does a rapid change in altitude affect you? You should keep these factors in mind when planning a trip to South America.\nIf you arrive in tropical South America from a temperate climate zone, be prepared to make adjustments to the heat. Seasons are reversed in the southern hemisphere, which intensives the effect of the heat and sun, if you travel there during the northern winter. Give your body time to adjust to the different climate, and drink a lot of water. Use sunglasses, sunscreen, as well as a hat to protect yourself from the effects of the tropical sun. It is best to use a sunscreen lotion with sun protection factor 30 or above. Remember to drink a lot of fluids.\nMany low-lying areas of tropical South America are very humid, which also takes some time to get used to. Let your body gradually adjust to the humidity and stay in a hotel with air conditioning. A sudden change in altitude from the lowlands up to the Andean high plateau often causes altitude sickness, due to the lower oxygen levels in high-altitude air. Although prescription drugs for altitude sickness are available, I have found that slowly traveling up the Andes works equally well, if you have extra time. If possible choose a destination halfway up the mountains and spend a few days there before ascending to your final destination. Coca tea, widely available and legal in Peru and Bolivia, is said to reduce the effects of altitude sickness.\nTo give you an idea of the elevation gain, if you arrive from either the Pacific coast (Lima, Peru’s capital is at sea level) or the Amazon basin (average elevation: 300 ft.) I have listed several destinations in the Andes and their altitude:\n- Cuzco (Peru):11,152 feet.\n- Arequipa (Peru):7,550 ft.\n- Puno (Peru, Lake Titicaca): 12,549 ft.\n- Copacabana (Lake Titicaca, Bolivia): 12,500 ft.\n- La Paz (Bolivia): 13,250 ft.\n- Cochabamba (Bolivia): 8,432 ft.\n- Quito (Ecuador): 9,350 ft.\nGetting Medical Help Abroad\nIf you develop a minor health problem keep in mind that across South America you can buy many prescription medications over the counter, and pharmacists often give health advice. If you decide to see a doctor, it is worth finding out about the local public health system. South American countries have a basic public health system where the locals as well as foreigners can see a doctor or nurse at a very low cost. If you prefer a private doctor or hospital, your consulate should be able to make recommendations, or you could ask at a pharmacy, your hotel, or the tourist information. It is a good idea to learn a few phrases in the local language about your health condition beforehand so you can tell the doctor about your problem. Payment is usually expected at the time of the consultation, even if doctor’s visits are included in your health insurance at home or your travel health insurance. Make sure you get an invoice or proof of payment listing the health condition and treatment, so you can be reimbursed after your return home.\nTo learn more about health concerns in South America, please visit our Health, Safety, and Insurance for Travelers and Expatriates Abroad section.']	['<urn:uuid:64b70db5-fdc7-4c4a-aeee-e54a16714ee1>', '<urn:uuid:86794cce-8158-44c1-94b2-0ad74a4d443b>']	factoid	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-13T04:56:11.774636	16	76	3671
31	need information about quality control what are the main concerns about impurities in drug development process	Impurities are the biggest concern regarding starting material choice for Active Pharmaceutical Ingredients (APIs). The key is to identify and clarify the points where contaminants can be purged. This involves creating a flow diagram of the material, molecule, and process to identify purification points, which could include extractions, crystallization, or chromatography. Impurities can range from metals to rogue solvents like benzene and chloroform. To control these, companies need to establish a framework for controlling quality and keeping contaminants below the threshold, which can be done through purge studies and spike tests. The data from these studies helps set specifications and convince regulatory authorities about quality control.	"[""There are many ways for companies to get approvals for expedited development. This is primarily based on trials and data from clinical studies. The biggest issue has always been how acceleration impacts CMC. In many cases, the data and the programs need time and cannot be expedited. So how do you ensure CMC doesn't become the bottleneck?\nTransitioning chemistry: Regulatory starting materials is the point where change control starts. This is also the point where GMP is introduced into the synthesis of a drug substance. Other changes before that cannot affect API or ultimate drug product.\nThe ICHQ guidelines: The International Conference on Harmonization compiled the informative ICHQ11. This guideline is a globally agreed framework for facilitating CMC changes efficiently and predictably in drug development.\nCompanies needing guidance can find a lot more information here on establishing drug substance process controls. Depending on specific circumstances, you may find that the ICHQ11 provides a little flexibility in selecting starting materials. The FDA adopts these guidelines.\nBased on ICHQ11, sponsors can develop a process understanding of future changes in synthesis and starting materials methods. It offers clarity on all the data required not just for approval but also for substance quality and patient safety.\nRegulators expectations: Understanding regulator expectations ensures that your starting material will have the requisite technical and quality elements to allow for commercialization when you submit NDA and BLA. The information in this application is what you would have accrued over time throughout development.\nBegin with the end in mind: Start thinking about controls as early as phase two for consistency and verifiable quality and purity levels. Everything up to the point of the regulatory starting material must be subjected to GMP.\nDo you have a perfect understanding of the medication that will be served to patients? This is the question that sums up all regulator concerns, questions, and expectations. Start here and compile all data needed to support your synthetic routes.\nApply Good Manufacturing Practices: GMPs are mandatory from the starting material forward in API. Generally, there is little oversight in starting materials, but regulators expect GMPs to extend further back. With a longer synthetic chain, there are fewer chances of impurities in the final Active pharmaceutical ingredient (API). Minimize switching or choosing new suppliers of starting materials for a consistent level of quality\nGather more data: It takes considerable time to develop a package to support a regulatory starting material proposal. The right package should be backed by extensive data. Detail the process, the points where impurities purge, crystallization points, and other elements that address regulator expectations for staring materials.\nUnderstand FDA vs. EMA on the matter: The FDA is flexible in applying ICH guidelines on a case-by-case basis. In Europe, these are more like rules. It is crucial to figure out how you'd fit this approach. For instance, rules will be different for commercially available materials as intermediate versus those that are available as a commodity.\nThe big question of impurities: Impurities are the biggest concern that underlies issues raised regarding starting material choice for APIS. Find and clarify the points where the contaminants can be purged. Start with a flow diagram of the material, the molecule, and the process and identify purification points, which could be extractions, crystallization, or chromatography.\nSpecificity helps: The more detailed you are about these materials purge, the better. If you do spike and fate tests and purge studies at that point and track progress throughout purification, that will strengthen your arguments for LBA and NDA. This downstream control process improves your understanding of what kind of controls you need to have on that proposed material and its quality. Downstream.\nPerfect upstream process control: Find out what controls the purity of the proposed starting materials. Impurities can range from metal to rogue solvents like benzene and chloroform. Establish a framework for controlling quality and keeping contaminants below the threshold. You can set specs based on data from purge studies and spike tests. In the end, you'll need to convince FDA or EMA how did this.\nEmbrace the ICH: The FDA essentially has given you their playbook. The ICH outlines all the things regulators have seen people didn't do that they should have done. Anyone in the drug business cannot afford not to read these guidelines.\nOr use the QBD Framework: Quality by design is another useful alternative guideline. You work by a systematic approach to development, beginning with predefined objectives, product and process understanding, and process control.\nHere scientists must think about what they need to control in the process going forward to eliminate the problem from the start.\nThe idea is that when you do a quality by design approach or an enhanced development approach, you want to know as much as you can about your process and where the things that you need to control are.\nKnow the process. Apply controls from the onset, and compile data.""]"	['<urn:uuid:4f31c7c8-4776-42ba-b866-51aa5e28db6d>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	16	106	818
32	how investments increase future value and what risks affect bond returns	Investments increase future value by spending cash on assets that can grow in worth over time, though this requires giving up current resources like time, money, or effort. As for bond returns, they are affected by multiple risk factors: default risk (risk that issuer won't make payments), interest rate risk (affects bond prices when rates change), reinvestment rate risk (risk of reinvesting at lower rates), and liquidity risk. These risks are reflected in the required return through risk premiums added to the risk-free rate.	['Investment is a term that explains the process of spending cash on an asset with the objective of boosting its worth in the future. The investment needs that you give up something existing in order to acquire the future property, which can be time, cash, or effort. Nonetheless, when done appropriately, it can be a greatly rewarding process. Investments can range from small amounts of money to big quantities of cash. To establish the ideal financial investment for your requirements, you need to initially recognize the fundamentals of investment.\nPurchasing stocks calls for a high degree of risk. While supplies might be a lot more attractive for short-term financial investment, they often tend to have lower returns than even more conservative choices such as bonds. A great strategy for long-lasting investing is to balance the quantity of threat with the quantity of return. This method involves diversifying your financial investment portfolio throughout multiple possession courses and possession types.\nInvesting is a great method to increase your cost savings, in addition to develop an emergency fund in case of emergency situation. Nevertheless, prior to you begin spending, it’s critical to comprehend your current monetary situation and also your individual preferences. This understanding will help you create a solid investing method. If you’re unsure of how to spend your cash, consult a monetary advisor. 401k to gold ira rollover\nInvestment is the procedure of purchasing items or services for the function of producing wealth. Investments may be either repaired possessions such as bonds or supplies, or they can be a range of other properties such as realty home. While there are no assurances of return, diversification aids you to lessen danger. Nevertheless, it might additionally limit your making capacity.\nStocks can be classified right into 3 types: big cap, mid cap, and also small cap. Big cap stocks have a tendency to be much less unstable and give much less danger than little ones. Large-cap supplies benefit long-term financiers, while mid-cap supplies are useful for diversity and also offer an excellent mix of security and also development.\nAs pointed out over, economists have created an extremely deep understanding of firm financial investment behavior. Those companies who optimize their profits will use this structure to their investments. Those that don’t are unlucrative as well as will eventually be eliminated from the competitive market. This is a natural process. It’s referred to as Darwinian choice.\nThe main difference in between a participating preferred stock as well as an ordinary shares is their voting legal rights. Preferred stock provides you the right to vote at investors’ meetings. Common stocks, on the other hand, pay dividends as well as might be liquidated to gain resources gains. Nonetheless, stocks are a risky investment as a specific business can fail. Bonds, on the other hand, offer a set earnings to capitalists.\nA financial institution that provides a warranty to capitalists can use a range of financial items. Federal insurance, as an example, offers coverage as much as $250,000 of savings account. However, monetary assurances are rare in spending. By contrast, conserving is an accumulation of money for future usage. While the risk of financial investment is higher, saving is a risk-free choice. This is due to the fact that resources is not used. The return on capital is higher when it has intensified.\nOne more sort of financial investment is assets. Commodities can be concrete or digital. On top of that, they can be inputs right into society. For instance, you can purchase power products. The price of these products fluctuates depending upon just how much need they have. This financial investment is best for more knowledgeable capitalists. There are many threats included with commodities, nonetheless.\nSeveral financial institutions provide a selection of investment alternatives for capitalists. Selecting which one is right for you will certainly depend upon your objectives as well as your danger resistance. In general, however, supplies offer the highest possible returns. Bonds, on the other hand, supply lower revenue, are much less unstable, as well as are a lot more steady. Nonetheless, they lug threats pertaining to rates of interest, credit score danger, as well as financial obligation. buying gold with ira funds\nAn additional choice is to purchase term strategies, health insurance plans, or pension. These are tax-efficient and might offer a greater degree of protection than other forms of financial investment. You can also purchase equity funds, government plans, as well as mutual funds. In addition, ULIPs are a great alternative if you want to buy equity.\nAs you can see, financial investment is a wonderful way to protect your future monetarily. When you buy your cash, you’re making certain that it will certainly boost in value in the future. It is necessary to consider your long-term investment objectives as well as select a technique that can provide a significant return. A few of the best choices for development include mutual funds, assets, and equity. Nevertheless, they are risky, so it is necessary to consider your financial investment objectives thoroughly before picking an economic lorry.\nFinancial investment is a term used to define the act of spending money to get a property with the hope of raising its value with time. To make a financial investment, you have to sacrifice something currently in your belongings, such as money or time, to obtain a future property. Today possession can be time, effort, or cash. Right here are several of the important things to think about when making a financial investment.\nInitially, you should recognize your objectives and also economic situation. Consider just how much risk you fit taking, how much cash you fit shedding, as well as the dimension of your family. You additionally may want to speak with a financial expert that can help you make the appropriate financial investment choice. This person will also be able to assist you clear up any questions you may have. Furthermore, you ought to attempt to create a diversified profile, to ensure that your money is spread out throughout a range of properties. In this manner, you can keep an appropriate equilibrium and reduce threat.\nAn additional kind of investment is assets. These consist of oil, grains, pets, currencies, and also rare-earth elements. Financiers purchase these products by purchasing product futures, which are contracts to acquire or market a particular quantity at a particular rate. This investment is primarily for more knowledgeable financiers, as commodity rates rely on the supply and demand in the marketplace. As an example, the Covid-19 pandemic created oil rates to plummet. goldco gold ira\nThere are a number of various kinds of financial investment, with some investing for social great, while others are created revenue. In either instance, the fundamentals that determine investment decisions are influenced by the degree of revenue as well as other aspects. When a firm can not maximize its revenue, it is called a negative investment and also eventually dies out in the affordable setting.', '2 Topics in Chapter Key features of bonds Bond valuation Measuring yield Assessing risk\n3 Key Features of a Bond Par value: Face amount; paid at maturity. Assume $1,000. Coupon interest rate: Stated interest rate. Multiply by par value to get dollars of interest. Generally fixed. (More…)\n4 Maturity: Years until bond must be repaid. Declines. Issue date: Date when bond was issued. Default risk: Risk that issuer will not make interest or principal payments.\n5 Call Provision Issuer can refund if rates decline. That helps the issuer but hurts the investor. Therefore, borrowers are willing to pay more, and lenders require more, on callable bonds. Most bonds have a deferred call and a declining call premium.\n6 What’s a sinking fund? Provision to pay off a loan over its life rather than all at maturity. Similar to amortization on a term loan. Reduces risk to investor, shortens average maturity. But not good for investors if rates decline after issuance.\n7 Sinking funds are generally handled in 2 ways Call x% at par per year for sinking fund purposes. Call if r d is below the coupon rate and bond sells at a premium. Buy bonds on open market. Use open market purchase if r d is above coupon rate and bond sells at a discount.\n9 Value of a 10-year, 10% coupon bond if r d = 10% V B = $100$1,000... + $100 100 01210 10% 100 + 1,000 V = ?... = $90.91 +... + $38.55 + $385.54 = $1,000. ++ (1 + r d ) 1 (1 + r d ) N\n10 10 10 100 1000 NI/YR PV PMTFV -1,000 $ 614.46 385.54 $1,000.00 PV annuity PV maturity value Value of bond ====== INPUTS OUTPUT The bond consists of a 10-year, 10% annuity of $100/year plus a $1,000 lump sum at t = 10:\n11 When r d rises, above the coupon rate, the bond’s value falls below par, so it sells at a discount. 10 13 100 1000 NI/YR PV PMTFV -837.21 INPUTS OUTPUT What would happen if expected inflation rose by 3%, causing r = 13%?\n12 What would happen if inflation fell, and r d declined to 7%? If coupon rate > r d, price rises above par, and bond sells at a premium. 10 7 100 1000 NI/YR PV PMTFV -1,210.71 INPUTS OUTPUT\n13 Suppose the bond was issued 20 years ago and now has 10 years to maturity. What would happen to its value over time if the required rate of return remained at 10%, or at 13%, or at 7%?\n14 M 1,372 1,211 1,000 837 775 3025 20 15 10 5 0 r d = 7%. r d = 13%. r d = 10%. Bond Value ($) vs Years remaining to Maturity\n15 At maturity, the value of any bond must equal its par value. The value of a premium bond would decrease to $1,000. The value of a discount bond would increase to $1,000. A par bond stays at $1,000 if r d remains constant.\n16 What’s “yield to maturity”? YTM is the rate of return earned on a bond held to maturity. Also called “promised yield.” It assumes the bond will not default.\n17 YTM on a 10-year, 9% annual coupon, $1,000 par value bond selling for $887 90 01910 r d =? 1,000 PV 1. PV 10 PV M 887 Find r d that “works”!...\n18 10 -887 90 1000 NI/YR PV PMTFV 10.91 V INTM B = (1 + r d ) 1 (1 + r d ) N... + INT 887 90 (1 + r d ) 1 1,000 (1 + r d ) N = + 90 (1 + r d ) N + + + + INPUTS OUTPUT... Find r d (1 + r d ) N\n19 If coupon rate < r d, bond sells at a discount. If coupon rate = r d, bond sells at its par value. If coupon rate > r d, bond sells at a premium. If r d rises, price falls. Price = par at maturity.\n20 Find YTM if price were $1,134.20. Sells at a premium. Because coupon = 9% > r d = 7.08%, bond’s value > par. 10 -1134.2 90 1000 NI/YR PV PMTFV 7.08 INPUTS OUTPUT\n21 Definitions Current yield = Capital gains yield = = YTM = + Annual coupon pmt Current price Change in price Beginning price Exp total return Exp Curr yld Exp cap gains yld\n22 9% coupon, 10-year bond, P = $887, and YTM = 10.91% Current yield= = 0.1015 = 10.15%. $90 $887\n23 Cap gains yield = YTM - Current yield = 10.91% - 10.15% = 0.76%. Could also find values in Years 1 and 2, get difference, and divide by value in Year 1. Same answer. YTM = Current yield + Capital gains yield.\n24 Semiannual Bonds 1.Multiply years by 2 to get periods = 2N. 2.Divide nominal rate by 2 to get periodic rate = r d /2. 3.Divide annual INT by 2 to get PMT = INT/2. 2N r d /2 OK INT/2OK NI/YR PV PMTFV INPUTS OUTPUT\n25 2(10) 13/2 100/2 20 6.5 50 1000 NI/YR PV PMTFV -834.72 INPUTS OUTPUT Value of 10-year, 10% coupon, semiannual bond if rd = 13%.\n26 Spreadsheet Functions for Bond Valuation See IFM10 Ch04 Mini Case.xls for details. PRICE YIELD\n27 Callable Bonds and Yield to Call A 10-year, 10% semiannual coupon, $1,000 par value bond is selling for $1,135.90 with an 8% yield to maturity. It can be called after 5 years at $1,050.\n28 10 -1135.9 50 1050 N I/YR PV PMT FV 3.765 x 2 = 7.53% INPUTS OUTPUT Nominal Yield to Call (YTC)\n29 If you bought bonds, would you be more likely to earn YTM or YTC? Coupon rate = 10% vs. YTC = r d = 7.53%. Could raise money by selling new bonds which pay 7.53%. Could thus replace bonds which pay $100/year with bonds that pay only $75.30/year. Investors should expect a call, hence YTC = 7.5%, not YTM = 8%.\n30 In general, if a bond sells at a premium, then coupon > r d, so a call is likely. So, expect to earn: YTC on premium bonds. YTM on par & discount bonds.\n31 r d = r* + IP + DRP + LP + MRP. Here: r d =Required rate of return on a debt security. r*= Real risk-free rate. IP= Inflation premium. DRP= Default risk premium. LP= Liquidity premium. MRP= Maturity risk premium.\n32 What is the nominal risk-free rate? r RF = (1+r*)(1+IP)-1 = r*+ IP + (r*xIP) ≈ r*+ IP. (Because r*xIP is small) r RF = Rate on Treasury securities.\n33 Estimating IP Treasury Inflation-Protected Securities (TIPS) are indexed to inflation. The IP for a particular length maturity can be approximated as the difference between the yield on a non-indexed Treasury security of that maturity minus the yield on a TIPS of that maturity.\n34 Bond Spreads, the DRP, and the LP A “bond spread” is often calculated as the difference between a corporate bond’s yield and a Treasury security’s yield of the same maturity. Therefore: Spread = DRP + LP. Bond’s of large, strong companies often have very small LPs. Bond’s of small companies often have LPs as high as 2%.\n35 Bond Ratings Provide One Measure of Default Risk Investment GradeJunk Bonds Moody’sAaaAaABaaBaBCaaC S&PAAAAAABBBBBBCCCD\n36 Bond Ratings and Bond Spreads (YahooFinance, March 2008) Long-term BondsYieldSpread U.S. Treasury4.40% AAA6.40%2.00% AA6.24%1.84% A6.27%1.87% BBB6.15%1.75% BB7.00%2.60% B8.00%3.60% CCC7.45%3.05%\n37 What factors affect default risk and bond ratings? Financial performance Debt ratio Coverage ratios, such as interest coverage ratio or EBITDA coverage ratio Current ratios (More…)\n38 Provisions in the bond contract Secured versus unsecured debt Senior versus subordinated debt Guarantee provisions Sinking fund provisions Debt maturity (More…)\n43 What is reinvestment rate risk? The risk that CFs will have to be reinvested in the future at lower rates, reducing income. Illustration: Suppose you just won $500,000 playing the lottery. You’ll invest the money and live off the interest. You buy a 1-year bond with a YTM of 10%.\n44 Year 1 income = $50,000. At year-end get back $500,000 to reinvest. If rates fall to 3%, income will drop from $50,000 to $15,000. Had you bought 30-year bonds, income would have remained constant.\n45 The Maturity Risk Premium Long-term bonds: High interest rate risk, low reinvestment rate risk. Short-term bonds: Low interest rate risk, high reinvestment rate risk. Nothing is riskless! Yields on longer term bonds usually are greater than on shorter term bonds, so the MRP is more affected by interest rate risk than by reinvestment rate risk.\n46 Term Structure Yield Curve Term structure of interest rates: the relationship between interest rates (or yields) and maturities. A graph of the term structure is called the yield curve.\n48 Relationship Between Treasury Yields and Corporate Yields Corporate yield curves are higher than that of the Treasury bond. However, corporate yield curves are not neces- sarily parallel to the Treasury curve. The spread between a corporate yield curve and the Treasury curve widens as the corporate bond rating decreases.\n49 Hypothetical Treasury and Corporate Yield Curves\n50 Bankruptcy Two main chapters of Federal Bankruptcy Act: Chapter 11, Reorganization Chapter 7, Liquidation Typically, company wants Chapter 11, creditors may prefer Chapter 7.\n51 If company can’t meet its obligations, it files under Chapter 11. That stops creditors from foreclosing, taking assets, and shutting down the business. Company has 120 days to file a reorganization plan. Court appoints a “trustee” to supervise reorganization. Management usually stays in control.\n52 Company must demonstrate in its reorganization plan that it is “worth more alive than dead.” Otherwise, judge will order liquidation under Chapter 7.\n53 If the company is liquidated, here’s the payment priority: Past due property taxes Secured creditors from sales of secured assets. Trustee’s costs Expenses incurred after bankruptcy filing Wages and unpaid benefit contributions, subject to limits Unsecured customer deposits, subject to limits Taxes Unfunded pension liabilities Unsecured creditors Preferred stock Common stock\n54 In a liquidation, unsecured creditors generally get zero. This makes them more willing to participate in reorganization even though their claims are greatly scaled back. Various groups of creditors vote on the reorganization plan. If both the majority of the creditors and the judge approve, company “emerges” from bankruptcy with lower debts, reduced interest charges, and a chance for success.']	['<urn:uuid:90ffcf5b-a107-47ef-a25f-56800321299a>', '<urn:uuid:f1c5c168-652a-4254-bc7b-c1088f98a121>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:56:11.774636	11	84	2903
33	What positive effects do government policies have on business growth, and what negative consequences can trade restrictions bring to international commerce?	Government policies can positively affect business growth by creating favorable economic and legal environments, providing education and training to improve workforce skills, investing in infrastructure like roads and ports for efficient trade, and supporting research and development. They can also help through reduced regulations and creating high-technology clusters for knowledge sharing. However, trade restrictions like tariffs can harm international commerce by damaging bilateral and multilateral trading relationships, potentially triggering retaliatory measures from other countries. This can lead to trade wars, reduced export opportunities, and negative effects on both balance of trade and payments. The restrictions can also limit technology transfer and increase costs for domestic consumers through higher prices.	"[""At SmartCapitalMind, we're committed to delivering accurate, trustworthy information. Our expert-authored content is rigorously fact-checked and sourced from credible authorities. Discover how we uphold the highest standards in providing you with reliable knowledge.\nGovernment and economic growth are linked because a government may spur growth by creating an economic, legal and regulatory environment in which businesses can thrive. A government also may foster growth through its education and training policy, increasing opportunities for workers to improve their skills. Direct government spending on infrastructure projects such as road and rail links, seaports and airports can ensure that domestic and foreign trade operates efficiently. Technology provides a major boost to economic growth, and a government can pursue policies that are favorable for research and development or technology transfer. The link between government and economic growth also may be seen in government's creation of better conditions for international trade through lowering tariffs, tax and duties and concluding free-trade agreements with potential trading partners.\nEconomic growth normally is one of the main objectives of government policy, pursued by attempting to achieve stability in currency inflation, interest rates and exchange rates. A government may particularly help business by reducing laws and regulations to the minimum necessary. The government also may help economic growth through its education and training policy, ensuring that the workforce is in a position to acquire and maintain relevant skills. Government spending also may ensure that higher education institutions link up with business through knowledge sharing to ensure that pure and applied research may lead the development of innovative products by commercial enterprises. A government may stimulate networking by creating an environment for high-technology clusters of business and academic bodies, leading to knowledge sharing and collaborative development of new technologies.\nGovernment and economic growth are related where government improves the infrastructure to enable traders to move their goods at a lower cost and with greater efficiency. Well-planned and maintained road and rail links between cities and with seaports and airports may enable businesses to flourish as they gain easier access to domestic and foreign markets. Overcrowded seaports may be supplemented by access to inland dry ports with direct road and rail links to sea and air terminals and onward links to consumer markets. In some cases, the economy of outlying regions may be transformed by opening up transport links via bridges and tunnels to overcome natural barriers. Movement of workers from home to workplace may be facilitated by the development of metropolitan and underground railway networks and motorway links.\nA favorable tax and regulatory environment for business may reduce the temptation for some businesses to migrate to jurisdictions with low taxes and minimal regulation. The relationship between government and economic growth is seen where foreign trade is encouraged through lower tariff barriers for imports and negotiation of better access to foreign markets for exports. The government may negotiate free-trade agreements or membership in free-trade areas, and can conclude treaties for protection of investments and elimination of double taxation for companies operating in foreign jurisdictions."", 'PROS AND CONS OF TARIFFS\nProsand Cons of Tariffs\nProsand Cons of Tariffs\nAtariff is a tax or charge levied on imports or exports entering orleaving a particular country. While the sole purpose of the tariff isto restrict trade, there are a number of advantages that accrue withthe imposition of tariffs. Generation of income, restricting tradeand protecting industries are the advantages that accrue with theimposition of tariffs. On the other hand, the imposition of tariffsin an economy carries some disadvantages along with itsimplementation in the economy. In most cases, the increase in theprices of goods and services, reduction of sales volume and traderelations are the cons of imposing tariffs in an economy. Thediscussion on tariffs will illustrate their advantages anddisadvantages in an economy.\nTheAdvantages of tariffs\nOneof the main advantages of tariffs is the protectionism function,where they act as tools of protecting local industries from externalcompetition. Most times, consumers will opt for foreign commoditiesthat are produced instead of buying the same commodity producedlocally. This trend forces the government to impose tariffs oncertain goods so as to protect their own domestic industries or atthe same time make extra money (Gandolfo& Trionfetti,2013).It should be noted that consumers are not prohibited in any way tobuy foreign goods, but the art of imposing tariffs on those foreigngoods makes them expensive (Northrup& Turney, 2003).The consumer will have no choice but to buy the locally availableproducts produce by the local industries.\nAsa result, another advantage of tariffs, creating jobs is achieved.The imposition of tariffs leads to the creation of jobs in an economydue to the protectionist function of tariffs levied on the imports ofgoods, which can be available in the local market. By imposingtargeted tariffs on the importation of certain products, thegovernment prevents the exporting of employment to other countriesand encourages the creation of jobs in the country. This is becauseimporting goods gives the foreign producers the local market andtakes the market away from the local manufacturers (Gandolfo& Trionfetti,2013).Therefore, the local industries cannot grow when the local populationis creating a market for the foreign producers other than their ownindustries. Imposing tariffs cuts the demand and production capacityof the foreign manufactures and gives the same to the localproducers, thereby increasing local production (Gandolfo& Trionfetti,2013).By increasing production levels in the country, the tariffs end upcreating jobs locally.\nInsuch circumstances, it is the responsibility of governments toprotect their domestic employment. As we have seen above, when thereis a stiff competition of imported goods with a country’s domesticgoods, there the domestic industries end up being threatened. Thiscan result in the domestic companies firing their own employees ordeciding to shift to oversee countries, all with the purpose ofcutting costs (Smith,2005).The effect of this will result in a country having an increasednumber of unemployed, unhappy electorates. When ascending to power,many leaders always promise to create employment to their localcitizens, and tariffs open a way of preserving jobs for those whoalready have jobs.\nTheimposition of tariffs is also an advantage to the infant industriesin a country, which are competing with established internationalmanufacturers. A government can also decide to impose tariffs solelyin order to protect its own infant industries that are still tryingto pick up (Smith,2005).When a country wants to come up with its own industry, producing andsupplying a particular type of a good, it must protect the localmarket from the dominance of foreign producers. To achieve this, agovernment must make imports expensive. At the same time, thegovernment must make exports to the local consumers expensive for theforeign producer to viable pursue. To do this, the government will beforced to impose high tariffs on similar products produced fromforeign countries and in the process discouraging local consumersfrom buying the product.\nAnotheradvantage of imposing tariffs is the protection of consumers fromcheap or low quality products from foreign markets. Governments canalso choose to protect their own local consumers when they feel thatsome imported goods are low quality and promote dumping of cheapproducts from foreign producers (Michaely,2009).In such a case, a government will impose high tariff on the good, sois to discourage foreign producers from exporting them to localmarkets. At the same time, the imposition of a high tariff will forcelocal consumers to avoid them as they would be highly expensive toimport.\nTheimposition of tariffs on a product to protect consumers from lowquality products is implemented when a government has no legalgrounds of imposing a total ban on the product (Gandolfo& Trionfetti,2013).This may be as a result of lack of evidence or adherence tointernational trade agreements that may be in favor of the exportingcountry. A good example is where country A imposes a high tariff onmeat or beef from country B. Country A does this if the governmentbelieves that the meat from country B is low quality, but not enoughreason to ban its importation. As a result, the meat imports fromcountry B to country A will reduce or end, because its consumers willfind alternative market to import from.\nAnothersignificant advantage of imposing tariffs by a government is thegeneration of income from the fee levied on imports and exports.Income by the government is earned through the specific types oftariffs levied in the form of cash. For instance, the governmentraises income from the Ad valorem tariffs placed by the importingnation while putting into consideration the percentage of the goodbeing imported overall value (Gandolfo& Trionfetti,2013).\nThegovernment also raises income from the specific tariffs, where afixed fee is always levied on any amount or value of a good beingimported. For example, a country could be charging $500 for all carsbeing imported into its market. The tariff is always flexibledepending on the type of good. For instance, all fresh fruitsimported could be levied at $10 while fresh animal products beingimported could be levied at $30. All this translates into income forthe government.\nTheDisadvantages of Tariffs\nOneof the disadvantages of tariff is inefficiency of local industries,particularly when the tariffs are used as tools for protecting localindustries. When local industries are protected from foreigncompetition through tariff imposition, they face an easy tradeenvironment which may make them compromise on quality(Baier & Bergstrand, 2001). Inthis case, we can see that the art of imposing tariffs on foreigngoods can result to the local industries to be less efficient as theyare not subjected to eternal competition. At the same time, tradewars might arise between nations where the foreign nation might hitback by imposing their own tariffs on all imported goods from a givennation.\nAnotherdisadvantage of imposing tariffs is the impact they have on thebilateral and multilateral trading relationships with othercountries. The imposition of a tariff may induce a retaliatoryrelationship where the concerned countries hurt each other’s tradeby imposition of tariffs on the other country’s exports (Gandolfo& Trionfetti,2013).This may be as a result of a country being motivated to imposetariffs as a retaliation strategy. A country may think that anothercountry has not played by the agreed rules and thus is forced toimpose high taxes on all imported goods entering from the othernation. Retaliation also comes in handy when another nation simplydecides to go against the host nation on foreign government policies.\nAnotherdisadvantage of tariffs is that they create a cost aspect in theirimposition. For tariffs to be effective, they require a country toincur a cost for their imposition to be implemented. This cost is inthe form of employees for tax authorities, tariff infrastructure andsystems put in place (Gandolfo& Trionfetti,2013).The disadvantage comes where the costs incurred outweigh the benefitsgenerated from the tariff. For example, if a tariff meant to protectdomestic producers leads to high administrative costs of imposing it,it ends up being a disadvantage. This may happen where with noreasonable competition, the price of commodities will rise, whichwill also guarantee the sales of those producers to also rise.\nAnotherdisadvantage of tariffs is that they can lead to reduced employment,instead of creating or protecting local employment. Tariffs may limitthe importation of new technology and production potential to acountry, especially tariff levied on machines, raw materials andentrepreneurial equipment(Baier & Bergstrand, 2001). Ifa tariff is imposed on such goods or resources, local productionreduces as producers end up relying only on the local technology. Ifthis happens, the need for the domestic producers to employ moreworkers reduces, which will mean that the jobs will reduce in thecountry (Michaely,2009).As a result, the employment in the country reduces or stays at theold low level, other than creasing through the use of foreigntechnology.\nOnesuch good example that can show how bad tariffs can affect acountry’s economy is the imposition of tariffs on the imposition ofsteel. This will have the short run of benefiting the local producerswho will benefit from high prices to a point that they can even makehigh profits. But in the long run, the increasing cost associatedwith steel will make the cost of production in the economy high. Thelarger cost incurred are then distributed evenly to all consumersthrough high prices (O`Rourke& Smith, 2007).In addition, the production of basic goods will reduce, since steelis used in most factories. The infrastructural development willreduce as almost all industries use steel from buildings, railway,communication, and energy industries among many others. The impact isa crumbled economy, which will rely on imports to sustain itsconsumer needs.\nAnotherdisadvantage of imposing tariffs is the possibility of hurting theeconomy of the country by reducing the quantity and gains of thecountry’s exports. Remember as all this is happening, the tariffincreased and this means that the government is generating moreincome to benefit the economy. The consumer will be left with onlytwo viable options, either to buy less of those expensive goods orseek solace in buying less of other goods. With this increase inprice would mean that the consumer income will be deemed to bereduced.\nMoreimportantly, imposing high tariffs on certain goods discourages theirexportation to export and reduces the quantity of exports. This hasthe negative effect of reducing the amount of revenue a governmentcollects on such commodities (Jones& Martin, 2008).At the same time, reduced exports mean that the country’s foreignexchange will reduce. As a result, both the balance of trade andbalance of payment gets affected negatively, and reduces.\nAnotherbasic disadvantage of tariffs is that they make life expensive forlocal consumers by increasing the price of the products. As far astariffs are concerned, the consumer is always on the receiving end.This happens because the proportion of tariffs levied on imports istransferred by importers to the consumer in terms of a high price(Baier,& Bergstrand, J. (2001).The only time that consumers can be said to benefit from tariffs iswhen in the long run, the domestic industries use the advantage ofthe tariffs to improve on the quality of the production instead ofrelaxing.\nItis therefore a disadvantage that consumers benefit from the effectsof tariffs, but at a cost (O`Rourke& Smith, 2007).This happens because tariffs are designed to mostly favor the localproducers and the government. When tariffs are imposed on certaingoods, they have the impact on making related domestic production tobe favored and not the consumers. On the other hand, imposing oftariffs on certain goods ensure that the government benefits directlythrough revenue collection and not mainly consumers.\nTheimposition of tariffs has both advantages and disadvantages to theeconomy, the producers, consumer and the government. Imposing tariffsearn the government income and gives it a tool to protect localmarkets and industries from foreign markets. Imposing tariffs isadvantageous to consumers as they are protected from low qualityproducts from foreign producers. At the same time, tariffs protectproducers from competition from foreign products throughprotectionism policies. However, tariffs attract a cost in theircollection, reduce exports and may limit the introduction of foreigntechnology. At the same time tariffs may end up punishing the localconsumers as they are transferred in terms of prices. Despite thediscussed disadvantages, tariffs are not only important but necessaryfor an economy and their advantages prove the reason why they shouldbe levied.\nBaier,S., & Bergstrand, J. (2001). The growth of world trade: tariffs,transport costs, and income similarity. JournalOf International Economics,53(1),1-27.\nGandolfo,G., & Trionfetti,F. (2013). InternationalTrade Theory and Policy.New York: SpringerScience & Business Media\nJones,V., & Martin, M. (2008). Internationaltrade.[Washington, D.C.]: Congressional Research Service.\nMichaely,M. (2009). Tradeliberalization and trade preferences.Hackensack, N.J.: World Scientific.\nNorthrup,C., &Turney, E. (2003). Encyclopediaof tariffs and trade in U.S. history.Westport, Conn.: Greenwood Press.\nSmith,J. (2005). In praise of tradeoffs. BMJ,330(7498)Retrived From, <http://dx.doi.org/10.1136/bmj.330.7498.0-g>29 October, 2015']"	['<urn:uuid:13debb6f-70e0-4cc0-9561-04aaddad9c27>', '<urn:uuid:809b5bf4-df61-42c2-8929-4f9de7462280>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	21	109	2508
34	How long does a high-pressure processing cycle take?	A high-pressure processing cycle takes about six minutes, compared to traditional sterilization which takes an hour or more.	['Deliver Extended Shelf Life For Dairy\nHealth-conscious consumers continue to seek fresh, minimally processed foods, challenging dairy manufacturers to find ways to maintain the high nutritional value of their products, while at the same time extending their shelf life. As the population grows and disposable income increases, consumer awareness of the high nutritional value of dairy products is fueling market growth in all regions, according to a report produced by Market Research Future.\nThe increasing popularity of dairy is a global phenomenon. Per capita volume growth of dairy in China, for example, is a strong 48 percent. Sales of plain yogurt alone in China are expected to see a 26 percent compound annual growth rate through 2020, and not far behind is India, at 16 percent, according to the Global Packaging Trends Report 2017 produced by Euromonitor and sponsored by leading packaging associations PMMI, the Association for Packaging and Processing, Australian Packaging and Processing Machinery Association, Italian Manufacturers of Automatic Packing and Packaging Machinery and the Processing & Packaging Machinery Trade Association.\nWith this growth comes a desire to extend the shelf life of dairy products. Even a few extra days of shelf life can be a significant benefit to the processor, the retailer and, ultimately, the consumer. Extending shelf life even beyond that — into the realm of an extra 30 or even 90 days — creates new and unique opportunities for the marketing of milk and other dairy products.\nIn order to achieve the desired extended shelf life, dairy producers look to packaging solutions. Why do some dairy products have longer shelf life than others? While this is a complex question, it typically comes down to factors such as packaging/processing method, packaging materials and hygiene.\nThe dairy industry uses various means and technologies aimed at achieving an extension in product shelf life. Traditionally, milk and other dairy products are treated at high temperatures to inactivate microbes and ensure product safety. The most common treatment is pasteurization, which can increase shelf life from seven to 20 days.\nWhile there are other common methods, heat treatment remains the standard, primary factor that determines the shelf life. That said, processing at high temperature lowers the nutritional quality of foods because many nutrients are easily altered due to these high temperatures.\nCold Processing Technologies\nTo overcome this problem, “cold processing” technologies using high pressure have been developed. Studies have revealed that high-pressure processing (HPP) can be used in the dairy industry to increase microbiological safety as well as to modify functional properties of food. This method has been considered equal to sterilization, which extends shelf life of foods due to its ability to inactivate bacterial spores at reduced heat and preserve desirable functional properties of foods better than conventional thermal processing.\nIn addition to extending the shelf life of dairy products, HPP treatment provides advantages over conventional processes due to its speed (a cycle takes about six minutes compared to traditional sterilization processes which takes an hour or more). Unlike thermal processing, food typically does not experience significant physical and chemical changes as HPP does not affect the flavor, nutritional value or texture properties of the product. One of the limitations of this process is that food must contain water, as the entire process is based upon compression.\nLike any post-packaging process, HPP places demands on the packaging materials and the technology has limits. Products must contain enough moisture to transmit the pressure throughout, and as little air or headspace as possible — making vacuum-packed products such as some cheese ideal. But HPP can accommodate a range of packaging formats including bottles, jars, trays and flexible packages — as long as the package can flex about 15 percent on any one surface to withstand HPP compression.\nIn addition to flex, the package graphics need to be water-friendly, as the packages will be immersed in water. Because of this, labeling is many times done post-HPP to ensure quality.\nDairy products require packaging with high quality, high barrier material to prevent contamination, extend shelf life and to maintain freshness all the way to the consumer. HPP requires packages to have a hermetic seal, as any leakers will be obvious when the package exits the pressurized system.\nNo matter the package or process used, the key to extended shelf life begins with hygiene. The shelf life of a chilled dairy product cannot be extended without first raising the levels of hygiene across the whole dairy. By implementing advanced hygienic design in filling and packaging machinery, product recalls can be prevented, while also simplifying cleaning and reducing product and material waste.\nHowever, to gain maximum benefit, it is important to look at the whole system. Extending shelf life is not just about technology and equipment, it is about a system. And its success is dependent on the hygienic strength of the entire production and distribution chain.']	['<urn:uuid:e9006b82-14a4-42ed-b8c1-7bcbe347f5fb>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	8	18	810
35	Which uses more exotic woods: Shaker cabinets or tropical style designs?	Tropical style designs use more exotic woods than Shaker cabinets. While Shaker cabinets typically use common hardwoods like cherry, maple, oak, and birch, tropical style designs specifically incorporate exotic woods such as bamboo, mahogany, rattan, teak, acacia, and rosewood for authenticity. These tropical hardwoods are often sourced from sustainable tree farms and tend to be more costly than the traditional hardwoods used in Shaker cabinets.	"['The different styles of Shaker cabinets have long been cherished for their simple and elegant design. They bring classic beauty to any space while maintaining a timeless appeal. Besides having quality construction, knowing its various styles is just as important. In this visual guide, we will explore the different styles of Shaker cabinets and delve into their design elements to consider when choosing the right style for your home.\nWhat are Shaker Cabinets?\nThe origins of Shaker cabinets can be traced back to the Shaker communities in the United States. The Shakers believed in minimalism and created furniture and cabinets that reflected their values. The designs were rooted in simplicity, quality craftsmanship, and functionality.\nClean, square-edged frames, recessed panel doors, and simple hardware characterize Shaker cabinets. They often feature a five-piece door with a flat center panel. The cabinets are typically made from hardwood, such as cherry, maple, or oak, enhancing their durability and natural beauty.\nThe Appeal of Shaker Cabinets\nOne of the main reasons for the enduring popularity of Shaker cabinets is their simple and timeless design. The clean lines and minimalist aesthetic make them suitable for both traditional and contemporary spaces. Shaker cabinets can seamlessly blend into any design scheme, whether you prefer a classic or modern look.\nAnother appeal of Shaker cabinets lies in their versatility and adaptability. They can be easily customized to fit any kitchen, bathroom, or living area. Shaker cabinets also provide a neutral backdrop, allowing you to experiment with different color schemes and decorative elements.\nThe Different Styles of Shaker Cabinets That Are Popular\nNow that we have given you a small introduction about Shaker cabinets, knowing their various will allow you to pick one that will fit with your home. Below are the most popular shaker style cabinets in terms of aesthetics. Read on to learn everything about them.\nTraditional Shaker Cabinets\nTraditional Shaker cabinets stay true to the original design principles. They feature simple, unembellished doors and drawer fronts with minimal hardware. These cabinets evoke a sense of timelessness and are ideal for homeowners who appreciate the charm of a classic aesthetic.\nContemporary Shaker Cabinets\nContemporary Shaker cabinets give a modern twist to the traditional Shaker style. They keep things simple and functional but with smoother lines and new materials. These cabinets usually have smooth, straight doors without any fancy decorations. If you want a modern look that still honors the classic Shaker design, contemporary Shaker cabinets are trendy.\nTransitional Shaker Cabinets\nTransitional Shaker cabinets are a mix of classic simplicity and modern style. They blend the traditional Shaker design with contemporary elements, creating a seamless combination of old and new. These cabinets keep Shaker cabinets’ clean lines and minimalist features but add extra details for a transitional look. You’ll often find a blend of materials like wood and glass and decorative accents such as crown molding or modern hardware.\nMid-Century Shaker Cabinets\nMid-century Shaker cabinets are a type of cabinetry that came about in the middle of the 20th century, taking inspiration from the traditional Shaker design. These cabinets are known for their precise and neat lines, minimal decorations, and emphasis on practicality. Mid-century Shaker cabinets have a timeless charm and can easily fit into different interior styles. Mid-century shaker kitchen cabinet styles offer a polished and streamlined appearance.\nChoosing the Right Style for Your Space\nWhen choosing a style of Shaker Cabinets for your space, consider your home’s overall theme and design. Traditional Shaker cabinets work well in heritage or rustic settings, while contemporary styles suit modern and minimalist interiors. Transitional Shaker cabinets offer a flexible option that can adapt to various design schemes.\nMaterials and Finishes\nShaker-style cabinets can be crafted from various materials, each offering unique characteristics. The most common materials used in Shaker cabinet construction include solid hardwoods such as maple, cherry, oak, and birch. These hardwoods are known for their durability, strength, and natural beauty. Additionally, plywood and medium-density fiberboard (MDF) are often used for cabinet boxes and interior components to provide stability and cost-effectiveness.\nIn terms of finishes, Shaker cabinets offer a range of options to achieve different looks. Natural finishes are popular, allowing the beauty of the wood grain to shine through. This can be achieved through clear coats or light staining to enhance the natural color and texture of the wood. Painted finishes are also common, with white being a classic choice for Shaker cabinets, offering a clean and timeless look. Other color options are available to suit various design preferences. Techniques such as distressing or glazing can also be applied to achieve a more rustic or antique appearance. The choice of material and finish for Shaker-style cabinets allows versatility in creating a desired aesthetic and complementing the space’s overall design.\nShaker Cabinet Hardware\nHardware for shaker cabinets plays a significant role in enhancing cabinets’ overall aesthetic and functionality. Here’s a closer look at the role of hardware in Shaker cabinet design:\n- Drawer Pulls and Knobs: Knobs or pulls on shaker cabinets are essential hardware components. Their design should complement the clean lines and simplicity of the cabinets. Opt for hardware with minimalistic, straightforward shapes and finishes. Common choices include brushed nickel, stainless steel, matte black, or oil-rubbed bronze. These finishes provide a subtle contrast against the cabinet’s surface while maintaining the cabinet’s timeless appeal.\n- Cabinet Hinges: Hinges are necessary to attach cabinet doors to the frame. Concealed or hidden hinges are often preferred for Shaker cabinets, as they maintain the clean and uncluttered appearance of the cabinet doors. Soft-close hinges are also popular, adding a touch of modern functionality by allowing doors to close gently and quietly.\n- Cabinet Handles and Pulls: If you have larger cabinet doors or want to add visual interest, consider incorporating handles or cabinet pulls for shaker cabinets. These elongated hardware pieces can be mounted horizontally or vertically along the center of the cabinet doors. Like drawer pulls and knobs, opt for hardware that complements the cabinet’s design and your kitchen’s overall style.\n- Placement: Where to place handles on Shaker cabinets? The hardware placement can significantly influence their visual impact. For drawers, center the hardware on the upper edge of the drawer front. For cabinet doors, hardware is often centered vertically on the stile (the vertical frame piece) and horizontally on the rail (the horizontal frame piece).\n- Consistency: Maintaining a consistent style and finish for all the hardware in your kitchen contributes to a cohesive and polished look. Choose hardware that coordinates with your faucet, lighting fixtures, and other kitchen elements for a harmonious design.\n- Customization: While Shaker cabinets are known for their simplicity, you can still add a touch of personalization through hardware choices. Consider experimenting with unique shapes, finishes, or materials to add a subtle twist to the classic design.\n- Functionality: Apart from their aesthetic role, hardware enhances the functionality of Shaker cabinets. Easy-to-grasp pulls, and knobs make opening and closing cabinet doors and drawers convenient. Soft-close hinges and drawer slides also improve the user experience by preventing slamming and ensuring smooth operation.\nIn conclusion, shaker cabinets with hardware are crucial in cabinet design, impacting your kitchen’s visual appeal and functionality. When selecting hardware for your Shaker cabinets, prioritize pieces that align with the cabinet’s clean and timeless aesthetic while also considering their practicality and contribution to the overall design of your kitchen space.\nMaintenance and Care For Shaker Cabinets\nProper maintenance and care are crucial for preserving the beauty and durability of Shaker cabinets. Here are some key practices to keep in mind:\nRegular cleaning is essential to remove dust, dirt, and spills from the cabinet surfaces. Use a soft, damp cloth to wipe down the cabinets, avoiding abrasive cleaners that could harm the finish. Dry the cabinets thoroughly after cleaning to prevent moisture damage.\nIt’s important to handle Shaker cabinets with care. Avoid slamming the doors or placing excessive weight on them, as this can strain the hinges and hardware. Utilize knobs or pulls to open and close the cabinets, as they provide a better grip and protect the finish.\nMoisture control is crucial to prevent warping, cracking, or discoloration of the wood. Keep Shaker cabinets away from excessive moisture and humidity, particularly in areas such as the kitchen or bathroom. Use proper ventilation and avoid placing cabinets near water sources.\nShaker Cabinets in Different Rooms\nShaker cabinets are versatile and can be used in various rooms throughout the home. The kitchen provides a classic and clean look; the bathrooms create a serene and spa-like atmosphere. Shaker cabinets can also be incorporated into other areas, such as laundry rooms, home offices, or entertainment centers.\nEnhancing Shaker Cabinets And Design Trends\nTo further personalize and enhance the functionality of your Shaker cabinets, consider incorporating accessories. Pull-out trays, spice racks, and drawer dividers maximize storage efficiency. Glass doors or open shelving can showcase your favorite dishware or decorative items.\nThe different styles of Shaker cabinets have stood the test of time and have remained popular for homeowners and interior designers. Their simplicity and adaptability align with current interior design trends prioritizing clean lines, minimalism, and functionality. By choosing Shaker cabinets, you can create a timeless and contemporary space.\nPros and Cons of Shaker Cabinets\nShaker cabinets are a popular choice in kitchen and bathroom designs, known for their timeless elegance and clean, minimalist aesthetic. They offer a versatile and adaptable option that can complement various interior styles. Like any design choice, Shaker cabinets have their own pros and cons. Understanding these advantages and disadvantages can help you make an informed decision when considering Shaker cabinets for your home. In the following sections, we will explore the pros and cons of Shaker cabinets, covering aspects such as design versatility, durability, maintenance, and cost, to provide a comprehensive view of this cabinet style.\n- Timeless and versatile design\n- It fits various design aesthetics\n- Durable and long-lasting\n- Customizable options available\n- Can increase the value of your home\n- It may be more expensive than other cabinet styles\n- Limited ornamental details for those seeking intricate designs\nThe different styles of Shaker cabinets are popular for homeowners looking for a timeless and elegant design. Shaker cabinets are great for different design styles and rooms in your home because they are simple, versatile, and adaptable. You can choose the perfect Shaker cabinets to create a beautiful and functional space. Consider the different styles, materials, and customization options. Our store, Badger Cabinets, is located in Oak Creek, Wisconsin, and offers a wide range of colors and finishes in Shaker cabinets for your home.\n1. Are Shaker cabinets only suitable for traditional-style homes?\nShaker cabinets are not limited to traditional-style homes. While a classic aesthetic complements traditional designs, Shaker cabinets can also work well in transitional, contemporary, and even modern interiors. Their clean lines and minimalist appeal make them versatile and adaptable to various design styles.\n2. Can I paint my Shaker cabinets a different color?\nYes, you can paint your Shaker cabinets a different color. Painting your cabinets is a popular way to update and customize their look to your desired color scheme. Ensure to properly prepare the surfaces, use appropriate primer and high-quality paint suitable for cabinetry, and follow proper painting techniques to achieve a smooth and professional finish.\n3. Are Shaker cabinets more expensive than other cabinet styles?\nThe cost of Shaker cabinets can vary depending on factors such as materials used, construction quality, finishes, and customization options. Generally, Shaker cabinets fall within a moderate to high price range compared to other cabinet styles. However, comparing prices and exploring options is important to find the best fit for your budget and design preferences.\n4. Can I install Shaker cabinets myself, or do I need professional help?\nWhile it is possible to install Shaker cabinets yourself if you have the necessary skills and experience, cabinet installation can be complex and time-consuming. It requires precise measurements, proper tools, and knowledge of structural considerations. If you’re unsure or inexperienced, it’s advisable to seek professional help to ensure the cabinets are installed correctly and securely.\n5. Can I add glass doors to my Shaker cabinets?\nYes, you can add glass doors to your Shaker cabinets. Glass doors can be stylish, allowing you to display and showcase items inside the cabinets. Shaker cabinets can be customized with glass inserts in the center panel or by replacing entire door panels with glass. This adds visual interest and a touch of elegance to your cabinets while still maintaining the timeless appeal of the Shaker style.', ""Living Room Designs With a Wood Grain Trim\nNo other building material adds warmth and character to a room quite like wood. While almost all trim in a home is made from wood, the majority of homes today feature painted wood trim, where the natural wood grain is hidden underneath layers of paint. However, many decorating styles incorporate natural wood trim, where the wood grain shows through and helps enhance each design style with pattern, dimension and texture. Although different finishes applied to wood trim make it practical to use in almost any decorating style, a few styles tend to feature natural wood trim more often than not.\nArts and Crafts-Style Design\nThe Arts and Crafts movement, which took place in the early 1900s, spawned decorating styles such as Craftsman and Mission. A Craftsman-style living room may be one of the best examples of a decorating style that incorporates a significant amount of natural wood, not only in the room’s furnishings but also in building materials and architectural features. Natural wood trim is a common design element around windows, doors, baseboards, stairs and fireplaces. Exposed ceiling beams and built-in furnishings such as bookcases and window seats are also typical in a Craftsman-style living room.\nLight-colored finishes are common in this casual, simplistic decorating style, which can include less formal types of wood sourced from local forests, such as pine and oak.\nMinimalist Japanese-Style Design\nMinimalist design styles focus on clean lines and open spaces that create a harmonious and serene atmosphere, as well as the use of natural fibers and colors. Japanese culture in particular has a large influence in this decorating style.\nA Japanese-inspired living room will include plenty of wood grain accents and furnishings in a simple, clean and uncluttered design. Common design elements include open floor plans that allow plenty of natural light throughout the interior and the use of natural colors and textures. Typical features include shoji screens, often made from pine and rice paper, and a lot of natural wood in the form of wood paneling, trim, ceiling beams and flooring.\nThe natural wood used in Japanese-inspired design is highly lacquered and very sleek with clean lines and both light and dark finishes.\nRustic Lodge-Style Design\nRustic design styles are closely tied to cultures and lifestyles that provide the inspiration for each particular style, such as Southwestern, ranch and lodge. Natural wood trim fits perfectly with any rustic-style living room but is most common in a lodge-style home.\nA lodge-inspired living room is the most rugged design style, with wood trim and accents that are very raw and rough. There is a close connection to the great outdoors. Decorative elements such as wood and stone have a more natural appearance.\nLodge-style living rooms can include impressive features such as vaulted ceilings with rugged wooden beams and massive stone fireplaces with beautiful wooden mantels. Large viewing windows framed in wood collectively make up an entire wall to showcase stunning mountain landscapes.\nTropical Style Design\nA tropical-style living room will also focus on natural elements. Tropical plants provide the greenery typical of this decorating style, which pairs perfectly with natural wood accents.\nExotic woods such as bamboo, mahogany, rattan, teak, acacia and rosewood make excellent choices for tropical-style accents and furnishings, adding authenticity to the design. Much of the tropical wood used for furniture and building material today is grown on tree farms where the wood is harvested in a responsible, sustainable manner. It's important to source tropical hardwoods from environmentally friendly manufacturers.\nTropical wood can have rich, dark finishes or light, natural finishes, just like any other type of wood. Exotic wood panels, trim and flooring may be more costly than other types of wood, but for those who can afford it, it is well worth the price.\n- Jupiterimages/Photos.com/Getty Images""]"	['<urn:uuid:6788e912-fd35-42b0-a14d-bfb1cd24ebae>', '<urn:uuid:4c568b2e-bf3c-48c5-bae7-e69642c6ef09>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T04:56:11.774636	11	65	2726
36	pearl harbor skull excavation crew forks metal coca cola bottle date found	The excavation crew found a skull along with forks, scraps of metal and a Coca-Cola bottle that researchers determined was from the 1940s.	['HONOLULU (AP) ? Forensic scientists are conducting tests on a skull unearthed at the bottom of Pearl Harbor to determine if it is a Japanese pilot who died in the historic attack on Dec. 7, 1941.\nAn excavation crew dredging the harbor recently made the startling discovery of the skull, which archeologists believe is from one of the Japanese aviators in the surprise attack. Archaeologist Jeff Fong of the Naval Facilities Engineering Command Pacific described the finding to The Associated Press and the efforts under way to identify the skull. He said the early analysis has made him “75 percent sure” that the skull belongs to a Japanese pilot.\nHe did not provide specifics about what archaeologists have learned about the skull, but said it was not from one of Hawaii’s ancient burial sites. They also contacted local police and ruled out the possibility that it’s from an active missing person case, said Denise Emsley, public affairs officer for the Naval Facilities Engineering Command Hawaii, which was being inundated with media calls Wednesday about the skull from international news organizations.\nThe items found with the skull, which was determined not to be from a Native Hawaiian, provided some clues: forks, scraps of metal and a Coca-Cola bottle Fong said researchers have determined was from the 1940s.\nFifty-five Japanese airmen were killed and 29 of their aircraft were shot down in the attack, compared with the 2,400 U.S. service members who died. No Japanese remains have been found at Pearl Harbor since World War II.\nPearl Harbor is home to the USS Arizona Memorial, which sits on top of the battleship that sank during the attack. It still holds the bodies of more than 900 men.\nThe skull remains intact despite being dug up with giant cranes and shovels.\nIt was April 1 when items plucked from the water during the overnight dredging were laid to dry. When it was determined a skull was among the dredged items, contractors were ordered to stop the work, Emsley said. “We definitely wanted it to be handled correctly,” she said.\n“That’s why it’s been kept quiet. We didn’t want to excite people prematurely,” she said.\nThe Joint POW/MIA Accounting Command on Oahu, charged with identifying Americans who were killed in action but were never brought home, has been asked to determine who the skull belongs to. The cranium was turned over to the command’s lab for tests that will include examining dental records and DNA, said John Byrd, the lab’s director and a forensic anthropologist.\n“We’re working on the case but the case is just in the early stages of analysis,” he said. “We’re not going to know much more about it for a while yet.”\nThe lab is the only accredited Skeletal Identification Laboratory in the United States. JPAC has identified more than 560 Americans since the command was activated in 2003. When more information is gleaned from the skull, other agencies could get involved including the Naval Criminal Investigative Service and the Japanese Consulate.\n“At this point it’s just a hypothesis, it’s not a conclusion,” Byrd said. “It might be very interesting or it could be very mundane.”\nIt’s rare to find remains in Hawaii, said Kohei Niitsu, an official at Japan’s Ministry of Health, Labor and Welfare in Tokyo. “The government usually sends a team to determine if the remains are indeed Japanese, and if this is confirmed, they are brought back to Japan,” Niitsu said.\nNAVFAC Pacific issued a statement late Wednesday saying it was too early to identify the remains.\n“Until we receive the final report of the forensic analysis being conducted by scientists at (JPAC), we won’t know with certainty whether the remains are a Japanese pilot or not,” the statement said.\nDaniel Martinez, the National Park Service’s chief historian for Pearl Harbor, said experts on Pearl Harbor know enough about the specific location where Japanese planes went down in the attack that they might be able to match the skull with a crewmember.\n“They landed in a variety places throughout Pearl Harbor and the island of Oahu,” Martinez said. “In the area of Pearl Harbor, we know what plane was shot down and who was in the crew.”\nMartinez said that beyond the historical significance of the finding, it is a reminder of a life lost.\n“I think that anytime you’re able to reclaim a casualty and perhaps even identify it, regardless of what country, it may bring closure to a family,” he said.\nAssociated Press writer Jay Alabaster in Tokyo contributed to this report.']	['<urn:uuid:c1878b66-70a4-47ef-a12c-3c6a6c67c82c>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	12	23	758
37	What are the key differences between growing wheat grass and alfalfa sprouts in terms of germination time and growth requirements?	Wheat grass and alfalfa sprouts have different growing requirements and timelines. Wheat grass takes longer to grow, requiring 3-5 days for initial germination plus another week to reach 8-10 inches tall for harvesting. It needs 1-2 inches of soil, specific trays, and a mix of 50% organic compost/potting soil and 50% peat moss. In contrast, alfalfa sprouts are quicker to grow, sprouting in just 3-5 days total. They can be grown more simply in a glass jar or small tray, requiring only water for soaking the seeds, and don't need soil at all.	['Wheat. Wheat is that cereal grain that we do so much with already: flour, bread, pasta, sprouts, and cake. You can grow grass with it too. Wheat grass grows from whole wheat “berries.” Organic, hard red winter wheat berries that haven’t been treated with chemicals or oxygen absorbers are the best wheat berries to use for growing wheat grass. The berry is really a seed, alive but dormant, so it needs oxygen in storage. 4,000-year-old wheat from Egyptian tombs has been known to grow so it probably doesn’t need very much oxygen. Health food stores and natural food co-ops where you can buy in bulk are the best bet for finding the wheat berries. Clerks might not know if the wheat has been treated. Buy it and try it anyway: just get started.\nTrays. Wheat grass will grow in only an inch of soil so you don’t need a large garden area. Thick rigid plastic trays like cafeteria trays or photographic processing trays are good. Cafeteria trays are usually about 10” X 14” and this is a convenient size to work with. Plant nursery seedling trays are another option. Just about anything is worth a try: metal snack trays, Pyrex baking dishes, anything that will hold an inch or two of soil. Cardboard is not recommended though. It falls apart after only one use and can be a watering mess. Restaurant suppliers and second hand stores are good sources for trays.\nSoil. A good soil mixture for the trays is 50% organic compost or potting soil and 50% peat moss. Mix it thoroughly, breaking up clumps and add a couple of teaspoons of rock dust, dolomite or green sand per tray (optional). If you are a gardener you probably already have a favorite seed starter mix and this will work too. It is best not to use outdoor garden soil because of the bugs and stray weed seeds.\nWater. Untreated well water is the best water to use but difficult to obtain for most of us. If your water is heavily chlorinated (can you smell it?) use filtered water or let it set in an open container 24 hours to let the chlorine gas off. A mister bottle and a watering can with a sprinkler head will be handy.\nGrow the Grass\n1) Soak the wheat berries for 12 hours or overnight. Use one cup of wheat berries per (cafeteria) tray and enough water to cover the berries by about 1-2 inches. Cover or put in a dark place. In hot weather the berries can start to ferment in this amount of time. They will be bubbly, foamy and maybe even smell. If this happens, you can throw them out and start over or plant them anyway. They might grow; wheat seems forgiving of mistakes like these.\n2) Plant the wheat berries. Prepare the trays by spreading the soil mixture about 1-2 inches deep or however much the tray will hold. In shallow trays like cafeteria trays, make a trough around the edge and mound the soil slightly in the center. This keeps the water in the trays and prevents overflow accidents. Water the soil well with the mister and spread the soaked wheat berries in a single layer. They can touch but should not be on top of each other. Cover with a thin layer of soil as thin as you can make it. Cover the tray with another tray or several layers of dampened newspaper and plastic over all. Place in a dark cool place.\n3) The wheat berries germinate and sprout. For the first 3-5 days uncover the trays daily for fresh air. Check for mold and wipe it off if you find it. Check for moisture and water with the mister if it seems dry. Cover and put the tray back in the cool dark place. You should be seeing roots and sprouts in this time frame. After 5 days, if there are no sprouts and/or lots of mold, you may have bad seeds or soaked them too long or over watered or had too much heat for germination.\n4) When the sprouts are 1 inch tall, uncover and mist with a dilute liquid seaweed supplement. This supplement step is optional but worth it if you have the seaweed available. Continue to keep the trays uncovered, in indirect sunlight until the blades of grass have grown to 8-10” tall. This may take another week. They need water everyday. Use the sprinkler can for the main watering and mist later if they look wilty. Use the seaweed only in the first days, otherwise the juice may taste like seaweed. Turn the trays if the grass begins to lean in one direction.\nJuice the Grass\nYou will need a serrated knife or pair of scissors, several bowls, shot glasses, measuring cups and a juicer.\nJuicers are manual hand-cranked or electric. Electric juicers should be low rpm (not over 50 rpm) because high speed oxidizes the grass and the grass fibers bind. Don’t even try to use a blender. Electric juicers are the only expensive part of this whole process. An electric juicer will pay for itself though if you consider all those $5.00 juice drinks at the juice bar. Hand-cranked models are cheaper but you have to do some work to get your juice.\n1) Harvest the wheat grass with scissors or a knife. Hold a bunch of wheat grass in one hand and cut as close to the soil as possible with the other hand. Have a bowl handy. The grass is so pretty, green and vibrant. You may not want to cut it but do it anyway.\n2) Get the juicer out. You will need a bowl to catch the expelled pulp and shot glasses or measuring cups to hold the juice. Feed the grass into the hopper and either crank or let electricity do the work. Soon a dark green liquid will come out with the pulp coming out separately. The pulp can be run through the juicer again. A light green foam comes out as well making a head on the juice.\n3) Clean the juicer right away. It is difficult to clean if left too long.\n4) After harvest you have a tray full of grass stubble. There are plenty of things to do with it. Continue to water it and get a second growth to juice again. It is a good addition to compost bins and worm bins. Break the mat of roots and soil into pieces before adding to the bin. You can also feed it to chickens. They love pecking out the berry and getting some fresh greens at the same time. Take the mat of roots, soil and grass stubble out in one piece for the chickens.\nUse the Juice -- Wheat grass juice recipes-Click here\nThe juice should be used within 30 minutes of harvest and juicing. It starts breaking down and goes bad within 12 hours.\nDrink it for a morning energizer, for detoxifying and cleansing. Instead of a vitamin pill from a factory or processing plant drink 1-2 ounces of wheat grass juice for your vitamins, minerals, enzymes, protein and chlorophyll. You probably won’t be drinking it for the taste. Some people have a hard time getting it down the hatch. One trick is to hold your breath and put a piece of parsley in your mouth. Just think how healthy it is too. That should help. The taste is very grassy (surprise) but has a sweet finish.\nDrink it as part of a fast. Wheat grass juice is an excellent super nutritional addition to a fasting program. Always drink plenty of water while fasting.\nWheat grass juice can be used in enemas and rectal implants for rapid cleansing. Juice can be used in douches as well.\nThe expelled pulp and juice can be used in first aid poultices applied to sunburned skin, rashes, boils and cuts.\nWheat grass juice has cosmetic uses. Use it as a hair conditioner and scalp treatment. Gargle with it for fresh breath and gum health.\nGive yourself a facial with wheat grass juice. First cleanse and steam the skin to open pores. Then apply undiluted fresh juice to the skin with cotton balls. Relax for 5 minutes and rinse.\nTake a bath in wheat grass juice. Usually wheat grass juice is used undiluted or straight, but for this application diluted is the way to go. Pour a few ounces into the bathwater and make a teabag of pulp to hang.\nThrow a wheat grass party. It is best to invite a mix of people: people who know about wheat grass but have never tried it and old hands. Introducing people to wheat grass juice is great entertainment.\nGreen beer. Instead of using food coloring to make green beer for St. Patrick’s Day, use wheat grass juice. It doesn’t take much to turn beer a beautiful green color. It’s much healthier than food coloring.', 'Can I grow alfalfa sprouts?\nAlfalfa sprouts grow quickly, sprouting in just three to five days. You can grow them in a glass jar or a small tray, and you only need 1 tablespoon of seeds to get 1 1/2 cups of sprouts. These nutritious sprouts are high in antioxidants and are a great addition to salads and sandwiches.\nHow do you germinate alfalfa seeds?\nPlace alfalfa seeds in a quart jar or other sprouting container. Add ¼-½ cup cool water, cover with a sprouting screen or mesh sprouting lid and soak 8 hours or overnight. Drain all water off the alfalfa seeds. Invert the jar over a bowl at an angle so that the seeds will drain and air will be able to circulate.\nWhat are alfalfa sprouts good for?\nThe bottom line. Alfalfa has been shown to help lower cholesterol and may also have benefits for blood sugar management and relieving symptoms of menopause. People also take it because it contains antioxidants, as well as nutrients like vitamin K, copper, folate, and magnesium.\nDoes alfalfa reseed itself?\nAlfalfa is a crop that naturally tends to lessen its density over time because alfalfa doesn’t reseed itself. Planting more alfalfa into current alfalfa fields is not recommended since alfalfa has autotoxicity characteristics. A good population/density is about 35 well-growing stems per square foot.\nIs alfalfa easy to grow?\nEasily grown and propagated, alfalfa adapts well to nearly any garden, tolerating a wide range of growing conditions. It makes a good drought-resistant plant too, as it doesn’t like wet feet. In fact, too much moisture can lead to mold growth. When growing alfalfa, choose an area with plenty of full sun.\nWhy are alfalfa sprouts bad for you?\nSprouts are a known source of foodborne illness . Reduce your risk of getting sick from sprouts. Children, older adults, pregnant women, and people with weakened immune systems should avoid eating raw sprouts of any kind (including alfalfa, clover, radish, and mung bean sprouts).\nHow many years will alfalfa last?\nAlfalfa can remain productive in stands from four to ten years or more, but as plant population declines renovation eventually becomes necessary.\nHow many years does alfalfa last?\nHow many times a year can you cut alfalfa?\nMaximum dry-matter yield of alfalfa and most forages is often obtained by harvesting the first cutting of the season at nearly full bloom and harvesting subsequent cuttings at 40 to 45 day intervals until late August or early September, referred to as a “3 summer-cut system.” This system produces forage relatively …\nWhen should you not cut alfalfa?\nIf the alfalfa stand hasn’t been allowed to flower at least once during the growing season, no matter the variety, it’s at a much higher risk for winterkill. It shouldn’t be harvested after the four- to six-week period before a killing frost.\nAre alfalfa sprouts toxic?\n(The toxin in alfalfa sprouts is canavanine, which can harm the immune system.) Aside from that concern, alfalfa and other types of sprouts may be contaminated with disease-causing bacteria.\nWhat happens if you don’t mow alfalfa?\nIf it stays dry those new shoots could die off. Once your alfalfa does receive enough rain for regrowth it might regrow a little faster if it’s cut, but it also will regrow following sufficient rain without cutting. In most cases, it’s not worth the cost and time involved to shred, clip, or harvest low-yielding hay.']	['<urn:uuid:97c80b91-6d46-4f66-8742-ad9021d019d7>', '<urn:uuid:fe7b1521-b162-4248-b8f6-6e46cd03bd21>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T04:56:11.774636	20	93	2063
38	how does balanced scorecard measure business success four main areas	The Balanced Scorecard measures business success through four key areas: 1) Financial measures - how the firm looks to shareholders/creditors, including metrics like return on equity and cash flow, 2) Customer measures - how the firm appears to customers, including factors like value for money and customer service, 3) Internal Business Processes - identifies the most important processes for meeting shareholder and customer expectations, 4) Learning and Growth - identifies the infrastructure needed for long-term growth, including aspects like innovation and employee capabilities.	"[""Key Concepts Managerial Cognition Business Model Stakeholders The Balanced Scorecard\nManagerial Cognition Involves managements’ beliefs and understanding about: Macro/Industry Environment (e.g., demographic trends, competitors) How to Compete (e.g., Strategic positioning: low cost, differentiation, focus) Appropriate Size and Diversity ( e.g., integrated healthcare systems) How to Organize (organizational structure)\nCognitive Biases in Strategic Management Examples: Confirmation Bias Availability Heuristic Non rational Escalation of Commitment Groupthink\nBusiness Models Answers the following questions: How do we make money in this business? What is the underlying economic logic that explains how we can deliver value to customers at an appropriate cost? Consider the Following: Who is the customer? What does the customer value? What activities will be performed? How will these activities be performed?\nLook at the Components of the Business How are Revenues Generated? How Does the Firm Lower Costs? Greater Profitability\nDell Corporation’s Business Model Primary Revenue Streams: Direct Sales Model for Individual Customers Relationship-Oriented Field Sales Force for Corporations and Institutional Customers Services (e.g., Training and Certification, Support, Professional, Deployment, etc.) Multiple Product Lines in Different Segments of the Industry (e.g., enterprise systems, client systems, printing and imaging systems, software and peripherals) Financial Services (Financing) ***Focused R&D and Branding via advertising\nDell Corporation’s Business Model Direct Sales Model and Extensive Market Segmentation Build to Order Manufacturing and Mass Customization Extensive Testing and Quality Control Partnerships with Suppliers and JIT Practices E-Commerce Technology (Sales and Service Support) Costs are Lowered Through:\nWhat are stakeholders? Who are the different organizational stakeholders? What are some of the objectives of these stakeholders?\nImpact of Stakeholders Relative influence and power over issues and initiatives Participation in strategic management process Expertise and validation Problem-solving and collaboration\nBalanced Scorecard Four Types of Measures: Financial: How does the firm look to its Shareholders/Creditors? Customer: How does the firm look to its Customers? Internal Business Processes: Identifies processes most important to meet shareholder and customer expectations. Learning and Growth: Identifies the infrastructure that the firm must build to create long-term growth.\nFinancial Measures Return on Equity Cash Flow Return on Assets Return on Investment Project Profitability\nCustomer Measures Value for money Competitive Price Customer Service Customer Retention Market Share Image and Reputation\nInternal Business Processes Superior Project Management Product development cycle Innovation Product Quality Measurement Order Process Time\nLearning and Growth Product/Service Innovation Empowered Workforce Information Systems Capabilities Motivation Employee Capabilities, Satisfaction, Retention, Productivity\nMission Critical Success Factors Critical Measures Financial Customer IBP L & G\nMission North Oakland Medical Centers is a multi-divisional urban healthcare system whose mission is to deliver high quality healthcare and health related services in northern Oakland County in a cost effective and efficient manner.\nFinancial Critical Success Factors and Measures Critical Success Factors: Increase in capitation contracts Reduce emergency room use Expand community philanthropy Critical Measures: Number of capitation contracts received % of capitation contracts relative to competitors % unnecessary emergency room usage Dollars raised (numbers and dollars of corporate donations)\nCustomer Critical Success Factors and Measures Critical Success Factors: Prompt Service High Quality Care Staff Attitude and Friendliness Good Food Critical Measures: Emergency room and Admission times Patient Referrals/Repeat Patients and External Ratings Patient Satisfaction Surveys Number of Complaints\nInternal Business Perspective Critical Success Factors and Measures Critical Success Factors: Cost Control Efficiency Selected Specialization Quality of Care Critical Measures: Cost per patient day Analysis of use of equipment and space, degree of automation Cancer, heart (provide value for the cost) Evaluations of doctors, staff and administrators, patient satisfaction\nLearning and Growth Critical Success Factors and Measures Critical Success Factors: Doctor research and creativity Continuous innovation State of the art technology Partnerships with research institutions Critical Measures: Number of professional presentations and publications Number and quality of new services offered in past 5 years Degree automation, expenditures on software/hardware, rate of increase of outpatients Number of joint activities\nOakland University Who are Oakland’s stakeholders? What are some of the objectives of these stakeholders?\nHow are these objectives linked to performance? How should performance be measured? What needs to be measured? How should the SBA best measure performance based on their stakeholders’ objectives?\nSBA Mission Statement The mission of the School of Business Administration (SBA) is to advance knowledge and enhance students' abilities to manage in a global business environment. The mission is achieved through a synergistic combination of teaching, scholarship, and professional service, with emphasis on the linkage of theory and practice, and the application and management of technology. Toward the achievement of these ends, the SBA promotes collaborative relationships among students, faculty, administrators and employers.""]"	['<urn:uuid:5fcd6cd2-224e-45e4-b2cc-393df737ed50>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	10	83	753
39	How do modern science fiction movies build on older ones?	Modern science fiction movies derive about 99% of their story elements from key benchmarks of past stories or films. Particularly after Star Wars in 1977, the remaining 16 entries in the book all rely significantly on films of the past, sometimes appearing to be merely another twist on earlier films.	['Review by C.J. Bunce\nAny list of 10 or more items these days quickly becomes the stuff of argument. But in the right context it can become the stuff of discussion and curiosity. A list of 50 items takes some work to prepare and if that list accompanies a genre that has spanned more than a century, then it really invites discussion. Which brings us to Turner Classic Movies and Running Press’s new look at the science fiction genre in Sloan De Forest’s Must See Sci-Fi: 50 Movies That Are Out of This World. This latest pop culture book to engage science fiction fans may show that, after all these years, the best and most important works of science fiction are not really all that controversial. Yet it wouldn’t really be worth picking up if it only confirmed readers’ love for epic films. Must-See Sci-Fi takes that next step and also serves that need of all fans of film to take another look at the classics and be open to those films we may have overlooked.\nConsisting of 50 approximately 1,000 word essays on each film across 114 years, from 1902 to 2016, Must-See Sci-Fi covers the significance of each film selected in its 280 pages, including a plot overview, key memorable scenes, plus some good behind-the-scenes trivia, as well as plenty of color and black and white photographs. From A Trip to the Moon in 1902 to Arrival in 2016, the book has a fairly consistent coverage (but weighted with more selections from the 1970s and 1980s, and the 1940s have no entries). Most will agree with the films included from George Méliès’s groundbreaking beginning through the 1960 adaptation of H.G. Wells’ The Time Machine. But controversial for one person may not be controversial for another. De Forest presents her case for those films you might not find on other lists–many firsts of sci-fi emphasized instead of the definite look at a sub-genre, like Alphaville, Solaris, Sleeper, The Man Who Fell to Earth, THX 1138, The Brother From Another Planet, A.I.: Artificial Intelligence, or Eternal Sunshine of the Spotless Mind. One great feature is a recommendation of two “watch-alike” films after each section–If you loved a film, you have two more films to track down and compare, and if you missed a film but don’t like the two suggested films, the book may telegraph your level of enjoyment once you screen the entry. Readers will also see the impact across a century of filmmaking from Mary Shelley’s Frankenstein and the works of both H.G. Wells and Jules Verne on these selections.\nKey to the fun of delving into science fiction film history is understanding the roots of science fiction–how modern science fiction 99% of the time derives (or combines) its story elements from key benchmarks from stories or films of the past. As the book progresses readers can see author De Forest frequently referring back to those sources, and after 1977’s Star Wars the remaining 16 entries all seem to rely significantly on films of the past–sometimes they even appear to be merely another twist on one of the films in the first half of the book. And yes, readers will find new discussion topics. La Jetée may be an incredibly fascinating short film, but is it more of a “must-see” than Terry Gilliam’s update 12 Monkeys? And how did a Woody Allen movie ever make the cut?\nEvery few years a new book attempts to unearth those movies that are integral to understand the scope and breadth of science fiction in film, like 101 Sci-Fi Movies You Must See Before You Die (2016), Top 100 Sci-Fi Movies (2011), Sci-Fi Movie Freak (2011), and this year’s book that accompanies the TV series, James Cameron’s Story of Science Fiction, which primarily covers films but also shares its space with sci-fi novels. Some of these chronicles are brief and come in magazine form, others are more scholarly in their reach. Some try to catch them all, like The Encyclopedia of Science Fiction Movies, and others don’t try hard at all, like Cult Sci-Fi Movies: Discover the 10 Best Intergalactic, Astonishing, Far-Out, and Epic Cinema Classics. One book even focuses on the sci-fi films that didn’t make it out of the studio: The Greatest Sci-Fi Movies Never Made (2008). Must-See Sci-Fi, a worthy follow-on to the 2016 broader work, Turner Classic Movies: The Essentials: 52 Must-See Movies and Why They Matter, has its own take on sci-fi history and a fresh approach to separate it from the rest.\nMust-See Sci-Fi is a fun addition to the sci-fi film bookshelf, a way to revisit some of those classics you love, and learn about some movie suggestions to track down for films you may have missed. Look for a foreword by Roger Corman, plenty of photographs and some rare behind-the scenes images, and a list of cited sources at the back of the book. New from Running Press and in an affordable paperback edition, TCM’s Must-See Sci-Fi: 50 movies That Are Out of This World is available now here at Amazon.']	['<urn:uuid:2acae6e3-1a6a-424a-bfd7-90b86a85d66f>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	10	50	847
40	traditional irish pumpkin carving vs modern american halloween pumpkin carving differences	Traditional Irish carving was originally done with turnips, not pumpkins. When Irish immigrants came to the US, they switched to pumpkins because they were more readily available and easier to carve. Modern American pumpkin carving has evolved into an art form, with artists like Ray Villafane creating lifelike sculptures, using specialized tools and techniques to remove layers of pumpkin surface to create features, compared to the simple lantern carvings of the past.	['As you are busily gearing up to get out and celebrate Halloween with your friends and family, perhaps you’ve actually wondered where on earth did this somewhat bizarre holiday come from? Who thought of trick or treating? Why do we dress up in costumes anyway? For all you curious folks, we’ve compiled a top 10 listing of the most important details of the history of Halloween. The holiday has changed a lot through history but one fact will always remain the same, Halloween is one of the most beloved holidays we celebrate every year and is considered to be the 2nd most commercially successful holiday celebrated in the U.S. As you don your best Halloween costume and hit the streets to do some serious trick or treating, here are some fun facts for you to keep in mind!\nPumpkins: Pumpkins are plentiful in the U.S. and are actually indigenous to the Western hemisphere. Normally harvested in October, pumpkins are most commonly orange, but these members of the gourd family also can be found in green, yellow, white, blue and tan! When the Irish immigrated to the U.S. they had been formerly making lanterns with scary faces in them out of turnips to use on ‘All Hallows Eve’, but they quickly switched to pumpkins because they were far easier to carve than turnips and were more readily available. A Halloween tradition was born!\nJack-o-Lanterns: The legend of the Jack-o-Lantern is actually based on a man named Jack. Legend states that a miserly, angry as well as evil man named Jack liked to play pranks on his fellow townspeople. One day Jack decided to mess with the devil himself and tricked him, trapping him up in a tree. Jack circled the tree trunk with crosses and other religious symbols and would not let the devil escape until he agreed that he would never take Jack’s soul into hell. Of course the devil had no choice but to agree to this bargain and he was allowed to go free. When Jack eventually died, he was far too evil to go to heaven but the devil was true to his bargain and did not take Jack’s soul into hell. Jack was forced to roam the earth for eternity. The devil did leave Jack with an endlessly burning ember, which Jack placed inside a turnip lantern to carry on his way as he eternally searched for his final resting place. The tradition of the lit Jack-o-Lantern carries on to this day!\nHalloween Parties: Back in ancient days, villagers would gather on All Hallow’s Eve for a celebration called Samhain, which marked the end of the Harvest and the beginning of their new year, which was November 1st. Large bonfires would be made and various rituals would be performed around these, including throwing the bones of slaughtered animals into the fire as an offering. This celebration was also thought to honor the dead, which these people believed could roam the earth freely in spiritual form on this night. It became commonplace to dress as one were dead in an effort to fool the spirits in hopes that any malevolent amongst them would leave them alone.\nImmigrants: Most of the traditions we have surrounding Halloween come from the Irish or other European immigrants who brought their ancient customs with them when they immigrated to the U.S. Halloween is an eclectic mixture of customs of the Irish, Catholic and the Romans. In particular, the highly superstitious Celts gave us many of the ideas for what we now know as Halloween. The Celts believed that the lines between the living and the dead were completely blurred on October the 31st and they were fearful of any mischief or problems that might be caused by these spirits when they returned.\nGhosts: Because of the ancient beliefs that the spirits of the deceased were out running rampant on All Hallows Eve, Halloween will always have a close association with ghosts and spirits. Throughout history it has been common to tell ghost stories and folk tales involving spirits around the time of Halloween. These traditions have carried on today and it is common for people to create and visit attractions that are designed to appear ‘haunted’ as part of a way to celebrate Halloween.\nMonsters: Aside from ghosts, over the years many other monsters have become associated with Halloween. Two of the most common are vampires and werewolves. Vampires are part of ancient folklore that tried to explain away why random plagues would cause groups of mysterious deaths in various villages. Superstitious people thought that these deaths must be due to a newly deceased person being actually ‘undead’ and rising from the crypt in order to feast on the blood of the living in order to maintain their life force. Potential werewolves, another mythological being, were picked out from amongst others by features such as uni-brows, hairy palms and having a middle finger that was longer than the other digits.\nCostumes: The first Halloween costumes were made in Ireland and were made of various types of make up and animal heads or skins. These costumes were made in an effort to ward off evil spirits, but during the celebration of All Hallows Eve, were also used in the ceremonies that were performed during various festivities. Fortunes were often told and it was common for various rituals to be performed during this time. Nowadays, although scary costumes are still worn on Halloween, the variety of costumes you will see cover many different categories including superheroes, pop stars, humorous costumes and historical figures as well.\nWitches: The word witch comes from the Old Saxon word ‘wica’, which actually means ‘wise one’. In ancient days, witches were originally considered to be healers who were very familiar with the use of herbs and other remedies to help the sick and otherwise afflicted. As Christianity spread throughout Europe, the church clergy unfairly labeled these ‘healers’, calling them devil worshipers or sorcerers, even though that was hardly the case. Early immigrants that still believed these old ancient superstitions regarding witches brought these rumors with them into the U.S.\nTrick or Treating: In the ancient days of Scotland and Ireland, it was common practice on All Hallows eve for the poor or less fortunate to go to the homes of more affluent people and offer prayers for the dead in exchange for food or money. This practice was once again brought to the states by immigrants and eventually evolved into the custom of trick or treating, as we know it today.\nCandy: Candy is the preferred ‘treat’ of trick or treaters, the most favored of all being chocolate bars. The number one chocolate bar favorite amongst trick or treaters is Snickers. Consumers will spend over two billion dollars on Halloween candy this year. Over twenty five percent of all candy purchased annually in the U.S. is bought for Halloween purposes.\nHalloween is one of the most beloved holidays out there. Knowing a little bit of the facts surrounding its magical history will help you to understand and enjoy it all the more! Happy Halloween!', 'As the days get shorter and the leaves changes color, it’s time for atmospheric pumpkin decoration. Here we have put together tips and tricks for pumpkin carving, so that the scare does not start when carving.\nIt’s all about the pumpkin\nPumpkins differ not only in terms of look and taste, but also in terms of shell hardness, thickness and flesh consistency. For Halloween decorations and carved pumpkins large, soft-shelled pumpkins with thin wall thickness are first choice. The varieties “Halloween” and “Jack o’Lantern” are particularly easy to carve and are easily available during the season. The 30-40 cm large specimens are perfect for carving with children and for classic pumpkin lanterns. Their disadvantage: Those pumpkins taste watery and bland and are unsuitable in the kitchen.\nSquashes like Red Kuri look gorgeous, but are often very hard and difficult to work with, especially for children. Adults achieve great results with Red Kuri.\nButternut squashes are especially suitable for superficial decorations e.g. with pumpkin carvers. They have a soft shell, a lot of flesh and only a small core casing. The hollowed out meat can later be used for delicious dishes.\nFor artists, it is worth taking a look at unusual pumpkin varieties. For example thick-fleshed and interesting-looking varieties such as Blue Hubbard are perfect for 3D carving of ornate faces and sculptures.\nPick pumpkins with a firm, smooth surface. Soft, wet or transparent looking spots indicate spoilage. Pumpkins with long stems later look particularly beautiful and full of character. Well healed scratches and scars on the pumpkin are not necessarily a blemish. On the contrary, they give the pumpkin face more personality.\nThe classic: carving a pumpkin lantern\nCarving pumpkin lanterns is great fun for the whole family because children and parents can work together. Especially younger children often need a lot of help to handle the heavy pumpkin. From the age of 3, many children are enthusiastic about helping their parents to scoop out seeds and help to choose what the face should look like. At the age of 10, children can handle the pumpkin all by themselves. Plan about 45 minutes for carving. With several children and pumpkins you will need a little longer.\nDecisive is the choice of the right tool. Anyone who has ever worked a pumpkin with a kitchen knife and tablespoon knows that it is not only exhausting and tedious, but can become really dangerous. Therefore, you should not save on a stable and versatile pumpkin saw and a suitable, sharp-edged spoon.\nAnd this is how to approach the pumpkin:\nYou need to make a lid. You can open it either circular, wavy or zig-zag. The easiest way is with a v-shaped angle cutter or with a pumpkin saw. The opening should be not too small, so that you have enough space when recovig the seeds later.\nTip: If you cut at a 45 ° angle towards the center of the pumpkin , the pumpkin lid will later fit particularly well and you will reach the inside more easily.\nThe seeds and excess pulp must be removed. To do this, scrape off the fibers and seeds from the pulp with a sharp-edged spoon. Where eyes, mouth and other cutouts should be, scrap the wall thinner until only about 3 cm (1.2 inch) remain.\nTip: You do not have to throw away the seeds of many pumpkin varieties. Dried and roasted in the oven they are a delicious snack.\nNow it’s time to get creative. If you use templates, you can attach them with pins or scotch tape to the pumpkin and pierce the outlines with a needle.\nIf you prefer to draw freehand, you can use a waterproof felt-tip pen or a regular pen. If you do not press too hard, you can erase it later with the scrubbing side of a dish sponge. If the pen goes on strike because fruit wax has accumulated, you simply clean the tip with a tissue.\nTip: Big “cartoon eyes” look very nice . For the eye is only superficially exposed with a pumpkin carver and only the pupil is cut out.\nStep 4 – Cut out the face\nIf the face is mapped out, you can start cutting out the openings for the eyes, nose and mouth with a pumpkin saw. To do this, the saw is simply pricked into the pumpkin and moved back and forth in the cutting direction with quick movements and slight pressure. A lot of power is not necessary at all, just keep on sawing with light and fast movements.\nStep 5 – The fine-tuning\nFinally, you can let the imagination play again. Eyebrows? Hair? Ears or scary scars? Such details give character to the pumpkin and are made quickly with special tools such as e.g. Pumpkin Carvers.\nIf you use pumpkin carvers to clear surfaces, you can play with the thickness of the pulp. The thinner the flesh behind the exposed surface, the stronger the light will shine through later.\nStep 6 – Illuminate\nThe pumpkin lantern is finished and ready for lighting. Whether with a tealight or a special LED light is left to personal taste. If you choose a tealight, make sure that the pumpkin is big enough so that there is enough space between the flame and the lid.\nExtend your pumpkin’s life\nUnfortunately, pumpkin lanterns stay not fresh for too long. Depending on the weather, they only last 2-6 days. Therefore, it is best to carve them just before Halloween. While various tips circulate to extend the life of the pumpkin lantern, such as spraying the pumpkin inside with hair spray you cannot stop the spoilage for more than 1-2 days .\nUp to 6 weeks fresh stay pumpkins which are only superficially decorated. If the core housing is not damaged and only the shell is scratched or peeled off a bit, the surface dries and forms again a solid shell. It prevents the penetration of germs and thus the spoilage. Such pumpkins can even be used later in the kitchen.\nThere is one option, however, to preserve small pumpkin art works permanently. All you need is a tight-fitting glass jar into which the pumpkin fits and lots of clear white vinegar. Freshly carved, covered with vinegar and hermetically sealed, the creepy works of art can survive many months and sometimes even years. If the vinegar becomes cloudy, carefully remove the pumpkin, clean the vessel and put it in fresh vinegar. This trick works best with hard-flesh pumpkins and sweet potatoes.\nSuperficial carvings – unusual and long lasting\nVery nice are pumpkins which are decorated only in the skin. In addition they are – if the weather and the quality of the pumpkin allow – quite long lasting. If the cuts in the shell are not too extensive, they dry up and the punpkin is protected from spoiling. The carved pumpkin can remain unlit but with lighting it is a very special eye-catcher. But it makes it much less durable.\nTo achieve the special effect, the pumpkin carvers remove only the shell in the desired places and maybe also a little flesh. With a 2-piece Pumpkin Carver Set one can already achieve great results. For more depth and dimension you can also use another pumpkin carver and other tools.\nStep 1- The planning\nWhat should the pumpkin look like? How many levels should it have? Different levels provide great 3D effects, especially with illuminated pumpkins. Like the pumpkin lantern you can draw it with a suitable pen or use paper templates.\nStep 2 – The outlines\nWith a V-shaped pumpkin carver, all lines and contours are cut out first. The sharp shape of the cut ensures impressive light and shadow effects later .\nStep 3 – Expose all areas\nWith the U-shaped carver you now remove the skin inside the outlines. Since it does not cut as deeply as the V-shaped carver, it creates an interesting 3-D effect.\nIf you want to work with several levels, the deepest level should be made first. For this use e.g. the pumpkin carver U-long.\nStep 4 – Illuminate (optional or anytime later )\nIf you want to illuminate the pumpkin, open it either from the bottom or from the lid, remove the core casing and excess pulp. The thinner you make the pumpkin wall at your carving, the better the effect later. But so not overdo it because the stability will suffer.\nTip: If you use a candle and open the pumpkin from the bottom, you should also provide a small air opening at the top.\nIf you take a look in the USA, where Halloween is celebrated big, then you quickly find out that pumpkin carving does not end with illuminated lanterns. There are true artists who make incredible things out of the big vegetables.\nRay Villafane, founder of Villafane Studios, is such an artist who carves incomparably lifelike, creepy and often amusing sculptures out of pumpkins. The technique behind it is not that difficult and surprisingly easy to learn even for beginners. We tested it in one of his workshops and loved it.\nWith scraping movements, the surface of the pumpkin is removed layer by layer until the desired features are created. Then sharp contours are drawn with fine tools, so that light and shadow transform the pumpkin into a living being.\nOf course, a kitchen knife is not enough here. In addition to the right tools, such as the Villafane Scupting Set, we recommend looking at the internet and Ray Villafane’s great step-by-step tutorials.']	['<urn:uuid:415ca75b-a9c5-470d-94c4-d87d44fb87fa>', '<urn:uuid:655d6191-6b6a-4813-91ae-2aef50e8ffd3>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T04:56:11.774636	11	72	2773
41	What resources support underrepresented students at University of Minnesota?	The University of Minnesota offers several resources including the Community of Scholars Program, Office of Diversity in Graduate Education, graduate student community organizations, Twin Cities Multicultural Resources and Services, Office for Equity and Diversity, and specific fellowship programs like the Diversity of Views and Experience fellowships and the Interdisciplinary Center for Global Change study.	"['At the EEB graduate program, we work to create a welcoming, collaborative and supportive community. The graduate program strives to and succeeds in providing stable funding to students throughout their PhD. Students develop their own research interests and directions, and freely take input from faculty all across our department and university. Research in ecology spans the theoretical, experimental and applied, across many ecosystems and scales. Below is a quick guide to applying here for graduate school (which also will help you apply to other places), followed by descriptions of some students whose work exemplifies the diversity of people and research interests in our graduate program.\nWe at the Graduate program in Ecology, Evolution and Behavior are very aware of the serious underrepresentation of many minority groups in our fields. We are committed to doing our part in making our program, and our fields, aware, inclusive and responsive to people of diverse origins and experiences. We have taken this on as a responsibility to our histories, our science, and our communities.\nThe University of Minnesota also has several excellent resources for supporting students from underrepresented groups through their graduate education. Please take a look at the Community of Scholars Program, Office of Diversity in Graduate Education, various graduate student community organizations, Twin Cities Multicultural Resources and Services, the Office for Equity and Diversity, and specific fellowship programs like the Diversity of Views and Experience fellowships and the Interdisciplinary Center for the study of Global Change. The Boreas Leadership Program also provides graduate students opportunities to build skills and training for public engagement and leadership.\nApplying to graduate school in EEB is different than applying to college. While you would usually apply to a program or school for undergrad, for graduate school you often apply to work with a particular faculty member. This requires that you connect with faculty before applying, in order to determine whether they are a good match with your interests as well as funding availability.\nBelow is a timeline that will help you navigate the process, as well as other resources from students in the program:\nSummer (one year before start date): Start looking through the faculty members in the program. Look through their lab or personal websites and think of whose research matches your interest and you find exciting. Keep in mind that faculty members are often associated with multiple programs and departments, so they might advise students in EEB even if their appointment is in a different department.\nLate summer/early fall: Contact faculty members you are interested in working with. You can often find instructions in the best way to contact them in their websites. Some prefer a call, while others prefer an email or letter with certain information such as a CV or transcript attached. If they don’t reply or answer the first time, try again, your email might have gotten lost in their inbox!\nFaculty will reply back with information on whether they have space and funding to take on a student. If they are not taking students that year, it is fair to ask them if they know of other faculty who they think might be a good match for you.\nFall Semester: Follow up with faculty members who seemed interested in taking you as a student. Set up a phone or Skype interview. This will allow both of you to figure out whether you are a good match for their lab and whether you should apply for the program. You can also contact current or past students in the lab if you have questions for them.\nTake your TOEFL (if applicable), and apply for fellowships such as the NSF GRFP. Ask for recommendation letters from previous mentors or professors who can speak to your ability to succeed in graduate school.\nLate Fall Semester: Turn in your application and other related materials!\nEarly Spring: You will be invited for interviews at schools interested in taking you as a student. This will be an opportunity for your potential adviser and the department to get to know you in person, as well as for you to see what the school and students are like. It is also a great opportunity to learn about funding options and other opportunities.\nSpring: Admission decisions are made and you will have until April 15 to make a final decision. Don’t be discouraged if your application is not successful. This can be due to limited funding and it is not necessarily indicative of your potential as a student.\nAlso, here is a document with more information and tips on the application process and graduate school in general.\nCristy Portales Reyes (Community ecologist studying temperate prairies)\nEEB Doctoral Candidate, Adviser: Forest Isbell\nWe know that chronic nitrogen enrichment causes diverse prairies and grasslands to become dominated by only one or two plant species, which are often exotic. These changes in diversity can persist even decades after nitrogen enrichment stops. I am working to understand what mechanisms keep biodiversity from recovering after nutrient enrichment. I am particularly interested in learning how above and below ground processes might interact to stabilize this alternative state in biodiversity. For example, mutualistic microbes might become ""cheaters"" under elevated nutrient levels, causing them to act more like parasites than mutualists to native plant species. So, how could we revert these grasslands to a high diversity state? We don\'t entirely know, but I hope that by understanding these stabilizing mechanisms we can find better strategies to help biodiversity recover.\nShanta Hejmadi (Evolutionary ecologist studying community assembly and extinction risk in birds of prey)\nEEB Doctoral Student, Adviser: Keith Barker\nI love birds. My research focuses on an especially charismatic group of birds, diurnal birds of prey in the orders Accipitriformes (Hawks, Eagles, Vultures) and Falconiformes (Falcons and Caracaras). These amazing birds occur all over the world, and have evolved convergently to their raptorial lifestyle and morphology. I am interested in the evolutionary history of raptors, how that history and their functional ecology have shaped communities of raptors all over the world, and how we can harness such data to predict extinction risk.\nI do most of my data collection in museums, which allows me to collect genome-quality DNA and morphometrics on birds from all over the world, including rare ones that I might never find in the wild. I am deeply committed to mentoring young scientists from diverse backgrounds, and I include undergraduate projects in my research at every opportunity.\nRachel Olzer (Studies behavior in crickets and its rapid evolution)\nEEB Doctoral Candidate, Adviser: Marlene Zuk\nI am currently a PhD candidate in the Department of Ecology, Evolution, and Behavior. My research is focused on understanding rapid evolution of alternative mating behaviors in populations of the Pacific field cricket located on the islands of Hawaii.\nIn addition to my primary research focus, I am interested in understanding diversity and equity in science through the lens of critical race and feminist theory and. Overall, I am interested in understanding the history of race relations in the United States- particularly the history of violence against and disenfranchisement of African Americans. I am currently focusing on the history of scientific racism in the collective fields of Ecology, Evolution, and Behavior.\nLike many scientists in the fields of EEB, I came to this field through a love of nature. I am interested in how early exposure to the outdoors affects career decisions early in life. Because of the fraught history of racism in the United States, many African Americans, have not had access to spaces where they could freely explore in nature. As such, my outreach focuses on nature-based play and the connection between recreating in the outdoors and developing a critical lens for studying the natural world.\nDanielle Drabeck (Evolutionary biologist studying molecular co-evolution between snakes and opossums)\nEEB Alumna and Postdoctoral Scholar, Advisers: Sharon Jansa and Antony Dean\nMy research focuses on understanding co-evolution and co-adaptation at the molecular level by studying a tribe of South American opossums on this larger goal by studying a tribe of South American opossums that prey upon Bothrops jararaca, a South American pit viper, and are resistant to their venom. B. jararaca venom is known to contain a C-type-lectin (CTL) protein, Botrocetin, which specifically targets the blood protein vWF (von Willebrand factor), and causes excessive systemic bleeding. Resistant opossums show accelerated evolution at the vWF- botrocetin binding site when compared to non-resistant species, suggesting an adaptation which blocks Botrocetin activity. I use physiological and biochemical assays to understand the potential coevolutionary relationship between these opossums and their venomous prey, by focusing on the interaction between vWF and Botrocetin-type venom components. Utilizing a wide breadth of techniques such as molecular biology, phylogenetics, and biophysics, I aim to understand how these proteins interact, and how coevolution might have shaped their physiological and kinetic properties.']"	['<urn:uuid:44a3b36e-45fc-4c5d-a45b-19c1e9ded7d7>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	9	54	1468
42	What's the timing of Tamil and Myanmar New Year, and how do people celebrate them?	Both Tamil and Myanmar New Year occur in April. The Tamil New Year falls on April 14th (or April 13th in leap years), while Myanmar's Thingyan festival runs from April 13th to 16th, with New Year's Day on April 17th. In Myanmar, people celebrate with a water festival where participants douse each other with water using guns, buckets, or cannons, along with traditional dances and music performances. They also focus on Buddhist traditions like offering food to monks and performing acts of kindness. For Tamils, the New Year is calculated based on astronomical events, specifically when the sun enters the constellation of Achchuvini. It's celebrated as a secular festival and is shared with other communities like the Sinhalese and Malayalis.	"[""Tamil New Year\n[TamilNet, Sunday, 13 April 2008, 02:30 GMT]\nThe Tamil New Year day falls on Sunday, 13th April, since 2008 is a leap year. On other years, it is 14th April. It is not only the Tamil New year, but also the New Year of Sinhalese and Malayalis. It is not appropriate to call it a Hindu New Year since vast majority of Hindus in India don't celebrate it a New Year. It is only an auspicious day called Mesha-samkraanti for them. But, the Sikhs celebrate it a New Year. It is equally inappropriate to call it a Buddhist New Year, as Buddhists don’t celebrate it universally. The calculation for the commencement of this New Year and the calendar based on it are entirely astronomical.\nOpinion Columnist Ampalam\nThe Tamil New Year day falls on Sunday, 13th April, since 2008 is a leap year. On other years, it is 14th April.\nIt is not only the Tamil New year, but also the New Year of Sinhalese (Alut Avurudu\n) and Malayalis (Vishu\n), even though the Kollam\nEra of Malayalam begins from the month of Aava'ni\nIt is not appropriate to call it a Hindu New Year since vast majority of Hindus in India don't celebrate it a New Year. Yet, it is an auspicious day called Mesha-samkraanti\nfor them. In Orissa, it is observed as Paana-samkraanti.\nThe Sikhs and some Assamese also celebrate it as New Year under different names.\nIt is equally inappropriate to call it a Buddhist New Year, as Buddhists don’t celebrate it universally.\nIn Sri Lanka, since both Sinhalese and Tamils share this occasion of New Year, it has been termed as Sinhala-Tamil New Year. In recent times, there is also a tendency to term it as Sinhala-Hindu New Year by certain sections.\nSince the people who practice this New Year are predominantly Tamils in Canada, the Canadian Prime Minister chose to term it as Tamil New Year and established a practice of wishing Canadian Tamils well on the occasion, in recent years. We can be sure that the Canadian Prime Minister must have also wished the Canadian Sikhs on this occasion choosing the term Vaisakhi.\nThe calculation for the commencement of this New Year and the calendar based on it are entirely astronomical.\nThere are two ways of reckoning the Year in the astronomical texts of South Asia: one is Solar (Sauramaana\nin Sanskrit) and the other is Lunar (Caandramaana\nin Sanskrit). The calendar and terminologies are mostly a combination of both (luni-solar).\nThe commencement of New Year observed by Tamils, Malayalis and Sinhalese is a solar reckoning, but it also involves a stellar perspective. In this calculation, the entry of the Sun into the first degree of the Fixed Zodiac (Nirayana\nin Sanskrit), i.e., the first degree of the constellation of Achchuvini\nor the first degree of the zodiacal sign of Aries (Mesha\nin Sanskrit and Thakar\nin Tamil) is taken as the beginning of the year.\nMost of the Hindus observe the Lunar New Year, based on the number of full moon and new moon days, which is commonly known as Yugaadhi\n). It falls in the solar month of Pangkuni\n(March-April), but its corresponding date with the Gregorian calendar, the Western calendar widely used in the world today, varies.\nThe co-existence of both the solar and lunar systems and even related controversies are known from the times of Mahabharata. When the Pandavas reappeared after their banishment of 13 years, Duryodhana countered them saying that the 13 years were only lunar years and they had not fulfilled the requirement according to solar years and therefore should get into banishment again. Bhishma had to intervene to say that 13 solar years were also over and Duryodhana's almanac was wrong.\nAnyway, for some reasons the Solar New Year found much currency in the southern parts of South Asia. It is associated with the Salivaahana\nEra, the reckoning of which is found predominantly in the inscriptions of South India and Sri Lanka. The Era begins in 77-78 AD.\nThe Solar New Year in April is also associated with the 60 years cycle that has rendered names for the years. This cycle is based on the calculation of the relative positions of Jupiter and Saturn, the two major planets of the solar system, supposed to be exerting special impact on the climate and natural phenomena in a year, in addition to the usual impact of earth's revolution around the sun. In this cycle, the relative positions of Jupiter and Saturn will repeat after 60 years.\nOnce again, the 60 years cycle is peculiar to Tamil astronomical texts. The names of the years are widely used only by the Tamils. The system is essentially a southern innovation despite the names being in Sanskrit.\nEven though the beginning of the year and the beginnings of the months are calculated on the basis of the sun's transit, it should be noted that the names of the months in Tamil indicate their origins from lunar calendar.\nThe names of the months originated from the names of the constellations in which the full moon will occur in that month. Chiththirai:\nfull moon in Chiththirai\nfull moon in Visaakam\nconstellation (α, β, γ, ι of Librae)\nfull moon in Moolam\n) constellation (ε, ζ, η, θ, ι, κ, λ, υ, μ and ν Scorpii)\nfull moon in Uththaraadam (Uththara-Aadam) constellation (ζ and σ Sagittarii)\nfull moon in Aviddam\nconstellation (α to δ Delphini)\nfull moon in Pooraddaathi\nconstellation (α and β Pegasi)\nfull moon in Achchuvini / Asvini\nconstellation (β and γ Arietis)\nfull moon in Kaarthikai\nfull moon in Mirukaseeridam\nconstellation (λ, φ Orionis)\nfull moon in Poosam\nconstellation (γ, δ and θ Cancri)\nfull moon in Makam\nfull moon in Uththaram\nThese full moon days are festival days in the classical and folk traditions of South Asia since time immemorial. Note how they mark the festival days in the Tamil tradition. For example the full moon day in Pangkuni\nwas celebrated in the sandy stretches of the Kaveri delta, as Pangkuni Muyakkam\n, according to Changkam\nliterature. Today, it is the most important annual festival day in the Siva temples of Tamil Nadu and Sri Lanka as Pangkuni-uththaram\nTherefore, the Tamil calender is actually Luni-Solar, despite its base of reckoning is in the sun's transit. This is because, accuracy is achieved better in calculations based on the sun's transit.\nA unique reckoning, exclusively based on the sun's transit through the constellations is the traditional Maldivian Calendar.\nComing under a common pattern of extreme peninsular India and Sri Lanka, comprising Tamils, Malayalis and Sinhalese, the Dhivehi (Maldivian) calendar also begins in April, with the entry of the sun into the constellation Asvini / Assidha\n. But, they take it as occurring on the 8th of April (the precession position of roughly 9th century AD).\nThe concept of the Maldivian calendar is unique to suit its equatorial location, climatic fluctuations and to help them in ascertaining the arrival of the schools of fish, compared to the agrarian based calendars of the rest of South Asia.\nThere are 27 months in this calendar based on the sun's transit in the 27 constellations (Nadchaththiram\n), each month having 13 or 14 days.\nThe original ideas behind the designing of the solar calendars were to make them coincide with the equinoxes and solstices.\nThe vernal equinox was occurring on the 14th of April, at the time when Varaha Mihira, the astronomer of Ujjaini, perfected the solar calendar. Today the equinox takes place on the 20 / 21st of March. The date advanced due to the precession of the earth's axis. For the same reason, the Winter Solstice, i.e., the returning of the sun from the tropic of Capricorn towards north, which was taking place earlier on the 14th of January, now occurs on the 21 / 22nd of December.\nOur astronomers were very well aware of this shift caused by precession, which in their term is Ayanamsa\n. However, they didn't want to change the dates of the New Year of April 14th and Thaip-pongkal / Makara-samkraanti\nof January 14th, which was originally the Winter Solstice.\nTheir argument is that the calendar is not merely solar but also stellar.\nThe beginning point of the zodiac in their opinion is a fixed star Asvini\n, and the sun's transit in it is important.\nFor the progressive year, for which the beginning date would change along with vernal equinox, they have another reckoning called, Saayana\nYear (of the Moving Zodiac, opposite to Nirayana\nor the Fixed Zodiac).\nThe justifications of the arguments need further scrutiny since the constellation Asvini\nitself was not the beginning of the zodiac in the remote past. It shifted from Mirukaseeridam\n, through Rohini\nas testified by literature and now is in Uththaraddaathi\nThe recent development is a call from Karunanidhi, the Chief Minister of Tamil Nadu to the world Tamils to observe Thaip-pongkal\nas the Tamil New Year Day. A law has been passed recently in the Tamil Nadu State Assembly to this effect in February 2008. Mr. Karunanidhi is reported to have said that this change marks the rise and revolution of Tamils.\nThe arguments for the need of this new myth are not clear.\nDisowning the beginning, cannot disown the concepts and science behind the system.\nWhatever pros and cons can be said about April New Year can also be said about Thaip-pongkal\n, which is Makara-samkraanti\nto the rest of India.\nThe only difference it can make is that the Tamils would lose one of their secular festivals of astronomical significance that links them closely with the Dravidian legacies of the South.\nThe critics of the DMK government's move feel that such exercises of 'shallow myths' in the name of the noble venture of forging a prestigious global Tamil identity, remind of the Jayalalitha government's effort to ban certain folk practices and the subsequent quick retreat from it.\nOne has to see how fervently and colourfully the April New Year is celebrated by the Sinhalese of Sri Lanka, reminding us of the forgotten folk culture of the Dravidian South. It was the same before the war among the Tamils of Sri Lanka too.\nAnyway, the doyen of Tamil writing, Puthumaippiththan, wrote long back in one of his short stories Kayittaravu\n, how the reckoning of time is an attribution we ourselves impose on us. (Naangka'laakap poaddukko'nda vakka'naika'l\nAuthentic, academic details on the topic can be found in L.D. Swamikkannu Pillai's works, 'Panchanka and Calender', 'Indian Chronology', and D.C. Sircar's book on Indian Epigraphy.\n12.04.08 Prime Minister of Canada greets Tamil New Year"", ""Thingyan or the Myanmar New Year is the longest and most significant festival in the country. Over 90 percent of the population is Buddhist, and Myanmar’s deep Buddhist roots appear in Thingyan, a celebration of the past, present, and future.\nJump ahead to these sections:\n- Religion in Myanmar\n- What is Thingyan (Burmese Water) Festival?\n- When is Thingyan Festival?\n- Thingyan Festival Traditions and Activities\nMyanmar (formerly known as Burma) is no stranger to celebrations of life. This small nation has one of the largest numbers of national holidays. In fact, there’s a public holiday for each month of the year.\nAlthough Thingyan, the largest celebration, might seem like a party, it’s actually an ancient tradition. In this guide, we’ll explore how to celebrate the New Year in one of the world’s best-kept, secret, bucket-list destinations.\nReligion in Myanmar\nMyanmar’s people are overwhelmingly Buddhist, and as a result, it’s not surprising that Buddhist rituals are a large part of the New Year. Before learning about Thingyan traditions, let’s explore what Buddhism is and why it’s important.\nThe people of Myanmar practice Theravada Buddhism, an ancient belief in nirvana, a place of peace and happiness. Buddhism is part of the national identity and has been since the year 1044. It’s said that Buddha, the religious leader, visited the country four times to spread his teachings.\nBuddhists believe in reincarnation. To escape the cycle of rebirth, a person must build positive energy or merit. Buddhists follow special traditions to do this.\nThe eight precepts are pillars of Buddhist teachings. They are especially important to follow during Thingyan. They include not harming any living creatures, giving offerings, and being kind to others.\nWhat is Thingyan (Burmese Water) Festival?\nThingyan is a four-day celebration of the New Year in Myanmar. It’s the most important holiday on the national calendar, showcasing its Buddhist beginnings and rich culture.\nThingyan is a time for people to learn from the past and look to the future. Before sharing how you can celebrate, let’s take a look at how it began.\nThingyan dates back to over 900 hundred years ago. It was borrowed from the Indian harvest celebration Holi. Like Thingyan, Holi celebrates the seasonal changes of the sun. People around the world celebrate seasonal changes like the Harvest of the Moon festival in Korea.\nAlthough the Myanmarese are Buddhist, their religion has ties with Hinduism. In fact, Thingyan’s origin story comes from the Buddhist translation of a Hindu myth. Buddhist call the Hindu God Indra, Thagyarmin.\nThe story says that Thagyarmin won a bet with another king. Although Thagyarmin cut off the king’s head, the king didn’t die but instead became a powerful being called Ganesha.\nHe is worshipped in cultures around the world for his lucky powers. The king’s decapitated head was too powerful to simply discard. Instead, it must be carried each year to prevent the end of the world.\nEach year, people gather to celebrate Thagyarmin’s arrival earthside. People believe he observes them to ensure they follow Buddhist teachings and traditions. It’s a special time where participants can interact with this celestial being directly.\nIt’s said that Thagyarmin was a man named Magha. Magha’s good works led him to the ultimate Buddhist goal—escaping the cycle of rebirth. Because he dedicated his life to being selfless, he was reborn as a king of celestial beings.\nDuring Thingyan, participants focus on selfless actions like Magha did, to ensure good fortune in the coming year.\nWhere it’s celebrated\nMyanmar celebrates Thingyan all over—from the countryside to major cities. Smaller towns celebrate traditionally with rituals, cleaning, and offerings. Larger towns like the capital city of Nay Pyi Taw set up padals or large stages. Performers take to the stage as the audience as large water guns hose down the audience.\nIt’s unusual to see people without their families during Thingyan. Many people choose to go back to their hometowns to celebrate or gather together in bigger towns. The government shuts down transportation and streets close, making room for stages.\nThingyan isn’t the only water festival in April. Laos and Thailand both have their own water festivities to celebrate life and honor tradition.\nWhen is Thingyan Festival?\nMyanmar doesn’t have the same calendar as other countries. Instead, they use a lunar calendar based on the position of the sun and moon. Unlike in the West, the New Year begins in April.\nThingyan begins on April 13, and festivities end on April 16. The celebration ends with New Year, which falls on April 17th of each year. Some families choose to prepare in advance as far as eight days before the New Year.\nThingyan Festival Traditions and Activities\nThe Thingyan festival is the largest, longest, and most exciting celebration of the year. It’s the only time of the year that restrictions are lifted for gatherings. Myanmar has traditionally been a military state. Although it's undergoing a transition to democracy, there are still rules that all citizens must follow.\nDuring Thingyan, the rules are relaxed. Participants of all backgrounds come together to reflect on the past year and plan for the future.\nFirst, let’s take a look at regional traditions. Depending on the minority group, the festival is celebrated differently. There are also different traditions in the countryside versus major cities.\nThere are seven ethnic minority states in Myanmar. Each minority group has their own customs to celebrate the New Year.\nFor example, the Rakhine minority group uses a traditional style of water throwing for courtship. Girls and boys choose a partner. Then, they water each other with special bowls on bamboo platforms. The girls make a special treat, nantha, while the boys sing and play music. Later, they offer the treat to monks for good fortune in the coming year.\nIn large cities like Yangon, the bamboo platforms are far more elaborate. They extend out into the streets with water cannons.\nConcerts with famous musicians are common. Music (both Myanmar-style and Western) plays while young people dance until sunset. Many people save funds for the entire year for a chance to partake in the festival.\nWalk down the street during Thingyan, and you’re likely to be soaked with a water gun, bucket, or cannon. Water is the most important part of the festivities. Although live music and hoses are a modern change, the tradition of water dates back to the tenth century.\nBack then, royals washed their hair in thingyantaw kor or ritual hair washing to prepare for the New Year. It’s said the hair washing purifies the mind, body, and spirit. Later, servants began splashing each other and the rest is Thingyan history.\nToday, after the water dousing festivities, it’s customary to focus on gaining merit or positive energy for the coming year. People wash the elderly’s hair or offer water to monks. These actions help people gain merit and wash away the wrongs of the past year.\nActs of kindness\nAs mentioned before, after the festivities subside, Myanmar’s people focus on acts of kindness to prepare a great start to the New Year. At this time, many people visit their family members if they haven’t already.\nOne of the most popular ways to show kindness is to offer food to the monks. This is a year-round practice, but monasteries are the busiest on New Year’s Day.\nEveryone must follow the eight precepts or principles of Buddhism. Some other kind acts include: releasing animals like birds and fish, cleaning homes, and making offerings to images of Buddha.\nFood and drink\nFood is a large part of any festival, and Thingyan is no exception. Treats and alcohol are plentiful to provide festival-goers with energy to participate.\nMyanmar’s most important crop is rice, so you’re very likely to find sweet and tangy rice dishes. Sticky rice balls and fresh fruit and vegetables are common staples.\nThe people of Myanmar are very proud of Thingyan. For a usually secretive nation, it’s a time to open doors to foreigners and showcase their culture.\nAround the nation, traditional dancers wearing long colorful dresses perform. Especially popular is the Thingyan Yein or group dance. Dancers use their hands and feet in synchronized movement while traditional music plays. Performers play the Pat Waing, a special drum circle reserved for important celebrations.\nCelebrate the New Year\nGood deeds bring all the people of Myanmar together on Thingyan. It’s a time to reflect on the past year and plan with an intent for the future.\nHave you thought about your future lately? Not all of us have the opportunity to visit Myanmar, but we can reflect on our goals for the next year. Starting end-of-life planning is a great way to begin your journey.\n- Lwin, Sandar. “Thingyan: Explained.” The Myanmar Times. 30 March 2016. www.mmtimes.com/special-features/220-thingyan-2016/19770-thingyan-explained.html\n- “Myanmar.” World Directory of Minorities and Indigenous Peoples. www./minorityrights.org/country/myanmarburma/\n- The New Light of Myanmar. Burma Library. www.burmalibrary.org/docs15/NLM-2013-04-17-red.pdf\n- “In Myanmar, Celebrating With Water, Letting Off Steam.” International Herald Tribune. 16 April 2009. www.nytimes.com/2009/04/17/world/asia/17yangon.html""]"	['<urn:uuid:5e29c653-8969-4052-a4a9-e16ebfca68c1>', '<urn:uuid:00c2ffc7-3ae2-4762-86e1-f3d363019bc3>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:56:11.774636	15	120	3269
43	dimensions weight buddhist statue hyderabad	The Buddha Statue in Hyderabad is 17 meters in height and weighs 320 tonnes. It is a monolithic statue that was carved by 40 sculptors under the guidance of Shri Ganapathi Staoathi.	['Sitara Recital, a Morning Concert held at city’s proud Monument Buddha Statue in Hussain Sagar lake on Sunday\nIt’s a wonderful experiment and hope it will be continued: Jayesh Ranjan, Principle Secretary, Govt of Telangana.\nHyderabad, December 13, 2021: First time ever in the history of Hyderabad city, a morning public concert ‘Shubhodayam’ held in Hussain Sagar Lake Near Buddha Statue to give music lovers a unique experience.\n230 plus music lovers witnessed the concert some of whom include Mrs and Mr Jayesh Ranjan, Principal Secretary, Govt of Telangana; Mr Anjaneyulu, General Manager, Tourism Department.\nSpeaking on the occasion Mr Jayesh Ranjan said, there are no words left to describe my experience. It is a wonderful experiment. It had created a beautiful moments for all of us. From parks, the morning concert as a concept is now moved to an historical monument. It is different kind of an event which left a long lasting impression in me. I have witnessed hundreds of concerts. Nothing had created such a memoarable moments as this. I hope this tradition of morning concerts will continue here at this historic monument for a very long time so that music lovers will continue to get entertained and it will also promote classical music as well as our great culture he said and appreciated Tatva Arts for conceptualising such a wonderful and never before concert.\nCouple of visitors from Chennai, Bhubaneshwar, Mumbai, Gurgaon who happened to have been in city and came to know about this novel concert at great monument of the same also came and witnessed it.\nThe Music Unlock Series once a month for one year will be organised by Tatva Arts in association with the Department of Language & Culture, Govt of Telangana.\nShri Ramprapanna Bhattacharya, performed the Sitar Recital. He was accompanied by Sri Thakur Harijit Singh on tabla. In his 70 minutes performance he enthralled audience from 7.30am. The concert concluded at 9am. He began it with Raag Ahir Bhairav and ended with Raag Magdham Bhairavi.\nI am very proud to perform here at Buddha statue. It is a proud moment for me. It is a great opportunity bestowed on me said Mr Ramprapanna Bhattacharya.\nThe music lovers were taken in boats to Buddha Status for a never before concert. From the young to old, teens to home makers, from far off places in city like Miyapur, LB Nagar, Ghatkesar, Shamirpet turned up to the concert.\nBuddha Statue when it was installed on 1st December in 1992, it was the tallest Buddha Statue in India. It is 17 meters height and weights 320 tonnes. It is a monolithic statue carved by 40 sculptures under the guidance of Shri Ganapathi Staoathi. It was transported from Raigur on a heavy carriage that had 192 wheels and it travelled 60 kilometers. Since then, it has been a proud monument for people nit just in twin cities but from all over India. It has been attracting many Buddhists to the city. Now an instrumental concert near it offered a memorable experience to the music lovers. It can be fondly remembered and cherished for a very long time.\nThe morning concerts as a new concept is the brainchild of Akhilesh Washikar, Art Connoisseur; Gajendra Shewanker, Tabla Guru, the duo is behind Style Tatva. Pritesh Patil, Prashant Sardeshmukh have been supporting Tatva Arts.\nThey have been organising these kinds of events at Public Parks like Melkote Park at Narayanguda, Botanical Garden at Kondapur, Indira Park etc and others are spreading happiness and living as a role model to many. They have organised 18 such free morning concerts at various parks in the city.\nThe entry to the concert was free. But, Rs 100/- per head was collected towards the cost of the boat to ferry music lovers to the venue.\nThe concert was strictly by invitation only.\nStrict COVID Protocols and COVID Appropriate Behaviour was observed. The seating was on the floor only. However, few chairs were provided for the convenience of elders.\nThe Morning Musical Concert in a Public Park is a unique concept offered to walkers to enjoy live music while walking. It has no stage. The music lovers either sit on the grass or walk and enjoy themselves. Now for the first time it was extended to Hussain Sagar Lake.\nWe have suffered enough in the last one year with Corona. If there is anything people want now is feel good and feel great. Medicine cures but music heals. That is the thought behind this. It is a free to listen and enjoy music concert.']	['<urn:uuid:eef18c36-b3a1-4154-b595-8dc432881a6e>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	5	32	761
44	When ordering Thai noodles, what is the correct sequence of specifications to ensure getting exactly what you want?	The sequence has 4 parts: First specify the type of noodles, then indicate if you want it dry (Hang) or with soup (Nam), followed by any additional information like 'No Beansprouts' or 'TomYam', and finally specify if you want special (extra) or normal portion.	['Saw a couple of tourists trying to order noodles and their faces were full of total puzzlement, I hope this helps. (Note: all the various pictures came from the web, none of them are mine).\nThere are several types of ‘noodles based meal’ that you’ll see on the streets of Bangkok, for example, Duck noodles, Fish dumpling noodles, Beef Noodles, Goose Noodles, Pork Noodles and so on, but the ordering process is almost the same. Hopefully it will make your life a little easier when you want to order some noodles in Thailand :)\nFirst Step: The Sequence\nThere is a sequence to the ‘order’, just like Starbucks, “Grande (Size), Decaf – Latte (Drink)”, of course it isn’t wrong to say it differently but it will help to get your message across especially when one doesn’t know the language well.\nThe Sequence is a simple one\n#1 [‘Type of Noodles’] +\n#2 [‘Dry or Wet (no soup or in soup)’] +\n#3 [Additional Information, such as ‘No Beansprouts’ or ‘TomYam’ (will get into this later), and etc.] +\n#4 [‘Special’ or ‘Normal’ (meaning extra or normal portion]\nIn short: Noodles Type + Soup? + additional info. + Special?\nThat is the gist of it, 4 commands in all, very simple trust me…\n#1 Type of Noodles\nBa Mhee = Yellow thin round noodles (made with eggs)\nComes in two type, most common ones are round egg noodles, the other is just flatter.\nSen Mhee = White thin round noodles\nSen Lhek = White flat small size noodles (Lhek means small)\nSen Yai = White flat ‘wider’ noodles (Yai means big)\nWoon – Sen = Glass Noodles (transparent, made from green beans, no carb :) )\nSen Mama, plain and simple it’s one of those instant noodles’ noodles.\n#2 Hang or Nam? (No Soup or with Soup)\nHang means ‘dry’ and Nam means ‘liquid/water’, so after choosing your type of noodles you’ll have to choose if you want your noodles without soup or with (in) soup. Of course you can also have your noodles without soup and have your soup as a separate dish (for free, although some may charge a little).\nLet say I have chosen Ba Mhee (egg noodles)\nThis is Ba Mhee Hang (a direct translation is ‘Egg noodles Dry’)\nThis is Ba Mhee Nam (a direct translation is ‘Egg noodles Liquid/Water’)\n#3 Additional Information\nAfter the noodles + soup/no soup it is time for #3 ‘Additional Information’, this is where you put in all your little quirks, to put it in ‘Starbuck’s language’ it would be like ‘low fat, one equal and etc.’.\nThese are some of the common ‘special order’\n- No Beansprouts\n- No Vegetables\n- Only round fishball dumpling\n- No Garlic\n- Not too much soup\n- Tom Yam (all the spices (hot) will be added so there is no need for you to add anything else)\nIf you don’t have any special demands that just skip this and go straight to #4.\n#4 Pee-Ses or Tam-ma-dar (Special or Normal/Standard)\nThis final direction has to do with the portion ‘Pee-ses’ (Special) means extra portions and in general they will charge you 5 to 10 THB more, and ‘Tam-ma-da’ (Normal/Standard) means standard portion and pricing.\nMoreover, you can order ‘specific’ type of extra portion, such as extra noodles, extra duck or extra fishball dumplings and so on.\nThere is no need to say ‘Tam-ma-da’ each time, your order will assume to be ‘standard’ unless you say ‘Pee-ses’\nSummary and Additional information\n- Ba Mhee + Nam + No Beansprouts\n- Sen Mhee + Hang\n- Sen Yai + Nam\n- Woon Sen + Hang + Pi Ses\nEtc… enjoy :)\nThe above four steps #1 + #2 + #3 + #4 should get you what ever you want, just don’t go to a duck noodles place and order noodles with fishball dumplings, lol. All of the noodle shops will have their ingredients display in front of the shop, if you see a duck then it is a duck noodle shops no need to try to read the signs :)\n‘Yen-ta-four’, these are common enough for me to tell you about it, this is a ‘fish’ based noodles dish but they added red bean curd, and it comes with ‘morning glory’ vegetables and squids. So if you see red noodles dish either with or without soup it will be probably be a ‘Yen ta four’.\nNot all shops sells Yen ta four, you’ll just have to look around. To order this, your first command will have to be, ‘Yen – Ta – Four’ then follow by your #1 + #2 + #3 + #4 direction, simple enough.\nOf course there are more to it than it is written here, you’ll just need to be adventurous and don’t be shy, people will always wanting to help and because all of the ingredients are more than often, all out for you to see, you can just point and see what you get :)']	['<urn:uuid:fdcd65c6-a38b-47c5-a70c-219bd1684593>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	18	44	836
45	What happens when you mix bleach with acetic acid?	When sodium hypochlorite (bleach) mixes with acetic acid, it forms hypochlorous acid, which can oxidize secondary alcohols to ketones.	"[""An oxidation-reduction scheme: borneol, camphor and isoborneol this experiment will illustrate the use of a green oxidizing agent, sodium hypochlo- rite (bleach) in acetic acid oxidation of borneol with hypochlorite sodium hypochlorite, bleach, can be used to oxidize secondary alcohols to ketones. Purposein this experiment, sodium hypochlorite (naocl) in acetic acid mixture was used to oxidize emdo-borneol (an alcohol) to camphor (a ketone) the product would then be purified by sublimation and then be analyzed by infrared spectroscopy and melting point testprocedure and observationsplease refer to the lab. Possibly the simplest reagent which has been reported to carry out the selective oxidation of a secondary alcohol is sodium hypochlorite in acetic acid61 given the very low cost of the reagents, this system has obvious potential for large-scale operation a modification of this procedure uses calcium hypochlorite, which has. Reduction of camphor with sodium borohydride (fast and irreversible) gives instead the isomer isoborneol as the kinetically controlled reaction product the intention is to collect practical experience of how to conduct useful chemical synthesis in the lab, while experimental methods published in an ordinary academic. Isobornyl acetate leads to borneol that is oxidized to form racemic camphor hypochlorous acid is obtained from the reaction of hypochlorite and acetic acid experimental dissolve 500 mg of (-)-borneol in 15 ml of glacial acetic acid in a 25 ml erlenmeyer flask with swirling at room temperature add about 40 ml of. Oxidation of borneol to camphor learning objectives: to illustrate the concepts of oxidation and reduction in organic chemistry and sodium hypochlorite decomposes in water slowly, and using old bleach could introduce a source of error in the experiment the reducing agent in the second step is sodium.\nOxidation of borneol to camphor using oxone and catalytic sodium chloride: a green experiment for the undergraduate organic chemistry laboratory this new experiment proved superior to the former bleach oxidation procedure previously used as an illustrative green reaction in these labs. Isoborneol and borneol the stronger reducing agent lialh4 may be used the mechanism of the reaction is still unclear thus proving a relatively complete oxidation of isoborneol to camphor the second experiment involved the oxidation of the isoborneol from the last experiment back to camphor using naclo as. Temperature in aqueous acetonitrile in the presence of potassium bromide (01 equiv), sodium hypochlorite (4 mol %) and polymer-bound h2po4 а (1 equiv) tempo-mediated oxidation of complex secondary alcohols to ketones using co-oxidant 6[a] by treatment ofs-borneol (26 31 mg, 02 mmol) with reagent 6.\nFull-text paper (pdf): greening the oxidation of borneol to camphor in the undergraduate organic laboratory experiment is designed such that a single improvement can drive the learning experience, while simultaneously encouraging the accumulation of data from lab oxidation of borneol with sodium hypochlorite6. Of oxygenated monoterpenes were also found to be more phytotoxic as compared with their experimental individual monoterpenoids the compounds tested were allo-ocimene (fluka), borneol (fluka), bornyl acetate (sigma), camphene (fluka), camphor (fluka), 3- sodium hypochlorite for 20 min (tinnin and kirk. Aswin garimalla, fifth year graduate teaching/research assistant made these videos and posted them in youtube for the benefit of organic lab undergraduate students under the advise of prof michael harmata , with the help of anne kroeger , lab assistant. Some products that contain oxidizing agents are bleaches, hair coloring agents, scouring powders, and toilet bowl cleaners the most common oxidizing agent in bleaches is sodium hypochlorite, naclo (sometimes written naocl) commercial bleaches are created by bubbling chlorine gas into a sodium hydroxide solution.\nA) should she use the oxidation of borneol or cyclohexanol as a model experiment the product of the oxidation of borneol, ie, camphor, is a solid it is filtered cyclohexanone and cyclopentanone are both liquids 62% aqueous sodium hypochlorite ie, 62 g/100 ml c) how many gallons of clorox® will she need. Free essay: sodium borohydride reduction of benzil introduction: the purpose of this experiment is for the students to learn how to use sodium borohydride to first, borneol is oxidized through a reaction with sodium hypochlorite at 400c to form camphor when the camphor is then reduced by sodium. The reaction: sodium hypochlorite (naclo household bleach) is a relatively cheap and environmentallyfriendly oxidizing agent capable of converting alcohols to ketones in the presence of acid, hypochlorous acid is presumably formed, which could oxidize the alcohol via the following mechanism: however, household.\nExperimental research the sterilization has done into sterile flask with hypochlorite solution 1:10 (v/v) followed by washing with distilled water twice flasks were shaken also, it contains a range of additional compounds, such as camphene, camphor, p-cymene, myrcene, borneol and linalool [39,40. Purposein this experiment, sodium hypochlorite (naocl) in acetic acid mixture was used to oxidize emdo-borneol (an alcohol) to camphor (a ketone) the product would then be purified by sublimation and then be analyzed by infrared spectroscopy and melti. Oxidation both commercially purified α-pinene and crude thermo-mechanical turpentine were used as raw materials in the catalytic one-pot synthesis of the experiments were performed in both batch and continuous reactor systems depending furthermore, camphene is a precursor to camphor - compound used in.\nWhich of the following activities in this experiment can be performed outside the fume hood keep it wet with solvent so less powder gets into the air, handle the silica gel in the fume hood, if some silica gel spills, wipe it up with a wet paper towel, not a you are using sodium hypochlorite to oxidize borneol to camphor. Previously washed in 1% sodium hypochlorite and rinsed in sterile deionized water, onto potato dextrose agar (pda, merck) plates amended with 25 mg/l streptomycin sulfate (merck) a 7-day-old culture was used for dna extraction by using the ezna plant dna kit (omega bio-tek, norcross ga, usa. A further experiment was carried out by dipping papayas in a coating formulated with both essential oils reduced decay caused by c gloeosporioides and r stolonifer by up to 50% and 40%, respectively, compared with the 100% 1% sodium hypochlorite and placed at the center of a petri plate con- taining potato. Through an oxidation-reduction reaction sequence, borneol is converted to isoborneol first, borneol is oxidized through a reaction with sodium hypochlorite at 400c to form camphor when the camphor is then reduced by sodium borohydride, isoborneol is formed the percent yeild of isoborneol collected was 564%, and.\nBorneol used in today's experiment kmno4 can also be used to oxidize alcohols 45 as an insect repellent and isoborneol and weller in 1980 (j safety and waste disposal 1 the example chosen to illustrate the oxidation of alcohols is the sodium hypochlorite oxidation of isoborneol to camphor: h3c h3c ch3 ch3. The ability of this high-valent iron salt for oxidizing organic substrates in nonaqueous media was studied using benzyl alcohol as a model, the catalytic activity of a implies prior formation of sodium ferrate from solutions of ferric chloride and sodium hypochlorite in the presence of sodium hydroxide addition of potassium. Which of the following can be extracted from an organic solvent with 6m of aqueous naoh what is the oxidizing agent in the following rxn borneol -- naocl/ch3cooh-- camphor --nabh4-- isoborneol sodium hypochlorite in the reduxn of camphor by nabh4, 2 diastereoisomers are obtained- with what product. Title: the green oxidation of borneol to camphor date conducted: february 8, 2012 april 27, 2012 abstract: the goal of this experiment was to perform a green oxidation of borneol to camphor, without the use of chromium synthesis since it is toxic to the environment reagent table: |chemical name |amount used.""]"	['<urn:uuid:327d85b0-4fd3-4309-a1f5-cf4c5d496127>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	9	19	1254
46	What's the purpose of white phosphorus in artillery?	White phosphorus smoke ammunition is used to release a smoke cloud that can obscure line of sight. This bombardment is typically useful for screening assaults against prepared positions, obscuring enemy artillery spotters, or covering retreating forces. The smoke cloud may move depending on wind direction and dissipates after a while.	"['Let\'s say that you managed to get to the part where you can select your troops to crush the capitalist pigs (or dirty communists). Now, you just bought 12 batteries of the finest artillery your selected nation can offer, as well as 5 flights of jets and helicopters. Now that you have clicked the start button, you want to start raining fiery hell on you enemies. And then it hits you.\nYou don\'t know what to do.\nThis guide is here to help new players in better understanding not only how to use fire support, but also to use it effectively.\nFire Support Edit\nFire support supplements direct fire (firing ordnance directly at a target in line-of-sight) with indirect fire (firing ordnance in a ballistic arc or dropping/firing ordnance from an aircraft). This can protect and assist ground forces by permitting attacks against targets behind terrain/obstructions or in defilade. It can also be used to suppress return fire and screen movements.\nUsed well it can greatly magnify the effectiveness of a military unit.\nUsing Fire Support Edit\nTRPs (or target reference points) are points at the map where when a fire mission is requested, the nearer the target area is to the TRP, the faster it arrives.\nTo place a TRP, at the deployment menu, click the TRP button (at the far top right). Then left-click anywhere on the map. Consider where you place your TRPs carefully, as you cannot change them during the course of a battle (in real-life TRPs are usually scouted in advance and pre-ranged) and you only have a limited number depending on how many artillery units you have brought.\nArtillery support is often provided by howitzers (for medium-range engagements) and multiple-launch rocket systems (MLRS) (for long-range engagements). Naval gun support is also classified similarly.\nTo use it:\n- Click the Fire Support button below the LOS button.\n- Now that a window has popped up, click the New Fire Mission button. Notice that a new fire mission has appeared on your list and the setting at the center of the window can be used.\n- Okay, now that you have your fire mission selected, you can now begin to edit the parameters of the mission:\n- The length and width of the target area.\n- The type of ammunition to be used. (HE, DPICM, Smoke).\n- How many tubes (artillery pieces) to be used for that fire mission.\n- How many volleys, or rather how many times each tube fires.\n- Whether if it fires when ready or at your command.\n- After you have set the parameters to your liking, left click the target button. A yellow area will appear on your view of the battlefield.\n- Left click where you want your fire mission to be placed. Wait for the timer. The timer is there to simulate the time taken to train the guns at the target. When the timer runs out, the fire mission will fire or, if you selected it to fire at your command, a message will display, saying that the fire mission is ready.\n- If you set the fire mission to fire at your command, click the fire support button then select your fire mission, then click fire. You can also click the fire all button, firing all available fire missions. Fire missions can be used repeatedly without the timer unless you have changed the target. When you no longer need the said fire mission, simply click cancel. You may also call multiple fire missions at the same time, given you have enough remaining tubes.\nMortars are short-range tube artillery. Where Howitzer/MLRS/Naval Gun support is usually on-call at regimental or brigade level, mortar units can often be made available directly to battalion or company-level.\n- Select the mortar. Then click the IF button on the menu that pops up.\n- Select the type of ammo, whether it be HE-FRAG, Smoke or Illumination rounds.\n- Now that you have selected the kind of ammo to be used, you then go to the main window, where you target where the mortar will fire. You cannot set the target area.\n- Now that it has its target set, it will begin adjusting and loading. For this, you must wait.\n- A message will appear, saying your mortar is ready to fire.\n- Select the mortar unit. Click the IF button then select whether to fire for effect, in which it will fire off all rounds of that type of ammunition, or to fire one round. You can hold fire and cancel the mortar fire mission as well.\nSpotting: Note that mortars are more effective when they have LOS to the target. Or if in defilade, they will be more effective if there is another unit (preferably a well-hidden reconnaissance unit) with LOS able to spot the fall of their rounds.\nAir Support Edit\nClose Air Support provided by Aircraft and helicopters add an extra dimension to the battlefield, ground commanders the ability to conduct precision strikes against the heaviest protected targets.\nThe disadvantages of Air Support are decreased total ordnance and short loiter times (particularly for fast jets). In a modern SAM-heavy environment the fragility of air units means that they cannot be used alone, and depend on ground forces identifying and eliminating enemy anti-air units.\n- To use them, click the FS button at the right.\n- A window will pop up. At the right, there is a list of available air support you can use. Click one of them. If you selected a helicopter support mission, go to step 4. Otherwise, go on to step 3.\n- Now, you will go onto the main window. Click where you want your air support to rain leaden fury onto unsuspecting enemies. Wait for the timer to count down.When it has counted down, aircraft will automatically enter the field of battle. Air support can be called on again after a waiting time, to simulate the time it takes for the aircraft to fly home and refuel and reload.\n- If you have selected a helicopter support mission, it is not as simple. After selecting the support mission, you will be prompted to place Battle Position 1 (or BP1). Left click on the map. Afterwards, you will be prompted to place TRP 1. The battle position is where the helicopters of that support mission will fly to. The TRP is the area in which they will look for enemy units.\n- Afterwards, a timer will count down. When it has counted down, select the helicopter support mission. It will prompt you to left click to move it to its BP1. You may do this at anytime, unless the mission timer has not finished yet. You may select it again to move it to its BP2. Helicopter missions have only 2 BPs.\nNote that air units require time to fly from their holding positions to their battle positions. Also, if enemy air is present, friendly air units will naturally prioritise protecting themselves against these.\nArtillery Tactics Edit\nWhile artillery is a helpful asset, never substitute a good strategy for massive amounts of support. Be prepared, and integrate support into your grand plan.\nSecond, artillery support is often used in warfare not to destroy, but rather to suppress enemy units and reduce their effectiveness. Savvy commanders will use artillery to soften enemy prepared positions before assaults, or to break up an attacking formation.\nArtillery can also fire obscuring smoke to screen friendly maneuvers.\nThe effectiveness of artillery also depends on the ammunition choice; it is important to identify targets and match up the appropriate ammunition to the enemy formation.\n- HE - High Explosive rounds are where the majority of the shell volume is explosive. This type of bombardment useful against softer-skinned vehicles. However, heavily-armored vehicles such as MBTs tend to be resistant to all but direct hits on thinner top armor. The density of the bombardment is lower when compared to DPICM, which makes this round less useful against infantry.\n- DPICM - Dual Purpose Improved Conventional Munition. This type of round explodes in the air and rains down smaller bomblets onto the target area. A portion of the bomblets are fragmentation rounds, and the rest are shaped-charge HE (with standoff detonator probes) for armor-piercing. The armor-piercing ability is limited by the relatively small explosive content and is typically only effective against thin-skinned vehicles such as APCs and IFVs. The density of bomblets makes this type of bombardment most effective against infantry and mechanized infantry.\n- Smoke - Upon detonation, this type of ammo releases a smoke cloud (usually white phosphorus), which may move depending on wind direction and dissipates after a while. Useful for obscuring line of sight. Other kinds of artillery ammo may produce dust clouds upon detonation, but these are not effective as obscurants compared to WP. This bombardment is typically useful for screening assaults against prepared positions, obscuring enemy artillery spotters, or to cover retreating forces.\n- Illumination - This type of ammo illuminates the surrounding area using magnesium parachute flares. Often used in night engagements to illuminate suspected areas where enemy units reside. Note that the illuminating source will blow with the wind, so check wind direction and strength before choosing target area.\nMortars in Armored Brigade equip three kinds of ammo: HE-FRAG, similar to the explosion of a FRAG grenade but larger, Smoke, and Illumination rounds.\nAside from setting target reference points or battle areas, aircraft will select targets and allocate weapons autonomously.\nBefore calling in an airstrike:Edit\nUse your ground forces to identify and neutralize (or suppress) enemy air-defenses, paying particular attention to approach corridors and battle positions.\nWhen the aircraft has arrived:Edit\nArrival of air units will inevitably flush out enemy AA units. Identify the positions that your air is taking fire from and use your ground forces or artillery to neutralize or suppress these positions.\nAircraft will naturally tend to focus on SEAD (Suppression of Enemy Air Defenses) before engaging other targets, so if your ground and artillery can take care of that for them, they will be able to spend more time (and munitions) on attacking targets that are more of a threat to your ground forces.\nFast jets will attack with high-speed passes across the length of the map, initially concentrating on identification of targets (skilled use of recon assets will reduce the amount of time they need to do this) and SEAD.\n- Guided munitions (for stand-off attack) will involve an approach and peel off once the weapon has been released. Examples include Paveway, JDAM, Maverick.\n- Unguided munitions (bombs) can require riskier overflights of troop concentrations. Examples include Mark-82, Rockeye.\n- Strafing runs are made when all other ordnance has been exhausted. Fixed-wing aircraft typically only carry enough ammunition for a few passes.\nThey will continue their attacks until either all munitions have been expended (radio call ""winchester""), they only have enough fuel to return to base (radio call ""bingo""), or are damaged/destroyed.\nAlso keep in mind the probability for the aircraft to become permanently unavailable after each pass is 20%, representing external factors like enemy air or missile interception, weather, or orders change.\nHelicopters will fly in from the allocated entry-exit point and make for the first battle position (BP). They will be vulnerable to hidden AA units along this flight corridor, so plan accordingly and make effective use of reconnaissance. They will take station at the battle position and attack towards the allocated TRP for that position.\nSpotter helicopters (such as OH-58s) are often unarmed but provide spotting capability over obstacles, either in support of ground forces, artillery, or other air units. This can permit use of stand-off munitions with reduced exposure of the firing units.\nNote that helicopters will need to be ordered to proceed to their next battle position from the FS menu, and will be vulnerable along their transit corridor.\nScatterable mine systems enable the commander to emplace minefields in enemy held terrain or in others areas where it is not possible to emplace conventional minefields. Mines can force the enemy into kill zones, change their direction of attack, spend time in clearing operations, or take evasive actions. Scatterable minefields are delivered by artillery and therefore laid without a clear pattern. They have their own target reference points (separate from artillery TRPs) which are only placed before the battle starts. It is very important to note that mines cannot be deployed outside of their TRPs. Mines are deployed during the game by selecting mine ammunition (if available) from fire support screen. These are less effective against heavily armored targets.']"	['<urn:uuid:0d3b355a-69de-40e7-bc8c-041cc1414d40>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	8	50	2103
47	Could you explain in detail how the Spanish immersion after-school program incorporates different activities to help children learn the language?	The program uses interactive classes that include arts and crafts, music and dance, traditional games, and storytelling. These activities are designed to expand vocabulary while introducing literary awareness in Spanish. The program immerses students in a total cultural environment while considering young children's general needs, language acquisition steps, and communicative interests to develop easy immersion techniques and materials.	['LANGUAGE IMMERSION AFTER SCHOOL PROGRAM\nFREQUENTLY ASKED QUESTIONS\nWhat is the Language Immersion After School Program at Nanaimo Language Centre?\nGreat question! There are many things young children can do better than us, and one of them is learning languages. This exciting program provides a unique opportunity for young students to immerse into a new language environment after school. Students will learn a new language in a fun and educational environment at an early age.\nWhat are the advantages of young children learning a new language?\nResearch shows that learning a second language boosts problem-solving, critical-thinking, and listening skills, in addition to improving memory, concentration, and the ability to multitask. Children proficient in other languages also show signs of enhanced creativity and mental flexibility.\nOther advantages include:\n- Nurtures curiosity, cultural sensitivity, empathy and tolerance\n- Facilitates learning progress\n- Boosts academic achievement\n- Enhances future career opportunities\n- Gives them a head start\n- Exposure to other languages at an early age provides an opportunity to tap into one’s natural ability\nWhat is a good age for my child to start learning a new language?\nIt’s never too early to begin learning a new language. It’s fun, it promotes healthy development, and the many cognitive and social benefits will last a lifetime.\nHow is your program designed? Do students have outdoor time?\nAbsolutely! Our interactive classes include:\n- arts and crafts\n- music and dance\n- traditional games\n- storytelling and so much more.\nThese activities help with concentrating on expanding vocabulary while introducing literary awareness in the target language. The idea is to immerse the students in a total cultural environment while taking into account young children’s general needs, language acquisition steps, and communicative interests to develop easy immersion techniques and materials.\nWhere are you located? What kind of outdoor space do you have access to?\nWe are located inside Nanaimo North Town Centre. Aside from our 3000 square feet of indoor classroom space, we enjoy the beautiful outdoor playground of NNTC as well as the field on the property. We are also within walking distance to Oliver Woods Community Centre.\nOccasionally, we bring students on field trips to different locations in town with no additional cost to parents.\nWhat are the instructors’ qualifications?\nThe program has been carefully developed by professionals with extensive experience in Canadian early childhood education and foreign language education to meet the needs of young students. A valid first aid/CPR certificate and criminal record check are also mandatory.\nWhat language(s) and age group are being offered?\nWe are currently accepting enrollment for students from age 5 to 8 into our Spanish Immersion After School Program.\nWhat time does the program take place?\nMonday to Friday, from school dismissal to 5:30pm.\nMy child doesn’t know this language, could I register?\nFor sure! This program is designed for students with no exposure to Spanish and/or students with limited experience in the language. Our aim is to make learning Spanish fun in a safe, after-school environment.\nHow many students in a class?\nTo ensure the quality of this program, the class size is 10-12 students. Therefore spaces are limited and we follow the first come first serve policy. Please register early to secure a spot.\nWhat are the fees?\nHere is the fee breakdown:\nRegistration fee $50 – one time only\nTuition: $359 per month per student\nIrregular early dismissal days: additional $10/hour\nLate Pick-up fee: $10 for each 15 minutes per student\nHow do I register?\nIt’s easy. Simply click here and register online. Spanish Immersion After School Program is the only full-time after school program we are accepting registrations for this moment.\nIf this is your first registration, please also click here to pay the registration fee.\nA short video about our program: https://drive.google.com/open?id=11PLaVzoQA2G_FbYzn5RJiaJuKUydvfJg']	['<urn:uuid:63a93b8d-490e-4ec1-bace-8b722cd75124>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	20	58	630
48	How do the data management approaches differ between moving objects databases that handle multiple transportation modes and the bridging efforts between HCI communities in terms of their longevity and success?	Moving objects databases take a more sustainable approach by developing comprehensive frameworks that can handle different environments (road network, bus network, indoor) simultaneously in a lasting way, while bridging efforts between HCI communities typically failed after a few years despite significant effort. The database approach creates permanent infrastructures to handle multiple modes, whereas the community bridges were temporary and couldn't overcome barriers like different publication priorities and methodological differences.	"['Recently, researchers start to explore the area of moving objects with multiple transportation modes due to novel applications on detecting transportation modes and advanced trip plannings or recommendations. Some work (e.g., Geolife Project in Microsoft) tries to detect outdoor transportation modes from raw GPS data in order to capture important characteristics and features of mobile users, as these pieces of information can help fully understand the mobility. From the viewpoint of the database community, the complete trip of humans should be managed by a database system, enabling novel queries associated with transportation modes and underlying environments. In spite of massive research work having been conducted on the area of moving objects databases, previous works mainly deal with one environment such as free space or road network. Evidently, a person\'s trip can cover different environments such as road network, bus network and indoor.\nWe investigate the following fundamental problems of managing and querying generic moving objects in a database system: (1) data model and data management; (2) data generator and query algorithms; (3) benchmark. A short description of each topic is listed below and the corresponding materials are provided.\nPh.d Thesis \xa0 Jianqiu Xu (supervised by Prof. Güting), Moving Objects with Multiple Transportation Modes, July, 2012, with the score ""magna cum laude"" (from Latin, meaning with great honor).\nThe current data models for moving objects focus on the location representation in a single environment and are not able to represent moving objects in different environments. The major difficulty is to have a framework which applies to all cases as real world environments have different features such as free and constraint movement, 2D and 3D. We propose a method that is able to represent the location of moving objects in different environments including road network, region-based outdoor, bus network, metro network and indoor.\n J. Xu and R.H. Güting. A Generic Data Model for Moving Objects, Geoinformatica, 2012.\n J. Xu. Research on Moving Objects with Multimodal Transportation Modes, ACM SIGMOD/POSD IDAR, 2011.\nDue to the difficulty of getting a large amount of real data including both moving objects and infrastructure data such as roads, bus routes and buildings, a tool is developed to create infrastructure data based on roads and public floor plans. There are a lot of public resources for such kinds of input data. To create generic moving objects, a navigation algorithm through all available environments is developed to find a path like from home to the office room. The complete path located in several environments is used to produce a moving object with different transportation modes.\n J. Xu and R.H. Güting. MWGen: A Mini World Generator, 13th International Conference on Mobile Data Management (IEEE MDM), pages 258-267, 2012. ppt\n J. Xu and R.H. Güting. Infrastructures for Research on Multimodal Moving Objects, 12th International Conference on Mobile Data Management (IEEE MDM), Demo, pages 329-332, 2011.\nTo evaluate the performance of a database system managing moving objects with transportation modes, we propose a benchmark that consists of a comprehensive set of datasets and a wide range of well-defined queries. Benchmark data are created in a realistic scenario in order to model human movement in practice. Optimization techniques are developed to reduce the query cost.\n J. Xu and R.H. Güting. GMOBench: A Benchmark for Generic Moving Objects, short paper, to appear in ACM SIGSPATIAL, 2012.\n J. Xu and R.H. Güting. Manage and Query Generic Moving Objects in SECONDO, Demo, PVLDB 5(12):2002-2005, 2012. poster\nGiven a polygon with obstacles and a query location, VPS (visible points searching) returns all obstacle vertices that are visible to the query location, i.e., the line between the query location and the obstacle vertex does not cross any obstacle. This is a fundamental problem in computational geometry and geographical databases in obstructed space. We propose a novel and fast algorithm that can find the results in O(N) where N is the total number of vertices, improving the existing method O(N+NlogN). This algorithm is used to find the shortest path for pedestrians where the whole walking area inside a city is modeled as a large polygon with many obstacles (buildings and streets without crossings).\n J. Xu and R.H. Güting. Visible Points Searching in Large Obstructed Space , Fernuniversität in Hagen, Technical Report, 2012.Experimental Materials: (1) Datasets; \xa0 (2) VPS-Scripts ; (3) Readme\nJianqiu Xu, e-mail: firstname.lastname@example.org\nRalf Hartmut Güting, e-mail: rhg@FernUni-Hagen.de', 'Computer science, design, cognitive and social sciences, human factors, information science, information systems, artificial intelligence, communication—each has an HCI/UX community. Other disciplines may also have groups sharing this focus. I have participated in many efforts to connect some of them, primarily through joint conference activity. We built bridges that some researchers and practitioners crossed to exchange information, but almost no bridge lasted more than a few years. Perhaps they served a purpose, but given the effort involved, we expected more durable structures.\nWe learn from bridge failures. To better understand why ours failed, I interviewed people who crossed a bridge a few times and then stopped. My goal was to aid future efforts. This year I participated in the CHI 2018 ""Bridging HCI"" workshop  focused on building ties between ACM SIGCHI and AIS SIGHCI. In reviewing the history, it occurred to me that conference bridges might be temporary for a reason. Short-term events and temporally constrained projects—think hackathons—could be more effective, drawing engaged members of multiple disciplines to tackle a problem that all consider important. They could unite to deliver a product: working software, a detailed policy, a shared resource.\nWithout disciplinary interaction, we can\'t stand on the shoulders of predecessors or bootstrap one another. Each community reinvents the work of other communities. It seemed sensible to establish trading posts at conferences, but it hasn\'t worked well. For centuries, disciplines coexisted and formed subdisciplines on their boundaries: mathematical physics, physical chemistry, biochemistry. Shouldn\'t we know how to do this by now? Are we different?\nIn some ways, yes, we are different. Moore\'s law gave rise to the rapid emergence of new disciplines and subdisciplines that were the focus of past HCI bridging efforts. I viewed some from outside, such as the early interaction of SIGCHI and SIGGRAPH, but I will describe those in which I participated.\nCHI and human factors. The first CHI conferences were co-organized by the new ACM Special Interest Group SIGCHI and the larger, established Human Factors Society. Many CHI founders had published in the human factors literature. This bridge was in place for the 1983 and 1985 CHI conferences, after which human factors participation largely ceased.\nSIGCHI and SIGOIS. SIGCHI initially centered on the use of personal computers and individual productivity tools. The older ACM Special Interest Group on Office Information Systems, SIGOIS, focused on the previous generation of computers, cabinet-size minicomputers. A minicomputer supported several people at terminals using word processors, spreadsheets, email, and databases. The COIS conference pioneered research on collaboration support, hypertext, and other topics. Efforts to coalesce the OIS and CHI communities failed. Soon CHI picked up these topics and spun off the CSCW and HYPERTEXT conferences. As minicomputers went from being a major computer industry to extinction, COIS themes and participation shifted, and it morphed into today\'s GROUP conference.\nMIS and CSCW in the 1980s. A management information systems (MIS) HCI community, centered in business schools, had formed in the late 1960s when mainframe computers replaced vacuum tube computers and applications were less confined to science and engineering. The organizational and workgroup orientation of MIS researchers with HCI interests seemed a natural match with CSCW when the latter appeared in the mid-1980s. The first two CSCW conferences had active MIS participation. Then the MIS researchers withdrew—another shortlived bridging effort.\nCSCW and ECSCW. In the late 1980s, the North American CSCW and European ECSCW communities seemed natural partners, but CSCW was dominated by computer and telecommunication companies interested in supporting small groups, whereas the Europeans were interested in large systems to support organizations and government agencies. We stumbled over differences in priorities and methods, and separated. ECSCW formed the CSCW journal with no editors from North America and for four years, only Chris Neuwirth of CMU and I were on both conference program committees. In the late 1990s, we realigned: CSCW saw the significance of the organizational contexts of workgroups, ECSCW saw organizations acquiring commercial ""groupware,"" and both saw a role for workplace ethnographies. Ten years ago they again diverged: CSCW engaged with social media, crowdsourcing, and games, while ECSCW remained focused on work. They overlap—for example, the healthcare domain attracts both. However, the split was difficult for someone who had worked on both continents and been part of both communities.\nCHI and AI. Working as an HCI developer or researcher in AI groups and labs for years, I participated in some Intelligent User Interface (IUI) and other conference events intended to bridge HCI and AI. It never happened. IUI is an active community unto itself, not a forum for the broader CHI and AI communities.\nHICSS, a forum for CHI, MIS, and software engineering. The annual Hawaii International Conference on Systems Sciences has at times been a unique community bridge for subcommunities of CHI, MIS, and software engineering. I organized HICSS mini-tracks and gave papers in other mini-tracks organized by CHI and MIS researchers. HICSS has endured, but these efforts were of limited duration.\nCHI and design. CHI reached out to design in the 1990s by sponsoring the Designing Interactive Systems conference series, but when few professional designers participated, it settled on software design. In the early 2000s, Richard Anderson led three impressive efforts to bridge CHI and design. He brought together CHI, SIGGRAPH, and AIGA members for three successful Design of UX (DUX) conferences. His two-day Development Consortium prior to CHI 2005, which drew HCI researchers and practitioners from the graphics, design, human factors, management, and computer science communities, led to CHI efforts to recruit more participation from the others—without lasting impact. His co-editorship of Interactions heightened its emphasis on design, which has continued and is not reliant on conferences.\nACM communities consider highly polished conference papers the pinnacle for most work, whereas other fields prize journal publication.\nCHI and information science. In the late 2000s, Schools of Information proliferated and spawned a successful conference series. The iSchools are polyglot, drawing faculty from all of the fields listed above. Seeing a strong potential connection between the CHI and iSchool communities, we increased CHI participation and shepherded iConference proceedings into the ACM Digital Library when I co-chaired iConference 2011. These bridges soon crumbled.\nMIS and CHI in the 2000s. Organizational IT focused on internal enterprise systems until e-commerce introduced online customers and suppliers. The MIS field, now called information systems, faced the task that CHI confronted in the 1980s: designing software for discretionary users. In 2001, the Association of Information Systems formed AIS SIGHCI with the goal of building a bridge to the CHI community. Several from CHI participated, but after a few years this bridge, like others, failed. A new generation of AIS SIGHCI leaders has revived this effort with the CHI 2018 workshop mentioned above and discussed below .\nSome see a bridge to another community as an opportunity to proselytize and convert, but many see an opportunity for mutual learning. What gets in the way? Long after conference-bridge alliances came apart, I contacted people who had participated for a while to ask why they stopped.\nA serious impediment is that ACM communities consider highly polished conference papers the pinnacle for most work, whereas other fields prize journal publication and use conferences for community building: presenting work in progress, getting feedback, and hearing about related work. Computer science remained journal oriented longer in Europe and still is in much of Asia. Human factors and MIS researchers responded to CHI and CSCW rejections in one of two ways: ""They don\'t like what I do"" or ""To polish my work enough to get it accepted, I would be most of the way to a journal publication that counts much more in my discipline than any conference paper, much less one in a different field."" Portfolio-oriented design disciplines also encountered withering rejection rates. Completing the disconnect, CHI and CSCW researchers who attended conferences in the other fields castigated the unpolished works-in-progress as shoddy.\nPeople who see where another field overlaps with their own may not see where they differ in priorities, methods, and theories. For example, one field is more interested in initial user experiences, another in making routine use more efficient. Common ground exists, but expectations are not met if the differences are not understood. For example, human factors and information systems researchers who attend CHI and CSCW conferences criticize the small participant samples and poor statistical analyses, the ""unscientific"" thinking-aloud method, and the lack of theoretical grounding in CHI studies.\nLinguistic and cultural differences are significant even among fields relying on English and the culture of science. Common terms can have markedly different meanings, leading to significant misunderstandings [2,3]. Culturally, the ""man-machine"" disciplines that arose in the \'50s and early \'60s and were focused on military and government systems alienated many in the Vietnam-era generation that populated CHI and avoided the use of male generics. Stylistic differences tripped up reviewers. For example, MIS emphasized theory, enumerated hypotheses, and created acronyms liberally. ACM reviewers were unable to overcome irritation with these conventions and felt the authors were dissing our conferences by not conforming to our conventions.\nThese barriers struck the people I spoke with as significant, and many remain in place. Today, another obstacle prevents fields from connecting via mutual conference participation.\nEngineers study bridge failures. They learn from them, yet bridges continue to collapse, often because new materials and production techniques introduce fresh uncertainties. Dynamic change can render experience obsolete. This magnifies a deal-breaking barrier to conference bridges: the role of conferences in maturing fields.\nAn emerging discipline confronts core issues. A more established discipline has reached a common understanding of its core issues, priorities, and methods. A new discipline seeing shared interests seeks to discuss foundational issues and contribute new ideas. An established field has little interest in that; its conferences report on new, specialized explorations. Two examples follow.\nIn 1983, CHI was an emerging field populated by psychologists and a few computer scientists. Human factors was an established field with an extensive literature. Human factors researchers could read the entire CHI literature in a day, but the reverse was not true. Not knowing the history, CHI researchers had difficulty interpreting human factors papers. We did not get the background at conferences.\nIn 2001, SIGCHI was decades old and AIS SIGHCI was the new field. I could read the SIGHCI literature in a day, but the CHI literature was extensive. SIGHCI was exploring the implications as the Web brought industry IT into its first contact with ""end users."" CHI had dealt with initial end-user contact in the 1980s and moved on. Conference attendance wasn\'t an effective bridge.\nThe gap is greater when two mature fields consider aligning. Each has a common understanding that its members rarely consciously think about. Discussions that could be fruitful won\'t naturally arise through joint participation at conferences where most presenters are graduate students. It is time to consider different approaches.\nGroups can collaborate by working together—or by division of labor, by creating a standard interface that enables them to exchange materials without discussion. When researching a system component, I would like a predictable interface to the components that others are working on. I do not want to rely on their experimental versions; I want to plug my current version into an existing system and see what difference it makes. If I need a machine-learning component to improve my medical application, I would like basic information and an API, not detailed descriptions of the latest tests of new deep-learning algorithms.\nWhen work in another discipline might be relevant to yours, which would you rather do: a) attend a course covering the other discipline; b) find, learn from, and work with someone in the other discipline; or c) attend research conferences to hear presentations by students in the other discipline who assume a shared understanding that you lack and are narrowly focused on experiments around the edges? I\'m leading the witness, but if c) seems unpromising, the conference-attendance approach to community exchange could be doomed... unless the conference meets in Hawaii every January.\nA field could create a one-day course that describes its fundamental concepts; surfaces its shared understandings, priorities, methods, and terminologies; and surveys topics that are currently actively engaged. This traveling bridge could be taken to other disciplines as pre-conference events for those interested, with other disciplines in turn appearing at its conference.\nMore energizing and quicker than going back to school could be option b), for which a model exists. In a hackathon, an organization sets aside two to five days for employees to work on self-selected projects in self-organized teams to produce something concrete. Teams attract people with different roles and skills. The creative energy and accomplishment of these sanctioned skunkworks projects can be impressive.\nTwo or more fields could agree on a significant large-scale challenge that engages a substantial group to converge for two or three days to carry it out, perhaps prior to a conference. The task could be to create a standard, draft and polish a major policy statement, or design and build a system to support a humanitarian cause. In the course of producing a tangible result, participants learn about each other. Would you consider signing on? It might have to rely on volunteers. Grant funding for large efforts is nice when available, but incentives change and groups often align only long enough to get the funding.\nMost past efforts to construct enduring associations through conference bridges lasted only a few years. The hurdles of publication venues, terminology, differing priorities and cultures, and overcoming the maturity gaps wore us down. But participants learned something about each other and many acquired a sense of another discipline\'s potential, should a future need arise. Our expectations were too high for us to declare victory and get out, but our efforts arguably served a purpose.\nThe CHI 2018 ""Bridging HCI"" workshop organized by AIS SIGHCI emphasized the conference-bridge model but also discussed the benefits of forming multidisciplinary teams and the hackathon model. Other topics included ways to overcome submission rejection and other challenges described above. Position papers by attendees from SIGCHI and SIGHCI fields are at http://www.bridginghci.org/papers/.\nNew communication and collaboration tools can support effective sharing and working across groups. The pace of change in technology use makes it desirable, if not imperative, to balance the growth of specialized efforts with efforts to connect them.\n1. Djamasbi, S., Galletta, D.F., Nah, F.F-H., Page, X., Robert Jr., L.P., and Wisniewski, P.J. Bridging a bridge: Bringing two HCI communities together. Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, New York, 2018, Paper W23; https://doi.org/10.1145/3170427.3170612. See also http://www.bridginghci.org/papers/.\nJonathan Grudin is a principal researcher at Microsoft and an affiliate professor at the University of Washington. He is an ACM Fellow and a member of the ACM SIGCHI CHI Academy. His book From Tool to Partner: The Evolution of Human-Computer Interaction was published in 2017. email@example.com\n©2018 ACM 1072-5520/18/09 $15.00\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\nThe Digital Library is published by the Association for Computing Machinery. Copyright © 2018 ACM, Inc.']"	['<urn:uuid:7d4c1470-2584-45f6-9f00-99499d33bc4f>', '<urn:uuid:4eede221-df63-4f01-986f-81a186bc3442>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T04:56:11.774636	30	69	3286
49	I need to work with content from multiple creative programs - what file formats can I import and export when working with this software?	Blender supports a wide range of import/export formats. For images, it handles JPEG, JPEG2000, PNG, TARGA, OpenEXR, DPX, Cineon, Radiance HDR, SGI Iris, and TIFF. Video formats include AVI, MPEG and Quicktime (on OSX). For 3D files, it supports 3D Studio (3DS), COLLADA (DAE), Filmbox (FBX), Autodesk (DXF), Wavefront (OBJ), DirectX (x), Lightwave (LWO), Motion Capture (BVH), SVG, Stanford PLY, STL, VRML, VRML97, and X3D.	['Blender is a free and open-source 3D animation suite software. It supports the entirety of the 3D pipeline—modeling, rigging, animation, simulation, rendering, compositing and motion tracking, even video editing and game creation.\nAdvanced users employ Blender’s API for Python scripting to customize the application and write specialized tools; often these are included in future releases. It is well suited to individuals and small studios who benefit from its unified pipeline and responsive development process.\nBlender is cross-platform and runs equally well on Linux, Windows, and Macintosh computers. Its interface uses OpenGL to provide a consistent experience. It has no price tag, but you can invest, participate, and help to advance a powerful collaborative tool: Blender is your own 3D software.\nFeature of Blender:\nBlender now features a powerful new unbiased rendering engine called Cycles that offers stunning ultra-realistic rendering.\nBlender’s comprehensive array of modeling tools make creating, transforming and editing your models a breeze.\nWith this software new rendering engine, the possibilities for materials are endless.\nTransforming a model into a possible character has never been easier!\nBlender offers an impressive set of rigging tools including:\n- Envelope, skeleton and automatic skinning\n- Easy weight painting\n- Mirror functionality\n- Bone layers and colored groups for organization\n- B-spline interpolated bones\nBlender’s animation feature set offers:\n- Character animation pose editor\n- Non-Linear Animation (NLA) for independent movements\n- IK forward/inverse kinematics for fast poses\n- Sound synchronization\nExperience the joy of sculpting organic subjects using the built-in sculpting feature set of Blender.\nFast UV Unwrapping\nEasily unwrap your mesh right inside Blender, and use image textures or paint your own directly onto the model.\nThe blender comes with a fully-fledged compositor built right in. That means no more exporting to third party programs, you can do it all without leaving the program.\nWhether you need a crumbling building, rain, fire, smoke, fluid, cloth or full-on destruction, Blender delivers great-looking results.\nIncluded in Blender is a complete game engine, allowing you to create a fully-featured 3d game right inside Blender.\nCamera and Object tracking\nThis software now includes a production-ready camera and object tracking. Allowing you to import raw footage, track the footage, mask areas and see the camera movements live in your 3d scene. Eliminating the need to switch between programs.\nLibrary of Extensions\nWith a large community of enthusiasts and developers, It comes loaded with a vast array of extensions that you can turn on or off easily.\nNovice and advanced users will love the ability to customize their layout completely. From simply splitting their viewport, to fully customizing it with python scripting, so it works for you.\nIt comes packed with import/export support for many different programs. Image: JPEG, JPEG2000, PNG, TARGA, OpenEXR, DPX, Cineon, Radiance HDR, SGI Iris, TIFF. Video: AVI, MPEG and Quicktime (on OSX). 3D: 3D Studio (3DS), COLLADA (DAE), Filmbox (FBX), Autodesk (DXF), Wavefront (OBJ), DirectX (x), Lightwave (LWO), Motion Capture (BVH), SVG, Stanford PLY, STL, VRML, VRML97, X3D.\n|Processor||64-bit quad-core CPU|\n|Memory||2 GB RAM (8 GB RAM Recommended)|\n|Graphics Card||OpenGL 3.2 compatible card with 2 GB video RAM (CUDA or OpenCL for GPU rendering)|\n|Display||1920×1080 pixels, 24-bit color|\n|OpenGL Version||2.1 (Blender 2.77 up to 2.79b)|']	['<urn:uuid:5b2c29de-fa7b-4cee-8ef8-966783508fae>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	24	65	533
50	how can infections spread in personal care and body art businesses when instruments not cleaned properly	Infections can spread in several ways when instruments are not properly handled: when contaminated instruments are not effectively cleaned and sterilized before use on another person, when clean instruments are placed on unclean surfaces, when sterile instruments are placed on unsterile surfaces or contact unsterile instruments, when operators share equipment or materials, and when used and clean instruments come into contact. Additionally, infection can occur when contaminated dressings, spatulas and single-use gloves are not disposed of immediately and when towels and other articles are not changed or thoroughly cleaned between clients.	['Personal care and body art businesses need to follow certain infection prevention and control requirements.\nAnyone working in such a business should download the full Health guidelines for personal care and body art industries and be familiar with its contents. The main points are summarised on this page.\nClient information sheets\nAs part of enhancing public awareness of infection risks, information sheets have been developed for clients receiving particular procedures. In some instances, clients must be provided this information in writing.\nView the copies of client information sheets.\nInfection prevention and control, and its importance\nPersonal care and body art businesses should supply professional, competent, safe and hygienic practices in clean premises. Unsafe or unhygienic practices can lead to the spread of infectious diseases, which can affect the health of the client and of the operator.\nViruses such as hepatitis B, hepatitis C and HIV can spread by blood-to-blood contact. Therefore, staff need to know the precautions required for any procedure that may involve skin penetration and possible blood contamination.\nPprocedures that do not involve skin penetration may spread infections such as:\n- staphylococcal infections – for example, impetigo\n- wart and herpes viruses\n- fungal infections – for example, tinea.\nInfection prevention and control – basic requirements\nLegislative requirements for personal care and body art businesses include:\n- The premises must be kept clean.\n- Any article used for penetrating the skin must be sterile at the time of use.\n- Any article that has penetrated the skin or is contaminated with blood or body fluids/substances must be disposed of immediately after use, or cleaned and sterilised before being used on another person.\n- Any other used article must be cleaned before being used on another person.\n- Operators must keep themselves and their clothing clean, and have no exposed cuts, abrasions or wounds.\n- Proprietors must provide written health information to each client about the potential health risks associated with skin penetration procedures.\nInfections – how they occur\nInstruments that penetrate the skin become contaminated with blood, or with body fluids or substances.\nInfection may then occur when:\n- strict operator hygiene is not observed\n- operators share the same equipment or materials\n- used and clean instruments come into contact with one another\n- clean instruments are placed on unclean surfaces\n- sterile instruments are placed on unsterile surfaces or come into contact with unsterile instruments\n- contaminated dressings, spatulas and single-use gloves are not disposed of immediately and appropriately after use\n- the structural facilities, furnishings and fittings of the premises cannot be, or are not, adequately cleaned between clients\n- towels and other articles used on clients are not changed or thoroughly cleaned between clients.\n- contaminated instruments are not effectively cleaned and sterilised before use on another person\n- single-use instruments are not discarded immediately after use.\nContact between blood and contaminated instruments, and then with open cuts, sores or broken skin, can also lead to infection. Both you and your client could become infected.\nRemember that blood does not have to be visible on an instrument or needle for infection to be transmitted.\nContamination is the spread of microorganisms from one item to another. Contamination can occur when:\nThe guidelines also refer to single-use items. Any item marked by its manufacturer as being for single-use must not be cleaned and sterilised for reuse on the same client at another time or on any other client.\nBenefits of compliance with these guidelines\nBusinesses have a legal responsibility to provide a safe service. A risk management approach enables them to take all reasonable precautions.\nIdentifying potential hazards and managing them reduces the likelihood of untoward incidents. Further, the provision of a consistent quality service improves both business reputation and client loyalty.\nThe cost of making equipment safe is small compared with the cost of infecting yourself or a client.']	['<urn:uuid:7303b340-2249-4c1a-ad16-ffc64484d8e2>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	16	91	644
51	Which materials are most commonly used in business packaging?	Plastics segment held the dominant position with 45.0% market share, followed by paper and paperboard with 29.12% market share. Other materials used include stainless steel and wood.	"['Global Industrial Packaging Market – Insights\nIndustrial packaging is essential for the packaging of industrial goods and products, during or after the manufacturing process. Industrial packaging is different from others types of packaging. It mainly focusses on the strength and thickness of the materials used along with secured lock mechanisms and closures. Various industries use high-quality packaging to meet international packaging standards of providing protective packaging. Industrial goods are often bulky, heavy, and sensitive to hazards or external contacts, thus industrial packaging involves the use of different material such as stainless steel, wood, and plastics.\nRapid surge in the construction industry is one of the major factors propelling growth in industrial packaging market. According to Coherent Market Insights’ analysis, the global construction industry is projected to increase its market with a CAGR of 5.8% during the period 2016 to 2021. Moreover, increasing popularity of recyclable packaging materials (plastics) and biodegradable packaging materials such as paper & paperboard materials along with ever-expanding industrialization in the emerging economies such as India, China, and Brazil will boost growth of the market in the near future. Other factors such as vast consumption of packaged food in food and beverages industry and advancements in technology such as nanotechnology in printing and films will aid industrial packaging with new opportunities.\nHowever factors such as inadequate marketing and branding of products, lack of proper distribution channels, unavailability of skilled manpower, and fluctuating prices of the raw materials used for the manufacturing of industrial packaging products will hamper growth of market.\nAsia Pacific holds the largest market share in global industrial packaging market. It is projected to exhibit the fastest CAGR of 7.4% during the forecast period. The emerging economies in Asia Pacific such as India, China, South Korea, Vietnam, Malaysia, and Thailand are expected to propel the growth of industrial packaging market in the near future. Furthermore, industries such as petrochemicals, automotive, agriculture, oil and gas, food and beverages, chemicals, and metal fabrication have a high requirement for transportation, safe handling, and storage. These factors augments the market growth in Asia Pacific.\nAmong materials, plastics segment held the dominant position in 2016, with 45.0% market share. Availability of numerous grades of plastic, easy handling due to light weight, and due to its chemical inert property, are some of the major factors driving the demand for this segment.. Paper and paperboard accounted as the second largest segment in 2016, with 29.12% market share. This is mainly owing to its eco-friendly and biodegradable materials, which are used by many end-use industries.\nAmong the end users, chemicals and pharmaceutical industries has the largest market share in the industrial packaging market, owing to high adoption rate of industrial packaging products. According to American Chemistry Council (ACC), due to availability of cheap and abundant ethane and shale gas, there is boom in chemical industry in U.S, with starting of 294 new projects by 2017. According to U.S International Trade Administration, the world market of pharmaceuticals witness a growth rate of 4.9%, and is projected to grow from US$ 1 trillion to US$ 1.3 trillion from 2015 to 2020. Thus, increase in the demand for packaging of chemical and pharmaceutical goods is expected to drive the growth of industrial packaging market over the forecast period.\nFigure 1. Global Industrial Packaging Market Share, By Material, 2016\nPlastic held the dominant position in 2016 and is expected to retain its dominance during the forecast period. Packaging holds a revenue of US$ 25.5 billion and is projected to exhibit a CAGR of 5.4% over the forecast period.\nAsia Pacific held the dominant position in the global industrial packaging market in 2016 and is expected to retain its dominance throughout the forecast period.\nMajor Players in the Global Industrial Packaging Market\nSome of the major players operating in the global Industrial packaging market include AmeriGlobe LLC, Beacon Converters, Bemis Company, Aphena Pharma Solutions, Cascades Inc., International Paper Company, United Drug Plc, and Sigma Plastics Group.\nIndustrial packaging involves the use of high standards and quality packaging material in order to provide protective packaging. However, packaging is not solely associated with the product safety aspects, while it also provide better product storage, safe material handling, enhanced product quality, and easy transportation of a wide range of products from industrial machinery to various appliances of daily use.\nSome of the major factors fuelling the industrial packaging market growth include construction industry, increasing trend of recyclable packaging materials such as green packaging, increasing consumption of packaged food items, growing demand for pharmaceutical and medical products, and growing industrialization in the emerging economies such as India, China, and Brazil.\nAmong end-use industry, food & beverages industry accounted as the major segment in the industrial packaging market. This is mainly due to the rise in the use of ready-to-use products, growing concerns about the safety and quality of packaged products from manufacturing to consumption of the food.\nPackaging plays an important role in protecting fruits and vegetables from damage. Corrugated boxes are mainly used for the packaging of fruits and vegetables, due to its stacking strength, serviceability, and low cost of production. Global agricultural packaging market is estimated to exhibit a CAGR of 5.8% over the forecast period, which in turn, expected to fuel the market growth.\nImport and export is the major factor augmenting the growth of industrial packaging market. According to the United Nations Economic and Social Commission for Asia and the Pacific (UNESCAP), the nominal exports and imports are expected to rise at 4.5% and 6.1% respectively by 2017.\nThe report segments the global Industrial packaging market on the basis of product type, material, end-use industry, and geography. On the basis of product type, the market is segmented into drums, pails, tubes, containers, sacks, crates, bulk boxes, intermediate bulk containers (IBCs), plastic pallets, and others. On the basis of material, the market is segmented into plastics, metals, wood, paper and paperboard, fiber and fiberboard, and others. On the basis of end-user industry, the market is segmented into automotive, electronics, construction, industrials, food and beverages, chemicals, oil & lubricants and agriculture and horticulture. On the basis of geography, the market is segmented into North America, Latin America, Europe, Asia Pacific, Middle East, and Africa.\nKey features of the study:\n“*” marked represents similar segmentation in other categories in the respective section.\nTable of Contents\n* Browse 150 market data tables* and 90 figures* on ""Industrial Packaging Market” - Global forecast to 2025.\nCoherent Market Insights followsa comprehensive research methodology focused on providing the most precise market analysis. The company leverages a data triangulation model which helps company to gauge the market dynamics and provide accurate estimates. Key components of the research methodologies followed for all our market reports include:\nIn addition to this, Coherent Market Insights has access to a wide range of the regional and global reputed paid data bases, which helps the company to figure out the regional and global market trends and dynamics. The company analyses the industry from the 360 Degree Perspective i.e. from the Supply Side and Demand Side which enables us to provide granular details of the entire ecosystem for each study. Finally, a Top-Down approach and Bottom-Up approach is followed to arrive at ultimate research findings.\nCoherent Market Insights desk research is based on a principle set of research techniques:\nCoherent Market Insights has a large amount of in-house repository of industry database. This is leveraged as a burner for initiating a new research study. Key secondary sources include:\nPreliminary Data Mining\nThe raw data is obtained through the secondary findings, in house repositories, and trade surveys. It is then filtered to ensure that the relevant information including industry dynamics, trends, and outlook is retained for further research process.\nHolistic approach is used to ensure that the granular and uncommon parameters are taken into consideration to ensure accurate results. The information from the paid databases are further combined to the raw data in order to standardize it.\nCoherent Statistical model\nWe arrive at our final research findings through simulation models. Coherent Data Analytics Model is a statistical tool that helps company to forecast market estimates. Few of the parameters considered as a part of the statistical model include:\nOnce the findings are derived from the statistical model, large volume of data is process to confirm accurate research results. Data analytics and processing tools are adopted to process large chunk of collected informative data. In case, a client customizes the study during the process, the research finding till then are benchmarked, and the process for new research requirement is initiated again.\nThis is the most crucial stage of the research process. Primary Interviews are conducted to validate the data and analysis. This helps in achieving the following purposes:\nThe primary research is conducted with the ecosystem players including, but not limited to:\nFinding it difficult to find the research that would cater to your business demands? Give us a chance to help you. One of our Research Consultants will connect to provide a customized solution.\nTalk to our research consultant to design an exclusive report as per your research needs.\nWe aim to fulfil client\'s research demands with tailored research solutions.\nWe aim to provide research studies in quickest turnaround time and in a much cost effective manner.\nWe cover each industry from supply and demand side with an aim to provide a most holistic research study.\nWe strive to provide most accurate and reliable research findings in our research reports.']"	['<urn:uuid:62108ecd-8742-4e49-b62f-a2d8311dac78>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	9	27	1577
52	dunhuang manuscripts current location museums	The majority of manuscripts from the Dunhuang cave are now housed in Beijing, Paris, London and St. Petersburg, with sizeable collections in Japan. Additional Silk Road documents and paintings can be found in museums and libraries throughout Europe, Asia and America.	"['INTERNATIONAL DUNHUANG PROJECT\nIntroduction: Dunhuang and the Cave of\nInformation from the International Dunhuang Project website:\nDunhuang is an oasis town in Chinese Central Asia west of Chang\'an (Xian), a former capital of China. The Gobi Desert lies to its northeast and the Taklamakan Desert and Tarim Basin to the west. The silk road coming from Chang\'an split at Dunhuang to follow the northern and southern borders of the desert where the rivers and spring waters used to support sizeable towns.\nBetween the 5th and 11th centuries Buddhist monks and devotees hewed scores of cave temples out of a cliff face a few kilometres outside the town. After this the site was little used until a secret sealed-up cave was discovered in 1900. Its entrance from the side wall of another cave had been plastered over and disguised but, when opened, a small cave crammed with ancient manuscripts, printed documents and paintings was open to view for the first time since the early 11th century. Wang Yuanlu, the Daoist priest who discovered the cave, gave some of the manuscripts and paintings to local Chinese officials over the next few years, including the magistrate of Dunhuang, Wang Li\'an (IDP News 7).\nThe cave\'s discovery coincided with a period of great international archaeological research in the area and Sir Aurel Stein was the first foreigner to gain access in 1907. Thereafter archaeologists from France, China, Russia and Japan were drawn to Dunhuang, and the great majority of manuscripts and documents from this one cave are now in Beijing, Paris, London and St. Petersburg, with sizeable collections in Japan. Documents and paintings from other Silk Road towns are to be found more widely in museums and libraries throughout Europe, Asia and America.\nApart from 14,000 paper scrolls and fragments from this cave at Dunhuang, the British Library Stein collection includes several thousand woodslips and woodslip fragments with Chinese writing, thousands of Tibetan and Tangut manuscripts, Prakrit wooden tablets in Brahmi and Kharosthi scripts, along with documents in Khotanese, Uighur, Sogdian and Eastern Turkic. All this material is included [via digitalisation] in The International Dunhuang Project and is being entered onto the IDP Database.\nComments from TWC, webmaster:\nThe dispersal of the Dunhuang Collection was a tragic event and many have described it as blatant robbery. However, in the light of events, these manuscripts also escaped the wars and civil unrest that plagued China throughout most of the 20th century. The rights or wrongs of the rape of Dunhuang will continue to be debated among historians and politicans for ages to come. The IDP undertaken by the British Library is one that will attempt to address the issue on hand - how to allow scholars and amateur history fans like me study this widely dispersed collection. The IDP seeks sponsorships from individuals or organisations and will add appropriate dedications onto the digitalised images. This is not unlike the early Silk Road pilgrims\' practices of building or commissioning of religious works of art to their deities or families. For details of the IDP project and the sponsorship scheme, visit the International Dunhuang Project website.\nBelow is the manuscript I have dedicated to my\nMS. no. and section Or.8210/P.8/R.1\nDate: Western (yy,mm,dd) 947.8.4\n""O Most Holy Vaisramana, Devaraja of the Northern Quarter, who commandest all the various classes of demons and spirits under Heaven, may those of us, thy followers, who can make their devotions and prayers to thee in respectful humility of heart, be all revarded with happiness and prosperity. The disciple Cao Yuanzhong of Qiaojun, Governor of the military district of Guiyi, specially promoted additional Grand Preceptor, having requested the artificer to carve this block for printing, now prays that the country may enjoy peace and its inhabitants prosper, that the spirits of land and grain may continually flourish, that the highways may be safe, and that peace and happiness may prevail in every clime. - Recorded on the 15th of the 7th month of \'dingwei\', the 4th year of Kaiyun in the Great Jin dynasty [4 Aug. 947].""\nComments by Dr Susan Whitfield:\nBack to TWC\'s Homepage']"	['<urn:uuid:ad54d9ba-d75f-4485-8098-da929ef33e29>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	5	41	684
53	I need to understand different data processing frameworks. What are the key differences between Graph Specification and Apache Spark when it comes to handling and processing data?	Graph Specification and Apache Spark handle data processing in distinctly different ways. Graph Specification uses an entity-component design pattern where data processing is handled through nodes connected by edges, with components defining the functionality. Data flows between nodes through sources (outputs) and sinks (inputs), and computation is performed by specific compute components. Meanwhile, Apache Spark processes data through its distributed computing framework, using Resilient Distributed Datasets (RDDs) stored in memory across worker nodes. Spark excels at large-scale data processing, running applications up to 100 times faster in memory and 10 times faster on disk compared to traditional methods. While Graph Specification focuses on modular, extensible AI applications with components that can be shared across applications, Spark provides a broader set of tools including SQL queries, machine learning (through MLlib), streaming data processing, and graph algorithms through GraphX. Graph Specification emphasizes application structure through its graph-based representation, while Spark emphasizes high-speed distributed processing through its in-memory computation model.	['Graph Specification is a format to describe high-performance AI applications in a modular and extensible way. It allows writing applications in a standard format and sharing components across multiple applications without code modification. Graph Specification is based on entity-composition pattern. Every object in graph is represented with entity (aka Node) and components. Developers implement custom components which can be added to entity to achieve the required functionality.\nThe graph contains nodes which follow an entity-component design pattern implementing the “composition over inheritance” paradigm. A node itself is just a light-weight object which owns components. Components define how a node interacts with the rest of the applications. For example, nodes be connected to pass data between each other. A special component, called compute component, is used to execute the code based on certain rules. Typically a compute component would receive data, execute some computation and publish data.\nA graph is a data-driven representation of an AI application. Implementing an application by using programming code to create and link objects results in a monolithic and hard to maintain program. Instead a graph object is used to structure an application. The graph can be created using specialized tools and it can be analyzed to identify potential problems or performance bottlenecks. The graph is loaded by the graph runtime to be executed.\nThe functional blocks of a graph are defined by the set of nodes which the graph owns. Nodes can be queried via the graph using certain query functions. For example, it is possible to search for a node by its name.\nA subgraph is a graph with additional node for interfaces. It points to the components which are accessible outside this graph. In order to use a subgraph in an existing graph or subgraph, the developer needs to create an entity where a component of the type\nnvidia::gxf::Subgraph is contained. Inside the Subgraph component a corresponding subgraph can be loaded from the\nyaml file indicated by location property and instantiated in the parent graph.\nSystem makes the components from interface available to the parent graph when a sub-graph is loaded in the parent graph. It allows users to link sub-graphs in parent with defined interface.\nA subgraph interface can be defined as follows:\n--- interfaces: - name: iname # the name of the interface for the access from the parent graph target: n_entity/n_component # the true component in the subgraph that is represented by the interface\nGraph Specification uses an entity-component design principle for nodes. This means that a node is a light-weight object whose main purpose is to own components. A node is a composition of components. Every component is in exactly one node. In order to customize a node a developer does not derive from node as a base class, but instead composes objects out of components. Components can be used to provide a rich set of functionality to a node and thus to an application.\nComponents are the main functional blocks of an application. Graph runtime provides a couple of components which implement features like properties, code execution, rules and message passing. It also allows a developer to extend the runtime by injecting her own custom components with custom features to fit a specific use case.\nThe most common component is a codelet or compute component which is used for data processing and code execution. To implement a custom codelet you’ll need to implement a certain set of functions like start and stop. A special system - the scheduler - will call these functions at the specified time. Typical examples of triggering code execution are: receiving a new message from another node, or performing work on a regular schedule based on a time trigger.\nNodes can receive data from other nodes by connecting them with an edge. This essential feature allows a graph to represent a compute pipeline or a complicated AI application. An input to a node is called sink while an output is called source. There can be zero, one or multiple inputs and outputs. A source can be connected to multiple sinks and a sink can be connected to multiple sources.\nAn extension is a compiled shared library of a logical group of component type definitions and their implementations along with any other asset files that are required for execution of the components. Some examples of asset files are model files, shared libraries that the extension library links to and hence required to run, header and development files that enable development of additional components and extensions that use components from the extension.\nAn extension library is a runtime loadable module compiled with component information in a standard format that allows the graph runtime to load the extension and retrieve further information from it to:\nAllow the runtime to create components using the component types in the extension.\nQuery information regarding the component types in the extension:\nThe component type name\nThe base type of the component\nA string description of the component\nInformation of parameters of the component – parameter name, type, description etc.,\nQuery information regarding the extension itself - Name of the extension, version, license, author and a string description of the extension.\nThe section :doc: GraphComposer_Dev_Workflow talks more about this with a focus on developing extensions and components.\nGraph file stores list of entities. Each entity has a unique name and list of components. Each component has a name, a type and properties. Properties are stored as key-value pairs.\n%YAML 1.2 --- name: source components: - name: signal type: sample::test::ping - type: nvidia::gxf::CountSchedulingTerm parameters: count: 10 --- components: - type: nvidia::gxf::GreedyScheduler parameters: realtime: false max_duration_ms: 1000000', 'The following topics will be covered in this blog:\nWhat is Spark Framework?\nApache Spark is a fast, flexible, and developer-friendly leading platform for large-scale SQL, machine learning, batch processing, and stream processing. It is essentially a data processing framework that has the ability to quickly perform processing tasks on very large data sets. It is also capable of distributing data processing tasks across multiple computers, either by itself or in conjunction with other distributed computing tools.\nTo crunch through large data stores requires the marshaling of enormous computing power. Some of the programming burdens of these tasks are taken by Spark. Working with an easy-to-use API eliminates much of the menial and high-intensity work of distributed computing and big data processing.\nFollowing are the features of Apache Spark:\n- Speed: Apache Spark helps run applications in the Hadoop cluster up to 100 times faster in memory and 10 times faster on disk. This is due to the ability to reduce the number of reads or write operations to the disk. The intermediate processing data is stored in memory.\n- Supports multiple languages: it provides built-in APIs in Java, Python, or Scala, opening up the options to write applications in different languages. For interactive querying, Spark comes with 80 high-level operators.\n- Advanced analytics: It supports MapReduce, SQL queries, machine learning, streaming data, and graph algorithms.\nSpark as a whole consists of various spark tools, libraries, APIs, databases, etc. The main components of Apache Spark are as follows:\nSpare Core is the basic building block of Spark, which includes all components for job scheduling, performing various memory operations, fault tolerance, and more. Spark Core is also home to the API that consists of RDD. Moreover, It provides APIs for building and manipulating data in RDD.\nApache Spark works with the unstructured data using its ‘go to’ tool, Spark SQL. It allows querying data via SQL, as well as via Apache Hive’s form of SQL called Hive Query Language (HQL). It also supports data from various sources like parse tables, log files, JSON, etc. It allows programmers to combine SQL queries with programmable changes or manipulations supported by RDD in Python, Java, Scala, and R.\nSpark Streaming processes live streams of data. Data generated by various sources is processed at the very instant by Spark Streaming. Examples of this data include log files, messages containing status updates posted by users, etc.\nGraphX is Apache Spark’s library for enhancing graphs and enabling graph-parallel computation. Apache Spark includes a number of graph algorithms that help users in simplifying graph analytics.\nApache Spark comes up with a library containing common Machine Learning (ML) services called MLlib. It provides various types of ML algorithms including regression, clustering, and classification, which can perform various operations on data to get meaningful insights out of it.\nApache Spark Architecture\nApache Spark has a master-slave architecture with its cluster made up of a single master and multiple slaves. This architecture relies on two abstractions:\n- Resilient Distributed Dataset (RDD)\n- Directed Acyclic Graph (DAG)\nResilient Distributed Datasets (RDD)\nThe Resilient Distributed Datasets consist of the set of data items that are stored on worker nodes in memory. Let’s looks at the meaning and function behind the term, RDD:\n- Resilient: Restores data on failure\n- Distributed: Data is distributed among different nodes\n- Dataset: Group of data\nDirected Acyclic Graph (DAG)\nDirected Acyclic Graph is a finite directed graph that holds the track of operations applied on RDD. It works on a sequence of data computations and has an arrangement of edges and vertices. The vertices represent RDDs and edges represent the operations applied on the RDD.\nHere, the graph refers to the navigation and directed and acyclic refers to how it is done. Let’s take a look at the Spark architecture.\nLet’s take a look at some of the other important elements in the above architecture.\nThe Driver Program runs the application’s main() function. It creates the SparkContext object, the purpose of which is to coordinate the Spark applications that run independently on a cluster of assets of processes. To run on a cluster, the SparkContext links to a different cluster manager type and then performs the following operations: –\n- Acquires executors on nodes in the cluster\n- Sends the application code to the executors where the application code can be defined at this stage by Python or JAR files passed to the SparkContext\n- The SparkContext then sends the tasks to the executors to run\n- The cluster manager allocates resources across applications. Spark has the capability to run on a large number of clusters.\n- There are various types of cluster managers such as Apache Mesos, Hadoop YARN, and Standalone Scheduler.\n- The Standalone Scheduler is a standalone spark cluster manager enabling the installation of Spark on an empty set of machines.\n- The worker node is the slave node\n- It runs the application code in the cluster\n- It is a process that is launched for an application on a worker node\n- It runs tasks and stores data in memory or disk storage\n- It reads and writes data to the external sources\n- Every application has an executor\n- A unit of work that is sent to an executor\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nRDD stands for Resilient Distributed Dataset. It is an important data structure present in Spark. RDD is basically the fundamental data structure that helps Spark in the in-memory computation functionality. It lets Spark perform in-memory calculations on large clusters in a fault-tolerant environment. Precisely speaking, RDD helps Spark to facilitate faster and more efficient MapReduce operations.\nMachine Learning in Apache Spark is performed with the help of the Spark MLlib. It is the machine learning library of Spark and is highly scalable. The most special features of MLlib is its high level algorithms and lightning speed.The Spark MLlib includes all the popularly used ML algorithms and utilities. Some of the machine learning algorithms featured by MLlib are regression algorithms, pattern identification/mining algorithms, classification, clustering, and more. Not only does it consist of higher-level algorithms but also certain lower- level machine learning primitives such as generic gradient descent optimization algorithms.\nSome of the commonly used Spark MLlib tools are as below:\n- ML Algorithms\nSpark SQL is a module present in Apache Spark which is used for structured data processing. It performs the role of a distributed SQL query engine and helps in the easy integration of SQL query processing with machine learning. Basically, it helps in facilitating an easy integration of relational processing with Spark’s functional programming. It was originally created as Apache Hive to run on top of Spark but due to the limitations of Apache Hive, it was integrated with the Spark stack and thus has now replaced Apache Hive.\nSpark Core is the fundamental element of all the Spark components. It enables functionalities like task dispatching, input-output operations, scheduling and more. Other important functions performed by Spark Core are fault-tolerance, management of memory, managing job scheduling, storage system interaction etc. These functionalities of the Spark Core can be accessed through tools like Scala, Java APIs etc. To be precise, the Spark Core is the main execution engine of the entire Spark platform and the related functionalities of Spark.\nWhat is DAG in spark?\nDAG stands for Directed Acyclic Graph. It constitutes many vertices and edges, where the vertices present in the DAG define the RDDs (Resilient Distributed Datasets) and the edges denote the specific operations to be applied on a particular RDD. The working of the DAG is based on ‘actions’. Whenever an action is called, the DAG submits to the DAG Scheduler post which the graph is further split into executable tasks/steps.\nApache Hadoop and Apache Spark\nOne of the biggest challenges with respect to Big Data is analyzing the data. There are multiple solutions available to do this. The most popular one is Apache Hadoop.\nApache Hadoop is an open-source framework written in Java that allows us to store and process Big Data in a distributed environment, across various clusters of computers using simple programming constructs. To do this, Hadoop uses an algorithm called MapReduce, which divides the task into small parts and assigns them to a set of computers. Hadoop also has its own file system, Hadoop Distributed File System (HDFS), which is based on Google File System (GFS). HDFS is designed to run on low-cost hardware.\nApache Spark is an open-source distributed cluster-computing framework. It is a data processing engine developed to provide faster and easy-to-use analytics than Hadoop MapReduce. Before Apache Software Foundation took possession of Spark, it was under the control of the University of California, Berkeley’s AMPLab.\nLearn about Apache Spark from Apache Spark Training and excel in your career as an Apache Spark Specialist.\nHadoop vs Spark\nLet’s take a quick look at the key differences between Hadoop and Spark:\n- Performance: Spark is fast as it uses RAM instead of using disks for reading and writing intermediate data. Hadoop stores the data on multiple sources and the processing is done in batches with the help of MapReduce.\n- Cost: Since Hadoop relies on any disk storage type for data processing, it runs at a lower cost. On the other hand, Spark runs at a higher cost because it involves in-memory computations that require high quantities of RAM to spin up nodes for real-time data processing.\n- Processing: In both platforms, data processing happens in a distributed environment. While Hadoop is suitable for batch processing and linear data processing, Spark is ideal for real-time processing as well as processing live unstructured data streams.\n- Scalability: Hadoop is quickly able to accommodate when there is a rapid growth in data volume with the help of HDFS. Spark, on the other hand, relies on the fault-tolerant HDFS for larger volumes of data.\n- Security: Although Hadoop is more secure overall, Spark can integrate with it to reach a higher security level. Spark uses authentication via event logging or shared secret, while Hadoop makes use of multiple authentication and access control methods.\n- Machine learning (ML): When it comes to Machine Learning, Spark is the superior platform because it has MLlib to perform iterative in-memory ML computations. Its tools can also perform classification, regression, pipeline construction, persistence, evaluation, etc.\nHaving outlined all these drawbacks of Hadoop, it is clear that there was a scope for improvement, which is whySpark was introduced.\nPrepare yourself for the industry by going through Top Hadoop Interview Questions and Answers now!\nHow Spark Is Better than Hadoop?\n- In-memory Processing: In-memory processing is faster when compared to Hadoop, as there is no time spent in moving data/processes in and out of the disk. Spark is 100 times faster than MapReduce as everything is done here in memory.\n- Stream Processing: Apache Spark supports stream processing, which involves continuous input and output of data. Stream processing is also called real-time processing.\n- Less Latency: Apache Spark is relatively faster than Hadoop since it caches most of the input data in memory by the Resilient Distributed Dataset (RDD). RDD manages distributed processing of data and the transformation of that data. This is where Spark does most of the operations such as transformation and managing the data. Each dataset in an RDD is partitioned into logical portions, which can then be computed on different nodes of a cluster.\n- Lazy Evaluation: Apache Spark starts evaluating only when it is absolutely needed. This plays an important role in contributing to its speed.\n- Fewer Lines of Code: Although Spark is written in both Scala and Java, the implementation is in Scala, so the number of lines is relatively lesser in Spark when compared to Hadoop.\nWant to grab detailed knowledge on Hadoop? Read this extensive Spark tutorial!\nUse Cases of Apache Spark in Real Life\nMany companies use Apache Spark to improve their business insights. These companies gather terabytes of data from users and use it to enhance consumer services. Some of the Apache Spark use cases are as follows:\n- E-commerce: Many e-commerce giants use Apache Spark to improve their consumer experience. Some of the companies which implement Spark to achieve this are:\n1. eBay: eBay deploys Apache Spark to provide discounts or offers to its customers based on their earlier purchases. Using this not only enhances the customer experience but also helps the company provide a smooth and efficient user interface to the customers.\n2. Alibaba: Alibaba runs the largest Spark jobs in the world. Some of these jobs analyze big data, while the rest perform extraction on image data. These components are displayed on a large graph, and Spark is used for deriving results.\n- Healthcare: Apache Spark is being deployed by many healthcare companies to provide their customers with better services. One such company which uses Spark is MyFitnessPal, which helps people achieve a healthier lifestyle through diet and exercise. Using Spark, MyFitnessPal has been able to scan through the food calorie data of about 90 million users that helped it identify high-quality food items.\n- Media and Entertainment: Some of the video streaming websites use Apache Spark, along with MongoDB, to show relevant ads to their users based on their previous activity on that website. For example, Netflix, one of the major players in the video streaming industry, uses Apache Spark to recommend shows to its users based on the previous shows they have watched.\nIf you have any queries related to Spark and Hadoop, kindly refer to our Big data Hadoop & Spark Community.\nWhy Use Hadoop and Spark Together?\nIf you are thinking of Spark as a complete replacement for Hadoop, then you have got yourself wrong. There are some scenarios where Hadoop and Spark go hand in hand.\n- It can run on Hadoop, stand-alone Mesos, or in the Cloud.\n- Spark’s MLlib components provide capabilities that are not easily achieved by Hadoop’s MapReduce. By using these components, Machine Learning algorithms can be executed faster inside the memory.\n- It does not have its own distributed file system. By combining Spark with Hadoop, you can make use of various Hadoop capabilities. For example, resources are managed via YARN Resource Manager. You can integrate Hadoop with Spark to perform Cluster Administration and Data Management.\n- Hadoop provides enhanced security, which is a critical component for production workloads. Spark workloads can be deployed on available resources anywhere in a cluster, without manually allocating and tracking individual tasks.\nCheck out our MapReduce Cheat Sheet in Hadoop.\nIncreased Demand for Spark Professionals\nApache Spark is witnessing widespread demand with enterprises finding it increasingly difficult to hire the right professionals to take on challenging roles in real-world scenarios. It is a fact that today the Apache Spark community is one of the fastest Big Data communities with over 750 contributors from over 200 companies worldwide.\nAlso, it is a fact that Apache Spark developers are among the highest-paid programmers when it comes to programming for the spark framework as compared to ten other Hadoop development tools. As per a recent survey by O’Reilly Media, it is evident that having these skills under your belt can give you a hike in the salary of about $11,000, and mastering Scala programming can give you a further jump of another $4,000 in your annual salary.\nApache Spark and Storm skilled professionals get average yearly salaries of about $150,000, whereas Data Engineers salaries get about $98,000. As per Indeed, the average salaries for Spark Developers in San Francisco is 35 percent more than the average salaries for Spark Developers in the United States.\nIntellipaat provides the most comprehensive ClouderaSpark course to fast-track your career!']	['<urn:uuid:7df4820a-0055-4fb9-a62b-d99ff71f43e1>', '<urn:uuid:0b20c5fc-0ac7-4a50-8377-d3f684b00866>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T04:56:11.774636	27	157	3553
54	image compression comparison jpeg vs png which better quality	PNG offers better quality than JPEG because PNG uses lossless compression where no image data is lost, while JPEG uses lossy compression that may lose some data. PNG is particularly good for detailed, high-contrast images and can handle transparency, though JPEGs generally have smaller file sizes.	"['Don’t know your JPEGs from your PNGs? Our guide to image compression explains the fundamental concepts and terminology you need to know, plus some handy tools that might make your life easier.\nWhether you\'re a designer working for print, the web or TV and film, image compression is a necessary tool that you\'ll use on a daily basis. Without it, we\'d be emailing files that take up hundreds of megabytes, trying to send large files would take hours, your landing page would never appear, and browsing the web would be like walking through treacle.\nAll modern design tools have complex built-in image compression algorithms that abstract the decision-making process from designers. But having a clear understanding of how and why image files are compressed can help ensure your output is accurate and reproduces as you intended. Here we\'ll walk you through the basic terms and concepts of image compression that all designers need to know, then look at some of the more useful tools on the market to help you out.\nOnce you’ve been working with images for a while, you’ll understand there are two fundamental types of image: bitmap and vector.\nIn very simple terms, bitmap images are like a mosaic of coloured pixels, which when viewed at a distance appear to be a single smooth image - this works well so long as you don’t need to enlarge the image to the extent that individual pixels become visible.\nVector, on the other hand, is a mathematical set of co-ordinates that describe a series of lines, shapes and fills. Vector images can be reproduced at any size without loss of quality by applying the co-ordinates at a different scale.\nAs a result, vector images are already very small in size, but aren’t suitable for photo-realistic images as a typical photograph has huge variation in texture and tone over a small area. Bitmap images need to be compressed, vector images (typically) don’t.\n03. Lossy image compression\nThere are two basic all-encompassing methods of compressing files: lossy and lossless.\nLossy works by throwing away information from the original file, ideally where you won’t notice that it’s gone, saving space as a consequence.\n04. Lossless image compression\nLossless image compression, in contrast to lossy image compression, retains all the original data, but at a cost: the file size tends to be significantly bigger. Examples of lossy file formats (where information is lost) include JPG, GIF and MP3, while lossless formats include EPS, TIF, PNG and PSD (although both PNG and TIF can also be lossy).\n05. How image compression works\nThere are many different image compression algorithms that each take a different approach to reducing file size. The most simple approach, for example, is called run-length encoding and works by analysing the file bit by bit looking for patterns (quite literally, examining each individual bit of the file - 8 bits in a byte, 1024 bytes in a kilobyte etc).\nWhere the analysis finds a pattern, such as ten zeros in a row, space can be saved by notating that as 10:0 instead of recording all ten zeros. This type of image compression is lossless, but isn’t very effective in reducing file size.\n06. JPEG image compression\nThe overarching principle is the same regardless of which format you’re using, but different formats use different systems to analyze the file and notate or generate patterns.\nJPEG image compression, for example, splits the image into a patchwork of blocks, applying compression to each block using a discrete cosine transform operation.\nThis is a pretty sophisticated mathematical process, but in simple terms colour information is removed, along with subtle changes in brightness, centered on the areas where these changes will be least perceptible to the human eye. As a result, although it may not be immediately obvious until you compare the original to the compressed version, information is lost. If you over-compress using JPEG, the effect is a patchwork, blocky image.\n07. MP3 compression\nMP3 compression works in a similar way to JPEG image compression, removing information from frequencies that the human ear is less likely to be be able to perceive. Audiophiles deride MP3 because they feel the nuances of an original recording are lost through the process of stripping out these frequencies to save filesize, and similarly artists and designers don’t like what JPEG does to their images when high compression is applied. Both are necessary to reduce the amount of data being transferred across the web, either as assets to a webpage or app, or sending on to printers.\n08. The GIF format\nDeciding upon the type and level of image compression to apply has traditionally been further complicated by licensing issues. Back in the 1990s, GIF (Graphic Interchange Format) was a very popular format for web image compression (and remains so today).\nThe format was developed in the late 1980s, and is limited to a single 8 bits per pixel, restricting the resultant file to a maximum of 256 colours (by comparison, JPEG comfortably supports 8 bits per channel per pixel, allowing for full colour reproduction). GIF also supports binary transparency (a pixel is either transparent or not) as well as multiple frames of image data, allowing for flip-book animation.\n09. The PNG format\nThe problem with GIF was down to the compression algorithm used by for the format. LZW is a lossless image compression technique that was patented by Unisys, who attempted to exercise their ownership by requiring all users of the GIF format to pay for a license. This legal action caused some software vendors to drop support for GIF entirely, and kick-started the development of the now popular PNG (Portable Network Graphics) format.\nPNG offers a better level of image compression than GIF, and additional features such as support for 24bits per pixel (3 channels of 8bit colour for each of Red, Green and Blue, plus a whole 8 bit channel for alpha - allowing 256 levels of transparency!).\nPNG does not support multiple frames in a single file. As a side note, there is a separate development called APNG that does support multiple frames, but this was rejected as an official extension of the PNG format in 2007 - it’s currently only supported natively by Opera and Gecko-based browsers such as Firefox.\n10. How to choose the right format\nThankfully, choosing the right format to save an image with is pretty straightforward...\nDecide where your image is going to be used. If you’re sending an image to print, you’ll need a high quality format. If you’re publishing to the web, you can get away with a lower quality image (and thus a smaller file size).\nDecide what size your image will be displayed at. For print, images are generally reproduced at 300 DPI (dots per inch) - which means that for every inch of reproduction size, you’ll need at least 300 pixels of information. For example, a 10"" x 8"" print will need to be an image at 3000 pixels (10 x 300) by 2400 pixels (8 x 300).\nOn screen, most images are displayed at 72 DPI, although this is changing with the advent of Retina high pixel density displays. Crucially, images on screen are shown 1:1 with display pixels, so if you want to show an image at 600 pixels wide, you typically only need to reproduce your image with 600 horizontal pixels (not withstanding the retina display complication!)\nFinally, decide on the most appropriate format for your use and size requirements:\n- GIF is not suitable for print or photo-quality images due to its limited colour palette, but it’s ideal for line art or animated web images.\n- JPEG is a great fits-all solution, in both print and web applications, but keep in mind its a lossy format so don’t use JPEG as an archive format - save your original image in a lossless format.\n- If you need transparency on the web, you’ve only got two options: GIF or PNG. If you need multiple levels of transparency, opt for PNG - but if you just need some pixels to be entirely transparent, GIF may fit the bill.\n- For print, TIFF and EPS are both universal formats that can have lossless (and lossy) image compression applied, so if in doubt (and if filesize isn’t too important), choosing one of these two formats will be a reasonably safe bet.\n5 handy tools\nAs you\'d expect, there are also a ton of image compression tools for the desktop available to help you. Here we\'ve picked five that may be worth checking out.\n- Platform: Windows\nFILEminimizer Pictures takes a quite drastic approach to image compression compared with many similar tools. By default it may change formats, resize images, re-encode JPEGs with a reduced quality and strip out metadata in an effort to shrink your pictures. (You can adjust most of this in the program\'s settings.)\nPoint it to your source files, chosen an output folder, perhaps configured an option or two, one click is enough to begin optimisation. And it\'s fairly quick, so you\'re not kept in suspense for very long. All in all, it\'s versatile, configurable and easy to use.\n- Platform: Windows\nFileOptimizer can compress JPG, GIF and PNG images, as well as work with executable files, archives, Microsoft Office documents, PDF files, and more. It has one of the most straightforward interfaces we\'ve seen: drag and drop your images onto FileOptimizer, right-click, choose Optimize and that\'s it.\nThis simplicity has a down side as your source files are replaced by the program, being sent to the Recycle Bin. Other than that, though, it\'s compression that really matters and here is where FileOptimizer delivers some great results.\n- Platform: Mac\nImageOptim is a Mac tool that optimises GIF, JPEG and PNG images by acting as a front end for a host of other applications: PNGOUT, AdvPNG, Pngcrush, extended OptiPNG, JpegOptim, jpegrescan, jpegtran, and Gifsicle.\nThe program is all about lossless compression - removing comments, colour profiles and so on - rather than re-encoding or reducing the number of colours. As a result it won\'t cut file sizes by quite as much as some other tools, but at least you don\'t have to worry about any loss in image quality.\nImageOptim\'s front end works well, too. Drag on a single file, a group, a folder or whatever, and it immediately starts compressing each image and displaying the results. The program isn\'t the fastest tool around, but unless you\'re giving it thousands of HD JPEGs that\'s probably not going to be a problem.\n- Platform: Mac\nJPEDmini is an interesting Mac app with a two-pronged approach to shrinking JPEGs. First, it uses a smart algorithm to figure out how much an image can be compressed without visible effects (so the program isn\'t lossless, but it\'s not just re-encoding at some fixed quality setting, too). And then the program uses its own JPEG encoder to try and deliver the most compact files possible.\nThere\'s plenty going on in the background, but none of this is visible to the user. Just drag and drop your pictures onto the program (a maximum of 20 per day in the free version) and it\'ll replace the originals with compressed versions.\nFor simple web graphics JPEGmini doesn\'t do anything too amazing. The compression possibilities ramp up considerably as your images get larger. If your graphics are on the bulky side then it\'s definitely worth giving the program a trial to see how it works for you.\n- Platform: Web\nMost image compression web services are heavily restricted, but Yahoo\'s Smush.It is a rare exception. You\'re limited to files of up to 1MB in size, yes, but otherwise the system will upload as many images as you need. There\'s no registration, no account creation, no daily quotas. Images are crunched them with a variety of tools, allowing you to download the results in a ZIP file.\nIf there\'s one problem with this approach it\'s that you\'re very isolated from compression tools themselves. You can\'t configure or tweak them in any way. The results you see are what you get and there\'s nothing whatsoever you can do to change them.\nOn balance we still prefer the freedom and configurability of a local tool, but if the convenience of an online service suits your needs then Smush.It may be a sensible choice.\nWords: Sam Hampton-Smith (1-10), Mike Williams (11-15)', 'PNG stands for “Portable Graphics Format”. It is the most frequently used uncompressed raster image format on the internet. This lossless data compression format was created to replace the Graphics Interchange Format (GIF). PNG file format is an open format with no copyright limitations.\nWhat is PNG format used for?\nThe PNG file format is widely used on websites to display high-quality digital images. Created to exceed the performance of GIF files, PNGs offer not just lossless compression, but also a much broader and brighter color palette.\nWhat is the difference between a PNG & JPEG?\nJPEG and PNG both are a type of image format to store images. JPEG uses lossy compression algorithm and image may lost some of its data whereas PNG uses lossless compression algorithm and no image data loss is present in PNG format.\nHow do I make a PNG image?\nConverting an Image With Windows\nOpen the image you want to convert into PNG by clicking File > Open. Navigate to your image and then click “Open.” Once the file is open, click File > Save As. In the next window make sure you have PNG selected from the drop-down list of formats, and then click “Save.”\nIs PNG better than JPEG?\nThe biggest advantage of PNG over JPEG is that the compression is lossless, meaning there is no loss in quality each time it is opened and saved again. PNG also handles detailed, high-contrast images well.\nHow do I convert .PNG to JPG?\nHow to convert PNG to JPG using Windows\n- Open the selected PNG file in the Microsoft Paint program.\n- Select ‘File’, click ‘Save as’\n- Type the desired file name in the ‘File name’ space.\n- Click the ‘Save as type’ dropdown menu and select ‘JPEG’\n- Click ‘Save’ and the file will be saved into the selected destination.\nIs PNG a image?\nPNG is a popular bitmap image format on the Internet. It is short for “Portable Graphics Format”. This format was created as an alternative of Graphics Interchange Format (GIF). PNG files don’t have any copyright limitations.\nHow do I know if my image is JPEG or PNG?\n- Open a file in a Hex editor (or just a binary file viewer). PNG files start with ‘PNG’, . jpg files should have ‘exif’or ‘JFIF’ somewhere in the beginning.\n- Use identify file like torazaburo wrote in the comments (part of imagemagick lib)\nWhich is bigger PNG or JPG?\nPNG is the largest image type for bigger images, often containing information you may or may not find useful, depending on your needs. 8-bit PNG is an option, but GIF is smaller. Neither are optimal options for photography, as JPG is much smaller than lossless PNG with only minimal loss of quality.\nWhich image format is best quality?\nBest file types for these general purposes:\n|For Unquestionable Best Image Quality||TIF LZW or PNG (lossless compression, and no JPG artifacts)|\n|Smallest File Size||JPG with a higher Quality factor can be both small and decent quality.|\n|Maximum Compatibility: Windows, Mac, Unix||TIF or JPG|\nCan I convert a JPEG to a PNG?\nIn Windows, open JPG in Microsoft Paint, and click File > Save as > PNG > Save. In Photoshop (Windows or Mac), go to File > Save as > Save as type > PNG > Save. … In Preview on Mac, select File > Export > Export As > Format > PNG > Save.\nHow do I convert a JPEG to a PNG file?\nHow to convert JPG to PNG\n- Upload jpg-file(s) Select files from Computer, Google Drive, Dropbox, URL or by dragging it on the page.\n- Choose “to png” Choose png or any other format you need as a result (more than 200 formats supported)\n- Download your png.\nWhen should I use PNG or JPG?\nThe general rule is to use JPGs for photographs, images that don’t have a transparent background, and other memory intensive files. And to choose PNGs for graphics, files with transparent backgrounds, and other images where clarity and color vibrancy are important.\nIs JPG or PNG better for Photoshop?\nBecause of their different compression processes, JPEGs contain less data than PNGs — and therefore, are usually smaller in size. Unlike JPEGs, PNGs support transparent backgrounds, making them preferred for graphic design.\nWhat is the best format for images on website?\nWebp is the best format for web.\nJPG and PNG are also good choices for the web. If your choice is between JPG or PNG, use JPG for photos and PNG for logos. That’s because a JPG is better compressed and loads faster, whereas a PNG will retain more detail and allows for a transparent background.']"	['<urn:uuid:a02bd69b-f722-43a4-8591-17c58a2a79fb>', '<urn:uuid:9fe6cac1-ff4d-4a3b-a37f-95147500903f>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T04:56:11.774636	9	46	2848
55	hardware platform constraints vs security breach types	Complex hardware architectures create gaps in development since current software analysis and optimization techniques aren't suited for maximum platform exploitation, while security breaches specifically target three areas: design security (IP protection), hardware security (physical attacks resistance), and data security (communication authentication).	['Modern software systems, such as those found in the Avionics Industry, the Automobile Industry, the Chemical Industry, Telecommunications, Consumer Electronics, Banking, etc. are highly critical from a socioeconomic standpoint both in terms of their use and of their design, maintenance and evolution. These systems share the characteristic of having to respect both functional and quantitative requirements in addition to their intended use. Examples of such requirements include specific execution times and energy consumption rates. Requirements compliance may depend on a number of factors including processor speed, bandwidth, battery capacity, resource availability, etc. Violation of these requirements entails, at the least, degraded service provision. It may also induce serious failures that can produce irreparable damage.\nThe increased complexity of systems where the software component plays a preponderant role, poses numerous technical problems in all cycles of software development: requirements analysis, specification, design, deployment, integration, coding and validation. In this general context, it is becoming increasingly difficult to grasp the global behavior of a system with current tools (languages, models, theories, methodologies, EDI, etc). It is therefore imperative to foster new directions in state-of-the-art tools and models, both in terms of foundations and technologies. Innovative methods and tools should be sought for, that allow a means of addressing the inherent complexity of these systems.\nThere are many factors that contribute towards the complexity of software systems. A non-exhaustive list follows:\n- The large number and heterogeneity of the constituent components. In the process of engineering complex systems there is a need for applying knowledge arising from different areas. As a consequence, the structure, functionality and properties of the tools and models resting on existing theories and industrial practices become increasingly difficult to apply.\n- Complex hardware architectures. The use of complex platforms (distributed, multiprocessor, programmable, wireless, etc.) is motivated by the need for added speed, accessibility, robustness, etc. However, this creates gaps in the development chain since current software analysis, development and optimization techniques are not suited for exploiting these platforms at their maximum capacity.\n- Domain specific, high-level programming and modelling languages. Their appearance responds to the need for higher levels of abstraction in system development, in order to describe them as independently as possible from execution platforms while at the same time harnessing the intrinsic power of the specific domain of application. This is the origin of a vast set of theoretical and practical problems related to the preservation of semantics across different phases of development.\n- Interaction of software subsystems with the environment and physical components with complex, continuous dynamics. In order to grasp the global behavior of the system and its effect on the environment with which it interacts, it is necessary to develop hybrid dynamic models, combining both discrete and continuous event based systems.\nIt is becoming clear that current tools and methods for the rigorous description, design and implementation of software systems fall short in addressing additional emerging requirements such as those discussed above. Putting these tools and methods to work requires a profound understanding of the dynamic behavior of these systems, of their constraints and also their requirements. Indeed, these additional elements must be fully integrated into the model.\nThe use of heavily automated methods that feed on high-level models is one possible response to the correctness and precision requirements without relegating quality in the sense that the automatically generated code can be guaranteed to respect the requirements described in the model. The application of this approach would help minimize ruptures in the development cycle, reducing the time required to obtain market-quality products and hence also their total costs.\nHowever, the production of a faithful model of dynamic behavior for complex systems poses significant scientific and technical challenges. In order to respect execution requirements it is imperative to relate, starting from the design phase, functional behavioral aspects with non-functional ones related to temporal constraints on the execution of components, synchronization of components required for their proper integration, constraints related to communication services, distribution of components dictated by the execution platform, temporal characteristics, data placement in different physical memory banks, temporal and spatial properties of dynamic memory administration and more.\nIn order to tackle these problems, the research activity of LIA focuses on the development of new semantic foundations for programming languages and software models, and new techniques for the specification, modeling, verification and synthesis of implementations. The approach taken by LIA is based on formal, symbolic methods and models allowing the specification of properties of systems, the possibility of automating the verification of these properties either by static or dynamic analysis, and also the use of descriptions that model the run-time environment or execution platform.\nThe research program proposes the development of methodologies and tools which can aid in the automatic synthesis of an implementation which is correct by construction, providing guarantees that the compilation process respects functional and quantitative properties, as required by application software and constraints on execution platforms.', 'Biggest security threats for embedded designers\nEmbedded system designers face a number of threats to the applications that they develop for the Internet of Things (IoT). One of the biggest threats comes from IoT devices that end-users can access, such as commercial networked HVAC systems, wireless base stations, power stations, network gateway systems, and avionics networking.\nAnother example is the connected car, including the advanced driver assistance system (ADAS) that encompasses intelligent, interconnected vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Since vehicles are fielded systems, they are accessible by people with malicious intentions. There can be serious consequences – up to and including loss of life – if, for instance, ADAS systems cannot ensure that V2V and V2I messages originate from a trustworthy source and are not modified between sender and receiver.\nWith these and other systems, software security, alone, has proven inadequate to protect user-accessible devices against known threats. What is needed is a combination of software and hardware security. For example, today’s FPGA SoCs can be used to implement a hardware security scheme that compliments the software and strengthens the system. Ideally, the hardware and software solution should combat three types of security breaches:\n1. Design security: This includes IP protection and ensuring that configuration bit streams are encrypted and protected. In addition, designs need to incorporate a method to ensure there is no overbuilding or cloning of the design possible.\n2. Hardware security: Designers also need to certify that user-accessible devices are resistant to physical attacks. For example, differential power analysis (DPA) attacks can extract keys and other vital device information.\n3. Data security: This element ensures that communications into and out of the device are authentic and secure.\nEmbedded system program managers and development teams must design these types of protections into their products while best leveraging the characteristics of the underlying platform. The result should be a robust protection network with no single point of failure. Some key methods for achieving this goal include:\n∑ Risk assessment: System penetration testing should be used for a detailed system evaluation, to assess critical system data/functions, discover vulnerabilities, enumerate threats, and outline the likelihood and consequence of system compromise.\n∑ Protection planning: Using risk assessments and any other compiled data, developers should seek to understand protection implementation costs and design options for mitigating identified system vulnerabilities and ensuring successful system verification and validation.\n∑ Attack scenario testing: This can include a black box approach, pitting experienced reverse engineers with state-of-the-art attack tools against a system in a deployed setting to reveal vulnerabilities that cannot otherwise be found during most other evaluation exercises.\n∑ DPA side-channel analysis and mitigation: Side-channel attacks are currently the most practical method for compromising cryptography implementations. It is important to regularly perform measurable, objective, and repeatable testing for resistance to side-channel attacks for applications where adversaries have the ability to observe side channels (i.e., power draw, timing, EM emanations) during on-device cryptographic operations.\nIn today’s cyber hacking world, it is essential for every public and private organization to proactively address security issues. Embedded system designers can help their customers in this area by creating secure designs that are protected from today’s rapidly evolving threats, including those posed by a rapidly growing ecosystem of interconnected, user-accessible hardware.']	['<urn:uuid:65aef285-df35-4ead-a4ce-04223ea9cfb6>', '<urn:uuid:6fd90e2b-d1fc-4ceb-b428-5c07dc11374d>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-13T04:56:11.774636	7	41	1357
56	permanent vs temporary installations effectiveness purpose	Permanent installations, like porticoes and monuments, serve to symbolize institutional permanence while potentially using ephemeral materials, as seen in examples like the Nissan sculpture which represents the permanence of an institution producing mobile vehicles. On the effectiveness side, temporary installations have shown distinct advantages in engaging communities, as demonstrated by projects like the Clyde trail and Shaun in the City sculptures. These temporary exhibits encourage interaction with local areas, attract hundreds of thousands of visitors, and can serve additional purposes such as fundraising for charitable causes. Both types of installations contribute to improving public spaces, but research suggests their impact is most significant when integrated into wider regeneration programs.	['292-Scultpure of the Nissan Technical Research Center\n– Project management\n– Quantity surveying\n– Structural engineering\nThe research centre and the landscape\nWishing to implant a work of art in the park surrounding the research center that they are erecting in the scientific park of Louvain-la-neuve, the NISSAN EUROPEAN TECHNOLOGY CENTRE (BRUSSELS) company, with its global approach to situations and constant care for integration, decided to commission its conception to Samyn and Partners, architects and engineers of the building.\nIssued from a reflection on keywords defined in agreement with the client, the project materializes itself into two successive porticoes that define the spatial entrance sequence to the site. This option concentrates into the two independent objects all the preciousness that was voluntarily kept away from the more sober building.\nSet on two reinforced concrete column bases acting as foundation and sheltering the upwards lighting, each portico is composed of the following stainless steel elements :\n– masts made of a circular tube ;\n– rings, made of square plates lined up on the mast ;\nin time, banners engraved with scriptures relating the history of the company can be attached to them ;\n– threads, made of fine rods, suspending the rings and windbracing the whole structure ;\n– the roof, made of duckboards, carried by the masts and stabilized by the threads.\nMaterials and permanence\nHistory presents numerous building examples pushing constructive possibilities to such a limit that the durability of the construction becomes smaller than the permanence of the ideas that it symbolizes. One thinks of Gothic cathedrals, regularly collapsed and rebuilt, or Shintoists temples, rebuild every fifty years.\nSome materials are more suitable than others to embody this ” ephemeral ” permanence. Steel and other metals, plastic, glass, are light materials. They work mainly in traction and/or in bending, which makes them especially suitable for mobile and ephemeral structures. It is for this reason that the automobile construction industry (i.e., fundamentally, the mobile construction industry) makes an intensive use of it.\nThose materials represent the permanence of ideas, borne by ephemeral objects.\nThe shape and the meaning : the square car\nThe portico is a rich archetype. Meanings that it has carried in history are numerous :\n– war and power (the triumphal arch),\n– peace (Saint-Louis’ arch, Missouri),\n– prayer (the temple),\n– welcoming (the entrance of the castle park).\nIn all instances, the meaning of the portico is expressed on a large scale by its overall shapes and decorations, and on a small scale by :\n– bas-reliefs sculpted on the walls of the triumphal arch,\n– sacred texts engraved on the gates of the temple,\n– heraldic symbols of the Lord forged in the fence of the castle, etc.\nThe Nissan portico set themselves in this tradition. Their general meaning comes close to the one of the gate of the castle. In addition, their silhouette being like a light, tense, ideal, effective projection of the massive facade of the building, they symbolizes Research.\nOn another level, the sculpture-porticoes, at once permanent and ephemeral, place themselves at the junction of the heavy and the light, the building and the car, the inert and the living – a rooted car. The auto-immobile doesn’t move, it symbolizes the permanence of an institution whose mission is to produce ephemeral and mobile cars.\nThe following aiku can be read on the base of the central mast :\nThe circle becomes the square.\nThe sun embraces the sea.\nHarmony with nature is the goal.\nCh. Bastin & J. Evrard\nFor plans sections and elevations, please refer to the archives section of the site available from the “references” menu.', '“...a new beginning of people and place…a voice calling out I belong…” (‘Watershed’ by Jim Carruth)\nBy Heather Cameron\nPublic art can be seen everywhere these days, from parks to town centres, from hospital settings to overlooking motorways.\nAlong with thousands of motorists, on my daily commute I pass one arguably iconic piece by award winning public artist Andy Scott – Arria, dubbed ‘Angel of the Nauld’ as Cumbernauld’s answer to Gateshead’s Angel of the North.\nIt is certainly an eye-catching piece, projecting different images at night when it is lit up by multi-coloured lights. Commissioned as part of a drive to regenerate the area, which had previously won the Scottish Carbuncle award, it was hoped it would “create a sense of place and provide a positive statement about the town”. But can public art really lead to such outcomes?\nValue of public art\nThere has been growing recognition in recent years of the contribution that public art can play in improving public spaces and potentially quality of life for residents.\nAccording to Public Art Online, the main assertions made about the value public art brings include that it:\n- Enhances the physical environment;\n- Creates a sense of place and distinctiveness;\n- Contributes to community cohesion;\n- Contributes to social health and wellbeing;\n- Contributes to economic value through inward investment and tourism;\n- Fosters civic pride and confidence;\n- Raises quality of life;\n- Reduces crime.\nA recent survey reveals that “artists, consultants, local authorities and organisations within the health and education sectors largely agreed that public art: played an important role in local, regional and national identity; improved the design of the environment; and performed an important social role”.\nNevertheless, with continuing cuts to public spending and increasing scrutiny as to how local authorities spend public funds, it is not unusual to hear people questioning the money spent on art.\nAlthough not always well received initially, such as in the case of Anthony Gormley’s Angel of the North, such installations can grow to achieve an iconic status which in turn can have a positive impact on the local community, particularly in terms of identity and belonging, thus arguably improving wellbeing.\nA recently published thesis from Durham University which uses the Angel of the North as a case study, found that 72% of local residents say the sculpture makes them feel good whenever they see it, and it makes 64% proud of Gateshead. Half of the respondents agreed that it made them feel part of a community.\nWhile most respondents said they felt good when they saw the angel, this varied from 61% in a high deprivation area to 80% in a low deprivation area, suggesting that public art alone is not enough.\nIndeed, a literature review by the Arts Council suggests that public art is most effective as part of a wider programme of regeneration. And our previous blog on street art highlighted its use in the regeneration of urban areas.\nBy using public art to enhance or improve public spaces, the perceptions of such places can undoubtedly be improved. It has been suggested that the use of poetry and text-based art can make public spaces feel safer and deter vandalism, as well as reconnecting a community with its history.\nArt and the perception of place\nEven temporary installations can have a positive impact, by encouraging interaction with the local area. The sculptures of Clyde, the official mascot of the 2014 Commonwealth Games, that were dotted throughout Glasgow during the Games formed the Clyde trail and involved designs by local children. An app was also created so people could follow the trail, hunting down the sculptures.\nSimilarly, the Shaun in the City trail in London has recently been extended due to popular demand. Hundreds of thousands of people have visited the sculptures since they arrived in March, with many a ‘selfie’ having been taken.\nThese sculptures are likely to have an indirect impact on children’s health too, with the London trail raising funds for Wallace & Gromit’s Children’s Charity to support sick children in hospitals throughout the UK. 70 sculptures will then feature in Bristol to raise funds for The Grand Appeal, the Bristol Children’s Hospital Charity.\nIn relation to health more specifically, extensive evidence demonstrating the positive impact art can have has been highlighted. In 2007, A prospectus for arts and health was published by the Arts Council. It includes a summary of research carried out in two hospitals, Middlesbrough General Hospital and the James Cook University Hospital, which compared hospital accommodation before and after the move into a newly developed building (the JCUH). One of the main questions related to the impact of new commissioned art work on users. Among the key findings were that artworks were largely valued for providing colour, distraction and a sense of calm in the public areas, and for some patients they made the hospital seem “less like a hospital”.\nSurely, at a time when there has been much economic decline, anything that lifts the mood of people, whether it be a huge metal sculpture at the side of a motorway, or a humorously designed sheep, can only be a good thing.\nRaising our quality of life: the importance of investment in arts and culture Centre for Labour and Social Studies (2014)\nCultural value and social capital: investigating social capital, health and wellbeing impacts in three coastal towns undergoing culture-led regeneration Sidney De Haan Research Centre for Arts and Health (2014)']	['<urn:uuid:7825ea93-4e3a-4f22-b5b1-3ca06fbf0fd0>', '<urn:uuid:f1fcb98c-9d54-4e9f-b042-a0dabf489dda>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:56:11.774636	6	109	1519
57	what causes mycotoxins in corn	Mycotoxins in corn are natural toxins produced by fungi. The contamination becomes more likely when corn kernels are wounded, particularly through feeding damage caused by insects like the European corn borer and other caterpillars. These wounds make it easier for fungi to infect the kernels and produce mycotoxins.	['Corn grain can be naturally contaminated by mycotoxins, natural toxins produced by fungi.\nWounding of the kernel (Figure 2) makes it easier for infection by the fungi that produce mycotoxins. European corn borer and other caterpillars commonly produce wounds that favor infection and mycotoxin contamination.\nThat being the case, does the use of genetically modified Bt corn—genetically engineered to provide control of certain insect pests—result in reduced mycotoxin contamination?\nFigure 1. Fusarium ear rot of corn, usually associated with fumonisin contamination.\nTo address this question, field studies have been conducted comparing Bt-hybrids to non-Bt hybrids that are otherwise genetically very similar. Thus far, the overall results are that meaningful mycotoxin reductions sometimes occur due to the Bt trait. A more detailed discussion follows.\nField studies in the USA and Europe have shown reduced Fusarium ear rot and reduced fumonisin contamination from Bt corn. These reductions have been associated with reduced insect feeding on kernels containing the Bt toxin.\nIn studies where reductions in fumonisin concentration occurred, these ranged from 20% to 90%, often bringing the grain below concentrations that pose risks to humans and the most sensitive animals.\nFigure 2. Aspergillus ear and kernel rot of corn, often associated with aflatoxin contamination. The fuzzy appearance inside the kernel is the sporulation of the invading fungus.\nIt is interesting to note that the application of synthetic insecticides to control kernel-feeding insects on non-Bt plants also sometimes reduces insect feeding and fumonisin contamination. This raises the question as to which is more sustainable: use of the Bt trait or application of insecticides. Either way, reductions in fumonisin contamination are highly desirable.\nAs with fumonisins, field studies have often documented reductions in aflatoxin contamination due to the Bt trait. Significant reductions were most common when aflatoxin levels were at moderate to high levels in the non-Bt corn.\nIn studies where reductions were documented, these commonly ranged from 50-90%. In some cases, aflatoxin reductions from the Bt trait were sufficient to bring the corn below 20 parts per billion, an important regulatory threshold for use of the grain in human foods. As with fumonisins, aflatoxin reductions from the Bt trait have generally been linked to reductions in kernel injury from insect feeding. In some cases, the Bt trait did not result in aflatoxin reductions. This may be due to several reasons:\nIn those fields, drought stress may have been more important in promoting aflatoxin accumulation than insect damage to kernels;\nAs noted below, the Bt trait isn’t effective against all kernel-feeding insects.\nThere are several caveats and complicating factors in this line of research, as follows:\nBt corn isn’t a “magic bullet”, eliminating all mycotoxin contamination. Indeed, sometimes reductions in mycotoxin contamination do not occur. This is true for both fumonisins and for aflatoxins. However, reductions of fumonisins and aflatoxins have been documented commonly in field studies, especially under conditions moderately to highly favorable for ear rot and mycotoxin contamination. Such reductions occur commonly enough—with no known “downsides” relating to consumption of the grain—that the appropriate use of Bt corn is considered to be beneficial both for food safety as well as for animal health.\nThe Bt toxin must be expressed in the corn kernel in order to reduce insect injury and, ultimately, mycotoxin levels. In some Bt corn hybrids, the Bt toxin is not expressed in the kernel. Such hybrids have no protection against wounds created by insect feeding, and therefore fumonisin contamination is not reduced in these hybrids.\nWhile the Bt trait is important in reducing mycotoxin contamination, the magnitude of mycotoxin reduction can depend on the genetic background of the corn hybrid. In other words, if you compare two hybrids that have the same Bt trait, one may have less fumonisin than the other, simply because of its genetic background.\nThe effectiveness of the Bt trait in reducing mycotoxin contamination depends on the insect pest present. For example, Bt corn is often effective at reducing feeding damage from the European corn borer, but not the corn earworm. Consequently, reductions in fumonisin contamination may occur if the European corn borer is the principal pest in a field, but not if the corn earworm is predominant.\nCorn hybrids that express the Bt trait in kernels sometimes exhibit reduced kernel contamination from fumonisins and aflatoxins. This is thought to be due to reduced insect feeding on the kernels.\nThe Bt trait is not a “silver bullet”, eliminating all mycotoxin risk. However, reductions occur commonly enough, with no known “downside”, that the Bt trait is thought to contribute to food safety and livestock health in both developed and developing countries. While it is well-documented that the Bt trait can reduce mycotoxin contamination, it is best used wisely, and only in fields with a moderate to high risk of damage from the target insect pests.']	['<urn:uuid:abde58d4-e219-4410-b5f6-5373d8c466bd>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	5	48	799
58	With all the talk about new military threats, I'm curious about why regular ground-based defense systems aren't enough anymore - what makes space-based tracking so important these days?	Space-based tracking has become crucial because traditional ground-based systems are showing gaps in their coverage, particularly against newer threats like hypersonic weapons. These weapons appear 10-20 times dimmer than traditional ballistic missiles when viewed from space and are maneuverable, making them difficult to track with ground-based sensors. The military is addressing this by developing a layered system of satellites in low Earth orbit (less than 1,200 miles up) to better detect and track these threats. Additionally, as explained by Richard Matlock from the Missile Defense Agency, the military needs to shift from terrestrial-based systems to space-based ones to achieve persistent global sensor coverage. This space-based approach would allow for tracking missiles through all phases of flight and potentially enable interception during the critical boost phase, just seconds after launch.	['WASHINGTON — In January 2017, one of the government’s newest weather satellites picked up the most bizarre signal: a wildfire moving at breakneck speeds across the Atlantic Ocean.\nNow, wildfires don’t spread across the ocean, and they certainly don’t move at the pace being reported. What was going on?\nIt turns out the satellite ― one of the National Oceanic and Atmospheric Administration’s GOES-R series — had accidentally detected a rocket launch off the Florida coast, mistaking the fiery exhaust of a United Launch Alliance Atlas V rocket for a wildfire.\n“It was actually — honest — by accident, where we saw a rocket launch on the East Coast of Florida,” L3Harris Technologies President of Space Systems Bill Gattle told C4ISRNET. “Our weather sensor actually sent a trigger and said there’s a fire — our weather sensor actually tracks forest fires or hot spots — and today there’s this fire moving very fast across the Atlantic and so they ought to go put it out.”\nThat accidental discovery set L3Harris on a multiyear journey to transform its infrared weather sensor technology into a missile detecting capability for the U.S. military. The move would have the potential to bring in billions: The U.S. Air Force doled out $1.86 billion for just two missile warning satellites in 2014. However, the competition is tight.\nTraditionally, the Air Force built one missile warning constellation at a time with limited overlap, with only a few companies awarded massive contracts. Lockheed Martin is the prime contractor for all four geosynchronous satellites that make up the Space-Based Infrared System, as well as the final two expected to launch later this year. Northrop Grumman was the major subcontractor, building the satellites’ sensors.\nL3Harris isn’t exactly a lightweight in DoD contracting — it’s No. 9 on the Defense News Top 100 list of global defense companies and brought in nearly $14 billion in defense revenue in 2019. For perspective, Lockheed Martin and Northrop Grumman secured about $57 billion and $29 billion in defense revenue, respectively.\nEntering the competition\nAfter the decision to shift its weather technology to missiles, L3Harris’ immediate plan was to compete to build SBIRS’ successor, the Next Generation Overhead Persistent Infrared. Like SBIRS, Next Gen OPIR will be made up of a handful of satellites in geosynchronous orbit with two more in highly elliptical orbit. L3Harris faced the biggest defense contractors in the business. But with billions of dollars in contracts at stake, the company went for it.\nStarting in 2018, L3Harris began investing “several millions of dollars” into the effort, according to a company spokesperson, including building a new payload production facility in Fort Wayne, Indiana.\nThe company quickly realized it needed to refine its weather sensor to meet the military’s needs.\n“When you’re doing a weather mission, you’re looking at the Earth, and you’re scanning the Earth for weather, but weather doesn’t move nearly as fast as a missile does,” explained Gattle. “We had to get to where we could actually track a missile very fast. So our sensor had to become much faster at tracking things.”\nThe company also had to invest in algorithms that can cut out background noise picked up by the sensor.\n“You have to be very sensitive to what’s happening in the background when you’re seeing missiles. There’s a lot of noise, if you will, and [with] infrared there’s a lot of heat signature off the Earth. So you have to be able to distinguish if what you’re seeing is an airplane, a missile — what is it?”\nThe company fell short in its first attempt to break into missile tracking. The Air Force selected Lockheed Martin to build the three geostationary Next Gen OPIR satellites, while Northrop Grumman will build two more for highly elliptical orbits. At first, it might have seemed like the investment was for naught. L3Harris would have to wait years to compete for whatever satellites are added to or replace that constellation.\nHowever, growing concerns over hypersonic weapons from the Pentagon and Congress opened new doors for the company.\nA hypersonic opportunity\nThe emergence of hypersonic weapons over the last few years poses a problem to America’s missile warning systems. When viewed from space, the weapons appear 10 to 20 times dimmer than traditional ballistic missiles, making it harder for satellites in geosynchronous orbit to pick them up. Because the weapons are maneuverable, they can theoretically move to avoid ground-based sensors. It became clear that the U.S. needed a new constellation of space-based sensors that could detect and track the new threat.\nThe solution that emerged is both simple and complex. Instead of relying solely on the exquisite sensors more than 22,000 miles above the Earth’s surface, the military will build a proliferated constellation located much closer to the planet’s surface in low Earth orbit — less than 1,200 miles up. From that lower vantage, it is easier for infrared sensors to pick up the hypersonic weapons, track them as they move around the globe, and provide the targeting data to destroy them.\nThis tracking effort effectively has three parts:\n- A proliferated constellation with wide-field-of-view sensors in low Earth orbit that will pick up and track hypersonic weapons as they move around the globe.\n- A data transport layer of satellites to connect the sensors on orbit and pass tracking data as the threat moves in and out of view of individual sensors.\n- A smaller constellation with more sensitive, medium-field-of-view satellites that will provide the final targeting data to a fires solution.\nThe Space Development Agency is building those first two sections as part of its National Defense Space Architecture, a proliferated constellation in low Earth orbit that will eventually be made up of hundreds of satellites. The Missile Defense Agency will develop the medium-field-of-view satellites, known as the Hypersonic and Ballistic Tracking Space Sensor, or HBTSS. Meanwhile, the Defense Advanced Research Projects Agency is building Project Blackjack to demonstrate many of the technologies needed to make the entire system work.\nIn October 2019, L3Harris got its first hint that it could crack the missile warning enterprise. MDA awarded the company a $20 million contract to design an HBTSS prototype, placing the business among heavyweights Northrop Grumman, Leidos and Raytheon.\nBecoming a contender\nA second win came months later in May 2020, when DARPA revealed L3Harris was in the running to build an electro-optical/infrared sensor for the Blackjack demonstration. To be clear, there was no direct path from a Blackjack contract to an actual program of record with either MDA or SDA.\nStill, the parallels between the DARPA demonstration and those efforts were clear, and a Blackjack program contract could be seen as a pretty significant leg up for those competitions.\nRegardless, L3Harris’ bid failed. The company’s proposal was too pricey, said Gattle, and the $37 million Blackjack payload contract went to Raytheon. In response, L3Harris invested to make sure it could build faster and present a more affordable solution. That would be key as the company prepared to bid on the SDA constellations.\nSDA released solicitations for its initial data transport layer and tracking layer satellites last May. At stake were four contracts and 28 satellites. L3Harris made its bids.\nThe company failed to win a contract for the data transport layer satellites, losing out to Lockheed Martin and York Space Systems, the latter of which is another newcomer to building satellites for the DoD. While it was still in the running for the HBTSS and tracking layer satellites, L3Harris was now 0 for 3.\nBased on feedback from SDA on its transport layer bid, L3Harris refocused its bid for the tracking layer around its ability to manufacture its own commercial bus for the satellite. That in particular was appealing to SDA, said Gattle.\n“That was enough to convince SDA that, yes, this was a commercial — I’ll call it a commercial-like — offering,” he said.\nReturn on investment\nThe real breakthrough — and reassurance that the company’s investments weren’t in vain — came soon after.\nLast October, SDA awarded the two tracking layer contracts to L3Harris and SpaceX. L3Harris received $193 million, while SpaceX got $149 million. The selection was a surprise: Neither of the winners had built a missile warning sensor for the government before. SpaceX hadn’t even built a satellite for the government before.\nWhile Elon Musk’s company fought its way into winning a massive Space Force launch contract last year and is working to integrate its Starlink broadband constellation with various weapons systems, no one expected it to be building missile tracking satellites for the military. SpaceX will build the satellite bus and subcontract for the actual OPIR sensor.\nWhile L3Harris had been in the running for several missile warning satellite contracts, it had never gotten one over the line.\n“A decade ago, we probably didn’t have a shot,” said Gattle. “To be honest, we were a component supplier. We would build a handful of anything that we built.”\n“This is the culmination for us of a pretty big pivot in our company. We were known mostly as a weather sensor company in this particular area, infrared,” he added. “Then a couple of years ago we saw that our infrared sensors on our weather satellites could actually pick up rocket launches.”\nChris Quilty, founder and a partner of Quilty Analytics, said L3Harris has accomplished “a remarkable string of contract successes” since it entered the satellite manufacturing business only a few years ago, noting an Air Force remote sensing constellation award, among others.\n”The fact that SDA awarded this contract to L3Harris is both a testament to L3Harris’ agility and an indication of SDA’s desire to open the aperture to new suppliers,” said Quilty, whose financial and strategic firm serves the space industry.\nL3Harris started 2021 with more returns on its investment: Of the four companies designing HBTSS prototypes, only L3Harris and Northrop Grumman secured contracts to build their prototypes. L3Harris won a $122 million award, while Northrop Grumman got a $155 million award.\nL3Harris’ journey from building weather sensors to building missile warning satellites kicked off in 2017, when the company’s technology accidentally detected a rocket launch off the Florida coast. It turns out that rocket was carrying the third satellite for the nation’s prime missile detection constellation.\nThere’s a certain poetry to that story. The accidental detection of a missile warning satellite being delivered to orbit inspired a newcomer to enter the field, and that company went on to win a chance to help build the U.S. military’s new missile warning architecture.\nIn 2017, L3Harris had ambitions and a weather sensor. In 2021, it’s a missile tracking company.\nNathan Strout covers space, unmanned and intelligence systems for C4ISRNET.', 'Missile defense tracking systems need to move to orbit, leaders say\nWASHINGTON — As the U.S. military works on developing its next-generation missile defense systems, more of the resources need to be focused in space, experts said Dec. 14.\n“It’s so important that we make this broader shift from a terrestrial-based system to a system that primarily plays from space in the next couple of years,” said Richard Matlock, program executive for advanced technology at the Missile Defense Agency.\nThe military has long relied on the Space-Based Infrared System, or SBIRS, as a warning against launches of ballistic missiles. But Matlock argued that the U.S. needs to supplement it with a more layered capability in orbit with many different satellites and systems that could not only detect, but track and target a missile through every part of its flight.\n“The architectures consisted largely of terrestrial sensors deployed on land, deployed on our ships, and interceptors also deployed in silos, in trucks, and in ships,” he said. “As we examine the impact of the evolving, more maneuverable, more complex threat on this, we begin to see gaps emerging in the future to our system, which is primarily based on our lack of persistent global sensor coverage.”\nThose gaps could be addressed by developing a “globally persistent space-based sensor array” that would include radar and electro-optical sensors, Matlock said, at an event hosted by the Missile Defense Advocacy Alliance (MDAA).\nBetter technology in orbit would also allow missile defense to expand to more stages of an attack. Currently, most of the United States’ defense efforts are focused on the middle of a missile’s flight, possibly when a reentry vehicle carrying a warhead is approaching its target. But the MDA wants to being focusing on disrupting enemy attacks during the boost phase, sometimes just seconds after a missile is launched.\nMatlock said the MDA is hoping to bolster its Space Tracking and Surveillance System (STSS) which is designed to provide the very tracking and targeting capabilities the agency is seeking. At first, the system could be used to extend the range of ground-interceptors, guiding them beyond the sight of radar. But further down the road, a space-based interceptor could be possible.\nMissile defense is starting to face new challenges too. Russia and China – as well as the U.S. – have invested various efforts in developing hypersonic missiles that would travel faster than Mach 5 and be very difficult to track and destroy given current means.\n“Right now we’re examining a number of architectures that would allow us to tackle that challenge,” Matlock said. “In the near term, we’ll be exploring what sensors we need in order to track those more effectively.… We’re exploring, and will be exploring with industry and others in a broader way, what a complete architecture might be and what systems we have today which might be able to address that threat with some modifications, as well as bringing in some of these newer technologies.”\nDirected energy weapons such as lasers could give the military the capability to combat both hypersonic missiles and to target launches during the boost phase.\n“My primary focus right now is in directed energy research,” Matlock said. “Our goal eventually is to integrate a high-powered, solid-state laser on a long-endurance unmanned aerial vehicle operating in the stratosphere where the atmospheric disturbance of the aircraft and the laser is significantly reduced.”\nSometime before the end of the current fiscal year, the MDA is looking to award contracts to two or more major defense companies for integrating and flight testing an airborne laser demonstration by 2021.\nMaj. Gen. Roger Teague, director of space programs for the U.S. Air Force acquisition office, echoed Matlock’s comments, saying that “space-based sensing absolutely underlines everything that we try to do to make sure we have eyes on the target.”\n“It starts in space, our ability to detect those missile threats that might be posed against the U.S. or our allies,” he said.\nThe Pentagon is studying what the right mix of on-orbit capabilities will be, Teague said, ranging from multiple smallsats to hosted payloads on commercial satellites to hardened exquisite military spacecraft. No firm due-date for a plan has been set yet.\n“We’re evaluating alternatives right now, so any number of different architectures and systems and capabilities are being explored,” he said. “There hasn’t been any hard decision made on any of that.”\nBut he noted that the Defense Department is beginning to look at what comes after SBIRS.\n“We need to start looking at what are those systems that are going to replace it,” Teague said. “We’re working that very aggressively through the department right now to understand what those future systems ought to be, what they’re capabilities might be, and how we might be able to best tap into the emerging technology that we see. In terms of on-orbit, what does that space layer need to look like?”\nNeither Matlock nor Teague discussed any specifics about what they expect from an incoming Trump administration, though MDAA founder and chairman Riki Ellison expressed hope that both the Air Force and MDA could see a sizable increase in their budgets.']	['<urn:uuid:127cb0ce-7149-44b9-a91b-3ccaa7f48f82>', '<urn:uuid:3fc47efa-e1a4-4868-9c74-4ebeb1163c5e>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T04:56:11.774636	28	129	2642
59	freezer storage guide produce bread soup	Different foods have varying storage times in the freezer: bread should be used within a month, fruits and vegetables can be stored for three to six months, and soups also last three to six months. For best quality, maintain a freezer temperature of zero degrees, with a maximum recommended storage time of 10 months for fresh vegetables.	"['When planning meals in advance, your freezer becomes your best friend. Foods like casseroles, lasagnas, and soups can be prepared ahead of time, frozen, and then moved to the refrigerator the night before the dish will be reheated.\nWhen prepping foods for the freezer, where do you start? Airtight seals are a must in order to prevent freezer burn. Casserole dishes that come with seal-able lids are a good choice as well as containers with lids that can be pushed down to squeeze out the extra air. Freezer bags are also a great option – just make sure to empty it of as much air as possible before sealing.\nDon’t feel limited to dinner foods when planning meals. Cooked pancakes freeze nicely and are a snap to reheat. Cook home style potatoes or hash browns and freeze them for a quick side dish for breakfast or even brinner – breakfast for dinner.\nWhen getting soup ready to be set back, cook, then let cool. Pour into airtight containers, leaving an inch or so for expansion. To prevent freezer burn, place a sheet of plastic wrap on top of the soup before putting on the lid.\nMeat freezes wonderfully as does bread and rice. When contemplating fruits and vegetables for freezer habitation, think about the water content. If they’re watery, like melons, lettuce, or cucumbers, they probably won’t freeze well. Other types of produce will soften, so it’s best to use them in cooked foods or smoothies.\nSpeaking of smoothies, those brown bananas that usually go to the compost heap make wonderful drinks. Cut them into slices and freeze in bags. They’re a great base for other fruits, juice, and yogurt, and they’ll make your smoothie smoother and sweeter without needing to add anything extra, like honey.\nOrganizing foods by preparing them ahead of time will save you last minute trips to the grocery store as well as meal time stress. A little planning and freezer prep will help make your kitchen a peaceful oasis.\nQuick freezer use-by dates for quality:\n• Bread – use within a month.\n• Fruits and Vegetables – three to six months\n• Soups – three to six months\n• Casseroles – use within a month\nFreezer Tomato Sauce\n1/2 cup extra-virgin olive oil\n1 small organic yellow onion, finely chopped\n3 organic garlic cloves, finely chopped\n2 tsp sea salt\n1/2 tsp freshly ground black pepper\n2 tsp red pepper flakes\n2 tsp dried oregano\n2 dried bay leaves\n5 leaves fresh organic basil, chiffonade or torn\n2 (32-ounce) cans or about 15 fresh and blanched, organic crushed roma tomatoes\nAdditional salt & pepper to taste\n1/2 to 3/4 cup chicken or vegetable stock\nIn large heavy pot, heat the olive oil over medium-high heat. Add the onion. Sauté for about 5 minutes, or until the onion is slightly translucent. Add the garlic, salt, and pepper and sauté another 3 to 5 minutes or until the onion is translucent and the veggies are tender, being careful not to brown the garlic.\nAdd in the tomatoes, bay leaves, basil, oregano, and red pepper flakes. Simmer, uncovered, over a low heat until the sauce thickens, about 1 to 2 hours, checking the seasonings occasionally. When the sauce has thickened, remove from heat and let cool slightly. Remove the bay leaves. Working in batches, puree the sauce in a food processor or blender until smooth.\nPour into airtight containers and freeze for up to one month or refrigerate for up to one week.\nWhen ready to use:\nAllow the sauce to defrost overnight in the refrigerator. Put sauce in a large pot over medium heat. Add the stock to thin the sauce out to the desired consistency. Taste and add more salt and pepper if necessary. Heat through and serve with your favorite pasta.', 'BUTLER — Freezing vegetables is a good option for people who want to preserve their garden produce but are daunted by the idea of canning.\n""The color, flavor and texture of the produce are often maintained when it is frozen, and the freezing process is easier and less time-consuming than canning,"" said Tammy Roberts, University of Missouri Extension nutrition and health education specialist.\nBLANCH BEFORE FREEZING\nBlanching, which involves boiling or steaming vegetables then immersing them in cold water, is necessary to inactivate enzymes, which are proteins that help with the ripening and maturing process, Roberts said. In unblanched vegetables, this process can continue in the freezer, though at a very slow pace.\nTiming is critical. ""Overblanching can result in a cooked product with reduced flavor, color and nutrients,"" she said. ""Underblanching can actually speed up enzyme activity, making the food undesirable for eating.""\nBlanching time depends on the vegetable and the size of the pieces to be frozen. Recommended blanching times in boiling water for common vegetables are listed below:\n» Green beans: 3 minutes\n» Broccoli, chopped or stalks: 3 minutes\n» Beets, small: 25-30 minutes; medium: 45-50 minutes\n» Brussels sprouts, small: 3 minutes; medium: 4 minutes; large: 5 minutes\n» Carrots, tiny, whole: 5 minutes; diced or strips: 2 minutes\n» Cauliflower: 3 minutes\n» Corn on the cob to freeze on small ears: 7 minutes; medium ears: 9 minutes; large ears: 11 minutes\n» Corn on the cob to cut for whole-kernel corn: 4 minutes; cool and cut from ear\n» Greens, such as spinach: 2 minutes\n» Shelled peas: 1˝ minutes\n» Snow or sugar snap peas: 2-3 minutes\n» Summer squash, such as zucchini, in slices or chunks: 3 minutes; grated: 1-2 minutes\nUse a gallon of water per pound of prepared vegetable. Place vegetables in a wire basket and lower into boiling water. Cover and begin counting blanching time as soon as the water returns to a boil.\nOnce the recommended blanching time has elapsed, immediately plunge the basket of vegetables into cold water. Keep the water cold by changing frequently, using cold running water or ice. Cool vegetables for the same amount of time as they have been blanched. Drain thoroughly. Pack in freezer bags or containers; remove as much air as possible before sealing bags; leave a half-inch of space if using rigid containers. Seal tightly, label and freeze.\nFREEZING: THE QUICKER, THE BETTER\n""The goal when freezing vegetables is to have the smallest ice crystals possible because they cause less damage to cell walls,"" Roberts said. To achieve this, the food needs to be frozen quickly. Many people set the freezer at minus 10 degrees Fahrenheit a day before they are going to freeze food. Once the food is frozen, the thermostat can be set back to zero degrees.\nDon\'t overload the freezer with unfrozen food. This will slow the freezing process, which makes for larger ice crystals and more cell damage. Only add the amount of food that will freeze within 24 hours — about 2 to 3 pounds of food per cubic foot of freezer space.\nFor best results, maintain a temperature of zero degrees in the freezer. Quality deteriorates quickly at higher temperatures. Ten months is the maximum recommended freezer storage time for fresh vegetables.\nA number of MU Extension guides on freezing, canning and other food-preservation topics are available at local MU Extension centers or for free download at extension.missouri.edu/preservation.']"	['<urn:uuid:ba64d31a-2977-4164-9ca2-1707dc347939>', '<urn:uuid:12df07a4-66de-4e9b-a20a-239ab2eb9c97>']	factoid	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-13T04:56:11.774636	6	57	1211
60	What are the rules and common mistakes for child car seats?	"According to Australian road laws, children up to 6 months must use rear-facing infant restraints, from 6 months to 4 years they need either rear-facing or forward-facing safety seats, and from 4-7 years they require forward-facing seats or booster seats. Common mistakes include turning car seats forward-facing too soon (should wait until age 2), moving to booster seats too quickly before outgrowing the 5-point harness limits, using booster seats incorrectly without proper belt positioning across hip bones and shoulder, and not using safety seats long enough - children should use them until reaching 80 pounds and 4'9\"" tall."	"['Road law amendments\nThe Australian Road Rules are a national body of laws governing the use of roads, footpaths and other shared spaces by drivers, cyclists, pedestrians and others. There are over 350 rules and they have been in operation since December 1999.\nThe operation of the rules is continually monitored and amendments take into account changes in road safety policy, driver behaviour and technology. Most changes to the rules are simple clarifications that improve their interpretation and/or operation.\nOther changes to road laws are made through amending relevant legislation.\n- Read more about road rules that apply in South Australia in The Driver\'s Handbook.\nRecent road law amendments include:\n- 1 December 2019 - Amendments to Australian Road Rules\n- 15 April 2017 – Road rule amendments\n- 13 December 2016 - Road rule amendments\n- 25 October 2015 – Cycling road rule amendments\n- 1 September 2014 – Road rule amendments\n- 1 September 2014 - 25km past emergency services\n- 2 March 2014 - Amendments to Australian Road Rules\n- 16 January 2014 – Amendments to the Road Traffic (Miscellaneous) Regulations 1999\n- 1 July 2010 – Child restraints\nFees correct at time of amendment.\n1 December 2019 – Amendments to Australian Road Rules\nThe State Government has implemented the following nationally-adopted amendments:\n- Specifying load restraint performance standards in law, with breaches to result in a three demerit point penalty and a $235 expiation fee for light vehicle drivers and operators or a $375 fee for heavy vehicle drivers. It is already an offence to drive or tow a vehicle with an insecure or overhanging load, however the amendment specifies what is meant by this. Vehicle loads must now be restrained to meet the performance standards outlined in the National Transport Commission\'s Load Restraint Guide.\n- Removing barriers to access for larger vehicles through small roundabouts.\n- Requiring drivers to indicate when pulling over to a stationary position (for example when waiting for another motorist to pull out of a parking bay).\n- Requiring a motor bike helmet to be in good repair and proper working order and condition, while recognising considerations of normal wear and tear. Riders should note that cameras are only able to be fitted to a motor bike helmet when approved by the helmet’s manufacturer.\nSome of the amendments to the Rules were based on pre-existing law in South Australia and will now apply throughout the rest of Australia.\n- Allowing cyclists to ride across road crossings and slip lanes (instead of having to walk their bikes).\n- Requiring motorists to give way to cyclists and pedestrians entering crossings, as well as while they’re on the crossing; and\n- Introducing nationally consistent Rules relating to motor bike lane filtering.\n15 April 2017 – Road rule amendments\nRoad rule changes now allow:\n- Bike riders will be able to ride across a marked foot crossing, a pedestrian crossing or a children’s crossing, but they must do it safely. Riders must keep to the left, give way to any pedestrian, and cross without delay. Riders must obey the pedestrian or bicycle crossing lights and not enter the crossing once the symbol turns red. Riders should slow down on the approach to the crossing and keep a sharp lookout for pedestrians and vehicles. The fine for failing to cross correctly is $115 which is inclusive of the $60 victims of crime levy. Regulations 9A and 32 of the Road Traffic (Road Rules – Ancillary and Miscellaneous Provisions) Regulations 2014.\n- All Vehicles must give way to pedestrians and cyclists when they are entering crossings and slip lanes, that is when they are about to cross. The fine for failing to do so could be up to $470 which is inclusive of the victims of crime levy. Regulation 9B of the Ancillary and Miscellaneous Regulations.\n- Private buses will be able to use bus lanes, but not ‘bus only’ lanes painted in red. A bus is ‘a motor vehicle built mainly to carry people that seats over 12 adults (including the driver)’. Regulation 12 of the Ancillary and Miscellaneous Regulations.\n- Segways to be used in the Riverbank Precinct as part of a guided tour authorised by the Adelaide City Council. Otherwise, they are not legal for use on South Australian roads.\n- Motor Bike Riders to Lane Filter, Road Traffic (Road Rules—Ancillary and Miscellaneous Provisions) Regulations 2014, Regulation 11C. Lane filtering is when a motor bike rider travels at low speed in between two lines of stationary or slow moving traffic travelling in the same direction.\nStrict conditions apply to lane filtering:\n- only permitted when safe to do so\n- only permitted at speeds of 30km/h or slower\n- only permitted by R and R-Date licensed motor bike riders. Learner’s Permit holders, P1 Provisional licence holders and moped riders with a car licence only must not lane filter\n- not permitted in a school zone or across pedestrian and children’s crossings\n- not permitted next to parked cars or between vehicles and the kerb, or on roundabouts\n- not permitted in bicycle, bus or tram lanes.\nPetrol-powered bicycles are prohibited on roads. They can be difficult to stop and pose a significant safety risk to riders. They are banned under regulation 48 of the Road Traffic (Miscellaneous) Regulations 2014. The fine is $418, inclusive of the victims of crime levy.\n13 December 2016 – Road road rule amendments\nRoad rule changes in December 2016 now allow\n- a child of up to the age of seven years with a disability or medical condition to sit in the front seat of a vehicle if the driver is carrying a medical certificate, provided the child is properly restrained in either an approved child restraint or one that is fit for purpose (ARR 266).\n- An exemption from wearing a helmet when pushing a motorbike with the engine not on (for example when at a service station) (ARR 270).\n- Motorbike riders to remove one foot from a footrest when the motorbike is moving (for example to stretch legs on longer journeys) (ARR 271).\n- The rider of a bicycle to cross at an intersection that is showing a green bicycle crossing light even if the traffic lights show a red or yellow traffic light (ARR 262A).\nIn addition, the rules clarify that only audio (not video) calls may be made or received when driving (ARR 300).\n25 October 2015 – Cycling road rule amendments\nRoad Traffic (Road Rules – Ancillary and Miscellaneous Provisions) Variation Regulations 2015, the Road Traffic (Miscellaneous) Variation Regulations 2015 and the Motor Vehicles Variation Regulations 2015 to allow all-age cycling on footpaths and to introduce an offence for a driver of a motor vehicle failing to overtake a cyclist by a prescribed minimum lateral distance with an expiation fee of $287 and 2 demerit points.\nDrivers will now be required to give a minimum of one metre when passing a cyclist where the speed limit is 60km/h or less or 1.5 metres where the speed limit is over 60km/h. To assist with compliance of this rule, a driver who has a clear view of any approaching traffic and can do so safely will be exempted from the following road rules:\n- driving to the right of the road (exemption from ARR 132) – this includes driving to the right of (double) continuous lines\n- driving on a dividing strip (exemption from ARR 137)\n- driving on a painted island (exemption from ARR 138)\n- no need to stay within a single lane or line of traffic (exemption from ARR 146)\n- crossing a continuous line separating lanes (exemption from ARR 147)\nAmendments to the Australian Road Rules were made to help clarify existing rules, create consistency within the Rule or remove ambiguities for road users. Most amendments do not involve a change in general driver behaviour.\nThe major changes are highlighted below:\nCLARIFY: Rule 128A – To make it clear that after giving way to pedestrians on the crossing, a driver may enter a children’s crossing or a pedestrian crossing that is immediately before an intersection without traffic lights and stop on it if the road beyond the crossing becomes blocked.\nCLARIFY: Rules 134 and 138 – A driver may cross a single continuous dividing line to park in angle parking on the opposite side of the road, provided that the driver does not need to perform a U-turn to reach the parking area. This includes crossing a painted island surrounded by a single continuous line.\nCLARIFY: Rule 153(4) – Bike lane road markings (not only signs) can be used to commence or end a bicycle lane (comprising both a white painted bicycle symbol and the words ""lane"" or ""end lane"" as appropriate).\nNEW: Rule 197 – Drivers are prohibited from stopping/parking on a traffic island (for example, in the centre of a roundabout). The expiation fee for stopping on a traffic island is $87.\nNEW: Rules 265(4) and 266(5B) – Passengers under one year old can sit on the lap of another passenger who is 16 years old or older when travelling in a bus, taxi or tow truck where no appropriate child restraint is available for use and they are not in the front row of seats.\nCLARIFY: Rule 266(2B) – When a booster seat is used it must be used with either a lap and sash type of approved seatbelt or an approved child safety harness.\nNEW: Rules 265(5) and 266(5C) – Drivers of correctional vehicles are exempt from the requirement to ensure that passengers in rear seating rows or a caged area are wearing seatbelts.\nCLARIFY: Rule 271 –To allow a motor bike rider to remove his/her feet from the footrests to manoeuvre the vehicle, such as reversing the bike into a parking space, or when the motor bike (at low speed <10km/h) is decelerating to come to a rest, or accelerating from a rest.\n1 September 2014 - New 25km/h Emergency Services speed Limit\nS83 of the Road Traffic Act 1961 - The speed limit was reduced from 40km/h to 25km/h to protect emergency service workers. The 25km/h Emergency Service Speed Zone applies on an area of road:\n- In the immediate vicinity of an emergency service vehicle that has stopped on the road and is displaying a flashing blue or red light; or\n- Between two sets of flashing blue or red lights that have been placed by an emergency worker at either end of a length of road on which an emergency vehicle has stopped.\n- It does not apply if you are driving on a road that is divided by a median strip and the emergency service speed zone is on the other side of the road beyond the median strip.\n2 March 2014 - Amendments to the Australian Road Rules\nMany of these changes are simply to clarify existing road rules and make them easier to understand. Specific details are outlined in the Government Gazette 13 February 2014 pp 917-934.\nThe following amendments are of particular note:\nRule 127 – Minimum distances between long vehicles.\n- A road train must now keep a minimum 200 metres distance behind another long vehicle unless driving on a multi-lane road or any length or road in a build-up area or overtaking.\n- Long vehicles (except a road train) must travel to a minimum 60 metres behind another long vehicle.\nRule 227 – Use of portable warning triangles in vehicles over 12 tonnes.\nThe way in which portable triangles must be laid out to identify a broken down vehicle has been clarified and the distance at which the triangles must be placed relative to the speed limit for the area in which the hazard occurs has been varied. If the triangles are not properly placed, the driver could face prosecution and would be held liable for any crash that results from a failure to comply with these requirements.\n- On roads where the speed limit is 80km/h or over, triangles must be used when the vehicle is not visible within 300 metres in all directions.\n- On roads where the speed limit is less than 80km/h or, triangles must be used when the vehicle is not visible within 200 metres in all directions.\nRule 137 – Driving across a dividing strip with a pavement bar.\n- Allows drivers to legally drive across pavement bars and markers to enter or leave a road removing unnecessary obstruction to traffic.\nRule 266 – Child restraints.\n- Allows a third child aged four or older to sit in the front seat of a vehicle (providing they are appropriately restrained) if there is no room for them in the back row of seats because of two other child restraints already fitted in the back seat.\n- Where there are no seatbelts fitted in the back row of seats, a child aged four or older can now sit in the front seat (providing they are appropriately restrained).\nRule 299 – Motorcycle riders using mobile phones as a GPS.\n- Allows motorbike riders to use mobile phones as a GPS in the same way as drivers of motor vehicles, provided that it is securely mounted and they do not touch or manipulate the device.\n16 January 2014 – Amendments to the Road Traffic (Miscellaneous) Regulations 1999\nRiding on footpaths\n- Allows adults (aged 18 and over) to ride on footpaths to accompany children aged less than 12 years.\nBicycle storage areas\n- Allows bicycle riders to enter bicycle storage areas located at traffic lights other than from a bicycle lane.\n- A $340 expiation fee for longboarding was introduced and applies when the speed limit of the road is over 60 km/h or when the road has two or more marked lanes in the same direction of travel. Skateboarding on a road with a dividing line or median strip or where the speed limit is over 50 km/h remains an offence and may incur a $52 expiation fee.\n1 July 2010 - Child restraints\nChild restraint laws approved by the National Transport Council in 2008, were adopted in South Australia on 1 July 2010. For details go to Seatbelts and child restraints.\nIn summary, the laws require:\n- Infants up to six months old to be restrained in a rearward facing infant restraint.\n- Children aged between six months and up to four years old to be seated in either a rearward or forward facing child safety seat.\n- Children from the age of four and up to seven years old to be seated in a forward facing child safety seat or booster seat (restrained by a correctly adjusted seatbelt or child safety harness).\n- Children from the age of seven years to be seated in a booster seat with seatbelt or harness or a correctly fitted and adjusted adult seatbelt.\n- Children under the age of four years to be seated in the rear of the vehicle (where the vehicle has two or more rows of seats).\n- Children from the age of four and up to seven years to be permitted in the front of the vehicle if all rear seats are already occupied by children up to the same age (where the vehicle has two or more rows of seats).', '5 Car Seat Mistakes Parents Make With Their Toddlers\nWhile babies need to be in rear-facing car seats, the setup for toddlers is a little different. Which direction do they face? Which seat do I use? When do I change to a booster? What does the law say? According to Safe Kids Worldwide, 73 percent of car seats are either installed or used incorrectly. Here is a list of five common car seat errors parents make with toddlers.\n1. Turning the car seat from rear-facing to forward-facing too soon\nThe recommendation is for children to remain rear-facing until age two or until they outgrow the rear facing limits for your car seat. Toddlers have relatively large and heavy heads compared to the rest of their body. Sitting rear-facing offers the maximum benefit to the head, neck and spine in the event of a crash. They begin in their infant car seats then transition into a convertible car seat that can be installed rear-facing once they have outgrown the infant seat. Many of the newer infant seats now accommodate toddlers up to 30 pounds and 32 inches tall.\n2. Moving too quickly to a booster seat\nOnce a child has reached the maximum height and weight for a rear-facing seat, they should be moved into a forward facing car seat with a 5-point harness. Keep your child in a 5-point harness in a forward-facing car seat until they reach the maximum height and weight limits according to your car seat. Many convertible car seats have harnesses that can be used up to 65 pounds.\n3. Using a booster seat incorrectly\nOnce a child has outgrown his or her forward-facing 5-point harness seat, they may transition to a booster seat. Like car seats, boosters vary in their fit and comfort levels. Choose one that fits easily into your back seat and that is comfortable for the child. Whether you choose a backed or back-less booster depends on the neck and head support of your car’s seats. The tops of their ears should not be taller than the top of the seat or headrest. If so, a booster with a back is the best option. Some children prefer a high-backed booster even if your seat backs are tall as the boosters offer some support for sleeping. If your preschooler likes to wiggle, unbuckle their seat, or lean forward, then they are not ready to transition to a booster.\n4. Using the safety belt incorrectly when using rear-facing, forward-facing and boosters.\nRead the manufacturer’s guide to learn the correct way to latch each seat into your back seat. With the exception of boosters, all car seats need to be attached to the car. Newer boosters are being made using the LATCH system as well but it is not a requirement at this time. When using a booster, make sure the lap portion of the belt fits snuggly across the hip bones of the child and not over their abdomen. The shoulder belt should be worn at all times across the clavicle, down across the sternum and then latched. If the child is putting the shoulder belt behind them or it rides up under their neck, the booster is not adjusted properly. In the event of an accident, wearing only the lap portion of the belt has been associated with severe internal injuries.\n5. Not using a safety seat\nChildren should remain in a safety seat until approximately 80 pounds and a height of 4 feet 9 inches tall. At that point the lap belt should fit snuggly across the hip bones and the shoulder belt should fit comfortably across the shoulder and down across the body over the sternum.\n- Rear-facing – Unmatched safety.\n- The Best Booster Seat.\n- How to find the right car seat.\nSafe Kids Worldwide\n- Buckle up: keeping your kids safe in the car.\nPowered by Bundoo®']"	['<urn:uuid:789d0b43-9aac-4931-bdb4-3f5c360af582>', '<urn:uuid:77454a51-4553-402b-b5ee-d69a95058b0b>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:56:11.774636	11	98	3198
61	baby language development social skills 12 18 months	Between 12-18 months, babies develop key language and social skills. They start using words to communicate, begin with common nouns like 'dog' or 'cup', and understand simple instructions. Socially, they become friendly toward others but don't play cooperatively yet, enjoy adult attention, and recognize themselves in mirrors. By 18 months, they refer to themselves by name and can identify body parts when asked.	"['Baby Care and Development/Baby Development\nTable illustrating child development stagesEdit\n|Age||Motor||Speech||Vision and hearing||Social development|\n|4-6 weeks||Smiles at mother|\n|3 months||Prone:head held up for prolonged periods. No grasp reflex||Talks a great deal||Follows dangling toy from side to side. Turns head round to sound||Squeals with pleasure appropriately. Discriminates smile|\n|5 months||Holds head steady. Goes for objects and gets them. Objects taken to mouth||Enjoys vocal play||Smiles at mirror image|\n|6 months||Transfers objects from one hand to the other. Pulls self up to sit and sits erect with supports. Rolls over prone to supine. Palmer grasp of cube||Double syllable sounds such as \'mumum\' and \'dada\'||Localises sound 45cm lateral to either ear||May show \'stranger shyness\'|\n|9-10 months||Wiggles and crawls. Sits unsupported. Picks up objects with pincer grasp||Babbles tunefully||Looks for toys dropped||Apprehensive about strangers|\n|1 year||Stands holding furniture. Stands alone for a second or two, then collapses with a bump||Babbles 2 or 3 words repeatedly||Drops toys, and watches where they go||dressing, waves goodbye, understands simple commands|\n|18 months||stairs holding onto Handrail|rail. Begins to jump with both feet. Can build a tower of 3 or 4 cubes and throw a ball||\'Jargon\'. Many intelligible words||cup with both hands. Feeds self with a spoon|\nCognitive and creativeEdit\nCreative development could very well be seen as how the child learns in its environment through experimenting in different ways of doing everything. 6-9 months\n- Looks for fallen objects by 7 months\n- Plays ‘peek-a-boo’ games\n- Cannot understand “no” or “Danger”\n- Watches people, objects, and activities in the immediate environment.\n- Shows awareness of distant objects (15 to 20 feet away) by pointing at them.\n- Responds to hearing tests (voice localization); however, loses interest quickly and, therefore, may be difficult to test informally.\n- Follows simple instructions.\n- Reaches for toys that are out of reach but visible\n- Recognizes objects in reverse\n- Drops thing intentionally and repeats and watches object\n- Imitates activities like playing drum loudly\nImage:Babysmile.jpg|thumb|A baby\'s first smile usually occurs four to six weeks after birth.\n- Average length is 50.8-68.6 cm grows approximately 2.54 cm per month.\n- Weighs an average of 3.6-7.3 kg\n- Gains approximately 0.11-0.22 kg per week.\n- Respiration rate is approximately thirty to forty breaths per minute.\n- Normal body temperature ranges from 35.7-37.5°C.\n- Head and chest circumference are nearly equal.\n- Head circumference increases approximately 1.9 cm per month until two months, then increases 1.6 cm per month until four months. Increases are an important indication of continued brain growth.\n- Continues to breathe using abdominal muscles.\n- Posterior fontanel closes by the second month.\n- Anterior fontanel closes to approximately 1/2 inch (1.3 cm).\n- Skin remains sensitive and easily irritated.\n- Legs may appear slightly bowed.\n- Cries with tears.\n- Eyes begin moving together in unison (binocular vision).\n- Rooting and sucking reflexes are well developed.\n- Swallowing reflex and tongue movements are still immature; continued drooling and inability to move food to the back of the mouth.\n- Grasp reflex gradually disappears.\n- Landau reflex appears near the middle of this period; when baby is held in a prone (face down) position, the head is held upright and legs are fully extended.\n- Grasps with entire hand; strength insufficient to hold items. Holds hands in an open or semi-open position.\n- Muscle strength and control improving; early movements are large and jerky; gradually become smoother and more purposeful.\n- Raises head and upper body on arms when in a prone position.\n- Turns head side to side when in a supine (face up) position; near the end of this period can hold head up and in line with the body.\n- Upper body parts are more active: clasps hands above face, waves arms about, reaches for objects.\n- Puts on 0.5 kg per month in weight, doubling birth weight\n- Grows about 1.3 cm in length per month; average length is 69.8-73.7 cm.\n- Head and chest circumferences are basically equal.\n- Head circumference increases approximately 3/8 inch (0.95 cm) per month untl six to seven months, then 3/16 inch (0.47 cm) per month; head circumference should continue to increase steadily, indicating healthy, ongoing brain growth.\n- Breathing is abdominal; ranges from twenty-five to fifty breaths per minute, depending on activity; rate and patterns vary from infant to infant.\n- Teeth begin to appear, with upper and lower incisors coming in first. Gums may become red and swollen, accompanied by increased drooling, chewing, biting, and mouthing of objects.\n- Legs may appear bowed; bowing gradually disappears as infant grows older.\n- True eye color is established.\n- Reflexive behaviors are changing:\n- Blinking reflex is well established\n- Sucking reflex becomes voluntary\n- Moro reflex disappears\n- When lowered suddenly, infant throws out arms as a protective measure.\n- Swallowing reflex appears allows infant to move solid foods from front of mouth to the back for swallowing.\n- Picks up objects using finger and thumb (pincer grip).\n- Reaches for objects with both arms simultaneously; later reaches with one hand or the other.\n- Transfers objects from one hand to the other; grasps object using entire hand (palmar grasp).\n- Handles, shakes, and pounds objects; puts everything in mouth.\n- Able to hold bottle.\n- Sits alone without support, holding head erect, back straightened, and arms propped forward for support\n- Pulls self into a crawling position by raising up on arms and drawing knees up beneath the body; rocks back and forth, but generally does not move forward.\n- Lifts head when placed on back.\n- Can roll over from back or stomach position.\n- May accidentally begin scooting backwards when placed on stomach; soon will begin to crawl forward.\n- Infants reach approximately 1-1/2 times their birth length by first birthday\n- Weight increases by approximately 0.5 kg per month; birth weight nearly triples by one year of age: infants weigh an average of 9.6 kg.\n- Respiration rates vary with activity: typically, twenty to forty-five breaths per minute.\n- Body temperature ranges from 96.4°F to 99.6°F (35.7-37.5°C); environmental conditions, weather, activity, and clothing still affect variations in temperature.\n- Head and chest circumference remain equal.\n- Continues to use abdominal muscles for breathing.\n- Anterior fontanel begins to close.\n- Approximately four upper and four lower incisors and two lower molars erupt.\n- Arm and hands are more developed than feet and legs (cephalocaudal development); hands appear large in proportion to other body parts.\n- Legs may continue to appear bowed.\n- Feet appear flat as arch has not yet fully developed.\n- Visual acuity is approximately 20/100.\n- Both eyes work in unison (true binocular coordination).\n- Can see distant objects (15 to 20 feet away) and points at them.\n- Reaches with one hand leading to grasp an offered object or toy.\n- Manipulates objects, transferring them from one hand to the other.\n- Explores new objects by poking with one finger.\n- Uses deliberate pincer grasp to pick up small objects, toys, and finger foods.\n- Stacks objects; also places objects inside one another.\n- Releases objects or toys by dropping or throwing; cannot intentionally put an object down.\n- Beginning to pull self to a standing position.\n- Beginning to stand alone, leaning on furniture for support; moves around obstacles by side-stepping.\n- Has good balance when sitting; can shift positions without falling.\n- Creeps on hands and knees; crawls up and down stairs.\n- Walks with adult support, holding onto adult\'s hand; may begin to walk alone.\nTrust versus Mistrust (Erik Erikson)\nToddlers (12-24 months)Edit\n- Rate of growth slows\n- Height increases approximately 5cm- 7.6cm with an average height of 81cm – 89cm\n- Weighs 9.6-12.3 kg; gains 0.13-0.25 kg per month; weight is now approximately 3 times the child\'s birth weight.\n- Respiration rate is typically twenty-two to thirty breaths per minute; varies with emotional state and activity.\n- Heart rate (pulse) is approximately 80 to 110 per minute.\n- Head size increases slowly; grows approximately 1/2 inch (1.3 cm) every six months; anterior fontanel is nearly closed at eighteen months as bones of the skull thicken.\n- Chest circumference is larger than head circumference.\n- Rapid eruption of teeth; six to ten new teeth will appear.\n- Legs may still appear bowed.\n- Body shape changes; takes on more adult-like appearance; still appears topheavy; abdomen protrudes, back is swayed.\n- Visual acuity is approximately 20/60.\n- Crawling|Crawls skillfully and quickly.\n- Stands alone with feet spread apart, legs stiffened, and arms extended for support.\n- Gets to feet unaided.\n- Most children walk unassisted near the end of this period; falls often; not always able to maneuver around obstacles, such as furniture or toys.\n- Uses furniture to lower self to floor; collapses backwards into a sitting position or falls forward on hands and then sits.\n- Enjoys pushing or pulling toys while walking.\n- Repeatedly picks up objects and throws them; direction becomes more deliberate.\n- Attempts to run; has difficulty stopping and usually just drops to the floor.\n- Crawls up stairs on all fours; goes down stairs in same position.\n- Sits in a small chair.\n- Carries toys from place to place.\n- Enjoys crayons and markers for scribbling; uses whole-arm movement.\n- Helps feed self; enjoys holding spoon (often upside down) and drinking from a glass or cup; not always accurate in getting utensils into mouth; frequent spills should be expected.\n- Helps turn pages in book.\n- Stacks two to four objects.\n- Enjoys object-hiding activities\n- Early in this period, the child always searches in the same location for a hidden object (if the child has watched the hiding of an object). Later, the child will search in several locations.\n- Passes toy to other hand when offered a second object (referred to as ""crossing the midline""-an important neurological development).\n- Manages three to four objects by setting an object aside (on lap or floor) when presented with a new toy.\n- Puts toys in mouth less often.\n- Enjoys looking at picture books.\n- Demonstrates understanding of functional relationships (objects that belong together): Puts spoon in bowl and then uses spoon as if eating; places teacup on saucer and sips from cup; tries to make doll stand up.\n- Shows or offers toy to another person to look at.\n- Names many everyday objects.\n- Shows increasing understanding of spatial and form discrimination: puts all pegs in a pegboard; places three geometric shapes in large formboard or puzzle.\n- Places several small items (blocks, clothespins, cereal pieces) in a container or bottle and then dumps them out.\n- Tries to make mechanical objects work after watching someone else do so.\n- Responds with some facial movement, but cannot truly imitate facial expression.\n- Produces considerable ""jargon"": puts words and sounds together into speech-like (inflected) patterns.\n- Holophrastic speech: uses one word to convey an entire thought; meaning depends on the inflection (""me"" may be used to request more cookies or a desire to feed self). Later; produces two-word phrases to express a complete thought (telegraphic speech): ""More cookie,"" ""Daddy bye-bye.""\n- Follows simple directions, ""Give Daddy the cup.""\n- When asked, will point to familiar persons, animals, and toys.\n- Identifies three body parts if someone names them: ""Show me your nose (toe, ear).""\n- Indicates a few desired objects and activities by name: ""Bye-bye,"" ""cookie""; verbal request is often accompanied by an insistent gesture.\n- Responds to simple questions with ""yes"" or ""no"" and appropriate head movement.\n- Speech is 25 to 50 percent intelligible during this period.\n- Locates familiar objects on request (if child knows location of objects).\n- Acquires and uses five to fifty words; typically these are words that refer to animals, food, and toys.\n- Uses gestures, such as pointing or pulling, to direct adult attention.\n- Enjoys rhymes and songs; tries to join in.\n- Seems aware of reciprocal (back and forth) aspects of conversational exchanges; some turn-taking in other kinds of vocal exchanges, such as making and imitating sounds.\n- Usually friendly toward others; less wary of strangers.\n- Helps pick up and put away toys.\n- Plays by themselves for short periods\n- Enjoys being held and read to.\n- Often imitates adult actions in play.\n- Enjoys adult attention; likes to know that an adult is near; gives hugs and kisses.\n- Recognizes self in mirror.\n- Enjoys the companionship of other children, but does not play cooperatively.\n- Beginning to assert independence; often refuses to cooperate with daily routines that once were enjoyable; resists getting dressed, putting on shoes, eating, taking a bath; wants to try doing things without help.\n- May have a tantrum when things go wrong or if overly tired or frustrated.\n- Exceedingly curious about people and surroundings; toddlers need to be watched carefully to prevent them from getting into unsafe situations.\nAutonomy vs. Shame and Doubt (will)\n- Seminars in child and adolescent psychiatry (second edition) Ed. Simon G. Gowers. Royal College of Psychiatrists (2005) ISBN 1-904671-13-6', 'Share this page\nLanguage development in children aged 1-2 years is amazing. Your child will start to understand what you say to him. And by the year’s end, you might even understand some of what he says to you! Here are some of the things you might expect at this age.\nAt 1-2 years, your child will learn to use and understand more words and more types of words. At first she’ll understand mostly nouns – for example, ‘dog’, ‘bus’ and ‘couch’. Eventually she’ll understand a few verbs – for example, ‘eat’ and ‘run’. Adjectives come next – for example, ‘big’ and ‘blue’.\nAt around 12 months, your child will understand the names of things nearby. For example, he’ll understand the words for:\n- common objects like ‘cup’ or ‘doll’\n- body parts like ‘tummy’ or ‘toe’\n- clothes like ‘sock’ or ‘hat’.\nBut your child will use the same word to refer to different things. For example, she might call all animals ‘doggie’.\nAt around 15 months, your child will point to things further away and ask you to name them.\nAt around 18 months, your child will refer to himself by name. A few months later, he’ll begin to understand and use ‘I’ to refer to himself. This is when he starts to realise he’s a separate person with his own ideas.\nDuring this year, your child will understand:\n- some familiar phrases like ‘Give me a kiss’\n- simple instructions like ‘Stop that’\n- very, very simple explanations.\nYour child will start being able to identify parts of her body and point to objects when asked – for example, ‘Show me the truck’.\nLanguage development includes learning to use words and sentences.\nAt around 12 months, your child will start using words to communicatewith you. His babble will start turning into real words. Your child might also enjoy saying the same word over and over.\nYour child will begin to put two words together as she nears two years – for example, ‘mummy car’, ‘me go’ or ‘sock foot’. Words like ‘the’ and ‘is’ will be missing in these two-word sentences. She’ll use only a few descriptive words at this age – for example, ‘big’, ‘red’ or ‘sunny’. Her word combinations will consist mainly of nouns and some verbs (‘dog eat’, ‘car go’).\nYour child will use a range of speech sounds, but it’s normal for him to pronounce words differently from the way adults say them. For example, he might say ‘tar’ instead of ‘car’, or he might leave off the ends of words altogether, like ‘ca’ instead of ‘cat’.\nYour child’s pronunciation will often be hard to understand. But by the time she’s two years old, someone who doesn’t know her well should be able to understand about half of what she says.\nLearning to have a conversation is part of language development.\nYour child might draw attention to something by saying ‘Wassat’ (‘What’s that?’) or pointing. And he knows that if you point to something and say ‘Look’, you’re showing him something.\nYour child will answer simple questions, and she’ll also understand the difference in your tone when you ask a question or make a statement. And she’ll know what it means when you say ‘no’ or ‘not’.\nYour child will try to make it easier for you to understand him by combining words, gestures and sounds and by changing the tone of his voice.\nAnd your child will enjoy imitating the actions of people around her, and start to understand how to get attention from others by ‘showing off’.\nThis website is a collaboration between Tusla and the Childhood Development Initiative (CDI)']"	['<urn:uuid:801c20b4-fe40-4d68-a5fb-9b2a725d44fb>', '<urn:uuid:0b7bdf47-b67e-4372-bdcd-7624e507c195>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-13T04:56:11.774636	8	63	2790
62	Do squash and table tennis have similar rules for replaying points?	Both sports have systems for replaying points but handle them differently. In squash, replayed points are called 'lets' and are used primarily for safety reasons when there is interference between players, such as to prevent being hit with the ball or racquet. In table tennis, 'lets' occur for minor infractions during service or return, such as when the ball touches the net during an otherwise legal serve, if service is made before the receiver is ready, or if there's an outside distraction. Additionally in table tennis, if a server causes two consecutive lets, a point is awarded to the receiver - a rule that doesn't exist in squash.	"[""Complete Singles Rules (as of January 2014)\nSummary of Changes\nONE MINUTE GUIDE TO THE RULES OF SQUASH\nSCORING - A match will consist of the best of three or five games at the option of the organizers of the competition. Each game will be played to eleven points and 90 seconds rest is given between each game. Either player may score points on each rally. The server, on winning a stroke, scores a point and retains the service; the receiver, on winning a stroke, scores a point and becomes the server. The player who scores eleven points by a margin of at least two points wins the game. If the score reaches ten-all, the game continues until one player has a margin of two points, whereupon that player wins the game. The score of each game will be recorded with the actual number of points won by each player; the winning players score is recorded first (e.g. 11-7, 3-11, 12-10, 10-12, 25-23).\nWARM-UP - Before the start of play, players have 5 minutes to warm up the ball together (2 + minutes on each side of the court)\nSERVE - The right to serve first is decided by the spin of a racquet. The server continues to serve until losing a rally. The opponent then becomes the server and so on throughout the game. The player winning the previous game serves first in the next. At the beginning of each game and at the first opportunity to serve after winning a rally, the server may serve from either side of the court. If the server continues to score points, he or she must then serve from alternate sides of the court. If the server serves from the wrong side, there is no penalty, but he or she must then serve from the correct side.\nTo serve, a player must, while leaving at least one foot in the service box, drop or throw the ball and strike it so that it travels directly on to the front wall between the service line (the red line approx. 6 ft. from the floor extending across the front wall) and the out line at the top of the front wall The ball must land in the back half of the opposite court on its return from the front wall, though it may touch the side wall or back wall first. The ball must not touch the walls before being struck by the server. Only one serve is allowed.\nPLAY - After a correct serve, players alternate hitting the ball until the rally ends, that is, when it is no longer possible to make a good return. A good return is one in which the ball is returned before the ball bounces twice, is returned above the tin, does not hit the ceiling or any wall on or above the out lines, and, at some point, touches the front wall. The ball may strike the back or side walls in any combination in its flight toward or away from the front wall.\nINTERFERENCE - Interference occurs when one player hinders the opponent's choice of movement to the ball or the striking of the ball. To indicate interference, the player interfered may call a (let÷.\nLETS - (Let, please÷ is called out loud by the player who has suffered interference. When a (let÷ is called, the rally does not count and is replayed. Lets are used for safety to protect players from dangerous situations such as being hit with the ball or racquet.\nEQUIPEMENT AND SAFETY\nUse Dunlop squash balls, the Official Ball of Squash Alberta. The Dunlop Pro is the main ball used for tournament play, but other speeds are available for different skill levels and teaching.\nRacquets have been made from wood, aluminum, ceramic material, graphite or graphite composite. Choose one that is the right weight and balance for you. Absorbent grips are recommended to prevent the racquet from slipping in the hand.\nShoes must have non-marking soles, strong heel and arch support and should not be raised. Shoes for outdoor activity must not be used because they collect mud, dirt, clay, and other material, which can be left on the court, causing slippery and dangerous conditions.\nEyeguards are protective equipment designed to protect the eyes from the impact of ball or racquet. It is strongly recommended that all players wear suitable eye protection. Approved eyeguards are mandatory for youth participants (under 19). Only eyeguards that appear on the Squash Canada approved list are acceptable for sanctioned activities. Black Knight is the Official Eyewear of Squash Alberta\nWear loose comfortable clothing designed for tennis, badminton or squash play. Shirts with short sleeves must be worn to absorb perspiration and prevent it from collecting on the floor and becoming slippery and dangerous."", 'Table Tennis Scoring Rules\nThe origins of table tennis can be traced back to the late 19th century, when upper-middle class Victorians in England began hitting a cork or rubber ball across their dining tables to simulate lawn tennis. While the game’s scoring was originally similar to its parent sport, unique rules were formalized in 1926 with the creation of the International Table Tennis Federation, which is still the global administrative body of the sport in 2010.\nScoring a Point\nOnce the ball is put in play by the server, either player can score a point. You receive a point if your opponent fails to return the ball over the net and land it on the opposing side of the table, or if he strikes the ball twice in succession, allows the ball to bounce twice, hits the ball with a piece of apparel or the side of the racket, touches the ball with his free hand or moves the playing surface. If the players are in wheelchairs, a player receives a point if his opponent does not maintain minimum contact with his seat, touches the table with either hand before hitting the ball or touches his foot to the floor.\nA let in table tennis occurs when a minor infraction is committed during the service or return of the ball. Lets are called when the ball touches the net assembly during an otherwise legal service, if the service is made before the receiver is ready, if a distraction outside of the player’s control occurs or if the umpire stops play. If a let is called at anytime, no point is scored. However, if a server is at fault for two consecutive lets, a point is awarded to the receiver.\nWinning a Match\nThe ITTF dictates that a table tennis match is composed of “the best of any odd number of games.” The number of games to be played in the match must be determined before the competition begins. A game is won by the first player to score 11 points while holding a two point lead. If both players reach 10 points, the game continues until one player leads by two points.\nThe Expedite System\nTable tennis is designed to be a fast-paced game, and the rules of the ITTF provide for quick scoring if a game is not completed within 10 minutes. Once the time limit is reached, the umpire stops the game and the player who served the interrupted rally serves again. The players then alternate serving, and if the receiving player makes 13 legal returns, he receives a point. The expedite system is not used if 18 or more points have already been scored within the 10-minute time limit. If the expedite system is put in place, it remains in place until the game is completed.\nWriting professionally since 2005, Ryan Haas specializes in sports, politics and music. His work has appeared in ""The Journal-Standard,"" SKNVibes and trackalerts. Haas holds a Bachelor of Arts in English and creative writing from the University of Illinois.']"	['<urn:uuid:aae1eab9-e639-42dd-9517-726f09078bc5>', '<urn:uuid:aaadf445-f595-41a6-99f3-a4f6b5174af3>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:56:11.774636	11	108	1306
63	What makes yeast a good organism for testing population dynamics in a laboratory setting?	Yeast is an efficient model because it is a simple, unicellular organism with only two basic functions: survival and reproduction. It lacks rational thinking that could impact experimental results, and it reproduces rapidly - taking only 72 hours to see results compared to weeks or months for multicellular organisms.	['How Environmental Factors Affect a Yeast Population’s Ability to Reproduce\nThe objective of this experiment is to emphasize the influence that limiting factors have on a population. This lab tests yeast, a common component in baking, against two environmental factors (changes in temperature or concentration) to see what effect these have on the population dynamics of the yeast over a period of 72 hours.\nThere are two sections of tests included in this experiment: biotic and abiotic factors. The abiotic factor being tested here is what effect the temperature of the yeast’s environment has on its ability or inability to reproduce efficiently. The lab tests the yeast in three separate temperature settings: a cold temperature (4ᵒC), room temperature (22ᵒC), and a hot temperature (30ᵒC).\nHere, the independent variable is the temperature of the yeast habitat and the dependent variable is the amount (in mL) of CO2 gas produced by the yeast. The biotic factor in this experiment is what effect the concentration of the yeast has on its ability or inability to reproduce efficiently. The lab tests three different concentrations of the yeast: add 0.25 mL, 0.5 mL, or 1 mL of yeast suspension to the test tube. The independent variable is the amount of yeast concentration added and the dependent variable is the amount (in mL) of CO2 gas produced by the yeast.\nYeast is a single-cell fungus that produces carbon dioxide (CO2) as a byproduct of cellular respiration. Therefore, the more yeast cells in an area, the more CO2 they will be able to produce. Yeast has many uses as a common ingredient in many foods and drinks, such as alcoholic beverages like beer and wine, and acts as a leavening agent in baking cakes, bread, and other foods by converting the fermentable sugars in the food into CO2. This is what makes the dough in many foods rise while baking.\nThis lab closely monitors a yeast population over a period of 72 hours, with various limiting factors being applied to the yeast population. These are factors that have the potential to greatly influence a population’s dynamics, sometimes positively and sometimes negatively. They are generally categorized into two groups: biotic and abiotic. Biotic factors pertain to life or living things, and some examples in an environment include predators that can kill or injure an animal of a species, competitors that make it more difficult for one animal to access vital resources like food and water, and pathogens or parasites that can quickly kill or weaken a species.\nIn most cases, the existence of predators is a good thing because it helps to keep the population from becoming unbalanced with the coexisting species living around them. However, if the predator population becomes too large, or if an abundance of new predators is introduced to the area, the population of this species will quickly decrease, and possibly be endangered or, after many years, extinct. In addition, having competition in an environment is important to keeping to well-balanced between plants and animals, but can backfire when there is too much competition, and plant life becomes scarce, unable to support the animals. This is true with all limiting factors: they can have a good or bad impact on a population.\nOn the contrary, abiotic factors pertain to non-living things, like sunlight, climate, temperature, and varying amounts of rainfall. For example, rainfall is essential in an ecosystem to hydrate both animal and plant life, and it is necessary for survival. However, too much rainfall at one period of time, or flooding, can wash away and kill many forms of plant life, damaging the populations that rely on plants for food, in turn. In addition, temperature can impact a population like yeast (which is tested in this experiment) positively or negatively.\nFor example, if the yeast’s environment is very warm, the yeast will be able to thrive in it. This is due to its activation in warmer temperatures. It is able to reproduce faster and more efficiently in a warm environment, thus producing more CO2. However, when placed in colder temperatures, it deactivates and, although it does not die or stop producing CO2, this drastically slows down the rate of reproduction and production of CO2. Therefore, these factors (both biotic and abiotic) can potentially, given the circumstances, greatly impact the dynamics of a population. This lab’s goal is to demonstrate these effects on yeast populations.\nYeast is the most efficient model to demonstrate these population dynamics because it can easily be closely watched and it is a simple organism. Testing multicellular organisms in a lab can be more challenging because many of them possess some sort of rational thinking method that can impact the results in a lab. Yeast is a simple, unicellular organism that has only two intentions in its life: survive and reproduce. This eliminates any impact that yeast’s intelligence could have on the experiment. This is one less control that needs to be worried about. Also, yeast’s reproduction is rapid.\nIf the lab were to test a multicellular organism, it would take weeks or even months to get a proper result. Using yeast, the lab only took 72 hours. Furthermore, collecting CO2 from the yeast is a viable method for determining the population growth of yeast because CO2 is a byproduct of the yeast’s cellular respiration process. As more yeast cells are produced, more CO2 will be produced because there will be more cells to produce the gas in the enclosed environment as they respire, as measured through the volume displacement method.\nA carrying capacity in a population is “the maximum number of individuals of a given species that an area’s resources can sustain indefinitely without significantly depleting or degrading those resources” (“Population Size”). Once the carrying capacity is reached or exceeded, this will take a toll on the environment as a whole. The resources that support this species would quickly diminish, and any other animals that this species preys on would become endangered because there are too many predators killing them. This is why a good balance of limiting factors needs to be maintained to evenly support an ecosystem as a whole.\nThis lab investigates these factors, applies them to a yeast population, and analyzes the results.\nEQUIPMENT AND PROCEDURE\nSafety goggles, lab aprons\nSeveral 1 mL graduated dropping pipettes\nPlastic conical tubes\nLarger glass test tubes\nAccess to incubator and refrigerator\nProcedure A (The Effect of Temperature on CO2 Production)\n1.) Gather materials. 2.) Place 0.5 mL of the yeast concentration into the small conical tube. 3.) Add 15 mL of the molasses solution into the same conical tube. This tube should almost be filled to the very top. If needed, add more molasses to fill the volume. 4.) Wearing gloves, carefully cover the end of the test tube with the index finger and invert it five times to mix the solutions thoroughly. 5.) Carefully slide the larger glass test tube over the smaller tube. Try to apply pressure on both tubes with the thumb and third fingers. 6.) Quickly invert the tubes so that the bottom of the conical tube is now facing up. A small amount of the fluid may come out of the conical tube; this is okay. 7.) Repeat steps 2-6 for two more test tubes of yeast. Label each test tube with “cold”, “room”, or “hot” to indicate the environment they will be placed in.\n8.) Using the measurements of the inverted tube, measure the volume of the liquid in the conical tubes and record the data. This is the initial reading. 9.) Place test tubes in three separate glass beakers. Then, place the beakers in the appropriate locations (i.e. the “hot” beaker in an incubator set at 30ᵒC, “cold” in a refrigerator at 4ᵒC, and “room” in an average environment at 22ᵒC). 10.) After 24 hours, measure the size of the bubbles with the measurements on the smaller tube. Record the data. 11.) Determine the total amount of carbon dioxide produced by subtracting the initial bubble size from the 24 hour bubble measurement. Record in data table. 12.) Return the tubes back to the appropriate locations.\n13.) Repeat steps 10-12 at 48 and 72 hour intervals.\n14.) When instructed, clean up the tubes and equipment.\nProcedure B (The Effect of Yeast Concentration on CO2 Production) 1.) Place 0.25 mL, 0.5 mL, and 1 mL of the yeast concentration into three separate\nconical tubes. Use glass beakers to hold each test tube to prevent spilling. 2.) Add 15 mL of the molasses solution into the same conical tube. This tube should be almost filled to the very top. If needed, add more molasses to fill the volume. 3.) Wearing gloves, carefully cover the end of the test tube with the index finger and invert it five times to mix the solutions thoroughly. 4.) Carefully slide the larger glass test tube over the smaller tube. Try to apply pressure on both tubes with the thumb and third fingers 5.) Quickly invert the tubes so that the bottom of the conical tube is now facing up. A small amount of the fluid may come out; this is okay. 6.) Repeat steps 2-5 for the two other test tubes.\n7.) Using the measurements of the inverted tube, measure the volume of the liquid in the conical tube and record the data. This is the initial reading. 8.) After 24 hours, measure the size of the bubbles with the measurements on the smaller tube. Record the data. 9.) Determine the total amount of carbon dioxide produced by subtracting the initial bubble size from the 24 hour bubble measurement. Record data. 10.) Repeat steps 8-9 at 48 and 72 hour intervals.']	['<urn:uuid:d7f08005-760c-47a2-8863-6285ea27c2fa>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	14	49	1612
64	My cows aren't getting pregnant. What does that important hormone progesterone do?	Progesterone is secreted by the corpus luteum (CL) and is essential for pregnancy. This hormone prevents the dairy cow from coming into heat and ovulating when pregnant. It prepares the uterus for pregnancy while preventing the cow from entering heat again.	['A working knowledge of the dairy cow’s estrous cycle and its associated hormones can help you\nimprove conception rates and boost reproductive efficiency. The following article outlines the most\ncommon reproductive hormones, the role each plays within the estrous cycle and how they relate to\nREpROduCTIvE HORmOnESDairy cattle reproduction is controlled by multiple hormones which are produced by numerous\nendocrine glands. These hormones are secreted by the glandular cells and pass into the blood where\nthey are transported throughout the body to complete their respective functions.1\n•Estradiol is produced by the Graafian follicle and is responsible for multiple functions including\nuterine development, onset of behavioral estrus and the increase of vaginal mucous secretion.\nAdditionally, estradiol cues the release of the gonadotropin releasing hormone from\n•Gonadotropin Releasing Hormone (GnRH) causes ovulation of the dominant follicle by\ncausing the release of the luteinizing hormone from the pituitary gland.\n•Luteinizing Hormone (LH) is secreted from the pituitary gland and travels through the blood to\nthe ovary after being signaled for release by the GnRH. LH is responsible for causing the follicle to\nrupture (ovulate) by acting on the ovary which will release the egg. Following ovulation the LH also\nstimulates the development of the corpus luteum (CL).\n•Follicle Stimulating Hormone (FSH) is also secreted by the pituitary gland and travels through\nthe blood to the ovary after being signaled for its release by the GnRH. FSH stimulates the growth,\ndevelopment and function of the follicle.\n•progesterone is secreted by the CL and is essential for pregnancy. This hormone also prevents the\ndairy cow from coming into heat and ovulating when pregnant.\n•prostaglandin (pGF2α) is secreted by the uterus and causes regression of the CL and a decrease\nin progesterone levels. If the cow is pregnant, the embryo will block the release of PGF2α and allow\nthe CL to continue to secrete progesterone.\nTHE ESTROuS CyCLEThe 21-day estrous cycle relies on each of the reproductive hormones for successful completion.\nThe hormones work in the following order on the day highlighted.\nFigure one below illustrates the hormone levels throughout the estrous cycle.\n0 15 16 17 18 19 20 0 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20 0 1 2 3 4 5 6\nday 0: Defined as the day of estrus. During this period, a cow will stand to be mounted. Heat\nusually lasts for an average of 8 – 12 hours but can sometimes last as few as four hours.2 During this\ntime the follicle structure on the ovary is present. Estradiol from the follicle triggers GnRH release.\nWhen GnRH is released, it will also cause the release of LH, which in turn induces ovulation and\nreleases the egg. The LH surge occurs at the onset of estrus (Day 0); ovulation occurs approximately\n32 hours following the LH pulse. If sperm are present when the egg is released, fertilization will\noccur—a fertilized embryo will stay in the oviduct three to four days before moving to the uterus.\nday 1 – 5: The follicle “luteinizes” to form the CL which secretes progesterone.\nday 9 – 10: The CL reaches maximum size and progesterone output.\nday 16 – 18: If a cow does not become pregnant PGF2α will be released by the uterus. As\nprogesterone levels decrease and an increase in GnRH pulsatility takes place, release of LH increases,\nand a preovulatory estrogenic follicle develops.1\nIf pregnancy has occurred, the developing embryo blocks the release of PGF2α and progesterone\ncontinues to be secreted. As mentioned previously, progesterone is necessary for pregnancy; it prepares\nthe uterus for pregnancy, while preventing the cow from ovulating and entering heat again.\nThe estrous cycle is continually repeated until a successful pregnancy has occurred.\nThe estrous cycle and associated hormones are the cornerstone of dairy cattle reproduction.\nDeveloping a better understanding and utilizing this knowledge on your dairy operation will allow\nyou to optimize reproductive efficiency with your herd.\n1 Whittler, J.C. Reproductive Anatomy and Physiology of the Cow. Department of Animal Sciences. University of Missouri.\nAvailable at: http://extension.missouri.edu/p/G2015. Accessed August 15, 2011.\n2 Burns, P.D. The Dairy Cow Heat Cycle. Colorado State University.\nAvailable at: http://www.cvmbs.colostate.edu/ilm/proinfo/cdn/2002/heat%20cyclesep02.pdf. Accessed August 15, 2011.\nKirk, J.H. Review of Reproductive Hormones For Dairy Cows. Veterinary Medicine Extension. University of California – Davis.\nAvailable at: http://www.vetmed.ucdavis.edu/vetext/INF-DA/INF-DA_REVREPRO.HTML. Accessed August 15, 2011.\nAlcohol Withdrawal Syndrome PHYSICIAN Pre-Printed Orders *ORDER* Check box to activate orders below if applicable. Cross off non-applicable orders with a single line. ALLERGIES: None Known Yes Psychiatric Consult “Suicide Risk Assessment Tool” post withdrawal (required for all patients once withdrawal completed) Implement Withdrawal Syndrome Precautions. Scor\nSTRESSE E DEPRESSÃO A noção actual de stresse ( stress ) vem de Hans Selye, um húngaro que viveu nos Estados Unidos da América e foi o autor de uma série de livros com um mesmo e elucidativo título: O Síndroma Geral da Adaptação (I, II, III, IV). Visivelmente preocupado com problemas de adaptação ao meio, Seley concebeu à partida o stresse como a reacção normal do organi']	['<urn:uuid:34202500-c4b8-4b8e-9bfc-0a66cd12d3eb>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	12	41	842
65	flat roof shingle slate durability compare	Flat roofs and shingles are more prone to weather-related issues compared to slate roofs. Flat roofs can experience water pooling and leaking during heavy rainfall, while shingles can break and require replacement if damaged. In contrast, slate roofs are extremely durable, lasting 60-100 years or more with proper maintenance. Slate tiles are constructed from rock, don't absorb water, and are fire resistant. They come in hard and soft varieties, with hard slate being particularly durable in harsh weather conditions. However, slate tiles are heavier, requiring additional roof support, and can crack if walked on improperly.	['Even the newest and most well-maintained roof isn’t impervious to rain and other weather elements.\nThere might be areas of your roof that are prone to pooling water, which can cause dripping or leaking during heavy rain.\nLearn about the 5 most common roof leaks and how to fix them.\n1. Flat Roofs\nIf you have flat roofs, it might not be able to keep up with heavy rainfall, especially if it lacks the kind of drainage designed for it. Flat roofs are more prone to leaking or dripping than with roofs with more shape. If it is also not properly sealed, there might be sections where water can pool.\nWhat you need to do is make sure your roof has properly sized gutters and downspouts where water can drain away instead of setting over the foundation. Check if eaves and flashings are properly sealed and in good standing. Always clean out the debris and other materials that can cause water to be stuck on your roof.\nDon’t forget to check the perimeter foundation of your home or building. Water can also accumulate here which can seep through the siding, cracks and crevices which can lead to the lower levels of your house to be wet or damp.\n2. Cracked Flashing\nFlashings are thin pieces of metal that are installed on the joints of your roof or under shingles to create a water-resistant barrier. They can be either concealed or exposed. If concealed, they usually have a rubberized coating overtop. If exposed, they will look like long runs of sheet metal.\nWhen flashings are cracked, the usual cause is the corrosion of the tar sealant due to time and weather elements. This will cause your flashings to be left exposed which can worsen its condition.\nOnce you locate the cracked segment, remove the nails used to secure the old flashing and take it out. Put on a new run of flashing in its place and secure it using roofing nails. Apply a liberal amount of roofing sealant to the nail heads.\n3. Broken Shingles\nShingles are the exterior layer of a roof so they are easy to spot. If your roof has different-colored patches, there might be missing or broken shingles.\nWhat you can do is slide a pry bar underneath the row of nails that connects the broken shingle to the one underneath it. Press down the shingle as you gently remove the nail. Replace with a new one and use four new nails to secure it.\n4. Plumbing Leaks\nOne source of roof leak is vent caps for ventilation exhaust fans. A fan duct that doesn’t have proper insulation will instigate air moisture which can lead to duct condensation.\nMake sure your bath fan duct is well insulated and that there are no signs of dampness or moisture within the attic or in the bathroom walls.\n5. Leaking Skylight\nSkylights leak during heavy rains. It might not always be because it is damaged but maybe the rain and runoff has made condensation to build up and leak through.\nBut if you always notice wet spots, your skylights might have been improperly measured and fitted upon installation, or there is decayed insulation along the skylights edges.\nYou can fix this by clearing off all debris so you can have a clear view of the window itself. If there are cracks, seal it off with a layer of clear silicone along its length. If the surrounding flashings are the culprit, replace as needed.', 'Whether it’s time to replace your roof or you’re building a new home, the roof you choose is essential to the longevity and visual appeal of your home. There are many roofing materials to choose from, including asphalt, which is the cheapest and most common, to tile and slate, which are more expensive.\nIn this article, we look at the slate roof and why it’s a good value for the price, and why you should consider this option when choosing a new roof for your home.\nWhat is a Slate Roof?\nSlate tiles are constructed from rock which comes from volcanic ash and clay. Slate roofs are among the oldest used and date back centuries because of the availability of the material and the longevity of the tiles. There are slate roofs built hundreds of years ago that still hold up to this day.\nThere are two types of slate roofs available — hard and soft. The hard variety is strong and durable, making it ideal for areas that experience harsh weather. Hard slate tiles are also fire resistant and don’t absorb water, which makes them less prone to warping.\nThe soft slate tile isn’t as durable as the hard slate, but it does retain the fire and water resistance of hard slate. People may opt for the soft variety because it is somewhat less expensive while providing much of the same advantages.\nHow Long Do Slate Roofs Last?\nAs mentioned, slate roofs can last for a hundred years or more with proper care and maintenance, and many factors come into play when discussing the longevity of a slate roof. On average, a well-maintained slate roof in ideal conditions lasts about 60 years.\nWeather plays a significant factor in the longevity of slate roofing tiles, and you will have to replace the underlay about every 30-40 years to maintain the health of the roof and the integrity of the tiles. Also, the longevity is mostly dependent on the maintenance you do, taking care to fix small problems before they turn into large ones down the road.\nWhat is the Cost of a Slate Roof?\nOne reason many people steer clear of slate roofs is the cost, and to be fair, it is one of the more expensive roofing tiles you can buy. If your current home has a slate roof, the cost to upkeep it is minimal, and you likely won’t have any large expenditures during ownership.\nHowever, if you’re installing a brand new slate roof, you can expect to pay about $1,500 per square, and in roofing terms, a square is a 10×10 foot area. By way of comparison, asphalt shingles typically cost about $200 per square, so you can see why so many people opt for asphalt over slate.\nAnother thing to consider when talking about the cost is the installation. Asphalt shingles are ubiquitous, and almost every contractor is familiar with this material. Because it’s so widely available, and because of its familiarity, the labor cost to install an asphalt roof is relatively cheap compared to more exotic materials such as slate or concrete.\nAlso, since not every roofing contractor is skilled at installing slate, you need to hire one who has the experience, which means higher labor costs. You don’t want to cut corners on a slate roof installation because doing so will cost you more money in repairs due to a shoddy job down the road.\nCommon Issues with Slate Roofs\nWhile slate roofs sound like the perfect option, if you want a long-lasting roof that offers protection against fire and the elements, there are some disadvantages to installing these as well.\nThe major disadvantage, as mentioned above, is the cost of the tiles themselves and the cost of labor for installation. Depending on the type of slate you decide to install, the roof can cost anywhere from $1,000 to $6,000 a square installed. Also, as mentioned, not every roofing contractor has experience installing slate roofs, which makes it more difficult to find a qualified installer in some areas.\nThickness of Tiles Can Be Uneven\nAnother disadvantage to slate tiles it that they’re sometimes not gauged, which means the thickness varies from tile to tile.\nSlate Tiles are Heavy\nThe weight of the roof is also something to consider. Slate tiles are heavy, and the deck of the roof needs reinforcing to deal with the weight. Slate roofs can weigh up to 1,500 pounds, and you’ll need an inspection of your roof’s support system to make sure it can handle the weight before you go down that road.\nDifficult to Find Tile Replacements\nFinally, slate roofing tiles can become damaged if you have roofers who have to go up and work on the roof. If tiles get broken, it can be challenging to find a replacement that matches the exact color.\nDo you Need a Contractor to Fix your Slate Roof?\nThe best advice if you want to keep your slate roof in tip-top condition is to hire a professional when you need fixing or maintenance; however, because of the high cost, you can usually handle minor repairs and fixes yourself if you know what you’re doing. When you have your slate roof installed, ask the roofing contractor if they offer a maintenance package or maintenance services to keep the roof in shape.\nThe good news is because slate doesn’t warp, corrode, or attract mold, it doesn’t require as much maintenance like other roofing materials. Make sure you keep trees that overhang the roof trimmed to avoid any damage caused by falling branches and be aware of animals that may climb up on the roof and crack the tiles. Other than that, you should be fine.\nSlate is a gorgeous and durable roofing material that is sure to give your home value and curb appeal. If you’re willing to shell out the high up-front expense and take care of it, you will have a roof that will likely last your entire lifetime.']	['<urn:uuid:eb1d7e46-5fbf-4e9c-a1a3-d1fd37fbccf0>', '<urn:uuid:4a7f4ee5-bd06-4f31-80cc-073061e12bc2>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-13T04:56:11.774636	6	95	1580
66	Why do some baby lampreys grow up to be non-parasitic?	Non-parasitism in lampreys is common because while parasitism offers better growth, it also has lower survival rates.	"[""Data from: Parasitism offers large rewards but carries high risks: predicting parasitic strategies under different life history conditions in lampreys\nEvans, Thomas M., SUNY College of Environmental Science and Forestry\nLimburg, Karin E., SUNY College of Environmental Science and Forestry\nPublished Apr 26, 2019 on Dryad.\nCite this dataset\nEvans, Thomas M.; Limburg, Karin E. (2019). Data from: Parasitism offers large rewards but carries high risks: predicting parasitic strategies under different life history conditions in lampreys [Dataset]. Dryad. https://doi.org/10.5061/dryad.352385k\nThe loss of parasitism in metazoan lineages is often seen as unlikely, but it has occurred in some lineages (e.g., leeches, lampreys). How and why parasitism is lost is aptly addressed by studying lampreys, because extant species include a range of feeding modes and parasitism has been lost repeatedly. An individual‐based model was developed to determine if variations in survival and growth rates in the larval and juvenile stages could favor parasitic or non‐parasitic strategies. A realization of the model for a Lampetra spp. population, a genus which includes parasitic and non‐parasitic animals, indicated that both strategies could be successful. A different model realization of the non‐parasitic species Lethenteron appendix also agreed with expectations, only non‐parasitic strategies were successful. Modeling anadromous Petromyzon marinus produced only parasitic animals, as expected, but suggested two different adult sizes should appear in the population, which has not been reported in the literature. Finally, a realization of an Ichthyomyzon castaneus population, only known to produce parasites, rarely selected for parasitism (~7% of model iterations), possibly because the population used to parameterize the model was unusual for the species. The results suggest that non‐parasitism in lampreys is common because parasitism, while offering better growth, also has lower survival. Additionally, non‐parasitic species may be generated at different rates because growth and survival thresholds in the model favoring parasitism are close to observed estimates in some populations. Loss of parasitism can occur when species have different tradeoffs in growth environments between life stages.\nData used to generate life history space figure\nThe data used to generate the figure showing life history space (Figure 4). Additionally, final adult weight is included. These data were generated by the model used within the paper.\nExtant max and min lamprey life histories data\nData used to generate Figure 1, the life history space occupied by lampreys. Values were the minimum and maximum values from Renaud 2011 publication (Lampreys of the world)* for species which were known to be parasitic or non-parasitic. Unusual species, (i.e., dwarf parasitics) were not included in these minimums and maximums to clarify the meaning of the figure, because it is conceptual. *Renaud, Claude. (2011). Lampreys of the World. An Annotated and Illustrated Catalogue of Lamprey Species Known to Date.\nLamprey Life Histories Conceptual Data.csv\nLamprey life history data book\nAll fecundity data extracted from Renaud's 2011 publication (Lampreys of the world): Renaud, Claude. (2011). Lampreys of the World. An Annotated and Illustrated Catalogue of Lamprey Species Known to Date.\nData for histograms_weight and counts\nData used to generate histograms, which were developed by taking the number of observations at an adult body size generated by each model realization.""]"	['<urn:uuid:7d6401d0-d777-4c86-97c7-8827efcdc6bd>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	10	17	519
67	I'm making art: what exactly is dithering used for in pixel art?	Dithering is a technique used to blend and transition between different colors in pixel art. It was originally used in 8-bit games due to color limitations, but is now used to create smooth transitions between shadows and non-shadow areas, making artwork less flat and more visually appealing. For example, it can blend black and white colors together or create subtle shading effects.	"[""Pixel Art Step by Step, Example One\nI decided to create a small pixel art illustration of my nameless\nThis time I wanted to make a road worker with a drill, as a sort of logo for the project.\nI relied heavily on the attributes and the pose of the puppet, because\nfacial recognition is limited.\nI came up with a drill, a road sign (that I didn't use), and a helmet.\nIn this example, you'll find the creation process from start to finish.\nOnce the idea was clear, I started to sketch.\nThe sketch was done quite fast, which, most of the time, is a good indication that a happy result is on the way.\nAs soon as the sketch is to your satisfaction, the conversion to pixel\nart can start.\nIf you sketch on paper by hand, the sketch needs to be scanned. This can easily be done in Adobe Photoshop. Adobe Photoshop has some nice features for adjusting contrast and size.\nOnce the sketch is the right size, it can be copied to MS Paint.\nUsually I keep an untouched version of the sketch as a reference image\n(see image 1); then I start to trace the outline of the sketch (see image 2) with a red\ncolor, in order to visually separate the outline from the sketch.\nAfter the outline is done, I remove the gray pixels so that the outline becomes clear (see image 3). The outline is the outline of the sketch. During the process, you will see this outline being fine-tuned.\nOnce the outline is isolated, I focus on the head and helmet (see image 4). The next step is the removal of the red color of the outline, and some extra adjustments to the head and helmet (see image 5).\nYou can change the color of every individual pixel by hand in MS Paint or Adobe Photoshop; select the red color from the active color palette, then, in the Select menu, choose Similar, and fill the selection with either CTRL + DELETE for background color, or ALT + DELETE for foreground color. Once the red pixels are black, copy and paste the selection back to MS Paint.\nThe head and helmet are adjusted, and the drill is drawn in more detail (see image 6).\nThe elbow on the right is adjusted to give the puppet a little bit more of an active appearance (see image 7). In the previous image the elbow is closer to the body, which is slightly less powerful than the new position.\nThe drill is moved from the body, so the puppet can lean more forward - a more stable position - to keep the drill in place. The outline of the previous image is quite good, so areas of color can be added as well. The right side of the helmet now gets a more detailed shading (see image 8).\nThe right hand of the puppet - left on the image - gets some extra\ndetail. There's an imaginary light on the left topside of the puppet.\nThe light causes a highlight on both gloves, and adds shadow underneath\nthe arm. The left topside of the helmet is adjusted as well. The outline\nis slightly less obvious, due to the reflection of the imaginary light\n(... you need some imagination for these kinds of details).\nBy using an imaginary light, some parts are lit, and other parts cast a shadow. The overall benefit from an imaginary light is contrast, which in general makes an pixel art illustration more vivid and spatial, or less flat.\nAgain the helmet is adjusted. The left side becomes slightly bigger; the contrast on the right side is increased. The arm on the right is drawn in more detail (see image 10).\nAgain the helmet is changed. The curve of the helmet is changed so that\nthe virtual reality goggles match exactly to the helmet (see image 11).\nThe drill and the lower body get some extra details (see image 12). A highlight is added to the safety glass of the helmet, and more details are drawn in the area around the arm (see image 13).\nThe next step is to add some extra details to the upper body and drill (see image 14). The contrast on the left side of the helmet is changed slightly, and the shadow on the front of the virtual reality helmet is dithered. Dithering is used to make the transition between shadow and non-shadow less obvious.\nDithering is explained in more detail in the 'Dithering' chapter.\nThe contrast of the right side of the helmet is increased, by removing the light grey color next to the black outline (see image 15).\nThe pixel art illustration looks quite good at this moment, and now it's\ntime for some extra fine-tuning.\nThe wires of the virtual reality goggles and the data gloves are added (see image 16). The highlight on the safety glass is back again, and the drill gets a little bit more contrast (see image 17).\nThe drill is slightly changed, and shadow is added. The shadow is mainly visible on the right side of the pixel art illustration, and separates the pixel art illustration from the background.\nThe pixel art illustration is made almost completely in MS Paint, except for the\nTo create the shadow, the overall shape of the pixel art illustration is used. The shape is filled with gray and moved two pixels to the right and two pixels to the bottom with respect to the original pixel art illustration. The horizontal shadow at the bottom is removed.\nThe shadow is created in Adobe Photoshop because drawing by hand in MS Paint takes much more time.\nFirst the pixel art illustration is copied from MS Paint and then pasted into Adobe Photoshop (see image 1).\nThe shape of the pixel art illustration needs to be selected, so the background is drawn yellow in MS Paint to separate the background from the pixel art illustration.\nThe 'Magic Wand Tool' in Adobe Photoshop is used to select the background. The tolerance should be zero; 'Anti-alias' and 'Contiguous' should be deactivated to select the yellow color alone.\nThe parts of the pixel art illustration, which we don't need for the creation of\nthe shadow, are selected.\nBy inverting the selection with 'Select' > 'Inverse' (SHIFT + CONTROL +I), the pixel art illustration itself is selected.\nThis selection is cut (CTRL + X) from the background and copied (CTRL + V) in a new layer 'illustration'. The background and the pixel art illustration are separated (see image 1).\nA new layer 'shadow' is added in the layers popup. The layer 'illustration' is selected again. The shortcut CTRL + A is used to select the pixel art illustration. The 'Move Tool' is selected in the toolbox, and Arrow Up is pressed followed by Arrow Down; the pixel art illustration is selected (see image 2). This selection needs to be filled with the shadow color.\nFirst the layer 'shadow' is selected in the Layers popup. Then, to fill the current selection, choose 'Edit' > 'Fill' (SHIFT F5) in the menu or - much faster - press ALT + DELETE (foreground color) or CTRL + DELETE (background color) (see image 3).\nThe layers 'shadow' and 'illustration' have to change position. The\nlayer 'illustration' is moved two positions two the left (use the left\narrow key) and two positions upwards (use the arrow up key) (see image 4).\nThe shadow at the bottom of the pixel art illustration is removed, and all layers are merged (see image 5).\nThe pixel art illustration is finished!"", 'Today I will show you a technique that most pixel artists use called dithering. Using this technique, we are going draw a small tilesheet that can be used in a tile map editor. Before we begin, make sure you have your favorite graphics editor, and a tile map editor installed. Let’s begin!\nDithering was a technique most artists used when creating 8-bit games. Since 8-bit games were limited to rendering 8 colors, this technique was used to transition two different colors. Dithering is best used for shading and highlighting. The image below is an example of dithering using black and white. These are two totally opposite colors, but as you can see, with the dithering technique these two colors blend quite nicely.\nThere are several ways do use dithering techniques, you can experiment with different patterns and see which one blends best. My favorite, however, is the example image I just showed. Keep in mind when working with tilesheets, you want the images to be seamless. That way when you use them in a tile map editor, you don’t see any gaps.\nUnlike the past, our modern computers can render several different colors, so we don’t necessarily have to use this technique. However, people today love the retro-style look, it has become more of a style of art.\nNow that you have an idea of what dithering is, we can get started on our tilesheet. Let’s begin!\nStep 1: Create Outlines\nTo show you how to use the dithering technique, we’ll draw something simple; I’m going to make a painted wooden support pole for an interior level. There will be three parts of this tilesheet: top, middle, and bottom. The middle part of our wooden pole will be seamless. We want to be able to stack the middle part, so we can adjust the size of the pole in our tile map editor. This image is a 96×32 tilesheet, but I have resized it to make it easier to see. Like always, we start with our outline. Now we can move on to step 2\nStep 2: Fill in Main Colors\nIn this case, we will have two main colors: red and brass. For those of you who are curious, I am not a 49ers fan! (Yes, I know their colors are red & gold) I only watch college football. 🙂\nStep 3: Add Shading Color\nPick a light source first. Most of my previous tutorials, the light source was the sun, and it was always coming from the top. In this case, however, it’s an interior object. The light source, in this case, may be a light bulb, a lantern, candles, or any man-made lighting object. In the level, I may want to place the lights between each pole, so the highlights will appear on each side of the pole. Now that we have this figured out, add a little bit of your shading color, but don’t overdo it. Remember, we will be using the dithering technique to blend the colors.\nStep 4: Add Highlights\nAdd you highlights to both sides of the poles, but again don’t overdo this. Since the brass part of our wooden pole is more reflective, we won’t worry too much.\nStep 5: Focus on the Middle Tile\nThe dithering will be done on the red painted part of our wooden pole. To make sure we can stack the middle tile, we will focus first on that. I like to use a select tool, to select that particular tile. To do this in photoshop, grab your select tool and move your cursor to the canvas. Hold down the shift key, click the left mouse button and drag while still holding the shift key. This makes it so you can select a perfect square. Drag it until you have a 32×32 pixel square. If your ruler isn’t set to measure in pixels, go to Edit > Preferences > Units & Rulers. Then in the units section, change rulers to pixels. To make sure you have the selection in the middle tile, drag your 32×32 selection to the top (Not using the move tool). Now drag your selection down. If you’ve noticed, now you can see by pixels, how far you’re dragging the selection. You want to move down by 32 pixels to get the selection to the middle tile.\nStep 6: Begin Dithering\nDither your shading first, then the highlights. Make sure your dithering is in an even patten. That way we can stack our middle tile.\nStep 7 Complete Dithering\nComplete the rest of the image by dithering the top and the bottom part of the pole. If the patterns match evenly, then you’re done.\nI hope you have enjoyed this tutorial, don’t forget to like, comment, share, and subscribe.']"	['<urn:uuid:837494f7-fd62-4209-8d0c-f1bb4ff10b76>', '<urn:uuid:87eee853-1969-4474-94bc-5e84107e4137>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-13T04:56:11.774636	12	62	2067
68	How do Domenica and Orson approach audience engagement in their films?	Both filmmakers deliberately leave room for audience interpretation and engagement. Domenica Cameron-Scorsese tries to avoid telling audiences what to think or feel, instead leaving space for them to connect emotionally and bring their own experiences to Almost Paris. Similarly, Orson Welles created works that were simultaneously tragic, elegiac, lyrical, satirical and surreal, allowing multiple layers of audience interpretation within his distinctive 'Wellesian' style.	"['Directing her first full-length theatrical production, Caryl Churchill’s A Number (2002) whilst she was in postproduction on her feature film debut Almost Paris (2017), saw Domenica Cameron-Scorsese’s film and theatrical journey synchronise. Premiering at the Tribeca Film Festival, where her short films Spanish Boots (2006) and Roots in Water (2010) also played, the festival has become a home to the actress turned storyteller.\nIn the aftermath of the mortgage-lending crisis, banker Max (Wally Marzano-Lesnevic) returns home. Living under the same roof with his parents, sister and her husband, as well as his niece offers him not only an opportunity to reconnect with his family, but discover the harsh economic realities facing ordinary people.\nThis family centred story perhaps echoes the close relationship Domenica shares with her parents, artist, journalist and teacher Julia Cameron and filmmaker Martin Scorsese. She performed in her mother’s play Four Roses (1999) and played opposite Daniel Day-Lewis in her father’s adaptation of Edith Wharton’s novel, The Age of Innocence (1993). On stage she has also worked with celebrated playwright Richard Nelson, playing the role of Dolly in Franny’s Way (2003). It is not not only at this point in her career that the stage and the screen have synchronised, but throughout her creative journey thus far have been two spaces running in parallel.\nIn conversation with Film Frame, Cameron-Scorsese reflected on transitioning from an observer of notable storytellers to a practitioner of the craft onstage and onscreen. She also discussed pursuing a communication with her audience, unexpected responses and the transformation of her creative identity with time.\nFilm Frame: Why a career in filmmaking? Was there an inspirational or defining moment?\nDomenica Cameron-Scorsese: First off, it’s the way that we tell stories. I think it’s the most immediate and visceral form of communication that we have found, and for me it’s a very personal way of talking to each other. I was born into a family of filmmakers; taken from the hospital directly to the set of New York, New York (1977). I grew up where a film set and a theatre certainly felt familiar, and had a kind of family pattern to it. So in that sense it was attractive, but also as I grew up my dad would communicate with me by sending me videos. He knew that I liked horses and so he would send me, oh goodness, The Red Pony (1949), and he knew that I liked animals so it would be something like Old Yeller (1957). And you know, I would be scarred for life because they all died. But little by little that was like a bridge for us, and as I got bigger he would show me movies that were special to him, and he would try to time it at a point where he thought they would be right for me. So there is this doubleness for me about films, both the wonder and magic of being able to create an entire world, but also the ability to say things that are sometimes hard to say, or hard to see. I grew up with that and initially I found acting was my space in that world, and I very much enjoyed it and felt that was mine. In fact, I avoided anybody saying anything about directing because I liked having my own dance space, and little by little I found myself making things.\nIn terms of were there very specific turning points, one was actually getting to work with Daniel Day-Lewis on a scene in my father’s film, The Age of Innocence. I had just turned fifteen at the time, and I had been acting professionally for about four and a half years. We had been rehearsing and working on a double act, and then it was time to do the scene. There was just this brief pause, maybe three seconds, and he turned around and it was like his eyes were a different colour. It was the closest thing to channelling I think I’ve ever seen. He was someone else, we were somewhere else; it was like time travel. It was magic and I thought: I want to be able to do that! And that really pushed me further along into acting, but also by getting to see what a generous director my dad is, and how he creates a staged space so that the best of people and their work comes forward. When you get to do that you ask: “Why would I want to do anything else?” It’s amazing!\nLittle by little I found myself drawn to stories where there wasn’t necessarily a role that I wanted to play, but they were people that I knew and understood, who I felt needed to be heard, and journeys that were profoundly moving for me. And what partly dragged me into directing was that along the acting path, I became more and more invested in the story.\nAnother turning point would be having had the amazing privilege of working with Richard Nelson as a writer-director on one of his plays, Franny’s Way. I saw the meaning that he gave the actors to play in, and the way that he also gave us spaces to fill the world. While it made my role of being a fifteen year old younger sister richer and deeper, what I found was I was drawn towards looking at the bigger picture, and the impact on things. He had encouraged me to direct as well, and so all of those were key moments.\nFilm Frame: Hitchcock is one of the most studied and discussed filmmakers, and yet for any director the process of crafting suspense reveals the gulf between understanding through observation versus doing. From both watching films as well as your observations of your father and Nelson, how has the practical experience of directing a feature film impacted your own appreciation of films and their filmmakers?\nCameron-Scorsese: I am glad you brought up Hitchcock right away because when I went to college, I was not supposed to end up a film major, I was supposed to study classics. The first class I took was taught by Jeanine Basinger who runs the Wesleyan Film Archives, and it was a Hitchcock class. What was fascinating for me, and this is very true because I love movies and I come from a family that loves movies, folks with a crazy knowledge about film, is there is this funny thing between the study and the analysis. And often times I would say the deconstruction of a process to try to understand how it works, versus the approach of trying to build or make something, and learning how it works from the physical viewing of it. I feel like I was in a blessed position to have the opportunity to both formally study film, but also to have had the chance to be on a lot of different film sets, and in different rehearsal processes. In terms of [laughs] having made it to the other side of my first feature, the profound respect I have for folks that have a long hard labour trying to bring their dream project into being.\nStudying Hitchcock, the first things I had to do was a shot list and to talk about the visual storytelling, the screenplay and the use of themes. When is there an editing rhythm that changes or a camera angle pattern, and what does that convey to the viewer? That’s great, but then you try to go and do that, and if you are anywhere near lucky, you get something that’s sort of comprehensible at a first pass. Everybody gets to have a first time and I guess I’m personally a bit of a perfectionist. I think things over and over, and that serves you only so far. Then there’s the part where you have to get in and roll your sleeves up. It is the twentieth day, you’ve lost your location and you really need this moment. You have to stop and ask: “What’s the moment about? Is it about the location or do we just really need to be on the actor’s face right now?” You have that realisation before going into the next scene and I guess it’s fun and scary in terms of creative problem solving.\nIt’s something to see these films and realise they aren’t being made the same way now. There is a gift in having more accessible technology, but there’s also some things that are going away. I think it is important to have a knowledge of working linear so that when you are flying non-linear, you are able to use those tools consciously to tell your story and to reach your viewer.\nFilm Frame: If we look at the history of film, the feel of the medium has changed across the decades. For example, the American gangster film of the 1940s has a different feel to the gangster film of the 1970s. I asked Agnieszka Holland whether this goes beyond technological development to a more aesthetic consideration, and she offered the opinion: “…it is something more mystical – a mystery that is included in the particular film, and which doesn’t age.” Do you perceive there to be a mysticism within cinema that influences its communicative nature?\nCameron-Scorsese: I’d have to say that it effects me on a lot of levels. One is I feel it is so challenging to make a film that you have to have a wonderful and crazy, maybe a little bit of a calling towards it, and a lot of faith just on the sheer mechanics of: Hey, I think this is going to work. There’s a certain amount of faith in action before you even get to the end result of it being something that is going to go and live on, and be impacted by your viewers subjectivity. Though I go with that, I also say that I feel there is a certain amount of, as we would maybe say, ‘movie angels.’ There are things that you just look at and you go: “I don’t know how we are going to pull that off.” Then it happens or some piece falls away and it changes for the better, that shift that needs to happen to tell the story more clearly. There has probably been a lot written about the sense of people coming together in a theatre to have this communal experience, watching a film where there’s a sense of being alone together in a group. What personally drives and connects me to the stories, which then hopefully get to live on a screen, is the opportunity to recognise our humanity. And for it to not always to be the most attractive types, but in the moment to remind people by putting a magnifying glass to some of the aspects. With Almost Paris I certainly felt drawn to trying to create a world with people that I hoped the audience would recognise and may be able to align with, and to have that little bit of a moment: Oh, right, that reminds me of my uncle who could be bad. Oh, maybe that’s what he felt. The audience are a part of the story and I try to hit them on an emotional level, a place that’s not necessarily cerebral, and try to leave room for the audience to feel what they are going to feel, and think what they are going to about this. My hope is if I leave a little bit of room for that, then they will come forward and connect all the more. It’s just a personal thing, but if I watch something and I feel like I’m being told what to think and how to feel, and when to feel it, I start to pull away from what’s being said. So it’s about how do you find that balance in the craft, and also in the intention.\nFilm Frame: Speaking with Carol Morley for The Falling (2014), she explained: “You take it 90 percent of the way, and it is the audience that finishes it. So the audience by bringing themselves: their experiences, opinions and everything else to a film is what completes it.” If the audience are the ones that complete it, does it follow that there is a transfer in ownership?\nCameron-Scorsese: Right now I am in the process of letting go of Almost Paris, enthusiastically, but still letting go of this first feature. We’ve carried this thing that has evolved for over two years and what I find is that in terms of ownership, and if you are looking at the extension of the auteur theory, the more control I have in the process as a filmmaker, ultimately the more I have to let go as it comes out into the world. Now, I would like to think that this creative venture is going to be accepted and welcomed because we ultimately made this film for people to hopefully connect to, and in doing take something away from it with them. So I can see that, but I also feel there are a lot of parts to it where once I’ve stopped, it will be defined by whatever the context is that it’s being released, and how it’s being viewed. If there happens to be a world event or if somebody has recently lost their job, or has had their adult children move back in with them and watches this movie, then in that way there is a shift that will happen. And the earlier part of letting go along the way as a filmmaker has been letting go of what I thought the idea would be, and of then getting to that place. I’ve heard someone say that a movie is made three times, in terms of your screenplay, the production and the edit. This project certainly evolved and grew, and changed throughout, and it was a pretty tremendous opportunity for me to understand how that works. It means when I’m watching it for the fourth time with an audience, I need to not be worrying about the shots that I didn’t get that would have worked so well in moments, but just look at how it lands. To come to a place of accepting that a millennialist is going to come to the film very differently than someone of my parents generation.\nFilm Frame: There is a line in the film about how tough people outlast tough times, which captures the essence of the story. Speaking of the context it plays in, Almost Paris deals with universal and timeless themes such as aspiration, love and companionship, which will allow it to endure, to not age, but connect with any contemporary audience.\nCameron-Scorsese: In terms of getting to watch this movie with different audiences, my hope was that it would be something that would be accessible to folks. But when I first watched it at Tribeca, it was surprising to find how much humour people found with it, and I was grateful for that. I had wanted to tell a story that leaned into the dynamics and connections that people have, and I feel there have certainly been wonderful films made about the recession that were very much about the mechanics and the business of that, and how it transpired. I’ve certainly been witnessing that families, whether it’s a recession or a health crisis, a job loss or a change in plans and circumstances, families really do have to try to pull together. This is something that we will maybe carry on seeing in the film eighteen months after Tribeca, in Jersey City. There was a line in the movie about how there are worse places — you can get sent to Jersey City [laughs]. Everybody laughed, but not quite why we thought, and so the context shifts that way. But I am hoping that the heart of the story will be something that five years from now will not feel dated.\nFilm Frame: Interviewing filmmaker Christoph Behl he remarked to me: “You are evolving, and after the film, you are not the same person as you were before.” Do you perceive there to be a transformative aspect to the creative process?\nCameron-Scorsese: Yes, I do think that there is an energy that a project will carry with it, whether that is the team of people who come together, or the thematic or underlying content of the piece itself. You do live with and work through that, and you learn how to hopefully grow and create under those conditions. And I do think it changes a person. I have been trying to make a feature since my early twenties and when it actually happened, which was turning thirty-nine, I was a different storyteller. I feel like my second feature is going to be different in terms of the pieces both formally, but also holistically that I had to adjust to here, to deal with the process.\nAlmost Paris is available on VOD courtesy of Freestyle Digital Media.\nImages provided courtesy of Prodigy Public Relations.', '|ROGUE GENIUS: An Orson Welles\nSponsored by the Hollywood\nForeign Press Association\nWhere do you begin with Orson Welles, a man with a talent and\nimagination so prodigious that he spanned radio, films, television, books, theater and\nexcelled in them all? From his first film masterpiece CITIZEN KANE - more often than not\ndescribed as one of the best movies ever made - to his checkered career fighting for\nfunding to realize his directorial vision, Welles stands alone, holding a special place in\nthe pantheon of cinematic greats. Welles himself (in F FOR FAKE) made the self-deprecating\nremark, ""I began at the top and have been working my way down ever since,""\nreferring to the popular misconception that his post-KANE career somehow never\ndelivered on his initial promise. In reality, Welles delivered again and again on that\npromise, in such dazzling and unexpected ways that audiences, critics and other filmmakers\nare still trying to catch up. How else can one describe a career that encompasses such\nfilms as THE MAGNIFICENT AMBERSONS, THE LADY FROM SHANGHAI, OTHELLO, TOUCH OF EVIL, THE\nTRIAL, an astonishingly rich legacy of television (including ""The Fountain Of\nYouth""), as well as legendary ""unfinished"" films such as THE OTHER SIDE OF\nTHE WIND and DON QUIXOTE? Although he had to jump through bigger and bigger hoops to\nsecure financing for his movies, dealing with an industry used to mediocrity, somehow he\nmanaged to create and put his art in the public eye for over four decades. A brilliantly\ndramatic actor, always delivering a droll performance with seemingly little effort, he was\na genius director, capable of creating works that were simultaneously tragic, elegiac,\nlyrical, satirical, playfully surreal and pulpy, miraculously managing to integrate all\nthese traits into a style that is immediately recognizable as ""Wellesian."" (Note: Welles CHIMES AT MIDNIGHT is currently unavailable for\nscreening, so unfortunately, we were not able include it in this retrospective.)\nWere enormously pleased to welcome\nStefan Droessler, director of the Munich Filmmuseum, which has amassed the worlds\nlargest collection of rare Welles materials, working closely with the filmmakers\nlong-time companion Oja Kodar. Mr. Droessler will present six different programs,\norganized around various themes, using clips from TV shows directed by Welles, guest\nappearances and cameos in movies and on TV, scenes from his uncompleted projects, and more\nmost never before seen in Los Angeles.\nFriday, February 13 7:30 PM\nCITIZEN KANE, 1941, Warner Bros.,\n119 min. Orson Welles was only 25 when he directed this masterpiece, and it remains\none of the most phenomenal motion pictures ever made. Trailblazing in so many aspects,\nfrom Gregg Tolands complex camera and lighting to Bernard Herrmanns score to\none of the finest ensemble casts (including Welles, Joseph Cotten, Everett Sloane and\nAgnes Moorehead) ever assembled. With an Academy Award-winning script by Welles and\nHerman Mankiewicz. Plus Welles very first short film, ""Hearts Of Age,""\n1934, 4 min.\nSaturday, February 14 5:00 PM\nBrand New 35mm Print!!\nTHE MAGNIFICENT AMBERSONS, 1942,\nWarner Bros., 88 min. Director Orson Welles poetic, tragic adaptation of the\nBooth Tarkington novel, centering on the fall of one wealthy family, with Stanley\nCortezs dynamic camerawork providing a panorama of turn-of-the-century America and\nthe decay of the old aristocracy. Infamously re-edited without Welles involvement,\nAMBERSONS, even its abbreviated form, is still an overwhelmingly rich masterpiece. With\nJoseph Cotten, Tim Holt, Anne Baxter.\nSaturday, February 14 7:30 PM\nBrand New 35mm Print!!\nTHE LADY FROM SHANGHAI, 1948,\nColumbia, 87 min. The camera is the star in one of director Orson Welles most\nphantasmagorical films, a dazzling noir thriller about a seaman, a crippled lawyer and his\nhomicidal wife pursuing each other through a ""bright, guilty world"" of\ninfidelity, deception and murder. The hall of mirrors climax is riveting. With Orson\nWelles, Rita Hayworth and Everett Sloane.\nSunday, February 15 4:00 PM\nOTHELLO, 1952, Castle Hill, 92 min.\nUntil its 1992 restoration, Welles wildly imaginative Shakespearean adaptation\nwas often overlooked, and nearly impossible to see in a decent print. Despite its initial\nbudgetary problems, which caused the shooting schedule to stretch out over three years (it\nwas started in 1949), it stands as one of Welles greatest visual poems. An\nastonishing achievement against nearly overwhelming odds. Starring Welles, Micheál\nMacLiammóir, Suzanne Cloutier.\nSunday, February 15 6:30 PM\nTOUCH OF EVIL, 1958, Universal,\n111 min. Orson Welles hallucinatory, off-kilter masterwork stars Charlton\nHeston in one of his finest roles as a Mexican policeman trapped on the wrong side of\nthe border, where a corpulent, corrupt cop (Welles) tries to stop him from digging into\nthe past. Janet Leigh co-stars as Hestons newlywed wife, menaced by\nleather-clad Mercedes McCambridge and her gang of juvenile delinquents. Co-starring Akim\nTamiroff, Marlene Dietrich, Joseph Calleia. Were screening the restored version,\nreconstructed in 1998 according to Welles original notes. Discussion\nfollowing with restoration producer Rick Schmidlin.\nWednesday, February 18 7:30 PM\nTHE TRIAL, 1962, 118 min. Franz Kafkas\nclassic novel of paranoia and conspiracy seems tailor-made for Orson Welles. A\nlabyrinthine, deliciously satiric, nightmare vision of a man (Anthony Perkins) accused of\nan unspecified crime that emerges as a subtle allegory of Welles own Catch\n22-tribulations working in the film industry. With a dream cast that includes Jeanne\nMoreau, Romy Schneider, Akim Tamiroff and Welles himself.\nORSON WELLES IN ITALY: ROSABELLA, 1993,\nChip Taylor Communications, 60 min. Dirs. Ciro Giorgini, Gianfranco Giagni. A stirring\ndocumentary exploring a twenty year period of Welles life when he lived and worked\nin Italy. There are rare behind-the-scenes film clips from OTHELLO, as well as the\nunfinished DON QUIXOTE and JULIUS CEASAR, plus interviews with various\nfriends and colleagues. (Note: ""Rosebud"" in the Italian version of CITIZEN KANE\nis translated as ""Rosabella"".) In Italian with English subtitles.\nFriday, February 20 7:00 PM\nTHE WELLES RARITIES Program 1: ""Its All Magic""\nOrson Welles had a lifelong obsession with magic in all its\nincarnations, evident in this collection of rare movie and TV appearances. The program\nincludes a clip from Edward Sutherlands FOLLOW THE BOYS, (1944, Universal, 5\nmin.) with Orson Welles and Marlene Dietrich performing his stage magic show; a rarely\nseen episode of the ""I Love Lucy"" show, where Lucy meets Orson Welles\n(1956, CBS TV, 26 min.); restored material from the\nunfinished MAGIC SHOW he was filming from 1976 until the end of his life; and more.\nStefan Droessler of the Munich\nFilm Archive to introduce the screening.\nFriday, February 20 9:30 PM\nTHE WELLES RARITIES Program 2: ""People\nAll his life Orson Welles liked to travel,\nand in his films and television work he very often portrayed people and places in Europe\nfor British or American audiences. In this program we see Welles visiting Paris, Italy and\nLondon, finding outsiders, film stars and typical Britains, who were interviewed (as well\nas occasionally impersonated!) by Welles himself. The episodes of ORSON WELLES\nLONDON (1968-71) are among the funniest comedy pieces he ever did. Also featuring: AROUND\nTHE WORLD WITH ORSON WELLES: ""Paris After Dark"" (Saint\nGermain-Des-Pres),"" (1955, Image Entertainment/Euro London Films, 28 min. Dir.\nOrson Welles); and more. Introduction\nto screenings by Stefan Droessler of the Munich Film Archive. Discussion following with\nactor Jonathan Lynn (""Orson Welles London"").\nSaturday, February 21 5:00 PM\nTHE WELLES RARITIES Program 3: ""Obediently\nYours: The Storyteller""\nOne of Orson Welles favorite roles\nwas narrator, either on-or-off-camera -- it didnt matter. In all the mediums and\nfilm forms, he loved to play this part: as invisible director in the CITIZEN\nKANE trailer (1941), interfering with the action as host in his legendary\ntelevision project ""THE FOUNTAIN OF YOUTH"" (1956, Desilu\nProductions/Viacom, 25 min.), or as pure storyteller in front of a camera in\nTHE GOLDEN HONEYMOON (1971). Even his never-finished project DON\nQUIXOTE (1956- 71) is based on his narration. Stefan Droessler of the Munich Film Archive to introduce the screening.\nSaturday, February 21 8:30 PM\nTHE WELLES RARITIES Program 4: ""Stage and Theater""\nBefore and after gaining worldwide critical renown for his\nmovies, Orson Welles excelled as a respected actor and director in theater. This program\nshows him on stage in Dublin, remembering the early days and reading from his theater play\nMOBY DICK REHEARSED. In 1971, at last he tried to film his play playing all\nthe parts by himself. As unknown as this film is his adaptation of Shakespeares\nTHE MERCHANT OF VENICE, which survives only as a fragment. Introduction to screenings by Stefan Droessler of\nthe Munich Film Archive.\nSunday, February 22 4:00 PM\nTHE WELLES RARITIES Program 5: ""Unfinished Works"" Orson Welles had more unfinished projects than any other major film director,\nsomething that has added immeasurably to his myth. This program gives a glimpse at some of\nthe incomplete works behind the legend. You will see scenes and sequences of THE DEEP\n(1967 69), THE OTHER SIDE OF THE WIND (1972) and THE DREAMERS (1978 85).\nIntroduction by Stefan Droessler\nof the Munich Film Archive.\nSunday, February 22 6:30 PM\nTHE WELLES RARITIES Program 6: ""Looking\nThe cinematic aesthetic behind Orson Welles creations, his\nfilmmaking views and methods are explored by the master himself.\nIn a British television series Welles tells\nthe story of his ""War Of The Worlds"" radio broadcast, and at USC he discusses\nwith the audience his film THE TRIAL. Plus, ""The Dominici Affair,"" (1999,\nLa Huit Prod., 52 min.) In 1955, Welles produced and directed a short series called\n""Around The World With Orson Welles"" for British TV, but one episode,\n""The Tragedy Of Lurs,"" was never completed. The subject was the murders of\nan English family camping in the French countryside and the trial of a local farmer for\nthe killings. Director Christophe Cognet chronicles the making of ""The Tragedy\nOf Lurs,"" and includes a complete restoration of all available material from the\nepisode directed by Welles. Introduction to screenings by\nStefan Droessler of the Munich Film Archive and FILMING THE TRIAL cinematographer Gary\nTuesday, February 24 7:30 PM\nBrand New 35mm Print!!\nF FOR FAKE, 1974, Janus/Criterion, 85\nmin. Orson Welles appears as ""himself"" (but which self? Master director?\nMagician? Media manipulator?) in this delightful essay on the nature of illusion, focusing\non all types of fakery and fakers, including notorious art forger Elmyr de Hory and\nfraudulent Howard Hughes-biographer, Clifford Irving. With Oja Kodar, Peter Bogdanovich. Discussion following with cinematographer Gary Graver.\nWednesday, March 10 7:30 PM\nNew 35 mm Print!\nTHE STRANGER, 1946, MGM/UA, 95 min. Director Orson\nWelles suspenseful study of an escaped Nazi war criminal (played by Welles\nhimself) living in a small Connecticut town, who is pursued by a Federal agent (Edward\nG. Robinson) to a no-holds-barred climax. Loretta Young gives one of her finest\nscreen performances as Welles unsuspecting wife. Ironically, this was Welles\nmost successful film at the box office.\nITS ALL TRUE: Based On An\nUnfinished Film By Orson Welles, filmed 1942; reconstructed 1993, Paramount, 87 min.\nDirs. Richard Wilson, Myron Meisel & Bill Krohn. A fascinating reconstruction of an\nincomplete 3-part film Orson Welles made for RKO at the request of Nelson\nRockefeller and the State Department to promote relations with Latin America during WWII.\nIncluded is material from all three sections (in various states of completion): ""My\nFriend, Bonito,"" ""Four Men and a Raft"" and ""The Story of\nthe Samba."" The reconstruction also features interviews with Welles and various\nBrazilians who worked on the project. Introduction to\nITS ALL TRUE by co-directors Myron Meisel and Bill Krohn, editor Ed Marx,\npost-production coordinator Tony Bozanich, and Cinematheque Board of Trustees member\nMichael Schlesinger, who was instrumental in restoring the film.\nWednesday, March 17 7:30 PM\nMACBETH, 1948, Paramount, 119\nmin. Dir. Orson Welles. Were very pleased to present this painstakingly restored (to\nits original form) version of MACBETH, led by the UCLA Film & TV Archives preservation\nofficer Robert Gitt. The film had been cut by 21 minutes, re-recorded to\n""Americanize"" the dialogue, and then rarely shown. Gitt tracked down the missing\nfootage and original, Scottish-accented soundtrack, plus the Jacques Ibert overture and\nexit music. Critic Stanley Kauffman wrote about the restoration: ""Whatever the\ndetails of Gitts job, Welles MACBETH is now a bold, exciting, innovative\nfilm."" The innovations cannot be overstated. Longtime Welles collaborator Richard\nWilson considered MACBETH ""the greatest experimental American film ever made under\nthe Hollywood studio system,"" and the restored footage includes a reel-long take. The\nstudio was driven mad by the many retakes the ten-minute sequence required. Eight parts\nWelles to two parts Shakespeare, MACBETH was shot around Salt Lake City and features\nlow-budget grandiosity, plus Welles in an intense, towering performance as the tormented\nScots king, ""one of the best elements of the film, thrilling and a bit poignant\nIn every one of the big moments, Welles rises to the heroic."" (Kauffman)\n(Program notes courtesy James Quandt/Cinematheque Ontario.) Discussion following with actress Peggy Webber.\nWednesday, March 24 7:30 PM\n""THE UNSEEN WELLES""\nWere very pleased to welcome Gary and Jillian Graver,\ndirectors and curators of the Orson Welles Archives here in Los Angeles, to present an\neye-opening program of additional Welles rarities and curiosities. A close collaborator of\nWelles since 1970, Gary Graver worked on many of the masters films, including THE\nOTHER SIDE OF THE WIND, FILMING OTHELLO and F FOR FAKE. (He was also cinematographer on\ndozens of other films, including a number of exploitation classics.) Renowned for their\ninformed, personal and fascinating presentations on all aspects of Welles career,\nthe Gravers have appeared at festivals, cinematheques and film institutes around the world\nwith similar programs. No Wellesian can afford to miss the chance, not only to see such\nrare footage as Welles Japanese whiskey commercials, and a unique reconstruction of\nthe ""Four Men On A Raft"" sequence from ITS ALL TRUE, but also to share an\nevening with the man who worked most closely with Welles for the last 15 years of his\nlife. (Program notes courtesy James Quandt/Cinematheque Ontario.) Program approx. 90\nminutes. Special Offer: Current American Cinematheque members will\nbe admitted free of charge to this screening.\nWednesday, March 31 7:15 PM\nNew 35 mm. Print! CONFIDENTIAL REPORT (aka MR. ARKADIN), 1955,\nJanus/Criterion, 99 min. Dir. Orson Welles. ""One of Welles most inventive and\nresonant films"" (J. Hoberman, Village Voice), CONFIDENTIAL REPORT\nretains the investigative structure of CITIZEN KANE the life of a rich and powerful\nman is recounted by several people who knew him but replaces KANEs tragic\nromanticism with a sordidness that is so far over the top as not to be believed. A\nruthless financier (Welles) hires a sleazy young cigarette smuggler to write a\n""confidential report"" on his past, hoping to erase the last traces of his infamy\nso that his beloved daughter will never find the truth about him. Welles decks out this\nmock-tragic ""chronicle of a death foretold"" with down n dirty rococo\neffects and tall tales, including that epitome of cynicism, the fable about the frog and\nthe scorpion. There are several distinct versions of the film we will be screening\na new print of the CONFIDENTIAL REPORT version. (Program notes courtesy James\nTHE BIG BRASS RING, 1999, Nu\nImage, 104 min. Dir. George Hickenlooper. In THE BIG BRASS RING, the screenplay Orson\nWelles co-wrote with Oja Kodar in 1982, Welles was meditating with prophetic\nenergy on American fate and the nature of power by telling the story of political\ncandidate Blake Pellarin, who is haunted by his past and settling scores with his\nenigmatic mentor, Kim Mennaker. Many years later, the previously unproduced script was\nadapted by director George Hickenlooper (THE MAN FROM ELYSIAN FIELDS, THE MAYOR OF\nSUNSET STRIP) and screenwriter F.X. Feeney, with a stellar cast including William\nHurt, Nigel Hawthorne, Miranda Richardson and Irene Jacob. ""THE BIG\nBRASS RING struck me as an opportunity to both pay a debt to an artist I revere and yet\ndramatize something deeply related to my own experience. It was clear that I wasnt\nout to make a lost Orson Welles film. What I wanted was to take this\nunfinished work and make something new and beautiful out of it."" George\nHickenlooper. (Program notes courtesy Stefan Droessler/Munich Filmmuseum.)\nIntroduction to THE BIG BRASS RING by director\nGeorge Hickenlooper and screenwriter F.X. Feeney.']"	['<urn:uuid:a55595e9-0ae7-474e-8fe2-04e53ffbcad7>', '<urn:uuid:dd5d4a44-5278-4758-b6b2-f719f64c1c37>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T04:56:11.774636	11	63	5544
69	What are the signs of a malfunctioning car thermostat?	A malfunctioning thermostat can be identified when it sticks closed, preventing coolant flow to the radiator. The thermostat should normally open to allow antifreeze/coolant flow when the engine reaches about 198F - 99C. To check for this condition after the engine has cooled to a safe temperature, inspect both upper and lower hoses. If the top hose is warm and the bottom hose is cool, this indicates the thermostat has stuck and needs testing or replacement.	"['Temperature Warning Light On\nLight On for Engine Coolant Temperature Hot All warning lights are meant to alert you that a problem in one of the operation systems in your car has or is malfunctioning. The engine temperature warning light is conveniently placed in the instrument cluster display panel. Warning lights generally come in two colors, orange and red. Red is reserved for the more serious malfunctions that need attention immediately while orange is meant to be serviced at your earliest convenience. When a particular system fails a warning light device has been designed into most car systems such as brakes, engine, transmission, seat belts, air bag, vital fluid levels and more. All engine temperature warning lights are colored red because if this light illuminates the engine is overheating. If left unattended the engine will suffer irreversible damage. When the engine overheats the engine pistons expand causing the cylinder wall to become scored. When cylinder wall scoring occurs the piston rings cannot seal to the wall allowing combustion bypass when the engine cools. This is why after an engine has been overheated it can have low power and cause the engine to burn oil. This condition is called blow-by and cannot be repaired unless the engine is rebuilt or replaced. If you think this condition has occurred to a car an engine compression test is needed. If this test shows that the compression is below the normal range you can suspect engine overheating has occurred. In sever engine overheating conditions the internal engine parts can swell forcing the engine to stop turning or seize. If the engine seizes due to overheating the internal parts of the engine are damaged and replacement or disassemble for repair is needed. A common cause for engine overheating is lack of coolant in the engine cooling system. When lack of coolant is present the heat transferring properties is lost. This absence of heat transfer is what causes the engine overheating. A car radiator cannot work if there is no coolant inside the system. The coolant level should be the first check after an engine has overheated. Never check engine coolant until the engine has cooled completely, preferably after the engine has cooled over night. Another popular reason for engine overheating is the thermostat can stick closed not allowing the coolant to flow to the radiator. The engine thermostat is designed to open allowing antifreeze/coolant to flow to the radiator when the thermostat inside the engine reaches about 198F - 99C. Even if the coolant is at the proper level this condition can occur. Only check for this condition after the engine had cooled down a safe temperature. Then check the temperature of both the upper and lower hoses. If the top hose is warm and the bottom hose is cool you can suspect the thermostat has stuck and needs to be tested or replaced. If your car is equipped with radiator cooling fans it is vital these fans work properly. If the cooling fan fail to operate the heat from the radiator will not be removed from the engine coolant causing the engine to overheat. If the engine is overheating the cooling fans should be on in full force. If the cooling fans have not come on the electrical system that power the fans must be checked. If an engine overheating condition has occurred one of the most popular causes for this condition is cooling fan motor failure. These fan or fans use a high amperage draw and are prone to failure. Other popular reasons for failure is a fuse or relay operation failure. Use a test light or voltmeter to test these circuits and replace burnt or shorted components with new. In most cases engine overheating is obvious and can be detected by additional engine noise, poor running or low power even before the engine temperature warning light is illuminated. If you car is equipped with both a temperature warning light and a engine temperature gauge it is a good idea to periodically inspect the gauge while driving. When cars are designed with a temperature gauge along with a warning light it is a ""redundant"" system and is used as a failsafe. In this case the warning light works in correlation with the gauge and will only illuminate when the gauge is in the upper range of the gauge span. Never depend on a warning light to illuminate when the temperature gauge is in the ""hot"" range. If the temperature gauge is in the ""hot"" range shut the engine off in a safe place on the highway. Allow the engine to cool before inspection. Sometimes the temperature gauge can go to the hot range and then back down to the cool range if this condition happens check the electrical contact at the temperature gauge sender electrical connection. If this contact is loose it will cause intermittent high resistance allowing the temperature gauge circuit to connect and then go to high resistance when the engine it running due to engine vibration. When the connections goes to high resistance the temperature gauge will go to the ""hot"" position ever thought the engine is not hot. Always make sure the connection is tight and making good contact with no corrosion. Even if the connection is tight corrosion can cause high resistance when the engine is running. If the cooling system radiator is plugged it can cause the engine to overheat. A radiator can become plugged by corrosion or debris in the cooling system. If either of these conditions occur the radiator cannot pass coolant through the cooling tube which are connected to the cooling fin of the radiator. In turn the engine coolant will be hot as it re-enters the engine block and cylinder heads causing the engine to overheat. To check for this condition (after the engine has cooled overnight) remove the radiator cap or upper radiator hose. Then with a flashlight shine the beam into the radiator and down or over to the cooling tube openings. If the opening look clogged the radiator must be replaced or serviced (boiled out) by a radiator repair shop. Some of the overly un-popular causes for engine overheating are internal engine gasket failures such as a head or intake manifold gasket failure. This condition is accompanied by low coolant level in most cases. In extreme cases the engine block or cylinder head can develop a crack due to manufacturing flaws. This condition will present itself in much the same way as a failed internal engine gasket but the remedy is much most expensive. Usually when this condition occurs the engine block or cylinder head needs replacement. Anytime your car engine fails you must consider he engine mileage as it might be more beneficial to replace the engine as a unit opposed to repairing an high mileage engine. Also if the car has seen better days it might be time to purchase a different car. This decision can be difficult especially if the car is still in good condition, just old. Do not attempt to open the engine cooling system at anytime unless the engine has cooled overnight. The engine cooling system is contained under pressure, usually about 18 psi. This coolant is hot about 230F or 102C and it will burn you or anyone standing with range of coolant projecting from the engine compartment. Sometimes a car hood can be hot from a result of an overheating engine. Before opening the hood touch the hood quickly to measure the hood temperature before opening. Never open a overheated hood, allow a cooling period, preferably over night. Common causes for the engine temperature light to be illuminated\n- Low Coolant Level - When the temperature light comes on the first thing that should be checked is the coolant level. If this level becomes low due to a leak the engine can over heat\n- Failed Coolant Fan Operation - The cooling fan must be operational or the engine will overheat causing the engine temperature light to come on.\n- Plugged Radiator - The car radiator must be operational, free form corrosion with clean cooling fins\n- Failed or Stuck Thermostat - If the engine thermostat fails to open it will not allow the coolant/antifreeze to flow from the engine to the radiator']"	['<urn:uuid:6d6eccbe-630c-4594-8711-670b693abfb4>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	9	76	1381
70	max minutes f16 full power flight	When operating at maximum capacity with external fuel stores, the U.S.-variant F-16 can sustain full afterburner performance for approximately 20 minutes before being forced to rely on emergency reserves, which would only provide an extra minute or so of full afterburner capability.	['An afterburner, a distinctive feature of certain engines, is renowned for it’s remarkable inefficiency, as it voraciously devours fuel at a staggering rate of up to three times more than conventional engines. Consequently, pilots conscientiously restrict it’s usage during missions, usually reserving it for mere moments. Although the mechanism behind an afterburner may appear uncomplicated, it’s functionality relies on an intricate balance of delicately calibrated tolerances, demanding meticulous precision and skill.\nHow Long Can Fighter Jets Use Afterburners?\nFighter jets equipped with afterburners have the ability to tap into an immense power source, but it comes at a cost. The afterburner, a key feature of these engines, is notorious for it’s inefficiency. When engaged, it consumes fuel at an accelerated rate, up to three times higher than regular engine operation. As a result, pilots are careful to employ this feature sparingly, usually restricting it’s use to a matter of minutes during a mission.\nThe concept underlying an afterburner may be relatively straightforward, but it’s operation is anything but. The afterburner system introduces additional fuel into the exhaust stream, which then ignites, resulting in a sudden surge of power. This process demands intricate engineering and meticulous maintenance to ensure it’s reliability and accuracy. Even slight deviations in the afterburners delicate tolerances can cause a cascade of issues, potentially compromising the engines overall performance.\nFurthermore, prolonging afterburner usage can impose excessive strain on various components, such as turbine blades and exhaust nozzles. These parts are designed to withstand the intense forces generated under normal operating conditions. However, when subjected to prolonged afterburner use, they may succumb to the increased heat and stresses, leading to potentially catastrophic failures.\nThey reserve it’s deployment for critical moments or situations, where maximum thrust is necessary to achieve specific objectives. By doing so, they strike a balance between harnessing the incredible power of the afterburner and conserving precious fuel.\nMoving on to the next topic, it’s fascinating to explore the endurance of the F-16 in full afterburner. At low altitudes, the F-16 is capable of burning an astonishing amount of fuel, exceeding 64,000 pounds per hour. When operating at maximum capacity with external fuel stores, the U.S.-variant F-16 can sustain this level of performance for approximately 20 minutes before being forced to rely on emergency reserves, which would only provide an extra minute or so of full afterburner capability.\nHow Long Can F-16 Use Afterburner?\nThe F-16, renowned for it’s exceptional performance and versatility, is no stranger to the use of afterburners. These powerful engines enable the aircraft to achieve impressive speeds and climb rates, making it a force to be reckoned with in the skies. However, the duration for which an F-16 can utilize afterburners isn’t indefinite.\nWith maximum external fuel stores, a U.S.-variant F-16 can sustain full throttle operations for roughly 20 minutes. Subsequently, the aircraft would rely solely on emergency reserves, which would provide an additional minute or so under such intense conditions. This limited timeline highlights the need for prudent fuel management during missions requiring extensive afterburner usage.\nIt’s worth noting that the duration of afterburner usage can also be impacted by altitude. Conversely, at low altitudes, where the atmosphere is denser, the afterburners consume fuel at a faster rate, limiting the duration they can be utilized.\nStrategies for Fuel Management During Missions Requiring Extensive Afterburner Usage\n- Optimize fuel consumption\n- Monitor fuel levels constantly\n- Use afterburner sparingly\n- Implement precise throttle control\n- Consider pre-flight weight reduction\n- Incorporate efficient flight planning\n- Perform regular engine maintenance\n- Utilize air-to-air refueling if available\n- Employ tactics to minimize time spent in afterburner\n- Train pilots on fuel-efficient flying techniques\nWatch this video on YouTube:\nThe Boeing F/A-18E and F/A-18F Super Hornet, derived from the McDonnell Douglas F/A-18 Hornet series, are renowned carrier-capable, multirole fighter aircraft. The F/A-18E variant, designed as a single-seater, and the F/A-18F variant, built as a tandem-seater, are upgraded versions of their predecessors. These impressive aircraft can internally store up to 14,400 lbs of fuel. In terms of fuel consumption, the powerful GE F414 engine can burn around 36,000 lbs/hr of fuel in full afterburner mode. Therefore, it’s typical for an F/A-18E/F Super Hornet to have a total fuel burn of approximately 72,000 lbs/hr in such conditions.\nHow Much Fuel Does a F18 Burn Afterburner?\nThe F/A-18E Super Hornet, equipped with the GE F414 engine, is a formidable aircraft in terms of fuel consumption. It possesses a substantial fuel capacity, with 14,400 lbs of fuel stored internally. However, when engaging the afterburner, the fuel consumption reaches another level entirely.\nThe afterburner is a significant feature of the F/A-18E, as it provides an immense boost of propulsion. With the afterburner activated, the GE F414 engine burns approximately 36,000 lbs/hr of fuel. This extensive fuel burn showcases the immense power output of the afterburner and it’s impact on the aircrafts overall fuel consumption.\nHowever, it’s important to note that fuel consumption may vary depending on factors such as mission requirements, aircraft configurations, and flight parameters.\nWhen the afterburner is engaged, the engine burns approximately 36,000 lbs/hr of fuel, resulting in a typical total fuel burn of 72,000 lb/hr under maximum power output conditions. However, it’s crucial to consider various factors that can affect fuel consumption when assessing the overall efficiency of this advanced multirole fighter aircraft.\nIn conclusion, the utilization of an afterburner in an engine comes at a significant cost in terms of fuel consumption. Consequently, pilots are cautious and restrict it’s use to only a few minutes during a mission to mitigate the fuel consumption impact. Balancing the benefits of enhanced thrust with the inherent cost of increased fuel consumption remains a crucial consideration in the design and operation of afterburning engines.']	['<urn:uuid:5e11055f-6713-4483-a2eb-970fbb674bda>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	6	42	955
71	daily recommended amount vitamin d units	You should get 800 to 1,000 units of vitamin D per day.	['Since the beginning of the coronavirus disease 2019 (COVID-19) pandemic, people have turned to vitamins and other supplements to boost their immune systems in hopes of fighting off COVID-19. Vitamin D, in particular, has received attention as a potential safeguard against infection.\nIn recent studies that examined the effect of vitamin D on COVID-19, one found that people who had a vitamin D deficiency were more likely to test positive for the virus that causes COVID-19 than those who had normal levels of vitamin D. Another study found high rates of vitamin D deficiency in people with COVID-19 who experienced acute respiratory failure.\nHowever, at this point, there is not enough data to recommend the use of vitamin D to prevent or treat the virus, according to the National Institutes of Health.\n“Researchers did some small observational studies that showed an inverse association between vitamin D levels and COVID-19, meaning the lower the vitamin D levels, the higher the COVID-19 cases,” says UNC Health family medicine physician Sarah Ruff, MD. “But common risk factors for both COVID-19 and vitamin D deficiency are age and obesity.”\nIn other words, vitamin D deficiency is more common in people who are older and people who have a body mass index of 30 or higher (obesity), and these factors also increase the risk of severe COVID-19 symptoms.\n“There’s no clear evidence that vitamin D supplementation would decrease your risk or the severity of COVID-19,” Dr. Ruff says. “That being said, regardless of COVID-19, everybody should make sure that they’re getting enough vitamin D.”\nThat’s because vitamin D supports the immune system by fighting off harmful bacteria and viruses in addition to helping with bone health. Vitamin D also:\n- Helps regulate how much calcium is in your bloodstream, which helps with heart function, among other roles\n- Corrects mineral imbalances to keep your kidneys healthy\n- Improves muscle function\n- Keeps nerves and nerve receptors healthy to encourage good brain-body communication\n- Reduces inflammation\nDr. Ruff says you should try to get 800 to 1,000 units of vitamin D per day. So how do you do that? She offers three tips for boosting vitamin D:\n1. Spend some time in the sun every day.\nThe most natural way to get vitamin D is with sunlight. Brief exposure to the sun without sunscreen—15 minutes daily—is the most efficient way to spur the production of vitamin D in the body.\n“Vitamin D is actually produced in our bodies,” Dr. Ruff says. “The sun’s energy turns a chemical in your skin into vitamin D, and that goes into the liver and then the kidneys, where it becomes active and your body can actually use it.”\nAlthough sun exposure increases your risk of skin cancer, Dr. Ruff says the recommended time per day needed to get adequate vitamin D is probably not enough to cause skin cancer.\n“Now that we’re all very worried about skin cancer, we don’t get the sun like we did in the olden days, and in order to get the vitamin D activation from the sun, you have to have bare arms, bare face and no sunscreen for 15 minutes a day,” Dr. Ruff says.\n2. Eat lots of eggs and fish.\nIf spending time in the sun is not an option, you can try to get vitamin D through your diet, though it’s nearly impossible to get an adequate amount through diet alone. Egg yolks and some types of fish contain vitamin D. Salmon is the best choice, Dr. Ruff says.\n“If you just wanted to get it naturally from fish and egg yolks, you have to eat a ton,” Dr. Ruff says. “One egg yolk is only 20 units. To get 400 units from fish, you’d have to have a 5-ounce salmon, 7 ounces of halibut, 30 ounces of cod or two 8-ounce cans of tuna every day.” That’s obviously not feasible.\nAlthough there are other ways to get vitamin D through our food, such as fortification in milk and cereals, each serving is typically about 100 units. So even if you get 100 to 400 units a day from the foods that you eat, that’s not enough.\n3. Take a vitamin D supplement.\n“Most patients should take a vitamin D supplement, especially in the winter,” Dr. Ruff says.\nDr. Ruff recommends vitamin D3, because that’s the kind the human body naturally makes. Vitamin D3 is animal-derived, whereas vitamin D2 is from plants. Both can help increase vitamin D levels in your body.\nAlso, don’t take more than the recommended amount.\n“It is a fat-soluble vitamin, which means if you take too much of it, you can’t urinate it out like a water-soluble vitamin, so it can build up in your fat in your body if you have too much,” Dr. Ruff says. “And it can cause toxicity (meaning it can make you very sick).”\nThink you may have a vitamin D deficiency? Talk to your doctor or find one near you.\nPhoto credit: ©Tashi-Delek – gettyimages.com']	['<urn:uuid:d003bb49-7f86-48b9-8463-efd9f0bc89ab>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	6	12	835
72	What makes the upper pond's location architecturally unique?	The pond's placement is unusual because garden ponds are typically located at lower levels of a site, but this one was intentionally placed at the highest level. This unique positioning allows the pond's surface to act like a large mirror that joins the ground and sky, creating an effect where paths appear to float between the real sky and its reflection in the water.	['Between the Earth & Sky\nPoints of View\n(1)View restricted by trees\n(2)Big pond seen from the villa on the hill\n(3)The connection between pond and sky as seen from the shore of the pond\nThis villa sits in spacious grounds at the foot of a mountain. The large site consists of three separate villas with gardens, referred to as the lower, middle and upper villas for their locations at different elevations on a hillside of terraced rice fields. They are connected by paths along which pine trees are neatly planted. The villa’s highest spot features views that change constantly as you move along, as seen in the case of Kiyomizu-dera Temple. Controlling the view is the most important factor here, and the dramatic scenes of the upper villa were designed with careful attention to this.\nFirst, you will visit the lower villa. This is centered around a pond, and brooks, trees and stones are arranged to create a garden on an immense scale. From the building facing the garden, you can see a whole overview of these neatly arranged natural elements and the very pleasant contrast between the moss and the white gravel. After viewing this garden and opening the door in the bamboo fence to exit the first villa, the vista opens up wide. Pine trees line the path ahead over the gravel plain, and the mountain looms over the site. The villa was originally built and surrounded with a fence as if to intentionally produce this dramatic change of vista.\nProgressing along the path of pine trees, you will reach the middle villa. The garden of this villa is basically the same as that of the lower one, and was also designed around a pond and brooks. The interior of the building is somewhat richer than that of the lower one. Incidentally, there are a number of well-trimmed hedges planted around the gate of the middle villa to restrict the view. These are taller than humans, and close inspection reveals different kinds of leaves in the tangled mass of hedge. Having these different kinds of leaves on the surface prevents seeing between leaves, making the facade of the shrubs look richer.\nLeaving the second villa, you head towards the upper villa on the hill at the highest location of the whole site. For a while, you will go up a gentle slope enclosed by pine trees, and will then see another bamboo fence near the gate to the upper villa. The villa lies inside this fence. Passing through the gate in the fence, you progress up a flight of narrow stairs. Low shrubs are planted on both sides of these stairs in a way that drastically limits the view. While climbing the stairs, you cannot see anything except the stairs themselves, the shrubs and the sky. As you move upward, the height of the shrubs gradually decreases and the view begins to open up. On reaching the top of the stairs, there is a building directly in front and a wonderful panoramic vista to the left. On the top of the hill, you look down on a big pond and the mountain and sky over it. Reaching this spot is clearly the highlight that the previous walk was building up to. The planner of the garden may have wanted to let people see only this panoramic view, and may have made the other garden just to emphasize this scene.\nThe upper building is located at the highest part of the site, and the big pond is also located at one of the highest locations. After going down the stairs, you walk along the shore of the pond and reach a spot with another impressive view. In this spot, the pond and the sky spread out in front of you, and between the pond and the sky are low shrubs, planted trees and paths. The sky and the vegetation are reflected in the gigantic mirror that is the surface of pond, so the path appears caught between the real sky and the reflected sky. Seen from the other side of the shore, it seems as if the path is floating in the sky.\nThe designer of the villa took great advantage of site’s characteristic hillside view in a surprising way. Although garden ponds are generally located on the lower level of a site, in this case it was placed at the highest level so that the surface of the pond acts like a big mirror joining the ground and the sky. It appears the designer must have had a deep love for the sky.']	['<urn:uuid:ec07aa01-cce7-4477-81d9-49f98376e1fb>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	8	64	766
73	photographic grid types comparison	Historically, photographic grids have been used in three main ways: to arrange serial or time-based imagery in logical sequences, to group similar pictures for comparison in typologies, and to combine fragments into a larger mosaic-like whole. However, the photographic grids discussed here don't follow these conventional forms - they are more like streams of consciousness, with loose associations, memories, and visual allusions that are experienced simultaneously rather than linearly.	['JTF (just the facts): A total of 13 photographic works and 9 other artworks, variously framed and matted, and hung against white walls in the main gallery space, the smaller side room, and the entry area.\nThe recent photographic works in the show include:\n- 1 set of 20 archival inkjet prints, 2017, sized roughly 36×57 inches overall\n- 1 set of 30 archival inkjet prints, 2015, sized roughly 45×69 inches overall\n- 2 sets of 40 archival inkjet photographs, 2008-2011, 2012, sized 48×94 inches (overall) each\n- 1 set of 88 archival inkjet prints, 2016, sized roughly 72×126 inches overall\n- 1 set of 88 archival inkjet prints, with table containing horned toad, seeds, wax top, and containers, 2009-2010, sized roughly 68×116 inches overall\n- 1 set of 130 archival inkjet prints, with table containing wood, beeswax, and bones of various animals, 2018, sized roughly 90×149 inches overall\n- 6 archival inkjet photographs, 2016, 2017, sized 13×19 inches each\nAdditional earlier works on view include:\n- 1 sunflower seeds from Amagansett on rice paper, 1993, sized 18×19 inches\n- 1 earth on muslin-mounted paper, handmade woven cord, owl feather from the Four Corners area, 1977, sized roughly 14x10x3 inches\n- 1 seeds from Rome and graphite on rice paper, 1995, sized roughly 32×32 inches\n- 1 graphite and earth on muslin-mounted rag paper, 1979, sized 95×61 inches\n- 1 seeds from Rome, ink, and watercolor on Chinese paper, 1995, sized roughly 15×16 inches\n- 1 graphite on paper, c1970, sized roughly 24×47 inches\n- 1 graphite on paper, 1969, sized roughly 15×19 inches\n- 1 graphite on Rives paper, 1969, sized 22×30 inches\n- 1 graphite from site in Guatemala on paper, 1978, sized 8×7 inches\n(Installation shots below.)\nComments/Context: Across the history of the medium, the photographic grid has been employed in a myriad of ways. It has been used to arrange and organize serial or time-based imagery, creating a logical progression from one image to the next in an extended sequence. It has been leveraged to group like pictures, either from a single body of work or in what came to be known as a typology, a specific grid form that allowed for rigorous comparison between images in the set. And it has been utilized to gather pieces into a larger whole, where smaller fragments come together to form a single aggregate image, like a mosaic.\nBut the photographic grids that Michelle Stuart has been making for the better part of past decade don’t really adhere to any of these typical rules or forms. Most feel something like a stream of consciousness, where loose associations, memories, visual allusions, and refrains are brought together in ways that are neither linear nor formulaic. While they are made up of individual frames, Stuart’s photographic works are effectively experienced all at once, like a montage, our eyes constantly building connections between discrete items before frenetically jumping on to the next, the whole becoming like a swirling wave of interlocked visual stimuli.\nThat Stuart should evolve toward this unexpected artistic form is not altogether unexpected. A sampler of her earlier work, going all the way back to the late 1960s, shows that she has been grappling with systems of display and organization from the very beginning. Her early drawings find her breaking up images of the cratered surface of the moon into tiles and iterations, and her interest in the texture and resonance of materials goes back to dented graphite works that look like rubbed stone. Decades later, Stuart comes back to the grid, arranging oily (and sometimes sprouting) seeds from various locations into squares, rectangles, indexes, and calendars, mixing natural elements with conceptual overlays and structures.\nThe largest and most recent work in this show, entitled These Fragments Against Time, brings together 130 individual images, plus a low table full of skulls, bones, fossils, and other natural history specimens. Most of the photographs are broadly celestial, with stars, planets, moons, and eclipsed orbs hovering in the dark emptiness of space, but these images are intermittently interrupted by early flying machines, zeppelins, and views of the ocean, as well as jawbones, dry lake beds, and ancient carvings. Experienced as one melded image/object artwork, it feels both aspitational and mystical, the reaching to the stars grounded in the bones (and the histories) of the past – it’s a bit 2001: A Space Odyssey, but more envelopingly intimate and personal. Formally, it is built on echoes of cosmic circles that bounce around the work with vitality.\nIn other grids, Stuart explores thematic groupings, temporal events, and linked ideas. In Trajectory of Evolutionary Correspondences, warm-toned images of plants, fossils, and microscopic nature come together in a nest of intricate patterns, the unidentifiable organic specimens seemingly floating in a photographic primordial soup. Landscape of Evil is more provocative. It combines imagery of firing squads, executions, guns, bomb explosions, and soldiers from across history, creating a unsettling portrait of our own self-destruction. While We Went About Etherized picks up a related thread, brocading spooky carnival acts (including a fortune-telling chicken), screaming faces, graves, and gas masks into a menacing meditation on our foggy-eyed existence. And Flight of Time works back to softer subject matter, building an ephemeral fading whisper out of birds in flight and flowers in bloom.\nStuart’s single image photographs appear to be made-to-be-photographed constructions of imagery and objects, perhaps using projections or other image layering techniques. These works are more oblique, their allusions less direct and obvious. Mostly, they lean towards the mysteriously evocative, with phantoms and dark silhouettes inhabiting non-places where emotions and histories percolate and simmer.\nBy liberally mixing archival imagery with her own photographs, Stuart effectively creates works that wander through time, with memories and past realities tightly woven into the present. Her grids enable the building of complex moods and motifs that live within her structure, where flashbacks flutter through the mind and tangents get followed and reincorporated. While a sense of self-conscious preciousness creeps into a few of these works, they mostly feel surprisingly flexible and amorphous. She’s taken a risk in trying to visually bridge the collective and the personal, and when she gets it right, her grids swell up into round, three-dimensional experiences.\nCollector’s POV: The photographic works in this show are priced as follows. The single images are $14000 each, while the sets of photographs range from $70000 to $200000, and likely beyond, as some of the largest works were on reserve (the prices therefore not available/disclosed). Stuart’s photographic works have little secondary market history, so gallery retail likely remains the best option for those collectors interested in following up.']	['<urn:uuid:6837211f-9fbe-41df-b2d4-07f1c98b4e4a>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	4	69	1104
74	How do the casualty numbers from the Gallipoli campaign compare to those of the Meuse-Argonne Offensive in terms of Allied losses?	In the Gallipoli campaign, about 58,000 allied soldiers died (including 29,000 British and Irish soldiers and 11,000 Australians and New Zealanders). In comparison, during the six-week Meuse-Argonne Offensive, the American Expeditionary Force suffered 26,277 killed and 95,786 wounded.	"['What happened at Gallipoli?\n- Rasul Dagsar\nIn a military disaster 100 years ago, about 58,000 allied soldiers died. What was the plan, exactly what went wrong, and what is Gallipoli’s legacy?\nA merged image of present day Gallipoli and the same scene on 25 April 1915 Australian soldier lies injured in the foreground as troops move among the dead and wounded on the beach at Anzac Cove. Photograph: Getty Images\nIn a military disaster 100 years ago, about 58,000 allied soldiers – including 29,000 British and Irish soldiers and 11,000 Australians and New Zealanders – lost their lives on the Gallipoli peninsula. A further 87,000 Ottoman Turkish troops died fighting the allies and at least 300,000 more on both sides were seriously wounded.\nWhat was the objective?\nConceived by Winston Churchill as the First Lord of the Admiralty, the plan was to knock out Ottoman Turkey, Germany’s ally, out of the war. The goal of the naval and land operation was to open up the Dardanelles straits, heavily mined and defended on its western shore by Turkish coastal forts and gun batteries on the 50-mile Gallipoli peninsula, to allied ships, capture Constantinople – present-day Istanbul – and so link up with Russia. Churchill saw the campaign as a way of breaking the attritional deadlock on the western front. “Are there not other alternatives?” he said at the time.\nWhat went wrong?\nThe repeated British and French bombardments beginning in mid-February against Turkish positions proved ineffective. A final attempt to force a passage up the Dardanelles in March ended in three allied battleships being sunk and three badly damaged. The two landings – one by British troops at Cape Helles, at the base of the peninsula, and the other on the western Aegean coast, in the area later known as Anzac Cove, by the Australian and New Zealand Army Corps – were met by fierce and disciplined Turkish opposition. Well dug in and heavily fortified on higher ground, they had been reinforced six times over. The allies had badly underestimated the capacity of the Turkish forces.\nHow much headway did the allies make?\nMost did not advance more than a few hundred metres from the shore where they were pinned down. The allies endured suffocating heat and were surrounded by rotting corpses that drew thick swarms of flies. They lacked water and thousands died from dysentery as disease spread. The campaign did divert large Turkish forces away from the Russians, but did not produce the desired strategic success.\nHow did the campaign end?\nThe allied commander, Sir Ian Hamilton, was replaced by Sir Charles Munro and the allies withdrew in January 1916. In contrast to the attack the retreat was considered a major success. Churchill resigned from the government and went to command an infantry battalion in France. The disaster hastened Asquith’s resignation and his replacement as prime minister by David Lloyd George.\nWhat is Gallipoli’s legacy?\nThe Australian prime minister, Tony Abbott, said it “left horrific scars and was, in a critical sense, our nation’s baptism of fire – and 8,000 Australians didn’t come back”. Anzac day on 25 April – in remembrance of the Gallipoli campaign – is one of Australia’s most important national occasions. For Turkey, the victory saw the triumph of Mustafa Kemal, a 33-year-old lieutenant colonel who commanded the 19th Turkish Division and famously told his men: “I don’t order you to attack; I order you to die.” As Mustafa Kemal Atatürk, he became the founding father of the Turkish republic in 1923.', ""| Quick Facts | Background | |\n| Chronology |\nWhere: The Verdun Sector, immediately north and northwest of the town of Verdun.\nCheck the Location on a Map of the Western Front\nWhen: September 26 - November 11, 1918\nAEF Units Participating: U.S. First Army commanded by General John J. Pershing until October 16th then by Lt. General Hunter Liggett.\nClick Here To See the disposition of US forces at the start of the battle.\nOpposing Forces: Approximately forty German divisions from the Army Groups of the Crown Prince and General Max Carl von Gallwitz participated in the battle with the largest contribution by the Fifth Army of Group Gallwitz commanded by General Georg von der Marwitz.\nMemorable For: Being the biggest operation and victory of the American Expeditionary Force in World War I.\nAdvancing through the Argonne Forest\nThe Meuse-Argonne Offensive was the greatest American battle of the First World War. In six weeks the AEF lost 26,277 killed and 95,786 wounded. It was a very complex operation involving a majority of the AEF ground forces fighting through rough, hilly terrain the German Army had spent four years fortifying. Its objective was the capture of the railroad hub at Sedan which would break the rail net supporting the German Army in France and Flanders and force the enemy's withdrawal from the occupied territories.\nThe bulk of the forces engaged in the initial onslaught had to be transferred from the St. Mihiel Salient ---- assaulted less than two weeks earlier ---- to a new jump off line north and northwest of Verdun. This new section of the front extended thirty miles east to west . The reshifting of forces in such a short period of time was one of the great accomplishments of the Great War. These logistics were planned and directed by Col. George C. Marshall establishing his reputation and preparing him to lead -- in the distant future --- American forces to victory in the Second World War.\nThe Meuse-Argonne Offensive: A Chronology\nAbbey Ruins atop Montfaucon\nUsed as German Observation Post\n25 September 1918\nAt 2330 the initial barrage by 2700 guns begins. Ten American divisions of 26,000 men each are poised to attack. They are organized in three corps, arrayed facing north from the edge of the Champagne in the west to the River Meuse in the East.\nThe corps and divisions are [west to east]: I Corps - 92nd [actually under French Command], 77th, 28th & 35th Divisions; V Corps - 91st, 37th & 79th Divisions: and III Corps - 4th 80th and 33rd Divisions. These divisions would be relieved, replaced and sometimes moved along the front during the different phases of the battle. Dismounted cavalry working as scouts and the Lt Tank Brigade are also deployed to support the assault.\n26 September 1918\nH-Hour at 0530. The initial advance is brisk with the exception that the 79th Division encounters difficulties capturing the look out post and strong point of Montfaucon. This delay holds up the entire advance for over a day and allows the Germans to recover from their initial shock and to reorganize.\nLandmarks captured include the Butte de Vacquois and the historic village of Varennes.\n27 September 1918 Montfaucon is captured, but the day and one-half delay allows German forces to escape and regroup avoiding a rout. This provest to be the most expensive missed opportunity of the AEF.\n1 October 1918 The Offensive is clearly not meeting expectations. The lack of roads in the Argonne creates a stupendous traffic jam. Pershing replaces several inexperienced divisions. Tank support because of the terrain and mechanical problems has not lived up to expectations.\n3 October 1918 To the west, the Lost Battalion of the 77th Division is surrounded. Things are bogged down along the line. Pershing shuffles his divisions for a renewed assault.\nThe new order of battle includes [west to east]: I Corps - 77th, 28th & 1st Divisions; V Corps - 32nd & 3rd Divisions: and III Corps - 4th 80th and 33rd Divisions.\nDoughboys Under Fire in the Village of Exermont\n4 October 1918\nFirst Army begins a major attack along the entire front, but this will just be the first in a series of attacks all resulting in high casualties with small gains in ground.\nClick Here To See the Advances of the First Army During October.\n7 October 1918\nSecondary flanking attack by I Corps reenforced by 82nd Division to relieve the Lost Battalion.\n8 October 1918\nLost Battalion survivors walk out. Sgt. York of 82nd Division wipes out nest of 35 machine guns and captures 132 German soldiers as part of relief operation. Pershing orders the French XVII Corps with American divisions to attack along the east bank of the Meuse.\n9-21 October 1918\nA see-saw battle on the east bank of the Meuse as the Germans continually resist and counterattack the US units of the French XVII Corps.\n11 October 1918\nThe new push slows down and Foch orders another pause. Summerall named to command the key V Corps.\n12 October 1918\nPershing orders a major reorganization. A Second US Army will be given responsibility for the reduced St. Mihiel Salient for an eventual thrust against Metz. Hunter Liggett will be given command of First Army replacing Pershing..\n13 October 1918\nAEF mounts another major attack with limited success. Cunel and Romagne in the central Argonne are captured. Romagne would become the site of America's largest overseas military cemetery.\nArgonne Traffic Jam\n16 October 1918\nGrandpre on the River Aire at the northwestern corner of the Argonne is reached. Pershing directs Liggett to take immediate command of First Army. Grandpre will not fall completely until October 23rd.\n17-31 October 1918\nAs Liggett reorganizes and plans a major assault, a series of local attacks straightens the American front from Grandpre in the west to the east bank of the Meuse.\n21 October 1918\nCunel captured; the third German defensive position, the Khiemhilde Stellung is broken.\n22 October 1918\nBy the 22nd of the month III and V Corps had secured the Bois de Foret and Bois des Rappes and had pushed to the norther and westen limits of the Bois de Bantheville. First Army prepares for final assault on Sedan.\n27 October 1918\nFrench 4th Army to the left [west] of the US 1st Army catches up to American front line. This is made possible by the success of two U.S. divisions, the 2nd and 36th, successfully capturing Blanc Mont Ridge in the Champagne and pursuing the enemy to the River Aisne\n31 October 1918 American units on the east side of the Meuse advance to Consenvoye Heights. First Army is ready for an advance across a broad front._ The Meuse-Argonne Offensive: Chronology Continues\nKriemhilde Section of Hindenburg Line\nNortheast of Romagne\n1 November 1918\nThe reorganized First Army now under the command of Lt. General Hunter Liggett begins the final pursuit to Sedan.\nThe divisions are once again reorganized [west to east]: I Corps - 78th, 77th, & 80th Divisions; V Corps - 2nd & 89th Divisions: III Corps - 90th and 5th Divisions; and the French 15th Corps with the US 79th, 26th and 81st Divisions. A major road construction program during October improved the logistical situation.\nThe first days advance by V Corps in the center is six miles. The Germans are shocked and order a withdrawal.\nClick Here To See the Advances of the First Army During November..\n2-3 November 1918\nThe 2nd Division marches right through the enemy positions and advances another five miles.\n3 November 1918\nIII Corps on the right forces a crossing of the Meuse south of Dun-sur-Meuse with the 5th Division forcing the bridgehead.\n5 November 1918\nLeading US units reach the hills overlooking Sedan. The First Army boundary is ordered to be shifted to the east to allow the French 4th Army the honor of capturing Sedan site of a defeat in 1870 and redirect 1st Armies route of advance.\n7-11 November 1918 Units already east of the Meuse continue advancing northward and the First Army Headquarters lays plans for taking the old fortress of Montmedy, the next logical objective.\n10 November 1918 As part of First Army's shift eastward units of the 2nd and 89th Divisions begin a west to east crossing of the Meuse.\n11 November 1918 Marshall Foch's armistice instructions arrive at 0600 hours. US units involved in the river crossings have reached Senegal Farm and Moulins by 1100 hrs, but some doughboys and marines do not hear about the cease-fire until noon time.\nClick here to visit Part II of The Big Show:\nGeneral Pershing's Official Battle Report\nTo find other Doughboy Features visit ourDirectory Page\n|For Great War Society\nClick on Icon\nFor further information on the events of 1914-1918\nvisit the homepage ofThe Great War Society\nAdditions and comments on these pages may be directed to:\nMichael E. Hanlon\n(firstname.lastname@example.org) regarding content,\nor toMike Iavarone (email@example.com)\nregarding form and function.\nOriginal artwork & copy; © 1998-2000, The\nGreat War Society""]"	['<urn:uuid:7719f65a-da9d-4e75-a788-cbd51b15de92>', '<urn:uuid:41fab0c0-ebab-46a3-a3c6-858d5e2ed430>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T04:56:11.774636	21	38	2097
75	levis strauss disney american brands compare founding	While both are iconic American brands, Levi Strauss created the first blue jeans in 1873, while Walt Disney began later and pioneered animated films and the modern theme park business. Both companies have deep American roots - Levi's revolutionized workwear with riveted denim pants, while Disney created entertainment that became globally recognized as quintessentially American.	"[""There are two stories to the legend behind Levi Strauss, the man responsible for the most worn garment in all the world…Levi jeans. Like all good stories, the lines between myth and fact are blurred, but the truth is simple: Everyone loves Levis.\nThe most famous jeans in the world might not have ever been, were it not for the California Gold Rush of 1849. German immigrant Levi Strauss had big plans when he headed out west with a supply of canvas, needles, thread and scissors, which he planned to sell to the miners out digging for their fortunes. But during the journey to the west, bored travelers bought Levi out of most of his supplies, leaving him with an abundance of canvas material and little else.\nEver the shrewd businessman, Levi planned to make tents for the homeless miners, but the gold diggers didn’t want tents, preferring to sleep beside their ‘diggins’ in the mud and muck. The one thing they did need was a pair of sturdy pants, as their city clothes fell apart from the constant wear and tear experienced in the abrasive business of mining. And so Levi gave the miners what they needed: strong, durable pairs of ‘waist-high’ overalls made out of canvas. And the rest is history…or is it?\nThe legend of Levi has been picked over and guessed at for over 100 years, and perhaps the truth will never be known. What we do know is that German immigrant Levi Strauss is responsible for his world-famous jeans, but maybe the story isn’t so glamorous. Maybe a fabrication just sold more jeans, or sounded better.\nUnfortunately, we will probably never know, as the Levi Strauss documentation was lost in a factory fire in 1913 when the great San Francisco earthquake nearly destroyed the heart of the West Coast metropolis. Records were lost, and according to the experts at Levi Strauss, the Gold Rush story may be pure myth. Perhaps the story is a much simpler one.\nYes, Levi did own a dry goods business, and yes, he was successful selling canvas, needles, and thread to his loyal customers. But maybe the real hero is Jacob Davis. Who? Yes, Jacob Davis was a tailor with a problem. He kept ripping the pockets of his pants, and had the ingenious idea of putting metal rivets at the stress points, the pocket corners and the button fly. Unfortunately, Davis’ idea cost $68 to patent, an amount a small businessman couldn’t afford. So he contacted one of his fabric suppliers, Levi Strauss, and partnered with him to create the best invention since sliced bread (although we think sliced bread came after, and is pretty darn good, too).\nThus, on May 20th, 1873, the world as we know it changed… and for the better. Levi Strauss was now in the business of making sturdy workpants guaranteed to withstand the most arduous strain and stress, thanks to a new-fangled invention: the metal rivet. Metal rivets were like industrial strength staples that secured the cloth with exceptional strength to withstand the most rigorous abuse put to clothes.\nBut the story doesn’t end there. Levi became famous for his riveted pants (then called workpants) made out of serge de Nimes, a sturdy twill fabric produced in France. Shorten that French phrase to ‘denim,’ and now we’re in business. In 1890, the first Levi’s 501 jeans were stitched, and the model soon became the Levi’s flagship. These tough and sturdy workpants made working life a whole lot easier, but when the Depression struck in the 40’s, Levis survived only because of mass acceptance for everyday wear.\nTimes were tough, and mothers couldn’t afford to keep their kids in clothes. Kids are hard on delicate fabrics, and pants were ripped and torn, worn and shred before the summer months were over. And so, wearing denim dungarees (still considered workpants only) was the only answer for durable pants in the time of strict fabric rationing.\nKids experienced a joy unknown to anyone who hasn’t run, crawled, rolled and splashed with reckless abandon for fear of ruining their ‘good clothes.’ Denim freed adolescents from the proper constraints of careful play when the almost-indestructible denim dungaree hit the market.\nDenim jeans and ‘teenager’ grew up together, literally. Before the baby boom prosperity of post-World War II America, youths were kids, and teens were young adults. But the disposable income teens were making during this financially abundant time didn’t need to go to put food on the table, so instead it went into music, cars, and most importantly, clothes. The teenager was born, and he wanted jeans.\nThe 50’s wouldn’t have been the same had it not been for the teenage uniform of straight-legged, cuffed hem jeans. Actors like the wild Marlon Brando and James Dean, the definitive rebel without a cause, popularized jeans for fashion’s sake, and teenagers couldn’t wait to slip into a pair of the indigo-dyed denim.\nBoys paired their straight-legged denim with tight white t-shirts, leather jackets and army boots. Girls put sloppy oversized white oxford shirts, bobby socks and saddle shoes with their jeans. Levis became the symbol of America, its freedom and its strength. Even in the turbulent 60’s and decadent 70’s, the soft cotton comfort of denim was a must for casual wear, and the back leather patch with the Levi logo meant quality and fit, guaranteed.\nVintage Levis became a big business in the 80’s when everything that could be done to a pair of jeans was—stonewashing, acid washing, painting, ripping, etc. Nostalgia turned the traditional Levis into collector’s items, but Americans weren’t the only ones nostalgic for the days of sock hops, malteds, and rock and roll. The Japanese embraced the ultimate in American culture, scooping up old denim jeans from the 50’s. But how to know what’s an old pair, and what’s a new? Before 1971, the red tag in the back pocket seam of Levis was a big, capital ‘E.’ When the classic jeans market exploded, these so-called ‘Big E’s’ became an instant status symbol, representing a genuine pair of old vintage denim.\nAnother marker of classic Levis was the akamimi, or red-line. Levis manufactured before 1986 commonly have a red-line selvage (end of material) on the inner seam. Traditionally, the straight side seam would utilize the extra space of the white-edged selvage and be incorporated in the outside leg seam. A red stitch line was apparent in the middle of the white selvage, which gave the jeans the name red-lines or akamimis, meaning ‘red ear.’ In the wake of the vintage denim revival, Levis reissued the old dark denim style…so if you’re a vintage collector, don’t get fooled by a new pair of reissues.\nDecade after decade, Levis remains a constant in quality, and a favorite for all generations. Levis don’t follow the fads…they set them with their classic shades of blue, from deep indigo to light stonewash. Denim is a friend, and Levis did it first—and, some would say, best.\nFashion Sub Categoriesgirl's apparel"", 'Wrapping a brand in the American flag can give it a boost, but it can also come off as crass.\nConsumers don\'t necessarily believe the message delivered to them in a commercial. They also have their own criteria for what makes a company American, according to Brand Keys CEO Robert Passikoff, whose company just completed its 15th annual survey of American brands.\n""It is important to note that these brand rankings do not mean that other brands are not patriotic, or that they don\'t possess patriotic resonance or intention,"" says Passikoff in a press release. ""Rational aspects, like being an American company, or \'Made in the USA,\' or having nationally directed CSR [corporate social responsibility] activities and sponsorship, all play a part in the personality of any brand. But if you\'re a brand that wants to differentiate and engage via emotional values, if there is believability, good marketing just gets better.""\nThis year\'s report surveyed 4,860 consumers, 16 to 65 years of age, balanced for political party affiliation. They were asked to ""evaluate which of 280 brands included in the 2017 survey were most resonant as to \'patriotism.\'"" The list contains some familiar names, but two entries in the top 10 (really the top 11, because there was a tie at No. 6) are new to the top 50.\nThe only new entrant in the top-10 Comcast\'s cable brand may also be the most controversial entrant on the list due to how politically polarizing some of its content is. It\'s worth noting that FOX News, arguably MSNBC\'s biggest rival, placed 15th on the list.\n9. Samuel Adams\nBoston Beer Company has a patriotic background. The company named its signature beer line after Samuel Adams, a man who helped organize the Boston Tea Party and a signer of the Declaration of Independence.\nWhile Adams\' name graces the front of each bottle of Boston Beer\'s various brands, it\'s not his image on the label. ""Fellow Patriot Paul Revere\'s portrait appears on the front of each bottle, allegedly due to Samuel Adams\' notoriously bad looks,"" according to BeerInfo.com, which also noted that Boston Beer is the largest American-owned brewery.\n8. Jack Daniels\nIt\'s hard to think of something more American than this whisky brewed in Lynchburg, Tennessee, for more than 150 years. Jack Daniel\'s, which took its name from creator Jasper Newton ""Jack"" Daniels, has been made using the same charcoal mellowing process it has employed since its founding. The original company, which is now owned by Brown-Forman, was the first registered distiller in the U.S.\n7. Ralph Lauren\nRalph Lauren the brand, created by Ralph Lauren the man, has helped pioneer a look that\'s quintessentially American. The company may not have created ""preppy,"" but it has driven the idea forward with its signature Polo line, which was inspired by the uniforms worn in the eponymous sport. This often-copied look in some ways embodies American dress to much of the world.\n6. Hershey, Twitter\nThe two brands that tied for sixth place represent two very different aspects of the American ideal. The Hershey Company, an iconic chocolate brand founded in 1894, offers an array of recognizable candies. Twitter, which has become a platform for political discourse alongside less serious discussion, represents the new age of American innovation as one of the world\'s leading social-media companies.\nFord has wrapped its brand in the American flag. The company, which began in Detroit in 1903, essentially created not only the U.S. automobile industry, but also the assembly line. Now, Ford is still a Detroit-based company, and it\'s hard to think of many products more strongly associated with America than the Mustang and the F-series pickup truck, which has been the nation\'s best-selling vehicle for 35 straight years.\nCoca-Cola has (for better or for worse) distributed its signature, American-as-apple-pie beverage all around the world. The company, which remains based in Atlanta, traces its history to 1886, when it was first sold at soda fountains as a supposed medicine. Now, there are few places on Earth where you can\'t get a Coke, and the company has grown well beyond its iconic cola.\n3. Walt Disney\nAnother global brand, Walt Disney has distinctly American roots. The company pioneered animated films and created the modern theme park business. Walt Disney World in Florida and Disneyland in California remain top travel destinations not just for U.S. families, but for people all around the world.\n2. Levi Strauss\nIf Americans had a ""uniform,"" it would likely include a pair of blue jeans. Levi Strauss created the first pair of blue jeans in 1873.\nThe company was actually founded based on a process created by tailor Jacob Davis. The company explains on its website:\nOne day the wife of a local laborer asked Jacob to make a pair of pants for her husband that wouldn\'t fall apart. Jacob tried to think of a way to strengthen his trousers and came up with the idea to put metal rivets at points of strain, like pocket corners and the base of the button fly. These riveted pants were an instant hit. Jacob quickly decided to take out a patent on the process, but needed a business partner to help get the project rolling. He immediately thought of Levi Strauss, from whom he had purchased the cloth to make his riveted pants.\nFrom that humble beginning came a pair of pants most Americans, and many people around the world, consider a wardrobe staple.\nWhile Jeep may not seem like an obvious choice to top this list, the brand actually topped the first Brands Keys most patriotic survey in 2002. The company was founded in the U.S. in the 1940s by Mark Smith, and while its products are sold around the world, the company still remains very popular in its home country.\nNow owned by FiatChrysler, Jeep still makes the kinds of rugged utility vehicles it originally made for the U.S. military during World War II. It has also expanded into more civilian-friendly formats like luxury SUVs and subcompact crossovers.']"	['<urn:uuid:118604af-500e-4ec0-9312-25442dfb4db4>', '<urn:uuid:ddd69714-e4be-477d-b828-8dc698e6a0b1>']	factoid	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T04:56:11.774636	7	55	2172
76	What benefits and challenges has iFarm technology shown in salmon farming?	iFarm technology has shown benefits in reducing sea lice treatments by 50% compared to conventional pens and demonstrated good fish welfare through harvest quality results. However, challenges include the need to further develop underwater feeding systems to achieve optimal feeding factors, and complexity in implementing software and hardware solutions for real-time multi-angle camera monitoring of individual fish.	"[""Innovation means using the knowledge we have to inspire change; to take a fresh look at our thinking to overcome barriers, enabling us to visualize all possibilities.\nAs part of Pathways to the Future, GSI members are always looking for innovations to help raise the performance of the salmon farming sector, but it is also important that innovation is not just an idea, but that we turn innovations into reality. Here are some examples of how GSI members have been doing just that.\n1. Feed conversion ratio\nFarm-raised salmon are carnivorous, and as such one part of their feed is sourced from wild pelagic fish species, which provide a diet rich in marine oils to ensure healthy growth. However, use of wild fish as an ingredient in aquaculture feed has received some negative associations due to its potential environmental impact. Given the expected growth of the farmed salmon industry, it is therefore imperative that the industry looks for alternative feed sources to reduce the dependency on wild stocks – which is where innovation comes in.\nThe feed conversion ratio (FCR), defined as the amount of feed needed to increase an animal’s bodyweight by one kilogram, is an important aspect of feed performance as it not only contributes to how efficient fish growth is, but also is an important measure of sustainability and a metric for the responsible use of resources. As members of the GSI, we therefore use this ratio to demonstrate the efficiency of turning feed into protein, and the lower it is, the more efficient salmon growth is.\nInnovation in FCR is not just about looking at the feed ingredients, but looking at genetics, the environment (temperature, oxygen and light) and also the fish husbandry. Together with improvements in the feed ingredients profile, the salmon farming industry has reduced its FCR from 1.9:1 in the 1980s to 1.15:1 on average today, making it one of the most efficient sources of protein available.\n“In the early days of aquaculture, farmers would feed raw fish directly to their fish, which created a lot of water pollution because the fish could not eat everything, so lots of nutrients were lost in the water. BioMar was one of the first companies to create a pelletized feed that significantly reduced the FCR and created a stable supply of feed. Since then, BioMar has adopted some very interesting feed production technology that has helped to create a better, more efficient feed pellet. Engineering feats such as advanced extrusion technology and vacuum coating made it possible to create a feed pellet that retained more nutrients, which makes for a better FCR.”\nNiels Alsted, VP Business Relations, BioMar Group (GSI Associate Member)\nTo learn more on the developments in FCR, please see this report from an FAO and GSI co-hosted FCR workshop in Costa Rica in 2015.\n2. Feed ingredients\nAs mentioned, feed ingredients play an important role in the efficiency of salmon farming, but they also play an important role in providing the farmed salmon with all the protein and essential nutrients they need for optimal health and growth. To help conserve resources, the industry has been looking for innovative alternative ingredients that allow for the inclusion of fatty omega-3 acids (EPA and DHA), without relying solely on fish oil. One of those innovative alternatives is the use of novel oils.\nHere, Einar Wathne, Seafood Leader of Cargill, provides an update on how Cargill has been supporting the development of these novel resources.\nCargill has worked closely with a supplier to develop a source of algal oils, which we have included in the diets of two key customers since early 2018. The algal oil will supply 20% of the EPA and DHA in the feed, saving 330 tonnes of fish oil. This has increased the EPA and DHA in the final fillet, boosting the benefits of eating farm-raised salmon.\nCargill is also continuing its research of canola, genetically modified to produce EPA and DHA. Trials in Chile on Atlantic salmon show that the fish store EPA and DHA from these oils as they would from fish oil. Further tests are planned before the commercial licenses for this material are expected in 2020.\nWe are also continuing our exploration of Calysta's FeedKind protein material, a sustainable ingredient made from fermentation of methane gas. It has been tested in a variety of feeds under different conditions, but is not yet available commercially.\nIn addition, small volumes of insect meal are becoming available and permitted to use after changes in EU and Canadian laws. We plan to carry out some customer trials in 2018, and in the meantime encourage collaboration through the value chain to make this novel raw material viable.\nNew and innovative raw materials are key to the future of aquaculture, but they face the same economic challenges as any start-up. It takes time and investment before they can compete with established raw materials such as fishmeal, fish oil and soy. So, what will it take for novel ingredients to become a true factor in nutrient supplies for fish feed?\nIn Cargill, we believe in active engagement with various suppliers, as this has already brought new and more sustainable raw materials into feed. We also think along the lines of a value-chain partnership where each player is willing to commit, compromise and share the costs of preparing for the future.\n3. Technological advancements\nFish farming has come a long way since it began, when it was simply pens in the ocean. Today, salmon aquaculture is one of the most technologically advanced farming systems in the world. From the design of the pens, to their location, to how the fish are fed and handled – salmon aquaculture is a science based on decades of knowledge and precision, and the industry continues to build on experience to further refine the farming process for the benefit of the fish, the environment and the consumer. One of the most exciting developments currently underway is the Cermaq iFarm technology.\nHere, Harald Takle, Technology Research Director of GSI Member Cermaq, tells us more.\niFarm – providing a health journal for each fish\niFarm is a technology focusing on farmed fish health and welfare, and recognizing fish as individuals – it thus has the potential to revolutionize fish farming.\nA fish thrives when it is healthy, has good nutrition, has sufficient living space of the right quality and is left alone. This is not very different from other forms of husbandry.\nIn today’s salmon farms we manage all the fish in a pen the same way. This means that if some fish in a pen have sea lice, all fish are treated. The fish are stressed by the treatment and we treat far too many fish.\niFarm uses digital recognition of the fish. This allows us to monitor factors including growth, sea lice, disease, lesions and other aspects that affect the health and welfare of the individual fish.\nIn addition, it is possible to separate healthy fish from the fish that need treatment, for example against sea lice. Thus, the extent of sea lice treatment will be dramatically reduced. In addition, mortalities in production will be reduced by 50–75%.\nIntroducing an iFarm sensor in a fish farm will reduce the environmental footprint of the farm while ensuring welfare for each fish. The technology can be used in net pens as well as in closed systems.\nThe digitalization delivered by iFarm will also provide authorities with real-time data of the status of all the fish and will improve and simplify the governance of fish farming.\nAccording to Cermaq, the development of iFarm requires six years with comprehensive scope of fish.\n4. Fish health\nFish health continues to be the number one priority for GSI members. However, like any animal, farm-raised salmon are also susceptible to illness and disease, and as farmers it is our responsibility to use our best knowledge to support optimal fish health and prevent illness as best possible. Whether this is through improving husbandry practices, working with the latest R&D for medicinal support, or by utilizing the latest technologies to help us improve fish health and fish welfare, we are always looking to innovate our practices to keep our farmed fish as healthy as possible.\nHere we see how Peter Bender, Managing Director and CEO of GSI Member Huon Aquaculture, has introduced new well-boat designs to improve management of fish health on its farms.\nAmoebic gill disease (AGD) has been a significant issue in the Tasmanian Atlantic salmon industry since it started in 1985. Although mortality due to AGD is minimal, the financial and logistical cost of regular freshwater treatments is substantial. Farmed fish growth can also be significantly impacted if handling procedures and treatments are not low-stress. Treatment methods have evolved at Huon Aquaculture Group (Huon) from simply moving fish into floating liners of freshwater which removes the disease from the fish, to current procedures utilizing world-leading innovative well-boat technology. Modern well-boats can bathe large numbers of fish in an effective, efficient and low-stress way. This is critical in enabling Huon to farm in offshore high-energy sites, which experience significant wave heights over 11 metres.\nHuon’s current well-boat (Ronja Huon) leased from Solvtrans is 76m in length, holds 3,000m3 and can pump 300 tonnes biomass per hour. However, Huon’s new well-boat (Ronja Storm) currently being built and due in Tasmania late 2019 is 116m in length, will hold a total of 13,000m3 and have a movement capacity of 700 tonnes biomass per hour. The Ronja Storm will have the capacity to treat an entire 240m circumference pen of farmed fish (i.e. 700 tonnes biomass) in one treatment. An on-board reverse osmosis plant will be able to produce 700 m3/hr at 0.3 parts per thousand. salinity using ultrafiltration technology. The boat will also have the latest technology for chilling water at 1°C/hr, oxygen generation, CO2 stripping, faster cleaning and disinfection systems, counting and biomass assessment.\nThis technology allows us to treat the fish in the quickest and most stress-free environment to keep the salmon healthy and happy. As part of the GSI Biosecurity taskforce, we have shared this development to help other members utilize the knowledge for their salmon farming operations.\n5. Circular economy\nThe circular economy aims to redefine growth, and instead focus on using resources for as long as possible, to extract the maximum value from them and reduce the amount of waste we produce. With this in mind, it provides an opportunity to rethink current production cycles and find ways to re-use or extend the life of products that we previously had not considered.\nOne way this is possible in the farmed salmon industry is to collect what we see as waste, and turn this into energy.\nHere, GSI Member Bakkafrost’s CEO Regin Jacobsen discusses how innovation is helping create a completely new industry through the circular economy.\nBakkafrost has employed a systematic approach to optimizing raw material use and minimizing waste since the company started 50 years ago. We have an ambitious investment strategy focussed on efficient growth to meet the growing demand for sustainably produced protein. This includes advancing circular solutions that reduce waste, reduce and avoid CO2 emissions, and stimulate sustainability through other industries.\nIn March 2018, Bakkafrost made a commitment to invest in the Faroe Island’s first biogas plant, which when in operation, will have the capacity to convert up to 90,000–100,000 tonnes annually of all waste from salmon and dairy farms. With the current input predicted, this will provide enough renewable heat for 400 homes and electricity for 1,900 homes. This is projected to save 11,000 tonnes of CO2 emissions and produce 40,000–45,000 tonnes of natural liquid fertilizer annually.\nThe renewable energy produced from waste will feed in to the national grid, contributing to the Faroe Island’s national target to have 100% electricity from renewable sources by 2030, which in October 2018 was already at approximately 75%.\nBakkafrost’s approach to the circular economy has received recognition at international sustainability awards.\nMore information on Bakkafrost’s two-year sustainability plan, including commitments to continue to increase transparency on how the company is addressing industry-wide sustainability issues, can be found in its 2017 Healthy Living Sustainability Report.\nInnovation is never a tick box\nThese are just a flavour of some of the innovations taking place across GSI member farms, but we expect to see many more over the coming years. As GSI members, we believe that greater innovation, and greater diffusion of innovation, will play an important role in realizing our goals much faster and at a greater scale than would otherwise be possible."", 'The first iFarm fish on the way to market\nThe results from the harvest show high quality of the iFarm fish, which confirms that the health and welfare of the fish has been good in the iFarm net pens. The very first iFarm fish was stocked in Martnesvika sea site in the autumn of 2020.\nUsing artificial intelligence and machine learning, Cermaq and BioSort aim to identify each individual fish in a pen and give individual follow-up and health journals. The iFarm project, which will develop individual aquaculture, is planned to run over five years. In phase 1, the main task has been adapting the iFarm equipment in the pen to the fish\'s behavior and ensuring that the fish are well and have good welfare.\n""The fish behavior we have observed in phase 1 tells us that the fish is doing fine with the iFarm equipment in the pen. This is further supported by the results from the harvest. The fish has had a good life in the iFarm pens,"" says Cermaq’s iFarm project manager, Karl Fredrik Ottem\nThe need for delousing was reduced by 50% in the iFarm pens compared to the conventional pens on the site and scoring of welfare indicators was good throughout the production.\nThe iFarm project is a development project with a high degree of innovation, and with many uncertainties. The iFarm concept cannot be combined with ordinary underwater feeding equipment, and the harvest results showed that there is a need for further development of the underwater feeding system to achieve as good feeding factor as we normally have in Cermaq. The first production has also provided important experience in terms of solutions for integration between various main components of iFarm and how these should be further developed to improve handling operations and daily operations.\nAlso, the first version of the iFarm sensor has been tested in phase 1 in Martnesvika sea site, which has resulted in important experiences regarding the camera arrangement, lighting, and data processing to enable a health record for each fish. ""Checking the fish in real time with cameras from multiple angles opens up to tell more about each fish, but at the same time it requires a lot from the software and hardware solutions we develop,"" says general manager of BioSort, Geir Stang Hauge.\nThe further development of iFarm - phase 2 - is taking place in Vesterålen. An entire sea site has been equipped with iFarm setups in all the pens, after the fish was stocked last autumn. In this phase, there will be real full-scale testing of the concept and technology.\n""We have learned a lot that we have already implemented in phase 2, and we have received a number of answers that take us to the next step. An important part of the innovation work is to find out what works and what does not work "", says Ottem.\nThe first iFarm fish was harvested at Steigen processing plant and has been sent to customers in Europe and Asia.']"	['<urn:uuid:1965eb97-219c-4f76-a8d2-d689771eae76>', '<urn:uuid:1b6b6b73-f34f-4522-9bdb-cd23fc9936a3>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	11	57	2599
77	agricultural machinery power transfer systems safety practices drive shaft maintenance procedures	For agricultural machinery power transfer systems, safety practices include wearing appropriate PPE (safety goggles, gloves, steel-toed boots), following proper lockout/tagout procedures, and ensuring vehicle support during maintenance. Regular maintenance procedures involve inspecting for wear and damage, proper lubrication of joints, maintaining correct balancing and alignment, and applying protective coatings. For shuttle shift systems, proper clutch usage varies by model - some require clutching while others, like certain Kubota models, don't need clutch operation when switching between forward and reverse. It's crucial to follow manufacturer guidelines for specific maintenance tasks and intervals, and ensure proper torque levels for fasteners.	['Exactly what is ‘shuttle shift’\nWhen compared to other models that may have 1 – 4 forward reverse speeds,’shuttle shift’ allows you to go from forward to reverse in whatever gear you’re in. It usually requires the use of a clutch. The big advantage is that you then have multiple reverse speeds.\nWhen switching from forward to reverse, JG Powershuttle does not require the use of the clutch, whereas Sync Shuttle does. You choose the range, gear, and then you just shuttle all day long. There is a clutch, but it is moved electronically, which is why gears do not grind. On my Kubota, the shuttle shift lets me go from forward to reverse smoothly without using the clutch (takes some getting used to).\nI’ve never heard of an infinitely variable shuttle shift before. Electro mechanical wet clutches are integrated with the transmission, so if you step on both pedals at the same time, you get an increase in rpm but no motion. It’s perfect for FEL. The hoe has a hand throttle.\nThere are enough different types of “shuttle shift” to drive you insane, and they all require clutching before shifting. Kubota’s GST (Glide Shift Trans) came out around 1990 and was a completely different beast. The term shuttle shift implies that some sort of lever has to be moved in order to change directions, whether you clutch or not.\nI have five sloping mountain acres where, at first, I’ll be doing a lot of loader work, making short trails and grading, where hydro is best, but later, when I move there, the tractor will primarily be used for snow removal, using either a blower or a loader.\nThe Kubota M4800 is a “gear” tractor with a partially synchronized transmission that lets you “shuttle” between low and reverse, but you have to use the clutch. Hydro would be best, but then I’d have to get a higher horsepower to have enough power left over to throw wet snow in the spring.\nWhich is better hydrostatic or shuttle shift?\nHydro is more expensive to buy, and it also has maintenance costs with fluid.\nDo you have to clutch a shuttle shift tractor?\nA Kubota shuttle shift transmission, which is commonly found in Kubota tractor transmissions used for cultivating, harvesting, and transporting hay, does not require the use of a clutch to switch between forward and reverse gears.\nWhat is tractor shuttle?\nA power shuttle is an additional unit used in transmissions that is commonly found in agricultural tractors. While the vehicle is moving forward, the driver can pull a lever to stop it and reverse it at the same speed. Power Shuttles are also known by other trade names such as Power Reverser.\nWhat is the number one selling tractor in the world?\nIndia’s Mahindra tractor brand, which has been around since the 1960s, is the world’s best-selling tractor brand.\nWhat does hydrostatic mean on a tractor?\nA tractor with a hydrostatic transmission uses hydraulic fluid and a variable displacement pump, which converts mechanical energy into hydraulic energy for use with a hydraulic motor.\nAre hydrostatic tractors reliable?\nWhen it comes to driving, a lawn tractor with hydrostatic transmission is more dependable, as this type of transmission provides much better control over inclined and rough terrains for heavy-duty work.\nWhat does HST mean on a tractor?\nEasy-to-use hydrostatic transmission (HST) improves tractor performance. The 3032E and 3038E Tractors come with a two-range hydrostatic transmission (HST) incorporated in a die-cast aluminum transmission case.\nCan you shift a tractor while moving?\nA manual transmission in a tractor does not have synchronized gears, so you should never shift while moving; shifting while moving can grind gears, shift forks, and do a lot of other bad things that can cause expensive damage to your tractor’s transmission, so it’s best to avoid it.\nWhat is hydraulic shuttle shift?\nHydraulic Shuttle Shift (HSD Models): This transmission works similarly to a regular gear drive transmission, with a gear range of 8 to 24 forward and reverse gears, but without the need to clutch to shift from forward to reverse.\nWhat is a gear drive tractor?\nA gear-drive transmission locks a tractor into one speed and is typically better suited for sloping and rough terrain. It’s a cost-effective option that allows for maximum tractor PTO performance.\nWhat is the difference between automatic and hydrostatic transmission?\nAn automatic transmission works similarly to a car’s transmission in that you control speed with the gas pedal. A hydrostatic transmission, on the other hand, works similarly to an automatic transmission but transfers power from the engine to the wheels using fluid rather than belts.\nWhat is the clutch used for in a tractor?\nThe clutch is only used to select the speed range in which the tractor will operate, so the modern manual transmission has come a long way in terms of operation.', 'As a professional manufacturer for propeller shaft, we have ;;262571520;2625719294;26209488491\nG16, 3200 RPM\nWhat maintenance practices are crucial for prolonging the lifespan of drive shafts?\nTo prolong the lifespan of drive shafts and ensure their optimal performance, several maintenance practices are crucial. Regular maintenance helps identify and address potential issues before they escalate, reduces wear and tear, and ensures the drive shaft operates smoothly and efficiently. Here are some essential maintenance practices for prolonging the lifespan of drive shafts:\n1. Regular Inspection:\nPerforming regular inspections is vital for detecting any signs of wear, damage, or misalignment. Inspect the drive shaft visually, looking for cracks, dents, or any signs of excessive wear on the shaft itself and its associated components such as joints, yokes, and splines. Check for any signs of lubrication leaks or contamination. Additionally, inspect the fasteners and mounting points to ensure they are secure. Early detection of any issues allows for timely repairs or replacements, preventing further damage to the drive shaft.\nProper lubrication is essential for the smooth operation and longevity of drive shafts. Lubricate the joints, such as universal joints or constant velocity joints, as recommended by the manufacturer. Lubrication reduces friction, minimizes wear, and helps dissipate heat generated during operation. Use the appropriate lubricant specified for the specific drive shaft and application, considering factors such as temperature, load, and operating conditions. Regularly check the lubrication levels and replenish as necessary to ensure optimal performance and prevent premature failure.\n3. Balancing and Alignment:\nMaintaining proper balancing and alignment is crucial for the lifespan of drive shafts. Imbalances or misalignments can lead to vibrations, accelerated wear, and potential failure. If vibrations or unusual noises are detected during operation, it is important to address them promptly. Perform balancing procedures as necessary, including dynamic balancing, to ensure even weight distribution along the drive shaft. Additionally, verify that the drive shaft is correctly aligned with the engine or power source and the driven components. Misalignment can cause excessive stress on the drive shaft, leading to premature failure.\n4. Protective Coatings:\nApplying protective coatings can help prolong the lifespan of drive shafts, particularly in applications exposed to harsh environments or corrosive substances. Consider using coatings such as zinc plating, powder coating, or specialized corrosion-resistant coatings to enhance the drive shaft’s resistance to corrosion, rust, and chemical damage. Regularly inspect the coating for any signs of degradation or damage, and reapply or repair as necessary to maintain the protective barrier.\n5. Torque and Fastener Checks:\nEnsure that the drive shaft’s fasteners, such as bolts, nuts, or clamps, are properly torqued and secured according to the manufacturer’s specifications. Loose or improperly tightened fasteners can lead to excessive vibrations, misalignment, or even detachment of the drive shaft. Periodically check and retighten the fasteners as recommended or after any maintenance or repair procedures. Additionally, monitor the torque levels during operation to ensure they remain within the specified range, as excessive torque can strain the drive shaft and lead to premature failure.\n6. Environmental Protection:\nProtecting the drive shaft from environmental factors can significantly extend its lifespan. In applications exposed to extreme temperatures, moisture, chemicals, or abrasive substances, take appropriate measures to shield the drive shaft. This may include using protective covers, seals, or guards to prevent contaminants from entering and causing damage. Regular cleaning of the drive shaft, especially in dirty or corrosive environments, can also help remove debris and prevent buildup that could compromise its performance and longevity.\n7. Manufacturer Guidelines:\nFollow the manufacturer’s guidelines and recommendations for maintenance practices specific to the drive shaft model and application. The manufacturer’s instructions may include specific intervals for inspections, lubrication, balancing, or other maintenance tasks. Adhering to these guidelines ensures that the drive shaft is properly maintained and serviced, maximizing its lifespan and minimizing the risk of unexpected failures.\nBy implementing these maintenance practices, drive shafts can operate reliably, maintain efficient power transmission, and have an extended service life, ultimately reducing downtime and ensuring optimal performance in various applications.\nWhat safety precautions should be followed when working with drive shafts?\nWorking with drive shafts requires adherence to specific safety precautions to prevent accidents, injuries, and damage to equipment. Drive shafts are critical components of a vehicle or machinery’s driveline system and can pose hazards if not handled properly. Here’s a detailed explanation of the safety precautions that should be followed when working with drive shafts:\n1. Personal Protective Equipment (PPE):\nAlways wear appropriate personal protective equipment when working with drive shafts. This may include safety goggles, gloves, steel-toed boots, and protective clothing. PPE helps protect against potential injuries from flying debris, sharp edges, or accidental contact with moving parts.\n2. Lockout/Tagout Procedures:\nBefore working on a drive shaft, ensure that the power source is properly locked out and tagged out. This involves isolating the power supply, such as shutting off the engine or disconnecting the electrical power, and securing it with a lockout/tagout device. This prevents accidental engagement of the drive shaft while maintenance or repair work is being performed.\n3. Vehicle or Equipment Support:\nWhen working with drive shafts in vehicles or equipment, use proper support mechanisms to prevent unexpected movement. Securely block the vehicle’s wheels or utilize support stands to prevent the vehicle from rolling or shifting during drive shaft removal or installation. This helps maintain stability and reduces the risk of accidents.\n4. Proper Lifting Techniques:\nWhen handling heavy drive shafts, use proper lifting techniques to prevent strain or injuries. Lift with the help of a suitable lifting device, such as a hoist or jack, and ensure that the load is evenly distributed and securely attached. Avoid lifting heavy drive shafts manually or with improper lifting equipment, as this can lead to accidents and injuries.\n5. Inspection and Maintenance:\nPrior to working on a drive shaft, thoroughly inspect it for any signs of damage, wear, or misalignment. If any abnormalities are detected, consult a qualified technician or engineer before proceeding. Regular maintenance is also essential to ensure the drive shaft is in good working condition. Follow the manufacturer’s recommended maintenance schedule and procedures to minimize the risk of failures or malfunctions.\n6. Proper Tools and Equipment:\nUse appropriate tools and equipment specifically designed for working with drive shafts. Improper tools or makeshift solutions can lead to accidents or damage to the drive shaft. Ensure that tools are in good condition, properly sized, and suitable for the task at hand. Follow the manufacturer’s instructions and guidelines when using specialized tools or equipment.\n7. Controlled Release of Stored Energy:\nSome drive shafts, particularly those with torsional dampers or other energy-storing components, can store energy even when the power source is disconnected. Exercise caution when working on such drive shafts and ensure that the stored energy is safely released before disassembly or removal.\n8. Training and Expertise:\nWork on drive shafts should only be performed by individuals with the necessary training, knowledge, and expertise. If you are not familiar with drive shafts or lack the required skills, seek assistance from qualified technicians or professionals. Improper handling or installation of drive shafts can lead to accidents, damage, or compromised performance.\n9. Follow Manufacturer’s Guidelines:\nAlways follow the manufacturer’s guidelines, instructions, and warnings specific to the drive shaft you are working with. These guidelines provide important information regarding installation, maintenance, and safety considerations. Deviating from the manufacturer’s recommendations may result in unsafe conditions or void warranty coverage.\n10. Disposal of Old or Damaged Drive Shafts:\nDispose of old or damaged drive shafts in accordance with local regulations and environmental guidelines. Improper disposal can have negative environmental impacts and may violate legal requirements. Consult with local waste management authorities or recycling centers to ensure appropriate disposal methods are followed.\nBy following these safety precautions, individuals can minimize the risks associated with working with drive shafts and promote a safe working environment. It is crucial to prioritize personal safety, use proper equipment and techniques, and seek professional help when needed to ensure the proper handling and maintenance of drive shafts.\nCan you explain the different types of drive shafts and their specific applications?\nDrive shafts come in various types, each designed to suit specific applications and requirements. The choice of drive shaft depends on factors such as the type of vehicle or equipment, power transmission needs, space limitations, and operating conditions. Here’s an explanation of the different types of drive shafts and their specific applications:\n1. Solid Shaft:\nA solid shaft, also known as a one-piece or solid-steel drive shaft, is a single, uninterrupted shaft that runs from the engine or power source to the driven components. It is a simple and robust design used in many applications. Solid shafts are commonly found in rear-wheel-drive vehicles, where they transmit power from the transmission to the rear axle. They are also used in industrial machinery, such as pumps, generators, and conveyors, where a straight and rigid power transmission is required.\n2. Tubular Shaft:\nTubular shafts, also called hollow shafts, are drive shafts with a cylindrical tube-like structure. They are constructed with a hollow core and are typically lighter than solid shafts. Tubular shafts offer benefits such as reduced weight, improved torsional stiffness, and better damping of vibrations. They find applications in various vehicles, including cars, trucks, and motorcycles, as well as in industrial equipment and machinery. Tubular drive shafts are commonly used in front-wheel-drive vehicles, where they connect the transmission to the front wheels.\n3. Constant Velocity (CV) Shaft:\nConstant Velocity (CV) shafts are specifically designed to handle angular movement and maintain a constant velocity between the engine/transmission and the driven components. They incorporate CV joints at both ends, which allow flexibility and compensation for changes in angle. CV shafts are commonly used in front-wheel-drive and all-wheel-drive vehicles, as well as in off-road vehicles and certain heavy machinery. The CV joints enable smooth power transmission even when the wheels are turned or the suspension moves, reducing vibrations and improving overall performance.\n4. Slip Joint Shaft:\nSlip joint shafts, also known as telescopic shafts, consist of two or more tubular sections that can slide in and out of each other. This design allows for length adjustment, accommodating changes in distance between the engine/transmission and the driven components. Slip joint shafts are commonly used in vehicles with long wheelbases or adjustable suspension systems, such as some trucks, buses, and recreational vehicles. By providing flexibility in length, slip joint shafts ensure a constant power transfer, even when the vehicle chassis experiences movement or changes in suspension geometry.\n5. Double Cardan Shaft:\nA double Cardan shaft, also referred to as a double universal joint shaft, is a type of drive shaft that incorporates two universal joints. This configuration helps to reduce vibrations and minimize the operating angles of the joints, resulting in smoother power transmission. Double Cardan shafts are commonly used in heavy-duty applications, such as trucks, off-road vehicles, and agricultural machinery. They are particularly suitable for applications with high torque requirements and large operating angles, providing enhanced durability and performance.\n6. Composite Shaft:\nComposite shafts are made from composite materials such as carbon fiber or fiberglass, offering advantages such as reduced weight, improved strength, and resistance to corrosion. Composite drive shafts are increasingly being used in high-performance vehicles, sports cars, and racing applications, where weight reduction and enhanced power-to-weight ratio are critical. The composite construction allows for precise tuning of stiffness and damping characteristics, resulting in improved vehicle dynamics and drivetrain efficiency.\n7. PTO Shaft:\nPower Take-Off (PTO) shafts are specialized drive shafts used in agricultural machinery and certain industrial equipment. They are designed to transfer power from the engine or power source to various attachments, such as mowers, balers, or pumps. PTO shafts typically have a splined connection at one end to connect to the power source and a universal joint at the other end to accommodate angular movement. They are characterized by their ability to transmit high torque levels and their compatibility with a range of driven implements.\n8. Marine Shaft:\nMarine shafts, also known as propeller shafts or tail shafts, are specifically designed for marine vessels. They transmit power from the engine to the propeller, enabling propulsion. Marine shafts are usually long and operate in a harsh environment, exposed to water, corrosion, and high torque loads. They are typically made of stainless steel or other corrosion-resistant materials and are designed to withstand the challenging conditions encountered in marine applications.\nIt’simportant to note that the specific applications of drive shafts may vary depending on the vehicle or equipment manufacturer, as well as the specific design and engineering requirements. The examples provided above highlight common applications for each type of drive shaft, but there may be additional variations and specialized designs based on specific industry needs and technological advancements.\neditor by CX 2023-10-02']	['<urn:uuid:7ed316e8-e584-4d71-8c4e-1594c831b098>', '<urn:uuid:a2fea4e0-4a76-463a-a6f1-91437050172d>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:56:11.774636	11	98	2931
78	Could you explain in detail how the presence of a logical progression in scientific investigation differentiates it from other forms of inquiry?	The method of science follows four distinct steps that ensure logical progression: First, framing the question based on careful observation or analysis of existing knowledge. Second, developing a testable hypothesis as a possible answer. Third, conducting an experiment to either find an answer or prove/disprove the hypothesis. Fourth, arriving at an answer that may be either a scientific fact of limited applicability or a broader generalization. At each step and between steps, existing knowledge and logical reasoning are used. This systematic approach allows science to obtain true and reliable answers to questions.	['There are no secrets in science. One can do and know all that a scientist does and knows. One does not need to have any faith in him as an individual and accept what he says without questioning it.\nThe only thing a scientist asks of one is to base his questioning on simple, untempered and uninhibited reason, the kind of reason that characterizes a child on whom society has not yet imposed its beliefs and prejudices. In fact, science is nothing more than questions asked and questions answered, and the method of science, no more than a method of asking questions and answering them, a method that “works” and can be used to solve problems that one faces in everyday life.\nScience, is indeed, far more than just physics, chemistry, biology, astronomy and mathematics. It is question of ideas and a way of thinking ; it is a culture and a philosophy of life, a philosophy which allows us to pursue truth without any prejudgment. What, then, is this attitude of mind, this culture and this philosophy of life? It turns out that all these – and other – concomitants and attributes of science emanate primarily from the method that science uses to acquire knowledge.\nCertain characteristics of this method, which we call “The Method of Science”, and of knowledge gained by the application of this method, create a value system within their framework, a value system which is rational and reasonable, which appeals to common sense, which is commensurate with knowledge and, above all which has a built-in corrective.\nIt, therefore, becomes specially important to understand the basis of the method of science, and to have a look at the attributes of knowledge gained through this method and at the value system of science, so that we may understand the culture, the attitude of mind and the philosophy of life, which are the essence of science. This is what we shall endeavour to do here. First let us see what the method of science proposes to do.\nLet us take a set of eight statements and look at the truth value of these statements:\n1. The sun rises in the west and sets in the east\n2. If you sow Sonora wheat, you will reap Sonora wheat\n3. Water can be made from hydrogen and oxygen\n4. Some objects can travel faster than light\n5. Atoms are unbreakable\n6. There is life out in space\n7. It is bad omen if a black cat crosses your path\n8. Family planning cannot solve any problem\nOut of these eight statements, in my opinion (and I hope you would agree with it), two statements (2 and 3) are right, and four (1,5,7 and are wrong; in the case of the remaining two (4 and 6), the answer is not known – that is, we cannot say as of today whether the statement is right or wrong. Given this situation, one many now ask the following questions:\na. How was the right answer arrived at?\nb. If we do not know the answer today, how will we be able to find the answer in the future?\nc. If you happened not to agree with my answer, how could I say with confidence that you were wrong?\nd. If you would like to verify my answer, how would you go about it?\nThe answer to all the above questions is: by using the method of science. The method of science allows one to obtain true and reliable answer to questions.\nThe steps in the Method of Science\nThere are four distinct steps in the method of science: the farming of the question, framing of a hypothesis, doing of an experiment, and arriving at the answer which may be a fact or a generalization in the form of a theory or law. At each step in this sequence and in going from one step to the next, we use existing knowledge and logical reasoning.\nThe questions in science arise out of careful observation or careful analysis of existing knowledge: there is no third origin of a question! In fact, if you have framed your question properly, you are already on your way to finding the answer. What, then, is a well-framed question? A well-framed question is the one for finding the answer of which means are available within the framework of the method of science. Such a question should lead to a hypothesis which can then be tested by an experiment. Let us take an example.\nOut of the million species that inhabit our earth, nearly seven hundred thousand, i.e. some 70 per cent of them (all insects), have six legs – indeed, a remarkable observation, having tremendous implications. Well-framed questions arising out of this observation would be:\na. How did all the seven hundred thousand or so species come to have six legs?\nb. Do these species have other common features?\nc. Could they have originated in nature from a common ancestor?\nThese are well-framed questions. The more carefully you observe, the better – framed your question would be\nThe second step, the hypothesis, is an answer we may consider possible. The single most important attribute of a good hypothesis – a scientific hypothesis – is that it must be testable. A testable hypothesis is one which can be tested by an experiment or on the basis of which a testable prediction can be made.\nFor example, a common place observation is that objects, it left unsupported in space or earth, will fall to the ground. One can make many untestable hypotheses to explain this phenomenon. One such hypothesis would be they fall to the ground because a particular friend of yours wills them to do so. Another would be: they fall to ground because God desires them to do so. None of these hypotheses is testable.\nTherefore, they are not scientific. On the other hand, if you make the hypothesis that an object unsupported in space on earth falls to the ground because there exists a force of attraction between the object and the earth, you can test the hypothesis. In fact, it was starting with such a hypothesis that Newton arrived at his famous Laws of Gravitation.\nThe third step in the method of science is the experiment. The experiment must have one of the following two objectives: either to find an answer to a question, or to prove or disprove a hypothesis. An experiment which does not attempt to meet one of these two objectives: is unlikely to be a good experiment.\nTo do an experiment, one must make an inventory of all the steps in the experiment, collect all the material one needs, and carry out all the steps with the utmost possible care. More important, one must record all the observations, paying attention to the smallest detail.\nAnd most important, one must record everything – expected or unexpected – whether one wished it to be so or not. It is in the doing of an experiment in this manner that the values of objectivity, lack of bias and the exercise of care in what one does, come into existence in science. An unexpected observation he made while trying to test quite different hypothesis.\nThe last step in the method of science is the answer. The answer generally takes one of two possible forms. It could either be scientific fact of limited applicability, or a generalization of wide applicability. Let us consider an example; the case of the two diseases, albinism and haemophilia.\nAlbinism is the lack of the pigment (called melanin) which gives the skin its natural colour; albinos have an unusual appearance and can be easily spotted: their skin is pinkish white, their hair has the colour of straw, and their eyes are sensitive to intense light and need to be protected.\nHaemophilics do not posses the ability to allow the blood to clot, so that a hemophilic would bleed to death because of a small wound which many of us would not even notice. The question is how are these diseases caused?\nIt has been observed that these two diseases are found in a cluster in families. It is also known that synthesis of melanin and clotting of blood are biochemical processes controlled by biological catalysts called enzymes. A reasonable hypothesis, therefore, would be that albinism and haemophilia are hereditary diseases caused on account of malfunction of an enzyme.\nOne could do two types of experiments to verify the hypothesis. First, one could look for other albionos or haemophilics in the family of an albino or a hemophilic and determine the familial pattern, if any, in regard to the occurrence of the diseases, for if the diseases are inherited, they must follow the laws of heredity.\nThe second experiment would be determine, by biochemical analysis, why the albionos cannot make the skin pigment, or why the blood of a haemophilic cannot coagulate, and then see if this biochemical deficiency is present in all haemophilics and albinos.\nBoth these investigations were done. It was found that these diseases are inherited according to the laws of heredity propounded by Mendel more than a hundred years ago, and now well understood. It was also shown that the disease was due to the lack of a particular enzyme in each case.\nTherefore, beginning with an observation, through the construction of a testable hypothesis, and by doing experiments, we came to the conclusion that an inheritable enzyme deficiency can cause a disease: in fact, one of the most important conclusions ever arrived at in biology.\nThe Uses of the Method of Science\nThe method of science can be used for various purposes. Let us consider three such uses.\na. To find an answer to a specific question, the like of which has been answered many times before. Here one asks the question, does the experiment, and arrives at the answer, without framing a hypothesis. One uses past experience and existing knowledge to design the experiment and to interpret the results of the experiment. For example, the question may be; find the density of a 50 paise coin.\nIndeed, scientists have found densities of objects many, Indeed, scientists have found densities of objects many, many times before and the procedure for doing so is well-known. One may, therefore, skip the hypothesis step and go on to determine the weight and the volume of the coin (the experiment). The density would be weight divided by volume (the answer).\nb. To find an answer to a question of which the answer is wholly or partially unknown. In such cases, one begins with an observation or an analysis of existing knowledge (the framing of the question). A hypothesis is then formulated and experiments designed to test the hypothesis; the result of the experiments are recorded. If the results support the hypothesis, the hypothesis could be the answer. If so, more experiments are done to test the validity of the hypothesis.\nIf the results of any of the experiments do not support the hypothesis, the hypothesis is modified and another set of experiments do not support the hypothesis. Once all the experiments that could be done to test the hypothesis have been done, and all of them have supported the hypothesis, one could say that the hypothesis is correct. The hypothesis, then, becomes the answer to the question initially asked. This is the sequence followed in scientific research.\nc. To find an answer to a question under conditions where an experiment is not possible. This is, perhaps, the most exciting application of the method of science. As an example, let us take the question: what should you do to help your brother who is suffering from typoid, to get well soon as possible? One may indeed construct many hypothesis as possible answers to this question.\nShould you go to a place of worship and pray? Should you let nature take its own course and hope for the best? Should you give him an extract prepared from spices or herbs recommended by your grandmother or a friend? Or should you seek the advice of a doctor trained in modern medicine? You obviously cannot test all the alternatives by experiments. Indeed, if you followed the method of science, you would determine which of these answers will be most compatible with existing knowledge.\nIf such an exercise is done carefully, you would probably come to the conclusion that it is the last suggestion, that is seeking the advice of a competent doctor trained in modern medicine, that is in agreement with existing knowledge and is likely to serve best the objective of curing your sick brother as quickly as possible. (Modern medicine provides a sure remedy for typhoid). The method of science can be applied in this manner for answering questions concerning a vast spectrum of human activity, form decision-making in daily life to ethics, politics, economics and social behavior.\nCourtesy : Angels, Devil and Science\n- A Collection of articles on scientific temper,\nPublisher : National Book Trust, India\n-to be continued in the next issue….']	['<urn:uuid:344136ca-0518-4a17-8178-b02f0a67bf76>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	22	92	2179
79	How do mental health professionals ensure the privacy and confidentiality of psychological assessment results when they need to be shared with other healthcare providers?	Psychological assessment results are typically shared with the referring physician or mental health agency who requested the testing. However, third-party parties can only see the results if the patient explicitly authorizes them, though there are some exceptions to confidentiality rules. It's important for patients to clarify exactly who will see their results before taking a psychological test.	['The term psychological assessment is often associated with mental illness, but it also refers to a variety of other conditions. A psychologist may perform psychological testing in order to determine a patient’s cognitive abilities and diagnose certain mental disorders. Depending on the condition, psychological tests can also be used to determine an individual’s intelligence level, learning abilities, and other characteristics. For example, a psychologist may administer the Stanford-Binet intelligence scale to evaluate a person’s intelligence.\nThe aim of psychological assessment is to identify comorbidity between two or more conditions in a person. In general, more than six in ten people with substance use disorders also have another form of mental illness. This double burden can significantly reduce the success of a treatment for substance abuse. According to a national study, certain clusters of psychiatric disorders are more common in people than you might think. If this is the case, then psychological assessment will be useful for you.\nSome psychometric textbooks distinguish between standardized tests and assessment. While tests involve the administration of standardized stimuli, assessments involve the collection and analysis of data. In psychometrics, assessment refers to the process of examining a range of information. These types of materials include questionnaires, inventories, protective tests, and observations. While most textbooks describe the two methods in the same way, they differ in some ways. For example, in testing, the test-taker is given a series of true or false questions, and the results are usually recorded by objective scoring methods.\nClinical psychologists make several judgments based on the results of psychological assessment. These judgments can include whether the client is a risk for suicide, whether the client needs treatment, or whether the patient needs to be hospitalized. Moreover, the results of psychological assessment help them choose the right treatment strategy for each individual client. They also help clinical psychologists determine which interventions will be most effective for the client. If the results of psychological assessment are positive, the results of the treatment may be even better.\nDespite the fact that psychological assessment is not a multiple-choice test, results of the test are usually shared with the referring physician or mental health agency who requested the testing. Third-party parties can see the results only if the patient authorizes them. However, there are exceptions to confidentiality rules. Before taking a psychological test, it’s important to clarify exactly whom will see the results. If you’re unsure of the information or the process, ask the psychologist how to share it with others.\nBehavioral health professionals can perform a psychological assessment by examining an individual’s personality. The purpose of a psychological evaluation is to help determine the causes of a person’s behavior and develop a plan for treatment. While psychometrics are a part of psychological assessment, the main purpose is to determine the best way to treat a person. However, there are a number of differences between the two. When it comes to the type of psychometric tests, a psychological assessment will depend on the individual’s individual needs.\nThe earliest forms of psychological testing date back to 2200 B.C. China, where emperors used to test officials to determine their suitability for ruling, and in Europe, a large number of psychologists adopted these methods after World War I. Many tests were designed for the army to identify potential soldiers with PTSD and shellshock. They eventually became the standard for psychological evaluation today. In the 1800’s, computers were used by examiners to administer the tests.\nA specialized tool for assessing depression is the CBT questionnaire. These questionnaires are designed to help physicians make an informed decision about treatment. The results of the tests are used to make treatment plans and determine the patient’s progress. Psychological assessment is a key part of the treatment process. As a result, the clinical psychologist must evaluate the results of each individual and develop a plan to address the problem. This assessment also provides baseline data for ongoing assessments, allowing clinicians to refine or refocus interventions.\nSingle scales are not perfect, but they are generally reliable within a given range. Each test has its own measure of ability, and psychologists must be experienced in interpreting them correctly. Moreover, single-scale tests must be used with caution because they were originally designed to fit the Kraepelin System, which is now out of date and no longer includes diagnostic grouping for psychopathic deviants. A psychologist must be able to determine whether these tests are relevant, central, or irrelevant.']	['<urn:uuid:0fd8ffa5-94f2-4d08-9f0b-0313a762566c>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	24	57	740
80	meaning hongbao chinese new year	Hongbao are red envelopes containing money that are given during the Spring Festival (Chinese New Year). During the 15-day festival, younger people greet their elders with 'Gong xi fa cai, hong bao na lai!' meaning 'Wish you a prosperous new year. Give me a red envelope!' The elderly give these red envelopes to youngsters and may place them under pillows on New Year's Eve. The red color is believed to protect children from evil spirits, similar to how Western children receive gifts from Father Christmas. While children particularly enjoy this tradition for the money inside, which they can spend as they wish.	"['From among a world of colors, red stands out the most. The color red is fresh and pure, and in China, we call it China Red. Chinese people are fascinated by the color red not only because of its intoxicating vibrancy but also because of its rich meaning in Chinese culture and history.\nIt is believed that no country in the world has ever adopted a color in such a thoroughgoing way as China, where red is a symbol that gives color to the soul of the nation. In the past, red stood for dignity and mystery. Even now, we adore the color much more than we love it. It is can be said that \'China Red\' is an eternal theme for China and an essential color for the Chinese people. ""China Red"" has become a quite popular word, attracting the world\'s attention.\nShooting red-colored things in China is an easy task, as its powerful presence can be found everywhere. All traditional red things have been playing special roles in China: the walls of ancient palaces, the national flag, Chinese knot, lantern, couplet, clothes, traditional paper-cuts for window decorations, and even red tanghulu (sugar-coated haws on a stick). Red is the color of the auspicious, signifying reunion, health, happiness, harmony, peace and prosperity. Only real things and events can fully display and explain its beauty. Only in its relationship with people can the color be alive and meaningful. Here we will reveal the cultural significance of China red.\nRed Chinese Knot (Zhongguo Jie)\nChinese knot is actually an ancient art of weaving and Chinese people have used knots as decorations since the Han Dynasty. Now people in China are still intrigued by the secrets and complicated beauty of traditional red knots. ""Knot"" In Chinese has the meanings of reunion, friendliness, warm, marriage, etc, so the red Chinese knot has come to be the symbol of reunion, luck, harmony, and love.\nTraditionally, a Chinese knot must be bent, tied and crafted from a single red rope, to express the endless circle of a happy life. That is the most important characteristic of Chinese Knot. By combining different knots or other auspicious adornments skillfully, a unique auspicious ornament, especially the red-colored which represents beauty, happiness and wishes, is formed.\nToday, Chinese knot has become an important element of international fashion. To fit with the needs of modern life, Chinese knot has various products, most of which are used as auspicious hanging and knitting clothing adornment.\nCombining China red, form and complexity in a very traditional type of harmony, the square-shaped “True Love Knot” is the most famous type of Chinese knot. Instead of the traditional way to pop the question ""will you marry me"", you can choose a light-hearted and humorous way to suggest marriage by giving your sweetheart the True Love Knot, indicating ""Let\'s tie the knot"".\nRed Lantern (Hong Denglong)\nA lantern is closely linked with the lives of Chinese people, as you can find it everywhere, especially during festivals and holidays. If you happen to do Beijing tours in this period, you\'ll see red lanterns hanging on the front gate of houses and stores, decorate the street in a unique way.\nLanterns in China are more than lighting devices and red lanterns are regarded as a basic symbol of the Chinese culture, a symbol of brightness, happiness and reunion. Both the gigantic lanterns at the Tiananmen Square and the small lanterns of a peasant\'s hut express the same warm, happy and joyous atmosphere. You see a red lantern and your mind thinks of China.\nSupposed to warn away evil spirits while brightening the mood with the red glow of good luck, red lanterns are considered a necessity of traditional Chinese festivals. In Lantern Festival (the fifteenth day of the first Chinese lunar month), the preferred warm red of traditional lanterns creates a convivial atmosphere at the family reunion as well as lights up the hope for the New Year. Lanterns, especially the red-colored, deserve adoration for the magical element they add to the atmosphere.\nThe cheongsam is a high-necked dress with distinctive Chinese features. The dress is buttoned on the right side, with a loose chest, a fitting waist, and slits up from the sides, all of which combine to enhance the beauty of the female shape. Easy to slip on and comfortable to wear, the cheongsam perfectly fits the beautiful figure of females.\nThe Cheongsam, especially the red-colored, is one of the most typical, traditional costumes of Chinese women. Even though Cheongsam is not often appearing in daily life, it is worth collecting, for it reflects the Chinese culture and arts with its national characteristics and strong happy colors and fashion designs. That\'s why brides in China choose red cheongsam as their most preferred dress, to perform the traditional Chinese wedding ceremony and create a happy environment.\nThe cheongsam enjoys a long history and is now enjoying a resurgence in popularity in the world of international fashion. Shanghai is the right place to shop this kind of garment. After evolvement and development, Cheongsam has become an inspiration for fashion designers. Regarded as a symbol of the Chinese nation, Cheongsam is often seen worn by women on formal or semi-formal occasions. In either case, it creates an impression of simple and quiet charm, elegance and neatness. No wonder cheongsam is so much liked by women not only of China but of foreign countries as well.\nRed Couplet (Duilian)\nPasting red spring couplet has been an important part of the Chinese New Year since ancient times. On Lunar New Year\'s Eve, families in both urban and rural areas make it a point to decorate their gates or apartment doorframes with the customary couplets.\nThe couplet, written on two strips of red paper, is composed of two poetic sentences which match and rhyme with each other to express hopes for the coming year. The red couplets, along with the New Year paintings on the door, serve as beautiful decorations as well as protective talismans.\nRed Paper-cut ( Jianzhi )\nPaper-cut art is another traditional handcraft created by Chinese people. People cut papers into animals, flowers or people in various patterns. Posted on the window, door lintel or desks for the festival atmosphere, paper cuttings are mainly used for decoration during the Spring Festival to welcome the Chinese New Year.\nIt can be very interesting to learn paper-cut, which is all made by hand. You need only a knife and paper. But, no mistake can be made during the process, otherwise, the work would be a fail.\nDaofu is one of the most popular forms of paper-cutting. The word ""fu"", which means good luck and happiness, is written on a square red paper and often displayed upside down (dao) on doors and elsewhere. The act of turning the word “fortune” upside down is called daofu, which literally means ""Good luck is coming!"" It is not only a play on words but a play with words. A paper-cut class can be arranged in your Xi\'an tours as the following picture suggests.\nRed Firecracker ( Bianpao )\nIn China, to celebrate happy events, firecrackers are set off not only for the color but for the \'Noise\' too. People in China believe that red firecrackers can not only make noises and bring good luck but scare off evil spirits and ghosts with the loud pop as well. The floor is covered by a layer of red color pieces.\nRed Pocket ( Hongbao )\nIt is a tradition in Chinese families for the youngsters to give their elders the following greeting during the 15-day Spring Festival: ""Gong xi fa cai, hong bao na lai!"" which means: ""Wish you a prosperous new year. Give me a red envelope!"" The older members of the family will then give the youngsters a red (for luck) envelope with a gift of money. The elderly may also place the red envelopes under their pillows on the Chinese New Year\'s Eve to give them surprises. The money is wrapped in red envelops called hongbao in Chinese. The custom in China is much like the one in western countries that Father Christmas comes down the chimney and places gifts in children\'s stockings when they are sleeping.\nAs in China color red is connected with good luck, the red envelopes are supposed to protect children from evil spirits. The custom is extremely popular with children. But what they really care about is the lucky money inside the red envelopes, which they can use to buy the things they like. It is no wonder Chinese kids love red so much!\nShould you wish to bring good fortune upon yourself then I am sure that you will be absolutely delighted to give your red envelopes to your lovely kids!\nIn China, red is more than just a color. It carries the ancient history and culture of the Chinese nation. China Red is filled with mysterious charm beyond description and it is right here in China waiting for you to feel, to discover!\nFind more special topics on:']"	['<urn:uuid:ee9f53d6-1b8c-4367-b818-55f6b21a557d>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	5	102	1511
81	I'm doing research on intracranial infections - what percentage of focal brain infections are caused by subdural empyema in the US, and what factors typically lead to it?	In the United States, subdural empyema accounts for 15-22% of focal intracranial infections. The most common predisposing factors are sinusitis, otitis media, and mastoiditis.	['The meninges is a membrane that envelopes the brain and the spinal cord. It is made up of 3 layers namely, the dura mater, the arachnoid space and the pia mater. The main function of the meninges is to protect the brain and the spinal cord enclosed within. It protects the brain and the spinal cord from any neurological trauma or injury. The subdural space (also known as subdural cavity or the epiarachnoid space) is the space that is potentially formed as the arachnoid mater separates from the dura mater. This occurs as a result of trauma or other pathological conditions such as subdural hematoma or subdural empyema. The subdural space is generally called as the potential or the artificial space.\nAn Overview of Subdural Empyema\nSubdural empyema is a condition which is characterised by collection of purulent material in the subdural space between the arachnoid mater and the dura mater. It is a type of abscess caused by intracranial focal collection of pus like material. It has been noted that about 95% of the cases affects within the cranium predominantly in the frontal lobe. A minor percentage of the cases involve spinal nexus. The former type or the intracranial type is associated with inflammation of the brain and surrounding meninges, and also compression of the brain.\nSubdural empyema is a life threatening infection and studies have shown that about 15-22% of all focal intracranial infection is caused by subdural empyema. The first case of subdural empyema was reported about 100 years ago. Before the discovery of penicillin, the mortality rate of subdural empyema was very high.\nSymptoms of Subdural Empyema\nThe symptoms of subdural empyema are secondary to increased intracranial pressure, meningitis and infection. Occurrence of cerebral infarction is quite distressing. The most commonly encountered symptoms of subdural empyema include:\n- Fever, nausea, vomiting, malaise, persistent headache, stiff neck, lethargy, confusion, drowsiness, papilledema, seizures, one sided weakness, abnormal sensation with numbness, blurry vision, difficulty speaking and coma.\nIf subdural empyema is not treated immediately, it can lead to permanent damage. It can lead to neurological complications and even death in certain cases.\nEpidemiology of Subdural Empyema\nIn United States, about 15 – 22% of focal intracranial infections are caused by subdural empyema. Sinusitis, otitis media and mastoiditis are the most common predisposing factors. With development in antibiotic, the mortality rate has reduced significantly. In developed countries the mortality rate is about 6 – 35% with morbidity rate (neurological deficit) of about 55%. The decrease in mortality rate is also because of the advancement in investigative procedures. Subdural empyema is more common among males than in females (accounting up to 80% of the cases). The exact reason for higher incidence in men is yet unknown. The most accepted theory being that there is a difference in the anatomical development of the paranasal sinuses in male and female. It can occur at any age; however it is most commonly seen between the ages of 10 – 40 years.\nCauses and Risk Factors of Subdural Empyema\nBacterial infection is the most common cause of subdural empyema followed by fungal and protozoal infection. In children, subdural empyema can be caused as a complication of meningitis. Other routes through which the infection can invade includes:\n- Penetrating head injury or traumatic head injury\n- Post-operative bacterial infection from the skin\n- Infiltration from surrounding area such as paranasal sinus, middle ear, osteomyelitis of the skull or mastoiditis\n- Dissemination from distant sites such as heart valves and lungs.\n- Other infectious diseases such as tuberculosis and malaria.\nPathophysiology of Subdural Empyema\nAs mentioned earlier, subdural empyema is caused by focal infection between the arachnoid space and the dura mater. The infection spreads rapidly until limited by structures such as falx cerebri, base of the brain, foramen magnum, tentorium cerebelli etc. The infection usually spreads unilaterally. Over a period of time the intracranial pressure increases due to spread on infection. This also leads to intraparenchymal penetration, disruption of blood flow and CSF or cerebrospinal fluid flow which leads to cerebral oedema and hydrocephalus. There may be septic venous thrombosis of contiguous veins and thrombosis of the cortical veins or cavernous sinuses which may lead to cerebral infarction. In children, subdural empyema may be secondary to meningitis.\nThe infection usually enters through the frontal sinus or the ethmoid sinuses. It can also enter through the sphenoid sinus, mastoid cells and the middle ear. The condition is predisposed by sinusitis, which is followed by spread of infection into the venous sinuses through thrombophlebitis. Infection can also spread due to erosion of the posterior wall of the mastoid or frontal sinus, or directly from an intracerebral abscess. Very rarely, the source of infection is a distant foci from complication of trauma, surgery or septicaemia.\nComplications of Subdural Empyema\nSubdural empyema is characterized by acute afebrile condition with rapid progressive neurological manifestation. If left untreated, subdural empyema can be fatal. It can lead to coma and permanent neurological deterioration.\nDiagnosis of Subdural Empyema\nDiagnosis and management of subdural empyema is done by an experienced neurologist. The symptoms of subdural empyema are closely evaluated. Based on the symptoms, investigative examinations are recommended for further evaluation and planning. A contract enhanced MRI of the brain is helpful in the diagnosis of the condition. Gadolinium enhanced MRI provides better clarity. A contrast enhanced CT scan may also be considered. A comprehensive blood work is also obtained for evaluation of the condition. Blood samples and other surgical specimens are usually cultured and studied anaerobically and aerobically. Gram staining and sensitivity tests are considered too. Lumbar puncture is not very helpful in this condition and it is often avoided. Lumbar puncture can lead to transtentorial herniation. In children and infants, subdural tap is done to relieve pressure.\nTreatment of Subdural Empyema\nTreatment for subdural empyema involves surgical drainage followed by antibiotic coverage based on the causative pathogen. Once subdural empyema is diagnosed, immediate surgical evacuation is advised. The most common surgical procedure that is considered for management of the symptoms of subdural empyema is called craniotomy. It is a surgical procedure which involves adequate brain exposure and exploration which in turn helps in better evacuation of the purulent material. Other surgical options include stereotactic burr hole placement. In rare cases with increased risk of mortality, surgical intervention may be contraindicated. Antibiotic coverage post-surgery is generally given for a period of 3 to 6 weeks. In some cases anticonvulsants and measures to reduce intracranial pressure is required.\nOther Treatment Options For Subdural Empyema Include:\n- Consultation with neurologist, otolaryngologist, and thoracic surgeons\n- Physical therapy, gait and balance training, speech therapy, occupational therapy etc.\n- Management of residual cognitive deficit by psychologist\n- Ophthalmological consultation\n- Home care aides and social work.\nSubdural empyema is a condition caused by infection and collection of focal purulent material in the space between the dura mater and arachnoid mater. It is most commonly predisposed by sinusitis, otitis media and mastoiditis. In children, subdural empyema may be predisposed by meningitis. The symptoms of subdural empyema are secondary to increased intracranial pressure and include increased temperature, nausea, vomiting, generalised weakness, headache etc. In adverse cases, it can lead to severe neurological issues, coma and even death. Hence, seeking immediately medical help is advised to reduce the damage to minimum.\n- Head and Face Injuries: Chronic Subdural Hematoma, Concussion, Epidural Hematoma\n- Chronic Subdural Hematoma: Causes, Symptoms, Treatment- CT, MRI, Surgery, Drainage']	['<urn:uuid:76267901-6b8a-4039-a356-5f3799884d1a>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	28	24	1241
82	canyoning footwear vs dog hiking boots	For canyoning, flat-soled shoes like tennis shoes are not appropriate - shoes must have grip soles for traction on slippery rocks and provide ankle support. Similarly, for dogs hiking, protective booties are essential to protect paws from sharp rocks and hot surfaces while providing necessary traction and grip on trails.	['The complete guide to canyoning will give you all the most important information you need to know about this amazing sport. Adventurers worldwide have fallen in love with the technical and thrilling adventure. Canyoning takes you through beautiful gorges, under waterfalls, down water slides and on the adventure of a lifetime. Read on to learn everything you need to know- from what to bring to where you can practice this amazing sport.\nWhat is Canyoning?\nCanyoning is also known as canyoneering in the US. This sport combines hiking, swimming, jumping, abseiling, and sliding through narrow canyon spaces. Most canyoning activities begin high up in the mountains, with some hiking required to get to the start. Canyoning courses take you down the mountain following the flow of water. The course can involve waterslides, waterfalls, whitewater sections, and dry sections.\nAnother term sometimes used interchangeably with canyoning is Aqua Hiking. Aqua hiking differs however in that it takes place in flatter canyon spaces. Because Aqua hiking is more of a walk or swim than a descent, you usually don’t use any rope. There is also very little climbing. This activity is far gentler than canyoning. It’s a good choice for less athletic people and families with young children.\nHow canyoning works\nLocated in the mountains, adventurers descend steep canyons, passing over waterfalls of different heights. The descent consists of the following techniques:\nCanyoning and canyoneering involve technical descents and require abseils (rappels), ropework, technical climbing, technical jumps, and/or technical swims. Ths sports is frequently done in remote and rugged settings. It can involve navigational and other wilderness survival skills, though most adventurers practice the sport with certified instructors.\nThe mountain canyons form narrow gorges with numerous drops, smooth sculpted walls, and spectacular waterfalls. Most canyons are cut into limestone, sandstone, granite, or basalt.\nCanyons can be very easy to descend or extremely difficult. A wide variety of canyoning routes are found throughout the world, and canyoning is enjoyed by people of all ages and skill levels.\nBesides being quite a challenge, people ultimately love canyoning for how beautiful and immersive it is. Abseiling down 30 metres underneath a waterfall is an amazing experience. Participants can swim through beautiful pools of water, slide down rocks and basically have fun! Some courses even have small Via Ferrata or Zipline sections as well.\nHistory of Canyoning\nThe term “canyoneering” was thought to have been first used by a member of John Wesley Powell’s expedition down the Colorado River in 1869.\nCanyoning is practised all over the world- and a version was likely developed individually in each country. For instance, cavers in the 1930s are credited with canyoning’s inception. Australian bushwalkers adventured through local canyons in the 1950s. In the U.S., hikers, climbers and paddlers explored the canyons in the 1940s and 50s. Because of this, different techniques and equipment have evolved uniquely in different countries.\nMany of the canyons used today for entertainment were once traversed by native people. Those people were likely wearing primitive clothing, had bare feet, sandals or moccasins. Ropes used generations prior involved scaling challenging vertical descents using simply hemp and manila ropes. It’s amazing to think of the courage they must have had!\nWinter canyoning is done in locations that are very cold and snowy, such as the Alps and the Pyrenees. These activities usually require you to wear a drysuit. The water can get down to 2°C so it’s necessary to keep from becoming hypothermic.\nFor example, you can check out this winter canyoning adventure in Gourcy in Ossau valley. While it is bound to be cold, canyoning here offers stunning views of the snowy mountains and 15-metre waterfalls in winter. It’s even suitable for beginners who want to brave the cold.\nWho should try Canyoning\nTypical canyoning courses allow participants aged 12 and up to take part. Aquatic hiking is usually suitable for ages 6 and up, and is usually recommended for families.\nWhile age is one factor, physical and mental fitness is another major consideration. Canyoning adventures are quite rigorous. Many of the courses only start after a significant amount of hiking, anywhere from 2 minutes to 2 hours!\nThe water temperature is often very cold, and someone sensitive to cold may not enjoy the activity (although you will be wearing a wetsuit!).\nCanyoning can also be difficult for some because once you begin the course, you have to finish it. There is no way out, so even if you get tired, you will have to keep going. Many canyoning courses require long walks back as well to the activity centre.\nSome routes have high jumps or abseils, which cannot be avoided. Some easier routes do have alternate paths and tougher sections can be avoided. The best plan is to be informed before you go.\nIt is also important to be alert, and not attempt this activity the day after a heavy night out. It requires concentration and technical manoeuvring, and your attention will be required in order to be safe and have fun.\nWho should not go Canyoning?\nIn most countries, the maximum group size is limited to 8 people. This is important because the instructor needs to be able to watch each participant carefully to ensure their safety. Groups larger than 8 are not suitable. It’s also important that everyone in your group has roughly the same fitness level. It is not possible to split groups up once you begin, and everyone must be able to finish.\nPersons with Vertigo, or previous bone injury, back injury, heart problems, or ankle issues are not recommended to join. You should make your activity provider aware of any other special medical issues and they can help you decide if canyoning is okay for you.\nWhat equipment is needed\nCanyoning is a rather technical sport. It’s recommended to equipment made specifically for this sport. uring The equipment needs to be durable and withstand lots of wear. Usually, professional instructors will provide you with the right kind of equipment.\nWhat will be typically provided by your instructor:\n- a wetsuit as well as 5mm neoprene socks which allow you to move in the water while maintaining proper body temperature\n- A harness fitted with lanyards and a descender which will be useful for progressions on ropes\n- A helmet\n- A bag and a waterproof container to put your lunch in for day trips or your valuables\n- A lifejacket\nYou will need to bring:\n- A pair of closed-toe shoes that will get wet\n- A swimsuit\n- Hair tie for long hair and glasses if you must keep them\n- Picnic lunch and bottle of water for day canyons\nConsidering what kind of footwear you bring is important for canyoning. Because canyoning is usually a fairly wet sport and takes place in colder climates, you will want to have the right kind of shoe.\nFlat-soled shoes, such as tennis shoes are not appropriate. Soles with grip are ideal, as they will add traction over slippery rocks. It is also important to have ankle support. Water shoes may seem ideal, but be sure to choose a pair with ankle protection.\nFamous Canyoning destinations\nTropical Reunion Island is known for being one of the most stunning places in the world to go canyoning. The Basalt rock canyons and crisp, clear waters create fun water slides. Grain Galet Canyon above Langevin River and Trou Blanc are two of the most popular places for canyoning courses.\nCanyoning in Sainte Suzanne is another famous spot on the island and is great for families and beginners. Sainte Suzanne, also known as Bassin Boeuf, offers two sections of canyoning.\nThe first part of the canyon is called “Sainte Suzanne Classique”. This section of Sainte Suzanne lasts about a half-day. It takes only a few minutes to reach the start of the course and offers small jumps into beautiful turquoise pools. The second part, called “Sainte Suzanne intégral”, is a more advanced section requiring greater athleticism. This course lasts a full-day and can be exhausting.\nThe Sainte Suzanne Integral includes a river cave crossing for 20 kilometres, 9-metre jumps, giant zip lines and abseiling sections. You can also learn more about canyoning in the Reunion Island in our article on the Top 5 Spots to go Canyoning on the Reunion Islands.\nMarc Canyon, Spanish Pyrénées\nThe Spanish Catalan Pyrenees are full of beautiful canyons to explore. Canyoning excursions at Marc Canyon in the Spanish Pyrenees, for example, are ideal for beginners.\nMarc Canyon, or Canyon de Marc, can be enjoyed by adventurers as young as 12. The walk to the starting point is only 10 minutes, and the course itself lasts 3 hours long. The route will take you through lush forests over a succession of abseils, and has a 25-metre zip line! There are lots of small waterfalls which create fun slip-and-slides along the way.\nLater you’ll have the chance to make an invigorating 8-metre jump into a natural pool of mountain spring water. This is a great option for groups and people with average levels of fitness who want to get wet-and-wild!\nLast, but definitely not least is Utah, USA. There are hundreds of canyons in Utah to go canyoning. Some of the best-known canyons are Ordville Canyon, Johnson Arch Canyon, and canyons throughout Zion National Park.\nCanyoning here is especially unique because of the smooth, red-rock formations and orange canyon waters. These technical canyons have huge abseils and are very fun. Canyoneering USA offers quite a bit of detailed information about each canyon, what level of skill is needed, and some of the best features.\nMystery Canyon in Zion National park, for instance, boasts a 34-metre abseil into a pool of sparkling waters below. There is also Pipe Springs, which requires a 2-mile hike to the entry point. It’s worth the walk though because this canyon has a 55-metre rappel!', 'Exploring dog friendly trails\nAre you an avid hiker but instantly feel stressed at the idea of bringing your furry friend along for the ride? Hiking with your dog can be a fun experience and needn’t be stressful, but preparation is key to ensuring a safe and enjoyable adventure. It’s important to be familiarise yourself with trails that are dog friendly, especially in or around national parks, as well as potential hazards including 1080 poison, dirty water sources, and more.\nIn this beginner’s guide to hiking with your dog, I break down how to prepare for a hike with your dog, what gear to take with you, and hazards to be aware of.\nHiking with your dog: Preparation\nChoose Dog-Friendly Trails\nResearch dog-friendly hiking trails in your area. Most national parks in Australia don’t allow dogs, however, certain states, for example South Australia are a little more relaxed on this subject. I recommend checking multiple sources, including your local state website to double check the rules on what’s allowed, and what isn’t. Dogs can have a significant negative impact on wildlife in Australia if taken into the bush. They can chase, attack, and kill native animals, including endangered species. They can also spread diseases to native animals so, make sure you understand the impact before heading out.\nBefore embarking on a potentially strenuous hike with your dog, you’ll want to ensure he or she is in good health and physically capable to take on the challenge. You may want to consult your vet for a check-up, especially if your dog is older or has any health concerns. Alternatively, you can try a short, easy trail to get your furry friend used to the physical demands of a hiking trail, before taking on some of the more challening ones.\nIf this is your dog’s first time hitting the trails, you’ll want to make sure you have some basic obedience commands down pat. This is crucial if you spot any hazards or notice your dog straying off the trail and into the bush. Simple commands like “sit,” “stay,” and “come” are crucial. Similarly, you’ll want to make sure your dog is comfortable and under control on-leash, especially with the excitement of new smells, sounds, and animals.\nHiking with your dog: What gear to bring\nDog First Aid Kit\nThe bush can be an exciting destination for your furry friend, but it can present a set of dangers, which range from cuts and scrapes, to run-ins with native wildlife. A comprehensive first aid kit, designed specifically for dogs is a vital piece of equipment when hiking with your dog, especially when you’re deep into a hiking trail and the nearest vet might be hours away.\nCollapsible Dog Bowl\nWhen you’re in the wilderness, a collapsible dog bowl is an absolute must. Being that it’s collapsible, it won’t take up much space in your backpack and will help keep your dog hydrated and away from potentially polluted streams and rivers.\nDog-Friendly Harness and Leash\nA sturdy dog harness helps to distribute pressure across your dog’s body, providing better control and reducing the risk of injury compared to a collar when hiking with your dog. Pair it with a reliable and (ideally) extendable leash to keep your dog secure on a hike.\nDog booties, alongside a dog first aid kit, are probably one the most important pieces of equipment you can take with you on a hike with your dog. They can help protect your furry friend from sharp rocks, bindis, and hot surfaces, whilst also giving your dog the traction and grip that he/she needs to take on a hiking trail. Make sure you pick booties that fit well to avoid discomfort.\nAn ID tag with your contact information is a must-have in case your dog gets separated from you on a hiking trail. There’s also a trend we’re seeing where dog owners are using Apple airtags to keep track of their furry friends. Definitely something to consider if you plan to go hiking with your dog on a regular basis.\nHiking with your dog: Hazards\nKeep an eye out for local wildlife, including snakes, spiders, and insects whilst you’re on the trails. It’s important that native animals and insects are undisturbed wherever possible, and therefore, it’s really important to keep your dog on a leash to prevent unexpected encounters. This not only protects your furry friend, but also ensures the local wildlife remain wild.\nWe all know that the weather in Australia can be unpredictable. Whether you’re in the tropical north, or the sub-tropical south, protect your dog from the elements with appropriate gear, such as a cooling bandana in the summer, a jacket for the colder months, and lots of water, regardless of temperature, to keep your dog hydrated. It’s also advisable to monitor weather conditions before hitting the trails to avoid bushfires, landslides and floods.\nStreams and Rivers\nKeeping your dog away from streams and rivers whilst hiking is crucial for their safety and well-being. Whilst a pool of water on a hot day may seem inviting, there are several hidden dangers in natural water bodies. Fast currents, unpredictable depths, and slippery rocks can pose a risks to your dog’s safety. Additionally, waterborne parasites such as giardia and harmful bacteria may be present, which can lead to serious issues for your furry friend. Always keep a source of water on hand for your pup, so that they’re not tempted to drink from streams or rivers.\nTick and Flea Infestations\nKeep your furry friend protected from ticks and fleas with spot-on treatments or collars before hitting the trails. Infestations are no fun for you or your dog, and it’s important to take steps to prepare wherever possible. If you suspect your furry friend has picked up a tick on their travels, it’s important to act quick. A comprehensive dog first aid kit will often include a tick remover so that you can find and remove the tick before it can injure your dog. Fleas can usually removed by combing through their fur and giving your dog a warm soapy bath.\nPoisons and Traps\nSodium Fluoroacetate or 1080 bait is widely used in National Parks across Australia to control foxes and other invasive species. Keep an eye out for signs and always keep your dog on a leash to ensure that they don’t accidentally consume this toxic substance. It’s also worth noting that some states use traps to catch and kill feral animals, therefore, whenever you’re taking your dog hiking in the bush, be aware at all times of potential hazards.\nAs you venture into the great outdoors with your furry friend, keep in mind that preparation is the key to a successful and enjoyable hike with your dog. Knowing where you can take your furry friend, what gear to take and being prepared for potential hazards will keep your stress levels low, and make hiking with your dog a breeze.\nSo, leash up, pack smart, and hit the trails, paw in hand.']	['<urn:uuid:72019b1d-285e-4cd3-86d6-359e16eb5c8a>', '<urn:uuid:4a94a067-8feb-49f2-b2ce-04f640009cb7>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T04:56:11.774636	6	50	2817
83	What challenges exist in implementing climate monitoring systems?	Several challenges exist in the project implementation, including: access difficulties to target communities due to mountainous topography of the main island, land ownership issues, security risks, and ensuring sustainable operations and management of equipment after the project ends. Reliable communication coverage to remote villages is also a critical challenge that needs to be addressed.	['Government of Papua New Guinea and UNDP come together to build climate change resilience\nTo discuss this issue, and further develop a project proposal focused on enhancing climate data and early warning systems, representatives from across Papua New Guinea’s government – including the Climate Change and Development Authority, Department of Agriculture and Livestock, National Weather Service, National Maritime Safety Authority, National Disaster Center, and Conservation and Environmental Protection Authority – joined technical specialists from UNDP, and partners from New Zealand’s National Institute of Water and Atmospheric Research (NIWA) and FAO in the capital Port Moresby last month.\nSafety and resilience in a changing climate\nAround 75% of households in Papua New Guinea are dependent on subsistence agriculture, and around 40% of the population lives on less than US$1 a day. For the heads of farming households, being able to anticipate an impending drought, frost or heavy rains, and being able to act in advance, can mean the difference between feeding your family and not having enough to eat; between making enough money to send your kids to school and not having enough to pay for the basics.\nFacing more frequent and intense weather events, driven by climate change, the PNG Government in 2016 identified the need for improved climate information and early warning systems. To address the issue, the government engaged with UNDP to develop a proposal for the Green Climate Fund.\nCrops in Papua New Guinea are highly sensitive to rain, and flooding can mean the loss of entire food gardens. In the mountain highlands, frost (which strikes in prolonged drought periods) can wipe out an entire harvest. © UNDP\nAbout the project\nOver the past months, the project team has honed their focus on improving data (gathering and sharing between organisations) and improving dissemination of information to at-risk communities – specifically smallholder farmers; people living in inland areas subject to river flooding; and small boat operators and passengers.\nThe team have completed a series of assessments – of current flood early warning systems; community risk; agriculture (current situation, vulnerabilities, risks, gaps); small boats; and food security – and begun work developing a Funding Proposal.\nThe Proposal encompasses a range of activities including:\n- Installing more weather monitoring stations, to improve quantitative data and to feed into risk modelling and rainfall thresholds\n- Strengthening a centralized climate-data and forecasting hub where government, NGOs and private entities can better access and share information and where institutions can co-produce sector-specific warnings/advisories\n- Improving the delivery of alerts to the community (via radio shows, mobile) around flooding, drought, agriculture and coastal boating conditions\n- Investing in community education and planning for enhanced decision-making (for example, helping households and smallholder farmers to understand and act upon advisories and to plan along longer-term timescales, rather than days or weeks ahead; ensuring they receive information on how to take action, for example: which crops to plant, how, where and when)\n- Conducting trainings at provincial level in operations and maintenance of equipment/stations; flood forecasting and early warnings\nCommunity buy-in and involvement in these activities is key to the project’s success. The project team has consulted communities in the initial stages, and will continue to do so. Further, the team is seeking to develop a model in which communities maintain ownership over weather monitoring towers and maintain the equipment.\nWith the onset of climate change, Papua New Guinea’s economy, environment and people are increasingly vulnerable. © Andrea Egan/UNDP\nEngaging the private sector\nAs well as the community as key stakeholders, the Government hopes to engage the private sector in support of the project and, through long-term partnerships, to ensure the sustainability of the systems after GCF funding expires. Over the coming months the project team plans to approach telecommunications, mining and construction companies to explore collaboration and co-financing.\nNext steps in proposal development\nMoving forward, the project design faces some challenges. Among them, access to target communities due to topography – the interior of the main island is mountainous and hard to reach – land ownership and security risks; planning for operations and management of equipment after the close of the project, to ensure sustainability; and communicating with remote villages (while AM radio ownership and mobile penetration is relatively high, more reliable coverage is critical).\nOver the coming months, the project team will continue to gather data, map risks (flooding, drought, other meteorological hazards) and conduct consultations to inform the project’s design.\nThe project team expects to finalise and submit a Proposal to the Green Climate Fund for consideration at the October Board Meeting in Songdo, Korea.\nFor more information on this project, please contact Ms. Aishath Azza, Regional Technical Specialist, Climate Change Adaptation, UNDP-Global Environmental Finance Unit, Bangkok Regional Hub, UNDP email@example.com Twitter @AzzaAishath\nTo learn more about UNDP’s ongoing climate change adaptation work in Papua New Guinea, click here.\nNote: The GCF proposal builds on an ongoing project around flood management in the north coast and islands region of PNG, financed under the Adaptation Fund.']	['<urn:uuid:7bf96993-76ef-44b5-8752-4da593c25638>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	8	54	831
84	real christmas day date history	The exact birth date of Jesus Christ is not recorded in the gospels. Christmas came to be celebrated on December 25th gradually. This date was formally established in Rome by Pope Liberius (352-66), and by the year 400, December 25th was generally accepted as Christmas throughout most of the Christian world, with the exception of Palestine where it was celebrated on January 6 until the mid-600s.	"['What is the actual date of the birth of Christ?\nOne would think that if anyone\'s date of birth were remembered exactly, it would be that of our Savior, Jesus Christ. Unfortunately, the gospels do not pinpoint the date of Christ\'s birth. The reason is probably that the focus of the gospels is on the kerygma or mystery of redemption — the passion, death and resurrection of Christ. This focus is also probably why St. Mark\'s Gospel does not even include the Christmas story, but begins with the Baptism of the Lord at the River Jordan. Easter, on the other hand, can be better dated because of its concurrence with Passover.\nPrior to the legalization of Christianity by the Emperor Constantine in the year 313, no universal date or even formal celebration of Christmas is found. For instance, Origen (d.255), St. Irenaeus (d. 202), and Tertullian (d. 220) do not include Christmas or its date on their lists of feasts and celebrations.\nAfter legalization, the Church was better able to establish ;universal dates of feasts and to organize the public celebration. Moreover, we now see the Church addressing controversies concerning Jesus as true God and true man and how He entered this world. Such concern would focus more attention on the importance of celebrating Christmas, the birth of our Lord.\nOn the more practical side of this issue, Roman pagans used to gather at the hill where the Vatican is presently located to commemorate the ""Birth of the Unconquered Sun"" This pagan feast was celebrated throughout the Empire either on Dec. 25 (according to the Julian Calendar) or on Jan. 6 (according to the Egyptian calendar). Although not proven with certainty, some historians credit Constantine, who declared Sunday as a day; of rest in the Empire, with replacing the pagan festival with that of Christmas. Interestingly, since the 200s, Jesus was honored with the title, ""Sun of Justice.""\nSomehow all of these elements converged to the formal celebration of Christmas on Dec. 25. For instance, Christmas was celebrated in Rome by Pope Liberius (352-66) on Dec. 25. On Dec. 25, 379, St. Gregory Nazianzus preached a Christmas sermon in Constantinople. In the Cathedral of Milan, St. Ambrose (d. 397) celebrated Christmas. Therefore, by; the year 400, generally, the birth of Christ was set on Dec. 25 with the exception of Palestine, where it was celebrated on Jan. 6 until the mid-600s, when it was then transferred to Dec. 25.\nAs an aside, the Feast of the Epiphany also emerged in Gaul (the Roman province of present-day France) about the year 361. This feast was moved to Jan. 6, which remains the official date.\nWhile the concern for exact dating may preoccupy us at times, I believe the most important point is celebrating the birth of our Lord. Remember that the title ""Christmas"" is derived from the Old English title ""Cristes Maesse"" which means ""The Mass of Christ."" This Christmas, may we lift up our hearts at the Holy Sacrifice of the Mass and receive our Lord, born into our souls through the grace of the Holy Eucharist.\nSaunders, Rev. William. ""The First Christmas."" Arlington Catholic Herald.\nThis article is reprinted with permission from Arlington Catholic Herald.\nFather William P. Saunders is pastor of Our Lady of Hope Parish in Potomac Falls and former dean of the Notre Dame Graduate School of Christendom College. Father has been writing his weekly ""Straight Answers"" column for the Arlington Catholic Herald since 1993. The above article is one of those ""Straight Answers"" columns. Father Saunders is the author of Straight Answers, Answers to 100 Questions about the Catholic Faith, a book based on 100 of his columns and published by Cathedral Press in Baltimore.Copyright © 2003 Arlington Catholic Herald\nback to top']"	['<urn:uuid:691311f8-7b02-4062-8128-3379d69a98fe>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	5	66	628
85	what company owns honeywell motion detector patent us6191688	The patent US6191688 B1 is owned by Honeywell International, Inc.	['|Publication number||US6191688 B1|\n|Application number||US 09/273,579|\n|Publication date||20 Feb 2001|\n|Filing date||22 Mar 1999|\n|Priority date||22 Mar 1999|\n|Also published as||DE60014807D1, DE60014807T2, EP1078343A1, EP1078343B1, WO2000057381A1|\n|Publication number||09273579, 273579, US 6191688 B1, US 6191688B1, US-B1-6191688, US6191688 B1, US6191688B1|\n|Inventors||William T. Sprouse|\n|Original Assignee||Honeywell International, Inc.|\n|Export Citation||BiBTeX, EndNote, RefMan|\n|Patent Citations (5), Referenced by (27), Classifications (20), Legal Events (4)|\n|External Links: USPTO, USPTO Assignment, Espacenet|\n1. Field of the Invention\nThis invention pertains generally to detecting attempts to bypass motion detectors, and more particularly to detecting, at power up of a motion detector, whether the motion detector has been masked.\n2. Description of the Background Art\nMotion detectors are widely used in alarm systems. State of the art motion detectors typically employ dual sensing technology, such as a microwave Doppler sensor combined with a passive infrared sensor (PIR), coupled with processing software. In most instances, the PIR sensor is the primary sensor and the microwave sensor is used as a secondary sensor to confirm a detection event from the PIR sensor. While the technology is reliable for detecting alarm conditions based on various sensed conditions, it is still possible to defeat a dual sensor motion detector by “masking” the PIR sensor. It is generally understood in the art that the term “masking” refers to placing a stationary object in front of a sensor, covering the sensor with a substance such as tape or paint, or the like. Even placement of a plate of glass or spraying clear varnish or hair spray over an infrared sensor window can be an effective mask. Most often, the PIR sensor is the target of masking since infrared signals are line of sight whereas microwave signals penetrate and bounce off of objects.\nUnderstandably, mask detection is important if high levels of security are to be maintained at all times and various approaches to mask detection have thus been developed. The simplest is to monitor PIR activity and declare a mask condition if loss of activity occurs for a predetermined period of time, although this method is prone to false mask detects since an empty room will cause a mask condition to be indicated. Another approach is to detect a mask condition during the actual act of masking. In dual sensor detectors employing a microwave Doppler sensor, high level microwave signals are generated when a person or moving object comes into close proximity of the sensor. Therefore, items can be readily detected by a microwave Doppler sensor when they are moving into a position that will block the sensor. Unfortunately, however, once moved into position, a stationary object essentially becomes invisible to a microwave Doppler detector. Another approach is to use a near-infrared emitter/detector pair which looks for a reflected beam. A high reflected signal level would indicate a mask condition because of an object being placed in close proximity. However, this approach is costly and has a relatively high power consumption level.\nTherefore, the most reliable approach to mask detection without incurring additional costs in price or power is to use the microwave Doppler sensor to detect close-up events; that is, movement to within approximately eighteen inches of the microwave Doppler sensor. Upon detection of the close-up event, a PIR detection window is opened. If PIR activity is detected during this window, then the mask detection routine ends. Otherwise, if no PIR activity occurs during that time period, a mask condition is declared.\nA serious threat to security still exists, however, when using microwave-based mask detection, since this technology is dependent upon seeing the actual act of masking. Therefore, such technology cannot detect a mask if power is removed from the detector, such as, if a detector loses power while a sensor is masked, or the system is powered down during the daytime, or someone masks the sensor during a power outage. In any of those cases, since the masking has already occurred, the sensor will not give an indication that masking has taken place when it is powered up again. Therefore, a need exists for a system and method for detecting that a sensor has been masked without causing the sensor to declare a false masking condition when power loss occurs in an empty building. The present invention satisfies that need, as well as others, and overcomes the deficiencies found in conventional technology.\nThe present invention pertains to determining if a motion detector is in a masked condition at the time power is applied to the detector. More particularly, the invention detects a situation where a person disconnects power to the detector by, for example, shutting down the power at the electrical panel, then masks the detector, and finally reapplies power.\nBy way of example, and not of limitation, to detect a mask condition in accordance with the present invention the detector is placed into a mask detection state when power is applied. Any infrared motion that is detected after the detector has warmed up and stabilized will terminate the mask detection state. However, if a predetermined amount of microwave sensor activity is detected within the field of view without detection of infrared activity, a mask condition is declared. This method of detecting a mask condition is based on the assumption that a large amount of microwave activity should be accompanied by at least a small amount of infrared activity if the infrared sensor has not been masked. The amount of microwave activity that required to trigger mask detection can be varied based on individual detector characteristics, but needs only be sufficiently large to avoid false mask detection resulting from microwave activity generated from radio transmitters, cellular telephones and other interfering sources.\nAn object of the invention is to detect attempts to bypass a motion detector.\nAnother object of the invention is to provide for reliable mask detection with virtually no additional component cost and virtually no additional power consumption as compared to using a near-infrared emitter/detector pair.\nAnother object of the invention is to determine if the infrared sensor in a motion detector has been masked.\nAnother object of the invention is to detect mask conditions in a motion detector after power up.\nAnother object of the invention is to detect masking of a motion detector occurring during a power outage.\nAnother object of the invention is to enable mask detection in a motion detector for a predetermined period after the motion detector is first powered on.\nAnother object of the invention is to detect masking of an infrared sensor in a motion detector using a microwave Doppler sensor as a trigger device.\nFurther objects and advantages of the invention will be brought out in the following portions of the specification, wherein the detailed description is for the purpose of fully disclosing preferred embodiments of the invention without placing limitations thereon.\nThe invention will be more fully understood by reference to the following drawings which are for illustrative purposes only:\nFIG. 1 is a functional block diagram of a dual-channel motion detector.\nFIG. 2 is a flow chart showing a power-on mask detection method according to the invention for use with the motion detector shown in FIG. 1.\nFIG. 3 is a schematic of an embodiment of a power-on reset circuit for the motion detector of FIG. 1.\nReferring first to FIG. 1, a functional block diagram of a dual sensor motion detector 10 is shown. Detector 10 includes an infrared channel 12 and a microwave channel 14, both of which output analog signals. The infrared channel typically comprises a pyroelectric sensor 16 and an amplifier system 18, while the microwave channel typically comprises a microwave emitter/detector as a Doppler sensor 20, a driver/supervisor circuit 22, and an amplifier system 24. The analog signals from both channels are converted to a digital form by an analog to digital converter (A/D) 26. A microcontroller 28 processes those signals and detects whether an alarm condition exists, and provides an output to an alarm relay 30. Microcontroller 28 typically includes one or more types of memory, such as read only memory or random access memory, for storing processing software and data, and can include A/D converter 26. Those skilled in the art will appreciate that other devices and subsystems could be included, and that the devices and subsystems shown may be interconnected in different ways than shown in FIG. 1.\nIt will be appreciated from the description that follows, that the invention can be implemented in software and/or firmware associated with a detector of the foregoing configuration or any other conventional detector having both infrared and microwave channels. Detector 10 is intended only to be an example of a conventional detector, and the present invention should not be considered as applying only to the detector shown in this example.\nIn general terms, the method of detecting a mask condition is based on the assumption that a large amount of microwave activity should be accompanied by at least a small amount of infrared activity if the infrared sensor has not been masked. It then follows that a predetermined amount of microwave activity without any infrared activity is indicative of a mask condition. It further follows that an unmasked sensor powered up in an empty room will not declare a mask condition since there will not be sufficient microwave activity to indicate a mask condition. And, while a masked sensor powered up in an empty room will also not declare a mask condition in the absence of microwave activity, if an intruder then enters the room, the detector would then declare a mask condition upon seeing the microwave activity generated. Alternatively, if the occupants return to the building after the sensor has been masked, their activity will cause the mask to be detected. Thus, the invention provides a reliable indication that something is wrong in the building without being subject to false mask conditions being declared.\nReferring now to FIG. 2, the steps of detecting a mask condition in accordance with the invention are shown. This method is preferably carried out by programming contained within microcontroller 28, but could be carried out by programming contained within a separate microcontroller. In addition, execution of this programming is preferably concurrent with normal activity and detection routines in the motion detector.\nAt step 100, the invention detects a power-on reset signal that is received by microcontroller 28. A conventional power-on detect circuit such as that shown in FIG. 3 is used to provide a power-on reset signal to the reset input found on most microcontrollers.\nIn the circuit shown in FIG. 3, Vs is the incoming power line to the motion detector, after transient suppression and a reverse polarity protection diode (not shown). Vdd is the regulated power supply voltage operating the microcontroller, and charges the capacitor C1. Initially with capacitor C1 starting out discharged, the reset line goes low and resets the microcontroller. When the charge on capacitor C1 goes above the 3.9 volt threshold of the zener diode CR1, the reset output goes high and allows the microcontroller to begin operation. If Vdd drops during operation, diode CR2 allows for quick discharging of C1 so that brown-outs can be quickly detected.\nNext, at step 102, the system waits for approximately sixty seconds to allow the amplifiers in the detector to stabilize. In addition, a power-on detect flag is set during this initialization period. This flag is used to the indicate that we are in a power-on mask detection state, so that the power-on mask detect routine is executed every time the alarm processing code runs through a new cycle. In other words, the power-on mask detect routine runs in parallel with the alarm processing code.\nAfter initialization, at step 104 the infrared sensor is tested to determine if any infrared activity has been detected. If so, the power-on detect flag is reset at step 106 and the system returns to normal operation at step 108. Since infrared activity was detected, there is no need to continue to evaluate whether a power-on mask condition exists. By clearing the power-on detect flag, the power-on mask detect routine will not execute the next time the alarm processing code runs through a new cycle.\nIf infrared activity was not detected at step 104, then at step 110 the microwave Doppler sensor is tested for a predetermined amount of activity. Using the detector configuration shown in FIG. 1, the threshold is approximately eight events in an approximately three-second moving window, although the window duration and threshold amount of microwave activity required to occur within that window can be varied based on individual detector characteristics. The threshold should, however, be sufficiently high as to avoid false mask detection resulting from microwave activity generated from radio transmitters, cellular telephones, movement in an adjacent room, and other interfering sources. In other words, the goal is to choose a threshold that detects that there is actually motion in the room being protected.\nIf the threshold amount of microwave activity is detected, at step 112 an infrared detection timing window is opened. Preferably this window is approximately fifteen seconds. A shorter widow results in faster mask detection, while a longer window results in higher false mask immunity. If infrared activity is detected within that window at step 114, the mask detection state is cleared at step 116, the power-on detect flag is cleared at step 106, and the system returns to normal operation at step 108. Alternatively, if no infrared activity was detected at step 114, the elapsed time is tested at step 118. If the window time period has not been exceeded, the infrared sensor continues to be tested and, if no infrared activity is detected when the window period has elapsed, a mask detect condition is declared at step 120.\nIt will be understood that the operable software or code for implementing the present invention may be written in various programming languages for various platforms using conventional programming techniques. Accordingly, the details of the operations code are not presented herein.\nAccordingly, it will be seen that this invention provides for reliable mask detection initiated by a power-on event. Although the description above contains many specificities, these should not be construed as limiting the scope of the invention but as merely providing illustrations of some of the presently preferred embodiments of this invention. Thus the scope of this invention should be determined by the appended claims and their legal equivalents.\n|Cited Patent||Filing date||Publication date||Applicant||Title|\n|US4242669 *||4 May 1979||30 Dec 1980||B. A. Security Systems Limited||Passive infrared intruder detection system|\n|US5499016 *||17 Feb 1993||12 Mar 1996||Aritech B.V.||Intrusion alarm system|\n|US5581237 *||26 Oct 1994||3 Dec 1996||Detection Systems, Inc.||Microwave intrusion detector with threshold adjustment in response to periodic signals|\n|US5796353||11 Apr 1995||18 Aug 1998||Pyronix Limited||Fault monitoring event detection device|\n|GB2308482A||Title not available|\n|Citing Patent||Filing date||Publication date||Applicant||Title|\n|US6351234 *||15 May 2000||26 Feb 2002||Digital Security Controls Ltd.||Combination microwave passive infrared motion detector with anti-masking evaluation|\n|US6402045 *||18 Jun 1998||11 Jun 2002||Totalförsvarets Forskningsinstitut||Method of generating a liquid mist|\n|US7256376 *||30 Mar 2005||14 Aug 2007||Optex Co., Ltd.||Microwave sensor and mutual interference preventing system between microwave sensors|\n|US7616109 *||9 Mar 2006||10 Nov 2009||Honeywell International Inc.||System and method for detecting detector masking|\n|US7671739||7 Mar 2007||2 Mar 2010||Robert Bosch Gmbh||System and method for implementing ranging microwave for detector range reduction|\n|US7679509||7 Mar 2007||16 Mar 2010||Robert Bosch Gmbh||System and method for improving infrared detector performance in dual detector system|\n|US7705730||7 Mar 2007||27 Apr 2010||Robert Bosch Gmbh||System and method for improving microwave detector performance using ranging microwave function|\n|US8164437||2 Feb 2006||24 Apr 2012||Pyronix Limited||Detection apparatus|\n|US8232909||30 Sep 2009||31 Jul 2012||Cooper Technologies Company||Doppler radar motion detector for an outdoor light fixture|\n|US8319638 *||14 Nov 2007||27 Nov 2012||Honeywell International Inc.||Motion detector for detecting tampering and method for detecting tampering|\n|US8410922 *||23 Nov 2010||2 Apr 2013||The Watt Stopper Inc.||Motion sensor with ultrasonic modulation|\n|US8451135||17 Aug 2009||28 May 2013||Robert Bosch Gmbh||Anti-masking system and method for motion detectors|\n|US8519883||30 Jul 2012||27 Aug 2013||Cooper Technologies Company||Adjusting the sensitivity of a PIR sensor or a doppler radar sensor disposed within a light fixture|\n|US8629771 *||25 Aug 2010||14 Jan 2014||John Anderson||Proximity sensors|\n|US20050236404 *||30 Mar 2005||27 Oct 2005||Masatoshi Tsuji||Microwave sensor and mutual interference preventing system between microwave sensors|\n|US20070210911 *||9 Mar 2006||13 Sep 2007||Honeywell International, Inc.||System and method for detecting detector masking|\n|US20080165002 *||26 Dec 2005||10 Jul 2008||Optex Co., Ltd.||Microwave Sensor|\n|US20080218339 *||7 Mar 2007||11 Sep 2008||Gregory Royer||System and method for improving microwave detector performance using ranging microwave function|\n|US20080218340 *||7 Mar 2007||11 Sep 2008||Gregory Royer||System and method for improving infrared detector performance in dual detector system|\n|US20080218341 *||7 Mar 2007||11 Sep 2008||Gregory Royer||System and method for implementing ranging microwave for detector range reduction|\n|US20090303069 *||17 Aug 2009||10 Dec 2009||Bosch Security System , Inc.||Anti-masking system and method for motion detectors|\n|US20100109934 *||30 Sep 2009||6 May 2010||Cooper Technologies Company||Doppler radar motion detector for an outdoor light fixture|\n|US20100283611 *||14 Nov 2007||11 Nov 2010||Honeywell International, Inc.||Motion detector for detecting tampering and method for detecting tampering|\n|US20110050446 *||25 Aug 2010||3 Mar 2011||Guidance IP, Ltd.||Proximity sensors|\n|US20110169629 *||2 Feb 2006||14 Jul 2011||Pyronix Limited||Detection Apparatus|\n|US20120130511 *||23 Nov 2010||24 May 2012||Jon Null||Motion sensor with ultrasonic modulation|\n|WO2006082405A1 *||2 Feb 2006||10 Aug 2006||Pyronix Limited||Detection apparatus|\n|U.S. Classification||340/506, 340/522, 340/567, 340/554, 340/552|\n|International Classification||G01P13/00, G08B13/22, G08B13/18, G08B13/191, G01S13/86, G01V11/00, G08B29/04, G08B13/24, G08B13/19|\n|Cooperative Classification||G08B29/14, G08B13/2494, G08B13/191|\n|European Classification||G08B29/14, G08B13/191, G08B13/24C2|\n|18 Jun 1999||AS||Assignment|\nOwner name: C & K SYSTEMS, INC., CALIFORNIA\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SPROUSE, WILLIAM T.;REEL/FRAME:010025/0820\nEffective date: 19990406\n|29 Jun 2004||FPAY||Fee payment|\nYear of fee payment: 4\n|17 Jul 2008||FPAY||Fee payment|\nYear of fee payment: 8\n|25 Jul 2012||FPAY||Fee payment|\nYear of fee payment: 12']	['<urn:uuid:da12b2d3-830a-48f0-8895-579f8aa47a10>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	8	10	2914
86	I heard India has special paths that tigers use to move between forests. Could you explain what these tiger corridors are and how they connect different parts of the country to help protect tigers?	India has 32 major tiger corridors that have been mapped by the National Tiger Conservation Authority and the Wildlife Institute of India. These corridors connect various tiger reserves across four major regions: Shivalik Hills & Gangetic Plains, Central India & Eastern Ghats, Western Ghats, and North East. For example, the Rajaji-Corbett corridor in Uttarakhand, the Kanha-Pench corridor connecting Madhya Pradesh and Maharashtra, and the Kaziranga-Karbi Anglong corridor in Assam. These corridors are crucial for tiger conservation as they allow tigers to move safely between protected areas. However, these corridors face challenges from human activities, including hunting by traditional communities in corridor areas, which can threaten tigers moving between protected habitats.	['Context: The Ministry of Environment, Forest & Climate Change has informed about the Tiger corridors in Country in Lok Sabha.\nPrelims: Current events of national and international importance.\nMains: GS III-\n- Conservation, environmental pollution and degradation, environmental impact assessment Disaster and disaster management.\nTiger corridors in India:\n- The National Tiger Conservation Authority in collaboration with the Wildlife Institute of India has mapped out 32 major corridors across the country.\n- These are operationalized through a Tiger Conservation Plan, mandated under section 38V of the Wildlife (Protection) Act, 1972.\nThe list of macro/landscape level tiger corridors are as under:\n|Shivalik Hills & Gangetic Plains||(i) Rajaji-Corbett||Uttarakhand|\n|(ii) Corbett-Dudhwa||Uttarakhand, Uttar Pradesh, Nepal|\n|(iii) Dudhwa-Kishanpur Katerniaghat||Uttar Pradesh, Nepal|\n|Central India & Eastern Ghats||(i) Ranthambhore-Kuno-Madhav||Madhya Pradesh, Rajasthan|\n|(ii) Bandhavgarh-Achanakmar||Madhya Pradesh, Chhattisgarh|\n|(iii) Bandhavgarh-Sanjay Dubri-Guru Ghasidas||Madhya Pradesh|\n|(iv) Guru Ghasidas-Palamau-Law along||Chhattisgarh & Jharkhand|\n|(v) Kanha-Achanakmar||Madhya Pradesh, Chhattisgarh|\n|(vi) Kanha-Pench||Madhya Pradesh, Maharashtra|\n|(vii) Pench-Satpura-Melghat||Madhya Pradesh, Maharashtra|\n|(viii) Kanha-Navegaon Nagzira-Tadoba-Indravati||Madhya Pradesh, Maharashtra, Chhattisgarh, Andhra Pradesh|\n|(ix) Indravati-Udanti Sitanadi-Sunabeda||Chhattisgarh, Odisha|\n|(xi) Nagarjunasagar-Sri Venkateshwara National Park||Andhra Pradesh|\n|Western Ghats||(i) Sahyadri-Radhanagari-Goa||Maharashtra, Goa|\n|(ii) Dandeli Anshi-Shravathi Valley||Karnataka|\n|(v) Nagarahole-Bandipur-Mudumalai-Wayanad||Karnataka, Kerala, Tamil Nadu|\n|(vi) Nagarahole-Mudumalai-Wayanad||Karnataka, Kerala, Tamil Nadu|\n|(vii) Parambikulam-Eranikulam-Indira Gandhi||Kerala, Tamil Nadu|\n|(viii) Kalakad Mundanthurai-Periyar||Kerala, Tamil Nadu|\n|North East||(i) Kaziranga-Itanagar WLS||Assam, Arunachal Pradesh|\n|(ii) Kaziranga-Karbi Anglong||Assam|\n|(v) Kaziranga-Papum Pane||Assam|\n|(vi) Manas-Buxa||Assam, West Bengal, Bhutan|\n|(vii) Pakke-Nameri-Sonai Rupai-Manas||Arunachal Pradesh, Assam|\n|(viii) Dibru Saikhowa-D’Ering-Mehaong||Assam, Arunachal Pradesh|\n|(ix) Kamlang-Kane-Tale Valley||Arunachal Pradesh|\n|(x) Buxa-Jaldapara||West Bengal|\nFurther, a 3 pronged strategy to manage human-tiger negative interactions has been advocated as follows:-\nMaterial and logistical support:\n- Funding support through the ongoing Centrally Sponsored Scheme of Project Tiger, is provided to tiger reserves for acquiring capacity in terms of infrastructure and material, to deal with tigers dispersing out of source areas.\n- These are solicited by tiger reserves through an Annual Plan of Operation (APO) every year which stems out from an overarching Tiger Conservation Plan (TCP), mandated under Section 38 V of the Wildlife (Protection) Act, 1972.\n- Inter alia, activities such as payment of ex-gratia and compensation, periodic awareness campaigns to sensitize, guide and advise the general populace on man-animal conflict, dissemination of information through various forms of media, procurement of immobilization equipment, drugs, training and capacity building of forest staff to deal with conflict events are generally solicited.\nRestricting habitat interventions:\n- Based on the carrying capacity of tigers in a tiger reserve, habitat interventions are restricted through an overarching TCP.\n- In case tiger numbers are at carrying capacity levels, it is advised that habitat interventions should be limited so that there is no excessive spillover of wildlife including tigers thereby minimizing man-animal conflict.\n- Further, in buffer areas around tiger reserves, habitat interventions are restricted such that they are sub-optimal vis-à-vis the core/critical tiger habitat areas, judicious enough to facilitate dispersal to other rich habitat areas only.\nStandard Operating Procedure (SOPs):\n- The National Tiger Conservation Authority has issued the following three SOPs to deal with man-animal conflict which are available in public domain:\n- To deal with emergency arising due to straying of tigers in human-dominated landscapes\n- To deal with tiger depredation on livestock\n- For active management towards rehabilitation of tigers from source areas at the landscape level.\nThe National Tiger Conservation Authority (NTCA):', 'By creating alternative livelihoods in India, tigers and humans may live more peaceful lives\nThe following was written by Wildlife Trust of India’s Jose Louies, recounting my recent visit to Central India where the hunter has become the hunted. --KA\nLike a warrior displaying old wounds, a young man shows us the months-old scars on his body.\n“I was injured on my thigh and back, and my left arm had a deep gash that started bleeding profusely within few seconds of the attack,” he explains.\nOnly he is no war hero.\nHe is a hunter—by birth and by nature—from a central Indian community settled a few kilometres away from a forest where threatened tigers roam. He is recounting an attack from a wild boar that occurred during a trip into the fringes of the forest.\nThey took traps, nets, snares and deadly spears, with which someone stabbed the boar to death that day.\n“This is crazy,” Kelvin Alie, director of the International Fund for Animal Welfare’s wildlife trade programme, mutters to himself while taking pictures of various hunting equipment that was being shown to us. “I can’t believe this.”\nA day later, Kelvin and I are out together in the forest again. Sitting silently in our jeep, we hear alarm calls from prey announcing the presence of a predator in the bushes around us. I can see spotted deer running for cover.\nWe know a tiger is nearby.\nShe walks down into the nallah, her stripes playing hide and seek with the sun filtering through the trees. A spotted deer stands frozen a few meters away, partially concealed behind a teak tree avoiding the tigress’ eyes.\n“This is the Baghinnalla tigress,” the guide whispers to us.\nHer coat glows in the morning sun as she stands on the slope. She looks back into the bush, head up, ears forward. That’s when we see the first cub, and then the second and then the third walk out to join her.\nWhat more can one ask for on a trip to see tigers in the wild?\nThe tigress is trying to cross the road to the hillock on the other side, but there are about eight safari vehicles on the road. The guides are quick to silence the excited crowd and the drivers of the vehicles ensure that there is safe passage for the feline family.\nShe crosses the road between vehicles and into the boulders on the hillock she goes. More alarm calls.\n“A tigress and three cubs on my first visit,” Alie says, smiling from ear to ear, examining the photographs we just took. “Man...that’s a good one.” We are joined by Dr. Ian Robinson, Vivek Menon, and Dr. Rahul Kaul in the jeep. Even with all the years of experience in the wild between them, today is an unforgettable day for each of us.\nI sit in the rear of the jeep browsing through my camera when suddenly I am looking at the previous day’s pictures. One is of a young man holding his spear and looking at me.\nThe community we visited yesterday is just a few miles away. Some of the hunters would be on the move now, as they also have families to feed. When they hunt, however, there will be no alarm calls from the animals as they move in for the kill.\nThis community was branded as a ‘criminal community’ during the Colonial period and still carries the stigma of that label. Cast off by mainstream society, community members either hunt wild animals or brew illicit alcohol for a living. (While all the men show us hunting equipment in the courtyard, the women and children are involved in transferring the hooch from bottles into larger jerry cans.) Their alleged involvement in acts of theft adds to their notorious image and they live as outcasts in many ways.\nThe fifty-something village headman tells us farmers call on them for assistance with nuisance wildlife that stray into the fields from the forest in search of food and water. “We can’t predict what we will eventually trap,” he explains.\n“It could be a wild boar, a spotted deer or even a leopard or tiger. Anything can be trapped in the snares that we conceal in animal footpaths. Once the animal is trapped, we close in with the spears.”\nHe gives me a snare made of sinew from the wild boar and blue bull. He tells me that the snare is many years old, and it could hold a large animal in place while it was speared to death. “Why don’t you join us for a hunting trip?” he asks. “The way you examine and play with the snare shows that you can be a good hunter.” Oh, the irony.\nAs we walk back to our car, I can read Alie’s mind. “It’s a different world in that village of hunters,” he says.\nThe traditional hunting communities of India are one of the major threats to the conservation of tigers in the wild.\nThey are semi-nomadic in nature, move around the landscape and hunt animals in the forests, fringes and farm lands adjoining the protected areas. They hunt in the corridors that connect critical tiger habitats with each other.\nA transient tiger is bad news for the villagers as they prey on cattle or hunt its prey in the fields and come into conflict with humans. These hunters are welcomed by many of these villagers as they consider these men to be problem solvers.\nThe young village boys carefully wrapping the traps and snares and the tiger cubs walking with their mother are the next generation inhabiting this landscape. They both will hunt for their survival and whenever their paths cross only one of them will live to tell the tale.\nIFAW-WTI is running campaigns in the central Indian landscape to ensure the survival of the big cats in the wild and guard them from poaching. Our team has initiated a project here with the community to explore alternatives to hunting.\nThrough various community interventions and methods, the project team plans to reduce the pressure this hunting community puts on the local wildlife. Through social intervention, we hope to make it easier for future generations of tigers and humans to coexist peacefully.\nFor more information about the IFAW-WTI efforts to protect tigers and people, visit our project page.']	['<urn:uuid:3a9282cc-001a-438c-b38b-c39b9213767e>', '<urn:uuid:47636698-4e8b-45a2-8909-737cd4418024>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T04:56:11.774636	34	110	1596
87	what is difference between freeware and shareware software license types	Freeware is copyrighted software that can be copied and distributed without charge, though modification is prohibited without owner permission. Shareware is similar but requires a purchase fee for permanent use - it's free only for evaluation purposes and typically turns itself off after a trial period (usually 30 days) unless an activation code is purchased.	"['3What are ethics?The rules of right and wrong behavior. Merriam-Webster defines ethics as “the principles of conduct governing an individual or group.”\n4The Three Categories of Ethics Professional- usually defined by your employer of company; these are general guidelinesSocial- defined by societyIndividual- defined by personal heritage and integral family values\n5Information EthicsRules that define right and wrong behavior in the computing professions.The basis of trust and cooperation among workers and organizationsIn most cases, these are defined by the “owner” of the computer.\n6Ethics vs. Laws Ethics often precede laws. Laws exist to protect software developers (copyright and licensing) and users (privacy) and there are punishments for breaking the law.There are no “global punishments” for ethics violations.\n7CopyrightsThe right of an author to copy, resell, lease, lend, rent and or distribute material. Copyrights did not exist when software was first being developed.\n9Copyright ScenarioJill just purchased a new game for her computer. Bill loves the game and wants a copy, but he cannot afford it. Bill asks Jill if he can borrow the CD and load the game on his computer. Is this all right under current copyright laws?\n10Solution….Copyright allows the owner of the copyright to ""distribute or lend"" the program. Jill does not own the copyright; she owns a copy of the program sold to her by the copyright owner. She cannot lend it to Bill to load on his machine. If Jill would read her “License Agreement,” she would know exactly what is right and wrong regarding copying the software.\n11Software LicensingLicense agreements are contracts between the purchaser and copyright owner.Many software companies offer site licenses or special licenses for companies, schools or government institutions.\n12Little Know FactoidWhen you buy software, you\'re not buying the software itself. You\'re buying permission to use the software. The permission you buy is called a license (like a hunting license or a driver’s license) and is defined by a license agreement that comes with the software. Usually the license agreement is printed in tiny type on the back of the box or on a piece of paper shrink-wrapped with the box. I think most people never read it (it\'s full of legalese and usually takes a magnifying glass to see clearly). However, I think most of us have seen the sticker on the box that says something like, ""Breaking this seal means you agree to the terms of the license.""\n13Four Types of Software Licenses public domainfreewareshareware""all rights reserved""\n14Public DomainPublic domain software is not protected by copyright law; it is software that was either created with public funds (and is therefore already owned by the public) or the creator has forfeited these rights to the public. Public domain software is pretty rare and is often unreliable (i.e., poor quality, virus-laden) since nobody\'s keeping track of it. The only public-domain software products I\'ve ever really wanted were a few graphics and fonts.\n15FreewareFreeware is copyrighted software that is licensed to be copied and distributed without charge. Some freeware (Pegasus Mail, Eudora Light, Netscape, Internet Explorer, etc.) is really good stuff, but it\'s not in the public domain. That is, freeware is free only because its license says it is; the software is still under the owner\'s control. Thus, most freeware licenses prohibit the sale or modification of the software without the owner\'s permission, and they often limit the groups that are able to get the software without paying.\n16SharewareShareware is the same as freeware, except that the owner stipulates a purchase fee for permanent use. That is, the software is licensed for copying and sharing without charge, but only for evaluation purposes. Anyone who decides to use the software long-term is to pay a specified fee to the owner. There is usually no official system for collecting these fees; users are expected to pay on their own (honor system) in exchange for documentation, a more powerful version of the software, or some other enticement. More and more shareware is programmed to turn itself off after a certain period of time (30 days is typical) unless the owner buys a special activation code from the copyright owner. Most of the “free” software in the world is actually shareware.\n17Software license with all rights reserved Licensed only for use by purchasers. Almost all the really good stuff (Word, Oracle, Java, PhotoShop, Windows 2000, etc.) is licensed this way. You can\'t legally use it or even possess it without the owner\'s permission, usually defined very carefully in the license agreement.\n18Typical License Agreement You can install and operate the software on only one PC at a time.You can make only one back-up copy of the software.If you give the software to anyone, you also give up permission to use the software.You are not to modify the software in any way.The software is not represented as fit for any particular purpose. (Yep—even if the box says ""Do your taxes faster than ever,"" you cannot legally assume the software is fit for doing your taxes). This is to keep you from suing the company if their software wrecks your life.\n19Software Piracy The illegal copying of software The estimated loss due to piracy is approximately $39 billion per year globally.Conviction on the first offense can result in a fine of up to $250,000 and a jail sentence of up to five years. (Bowyer, Ethics & Computing, 1996)\n20ScenarioA student is working on a class assignment. The student doesn’t have the specific software used at school, so she asks her teacher to let her take a copy of the software home. Is this ethical?\n21SolutionSince the software is licensed to the school, this is not legal or ethical.\n22Ethically and legally, society should recognize that individuals creating “intellectual property” have the right to receive credit and be protected from the incorrect use of their creations.\n23Intellectual Property The result of intellectual activities in the arts, science and industry (Beekman, Computer Confluence, Prentice Hall, 2001) and as such is the property of the creator.\n24Fair Useprovisions that let you paraphrase or quote small portions of someone else’s work, provided that you give proper credit to the author for the material you useIf you don’t give credit, you will probably give at least some of your readers the impression that the material you borrowed originated with you. That’s bad. It’s so bad, it has a name, plagiarism, and it’s a form of fraud.\n25Giving CreditAdd an in-text reference to the author and publication year of the work used right after the borrowed material in the paper.Add a full citation of the work (author, title, publisher, publication year, etc.) at the end of your paper.\n26Electronic Information More and more information is available via CD-ROMs, the Worldwide Web, online databases, and other electronic communication channelsQuoting and citing information is just as important with computer sources as with traditional written works. Even though computers and the Internet allow easy access for cutting and pasting items, it is considered plagiarism when the creator has not given permission and/or been given credit for his/her work.\n27Misuse of InformationWhen using intellectual property, it should be used in the context originally intended and presented correctly so the meaning is not changed from the original intent.Blatant misuse of information in which the user knows he/she is plagiarizing but chooses to use the information anyway is an example of fraud. The Merriam-Webster\'s Collegiate Dictionary defines fraud as ""the act of deceiving or misrepresenting"" (Merriam-Webster, Inc. ""fraud"", 2001). Accidental plagiarism is a form of negligence. Negligence occurs when an individual is unaware or ignorant that he/she has misused intellectual property. Both fraud and negligence are ethically wrong and illegal.\n28ScenarioRalph Williams wrote a paper for his Geography class about the Grand Teton Mountains. He found some information on the web that worked well into his paper. He did not intend to claim the information as his own but he did not mention in his paper where he got the information. Is this okay?\n29SolutionHe should have quoted the material and then reference the site from which the material was taken. This is true even when the material used is found on the World Wide Web.\n30PrivacyInformation can be categorized as that which is public and that which is private. Public information is assumed to be able to be used without permission. Private information cannot be legally used without permission from its owner.\n31A Difficult Balance Private Info. About You grades in specific courses Public Info. About Youfull namehome addressphone numberdebts you owecourt judgments against youand police actions relating to youcourses you’ve takenPrivate Info. About Yougrades in specific coursesmedical records\n32Inappropriate Use of E-mail SpammingFlamingTrolling\n33SpammingBulk, mass, or repeated posting or mailing of substantially identical messages. The emphasis is on the multiple sending, either many copies to one destination, or one copy to many destinations. (Kevin Kadow, Internet Dictionary, ""spam"", 1994 )\n34FlamingA public post or message that expresses a strong opinion or criticism. Flames can be fun when they allow people to vent their feelings, then return to the topic at hand. Others are simply insulting and can lead to flame wars. (Albion.com, Netdictionary, ""flame"", 2000 )\n35Trolling (also known as flame bait) An inflammatory post that is designed to provoke a flame war or flame responses. (Albion.com, Netdictionary, ""flame bait"", 2000 ) Trolling is basically sending messages that contain statements intending to bait an argument or confrontation.\n36Messages that continue after the recipient asks that they stop are defined as a nuisance and are illegal. Nuisance messages that are threatening or intimidating are termed harassment and are even more illegal. It\'s interesting to note that nuisances and harassment are defined by the messages\' receiver, not by the sender. That is, if you ask me to stop sending you messages, I have to even if I don\'t think there\'s anything wrong with my messages.\n37Acceptable Use Policy (AUP) outlines what types of uses are acceptable and which are prohibitedalthough a particular institution’s AUP will be unique, most have the following three categories (see next slides for more info) defining rights and responsibilities in using equipment in a legal and ethical mannerlegal documents that should be understood and followed\n38Personal Computer Files (documents) Storage locationStorage space allocationTypes of files allowedPrintable items and quantity\n39Personal and Private Information Sharable informationand personal file privacyMonitoring on-line activities\n40Ethical Conduct Care and consideration of equipment Respect rights of other usersPolicies concerning games, personal Web browsing, chat, and instant messagingProtect system securitySoftware and plug-in installationUnacceptable topics, words, and graphics for papers, , and Web-page viewing (such as pornography, profanity, terrorism, illegal substances,etc.)\n41ScenarioBen Flying Eagle is finished with his assignment for the day so he wants to get on Yahoo to chat with his friend in another class. The school’s AUP states that chat is not allowed on school computers because too many unsafe situations occur in chat rooms. Ben isn’t planning to use a public chat room, he just wants to have a private conversation with his friend. Is he breaking the school’s AUP if he chats with his friend? Is he breaking the school’s AUP if he uses Instant Messaging instead of Yahoo Chat?\n42SolutionIf the AUP does not differentiate between public and private chat, chatting is not allowed.\n43Code of EthicsMany professional organizations have developed codes of ethics, which have been widely adopted as the “basic” code for many companies and universities. While the professional codes of ethics have slight differences in emphasis, they are in agreement on general principles.\n44Code of Ethics at Colleges and Universities Many higher institutions of learning also maintain a code of ethics. Usually, one of the sections deals specifically to plagiarism of Intellectual Property at the college or university.Examples:Utah State University: Article 5 Section 3 lists the violations of University standards.University of Utah: Section V defines “Academic Dishonesty” including plagiarism.Weber State University: Article IV Section D part 7 mandates appropriate use of computer systems.\n45“We cannot count on the legal system to be a complete and correct guide to moral behavior, either for us as individuals in society or as members of a profession. Nor can we expect the professional codes of ethics to be complete, consistent and correct for all situations. There is no formal monitoring for compliance and little penalty that can be assessed against violators. Goodness cannot be defined through a legalistic enumeration of dos and don’ts. People must be able to use their internal sense of ethics to fill the holes and resolve the conflicts that inevitably occur when following any code of ethics” (Bowyer, Ethics & Computing, Computer Society Press, 1996).\n46Works CitedGomm, Stacie, et al.”Ethics Teaching Module.” Utah State University. 15 December <http://cil.usu.edu/etmodprint.htm>']"	['<urn:uuid:01c0b24e-c8ba-4e14-8fb5-4df099b750a3>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	10	55	2103
88	What scientific evidence exists linking the substitution of unsaturated fats for saturated fats to improvements in cardiovascular health outcomes?	According to an analysis of the Nurses' Health Study that tracked 80,000 initially healthy female nurses, replacing just 5% of calories from saturated fat with calories from unsaturated fat reduces the risk of coronary heart disease by 42%. This substitution was found to be more effective at preventing heart attacks than simply reducing overall fat intake. Additionally, short-term dietary trials demonstrate heart benefits when saturated fat intake is lowered while unsaturated fat intake is increased, showing improvements in blood cholesterol levels and insulin sensitivity.	['By: JoAnn E. Manson, MD, DrPH, and Shari S. Bassuk, ScD\nContributing Editors, Brigham and Women’s Hospital, Harvard Medical School\nIf you saw last month’s news headlines declaring that saturated fat is no longer deemed harmful to your heart, you may be (understandably!) confused. After all, for years, clinicians and scientists have recommended reducing saturated fat for heart health. Is it time to rethink this advice? Hardly. Here’s the deal.\nThe research that sparked the recent news splash was an analysis by Canadian researchers of up to a dozen long-term observational studies of diet that included a total of 90,000 to 339,000 participants from various countries. These study volunteers reported on the foods they typically ate. Researchers then tracked the health of these folks for years, sometimes decades. The analysis found no association between consumption of saturated fat — dairy foods (e.g., cheese, butter, and milk) and meats are two main sources — and future risk of coronary heart disease, ischemic stroke (strokes resulting from a blocked vessel), diabetes, or deaths from cardiovascular disease or all causes. Less surprisingly, the analysis also found that trans fats are harmful to health, with the highest intakes of trans fat linked to a 21% higher risk of coronary heart disease and a 33% higher mortality rate compared with the lowest intakes. The findings were published on August 12 in the medical journal The BMJ.\nUnfortunately, the news coverage of this analysis often missed a key point. It’s not just the amount of saturated fat you eat, it’s also what you replace those calories with (the quality of your overall diet) that affects your health. Most people naturally tend to keep their calorie intake at a constant level over time (scientists call this “calorie preservation”). If they cut saturated fat calories out of their diet, the missing calories have to come from somewhere else (“calorie substitution”). People with lowered saturated fat intake may replace those calories with other unhealthy foods such as refined carbohydrates (e.g., white bread, white rice) or sugary beverages. So although they are eating less fat, their overall diet is no better — and may even be worse — than people who don’t try to limit saturated fat. .\nNumerous studies show that substituting unsaturated fat — found in fish, nuts, and plant oils — for saturated fat improves health. For example, a careful analysis of observational findings from the Nurses’ Health Study, in which my colleagues and I (JEM) tracked 80,000 initially healthy female nurses for many years, suggested that replacing just 5% of calories from saturated fat with calories from unsaturated fat cuts risk of coronary heart disease by 42% and is more effective at preventing heart attacks than simply reducing overall fat intake. Short-term dietary trials also show heart benefits of lowering saturated fat intake while boosting unsaturated fat intake, including improvements in blood cholesterol levels and insulin sensitivity.\nThe most harmful type of dietary fat is trans fat, also known as partially hydrogenated vegetable oil. These fats are a double whammy: they boost the “bad” LDL cholesterol and lower the “good” HDL cholesterol. New government regulations are reducing the presence of artificial trans fats in the food supply, but such fats are still found in many products. What’s the take-home message? The type of fat does in fact matter, so choose foods with healthy unsaturated fat (fish, nuts, and most plant oils), limit foods high in saturated fat (butter, whole milk, cheese, coconut and palm oil, and red meats), and try to avoid foods with trans fat. Achieving the last goal can be tricky. In supermarkets, check package labels carefully. The best way to tell if trans fat is present is to read the ingredient list; if the phrase “partially hydrogenated oil” appears, then trans fats are indeed lurking.\nMany experts and professional societies, including the American Heart Association, advise a dietary pattern that (1) emphasizes vegetables (richly colored vegetables, including dark leafy greens, are best, and white potatoes don’t count), fresh fruits, and whole grains (whole-grain cereals, breads, rice, and pasta); (2) includes fish, beans, nuts and seeds, poultry, low-fat dairy products, and non-tropical plant oils (such as canola or olive oil, but not coconut or palm oil); and (3) limits sweets, sugary drinks, and red meats. Well-known examples are the Mediterranean and DASH diets. People who eat such diets have consistently had much better health outcomes than those who do not.']	['<urn:uuid:dc46bd42-057c-42e5-9273-512f463e1347>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	19	84	736
89	Can both aeonium and lettuce plants grow well in shade?	Neither plant grows optimally in full shade, but they have different shade tolerances. Some lettuce varieties like butterhead and crisphead can grow adequately in partial shade, preferring morning sun and afternoon shade. Aeoniums, however, can only tolerate shade for a few hours and many types will not develop their full leaf colors without full sun exposure. Both plants generally need at least 6 hours of direct sunlight for optimal growth.	"['What Is the Aeonium Plant?\nSucculents can be interesting and fun plants to grow, thriving outdoors in areas with dry summers and warm winters. In regions where winters are cool, they appreciate summer sun outdoors and then can grow indoors as houseplants when weather cools. Aeonium plants (Aeonium spp.) make up a group of succulents that are native to Africa and typically have leaves radially arranged in rosettes.\nAlthough dozens of plants belong to the genus Aeonium, they share a number of common characteristics. All have leafless stems topped by leaves that are radially arranged in a structure called a rosette. Leaves in the plant\'s rosettes are thick and have a waxy outer covering, and stems tend to be thick and resemble branches on larger specimens. Some large aeoniums can reach a mature height of 5 feet or more, while others are compact and form mounds of short-stemmed rosettes. All aeoniums are frost-tender, although several can tolerate a few hours of freezing temperatures. Generally, they are suitable for growing outdoors in U.S. Department of Agriculture plant hardiness zones 9 through 11.\nLight, Soil and Potting\nAeoniums thrive in full sun and need at least six hours of full sun each day. They can tolerate shade for a few hours, but many types will not develop their full leaf colors without full sun. They generally prefer a sandy soil mix with good drainage appropriate for cacti. When potted, they do best in terra cotta pots that allow soil to dry readily between waterings. Aeoniums cannot tolerate soil that remains moist for long periods; stems will soften and eventually rot when plants are kept under wet or soggy conditions.\nFeeding and Maintenance\nMost aeoniums grow actively during fall and spring, and respond to fertilizing every four to six weeks with a balanced, 10-10-10 formula while growing. When plants slow their growth during winter and summer, you should withhold fertilizer and cut back on watering. Aeoniums are unusual because rosettes tend to die after they flower. You can either cut these spent rosettes back when they fade, or to prevent flowering, take cuttings of terminal stems and plant these in moist soil. This helps control the size of your plant and stops it from blooming, preventing dieback of terminal rosettes and helps increase the size of your aeonium planting.\nDozens of aeonium species exist, each containing many cultivars. Some of the best choices include the black rose aeonium (A. arboreum ""Zwartkop""), which has nearly black rosettes and can reach a height of 5 or 6 feet in warm climates; purple aeonium (A. arborerum ""Atropurpureum""), with purple-red rosettes on a 2- to 3-feet tall plant; and green rose buds (A. aureum ""Green Rose Buds""), a plant that forms up to 50 rosettes that are 4- to 10-inches wide. They have an apple green color on a compact plant that tolerates partial shade. Some aeoniums are multi-colored, such as Haworth\'s aeonium (A. haworthii ""Variegated""), with red-edged green leaves, and the cultivar called sunburst aeonium (A. arboreum ""Luteovariegatum""), a cold-tolerant, bushy plant with green leaves outlined in pale yellow.\nJoanne Marie began writing professionally in 1981. Her work has appeared in health, medical and scientific publications such as Endocrinology and Journal of Cell Biology. She has also published in hobbyist offerings such as The Hobstarand The Bagpiper. Marie is a certified master gardener and has a Ph.D. in anatomy from Temple University School of Medicine.', 'Lettuce plant is cool-weather plant that does well when exposed 6 to 8 hours of sunlight. However, how many hours does lettuce need sunlight may vary depending on the variety you choose to grow.\nRomaine Lettuce variety is most heat-tolerant that prefers sunnier location. But, they do grow in cooler climates and prefers moist-rich growing medium.\nSparx, Coastal Star and Jericho are cultivars of Romaine lettuce that needs 8 hours of sun exposure.\nWhile other lettuce varieties like butterhead and crisphead lettuce like morning full sun and shade in afternoon.\nAs a gardener you must remember that sunlight exposure for a plant differs depending on various factors.\nHow Much Sunlight Does Lettuce Need to Grow?\nSome Lettuce varieties are more heat-tolerant than others. Additionally, environmental factors like elevation and air temperature can affect the amount of sun your plants need to stay healthy.\nRomaine lettuce is one of the most heat tolerant lettuce, that can grow in full sun. But, it too prefers moist and nutrient-rich growing mediums.\nThis cultivar of lettuce needs 8 hours of direct sunlight to thrive.\nSome types of lettuce, like butterhead and crisphead, like to be in the sun in the morning and then have shade in the afternoon.\nLettuce needs 6 to 8 hours of direct sunlight each day. Too much sun can hurt the lettuce, making it hot and stressed.\nCheck on your plants often. If it is too hot, give them shade during the middle of the day.\nLettuce needs some shade to keep the leaves from getting too much sun and wilting.\nGrowth stages of lettuce also influence the amount of sun exposure the plant needs to grow.\nBefore that lets, understand about full sun, partial sun and direct sunlight.\nAlso Read: How to: Growing Lettuce In Raised Beds?\nWhat is full sun?\nFull sun means that the plant gets sunshine directly for at least 6 hours each day.\nWhat is partial sun?\nPartial sun means a plant gets some direct sunlight and some light that is not from the sun. The plant will get around 4 to 6 hours of sunlight in total.\nWhat is direct sunlight?\nWhen the sun’s light shines on a plant without anything in the way, that is called direct sunlight.\nLettuce needs a lot of sunlight when it is just starting to grow. It needs 8 hours of direct sunlight every day. When the lettuce matures, it can get by with 6-8 hours of sun each day.\nMid-growth and Mature Plants\nWhen lettuce is growing, it needs 6 hours of direct sun each day. As it grows older, it can handle less sunlight – usually between 4 and 6 hours daily, based on the type of lettuce.\nTo help your plants grow and make food, you need to watch them carefully. Change how much sun they get if needed.\nLettuce does not need a lot of sun to grow. You can start growing lettuce indoors in an area that gets some morning sunlight.n\nWhen your lettuce plant grows tall enough, move it to a raised bed or container.\nCheck this out: How to Grow Lettuce Microgreens?\nCan you grow lettuce in partial shade?\nYes, you can grow lettuce in a spot that does not have direct sunlight. But it won’t do as well as if it had 6 to 8 hours of direct sunlight each day.\nIf your area does not get enough sunlight, you can try using special lights that help plants grow. You can also use extra light to help with natural sunlight. You can also grow lettuce that does not need as much sun, Butterhead and Crisphead lettuce are such varieties.\nDo lettuce need full sun to grow?\nDifferent types of lettuce need different amounts of sunlight. For example, Romaine lettuce needs 8 hours of direct sun to grow well. But Butterhead and Crisphead lettuces do not need as much sunlight – they can do okay with only some shade.\nTo help your plants grow and give you a good harvest, watch them carefully. Change how much sunlight they get when needed.\nCan I Grow Lettuce in Indirect sunlight?\nYes, you can grow lettuce without direct sunlight. But it won’t grow as well. It needs 6 to 8 hours of direct sunlight to be healthy and make lots of lettuce.If your plant doesn’t get enough sun, you can use special lights to help it grow. Or you can add extra light to the natural sunlight it gets.\nCan I Grow Lettuce in Shade?\nSome kinds of lettuce can do okay in a little shade. But, it usually needs at least 6 to 8 hours of direct sunlight each day to grow and make lots of leaves.\nWhat happens when lettuce gets too much sun exposure?\nToo much sun on lettuce can make it droop and turn brown. Check your lettuce often to make sure it does not get too much sun. If the sun is very bright, you can give your plants some shade during the hottest part of the day.\nIf the lettuce is getting a lot of sun, give it some shade. That way the leaves won’t get too hot or wilt. Make sure the soil has just the right amount of water. Make sure you have a way for extra water to drain away so it doesn’t just stay in one spot.\nCan I grow lettuce indoors?\nYou can grow lettuce inside with lights. But it won’t be as good as if you grow it outside in the sun. It is best to also use window or skylight light to help your indoor lettuce grow.\nSigns that lettuce is not getting enough sunlight\nIf your lettuce does not get enough sunlight, it may not grow very well. The leaves might also turn yellow.\nYour plants may get weak if they don’t have enough sun. Look at your plants often to make sure they are getting enough sunlight. If not, adjust how much sun your plants get so that they stay healthy and grow well.\nLettuce Growing Tips\n1. Start growing your lettuce in a pot inside. When the plants are tall enough, you can move them outside or put them in a raised bed.\n2. For the best growth and yield, make sure to give your plant 6-8 hours of sunlight each day.\n3. If your region does not get enough sunlight, you can use lights to make it brighter.\n4. If you do not get enough sunshine, plant lettuce that can grow in the shade like Butterhead or Crisphead.\n5. Watch your plants closely. If needed, give them more or less sun to help them grow and produce the best yield.\n6. Water your lettuce plant regularly to avoid under-watering.\n7.Harvest your lettuce when the leaves are soft and a bright green color.\nLettuce needs lots of sunshine to grow. Some types need 8 hours of direct sun, while others can do okay with some shade.\nWatch your plants closely. Make sure they get the right amount of sun.For those seeking an alternative to the more sun-dependent type of lettuce, Butterhead and Crisphead are great choices.']"	['<urn:uuid:f4fef257-c053-4976-a403-cf65e0115b9b>', '<urn:uuid:efb400d5-6b6f-470a-a182-fa282a118f72>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:56:11.774636	10	70	1757
90	which is older sauce harissa or moretum pasta seasoning	Moretum is older, as it dates back to ancient Roman times and is described in verses attributed to Virgil. In comparison, harissa is a North African condiment found mainly in Tunisia, Morocco and Algeria that is still commonly used today.	['Harissa is a favourite spicy condiment from North Africa found mainly in Tunisia, Morocco and Algeria. Tunisian cuisine commonly includes all kinds of exciting variations of harissa in predominantly chicken, seafood and lamb dishes. Not as scarce as you might think in the rest of the world, harissa paste can be bought readymade in tubes and tubs in certain stores and delis. However, a good base harissa paste is really quick and easy to make and it’s so much tastier than the store bought versions. Depending on your taste preferences and those of your guests you can make it as hot, sweet or smoky as you like. What is important is your choice of chilli. The type of chilli you use will impact flavour and kick. For a very mild harissa you can use roasted red bell peppers instead of chilli. We use bell peppers with a touch of chilli to give ours a light zing so as not to overpower diners with spice-sensitive palettes.\nRecipe by: GOLD Restaurant\nServes: 8 (starter)\nPreparation: 30 minutes (plus two hours for marinating)\nCooking: 30 minutes (frying)\nT = tablespoon\nt = teaspoon\n- 2 medium red peppers and 1 red chilli OR 6 red chillies\n- 1 ½ t caraway seeds\n- 1 ½ t coriander seeds (or fresh)\n- 3 T garlic, finely chopped\n- 3 T extra virgin olive oil for paste\n- 1 ½ cups vegetable oil for frying\n- 1 cup boiling water to soak chillies (chilli-only recipe)\n- ¼ cup boiling water to soak chilli (red pepper and chilli recipe)\n- ½ cup freshly squeezed lemon juice\n- 16 medium-sized chicken wings (or small drumsticks)\n- Salt to taste\nTo make the harissa paste:\nIf you are following the spicier chilli-only recipe, place the chillies in a bowl. Add a cup of boiling water and allow them to soak for 20 minutes or so until soft.\nRed pepper and chilli recipe\nIf you are following the red pepper and chilli recipe, slice your peppers lengthways into long quarters. Remove the stems and seeds. Place the quarters in a skillet for 2 to 3 minutes on a medium heat and dry roast to char slightly on both sides. Remove peppers, allow them to cool and chop finely. Soak your chilli in a small bowl with ¼ cup of boiling water for 20 minutes or so until soft.\n- Place the caraway, coriander and cumin spices in a skillet and dry fry over a low to medium heat for a minute, swirling them to prevent burning. You can tell they’re ready after 3 minutes or so when they become fragrant.\n- Remove the skillet from the heat and transfer the roasted spices to a grinder and blitz (or mortar and pestle and grind) until powdery.\n- Drain the chilli/chillies and reserve the liquid for later.\n- Remove and discard the seeds and stems of your chillies. If you are following the red pepper and chilli recipe you will have already done this in Step 1.\n- Combine the chillies and/or red peppers and all the dry ingredients in the bowl of a food processor (or use your mortar and pestle). Put the food processor on low and slowly pour in the extra virgin olive oil to form a puréed, thick paste. If you want your paste a little thinner add some of the reserved chilli water until you get the consistency you want.\nTo prepare and cook the chicken:\n- Place your wings or drumsticks in a bowl and season with salt. Add your harissa paste to the bowl and rub it into the chicken with your hands. Cover and leave to marinate for at least two hours.\n- Fry the wings or drumsticks in a pan in hot oil on a medium heat until the skin on both sides is crispy.\n- Serve hot.\n- Harissa paste can pre prepared and kept in the refrigerator for up to three weeks. In fact, the flavour deepens if left for a day or two. If you have left over paste keep it in a sealed jar with a thin layer of extra virgin olive oil poured conservatively on top before sealing. Every time you use the harissa, add a fresh thin layer of olive oil.\n- You can also add to your harissa paste, fresh or dried mint, fresh cilantro, tomatoes, cayenne pepper or paprika. It all depends on your personal preference. Have fun creating your own signature brand of harissa paste.', 'Trofie with Genoese pesto (La Cucina Italiana)\nBy Rosario Scarpato\nThere’s no doubt, it’s the most loved raw sauce of the world and, as such, it’s just as famous as mayonnaise, if not, even more so. ;We’re talking about Pesto, about the only genuine one; the Genoese one (not ‘alla genovese’). Basil, garlic, pine nuts, Grana Padano or Parmigiano Reggiano and Pecorino cheeses, extra virgin oil and a few granules of sea salt. All the other diverse variations floating around the world are nothing more than unsuccessful bogus or aberrations of the original. Pesto is an ageless benchmark and a contemporary symbol of Italian cooking around the world. It’s enough to think that for dressing pasta only tomato sauce is used more.\nIn the name of Mortar & Pestle\nAlready in 1876, ‘pesto’ was entered by Giovanni Casaccia in his Genoese Italian dictionary as a word autochthonous of Genoa. The word comes from ‘pestare’, to crush something with a pestle to reduce it to powder, a mash or to the thinnest of layers. Therefore, pesto is also known as ‘battuto genovese’ (Genoese mince or mash). From ‘pestare’ also comes the word ‘pestle’ (‘pestillium’ in Latin) that together with the mortar is the utensil used for making this sauce; a wooden pestle, that is, of hard, compact wood, such as boxwood or pear and a mortar of white marble, of Carrara, unpolished inside. The word ‘mortar’ is derived from the Latin ‘mortarium’, a recipient in which ingredients are minced or mashed, historically not only used in kitchens but also in traditional pharmacies. In the proto-kitchens of many people of the Earth, similar utensils are found, although made out of different materials, such as the ‘molcajete’ and the ‘metate’ of Central America and the Japanese ‘suribachi’.\nThe ancestors: moretum and agliata\nGenoese pesto probably descends from the ‘moretum’ of the ancient Romans, a green paste obtained from cheese, garlic and herbs, the preparation of which is described, moreover, in a verse attributed to Virgil. Pesto, as we know it today, was not well known in the times of Cristoforo Colombo (1451 -1506), the world’s best known Ligurian. But in the Middle Ages, there was a sauce, defined plebeian by some authors, which can be considered, in some manner, to be the predecessor of pesto. It was called ‘agliata’, simply a mash of walnuts and garlic. For centuries, the latter ingredient has occupied a crucial place in the nutrition of the Ligurians, especially for those who went to sea. And there were many who did, for Genoa and Liguria have had ancient maritime traditions. These seafarers ingested great quantities of it since they believed it warded off illnesses and infections during the long voyages in conditions of extreme hygienic precariousness.\nThe first recipes\nMashed garlic is mentioned in the documents of the City of Genoa of the 17th century, while the recipe for true pesto starts to appear only in the 19th century. During the first half of the latter appear recipes that do not contemplate pine nuts and in 1863 Giovanni Battista Ratto published La Cuciniera Genovese, considered to be the first and most complete book on the gastronomy of the Region of Liguria and in which the recipe for pesto, with pine nuts, is the following: “Take a clove of garlic, basil (‘baxaicö’) or, when that is lacking, marjoram and parsley, grated Dutch and Parmigiano cheese and mix them with pine nuts and crush it all together in a mortar with a little butter until reduced to a paste. Then dissolve it with good and abundant oil. Lasagne and troffie [Liguria kind of gnocchi] are dressed with this mash, made more liquid by adding a little hot water without salt.”\nThe presence of Dutch cheese instead of pecorino should be of no surprise; first of all, various recipes of that time mention a generic ‘cacio’ (cheese), and then because Gouda was plentiful in Genoa since the city’s maritime commerce with Northern Europe. Furthermore, the differentiation between Parmigiano Reggiano and Grana Padano cheese is a recent one, it is very probable that until the institution of the respective consortia of protection (in the 1950’s), grana types of cheeses were used in pesto without distinguishing one from the other. Parsley or marjoram as alternatives to basil is a concession resulting from the fact that, then in Liguria, where this plant originating from India was abundant, but only when in season.\nPesto Genovese is for pasta!\nRatto’s recipe for Pesto states unmistakably that it is a sauce for dressing pasta, for lasagne, and for ‘troffie’ or ‘trofie’ that in Liguria are elongated and twisted gnocchi, with pointed extremities and fatter waists. These are not to be confused with ‘trofiette’ that are smaller and in fact are more used with Pesto. Trofie, kneaded out of white flour are a speciality of the town of Recco in the Province of Genoa, the same town that gave birth to the famous focaccia. Trofie and trofiette, there’s even a version made out of chestnut flour, are, therefore, fresh, handmade pasta. Genoa however is also an Italian capital of the production of dried pasta. Already in 1279, when Marco Polo was still in China, the Genoese Ponzio Bastone left a basket of dried pasta, in inheritance to his sons. Genoese Pesto honours this great tradition, so the dried pasta variation of the dish is ‘trenette’ (or ‘trinette’), with a shape similar to ‘mafalde’ but thinner than linguine or fettuccine.\nVariation in progress\nIn the 1800’s, the pasta al pesto was considered to be a working class dish and nowadays the recipe of that time has remained substantially the same. There was and there is still in Liguria the habit of adding potatoes, broad beans or French beans, and sometimes zucchini cut into small pieces and boiled together with the pasta. Especially in Genoa, potatoes and French beans are added to classic or improved (avvantaggiate) trenette, that is, ones made out of whole wheat flour, or to trofiette. Rules are not always fixed. For some, “avvantaggiate” are trenette to which vegetables have been added and there is, furthermore, a purist school that categorically excludes potatoes from trofie. In general it is said that in Liguria it’s difficult to find two equal versions of pesto, because of the variations, sometimes within the same family, such as the addition of walnuts, ricotta or other cheeses. This has happened with various typical Italian dishes, many of which have ‘terminated’ their evolution only within the most recent decades. In Italian cooking, the variations of a dish not only represent the wealth of diversity, but also an indirect legitimisation of its generally accepted version.\nIt’s very possible that, as Giuseppe Gavotti wrote in 1973 in his Cucina e vini di Liguria, the original pesto was ‘uncouth’, that is, it contained a lot of garlic. In 1965, even the great gastronomist Massimo Alberini, in his I Liguri a Tavola. Itinerario gastronomico da Nizza a Lerici (The Ligurians at the table, a Gastronomic Itinerary from Nice to Lerici) made a similar commentary, letting us understand that the pesto recipes of the 1800’s were somewhat stingy with the basil, using only a couple of leaves, and abounding in garlic, using three or four cloves of it. Certainly the Arab-Persian taste, which dominated the sauces of Genoa from the Middle Ages until the 1800’s, had a lot to do with this. As well as the predilection for and the ‘need’ of garlic by the Ligurian seafarers, who considered it as almost medicinal. Invariably, today’s Pesto is far more lady-and-gentleman-like and balanced, with a far more noticeable presence of basil, preferably Ligurian basil and ideally basil from the Genoese hills of Pra, with a long, thin leaf.\nPDO ingredients. When possible\nIn the same text, Alberini underlined the role of the olive oil in the recipe; it should neither be too fruity nor too bitter. The Ligurian extravirgin olive oils are ideal. Just as Genoese basil, the exrtavirgin olive oil of the Riviera Ligure has obtained the D.O.P. granted by the European Union in recognition of its quality and typicality. Furthermore, for a pesto that does honour to the territory of origin are to be used: a) the garlic of Vessalico, in the Province of Imperia, of delicate flavour and particular digestibility, b) Italian pine nuts and c) the coarse salt of the Cervia salt flats. It’s obvious that outside Italy it’s not always possible to find all these autochthonous ingredients. The production of Italian pine nuts, for example, has noticeably dropped in recent years due to the Pine Processionary, a parasitic devastation for the pine nut pine. Therefore, in order to prepare an authentic pesto, the minimum necessary requirement is to have quality ingredients and to follow the original recipe.\nIt’s easier to find Grana Padano and Parmigiano Reggiano cheeses on the international market, and especially advisable are those that are good and mature. It’s relatively easy to find pecorini, ewe cheeses, from Romano cheese that is judged by some to be a little aggressive to Fiore Sardo cheese, ideal when seasoned six months.\nConsorzio del Pesto Genovese and Pesto Championship\nThe Genoese Pesto Consortium has been formed by the Region of Liguria and associations of producers. Its mission, as its president, Andrea Della Gatta, chef and owner of Trattoria Andrea un Genoa, explains is “to safeguard the traditional recipe for pesto, along with all its quality ingredients and first and foremost, obviously to use Liguria basil.” The Consortium has presented a request for the recognition of quality STG (Specialità Tradizionale Garantita, Guaranteed Traditional Speciality) for pesto. The Genoese Associazione Palatifini, led by Sergio and Sara di Paolo and by Roberto Panizza, organises the World Championship of Mortar-made Genoese Pesto every two years in Genoa, to confirm its Ligurian imprint and its universality as a foodstuff of quality and taste. The Palatifini initiative deserves particular praise because the mortar and pestle, both at domestic as well as professional level, have given way to the processor. From the gastronomic point of view, as pointed out by the chef Enrico Tournier: “When we prepare pesto alla genovese with the classic mortar and pestle, we subject the instrument to the product while respecting the right proportions of the ingredients because we are the ones to decide the proportions of the doses. While when using a mixer, in order to succeed in cutting the basil we have to add more oil than otherwise necessary, thus altering the doses and in such manner we subject the product to the instrument.”\nGenoese Pesto in the world\nPesto has reached great popularity in the world, also thanks to the crews of the mercantile ships and the passengers that set sail from the Port of Genoa to most diverse of destinations. Pesto found itself at home in La Boca, the ‘Genoese’ district of Buenos Aires, and started to spread out in the main ports of the USA Immediately after the Second World War, some companies began exporting pesto in jars to the US. And at the same moment, the first recipes appeared in American newspapers. According to many sources, pesto reached its greatest popularity is the United States in the ‘80’s of the last century. At the beginning of the ‘90’s its popularity grew even more when Frank Sinatra started to commercialise a pesto sauce that carried his face on the label. Also its counterfeiting began, with the supermarkets invaded by approximate copies of the original. In the meantime, pesto became to be really successful in the Italian restaurants around the world. A new breed of Italians, Ligurians chefs and restaurateur placed it on the menus and made the dish according to the original recipe. Zeffirino was, without a doubt, the pioneer of this generation. These days pasta, be it trenette, trofie, gnocchi or lasagne, with pesto is amongst the best known Italian dishes in the contempory world.']	['<urn:uuid:a819ce01-b1b4-4a46-9e9c-040c795e020b>', '<urn:uuid:28503093-a50b-4aa5-a861-76c6371d467c>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T04:56:11.774636	9	40	2733
91	How do farmers control pests in Tennessee's cotton fields?	In Tennessee, farmers typically make two to three insecticide applications per year to control tarnished plant bugs. The region also deals with clouded plant bugs, which tend to appear in areas with less spraying but are relatively easy to control with insecticides. Studies have shown that well-timed insecticide applications can help reduce problems like hard lock in cotton bolls.	['Plant bugs and stink bugs are perhaps the most well-equipped of cotton pests when it comes to feeding on cotton bolls, thanks to their long beaks.\nBut their messy eating habits can often transmit nasty pathogens into the boll and cause extensive boll rot.\n“They aren’t elegant feeders,” said University of Tennessee Extension entomologist Scott Stewart, author of a paper on the relationship between insect boll feeding and boll rot. “The plant bug and stink bug inject enzymes into the boll to dissolve things, then suck it back up. Then they move onto the next boll. So there is certainly a lot of opportunity for exchange of pathogens.”\nThe tarnished plant bug is the No. 1 pest in the Mid-South today, but once a boll accumulates between 225 and 250 DD-60s, it’s virtually safe from tarnished plant bug damage, Stewart says. “Most of the time, it will feed on squares and blooms unless it’s forced not to.”\nIn the central part of the Mid-South, which is considered the hot spot for tarnished plant bug, cotton producers can make 10 to 12 applications a year for the pest. “It’s not that way across the entire Mid-South. In Tennessee, it would be typical to make two to three applications a year,” Stewart said.\nWest Tennessee is more of a center for clouded plant bug, “which tends to show up where you’re not doing a lot of spraying, and it’s relatively easy to kill with insecticides,” Stewart said. “It’s a little bigger than the tarnished plant bug, and is much more inclined to feed on bolls. They can wear a boll out, which obviously creates an opportunity for pathogen introduction.”\nThe stink bug complex, primarily green, southern green and brown stink bugs, “feeds a little differently than tarnished and clouded plant bugs,” Stewart said. “They specifically target seeds. The damage can vary quite a bit and considerably, a lot of which is related to whether or not you get secondary rots.”\nMinor stink bug injury might include loss of seed and/or lint staining. Major injury might start with a small amount of feeding, but secondary rot may have a significant effect on the boll.\nStewart says looks can be deceiving with stink bug damage. “You can see the characteristic pock marking on a boll. Sometimes, you pop the boll open and there is very little damage from the insect. Other times, it’s on its way to being completely lost.”\nA test in Jackson, Tenn., in 2009, showed that when high infestations of stink bugs were not treated, over 90 percent damage occurred with a loss of 500 pounds of lint compared to better treatments. “Most of this was from the direct feeding of the stink bug, but there clearly was some relationship between insect feeding and hard locks,” Stewart said.\nCaterpillar pests can also play a role in boll rots, according to Stewart. “With Bt cotton, we’re primarily talking about bollworm and fall armyworm. What we’re seeing with the new Bt technologies is that even when the Bt technology kills the worm, a lot of times the worm will try to feed on the boll. Depending on the caterpillar and the expression of Bt toxin, they may penetrate the carpel wall or they may not.”\nA heavy infestation can result in a large number of bolls exhibiting this type of feeding, but Stewart says entomologists are not sure of the extent to which this causes problems for cotton producers.\nAnother west Tennessee study shows interesting results on the impact of fungicides and insecticides on boll rot. Researchers planted a non-Bt cotton, Phytogen 315, and applied insecticides three times beginning July 20, “when we first broke threshold for tarnished plant bug. Our treatment was a mixture of Karate and Bidrin,” Stewart said.\nOne fungicide application of Quadris was made on Aug. 10, about the third week of bloom when cotton was again at threshold for clouded and tarnished plant bug, and there were populations of stink bug as well. The study was complicated due to the excessive rain that fell in the fall of 2009, which caused a significant amount of weather-caused hard lock.\nThe study showed a very clear statistical relationship between the percentage of hard lock and insecticide spraying. “We reduced hard lock by spraying insecticide,” Stewart said. “There was not such a clear relationship with fungicide. There was a small benefit, but it didn’t show up statistically.”\nStewart says the continued development of Bt cottons “is going to further reduce the impact of our caterpillar pests directly on the crop or on incidental rots. Bugs are going to continue to rise in importance.”\nGeorgia research backs up Stewart’s observations on stink bugs. Georgia entomologist Michael Toews says, “It’s clear that stink bugs are transmitting boll rot pathogens into developing cotton bolls. It’s highly variable across years and across fields. We’ve developed very tight sampling thresholds that work very well. But much of that data was developed over the last three years and we haven’t had a lot of pathogen data during that time.”\nA Georgia study indicates that spraying for stink bugs weekly at bloom increased lint yields by 67 pounds over the untreated check. But according to Toews, “By the time you make 6.5 sprays, you’re losing money.”\nResearchers sprayed two times using a 10 percent damage threshold, which showed a slight increase in yield but still produced a negative net return.\nToews noted that well-timed applications at 20 percent damage showed a positive net return. “The most exciting result is the dynamic approach, where we’re more aggressive during weeks of bloom when we know we’re making the bulk of our crop. We were less aggressive early during bloom and less aggressive late in the bloom cycle.”']	['<urn:uuid:64de931c-63f9-42d9-ae7c-78e9cfca367e>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:56:11.774636	9	59	957
92	what different branches psychology help people work life emotions	Several psychology branches focus on helping people in different life domains. Clinical psychology addresses mental disorders and behavioral adjustment. Counseling psychology deals with personal and interpersonal functioning across the lifespan, including emotional, social, vocational and educational concerns. Industrial-organizational psychology applies psychological principles in workplaces to improve both organizational performance and employee wellbeing. Clinical health psychology focuses on the connections between behavioral, emotional, cognitive and biological components to promote health and treat illness. These specialties all use scientific knowledge to help people improve their functioning and quality of life in various settings.	"['Results 1–10 of 16 for ""Psychology Subfield""X related to ""U.S. mental health system needs less..."" Refine Your Search Refine Your Search TopicEducation (4)Aging (1)Emotional health (1)Personality (1)Sport & exercise (1) 3 more... [+] Testing issues (1)Therapy (1)Workplace issues (1)Hide detailsDocument TypePsychology SubfieldXYear2011 (1) Results 1–10 of 16 Previous 1 2 Next Relevance Title A-Z Title Z-A Newest First Oldest First Sort by: 1.Quantitative Psychology Designs Research Methods to Test Complex Issues Quantitative psychologists study and develop the methods and techniques used to measure human behavior and other attributes. Their work involves the statistical and mathematical modeling of psychological processes, the design of research studies and the analysis of psychological data.Psychology Subfield 2.Public Description of Industrial and Organizational PsychologyIndustrial and Organizational Psychology is a general practice specialty of professional psychology with a focus on scientifically-based solutions to human problems in work and other organizational settings.Psychology Subfield 3.Public Description of Clinical Health PsychologyThis specialty applies scientific knowledge of the interrelationships among behavioral, emotional, cognitive, social and biological components in health and disease to the promotion and maintenance of health; the prevention, treatment and rehabilitation of illness and disability; and the improvement of the health care system. Psychology Subfield 4.Professional Geropsychology Public Description This specialty helps older persons and their families maintain well-being, overcome problems, achieve maximum potential during later life, appreciate the diversity among older adults, the complex ethical issues that can arise in geriatric practice and the importance of interdisciplinary models of care.Psychology Subfield 5.Behavioral and Cognitive Psychology Public DescriptionThis specialty in professional psychology emphasizes an experimental-clinical approach distinguished by use of principles of human learning, development and theories of cognitive processing to promote meaningful change in maladaptive human behavior and thinking, and more.Psychology Subfield 6.Public Description of Clinical PsychologyClinical Psychology is a general practice and health service provider specialty in professional psychology. Clinical psychologists assess, diagnose, predict, prevent and treat psychopathology, mental disorders and other individual or group problems to improve behavior adjustment, adaptation, personal effectiveness and satisfaction.Psychology Subfield 7.Psychopharmacology Public DescriptionThis proficiency applies psychopharmacological principles, scientific data and clinical practices to individual psychopathology and problems, and blends the scientific study of behavior, its biological basis, and the interaction of medication to produce therapeutic changes in normal and abnormal functioning.Psychology Subfield 8.Industrial and Organizational Psychology Provides Workplace SolutionsIndustrial and organizational psychologists study and assess individual, group and organizational dynamics in the workplace. They apply that research to identify solutions to problems that improve the well-being and performance of organizations and their employees.Psychology Subfield 9.Counseling PsychologyCounseling psychology is a general practice and health service provider specialty in professional psychology. It focuses on personal and interpersonal functioning across the life span and on emotional, social, vocational, educational, health-related, developmental and organizational concerns.Psychology Subfield 10.Rehabilitation Psychology Helps People Overcome ChallengesRehabilitation psychologists study and work with individuals with disabilities and chronic health conditions to help them overcome challenges and improve their quality of life.Psychology Subfield Previous 1 2 Next Relevance Title A-Z Title Z-A Newest First Oldest First Sort by: ADVERTISEMENT Results 1–10 of 16 for ""Psychology Subfield""X related to ""U.S. mental health system needs less...""', 'History of Industrial and Organizational Psychology\nIndustrial and Organizational Psychology\nThe World of Psychology\nThe world of psychology is divided into many disciplines. Each discipline shares commonalities in that they each seek an improve understanding of human nature, behavior, and mental functions. Within some fields, it is enough to gain insight into these subjects through study and research. In other fields, it is desired to use the knowledge that has been gained for the purpose of creating change. When most people think of psychology, they think of the fields of clinical and abnormal psychology where the practitioners within the field seek to assist individuals who may have mental disorders and to create change by helping those individuals to improve their own lives. While this is what most people think of concerning psychology, it is not the only discipline of the field. The field of industrial and organizational psychology seeks to apply the principles of psychology to create change within the business world and in doing so improve the way organizations function as well as improving the quality of the working experience of individuals.\nHow Do We Define Industrial and Organizational Psychology?\nIndustrial and organizational psychology is defined as the study of people working and the application of the principles of psychology to the organizational and work environment (Spector, 2008; Jex, 2002). Industrial and organizational psychology is a field of psychology which is concerned with both the study of psychological principles as a science and the application of those principles (Spector, 2008; Jex, 2002).\nThe Industrial Side\nIndustrial and organizational psychology is like a coin with two sides. Industrial psychology is the first side of that coin. The main focus of the industrial side of the coin is understanding human behavior in order to improve organizational efficiency, employee selection, employee training and to more efficiently design jobs (Spector, 2008; Jex, 2002).The industrial side of industrial and organizational psychology is a top down perspective which views human behavior to assess ways in which the organization can benefit from the application of psychological principles (Spector, 2008; Jex, 2002).\nThe Organizational Side\nThe organizational side of the coin is the inverse of this. The organizational side focuses on understanding behavior in order to enhance employee satisfaction and well-being within the work place (Spector, 2008; Jex, 2002). Spector (2008) explains that “organizational topics include employee attitudes, employee behavior, job stress, and supervisory practices” (p. 5). Judging by the focus of organizational topics it can be stated that the organizational side of the field is a bottom up perspective which focuses on behavior in order to improve the quality and satisfaction of the individuals within an organization rather than the organization as a whole.\nThe Two Sides Together\nThough each of these two sides is focused from a different perspective they are not mutually exclusive in their goals, their applications or their topics of interest. Spector (2008) uses the example of motivation to explain the dual nature of subjects studied by industrial and organizational psychologists, stating that motivation “is relevant to the [industrial] concerns of employee efficiency and performance, but it is also relevant to the [organizational] concern with the happiness and well-being of employees” (p. 5).\nThe Early Years Of I/O Psychology\nThe field of industrial and organizational psychology was spawned in the 1800s out of experimental psychology (Spector, 2008; Koppes, 2007). Hugo Münsterberg, Walter Dill Scott and James Mckeen Cattel were early pioneers of the field of industrial and organizational psychology (Spector, 2008; Koppes, 2007). Münsterberg and Cattel both trained under Wilhelm Wundt graduating from his doctoral program in Germany before relocating to the United States (Koppes, 2007). These pioneers brought the study and application of psychological principles into the world of business and industry (Spector, 2008; Koppes, 2007). According to Koppes (2007) “In the beginning the objective of an industrial psychology was to improve organizational goals (productivity and efficiency) primarily by applying psychology with an emphasis on individual differences, through selection and training” (p. 314). The early years of this branch of psychology focused on the industrial side of the field (Spector, 2008; Koppes, 2007). During this time there was a marriage within the field of industrial psychology between the principles of psychology and the field of engineering (Spector, 2008; Koppes, 2007). Many of the individuals who helped to influence the field of industrial psychology had backgrounds in engineering, some had backgrounds in history and law (Spector, 2008; Koppes, 2007; Jex, 2002).\nIndustrial Psychology and the First World War\nIndustrial psychology prospered due to World War I (Spector, 2008; Koppes, 2007). When the United States entered the first World War psychologist were called upon to develop a program for psychological evaluation or recruits as well as a means for selecting personnel for specific jobs within the military (Spector, 2008; Koppes, 2007). The group of psychologist working with the military were led by Robert Yerkes (Spector, 2008; Koppes, 2007). According to Spector (2008) “the best-known accomplishment of the group was the development of the Army Alpha and Army Beta group tests for mental ability” (p. 12). Koppes (2007) explains that these mental ability test “paved the way for large-scale intelligence testing and for later expansion of psychological testing into government, industry, and education” (p. 315). Between the first and second World War the field of industrial and organizational psychology expanded rapidly (Spector, 2008; Koppes, 2007; Jex, 2002).\nThe Hawthorne Studies\nAccording to Spector (2008) “One of the most important events of this period was the Hawthorne studies, which continued for more than 10 years at the Western Electric Company” (p. 12). The Hawthorne studies became a pivotal point in the evolution of industrial and organizational psychology because it was primarily responsible for the development of the organizational side of the field (Spector, 2008; Koppes, 2007; Jex, 2002). The Hawthorne studies inadvertently revealed the human side of organizations. In another attempt to study methods of increasing efficiency and productivity it was learned that social aspects of an organization, such as work groups and the worker\'s knowledge that they are being watched, affected the workers behavior and performance (Spector, 2008; Koppes, 2007; Jex, 2002). The understanding that social aspects of the work environment had an effect on behavior led to psychologists examining the work environment from the perspective of the individual employees (Spector, 2008; Koppes, 2007; Jex, 2002).\nWorld War II and Division 14 of the APA\nThe emergence of World War II again allowed the field of industrial and organizational psychology to expand due to the growing demands of the military (Spector, 2008; Koppes, 2007; Jex, 2002). World War II not only widened the scope of the field of industrial and organizational psychology, it also opened doors professionally for industrial and organizational psychologists. Before the second World War the American Psychological Association was not concerned with experimental or applied fields within psychology (Spector, 2008; Koppes, 2007). In response to the changes occurring in psychology at the time the APA created Division 14, Industrial and Business Psychology (Spector, 2008; Koppes, 2007; Jex, 2002). This arm of the APA went through a couple of changes and eventually evolved into the Society for Industrial and Organizational Psychology (Spector, 2008; Koppes, 2007; Jex, 2002).\n- Koppes, L (2007). ""History of Industrial/Organizational Psychology in North America."" Encyclopedia of Industrial and Organizational Psychology. Ed. Steven G. Rogelberg. Vol. 1. Thousand Oaks, CA: Sage Reference, 2007. 312-317. Gale Virtual Reference Library. Web. 5 Mar. 2011.\n- Jex, S (2002). Organizational Psychology: A Scientist-Practitioner Approach. Retrieved from the University of Phoenix eBook Collection database.\n- Spector, P (2008). Industrial and Organizational Psychology: Research and Practice (5th ed). Retrieved from the University of Phoenix eBook Collection database.\n© 2012 Wesley Meacham']"	['<urn:uuid:8d479ae8-47c2-4837-bde3-3ef246225250>', '<urn:uuid:d5698e38-0a0d-41b5-8f3d-7f54b22f7d02>']	open-ended	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-13T04:56:11.774636	9	91	1789
93	What are the historical discussions about mathematical self-evidence, and how do coral reefs respond to environmental threats?	Historically, there were debates about mathematical self-evidence, with Omar Khayyam criticizing Euclid for demonstrating obvious things while leaving other propositions unproven. For instance, Khayyam believed that proofs about equal angles in circles were unnecessarily demonstrated. Regarding coral reefs' response to threats, they have developed a survival mechanism where, instead of just turning white when stressed, they can produce a protective pigment layer that creates a neon colored display, which helps attract algae back to the coral in an optical feedback loop.	"[""My math history class is currently studying non-Euclidean geometry, which means we’ve studied quite a few “proofs” of Euclid’s fifth postulate, also known as the parallel postulate. I’ve written about this postulate before. There are many statements that are equivalent to the parallel postulate, including the fact that parallel lines in a plane are equidistant. This postulate is independent from the rest of Euclid’s assumptions, but for centuries, mathematicians tried to prove that the parallel postulate followed from the rest of Euclidean geometry.\nOmar Khayyam, the eleventh and twelfth century Persian polymath who is probably best known in the West as a poet (“a jug of wine, a loaf of bread—and thou…”), is one of the many mathematicians who tackled the parallel postulate. In 1077, he wrote a Commentary on the Difficulties of Certain Postulates of Euclid’s Work, which deals with the parallel postulate and some of Euclid’s work on ratios and proportions.\nThrough the magic of interlibrary loan, I was able to get my hands on a copy of the hard-to-find book Omar Khayyam the Mathematician by R. Rashed and B. Vahabzadeh. Among other things, it contains an English translation of Khayyam’s commentary, so my class and I were able to see exactly how Khayyam approached the theory of parallels. (Or at least as nearly as possible without reading Arabic.)\nUnlike many of his predecessors, Khayyam did not try to show that Euclid’s fifth postulate followed from the rest of the postulates and axioms; instead, he says that Euclid should have started with different postulates that Khayyam believed were more self-evident. He proposes a total of five new postulates that should be added to Euclid's Elements. From these, he proves eight new propositions to replace Euclid’s propositions 29 and 30, the first two propositions of the Elements that require the parallel postulate. Khayyam didn’t know it, but his first three propositions are some of the very first theorems in non-Euclidean geometry. But that’s not what this post is about. This post is about some passages from the commentary that I found delightfully grumpy.\nKhayyam mostly seems grumpy about what he views as Euclid’s inconsistency in his assumptions. Euclid demonstrates some things that Khayyam thinks are obvious and doesn’t demonstrate some things that Khayyam thinks need demonstration. The first grumpy passage comes early in the commentary. Khayyam has just given a heuristic proof that Euclid's parallel postulate follows from the nature of straight lines and right angles. (Emphasis from the translation by Rashed and Vahabzadeh.)\nSo it is because of this that Euclid was of the opinion that the cause of the meeting of the lines...is the two angles being less than two right angles.\nAnd this opinion is true, but one cannot build upon it except after other demonstrations, for it is them which induced Euclid to admit this premise and build upon it without demonstration. But upon my life! these are very hypothetical propositions,...\nAnd how can one allow Euclid to postulate this proposition [the parallel postulate] because of this opinion while he has demonstrated many things much easier than these? (As when he proves in the third Book, that equal angles at the centres of equal circles cut off from the circumference equal arcs. But this idea is well known from the principles, for equal circles apply to one another, and equal angles likewise. Consequently the arcs will inevitably apply to one another; therefore they will be equal. So one who proves something like this, how much will he stand in need of proving something like that!—And as when he proves in the fifth Book, that the ratio of the same magnitude to two equal magnitudes is the same. But as ratio falls within magnitude qua magnitude, why should this need a demonstration? Since the two equal magnitudes are equal qua measure, there is no difference whatsoever between them; therefore they are from this viewpoint truly the same: there is no alterity whatsoever between them, except the alterity of number and no more.)\nKhayyam must have been particularly bothered by Euclid's proofs of theorems involving circles and equal angles because he came back to them later in the commentary.\nDo you not see that whosoever conceives the reality of the circle, the reality of the angle, and the reality of the ratio between magnitudes, will know with a modicum of reflection, that the ratio of the angles which stand at the center is equal to the ratio of the arcs on which they stand? Yet this notion has been demonstrated by Euclid in proposition 36 of Book VI.\nAfter outlining the five postulates Khayyam believes Euclid neglected to include but should have, he wrote:\nAnd there are more obvious primary premises than this; but Euclid has not produced most of them in the beginning of the work, although he has produced primary things which one could very well do without. And he had to either not produce them at all, or produce all of them without excluding any of them, even if they are obvious.\nI admit that I am probably reading too much into Omar Khayyam’s mood; after all, we are separated by more than 900 years, a couple of continents, and several rounds of translation. It's a bit presumptuous for me to claim any knowledge of his emotional state. But imagining him writing his commentary grumpily helps me see him as a person the same way listening to Peter Schickele read Bach's grumpy letters about spilled wine and low pay makes me feel like maybe Bach and I aren't so different. Omar Khayyam—he's just like us! Geometry sometimes makes him grumpy."", 'Cecilia D’Angelo, a molecular coral biology lecturer at the University of Southampton. Jörg Wiedenmann, Elena Bollati & Cecilia D’Angelo/University of Southampton, Palawan colourful bleaching image by Ryan Goehrung/University of Washington\nBleaching events used to be few and far between, but they now occur nearly every year. After the coral is exposed, it often breaks down and dies, altering the ecosystem for the diverse array of life that relies on it. Corals stand little chance of bouncing back from these events — but a new study suggests they have an unusual survival method: taking on a vibrant neon color.\nWhen bleaching events occur, extended heat spikes cause corals to turn a ghostly white, often leading to their death. Even slight increases in annual ocean temperatures can wreak havoc on this relationship, expelling the algae from the coral’s tissue and exposing its white skeleton. These corals can still undergo some of their normal functions for a short period of time as they hope their algae come back — whereas drastic changes in ocean temperature almost always lead to coral death. Reports of colorful bleaching during the most recent mass bleaching event in the Great Barrier Reef in March and April gave scientists hope that patches of the system have a chance to recover. “Bleaching is not always a death sentence for corals, the coral animal can still be alive,” said Dr. They found that colorful bleaching events occur when corals produce “what is effectively a sunscreen layer” on their surface to protect against harmful rays and create a glowing display that researchers believe encourages algae to return. The Ocean Agency describes the process as a “chilling, beautiful and heartbreaking” final cry for help as the coral attempts to grab the algae’s attention. “Our research shows colorful bleaching involves a self-regulating mechanism, a so-called optical feedback loop, which involves both partners of the symbiosis,” lead researcher Professor Jörg Wiedenmann of the University of Southampton said in a press release. Climate Change\nDying coral reefs turn vibrant neon in apparent survival effort\nScientists say climate change is turning coastal Antarctica green\nStudy: Climate change makes a Dust Bowl heat wave more likely\nAir pollution is already spiking in China with virus lockdown lifted\nIndia’s carbon emissions fall for the first time in four decades\nMore in Climate Change\nResearchers at the University of Southampton’s Coral Reef Laboratory studied 15 colorful bleaching events worldwide between 2010 and 2019 — including one in the Great Barrier Reef, the world’s largest coral reef system — and recreated those ocean temperatures in a lab. But “colorful bleaching” has the opposite effect: the dying corals gain more pigment, and glow in shades of bright pink, purple and orange. Scientists first spotted the mysterious neon coral a decade ago, but they had been unable to figure out why it occurred. As the recovering algal population starts taking up the light for their photosynthesis again, the light levels inside the coral will drop and the coral cells will lower the production of the colorful pigments to their normal level.”\nColorful bleaching Acropora corals in the Phillipines. Dying coral reefs turn vibrant neon colors in apparent last-ditch effort to survive\nBy Sophie Lewis\nMay 22, 2020 / 4:48 PM\n/ CBS News\nScientists use mini-satellites to save coral reefs\nFor years, coral reefs around the world have been devastated by mass bleaching events as the oceans continue to warm due to climate change. “If the stress event is mild enough, corals can re-establish the symbiosis with their algal partner.”\nThe internal changes that allow colorful bleaching to occur. In 2017 alone, nearly half the corals on the Great Barrier Reef died — and experts say we are running out of time to save them. “Unfortunately, recent episodes of global bleaching caused by unusually warm water have resulted in high coral mortality, leaving the world’s coral reefs struggling for survival,” D’Angelo said. Scientists emphasized that while colorful bleaching is a good sign, only a significant reduction of greenhouse gases globally — in addition to improvement in local water quality — can save coral reefs beyond this century.\n“Now that we know that nutrient levels can affect colorful bleaching too, we can more easily pinpoint cases where heat stress might have been aggravated by poor water quality,” researchers said. Ryan Goehrung, University of Washington\nCoral reefs support more species per unit area than any other marine environment, including about 4,000 species of fish, 800 species of hard corals and potentially millions of other undiscovered species, according to the National Oceanic and Atmospheric Administration. Colorful bleaching Acropora corals in New Caledonia.\nRichard Vever/The Ocean Agency/XL Catlin\nCoral animals symbiotically coexist with tiny algae, providing them with shelter, nutrients and carbon dioxide in exchange for their photosynthetic powers. Together, these actions can secure a future for coral reefs.”\nFirst published on May 22, 2020 / 4:48 PM “The resulting sunscreen layer will subsequently promote the return of the symbionts. This study, published Thursday in the journal Current Biology, suggests the corals change color as a last-ditch effort to survive. “This can be managed locally, whereas the ocean heat waves caused by climate change will need global leadership. Disruptions to coral reefs have far-reaching implications for ocean ecosystems.\nIt’s not just warming oceans that cause colorful bleaching. Researchers say changes in nutrient levels within coral reefs due to fertilizer run-off from farms also lead to bleaching events — a problem that can be fixed at the local level. Experts believe only coral that has faced mild or brief disturbances, rather than extreme mass bleaching events, can attempt to save itself using this process.']"	['<urn:uuid:77caad17-ffe9-4c5f-8e32-2a642b0a2002>', '<urn:uuid:369f1095-9589-4d83-9007-35418b336092>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	17	81	1872
94	How does racial diversity in American schools compare to the past, and what problems do minority students face in terms of academic performance today?	In the mid-1970s, non-Hispanic white children represented about 85 percent of the U.S. child population, but today they are only slightly more than half. Unfortunately, black, Hispanic and American Indian children have fewer opportunities than other children, leading to disparities in academic performance. These disparities consistently show up on standardized tests, grade-point averages, and other measures, creating what's known as achievement gaps between Black and White, Latina/o and White, and recent immigrant and White students.	"[""New online database tracks wellbeing of US children\nThe Institute for Child, Youth and Family Policy launches new tool to analyze child wellbeing and equity across racial and ethnic lines\nThe Institute for Child, Youth and Family Policy (ICYFP) at Brandeis University’s Heller School for Social Policy and Management has launched the first nationally comprehensive, interactive online database for tracking and analyzing the wellbeing and equity of U.S. children across racial and ethnic groups.\nThe site, diversitydatakids.org, allows users to create customized profiles, rankings and maps that make data visual, accessible and understandable. It is supported by the W.K. Kellogg Foundation and the Robert Wood Johnson Foundation (RWJF), in partnership with the Kirwan Institute for the Study of Race and Ethnicity at Ohio State University.\nBNow spoke with Dolores Acevedo-Garcia, director of ICYFP and principal investigator of diversitydatakids.org about the project and what it can provide.\nYour database looks specifically at children's wellbeing across racial and ethnic groups. Why look at the data across those lines?\nThere are three main reasons why it is critical to look at child wellbeing by race and ethnicity. First, our child population today is much more diverse than it was a few decades ago. In the mid-1970s, non-Hispanic white children represented about 85 percent of the U.S. child population, but today they are only slightly more than half. Second, unfortunately, not all children have the same opportunities for healthy development. On average, black, Hispanic and American Indian children have fewer opportunities than other children. These inequities in opportunity feed large disparities in child outcomes down the road in development, health and education. Finally, we need better information about programs and policies that improve wellbeing for all children while reducing inequities. Often we have information about whether a program or policy works overall, but not whether it reduces inequities and improves outcomes for vulnerable children. Diversitydatakids.org policy equity assessments address these issues. We have a moral, economic and civic obligation to tackle these inequities through a deepened understanding of which strategies, policies and programs are effective.\nThe database can zoom out and look at trends nationally and zoom in and look at smaller geographic levels. Can you give an example of what information at a specific geographic level can tell us about child wellbeing?\nThe racial and ethnic composition of the child population differs across the country, and opportunities and inequities also differ. This means we need more tailored policy solutions rather than a “one-size-fits-all” approach. Diversitydatakids.org presents data for up to seven different geographic levels (depending on data availability): neighborhood, county, city, school district, metropolitan area, state and nation. Different geographic levels allow us to answer different questions. For example, we examine the extent of Family and Medical Leave Act (FMLA) coverage across states for working parents, recognizing that parents’ time to attend to their children’s needs may not be available to parents of all racial/ethnic, immigrant and socioeconomic backgrounds. We are interested in the state level because states are able to enact expansions (via state-level legislation) to cover more parents. Our data support the work of groups that advocate for expansions of family and medical leave at the state level.\nWhat other kinds of questions can this database answer?\nDiversitydatakids.org is a very rich data resource. It has broad and deep geographic and thematic coverage, always with a focus on race/ethnicity and equity; often the data are available for multiple years. Also, the reports users can get from the site are highly customizable. These two features — the depth and breadth of our indicators, and customizability — mean that our data can be used to answer a wide range of questions. For example, one user may be interested in low birthweight for Hispanic babies by county, while another user may want to know about school segregation between white and black children in the 100 largest metropolitan areas or school districts, while another user may want to know the proportion of low-income children in a specific state that are served by Head Start.\nWhat can this database help accomplish?\nDiversitydatakids.org offers the first comprehensive, equity-focused information system to monitor progress towards improved wellbeing for children of all racial/ethnic groups through unique data and policy indicators and analysis. Until now, despite increasing diversity and inequities, the public, policymakers and practitioners must piece together information from disparate, fragmented sources. Diversitydatakids.org helps fill this gap. One possible use of our data is to help state and local groups understand equity issues in their communities and how their communities compare with other areas, which they can use in reports and policy advocacy. Another possible use of our data is our equity-focused analyses, which help put an important equity issue in the spotlight. In September 2010, our original website released a study of school segregation in the 100 largest metro areas, which received extensive publicity, especially in the Boston Globe, where it was covered in the lead story and editorial.\nRead more about diversitydatakids.org at the W.K. Kellogg Foundation."", '5 edition of achievement gap in U.S. education found in the catalog.\nachievement gap in U.S. education\nIncludes bibliographical references and index.\n|Other titles||Achievement gap in United States education., Achievement gap in US education.|\n|LC Classifications||LC213.2 .S57 2005, LC213.2 .S57 2005|\n|The Physical Object|\n|Pagination||vii, 188 p. ;|\n|Number of Pages||188|\n|LC Control Number||2004026788|\nThe School Readiness Gap. educators, researchers, and policymakers have puzzled over so-called achievement gaps—the disparities in academic performance by race and ethnicity that consistently show up on standardized tests, grade-point averages, and a host of other measures. Using data from the U.S. Department of Education’s Early. From the Achievement Gap to the Education Debt: Understanding Achievement in U.S. Schools by Gloria Ladson-Billings The achievement gap is one of the most talked-about issues in U.S. edu-cation. The term refers to the disparities in standardized test scores between Black and White, Latina/o and White, and recent immigrant and White students.\nHow covid has laid bare the vast inequities in U.S. public education Hundreds of high school students and other supporters participate in the March to Close the Gap in . since At grade 8, the achievement gap between non-ELL and ELL students in reading scores was 3 points smaller in than in (47 points), but not measurably different from the achievement gap in Page 3 of 3. Reference table: Digest of Education Statistics , tab Glossary: Achievement gap, English language File Size: 1MB.\nGaps in life outcomes, by ethnicity. The paper cited above, Duncan (full citation (), shows that African-Americans and other minorities are more likely to have low socioeconomic status than achievement gap between different ethnicities looks broadly similar to the achievement gap between people of different socioeconomic status, but there is frequently more and better data. The achievement gap does not account for the vexing ways both historical and contemporary systemic factors, such as racial and economic segregation and the underfunding of public schools, has impacted the kind of education students who have less privilege often receive.\nNational electrical code, 1968\nProject Learner goals final report\nJacobean and Caroline stage\nOn the South China track\nNew Rudmans questions and answers on the ... NTE specialty area examination in ... early childhood education (KG.-3)\nWriting a Story\nThe arousal-performance relationship\nWesleyan and Tractarian worship\nSpecial maps of Persia 1477-1925\nPlanning for play for the mentally handicapped.\n4th International Symposium on Electron and Photon Interactions at High Energies, Liverpool, September 14th-20th, 1969\nThe Achievement Gap in U.S. Education: Canaries in the Mine Paperback – Cited by: 5. The Achievement Gap in U.S. Education examines: Why learning is viewed negatively by students, Why good teaching practices are relatively rare in U.S.\nschools, Why poor teaching practices occur more frequently in minority and poor districts, Why the accountability movement and its emphasis on high-stakes tests will fail to produce improvements.4/5.\nThere has been very little progress in closing the Black/White achievement gap in education. In this book, the author Mano Singham takes a look at this problem in the context of larger political realities and contends that in order to understand this gap, we must determine what is happening within the educational system as a by: 5.\nFree business-day shipping within the U.S. when you order $25 of eligible items sold or fulfilled by Amazon. Or get business-day shipping on this item for $ (Prices may vary for AK and HI.) Note: Available at a lower price from other sellers that may not offer free Prime shipping/5(5).\nThe Achievement Gap Is \'More Glaring Than Ever\' for Students Dealing With School Closures The Achievement Gap Is \'More Glaring Than Ever\' for Students Dealing With School Closures Mosijah Roye, Achievement gaps have been narrowing because Black and Hispanic students’ scores have been rising faster than those of White students.\nAchievement gaps have been closing because Black and Hispanic students’ scores have improved very rapidly over the last 30 years. progress of U.S. students in grades 3–8.\nEfforts to reduce the achievement gaps in reading and math have resulted in some reductions; however, the gaps between White students and their African American and Hispanic peers, and between stu-dents from high and low socioeconomic households still exist.\nAchievement Gaps Achievement gaps occur when one group of students (such as, students grouped by race/ethnicity, gender) outperforms another group and the difference in average scores for the two groups is statistically significant (that is, larger than the margin of error).\nThe “achievement gap” in education refers to the disparity in academic performance between groups of students. The achievement gap shows up in grades, standardized-test scores, course selection, Author: Susan Ansell.\nThe Global Achievement Gap, a new book by Tony Wagner, co-director of the Change Leadership Group, examines the U.S. education system in the 21st century, considers why American students are falling behind their international peers, and proposes methods to begin to correct the downward slide.\nIn this Q&A, Wagner addresses this gap and offers advice on how educators can begin to close it. Evidence for the achievement gap is most commonly found in dropout rates, test scores, college enrollment rates and other measurements of success in schools.\nMore than an illustration of student success, the achievement gap speaks to systemic issues we deal with as a nation. The achievement gap between white students and African American and Hispanic students has been debated by scholars and lamented by policymakers for years.\nThis book provides, for the first time in. The achievement gap is one of the most talked-about issues in U.S. education. The term refers to the disparities in standardized test scores between Black and White, Latina/o and White, and recent immigrant and White students.\nThis article argues that a focus on the gap is by: One study claims that about half the test score gap between black and white high school students is already evident when children start school. A variety of different tests at kindergarten entry have provided evidence of such a gap, including the U.S.\nDepartment of Education’s Early Childhood Longitudinal Survey of Kindergarten children (ECLS-K). Part III — The Knowledge Gap (Sept. 23): Longtime education journalist Natalie Wexler joins On Point to discuss her new book, ""The Knowledge Gap: The Hidden Cause of America’s Broken Education.\nOne, promoted by a number of education reformers, advocates, and the business community, contend that schools do have the capacity to close the achievement gap, but don’t have the will to do so.\nArguing that a number of schools do, against all odds, successfully narrow the achievement gap for their students, these groups suggest replicating. The achievement gap points to a profound educational discrepancy among various student groups, but there are steps school administrators can take to shrink this gap.\nExplore some of these gap-shrinking strategies, and how they can be implemented in a school setting. Researchers found a “very strong link” between racial school segregation and academic achievement gaps. Every school district with “even moderately high” segregation had a large achievement gap.\nIn the U.S., research studies into the causes of gaps in student achievement between low-income minority students and middle-income white students have been ongoing since the publication of the report, “Equality of Educational Opportunity” (more widely known as the Coleman Report), commissioned by the U.S.\nDepartment of Education. In the education community, “closing” achievement gaps is widely considered to be one of the major challenges facing the American public-education system. It is also tends to be one of the top priorities identified by educators, policy makers, elected officials, and others working to improve the education system and individual schools.\nSimilar Items. Bridging the achievement gap Published: () ; Minding the achievement gap one classroom at a time by: Pollock, Jane E., Published: () ; Dismantling educational inequality: a cultural-historical approach to closing the achievement gap / by: Portes, Pedro R.\nPublished: ().Educators across the United States are struggling to close the achievement gap — the differences in performance among various groups of students. While race, ethnicity and poverty play prominent roles in the U.S.\nachievement gap, gender, family stability and student interest in subjects like math and engineering also contribute. The authors use a massive dataset that covers achievement in grades in about half the school districts in the United States; notably, these districts include 96% of all Black public school Author: Dick Startz.']"	['<urn:uuid:fda56c47-379c-4861-aa32-855862cc7ace>', '<urn:uuid:762f3029-b002-4b1c-be68-b47ff579b31e>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:56:11.774636	24	75	2210
95	highest position sterophonics dakota song uk	Dakota was the first Stereophonics single to reach number 1 on the UK Singles Chart.	"['It was the first Stereophonics single to reach number 1 on the UK Singles Chart or to chart on the US Modern Rock Tracks chart. The song spent a total of 44 weeks on the UK singles chart, the most any Stereophonics single has managed, and it also topped the download chart, remaining on the chart for 22 weeks, half of which were spent in the top 10. The song has been compared to the works of U2. It also made an appearance in the video games FIFA Manager 06 and Pro Evolution Soccer 2010.\nKelly Jones started writing the music for ""Dakota"" in January 2004. He was staying in a hotel room in Paris while Stereophonics were on a promo tour for You Gotta Go There To Come Back and was trying to find a new sound. A month later, while the band were on tour in the United States, he wrote the lyrics in Vermillion and the song\'s working title was originally named after it. After Slipknot released a song from their third album entitled ""Vermilion"" and Mercury Rev released their sixth album with a song included on it also called ""Vermillion"", the band decided to change the title of the song. The name of the song was changed to ""Dakota"", named after the apartment building in New York. One of the song\'s signature lines, ""Take a look at me now"", was first used - in a similar melody - in their earlier single ""Since I Told You It\'s Over"".\nDakota is in use for both girls and boys. It was the 203rd most popular name for American boys in 2007, having ranked in the top 100 most popular names from 1995 to 2000. It first appeared among the top 1,000 most popular names for boys in the United States in 1985. It was the 239th most popular name for American girls in 2007. It has ranked among the top 400 names for American girls since 1991. The name has been in occasional use for both sexes in the United States since at least the 1940s.\nThe Eastern Dakota are the Santee (Isáŋyathi or Isáŋ-athi; ""knife"" + ""encampment""), who reside in the eastern Dakotas, central Minnesota and northern Iowa. The Western Dakota are the Yankton, and the Yanktonai (Iháŋktȟuŋwaŋ and Iháŋktȟuŋwaŋna; ""Village-at-the-end"" and ""Little village-at-the-end""), who reside in the Missouri River area. The Yankton-Yanktonai are collectively also referred to by the endonymWičhíyena, and have in the past been erroneously classified as Nakota.\nThe word Dakota means ""ally"" in the Dakota language, and their autonyms include Ikčé Wičhášta (""Indian people"") and Dakhóta Oyáte (""Dakota people"").\nThe Dakota are one of the three major linguistic groups of Siouan-speaking people (the others being the Lakota (Thítȟuŋwaŋ or Teton) and Nakota (Assiniboine), and one of the two major ethnic groups still speaking a relatively mutually-intelligible common language - D/Lakota. The Dakota include the following bands: Santee (Isanti), including the Bdewákaŋthuŋwaŋ (Mdewakanton), Waȟpéthuŋwaŋ (Wahpeton), Waȟpékhute (Wahpekute), Sisíthuŋwaŋ (Sisseton), and the Iháŋkthuŋwaŋ (Yankton), and Iháŋkthuŋwaŋna (Yanktonai).\nDakota was a brand of cigarettes introduced by R.J. Reynolds Tobacco (RJR) in 1990. Their marketing was targeted towards young blue-collar ""virile females"", in an attempt to displace the Marlboro brand without diluting Reynolds\' dominant Camel brand\'s appeal to males.\nAfter test-marketing in Houston, Tucson, Phoenix, and Nashville didn\'t yield the desired results,\nthe brand was withdrawn.\nPrior to the marketing campaign, its details were leaked to the Washington Post. In spite of the manufacturer\'s denial to have specifically targeted young females, this revelation sparked widespread discussion of targeted advertising in general in the media.\nA song is a single (and often standalone) work of music intended to be sung by the human voice with distinct and fixed pitches and patterns using sound and silence and a variety of forms that often include the repetition of sections. Written words created specifically for music or for which music is specifically created, are called lyrics. If a pre-existing poem is set to composed music in classical music it is an art song. Songs that are sung on repeated pitches without distinct contours and patterns that rise and fall are called chants. Songs in a simple style that are learned informally are often referred to as folk songs. Songs that are composed for professional singers are called popular songs. These songs, which have broad appeal, are often composed by professional songwriters, composers and lyricists. Art songs are composed by trained classical composers for concert performances. Songs are performed live and recorded. Songs may also appear in plays, musical theatre, stage shows of any form, and within operas.']"	['<urn:uuid:714fe37d-44e9-40b1-927c-693d39914bff>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	6	15	766
96	I've been researching the historical development of electrical stimulation therapies - could you explain how ancient pain management using electricity evolved into modern TENS technology, and how does this compare to today's cardiac pacemaker implantation procedures?	Electrical stimulation for pain control dates back to ancient Greece (63 A.D.), when standing on electrical fish was used for pain relief. Through the 16th-18th centuries, electrostatic devices were used for pain management, with Benjamin Franklin advocating this method. The first modern, wearable TENS was patented in 1974, initially used for testing pain patients before spinal cord electrode implantation. In contrast, modern pacemaker implantation is a sophisticated procedure performed under local anesthetic, taking about an hour in a cardiac catheterization laboratory. It involves creating a 3-4 cm incision below the collarbone, finding a vein for lead placement, positioning the leads in the heart, and creating a pocket for the pulse generator either under the skin or chest wall muscle.	"[""Basic science studies show that high and low frequency TENS produce their effects by activation of opioid receptors in the central nervous system. Specifically, high frequency TENS activates delta-opioid receptors both in the spinal cord and supraspinally (in the medulla) while low frequency TENS activates mu-opioid receptors both in the spinal cord and supraspinally. Further high frequency TENS reduces excitation of central neurons that transmit nociceptive information, reduces release of excitatory neurotransmitters (glutamate) and increases the release of inhibitory neurotransmitters (GABA) in the spinal cord, and activates muscarinic receptors centrally to produce analgesia. Low frequency TENS also releases serotonin and activates serotonin receptors in the spinal cord, releases GABA, and activates muscarinic receptors to reduce excitability of nociceptive neurons in the spinal cord.\nIn palliative care and pain medicine, TENS units are used in an attempt to alleviate neuropathic pain (pain due to nerve damage). Some patients benefit from this approach, while others may not, depending on individual differences , and pain threshold. Further use is documented in the attached references: in obstetric care, particularly in labour; knee pain; lithotripsy or bladder-stone removal; limb pain.\nElectrical stimulation for pain control was used in ancient Greece, 63 A.D. It was reported by Scribonius Largus that pain was relieved by standing on an electrical fish at the seashore. In the 16th through the 18th century various electrostatic devices were used for headache and other pains. Benjamin Franklin was a proponent of this method for pain relief. In the 1900s a device called the electreat, along with numerous other devices were used for pain control and cancer cures. Only the electreat survived into the twentieth century, but was not portable, and had limited control of the stimulus.\nThe first modern, patient-wearable TENS was patented in the U.S.A. on June 18, 1974 (). It was initially used for testing the tolerance of chronic pain patients to electrical stimulation before implantation of electrodes in the spinal cord dorsal column. The electrodes were attached to an implanted receiver, which received its power from an antenna worn on the surface of the skin. Although intended only for testing tolerance to electrical stimulation, many of the patients got so much relief from the TENS itself that they never returned for the implant.\nA number of companies manufacturing TENS appeared after the commercial success of the Medtronic device became known. The neurological division of Medtronic, founded by Don Maurer, Ed Schuck and Dr. Charles Ray, developed a number of applications for implanted electrical stimulation devices for treatment of epilepsy, Parkinson's disease, and other disorders of the nervous system. Maurer founded Empi, Inc. in 1977, and in the late 1980s purchased the TENS product line from Medtronic. Today many people confuse Tens with Ems. Ems and Tens devices look similar and both use long electric lead wires and electrodes. Tens is for blocking pain, where Ems is for stimulating the muscle.\nTENS Electrodes should never be placed:\nTENS should also be used with caution in people with epilepsy or pregnant women (do not use over area of the uterus as the effects of electrical stimulation over the developing fetus are not known). TENS should not be used by people with an artificial cardiac pacemaker due to risk of interference and failure of their implanted device. Possible failure of these warnings can result in a cardiac arrhythmia."", 'What is a pacemaker?\nA pacemaker is an implanted device that monitors your heartbeat and prevents your heart from beating too slowly. It consists of a box (‘pulse generator’) attached to one or two wires (‘leads’). The pulse generator is usually placed underneath the skin or muscle beneath your collarbone. The lead or leads pass inside the large veins of your chest from the generator to your heart. The leads transmit information about your heartbeat to the generator. If your heart beats too slowly the pacemaker sends out a tiny electrical impulse to ‘pace’ the heart, i.e. to make it beat at an appropriate speed. If your heart then speeds up again, this is detected by the pacemaker, which then stops pacing your heart.\nWhat does pacemaker implantation involve?\nA pacemaker is usually implanted under local anaesthetic; you can also have some sedation to make you feel sleepy and more relaxed. The procedure generally takes about an hour and is performed in a ‘cardiac catheterisation laboratory’. Your heart rhythm, blood pressure and the oxygen level in your blood are monitored continuously. The doctor puts local anaesthetic under the skin below your collar bone. The anaesthetic stings for a few seconds as it gets to work but the area rapidly goes numb. An incision about 3-4 cms long is made below the collarbone. The next stage of the procedure is for the doctor to find a vein through which the leads are passed to the heart. Once the leads are in the veins they can be positioned in the heart. This is not in any way uncomfortable although you may feel a few extra heart beats for a short period. Care is taken to find the ideal position for the lead or leads and you will hear numbers being called out, referring to measurements made through the leads. When the doctor is happy that the leads are in the best position, they are stitched in and a pocket is created to house the pulse generator. This pocket may be underneath the skin or deeper, under the muscle of the chest wall. Finally the incision is closed up with stitches. For the skin stitch some doctors use a dissolvable suture while others prefer a suture that is taken out at your doctor’s surgery after 7 days. You will be told which you have.\nCan complications occur?\nThis procedure cannot be performed without some element of risk; you need to understand the risks before you sign the consent form. It is important that you understand that these risks are potential and unpredictable. The majority of patients, however, have no problems. The potential risks are listed below for your information:\nPneumothorax (air leak around the lungs): this occurs in approximately 1 in 100 patients. The veins used to get the pacemaker wires to your heart lie very near to the lung. It is possible to nick the lung on the way past causing an air leak. Sometimes this resolves on its own. Occasionally a chest drain is needed; if so your stay in hospital may be delayed by several days.\nPericardial effusion (blood leak around the heart): this occurs in approximately 1 in 500 patients. When the wire is positioned in the heart it can cause a fluid collection around the outside of the heart. Sometimes this resolves on its own but occasionally a drain is needed; very rarely an operation is needed. If so your stay in hospital may be delayed by several days.\nInfection: this occurs in approximately 2 in 100 patients. Antibiotics are given routinely to help prevent this. In the unlikely event that the pacemaker does become infected, it will probably need to be removed and a new pacemaker implanted.\nHaematoma (severe bruising): this occurs in approximately 4 in 100 patients. Rarely this bruising may need drainage. Patients taking blood thinning drugs are more prone to haematoma formation.\nWire displacement: this occurs in approximately 2 in 100 patients. This causes the pacemaker to pace inappropriately. In most cases the wires need to be repositioned requiring further surgery.\nDeath: this is fortunately an extremely rare complication\nWhat to expect after pacemaker implantationYou can expect your shoulder and arm to be sore for a few days afterwards. Simple pain-killers like paracetamol are usually helpful. It is important to keep the arm and shoulder moving, to avoid a ‘frozen shoulder’; however you should be gentle and not use the arm for heavy lifting for a few weeks, until your first pacemaker check. Try not to lift your arm above shoulder height again until the first check. You should avoid getting the wound wet for about a week after the implant. The wound usually appears a little bruised for several days afterwards. Although the bruising may become more prominent after you go home, any discomfort and redness around the wound should get better rapidly. If you have concerns about the wound it is important that you contact us directly.\nFollow-up after pacemaker implantationAfter discharge from hospital you will need to be followed up regularly. The first visit is usually about six weeks after implant and usually every 9-12 months thereafter although there is some variation. At these visits your pacemaker is checked to make sure that it is working well and to make any adjustments necessary to the way it is programmed. This visit usually lasts about twenty minutes.\nWhat happens when the battery runs down?Many factors determine how long a pacemaker battery will last. Currently available generators typically last about 7-10 years under standard conditions. The pacemaker battery behaves in a predictable fashion. By following you up regularly we can determine accurately when the pacemaker generator will need replacing. The pacemaker will be functioning perfectly normally throughout this time and you will be readmitted in a planned fashion to replace the pacemaker generator. When the generator is replaced the procedure is similar to that of the original pacemaker implant although we aim to use the original leads and simply attach a new generator.\nIs there anything you should avoid with a pacemaker?In almost every respect life with a pacemaker is completely normal. Household appliances and mobile telephones do not interfere with your pacemaker’s function, although you should not keep your mobile telephone in a pocket directly over the pacemaker. You will receive a pacemaker card giving full information about your pacemaker. If you are flying you should hand this card to security to show you have a pacemaker and you will not have to go through the security gate. If you are concerned about interference from a particular piece of equipment at home or at work, ask your consultant or the cardiac physiologists in the pacemaker clinic.\nDriving after a pacemakerYou should not drive for one week after pacemaker implantation if you hold an ordinary (group 1) licence or six weeks if you hold a vocational (group 2) licence. The DVLA rules are revised periodically and the current rules can be viewed at http://www.dvla.gov.uk/drivers/dmed1.htm.\n- Pacemaker (PM)\n- Implantable Cardioverter Defibrillator (ICD)\n- Cardiac Resynchronisation Therapy (CRT)\n- AF Ablation\n- Atrial Flutter Ablation\n- WPW Ablation\n- AVNRT Ablation\n- AT Ablation\n- VT Ablation\n- AV Node Ablation']"	['<urn:uuid:d1ba6c6f-34f0-4e98-a13b-b63a37818e55>', '<urn:uuid:01083671-1769-49f9-b5fc-305124ee8ce8>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:56:11.774636	36	119	1753
97	trying understand essential factory computer protection measures common security methods	Several key cybersecurity controls are commonly implemented in manufacturing environments, including: network segmentation for a 'zone and conduit' design; firewalls, routers, and 'data diodes'; role-based access level implementations with automatic logouts; password security improvements (removing default passwords, developing complexity guidelines, creating rotation policies); defense-in-depth coverage of applications and operating systems; and blocklist and allowlist methodologies. Additionally, operational controls include creating and updating engineering documentation and developing standard operating procedures (SOPs).	['Protect Your Plant Floor\nIt is crucial for manufacturers to have procedures and systems in place that help identify, eliminate, and mitigate industrial cybersecurity and networking challenges. Maintaining these procedures and systems requires the development and execution of a cybersecurity strategy. Since developing a comprehensive cybersecurity strategy can be difficult, we offer a variety of cybersecurity solutions designed for new and existing manufacturing environments and cybersecurity services for projects at any stage of the project lifecycle. Below is a summary of our cybersecurity capabilities.\nPrior to creating or modifying your cybersecurity strategy, we recommend performing a cyber posture assessment to determine the current state of your organization’s cybersecurity efforts. Our cyber posture assessment involves conducting a best practice review and hosting a collaborative workshop that focuses on the end goal of implementing the right standards and strategies to get your organization where you want it to be. The end goal of this posture review is to unify efforts and expectations across stakeholders within the organization.\nFor organizations that have not yet adopted cybersecurity standards and standard operating procedures (SOPs), we can work side-by-side with key stakeholders to advise on proper requirements development and create or modify standards and procedures. We also can advise on how these standards will apply throughout the system lifecycle and affect RFQ, requirement, and test documentation to ensure implementation of secure systems.\nWhen an organization already has a defined set of cybersecurity standards in place, we can perform a cybersecurity assessment on the manufacturing systems and networks to identify areas with excessive or unrecognized risks. During this assessment, we compare the actual systems operating to the agreed-upon standards as well. Assessment deliverables typically include a report documenting the current state, existing gaps, and prioritized recommendations for mitigating risks.\nAfter assessing an organization’s current cybersecurity practices, we can develop and help implement a mitigation plan that uses a variety of controls to alleviate or eliminate discovered cybersecurity weaknesses.\nFor existing systems, we can consult on current designs and provide modifications as needed. For new systems, we also can include the following cybersecurity design services as part of a bigger project:\n- Network architecture design\n- Firewall planning and utilization\n- SOPs and policies\nCyber Security Controls Implementation\nWe regularly implement a wide variety of cyber controls as stand-alone cyber projects or within an automation project including the following:\n- Network design documents and/or modifications including:\n- Network segmentation for a “zone and conduit” design\n- Firewalls, routers, and “data diodes”\n- Active Directory servers\n- Role-based access level implementations, including automatic logouts where appropriate\n- Password security improvements, including:\n- Removing default passwords\n- Developing password complexity guidelines\n- Creating password rotation policies\n- Defense-in-depth coverage of both application and operating systems (OSs), as well as OT-specific hardware\n- Blocklist and allowlist methodologies\n- OS upgrades and/or patching\n- SCADA, HMI, and other software upgrades to support the above items\n- Operational Controls including:\n- Creating and updating engineering documentation\n- Drafting or modifying SOPs\nTesting/Validating Implemented Controls\nBased on an organization’s needs, we can carry out system testing following a new controls implementation or other system modifications or as part of functional testing of a new system. The different levels of system testing we can perform range from simple configuration verifications to document generation to using tools such as Wireshark and Nmap to validate that systems conform to specification.']	['<urn:uuid:991610aa-50eb-459d-94be-96b3650ecb42>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T04:56:11.774636	10	70	563
98	How do bot flies affect horses throughout their lifecycle, and what seasonal patterns are observed in their egg-laying behavior?	Bot flies lay eggs on horses' legs, chest, jaw and face. Once the eggs mature into larvae, they crawl toward the mouth and develop in the tongue or gums before being swallowed and colonizing the stomach lining for nearly a year. Regarding seasonal patterns, there is a significant peak in egg-laying activity in mid-December and mid-January, with more than 150 eggs counted during these peaks. The egg-laying activity gradually decreases until April and completely ends in May. Rainfall has a negative influence on the egg-laying behavior of these parasites.	['Las tres especies más importantes son Gasterophilus intestinalis, G. nasalis y G. partes del mundo, las tasas de infección larvaria son altas entre los equinos. Período de oviposición de Gasterophilus nasalis y G. intestinalis en equinos. the egg laying period of Gasterophilus nasalis and to examine whether there is a . Gasterophilus intestinalis was found in % and G. nasalis in % of larvarias de Gasterophilus en equinos sacrificados en el rastro de Ixtapalapa.\n|Published (Last):||25 December 2016|\n|PDF File Size:||20.7 Mb|\n|ePub File Size:||7.73 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nAdult bot flies then emerge and the cycle repeats. In March and Aprila consistent increase in the numbers of eggs laid by G. These are diptera larvae flies of Gasterophilus gender; G. It was concluded that in Chile, G.\nGastric arthropode larvae\nNo, peripheral nerve sheath tumors Nodular lesions in a horse liver The case of the pig with its belly full of foam Haemorrhages in a pig carcass: You can help Wikipedia by expanding it. Larvae can cause small ulcers surrounded by a margin of hyperplastic squamous epithelium and inflammation of the gastric mucosa, but usually do equinis have much pathological relevance. This page was last edited on 8 Septemberat Climatic data were obtained from a meteorological station close by.\nIn other projects Wikimedia Commons. From Wikipedia, the free encyclopedia. Gasterophilus LeachSpecies G.\nPeríodo de oviposición de Gasterophilus nasalis y G. intestinalis en equinos: VIII Región, Chile\nGreen mass on the urinary bladder of a pig carcass. You can leave a response or trackback from your own site. The quantity of eggs laid in the intermandibular region of the 10 horses, peaked in mid December eggs and again in mid January eggs. Gastric arthropode larvae Presence of larvae in the cardia of the stomach of a Catalan Pyrenees, female, 12 months old horse. The egg laying period of G. Gasterophilus, oviposition, equine, Chile.\nAfter each egg count, a sample of eggs was removed for laboratory analysis while the remaining eggs were stained with an aqueous solution of methylene blue in order to distinguish them from those eggs laid during the following two weeks.\nPor otra parte, se demuestra la presencia de G. This veterinary medicine —related article is a stub. The egg laying period of G. Views Read Edit View history.\nThe adults have non-functional mouthparts, so they cannot feed. In summer the larvae are shed within the feces as a pupae. Follow any responses to this post through RSS 2.\nThere was a problem providing the content you requested\nGasterophilus are nonpathogenic, and seldom cause disease unless large numbers of larvae are present. Afterwards, the quantity of eggs laid dropped to every two weeks until the beginning of April and ended completely in May Gasterophilus Leach Climatic data were obtained from a meteorological station close by. This article related to yasterophilus of the muscomomoph flies superfamily Oestroidea is a stub.\nMore than eggs were counted in the 10 animals, confirming the presence of G.\nThese eggs are laid on different portions of the body according to the euinos Gasterophilus species. Rainfall negatively influenced the egg laying of the parasite. The quantity of eggs laid in the intermandibular region of the 10 horses, peaked in mid December eggs and equnios in mid January eggs. The horses were kept in a pasture without antiparasitic treatment during the study.\nSe concluye que G. Later they migrate to the periodontal space, where a purulent exudate can be observed, particularly on the lingual surface of the upper molars.\nGasterophilus – Wikipedia\nEgg laying period of Gasterophilus nasalis and G. Articles with ‘species’ microformats Wikipedia articles incorporating a citation from The American Cyclopaedia Equlnos articles incorporating a citation from The American Cyclopaedia with a Wikisource reference All stub articles. In March and Aprila consistent increase in the numbers of eggs laid by G.\nThe moment when G.', 'While there is the potential for a horse to be infected by more than 150 different species of internal parasites, there are four main types: bloodworms, small strongyles, roundworms and tapeworms.\nLarge Strongyles (Bloodworms)\nBloodworms, such as the large strongyles (Strongylus vulgaris), wreak damage as they migrate through the interior of the equine bowel and attach to the lining of the cecum or colon. They cause hemorrhage, irritation, ulcers of the intestinal lining, and/or anemia.\nInvasion of the cranial mesenteric artery (the primary blood supply to the intestinal tract) by Strongylus vulgaris larvae is termed verminous arteritis. Blood flow can be reduced by 50% from inflammation at the root of the artery and thrombi can form, potentially dislodging to block smaller blood vessels supplying the intestines. Even mild interference with intestinal blood flow leads to abnormal motility problems and the potential for diarrhea, weight loss, and/or colic.\nTwo other forms of large strongyles exist: Strongylus edentatus migrates through the lining of the intestinal tract (peritoneum) while Strongylus equinus journeys through the liver and pancreas. Inflammation in these organs can lead to unthrifty condition or colic.\nSmall strongyle (cyathostomins) larvae encyst within the large intestinal lining for a period of arrested development, sometimes as long as two years. Once mature, the larvae emerge from their cystic sacs, creating physical damage to the bowel along with release of debris and byproducts that elicit focal inflammation of the bowel. Synchronous emergence of encysted larvae (thousands at a time) can seriously compromise a horse’s health from diarrhea, weight loss, poor growth, low blood protein, anemia, and/or colic.\nParasite-related problems from small strongyles are more prevalent in horses stressed by poor management, inadequate nutrition, illness, crowding, transport, training and competition or those with poor immune systems or Cushing’s disease. Subclinical issues can occur in more robust individuals, such as decreased feed efficiency and/or diminished performance quality.\nAscarids are roundworms that particularly affect young horses less than 18 months of age until the youngster can mount an effective, native immune response. A heavy infestation causes a distinctive unthrifty appearance—pot-bellied and a rough hair coat—along with weight loss, poor growth, lack of energy and colic. Ascarids can also trigger respiratory problems—fever, cough, nasal discharge, and possible pneumonia —as they migrate through the lungs on their journey to the intestinal tract.\nThe most concerning ascarid problem is small intestinal blockage by a tangled mass of 10-inch long adult worms, particularly following deworming treatment in a horse that has not been on a regular parasite control program.\nTapeworm infections require an intermediate host, the oribatid mite, to perpetuate their life cycle. A horse on pasture or eating contaminated hay might ingest mites infected with tapeworms. This initiates development into a mature tapeworm in 6-10 weeks. Its sucking mouthparts attach to the intestinal lining, creating localized Inflammation near the junction of the small intestine (ileum) and cecum. Potential ulceration, scar tissue and thickening of the intestinal wall can occur.\nTapeworm-infected horses are eight times more likely to develop spasmodic (gas) colic than uninfected horses. Other serious repercussions of tapeworm infection include ileal impaction, small intestinal rupture, or an intussusception (the bowel telescopes within itself). These conditions occur due to motility derangements created by inflammation and disruptions to nerve conduction in the bowel.\nBotflies lay their eggs on the hairs of a horse’s legs, chest, jaw and face. Once bot eggs collect on a horse’s muzzle through scratching or direct fly deposits, they mature into larvae that crawl toward the mouth. Development occurs in stages within the horse’s tongue or between the teeth in the gums and interdental spaces. When swallowed, bot larvae colonize the lining of the stomach, residing there for nearly a year. Upon release, they quickly transit through the bowel, pass through the manure, and burrow into the ground to pupate and hatch into flies in late spring.\nDamage caused by bot larvae is mostly restricted to erosions in the mouth and stomach. Most horses are relatively unaffected by their presence.\nOther Worm Types\nOther internal parasites that can create problems for the horse include:\n- Pinworms elicit tail rubbing.\n- Threadworms cause diarrhea in foals.\n- Lungworms, obtained through contact with infected donkeys, cause respiratory problems.\n- Migration of Oncocherca cervicalis filaria beneath the skin causes intense itching.\n- Stomach worms (Habronema and Draschia) hatch into flies that cause summer sores.']	['<urn:uuid:f19837fd-79d5-4b34-b51e-f5a89e77f4c6>', '<urn:uuid:85f30699-530c-4e54-b686-8688a6f4aec1>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:56:11.774636	19	89	1367
99	apartment building entrance security features prevent burglary according experts	Security experts recommend a building should have a well-illuminated facade and entrance at night, a strong locked exterior door, and a locked inside door to the staircase or elevator. Even better is having a communal entrance monitored by a receptionist or concierge.	['Even with my police background, I sometimes wonder if it’s truly possible to avoid a home burglary, or if it’s only a matter of good or bad luck. One thing for sure is that there is never a lack of victims filing reports. In 2014, 378,776 burglaries were reported in France alone. And these burglars know no limits.\nWhen I entered police school, I was surprised to learn that burglars will often even steal your blankets in order to protect your TV during the post-theft transportation! Another surprising (and sad) find was that many victims are so traumatized by having their privacy violated that they often end up moving within one year following the burglary.\nLuckily, security experts know a few secrets that can help limit your risk of burglaries, starting with carefully choosing or examining your lodgings.\nApartment shopping: what to look for?\nFirst of all, whenever you are house hunting, you can look at each potential place in the eyes of a burglar, using our pro tips:\n- Lighting: The facade and entrance of the building must be well-illuminated at night\n- Entrance: The building should have a strong, locked exterior door and a locked inside door to the staircase or elevator. Even better is a communal entrance monitored by a receptionist or concierge.\n- Floor choice: Avoid apartments on the ground or first floors (easy access for the burglars!)\n- Door: The best apartment doors are made from solid wood or metal, with a 3-point deadbolt lock and anti-pry metal plates (plates that are bolted to the door and cover the gap between the door and door frame).\n- Balcony: If there is a balcony, it should not be accessible from the neighboring apartments, or by climbing the drain pipes, etc.\n- Windows: All windows should be equipped with roll-down or hinged shutters.\nWhat can I do to protect my current lodging?\nIf you aren’t currently in the housing market, you can still protect your home by investing in one or more of these anti-burglary gadgets:\n- Cameras: Security cameras, real or fake, depending on your budget\n- Anti-pry strips: They prevent criminals from prying the door away from its frame.\n- Alarm: Real or fake, again depending on your budget (fake alarm keypad, fake stickers…)\n- Anti-climbing devices: Some apartments are very easily accessible by climbing gutters and balconies. Metal spikes that are often used to deter birds can also deter thieves!\n- Window protections: Roll-down shutters and shatter-resistant window films can also be useful because windows are usually the easiest point of entry into a home.\nPolice Tips for Preventing Burglaries\n- Use a radio to fool potential burglars into thinking someone is home!\nThe French Police reminds us that burglars often take advantage of our mistakes, like leaving a door unlocked or a window open, or hiding our keys under the carpet, the planter, or leaving them in the mailbox (burglars will find it!) Another very good piece of advice is to turn on your light, radio or TV on when you leave your home, in order to fool a potential burglar into thinking someone is home.\n- Use light to make burglars feel more vulnerable!\nScotland Yard insists on having a strong door (reinforced with metal strips), a well-illuminated home (makes burglars feel more vulnerable and at risk of being seen) and an alarm with flashing lights!\nHow can I protect my precious valuables?\n- Computer: I have met many victims who have lost all their personal data and pictures because of a burglary. I can tell you this is devastating. My advice is to have an automated virtual backup (ex: Dropbox, Google Drive, etc).\n- Wine Collection? One last piece of advice: if your home also comes equipped with a French cave, don’t fall into the trap of so many unfortunate victims and store your most prized possessions in an unprotected basement. Storage closet/locker thefts are some of the easiest targets for thieves because of their underground location and often shoddy construction. So if you’ve spent the past 10 years building up a serious wine collection, don’t forget to take safety into consideration along with the humidity and temperature of the room!\n- Cash, Jewelry and IDs: A safe, securely anchored to a solid surface and, ideally, concealed, is the best way to protect your valuables and sentimental items.\nMore reading on how to protect your home:']	['<urn:uuid:590e3bfe-1b7b-40ef-a9a1-f23a9985bfb0>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T04:56:11.774636	9	42	735
100	I spotted a bird with red chest in my garden. Could it be a robin?	Yes, that's likely a robin, which is one of the easiest garden birds to identify. They have a distinctive reddish-orange chest that puffs in and out as they breathe. Robins are commonly seen before spring starts and are known to be attracted to gardens by food sources like peanuts, earthworms, mealworms, fruits (apples, pears, plums) and suet cakes. They also like gardens with nest boxes.	['Interested in the birds that seem to regularly visit your garden, but don’t know much about them? They’re called garden birds. Garden birds are species of birds which frequently visit gardens in search of resources such as food, water and shelter. Most gardens with their flowers, meadows, trees, fruits and vivid display of colours are a hotspot attraction for many insects, birds and animals. Garden birds are no exception to this! They are drawn to sturdy trees and glossy shrubs, any place where they can find food or water. Lawns and yards are the most ideal habitat for garden birds like these.\nWhy should you be welcoming of birds in your garden?\nGarden birds are part of the natural ecosystem; they help maintain a healthy balance in your garden. Bird droppings are rich in elements such as nitrogen, potassium and phosphate – all of which are key to helping a plant grow. These are the ingredients that you look for in fertilizer when buying one over the counter. Bird droppings therefore serve as nature’s fertilizer for plants and foliage in your garden and you shouldn’t try to get rid of them!\nBalance Food chain\nBirds also eat small animals, plants and insects such as earthworms. They can also help control the spread of insects and pests in your garden. Garden birds serve a key role in the food web and larger feeding relationship between different organisms in any habitat.\nSeveral birds are also ‘nectar-feeding’ which means that they actually help move pollen across the landscape. This can help the flora in your garden to flourish. Common examples of such birds include the hummingbird and sunbird.\nHelp seed dispersal\nBirds can also help disperse seeds if they are a fruit-eating species. Often the seeds can pass through their digestive tract, completely unharmed by the body’s chemicals, and are excreted in faeces. This can help plants grow in new places.\nWhy are garden birds important?\nApart from offering a long list of environmental and natural benefits to the land, you should value garden birds as a species whose population has greatly dwindled down over the years. Garden birds used to be extremely common once upon a time, but their numbers have greatly fallen since the advent of urban development. Several of their habitats and homes have been destroyed at the hands of agricultural intensification and land cleared up for industrial/agrarian purposes.\nBiodiversity of garden birds in the UK\nThe irony is that most of the ‘natural’ habitat for garden birds are not found in wilderness areas such as forests and wild fields, but rather in man-made gardens themselves. Acres of land formed by gardens spread across the UK form a larger habitat for garden birds compared to natural reserves. Several species are in trouble such as the house sparrows which have been brought down to 29% of their original population.\nWelcome Garden Birds in your space\nFor this reason, creating welcoming gardens is one way to be hospitable towards these poor garden birds who have already had such a tough time. Moreover, it’s a two-way street where both your garden and the birds that frequently visit it mutually benefit each other.\nWhat are a few common birds that you will come across in your garden?\nDifferent seasons may invite different varieties of garden birds to your home. Here are a few of the most common garden birds in the UK that you are most likely to see in your garden.\nThe Robin is one of the easiest birds to identify in any garden. Robins are typically attracted to gardens by food sources such as peanuts, earthworms, mealworms, fruits (apples, pears, plums) and suet cakes. They are also drawn to gardens with nest boxes in them because they enjoy tucking themselves in the bird hole.\nA robin is extremely easy to spot in the wild, with its reddish-orange chest puffing in and out as it breathes like a warrior. With its beady eyes and feathery gown, the bird is most commonly season before spring starts. The bird has long represented good luck and been symbolic for Christmas.\nThe Great Tit is the largest bird of the tit family, in all of the UK. This bird is also fairly easy to spot with its elliptical blue head poking out, above soft patches of feathery white around its cheeks. With a tuft of golden-yellow feathers around its chest, wrapped in deep-sea blue – the Great Tit can be a colourful visitor for your garden.\nThe bird is most popular for its two-syllable song that is a giveaway. Next time you hear its friendly chirping, you will know who has decided to drop by for a visit.\nThe goldfinch is an extremely vibrantly patterned bird, coloured all over in shades of bright red and orange with yellow patches all over its wings. The bird itself is quite friendly and sociable, although it breeds in lose numbers.\nThey’re fine beaks are suited to being able to extract seeds with ease. If you want to attract goldfinches to your garden, consider adding bird tables and feeders. Don’t expect visits from it during winter, because most of them migrate during the cold season.\nEven though this bird species is called ‘blackbird’ the name holds completely true for only males. If you see a bird with a bright orange beak, lathered in brown-black feathers with spots and streaks across the chest area – you might be looking at a female blackbird!\nThe blackbird is quite easy to identify in broad daylight, but past dusk it can hide well in dark corners. But don’t worry, sometimes you will be able to identify it by the mellow song it sings and know that a blackbird has visited you.\nThe Bottom Line\nGarden birds can add more life to your space and entertain you with their stunning songs. If you want to enjoy the benefits of garden birds as well as enjoy their chirping every morning, make your gardens welcoming enough for the local garden birds. You will not regret hosting these guests!']	['<urn:uuid:7e82961e-4271-4a66-901b-1ce03d11ec43>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:56:11.774636	15	65	1010
