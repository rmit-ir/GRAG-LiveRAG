qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	I care about wildlife and nature - how do rain gardens support local ecosystems, and what are the ways to make outdoor spaces more environmentally friendly through water management?	Rain gardens significantly support local ecosystems by creating diverse and healthy environments for wildlife. They are planted with native plants that attract pollinators, which helps regulate the ecosystem by maintaining plant diversity and improving the health of local pollinators - crucial for food production and natural habitats. Regarding environmentally friendly water management outdoors, you can make your space more eco-friendly by using mulch in gardens (which reduces water evaporation by up to 70%), watering plants in early morning or evening to minimize evaporation, installing rain barrels to collect mineral-rich rainwater for plants, and checking soil moisture before watering (if soil is moist 10cm below the surface, watering isn't needed). These practices help preserve water resources while supporting local wildlife.	"['What Is a Rain Garden and How Does it Work?\nRain gardens are shallow depressions filled with loose, deep soil to allow for the natural filtration of stormwater. They are planted with low maintenance native plants that attract pollinators. By adding rain gardens to your landscaping you can significantly reduce the risk of flooding on your property while preventing water pollution and supporting the biodiversity of our Toronto neighborhoods.\nBy putting in that rain garden, you are basically putting in a living sponge around your home and building a little line of defense that again really helps to offset the stormwater that flows in the lake with a great force.\n– Marc Yamaguchi, Rain Gardens United\nCheck out this video by Rain Gardens United about making Toronto rain garden friendly:\nThe Many Benefits of Building a Rain Garden\nSignificant Reduction of The Potential of Flooding in Your Home\nAside from making your property more attractive, a rain garden is also a practical preventative measure that can greatly benefit your home. The garden greatly reduced the potential of flooding, drainage problems, and stream bank erosions by acting as a barrier between rainwater and your home.\nPreserves the Natural Water Cycle and Protects Local Rivers, Lakes, Fish and Drinking Water Sources\nCanada has 7% of the world’s renewable fresh water.\nAside from benefiting your own property, these gardens are ecologically friendly and help build resiliency for cities by combating the effects of climate change. The filtration process helps the natural water cycle as well. By limiting the amount of water that enters the local storm drain system, the garden reduces the number of pollutants from yards and roads that run into waterways.\nCreates a Diverse and Healthy Ecosystem\nImproving the health of bees and other pollinators is a necessity. Without pollinators, much of the food we eat and the natural habitats we enjoy would not exist.\nNative plants attract pollinators which helps regulate the ecosystem by maintaining a diversity of plants and can help improve the health of Toronto’s pollinators by making your outdoor space pollinator-friendly.\nLow Maintenance solution\nRain gardens are planted with beautiful and hardy plants which don’t require a lot of upkeep because they require little to no watering which results in a low maintenance garden that looks beautiful. Native plants are perennials that will return every year, making your landscaping look more luscious as time passes.\nhow to Build a Rain Garden\nStep i: Dig a depression.\nMake sure to call Ontario One Call before you dig! Once your locates are marked, and you’ve decided on the location, the size and the style of your new garden, it’s time to start digging! Ideally, rain gardens should be dug to a depth of approximately 85cm or deeper if you want a smaller garden.\nStep ii: create a soakaway & connect the downspout\nA soakaway area balances your garden and is a key to the effectiveness of your garden. A soakaway pit is a hole in the ground filled with a permeable material like gravel to allow for water to percolate through into the soil.\nFirst, we add a layer of ¾ crushed stone wrapped in filter cloth landscaping fabric. Then we top this with an absorbent soil mix which usually contains a combination of compost and sand. This mix is designed to allow water to infiltrate the garden easily and quickly.\nStep iii: add plants & river rock\nThis is the fun part! There is a range of native plants to choose from that are both water-tolerant and drought-tolerant. You can choose the plants that fit your aesthetic! We then add river rocks and boulders on the top layer of the garden to help slow the velocity of water coming out of the downspouts.\nIf you’re looking for plants in the GTA, we highly recommend the nursery Native Plants in Claremont, they’re committed to promoting Ontario’s native plants and specialize in seed grown native perennial wildflowers, grasses, and shrubs.\nStep iv: top with Mulch.\nOnce all of the plants are in, we cover your new garden with a layer of mulch or wood chips. This layer is from 5-10cm and is an essential part of the process. The mulch helps with the filtration and absorption of rainwater and assists in keeping out weeds while also resisting drought. Lastly, it ties your garden together by giving it a neat and tidy appearance.\nDon’t Want to Build it Yourself?\nLet Us Do the Heavy Lifting!\nSchedule a rain garden consultation\nIf you live in Toronto or the GTA and would like to hire someone to install your rain garden get in touch with us to schedule a consultation and estimate! We work with the best people to provide a quality service and experience.\ndecide on a concept design and any additional landscaping elements or stonework\nWe work with the experts, Rain Garden United, to put together a plan and concept design to create the best option for your property. We’ll consult with you to decide on the location, size, and style of your rain garden. Want to add an interlock patio or natural stone steps? Inka Stoneworks specializes in hardscaping and can handle any additional elements that you’d like to add to your project.\nHow long will it take to install? The project installation could range from 1-2 days for a small garden and up to a week for a large garden.\nHow much does a rain garden Cost?\nBuilding a rain garden can be a cost-effective landscaping solution for your yard. If you want to hire a contractor to install the garden for you, the price will depend on the size and scope of the project. Remember, this is an investment! At Inka Stoneworks, installing a rain garden will range anywhere from $3000 to $15,000 depending on the size and complexity of the landscaping project. This includes the project consultation and plan, excavation, selecting the right native plants, installing the garden and disposal of any unwanted materials.\nWant to learn more about rain gardens? Check out Rain Gardens United, they’re a non-profit dedicated to building rain gardens, educating the public about stormwater management, and lobbying governments to include rain gardens as part of that management. Another great resource is TRCA’s guide to building and maintaining a rain garden.\nIf you’re looking for a solution to manage stormwater on your property, rain gardens can be the perfect option. By investing in your landscape you’ll add curb appeal while protecting the environment.', ""Why Save Water?\nWhy with a river the size of the Clutha flowing through our district, do we need to save water?\nWhile there is not necessarily a water shortage in the district, it costs money to treat and distribute water for domestic consumption. Saving water is good for our pockets and good for the environment.\nAs ratepayers we pay for both the water we use and the infrastructure needed to treat and pipe that water.\nReduced water use means lower total water production and wastewater treatment costs. Using only the water we actually need not only saves money now it also puts off the need for more infrastructure such as dams and reservoirs, pipelines, and water treatment plants and even the need to find new water sources to service a growing demand.\nSaving water also helps reduce wastewater treatment costs. The water used around your home including your washing machine, bathrooms and toilets, is discharged to the sewer for treatment and disposal at the wastewater treatment plant. Saving water also helps delay the need to increase wastewater treatment plant capacity and reduces the volume of wastewater treated and discharged into the rivers and waterways.\n- Pumping water from the source, treating it and piping it to your door is expensive. If we all use less water we can lower the cost of providing it.\n- Excessive summer water use means the water supply system has to work hard to keep up with demand. In some places excessive use can affect availability for fire fighting.\nEvery Drop Counts\nWhen it comes to conserving water, every drop counts. You can do your bit to help save water by:\n- Reporting leaks from hydrants, a toby (your main water shut-off valve) or anywhere else\n- Fixing leaking taps, toilets, showerheads and other water-using devices around your home or business.\n- Learn how to use your meter to check for water leaks. Most are easy to detect and repair with basic know-how and the right tools.\nFor water saving tips around your home, school or workplace check out the sections below.\nCouncil's Water Conservation Efforts\nCouncil is also working to reduce its water use by:\n- Installing more efficient watering systems on town reserves and gardens and using untreated bore water where possible\n- Providing public education campaigns promoting wise water use.\n- See more about our Water Conservation Garden located in Cromwell.\n- Read the Changing How We Use Water brochure Council recently produced in conjunction with Jo Wakelin from Otago PolytchnicCentral Campus, which gives great advice about landscaping with less water in Central Otago.\nSave Water at Home\nThere are many ways to save water around your home. Much of the water we use is wasted and a few small changes can help save water and we don’t mean by going short or without water. Try these:\n- Singing shorter songs in the shower: every minute that you don’t spend in the shower saves you seven litres of water. Reduce your shower by one minute, and you’ll save about 2500 litres of water a year.\n- Turning off the tap when brushing teeth or shaving: a running tap wastes 10 litres per minute.\n- Plug that sink when you’re preparing vegetables. It’ll save you up to 10 litres per minute.\n- No half measures in the kitchen or laundry: a full load of dishes or laundry uses less water and power than two half loads.\n- A scrape will do! Today’s modern dishwashers and detergent mean there’s no need to pre-rinse, just scrape and pop the dishes in the machine – give it a go, you might be surprised by the results.\nOther things you can do include:\nCheck for leaks\nLeaking taps, toilets, showerheads and other water-using devices are major wasters of water. Just one leaking tap can waste 2000 litres per month and increase your water bill unnecessarily. Most leaks are easy to detect and easy to repair with basic know-how and the right tools. Use your water meter to check for leaks.\nUse Water saving devices\nThere are many water saving devices you can buy that reduce your water use – flow regulators (to reduce the flow of water), dual flush toilets and water displacement devices that go in the cistern (‘gizmos’) for single flush toilets. Many devices aren’t very expensive.\nUse water wise appliances\nShop with the stars! Save water and money in the long run by choosing water wise appliances. Look for the stars. The more stars on the label the more water efficient the appliance. So if you’re replacing an old washing machine or toilet choose models with four stars or more. More information about water efficiency.\nSave Water at School\nSave water around your school. Look for water saving plumbing solutions and promote good water use habits. Water-saving mechanisms in schools educate young people about using water wisely and the need to protect the environment. What children see at school can influence behaviour at home.\n- Fix Leaks\nSchools can save water by promptly fixing all leaks. Students should be encouraged to notify teachers as soon as they detect a problem in a restroom or water fountain. A water audit of your school can find hidden leaks that can waste lots of water and money.\n- Install water-wise plumbing\nSchools can install waterless urinals, dual-flush toilets, and taps that turn off automatically after a set amount of time.\n- Promote Wise Water Use\nTalk to children about smart ways to use water such as hand washing techniques and turning taps off.\n- Incorporate water-wise landscaping\nUse plants that don't require much water in landscaped areas. In addition, water crystals can be added to soil to enhance water retention by as much as 40 per cent.\nHard surfaces should not be hosed except in exceptional circumstances. Where possible use a broom or a mop.\nSave Water at Work\nWater is a valuable resource yet all too often we take it for granted. Whether your business is large or small it can benefit from using water wisely. Using water wisely is part of planning for a sustainable future and demonstrates corporate responsibility.\nBenefits of saving water at work:\n- Save your business money. Saving water will have a direct and positive impact on your water bill. Reducing hot water use will also reduce the energy needed to heat water and therefore reduce your electricity/gas bill.\n- Be seen as environmentally friendly. Demonstrate to customers the importance your business places on caring for natural resources and the environment.\n- Water treatment uses energy. Save water and you’ll reduce your carbon footprint.\nFive tips for saving water at work\n- Install dual flush toilets\nA full toilet flush sends 12 litres of water down the drain. A dual flush toilet can use as little as three litres in a flush. If you have a single flush toilet, you could install a cistern displacement device, often called a “gizmo”. Depending on the size of the cistern, these devices can save one to two litres each time the toilet is flushed. If you do use one, make sure you check the manufacturer’s instructions to make sure the device is suitable for your toilet.\n- Repair dripping taps\nFixing these could save as much as 2000 litres of water a month. In most instances all that’s required is a new washer.\n- Install urinal controls\nOn average, an unmanaged cistern flushes four times an hour 24 hours a day. Sensor controlled urinals can reduce the amount of water used as the urinal only flushes after use.\n- Measure and monitor\nUse meter reads to compare your water use so that you can identify any unusual patterns.\n- Make your staff aware of how they can save water\nReporting leaking or dripping taps and using a plug in the sink when you rinse dishes are easy ways to save water in the workplace.\nSave Water in the Garden\nBeing water wise in the garden and outside your home or business means we reduce the amount of high quality drinking water used on lawns, plants and pavements. Sprinklers are the biggest cause of unnecessary water usage as the water often goes where it is not needed and it’s easy to over water.\nWater wise tips in the garden and outside\n- It’s a lawn not a lake! Even in the height of summer, more water is not necessarily better\n- If the grass springs back when you walk on it, there’s no need to water.\n- Install a rain barrel: rainwater is rich in natural minerals and plants love it. A rain barrel is a great way to cheer up your plants and keep your water bill down.\n- Use a bucket to wash your car. This uses up to 125 litres less water than a hosepipe does.\n- Simple mulch for your garden reduces the water that evaporates by up to 70 per cent and means less time spent watering.\n- Water in the early morning or evening to reduce water lost to evaporation.\n- Sweep paths and drive ways instead of washing them.\n- Use a watering can or a hose with a hand-held trigger to minimise wastage and direct water only where needed.\nFor more efficient irrigation\n- Check soil moisture - if your soil is moist 10cm below the surface, you don't need to water.\n- Water in cool, settled weather - water your garden on calmer days, in the cool of early morning or in the evening.\n- Aim low and slow - water close to the ground at a rate the soil can absorb.\n- Using a sprinkler - established plants should only need 30 minutes watering once or twice a week in dry weather.\n- Use mulch - mulch protects your soil from the drying effects of wind and sun, and can cut evaporation\nFor more information download our Changing How We Use Water: a guide to landscaping with less water in Central Otago brochure.\nAlternate Water Sources\nBrochures with More Tips""]"	['<urn:uuid:b7144706-01ba-4534-b459-e0910a843c10>', '<urn:uuid:a4e3c33b-ca38-4bfe-ad73-a2a13dadcc68>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	29	119	2756
2	wine expert how unopened champagne storage time vintage non vintage difference damaged bottle signs	Vintage Champagne (made with grapes from a specific year) can last up to 15 years if stored properly, while non-vintage Champagne (made with grape blends from different years) typically lasts 3-4 years. For proper storage, bottles should be kept in a dark, cool (50 degrees), humid place, lying horizontally. As for signs of damage, while cosmetic issues like damaged labels or white crystals around the cork don't affect the wine, serious problems include cork disintegration, vinegar taste that persists after aeration, moldy smell (indicating a 'corked' wine), or pushed-out corks suggesting the wine has been 'maderized' or overheated.	"['Champagne Can Expire. Here\'s What to Know Before Serving\nThere are occasions when nothing but a little bubbly will do. The problem? We may not always be as prepared for them as we would like. Maybe we\'ve forgotten a loved one\'s big birthday, didn\'t realize our special someone would get promoted, or didn\'t expect a couple\'s spur of the moment engagement. Unless you\'ve got a fully stocked, expertly curated and moderated wine cellar, unexpected surprises like these may lead to panicked searches for a bottle of Champagne or sparkling wine.\nSo let\'s say you luck out and find an unopened bottle, but you\'re having trouble remembering when you received it. Depending on how old your bottle of bubbles is and how you\'ve been storing it, it may be fine. But then again, it may not be quite so nice. Before you get down to the business of uncorking, it\'s important to consider some key information.\nHow Long Does a Refrigerated, Open Bottle of Champagne Last?\nThe short answer: not long. Just like a soda or sparkling water, the longer an open bottle of Champagne or sparkling wine sits, the quicker you\'re going to lose carbonation. There are lots of effective stoppers that can help slow down the oxidation, but as a rule, you should stop it up, refrigerate it, and then try to drink whatever is left in an open bottle in one to two days. (But honestly, only serve an already-open bottle of wine to your celebrant as a last resort.)\nHow Long Does Unopened Champagne Last?\nIn order to figure out how long your bottle of sparkles is acceptable to serve, you\'re going to need to look at the label. If you see a year, it is a ""vintage"" bottle, meaning your Champagne or sparkling wine was made with grapes exclusively from that particular year\'s harvest. If you don\'t see a year, it means it was made with a blend of grapes from different years.\nAs you might expect, the ""vintage"" bottle is going to last longer—up to 15 years if stored properly. A ""non-vintage"" Champagne may only last about 3 to 4 years. Of course, there are exceptions to these rules—some Champagnes and wines are just not very good from the get-go. But what\'s the proper way to store unopened bottles, you ask? Ideally, if you can find a spot that is dark, cool (50 degrees or so), humid, and allows you to lie the bottles horizontally (it might be worth investing in a rack, that\'s going to keep your Champagne in good condition. Just do the best you can.\nHow Do I Know If the Champagne Has Gone Bad?\nIf your bubbly has no bubbles, it\'s probably past its prime. And if you notice the color has gone a little too golden or smells sad and sour, it\'s likely not at its peak. The good news? All of this is very subjective. What\'s ""bad"" to some is tasty to others. The better news? You won\'t get sick sipping a ""bad"" Champagne. The worst that can happen here is that you may add a little sourness to an otherwise sweet toast!', 'Spotting a faulty wine\nSometimes, however, through no fault of your own, the wine in a bottle might be damaged upon arrival.\nThis is very rare; a recent study found only 1.2% of wines were corked or otherwise defective. But it does happen every so often.\nIf you have a damaged wine, it will not taste like it is supposed to; it might even taste terrible. Even the best wine in the world will make critics want to smash up their wine goblets in disgust if it is corked or damaged.\nBut how do you tell?\nAfter all, some bottles can look beat up or strange on the outside, yet the wines still taste excellent!\nAnd the wine may not taste great to you, yet still be fine.\nWe have made you a little chart.\n|The label is damaged.||✔||A label can get damaged during delivery or storage without any impact on the wine inside the bottle.|\n|There are little white crystals around the cork.||✔||The crystals are harmless! They are simply the sign that the wine was not filtered before bottling, which is often the case for finer wines.|\n|There is a small amount of sediment at the bottom of the wine bottle.||✔||Common for aged wines. You can decant the sediment out when you serve the wine.|\n|There are small bits of cork floating at the top of my bottle or glass.||✔||This probably just means that you pushed the corkscrew all the way through the bottom of the cork when you opened the bottle. Perfectly fine. (But it might also mean that the cork is damaged, in which case the wine might be bad. See next entries)|\n|When I opened the bottle, the cork started to disintegrate and flake into tiny pieces.||?||?||Might be faulty, or might be good. Older bottles often do this, but are perfectly fine. Try the wine. If it tastes funny, aerate it for an hour and try it again. If it still tastes funny, you may have a problem.|\n|When I try my wine, it tastes like vinegar. I’ve left it out for an hour or two, to let it aerate, but it still tastes the same.||✔||This means the wine has been over-oxidized, at some point in its aging process. Air has seeped in past the cork, and has made the wine dull, flat, and lifeless.|\n|The wine smells like a wet basement or dirty socks — moldy and mildewy. It tastes raspy, astringent, and sometimes like paint thinner.||✔||This is what’s called a “corked” wine. It happens when the wine has come into contact with a contaminated cork (one infected with microorganisms) back at the winery.|\n|The cork was pushed out before I even started to open it. The wine reminds me of Madera.||✔||The wine has been overheated, cooked, or “maderized”. Literally, the wine’s gotten so hot, it has baked in its bottle. This is often due to incorrect storage of wines.|\n|I didn’t order a champagne or sparkling wine, but for some reason, there are bubbles in my wine.||✔||This means the wine has accidentally undergone a secondary fermentation process during its shipment. We do not advise you drink it.|']"	['<urn:uuid:61d43234-ab16-4e1f-bb5c-ad2e5ec59079>', '<urn:uuid:1ba78d42-b8d4-48f9-a6e9-4d65fb344355>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	14	98	1047
3	I'm studying the relationship between body composition and longevity - how does fat distribution, metabolic rate, and exercise duration interact to affect lifespan and health outcomes?	Fat distribution significantly impacts longevity and health - stomach fat is harmful while hip and thigh fat can be beneficial. Stomach fat releases damaging free fatty acids and hormones that can lead to heart disease, while hip and thigh fat stores these fatty acids safely. Exercise is key for targeting stomach fat reduction - for example, jumping rope for 28 minutes or walking for 62 minutes can burn 310 calories. Additionally, exercise increases metabolic rate both during and after activity, and having more muscle mass increases basal metabolic rate since muscle cells are 8 times more metabolically demanding than fat cells.	['If you think you’ll live longer just because you lose weight, you’re wrong. New research shows that you’re only likely to live longer if you shed the fat on your stomach, not the fat on your hips or thighs.\nWe’ve known for a long time that fat increases your risk of getting numerous ailments, such as diabetes, heart diseases and cancer. Fat also cuts the time you can expect to live.\nSo it may come as a surprise that several recent public health studies show that losing weight can cut your lifespan.\nThe new research confirms that weight loss shortens the lifespan of healthy middle-aged people. But the study has a new finding: your lifespan depends on where on your body the fat you lose is located.\nThe researchers conclude that if you want to live longer, you should try to cut stomach fat and hold on to the fat on your hips and thighs.\n“We are among the first researchers to come with nuanced advice on weight loss by showing that living longer isn’t simply a matter of losing weight, but cutting stomach fat while keeping hips and thighs fat,” says Tina Landsvig Berentzen, a PhD at the Copenhagen University Hospital’s Institute of Preventive Medicine.\nBerentzen and her colleagues assessed the health of 26,625 men and women, aged between 50 and 64, in two periods – 1993–97 and 1999–02. These people were subsequently followed, through Denmark’s various national registers of diseases and deaths, until the end of 2006.\nThe study shows that a healthy person in the 50-64 age range who loses weight (measured as BMI units – see factbox) has a 9 percent higher risk of dying – but the same person cuts the risk of dying by 9 percent by reducing his or her waist measurement by 5cm.\nThis applies to all healthy people in the age range – and not just those who are already overweight or apple- or pear-shaped.\nThe new results are underlined by numerous other studies, which also show that fat’s damaging effects on health depend to a high degree on where the fat is located on the body – whether you are shaped like an apple or a pear.\nFat isn’t simply fat: because of a number of differences between the body’s various fat deposits, stomach fat is bad for you, while fat on the hips and thighs is beneficial.\nStomach fat – in particular the fat in the abdominal cavity around the organs – transmits many free fatty acids and numerous hormones (adipokines), which can damage the body and can contribute to the development of heart diseases.\nFat deposits on the hips and thighs store free fatty acids, so they do not harm the body. Hip and thigh fat also secretes hormones with beneficial effects on the body.\nThis means that a large butt can be an advantage in health terms. Weight loss alone may not be enough – reducing stomach fat while retaining hip and thigh fat is important to our health and life expectancy.\nThe Berentzen study provides no clues to how we ensure we reduce our stomach fat. But other studies indicate that sports and a diet rich in fibres reduce stomach fat.\nIn addition, it seems that moderate weight loss cuts proportionately more stomach fat than other fat – but this relationship is changed by greater weight loss because stomach fat is often the first to be cut when you lose weight.\nHowever, losing weight – particularly in specific locations on the body – is difficult, especially in the long term. So we are better off if we prevent stomach fat through healthy living with plenty of exercise, a fibre-rich diet, good sleep and no stress.\nWhy is some fat better than other fat?\nStomach fat – in particular the fat in the abdominal cavity – is bad for our health, while fat on the hips and thighs is beneficial.\nThe health-related differences between the fat deposits are due to:\n• The fat cells’ metabolism of the unhealthy free fatty acids\n• The size and number of the fat cells\n• The type and quantity of adipokines secreted by the fat cells\n• Whether the fatty tissue has been infiltrated by other cell types, such as macrophages, which are part of the immune system', 'What is Metabolism?\nFrom the viewpoint of diet and weight loss, metabolism is the process whereby the body converts food into energy (or uses it for bodily repairs) or stores it as fat for future use. In simple terms, if we have a faster metabolism we burn calories more efficiently and store less fat. If we have a slower metabolism we burn calories less efficiently and therefore store more calories as fat.\nIn very simple terms, our Metabolic Rate is the rate (and efficiency) with which our body performs these metabolic tasks.\nMetabolic Needs and Calorie Usage\nOur Basal Metabolic Rate (or Resting Metabolic Rate) is the number of calories we need to burn in order to fuel essential bodily processes and keep our organs and tissues in working order. Typically, this accounts for 60 per cent of all calories consumed. The heavier the person, the more calories he/she needs to fuel these essential processes.\nAll other calories are used to meet our additional energy needs based on our level of physical activity.\nWhat Determines Our Metabolic Rate\nSeveral things determine our metabolic rate, including:\n- Our Genes\nSome people have a naturally faster metabolism\n- Our Age\nAs we age, our calorie needs decrease. On average it drops 2 per cent each decade.\n- Our Muscle-to-Fat Ratio\nMuscle cells are about 8 times more metabolically demanding than fat cells. So the greater the our proportion of muscle to fat, the faster our metabolic rate.\n- Our Activity Level\nExercise burns calories. Even after we stop exercising, the effect continues. So we burn calories at a faster rate for several hours afterwards.\n- How Often We Eat\nOur metabolic rate increases during digestion of food, a process known as the Thermal Effect of food. In simple terms, if we go too long without food (5 hours for men; 3 hours for women), our body thinks there is a food shortage and our metabolism tends to slow down.\n- Our Health & Nutrition\nMetabolism is governed by thousands of separate chemical reactions. To perform this process efficiently, our body needs a constant supply of nutrients. Without these nutrients (like vitamins and minerals, esp. vitamins B2, B3, B5, B6 & vitamin C) metabolism can become inefficient and sluggish. The best way to ensure good dietary nutrition is to follow a balanced diet.\nFat-burners, Pills & Supplements\nDo pills or supplements help to speed up metabolism and weight loss? The answer is No. There are many so-called ‘fat-burning’ pills and supplements on offer, which claim to speed up metabolism and weight loss. Be warned! This method of raising your metabolism is not considered safe. Nor has it been proved to assist long term weight loss.\nHow to Raise Metabolism – 4 Practical Conclusions\n1. Take regular exercise\nThis should be aerobic exercise that makes you breathe more heavily. Exercises that build muscle are also useful.\n2. Eat Breakfast and Eat Regularly\nTo benefit from the Thermal Effect of food and ensure that your body burns calories willingly, eat breakfast and then eat regularly throughout the day.\n3. Eat a Balanced Diet\nThis helps to maintain a good supply of nutrients which in turn keeps your metabolism working smoothly. Weight Loss Diet is a good choice.\n4. Avoid Pills\nFor the sake of your health and weight, avoid all supplements that claim to offer fat-burning and weight loss benefits.\nMetabolism – The Bottom Line\nThe only safe and effective way to raise your metabolism is to take more exercise. The harder you work, the faster your metabolic rate.\nHow to Raise Your Metabolism\nThe healthy way to raise your metabolism is to increase your exercise. Avoid fat burners, pills or fat burning supplements – exercise is still the only effective way to raise your basal/resting metabolic rate. Exercise also raises your muscle-fat ratio and because muscle is more metabolically active (i.e. it needs more calories to sustain it) than fat, you also lose weight.\nExercise and Weight Loss\nLet’s say you eat 2 cups caramel-coated popcorn (approx 310 calories).\n- If you jump rope, you can burn off all these calories in about 28 minutes.\n- If you walk at 3mph, you can burn off these calories in about 62 minutes.\n- In addition, exercise helps to raise your metabolic rate, which makes it easier to lose weight.\n- Finally, exercise also helps you to lose weight by boosting your motivation to stick to your weight loss program.\nNote: all calorie-burn figures are approximate. They are based on a 150 pound woman. If you weigh more, you’ll burn more calories. If you weigh less, you’ll burn less calories.']	['<urn:uuid:d1d2fec7-0c81-4e82-9246-e9df28cdbd8f>', '<urn:uuid:16c236d0-1e2d-4d4f-a057-bfdc954254ec>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T18:38:36.516973	26	101	1491
4	Which digestive system parts benefit from prebiotics versus probiotics?	Probiotics primarily benefit the large intestine (colon) by creating an ecosystem of beneficial bacteria that ferments fiber and produces nutrients for colon health. Prebiotics, on the other hand, work throughout the digestive tract, feeding beneficial bacteria and improving nutrient absorption in both the small intestine (particularly the jejunum where most nutrients are absorbed) and the large intestine, enhancing overall digestive function and gut health.	['Digestion is a process whereby the body prepares food for absorption into the blood stream. The digestive system is a series of organs joined together that begin at the mouth and end with the anus. They include the mouth, esophagus, stomach, small intestine, colon, rectum and anus. Other organs such as the salivary glands, pancreas, liver and gallbladder are involved in the digestive process but are not, anatomically, considered to be part of the digestive system.\nThe Flow of Food within the Digestive Tract\nThe Flow of Food within the Digestive Tract\nMastication (chewing of food) is the first stage of digestion. Mastication initiates digestion by breaking down large aggregates of food and stimulating the release of digestive enzymes contained within the saliva of the mouth. Tiny glands along the upper digestive tract are embedded in the mucosa lining. These glands secrete digestive enzymes that catalyze the breakdown of starches and fats.\nBefore the food enters the stomach, it first passes through a gullet that connects the mouth to the stomach, called the esophagus. The esophagus is important, because it serves as an air lock to prevent back flow of partially digested foods and stomach acids back into the throat and mouth. GERD (Gastroesophageal reflux disease) is a condition where the lower esophageal sphincter (LES – a valve between the esophagus and the stomach) is not functioning effectively. Individuals with GERD experience a spontaneous “rise up” of stomach acid and food contents into the esophagus. This leads to heartburn or acid indigestion.\nThe esophagus opens into the stomach, where proteins are broken down into small peptides. The high acid content of the stomach is crucial for digestion and protecting the body from bacteria or toxins present in food, by acting as a decontamination chamber. Ingested food is stored in both the fundus (top of the stomach) and the body of the stomach. Once food enters into the stomach, hydrochloric acid (HCl) is secreted from the gastric fundal mucosa (lining of the stomach). HCl denatures proteins and activates the enzyme pepsinogen, which becomes pepsin. Pepsin then hydrolyzes the denatured proteins, resulting in several small peptides. Fats are also hydrolyzed in the stomach, but at this point they are partially broken down from their exposure to lipase in the saliva.\nThe “grinding” of food occurs in the antrum (lower part of the stomach). The antrum secretes gastrin (enzyme that regulates acid production) and controls the amount of food released into the intestine by way of the pyloric spinchter. Once the food-acid-enzyme mixture (chyme) enters the intestine; secretin and cholecystokinin are released, which in turn stimulates the pancreas to secrete pancreatic enzymes into the lumen (the lining) of the duodenum (the first segment of the small intestine).\nThe small intestine allows the body to maximize digestion and absorption because it has an expanded surface area with inner folds called plicae, villi and mircovilli. This high degree of folding enhances its ability to absorb nutrients. The duodenum is the neutralization chamber where the chyme is mixed with the pancreatic juices. The pancreatic juices include: bicarbonate along with enzymes that digest proteins (trypsin, chymotryosin, carboxypeptidase, and elastase), enzymes that digest fats (lipase and phospholipase), and amylase (enzyme that digest carbohydrates).\nThe jejunum (middle section of the small intestine) is where most nutrients are absorbed. Amino acids, as well as most vitamins and minerals, are absorbed in the jejunum. A few nutrients, like iron and calcium, are absorbed in the duodenum. Active fat absorption occurs in the duodenum and the jejunum. Before fat can be transported into the body and absorbed, it must first be solubilized and aggregated into small fat droplets, called miscelles. Bile, produced by the liver (and stored in the gall bladder), acts to solubilize fat. This process is particularly important for the absorption of fat-soluble vitamins (vitamins A, D, E, and K), and for cholesterol absorption. Most starch is digested in the duodenum and jejunum, the first and second segments of the small intestine.\nThe ileum is the end of the small intestine. It is where the digestion of nutrients is completed and bile salts are reabsorbed. Roughly 90 percent of the nutrients have been removed from most foods at this point.\nThe Large Intestine\nThe large intestine does not play a significant role in the digestive process of most foods, but it is important in the conservation of sodium and water. However, the food that does make it into the large intestine is primarily fiber. The large intestine is home to an ecosystem of bacteria called probiotics (pro-life) that can ferment much of this fiber and produce many nutrients necessary for a healthy colon. Colonic fermentation produces a series of short-chain fatty acids that are important in removing free radicals and promoting proper function.\nProbiotics include the bifidobacteria and lactobaccillus genuses. Probiotic bacteria keeps pathogenic (disease-promoting bacteria) from colonizing within the colon. Certain fibers in food, called prebiotics, specifically support probiotic bacteria. Prebiotics include such molecules as inulin and fructooligosaccharides, (which are found in chicory and Jerusalem artichoke), and may include some other carbohydrates such as galactooligosaccharides, arabinogalactans, and arabinoxylans, which are found in soy, rice fibers, and in larch tree extracts.\nRectum and the Anus\nThe rectum and the anus allow for controlled elimination of stool.', 'Prebiotics are compounds in food that help the beneficial bacteria of the intestine grow and do their job of breaking down and absorbing food better. Essentially they feed the kinds of organisms that probiotics are rich in (1).\nPrebiotics are usually foods that have a lot of fiber, and they help the bacterial ecosystem of your gut process foods and nutrients on an improved basis.\nIts cousin, probiotics, are tiny, living organisms which help your body and your digestive system function at optimal capacity. For years now, they’ve been all the rage. People have consumed probiotics wherever they can: in yogurt, other dairy products, specialized drinks like kombucha, and even traditional fermented foods like kimchi (2).\n1Prebiotics improve digestive health. The largest and most often-touted benefit of prebiotics is that they help your digestive system do its daily work. Prebiotics help the healthy, beneficial bacteria that live in your gut grow and flourish.\nThis balances out any toxins, bad bacteria, or unhelpful microorganisms that may be living down there as well. It’s really simple: research directly shows that prebiotic intake increases the population of healthy intestinal microflora to improve digestive health (3).\nThe result is that your body processes food and even toxins at maximal capacity, getting as many of the good nutrients as possible while expelling as many bad ingredients as it can.\nThis is also the root cause that you can thank for many of prebiotics’ other benefits. As it turns out, gut health is important not just for your belly, but for many of your body’s other physiological systems and functions.\n2Prebiotics can help treat chronic digestive conditions. Chronic disorders of the gut like ulcerative colitis or Crohn’s disease, as well as any kind of digestive inflammation or infection, aren’t just annoying, they can have a profound impact on the way you live your life, as well as lead to other health risks and complications.\nSuch conditions impact the food you eat, whether or not you’re able to drink, even whether some social situations, like going out to dinner or grabbing drinks after work with coworkers, are available to you. A 2012 study published in the Journal of Nutrition showed that prebiotics have such a huge impact on the body that they can actually help relieve some of the symptoms of such diseases (4).\nAnother 2007 study demonstrated that prebiotics with a lot of fiber in them help stabilize your gut, reducing the risk of such chronic conditions as well as their severity (5).\n3Prebiotics can help with diarrhea. Not only are prebiotics a wonder-food for those who suffer from chronic conditions like ulcerative colitis, but they can also help the average person who experiences a bad bout of diarrhea after eating something that their stomach doesn’t agree with. Numerous studies show that both prebiotics and their close cousin, probiotics, have positive effects when used to treat diarrhea (6).\nEven among reviews and studies that don’t have conclusive findings, prebiotics are marked as a promising method for both avoiding diarrhea in the first place and for preventing it from happening again (7).\nFurther research is needed, however, to figure out the best timing and dosage for using prebiotics to combat diarrhea.\n4Prebiotics may help with stress and mental health. In recent years, one of the hottest topics in research has been the connection between gut function and mental health (8).\nHaving a healthy, well-functioning digestive system is starting to look more and more like a factor in having an equally healthy mind, with both depression and anxiety being linked to inflammation in the gut in some cases (9).\nA 2015 study showed that when probiotics were administered to healthy patients, prebiotic supplements were shown to reduce the body’s production of cortisol (10).\nCortisol is a natural response that’s developed as a part of our fight-or-flight systems, but it can have negative effects on mental health (11).\nIf prebiotics can help lower cortisol levels and prevent stress, they may have a role to play in combating stress-related mental health disorders like anxiety.\n5Prebiotics may reduce the risk of cancer. A 2010 review published in The British Journal of Nutrition showed that there is a reduction in the numbers of tumors and cancers in those who have a diet rich in prebiotics (12).\nOf course, this applies mainly to cancers of the digestive system and its related areas, like colon cancer. Eating prebiotics is certainly good for your health, but it may not necessarily help with, say, skin cancer.\nDespite the fact that more research is needed, some studies have shown that prebiotics may even be able to prevent human colorectal cancer, not merely alleviate the symptoms or the severity. Eating a diet rich in prebiotics may be one of the best ways to avoid getting cancer in the first place (13, 14).\n6Prebiotics can help prevent obesity. Prebiotics can legitimately impact your diet in a positive way, keeping you healthy and lowering unnecessary or unhealthy levels of fat in the body. In regards to weight, prebiotics have the potential for, essentially, two different functions.\nOn the one hand, a 2015 study showed that prebiotics might help you maintain a healthy body weight (15).\nBecause they’re rich in fiber, your body will feel full faster, with a much smaller amount of carbs, calories, trans fats, and other unhealthy things you might be taking in through your food. In general, diets rich in fiber are good for both obesity and diabetes mellitus — meaning a diet rich in prebiotics likely is as well (16).\nThe other way it combats obesities is due to the effects that prebiotics have on your gut microflora. Prebiotics might impact the way your body processes food, helping to keep you leaner and more muscular. In one study, the usage of prebiotics significantly decreased obesity in a population of overweight children (17).\n7Prebiotics may help with eczema and other skin conditions. When children or babies develop eczema, the parents are often given a prescription for probiotic or prebiotic supplements or baby food.\nWhile there is weak evidence that probiotic usage helps eczema directly, further research is required on the effects of prebiotics (18). For the meantime, it seems safe to assume that prebiotics may have a marginal beneficial impact on eczema development in children.\n8 Prebiotics can benefit your cardiovascular system. Manydiets which are rich in fiber, and thus prebiotics, are proven to be beneficial for cholesterol levels (19).\nThere’s plenty of evidence to suggest that fiber-rich foods, like prebiotics, are even helpful across a wide range of cardiovascular disease and conditions, not just ADL cholesterol (20).\n9Prebiotics can benefit bone health and help prevent fractures. Regular consumption of prebiotics can actually improve how healthy your bones are, leading to fewer breaks, fractures and other issues with your skeletal system. Since prebiotics increase the efficacy of your dietary system, feeding the microorganisms that help it flourish, a diet rich in prebiotics also increases your body’s uptake of essential nutrients and minerals, like calcium, iron or magnesium (21).\nSuch research suggests prebiotics aren’t just good for preventing fractures. They may also help with fighting against chronic bone conditions like osteoporosis.\n1Prebiotics and their close cousin, probiotics, may worsen the gas and bloating you seek them out for. Even though a diet rich in fiber is almost always a good choice, for some users it could actually make it harder to pass stools or solve underlying digestive issues by increasing gas and bloating.\nOf course, this is only a subset of users. In most trials, prebiotics are shown to have positive effects on such digestive discomfort.\nHowever, this possible side effect is still good to keep in the back of your head just in case. If you begin taking prebiotics and you actually notice that your symptoms are starting to worsen in a serious and noticeable way, then it may be worth it to put prebiotics on hold until you can talk to your doctor about it it.\n2There may be side effects not yet fully understood by scientists. According to a 2018 study in Annals of Internal Medicine, scientists found that health risks, adverse effects, and bad side effects often aren’t really reported in clinical and animal trials concerning prebiotics and probiotics (22).\nThey’re both relatively new approaches to understanding and trying to aid human gut health, so the adverse risks haven’t necessarily been ironed out yet.\n3While prebiotics certainly increase the amount of microflora living in your gut, they may not necessarily help with the diversity of said bacteria the same way that probiotics do (23). Whether that’s a good or bad thing remains to be seen. That said, prebiotics aren’t a normal “drug” like antibiotics or retroviral medications.\nThough supplements may come in pill form, they’re mostly just fiber, which is totally a natural component of a regular diet. That means that overall, you can expect fewer complications from prebiotics than you might a typical medication or a more contentious supplement.\nUnfortunately, prebiotics are a relatively new addition to the world of internal medicine and gut health (24).\nWhile it’s great that such an innovative approach is being researched and is now relatively available to the public, it also means that there’s not that much information out there on dosing and how much a given person should take per day. There is no consensus among the available scientific evidence about what works best for most people.\nThat said, there are some general guidelines one can follow when introducing any new supplement to one’s diet.\nIt’s probably better to begin with a small amount, no more than 1 or 2 grams a day, for at least a week or two. After that, users can begin adding more and more in small amounts, monitoring carefully how their body is reacting to the new dietary changes.\nAre prebiotics safe to use? Yes, as mentioned earlier in this article, prebiotics haven’t been fully researched in terms of adverse side effects. That being said, there are no reports of people dying or suffering from health problems due to a diet high in prebiotics. If you’re worried about it, you can simply try adding prebiotics into your diet on a slow, steady basis – monitoring any significant changes in your body.\nAre prebiotics safe for pregnant women? Again, yes. There are no recorded health issues associated with taking prebiotics or eating fiber-rich foods while you’re pregnant. Of course, adhering to a good diet for your pregnancy in all respects is still necessary. You can’t simply take prebiotics and expect they’ll fill in any gaps caused by malnutrition. However, as long as you’re taking prebiotics in addition to an otherwise healthy diet, and you’re taking them in reasonable amounts, there’s no reason it should affect your pregnancy.\nWhat foods are rich in prebiotics? Prebiotics are everywhere. Chances are, you’re probably even getting some in your normal diet. However, if you want to load up on them, some foods you can look out for are Chicory root, dandelion greens, garlic, leeks, onions, and bananas. If you’re having a hard time remembering specific foods with prebiotic content, just think about it: prebiotics are often rich in fiber. So if you’re eating foods that are known for being full of healthy fiber, like asparagus, you’re probably getting some prebiotics along with it.\nAre prebiotic supplements an option? Of course, you might not be the kind of person who wants to get your prebiotics from your food. Maybe you don’t have the time nor the inclination to purchase that much fresh produce on a regular basis. If that’s the case, you don’t have to worry, there are plenty of prebiotic supplements available to consumers who want them.\nAre prebiotics the same thing as probiotics? No, probiotics are foods or supplements loaded with good bacteria, the kind that your gut needs to function healthily. Prebiotics are foods high in fiber, which are good for feeding the microorganisms that already exist in your gut. It’s a subtle but important difference.\nCan I take prebiotics and probiotics at the same time? Yes. Probiotics and prebiotics do different things. In fact, there’s even a category of product that offers probiotics and prebiotics in the same package. They’re called synbiotics. That said, prebiotics and probiotics are both supplements that affect your digestive system, so it may not be wise to take a huge dosage of both simultaneously, or to very suddenly start taking a high dosage of one while already taking the other. In simultaneous use cases, gradually and carefully upping your dose is likely the best bet.\nDo prebiotics interact with drugs? In some cases, yes, though it may not be in specifically the way you’re thinking. For example, prebiotics can help with diarrhea or stomach upset caused by antibiotics (25). Because the side effects of prebiotics aren’t particularly well-researched, however, it may be worth it to ask your doctor before taking them if you’re already on another course of medicine.\nCan I drink alcohol while taking prebiotics? Yes, alcohol and prebiotics don’t really have any interactions to speak of. Drinking (a moderate amount) while also taking prebiotics (in moderation) is completely safe.\nWhere do prebiotics come from? There are a wide variety of prebiotics on the market, so it can be hard to pin down exactly where they come from. That said, if you buy a bottle of prebiotic supplements, it’ll most likely say where the root source is. In general, prebiotics are either totally naturally or synthetically manufactured. You could find inulin, for example, in asparagus, or you could find it in processed ingredients like chicory root extract, or in a processed form such as oligofructose.\nIs raw apple cider vinegar a prebiotic? Yes, apple cider vinegar is a prebiotic, which feeds probiotics (the healthy microflora in your inner ecosystem).\nAre prebiotics better than probiotics? Neither is better or worse as they both complement each other in managing the gut bacteria in the body.\nDo prebiotics help with weight loss? Yes, prebiotics can help with weight loss by promoting a healthy gut flora while also increasing satiety.\nIs Metamucil a prebiotic? While psyllium is not considered a prebiotic it is considered to have prebiotic effects. Psyllium is somewhat resistant to fermentation, however, a small portion of psyllium fibers can be fermented by intestinal bacteria.\nHow do prebiotics work? A prebiotic is a special type of soluble fiber that is used mostly by the beneficial good bacteria as a fuel. These good bacteria, in turn, produce certain substances that acidify the colon (a very good thing) and serve as a nutrition source for the colon’s own cells.\nTo sum it up, prebiotics are fiber-rich foods or supplements that help the microorganisms in your digestive system grow and flourish. By doing so, prebiotics improve your digestive health and can help with chronic digestive disorders.\nMoreover, the way prebiotics improve your gut health may have a much wider positive impact, improving cardiovascular health, reducing your risk for cancer and decreasing stress, among other effects.\nAs of now, there aren’t any major, life-threatening side effects reported from prebiotic usage, but more research is required. In any case, prebiotic diets seem linked to a wide variety of health benefits and they could be a great option for those seeking better digestive health, young and old alike.']	['<urn:uuid:d298cf1e-2be0-434f-9939-8964f672d69f>', '<urn:uuid:a809fd2b-a1a5-41c7-9f9b-58b1ea38233b>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T18:38:36.516973	9	64	3429
5	I'm researching soil contamination detection technologies - has anyone successfully used hyperspectral remote sensing to identify hydrocarbon presence in soils?	Yes, HyMap hyperspectral remote sensing has been successfully used to detect hydrocarbons in soil. A specific algorithm called the Hydrocarbon Index was developed for hyperspectral detection of hydrocarbons.	['- About this Journal ·\n- Abstracting and Indexing ·\n- Advance Access ·\n- Aims and Scope ·\n- Annual Issues ·\n- Article Processing Charges ·\n- Articles in Press ·\n- Author Guidelines ·\n- Bibliographic Information ·\n- Citations to this Journal ·\n- Contact Information ·\n- Editorial Board ·\n- Editorial Workflow ·\n- Free eTOC Alerts ·\n- Publication Ethics ·\n- Reviewers Acknowledgment ·\n- Submit a Manuscript ·\n- Subscription Information ·\n- Table of Contents\nApplied and Environmental Soil Science\nVolume 2012 (2012), Article ID 751956, 11 pages\nQuantitative Analysis of Total Petroleum Hydrocarbons in Soils: Comparison between Reflectance Spectroscopy and Solvent Extraction by 3 Certified Laboratories\n1Porter School of Environmental Studies, Tel-Aviv University, Tel-Aviv 69978, Israel\n2Remote Sensing Laboratory, Tel-Aviv University, Tel-Aviv 69978, Israel\n3Geography and Human Environment Department, Tel-Aviv University, P.O. Box 39040, Tel-Aviv 69978, Israel\n4The Soil Erosion Research Station, Ruppin Institute, Emeck Hefer 40250, Israel\nReceived 9 January 2012; Revised 29 March 2012; Accepted 3 April 2012\nAcademic Editor: Jose Alexandre Melo Dematte\nCopyright © 2012 Guy Schwartz et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n- M. S. Hutcheson, D. Pedersen, N. D. Anastas, J. Fitzgerald, and D. Silverman, “Beyond TPH: health-based evaluation of petroleum hydrocarbon exposures,” Regulatory Toxicology and Pharmacology, vol. 24, no. 1, pp. 85–101, 1996.\n- P. Boffetta, N. Jourenkova, and P. Gustavsson, “Cancer risk from occupational and environmental exposure to polycyclic aromatic hydrocarbons,” Cancer Causes and Control, vol. 8, no. 3, pp. 444–472, 1997.\n- G. D. Ritchie, K. R. Still, W. K. Alexander et al., “A review of the neurotoxicity risk of selected hydrocarbon fuels,” Journal of Toxicology and Environmental Health B, vol. 4, no. 3, pp. 223–312, 2001.\n- Environmental Sciences Division, Use of Gross Parameters for Assessment of Hydrocarbon Contamination of Soils in Alberta, Oxford, UK, 1993.\n- United States Environmental Protection Agency (USEPA), Test Method for Evaluating Total Recoverable Petroleum Hydrocarbon, Method 418.1 (Spectrophotometric, Infrared), Government Printing Office, Washington, DC, USA, 1978.\n- United States Environmental Protection Agency (USEPA), Methods for Chemical Analysis of Water and Wastes, Government Printing Office, Washington, DC, USA, 1983.\n- G. Schwartz, G. Eshel, and E. Ben-Dor, “Reflectance spectroscopy as a tool for monitoring contaminated soils,” in Soil Contamination, Intech, 2011.\n- R. S. G. Gómez, T. Pandiyan, V. E. A. Iris, V. Luna-Pabello, and C. D. de Bazúa, “Spectroscopic determination of poly-aromatic compounds in petroleum contaminated soils,” Water, Air, and Soil Pollution, vol. 158, no. 1, pp. 137–151, 2004.\n- J. Krupcík, P. Oswald, D. Oktavec, and D. W. Armstrong, “Calibration of GC-FID and IR spectrometric methods for determination of high boiling petroleum hydrocarbons in environmental samples,” Water, Air, and Soil Pollution, vol. 153, no. 1–4, pp. 329–341, 2004.\n- G. Xie, M. J. Barcelona, and J. Fang, “Quantification and interpretation of total petroleum hydrocarbons in sediment samples by a GC/MS method and comparison with EPA 418.1 and a rapid field method,” Analytical Chemistry, vol. 71, no. 9, pp. 1899–1904, 1999.\n- P. Lambert, M. Fingas, and M. Goldthorp, “An evaluation of field total petroleum hydrocarbon (TPH) systems,” Journal of Hazardous Materials, vol. 83, no. 1-2, pp. 65–81, 2001.\n- E. Saari, P. Perämäki, and J. Jalonen, “A comparative study of solvent extraction of total petroleum hydrocarbons in soil,” Microchimica Acta, vol. 158, no. 3-4, pp. 261–268, 2007.\n- M. Villalobos, A. P. Avila-Forcada, and M. E. Gutierrez-Ruiz, “An improved gravimetric method to determine total petroleum hydrocarbons in contaminated soils,” Water, Air, and Soil Pollution, vol. 194, no. 1–4, pp. 151–161, 2008.\n- E. A. Cloutis, “Spectral reflectance properties of hydrocarbons: remote-sensing implications,” Science, vol. 245, no. 4914, pp. 165–168, 1989.\n- I. Schneider, G. Nau, T. V. V. King, and I. Aggarwal, “Fiber-optic near-infrared reflectance sensor for detection of organics in soils,” IEEE Photonics Technology Letters, vol. 7, no. 1, pp. 87–89, 1995.\n- B. R. Stallard, M. J. Garcia, and S. Kaushik, “Near-IR reflectance spectroscopy for the determination of motor oil contamination in sandy loam,” Applied Spectroscopy, vol. 50, no. 3, pp. 334–338, 1996.\n- Z. Zwanziger and F. Heidrun, “Near infrared spectroscopy of fuel contaminated sand and soil. I. Preliminary results and calibration study,” Journal of Near Infrared Spectroscopy, vol. 6, no. 1–4, pp. 189–197, 1998.\n- D. F. Malley, K. N. Hunter, and G. R. B. Webster, “Analysis of diesel fuel contamination in soils by near-infrared reflectance spectrometry and solid phase microextraction-gas chromatography,” Soil and Sediment Contamination, vol. 8, no. 4, pp. 481–489, 1999.\n- B. Hörig, F. Kühn, F. Oschütz, and F. Lehmann, “HyMap hyperspectral remote sensing to detect hydrocarbons,” International Journal of Remote Sensing, vol. 22, no. 8, pp. 1413–1422, 2001.\n- F. Kühn, K. Oppermann, and B. Hörig, “Hydrocarbon index—an algorithm for hyperspectral detection of hydrocarbons,” International Journal of Remote Sensing, vol. 25, no. 12, pp. 2467–2473, 2004.\n- K. H. Winkelmann, On the applicability of imaging spectrometry for the detection and investigation of contaminated sites with particular consideration given to the detection of fuel hydrocarbon contaminants in soil, Ph.D. thesis, Brandenburgische Technische Universität Cottbus, 2005.\n- G. Schwartz, G. Eshel, M. Ben-Haim, and E. Ben-Dor, “Rapid methods for classification and quantitative assessment of petroleum hydrocarbons pollution in soil samples using reflectance spectroscopy,” EGU 2009-11441-2, Vienna, Austria, 2009.\n- S. Chakraborty, D. C. Weindorf, C. L. S. Morgan et al., “Rapid identification of oil-contaminated soils using visible near-infrared diffuse reflectance spectroscopy,” Journal of Environmental Quality, vol. 39, no. 4, pp. 1378–1387, 2010.\n- T. Lammoglia and C. R. de S. Filho, “Spectroscopic characterization of oils yielded from Brazilian offshore basins: potential applications of remote sensing,” Remote Sensing of Environment, vol. 115, no. 10, pp. 2525–2535, 2011.\n- J. Dan and H. Koyumdjisky, “The soils of israel and their distribution,” European Journal of Soil Science, vol. 14, no. 1, pp. 12–20, 1963.\n- S. S. Staff, Keys to Soil Taxonomy, Government Printing Office, 2010.\n- D. L. Carter, M. M. Mortland, and W. D. Kemper, “Specific surface,” in Methods of Soil Analysis Part I. Soil Science, A. Klute, Ed., pp. 413–422, Society of America, Madison, Wis, USA, 1986.\n- G. Eshel, G. J. Levy, U. Mingelgrin, and M. J. Singer, “Critical evaluation of the use of laser diffraction for particle-size distribution analysis,” Soil Science Society of America Journal, vol. 68, no. 3, pp. 736–743, 2004.\n- A. Pimstein, E. Ben-Dor, and G. Notesco, “Performance of three identical spectrometers in retrieving soil reflectance under laboratory conditions,” Soil Science Society of America Journal, vol. 75, no. 2, pp. 746–759, 2011.\n- G. Schwartz, G. Eshel, M. Ben-Haim, and E. Ben-Dor, Reflectance Spectroscopy as a Rapid Tool for Qualitative Mapping and Classification of Hydrocarbons Soil Contamination, Tel Aviv, Israel, 2009.\n- G. Schwartz, G. Eshel, M. Ben-Haim, and E. Ben-Dor, Quantitative Assessment of Petroleum Hydrocarbons in Situ by Diffused Reflectance Spectroscopy and a Penetrating Optical Sensor, GFZ, Potsdam, Germany, 2010.\n- G. Schwartz, G. Eshel, and E. Ben-Dor, An Operational Spectral Based Model to Predict Soil Petroleum Hydrocarbon Content in Field Samples, Edinburgh, Scotland, 2011.\n- G. Schwartz, Reflectance spectroscopy as a rapid tool for qualitative mapping and classification of hydrocarbons soil contamination, Ph.D. thesis, Tel Aviv University, 2012.']	['<urn:uuid:fc46b3f1-85c3-451f-9596-542b69b881ac>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	20	28	1215
6	drone autonomy features and navigation sensors	Drones employ multiple sensors and navigation systems working together. The Phantom 4 Pro V2.0 features FlightAutonomy with 6 cameras forming a navigation system that enables precise hovering without GPS and obstacle avoidance at speeds up to 31mph. For navigation, drones use GPS signals, inertial measurement units, and various sensors to gather environmental data, measure acceleration, and determine position. Advanced autonomy levels, like Level 4A, allow drones to fly autonomously at 2m/s and collect environmental data covering up to 16 million cubic meters in one flight. However, most commercial drones still operate at Level 2 or 3 autonomy, requiring some human operator communication.	"[""DJI Phantom 4 Pro V2.0 Quadcopter\nVideo ISO Range\nMax Flight Time\nREGISTER YOUR DRONE! Safety Tips! No Fly Zones!\nThe new Phantom 4 Pro V2.0 features an OcuSync HD transmission system, which supports automatic dual-frequency band switching and connects to DJI Goggles wirelessly. Like the Phantom 4 Pro and Advanced, the Phantom 4 Pro V2.0 is equipped with a 1-inch 20-megapixel sensor capable of shooting 4K/60fps video and Burst Mode stills at 14 fps. Plus, its FlightAutonomy system includes dual rear vision sensors and infrared sensing systems for a total of 5 directions of obstacle sensing and 4 directions of obstacle avoidance.\nQuieter, More enjoyable flights\nNew aerodynamic 9455S propellers and new FOC ESC drivers offer improved noise control and sinusoidal current for increased stability. Aircraft noise has been reduced by up to 4dB* (60%) for quieter, more enjoyable flights.\nNew OcuSync Video Transmission\nThe new Phantom 4 Pro V2.0’s remote controller uses OcuSync video transmission,* which employs time-division multiplexing technology to send control signal and receive video signal simultaneously. When the controller is powered on, the OcuSync system scans for the frequency band with lowest interference and switches accordingly to maintain clear transmission. Also, thanks to OcuSync, the Phantom 4 Pro V2.0 wirelessly connects to DJI Goggles. Free from cable constraints, you can enjoy exciting FPV flight at speeds of up to 72kph (45mph) in S-mode.\nThe Phantom 4 Pro V2.0 remote controller utilizes the OcuSync HD video transmission system that operates on both 2.4GHz and 5.8GHz frequency bands, with a max transmission range of 7 km. Assessing environmental conditions, the aircraft chooses the best band in which to operate, helping to reduce signal interference.\nFast Switching of Frequency Point\nOcuSync video transmission technology can sense environmental interference in real time. When the interference is strong, the system switches to the frequency point with the least amount of intrusion.\nCamera With 1-Inch 20MP Sensor\nThe onboard camera has been redesigned to use a 1-inch 20-megapixel CMOS sensor. A custom engineered lens made up of eight elements is arranged in seven groups, it is the first DJI camera to use a mechanical shutter, eliminating rolling shutter distortion which can occur when taking images of fast moving subjects of when flying at high speed. In effect, it is as powerful as many traditional ground cameras. More powerful processing supports H.264 4K videos at 60fps or H.265 4K at 30fps, both with a 100Mbps bitrate. Advanced sensors and processors ensure everything is captured with more image detail and the image data needed for post-production.\nSensor size is more important to image quality than the number of pixels because a larger sensor captures more information in every pixel, improving dynamic range, signal-to-noise ratio, and low light performance. The 1-inch 20-megapixel CMOS sensor in the Phantom 4 Pro V2.0 is almost four times the size of the Phantom 4's 1/2.3in sensor. It uses larger pixels and has a maximum ISO of 12800 as well as increased contrast. It is good enough for images to be used immediately, while also capturing enough detail for advanced post-processing.\nProduction Optimized 4K\nAn enhanced video processing system allows video to be captured in cinema and production optimized DCI 4K/60 at a bitrate of 100Mpbs, enabling you to get high-resolution slow motion shots. The Phantom 4 Pro V2.0 also supposed the H.265 video codec. For a given bitrate, H.265 doubles the amount of image processing as H.265, resulting in significantly enhanced image quality. Record in the high dynamic range D-log mode to make the most of this image data for color grading.\nHigh Resolution Lens\nThe resolution and contrast of a lens is critical to image quality, because only a quality lens can capture sharp, vivid photos at high resolutions. The brand new Phantom 4 Pro V2.0 camera has an aerial optimized F2.8 wide-angle lens with a 24mm equivalent focal length. It features eight elements - 2 aspherical - arranged in seven groups that fit into a smaller, more compact frame. Its images are consistently detailed with low distortion and low dispersion, ensuring that photos and videos are sharp and vivid.\n5-Directions of Obstacle Sensing\nFlightAutonomy is expanded with an additional set of high-resolution stereo vision sensors placed at the rear in addition to the pair placed at the front as well as infrared sensing systems places on the left and right sides. This network creates a total of 5-directions of obstacle sensing and 4-directions of obstacle avoidance, protecting the Phantom 4 Pro V2.0 from more obstacles and giving filmmakers the confidence to capture more complex images.\n6-Camera Navigation System\nThree sets of dual vision sensors form a 6-camera navigation system that works constantly to calculate the relative speed and distance between the aircraft and an object. Using this network of forward, rearward and downward vision sensors, the Phantom 4 Pro V2.0 is able to hover precisely in places without GPS when taking off indoors, or on balconies, or even when flying through windows with minimal pilot control. In the unlikely event that front and downward sensors are unable to operate, the aircraft can still hover in place using the rear stereo vision system. The Phantom 4 Pro V2.0 also is able to fly in complex environments at a flight speed of up to 31mph while avoiding obstacles in its path. In Narrow Sensing Mode, the Phantom 4 Pro V2.0 narrows its sensing scope enabling it to see in more detail and fly through small spaces.\nWhat is FlightAutonomy\nFlightAutonomy is an advanced aerial intelligence and flight automation platform. It is made up of a complex network of ten component groups including seven cameras. The Phantom 4 Pro V2.0 acquires a real-time view of its environment and information about the height and position of nearby obstacles to create a 3D map is can locate itself within. The IMU and compasses collect crucial flight status, speed and direction data, while redundant IMUs and compasses monitor critical data and rule out possible errors, dramatically enhancing the reliability of flight.\n1080p Live View\nWith bi-directional perception and error-resilient technology, the Phantom 4 Pro V2.0 saves up to 30% of its bandwidth and solves common problems such as video lag and even compression artifacts within video transmission, providing crystal-clear live video at up to 1080p.\nMultiple Device Connectivity\nOcuSync can connect to multiple devices at the same time. It can link two remote controllers* or a remote controller and set of DJI Goggles. OcuSync can sense aircraft altitude and the relative position of aircrafts while choosing the ideal antenna combination for Goggles, the remote controller, and aircraft for optimal signal transmission.\n* Linking of two remote controllers will be available in a future firmware update.\nSee A Different World\nThe Phantom 4 Pro V2.0 wirelessly connects to DJI Goggles, putting you directly in the heart of the action with dual LTPS displays that offer a high-quality 1080p live camera view with an 85° FOV. Connected via OcuSync, users can experience fast and clear FPV flight with incredibly low latency.Plus, controlling your Phantom with DJI Goggles is more intuitive than ever. Just enable Head Tracking, which allows you to move the aircraft or gimbal simply by turning your head.\nProfessional aerial imaging benefits from longer flight times. The Phantom 4 Pro V2.0 has a maximum flight time of 30 minutes, providing more time in the air to capture the perfect shot. The DJI GO 4 app shows battery lifetime and calculates remaining flight times according to distance traveled and more. It will give alerts when it reaches a minimum safe amount required for a safe journey back to the takeoff point. An advanced battery management system is also in place to prevent overcharging and over draining. When placed in longer term storage, batteries will discharge power to maintain good health."", 'A drone’s primary features include its camera, sensors, and navigation system. It also has an autonomy level. Each of these features will determine how smooth and safe its flight will be. These factors will help to decide which drone is right fit for you. Continue reading to learn more about what features you should look for when buying a drone.\nDrone sensors are an important part of a drone’s system. They gather information from the surroundings to keep the drone’s position, determine its speed, and avoid obstacles. They provide valuable data to drone designers. There are many sensors on the market, so it is important to choose the right one for you.\nAdvanced sensors on drones can collect specialized data. Researchers can combine multiple sensors to create a richer view of an area. This results in better data. A drone can create orthomosaics and 3D models using the right sensor. These data can be processed with algorithms to identify water quality and plant disease, and create surface composition surveys.\nAnother important sensor for drones is the accelerometer. These sensors measure linear acceleration in both the vertical and horizontal directions. These sensors are used to calculate the drone’s altitude, speed, direction, and altitude. They can also detect vibrations. These sensors, despite being tiny, are an essential part of a drone’s system. These sensors are essential for drones to maintain a precise altitude when hovering or flying.\nDrones are gaining a lot of attention. The technology is expected to improve our lives in many ways, from making us safer and more efficient. Drones must be stable and accurate, communicate well, and reach the desired height. It is crucial to ensure that your drones can do all you ask them to.\nYou can mount the sensors on a drone directly above or below the rotating rotors. This is because the airflow is stronger above and below the rotors. Different drone designs can use this airflow to their advantage. You can mount video imaging detectors above or below the drone’s body.\nA drone’s navigation system can be incredibly valuable, but it can’t fly without sensors. Fortunately, drone manufacturers are expanding the range of applications available for drones, including AI-based piloting and smarter sensors.\nThe navigation system of a drone is designed to keep track of its position in real time. The system consists of two main parts: the high-level controller (or controller) and the low-level one (or controller). The high-level controller is designed to enforce the drone’s path while the low-level controller is designed to make the drone respond to any input. It uses an artificial vector field and uses the drone’s mass as a parameter.\nA drone’s navigation system must be capable of identifying and navigating in 3D space. It is essential to ensure the safety of the drone and its operator. It must be able safely land and perform BVLOS missions. A robust navigation system will help protect a UAV from harm, and will make it possible for a drone to complete the mission safely without requiring the assistance of a pilot.\nOne of the most popular drone navigation systems is a GPS-based system, which is used to track the position of the aircraft and determine its direction. The GPS system must also be protected against jamming. Jamming of GNSS signals is possible and simple jammers can even be purchased online. Many drone developers have tried to develop safe landing protocols that allow drones to use GPS. While this method has had some success, it is not yet capable of providing accurate positioning data in all situations.\nCurrently, commercial drones rely on GPS signals and inertial measurement units to determine their location. Small drones are unable to carry heavy sensors, so they can’t be carried around with a lot of weight. Sensors must be small and lightweight. The navigation systems of these drones should not be able to be too complex and bulky. Small drones cannot fly long distances without a GPS signal.\nDrones can also be equipped with GPS and autopilot functionality. This enables the operators to plan the drone’s flight path before the actual flight begins. Operators can also use the autopilot feature on GPS drones to set autonomous flight instructions at waypoints such as hovering for a certain amount of time.\nAutonomy is a key characteristic of unmanned aerial vehicles. Customers can benefit from drones that are able to plan their flights, avoid collisions, take photos and videos without human intervention. Autonomy is also essential for ensuring the quality and consistency of the data captured. Autonomous flight planning reduces the cost of drone inspection programs and can ensure that drone data is accurate, consistent, reliable, and reliable.\nExyn drones are equipped with Level 4A autonomy, which allows them to fly autonomously at speeds of more than 2 meters per second and collect data about their environment. Drones equipped with Level 4A Autonomy are capable of covering an area up to 16 million cubic meters in one flight. This is equivalent to nine football stadiums.\nDelivery will be the most common use for drones. Aerial taxi services will require autonomy and self-flying capabilities. They will be able follow a target object and navigate towards its GPS coordinates. However, self-flying drones can go out of control and become a danger to human users.\nAlthough Level 3 autonomy doesn’t equal full autonomous flight, it is a major milestone in the development and use of pilotless drones. Most players in the UAV industry have attained Level 2 or 3 autonomy. However, they still need to communicate with a human operator to operate safely.\nAlthough autonomous flight sounds like a great use case for autonomous drones (and it is), farmers would be better off manually piloting the drone. Farmers who are inspecting their fence lines might find it very useful if the drone can self-pilot. Farmers may be better off taking a break if the drone is unable to self-pilot.\nA drone equipped with a camera is a great way to get aerial shots of the environment. However, there are many factors to consider before flying a drone for imaging purposes. First, decide what area of interest you wish to cover. Then you should consider how high to fly the drone. The altitude of the drone will affect the number of images that the camera can take. A higher altitude means more fields covered per image, but it also reduces the resolution of the images. The resolution of images will depend on the sensors and the altitude at which the drone is flying.\nWhile some drones come with in-built cameras, the highest-end models will let you mount specific camera equipment on the drone. This can be beneficial for those who plan on filming a film. You can also use a drone for inspection purposes, such as filming farmland or inspecting industrial equipment.\nA night vision lens is another feature you should look for in a drone. This lens can record thermogram videos at 30 frames per second. These thermal lenses also come with 16x zoom. You can also use picture in picture mode, which allows you to overlay thermal images on regular camera images.\nA drone camera’s ability to capture high-quality images is another advantage. A high-quality camera with a gimbal mount will be able to maintain the camera’s position even during turbulence. This is particularly useful if you are planning on aerial photogrammetry projects. The photographs can be used to map out a field or monitor crops. A high-end camera with a high-resolution sensor is important for this kind of project.\nPrivacy is another consideration. While drones are relatively cheap, they can pose privacy concerns. Some states have laws that prohibit aerial photography on private property.']"	['<urn:uuid:dabdc5c7-ea61-4bae-8207-ef4940afd577>', '<urn:uuid:d06db51d-ca05-4f50-bb80-b6ef40ed0df1>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	6	102	2580
7	steam jet ejector function troubleshooting steps	Steam jet ejectors remove non-condensable gases and vapor from condensers using high-pressure steam expanded through nozzles, reaching velocities of 1220 m/s. When troubleshooting these systems, the first step is checking for recent condition changes (like switching boilers or feed rates) and whether the problem occurred suddenly or gradually. Common issues include utility problems with motive steam supply (which should be -0%/+20% of design pressure) and cooling water supply conditions.	['Appliction of Air Ejectors:\n1.Steam Jet Air Ejector:\nIt is one of the types of air ejector which is used in the steam like near the condenser to remove the non condensable gases and some vapour entering into main condenser by an air ejector and it is cooled by the main condensate and released in the ejector condenser.\nA steam is used as the motive fluid to withdraw air and dissolved gases from the condenser by the ejector action. In each stage of the steam jet ejector, high pressure steam is expanded in a convergent /divergent nozzle. The steam leaves the nozzle at a very high velocity in the order of 1220 m/s and a proportion of the kinetic energy in the steam jet transferred by interchange of momentum to the body of air which entrained and passes along with the operating steam through a diffuser in which the kinetic energy of combined steam is re-converted to pressure energy.\nThe maximum pressure ratio that can be obtained with a single stage is roughly 5:1 and consequently it is necessary to use two or even three stages in series to establish a vacuum in the order of 724mm of Hg with reasonable steam consumption.\nThere are a variety of ejectors designed in service which work on the same principle. Older unit have heavy cast steel which serves as a vapour condenser and also contains diffusers. These are arranged vertically the steam entering at the top. More recent design has the diffuser arrangement externally and vapor condenser shell is some what lighter in construction.\nHorizontal singe element two stage air ejectors this unit comprises a stack u-tubes contained in a fabricated mild steel condenser shell on which is mounted a single element two stage air ejector.\nThe condensate from the main or auxiliary condenser is used as the cooling medium. High velocity operating steam emerging from the 1 st stage ejector nozzle entrains the non-condensable and vapour from the main condenser and the mix discharge into the inter condenser.\nMost of the steam and vapour is condensed when it comes into contact with the cool surface of the tubes, falls to the bottom of the shell and drains to the main or auxiliary condenser.\nThe remaining air and water vapour are drawn into the second stage ejector and discharged to the steam drain tank and non-condensable gases are at last discharged to the atmosphere through vacuum retaining valve.\n2.Fresh Water Generator:\nThe next main application of the air ejector in marine field is in fresh water generator as it is used to remove the air and non condensable in the evaporator chamber so as to maintain the vacuum inside the chamber. Thus the efficiency of the generation increases at low temperature of the sea water.\n3. Self Priming of Centifual Pumps:\nIt is also employed in priming of the centrifugal pumps by the air ejector, which removes the air inside the casing of the pump by the suction effect created by the air ejector thus by flooding casing with the liquid so that it helps in stating of the pump.', 'This article is going to cover the troubleshooting process and outline some of the steps that can be taken to successfully troubleshoot an ejector vacuum system. While this article is not a full comprehensive troubleshooting guide, it should provide some high level thought processes to follow when attempting to troubleshoot a vacuum system. Graham Corporation also has engineers available to assist with any troubleshooting needs.\nWhen approaching a vacuum system that’s in need of troubleshooting, the first question you should be asking yourself is, Do I know of any conditions that have recently changed? Have we brought another unit online? Did we switch boilers? Have we changed the unit’s feed rate or feed stock? Has something been opened and worked on? Have we turned off a cooling tower fan? Often vacuum problems can be associated with some change or action that has impacted the operating condition of the vacuum system.\nOne other useful observation when troubleshooting a system is, Did the problem gradually get worse or did the problem occurred suddenly? If the problem occurred suddenly then the ejector system has most likely broken performance and the internal shockwave in one or more of the ejectors have dissipated. A slow gradual loss of performance is more often associated with a gradual increase in loading, a worsening motive steam leak, corrosion, fouling, or wear. If your equipment had experienced a step change in performance, the cause is more likely to be related to a specific event or change in process conditions.\nOne of the most common causes of vacuum system problems is an issue with the system’s utilities. Ejectors require a motive steam supply that is -0% / +20% of the design pressure and not wet or overly superheated beyond the design. Ejector systems also require a cooling water supply flow rate that is at or higher than design and no hotter than the design supply temperature. Many vacuum system problems can be traced back to issues with a system’s utilities. Verifying that your ejector system has the correct utility conditions is one of the first things that should be done when a system is not operating per the design.\nAnother common cause of vacuum issues is a change in the system’s loading. Since changes in loading result in a direct impact on a system’s vacuum, verifying that you are not overloading a system is important. An increase in condensable loading can directly impact the capacity of system in accordance with the first stage ejector’s performance curve. An increase in noncondensable loading or an air leak can overload the downstream equipment causing the system’s performance to break operation. The type of load coming over into a vacuum system can be differentiated because the condensable load will have a larger impact on the front end of the system but will mostly have been condensed by the time the flow reaches the tail end of the system. The tail end of the system will mostly be seeing noncondensable loading.\nIn an ejector system, the front end of a system is supported by the downstream equipment. An ejector does not set its discharge pressure, the downstream piping and equipment sets that pressure. If an ejector’s discharge pressure is higher than design, the ejector is being asked to compress out to a higher back pressure which requires more work from the ejector. If this can’t be achieved, the vacuum in the system breaks down. This means that the tail end of the vacuum system supports the upstream ejectors. If the last stage ejector is not able to operate at the right suction pressure, then the rest of the system upstream of that ejector will not be working correctly because they don’t have the right discharge pressure. If a problem develops with a specific ejector or condenser, the operating pressures in the system will be poorer upstream of that point. This can be used to help identify which component in the vacuum system is the source of a specific problem.\nIn order to fully troubleshoot a vacuum system, a survey of the vacuum system is required. This includes measurements on the system’s cooling water and motive steam. It also requires pressure and temperature readings to be taken on the process at the inlet and outlet of each ejector and condenser. This provides one with a full understanding of the pressure profile of the system from the process vessel through to the discharge of the system. When these numbers are compared to the system’s design, it allows for conclusions to be drawn with regards to the cause of the system’s performance issues. However, caution is required when diagnosing the “cause” of a vacuum problem as sometimes the vacuum issues are being caused by more than one issue. Multiple compounding issues are not uncommon. An example of the survey data that can be gathered is shown below.']	['<urn:uuid:29d884b3-ac69-4e2b-a587-1fbd5c8384db>', '<urn:uuid:ee7c6767-5fb5-4450-8d69-c1c3a8a3579f>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	6	69	1327
8	As someone fascinated by tennis venues, I'd love to know about both the tournament's current home and its historical significance in indoor tennis - what makes this competition special in terms of playing conditions?	The ATP Finals is currently held at Turin's Pala Alpitour stadium in Italy. It has historically been considered the most important indoor tennis tournament on the world tour, featuring controlled conditions for both surface type and illumination. While it has primarily been played on indoor hard courts and carpet, there was one notable exception in 1974 when it was played on grass in Melbourne.	"['Top five players with most titles won in ATP Finals\nNovak Djokovic is the only active player in the list.\nThe ATP Finals is the season-ending championship of the tennis tour. It is the most significant men’s tennis event after the four Grand Slams, and it features the top eight singles players and top doubles team based on their results throughout the season. The ATP competition was first held in 1970 and has been conducted every year.\n54th edition of the tournament, is scheduled to take place at Turin’s Pala Alpitour stadium in Turin, Italy, from 12 to 19 November 2023. Roger Federer and Novak Djokovic share the record for most ATP Finals titles, with six each. Federer won his titles in 2003, 2004, 2006, 2007, 2010, and 2011 with Djokovic in 2008, 2012, 2014, 2015, 2018, and 2021.\nIn this article we will take a look at the top five players with most titles won at the ATP Finals:\nRoger Federer (6)\nThe Swiss maestro, Roger Federer, is one of the most successful players in ATP Finals history. He has won the tournament a record six times, and has also reached the finals three other times. Federer is known for his elegant style of play and his incredible shot-making ability. He is also one of the toughest players mentally on the tour.\nHis balletic court presence and strategic brilliance have made him a perennial contender, securing victories in 2003, 2004, 2006, 2007, 2010, and 2011.\nNovak Djokovic (6)\nDjokovic, asserting his dominance in the recent tennis landscape, is another one of the most accomplished players in ATP Finals history. The 24-time grand slam winner has won the tournament a record six times, and has also reached the final three other times. The Serbian is known for his incredible athleticism and relentless game.\nIvan Lendl (5)\nLendl was one of the most dominant players in ATP Finals history. He won the tournament five times, and he reached the finals thirteen consecutive times. His reign in the tournament spanned from 1981 to 1987, showcasing a level of consistency and excellence that defined an era.\nPete Sampras (5)\nThe American legend Pete Sampras shares the spotlight with five ATP Finals titles, a testament to his prowess on the court during the 1990s. His victories came in 1991, 1994, 1996, 1997, and 1999.\nIlie Nastase (4)\nThe Romanian tennis legend Ilie Nastase was one of the most exciting players in ATP Finals history. He won four titles, showcasing his skill and dominance in the early 1970s. Nastase’s victories came in 1971, 1972, 1973, and 1975. Ilie Nastase was known for his aggressive style of play and his powerful serve.\nThese tennis legends, with their multiple ATP Finals titles, have not only left an indelible mark in the tournament but have also contributed to the rich tapestry of tennis history. Their strategic brilliance, unwavering consistency, and ability to shine on the grand stage have solidified their places as true legends of the sport.\nThe ATP Finals, through the years, has been witness to the brilliance of these players, each adding a unique chapter to the tournament’s illustrious legacy.', '|Surface||Hard – indoors|\n|Draw||8S / 8D|\n|Prize money||US$5,700,000 (2020)|\n|Current champions (2020)|\n|Men\'s singles||Daniil Medvedev|\n|Men\'s doubles|| Wesley Koolhof|\nThe ATP Finals is the second highest tier of annual men\'s tennis tournaments after the four Grand Slam tournaments. A week-long event, the tournament is held annually each November at the Pala Alpitour in Turin, Italy. The ATP Finals are the season-ending championships of the ATP Tour and feature the top eight singles players and doubles teams of the ATP Rankings. The tournament has been one of the popular candidates for the monicker of ""the fifth grand slam"". The tournament was first held in 1970, although it was known under a different name. Roger Federer holds the record for the most singles titles with six, while Peter Fleming and John McEnroe hold the record for the most doubles titles with seven. In the current tournament, winners are awarded up to 1500 ranking points; with each round-robin loss, 200 points are deducted from that amount.\nThe event is the fourth evolution of a championship which began in 1970. It was originally known as the Masters Grand Prix and was part of the Grand Prix tennis circuit. It was organised by the International Lawn Tennis Federation (ILTF). It ran alongside the competing WCT Finals. The Masters was a year-end showpiece event between the best players on the men\'s tour, but did not count for any world ranking points.\nIn 1990, the Association of Tennis Professionals (ATP) took over the running of the men\'s tour and replaced the Masters with the ATP Tour World Championship. World ranking points were now at stake, with an undefeated champion earning the same number of points they would for winning one of the four Grand Slam events. The ITF, who continued to run the Grand Slam tournaments, created a rival year-end event known as the Grand Slam Cup, which was contested by the 16 players with the best records in Grand Slam competitions that year.\nIn December 1999, the ATP and ITF agreed to discontinue the two separate events and create a new jointly-owned event called the Tennis Masters Cup. As with the Masters Grand Prix and the ATP Tour World Championships, the Tennis Masters Cup was contested by eight players. However, player who is ranked number eight in the ATP Champion\'s Race world rankings does not have a guaranteed spot. If a player who wins one of the year\'s Grand Slam events finishes the year ranked outside the top eight but still within the top 20, he is included in the Tennis Masters Cup instead of the eighth-ranked player. If two players outside the top eight win Grand Slam events, the higher placed player in the world rankings takes the final spot in the Tennis Masters Cup.\nIn 2009, the Masters was renamed the ATP World Tour Finals and was held at The O2 in London. The contract ran through 2013, but was extended up to 2015 in 2012, and another time until 2018 in 2015. In 2017 the event was renamed the ATP Finals and the contract with the O2 Arena was extended to 2020. In December 2018 it was announced that London, along with Manchester, Singapore, Tokyo and Turin were on a shortlist of five cities which made the cut from an initial list of 40 to host the event from 2021. In April 2019 the ATP announced that Turin is going to host the ATP finals from 2021 to 2025.\nFor many years, the doubles event was held as a separate tournament the week after the singles competition, but more recently they have been held together in the same week and venue.\nFor most of its history, the event has been considered as the most important indoor tennis tournament on the world tour (there were a few exceptions, when the event was organized outdoors: 1974 Melbourne & 2003–2004 Houston), allowing for controlled conditions of play, regarding both surface type and illumination system.\nIn recent years it has been played on indoor hard courts, however, indoor carpet has featured for many editions previously. Once when Melbourne hosted it in 1974 the grass courts of Kooyong Stadium were used and occurred a few weeks before the 1974 Australian Open, which were also played on grass. Apart from 1974, all tournaments have been on a hard court variant, which has prompted calls, primarily from Rafael Nadal to feature a mix of surfaces and include clay courts. However, this has drawn criticism as well as suggestions to reduce the number of clay court tournaments in the season and the ATP are not keen to change this aspect of the tournament.\nThere are eight players or teams, and playing is mandatory except for injury or other good cause.\nQualification is as follows:\n(a) the top seven players in the ATP rankings (b) up to two grand slam winners ranked between 8 and 20 (in order of ATP ranking, if any such players exist) (c) the next players in the ATP rankings, until the quota of eight is reached.\nPoints, prize money and trophies\nThe ATP Finals currently (2020) rewards the following points and prize money, per victory:\n|Round Robin (each of 3 matches)||$153,000||$30,000||200|\n- 1 Prize money for doubles is per team.\nThere is also an appearance fee of $153,000 singles, and $68,500 per doubles team. The two alternates are paid $73,000 (singles) and $25,000 (doubles teams).\nAn undefeated champion would earn the maximum 1,500 points, and $2,114,000 in singles or $354,500 in doubles.\nUnlike all other singles events on the men\'s tour, the ATP Finals is not a straightforward knock-out tournament. Eight players are divided into two groups of four and play three round-robin matches each against the other players in their group. The two players with the best records in each group progress to the semifinals, with the winners meeting in the final to determine the champion. Though it is theoretically possible to advance to the semi-finals of the tournament with two round-robin losses no player in the history of the singles tournament has won the title after losing more than one round-robin match.\nThe current round robin format of two groups of four players progressing to a semifinal and final, has been in place for all editions of the tournament except the following years:\n- 1970, 1971 – Round robin with no semifinals or finals, winner decided on best performed player\n- 1982, 1983, 1984 – 12 player knock-out tournament with no round robin. The top four seeds in the event received a bye in the first round.\n- 1985 – 16 player knock-out tournament with no round robin\nAs of 2019, the top two players from each group advance to the semi-finals. Round-robin standings are determined by: 1) Number of wins; 2) Number of matches; 3) In two-players-ties, head-to-head results; 4) In three-players-ties, percentage of sets won, then head-to-head result (if two players tied in percentage of sets won and third one is ""different"") or percentage of games won if all three players have same percentage of sets won, then head-to-head results; 5) ATP rankings.\nThe tournament has traditionally been sponsored by the title sponsor of the tour; however, in 1990–2008 the competition was non-sponsored, even though the singles portion of the event as part of the ATP tour was sponsored by IBM. In 2009, the tournament gained Barclays PLC as title sponsor. Barclays confirmed in 2015 that they would not renew their sponsorship deal once it expires in 2016.\nOn 10 September 2020, NItto Denko announced it will extend its title partnership of the ATP Finals for another 5 years, until 2025.\n|Tokyo, Japan||1970||Carpet (i)||Tokyo Metropolitan Gymnasium||6,500|\n|Paris, France||1971||Stade Pierre de Coubertin||5,000|\n|Barcelona, Spain||1972||Hard (i)||Palau Blaugrana||5,700|\n|Boston, United States||1973||Carpet (i)||Boston Garden||14,900|\n|Melbourne, Australia||1974||Grass||Kooyong Stadium||8,500|\n|Stockholm, Sweden||1975||Carpet (i)||Kungliga tennishallen||6,000|\n|Houston, United States||1976||The Summit||16,300|\n|New York City, United States||1977–1989||Madison Square Garden||18,000|\n|Frankfurt, Germany||1990–1995||Festhalle Frankfurt||12,000|\n|Hanover, Germany||1996–1999||Carpet (i)||Hanover Fairground||15,000|\n|Hard (i) (1997)|\n|Lisbon, Portugal||2000||Hard (i)||Pavilhão Atlântico||12,000|\n|Sydney, Australia||2001||Acer Arena||17,500|\n|Houston, United States||2003–2004||Hard||Westside Tennis Club||5,240|\n|Shanghai, China||2005–2008||Carpet (i)||Qizhong Forest Sports City Arena||15,000|\n|Hard (i) (2006–2008)|\n|London, United Kingdom||2009–2020||Hard (i)||O2 Arena||20,000|\n|Turin, Italy||2021–2025||Pala Alpitour||16,600|\n|Titles||Player||Years Won||Years Runner-up|\n|6||Roger Federer||2003, 2004, 2006, 2007, 2010, 2011||2005, 2012, 2014, 2015|\n|5||Ivan Lendl||1981, 1982, 1985, 1986, 1987||1980, 1983, 1984, 1988|\n|Novak Djokovic||2008, 2012, 2013, 2014, 2015||2016, 2018|\n|Pete Sampras||1991, 1994, 1996, 1997, 1999||1993|\n|4||Ilie Năstase||1971, 1972, 1973, 1975||1974|\n|3||Boris Becker||1988, 1992, 1995||1985, 1986, 1989, 1994, 1996|\n|John McEnroe||1978, 1983, 1984||1982|\n|2||Björn Borg||1979, 1980||1975, 1977|\n|Lleyton Hewitt||2001, 2002||2004|\n|1||Andre Agassi||1990||1999, 2000, 2003|\n|Stan Smith||1970||1971, 1972|\n|0||Vitas Gerulaitis||1979, 1981|\n|Jim Courier||1991, 1992|\n|Rafael Nadal||2010, 2013|\n|Dominic Thiem||2019, 2020|\n|Juan Carlos Ferrero||2002|\n|Juan Martín del Potro||2009|\n- Active players marked in bold.\n|Titles||Player||Years Won||Years Runner-up|\n|7||Peter Fleming||1978, 1979, 1980, 1981, 1982, 1983, 1984|\n|John McEnroe||1978, 1979, 1980, 1981, 1982, 1983, 1984|\n|5||Mike Bryan||2003, 2004, 2009, 2014, 2018||2008, 2013|\n|4||Bob Bryan||2003, 2004, 2009, 2014||2008, 2013|\n|Daniel Nestor||2007, 2008, 2010, 2011||1998, 2006|\n|3||Anders Järryd||1985, 1986, 1991||1989, 1992|\n|Rick Leach||1988, 1997, 2001|\n|2||Todd Woodbridge||1992, 1996||1993, 1994|\n|Mark Woodforde||1992, 1996||1993, 1994|\n|Max Mirnyi||2006, 2011||2009, 2010|\n|Jacco Eltingh||1993, 1998||1995|\n|Paul Haarhuis||1993, 1998||1995|\n|Nenad Zimonjić||2008, 2010||2005|\n|Stefan Edberg||1985, 1986|\n|Jonas Björkman||1994, 2006|\n|Henri Kontinen||2016, 2017|\n|John Peers||2016, 2017|\n|1||Sherwood Stewart||1976||1982, 1984|\n|John Fitzgerald||1991||1989, 1992|\n|Mark Knowles||2007||1998, 2006|\nMost singles titles:\nMost consecutive singles titles:\n- Novak Djokovic – 4 (2012–2015)\n- Ivan Lendl – 3 (1985–1987)\nIlie Năstase – 3 (1971–1973)\n- Roger Federer – 2 (2003–2004), (2006–2007), (2010–2011)\nPete Sampras – 2 (1996–1997)\nJohn McEnroe – 2 (1983–1984)\nBjörn Borg – 2 (1979–1980)\nLleyton Hewitt – 2 (2001–2002)\nIvan Lendl – 2 (1981–1982)\nMost singles match wins:\nMost singles match wins % (minimum 10 matches played):\n- Ilie Năstase – 88.0% (22–3)\n- Ivan Lendl – 79.6% (39–10)\n- Roger Federer – 77.6% (59–17)\n- Boris Becker – 73.5% (36–13)\n- Björn Borg – 72.7% (16–6)\nMost singles appearances:\n- Roger Federer – 17 (2002–2015, 2017–2019)\n- Andre Agassi – 13 (1988–1991, 1994, 1996, 1998–2003, 2005)\nNovak Djokovic – 13 (2007–2016, 2018–2020)\n- Ivan Lendl – 12 (1980–1991)\n- Boris Becker – 11 (1985–1992, 1994–1996)\nJimmy Connors – 11 (1972–1973, 1977–1984, 1987)\nPete Sampras – 11 (1990–2000)\nYoungest singles champion:\n- John McEnroe – 19 years, 11 months (1978)\nOldest singles champion:\n- Roger Federer – 30 years, 3 months (2011)\nMost doubles titles:\nMost consecutive doubles titles:\n- Peter Fleming – 7 (1978–1984)\nJohn McEnroe – 7 (1978–1984)\n- Stefan Edberg – 2 (1985–1986)\nAnders Järryd – 2 (1985–1986)\nBob Bryan – 2 (2003–2004)\nMike Bryan – 2 (2003–2004)\nDaniel Nestor – 2 (2007–2008), (2010–2011)\nHenri Kontinen – 2 (2016–2017)\nJohn Peers – 2 (2016–2017)\nMost doubles match wins:\n- Mike Bryan – 42\n- Bob Bryan – 38\n- Daniel Nestor – 34\n- Todd Woodbridge – 29\n- Anders Järryd – 25\nMark Woodforde – 25\nMost doubles match wins % (minimum 10 matches played):\n- John McEnroe – 100% (14–0)\n- Peter Fleming – 88.9% (16–2)\n- Anders Järryd – 78.1% (25–7)\n- Jacco Eltingh – 76.9% (20–6)\n- Stefan Edberg – 75% (9–3)\nMost doubles appearances:\n- Mike Bryan – 16 (2001, 2003–2006, 2008–2018)\n- Bob Bryan – 15 (2001, 2003–2006, 2008–2017)\n- Daniel Nestor – 15 (1995–1998, 2003–2012, 2014)\n- Leander Paes – 14 (1997–2001, 2005–2013)\n- Mahesh Bhupathi – 12 (1997–2001, 2003–2004, 2008–2012)\nMark Knowles – 12 (1995–1998, 2001, 2003–2009)\nYoungest doubles champion:\n- John McEnroe – 19 years, 11 months (1978)\nOldest doubles champion:\n- Mike Bryan – 40 years, 7 months (2018)\nYear-end championships triple\nWinning the Masters Cup, the WCT Finals and the Grand Slam Cup during his career.\n- The event at which the year-end championships triple was achieved indicated in bold below:\n|#||Player||Grand Prix/ATP Masters Cup||WCT Finals||Grand Slam Cup|\nYear-end championships doubles\nWinning the Masters Cup and the WCT Finals, or the Masters Cup and the Grand Slam Cup, or the WCT Finals and the Grand Slam Cup.\n- The event at which the year-end championships double was achieved indicated in bold below:\nMasters Cup – WCT Finals\n|#||Player||Grand Prix/ATP Masters Cup||WCT Finals|\n* he later completed the Y-EC Triple\nMasters Cup – Grand Slam Cup\n|#||Player||Grand Prix/ATP Masters Cup||Grand Slam Cup|\n* with the 1996 Grand Slam Cup title he also completed the Y-EC Triple\nWCT Finals – Grand Slam Cup\n|#||Player||WCT Finals||Grand Slam Cup|\n* with the 1996 Grand Slam Cup title he also completed the Y-EC Triple\nYear-end championships generations double\nWinning the ATP Finals and the Next Gen ATP Finals during his career.\n- The event at which the year-end championships generations double was achieved indicated in bold below:\n|#||Player||ATP Finals||Next Gen ATP Finals|\n- ""Why Indian Wells Is Almost (But Not Quite) a Fifth Slam"".\n- John Barrett, ed. (1991). The International Tennis Federation : World of Tennis 1991. London: Collins Willow. pp. 116, 140. ISBN 9780002184038.\nBesides the prize money of $2,020,000, there were also ranking points at stake for the first time at a season ending play-off\n- Piers Newbery (3 July 2007). ""London to host World Tour Final"". BBC Sport. Archived from the original on 7 March 2016.\n- ""ATP finals to stay in London through 2015"". The Times Of India. Archived from the original on 2012-11-08. Retrieved 2012-07-11.\n- ""ATP World Tour Finals to be showcased in London till 2015"". Archived from the original on 2012-11-13. Retrieved 2012-09-11.\n- ""ATP Confirms London As Host City Through 2018 As 2015 Season Finale Is Officially Launched | ATP World Tour | Tennis"". ATP World Tour. Archived from the original on 2016-11-17. Retrieved 2016-11-16.\n- ""ATP World Tour Finals to stay in London till 2020 under new title sponsor"". The Guardian. 25 May 2017. Archived from the original on 25 May 2017.\n- ""ATP Extends Season-Ending Finale In London Through 2020 With New Title Partner Nitto Denko Corporation"". Association of Tennis Professionals (ATP). 25 May 2017.\n- ""ATP Finals: Manchester & London on five-city shortlist to host event from 2021"". BBC Sport. 2018-12-14.\n- ""Turin To Host ATP Finals From 2021 To 2025"". ATP. 2019-04-24. Retrieved 2019-11-15.\n- ""ITF Tennis - Pro Circuit - Masters Singles - 10 December - 15 December 1974"". www.itftennis.com. Retrieved 2018-12-17.\n- ""I never played ATP Finals on clay or outdoor, complains Rafael Nadal"". Tennis World USA. Retrieved 2018-12-17.\n- ""Darren Cahill calls for ATP to make surface change at ATP Finals"". Tennis World USA. Retrieved 2018-12-17.\n- ""ATP urged to change Finals surface to give Rafael Nadal a better chance"". Tennis365.com. 2018-12-15. Retrieved 2018-12-17.\n- Ubha, R. (5 November 2013). ""Nadal and Federer at loggerheads over ATP World Finals"". CNN. Retrieved 17 December 2018.\n- ""Does the clay-court season take up too much of the tennis calendar?"". ESPN.com. 2018-05-22. Retrieved 2018-12-17.\n- ""ATP Finals won\'t be played on clay, says Chris Kermode"". Tennis World USA. Retrieved 2018-12-17.\n- ""ATP sets Double Challenge Cup for Jan. 29-Feb. 2 in Bangalore"". Associated Press AP. 2002-01-16. Archived from the original on 2012-11-04. Retrieved 2008-11-03.\n- ""Points and Prize Money - Nitto ATP Finals"". nittoatpfinals.com. Archived from the original on 1 November 2017. Retrieved 9 May 2018.\n- ""Thomas Lyte lifts Webb Ellis Cup"". 2015-09-15. Archived from the original on 2015-11-17. Retrieved 2015-11-13. Thomas Lyte Lifts Webb Ellis Cup\n- ""In pictures: Sporting trophy workshop"". BBC News. 9 May 2018. Archived from the original on 27 November 2017. Retrieved 9 May 2018.\n- ""Semi-final Qualifying Procedure"". Nitto ATP Finals.\n- ""ATP agree $35 million deal for showpiece tournament"". Reuters. 2008-06-18. Archived from the original on 2010-11-07.\n- ""Barclays to end World Tour Finals sponsorship"". BBC News. 4 November 2015. Archived from the original on 7 November 2015. Retrieved 13 November 2015.\n- ""ATP extends season-finale in London through 2020 with new title partner Nitto Denko Corporation"". London: Nitto ATP Finals. 25 May 2017. Archived from the original on 27 September 2017. Retrieved 28 August 2017.\n- ""ATP & Nitto Denko Corporation Extend Partnership Until 2025"". ATP Tour. 10 September 2020. Retrieved 20 November 2020.\n- ""Two for Smith"". The Province. 6 December 1971. p. 17.\n- ""That Rumanian black magic"". Sports Illustrated. Retrieved 27 October 2019.\n- O2, The. ""Event space capacities, The O2"". www.theo2.co.uk. AEG, 2016. Archived from the original on 20 December 2016. Retrieved 8 December 2016.\n- Turin To Host ATP Finals From 2021 To 2025\n- ""Barcays ATP World Tour Finals – Historical Stats"". ATP Tour. Retrieved 6 December 2014.\n|Wikimedia Commons has media related to ATP Finals.|']"	['<urn:uuid:8703b5d8-8a36-4e58-b58f-735895a218fd>', '<urn:uuid:264083c5-bb62-4cd0-9564-83c7f8d25e95>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:38:36.516973	34	64	3245
9	What are the two types of export controls in Japan's system?	There are two types of controls in Japan's export system: 1) Category list control, which requires exporters to apply for a license when exporting listed items to a foreign country, and 2) Catch-all control, which requires licenses when items will be used for applications related to Weapons of Mass Destruction (WMD) or conventional arms.	['Japan’s export control system is very complicated. It is not a simple set of laws and regulations but instead is a complex mix of primary and secondary legislation. It is this process that has led to confusion for exporters when navigating the complexities of these controls.\nTo demonstrate how complex the requirements are, below is a high-level overview of Japan’s export control system.\nLaws and regulations that control export\nAccording to the law, all goods for export are required to obtain export permission by Japanese Customs prior to exportation. If a good to be exported is subject to other laws and regulations that require license or approval, the exporter should obtain license or approval in advance, and provide proof of this approval at time of export declaration.\nThe laws that restrict exports fall into two categories:\n- Trade related laws:\n- Other laws:\ne.g. Plant Protection Act\n(Other laws are listed below)\nLicense and approval by Foreign Exchange and Foreign Trade Act\nArticle 48 of the Foreign Exchange and Foreign Trade Act stipulates that export license and approval shall be specified by Cabinet Order. This Cabinet Order is the “Export Trade Control Order” that specifies the types of goods subject to export license requirements and approval by the Minister of Economy, Trade and Industry (METI).\nGoods subject to export license or approval under the Foreign Exchange and Foreign Trade Act include:\n- License (Appended Table 1): Strategic Goods that are considered to obstruct the maintenance of international peace and security\n- Approval (Appended Table 2): Specified goods; specified regions, processing trade deal\nThe goods required to obtain a license by the METI are listed as item 1 to 16 in the Appended Table 1 of the Export Trade Control Order. Goods listed in Table 1 are considered to obstruct the preservation of international peace and security .\nThere are two types of controls:\nExport licenses under the category list control, requires exporters to apply for a license when exporting or transferring the listed items to a foreign country.\nExport licenses under the category catch-all control, requires the same when the items being exported will be used for certain applications related to Weapons of Mass Destruction (WMD) or conventional arms. Under catch-all control, exporters must obtain a license if: 1) they are aware that the item will be used for the development, manufacture, use or storage of WMD or 2) they are informed by METI that there is a considerable risk and the item in question will be used for the development, manufacture, use, or storage of WMD.\nThe catch-all control is not applied to exports destined for the following 27 countries that have high level export control systems and are members of international treaties and regimes restricting WMD: Argentina, Australia, Austria, Belgium, Bulgaria, Canada, the Czech Republic, Denmark, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Republic of Korea, Luxemburg, the Netherlands, New Zealand, Norway, Poland, Portugal, Spain, Sweden, Switzerland, the U.K., or the U.S.\nThere are two areas that require export approval:\n- Goods listed in the Appended Table 2 of Export Trade Control Order\n- To ensure domestic supply; e.g. eels (fry for fish culture)\n- To maintain an orderly market; e.g. vessels equipped with fish catching equipment or machines\n- Under international treaties; e.g. animals and plants specified by Washington Convention, frozen baby clams\n- Restricted items; e.g. narcotics, counterfeit currencies, goods that may infringe patent rights\n- Goods under a contract for processing\nRaw materials to be used for processing work under a contract , falling under the scope of ‘Designated Processing Works’, which are specified by METI.\nLeather products re-importing to Japan (shoes, bags, wallets, leather clothes, fur products, etc.) that have been processed overseas after exporting as materials (raw leather, tanned leather, finishing leather, etc.) from Japan require approval from METI.\nThe goods under contract for processing are not subject to this rule, or approval before export, if the goods are less than 1 million yen of the contract price.\nLicense required and approval by other laws related to exports\nAct on Protection of Cultural Properties; e.g. important cultural properties or art treasures, natural monuments, important tangible folk cultural properties\nProtection and Control of Wild Birds and Mammals and Hunting Management Law; e.g. wild birds and mammals and its processing goods, eggs of birds\nNarcotics and Psychotropic Control Act; e.g. narcotics, psychotropics, materials of narcotics and psychotropics\nCannabis Control Act; e.g. cannabis, products made from cannabis\nOpium Control Act; e.g. opium, opium poppy\nStimulants Control Act; e.g. stimulants, materials of stimulants\nRabies Prevention Act; e.g. dogs, cats, racoons, fox, skunk\nAct on Domestic Animal Infectious Diseases Control; e.g. cloven-footed animals, horses, chickens, duck, rabbits, honey-bees\nPlant Protection Act; e.g. plants (flower plants, pteridophytes or bryophytes (including their parts, seeds, fruits and processed goods such as straw mat or straw bale or other similar products)), injurious plant, injurious animal (insect, mite)\nRoad Transport Vehicle Act; e.g. used cars\nToday many companies interact with foreign governments and companies, as they expand their business activities across borders. The Japanese Export Control system is quite complicated. Like most countries, to comply with the many international schemes and regulations, exporters need to understand the requirements when navigating through the complexities. Knowing the country’s rules might not be sufficient enough without a good self-management program that addresses export controls; though having a good base understanding is the first step when setting up an export controls program.\n Item 1-4: Weapons and WMD: e.g. military vessels, nuclear fuels; Item 5-14: Strategic goods (e.g. metallic magnetic materials, numerically-controlled coordinate measuring equipment, semiconductor substrates, telecommunication optical fibers, communication cable systems capable, gyroscopes, underwater robots); Item 15: Sensitive goods (e.g. communication jamming equipment, Information security equipment); Item 16: WMD related goods except for food and timbers\n When the items to be exported are either on Attachment List No. 1 to the Export Trade Control Order (goods), the exporter should apply for an export license to METI.\n When the exporter is “informed” by METI, or “knows” that the end-use or end-user of the items to be exported is related to the development, manufacture, use, or storage of WMD, or conventional weapons, the exporter should apply for an export license to METI.\n Contracts that engages exporting materials on condition that 1) a consignee outside Japan manufactures (processes) on a commission basis and 2) the products after processing will be reimported in Japan.']	['<urn:uuid:aa536438-20e7-418b-ab9b-aeb1f36f9eaa>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	11	54	1064
10	waterproofing basement solutions effectiveness health risks	The efficient solutions for basement waterproofing include using drainage control apparatus with vertical and horizontal legs, and installing sump systems. However, even after waterproofing, health risks can occur as mold spores can develop within 48 hours of flooding and spread through ducts throughout the home, and sewage water can contain harmful e-coli bacteria that can contaminate soft surfaces.	"['A building, which has a basement, has a great chance to face the problem of seepage of water or penetration of water through the wall of the basement especially at the time of heavy precipitation. This water from seepage stay at the floor line of a basement and if not controlled this condition can cause severe damage to the interior wall of the basement and the contents of the basement. This type of water seepage can get into the basement through the crack of the foundation walls and can develop over time if not it handled properly.\nPorous building materials, such as concrete block, are capable of percolation and seepage of water through the building material itself and into the interior portion of the structure. Another source of moisture arises from capillary action and water vapor.\nFacts and Ways of Water Intrusion into the Basement :\nWater can enter a basement in several ways. The conventional basement consists of three elements. Such as\n- A foundation wall.\n- A footer on which the wall rests.\n- A floor slab.\nUnderground water and Groundwater is the main source of water which will enter a basement.\nIf the water table rises above the level of the basement floor water can enter the basement through the side wall.\nSo what is the solution to this problem?????????????? The answer is basement waterproofing. Below we have discussed briefly the basement waterproofing, methods, which methods are efficient etc.\nWhat is Basement Waterproofing?\nBasement waterproofing means the technique and the materials to prevent seepage of water into the basement or the penetration of water into the basement of a house or structure. This waterproofing is for an underground room of a structure.\nMethod of Basement Waterproofing\nThere have many ways of solving the problem of seepage of water but all the way is not truly efficient because of cost and difficulty of installation.\nSome Method and the fact about those methods :\n- Trying to seal the crack after the formation of a crack of a foundation wall. It can be done at inside or outside. It is not only costly but also ineffective.\n- Moisture resistant flashings or coatings can be a solution but Moisture-resistant flashings or coatings tend to fracture and tear due to building expansion, settling, and careless installation.\n- By setting a plywood board against the foundation wall before the pouring of concrete slab water can be drained from the interior of the basement. Then the board is removed while the concrete is still ""green\' and not completely set. But this procedure has some disadvantages:\n- it causes damage to the edge of the concrete floor;\n- it results in additional labor costs,\n- it may cause the concrete floor to shift.\nSo an efficient method is highly desirable for waterproofing solution. Two methods, which are considered as efficient, are described below.\nEfficient Methods of Basement Waterproofing\nApparatus: Drainage Control Apparatus\nPart of the Apparatus: Vertical leg, Horizontal leg, Embossments, Longitudinal spacer slip, Drainpipe.\nMethodology: A drainage control apparatus has been used in this method. A vertical leg of this apparatus has been set up to the vertical side wall of the basement and a horizontal leg has been set up to the top of the foundation footing. The vertical leg consists of an embossment at the bottom end of the vertical leg. The vertical leg also consists of a longitudinal spacer lip at the upper end of the vertical leg. Both the embossment and the spacer lip touch vertical side wall of the basement to maintain a gap between the vertical leg and the vertical side wall. The horizontal leg consists of channels to flow water into a drain pipe. For the corner of the basement, the apparatus is named as corner drainage control apparatus but the setting of the apparatus is same. After that concrete is poured to fulfill the slab and then the vertical portion of the apparatus has been cut off to level the apparatus with slab end. This invention is quite easy to install and it is also advantageous because it can drain water from the interior of the basement.\nA sump system in geometry is rectangular. It has two side elements with apertures to exchange the groundwater while blocking debris. In another side element, an adjustable inlet had been provided to connect other elements of a basement waterproofing system to the sump liner. After that in the sump liner, a base configured can be used to provide a built-in stand for the sump pump. The pump stand contains a lip including notches to allow the fallen of debris and collection in a trough around the periphery of the base. The base further provides an underside cavity which accommodates an obstruction in the floor of the sum hole. A removable lid used to facilitate the access of the interior of the sump liner and also contains a breakaway feature to accommodate discharge piping. The sump liner can be oriented into the sump hole to protect the basement foundation from the adverse effect of erosion.\n- Andras, S. Basement sump system, and method. U.S. Patent 8973324B2 filed January 24, 2013, and issued March 10, 2015.\n- Geske, D. R. Apparatus, and method for waterproofing basements. U.S. Patent 4869032A filed September 25, 1987, and issued September 26, 1989.\n- Read, R. R. Basement Waterproofing. U.S. Patent 5845456A filed December 14, 1990, and issued December 8, 1998.\n- DiFiore, D. Basement waterproofing system. U.S. Patent 4136500A filed March 30, 1978, and issued January 30, 1979.', 'You walk down the stairs to your basement and the realization isn’t a pleasant one. Flooding in basements can be caused by a variety of sources including ground water and runoff seasonally or after an unusual volume of natural precipitation. Improperly sealed foundations can allow ground water to enter through the concrete floor or through cinder blocks. Some basement floods are caused by cold weather and burst pipes or even backed up eaves troughs and gutters.\nNo one plans to have a flooded basement, which is why when they happen they cause so much damage and inconvenience to the home owner. A basement flood is not something people anticipate having to deal with, and few people are equipped to manage the situation or have the appropriate expertise or equipment in place. So if it has happened to you and you were not ready, you aren’t alone but we can help you be prepared in the event that it happens again.\nIn addition to the tremendous inconvenience and expense of a flood, there are a number of other considerations regarding health and safety, as well as documenting loss and other necessities for your insurance damage claim. We’ll take you through step-by-step to help you respond to your basement flood situation like a professional.\nAssessing the Risk of Injury\nBefore you enter your basement or step into the water, do a careful evaluation of the water level versus the electrical outlets. Without thinking, some homeowners have inadvertently stepped into water that is electrically charged. Even if the water level is well below the electrical outlets in your basement, if you had appliances on the floor or even a power bar with multiple outlets for your television or other equipment, the water could be carrying a deadly electrical charge.\nRemember that even when the power is out in your house, the water in your home and basement can still carry an electrical current from your neighbors or from a source near you. Do not go into your basement (it’s not safe). Call an experienced electrician to disconnect your home from the power grid by removing the meter from its socket. Only then can you be certain that you have reduced the risk of electrocution and after an assessment by a licensed electrician is advised.\nFrom Sewage and Bacteria\nIt is actually not easy to tell from the onset what the source of the flooding is, and it can be a one or multiple sources, but quickly identifying if some, or all of the water in your basement is sewage is important to protecting your health. However if the cause is a rupture to the sewage or a back-up of sewage, the appropriate precautions must be taken to prevent infection from e-coli and other harmful bacteria.\nConsult with your municipal utility who can test the water onsite to evaluate sewage content. Remember that e-coli is a bacteria that can spread quickly and soft surfaces such as clothing, furnishings, carpeting and even drywall can absorb and house the bacteria. While non-porous items can be cleaned it is common practice to discard property that has been soiled by sewage and flooding, rather than encounter health risks.\nEven after water has been removed from a flooded basement, mold spores can remain. It takes less than 48 hours after a flood for spores to develop in standing water, creating a longer term health problem for harmful mold behind paneling, under flooring or inside insulated walls. Harmful mold from a flooded basement can also infiltrate ducts, spreading mold spores throughout the home.\nFrom the Structure\nWooden walls and brick foundations can be compromised by flooding. Typically the water present on the floor represents a small portion of the water that has seeped into the foundation and building blocks of the basement walls. As you remove water from the basement, this water level drops however it can also lead to partial wall collapse.\nWhen entering a flooded basement ensure that you are wearing protective gear such as rubber boots, gloves and depending on the presence of mildew, an air filtration mask to avoid allergic reaction or breathing issues. Conduct a survey of the wood (if any) structures in your basement to determine if they are load bearing and at risk or showing signs of stress and possible collapse.\nWater and Property Damage and Valuation\nWhen you have established that it is safe to begin restoring your basement and cope with the water issue, it is also time to review your insurance coverage. Before you move anything ensure that you are accurately documenting all damage that pertains to the flood.\nBegin by taking pictures of your damaged belongings and make a manifest of all your property that was impacted by the flood and approximate value; you can greatly decrease the processing time in most cases for your claim. Be prepared to send everything to your insurance adjuster for legal documentation.\nIt is advised that you contact one or more contractors to evaluate the structural damage to your home as a result of the flooding incident. Damage to flooring, walls, foundation, electrical and plumbing as well as your furnishings and belongings should be submitted as part of your insurance claim.\nRestoring Your Basement\nWhen it comes time to start removing the water from your basement, there are a number of options depending on the volume of water.\nStep 1: Pump and Remove Water\nAn effective pump can be rented from your local home improvement or tool rental site to help you remove the water. Most pumps come equipped with a discharge hose to carry water to your sump pump hole or drain, or out the window and away from the house depending on the severity and type of flood.\nOnce the large volume of water has been removed you can clean up the rest to help your basement dry with the assistance of a Wet Vac or similar equipment that allows you to vacuumed up puddles and left over water.\nStep 2: Drying or Curing Your Basement\nBefore rebuilding it is important to allow ample time for your basement to ‘air out’ and dry thoroughly. This will help to prevent the growth of bacteria or mold within the space.\nStep 3: Inspection and Removal\nDiscard any soft surface furnishings and belongings that may have become contaminated during the flood. While they may be dried out and deodorized, mold spores and bacteria can remain in foam and fabrics.\nStep 4: Mold Treatment\nHire a professional to treat your basement as well as your duct work to prevent any of the contamination from the flood from reaching the upstairs area of your home.\nStep 5: Evaluate and Repair\nFind the source of the foundation leak. Sometimes flooding happens through a window well which can be easily fixed with proper sealing. However in the event of a leak through the concrete floor or from the foundation, a residential engineer will need to be consulted to evaluate the best way to install appropriate drainage. This is typically done by digging a channel in the concrete and installing weeping tile to help create natural runoff from underground sources.\nIn the event of a foundation leak within the walls or cinder blocks, additional drainage and waterproof sealing of the wall will be required. This can include an excavation of your exterior foundation wall to install new drainage and tile, as well as to seal the exterior wall with a water resistant silicon barrier.\nIf the ground beside your basement is sloped inward toward the wall, ground water may be traveling into the foundation by gravity alone. Fixing the gradient of your surrounding landscape may save you from further flooding. Remember that landscape should slope down and away from any wall to divert water elsewhere.\nThe installation of a quality sump pump will reduce and in most cases eliminate future flooding of your basement. Whenever the ground water level raises to a point that it is higher than the foundation of your home, a quality sump pump will automatically begin to divert and discharge the water away to prevent flooding. In older homes this discharge may be into a sink or into the sewer which is now prohibited by most municipalities. Consult with a waterproofing experts to discuss your best option with regards to safety, restoration and reclamation of your flooded basement and preventative measures to prevent future loss and expense.']"	['<urn:uuid:b796d0aa-4d4c-4824-bbc9-61b51569a097>', '<urn:uuid:cb05d56f-7e6c-4e5b-bfad-8ca55593a74a>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T18:38:36.516973	6	58	2326
11	where horse statue bronze biggest el paso	The largest bronze equine statue in the world, measuring 36 feet (11 m) high, stands in El Paso, Texas. It depicts a rearing Andalusian stallion ridden by Mexican conquistador Don Juan de Oñate.	['Andalusian breed information\nAndalusian horse general information\nCOLORTheir most common coat color is gray, although they can be found in many other colors.\nSIZE15.1 – 15.3 hands high\nORIGINSpain, Iberian Peninsula\nUSESAndalusians today are used for show jumping, western pleasure and many other classes at horse shows.\nINFLUENCEBecause of the influence of the later Habsburg families, who ruled in both Spain and other nations of Europe, the Andalusian was crossbred with horses of Central Europe and the Low Countriesand thus was closely related to many breeds that developed, including the Neapolitan horse, Groningen, Lipizzaner and Kladruber. Spanish horses were used extensively in classical dressage in Germany from the 16th century on. From this use, they influenced many German horse breeds, including the Hanoverian, Holstein, East Friesian and Oldenburg. Dutch breeds such as the Friesian and Gelderland also contain significant Spanish blood, as do Danish breeds such as the Fredericksborg and Knabstrup. Andalusians were a significant influence on the creation of the Alter Real, a strain of the Lusitano, and the Azteca, a Mexican breed created by crossing the Andalusian with American Quarter Horse and Criollo bloodlines. The Spanish jennet ancestors of the Andalusian also developed the Colonial Spanish Horse in America, which became the foundation bloodstock for many North and South American horse breeds.\nTEMPRERAMENTAndalusians tend to be docile, but also intelligent and sensitive. When treated with respect they are quick to learn, responsive and cooperative.\nAndalusian horses are both elegant and strongly built. Members of the breed have heads of medium length, with a straight or slightly convex profile. Ultra convex and concave profiles are discouraged in the breed, and are penalized in breed shows. Necks are long and broad, running to well-defined withers; chests massive; backs short; hindquarters broad and strong, with well-rounded croups. \\r\nAndalusians began to be exported from Spain in 1962. The first Andalusians were imported into Australia in 1971, and in 1973 the Andalusian Horse Association of Australia was formed for the registration of these Andalusians and their offspring. Strict quarantine guidelines prohibited the importation of new Andalusian blood to Australia for many years, but since 1999, regulations have been relaxed and more than half a dozen new horses have been imported.\\r\nalso rely on imported stock, and today, all American Andalusians can be traced directly to the stud books in Portugal and Spain. There are around 4,500 animals living today in the United States, where the International Andalusian and Lusitano Horse Association (IALHA) registers around 400 new foals every year. These numbers indicate that the Andalusian is a relatively rare breed in the United States. As of 2003, there were 75,389 living horses registered in the stud book, and they constituted almost 66 percent of the horses in Spain. Breed numbers have been increasing during the 21st century.\\r\nAndalusian fun facts\nAndalusians have been present in films ranging from Gladiator to Interview with a Vampire, and Lara Croft Tomb Raider: The Cradle of Life to Braveheart. The horses have also been seen in such fantasy epics as The Lord of the Rings film trilogy, King Arthur, and The Chronicles of Narnia: The Lion, the Witch and the Wardrobe. In 2006, a rearing Andalusian stallion, ridden by Mexican conquistador Don Juan de Oñate, was recreated as the largest bronze equine in the world. Measuring 36 feet (11 m) high, the statue currently stands in El Paso, Texas.']	['<urn:uuid:7ad80cc7-5703-49ad-9d3d-a154ec516d3e>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	7	33	563
12	As a wildlife conservationist studying historic hunting impacts, I'd like to know about past hunting pressures on Spermophilus undulatus - what were the hunting levels for international skin trade in the late 1950s?	Between 1958 and 1960, it was estimated that 418,400-551,000 individuals were killed annually for the international trade in skins. However, this hunting activity is now believed to have ceased.	"[""|Scientific Name:||Spermophilus undulatus|\n|Species Authority:||(Pallas, 1778)|\nArctomys altaicus Eversmann, 1841\nCitellus eversmanni (Brandt, 1841)\nCitellus eversmanni Obolensky, 1927 subspecies transbaikalicus\n|Red List Category & Criteria:||Least Concern ver 3.1|\n|Assessor/s:||Shar, S. & Lkhagvasuren, D.|\n|Reviewer/s:||Amori, G. (Small Nonvolant Mammal Red List Authority) & Tsytsulina, K. (Global Mammal Assessment Team)|\nThis species has a large population size and a wide distribution. No decline in population size has been detected, and there are no known widespread major threats.\n|Range Description:||Distributed in mountain forest-steppes and steppes from Southern Siberia and Altai (Russia) to Manchuria. Two isolated population in north, in Yakutia (S. u. jacuensis) and east of the range, between Amur and Burei Rivers in Russia and N Heilongjiang in China (S. u. menzbieri). In China distributed in Altai and Sayan mountains; Xinjiang, Altai mountains; Xinjiang, Tian Shan mountains. In Mongolia found in Mongol Altai, Hangai, Hövsgöl and Hentii mountain ranges, as far south as Aj Bogd Massif in Trans Altai Govi Desert. Recently recorded in the northern portions of Ikh Nartiin Chuluu Nature Reserve in Eastern Govi.|\nNative:China (Heilongjiang, Xinjiang); Kazakhstan; Mongolia; Russian Federation\n|Range Map:||Click here to open the map viewer and explore range.|\nIt is common and sometimes abundant species. Population size very much differs depending on the habitat. For example, in Transbaikalia in steppe river valleys population density is up to 250 burrows per ha (Badmaev, 1996). In Tuva it is about 100 burrows per ha (Ol'kova, 1962). In other habitats density is much lower, in wormwood and cereal steppe it is about 5 animals per ha, and in subalpine zone it is 1-2 animals per ha (Shilova and Savinetskaya, Biodiversity Conservation Centre web-site\n|Habitat and Ecology:||Inhabits semi-desert and mountain steppe habitats along the edge of forests. In China found in thinly wooded savannas and grassy steppes bordering the Gobi desert. In addition to grasslands, they occupy bushy terrain among oaks and white or black beech groves, alpine meadows, and wet areas along river valleys. Lives in colonies with a labyrinth of burrows. Burrows are characteristically 8-13 cm in diameter and surrounded by a large mound of soil (up to 2 m in diameter and 40 cm high) Diurnally active; although most active at dawn and dusk. Diet consists of green vegetation and seeds, but also insects. Before hibernating makes a store of vegetation to utilize following arousal. Hibernates from October until late March/mid April. Reproduce once per year in spring; litters of 3-9 young are produced following a 30 day gestation. Start mating on second year.|\n|Major Threat(s):||Hunting for international trade in skins once occurred at high levels, between 1958 and 1960 it was estimated that as many as 418,400-551,000 individuals were killed annually (Stubbe, 1965). This activity is now believed to have ceased and no other threats are known to be impacting upon this species at present.|\n|Conservation Actions:||Occurs in some protected areas (approximately 11% of the species’ range in Mongolia).|\nBadmaev B. B. 1996. Ecology of Long-Tailed Ground Squirrel Citellus (Spermophilus) undulatus Pall. 1778, in Western Transbaikalia. Moscow State University.\nOl'kova, N. V. 1962. Ecological features in long-tailed ground squirrel in connection to its epidemiological and economic value. Irkutsk.\nSmith, A. and Xie, Y. 2008. The Mammals of China. Princeton University Press, Princeton, New Jersey.\nStubbe, M. 1965. Jagd, Jagdgesetz und Wild in der Mongolischen Volksrepublik. Beiträge zur Jagd- und Wildforschung 4: 163-178.\nWilson, D. E. and Reeder, D. M. 1993. Mammal Species of the World. A Taxonomic and Geographic Reference. Smithsonian Institution Press, Washington, DC, USA.\n|Citation:||Shar, S. & Lkhagvasuren, D. 2008. Spermophilus undulatus. In: IUCN 2012. IUCN Red List of Threatened Species. Version 2012.2. <www.iucnredlist.org>. Downloaded on 19 May 2013.|\n|Feedback:||If you see any errors or have any questions or suggestions on what is shown on this page, please fill in the feedback form so that we can correct or extend the information provided|""]"	['<urn:uuid:8cfd9456-323b-4fca-99c7-cad98e6b4917>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	33	29	639
13	fertility preservation method young males cancer	Scientists have developed a new hope for fertility preservation in young boys with cancer through sperm generation from frozen tissue. The procedure has been successfully tested in non-human primates, marking the final stage before human trials. At UPMC, 110 boys currently have their samples stored. The technology demonstrated success by producing healthy babies in animal trials, where sperm from frozen tissue was used to fertilize 138 eggs, resulting in 11 successful embryo implantations.	['Washington Post – ‘Growing’ sperm from frozen tissue is “seen as next generation of assisted-reproduction therapy,” as scientists say it will “advance offers new hope for fertility preservation in young boys with cancer.”\nLead author Kyle Orwig, Ph.D., and researchers* with the Fertility Preservation Program in Pittsburgh located within the University of Pittsburgh Medical Center (UPMC), posted their study and findings – Autologous grafting of cryopreserved prepubertal rhesus testis produces sperm and offspring – in Science on March 22, 2019.\nSamples of these sperm were then used to fertilize 138 eggs, with 11 of the successful embryos implanted into females. The healthy babies produced confirmed the entire procedure had been a success.\n“The ability to produce a healthy live offspring – the gold standard of any reproductive technology – has not been achieved until now,” said Dr Adetunji Fayomi, who led the study.\nPerforming these trials in non-human primates marks the final stage in the development of this procedure before trying it in humans.\nWith the successful birth of Grady, the scientists say they are now ready to take their new technology to the clinic.\nSo far, 110 boys have their samples stored at UPMC.\nA similar treatment is available for prepubescent girls undergoing cancer treatment. This involves harvesting and freezing an ovary to be re-implanted in the future. This process is also in the experimental phase, and UPMC currently has 25 frozen ovaries saved.\nThe re-implanting process has been studied on animals for decades with success. In animal trials, the samples have been viable for up to 14 years. Head of UPMC Magee’s Fertility Preservation program Kyle Orwig says he thinks it’s time to study the re-implanting process in people.\n“I think we have patients today that would be eligible for the technology,” he said. “I don’t think we need to wait 14 years, I think it’s time to take that technology to the clinic now.NPR\n*[List of researchers and their affilitations:\nAdetunji P. Fayomi1,2,3, Karen Peters3, Meena Sukhwani3, Hanna Valli-Pulaski2,3, Gunapala Shetty4, Marvin L. Meistrich4, Lisa Houser5, Nicola Robertson5, Victoria Roberts5, Cathy Ramsey5, Carol Hanna5, Jon D. Hennebold5, Ina Dobrinski6, Kyle E. Orwig1,2,3,*\n1-Molecular Genetics and Developmental Biology Graduate Program, University of Pittsburgh School of Medicine, Pittsburgh, PA, USA.\n2-Department of Obstetrics, Gynecology, and Reproductive Sciences, University of Pittsburgh School of Medicine, Pittsburgh, PA, USA.\n3-Magee-Womens Research Institute, University of Pittsburgh School of Medicine, Pittsburgh, PA, USA.\n4-Department of Experimental Radiation Oncology, The University of Texas MD Anderson Cancer Center, Houston, TX, USA.\n5-Assisted Reproductive Technology Core, Oregon National Primate Research Center, Beaverton, OR, USA.\n6-Department of Comparative Biology and Experimental Medicine, Faculty of Veterinary Medicine, University of Calgary, Calgary, Alberta, Canada.\nDid you know scientist recently unfroze ram sperm and created healthy born lambs?\nGizmodo – “In 1968, veterinary scientist Steven Salamon froze pellets of the Merino sheep sperm using liquid nitrogen, which he did to test the long-term viability of semen cryopreservation.”\nSemen frozen back in 1968 has been used to impregnate dozens of Merino ewes, resulting in healthy lambs. The Australian scientists who made it happen say it’s the oldest sperm ever used to produce offspring.\nA research team led by Simon de Graaf from the Sydney Institute of Agriculture and School of Life and Environmental Sciences impregnated 34 Merino ewes with the thawed out 50-year-old ram sperm, according to a University of Sydney press release. Incredibly, the sperm resulted in birth rates comparable to semen frozen for 12 months.\nThis latest experiment suggests sperm can remain frozen for at least 50 years to no discernible ill effect. The news bodes well for not just veterinary scientists, but also individuals at risk of losing their fertility, such as males undergoing chemotherapy for cancer. As de Graaf told Inverse, “What is true for the sheep is also true for humans.”']	['<urn:uuid:c5f43fe9-8613-4586-b292-54c63727fa7b>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	6	73	634
14	is house of david mentioned tel dan stela osorkon jar	No, the House of David is only mentioned in the Tel Dan stela found in the city wall at Tel Dan. The Osorkon jar found at Samaria does not mention the House of David, it only bears the cartouche of Osorkon II.	['Last summer, archaeologists excavating at Tel Dan (biblical Dan) found a fragment of a stela (inscribed stone) in the remains of a city wall that scholars acclaim as “one of the most important discoveries in the annals of Biblical archaeology” (Wood, 1993, 6:121). The stone fragment seems to have been from a victory stela erected at Dan by a conquering Aramean (Syrian) army. When the Israelites eventually reclaimed the city, they destroyed the stela and used its fragments in various structures (Shanks, 1994, 20:39). Professor Avraham Biran, the archaeologist heading the excavation, has dated the stela to the first half of the ninth century B.C. (Shanks, 1994, 20:38).\nThough only thirteen partial lines remain of this once-impressive monument, they contain an unparalleled literary jewel. Lines 8 and 9 explicitly mention the “king of Israel” and the “House of David,” which the conquering army defeated [The drawing on the left depicts the lower portion of the basalt stela from Tel Dan. The engraved inscription is written in paleo-Hebrew. The two highlighted areas are translated “king of Isreal” and “House of David,” respectively.] These statements are important for several reasons. First, this is the only extant, extrabiblical document that unquestionably mentions the name David (perhaps it also appears in the Mesha stela, better known as the Moabite stone; see Lemaire, 1994). Even more remarkable is the fact that his name appears in the familiar phrase “House of David.” Given the date of the stela, this serves to confirm the biblical usage of this designation (cf. 1 Kings 12:19, 14:8, Isaiah 7:2, et al.).\nSecond, though critical scholars have tended to minimize the importance of Israel and Judah during this historical period, the inscription supports the significance that the Bible attaches to these two kingdoms. Third, the tentative date of this discovery corresponds historically with 1 Kings 15:9-20 in which Ben-Hadad, King of Syria (Aram), attacked several Israelite cities including Dan. Some scholars argue that the stela is an exact parallel to this sacred account.\nHowever, there seem to be some differences between the details of 1 Kings 15:9-20 and the ancient stela fragment. Most conspicuously, the stela suggests (if accurately translated) that the Syrian army destroyed both Israel and Judah, but the biblical text indicates that Syria and Judah were allies against Israel. These discrepancies do not necessarily mean that either account is inaccurate. It may be that the stela refers to another battle not mentioned in the Bible, and it is very likely that there were several skirmishes involving Syria. But the stela does demonstrate that Syria (Aram) had military conflicts with Israel, lending corroborative testimony to the historical reliability of the biblical text.\nNo doubt, analysis of and debate over the stela will continue for some time. We can be certain, however, that the name “David” has been found in a ninth-century B.C. text other than the Bible. That incontrovertible fact is yet another ancient witness to biblical credibility.\nLemaire, Andre (1994), “ ‘House of David’ Restored in Moabite Inscription,” Biblical Archaeology Review, 20:30-37, May/June.\nShanks, Hershel (1994a), “ ‘David’ Found at Dan,” Biblical Archaeology Review, 20:26-39, March/April.\nShanks, Hershel (1994b), “New Inscription May Illuminate Biblical Events,” Biblical Archaeology Review, 20:38, March/April.\nWood, Bryant (1993), “New Inscription Mentions House of David,” Bible and Spade, 6:119-121, Autumn.', 'Jeroboam II and Osorkon II\nThe conventional timetable has Ahab, the king of the Northern Kingdom of Israel, as a contemporary of one of the kings of the Libyan Dynasty, usually Osorkon II. And almost regularly reference is made to archaeological evidence called to substantiate this synchronism; it is worded thus: Osorkon II. He was a contemporary of Ahab, for in his palace at Samaria an albaster vase bearing the name of Osorkon II was found.(1)\nIn the chapters VI to VIII of Ages in Chaos, dealing with the el-Amarna period, it is demonstrated that Ahab was a contemporary of the later kings of the Eighteenth Dynasty, Amenhotep III and IV (Akhnaton), and that over sixty-five letters on clay tablets addressed by him to these kings are still in existence, in originals, as written by the royal scribes, found in the ground of el-Amarna. Having been a contemporary of these pharaohs, the synchronization of Ahab with Osorkon II of the Libyan Dynasty cannot but be grounded in error. To expose the error of the quoted sentence, we have to check the records of the excavators.\nDuring the years 1908-1910 the Harvard University archaeological expedition at Sebastieh, ancient Samaria, uncovered the foundation of a palace. It was tentatively identified as the palace built by Omri and enlarged by Ahab.\nLike the unearthed portions of the city wall of Samaria, the palace was built on virgin rock. The biblical passage about Omri building his capital on an unoccupied hill was regarded as confirmed. The floor of the palace was covered with layers containing the remains of later structures; but no vestige of earlier structures was found under the floor, nor were any signs of settlement prior to the time of Omri, except for a neolithic encampment, unearthed on the site of Samaria.\nOn the floor of the palace numerous small Egyptian objects were found, among them scarabs (signets). The carvings on the scarabs are mostly decorative designs, but on one of them a cartouche, or royal name, was found engraved. The cartouche was that of Thutmose III. Since there was no plausible explanation for the presence of the cartouche of Thutmose III in the palace at Samaria, presumably built about six centuries after this pharaoh had died, the excavators suggested: This may be a local imitation of an Egyptian scarab. (2) As we have seen in the first volume of Ages in Chaos, Thutmose III reigned only a few decades before Omri; the cartouche apparently is genuine.\nA jar with the cartouches of Osorkon II was found near the palace of Samaria and it was brought forth as an evidence for the contemporaneity of Osorkon II and Ahab. Scores of ostraca were also found in Samaria. Ostraka, or potsherds inscribed with ink, were less expensive than burnt clay tablets or papyri; they were used when it was not expected that the writing would be preserved in an archive. Wine and oil when delivered were accompanied by these shards.\nThe ostraca of Samaria are inscribed with the names of persons or towns that delivered oil or wine to the kings palace; they are dated in the ninth year, in the tenth year, in the seventeenth year, of the king, but the name of the king is not mentioned.\nIn various books and articles it is asserted that the jar of Osorkon, contemporary of Ahab, was found in the same debris as the ostraca,(3) and it has been concluded that the ostraca of Samaria refer to the ruling years of Ahab. But is it true that these inscribed shards were found in the same debris as the Osorkon jar? And then, is it true that the ostraca of Samaria date from the reign of Ahab?\nThe report of the excavation gives the location precisely:\nThe southern wall of the Osorkon House [so-called because of Osorkons jar] was built in part over the foundations of the north wall of rooms 406, 407, and 408. The foundations of the assumed northern part of the Ostraca House must have been destroyed previous to the construction of the Osorkon House.(4)\nIt follows that Osorkons jar came to its location later than the ostraca came to theirs. This nullifies the argument that the jar must be of the same age as the ostraca. Thus even had the ostraca been inscribed during Ahabs reign, Osorkons jar found its place at a definitely later date. But of what age are the ostraca?\nThe archaeologists at first reasoned thus: Since Osorkon II is known to have been a contemporary of Omri and Ahab, and since Omri reigned but twelve years, and the ostraca mention the seventeenth year of the king, they must have been written in the days of Ahab. It follows that the ostraca of Samaria are about the same age as the Mesha stele of the middle of the ninth century.(5)\nA comparison of the Hebrew signs of the Samaritan ostraca with the Hebrew characters of the Mesha stele shows a definite change in the writing of single letters. The same characteristics found in the Samaritan letters reappear in the Shiloah inscription of King Hezekiah, dating from close to -700. How to explain that the characters of the ostraca, a quarter of a century older than the stele of Mesha, are more directly related to the later characters of the Shiloah inscription?(6) This compelled the researchers to advance the hypothesis that the Hebrew letters passed through a retrograde stage of development before resuming their progress, or that in Moab the development was slower than in Samaria.\nIn subsequent excavations at Samaria ivories with Hebrew letters were unearthed. These letters were found to be of the same type as those on the stele of Mesha and to have therefore originated in the ninth century. They are of a more archaic type than the characters of the ostraca of Samaria.(7)\nThe conclusion has now for some time been generally accepted that the Samaritan ostraca were written not in Ahabs time, but in the time of one of the last kings of Samaria. Of the kings of Israel after Ahab, only Jeroboam II and Pekah reigned for more than seventeen years. Th scholarly opinion arrived at an almost unanimous conclusion that the ostraca were written in the days of Jeroboam II (ca. -785 to -744).(8) This conclusion appears to be correct.\nThe house that sheltered the jar of Osorkon II in Samaria was built on the ruins of the house that sheltered the inscribed potsherds. Since the ostraca were written in the days of Jeroboam II, one of the last kings of Israel to reign in Samaria, every ground for making Pharaoh Osorkon II a contemporary of Ahab because of the findings in Samaria vanishes. Judged by these findings, Osorkon II was not only later than Ahab, but also later than Jeroboam II.']	['<urn:uuid:12dd95c4-1617-48fe-af5f-adb6d611b293>', '<urn:uuid:aefaf95c-d1bb-4ea2-9871-9322b4ec0788>']	factoid	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T18:38:36.516973	10	42	1686
15	What role do smartphones play in tracking performance metrics during outdoor activities, and how do skin type differences affect the interpretation of outdoor exposure data?	Smartphones can track various performance metrics during outdoor activities using apps that measure speed, distance, and time, with features like real-time results and data streaming through OBDII analysts like Automatic. However, the interpretation of outdoor exposure data varies significantly based on skin type - people with naturally very dark skin need three to six times more sun exposure than those with fair to olive skin, which affects how outdoor activity duration should be measured and interpreted.	"['A track day is inherently a great day. Using only the advanced telemetry found in a smartphone, it\'s now easier than ever to show your results—and earn the respect of your track rat friends. Add any or all of these apps to supplement the basic roster of maps, compass, stopwatch, and voice recorder, for a more enjoyable track day.\nRecord the Basics\nFor track day novices, an app like TrackAddict is a great tool to have. At no cost to download, the free version of the app displays lap times and vehicle speed, making it a far better use of your smartphone\'s accelerometer than the integrated stopwatch.\nLearn From Your Mistakes\nTrackAddict Pro claims to recognize hundreds of circuits—from rally courses to drag racing—and can display your speed through the straights and bends. Connect your OBD accessory for even more data displayed in-app. If you take track days seriously, we recommend shelling out $8.99 for the full TrackAddict Pro.\nThe All-In-One Solution\nTrack day logging doesn\'t get more comprehensive than the easy-to-use Track Attack app. From the moment you pull up to one of hundreds of courses stored in the app\'s memory, Track Attack recognizes the circuit and times your way through it. Put your money where your racing shoes are and plunk down $14.99 for the full version.\nThe Full Picture\nHave a VBOX data recorder lying around? In that case, it behooves you to download the VBOX Sport Performance Test app, which beams the data—lap times, straightline acceleration, and the like—received straight to your smartphone. The app download is free, but even the most basic VBOX will set you back several hundred dollars.\nVideo On Demand\nIf you bring your wifi-enabled GoPro to the track, and prefer to capture video ""the old-fashioned way,"" there\'s still an app to facilitate capturing the best possible shots. Download the free GoPro app and beam video straight onto your phone, making it easier to position your camera and obtain the proper, wide-angle view.\nTime, Speed, and Distance\nEven if there aren\'t rally stages in your neighborhood, that doesn\'t mean you can\'t use Rally Co-Pilot, the app designed to ""calculate special stages and save them."" The app can provide real-time results of average speed, distance, and time remaining, using separate stopwatches. Who needs a co-driver when app tech is this good?\nWith only your iPhone at your disposal, you can be almost as handy as your local mechanic. Plug in Automatic, our favorite OBDII data analyst, and information about fuel consumption, miles logged, and mechanical trouble can be streamed directly to your smart device. After track day use, you probably won\'t want to share your ""driver score"" with your insurance company.\nFor the Android User\nDon\'t have an iProduct? Don\'t despair. The Trackmaster app for Android phones is nearly as all-inclusive of a data recorder as any of the other apps featured here. As a bonus, the app claims that you can ""send your data to Google Earth for desktop replay, analysis, and editing.""', ""Vitamin D is essential for strong bones, muscles and overall health. Ultraviolet (UV) radiation from the sun is the best natural source of vitamin D, but it is also the main cause of skin cancer. A balanced approach to sunlight exposure can help you avoid vitamin D deficiency, which can have significant health effects.\nVitamin D is essential for strong bones, muscles and overall health. Ultraviolet (UV) radiation from the sun is necessary for the production of vitamin D in the skin and is the best natural source of vitamin D.\nUV radiation from the sun is also the main cause of skin cancer. Taking a balanced approach to sun exposure can help make sure you get enough vitamin D while minimising your skin cancer risk.\nTo boost your vitamin D levels when in the sun, expose at least your face, arms and hands, or equivalent area of skin. For vitamin D to be produced, your skin must not be covered with clothing.\nProlonged sun exposure will not increase vitamin D levels further, but will increase the risk of skin cancer. Solariums should never be used to boost vitamin D, as they emit dangerous levels of UV that increase the risk of skin cancer.\nDaily exercise also assists with the body’s production of vitamin D.\nHealth effects of low vitamin D\nVitamin D is a hormone that controls calcium levels in the blood. It is crucial for bone and muscle development, and for preventing osteoporosis. Vitamin D deficiency may not result in any obvious symptoms, but without treatment it can have significant health effects and increase a person’s risk of musculoskeletal conditions, such as:\n· Bone and muscle pain\n· Rickets (soft, weakened bones) in children\n· Osteopenia (weak, fragile bones) in older adults.\nAs well as maintaining your vitamin D levels, you also need adequate calcium in your diet to help prevent these conditions.\nLow vitamin D has also been linked to an increased risk of:\n· Multiple sclerosis\n· Diabetes (type1 and type 2)\n· Various types of cancers (particularly colon cancer)\n· Heart disease\n· Mental health conditions (including schizophrenia)\n· Worse outcomes in stroke\n· Altered immunity and other autoimmune diseases.\nMore research is needed to see whether increasing vitamin D levels could help prevent any of these conditions.\nPeople at risk of low vitamin D\nPeople who are at increased risk of low vitamin D include:\n· People with naturally very dark skin – the pigment in skin (melanin) acts as a filter to UVB (ultraviolet B) radiation and reduces the amount of vitamin D that the body makes in the skin.\n· People with little or no sun exposure – Some groups of people are at particular risk of receiving little or no sun exposure. They include:\n· Older adults – especially people who are frail, in medium-to-long-term residential or aged care, and housebound people\n· People who wear covering clothing for religious and cultural reasons\n· People who deliberately avoid sun exposure for cosmetic or health reasons\n· People at high risk of skin cancers and who therefore avoid exposure to the sun\n· People hospitalised or institutionalised for long periods\n· People with a disability or chronic disease\n· People in occupations with little sun exposure, such as office workers, taxi drivers, factory workers or night-shift workers.\n· Breastfed babies with other risk factors for low vitamin D – breastfed babies who fall into the risk categories above or have mothers with low vitamin D. Babies get their initial store of vitamin D from their mothers. While infant formula is fortified with vitamin D, breastmilk contains very little. Therefore, breastfed infants rely more heavily on their mothers’ initial stores. If the mother of a breastfed baby has low vitamin D (or had low vitamin D during pregnancy) then her baby is also at risk of low vitamin D.\n· People with medical conditions or medications affecting vitamin D metabolism, including:\n· End-stage liver disease\n· Kidney disease\n· Conditions that cause fat malabsorption (such as cystic fibrosis, coeliac disease and inflammatory bowel disease)\n· Use of some drugs that increase the breakdown of vitamin D (such as rifampicin and some anticonvulsants).\nIf you are concerned that you or your child may be at risk of vitamin D deficiency, you should discuss this with your doctor.\nVitamin D and food\nThere are small amounts of vitamin D in some foods such as fish, eggs and UV-irradiated mushrooms, but it is difficult to obtain enough vitamin D from diet alone. Most people only get five to 10 per cent of their vitamin D from food. Margarine and some types of milk have added vitamin D.\nVitamin D and safe sun exposure\nUV levels vary depending on the time of year, and the amount of sun exposure required varies accordingly. Too much sun exposure can increase the risk of skin cancer, so it’s important to find a sensible balance between sun exposure for vitamin D and protection against skin cancer.\nThe ‘daily sun protection times’ indicate when the UV is forecast to be three or above. During these times, people with fair to olive skin should use a combination of sun protection measures (sunscreen, hat, protective clothing, sunglasses and shade).\nPeople with naturally very dark skin may not need to wear sunscreen, but should still protect their eyes from the sun, with sunglasses or a hat. People with this skin type may need three to six times as much sun exposure as people with fair to olive skin.\nCheck the SunSmart UV Alert for daily sun protection times for your location, available as a free SunSmart smartphone app, or online (sunsmart.com.au).\nUV levels in Victoria\nAs shown in the table below, from September to April, average UV levels in Victoria are three and above for much of the day. This level of UV increases the risk of overexposure and skin damage, and requires sun protection. Be extra cautious in the middle of the day when UV levels are most intense. Sunscreen use at these times is advised, and should not put people at risk of vitamin D deficiency.\nFrom May to August, average UV levels in Victoria are low (below three), making it a great time to roll up your sleeves and get some winter sun.\nTable 1. Safe sun exposure in Victoria, to reduce the risk of low vitamin D\n|Time of year||Skin type||UV exposure required||Sun protection required||Other|\n|September to April |\n(UV levels three and above for much of the day)\n|Fair to olive skin||A few minutes of mid-morning or mid-afternoon sun exposure to the face, arms and hands (or equivalent area of skin) most days of the week.||A combination of sun protection measures is needed during these months:\n· protective clothing sunglasses\nBe extra cautious in the middle of the day when UV levels are most intense.\n|Naturally very dark skin||Three to six times the level of exposure required by people with fair to olive skin.||It may not be necessary for people with this skin type to wear sunscreen. Eyes should be protected from the sun with sunglasses or a hat.||Vitamin D supplements may also be required.|\n|May to August |\n(UV levels mostly below three)\n|Fair to olive skin||Approximately two to three hours of midday sun exposure to the face, arms and hands (or equivalent area of skin) spread across the week.||Not needed, except for people:\n· outside for extended periods\n· near highly reflective surfaces such as snow or water.\n|Naturally very dark skin||Three to six times the level of exposure required by people with fair to olive skin.||Not needed, except, perhaps, for people:\n· outside for extended periods\n· near highly reflective surfaces such as snow or water.\nEven in these conditions, it may not be necessary for people with this skin type to wear sunscreen, but you should still protect your eyes from the sun with sunglasses or a hat.\n|Vitamin D supplements may be required as it may not be possible to maintain vitamin D levels through sun exposure alone at this time of year.|\nLow vitamin D\nIf you are concerned about your vitamin D levels, seek advice from your doctor. A simple blood test can determine your level and assess your risk. If you are at risk of low vitamin D, you will need to have regular medical check-ups to monitor your levels.\nYour doctor may recommend vitamin D supplements, which should be taken strictly as directed. Once low vitamin D is treated, the aim is to maintain normal vitamin D levels.\nWhere to get help\n· Your doctor\n· Maternal and child health nurse\n· Dietitians Association of Australia Tel. 1800 812 942\n· NURSE-ON-CALL Tel. 1300 60 60 24 – for expert health information and advice (24 hours, 7 days)\nThings to remember\n· Vitamin D is essential for strong bones, muscles and overall health.\n· The sun is the best natural source of vitamin D, but you need to balance sun exposure with skin cancer risk.\n· Daily exercise also assists with the body’s production of vitamin D.\n· Solariums should never be used to boost vitamin D as they emit dangerous levels of UV that increase the risk of skin cancer.\n· Some people are at increased risk of low vitamin D – this includes people with naturally very dark skin and people who have very low exposure to sunlight.\nYou might also be interested in:\n- Nutrition - women's extra needs.\n- Orienteering - preventing injury.\n- Vegetarian and vegan eating.\n- Vitamin and mineral supplements.\n- Vitamins - common misconceptions.\nWant to know more?\nGo to More information for support groups, related links and references.\nThis page has been produced in consultation with and approved by:\n(Logo links to further information)\nDepartment of Health\nFact sheet currently being reviewed.\nLast reviewed: April 2012\nContent on this website is provided for education and information purposes only. Information about a therapy, service, product or treatment does not imply endorsement and is not intended to replace advice from your doctor or other registered health professional. Content has been prepared for Victorian residents and wider Australian audiences, and was accurate at the time of publication. Readers should note that, over time, currency and completeness of the information may change. All users are urged to always seek advice from a registered health care professional for diagnosis and answers to their medical questions.\nContent on this website is provided for education and information purposes only. Information about a therapy, service, product or treatment does not imply endorsement and is not intended to replace advice from your qualified health professional. Content has been prepared for Victorian residence and wider Australian audiences, and was accurate at the time of publication. Readers should note that over time currency and completeness of the information may change. All users are urged to always seek advice from a qualified health care professional for diagnosis and answers to their medical questions.\nFor the latest updates and more information, visit www.betterhealth.vic.gov.au\nCopyight © 1999/2014 State of Victoria. Reproduced from the Better Health Channel (www.betterhealth.vic.gov.au) at no cost with permission of the Victorian Minister for Health. Unauthorised reproduction and other uses comprised in the copyright are prohibited without permission.""]"	['<urn:uuid:359c8054-2816-454d-953d-a2d4b0bd2a4f>', '<urn:uuid:76e31a20-ee56-4e80-90f0-80f6714a13ae>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	25	76	2376
16	dog handler training different approaches compare	Different approaches exist for dog handler training across disciplines. In hunting dog training, trainer John Sykes emphasizes psychological aspects and common sense, gradually introducing gunshot sounds starting with wooden boards clapping and moving up to different gauge shotguns. Meanwhile, search and rescue handlers like Cathy Farrell and Ronnie Perry focus on maintaining their dogs' work ethic through regular training exercises in simulated disaster environments, developing strong handler-dog bonds over years of partnership in real emergency response situations.	"['It’s wing season time in North Idaho.\nThat means hunters have the opportunity to shoot upland game and snip the wing on a dead grouse to use as a dog-training tool.\nGrouse wings are a standard tool for bird dog aficionados as well as the newly indoctrinated hunting dog owner whose flop-eared pup was purchased with future upland game seasons in mind.\nTossing a bird wing for a pup to retrieve, or dancing one on the end of a fishing line to teach a pup to point, has been standard fare for bird dog owners since the dawn of “The American Sportsman” television series.\n“The American Sportsman,” a classic weekend hunting show that began when the RCA Victor came with a walnut cabinet, is, like TV cabinets, no longer around.\nUsing a wing to teach a dog, though, is sort of a tradition.\nIt’s one that John Sykes doesn’t like.\n“Wings aren’t a good idea,” said Sykes, a Palouse-based hunting dog trainer.\n“They get a dog to like to chew on things,” Sykes said.\nMost first-time and many experienced dog owners who have learned to toss wings to eager pups don’t employ the kind of discipline it requires to introduce a puppy to a bird wing, Sykes said.\nHe sees a lot of those dogs at his Palouse River Kennels west of Potlatch.\n“It’s easier to train a dog than to retrain a dog,” Sykes said.\nWhen Sykes trains dogs, he takes the animal’s genes into account. German shorthaired pointers are among the most popular pheasant- hunting dogs in the U.S., Sykes said. But because they were initially bred to hunt a variety of game including fox and deer, they tend to chew on stuff. Tossing a wing to a lanky, puddle-eyed shorthair pup, and letting it have its way, releases its desire to gnaw and bite. That can transfer into the dog chewing up the dead birds that hunters want gently returned to hand.\n“They tend to be hard-mouthed dogs anyway,” he said.\nThat means they clamp down on a bird where other breeds, retrievers for example, are more prone to gum a bird instead of bite it.\nThere’s another problem with the wing-tossing scenario, said Sykes, who is also a falconer and trains his own dogs to work hand in hand with his birds of prey.\nMost hunters display the same eagerness as their pups. Patience is a key to training a sound bird dog. Sykes doesn’t like to start training a dog until after it has shed its baby teeth.\n“Somewhere between four and six months they are teething,” he said. “They want to chew on something because their gums hurt and it makes them feel good.”\nThere’s another reason to be patient and let a pup grow a bit.\n“We’re kind of waiting on brain cells at that point,” Sykes said.\nUsing the proper stimulus, dogs that are more mature than a sprightly, needle-toothed 3-month-old are easier to train.\nWhen Sykes trains dogs at his kennels — he’s usually booked and stops taking in dogs in the dead of winter — it is with a dose of psychology that he calls common sense.\nHe uses it to shoot down traditional training methods such as the practice of popping a gun when a dog is eating in an effort to get pooch to equate the shotgun blast with comfort. The dog won’t be afraid of the noise if he has his snout in a bowl of kibbles when first he hears the bang, according to the theory.\nA gun shy dog is better suited for the couch than the field, right?\nWhat really happens when you shoot a gun near a feeding dog, Sykes said, it trains the dog to eat birds when it hears the blast. Or, something like that.\nIt’s better to pop a blank when the dog has a bird in its mouth, Sykes said. He uses clipped wing pigeons.\nWhen you do that, he said, the dog immediately associates the bang with a downed bird.\n“That noise means something good is going to happen,” he said. “And that something is a bird.”\nThe dog doesn’t associate the sound with food.\nThings go easier from there on out.\nNo eating of your prized, wing-shot pheasant, no raised hackles among hunters, no ruffled feathers, just a dog with a bird in its mouth, bringing the bird back to the shooter because that is the behavior it connects with a gunshot.\nTo introduce a pup to the sound of a gunshot, Sykes first claps two pieces of 2-by-4 together. He gradually moves up to pulling the trigger on a 410 shotgun, weeks later he works up to a 20 or 12 gauge.\nHe doesn’t use a .22 he said, because it is too loud.\n“Dogs can hear the bullet move up the riflings,” he said. “It’s a lot louder for a puppy than it sounds to us.”', 'It\'s a retirement celebration for some very special search and rescue responders at Disaster City in College Station.\nNews 3 went inside the rubble to see responders go to work, who happen to have four legs.\n9-year-old dogs Blade and Rose have been an important part of Texas Task Force 1 for a combined 16 years and Saturday was the last time for them to train before they retire.\nIt\'s a bittersweet day between man and his best friend.\nDisaster City is the place where labs Blade and Rose come to train.\nWhile it\'s work, these dedicated dogs see it as their playground and bark on command once they discover a person trapped in the simulated disaster.\n""We like to go out on a high note more or less I mean we know at this age you see little things as a handler that they\'re starting to have a little maybe arthritis or something. So they\'re just not as young in their body as they think in their mind they are,"" said Cathy Farrell, a Texas Task Force 1 Canine Search Specialist.\nFarrell has called Blade partner since he was two.\n""Let\'s make sure you\'re not hurt anywhere. I saw you slide in there between the rocks. Yeah. Good boy,"" said Farrell.\nAfter covering state and federal disasters including hurricanes, tornadoes, and floods these rescue dogs new job will be being household pets.\n""It\'s a bond…. Good girl. You ready to go?,"" asked Ronnie Perry with the task force.\nPerry is also parting ways with Rose.\n""As her work ethic and wanting to do it you really can\'t tell one day to the next that she\'s to that age of ready to retirement and so as handlers we have to look out for our dogs and know that it\'s time,"" he said.\nAs they transition to a more restful life their trainers have taught them tricks to keep these active dogs stimulated even in retirement.\nRonnie Perry will begin training with a new canine partner soon.\nMeanwhile Cathy Farrell is also retiring from Texas Task Force 1.\nThis weekend marks the retirement of three Search and Rescue Canines, and three Canine Search Specialists from Texas Task Force 1. Rose, Blade and Maggie will retire after a combined 25 years of service with the State’s Urban Search & Rescue Team, which also serves as one of the 28 FEMA Task Forces. These canines have deployed with Texas Task Force 1 since 2005 including Hurricanes Katrina, Rita and Ike, tornadoes in South Texas as well as floods in Central Texas. Susann Brown, one of Rose’s handlers, and Mike McKenna, Maggie’s handler, will retire as Canine Search Specialists but will remain on TX-TF1 in different positions. Ronnie Perry, a second handler for Rose, will begin training a new canine partner soon. Cathy Farrell, Blade’s handler, will retire from the Task Force along with Blade. Susann, Mike and Cathy have 57 years of canine SAR canine training experience between them. The group will be involved with training as part of a Disaster K9 Workshop this weekend at Disaster City.\nWho is TX-TF1?\nTX-TF1 is sponsored by the Texas Engineering Extension Service (TEEX) and TX-TF1 has deployed over 80 times since 1997, including the Space Shuttle Columbia tragedy, Sept. 11th World Trade Center attack, and Hurricanes Katrina, Rita, and Ike. TX-TF1 can be activated by the Texas Division of Emergency Management or as one of the Federal Emergency Management Agency’s (FEMA) 28 sanctioned urban search and rescue teams.\nMembers of TX-TF1 range from firefighters and medical personnel, to structural engineers, and come from all areas capable of reporting to College Station within a four-hour window. These regions include the Dallas/Fort Worth, Houston, Austin and San Antonio areas.\nTo comment, the following rules must be followed:\nComments may be monitored for inappropriate content, but the station is under no legal obligation to do so.\nIf you believe a comment violates the above rules, please use the Flagging Tool to alert a Moderator.\nFlagging does not guarantee removal.\nMultiple violations may result in account suspension.\nDecisions to suspend or unsuspend accounts are made by Station Moderators.\nQuestions may be sent to firstname.lastname@example.org. Please provide detailed information.']"	['<urn:uuid:e1c2901c-2cec-4871-b3f6-6afd9f3023d1>', '<urn:uuid:fb76b701-47db-4537-b4e3-a179e134a32d>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T18:38:36.516973	6	77	1520
17	R2P conflict resolution peaceful military measures comparison	The R2P doctrine emphasizes peaceful measures as the preferred approach to protecting populations, including capacity building, mediation, and sanctions. Military intervention is only considered as a last resort when peaceful means prove inadequate. The UN Security Council must authorize any military action, as demonstrated in Resolutions 1674 and 1706. According to UN Special Advisor Edward Luck, out of eight or nine R2P invocations, only the Libya case involved sanctions and military force, showing that peaceful applications of R2P significantly outnumber military interventions.	['The doctrine of the responsibility to protect was first elaborated in 2001 by a group of prominent international human rights leaders comprising the International Commission on Intervention and State Sovereignty. Under their mandate, the Commission sought to undertake the two-fold challenge of reconciling the international community’s responsibility to address massive violations of humanitarian norms and ensuring respect for the sovereign rights of nation states.\nLed by Gareth Evans, former Foreign Minister of Australia, and Mohamed Sahnoun, Special Advisor to the UN Secretary-General, the Commission issued its report in December 2001. Focusing on the “right of humanitarian intervention”, this report examined when, if ever, it is appropriate for states for take coercive – and in particular military – action, against another state for the purpose of protecting populations at risk. In essence, the group concluded that when a group (or groups) of people is suffering from egregious acts of violence resulting from internal war, insurgency, repression or state failure, and the state where these crimes are taking place is unable or unwilling to act to prevent or protect its peoples, the international community has a moral duty to intervene to avert or halt these atrocities from occurring.\nThe “responsibility to protect” doctrine received renewed emphasis in 2004 when the United Nations Secretary-General Kofi Annan created the High-Level Panel on Threats, Challenges, and Change. The Panel was established to identify major threats facing the international community in the broad field of peace and security and to generate new ideas about policies and institutions aimed at preventing or confronting these challenges.\nAfter a year of deliberations, the panel issued its findings in a report entitled A More Secure World: Our Shared Responsibility, in December 2004. The report provided a new assessment of the numerous challenges ahead for peaceful interstate relations and made recommendations of the necessary if these challenges are to be met effectively through collective action.\nWith respect to R2P, the Panel endorsed this “emerging norm”, stating that:\n“…There is a growing recognition that the issue is not the â€œright to interveneâ€� of any State, but the â€œresponsibility to protectâ€� of every State when it comes to people suffering from avoidable catastrophe â€” mass murder and rape, ethnic cleansing by forcible expulsion and terror, and deliberate starvation and exposure to disease. And there is a growing acceptance that while sovereign Governments have the primary responsibility to protect their own citizens from such catastrophes, when they are unable or unwilling to do so that responsibility should be taken up by the wider international community â€” with it spanning a continuum involving prevention, response to violence, if necessary, and rebuilding shattered societies.\nWe endorse the emerging norm that there is a collective international responsibility to protect, exercisable by the Security Council authorizing military intervention as a last resort, in the event of genocide and other large scale killing, ethnic cleansing or serious violations of international humanitarian law which sovereign Governments have proved powerless or unwilling to prevent.” (Report: A More Secure World: Our Shared Responsibility, paras 201 and 203)\nIn September 2005, R2P was once again enlivened, this time with the full support of the international community. At the 60th session of the U.N. General Assembly gathering, 191 heads of state and government representatives unanimously endorsed a resolution supporting the Responsibility to Protect doctrine. This resolution laid the foundations for a new global moral compact between every State and every population on earth. As adopted, atrocity crimes â€“ genocide, crimes against humanity (including ethnic cleansing) and war crimes – were considered a universal concern and therefore were responsibility of the international community.\nDuring the 2005 General Assembly World Summit, world leaders stated:\nâ€œEach and individual State has the responsibility to protect its populations from genocide, war crimes, ethnic cleansing and crimes against humanity. This responsibility entails the prevention of such crimes, including their incitement, through appropriate and necessary means. We accept that responsibility and will act in accordance with it. The international community should, as appropriate, encourage and help States to exercise this responsibility and support the United Nations in establishing an early warning capability.\nThe international community, through the United Nations, also has the responsibility to use appropriate diplomatic, humanitarian and other peaceful means, in accordance with Chapters VI and VIII of the Charter, to help protect populations from genocide, war crimes, ethnic cleansing and crimes against humanity. In this context, we are prepared to take collective action, in a timely and decisive manner, through the Security Council, in accordance with the Charter, including Chapter VII, on a case-by-case basis and in cooperation with relevant regional organizations as appropriate, should peaceful means be inadequate and national authorities are manifestly failing to protect their populations from genocide, war crimes, ethnic cleansing and crimes against humanity. We stress the need for the General Assembly to continue consideration of the responsibility to protect populations from genocide, war crimes, ethnic cleansing and crimes against humanity and its implications, bearing in mind the principles of the Charter and international law. We also intend to commit ourselves, as necessary and appropriate, to helping States build capacity to protect their populations from genocide, war crimes, ethnic cleansing and crimes against humanity and to assisting those which are under stress before crises and conflicts break outâ€� (A/RES/60/1, para. 138 and 139).\nFollowing on the momentum of the World Summit endorsement, the U.N. Security Council included in Resolution 1674 (28 April 2006) on the protection of civilians in armed conflict a re-affirmation of R2P. Then in Resolution 1706 (31 August 2006), which sought to provide a peacekeeping mission to Darfur, the Council once again recognized the importance of R2P and re-asserted its commitment to the principles of this doctrine.', 'In 2005 the United Nations General Assembly agreed to adopt the principles of responsibility to Protect (R2P).\nBy doing so the individual governments promised to do their utmost to protect and prevent their populations from atrocities that can be categorized as genocide, war crimes, ethnic cleansing or crimes against humanity. These are the broad pillars of focus, when talking of the R2P concept and when and how to act in case of atrocities being committed. More details about what was agreed can be found in the 2005 Summit Outcome Document.\nIn cases where there is a need for the international community to mobilize collective action and act in accordance with the R2P, it is a criterion that any involvement in a crisis does not take place without a UN mandate to confer legitimacy. That is an important element in the concept of R2P, in part to prevent states from exploiting the R2P concept as a convenient political tool that could be used as justification for an intervention.\nThe concept of R2P is thus not intended to be activated every time a state seemingly fails to protect their own citizens, and should be considered by the international community only when one finds evidence of acute violence against populations. The violence should furthermore, as mentioned, be categorized as genocide, war crimes, ethnic cleansing or crimes against humanity.\nIn 2005 the UN member states agreed that the international community must activate R2P and make use of peaceful means to protect populations from massive crimes. The use of peaceful means is at any times the preferred and the intended way to use R2P. Edward Luck, special advisor to Secretary General Ban Ki-moon on R2P, said recently to the UN News Centre that the examples when R2P have been used without force are underreported comparing to when force have been applied, but nevertheless outnumbers them: “If you actually look at the last several years, we’ve invoked the responsibility to protect, at least on the (UN) Secretariat side eight or nine times. Only in one of those cases, with Libya, was it tied to the use of sanctions or military force.”\nA precise manual that explains when to activate R2P in practice does not exist, as governments agreed to discuss activation of R2P on a case-by-case basis. In a seminar in Brussels recently Edward Luck emphasized this point: “R2P is not specific. It is a principle that does not dictate any specific actions or tactics, and it should fit each individual case. Because R2P is about options, different alternatives to solutions, we have to be open to new ideas and take every case individually.”\nUse of force\nThe recent conflict in Libya, where Libyan rebels seek to overthrow Colonel Muammar Al-Qadhafi, stands out as the most striking and dramatic use of the R2P concept, with direct reference to UN-resolution 1973 accepting use of military force.\nThe use of military force in accordance to the Responsibility to Protect (R2P) concept is not in any way the main tool of the doctrine – force should only to be used as a last resort, if use of peaceful means prove inadequate in order to prevent atrocities from taking place. Of other tools to be considered prior to considering the use of force, include capacity building, mediation, and sanctions directed towards a country that fails to protect its population.\nFor more information on R2P:\nThe Brussels based United Nations Regional Information Centre for Western Europe - UNRIC provides information on UN activities to the countries of the region. It also provides liaison with institutions of the European Union in the field of information. Its outreach activities extend to all segments of society and joint campaigns, projects and events are organized with partners including the EU, governments, the media, NGOs, schools and local authorities.\nUnited Nations Regional Information Centre for Western Europe (UNRIC Brussels)\nResidence Palace, Rue de la Loi/Wetstraat 155, Block C2,7th and 8th floor, Brussels 1040, Belgium\nTel.: +32 2 788 8484 / Fax: 32 2 788 8485']	['<urn:uuid:72e08129-9c76-48e7-9407-b610fd836d4c>', '<urn:uuid:787e4247-c63d-46ad-9af3-0ad3314c142a>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	7	82	1616
18	disaster management plan essential services prioritization guide importance explain	A business continuity plan outlines the most vital processes within a company and provides a plan for those processes to remain operational during emergencies, preventing critical business functions from stopping suddenly. When a crisis hits, the BCP helps pare back business functions to decide what is essential and what can be shelved, including redeploying talent from less crucial projects to more pressing problems.	['Updating your business continuity plan and disruption-proofing your business\nBy ATB FInancial 10 August 2020 6 min read\nAlbertans are no strangers to crises, from floods to fires to oil-and-gas recessions, and now global pandemics. But how business owners in the province react to catastrophic events can make the difference between just getting by and succeeding with a resilient roadmap ahead.\nThat’s where the business continuity plan [BCP] comes in. Essentially, this is a document that outlines the most vital processes within your company and lays out a plan for those processes to remain operational in the event of an emergency so that critical business functions don’t come to a sudden stop.\nIt could be an evolving document, based on how your company has fared in a certain crisis and the level of emergency you’re facing with this new disaster. Every response may have to be tailored to each event, but the core principles often remain the same.\nBryan Rabik, managing director, Alberta North Real Estate at ATB, said in a recent post on the topic: “[Your BCP] should be scaled to reflect the crisis at hand, so a BCP for a branch of a company will look different than one that focuses on the impact of an emergency affecting that firm enterprise-wide.”\nTo get deeper insight into disruption-proofing your business, we spoke with Sandi MacGregor, Entrepreneur Strategist from the Calgary Entrepreneur Center. MacGregor explores what makes a well prepared and evolving BCP, why it should be accessible to everyone, and the questions you may want to ask yourself to create an action plan that will strengthen your company’s responses to any crisis.\nSavvy business leaders don’t rely on a static business continuity plan\nYour BCP acts as a go-to anchor in understanding how decisions today will impact the business in the future. It can help you prioritize the right decisions and actions you need to take when a sudden emergency strikes your market. “A BCP lets you continue operations so you don’t have to shut down everything when the unexpected happens,” MacGregor explains.\nWhen you first compile a BCP, you’ll want to detail how to manage the many areas of your business, from staffing to IT protocols to prioritizing essential services that drive the business forward. But it shouldn’t be static and instead evolve as new crises emerge.\n“Ensuring it’s always in line with company objectives, the BCP could be updated to include, say, how to stay in touch with staff if work-from-home policies are in place, which we are seeing during the current pandemic,” MacGregor says. “Or you want to constantly evolve the budget for something like buying new equipment if that becomes important during a crisis.”\nYour BCP should live in the online cloud, as opposed to becoming rooted in an obscure place on the company site, available to a select few. Make it accessible to everyone, MacGregor advises, so key staff can help update it by offering their input when appropriate.\nBy making the BCP available to staff, you’re also sending a signal to the company that transparency is a key pillar of running your business, which can go a long way in bolstering positive sentiments about how the company is managed.\nNot business as usual so what are you going to do?\nWhen a crisis hits, you have to realize you are now wading into an odd new normal that upends the usual day-to-day processes. So it’s best to lay out in your BCP how your team will pare back your business functions to decide what is essential and what can be shelved.\nLook at your pool of talent and decide what you may not need and redeploy their skills to more pressing problems. For example, IT architects or engineers working on new projects may have to be shifted to work on throwing your company a life-preserver in order for staff to access the right software and hardware if they can’t work in the office as they normally do.\nTechnology can play an important role when you want to stay relevant during a downturn. ATB’s Rabik offers an example of an Albertan company that didn’t take the COVID-19 pandemic lying down: “As their competitors were shutting down, this company had the technology to service their customers. They had the technical ability to continue a process their competitors didn’t have.”\nTo cite another example out of province, Anderson Group, a Quebec-based agriculture equipment manufacturer, shared with Business Development Canada how “having a strong continuity plan helped save the business after 75 per cent of its factory was destroyed in a fire.”\nCrisis management requires documentation, asking the hard questions\nTo effectively evolve your BCP from disruption to disruption, you’ll want to document everything, MacGregor says. Albertan businesses that put down on paper (or cloud doc) how they survived and maybe even thrived through a disaster like the floods from several years ago can be better positioned to take on other natural disasters.\n“And write down how your business is managing through this pandemic and how you kept everything going,” MacGregor notes. “Maybe you went with different suppliers or used different shipping companies this time, and that can help you down the road with other similar emergencies.”\nIt may be overwhelming to you when you consider the many priorities you have to assess when updating a BCP. Thankfully, it’s a major topic of discussion among Canada’s business community. The Canadian Chamber of Commerce has offered several questions you should ask yourself when you face a crisis, all of which can be answered in your evolving BCP:\n- How and when are you communicating to internal and external stakeholders and managing the flow of information?\n- What is your plan for recovery?\n- How are you planning for significant staff absences?\n- If you were forced to close your doors for two weeks or more, do you have access to a line of credit that will cover ongoing expenses until you can reopen and your cash flow resumes?\n- What steps can you be taking to protect the health and safety of your staff and visitors to your workplace?\n- What mental health services could be provided during a crisis for staff who may vulnerable to such sudden departures from normal working schedules?\nFinally, you don’t have to undertake all of this planning alone. You may want to reach out to strategic partners to ask for advice, learn from their experiences, and perhaps request for relief (such as from clients/vendors). Connecting with other industry leaders in your field is another way to get some feedback on how the sector has been impacted as a whole. Finally, consider discussing your BCP’s future with your financial advisors so you have a full picture of what goes into budgeting, cash flow forecasting and available government relief programs.\nAbove all else, do your best to stay focused in order to help mitigate any anxiety you may have around your business falling apart due to the crisis at hand. Staying calm may be easier said than done, but if you have a thorough business continuity plan that evolves with each disruption you face, your business will be in a better position to take on whatever comes your way.\nIf you’re looking for a deep dive on everything you need to know around how to grow your business and make it more resilient, our ATB X Accelerator program might be just the place for you. Alternatively, feel free to reach out to one of our entrepreneur strategists to explore where you are with your business, where you want to be, and how to get there!']	['<urn:uuid:4f8cc01a-0c52-4f70-ba0e-d157f019bb26>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	9	63	1271
19	what temperature humidity conditions required dry natural lacquer varnish	To dry natural lacquer varnish, it requires a steady temperature of 20-25ºC and a humidity factor of 60 to 80 percent. This process occurs in a wooden chamber called a muro.	['The organic beauty of lacquerware\nThe lacquerware tradition in Yamanaka Onsen began some 400 years ago, when a community of woodworkers settled in this hot-spring area—drawn by the curative waters, perhaps, but also by the dense forests available to them.\nA deep bowl made of tochi (Japanese horse chestnut) by Takehito Nakajima.\nThese woodturners used lathes to cut, sand, face, and otherwise shape wood into bowls and trays that they then lacquered themselves. Today close to 60 turners carry on their craft here, along with the lacquer artisans and traders who settled in the area later.\nIt’s not uncommon for those encountering wooden lacquerware for the first time to be taken aback by the material’s light feel and smooth finish. The combination strikes some observers as less like high-quality wood and more like plastic.\nMarks on the trunk of a Japanese lacquer tree show where it has been tapped.\nSo what, exactly, is the appeal of lacquerware?\nThe main raw material of lacquer is sap collected from trees of the sumac family. The key component of varnish gathered in Japan is urushiol, a high-molecular compound, allergenic oil, and skin irritant.\nAs if handling this substance alone weren’t troublesome enough, in order to dry the varnish its naturally occurring enzyme laccase must undergo oxidized polycondensation, a process requiring a steady temperature of 20–25ºC and a humidity factor of 60 to 80 percent. A wooden chamber known as a muro is used to this end.\nOnce it dries, however, natural lacquer is incredibly tough. It is resistant to water, heat, salt, alkali, and acid, and impervious even to nitrohydrochloric acid, a substance that can melt metal. A 2,000-year-old piece submerged in muddy water was found with its shine still in place. What’s more, a lacquer finish is both germ-resistant and antiseptic.\nFlowering white clover is rendered in brilliant gold makie by Yamazaki.\nLacquerware is also ecological. The bark of a varnish tree is cut and its sap collected drop by drop. Using traditional methods, only about 200 grams of sap can be recovered from a tree that has been growing for 15 years. Imagine! A raw material that will last more than 1,000 years can be recovered in 15.\nLacquer imparts a translucent effect to the surface it coats. When people first found the hardened sap in the forest, their instinct was to use it as an adhesive. It was later adapted as varnish once its beauty and protective properties were discovered.\nA vermilion-lacquered comb from the prehistoric Jomon era, discovered in Fukui prefecture, has shown that lacquering techniques, as well as our quest for artistic expression, were well in place as early as 6,000 years ago.\nWoodturning and painting techniques\nThe artisans Takehito Nakajima and Mushu Yamazaki, heirs to Yamanaka’s legacy of woodworking and lacquerware, work at the forefront of these crafts today.\nNakajima is one of the top woodturners in Japan. He works with entirely with tools he has made himself, and does lacquering as well.\nIn Yamanaka, no clamps or other metal fittings are used to hold wood in the electric lathe—the piece is simply set into a wooden frame that protects it from scratches or other damage. This makes it easy to change the turning direction at any moment so that an entire bowl can be shaped in just a few minutes. Once the lathing is finished, the piece can be quickly removed and the next one set in place. Nakajima reports that a turner from overseas took one look at his setup and remarked, “That’s crazy! Doesn’t it scare you to work that way?” In fact, he explains, it’s a clever way to bring out the inherent suppleness of the wood.\nAn elegant bowl emerges beneath his hands as Takehito Nakajima works a piece of wood with tools he has made himself. Turners in Yamanaka typically use wood that is cut crosswise against the grain, which results in more durable wares.\nEverything Nakajima makes is magnificent, but his sake cups have special appeal. As their design is carefully considered right down to the texture and touch of the piece on one’s lips, these works of art are a sublime way to savor sake. Nakajima’s mastery is evident in the feather-lightness of his vessels as well as their ultrathin walls. His recent works feature translucent lacquer with the gloss of marble, a new form of expression.\nYamazaki, whose works are sought after by collectors the world over, is, equally, one of the top lacquer painters in the country.\nIn the decorative art form of makie lacquer painting, designs are built up three-dimensionally, often sprinkled with gold or silver powder. Beyond artistic talent, patience and an abiding affection for each piece are requisite. As only the slightest bit of lacquer can be applied before it is set to dry and the next coat begun, the process is long and labor-intensive.\nFountain pens by makie artist Mushu Yamazaki. Multiple coats of lacquer were polished down to portray karashishi lions, shells, and other good-luck symbols, turning a writing instrument into a luxury article of understated elegance.\nYamazaki takes on restoration projects, too, preserving masterpieces and other antiques in exquisite form for future generations. His painstaking care, extending even to parts that are hidden, can at times only be confirmed with a magnifying glass. It is no exaggeration to place his trailblazing work in makie in league with that of the great Edo-period master Shibata Zeshin (1807–1891).\nThe flounder-shaped plate “Karei” by Yamazaki is an intricately detailed modern piece featuring not only makie but also the kanshitsu technique, in which multiple layers of hemp cloth are lacquered over a mold.']	['<urn:uuid:e3f59341-d36b-4de6-8cf8-2a3003040a40>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	9	31	938
20	How do computers process and compare text strings?	Computers can process text strings using specialized instructions like SSE 4.2, which includes operations for comparing and searching strings. These operations include comparing if bytes are equal, checking if characters belong to a set, matching character ranges, and searching for substrings. Additionally, C++ provides library functions through the cstring header specifically designed for string processing, allowing operations on C-style strings.	"['The new instructions\nSSE 4.2 introduces four instructions (PcmpEstrI, PcmpEstrM, PcmpIstrI, and PcmpIstrM) that can be used to speed up text processing code (including strcmp, memcmp, strstr, and strspn functions).\nIntel had published the description for new instruction formats, but no sample code nor high-level guidelines. This article tries to provide them.\nMovDqU xmm0, dqword[str1] PcmpIstrI xmm0, dqword[str2], imm\nPcmpIstrI is one of the new string-handling instructions comparing their operands. The first operand is always an SSE register (typically loaded with MovDqU). The second operand can be a memory location. Note the immediate operand, which consists of several bit fields controlling operation modes.\nThe heart of a string-processing instruction is the aggregation operation (immediate bits [3:2]).\nEqual each (imm[3:2] = 10). This operation compares two strings (think of strcmp or memcmp). The result of comparison is a bit mask (1 if the corresponding bytes are equal, 0 if not equal). For example:\noperand1 = ""UseFlatAssembler"" operand2 = ""UsingAnAssembler"" IntRes1 = 1100000111111111\nEqual any (imm[3:2] = 00). The first operand is a character set, the second is a string (think of strspn or strcspn). The bit mask includes 1 if the character belongs to a set, 0 if not:\noperand2 = ""You Drive Me Mad"", operand1 = ""aeiouy"" IntRes1 = 0110001010010010\nRanges (imm[3:2] = 01). The first operand consists of ranges, for example, ""azAZ"" means ""all characters from a to z and all characters from A to Z"":\noperand2 = ""I\'m here because"", operand1 = ""azAZ"" IntRes1 = 1010111101111111\nEqual ordered (imm[3:2] = 11). Substring search (strstr). The first operand contains a string to search for, the second is a string to search in. The bit mask includes 1 if the substring is found at the corresponding position:\noperand2 = ""WhenWeWillBeWed!"", operand1 = ""We"" IntRes1 = 000010000000100\nAfter computing the aggregation function, IntRes1 can be complemented, expanded into byte mask or shrinked into index. The result is written into xmm0 or ECX registers. Intel manual explains these details well, so there is no need to repeate them here.\nOther features of SSE 4.2\n- The strings do not need to be aligned.\n- The processor properly handles end-of-the-string case for zero-terminated strings and Pascal-style strings.\n- You can use the instructions with Unicode characters, signed or unsigned bytes.\n- Four aggregation operations can be used to implement a wide range of string-processing functions.\nThe following strcmp and strlen functions were written and published when the processors with SSE 4.2 support were not available yet. Later they were tested on real hardware and found to be correct.\n; compile with FASM ; Immediate byte constants EQUAL_ANY = 0000b RANGES = 0100b EQUAL_EACH = 1000b EQUAL_ORDERED = 1100b NEGATIVE_POLARITY = 010000b BYTE_MASK = 1000000b ; ==== strcmp ==== strcmp_sse42: ; Using __fastcall convention, ecx = string1, edx = string2 mov eax, ecx sub eax, edx ; eax = ecx - edx sub edx, 16 STRCMP_LOOP: add edx, 16 MovDqU xmm0, dqword[edx] ; find the first *different* bytes, hence negative polarity PcmpIstrI xmm0, dqword[edx + eax], EQUAL_EACH + NEGATIVE_POLARITY ja STRCMP_LOOP jc STRCMP_DIFF ; the strings are equal xor eax, eax ret STRCMP_DIFF: ; subtract the first different bytes add eax, edx movzx eax, byte[eax + ecx] movzx edx, byte[edx + ecx] sub eax, edx ret ; ==== strlen ==== strlen_sse42: ; ecx = string mov eax, -16 mov edx, ecx pxor xmm0, xmm0 STRLEN_LOOP: add eax, 16 PcmpIstrI xmm0, dqword[edx + eax], EQUAL_EACH jnz STRLEN_LOOP add eax, ecx ret ; ==== strstr ==== strstr_sse42: ; ecx = haystack, edx = needle push esi push edi MovDqU xmm2, dqword[edx] ; load the first 16 bytes of neddle Pxor xmm3, xmm3 lea eax, [ecx - 16] ; find the first possible match of 16-byte fragment in haystack STRSTR_MAIN_LOOP: add eax, 16 PcmpIstrI xmm2, dqword[eax], EQUAL_ORDERED ja STRSTR_MAIN_LOOP jnc STRSTR_NOT_FOUND add eax, ecx ; save the possible match start mov edi, edx mov esi, eax sub edi, esi sub esi, 16 ; compare the strings @@: add esi, 16 MovDqU xmm1, dqword[esi + edi] ; mask out invalid bytes in the haystack PcmpIstrM xmm3, xmm1, EQUAL_EACH + NEGATIVE_POLARITY + BYTE_MASK MovDqU xmm4, dqword[esi] PAnd xmm4, xmm0 PcmpIstrI xmm1, xmm4, EQUAL_EACH + NEGATIVE_POLARITY ja @B jnc STRSTR_FOUND ; continue searching from the next byte sub eax, 15 jmp STRSTR_MAIN_LOOP STRSTR_NOT_FOUND: xor eax, eax STRSTR_FOUND: pop edi pop esi ret\n- Intel Software Developer Manuals.\n- comp.arch discussion about the new string-processing instructions.\n- Optimizing strlen on processors without SSE4 support.\n- A review of Intel Nehalem processor, which includes support for the text processing instructions.\nTypo in Intel manual: on figure 5-1, ""imm8[6:5]"" near Optional boolean negation should be ""imm8[5:4]"".', ""Get To Know All About Library Functions In C++ With Examples.\nLibrary functions which are also called as “built-in” functions are the functions that are already available and implemented in C++.\nWe can directly call these functions in our program as per our requirements. Library functions in C++ are declared and defined in special files called “Header Files” which we can reference in our C++ programs using the “include” directive.\n=> Visit Here For The Complete C++ Course From Experts.\nWhat You Will Learn:\nFor Example, to include all the built-in functions related to math, we should include <cmath> header as follows:\nSome of the standard library header files that are used in C++ are tabularized as below. These headers replace their respective counterparts with “.h” extension.\nFor Example, <iostream> replaces <iostream.h> header file.\nThe Header files are briefed along with their description below.\n|iostream||This header contains the prototype for standard input and output functions used in C++ like cin, cout, etc.|\n|cmath||This is the header containing various math library functions.|\n|iomanip||This header contains stream manipulator functions that allow us to format the stream of data.|\n|cstdlib||The header cstdlib contains various functions related to conversion between text and numbers, memory allocation, random numbers, and other utility functions.|\n|ctime||ctime contains function prototypes related to date and time manipulations in C++.|\n|cctype||This header includes function prototypes that test the type of characters (digit, punctuation, etc.). It also has prototypes that are used to convert from uppercase to lowercase and another way around.|\n|cstring||cstring header includes function prototypes for C-style string-processing functions.|\n|cstdio||This header contains function prototypes for the C-style standard input/output library functions which we included initially in stdio.h|\n|fstream||Function prototypes for functions that perform input/output from/to files on disk are included in fstream header.|\n|climits||climits header has the integral size limits of the system.|\n|cassert||cassert header contains macros and variables for adding diagnostics that help us in program debugging.|\n|cfloat||This header file contains the size limits for floating-point numbers on the system.|\n|string||The header string defines the class string of the C++ Standard Library.|\n|list, vector, stack, queue, deque, map, set, bitset||All these headers are used for Standard Template Library (STL) implementation. Each of these headers contains the respective class definition and function prototypes.|\n|typeinfo||This header contains various classes for Runtime Type Identification (RTTI).|\n|exception, stdexcept||All the classes and functions used for exception handling in C++ are included in these two headers.|\n|memory||This header is used by the C++ standard library to allocate memory.|\n|sstream||Functions that read input from strings in memory and output to strings in memory require functions prototypes from sstream header to implement the functionality.|\n|functional||Used by C++ standard library algorithms.|\n|iterator||Function prototypes and classes in this header are used by Standard Template Library to traverse through or iterate through the data inside containers.|\n|algorithm||Methods that act on STL container data are included in this header algorithm|\n|locale||To process data in the original natural form for different languages or locales (currencies, character presentation, etc.), the locale header definitions are used.|\n|limits||This header defines the data type limit for Numbers on each platform.|\n|utility||This header contains utility functions and classes used by the Standard C++ library.|\nWe have already used most of these headers throughout our tutorial so far. Notable is <iostream>, <string>, <ctime> headers that we have used from time to time.\nIn our STL tutorials, we will be making use of all the container headers and <algorithm> as well as <iterator> header. Similarly, when we learn file I/O and exception handling, we will be using respective headers.\nIn this tutorial, we will mostly deal with <cmath> and <cctype> headers and discuss the various function prototypes that they support. The function prototypes from these headers are extensively used in C++ programming.\nThis header contains various function prototypes related to mathematical functions. Some of the prototypes that are used extensively are listed here.\n|sqrt(x)||Accepts any non-negative numeric parameter x and returns the square root of this number x|\n|pow(base,exponent)||Raises the ‘base’ value to the power specified by the exponent. Returns base^exponent.|\n|exp(x)||Takes any number (positive, negative or zero) as a parameter and returns exponential (Euler’s number) e raised to the given parameter|\n|fabs(x)||Returns absolute value of an argument.|\n|log(x)||Returns the natural logarithm (to the base e) of value x|\n|log 10(x)||Return the logarithm (to the base 10) of value x|\n|sin(x)||Returns sine of the angle x (in radians)|\n|cos(x)||Returns cosine of angle x (in radians)|\n|tan(x)||Returns tangent of angle x (in radians)|\n|asin(x)||Returns inverse sine (in radians) of number x|\n|acos(x)||Returns inverse cosine (in radians) of number x|\n|atan(x)||Returns inverse tangent (in radians) of number x|\nThis header contains function prototypes that are mainly used for converting the character to upper/lower case or to check if a character is a digit etc.\nFunction prototypes included in <cctype> header are listed as below:\n|toupper(ch)||Takes in character ‘ch’ as an argument and returns the uppercase equivalent of ch if it's present otherwise returns ch.|\n|tolower(ch)||Takes in character ‘ch’ as an argument and returns the lowercase equivalent of ch if it's present otherwise returns ch.|\n|isalpha(ch)||Returns non-zero if ch is alphabet otherwise 0.|\n|isalnum(ch)||Returns non-zero if ch is alphanumeric (alphabet or number) otherwise 0.|\n|isupper(ch)||Returns non-zero value if ch is uppercase otherwise 0.|\n|isdigit(ch)||Returns non-zero value if ch is a number otherwise 0.|\n|islower()||Returns non-zero value if ch is lowercase otherwise 0.|\nWe also have another header <stdlib> that includes various useful library functions that are used extensively in C++ programming.\nWe have listed some of the popular functions in <stdlib> below:\n|abs(x)||Returns absolute value of an integral argument x|\n|atof(const char* str)||Converts string to double; returns double|\n|atoi(const char* str)||Converts string to int; returns an int|\n|atol(const char* str)||Converts string to long int; returns a long int|\n|atoll(const char* str)||Converts string to long long int; returns a long long int|\n|strtod||Converts string to double|\n|strtol||Converts string to long int|\n|strtoul||Converts string to unsigned long integer|\n|strtof||Converts string to float|\n|strtold||Converts string to long double|\n|strtoull||Converts string to unsigned long long integer|\n|strtoll||Converts string to long long integer|\n|srand(int seed)||This is a pseudo-random generator that is initialized to argument ‘seed’|\n|qsort||Sorts elements of the array in ascending order(internally uses quick sort method)|\n|abort||Aborts the process resulting in abnormal program termination|\n|atexit||Has function passed as an argument which is executed resulting in normal program termination.|\n|malloc(size_t size)||Used to allocate memory specified by size and return a pointer to it|\n|calloc ( size_t num, size_t size)||Allocates memory of (num*size) bytes with all bits initialized to zero|\n|free(void* ptr)||Deallocates memory block allocated by malloc, calloc or realloc function call.|\n|realloc ( void* ptr, size_t size )||Resizes the memory block pointed to by ptr that was initially allocated using malloc or calloc function call.|\n|quick_exit||Normal termination of the process after returning control to the host environment.|\n|system||Invokes command processor to execute system command passed as an argument|\n|getenv||Retrieves the value of environment string passed as an argument to the function|\n|wctomb||Convert the wide character to a multibyte sequence|\n|wcstombs||Convert wide character string to multibyte string|\nIn this tutorial, we have gone through some of the header files supported by the C++ standard library.\nWe also discussed some popular library functions that are used by programmers. This list of functions is not exhaustive as each header of the C++ standard library contains too many functions for the benefit of programmers.\nIn our upcoming C++ tutorials, we will come across more library functions.\n=> Visit Here For The Exclusive C++ Training Tutorial Series.""]"	['<urn:uuid:6251f4ac-79e5-4e04-b725-da3647a3693b>', '<urn:uuid:4c593dd1-0d9f-4b12-a674-aa9c26c1dd5a>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T18:38:36.516973	8	60	2018
21	What coordinate system is used for UAV navigation and how does roll affect aircraft stability?	UAVs use the North-East-Down (NED) coordinate system, with the body frame attached to the center of mass. During an aileron roll, the aircraft experiences adverse yaw, causing it to deviate from its path, and loses most lift when the wings are vertical, with only minimal lift generated by the fuselage.	"['Guidance model for fixed-wing UAVs\nfixedwing object represents a reduced-order guidance model for\nan unmanned aerial vehicle (UAV). The model approximates the behavior of a closed-loop system\nconsisting of an autopilot controller and a fixed-wing kinematic model for 3-D\nThis object requires you to install the UAV Library for Robotics System\nToolbox™. To install add-ons, use\nroboticsAddons and select the desired add-on.\nFor multirotor UAVs, see\nmodel = fixedwing creates a fixed-wing motion model with\ndouble precision values for inputs, outputs, and configuration parameters\nof the guidance model.\nmodel = fixedwing(DataType) specifies the data type precision\nDataType property) for the inputs, outputs, and configurations\nparameters of the guidance model.\nName— Name of UAV\n""Unnamed""(default) | string scalar\nName of the UAV, used to differentiate it from other models in the workspace, specified as a string scalar.\nConfiguration— UAV controller configuration\nUAV controller configuration, specified as a structure of parameters. Specify these parameters to tune the internal control behavior of the UAV. Specify the proportional (P) and derivative (D) gains for the dynamic model and other UAV parameters. The structure for fixed-wing UAVs contains these fields with defaults listed:\n[min max] angle in radians)\nModelType— UAV guidance model type\nThis property is read-only.\nUAV guidance model type, specified as\nDataType— Input and output numeric data types\nInput and output numeric data types, specified as either\n\'single\'. Choose the data type based on possible software or\nThis example shows how to use the\nfixedwing guidance model to simulate the change in state of a UAV due to a command input.\nNote: To use UAV algorithms, you must install the UAV Library for Robotics System Toolbox®. To install, use\nCreate the fixed-wing guidance model.\nmodel = fixedwing;\nSet the air speed of the vehicle by modifying the structure from the\ns = state(model); s(4) = 5; % 10 m/s\nSpecify a control command,\nu, that maintains the air speed and gives a roll angle of\nu = control(model); u.RollAngle = pi/12; u.AirSpeed = 5;\nCreate a default environment without wind.\ne = environment(model);\nCompute the time derivative of the state given the current state, control command, and environment.\nsdot = derivative(model,s,u,e);\nSimulate the UAV state using\node45 integration. The\ny field outputs the fixed-wing UAV states based on this simulation.\nsimOut = ode45(@(~,x)derivative(model,x,u,e), [0 50], s); size(simOut.y)\nans = 1×2 8 904\nPlot the change in roll angle based on the simulation output. The roll angle is the 7th row of the\nYou can also plot the fixed-wing trajectory using\nplotTransforms. Create the translation and rotation vectors from the simulated state. Downsample (every 30th element) and transpose the\nsimOut elements, and convert the Euler angles to quaternions. Specify the mesh as the\nfixedwing.stl file and the positive Z-direction as\n""down"". The displayed view shows the UAV making a constant turn based on the constant roll angle.\ndownsample = 1:30:size(simOut.y,2); translations = simOut.y(1:3,downsample)\'; % xyz-position rotations = eul2quat([simOut.y(5,downsample)\',simOut.y(6,downsample)\',simOut.y(7,downsample)\']); % ZYX Euler plotTransforms(translations,rotations,... \'MeshFilePath\',\'fixedwing.stl\',\'InertialZDirection\',""down"") hold on plot3(simOut.y(1,:),-simOut.y(2,:),simOut.y(3,:),\'--b\') % full path xlim([-10.0 10.0]) ylim([-20.0 5.0]) zlim([-0.5 4.00]) view([-45 90]) hold off\nThe UAV Library for Robotics System Toolbox uses the North-East-Down (NED) coordinate system convention, which is also sometimes called the local tangent plane (LTP). The UAV position vector consists of three numbers for position along the northern-axis, eastern-axis, and vertical position. The down element complies with the right-hand rule and results in negative values for altitude gain.\nThe ground plane, or earth frame (NE plane, D = 0), is assumed to be an inertial plane that is flat based on the operation region for small UAV control. The earth frame coordinates are [xe,ye,ze]. The body frame of the UAV is attached to the center of mass with coordinates [xb,yb,zb]. xb is the preferred forward direction of the UAV, and zb is perpendicular to the plane that points downwards when the UAV travels during perfect horizontal flight.\nThe orientation of the UAV (body frame) is specified in ZYX Euler angles. To convert from the earth frame to the body frame, we first rotate about the ze-axis by the yaw angle, ψ. Then, rotate about the intermediate y-axis by the pitch angle, ϕ. Then, rotate about the intermediate x-axis by the roll angle, ϴ.\nThe angular velocity of the UAV is represented by [p,q,r] with respect to the body axes, [xb,yb,zb].\nFor fixed-wing UAVs, the following equations are used to define the\nguidance model of the UAV. Use the\nfunction to calculate the time-derivative of the UAV state using these governing equations.\nSpecify the inputs using the\nThe UAV position in the earth frame is [xe, ye, h] with orientation as heading angle, flight path angle, and roll angle, [χ, γ, ϕ] in radians.\nThe model assumes that the UAV is flying under a coordinated-turn condition, with zero side-slip. The autopilot controls airspeed, altitude, and heading angle. The corresponding equations of motion are:\nVa and Vg denote the UAV air and ground speeds.\nThe wind speed is specified as\nfor the north, east, and down directions. To generate the structure for these inputs, use\nk* are controller gains. To specify these gains,\nConfiguration property of the\nFrom these governing equations, the model gives the following variables:\nThese variables match the output of the\n Randal W. Beard and Timothy W. McLain. ""Chapter 9."" Small Unmanned Aircraft Theory and Practice, NJ: Princeton University Press, 2012.', 'The aileron roll is an aerobatic maneuver in which an aircraft does a full 360° revolution about its longitudinal axis. When executed properly, there is no appreciable change in altitude and the aircraft exits the maneuver on the same heading as it entered. This is commonly one of the first maneuvers taught in basic aerobatics courses. The aileron roll is commonly confused with a barrel roll.\nThe aileron roll is commonly executed through the application of full aileron in one direction. In some lower powered general aviation and aerobatic training aircraft, prior to applying aileron input, the pilot must begin the maneuver by trading altitude for airspeed (i.e. diving). This helps achieve enough airspeed to complete the roll without losing rudder and aileron control. The minimum airspeed needed depends on the aircraft\'s design, but is generally about 120 to 200 knots. Because full aileron is applied, structural limitations prevent many aircraft from performing the maneuver at very high speeds.\nStarting from level flight, the pilot pitches the aircraft up about 10 to 30 degrees above the horizon, into a brief climb. The purpose of pitch-up is twofold. This causes an increase in altitude which minimizes altitude loss and airspeed gain. As the aircraft begins to roll, it starts to lose lift. When the wings are vertical, the only lift generated is a small amount from the fuselage, and the aircraft will begin to lose altitude. The brief climb compensates for the loss, allowing the aircraft to complete the roll at the same altitude the maneuver began. When the aircraft is completely inverted, the increased pitch results in greater angle of attack, enabling the inverted wing to generate lift.\nAfter the initial pitch-up, the pilot places the elevators in the neutral position. Failure to do this will cause the aircraft to continue pitching up during the upright part of the maneuver, and downward in the inverted part, resulting in something resembling a barrel roll. The pilot then applies full aileron, accomplished by moving the stick to either the right or left. As the aircraft rolls about its longitudinal axis, the nose will begin to drop. Upon completing the roll, the nose will usually be 10 to 30 degrees below the horizon, so the pilot will need to pitch-up to return to level flight.\nAn aileron roll is an unbalanced maneuver. As the roll begins, the aircraft will have a tendency to yaw away from the angle of bank, referred to as ""adverse yaw."" The pilot will usually need to apply the rudder in the direction of the bank to keep the aircraft balanced. An aircraft performing an aileron roll will actually fly along a slightly helical path, and a very light, positive g force will be maintained.\nAn aileron roll is similar to the slow roll, and the two maneuvers are often confused with each other. However, unlike a slow roll, an aileron roll is performed at the maximum roll rate, and is uncontrolled in the pitch axis. It consists of a constant attitude change during the maneuver; from the initial pitch-up to the plane following a slight corkscrew path as the nose drops, followed by the final pitch-up. If the pilot picks a reference point on the horizon, directly ahead of the plane, the nose will actually appear to trace a shape similar to the letter ""D"" above this reference point.\nThe aileron roll is commonly used in air shows and aerial combat training. The use of the pure aileron roll in air combat is contentious, but many common maneuvers bear heavy dependence on the aileron roll. Examples of this are the Immelmann turn, barrel roll, and Split S.\nAn aileron roll carried out by a pilot as a sign of victory or celebration is known as a victory roll.\nTest pilots commonly employ the aileron roll to evaluate an aircraft\'s turning characteristics (e.g. time to turn).\n- Aerobatic Figures\n- Geza Szurovy, Mike Goulian (1994). Basic Aerobatics. Tab Books. pp. 49–51.\n- Ward; et al. (2006). Introduction to Flight Test Engineering (3rd ed.). New York: Kendall Hunt.']"	['<urn:uuid:414bb8c7-937c-47f7-8613-bd750af773a6>', '<urn:uuid:84a96067-3d4e-4504-9794-7adaf3d8c259>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	15	50	1564
22	mountaintop removal disease risks compared natural gas fracking health effects	Both mountaintop removal coal mining and natural gas fracking operations pose significant health risks to nearby communities. Near mountaintop removal sites, residents experience higher rates of respiratory illness, lung cancer, heart disease, chronic kidney disease, and birth defects. Similarly, communities near fracking sites report health problems including coughs, nose and throat irritation, sleeplessness, headaches, stress, anxiety, eye and skin irritation, and gastrointestinal problems. A study in Pennsylvania found that in the five most heavily fracked counties, infant mortalities increased from 36 to 60 cases after fracking expansion. Both industries have also been linked to social stress in communities, with fracking specifically noted to divide communities, pitting neighbor against neighbor.	"['Since the 1990s, small bands of Appalachian residents, regional environmental groups, and more recently the EPA have fought what often seemed like a futile battle against mountaintop-removal mining, the radical practice of blowing the tops off mountains to get at the coal seams underneath. The coal companies, backed by local political establishments and conservative jurists skeptical of possible regulatory overreach, have fended off multiple attempts to shut down mountaintop operations. As a result, an ever-widening swath of Appalachian peaks and valleys has been obliterated: approximately 2,200 square miles, according to the EPA, in what is likely a conservative estimate because the footprint often extends beyond the permit zones. That’s an area almost the size of Delaware.\nThat expanse kept growing as the battles mostly went in coal’s favor. Until this month, that is, when environmental groups won a decisive legal victory over a coal company. It may prove to be turning point in the war over the mountaintops, and for the future of coal.\nOn Nov. 15, St. Louis-based Patriot Coal agreed to phase out its mountaintop excavations and redirect its efforts back to underground mining. Adding a symbolic punch, Patriot agreed to decommission its two draglines—enormous boom excavators that do the actual mountaintop demolitions—and can sell them only on the condition that they’re never used in the Appalachian coalfields again. Coal executives usually shrug off complaints about mountaintop-removal impacts as the grumbling of dilettantes and naysayers who don’t understand the need for mining jobs. Yet here was the practically unheard-of spectacle of Patriot’s CEO, Ben Hatfield, acknowledging that mountaintop removal affected both people and ecology: “Patriot Coal recognizes that our mining operations impact the communities in which we operate in significant ways, and we are committed to maximizing the benefits of this agreement for our stakeholders, including our employees and neighbors,"" Hatfield said in court. ""We believe the proposed settlement will result in a reduction of our environmental footprint.""\nTrue, a specific chain of misfortunes brought Patriot to the table. It’s looking to exit from bankruptcy, the result of falling coal prices, bad business decisions, and huge pension obligations. It was in this context that Appalachian environmental groups forced their issue. Patriot was also on the hook for hundreds of millions of dollars to clean up discharges of the mining pollutant selenium from its operations. Pulling the plug on mountaintop removal was part of the cleanup settlement. Other companies aren’t planning to follow suit: “No, the decision and agreement is theirs and it does not affect our mining plans,” a spokesman for Alpha Natural Resources, the leading surface mining company in West Virginia, told the Charleston Gazette.\nBut Patriot’s woes aren’t unique. They grew out of endemic problems with mountaintop removal that have been getting worse, putting mounting legal, regulatory and economic pressure on the companies that do it.\nAppalachian forests are an ancient and unique ecosystem, one of the oldest in the world, with some of the highest levels of biodiversity to be found in a temperate zone. Destroying them is almost by definition ecological disaster. But because mountaintop removal in Appalachia is a relatively new phenomenon, it’s only in the past few years that scientists have begun to fully document not just the extent, but the nature of the damage.\nMountaintop removal is a three-step process. In the first, peaks are dismantled, descending layer by layer, using explosives and draglines, and the coal harvested. I’ve been to Patriot’s Hobet mine in Boone County, W.Va. Its sprawling operation—the largest single mountaintop-removal mine, covering more than 25 square miles—evokes the smoky, cratered, lifeless aftermath of a World War I battlefield. In the second phase, the vast piles of rock and soil debris this creates—larger in volume than mountaintops themselves, because of the extra space between all those boulders—are dumped in neighboring valleys, creating so-called “valley fills.” Finally, the lopped-off mountaintop is usually bulldozed and planted with something green, creating artificial meadows where none existed before.\nThe worst problem isn’t actually the missing peaks. Forests can be replanted, and sometimes the more virtuous companies actually do this. It’s what the mines do to the delicate mountain waterways. Dumping a mountaintop’s worth of rock into a valley doesn’t just obliterate all life there. It stops up headwater streams, literally the wellspring for the forest ecosystem and the point of origin for entire watersheds. So far more than 1,200 miles of stream beds have been buried. These cannot be restored. And rainwater that once would have fed those nascent brooks passes through a filter of mashed-up mountain instead.\n“There are coal residues mixed with shale rock that’s been buried for millennia, trace metals, pyrite, all these salts weathering out of the rock. Then it’s sitting right in the stream network,” says Emily Bernhardt, a Duke University biologist who has been studying the impacts of mining. “As it rains, it’s basically a chemical battery, if you will, that is constantly releasing its contents into that stream network.”\nThis “battery effect” generates a range of problems for miles downstream. It injects high levels of mineral ions into the water, where high electrical conductivity harms many kinds of aquatic life. Hard-hit species include mayflies, which happen to sit at the bottom of the forest food chain and are a broad indicator of ecosystem health. An EPA study that first drew wide attention to this problem in 2008 said: “Our results indicate that [mountaintop mining] is strongly related to downstream biological impairment.”\nIt’s these downstream impacts that came back to haunt Patriot. Selenium is a micronutrient that bioaccumulates up the food chain and has been linked to a variety of problems, including deformed fish. Selenium contamination downstream from Hobet and other Patriot mines is chronically high. Patriot had thus violated the terms of the Clean Water Act it promised in its permits to observe. Environmental groups took to the courts to get those terms enforced. Cleaning up selenium contamination is complex and expensive; when Patriot agreed to a major cleanup last January, it took on a crushing obligation estimated at $400 million.\nAnd that was the whole idea, the environmentalists say. Until now, companies have not paid a significant price for the collateral environmental damage they cause. “When you look at a company like Patriot which has scores of outfalls across dozens of permits, you’re taking some serious money to come into compliance,” says Derek Teaney, an attorney for Appalachian Mountain Advocates, one of the groups that brought the litigation. “It just revealed, in stark terms, exactly what expenses there are when a company is forced to internalize these costs rather than just having the environment bear them.”\nMountaintop removal’s impacts on human health are fuzzier, but here too new findings are painting an alarming picture. Since 2009, Michael Hendryx, a professor of public health at West Virginia University, has produced a series of peer-reviewed studies with various colleagues that show an association between proximity to surface mines and various health problems: cardiovascular illness, cancers, and birth defects.\nHendryx has begun investigating whether there is a causal connection for any of these. Finding a systematic link between local environmental contamination and such a wide range of health problems would be quite unusual. But in a study published in September, he and several colleagues collected particulate matter from around a mountaintop site in southern West Virginia, consisting mostly of sulfur and silica: that is, dust from the rocks pulverized by demolition explosions and other mining activities. They exposed rats to it, and their lung tissue showed telltale signs of stress that’s a common precursor to cardiovascular problems. Next up, Hendryx says he is planning to collect more samples from air, water, and soil around mountaintop sites.\nAs with the environmental evidence, such public health findings are likely to cause more legal and regulatory problems for coal companies. Of course, the companies have been aggressive, and mostly successful, at defending their interests in the courts. After years of indifference to the issues under Bush, the Obama EPA has tried to assert more authority over mountaintop permits. The EPA is supposed to enforce the Clean Water Act, and the more aggressive posture is part of the “War on Coal” that energy companies furiously denounce. But the agency’s efforts have so far been rebuffed by the courts as an overreach: Under the weird legal regime that governs mining, it’s the Army Corps of Engineers, not the EPA, that has the ultimate say-so over those permits. This is why the Patriot settlement is a breakthrough: It is a crack in the legal firewall that has protected mountaintop removal for 20 years. If more selenium suits follow, it may crack further.\nLong-term, such big cleanup costs could be a big drag on surface mining, which is already facing serious economic headwinds. The Appalachians have been actively mined for more than a century, a longer time than any other region of the United States. The easy-to-get stuff is long gone. Mountaintop removal is a product of this trend. Blowing the tops off mountains offers an economies-of-scale approach to coal mining: bigger mines, fewer employees, lower operating costs per unit. Until recently, aggressively expanding the practice helped make up for plunging productivity. (In central Appalachia, coal mine productivity fell 45 percent, an average of 5.9 percent per year, between 2000 and 2010.) But overall production has continued to fall. Add to that increasing competition from natural gas. Over the past year, low gas prices have driven down the domestic price of coal, and the Appalachian coal industry has been particularly hard-hit. Some producers are shifting their focus to metallurgical coal, a higher-quality form that is mined underground, rather than the thinner seams exposed by mountaintop mines.\nSomeday, some combination of economic forces and regulatory control will make mountaintop removal untenable. Even when that happens, though, there’s no way to rebuild a mountaintop or unfill a valley. Which means those blots on the landscape, and the environment, will linger for decades—or centuries.', 'By Elizabeth E. Payne\nCentral Appalachia has a wealth of natural resources, but extracting fossil fuels has come at a high price to the communities living near mountaintop removal coal mining and natural gas fracking sites.\nMortality rates in coal-mining counties of Appalachia lagged 24 years behind the national average, according to a 2008 study by Dr. Michael Hendryx, then of West Virginia University. That’s after adjusting for the social and economic inequalities — such as levels of education and rates of poverty — that affect life expectancies across the region as a whole.\nAdditional studies have documented increased rates of illnesses including cancers and respiratory and heart diseases. And a similar pattern is emerging in communities near fracking sites.\nInitiatives such as the Mountain Air Project — a five-year study on the reduced lung health in Eastern Kentucky — are tackling the health disparities in Appalachia. Other research is focused on identifying possible causes, such as the extractive industries themselves.\nA Truckload of Problems\nWinding through the hills and hollers of Central Appalachia, small mountain roads are bearing far more traffic than they were ever intended to. A steady stream of trucks going back and forth from mountaintop removal coal mining and natural gas fracking sites is doing more than inconveniencing local communities. It’s making them sick.\n“This unique geographic characteristic of the region means that roadways typically travel along valley bottoms, and homes are concentrated along these roadways. So the potential for particulate matter pollution to be exacerbated in this region is quite large,” says Dr. Viney Aneja, a professor in the Department of Marine, Earth and Atmospheric Sciences at North Carolina State University.\nParticulate matter pollution refers to particles in the air of different sizes and from different sources. The smaller the particles are, the more risk they pose to human health. Larger particles, such as pollen or dust, are roughly one-fifth the diameter of a human hair and can cause irritation to airways. Smaller particles, roughly one-twentieth the diameter of a human hair, can form from chemical reactions between naturally occurring and industrially derived gases and liquids. The resulting compounds, such as sulfuric and nitric acids, lead to further health risks.\nEven smaller particles, known as ultrafines, are one-millionth the size of a human hair and can penetrate deep into lungs on a cellular level. Such particles come from gas and diesel engines and energy production, among other sources, and research is beginning to demonstrate significant health risks from exposure.\nIn August 2008, Dr. Aneja spent two weeks collecting information about the amount of small particles polluting the air along roadways near Roda, Va. His research was initiated after residents complained about the problem and was supported in part by the Sierra Club.\nAneja’s research tested for comparatively large particles in the air resulting from coal trucks passing along the roadways, and it found that during times of heavy traffic the area had up to three times more particulate matter than recommended by the U.S. Environmental Protection Agency’s air quality standards.\nAccording to The Allegheny Front, Dr. Michael McCawley, interim chair of the Department of Occupational and Environmental Health Sciences at West Virginia University, has made a similar connection with trucks traveling to and from fracking well pads.\nWhat the Frack?!\nAs natural gas increasingly displaces coal as the nation’s leading source of electricity, equally dismal health impacts are emerging in the communities living near fracking sites.\nDr. Jill Kriesky is the associate director of the Southwest Pennsylvania Environmental Health Project, a nonprofit public health organization focused on the well-being of communities near the state’s natural gas extraction sites. The group was founded five and a half years ago in order “to provide timely, trusted and accurate information for residents who think their health has been impacted by shale gas drilling,” according to Kriesky.\nThe organization offers environmental and health assessments to help track changes in individuals’ health and monitor the air and water in their homes. They are also collecting information for a national health registry for people who believe their health is or could be impacted by unconventional oil and gas development.\nAccording to Kriesky, the health problems the organization is witnessing correspond to those found through academic research and include coughs and irritation in the nose and throat, sleeplessness, headaches, stress and anxiety. Irritation to eyes and skin, as well as gastrointestinal problems and effects on newborn birth weights and birth outcomes, have also been observed.\nThis observation is supported by a study published in April 2017 that found while the infant mortality rate in Pennsylvania fell overall during the periods studied, in the five most heavily fracked counties the total number of infant mortalities increased from 36 between 2003 and 2006 — before the expansion of fracking — to 60 between 2007 and 2010, after the expansion of fracking. The study found some association with contaminated well water.\nKriesky said that the Environmental Health Project has also noticed an increase in social stress suffered by communities near fracking sites.\n“The community’s completely divided, and industry does that on purpose, so they can pit neighbor against neighbor,” says Lois Bower-Bjornson, whose family lives within 1,000 feet of a natural gas well pad in Southwest Pennsylvania.\nMore than two dozen well pads surround her home, as do retaining ponds that hold fracking waste. A transmission line crosses her property, and a compressor station is only two miles away.\nBower-Bjornson’s children have suffered nosebleeds and skin irritation, though she is quick to note that others have it worse.\nA Mountain of Evidence\nDr. Michael Hendryx has been studying the health impacts of mountaintop removal for more than a decade. Now a professor in the School of Public Health at Indiana University, Hendryx began his research on the subject while teaching at West Virginia University.\nHe has authored or co-authored more than 30 scientific studies about the health impacts of surface mining, and his results are clear: mountaintop removal coal mining has a “significant and meaningful” impact of the overall health of the communities near the mines.\n“When I think about the health effects that we’ve seen that are uniquely related to mining that come through most strongly and most consistently, I would say that people who live in those mining communities are more likely to have respiratory illness, chronic forms of lung disease like COPD,” Hendryx says. “They’re at greater risk for lung cancer. They’re at greater risk for heart disease. They’re at greater risk for chronic forms of kidney disease. They’ll report poorer health-related quality of life, they’ll report an increase in the number of illness symptoms they experience over time.\n“There’s also some evidence, though it’s only one study, that the risk of birth defects will be higher for mothers who have babies that live in this region,” he continues. “Those are the kinds of effects that come through most strongly in our research.”\nHendryx has also published a study demonstrating a link between surface mining and an increased risk for depression and depressive symptoms, a result, at least in part, of the radically altered natural environment caused when familiar mountains are destroyed.\nFor those living near a surface mining site, Hendryx recommends improving the air quality of their homes to help avoid some health risks.\n“The risks are most clearly related to air-related problems,” he says. “There are some measures people can take to try and filter indoor air more effectively. That could be beneficial … [Also], don’t smoke; smoking rates are high in these communities and they’re only going to make things worse. If you can take other measures to improve your health and engage in healthy behaviors, try and do that.”\nFederal Agencies Finally Take Notice\nWith all of this observed and documented impact, the federal Office of Surface Mining Reclamation and Enforcement announced in 2016 that it would fund a study by the National Academies of Sciences, Engineering, and Medicine to review existing scientific research on links between adverse health impacts and mountaintop removal coal mining. The committee formed to tackle this project will summarize state and federal regulations that govern the practice and survey relevant scientific literature to identify short- and long-term health effects for populations living near these sites and the causes for them.\nThe committee held its first meeting in Washington, D.C., in March 2017, and the project is expected to take two years. A hearing that allowed for public input was held in West Virginia in May, with another session planned for Kentucky in late August.\nAs the health impacts of mountaintop coal removal are finally being recognized, many worry that the switch to natural gas may carry just as many risks.\n“There is a lot that is still unknown in terms of health impacts [from natural gas drilling], but there are certainly enough indicators in the direction of health impacts to be concerned about, that it doesn’t seem like we should be going full force ahead with this industry,” says Kriesky of the Environmental Health Project. “We should be taking a pause and figuring out what really is happening here in terms of people’s health.”\nVisit appvoices.org/nas-hearings for details on the National Academy of Sciences hearings.\nEditor’s Note: On Aug. 18, the U.S. Department of the Interior sent a letter to the National Academy of Sciences ordering it to halt its review of the links between mountaintop removal coal mining and human health impacts. For more information on this development, read a blog from Appalachian Voices’ Central Appalachian Program Manager Erin Savage.']"	['<urn:uuid:36532fb1-ee8a-4dfe-8bc7-e52faf98869b>', '<urn:uuid:32ab6a01-ec71-4411-9f8a-b7c6cd80d309>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	10	109	3245
23	Is sugar as addictive as drugs like cocaine?	Yes, sugar can be highly addictive since it stimulates the brain's dopamine pathway just like drugs do. Neuroscientists have shown that sugar leads to dopamine release similar to drugs like cocaine and heroin. Like drugs, sugar can also lead to changes in dopamine receptors, increasing tolerance and requiring more sugar over time for the same effects.	['What’s going on in the brain behind addiction?\nLet’s explore the brain chemistry behind pleasure and addiction. While addiction is a complex condition that we still don’t know everything about, there are certain neuroscience concepts that I’ve summarized to help better explain what we do know about this mental illness.\nWhat Is Addiction?\nThere are various definitions of addiction. The National Institute on Drug Abuse (NIDA) defines addiction as a chronic, relapsing brain disease that’s characterized by compulsive drug seeking and use despite harmful consequences. Brain changes caused by addiction can be long lasting and can lead to many harmful, often self-destructive, behaviors.\nWhat Can You Become Addicted To?\nWhen we think of addiction, we tend to think of drugs and alcohol. However, one can become addicted to anything that stimulates the brain’s dopamine reward pathway. Video games, shopping, sugar, thrill-seeking, and social media are just some potential sources of addiction. In the case of social media, the variable reward system associated with receiving “likes,” comments, and followers boosts dopamine release, fostering feelings of pleasure and perpetuating a cycle of dependence. While many things have the potential to become addicting, how do we determine when dependence is a problem?\nIn order to officially diagnose a substance use disorder, clinicians use the Diagnostic and Statistical Manual of Mental Disorders (DSM-5). The DSM-5 was created by hundreds of international experts on behavioral health with the purpose to improve diagnoses, treatment, and research. The essential feature of a substance use disorder is a group of cognitive, behavioral, and physiological symptoms that show a person continues using a substance despite significant substance-related problems.\nChanges in Brain Chemistry\nAlcohol and drugs produce their psychoactive effects by altering brain chemistry. More specifically, neurotransmitters, or chemical messengers in the brain, responsible for daily functions (e.g., learning, memory, appetite, pleasure, pain) are altered by substance use. A key neurotransmitter in the process of pleasure and addiction is dopamine.\nDopamine and Reward\nIn addition to its primary role in pleasure and addiction, dopamine is also involved in processes like learning, cognition, memory, and movement. Most of your dopamine is generated deep in the midbrain and released in three pathways across the brain as shown below.\nIt’s natural for people to seek out experiences that result in pleasure by stimulating the dopamine reward pathway. For example, natural processes that stimulate the dopamine reward pathway include exercising, eating, meditating, and having sex.\nNatural Reward vs. Substance-Induced Reward\nWhen people seek out experiences that feel good, these experiences boost dopamine. When people use drugs, dopamine transmission can be 3-5x greater than natural sources of pleasure. As you can imagine, this is not sustainable.\nDown-Regulation of Dopamine\nContinual activation of the dopamine pathway reduces the availability of dopamine in the brain. This reduction, or down-regulation, in dopamine availability has a blunting effect on the natural reward circuit. This partly explains why those who abuse substances may find it difficult to enjoy natural pleasures that they previously did. Something that once naturally caused pleasure no longer does as the reward circuit has been desensitized.\nIs Sugar Addiction Real?\nAs mentioned earlier, one can become addicted to any substance or activity that stimulates the brain’s dopamine pathway. Sugar, for example, can be a highly addictive substance that can change one’s brain chemistry. Neuroscientists have shown that sugar leads to dopamine release just as drugs do (e.g., cocaine, heroin). Also, sugar can lead to changes in dopamine receptors, increasing tolerance and requiring more sugar over time for the same effects.\nSince the dopamine pathway is involved with both sugar consumption and drug use, research has shown that those addicted to drugs can have increased cravings for sugar when they’re in early sobriety. This effect, known as cross-tolerance, illustrates that addiction to one substance makes it easier to become addicted to another substance that may use the same brain mechanisms. For example, marijuana is often referred to as a “gateway drug” as it may introduce the dopamine pathway to seek further experiences of drug use.\nSelf-Medicating Mental Illness\nAs discussed in the neuroscience of depression and anxiety, mental illness can be caused by an imbalance in the brain’s neurotransmitter activity. Mental illness and substance abuse are often comorbid and bidirectionally influence each other. Feelings of depression, for example, can lead to self-medication with substance use as it may the depression by stimulating dopamine release. External dependence on regulating one’s mood can quickly develop into a vicious cycle that leads to dopamine depletion and dependence.\nAddiction as a Brain Disease\nSubstance abuse can cause lasting changes to brain chemistry, especially the dopamine reward pathway. Limited availability of dopamine in certain areas of the brain may desensitize the reward circuit to natural sources of pleasure. People may choose continued substance use to compensate for the deficit in dopamine availability, thus perpetuating the cycle of addiction.\nIt’s possible for one’s brain to return to its baseline availability of dopamine after substance abuse. Research has shown that extended periods of sobriety, lasting at least twelve months, may be required for partial recovery of normal dopamine function. The time it takes for one’s dopamine functioning to revert to its normal state may be longer than some people are able to abstain from substance abuse.\nHowever, some people who receive treatment may not be able to return to normal dopamine pathway functioning depending on one’s history of substance abuse. As it can be incredibly challenging to retrain the brain’s cravings for substance-induced pleasure, and avoid triggers for substance use, rates of relapse are greatest in early sobriety.\nWithdrawals: a Price of Addiction\nHave you ever experienced irritability, fatigue, or cravings after you stopped eating sugar, drinking caffeine, or smoking marijuana? Most substance use results in withdrawal symptoms that can range from mild headaches to severe flu-like symptoms — depending on the substance in question and how chronic your dependence was. You feel irritable, sick, and fatigued because your brain is struggling to cope without the substance it became dependent on to feel normal. Also, contrary to popular belief, chronic substance users don’t solely continue using a drug to get “high,” but rather to prevent withdrawal symptoms that can interfere with daily life.\nMedication-assisted treatment, or “MAT,” is the use of medications in combination with counseling and behavioral therapies to treat substance use disorders. It can allow people to live satisfying, functional lives by relieving withdrawal symptoms and psychological cravings. For example, in the case of opioid addiction, methadone, is an approved MAT. Methadone “tricks” the brain into thinking it’s still getting opiates that it’s become dependent on. When used as prescribed, methadone mitigates withdrawal symptoms and does not cause one to feel “high.” People may be on methadone for weeks, months, years, or a lifetime. When they wean from methadone depends on factors such as their biology, environmental stability, and psychological readiness to cope with potential cravings and triggers for drug use. For more on opioid dependence and narratives on substance abuse, visit my series on Untold Stories of Addiction.\nAddiction is a chronic, relapsing brain disease that’s characterized by compulsive drug seeking and use despite harmful consequences. Brain changes caused by addiction, such as dopamine depletion, can be long lasting and can lead to many harmful, often self-destructive, behaviors. Also, addiction goes far beyond alcohol and drugs as humans can become dependent on anything that stimulates the brain’s dopamine reward pathway. Video games, shopping, sugar, thrill-seeking, and social media are just some potential sources of addiction. Understanding the process of addiction is critical for recovery. While cravings, withdrawal symptoms, and self-medication can pose barriers, recovery is possible and there is hope.\nIs there something you’ve become dependent on? Do you experience cravings and compulsive behaviors? These are questions we must all periodically ask ourselves, especially in a society that normalizes and profits from addiction.\nAddiction Treatment Resources\nIf you or a loved one is struggling with addiction of any kind, please know that there is treatment available. Utilizing SAMHSA’s (the Substance Abuse and Mental Health Services Administration) website or helpline is a great place to start for more information.\nSAMHSA’s National Helpline, 1-800-662-HELP (4357): The National Helpline is a free, confidential, 24/7, 365-day-a-year treatment referral and information service (in English and Spanish) for individuals and families facing mental and/or substance use disorders.\nI’d like to express a huge thanks to the professor of my Addictions course in graduate school, Dr. Eric Devine. The knowledge gained from his class was an impetus for my work as a clinician in the field of addiction. To my readers, thank you for being a part of my journey as a mental health counselor.\nOne Comment Add yours']	['<urn:uuid:80f0cf85-8a77-415c-b1ed-c8fa6cc71c4b>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	8	56	1441
24	How did Team Rubicon start as a disaster response organization, and what is their current size and volunteer structure?	Team Rubicon started in 2010 when two former Marines, Jake Wood and William McNulty, responded to the Haiti earthquake with 6 others and medical supplies. From that initial mission, they've grown into an organization with nearly 35,000 volunteers across ten regions. They operate as both a tactical branch that can integrate into existing command structures and as a non-governmental organization, providing free disaster response services while maintaining a roster of volunteers ready to deploy throughout the United States.	"[""It was 2010 when a couple of newly-discharged Marines, Jake Wood and William McNulty, watched in horror as the Haiti earthquake unfolded. They signed up 6 others, raised funds from family, packed up medical supplies and flew to the Dominican Republic. They rented a truck and crossed the river border between the Dominican Republic and Haiti. They thought of it as their own personal Rubicon, the point at which they vowed not to turn back from their task of providing aid. In fact, they found themselves very well equipped for the immediate disaster assistance deemed “too dangerous” by other aid organizations. Like Marines have always done, they went well beyond the established safety zones to reach those in most desperate straits and set up the basic infrastructure that enabled more well-established aid organizations to come in after them.\nThey didn't know it at the time, but this sort of disaster relief would become central to their post-military lives—would play in helping thousands of other veterans to build successful post-service lives through a new veterans organization: Team Rubicon. The idea: use the same skills honed in the military—sometimes using some of the same equipment—to be early responders in the wake of natural disasters.\nFounder Jake Wood is a tall, classically handsome former University of Wisconsin football player who went on to serve four years in Iraq and Afghanistan as a Marine sniper, earning a Navy and Marine Corps Commendation Medal. He is the author of a book on leadership, Take Command, that translates his service experience into broader lessons on leadership. McNulty is also a former Marine who worked in both infantry and intelligence before joining the staff of the Under Secretary for Defense, the Defense Intelligence Agency, and the National Security Council's Iraq Threat Finance Cell. Both men serve on Rubicon staff but Wood, as CEO, is the public face of the organization. The two concluded from that first Haiti mission that former soldiers can perform these rescue tasks with speed, efficiency, and impact, far outpacing more well-established organizations with larger budgets.\nOver the two years following the 2010 Haiti earthquake, Team Rubicon took shape in response both to natural disasters and personal tragedy. Wood's Marine sniper partner committed suicide in 2011. This caused the TR leadership to take more seriously the issues veterans have in adjusting back to civilian life. TR mobilized for the Joplin, MO, tornadoes in 2011, “Superstorm” Sandy in 2012, and the Moore, OK, tornadoes 6 months later. In each case, TR provided a rapid response team and volunteer leadership skills, in particular in organizing “drop in” volunteers while excelling at the logistics of clean up and recovery. Their work in the hard-hit Far Rockaway section of New York City after Sandy was particularly notable, as they used the latest technology to document flooding damage, developed anti-mold protocols and moved with such speed and efficiency that they got the attention of FEMA. With good reason. While FEMA was handing out written surveys to homeowners (which were supposed to be manually entered into a database in Washington), TR used military-quality global mapping software, immediately saved in the cloud, to record exact amounts of damage to each home, information immediately usable for volunteer deployment. A great deal of TR data analysis is conducted through Palantir software, originally used by U.S. intelligence but now with broad usage in the private sector, particularly with financial services firms. (One of Palantir's founders is PayPal founder and philanthropist Peter Thiel.)\nSays Jake Wood: “When we show up, it's structured, military, technical, and innovative.”\nAt a time when veterans are so often portrayed as suffering from PTSD or drug addiction, the growth of Team Rubicon tells another story. Its ten-region database now includes nearly 30,000 volunteers. Jake Wood is clear about the importance of helping communities hit by natural disasters—and helping the vets themselves. He stresses the importance of “the power of purpose in people's lives” and believes purpose brings “real behavior health impacts.”\nAt the same time, the help Team Rubicon provides on the ground is distinct and impressive. The organization responds to disasters both international (e.g., Nepal earthquake) and domestic (e.g., Houston, TX, floods). And it makes sure that veterans learn the post-military skills they'll need (chainsaw and heavy equipment operation, emergency medical response, and “muck-out” training) so that the national headquarters in Los Angeles can send out specific calls for volunteers with specific training—and not just after disasters. The group's regularly-scheduled service projects include work in national parks and even VFW halls.\nThe TR Hurricane Sandy response did more that get the attention of FEMA. The Goldman Sachs Foundation heard about the team's work and called to offer financial support. This was the beginning of a fruitful partnership. Goldman sent “busloads” of volunteers to help with the work, TR organized the volunteer time for maximum impact, and now Goldman is a major partner of TR in disaster response. TR struggles (as all relief organizations do) with finding enough financial resources to cover each disaster operation before the next disaster hits and people's attention and giving moves on. Last year TR lost money on all but two of its disaster responses. TR accepts no government support and operates on a budget of $7.5 million this past year, growing to just over $10 million in the coming year. Corporate & foundation grants account for over $7 million (including Goldman Sachs, Home Depot Foundation, Rockefeller, Citigroup, American Express, Bank of America, and Newman's Own), grassroots support is just over $1 million, events bring in about $1 million and major gifts (large individual gifts; for example, Lloyd Blankfein is a contributor) total $650,000.\nIn 2014 TR responded to 30 disasters, domestic and international, engaging 917 volunteers in 239 days of volunteer work amounting to 50,701 hours of service. Categories of work break down into disaster assessment, volunteer management, debris removal and expedient home repair. The various regions also organized 106 service projects, 73 training sessions, 37 fitness events, and 134 social engagements. The organization's reach is impressive: volunteers are clearing fire lines in national parks and preparing neighborhood resiliency plans in San Francisco in advance of possible earthquake events. All projects are coordinated and supported through an extensive social media platform that reaches not only what Wood calls the “marine corps mafia,” but men and women from all branches of the armed services and reserves along with non-military volunteers (mostly those with first-responder training or relevant skills).\nOne of the logistical challenges is that most TR volunteers are clumped around major urban centers or in the south around military bases. Most natural disasters occur in parts of the country where there are relatively few people, meaning relatively few TR volunteers (Hurricane Sandy and the Texas floods being notable exceptions). With access to skilled logisticians, TR deploys volunteers who live closest to the disaster, sets up temporary sleeping quarters in close-by schools or other shelters, and utilizes community organizations for meal provision while the work goes on. They train a corps of volunteers to lead Instant Management Teams (IMTs) who are the first to arrive and track damage, communicate specific volunteer needs, and set up a base. When they arrive to help, often literally working under large TR banners, local business owners often offer donated equipment, or rentals are secured in neighboring towns. Disaster relief efforts are coded by type as they occur, indicating if it warrants a regional or national response. Volunteers are reimbursed travel costs (within parameters) based on a sliding scale that depends on the type of disaster and the number of days the volunteer worked on the project. TR is usually able to arrange donated flights or reduced ticket costs through various airlines (and covers mileage costs for those who drive).\nTR got started with a group of vets from the post 9-11 generation but has grown quickly as veterans of all ages come forward to participate. Wood mentioned that Vietnam vets “still need this” community outreach, for instance, and having older vets working with younger has been to the benefit of both. More successful vets have opened doors for jobs for those just returning from war. Younger vets have brought technical skills and enthusiasm to the organization as they look for ways to use their military training to find their new life mission.\nOne female vet talks about how her volunteer service with TR after the 2013 Tacloban (Philippines) typhoon quite literally “turned [her] life around.” It gave her a sense of purpose and feeling of empowerment that spurred a job change and an entirely new outlook on life. Another Marine reservist and Gulf War veteran's first outing with TR was working to repair a community service center for disabled vets in Virginia. He described himself as unwilling to let others into his life at the time, but bonded with the TR members who were struggling with the same issues. He left law enforcement to go into the field of emergency planning for the federal government and participates in TR events whenever he can. Yet another vet was just back from Nepal working on earthquake relief. He described how lots of aid organizations were clustered around Katmandu because it was easier to work there. The 65-person TR team drove 5 hours up into the mountains and witnessed countless scenes of total devastation and no disaster teams present at all. Working with assistance from TR headquarters, the team narrowed the needs down to an “achievable mission,” setting up a base where combat medics could treat neighbors in the nearby villages, set up hospital tents, and conduct search-and-rescue missions. After that was complete, the team cleared out the rubble so that a school located at the top of a mountain in one of the villages could get back up and running, all the while enduring major earthquake aftershocks. These tasks were accomplished with the political help of Sir Edmund Hillary's granddaughter who “fell in love” with TR and is revered in Nepal. The UK chapter of TR sent over former British soldiers who are native Gurkas and knew the customs and spoke the local language.\nAs to future plans, it seems the sky is the limit. Team Rubicon has had conversations with the Secretary of the Veterans' Administration who is very interested in “the power of purpose in people's lives,” wanting to find a way to work more closely with TR. Elsewhere, Wood is interested in creating apprenticeships so that vets can obtain job training/trade skills as part of new long-term recovery work, noting that rebuilding work has not usually been done well following natural disasters. Once trade skills become part of the TR portfolio of volunteer skills, they could bid on large rebuilding contracts, many of which offer incentives for veterans to participate. Finally, the organization is adding one paid staff member to each region (regional groups being run entirely by volunteers now) in order to build a deeper volunteer pool, offer more training and build these sectors into closer communities.\nWood argues persuasively that there is no organization operating within the U.S. that has similar rapid-response capability and technical training. Interestingly, he admires the disaster response work done by the Southern Baptist organization, but points out that they cannot match TR's in-depth skills. The TR insignia is a cross turned on its side with two streams running through: that gap between the streams signifies the window of opportunity for TR responders, the time between the onset of the disaster and the arrival of conventional aid organizations. Once those aid organizations arrive, TR moves on. That gap also signifies the desire to bridge the gap between military and civilian life by providing veterans with a purpose (disaster relief), a community (through serving others), and self-worth (from seeing the impact one individual can have). The cross is a symbol of medical aid and is turned on its side to represent a new aid paradigm. In all these respects, it's clear that TR sees itself as a reinvented Red Cross. It is a grand ambition and a worthy one."", ""Team Rubicon is an international non-profit disaster response organization that unites the skills and experiences of military veterans with first responders to rapidly deploy disaster response teams, free of charge to communities affected by disasters across the country. Team Rubicon currently maintains a roster of 35,000+ volunteers able to deploy throughout the United States.\nTeam Rubicon stands able to deploy volunteers in the following mobilization times:\n- 24 Hours\n- 10 Volunteers\n- 48 Hours\n- 20 Volunteers\n- 96 Hours\n- 30 Volunteers\nTeam Rubicon maintains complete records of volunteers, hours worked, and work-orders so as to assist local, county, state, and tribal governments in applying for federal public assistance grants.\nTeam Rubicon Has the Following Capabilities:\nGive me a cup of leadership (coffee that is) and we’ll make sure we have the largest impact possible with the resources available during this disaster response.\nDamage & Impact Assessment\nSomething bad happened – but, really, how bad was it? We get there fast to map from the impact on the homeowner to the impact on the state.\nDisaster Mapping and Work-Order Management\nAll the bad news collected is mapped and provided as public information to help a community recover – We track everything we do too, this helps reduce the community's burden in recovery grants.\nThere’s a lot of stuff where it doesn’t belong, we help get things back to normal.\nHazard Mitigation (Fire/Flood)\nWe’re not all about disasters – we can do a lot to prepare and reduce the impact of disasters – Fire breaks, potential flood debris clearing, if it limits the impact of a disaster, you name it, we’ll get it done.\nExpedient Home Repair\nWhen things get ugly, one of the hardest parts is prepping for the skilled labor to come in and make it pretty – we’re free, fast, and do great work.\nSpontaneous Volunteer Management\nA lot of people want to help, we help communities make all those good people more organized.\nMedical (Intl. Only)\nOn international operations, TR deploys medics, surgeons, EMTs, nurses, and other medical personnel to staff field hospitals, provide emergency medicine, or organize a DMAT (Disaster Medical Assistance Team).\nTeam Rubicon deploys equipped with all the resources required to provide the capabilities above including:\n- Personal protective equipment\n- Hand tools\n- Solar-powered electrical suites\n- Palantir intelligence analysis and mapping suite (in mobile and static platforms)\nAll incident management personnel are trained in the application of Incident Command System (ICS) in compliance with the National Incident Management System. Volunteers are trained in the basics of ICS. Team Rubicon deploys as a self-sustaining unit.\nTeam Rubicon has responded to numerous natural disasters across the United States. The flexibility of our organization allows us to operate in two distinct ways:\nTactical Branch, Single Resource Unit, or Task Force\nTeam Rubicon can work under the direction of a governmental entity or agency (based on requested function) and integrate fully into an existing Incident Command System structure to perform disaster response functions.\nNon-Government Organization (NGO)\nTeam Rubicon is a voting member of the National Voluntary Organizations Active in Disaster (NVOAD). Team Rubicon can integrate into national, state, or community VOAD efforts to provide its services to survivors, free of charge.""]"	['<urn:uuid:451ca0db-b87b-4c8f-99b3-8134b5186d7c>', '<urn:uuid:4b5e74c8-49cc-44af-9245-1b1b8771c763>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	19	78	2543
25	I keep seeing some colorful spray paint marks on the streets in LA and wonder what they mean. Can you explain what these different colored markings on the pavement are for?	These spray-paint marks are warning signals that utility contractors use to indicate different underground infrastructure systems. Each color represents a specific type of utility: red marks indicate electric lines, yellow is used for flammable or gaseous materials, green represents sewers, blue shows drinking water locations, and orange markings indicate communications infrastructure. These markings help identify the complex network of hidden utilities that run beneath the city's streets.	['Submerged a little over a mile off the coast of Will Rogers State Beach in Pacific Palisades, an array of twenty-four silicon-iron alloy electrodes hangs suspended in concrete enclosures. Acting as a grounding device, this structure is paired with 1,067 cast iron anodes laid out in a 3,400-foot-wide ring, buried in a two-foot-deep trench of petroleum coke some 850 miles away on the Oregon-Washington border. Together, these two structures serve the Pacific Intertie, North America’s longest, highest voltage direct-current transmission line. Capable of delivering some 3,100 megawatts, the Pacific Intertie brings current from the Bonneville Power Administration in the Pacific Northwest to Los Angeles, providing nearly half of the Los Angeles Department of Water and Power’s capacity in the summer. In the winter, when cooling needs in the south decline and electricity demands in the north climb, Los Angeles returns the favor, sending electricity back up the lines.\nThe Pacific Intertie’s transmission towers each carry only two wires. Normally, in what is called bipolar mode, one transmits current while the other acts as a ground. When a wire needs to be taken off-line for repairs, the remaining one is dedicated to transmitting current. To provide the ground, the two electrodes are activated, giving electricity a return path through the ocean and earth itself. Conveying an invisible force between them, these two megalithic structures power an entire city.\n— Kazys Varnelis, The Infrastructural City\nThis is Los Angeles — the consummate infrastructural metropolis, famous for its networks of freeways and its dispersed, vehicle-based urbanism. This is also the departure point for The Infrastructural City: Networked Ecologies in Los Angeles, an anthology of essays examining contemporary LA and contemporary urbanism. Today’s metropolis, as described by volume editor Kazys Varnelis, depends upon layers of infrastructural networks — not just freeways — that connect the metropolis globally. Yet they also render the city susceptible to catastrophic damage, or at least interruption, if these networks are violated or short-circuited: a strong storm with high winds, for instance, could knock out the transmission lines described above and paralyze the city. “Cobbled together out of swamp, floodplain, desert, and mountains, short of water and painfully dependent on far-away resources to survive, Los Angeles is sited on inhospitable terrain, located where the continent runs out of land,” writes Varnelis. “No city should be here.”\nYet Los Angeles persists, sustained by its extensive infrastructures — “life-support systems,” as Varnelis characterizes them. Roads, freeways, rail corridors, ports; electric lines, gas lines, oil lines, communication lines; imported water for drinking, for cleaning, for treating waste, for irrigating crops: each system built according to its own independent logic, then overlaid on a landscape — and an increasingly complex urban terrain — to maximize efficiency, to maximize flows. Through time, these infrastructures have snaked over and under and across one another, with expanding regional and continental and sometimes global reach. Fantastic images like Andreas Gursky’s 1998 photograph — a night-time panorama of the city, shimmering and apparently endless — belie the contradictions of LA: the drama of its geography, poised on the edge of the continent and the world’s largest ocean; the romance of its image-based culture; and the magnitude of the systems that underlie and power its great surface expanse.\nBut Los Angeles is in crisis. Its population has skyrocketed — from 3 million in 1940 to over 16 million today [metropolitan statistics, from Encarta] — and its infrastructures are over-used and crumbling. Initiatives to extend, expand and upgrade these infrastructures have failed — in part due to the fiercely independent character of the populations that have settled here and who resist comprehensive planning; in part due to the fact that actual decision-making occurs on a more localized level and at a very quick pace, as Roger Sherman’s contribution to the anthology argues. Exacerbating these challenges, public funds for such initiatives — long scarce — have dwindled dramatically in recent months, due to national recession and California’s prolonged budget crisis.\nWhat to do?\nAs conceived by Kazys Varnelis — formerly on the faculty of the Southern California Institute of Architecture, now director of the Network Architecture Lab at Columbia — The Infrastructural City is an ambitious effort to construct an intellectual framework for answering this question. The book begins with three important assertions.\nFirst, Varnelis argues that organized urban planning in LA has failed and will continue to fail. This is not a new argument; Varnelis cites Reyner Banham’s influential 1972 book, Los Angeles: The Architecture of Four Ecologies, as one of the first serious studies of a city “driven by competitive interests, government agencies, pressure groups, and, above all, individuals.” Yet today, Varnelis continues, competition among these forces has only intensified, producing a “vicious stalemate, an urban trench warfare that effectively undoes the city’s ability to fix its problems.”\nSecond, Varnelis insists that LA must be engaged on its own terms, non-judgmentally, and not in irrelevant comparison to older cities with different urban origins. He places himself squarely in the lineage of German urban geographer Anton Wagner, whose 1935 analysis of Los Angeles — with its descriptions of “drilling tower forests” and “façade landscapes” — captured the essence of the city’s emerging character. To comprehend the challenge of fixing, or at least tweaking, LA, says Varnelis, we need to comprehend the very stuff of the city — its roads and bridges and pipes and wires, its endless and complex infrastructural networks, and the protocols that govern them.\nThird, and most critically, Varnelis argues that what results from this seeming mishmash of autonomous infrastructures is a city, an urbanism, with its own (hybridized) social, environmental and political dynamics — its own intertwined “ecologies” worthy of study themselves. Again, Varnelis is worth quoting at length:\nSetting out to understand this city, and by extension all contemporary cities, we treat it in terms of networked ecologies, a series of codependent systems of environmental mitigation, land-use organization, communication and service delivery. In our analysis, these infrastructures form the basis of the contemporary city, but they are vastly different from the infrastructures of old. Rather than being executed in conformance with the outline of a plan, they are networked, hypercomplex systems produced by technology, laws, political pressures, disciplinary desires, environmental constraints and myriad other pressures, tied together with feedback mechanisms. Networked ecologies embody the dominant form of organization today, the network, but these networks can be telematic, physical, or even social. What matters is that we do not think of these ecologies as discrete terrains as Banham did, but rather as the sort of networks that artist Mark Lombardi drew — inextricable and impossible, like balls of yarn after visitation by a litter of kittens.\nVarnelis organizes his interrogation of Angeleno metropolitanism into three parts, or what he calls “scales of networks”: landscape, urban fabric, and object. Each of the eleven essays falls within one of these scales, and each takes on one of the city’s constituent systems, or tendencies. Collectively they establish a provisional framework for understanding how this complex metropolis works. The essays range widely in topic, from water supply, flood control, oil resources and gravel mining, in the landscape section; through traffic, telecommunications, palm trees and cell phone structures, in urban fabric; to property, distribution networks; and prop houses (which supply props for movies and television shows), in object. Interspersed throughout are photo essays, by Lane Barden, depicting three physical networks: the river (the Los Angeles River), the street (Wilshire Boulevard), and the trench (the Alameda Corridor Transportation Authority’s rail and truckway). The photographs are striking for their portrayal of these urban corridors as at once mundane and unique, generic yet defining. (At first glance, it’s even difficult to distinguish one from the other, since all are laced with power lines, bridges, channels, roadways and rail corridors, and surrounded by subdivisions, warehouses, distribution hubs, glass-skinned buildings, parking lots and automobiles.)\nImplicitly the essays underscore the problematics of these territories and infrastructures as much as they construct a footing for new interventionist practices. The book’s broader categories, for instance, are not necessarily singular, or restrictive: while Varnelis organizes the topics according to scale, the terms he chooses for these scales — again, landscape, fabric and object—are generic enough to have wider implications. Nor do the individual essays fit comfortably (or exclusively) within their sections: the essay on trees, for instance, is placed in “urban fabric,” yet it could work as easily in “landscape,” or even “objects.” Varnelis’s categorizations are both useful reference points and yet usefully contestable — very much in tune with the nature of this complicated, contested city.\nDavid Fletcher’s contribution — “Flood Control Freakology”— paints an especially full picture of the complexities that both impinge upon and grow from a single infrastructural system, in this case the Los Angeles River. Once a meandering river whose flow fluctuated seasonally and whose banks were lined with verdant marshes and sand bars, the river has long been channelized and lined by concrete—very much the edgy, barren culvert familiar from the ‘70s television show CHiPs and the Terminator movies (my own media touchstones, not the editor’s). Yet, Fletcher argues, the river corridor cannot be understood as a singular entity — its 51-mile course is marked by multiple owners and regulatory complexities, by diverse maintenance and operational protocols, and by fragmented (and often emergent) ecosystems that are born from this engineered channel and its infrastructural fabric of rail, road, telecommunications and port systems. “Standing under a bridge, in the river, you might be on private property, in city-owned water, in a channel built and maintained by the federal government within a county easement, and in the air rights of the California Department of Transportation,” writes Fletcher. “Railway and utility easements will flow on either side of you, conducting goods from the port and power to adjacent municipalities.”\nFletcher’s “freakologies” are compelling examples of the general phenomena and arguments of the book. Freakologies — Fletcher’s term — are the offspring of dynamic environmental flows (of water, minerals, plant communities and animals) and the manifestations of artificial infrastructural systems (concrete channels, steel bridges, vertical walls and stormwater and sewage discharge pipes) that sustain LA’s economy. These “freak ecologies” include: emergent vegetation nurtured by the scarce organic nutrients and silts caught up in the inorganic debris dumped into the channel (junk cars, shopping carts, old clothing, plastic bags); squatter camps under bridges and in storm drains, their occupants cashing in cans and bottles carried by floodwaters at recycling centers, washing clothes and bodies in the low-flow channels, lighting combustible waste and logs at night to make communal bonfires; new stands of mixed native and exotic, ornamental and agricultural plants swept downstream from yards and nurseries and nature reserves, feeding off effluent from treatment plants and car washes; bat colonies and swallow nests, tucked beneath overpasses, that help control disease outbreaks by consuming vast quantities of the mosquitoes that breed in the river’s standing water; and the largest concentration of black-necked stilts in the United States, which feed off the invertebrates that thrive in the river’s algae fields — fields that themselves result (accidentally) from the channel’s shallow configuration and the high volume of nutrient-rich sewage discharged into it. This is the stuff of the networked, infrastructural city — not an idealized, romanticized, riparian past that could no longer thrive here — and this, according to Fletcher, forms the basis for a rich discussion of its future.\nRoger Sherman’s essay, “Counting (on) Change,” describes the fragmented and often informal decision-making that affects individual properties and, by extension and in aggregate, the larger urban fabric. “Cities today develop at a rate that outpaces architects’ and planners’ efforts to shape them,” says Sherman. “Political and economic circumstances change so rapidly that by the time a plan is realized, it is often already obsolete; a mere election or economic downturn can radically alter the assumptions and objectives of a project or master plan.” In Sherman’s view, progress and development occur in “realizable chunks or increments, placing an emphasis more on augmentation than organization.”\nTo illustrate this, he documents the localized and negotiated manner in which moments of the city fabric are shaped, and the often unexpected (and cross-bred) results. He offers provocative examples: the “socioeconomic coupling” of car wash and juice bar that exploits shared physical space, customers, and operational efficiencies (e.g., the bar’s dishes are rinsed in the car wash’s rag machines); the regulation Wiffle ball field and viewing stands that occupy an otherwise unusable gap at the end of an Encino cul-de-sac, wedged between private houses and a parcel controlled by the Department of Water and Power; a non-operational oil derrick sitting on a lot amid Century City’s commercial towers, now sheathed in fabric painted with a floral pattern and maintained by Portraits of Hope, a local non-profit serving terminally ill children; and the most recent configuration of a series of adjoining sites in Signal Hill that productively accommodate a café, three working derricks owned by Shell Oil, Jack the Shoeshine Man, a parking lot for the Pacific Century bank, and two Craftsman-style homes belonging to the Denny and Hockenbrocht families. Drawing on the more informal, fragmented way decisions are made in LA, Sherman argues for the re-conception of city-making as a set of localized, nimble and adaptive practices, or “protocols” — in Sherman’s words, “a loose set of rules flexibly dependent upon the responses of . . . bargaining partners” — as opposed to official planning or decision processes and guidelines. Sherman also argues that architects and urban designers should embrace and adopt the flexible tactics of negotiation as a more viable way to shape, if only fleetingly, moments in the infrastructural city.\nVarnelis’s own essay — “Invisible City” — examines the city’s largely unseen telecommunication networks. He describes the unplanned confluences of LA’s hidden infrastructures, as signaled by the warning marks that utility contractors spray-paint on patched-up asphalt — red for electric, yellow for flammable or gaseous materials, green for sewers, blue for drinking water, and orange for communications. LA’s long-distance transmission cables — including the high-bandwidth fiber optic cables that connect Asia and North America and facilitate a major piece of the global economy — are part of a telecommunications economy that has supplanted aerospace as one of the region’s leading global industries and helped to support the rise of Hollywood’s entertainment economy. The system converges in a single-use, vertical hub centered at One Wilshire, a bland high-rise distinguished only by the extra-large cooling units on its roof and the proliferation of orange markings on surrounding paving. Varnelis wants us to pay less attention to the showy architecture that dominates post-Bilbao urban design discourse — Frank Gehry’s Disney Concert Hall, for instance — and more to One Wilshire. Drawing on Lewis Mumford’s mid-20th-century work on technology and the city, Varnelis posits: “Our networked society is increasingly dominated by what Lewis Mumford called the unseen world of cables, wires, connections, codes, agreements, and capital. Today more than ever, the role of this invisible city in determining the structure of the urban area is vast. Visible form is merely an irruption of other forces, a graphic user interface for a more powerful command line below.”\nOne Wilshire has evolved into a hub in which individual, private telecom firms can connect into one another’s networks in the fourth-floor “Meet-Me Room”; in turn this hub has spawned the proliferation of related telecommunication infrastructures in a dozen surrounding buildings converted to this use, collectively forming a “new concentration [of resources, of emergent urbanism] at [a] command point . . . in the organization of the world economy.” Varnelis suggests that it is here, among the operating systems and protocols of LA’s unseen infrastructure, that we might begin to re-imagine and re-program the city: “[T]he task of cognitive mapping lies at the point in which media and cities, network and economy, substructure and superstructure, become inextricable. The real operating system, not the graphic user interface, is our concern. Only by engaging the code below can we remain relevant to future cities.”\nNo wonder, then, that he invokes the hacker as perhaps the best equipped to re-tool or re-code the city. The Infrastructural City argues convincingly that the layering of transportation, communications, hydrologic and power systems atop one another and atop a semi-arid terrain is giving rise to new hybridized or mutated social-environmental-technological dynamics that are unique and robust and deserving of serious critical reflection. Underlying this position is an unstated realization — that LA, only now, is mature enough to have developed these emergent, intrinsic and complex metropolitan ecologies — freakish though they may be. Varnelis suggests that the book might function best as a field manual for the metropolitan hacker, whose gateway may be one of a million local points on a myriad of overlaid continental and global networks of exchange that intersect at this sunny piece of land on the edge of the Pacific Ocean. The collection’s various maps, diagrams and photographs — clear, precise and cross-referenced — underscore its potential for such covert operations, and perhaps for more mainstream and touristic agendas as well — itineraries for the 21st-century metropolitan flaneur.']	['<urn:uuid:75ddbd5a-1896-4bb3-b8c0-d9203b0da976>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	31	67	2850
26	surface tension difference mercury water	Water and mercury have very different surface tensions. Water's surface tension is approximately 73 dynes/cm while mercury's surface tension is much higher at about 465 dynes/cm. Due to this high surface tension, mercury can form beads that can be pushed around on a surface, though this is not safe to do due to mercury's hazardous nature.	"[""Welcome to PhysLink.com - Your physics and astronomy online portal. Stay a while! Check out our extensive library of educational and reference materials. Also, check out our fun section!\nIs a drop of water from a dropper equal in volume to a drop of mercury from the same dropper?\nAsked by: Bill Somerlot\nThe size of a drop of water and a drop of mercury from the same dropper will be different.\nAssuming that you've filled the dropper to the same level and squeeze the dropper at the same rate, the size of the drop when it separates from the tip of the dropper and falls will be based primarily on two quantities. The first is the surface tension of the liquid and the second is the density of the liquid.\nThe density is the easiest part to explain. The higher the density, the more mass you have in the same sized drop. Assuming you're doing this in a gravity environment, more mass means more weight. Which means that for the same sized drop, mercury, which has a much higher density than water, will weigh significantly more, and will therefore have more force pulling it downwards. Mercury has a density of about 13.6 g/cm3, while water has a density of 1 g/cm3.\nSurface tension, in this case, provides a resistance to the downward force of gravity. Think of a glass of water which has been filled to the top. Not just full, but as full as it can be without any spilling over. If you look at the top of the glass, there's actually a dome of water which is above the top of the glass. If you were to add a drop of soap to this bubble, water would spill over the top of the glass, due to the fact that soap reduces the surface tension of the water.\nBack to the question at hand, water has a much lower surface tension that mercury. Water's surface tension is about 73 dynes/cm which mercury is about 465. If mercury weren't hazardous to your health, you could sit there with a bead of mercury on your desk and basically push it around, due to its high surface tension.\nSo as we see, mercury is at both an advantage and a disadvantage. It has higher surface tension, which allows it to bead up more which, alone, would allow the formation of a larger drop. On the other hand, it has a higher density, which, alone, would result in a smaller drop. Which of these two factors wins out, I can't say. But the drop sizes will be different, barring a coincidence of epic proportions.\nOn an interesting side note, the amount of mercury and water sucked up into the dropper initially would also differ, owing to the different densities. You would get less mercury in the dropper than you would water if you squeezed the bulb equally in both cases.\nHere are our physics & astronomy bestsellers:\nMagnetic Levitator - Classic\n3D Magnetic Field Tube\n12 inch Galileo Thermometer\nCherry Wood Levitron\nMini Plasma Ball\nDeluxe Water Rocket Set\nSpace Wonder Gyroscope""]"	['<urn:uuid:4aa3480a-65dd-4521-b774-068e1acdd493>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	5	56	520
27	compare skills training padi divemaster jsska kayak instructor	Both PADI Divemaster and JSSKA kayak instruction programs offer professional training but have different focuses. JSSKA instructors like those at Green Wave Paddling and Tuktupaddles.com are ACA certified and teach paddling skills, with some also teaching specialized skills like paddlemaking. PADI Divemasters, on the other hand, are trained to be diving leaders who mentor others, gaining skills to prevent and manage problems in the water while leading dives to depths up to 40 meters.	"[""(in alphabetical order)\n*Listings marked with an (*) indicate that JSSKA members recieve discounts from these retailers and instructors\n|Alliance For A Living Ocean The mission of the Alliance for a Living Ocean shall be to promote and maintain clean water and a healthy coastal environment through education, research and active participation.|\n|Clean Ocean Action Be the solution to ocean pollution!|\n|Environment New Jersey Environment New Jersey is a statewide, citizen-based environmental advocacy organization|\n|Garden State Environet First in NJ for environmental news and information|\n|Hackensack Riverkeeper Protect and preserve the Hackensack River|\n|NY/NJ Baykeeper The Baykeeper's mission is to protect, preserve, and restore the ecological integrity and productivity of the Hudson-Raritan Estuary - the most urban estuary on the planet|\n|NJDEP Division of Fish & Wildlife Environmental agency dedicated to the protection, management, and wise use of NJ's fish and wildlife resources|\n|NJ Trails Plan The New Jersey Trails Plan Update will address and analyze key issues related to trails development and the State Trails System, establish a vision for New Jersey’s trails, and recommend strategies and actions required to move toward that vision.|\n|Save Barnegat Bay SAVE BARNEGAT BAY is a not-for-profit environmental group working to conserve undeveloped natural land and clean water throughout the Barnegat Bay watershed.|\nFISH & WILDLIFE\nOnline License Sales Get a fishing or hunting license online. Fishing license needed to launch at Round Valley Reservoir.\nKayaking Gear List - list of gear from Sea Kayaker Magazine.\nLESSONS & KAYAK INSTRUCTION\n*Green Wave Paddling is owned by ACA certified instructor Bryan Mitchell. Instruction is offered to JSSKA members at a discount. A portion of the proceeds of Bryan's lessons are donated to non-profit environmental organizations.\n*Tuktupaddles.com is owned by certified instructor and paddlemaker Chris Raab. Instruction is offered to JSSKA members at a discount. Chris also teaches paddlemaking skills and Greenland skills..\n*The Jersey Paddler Store also offers kayaking instruction throughout the year at 1756 Rt 88 West, Brick, NJ. 1-888-22-KAYAK. Ask if the class is offered at a discount to JSSKA members.\n*The Paddle Shack store, located at 5045 Mays Landing-Somers Point Rd, Mays Landing, NJ 08330. 609-909-5250 gives classes and winter pool sessions. Ask if the class is offered at a discount to JSSKA members\nThe local park systems in Monmouth & Ocean Counties offer beginner instruction and guided trips for sea kayaking and kayak surfing. Contact the parks department in your area to find an up to date schedule of instructions.\nAmerican Canoe Association (ACA) also offers padding skills training both in person and through on-line courses.\nPages created and maintained by JSSKA members.\n|NJ Paddle Zone NJ paddling resources, conservation, and trip reports|\nPaddling organizations, outfitters, instruction, and more.\n|River Rat Race April race conducted by the Athol Lions Club in conjunction with Orange Lions Club on the Millers River, New England|\n|USPP Upper Susquehanna Pedalers and Paddlers Spring race.|\n|YMCA Cortalnd Canoe Classic Spring race for recreational and competitive canoes and kayaks|\n|Run of the Charles Charles Watershed Association Spring canoe and kayak race|\n|NYMCRA New York Marathon Canoe Racing Association - calendar with assorted race schedule|\n|Schuykill River Regatta May race organized by the Keystone Canoe Club|\n|Edward N Wessels Canoe Regatta Sydney, New York race|\n|NYC Mayors Cup Elite 26.7 mile marathon kayaking race around Manhattan Island.|\nRETAILERS & RENTALS\n*Adventure Scuba Inc. 1948 Main Ave Suite #5 Conshohocken Pa 19428. (866) 4U2 -DIVE or (610) 832-2570. Cold water gear, neoprene, fabric and latex repairs, including gasket replacement. JSSKA members recieve a 10% discount when you show your valid membership card.\nAmigos Gasket/Dry Suit Repair Shop Specializing in gasket repairs. Located in North Carolina. Phone: 800-483-0188.\nBel Haven Paddlesports 1227 Route 542 Green Bank, NJ 08215. 1-800-445-0953. Canoe and kayak rentals and sales.\n|*EMS Locations across the US. The Eatontown store gives a discount to JSSKA members when you show your membership card. 133 State Highway 35, Eatontown, NJ 07724 732-460-0188|\n|First Bridge Marina & Kayaks 500 Great Bay Blvd, Tuckerton, NJ 08087. 1-800-50KAYAK. Kayak tours and rentals.|\n|*Jersey Paddler 1756 Rt 88 West, Brick, NJ. 1-888-22-KAYAK. Sales, workshops, instruction. JSSKA members receive a 15% discount when you show your JSSKA card.|\n|Kayakshed Hood River, OR. Shop online. Rentals and instruction available if you happen to be in the area.|\n|NJ Kayak 409 East Bay Ave, Barnegat, NJ 08005. 609-698-4440. Sea kayak instruction, sales, rentals and tours.|\n*Paddle Hut 1 Atlantic St., Highlands, NJ, 732-872-6600, adjacent to the Clam Hut Restaurant. Year round kayak storage, launch, shower and changing facilities. Members receive 10% off all services for 2009.\n*Paddle Shack 5045 Mays Landing-Somers Point Rd, Mays Landing, NJ 08330. 609-909-5250. Tours, sales, special events. JSSKA members receive a discount when you show your JSSKA card.\n|Shark River Kayaks 921 Main Street, Belmar, NJ 07719. 877-TO-KAYAK. Kayak tours and rentals plus lessons, sales and surfing.|\n|ACA Beginners Guide A great resource for learning about kayak selection, paddlesport terms, essential equipment, learning to paddle, basic paddling skills, and self rescue.|\n|ACA SmartStart for Safe Paddling|\n|ACA Guidelines for Safe River Paddling|\n|American Red Cross Jersey Coast Chapter|\nFloat Plan sample float plan from Sea Kayaker Magazine\n|Guidelines for Safe Trip Participation All paddlers should be familiar with these before paddling on any posted trip.|\n|Hypothermia Links Hypothermia Kills! Be prepared for cold water paddling.|\n|Lightning Safety Article Very informative primer on lightning|\n|National Lightning Safety Institute National Lightning Safety Institute is a non-profit, non-product advocacy of lightning safety for both people and structures|\nRisk Factors to Assess in trip preparation from Body Boat Blade International\nThe Safe Harbor is a free online educational resource for safe boating in the ports of NY/NJ.\nUnited States Coast Guard Main page for the Coast Guard.\nU.S. Coast Guard Small Boat Stations --gives phone numbers and contact information for individual stations in the fifth district.\nTIDES & WEATHER & NAVIGATION\n|National Weather Service Link to Eastern Region Headquarters|\nnowCOAST Real-time environmental observations and NOAA Forecasts\nMarine Traffic Real Time locations of marine traffic in NY Harbor, Hudson River, Raritan Bay, Sandy Hook and other locations worldwide.\n|Saltwater Tides Tide charts courtesy of Stripersonline.com.|\n|Weather Underground Offers general and marine forecasting|\nWWW Tide and Current Predictor Customizable tide and current predictor\nTides On Line Tides for New Jersey & beyond\nNew Jersey High & Low Tides (from saltwater tides)\nDiscussion of anemometer & wind velocity from seakayaker magazine.\n|Great Calusa Blueway Florida Paddle blueway trails in Fort Myers/Sanibel area of Florida.|\n|Canot-Camping La Vérendrye Offers canoe routes on the La Vérendrye Wildlife Reserve, Quebec, CA.|\n|Euforia Expeditions Kayaking in Costa Rica|\n|Florida Bay Outfitters Florida Keys kayak and canoe center|\n|GORP Resources to help you explore your world - very extensive!|\n|Island Expeditions Kayaking in Belize|\n|Kayaking Tours British Columbia Offers sea kayaking Vancouver Island tours around British Columbia, Canada|\n|Northern Forest Canoe Trail The Northern Forest Canoe Trail (NFCT) links the waterways of New York, Vermont, Québec, New Hampshire and Maine.|\nOsprey Sea Kayak Adventures Sea Kayaking tours, travel and instruction in Massachusetts, Cape Cod, Rhode Island, The Bahamas, and Belize.\nSea Kayak Adventures Sea Kayaking and Whale Watching Tours in British Columbia and Baja.\n|Sea Kayak Georgia Offering instruction or outfitting on Tybee Island.|\nSea Kayak Greece Outfitting and accomodations.\nHANDMADE BOATS/BOAT BUILDING RESOURCES"", ""PADI Dive Courses\nPADI Discover Scuba Diving - £25\nBefore you take the plunge with a full course, come and have a go.\nDiscover Scuba is a chance for a non-diver to find out what it is like to scuba dive with one of our professional team in the safety of a heated swimming pool.\nAll you need to bring is swimwear, we provide the rest.\nYou'll get an hour talking about all aspects of diving followed by an hour in the pool.\nPADI Scuba Tune Up - £50\nHaven’t been scuba diving lately? Feeling a bit rusty? The Scuba Review program is just the refresher you need to brush up on your knowledge and skills. A PADI Professional will get you in the water having fun and feeling comfortable again. It’s quick and easy, and a good way to prepare for your next PADI course or get ready for adventure on a diving vacation.\nPADI Openwater Diver - £400\nGain your PADI certification which allows you to scuba dive all over to world to a depth of 18 metres.\nSix lessons in the classroom and swimming pool are followed by four dives in a training lake.\nIf you don't fancy diving in a lake in the UK, ask us about the referral option.\nThis allows you to complete the four qualifying dives abroad.\nPADI Advanced Openwater Diver - £295\nDevelop your diving experience, by taking part in the PADI Advanced Open Water Course.\nThis course consists of two compulsory dives, Deep and Navigation and three elective dives.\nWreck Diver, Peak Performance, Search and Recovery or Dry Suit.\nExpand your interests and enjoy diving more.\nPADI Rescue Diver - £275\nScuba divers describe the PADI Rescue Diver course as the most challenging, yet most rewarding course they’ve ever taken. Why? Because you learn to prevent and manage problems in the water, and become more confident in your skills as a diver, knowing that you can help others if needed. During the course, you learn to become a better buddy by practicing problem solving skills until they become second nature. Plus, the course is just fun – it’s serious, but still allows for lots of laughter in between the focused learning.\nPADI Emergency First Response (First Aid) - £85\nEmergency First Response training focuses on building confidence in lay rescuers and increasing their willingness to respond when faced with a medical emergency. Course participants learn simple to follow steps for emergency care and practice applying skills in a nonstressful learning environment. All courses are supported by self-study manuals, videos and quick reference cards to enhance learning and allow you start learning right away.\nPADI Rescue Diver/EFR Package - £340\nCombine both courses together for a lower price.\nPADI DiveMaster - £ASK\nLove scuba diving? Want to share it with others on a whole new level? Take the PADI Divemaster course and do what you love to do as a career. Scuba divers look up to divemasters because they are leaders who mentor and motivate others. As a divemaster, you not only get to dive a lot, but also experience the joy of seeing others have as much fun diving as you do.\nPADI Boat Diver - £99\nMuch of the world’s best scuba diving is accessible only by boat. Whether you’ve never made a boat dive or you’ve logged dozens, the PADI Boat Diver Specialty course will benefit you because boats in various parts of the world do things differently. Scuba diving from a boat is fun and relatively easy because you usually descend directly onto your dive site.\nPADI Deep Diver - £149\nThe lure of the deep. There’s something exciting and mysterious about exploring deeper dive sites while scuba diving. Sometimes it’s a wreck that attracts you below 18 metres/60 feet, and on wall dives it may be a giant fan or sponge. Whatever it is, to scuba dive with confidence at depths down to 40 metres/130 feet, you should take the PADI Deep Diver Specialty course.\nPADI Digital Underwater Photographer - £149\nUnderwater photography is one of the most popular diving specialties, and with so many underwater cameras to choose from, it has become easier and more fun than ever to capture images of your underwater scuba adventures. The PADI Digital Underwater Photographer course gets you going quickly, whether you use a point-and-shoot camera or a sophisticated DSLR like the pros.\nPADI Drift Diver - £99\nThe PADI Drift Diver Specialty course teaches you how to enjoy going with the flow as you scuba dive down rivers and use ocean currents to glide along. It feels like flying – except that you’re underwater using scuba equipment. Drift diving can be relaxing and exhilarating at the same time. If this sound like fun, then the Drift Diver course is for you.\nPADI Drysuit Diver - £149\nWant to stay warm? Want to extend your scuba diving season? Then dive dry. A dry suit seals you off from the water and keeps you comfortable, even in surprisingly cold water. There is incredible diving in the world’s cooler regions and in some areas, conditions are even better in colder months. Becoming a dry suit diver allows you to expand your boundaries and dive more places, more often.\nPADI Emergency Oxygen Provider - £99\nKnowing how and when to use emergency oxygen is a great skill to have and means you’re ready to help others should the need arise. Becoming a PADI Emergency Oxygen Provider lets you breathe easy knowing that you can recognize scuba diving illnesses treatable with emergency oxygen, and are prepared to offer aid.\nThere are no prerequisites, age restrictions or water sessions required for this course – it’s open to everyone. Scuba divers, snorkelers and anyone who is around divers – boat crew, lifeguards, etc. – will benefit from having this training.\nPADI Enriched Air/Nitrox - £99\nThe PADI Enriched Air Diver course is PADI’s most popular specialty scuba course. Why? Because scuba diving with enriched air nitrox gives you more no decompression time, especially on repetitive scuba dives. If staying down longer and getting back in the water sooner sounds appealing, then don’t hesitate to become an enriched air diver.\nPADI Equipment Specialist - £99\nDon’t miss a dive due to minor issues with your scuba diving equipment. Whether it's a missing o-ring, wetsuit tear or a broken fin strap, the PADI Equipment Specialist course teaches you to manage basic repairs and adjustments. You'll also learn more about how your gear works, making you more comfortable with it and better prepared to take care of your investment.\nPADI Multilevel Diver - £99\nIn the old days, dive profiles were calculated from the surface down to a maximum depth, then back to the surface. Now, dive computers continually analyze your depth – giving you more bottom time for going shallower and allowing you to maximize your dive time. If you’d like to understand more about dive computers and learn how you can use tools like the eRDPML to plan multilevel dives, then the Multilevel Diver Specialty course is for you.\nPADI Night Diver - £149\nThe thought of dipping below the surface at night seems mysterious, yet so alluring. Although you’ve been scuba diving at a site many times before, at night you drop into a whole new world and watch it come to life under the glow of your dive light. The scene changes as day creatures retire and nocturnal organisms emerge. If you’ve wondered what happens underwater after the sun goes down, sign up for the PADI Night Diver Specialty course.\nPADI Peak Performance Buoyancy - £99\nExcellent buoyancy control is what defines skilled scuba divers. You’ve seen them underwater. They glide effortlessly, use less air and ascend, descend or hover almost as if by thought. They more easily observe aquatic life without disturbing their surroundings. You can achieve this, too. The PADI Peak Performance Buoyancy Specialty course improves the buoyancy skills you learned as a new diver and elevates them to the next level.\nPADI RNLI Diver Sea Survival - £135\nWould you like to develop new skills to be better prepared to deal with the unexpected at sea? Are you interested in learning how to respond in the event of a diving related emergency underwater and at the surface? Then the PADI RNLI Diver Sea Survival Specialty Course is for you.\nThe PADI RNLI Diver Sea Survival Specialty Course will provide you with the skills and knowledge to deal with unexpected emergency situations whilst diving. You will also learn how to initiate a faster and more effective rescue. Mastering sea survival skills will give you additional confidence if you find yourself in an unwanted situation whilst diving.\nWhat skills will I gain during this course?\nDiving in low-visibility conditions\nHow to deal with out-of-air emergencies\nHow to deal with an emergency on the surface\nNavigation and safety equipment on dive boats\nUse of surface marker buoys (SMBs)\nWays of calling for help\nThe PADI RNLI Diver Sea Survival Course contains knowledge development and 2 open water training dives to practise the skills and techniques learned.\nPADI Search & Recovery - £149\nIt happens: People accidentally drop things from docks, off boats or even while scuba diving. If you’ve ever lost something in the water and wanted to go find it, then the PADI Search and Recovery Diver Specialty course is for you. There are effective ways to search for objects underwater that increase your chances of success. And there are good and better methods to bring up small, large or just awkward items. Search and recovery can be challenging, but a whole lot of fun.\nPADI Self Reliant Diver - £200\nThe PADI Self Reliant Diver Specialty course introduces you to basic concepts of self sufficiency and redundancy underwater. You will discover the special considerations and planning that are needed to dive safely without buddy backup. This course will make you the ultimate buddy especially to yourself, increasing confidence and skill.\nWhether taking photographs or just drifting along on a current knowing that you have the skill and equipment to be perfectly self reliant is an amazing confidence boost. Every dive becomes relaxed.\nDuring your PADI Self Reliant Diver certification course, you learn about:\nPADI Sidemount Diver - £200 (not including equipment hire)\nHaving scuba tanks on your back isn’t a requirement for exploring the underwater world. Many scuba divers have discovered the joy of mounting cylinders on their sides. Sidemount diving gives you flexibility and streamlining options. Plus, you don’t have to walk with heavy cylinders on your back – just enter the water, clip them on and go. Sound interesting? Sign up for the PADI Sidemount Diver Specialty course.\nPADI SMB - £99\nThe SMB Diver course covers the knowledge and techniques required for the safe use of Surface Marker Buoys and delayed Surface Marker Buoys. We will also look at the use of reels within diving. The course involves a classroom presentation and discussion and two open water dives.\nPADI Underwater Navigation - £149\nBe the scuba diver everyone wants to follow because you know where you are and where you’re going. The PADI Underwater Navigator course fine-tunes your observation skills and teaches you to more accurately use your compass underwater. If you like challenges with big rewards, take this course and have fun finding your way.\nPADI Wreck Diver - £149\nWhether purpose-sunk as an artificial reef for scuba divers, or lost as the result of an accident, wrecks are fascinating windows to the past. Ships, airplanes and even cars are fascinating to explore and usually teem with aquatic life. Each wreck dive offers a chance for discovery, potentially unlocking a mystery or spying something others have missed. The PADI Wreck Diver Specialty course is popular because it offers rewarding adventures while observing responsible wreck diving practices.\nProject Aware - £49\nThe Project AWARE Specialty course is an introduction to Project AWARE as a global movement for ocean protection. It focuses and expands on the 10 Tips for Divers to Protect the Ocean Planet to provide guidance and help you make a difference for ocean protection every time you dive, travel and more. Remaining as a dry course it is ideal to engage divers and non-divers alike as well as a perfect solution to those bad weather days.\nProject Aware - Coral Reef Conservation - £49\nEveryone likes to scuba dive or snorkel in warm, clear water on a vibrant coral reef, yet many people know little about what they’re seeing or the importance of reef ecosystems. The Coral Reef Conservation Specialty course helps you appreciate the complexity of these habitats and teaches you how you can help conserve these vital systems.\nAnyone who has an interest in the aquatic world can take this course. There are no prerequisites or age restrictions and no water sessions are required to earn this non-diving certification.""]"	['<urn:uuid:68c5b8cf-5577-456c-9bfe-8416118da5b1>', '<urn:uuid:5f09eb6f-870a-424f-9cd4-c76170002d35>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T18:38:36.516973	8	74	3365
28	amount urine normal person per day	The human body excretes approximately 0.5 to 2 Liters of urine every day.	['The urinary tract is a part of human excretory system which is one of the most important systems of human body. The urinary tract consists of a pair of kidney, ureters, urinary bladder and the urethra. The urinary system performs the removal of body waste along with maintenance of blood volume. It maintains the blood pressure by regulating the body metabolite pharmacokinetics, blood pH and electrolyte balance. The kidneys have rich arterial network and are composed of functional units called nephrons. These nephrons perform the filtration of renal blood and absorb the essential ingredients from the renal blood like water content and sodium, excreting the fluid containing wastes termed as urine. The urine is then pushed through the ureters (urinary tubes) and gets stored in the urinary bladder. During micturition (an act of passing urine), the muscles of the bladder contract to void the bladder and expel the urine through the muscular opening termed as urethra. In females, the urethra is relatively shorter than males.\nAny infection in the urinary tract is termed as urinary tract infection. It is the most common infection in human body after respiratory infections. The infection can involve any part of the urinary system like ureters, bladder or urethra; however, it commonly occurs in urethra and urinary bladder as they are anatomically located towards the lower part of the tract. The upper urinary tract infection is relatively less common than the lower urinary tract infection and appears to be more severe. Human body excretes approximately 0.5 to 2 Liters of urine every day.\nHow Long Does UTI Last?\nThe duration of UTI or urinary tract infection depends on the part of urinary tract involved. The upper UTI or upper urinary tract infection takes more time to resolve post the treatment or may extend up to weeks depending upon the severity; however, the lower UTI may subside in 24-42 hours by antibiotics. Recurrence of UTI may also lead to increased duration. In children, the UTI is seen to last for 10-12 days.\nEtiology of Urinary Tract Infection\nMost commonly UTI is caused by microbes that are not visible to the naked eyes and are only seen under microscope. Mostly the infections are due to bacteria; however, it can be caused by fungus or virus as well. The most common bacteria that affect the urinary tract are Escherichia coli.\n- A sexual intercourse can also lead to UTI.\n- Ongoing blood stream infection, kidney disorder or conditions like kidney stones, prolonged hospitalization making patient prone to infections, decreased immunity, use of public toilets, poor genital hygiene, spinal cord injury causing difficulty in bladder emptying, menopause, are some of the causes.\n- Other risk factors are diabetes, obesity, family history, anatomical structure locations of human body parts.\n- Urinary infection is more sever in children as compared to the adults and is more frequent in females.\nHow To Know If You Have UTI?\nThe symptoms of urinary infection consist of fever (101F) or chills (Malaise), weakness, weight loss, vaginal pain, lower back pain, abdomen pain; however, some of the symptoms depend on the location of the infected area.\nSymptoms of lower UTI are:\n- Increased urinating frequency however the amount of urine is decreased (the amount of urine excreted is relatively less as compared to the increased frequency)\n- Burning sensation during urine passing\n- Increased micturition\n- Foul smell in urine\n- Urine appears more cloudy and dark in color\n- Occurrence of blood in the urine\n- Inflammation in bladder lining.\nThe upper UTI involves kidney and are more severe in nature. These infections can lead to complication that may be life threatening as the bacteria can enter in the blood stream via infected kidneys. Also, incompletely treated lower urinary infection can also cascade to the upper part of urinary tract. This condition is termed as Urosepsis.\nSome associated symptoms of Upper UTI are:\n- Severe micturition\n- Severe burning sensation during urination\n- Incomplete bladder voiding feeling\n- Pain in lower abdomen\n- Pain in lower back\n- Thick urine consistency\n- Nausea, vomiting\n- Annoyed behavior\n- Difficulty in breathing\n- Irregular heart rate\n- Irregular body temperature\n- Increased rate of breathing.\nAn untreated urosepsis can lead to the fatal consequences like shock.\nManagement of UTI\n- Antibiotics are the first line of treatment. Commonly used medications are Amoxicillin, Bactrim, and Ciprofloxacin.\n- Patients with prostate involvement require 4-5 weeks antibiotics whereas females with infected kidney may need 5-7 days regimen.\n- IV fluid and antibiotic infusion is given to patients with severe infection followed by oral medications for a week or two.\n- Pain medication to avoid burning sensation during urination is also prescribed.\n- If infection is fatal, hospitalization and continuous monitoring is recommended.\n- Drinking plenty of water is advisable to stay well hydrated.\n- It is recommended to avoid beverages like coffee, tea. One must have cranberry juice is as a treatment for UTI.\n- Following less spices diet helps reduce the bladder irritation and burning sensation.\n- Maintenance of personal hygiene, especially during menstruation is advisable.\n- Urination with complete bladder voiding is recommended post sexual intercourse.\nRegular follow up visit with the urologist and repeat urinalysis is recommended for better prognosis.\n- Cystitis or Urinary Tract Infection (UTI): Symptoms, Treatment, Home Remedies\n- Common Urinary Tract Infections (UTI) & Ways to Prevent it\n- Can you get Trichomoniasis from a UTI?']	['<urn:uuid:995dd781-440b-4c89-b77b-3992f0cc9e13>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	6	13	904
29	how much extra money garden bench powder coating	A powder-coated garden bench typically costs $50 to $100 more than a regular steel garden bench. For example, if an ordinary steel garden bench costs $200, getting a powder-coated version would be $250-$300.	['Here we share our powder coated outdoor furniture design guide including what is, how long it lasts, its cost, how to clean, ways to paint, and how to restore it. Enhancing the aesthetics of outdoor furniture can be challenging for first-time homeowners. People can choose from different design finishing options, often depending on their objectives and budgetary concerns. One of the most exciting outdoor furniture finishing methods is powder coating.\nHigh quality and durable finish, powder coating can also give fixtures and furniture an elegance that no paint or other finishing process can provide. Learn more about this process to determine if powder-coated outdoor furniture is right for you or not.\nWhat is Powder Coated Outdoor Furniture?\nPowder-coated outdoor furniture is any piece of metal furniture that receives a fresh coating of dry powder to improve its aesthetics while ensuring a longer-lasting finish than conventional paints.\nBenches, garden chairs, sofas, tables, bistro sets, gazebo furniture, and other pieces of outdoor furniture will look more awesome after an application of premium-quality powder coating.\nPowder coatings are a combination of ultra-strong polymer resins, curatives, leveling agents, pigments, and flow modifiers. Manufacturers melt-mix these ingredients, cool them, and ground them to a powdery consistency.\nFurniture manufacturers apply the powder coating using an electrostatic spray deposition method, allowing the powder molecules to bond with metal components during curing.\nThe result is long molecular chains with exceptional density and strength. This process allows the powder-coated furniture to last longer than painted pieces of furniture.\nHow Long Does Powder Coating Last?\nThe unique formulation of powder coating gives it exceptional durability that can last up to two decades. It has remarkable resistance against weather, corrosion, and chemicals, making powder coating suitable for outdoor applications.\nHomeowners will never worry about their outdoor furniture losing its color because it will take prolonged exposure to extreme weather conditions to put a dent into the powder coat.\nUnfortunately, one must apply the powder coating correctly to ensure stable and permanent adhesion of the powder molecules on the furniture’s surface. Urethanes and fluoropolymers offer unparalleled longevity for powder coatings.\nPowder Coated Outdoor Furniture Cost\nA piece of powder-coated outdoor furniture can cost homeowners several tens to a few hundreds of dollars more than the average price of the furniture.\nFor example, if an ordinary steel garden bench costs $200, getting a powder-coated garden bench can be pricier by $50 to $100.\nOne must remember the unique process of making a powder-coated piece of furniture, not to mention the method’s durable nature. The more layers of powder coating, the more expensive the outdoor furniture is.\nHow Much Does It Cost to Powder Coat Patio Furniture?\nBecause of the enduring elegance and protection of powder coating, many homeowners now choose to powder coat almost every piece of furniture they have. This finishing process is not only applicable for the outdoors. It can also be an exceptional method indoors.\nSo, how much will it cost the average homeowner to powder coat his patio furniture?\nThe answer depends on the type of powder coat one wants to use. The amount of powder coating and the relative difficulty of application are also essential considerations.\nAcross the US, homeowners can expect to pay around $200 for every piece of furniture. Some areas may charge lower or higher, putting the price range to within $100 and $350 per furniture.\nIt should not surprise homeowners that luxury accommodations require the best, jacking up their per-furniture cost to $900 or more.\nDoes Powder Coated Steel Rust?\nPowder coatings provide metal surfaces with impervious armor against the elements. When applied correctly, complete with the crucial preparatory processes, a powder-coated surface will never rust. That is why many resorts and beachfront establishments choose to powder coat their furniture pieces.\nWhile powder-coated furniture pieces are impervious to corrosion, they are not immune to surface damage. Scratches, scuff marks, and nicks can still occur.\nWhen these imperfections develop, they create a point for moisture to enter and potentially lead to rust formation.\nCan You Paint Over Powder Coat?\nWhile powder coatings last many years, they can still suffer from damage, including abrasions, cuts, scratches, and others. Sometimes, people also want to give their furniture a fresh look.\nIt is possible to apply a coat of paint over the powder-coated surface, provided one observes the right surface preparation and uses only the correct materials.\nSurface preparation is crucial because powder-coated surfaces have a flat and smooth finish. Paint molecules will not be able to adhere or bind to such a surface. That is why one must sand the surface to give paint molecules something to hold onto.\nHow Do You Restore Powder Coated Outdoor Furniture?\nProlonged exposure to the elements can shorten a powder coating’s lifespan, causing it to fade. Restoring it can be challenging because it requires careful surface preparation.\nThe first step to restoring one’s powder-coated outdoor furniture is removing surface contaminants with a degreaser before cleaning it with the correct cleaning agent. Sanding the surface creates a framework for powder molecules to adhere to, allowing homeowners to restore their outdoor furniture.\nA specialist acrylic powder can restore the coating, ensuring the application of multiple coats to achieve a stunning finish.\nHow to Clean Powder Coated Aluminum Patio Furniture?\nRegular cleaning of powder-coated aluminum patio furniture can extend its aesthetic and protective qualities. Start by preparing a warm soapy solution, using a mild dishwashing liquid to remove grime and dust from the surface.\nAlways use a lint-free cloth to apply and gently rub the soapy solution on the powder-coated surface. If the furniture comes with a glass top, it will be wise to use a glass cleaner. You should ensure none of it gets onto the powder coating.\nFinish the cleaning process with a cold water rinse before drying the furniture completely. If one notices residues on the surface, a soft sponge is ideal instead of a lint-free rag.\nPowder coated outdoor furniture can give patios, gardens, gazebos, and porches an elegant look that last a few decades. Not only does it beautify one’s landscape, but powder coating can also extend the lifespan of one’s furniture because of its unique surface bonding capabilities.\nHaving one’s outdoor furniture may look like an expensive venture. If one can extend the fantastic finish by twenty years, powder coating will still turn out a more cost-effective finishing option.\nSee more related content at our article about maintenance-free outdoor furniture on this page.']	['<urn:uuid:2c783e71-0f8b-4ff5-9c9b-9e840f03aae1>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	8	33	1075
30	winter landscape contrast color composition techniques	To create compelling winter landscape photos, look for contrast and color elements like exposed greenery or colorful rooftops to avoid featureless white images. Place the horizon high in the frame with the scenery as the focal point. Using color accents, particularly bright colors like red and orange, can create high-impact compositions. Additionally, shooting in different conditions and times of day can dramatically change the mood and create diverse winter photos.	"[""Winter is here again, and with it come fresh opportunities to take beautiful photos. If you’ve always wanted to capture the majesty of snow-covered landscapes, keep reading for these simple photography tips.\n01. Keep your equipment safe\nThe cold temperatures of winter can be harsh on your equipment. Since snow can easily wet your gear, consider buying a waterproof bag to keep everything safe and dry.\nA drastic change in temperature can also cause lens fogging—even when your camera is in your bag. To avoid this, place it inside a ziplock bag, and make sure to remove most of the air before sealing it. Doing so prevents moisture from getting in and causing condensation. If you want to be extra-careful, you can even place silica gels in the ziplock bag to suck out the remaining water particles.\n02. Bring extra batteries\nBatteries lose power quickly when it’s cold. One way to help solve this problem is to place a hand warmer near where you keep your batteries. Doing so keeps them warm and prevents them from running out of juice too quickly.\nHowever, even if you keep your batteries warm in your bag, they’ll still start losing power as soon as you take them out into the cold. The most practical solution for this problem is to pack more than you think you’ll need.\n03. Use exposure compensation\nFor the most part, exposure meters in modern cameras are highly accurate. But even high-end meters can get confused when it comes to photographing a scene that’s mostly white. That’s why when you try to capture snow in Auto Mode, it often looks greyish, rather than pure white.\nSo, how do you make sure the snow looks white in your images? The simple answer is to use exposure compensation (the button with a +/- sign in most cameras). This increases or decreases the actual exposure value of your image by 1 to 2 units.\n04. Snap on a polarizing filter\nWhen you go out in the snow, sometimes the glare is so bright that you need sunglasses to see properly. That’s more or less what a polarizing filter does to your camera sensor; it allows it to capture a scene much more clearly.\nPolarizing filters come in different sizes, so if you’re thinking of buying one, always make sure that it matches your lens' diameter.\nTo use the polarizing filter, simply twist it until you get rid of the glare and achieve vivid colors. Just make sure that you don’t shoot directly in front of the sun, otherwise, this could diminish the effect of the filter and produce weird colors.\n05. Use a small aperture\nTo make sure everything in your photo is sharp, stick to a small aperture (somewhere between f/8 to about f/22). Since these values have a large depth of field, everything from the foreground to the background will be in focus.\nIn most situations, you don’t need to go beyond f/16. Using the narrowest aperture, such as f/22, can sometimes create blurry results due to diffraction. In other words, when the lens opening is too small, it bends the light waves so much that it creates distortion.\nTo keep things simple, use Aperture Priority and let your camera adjust the shutter speed for you.\n06. Prioritize composition\nBefore you press the shutter, think like Ansel Adams and visualize what you want to see in your final photo. Would you like to include the mountains in the background, or do you want to keep them out of the frame? Whatever you choose, make sure that’s exactly what you see in your viewfinder.\nTo emphasize the snowy landscape, place the horizon high in the frame. Remember that the sky should help enhance the mood of your image, but not dominate it. In the end, the scenery should still take center stage.\n07. Look for color and contrast\nSometimes, snow turns everything so white that it’s hard to distinguish the different elements in the landscape. If you don’t want to end up with featureless images, train yourself to start looking for contrast. It could be anything from exposed greenery to colorful rooftops of houses.\nThe point of adding color and contrast to your images is to give your viewers a point of focus. If everything looks too white, it could take a while for people to figure out what they’re looking at. And if you don’t grab their attention in an instant, there’s a good chance that they'll quickly lose interest in your photos.\n08. Shoot in different conditions\nThe mood of a landscape can change dramatically throughout the day. To evoke different emotions in your images, try capturing sceneries at various times of day and in different weather conditions. Ideally, you’d want to take photos when there’s fresh snow, but that doesn’t mean you should only wait for this one moment before you start clicking the shutter.\nYou’ll be surprised at how scenic a place can appear if you catch it before sunrise. Similarly, capturing a landscape when it’s overcast provides a completely different atmosphere than when the sun is out. Challenge yourself and learn how to take photographs no matter the condition. You'll be rewarded with a beautifully diverse collection of winter photos.\nWinter is such a magical season for going out and shooting winter landscapes. Of course, taking these types of photos can require some technical skills. For the most part, however, it’s all a matter of preparation. Once you're sufficiently prepped for the outdoors, you'll be ready to grab your camera and start exploring."", 'Photography has a lot of bitter-and-sweet challenges; and one of them is shooting white stuff properly. By properly we mean with the appropriate tonal range that enables winter landscapes to look rich in texture and full of character.\nHere you’ll find some valuable information for you to make the most out of your winter landscape photography, as well as some nice winter photography ideas and tips and one simple yet challenging assignment on abstract winter landscape imagery.\nYour Camera Doesn’t Like Bright Stuff\nCamera manufacturers have made a huge effort for developing precise tools for measuring light. The problem is that these all aim to achieve perfection, and when it comes to bright stuff, your camera will try to dim down the reading.\nIn photography there is something usually known as “exposing to the right”, and that is the habit of slightly overexposing light in a photograph. This is an absolutely common practice when it comes to winter landscape photographs, and here is how it works:\nWhen you read light with your camera’s built-in light meter, you’ll see that it will make a very conservative reading of the light in front of it. This means that after shooting a centered exposure you’ll notice that the white areas actually look gray, and even a bit dull. Here is when the overexposure technique comes into play. There is no exact science and your decisions will be driven but the amount of brightness in front of you. But you can overexpose +2/3 of a stop to even 2 stops, again, that will depend on you.\nThe reason why is so important to expose to the right while making snow landscape photos is because it is easier for your sensor to recover information from bright zones rather than from the darks and shadows. Therefore, you’ll be compensating all the blown highlights in the raw development phase of your workflow.\nThis results in properly lit images with information across the whole frame. From shadows to lights, you’ll have rich texture, giving that highly desired dramatic punch from fine art landscape photographs.\nWhite isn’t That White\nWhen it comes to winter landscape photographs, white will be all over the place, and you’ll highly notice something odd. Your photos will have warm or cold color casts that doesn’t really resemble perfect whiteness.\nThis can be corrected by using the white balance setting on your camera. If you go out there shooting in the early hours of the day or during sunset, you’ll likely get some blue casts, and if you shoot during the regular hours of the day, you might get some warm tones as well.\nFor these situations in which you start getting too blue casts, make sure to compensate with “cloudy” or “shade” white balance modes. That will give you a better result in camera. You can also set your camera’s white balance mode to “auto”, but mind that this will give you inconsistent results as the sun or clouds move around you. Of course, the best way to compensate white balance is during the raw development stage of the photographer’s workflow, but there’s nothing wrong with giving well-crafted inputs to it.\nBeyond that, winter landscape photography shares the same principles of regular landscape imagery. Therefore, scouting, tripods and ND filters is mandatory. Well, that is enough about technicalities, let’s get some fun with these winter photo ideas:\nGet a Different Perspective by Using a Drone\nDespite being so popular, drones are still offering extremely unique ways of seeing life around us. When it comes to winter landscapes colors get sort of muted out, making everything closer to graphic design or painting. Even the most common objects around us look compelling from a higher point of view.\nSome photographers like the bird-like vantage point they get with drones, and some others like shooting straight down to the ground. No matter which one is your favorite, you’ll get a more compelling approach towards winter photography with these amazing tools. Oh, and moving around with snow is harder, so kudos to drones that enable us to move quicker around the woods.\nStart Using Color Accents\nAs said before, winter photography offers a lot of graphic-design-like layouts. This happens because there is always a strong contrast between both extremely bright and dark zones in the frame. This is notoriously good looking; but sometimes things can get a bit more interesting when a colored accent pop’s inside the frame. And we are talking about bright colors like red and orange. Take that into account and your compositions will have huge impact from here on!\nHigh-Keys are BAE\nHigh-key exposures always give a dream like nature to photographs, but they usually are better executed during controlled lighting situations. That obviously means studio photography, but we nature lovers hate being inside a studio. Snow photography will give you absolute freedom for high-key images, just mind the exposure compensation we mentioned before.\nWhat About that Assignment?\nAfter these brief yet insightful winter photography tips and ideas you’ll be better known for creating some beautiful photographs. In order to think a bit outside the box, we invite you to seek some nice abstractions around you.\nYou probably won’t find using a telephoto lens in a regular landscape photo ideas blog post elsewhere, but using a long lens is the best way for spotting abstract images in nature.\nThe important thing about Abstract photos is that they don’t totally reveal the complete nature of an element, while still keeping the power of telling enough about the element.\nWrapping it Up!\nRemember that cameras will always try to achieve perfection, but that doesn’t necessarily mean interesting or creative photographs. Any photograph created with purpose should not only compel the audience, but be able to reflect the photographer’s vision of the world. We do hope that this post might be able to trigger some new talented photographers out there; and don’t expect that winter landscapes will come to you; you need to go out there and discover them by yourselves!']"	['<urn:uuid:6df4969d-e4d4-4c2d-be42-ec93743c702e>', '<urn:uuid:af5d5c11-61b6-4675-8774-a2b73c7f7c60>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-12T18:38:36.516973	6	70	1931
31	military career organizations helping veterans employment mental health assistance	Organizations like Nate Boyer's Merging Vets & Players pairs transitioning veterans with pro football players to assist with their transition, while the VA provides comprehensive support including counseling, educational support, and vocational rehabilitation. Veterans face both employment challenges, with 65-80% leaving without jobs, and social difficulties, with 61-68% reporting adjustment issues, requiring both career services and mental health support for successful reintegration.	"['10 questions only a veteran would ask Green Beret-turned-football legend Nate Boyer\nThe former Seahawks player shares his go-to MRE, favorite war film and what he learned in the Army that helped in football.\nNate Boyer has had a hell of a career — several in fact. He’s been a Green Beret, a professional football player, philanthropist and veterans advocate, actor and recently he’s added “director” to that list.\nAfter working with refugees in Sudan, he joined the Army in 2005. He ended up part of Special Forces the next year and deployed with Operational Detachment Alpha 0324, 10th Special Forces Group to Iraq in April 2008. That deployment lasted until January 2009 and was followed by another tour, this time to Afghanistan from April to August 2013 with Special Operations Joint Task Force-Afghanistan. From April to August the following year he deployed with ODA 3116 3rd Special Forces Group. Boyer also took part in training missions around the globe, from Israel in 2009 to Bulgaria in 2011 and Greece in 2012.\nAfter leaving active duty in 2009, Boyer joined the National Guard and enrolled at the University of Texas. There he joined the football team as a walk-on with no prior football experience. He ended up a long snapper and won the Armed Forces Merit Award. He ended up going pro, signing with the Seattle Seahawks in 2015 as a free agent.\nSubscribe to Task & Purpose Today. Get the latest military news, entertainment, and gear in your inbox daily.\nSince leaving the NFL, Boyer has been involved in philanthropy. His organization, Merging Vets & Players, pairs transitioning veterans with pro football players and athletes, many of them also transitioning to life outside of their careers in uniform.\nAnd now, the 41-year-old is months away from his first directorial debut, MVP, which tells the story of a recently retired NFL player and that of a homeless veteran suffering from PTSD, played by Boyer, as the two try to find their way apart from a career, a community, and a way of life, that came to define them.\nTask & Purpose had a chance to talk with Boyer ahead of this year’s Army-Navy football game in Philadelphia, Pennsylvania, and took the opportunity to ask him about the essentials of military life.\nHere are 10 questions only a veteran would ask Nate Boyer:\nTask & Purpose: What’s your favorite war film and why?\nNate Boyer: Saving Private Ryan. Just the brotherhood, the camaraderie, the banter between the guys beyond all the war stuff I just felt like it’s so genuine.\nWhat’s your go-to MRE recipe?\nGo-to MRE is gonna be chicken with noodles especially because you’ve got the jalapeno cheese sauce, plus you’ve also got the peanut butter M&Ms — very crucial.\nRip Its, yes or no?\nYes. Orange, sugar-free. Let’s go!\nThe zombie apocalypse kicks off. What’s the first thing you do?\nWell, I’m a Green Beret, so obviously hearts and minds, you know what I mean. I gotta get in with the people or the zombies or whatever and just really understand where they’re coming from and try to win that rapport.\nWhat’s the dumbest thing you ever did as a junior soldier?\nI do not want to dispel that information. That one stays with me to the grave, but it was dumb, but there’s a lot of dumb stuff. We’ve all done it, but I’ll leave it there.\nFinish this sentence: “You shouldn’t join the military if…”\nYou shouldn’t join the military if you think you’re gonna be in Delta Force.\nIf you could go back and talk to yourself at the recruiter’s office, what would you say?\nI would say “do it.” I would say “it’s going to be nothing like you expect.” I would say “think about your future while you’re in the present,” but I would say “sign that thing. Don’t think twice.”\nProudest military moment?\nProudest military moment was probably earning the Green Beret. You know what, I’m gonna change that answer. My proudest military moment was carrying my best friend’s casket, Brad Keys. Being a part of that, being asked to be a pallbearer in that funeral. I mean, he was my hero. He was the ultimate leader. He was my mentor and my brother.\nWhat’s one thing you learned in the Army that you carried with you in your football career?\nAim small, miss small. That’s the number one thing; attention to detail. That was the key. Block out the noise.\nWhat’s the most important thing to remember in a firefight?\nIn a firefight always remember to keep your finger off the trigger unless you intend to destroy what’s on the other end of that rifle barrel.\nThe latest on Task & Purpose\n- The king is dead: Why would America want to retire the F-22?\n- We salute the Marine Corps vet who chugged a beer from her prosthetic leg at a Lakers game\n- Combat medic ‘voluntold’ to attend Army Best Squad competition ends up dominating the range\n- Air Force F-35 maintainer shares what it’s like keeping ‘a flying computer’ ready to fight\n- Why modern technology hasn’t rendered trench warfare useless in Ukraine\nWant to write for Task & Purpose? Click here. Or check out the latest stories on our homepage.', ""Public understanding of the needs of military veterans has focused largely post-traumatic stress disorder, traumatic brain injuries, suicide rates, and poor conditions at the Walter Reed Army Medical Center.\nBut the great majority of the veterans of post-9/11 wars need social services that will help them transition back to civilian life.\nBetween 65 percent and 80 percent of veterans surveyed between 2014 and 2016 left the military without a job.\nThat is the assessment of the latest study by the Costs of War project based at Brown University’s Watson Institute for International and Public Affairs, which uses research to explore the human, economic, and political costs of the post-9/11 wars in Iraq and Afghanistan and the related violence in Pakistan and Syria.\nThe new study, which focuses on the difficulties the newest generation of post-combat veterans face as they reintegrate to civilian life, reveals patterns of need and reviews the US Department of Veterans Affairs response to those needs.\n“From changing residences and finding jobs to securing educational and vocational training and settling back in with family, veterans face post-war difficulties that are costs of war not easily captured in health care spending totals or in the epidemiology of combat-related health problems,” says Anna Zogas, a researcher at the University of Washington.\nNavigate a daily routine\nThe research shows that the VA, the nation’s largest integrated health care system, changed its policies to better assist veterans and more than doubled spending between 2002 and 2015 on programs related to employment and education, two top areas of need according to veterans.\nThe non-medical services provided in tandem with health care are key to helping veterans leave the “disciplined, rigid, high-stakes environment” of the military and reintegrate with family, find work, pursue education, and navigate a daily routine that is no longer predictable, orderly, and mission-focused, Zogas says.\n“Distinguishing strained social relations from specific medical problems is important because how we define problems shapes our efforts to solve them,” Zogas writes in the study.\nEnlisted service members who fought in post-9/11 wars have been leaving the military at a rate of roughly 250,000 each year, and the Department of Defense estimates that this rate will remain at 230,000 to 245,000 per year through 2019.\nThe difficulties veterans have in easing back into civilian life correlate with age, education level, and work experience—many of these veterans are young and lack a college degree. In 2008, the year in which the highest concentration of US troops deployed overseas, 41 percent of service members were 25 years old or younger, and only 18 percent had a bachelor’s degree.\nThis means that hundreds of thousands of young post-combat veterans leave the military each year, many of them without established careers to which they can return. Those who enter a college or vocational program may be older than traditional students and may have families to support.\nBetween 65 percent and 80 percent of veterans surveyed between 2014 and 2016 left the military without a job. Case managers of a career services program suggest veterans have unrealistic expectations of employment options and salaries in the civilian job market. Although they are highly motivated to work hard and move up corporate hierarchies, veterans are frequently frustrated by having to start in low-paying, entry-level positions and many report feeling like they were starting over completely.\nYoung veterans also report social difficulties with greater frequency than the rate at which they are diagnosed with specific mental health concerns. In 2008, 96 percent of a group of post-9/11 combat veterans surveyed reported that they were interested in receiving services to ease “community reintegration problems,” even though they were already using VA primary care or mental health services.\nMore recently, in studies published between 2014 and 2016, veterans of the post-9/11 wars reported adjustment difficulties at rates between 61 percent and 68 percent. Even without the most troubling health issues, such as traumatic brain injuries or PTSD, many post-combat veterans contend with physical and mental health concerns—from musculoskeletal ailments to mental disorders, insomnia and headache—that can make reintegrating with family and succeeding in school and work difficult.\nBefore 2008, veterans were required to secure official recognition of combat-related disabilities before they could receive free medical treatment. That year, the VA streamlined the process and allowed all veterans to start receiving free care immediately.\nThat change cut down on red tape, Zogas says, making it possible for veterans transitioning into civilian life to receive supportive services—from group to individual counseling and educational support—without a prior medical diagnosis. It also accounts for the possibility that some combat-related symptoms take months to fully manifest, which signals a new responsiveness to the need for services that aid veterans in adjusting to civilian life.\nIn addition to ensuring that veterans are more easily able to access medical care, the VA increased spending on programs related to employment and education from 3.6 percent of total spending in 2002 to 8 percent by 2015, according to the study.\nVA services for recent veterans include training in study skills, strategies for improved sleep, financial management, parenting, and services that help reduce veterans’ feelings of social isolation, such as outdoor activities organized by recreational therapists.\n“In some cases, this support takes shape through formal programming, such as parenting classes run by psychologists inside VA clinics,” Zogas writes, or classes that help veterans prepare to participate in college classes. Zogas cited “an eight-week course, wholly designed and taught by VA therapists in a VA clinic, in which a class of post-9/11 veterans sat together in a conference room for two hours a week, learning about cognition and memory in a way that mimics a college classroom.”\nThese kinds of programs can help veterans prepare to acclimate to college classrooms that will likely be populated by 18- to 22-year-olds.\n“In a college or university, the concerns of traditionally aged students can seem trivial to people with combat experience, making it difficult for veterans to relate to their peers,” Zogas writes. “Less than 0.5 percent of the country’s population is serving in the military at any time, and post-9/11 veterans are a minority among their peers and families.”\nThe VA also provides vocational rehabilitation, including support for apprenticeships and on-the-job training, post-secondary training at a college, technical or business school, and supportive case management.\nVA spending on rehabilitation and investment in veterans’ education and job training is not a cost of war in the strictly financial sense but that it represents an effort to address the day-to-day difficulties veterans returning to civilian life face.\nIn addition to formal vocational rehabilitation programs and the GI Bill, VA clinicians provide services tailored to young veterans’ non-medical needs within the relative isolation of their own clinics. This makes it almost impossible to gain a top-down perspective on how the institution and its operations help veterans transition back to civilian life.\n‘The VA has moved toward institutionalizing meaningful support for ‘transitions,'” Zogas says. “If recent projects under VA Secretary Robert A. McDonald’s strategy for improving the organization materialize into formally organized, meaningful, sufficiently funded and sufficiently researched programs to assist veterans in their transitions, it would be a step toward taking veterans’ own stated post-military social, educational and employment needs as seriously as the military takes their training.”\nSource: Brown University""]"	['<urn:uuid:a960ee20-e8b4-49d4-93b4-69b3686b2776>', '<urn:uuid:ab580052-97c9-4f04-a196-24ca5742776c>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T18:38:36.516973	9	62	2095
32	Could you describe the practical steps and home preparations that would be helpful for someone to take before undergoing mastectomy surgery?	It's important to organize your home before surgery since recovery takes time and arm movement will be restricted, especially if lymph nodes were removed. You should ensure frequently used items are easily reachable, do a large grocery shop, and stock the freezer with ready meals. It's advised to prepare a capsule wardrobe of soft, comfortable clothes that are easy to put on, preferably made from natural fabrics like cotton, silk or bamboo, avoiding clothes with back-fastenings, tight sleeves, or too many buttons. It's also recommended to gather entertainment materials like films, box sets, and magazines since concentration might be challenging in the first few weeks after surgery.	['Planing ahead for life following mastectomy surgery can make things easier and help your recovery. Here are some practical tips...\nThere’s no easy stage in what is sometimes called the cancer journey; every stage, from pre-diagnosis onwards brings its particular challenges. Initial tests make you apprehensive; a confirmed diagnosis puts you into shock and then, before you know it, you’re on the hamster wheel of treatment, whatever your treatment involves. You’re trying to get your head round so many things, including telling loved ones, telling your employer, and finding out as much as you can about what to expect and how best to take care of yourself during treatment and beyond.\nIf you have been diagnosed with breast cancer and have been advised that you will need a mastectomy of any kind, you may also tip over into a form of grief. A mastectomy, like any radical surgery, is going to alter your appearance in what will seem like a life-changing way. The prospect can be overwhelming even for the most positive person and even if your prognosis for recovery is good.\nWe’ve put together a roundup of things to be aware of, or think about, that we hope will reassure you and make preparing for life after a mastectomy just that little bit easier.\nTalk to your breast cancer specialist nurse\nYou’ll almost certainly be feeling anxious about your mastectomy so it’s a good idea to make a list of your questions and go through them with your breast cancer nurse before surgery.\nThey’re very understanding, as well as knowledgeable, and no matter how odd or embarrassing you think a question might be, the chances are that someone else will have asked the same question before. You can talk to your breast care nurse about every aspect and at every stage of your treatment, and after treatment is finished. Think of your nurse as your best friend.\nPreparing for early post-mastectomy days\nIt takes time to recover from a mastectomy and if you’ve had some or all of the lymph nodes in your armpit removed, you’ll find that your range of arm and shoulder movement is much more restricted. You shouldn’t and won’t be able to do any heavy lifting, so it’s worth spending some time before surgery organising things at home to make life easier, for example, make sure that anything you need to use daily or frequently is easy to reach.\nFill the fridge and freezer – and eat well\nDo a big shop before you go into hospital and make sure that you have a good supply of your favourite foods. You may not feel like eating much but having good, easily prepared food that you enjoy makes such a difference and will help your recovery. Fill the freezer with nourishing ready meals, either that you have made beforehand or from your favourite frozen food range. If you live alone or have no-one else who is able to do food shopping, opt for home deliveries, such as a weekly fruit and veg box, for the first few weeks after surgery.\nWhat to wear – post-surgery clothes\nGo through all your clothes and put together a capsule wardrobe of soft, comfortable items that you can easily slip your arms into, or step into after surgery. Natural fabrics like cotton silk or bamboo are ideal and have some warm wraps or shawls and one or two throws or blankets so that you don’t get chilly while you are resting. Try to avoid clothes with back-fastenings, tight sleeves, or too many buttons.\nIndulge yourself with films, box sets and magazines!\nYou’ll be resting more than usual in the first few weeks after surgery so why not use the time to catch up on all those box sets or films you’ve wanted to watch? It’s a good idea to put together a list and order them beforehand. Most larger libraries also have a good DVD selection of films and television series, so call into your nearest library to see what’s on offer. Many people find that concentration can be a challenge for the first few weeks after surgery and even the keen readers can find it hard to get through a book, so magazines are ideal.\nPlanning for the long-term after a mastectomy\nFeeling good and learning to accept the way you look\nA mastectomy alters your appearance and posture and this can easily affect your self-confidence. Everyone reacts in their own way so don’t be hard on yourself if you are struggling to accept the changes in the way you look. It’s generally agreed that the sooner you feel ready to look at yourself in the mirror, the sooner you become used to the new you.\nIf you have a partner, encourage them to be as brave as you, as it is an important first step towards reconnecting and rebuilding intimacy. Breast Cancer Care has some excellent advice on how to do this step by step.\nBreast prostheses and nipples\nIf you are not having breast reconstruction during your mastectomy or you are having reconstruction at a later stage, your breast care nurse may recommend using an extra gentle breast prosthesis (artificial breast form) like this one.\nA prosthesis sits neatly inside your bra and will help to give your breast a more even look – so wearing one could help to restore your sense of self-esteem too.\nIf you have had a nipple removed during surgery, you can also use an easy-on, easy-off self-adhesive artificial nipple for a more natural look.\nWhat to wear post-surgery - bras, swimwear and activewear\nDo ask your breast care nurse for advice on how to choose a post-mastectomy bra. It’s important to have a style that fits well so if you have a local lingerie store, a department store with a lingerie section, that offers a bra fitting service, you might want to go along for an initial fitting.\nPost-surgery bras now come in a range of styles, including compression bras, which help reduce swelling, irritation and lymphoedema, front-fastening bras which are ideal if you have limited arm and shoulder mobility, and non-wired tankini tops.\nIf you have to use a drain post-surgery, there’s an innovative bra and robe combination called a Brobe. As well as a specially designed front-fastening bra, it has pockets to hold drains and easy access sleeves.\nIf you’re planning a holiday or are a keen swimmer look for post-mastectomy swimwear. The best styles are cut higher under the arm for security, have inbuilt pockets for breast forms, and special cup shaping that holds the breast form close to the chests.\nAnd once you are ready to return to exercise, whether it’s yoga, Pilates, circuit training or running, for example, do make sure you wear a post-surgery sports bra. Look for one that gives you extra support, is moisture-wicking, wire free and seamless.\nMastectomy tattoos - Make a feature of the new you\nPost-mastectomy tattoos are proving to be a popular third choice, an alternative to breast reconstruction or no reconstruction. If the idea appeals, you’ll need to wait to have them done until your mastectomy wound has completely healed and your immune system has recovered but that gives you time to think about and choose a tattoo design.\nKris Hallenga was diagnosed with stage 4 terminal breast cancer at 23 but, 10 years on, she is leading a busy active life: she founded CoppaFeel!, the breast cancer awareness charity for younger women. Three years ago, Kris decided to have a mastectomy tattoo; it features a tightrope artist, ‘…to represent the balance of any illness with life and everything else.’ You can read Kris’s story here.\nFind out more in our article on mastectomy tattoos here.\nOrganisations that offer breast cancer advice, information and support:\n- Live Better With articles on all aspects of mastectomy, including Preparing for a mastectomy: what to expect\nVisit the Live Better With Cancer Community Forum – for information, advice, and tips on getting ready for life after a mastectomy and to share your own questions and suggestions.']	['<urn:uuid:cb98fee7-f45d-417f-b733-6a398efcc2da>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	21	107	1345
33	How do membrane proteins interact with lipids, and how has sequencing technology evolved to study molecular structures?	Membrane protein interactions with lipids have been studied through electron crystallographic analysis of aquaporin-0, revealing nine DMPC molecules without specific binding sites, demonstrating non-specific protein-lipid interactions. The bilayer structure adapts to accommodate the hydrophobic belt of membrane proteins, with ongoing research into how different lipid characteristics affect these interactions. Meanwhile, sequencing technology has dramatically evolved - while the first human genome took 13 years to sequence, current technology can sequence an equivalent genome in just 24 minutes. This technological advancement has enabled deeper genomic insights and the ability to analyze vast amounts of molecular data, with sequencing capabilities now rivaling major data sources like YouTube and Twitter in terms of data volume.	['Department of Cell Biology\nMolecular electron microscopy (EM) is a versatile tool to analyze the structure of molecules that are not easily amenable to structure determination by the more established techniques of X-ray crystallography and NMR spectroscopy. Our group uses single-particle EM to visualize macromolecular complexes and electron crystallography of two-dimensional crystals to determine the structure of membrane proteins.\nWe are interested in understanding how membrane proteins interact with their surrounding, annual lipids, but direct structural information on the position and conformation of lipids surrounding membrane proteins has been hard to obtain. Much of our knowledge derives from crystal structures of membrane proteins with specifically bound lipid molecules. Our electron crystallographic analysis of aquaporin-0 (AQP0) reconstituted with the lipid 1,2-dimyristoyl-sn-glycero-3-phosphocholine (DMPC) into double-layered 2D crystals at 1.9-Å resolution revealed nine DMPC molecules (see the figure). Since AQP0 has no specific lipid binding sites and DMPC is not a native lipid, our structure shows a nonspecific mode of protein-lipid interactions. AQP0 forms 2D crystals with many different lipids, offering an unusual opportunity to ask fundamental questions about how lipids interact with membrane proteins. By visualizing the structure of AQP0 2D crystals obtained with lipids with different acyl chains and different headgroups, we hope to learn how the bilayer structure adapts to accommodate the hydrophobic belt of membrane proteins and which lipid characteristics are most important for interactions with membrane proteins.\nComplexes involved in vesicular transport are a second focus of our group. Eukaryotic cells have to moveproteins and other cellular components between their organelles. This intracellular trafficking is mediated by transport vesicles that bud off from the membrane of the donor compartment and dock at the membrane of the target compartment, with which they fuse in a process catalyzed by SNARE (soluble NSF [N-ethylmaleimide sensitive factor] attachment protein receptor) proteins. The individual steps in vesicular transport are catalyzed by a large collection of proteins that form elaborate complexes that, in turn, interact with each other and drive the process. We are currently focusing on the so-called “multisubunit tethering complexes” (MTCs), which play a central role in organizing the events that occur when a vesicle arrives at its target membrane. Through interactions with small GTPases, primarily those in the Rab family, as well as with vesicle coat proteins, and phospholipids, MTCs mediate the initial, reversible interaction of a transport vesicle with its target membrane. In addition, MTCs interact with SNARE and SM proteins, thus coupling vesicle capture to the membrane fusion machinery. MTCs can be subdivided into three groups: MTCs functioning in the secretory pathway (Dsl1, COG, GARP, and exocyst), MTCs of the endolysosomal pathway (HOPS and CORVET), and transport protein particle (TRAPP) complexes. MTCs in the secretory and endolysosomal pathways are Rab effectors and are thought to promote tethering by interacting with Rabs and SNAREs. In contrast, TRAPP complexes function as GEFs for the Rab GTPase Ypt1/Rab1 and combine tethering with coat recognition. To establish a basis for understanding how MTCs can orchestrate these diverse events and to obtain mechanistic insights into these processes, we use single-particle EM to establish the subunit organization of the various MTCs, to characterize their structural dynamics, and to visualize how they interact with Rabs, coat proteins and SNAREs.\nHite, R. K., Li, Z., and Walz, T. (2010) Principles of membrane protein interactions with annular lipids deduced from aquaporin-0 2D crystals. EMBO J. 29: 1652-1658.\nYip, C. K., Berscheminski, J., and Walz, T. (2010) Molecular architecture of the TRAPPII complex and implications for vesicle tethering. Nat. Struct. Mol. Biol. 17: 1298-1304.\nLees JA, Yip CK, Walz T, Hughson FM. (2010) Molecular organization of the COG vesicle tethering complex. Nat. Struct. Mol. Biol. 17: 1292-1297.\nAponte-Santamaría, C., Briones, R., Schenk, A. D., Walz, T., and de Groot, B. L. (2012) Molecular driving forces defining lipid positions around aquaporin-0. Proc. Natl. Acad. Sci. U.S.A. 109: 9887-9892.\nPage created and maintained by Xaq Pitkow', 'By: Alison Cranage\nThe Sanger Institute was set up to uncover the code of life – the human genome. We opened our doors 25 years ago and became the largest single contributor to the human genome project. The principles that sat behind those endeavours are still fundamental – tackling the biggest challenges, openness and collaboration. Those principles have also helped to make Sanger one of the world’s leaders in genomics and biodata.\nThe Human Genome Project transformed science. The seemingly simple order of four letters of DNA changes how we understand life. Vast new areas of research have opened up, impacting biology, medicine, agriculture, the environment, businesses and governments.\nAlongside our sequencing facilities, our activities and research have grown to utilise genomic knowledge. Now we are using genomics to give us an unprecedented understanding of human health, disease and life on earth.\nSequencing at scale\nFrom the completion of the first human genome in 2003, we moved to the 1,000 and 10,000 genomes projects. Being able to compare sequences between individuals enables the understanding of diversity, evolution and the genetic basis of disease.\nOne of our latest projects is to work with UK Biobank to sequence the genomes of 50,000 individuals. Participants have already provided a wealth of data about their health and their lives – from blood samples to details of their diet. Linking this information to sequence data means we can understand more than ever before about the connections between our genomes and our health.\nSanger researchers also sequence the genomes of pathogens and other organisms, as well as people. We have published the genomes of thousands of species – from deadly bacteria to worms to the gorilla. This enables research into evolution, infections, drug resistance, outbreaks, symbiosis, biology and host parasite interactions.\nOur sequencing teams, led by Dr Cordelia Langford, are constantly developing the technology to improve both accuracy and speed. In early 2018, we celebrated sequencing over five petabases of DNA (if you typed it all out, it would take 23 million years). The first petabyte took just over five years to produce. The fifth, just 169 days. The amount of genomic data now rivals that of the biggest data sources in the world – YouTube, Twitter and astrophysics.\nThe Sanger Institute is not only developing sequencing technology but also leading research in computational science, IT and bioinformatics, developing new ways to store and analyse petabytes of genomic and bio-data.\nFrom sequence to clinic\nHow genome sequencing, or the sequence of any given individual, can be used hasn’t always been clear. But in the case of rare genetic diseases, it can change lives.\nThe Deciphering Developmental Disorders (DDD) study started 8 years ago, led by Dr Matt Hurles at the Sanger Institute. Over 13,600 children with rare developmental conditions, but without a diagnosis, joined the study. Sanger researchers, working together with clinical geneticists, have used genome sequencing to diagnose their conditions. 40 per cent of the children now have a diagnosis – giving the families some of the answers they were searching for. Knowing the genetic cause of a condition can help doctors manage it, help families connect with others as well as plan for the future.\nThe ability of researchers to rapidly sequence and analyse bacterial genomes is also leading to advances for patients.\nDr Julian Parkhill and colleagues showed it was possible to track an MRSA outbreak in a neonatal ward in real-time. By sequencing MRSA isolates from patients and staff, they could track the outbreak, following its path from person to person. This enables clinicians to prevent further transmission and bring the outbreak under control.\nNow, it is UK policy to sequence the genomes of pathogens in an outbreak.\nBut disease knows no borders. Pathogens can easily spread around the globe. Professor David Aanensen, group leader at the Sanger Institute, is also Director of the recently established Centre for Genomic Pathogen Surveillance. The centre co-ordinates global surveillance of pathogens (such as MRSA and the flu virus) using whole genome sequencing. The data is openly available. Countries around the world can monitor the rise and spread of pathogens as well as their growing resistance to antibiotics. This enables swift action – with the aim of stopping transmission and saving lives.\nThe forefront of human genomics\nThe rapid development of technology has led to the ability of researchers to sequence the DNA, or RNA, from a single cell. Previously, much larger quantities of material were needed. Single cell RNA sequencing is a powerful tool. It allows the study of an individual cell’s activity, functions and composition. And high throughput machines means hundreds of thousands of cells can be analysed at once.\nThe Human Cell Atlas is capitalising on these advances. The international collaboration is co-led by Dr Sarah Teichmann at the Sanger Institute. Launched in 2016, scientists are using Next-Generation Sequencing to sequence 30-100 million single cells from the human body – out of a total of roughly 37 trillion. The aim is to create a comprehensive, 3D reference map of all human cells. This will lead to a deeper understanding of cells as the building blocks of life. It will form a new basis for understanding human health and diagnosing, monitoring, and treating disease.\nLike the human genome project before it, this huge project will disrupt science and human biology. And like the human genome project it will drive technology to make it possible.\nThe diversity of life\nBeyond human health, genome sequence data allows the study of evolution, biology and biodiversity.\nFor our 25th anniversary we have sequenced a more diverse range of species than ever before. 25 different species that represent biodiversity in the UK – from the golden eagle to the humble blackberry. Sequencing new species will push development of our technologies as each presents unique challenges. The sequences themselves will aid research into population genetics, evolution, biodiversity management, conservation and climate change.\nBut 25 species is just the beginning. Every single living thing has a genome, made up of exactly the same molecules of DNA or RNA. We want to uncover how the order of those molecules lead to the diversity of life on earth.\nIt took 13 years to sequence the first human genome. When the project began, no-one knew where it would lead. Now we sequence the equivalent of one gold-standard (30x) human genome in 24 minutes – faster and deeper genomic insights are enabling discoveries that improve health and our understanding of biology. These insights are happening right now, and they will lead to unimagined benefits for future generations – all possible from a sequence of four letters of DNA code.\n- News story: First ‘non-gene’ mutations behind neurodevelopmental disorders discovered\n- News story: The Finished Human Genome – Welcome to The Genomic Age\n- News story: New global health initiative for genomic surveillance of antimicrobial resistance funded by NIHR\n- News story: Tracking MRSA in real time\n- News story: What have we got in common with a Gorilla\n- Profile: David Aanensen\n- Profile: Matt Hurles\n- Profile: Cordelia Langford\n- Profile: Julian Parkhill\n- Profile: Sarah Teichmann\n- Project: 25 Genomes Project webpage\n- Project: The Centre for Genomic Pathogen Surveillance\n- Project: Deciphering Developmental Disorders (DDD)\n- Project: Human Cell Atlas web page\n- Organisation: Wellcome Sanger Institute']	['<urn:uuid:56ab9c5d-2d13-4f59-84ff-75127350c9e4>', '<urn:uuid:a77a963c-53e8-4f9d-85da-0a576dcc3081>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	17	112	1860
34	homemade sparklers ingredients what stuff does sparkler have inside	Sparklers consist of a metal wire dipped in a pyrotechnic compound made up of three components: a metallic fuel (usually aluminum or magnesium for white sparks, or iron/ferrotitanium for gold sparks), an oxidizer (generally potassium nitrate), and a binding material (flammable starch).	"['How Do Fireworks Actually Work?\nby Sarah Dobbs\nEach year, as the Fourth of July approaches, the sound of explosions starts to become a normal part of the evening. Fireworks have existed in one form or another for around 1000 years, and they show no signs of going away anytime soon. But how do they work? Most of us just know to light the fuse and stand back. Let’s take a closer look …\nRocket-type fireworks can produce all kinds of different effects when they go off, but the basic structure of an aerial firework stays more or less the same. Each rocket is made up of the following parts: a mortar, fuses, propellant powder, a shell, a bursting charge, and a collection of ""stars."" The mortar is the outer container, and the fuse is, of course, the piece that you light. When the fuse burns down, the propellant ignites and shoots the firework into the air.\nWhen it’s airborne, a second explosion is triggered inside the shell by a time delay fuse. The bursting charges set off the stars—small, explosive pellets made of fuel and metallic compounds that create the lights in the fireworks display. Different metals create different colors when they ignite: barium goes green, calcium salts go orange, magnesium goes white, copper is blue, lithium turns red, and sodium becomes gold. And the arrangement of the stars will determine the shape of the explosion—so if they’re packed in a heart shape, they should reproduce that heart shape in the sky.\nOther effects can also be built in by adding various ingredients; different kinds of fuel can create sound effects, for example, like the whistling or screaming noises some rockets make as they shoot into the sky. Stars can be made up of layers of different metallic compounds, to create multicolored explosions. And in some more complex fireworks, there may be several stages of explosions; in that case, there are generally multiple fuses inside the shell, and as each burns down, a different explosive goes off.\nOf course, not all fireworks are of the shoot-into-the-air-and-go-bang variety. Fountains don’t take off, and generally don’t go bang, either; instead, they stay where they’re placed and give off a cascade of sparks—like a fountain, but with pyrotechnics instead of water.\nUsually conical in shape, fountains consist of a paper or plastic tube, with clay plugs at either end. Inside the tube are a couple of different kinds of fuel, plus the metal compounds that create the sparks. When the fuse is lit, the fuel ignites, and sparks are forced out of an aperture in the top of the fountain.\nAgain, different metals create different colors and effects. Multi-stage effects can be created by bundling multiple tubes together, so that as one finishes another starts, adding different colors or sound effects to the display.\nCatherine wheels are another common type of firework, and again the same kinds of ingredients are used to create a slightly different effect. Named for the unfortunate Saint Catherine, these fireworks are generally fixed to a pole or a mount, so that they can spin as they burn, creating a spiral of sparks.\nBigger Catherine wheels tend to have a plastic disk at their center, with “gerbs” attached around the edge. The gerbs are similar to fountains, in that they’re tubes filled with the mixture of ingredients that create the effects; when lit, the thrust from the explosives makes the wheel turn as they burn. And again, the effect can be made more elaborate with multi-stage effects and different colors; each gerb might be different, so that the wheel changes as each one ignites in turn.\nSmaller Catherine wheels might, instead, be made up of a single long, thin tube coiled into shape around a smaller central disk. Again, the thrust of ignition makes the wheel spin.\nThe only firework you should ever hold in your hand once it’s lit is a sparkler—a Fourth of July staple. Unlike most other fireworks, they don’t explode with a bang, but gently fizzle for around a minute, as a ball of sparks makes its way down a metal wire. And they’re pretty simple: basically, the metal wire is dipped into a pyrotechnic compound that’s made up of a metallic fuel, an oxidizer, and a binding material.\nThe metallic fuel is what creates the sparks; it’s usually aluminum or magnesium, which creates white sparks, but some sparklers may use iron or ferrotitanium for gold sparks instead. The oxidizer, which provides the oxygen to keep the spark going, is generally potassium nitrate. And then a binding material, a kind of flammable starch, keeps the mixture together, and burns away once the sparkler is lit.\nHopefully, none of that has taken away any of the magic of a good fireworks display. If nothing else, you’ll be able to impress your friends by quietly musing “oooh, barium” next time you see a green firework.\nHave you got a Big Question you\'d like us to answer? If so, let us know by emailing us at email@example.com.']"	['<urn:uuid:49cb37a6-292d-4cdf-9ff1-1eda8d4a9c23>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	9	42	840
35	Between ringlegged earwigs and peregrine falcons, which species is flightless?	Ringlegged earwigs are wingless and flightless, while peregrine falcons can fly and are actually the fastest animal on Earth, capable of reaching speeds over 200 mph in stoop dives.	"['Earwigs are a relatively small group of insect that belong to the Order Dermaptera. Earwigs often upset people when discovered indoors. Their forceplike tail appendages make them look dangerous, but they are quite harmless. Earwigs run rapidly around baseboards, and they may emit a foul-smelling, yellowish-brown liquid from their scent glands when disturbed or crushed. Earwigs are mainly active at night, usually hiding during the daytime. They\'re often found in clusters hiding in dark crevices like door or window frames. Earwigs normally live outdoors and do not establish themselves indoors, though the ringlegged earwig is a common resident in greenhouses. Earwigs are harmless to humans and animals, though if picked up and restrained, adult earwigs can give a slight pinch with the forceps. While mainly predaceous on other insects, earwigs often feed on flower petals, soft vegetables and fruits, or seedling plants when hot and dry conditions persist.\n|Common Name||Scientific Name|\n|European Earwig||Forficula auricularia L.|\n|Ringlegged Earwig||Euborellia annulipes (Lucas)|\nThe name earwig actually comes from an Old English name, ēarewicga (meaning ""ear insect""), which was derived from the superstition that earwigs can enter your ears at night and burrow into your brain to lay eggs. This belief is totally unfounded, though earwigs will occasionally seek out ear canals of campers as dark, moist hiding places. This can obviously be a traumatic experience!\nAdults of the European earwig are usually winged, while the ringlegged earwigs are wingless. If wings are present, the first pair are hard, short, and scale-like, while the second pair are membranous, fan-shaped, and folded under the hard first pair of wings. Tips of the second pair of wings usually protrude from under the first pair. The European earwig ranges from 1/2 to 3/4 inch long, with banded legs and a reddish head. The ringlegged earwig ranges from 1/2 to 3/5 inch long and is black-yellowish underneath with legs having white crossbands at the joints.Young earwigs (nymphs) are similar to adults except that the nymphs lack wings and the cerci are short.\n|European earwigs.||Ringlegged earwig.|\nLife Cycle and Habits\n|Female European earwig tending her eggs under a stone (which has been lifted away).|\nEarwigs develop from egg to adult through gradual, or incomplete, metamorphosis with four to five nymphal instars. Generally, most earwigs lay eggs in late winter into early spring in chambers in the ground, under logs, or under stones. Usually, 20 to 50 smooth, oval, pearly-white or cream-colored eggs are produced. The female moves, cleans, and provides maternal care by protecting the eggs and new young until the first molt.\nWhen spring weather is suitable, the female makes an opening to the soil surface where the young nymphs can leave the nest and forage for food. The small nymphs often return to their burrow during the day, but soon they are large enough to fend for themselves without a burrow. Earwigs are active at night and are often found around lights preying on other small insects that come to lights. During the day, earwigs hide in moist, shady places beneath stones, boards, sidewalks, and debris. They are rapid runners and feed on mosses, lichens, algae, fungi, insects, spiders, and mites, both dead and alive.\nEarwigs rarely fly and are unable to crawl long distances, but they often hitchhike in laundry baskets, cut flowers, luggage, newspapers, lumber, baskets of fruits and vegetables, automobiles, etc. They prefer moisture and may migrate indoors during periods of prolonged heat and drought. Forceps at the end of the abdomen are used to defend the nest, capture prey, probe narrow crevices, and fold or unfold wings.\nEarwigs require moist, cool places and are found in damp crawl spaces, in flower gardens near the home, in mulches, in compost piles, in trash, under boards, and in wood piles. After entering houses, they feed on sweet, oily or greasy foods or houseplants. They are attracted to lights.\nIf earwigs are persistently invading a home or building, one should first attempt to control the earwigs outdoors. Since earwigs breed in soil chambers and spaces under landscape stones and timbers, reducing mulch layers and landscape structures can help reduce populations. Since the European earwigs feed primarily on soft-bodied insects, use plants that are less prone to infestations of aphids and scales, or treat these insects when they are found.\nDuring dry, hot weather, earwigs sometimes migrate indoors in search of cool and moist habitat. They are easily killed by residual insecticide treatments in cracks and crevices, along baseboards, beneath cabinets, along door and window sills, and in other hiding places during the day. Unfortunately, control will be short-term due to new earwig entry from outdoors. Therefore, the best strategy is to better seal around doors and windows, which serve as the major entry points.\nEarwigs need and are very attracted to moisture. High populations, practically invisible during the day, may be present around foundations, in landscaped yards, in mulch, under boards, etc. Be sure to eliminate damp, moist conditions in crawl spaces under houses, around faucets, around air-conditioning units, and along house foundations. Rain gutters and spouts should carry water away from the house foundation. Use caulking compound, putty, and weather stripping around doors, windows, pipes, and other entry sites, especially at the ground level. Change landscaping by creating a clean, dry border immediately around the foundation wall. Gravel or ornamental stones can make a barrier against earwigs and other pest invaders.\nEarwigs can be encouraged to cluster in trapping sites. Place burlap bags, canvass, boards, newspapers, or other cover material on top of mulch, around shrubbery, and on top of or around similar habitats. Earwigs will hide under these sheltered areas, where they can be collected early in the morning. Shake specimens into hot, boiling water or into a pan of soapy water. Indoors, remove with broom and dustpan or by vacuum sweeper, but be sure to dispose of the vacuum sweeper bag to avoid having the earwigs crawl back out.\nThere are many insecticides labeled for earwig control. Indoor treatments should supplement outdoor treatments since earwigs do not become established indoors. Dusts and residual sprays are effective when applied around baseboards, beneath cabinets, and around other hiding places at the floor level.\nOutdoors, treat in a three to six foot band around the building adjacent to the foundation (called a perimeter treatment) to stop or limit earwigs from getting indoors. Apply, if needed, beginning around the end of spring and throughout the summer to the building foundation, subfloor crawl spaces, flower beds, turf, or mulch in late afternoon. Apply during late spring and summer to control young earwigs. Liquid sprays, dusts, and granular insecticides are effective when used as perimeter treatments. Always read the pesticide label and follow directions and safety precautions.', 'Did you know that golden eagles can be found gliding through the skies of Nevada?\nGolden eagles hold their powerful wings in a V formation, allowing them to soar at speeds of 28 to 32 mph. They can reach up to 120 mph when on the hunt and dive at an astonishing rate of up to 200 mph. Imagine being able to move that quickly!\nAlongside the golden eagle is a host of other fast creatures in Nevada. Some you probably never even thought about as being quick. These incredible creatures have adapted to their environment by being able to outrun predators, escape danger, and survive in a challenging habitat.\nFollow along as we look at some of the fastest animals in Nevada.\nThe pronghorn is the fastest land mammal in North America and can run up to 60 mph, making them one of the fastest animals in Nevada. They got their name from their horns — which point backward towards their rump and then “prong.”\nThese ungulates have specially adapted muscles designed for explosive bursts of energy that help power their impressive speed. On top of that, their body temperature runs relatively high at 100 degrees Fahrenheit!\nThese creatures are usually 4 to 5 feet long from nose to tail and stand about 3.5 feet tall at the shoulder. Males weigh a bit more than females, with males typically heading up to 145 pounds and females up to 85 pounds.\nPronghorns can run at speeds of up to 60 mph for distances when something’s chasing them. At a normal running pace, they’ll reach speeds of 40 mph due to their unique strides. These speedsters have different ways of walking that help them cover as much as 20 feet per step! To protect themselves against uneven ground or firm surfaces, pronghorns have pointed and padded double hooves.\nUnfortunately, rising traffic levels could pose a threat to this species’ well-being. Studies show they might see roads as a risk, causing them to socialize less. The best solution will be traffic plans that take into account this animal’s need to roam.\nJackrabbits can reach up to 40 mph! They have incredibly long back legs with powerful muscles that allow them to make tremendous leaps. Their jumping skills help them to evade predators more quickly than other species.\nDid you know that the white-tailed jackrabbit is not actually a rabbit but a hare? This species is the largest of all the hares in Nevada, and they have some pretty unique characteristics. They have long ears and legs, giving them a dusky gray color with a white tail, which turns completely white in winter!\nYou can find these fast jackrabbits in various habitats across North America. These include open grasslands, pastures and fields, forested areas, and high alpine tundra. This speedy hare is a strict herbivore and feeds primarily on green plants and flowers during summer. Jackrabbits are also nocturnal, meaning it is most active at night. During the day, they rest in shallow depressions in the ground called “forms.”\nThe peregrine falcon is an awe-inspiring bird of prey. It is the fastest animal on Earth, capable of reaching speeds over 200 mph in stoop dives to hunt its prey! Its sharp talons and keen eyesight make it an excellent hunter. Nicknamed the “wanderer,” the peregrine falcon is known for its long migratory flights. They’re the ultimate endurance flyer.\nWhat does this fast falcon eat to stay energized? Their diet consists of various other birds and small mammals, such as mice and squirrels.\nhis fast bird is a global species, present on all continents except Antarctica. In Nevada and other urban areas, they take advantage of tall buildings to make their nests, allowing them to become beloved local celebrities.\nCoyotes can run a jaw-dropping 43 mph! That’s why they earned a spot on our list of the fastest animals in Nevada. Along with being fast, coyotes also have a reputation for being adaptable and smart. They can adapt to almost any habitat, making them comfortable in urban and wild settings.\nCoyotes are incredibly adaptable in their diet too. Opportunistic omnivores can eat various foods depending on what is available. Primarily, their diet consists of smaller animals like cottontail rabbits and ground squirrels — but they will also scavenge for carrion, grasses, fruits, seeds, and insects. No matter the circumstance or availability of food sources, these resilient animals always find a way to survive! And they do it as a team.\nBelieve it or not, coyotes are highly social creatures who often live in packs with dominant alpha males and females. Only the alphas reproduce the pack during the breeding season from January to March. The other pack members take on the important task of defending their territory.\nBig Brown Bat\nThe big brown bat is one of the fastest flying mammals in Nevada, reaching up to 40 mph. Bats use their wings as “sails” that help propel them forward at incredible speeds. They zig-zag around branches and trees on their way!\nThe big brown bat is another endurance flyer. Once a bat starts to fly, they never stop. When they want to drink, bats find a body of water, open their mouths, and swoop down for a sip! When they want to eat, these creatures use echolocation to find their way and hunt their prey.\nIn addition to being essential pollinators, they are also great indicator species since they need particular habitats to survive. For example, they require humid air with trees or other tall structures near water to roost and feed.\nThey can also live in large colonies, with as many as 400 bats living in one place! They typically perch during the day hanging upside down and then emerge at dusk each night to start hunting for food.\nThe desert sidewinder is a small snake native to Nevada that can reach up to 18 mph. These desert dwellers are an incredible species, and it’s no wonder they’ve been able to adapt to such harsh conditions — they’re fast and agile.\nThe sidewinder moves by throwing its body into a series of tight “S” shaped coils, inching forward with each loop. This unique form of locomotion enables the sidewinder to move quickly over sandy surfaces while avoiding predator detection.\nAlso known as the “horned rattlesnake,” sidewinders thrive in the Mojave Desert. Aside from its eye-catching horns and distinctively patterned scales, this species has some unique characteristics that make it particularly interesting!\nFor starters, sidewinders are the fastest-moving rattlesnakes. Secondly, these snakes don’t lay eggs but give birth to live young. Female sidewinders store their eggs inside their bodies until they are ready to hatch. Once hatched, these snakes birth live young — usually in litters of 5 to18!\nLastly, these venomous reptiles use their tails to attract prey. They’ll rattle their tails to spark the curiosity of lizards. When the lizards come by to see what the shaking is all about, the sidewinder will lunge out of the sand and strike!\nMoose are also known as “rubber-nosed swamp donkeys” because they seem not to care about very wet terrain or bogs. They are, however, incredibly fast runners. Moose calves can outrun a human by the time they’re five days old! And if that wasn’t impressive enough, moose can kick in any direction with their front hooves.\nMoose are great swimmers too! They can zoom through the water at up to 6 mph and go for two hours straight without stopping. Even more impressive, there have been reports of a moose swimming an unbelievable 10 miles in one go!\nFor a while, there weren’t many moose in Nevada. But now, the moose population is making a comeback. An estimated 100 live in Nevada, and the numbers are rising. Moose are expanding their territory to Elko and Humboldt counties in Nevada. These habitats provide them with the perfect wet riparian areas, and plenty of aspen and mahogany trees to eat. Some moose have been seen as far south as the Ruby Mountains southeast of Elko.\nDid you know that Nevada is home to a unique species of deer? Mule deer are the only native deer in the state and can easily be identified by their long, mule-like ears. Their coloring varies from light ash gray to dark brown, with a white patch on their throats to complete the look. They’re one of the most common animals in Nevada and can reach up to 40 mph.\nThese deer are fast and hardy. They have a life span of 10-12 years and can weigh over 300 pounds! Male mule deer will sport impressive antlers during the breeding season from late November to mid-December. But even with all that headgear, they can still easily outrun predators. They can even outrun a horse at full force!\nThis four-legged mammal has adapted specialized leg muscles that boost their speed to outrun predators and swiftly maneuver through thickly forested areas. Their legs also help them swim quickly too. However, they rarely use the water as means of escape from predators. Instead, they rely on their swift feet to get them out of danger.\nGreat Horned Owl\nNext on our list of the fastest animals in Nevada, we have the great horned owl. This fast-flying bird of prey can reach speeds of up to 40 mph! They have feathered faces that help reduce wind resistance in flight and large eyes that help them see well at night. Their short but wide wings help them quickly fly through the forest, and their soft feathers allow for silent flying so they can sneak up on unsuspecting prey!\nGreat horned owls’ big eyes and wide pupils give them a strong advantage when hunting. Unlike humans, their eyes don’t move in their sockets. Instead, they can swivel their heads to look in any direction to find a meal.\nThese fast and voracious eaters enjoy a wide variety of prey, from small rodents to skunks and geese. Similarly to other owls, they sometimes swallow their meal in one fell swoop, then regurgitate pellets containing the bones, fur, and other indigestible parts of the meal.\nDespite the name, this bird doesn’t have any horns! The great horned owl gets its name from its distinctive tufts of feathers on its head, known as plumicorns. Scientists may not know why great horned owls have distinctive tufts of feathers on their heads, but there are a few fun theories. These tufts could help identify each other in the forest or blend into the environment and look less like prey. It’s an interesting mystery that has yet to be solved.\nGreat Basin Collared Lizard\nThe Great Basin collared lizard is another one of the fastest animals in Nevada! These little guys can reach speeds of 15 mph. When you add in their impressive bite force, it makes them formidable hunters. Their diet consists mainly of insects, spiders, small mammals, reptiles, and other small animals.\nCommonly known as desert or Mojave black-collared lizards, their range extends from southeastern Oregon to southern Arizona. Collared lizards are known for their bright colors, including shades of yellow, orange, and red. They also have a unique black collar that runs from behind the head to the shoulder region.\nWith the ability to run up to 15 mph, they can easily outrun predators such as foxes, snakes, and birds. Even more impressive is their astounding agility! These lizards can run on two legs! Although they can only go this fast for a short time, it gives them enough time to hide in small spaces.\nWhen seeking shelter, these reptiles prefer sloping rocky areas. Their favorite spots include canyons, rock piles, washes, and slopes. Here, they can bask in the sun while keeping an eye on their surroundings for potential prey and predators.\nDid you know that smaller black bears can run up to 30 mph? That’s pretty fast! They can even do it uphill, downhill, and on level ground. But if a bear puts on too much winter weight, it’ll overheat and tire quickly. So the leaner the bear, the faster the speed.\nBlack bears are the only species of bear found in Nevada. And while they may be named after their color, they come in many shades. A black bear’s coat can range from blonde to brown to cinnamon.\nThese majestic beasts have a keen sense of smell, thanks to their small eyes, rounded ears, and long snouts. This helps them sniff out food items up to miles away! These animals are very determined too. This video of a black bear ripping out a wall to get a meal is a great example.\nBelieve it or not, there are 500 black bears found in the mountainous western part of Nevada. They are part of the Sierra Nevada black bear population, including bears from northern California and southern Oregon! How incredible is that?!\nFinal Thoughts on Fastest Animals in Nevada\nNo matter what time of year it is or what animal you spot in Nevada, one thing’s for sure: speed reigns supreme in the desert. From jackrabbits to mule deer and beyond, these incredible creatures will always leave you in awe of their agility and strength.\nThis state is home to many fast-moving animals that can outrun predators, escape danger, and survive in harsh desert environments. We only had time to name some of the fastest animals in Nevada. So here’s a quick shout-out to the kangaroo rats, wild horses, and the swift fox! If you don’t know what a kangaroo rat is, you owe it to yourself to check out this article.\nFrom fast flyers to speedy slithers, Nevada offers the incredible opportunity to observe some of nature’s speediest species in action. The next time you go exploring, keep your eyes open for these swift creatures! You might catch a glimpse of them as they move through their habitat.\nMore from A-Z Animals\nThe Featured Image\nThank you for reading! Have some feedback for us? Contact the AZ Animals editorial team.']"	['<urn:uuid:c9a27ab3-2960-4f0b-8525-bdc886f68cde>', '<urn:uuid:a3618f35-0169-4b8d-985c-4f3725020d9a>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T18:38:36.516973	10	29	3452
36	fircosoft experience sanctions screening pioneer	FircoSoft has over 20 years of experience in providing watch list filtering and was the first company to receive the SWIFT label for a watch list filtering solution in 2001. Since 2000, FircoSoft has been ranked #1, best-of-breed and received numerous accolades and industry recognition for its watch list filtering solution from many organizations and analyst firms.	['- Financial crime compliance is a complex and costly challenge\nFinancial crime compliance is a complex and costly challenge\nAt the request of our community, we’ve significantly extended our focus into the financial crime compliance domain. We’ve already built a strong portfolio of services through collaboration and by working with partners who are the experts in their field. Now, we’re planning for the future.\nOur partnership approach\nAs a member-owned cooperative, we work closely with our members and users to develop the products and services that meet their needs. This collaborative approach also means we have a strong competence in partnering with expert providers to deliver the best products and services, in the right time frame.\nWe’ve worked successfully with expert partners to build a portfolio of financial crime compliance services to help our users address the cost, effort and complexity of meeting compliance obligations. As we plan for future needs, we expect partnering to remain an important element in our approach.\nOur partner for Sanctions Screening\nRecognised as pioneer and leader in the financial crime market, FircoSoft has over 20 years’ experience in providing watch list filtering and was the first company to receive the SWIFT label for a watch list filtering solution in 2001.\nIn 2011, following an open selection process, we selected FircoSoft to provide the underlying components for our Sanctions Screening service.\nSince 2000, FircoSoft has been ranked #1, best-of-breed and received numerous accolades and industry recognition for its watch list filtering solution from many organisations and analyst firms.\nOur partner for Sanctions Testing\nWe partnered with Omnicision, a financial crime consulting and solutions company, during the development and launch of our Sanctions Testing service. Omnicision brought unique skills to the project: a global perspective of business risks, challenges and opportunities; knowledge and experience of the systems that underpin our members’ business operations – and the technical and analytical know-how to design successful systems and bring them to enterprise-scale production.\nIn 2014, SWIFT acquired Omnicision, a move that underlines our commitment to investing to meet the needs of our communities in the key area of financial crime compliance.\nDow Jones Risk and Compliance\nDow Jones Risk and Compliance is a global provider of risk management and regulatory compliance information, delivering targeted content to organisations around the world. It provides data feeds of relevant information, online tools and custom reports that help firms be more effective and efficient in their anti-money laundering (AML), anti-bribery and corruption, economic sanctions, third party due diligence and commercial risk operations.\nDow Jones’s Sanctions Ownership Research data is now included in SWIFT’s Sanctions Screening service, and its Sanction Alert data feeds into Sanctions Testing. This enhancement allows compliance officers and risk managers to proactively test and monitor their transactions against high-quality enriched Sanctions data.\nDow Jones Risk and Compliance data is produced to the highest quality standards. Its comprehensive and ongoing research on sanctions ownership has resulted in a data set containing over 5,000 companies owned or controlled by listed individuals and entities.\nFor more information, visit the Dow Jones website.\nPlanning for the future\nAs we plan for the future needs of our community, we are always looking for suitable technology partners who share our vision: to provide solutions that help the financial community to meet the challenges and operational burden of financial crime compliance effectively, and share the costs.\nTo find out more about how we will partner to deliver future services, please contact us.']	['<urn:uuid:60b214e0-db1c-4e60-91f7-61936fde3af3>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	5	57	575
37	How do algorithmic systems function as both analytical tools and social actors, and what ethical implications emerge from their dual role in society?	Algorithmic systems operate as both analytical tools and social actors in complex ways. As analytical tools, they process data to make decisions in various contexts, from detecting medical conditions to determining credit worthiness. However, they also function as social actors, as demonstrated by the virtual improviser that engages with human musicians and elicits responses about creativity and human interaction. This dual role raises ethical concerns, as these systems can simultaneously reveal social patterns while perpetuating biases. For instance, while they might inject objectivity into previously subjective decisions, they can also systematically disadvantage certain groups through their underlying assumptions and data inputs. This is evident in cases where algorithms have led to discriminatory outcomes in hiring, policing, and financial decisions.	"['In a recent article in Cultural Anthropology, Nick Seaver (2018) asks, “What should an anthropology of algorithms do?” Seaver urges anthropologists to avoid portraying algorithms as entities empirically and analytically separable from social life, omnipotent in their capacity to override “the human.” Occupying what Seaver (2018, 380) calls the “analog slot,” the human here connotes a catch-all bundle of qualities such as spontaneity, creativity, and affect. While attending to very legitimate fears of bots, hacks, and the genocide-stoking potential of algorithms, Seaver’s provocation calls for a commitment to careful empirical analysis that explores the situated lifeworlds in which humans shape, and are shaped by, algorithms.\nWhat methodological approaches are best suited to such a project? How might we devise new ethnographic practices to challenge the “analog slot,” without either collapsing ontological differences between programmer and programmed or falling back on a stagnant analytic of hybridity? Given the extent of our intricate imbrication, might we consider not only an anthropology of algorithms, but also an anthropology with algorithms?\nRitwik Banerji, an ethnomusicologist, programmer, and saxophonist, provides a sonic starting point for experimental ethnography with algorithms. Banerji practices free jazz or free improvisation. Building on his career as a musician, he programmed a “virtual improviser” or “player-program,” an algorithm that can respond to and interact with human musicians in real time. Named after the software Max/MSP, Maxine is less a coherent physical apparatus than a network of feedback loops between algorithm, sonic input, and human musicians. Banerji programmed Maxine’s algorithm to produce spontaneity. Yet, in our interview, he reflected that this algorithm projects into the world his own interpretation of improvisation, interaction, and creativity. Built for spontaneity but shaped by human sociotechnical history, Maxine exemplifies Seaver’s (2018, 377) definition of an algorithm as “sociotechnical fabric.”\nIs “programmed spontaneity” an oxymoron? Banerji leaves this question open to debate. In a multisensory twist on elicitation techniques, he invites improvisers to play with Maxine, then interviews the human players about their experience. Banerji attends to how individual musicians orient themselves to Maxine and construe Maxine’s relation to “the human.” Situated understandings of what constitutes “the human” come into relief, as do often surprisingly honest reflections about the rules of engagement within a musical genre supposedly free of all constraints.\nAlgorithms occupy several positionalities in Banerji’s work: ethnographic interpretation, interlocutor, ethnographic Other, and perhaps most provocatively, “co-ethnographer.” In my interview with Banerji, he describes how players unaccustomed to improvising with an apparatus like Maxine liken it to a childlike figure, a cultural outsider unfamiliar with the rules of engagement. In other words, they position Maxine as an ethnographic Other. Second, Maxine is a sociotechnical infrastructure; by tweaking the positions of the speakers and microphones, Banerji can change how and to whom Maxine “listens,” and the degree to which Maxine interacts with the external sonic environment. Third, Maxine is itself an ethnography; “algorithmic architecture is already one type of ethnographic representation,” Banerji says, sonically enacting Banerji’s own interpretation of what constitutes interactivity in the first place.\nFinally, Banerji figures Maxine as a “co-ethnographer.” Maxine’s presence gets improvisers to speak the unspeakable, venturing explicit theorizations of what makes a musical interaction creative and alive, even though free improvisation purports to transcend codified principles of musical interaction. Without Maxine, improvisers would be too bound by ideals of romantic neoliberal freedom—and a reticence to criticize their peers—to bring implicit frameworks to conscious attention, Banerji argues.\nWhat better to challenge the persistent, analytically constrictive binary between human affectivity and cold algorithmic calculus than an algorithm that breaches the very heart of romantic humanism: spontaneous play? Maxine is invited up onto the (literal) stage of co-present creativity. As players in Banerji’s elicitation sessions reevaluate what makes a musician human, neoliberal ideas of unconstructed freedom collide with the creative generativity of an algorithm. More precisely, particular notions of freedom collide with the creativity of Maxine’s “socio-technical fabric,” a tapestry of sonic infrastructure and ethnographic interpretation.\nMaxine’s very physicality challenges notions of the algorithm as a discrete, bounded entity. Its onstage presence consists of a laptop, amplifiers, and speakers. These are commonplace items in improv sessions. Until the music starts, one might not notice Maxine at all. However, once Maxine sonically engages with other players, the disorienting, often discordant performance presents a uniquely tangible encounter with the dual power of algorithms to both reflect and shape social reality.\nBanerji’s experimental ethnography of play and creativity may seem a departure from urgent concerns about the role of algorithms in stoking racism, anti-democratic movements, and genocide. However, this series—which includes an interview with Banerji, a performance recording of Maxine, and “liner notes” on that performance—stands productively adjacent to these pressing political problems. Its invitation to musical and methodological play may give us a broader range of tools with which to engage the profound ways in which algorithms shape and are shaped by human experience.\nWhat follows is a multi-modal assemblage which includes a performance recording featuring Maxine and trumpet player Axel Dörner, Banerji’s reflections on their session, and an interview I conducted with Banerji.\nSeaver, Nick. 2018. ""What Should an Anthropology of Algorithms Do?"" Cultural Anthropology 33, no. 3: 375–85\nPosts in This Series\nOn November 9th, 2015 in Berlin-Tempelhof, Ritwik Banerji recorded an improvisation session between Maxine and Berlin-based improviser Axel Dörner. Listen to th... More\nIn November 2018, I had a wide-ranging conversation with musician, programmer, and ethnomusicologist Ritwik Banerji, who had recently completed his PhD in ethno... More', 'When you browse online for a new pair of shoes, pick a movie to stream on Netflix or apply for a car loan, an algorithm likely has its word to say on the outcome.\nThe complex mathematical formulas are playing a growing role in all walks of life: from detecting skin cancers to suggesting new Facebook friends, deciding who gets a job, how police resources are deployed, who gets insurance at what cost, or who is on a “no fly” list.\nAlgorithms are being used—experimentally—to write news articles from raw data, while Donald Trump’s presidential campaign was helped by behavioral marketers who used an algorithm to locate the highest concentrations of “persuadable voters.” But while such automated tools can inject a measure of objectivity into erstwhile subjective decisions, fears are rising over the lack of transparency algorithms can entail, with pressure growing to apply standards of ethics or “accountability.”\nData scientist Cathy O’Neil cautions about “blindly trusting” formulas to determine a fair outcome. “Algorithms are not inherently fair, because the person who builds the model defines success,” she said.\nO’Neil argues that while some algorithms may be helpful, others can be nefarious. In her 2016 book, Weapons of Math Destruction, she cites some troubling examples in the United States:\nPublic schools in Washington, D.C. in 2010 fired more than 200 teachers—including several well-respected instructors—based on scores in an algorithmic formula which evaluated performance.\nA man diagnosed with bipolar disorder was rejected for employment at seven major retailers after a third-party “personality” test deemed him a high risk based on its algorithmic classification.\nMany jurisdictions are using “predictive policing” to shift resources to likely “hot spots.” O’Neill says that depending on how data is fed into the system, this could lead to discovery of more minor crimes and a “feedback loop” which stigmatizes poor communities.\nSome courts rely on computer-ranked formulas to determine jail sentences and parole, which may discriminate against minorities by taking into account “risk” factors such as their neighborhoods and friend or family links to crime.\nIn the world of finance, brokers “scrape” data from online and other sources in new ways to make decisions on credit or insurance. This too often amplifies prejudice against the disadvantaged, O’Neil argues.\nHer findings were echoed in a White House report last year warning that algorithmic systems “are not infallible—they rely on the imperfect inputs, logic, probability, and people who design them.”\nThe report noted that data systems can ideally help weed out human bias but warned against algorithms “systematically disadvantaging certain groups.”\nZeynep Tufekci, a University of North Carolina professor who studies technology and society, said automated decisions are often based on data collected about people, sometimes without their knowledge. “These computational systems can infer all sorts of things about you from your digital crumbs,” Tufekci said in a recent TED lecture. “They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy.”\nSuch insights may be useful in certain contexts—such as helping medical professionals diagnose postpartum depression—but unfair in others, she said. Part of the problem, she said, stems from asking computers to answer questions that have no single right answer. “They are subjective, open-ended and value-laden questions, asking who should the company hire, which update from which friend should you be shown, which convict is more likely to reoffend.”\nFrank Pasquale, a University of Maryland law professor and author of The Black Box Society: The Secret Algorithms That Control Money and Information, shares the same concerns. He suggests one way to remedy unfair effects may be to enforce existing laws on consumer protection or deceptive practices.\nPasquale points at the European Union’s data protection law, set from next year to create a “right of explanation” when consumers are impacted by an algorithmic decision, as a model that could be expanded. This would “either force transparency or it will stop algorithms from being used in certain contexts,” he said.\nAlethea Lange, a policy analyst at the Center for Democracy and Technology, said the E.U. plan “sounds good” but “is really burdensome” and risked proving unworkable in practice. She believes education and discussion may be more important than enforcement in developing fairer algorithms.\nLange said her organization worked with Facebook, for example, to modify a much-criticized formula that allowed advertisers to use “ethnic affinity” in their targeting.\nOthers meanwhile caution that algorithms should not be made a scapegoat for societal ills. “People get angry and they are looking for something to blame,” said Daniel Castro, vice president at the Information Technology and Innovation Foundation. “We are concerned about bias, accountability and ethical decisions but those exist whether you are using algorithms or not.”']"	['<urn:uuid:7b09b334-5191-419a-a1bf-bfa938c4ce08>', '<urn:uuid:6ac4c8f9-4485-43b7-b764-b48a249516fb>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:38:36.516973	23	119	1688
38	thai customs foreign assignment culture adaptation	The challenges of adapting to Thai customs while on foreign assignment are significant. Thai culture has strict social protocols, including never touching someone's head, avoiding public displays of affection, and showing proper respect to monks and elders. Social status and hierarchy are extremely important, affecting everything from greetings to who pays the bill. For international assignees, understanding these customs is crucial for success, as Thai society has unique elements like the 'Thai smile' which can mask various emotions, and the concept of 'loss of face' which can lead to serious consequences if violated. From an international HR perspective, such cultural adaptation challenges must be carefully considered when selecting and preparing employees for assignments in Thailand.	['Video #51 | August 30, 2018\nIn this video I’m going to introduce you to 13 Thai customs and traditions that you need to know.\nClick on the following link to check out more on Bangkok and Thai Culture.\nWelcome to the fifty-first video of Bangkok Unmasked! The YouTube channel that helps you get the most out of your visit to Bangkok city! If you’re new here, please consider subscribing! In this video I’m going to introduce you to 13 Thai customs and traditions that you need to know.\nSo why is this information important? When we live in a clan, tribe, group or a society we have to follow some set of rules, principles, policies or laws which becomes customs.\nI firmly believe that if you want to fit in, and make real friends during your visit to Thailand, it’s important to understand the local customs and traditions. If possible, one should try to adapt accordingly.\nA good number of the Thai customs and traditions which I’m going to discuss came as a complete surprise to me. I hope that this information makes your holiday, business trip or adventures in Thailand easier. My first couple of years in Thailand were great. That said, they would have been much easier if I had known all this stuff before I got off the plane!\nLet’s get started!\nThai customs and traditions #1. The body.\nThe Thai place great importance and significance to certain parts of the body. The head is seen as the most spiritual part of the body. You should never touch a person’s head. Note. This goes for children too. An affectionate ruffling of a child’s hair might cause offence to Thai parents.\nMost Thai are not touchy-feely. Not at all in fact! It’s not common to see Thai couples holding hands or hugging in public. Kissing and other public displays of affection are an absolute no-no.\nThe feet are seen as dirty and symbolically low. You should never touch somebody or something with your foot. When sitting try to sit with the sole of your foot pointing outwards. The main point here is to avoid pointing your foot at a person, or especially any Buddhist image. Absolute no-no’s are also to hold doors open with your feet, pushing a bag along with your feet, or otherwise using your feet in place of your hands.\nPointing with a finger is also seen as impolite, as is beckoning somebody with a crooked finger. The Thai will often use their lips to point, pushing out the lips to indicate a direction. Alternatively, use the whole hand, with all fingers outstretched, to point. To beckon somebody, the palm should be face down with all fingers extended and the action from the wrist.\nThai customs and traditions #2. Sexual Tolerance.\nThailand has long enjoyed a reputation for sexual tolerance, based more on non-confrontational [rather than progressive] attitudes. The country is very safe for LGBT travelers.\nTranssexuals, also known as krathoeys or ladyboys, are highly visible in mainstream society, from scantily clad teens to high-profile celebrities.\nThere is of course a caveat. While there is sexual tolerance in Thailand this will quickly evaporate if, as mentioned before, public displays of affection are overtly visible. Do whatever you want, within reason, behind closed doors. Do it in public and things will likely go sideways very quickly!\nThai customs and traditions #3. Beware the ‘Thai smile’.\nMany new visitors to Thailand believe that it’s a warm, happy, and welcoming nation because of the constant smiles. Indeed, the Tourism Authority of Thailand leveraged this fact to create the slogan ‘land of smiles’ back in the 1980s.\nHowever, Thai smiles don’t always show happiness. In many instances they’re used as a mask. This is not to detract from the fact that most Thai people are kind, welcoming, and all-round good people. I do though want to emphasize that a smile may not be what you think it is. From my experience, in Thailand situations can go from smiles to extreme violence very quickly. This often leaves foreigner extremely confused as they were misunderstanding the smile. I will do a video on this in the near future.\nThai customs and traditions #4. “Yes” and “no” don’t always mean what you think they do.\nIf a Thai person says no outright, it means a firm no. However, yes doesn’t always mean yes. The Thai generally don’t like letting others down and will often agree to things even if they don’t want to, or even have no intention to, follow through on what they have agreed upon. To say that this can be frustrating is an understatement.\nNote. It’s rare for a Thai person to admit to not knowing something. If for example you’re asking for directions, don’t expect the directions to be correct.\nThen there’s the Thai custom of kreng jai [เกรงใจ] which is somewhat related. To discuss this is a YouTube video in itself. Briefly, it’s customary to be ‘kreng jai’ i.e. to be considerate, and not inconvenience someone else. Many Thai though often take kreng jai to great extremes, and it can lead to some truly crazy situations. One common example is where a group of Thai knows that a senior is talking nonsense, but they don’t/can’t call them out on it due to kreng jai. This often leads to people feeling obligated to follow through on instructions that they know are ridiculous.\nThai customs and traditions #5. Family comes first.\nFamily pretty much always comes first in Thailand. Also, a much greater emphasis is placed on the extended family than in most western countries.\nIn Thailand it’s normal for extended families to live close to each other. It’s also very common for children to be raised by grandparents or aunts and uncles if their parents need to work elsewhere. Younger members of the family are expected to help take care of older members. This is often financially, or by doing a range of chores.\nNote. It’s very common when speaking Thai to also use family terms such as “brother”, “sister”, “aunt”, “uncle”, “mother” and “father” as personal pronouns. This can often apply to strangers where there is no blood relation to them at all.\nOne other note. The Thai-Chinese differ from the Thai in so far as they often have a rigid family hierarchy. Each member of the family has a specific name which depends on their age, and where they are in the family in relation to the other members.\nThai customs and traditions #6. Mai bpen rai.\nA common phrase heard in Thailand is ‘mai bpen rai’. This roughly translates to ‘no problem’, or ‘never mind’. Foreigners often misinterpret the phrase and incorrectly assume that the Thai don’t ever take offense or take things too seriously. To believe this is grossly misunderstanding the Thai. They often do care but have been conditioned to respond in such a way.\nThe Thai rarely display negative emotions. Anger, tantrums, and public crying are uncommon. This however doesn’t mean they aren’t displayed in private, or that the emotions aren’t there. It simply means that a person doesn’t want to lose face by showing their feelings in public.\nThai customs and traditions #7. Social status is important.\nAs an outsider, this point is very easy to miss. Age, family connections, job, education, and income are all factors to a Thai’s perceived status in society. When the Thai meet, they will perform different versions of the greeting, or wai, depending on a person’s status. There are social rules concerning who should offer the first wai. Note. When socializing and a bill is presented, it’s traditional for the person of the highest social status, generally the highest earner, to pay.\nThe Thai also have words that indicate a person’s age. ‘Pee’ is used before an older person’s name to show respect. ‘Nong’ is used for somebody younger than them. With this said, it’s possible for a younger person to be referred to as ‘Pee’ if they have a higher social status. Frankly, it can become very confusing in certain social situations!\nThai customs and traditions #8. Easy ways to show respect.\nThe use of language and the wai are a couple of ways for the Thai to show respect. There are a couple of easy things that foreigners should do in social situations to show respect.\n1. Bring a small gift when you visit someone. Especially at their home. It doesn’t have to be something expensive. Food is a very standard gift. Some decent fruit, or possibly a Western food product that they might like will work.\n2. Always remove your shoes before entering a home. It’s a major sign of disrespect to not do so.\nThai customs and traditions #9. ‘Loss of face’.\nAs most are aware, shaming a Thai [especially publicly] will cause a ‘loss of face’. If I was you, I’d avoid making a Thai lose face. Unless this can’t be avoided. It’s not uncommon for Thai people, particularly in rural areas, to deal with perceived injustices outside of legal frameworks. Acts of extreme violence have been committed due to ‘loss of face’. In some cases, it won’t simply be one individual getting involved. Families, groups of friends, or even entire communities may administer punishment as they see fit.\nThai customs and traditions #10. Patriotism.\nThai society has three pillars; nation, monarchy and religion. National pride is a huge part of the Thai psyche. Many Thai will proudly tell you that they’re the only nation in Southeast Asia to have never been colonized. Frankly, in my opinion, this point is debatable. However, it’s a long conversation for another YouTube video.\nIn many places throughout the country the national anthem will be played twice a day. It’s expected that people will stop and stand until the song has finished. The King’s anthem is also played before a movie starts at the cinema. Again, you must stand for this.\nThai customs and traditions #11. Thai religion is somewhat unique.\nThailand is a Buddhist country. Specifically, Theravada Buddhism. However, the religion practiced in Thailand is somewhat different to other Buddhist nations. Thai Buddhism has been infused with many outside elements. Specifically, Hindu practices and traditional Chinese beliefs. Animist beliefs also play a large role in making Thai Buddhism what it is today.\nWhile in Thailand you’ll notice that many Thai people wear amulets or carry talismans. This is despite Buddhism prohibiting attachment to material objects. Belief in ghosts and spirits is also strong in Thailand. This stems from traditional Animist beliefs. Most Thai houses and office buildings will have spirit houses for example.\nThai customs and traditions #12. Superstitions.\nTo say that superstitions are rife is something of an understatement. Many superstitions are connected to the belief of ghosts and spirits, and they really do drive most critical life decisions.\nIt’s customary for people to consult a fortune teller or monk for an auspicious date before arranging a wedding, buying a home, or test driving a car. As an aside my marriage date, 13 years ago, was determined by my mother-in-law’s visit to a monk in Singburri!\nThe tradition of giving newborn babies a nickname originated from a desire to trick malevolent spirits who may want to steal the baby away. Traditionally, people avoided complimenting parents on their new baby too, fearing making the child seem too desirable to the spirits. There are also superstitions related not being able to attend a wedding if you have had a recent death in the family. Plus, ghosts telling you winning lottery numbers, and more.\nThai customs and traditions #13. Monks are highly respected.\nWhile in Thailand you’re sure to see plenty of monks. They’re pretty much everywhere. Monks are highly revered and respected in Thai culture and disrespecting a monk is a huge no-no.\nMonks generally get designated seats on public transport, and people should always give up their seat to a monk if no other seat is available. Women should be especially careful of their actions around monks. Touching a monk is a big no-no. Also, women must not directly hand things to them. Any offerings should be placed on a monk’s dish, rather than into their hands. Women also mustn’t sit next to a monk, or their belongings.\nIf you act inappropriately around a monk in Thailand, you can be sure that all the Thai around you will quickly point out your errors with great disapproval.\nIn general, the Thai are generally pretty tolerant and understand that foreign visitors will sometimes make mistakes. As long as it’s not something major, like showing disrespect to the religion or monarchy, yelling in public, or walking around semi-naked, most Thai will simply overlook cultural differences.\nAnyway, that’s it for this video. Expect a new video next week.\nFor all you techies out there, this video was shot on a Samsung Galaxy S8 Plus and edited using HitFilm Express.\nTo check out details on arranging a bespoke Bangkok tour with experienced tour guides, please click on the link in this video’s description section.\nFinally, please don’t forget to subscribe to this channel through the button below! Also, I’d like to hear from you if you have any questions or comments regarding Thai customs and traditions. Maybe you know some customs and traditions that I missed. Please do reach out to me through the comments section of this video!\nThank you very much for watching. I’ll see you next week. Goodbye.', 'Employee Job Relocation Case Study Solution\nInternational Human Resource Management\nThe global economy and the technological advancement which have evolved over the years have influenced how multinational organizations carry out their business activities. Today, most organizations cannot remain in the domestic market and make a profit at the same time (Dickmann & Harris, 2005). In this case, most firms have been motivated to send their employees to foreign countries as part of a competitive strategy. They employees who are assigned to work overseas are known as expatriates. The success of sending employees abroad depends on the ability of the organization to hire the right talent. Besides, such employees must be hired at the appropriate time and must be sent to the right countries. The expatriates play a significant role in expanding and developing new markets for their firms abroad (Armstrong & Taylor, 2014). Besides, they control foreign operations by implementing the right technology so as to adapt to the international market. The growing number of the international assignments has changed the role of the human resource management. Sending competent and talented employees is a motivational strategy employed by HR to ensure that they retain their skilled workforce by exposing them to different work environments (Dickmann & Harris, 2005). The success of most global organizations depends on the international assignments. Most global companies send their employees abroad so as to lift the global image reputation of the organization and also to cultivate the potential of their employees. Taking international assignments enhances future career progression and growth or advancement. The employees who are willing to be deployed as expatriates are considered as the greatest asset for the organization (Armstrong & Taylor, 2014). Failure to take international assignments is usually considered as a primary draw back to the organization. Therefore, the employers are required to consider various legal and economic factors before sending their employees abroad to act as expatriates. Despite the advantages of taking international assignments, employees are obliged to consider various factors before declining or accepting the offer.\nVarious factors such as improved salary package, the opportunity for career progression and growth and international cultural experience are some of the factors that may entice employees to accept to relocate to foreign countries (Dickmann & Harris, 2005). However, various expatriation failures may obstruct employees from taking such assignments. Such factors include demographic factors, family related issues, culture and environment of the foreign country, prior experience of the foreign country, and job-related issues.\nAccording to the most studies which have been conducted by various researchers; family matters have been the leading contributing factor that influences the ability of employees to take international assignments. Moving to a foreign country to take up an international assignment not only affect the expatriate but also the entire family (Armstrong & Taylor, 2014). In most cases, the expatriates do consider their spouses and children before making the final decision to relocate to other countries. Sometimes the family members fear for the political and environmental factors in the foreign countries thus discouraging the expatriates from taking international assignments. Other family members who have a prior experience of the performance of the company in the foreign countries may also influence the decision of taking an international assignment.\nThe growing reluctance of the expatriates to take international assignments is caused by the inconvenience and family disruptions one is likely to experience especially with the spouse and the children (Dickmann & Harris, 2005). The willingness to relocate is influenced by various factors such as disruption of the children’s schooling and the career of the partner. In western countries, the expatriates are usually accompanied by their children and spouse when moving to a foreign country. In this case, the spouse will be forced to quit their current job. Besides, the children will be compelled to look for new schools. It is always difficult to secure employment opportunity in a foreign country due to lack of social support and network (Armstrong & Taylor, 2014). As such, the spouse of the expatriate will be forced to become jobless while seeking for a new employment opportunity in a foreign country. The support that family members give to an expatriate plays a significant role in determining whether to take the international responsibility (Noe, Hollenbeck, Gerhart, & Wright, 2006). The attitude of the spouse plays an important role in influencing the employee to take up global roles. Today, most women are full-time employees. In this case, they consider themselves as dual income families. Dual income families are those who hold senior management positions in various organizations. As such, employees will take into consideration whether taking international assignment will influence their professional career. In most cases, those who belong to dual income families find it hard to relocate to foreign countries (Armstrong & Taylor, 2014). This is because relocation will disrupt the cash flow or income. Besides, the spouse will also be reluctant to quit the current position since this will affect the future career prospects. Therefore, employees who are committed to their jobs and families will find it hard to relocate and accept international assignments.\nFamily stress also is a potential factor that influences the willingness of an employee to either take or decline an expatriate role (Dickmann & Harris, 2005). For instance, the ability of the family to adapt to the foreign environment will influence their decision to accept or reject the international assignment. Employees who experience family problems or who are taking care of ailing parents or spouse may opt to decline an international assignment. Therefore, family plays a significant role in determining whether an employee would accept or reject an expatriate position.\nThe packages that employees are expected to get influence their willingness to take international assignments. Most employees will always be motivated to relocate if the organizations they work for a promise to offer the attractive packages (Dickmann & Harris, 2005). Such packages include increased allowances, guaranteed promotion and taking up more challenging roles and responsibilities. If the package an employee is being offered is not satisfactory, the staff may fail to take the international assignment. Most employees do believe that international assignments should be a stepping stone for career progression and advancements. Possible future career advancement is likely to motivate employees to take international assignments. However, employers who give lesser attention to their employees’ progress while finding it hard to convince their employees to take international assignments. The psychological factors related to expatriation also influence the choice of employees in taking up international assignments. The employers who value the non-work needs of employees can quickly affect the decision of their employees in taking international assignments. The ability of the company to continue taking care of the feelings of the employee in the foreign country plays a major role in influencing their decisions (Dickmann & Harris, 2005). Besides, a comprehensive career plan will also motivate employees in taking international assignments. Employees who have been promised of promotion and better packages upon their return would accept the expatriate role quickly. There is a positive correlation between the attractive packages and relocation of an employee in taking expatriate positions. This is because employees who relocate to foreign countries are assured of better packages and promotion upon return after accomplishing the expatriate role.\nThe Environment and Culture of the Foreign Country\nDoing an international assignment will expose an employee to different kinds of environment and cultures (Dowling, 2008). Besides, the employee will learn a new language. As such, the employee will be motivated to take up the expatriate role. The environmental factors that influence the decisions of employees in taking up international assignments include political stability, the cost of living, social amenities such as healthcare services, climatic conditions, and the lifestyles. These factors influence the decision of employees in taking up expatriate positions. On the other hand, culture refers to the values, norms, and practices which people ascribe. The urge to learn a new culture in a foreign country will influence the decision of the employee concerning international assignment. On the other hand, inability to adapt to a new culture and environment will affect the decision made by the employee concerning the global roles. If an employee suspects that the new environment will expose him or her to stress and anxiety then he or she will opt to remain in their home country.\nThe expectations of the employees concerning relocation to the new destination are also instrumental in influencing the decision to relocate (Dickmann & Harris, 2005). Positive and desirable foreign environment will affect the decision of the employee to move and take expatriate roles. The environment and culture are receptive; the employee would be willing to move to take an international assignment. Besides, political stability, favorable living conditions and economic development in a foreign country would motivate the employee to accept an expatriate role.\nEmployee’s Prior Experience to Expatriation\nThe previous relocation experience influences the willingness of the employee to accept an international assignment. Employees who have worked in foreign countries are always motivated to take other international assignments (Noe, Hollenbeck, Gerhart, & Wright, 2006). The ability to adapt to the previous location is a motivating factor. This is because the employee is sure that he or she can quickly adapt to the new environment. As such, they will be willing to accept the international assignments in a foreign country. Employees who have prior experience do possess the requisite skills, knowledge, and attitude that one needs to undertake the expatriate role (Dowling, 2008). The previous experience of an expat role enables employees to form realistic expectations concerning the international assignment. Therefore, prior relocation experience is a contributing factor that motivates employees to take international assignments.\nThe Employee’s Personality Traits\nThe character traits that employees possess play a significant role in accepting international assignments. The employee’s personality traits also determine how employees will be receptive to the expatriate position. In most cases, three personality traits are used to determine whether the employee would be willing to take up an international assignment. Employees who are ready to take persevere, take the risk, and motivated to achieve will be prepared to take expatriate roles (Dickmann & Harris, 2005). These roles are usually linked to an entrepreneur. It is believed that entrepreneurs are always to take up risks and explore new environments. Entrepreneurs are always clear about what they want to achieve (Dowling, 2008). Therefore, they will be willing to take expatriate roles so as to progress in their career. As such, the personality trait that an employee possesses plays a significant role in their willingness to accept an international assignment.\nThe Age of an Employee\nAge is a factor that determines whether an employee will be willing to relocate to another country to take up an international assignment. Employees who are above the age of 50 will not be ready to take expatriate roles since they have family commitments and personal goals that they want to achieve in their home country. Besides, they have reached the peak of their career and may not benefit much from an expatriate role. On the other hand, younger people will be willing to take expatriate positions since they are still looking for a stepping stone to career progression (Dowling, 2008). Besides, international roles will allow them to explore and learn about the new environment. In most cases, younger employees who move to foreign countries to take up international assignments are promised promotion upon their return. As such, they will be motivated to take expatriate roles in foreign countries.\nHealth Condition of the Employee\nThe employee who suffers certain chronic diseases may find it difficult to relocate to foreign countries. For instance, employees who suffer from hypertension and other lifestyle diseases may find it difficult to adapt to the new environment. As such, they will be willing to remain in the home country rather than moving to foreign countries to take up international assignments.\n- Armstrong, M. and Taylor, S., 2014. Armstrong’s handbook of human resource management practice. Kogan Page Publishers.\n- Bender, S. and Fish, A., 2000. The transfer of knowledge and the retention of expertise: the continuing need for global assignments. Journal of knowledge management, 4(2), pp.125-137.\n- Baruch, Y., Budhwar, P.S. and Khatri, N., 2007. Brain drain: Inclination to stay abroad after studies. Journal of world business, 42(1), pp.99-112.\n- Bonache, J., Brewster, C. and Suutari, V., 2001. Expatriation: A developing research agenda. Thunderbird International Business Review, 43(1), pp.3-20.\n- Brewster, C., Sparrow, P. and Harris, H., 2005. Towards a new model of globalizing HRM. The International Journal of Human Resource Management Group, 16(6), pp.949-970.\n- Chiaburu, D.S. and Marinova, S.V., 2005. What predicts skill transfer? An exploratory study of goal orientation, training self‐efficacy and organizational supports. International journal of training and development, 9(2), pp.110-123.\n- Carr, S.C., Inkson, K. and Thorn, K., 2005. From global careers to talent flow: Reinterpreting ‘brain drain’. Journal of World Business, 40(4), pp.386-398.\n- Daniels, J.D., Radebaugh, L.H., Sullivan, D.P. and Salwan, P., 2009. International business. Pearson Education India.\n- De Jong, J.P. and Den Hartog, D.N., 2007. How leaders influence employees’ innovative behaviour. European Journal of innovation management, 10(1), pp.41-64.\n- Dickmann, M. and Harris, H., 2005. Developing career capital for global careers: The role of international assignments. Journal of World Business, 40(4), pp.399-408.\n- Dowling, P., 2008. International human resource management: Managing people in a multinational context. Cengage Learning.\n- Jokinen, T., 2005. Global leadership competencies: a review and discussion. Journal of European Industrial Training, 29(3), pp.199-216.\n- Harzing, A.W. and Pinnington, A. eds., 2010. International human resource management. Sage.\n- Kraimer, M.L., Wayne, S.J. and JAWORSKI, R.A.A., 2001. Sources of support and expatriate performance: The mediating role of expatriate adjustment. Personnel Psychology, 54(1), pp.71-99.\n- Marler, J.H., Woodard Barringer, M. and Milkovich, G.T., 2002. Boundaryless and traditional contingent employees: Worlds apart. Journal of Organizational Behavior, 23(4), pp.425- 453.\n- Noe, R.A., Hollenbeck, J.R., Gerhart, B. and Wright, P.M., 2006. Human resource management: Gaining a competitive advantage.\n- Stahl, G.K., Miller, E.L. and Tung, R.L., 2002. Toward the boundaryless career: A closer look at the expatriate career concept and the perceived implications of an international assignment. Journal of World Business, 37(3), pp.216-227.\n- Yan, A., Zhu, G. and Hall, D.T., 2002. International assignments for career building: A model of agency relationships and psychological contracts. Academy of Management Review, 27(3), pp.373-391.']	['<urn:uuid:e03aa467-ead0-4f64-9e59-1f370ece08e6>', '<urn:uuid:19f9a3f1-1954-4d36-a5e2-02e6fecb4138>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	6	115	4631
39	What happened in the year 712 at the Synod of Constantinople when Emperor Philippicus tried to re-establish Monothelitism?	At the Synod of Constantinople in 712, called by Emperor Philippicus who favored Monothelitism, the majority of Greek bishops, including Germanus, reportedly yielded to the imperial will to condemn the Acts of the Sixth General Council of 681 and re-establish Monothelitism.	['Patriarch of Constantinople (715-30), born at Constantinople towards the end of the reign of Emperor Heraclius (610-41); died there 733 or 740. The son of Justinianus, a patrician, Germanus dedicated himself to the service of the Church and became a cleric at the cathedral of the metropolis. Some time after the death of his father, who had filled various high official positions, at the hands of the nephew of Heraclius, Germanus was consecrated Bishop of Cyzicus, but the exact year of his elevation is not known. According to Theophanes and Nicephorus, he was present in this capacity at the Synod of Constantinople held in 712 at the insistance of the new emperor, Philippicus, who favoured Monothelitism. The object of the council was to re-establish Monothelitism and to condemn the Acts of the Sixth General Council of 681. Even Germanus is said to have bowed to the imperial will, with the majority of the Greek bishops. However, immediately after the dethronement of Emperor Philippicus (713) his successor, Anastasius II, restored orthodoxy, and Monothelitism was now definitively banished from the Byzantine Empire. If Germanus really yielded for a short time to the false teachings of the Monothelites, he now once more acknowledged the orthodox definition of the two wills in Christ. John, Patriarch of Constantinople, appointed by Philippicus to succeed the deposed Cyrus, sent to Pope Constantine a letter of submission and accepted the true doctrine of the Church promulgated at the Council of 681, whereupon he was recognized by the pope as Patriarch of Constantinople. On his death Germanus was raised to the patriarchal see (715), which he held until 730. Immediately (715 or 716) he convened at Constantinople a synod of Greek bishops, who acknowledged and proclaimed anew the doctrine of the two wills and the two operations in Christ, and placed under anathema Sergius, Cyrus, and the other leaders of Monothelism. Germanus entered into communication with the Armenian Monophysites, with a view to restoring them to unity with the Church, but without success. Soon after his elevation to the patriarchal dignity the Iconoclastic storm burst forth in the Byzantine Church, Leo III the Isaurian, who was opposed to the veneration of images having just acceded to the imperial throne (716). Bishop Constantine of Nacoleia in Phrygia, who like some other bishops of the empire condemned the veneration of the pictures and images of Christ and the saints, went to Constantinople, and entered into a discussion with Germanus on the subject. The patriarch represented the traditional use of the Church, and sought to convince Constantine of the propriety of reverencing images. Apparently he was converted to the teaching of the patriarch, but he did not deliver the letter entrusted to him by Germanus for the Metropolitan of Synnada, for which he was excommunicated. At the same time the learned patriarch wrote to Bishop Thomas of Claudiopolis, another Iconoclast, and developed in detail the sound principles underlying the reverencing of images, as against the recent innovations. Emperor Leo III, however, did not recede from his position, and everywhere encouraged the iconoclasts. In a volcanic eruption between the islands of Thera and Therasia he saw a Divine judgment for the idolatry of image- worship, and in an edict (726) explained that Christian images had taken the place of idols, and the venerators of images were idolaters, since, according to the law of God (Exodus 20:4), no product of the hand of man may be adored. Immediately afterwards, the first Iconoclastic disturbances broke out in Constantinople. The Patriarch Germanus vigorously opposed the emperor, and sought to convert him to a truer view of things, whereupon Leo attempted to depose him. Germanus turned to Pope Gregory II (729), who in a lengthy epistle praised his zeal and steadfastness. The emperor in 730 summoned the council before which Germanus was cited to subscribe to an imperial decree prohibiting images. He resolutely refused, and was thereupon compelled to resign his patriarchal office, being succeeded by the pliant Anastasius. Germanus withdrew to the home of his family, where he died some years later at an advanced age. The Ecumenical Council of Nicaea (787) bestowed high praise on Germanus, who is venerated as a saint in both the Greek and the Latin Church. His feast is celebrated on 12 May. Several writings of Germanus have been preserved, viz., “Narratio de sanctis synodis”, a dialogue “De vitae termino”, a letter to the Armenians, and three letters on the reverencing of images, as well as nine discourses in the extravagant rhetorical style of the later Byzantines. Of doubtful authenticity is the “Historia ecclesiastica et mystica”, also attributed to him.']	['<urn:uuid:32cd946f-df88-44f8-b959-d019332a261d>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	18	41	769
40	What are the differences in admission requirements between Hammitt Junior-Senior High and King Academic School in terms of who can attend?	Hammitt Junior-Senior High School admits students through referrals from their local public school districts based on their need for intensive special education services, particularly those with behavioral issues or autism. In contrast, King Academic School of Excellence requires students to reside within Sheldon ISD boundaries, have passed the Algebra I STAAR exam, be designated as at-risk, and either have 10+ credits or undergo a special review process if they have fewer credits.	"['Students at Hammitt Junior-Senior High School (ages 12-21) are referred to the program by their local public school district based on their need for more intensive special education services to achieve success in school. Many students attending Hammitt Junior-Senior High School have a history of verbal or physical aggression which has resulted in suspension from the public school setting. Student profiles may also include conflicts in their relationships with home/family members, a history of police/juvenile probation involvement, psychiatric interventions including prescription medications and/or in-patient treatment, truancy, low self-esteem, below grade level in academic achievement, poor social skills, and a severe lack of anger control and coping skills.\nStudents with autism who attend Hammitt Junior-Senior High School typically represent the more severe students on the Autism Spectrum, with complex sensory, communication, and behavioral needs that have significantly impacted their ability to function independently and make educational progress in less restrictive learning environments.\nHammitt Junior-Senior High School is committed to making school a positive learning experience for our students.\nStaff form partnerships with the referring public school district, parent/guardian, and other community-based learning sites and services to provide our students with a challenging educational experience that will prepare them for adulthood. We encourage each student to develop positive relationships with others in their school, home, and community environments and to excel at communication, cooperation, and learning.\nThe key components of Hammitt Junior-Senior High School include:\n- A highly structured, positive learning environment to build self-esteem and encourage achievement.\n- A flexible curriculum and personalized, interactive instruction.\n- Small class sizes and a low student-to-faculty ratio.\n- Community-based learning experiences focused on career development and independent living skills.\n- Integrated computer technology to assist students with instruction and build basic computer competencies. We have a full computer lab and a full-time educational technology teacher. We utilize smart boards, iPads, and other tablets in our classrooms.\n- Therapeutic support services including individual/group counseling and interventions for students in crisis.\n- Summer educational services as needed.\nAcademic and Instructional Programming\nInstructional services are provided to students based on their special education needs and learning objectives outlined as part of the student’s Individual Educational Plan (IEP) developed with the student, parent, and referring school district. Academics are taught by special education certified staff, focusing on each student’s unique learning needs. Learning has been a source of frustration for many of our students, so Hammitt Junior-Senior High School provides additional staffing and technology supports to assist students with successful work completion and earning credits towards graduation. Speech/Language and School Nursing services are provided as specified in the IEP and on a consultant basis to classroom staff to enhance the provision of instructional classroom services. Community-based learning sites, career exploration, and volunteer service opportunities are included with traditional academic instruction as part of the overall learning experience. Daily Living Skills including budgeting, consumer economics, nutrition, and meal preparation are taught in our fully equipped kitchen lab. Life-long health and wellness is emphasized as part of the physical education program. Life and leisure activities are utilized to bring awareness to students about the need for healthy life choices and the importance of physical fitness.\nVocational Education/Skill Development Services\nAfter entering the program, students are assessed to determine their career interests, goals, and preparation for entering the world of work. Students at Hammitt Junior-Senior High School are either involved in career exploration through hands-on involvement in community-based learning sites, opportunities to experience work through on-campus work jobs, or paid employment through a formal work-study placement with a community employer or sheltered workshop.\nAs part of the Vocational curriculum, classrooms also volunteer as a group to help those less fortunate. Several classrooms help out at Midwest Food Bank to pack care packages for people that have become victims of natural disasters or assist at the Center of Hope Food Pantry to help those in need of food and basic items. Through these experiences, students can practice social skills needed in a work environment and contribute volunteer service learning hours as part of their career exploration activities.\nThe Vocational Educational Program also partners with different community employers to assist students in developing vocational skills through hands-on learning at a job site. As part of this Skill Development Program, students go to work once a week with a job coach and experience the world of work in a community-based employment setting. If you are interested in becoming a Skill Development Partner with Hammitt Junior-Senior High School, call (309) 452-1790 and ask for the Coordinator of Community Education.\nThe Hammitt Junior-Senior High School clinical staff provides counseling, crisis intervention, and other therapeutic supports to the students and families served through the program. These staff members provide clinical consultation and crisis response to assist all staff at HJSHS in their interventions with students. Clinical staff members also provide assistance to teaching staff by collecting data, coordinating information regarding the student and family with other service providers, and implementing behavior intervention strategies consistent with the social learning model of the program. Throughout the day, they have the flexibility to pull students from class and provide that one-on-one encouragement that our students need for success in school. Counseling groups are also conducted once a week to help students with life and social skills needed to be successful in their homes and communities.\nHammitt Junior-Senior High administrators, instructional staff, related service personnel, and clinical staff work together as a therapeutic team; committed to assisting students in developing a wide range of skills that are necessary to succeed in all aspects of life. Students completing the Hammitt Junior-Senior High program show\nimproved attendance rates, increased achievement of credits towards graduation, and are given an opportunity to return to their local public school for completion of high school as appropriate. Some students remain in the program through graduation from high school, with staff providing case management and support services to transition these students into post-secondary settings. The continual success of our HJSHS students, both in and out of the classroom, is a reflection of the dedication of this team and the partnership with students, families, and the referring school district.\nStudents are referred to Hammitt Junior-Senior High School by their local public school district, through the IEP process, based on their need for more intensive special education services to achieve success in school. Please contact your public school’s social worker or another staff person for more information.\nVocational Business Partners\nWe are fortunate to have a variety of employers who partner with Hammitt Junior-Senior High School. Here is a list of our current employment partners in the community.\nThere are ongoing needs to help support the programs and services of The Baby Fold. The document below contains specific needs for Hammitt Junior-Senior High.', 'King Academic School of Excellence\nKASE offers the students of Sheldon ISD a high school experience without the distractions of a traditional high school.\nWe offer a flexible schedule that allows you the ability to better manage the challenges of life. We have a morning session from 7:10 a.m.—11:10 a.m. and an afternoon session from 11:40 p.m.—3:40 p.m. You are required to come to one of the sessions five days a week. You also have the option to stay for the entire day, which allows you to finish your courses faster and graduate sooner.\nYou are able to complete the requirements for the foundation graduation plan as well as the foundation plan with an endorsement.\nOne of the amazing options that KASE offers is the ability for you to complete your high school courses in the morning and take dual credit courses at San Jacinto College in the afternoons or evenings. It is quite possible for you to graduate from high school with an industry certification. This would allow you to start working in a well paying job straight out of high school.\nWho is eligible to attend KASE?\n- You must reside within the physical boundaries of Sheldon ISD.\n- You must not have already received a GED or high school diploma.\n- You have to be designated as at risk of not graduating based on the Texas at risk indicators.\n- You must have previously passed the Algebra I STAAR exam.\n- If you have 10 or more credits you will be admitted to KASE if seats are available.\n- If you have fewer than 10 credits you will have to meet with your Academy Principal, Academy Counselor, KASE Academy Representative, and your parent(s) in order to decide whether KASE is the appropriate instructional setting for you.\nHow do I apply for KASE?\nYou can print the application from the Documents page here on this web site or get one from your counselor at KHS. You will need to fill it out and return it to your counselor or bring it directly to KASE.\nCan I walk at CE King High School\'s graduation?\nYes, when you have met all graduation requirements you will be able to participate in the next available CE King High School graduation ceremony. You can also take part in Prom and Homecoming.\nWhat does it mean to be ""At Risk""?\nThere are several criteria that could qualify you as being at risk. You are at risk if you meet any of the following criteria:\n- You made a 70 or below in two or more classes this year or last year.\n- You were not advanced from one grade level to the next for one or more school years.\n- You did not pass one of your end of course exams in the past year.\n- You are pregnant or you are a parent.\n- You were placed at a discipline alternative campus this year or last year.\n- You were expelled from school this year or last year.\n- You are on parole or probation.\n- You have previously dropped out of school.\n- You are designated as a student of limited English proficiency.\n- You have been in the custody of the Department of Protective and Regulatory services within the last two years.\n- You are designated as being homeless.\n- You resided in a residential placement facility in the district, including a detention facility, substance abuse treatment facility, emergency shelter, psychiatric hospital, halfway house, or foster group home in the preceding school year or in the current school year.\nWhat will be expected of me once I am accepted into KASE?\nYou are required to be in attendance at KASE at least 20 hours a week plus one hour a week for each STAAR exam that you have yet to pass. If you fail to meet this requirement you will be placed on a probationary period of three weeks. If you fail to meet the attendance requirement during the probationary period and you cannot produce a valid reason/excuse for your lack of attendance, you will be removed from the KASE program. The KASE campus is open from 7:10 a.m.—3:40 p.m. each day. You can make arrangements to attend other times in order to make up hours.\nYou are required to complete a minimum of .5 credits each three week period to stay in good standing. If you fail to meet this minimum requirement, you will be placed on a three week probationary period. During this probationary period you will be required to complete the .5 credit that you did not complete in the previous three week period along with another .5 credit. If you fail to meet the terms of your probation, you and your parents will be required to meet with a KASE administrator to discuss possible removal from the KASE program.\nStudents are expected to treat the staff and each other with civility.\nIs there a dress code at KASE?\nYes, students are expected to follow the Sheldon ISD dress code.\nWhat technical certification programs can I enroll in at San Jacinto College?\nThis new program will prepare you to work as a supervisor in the construction industry.\nCollision Repair (ABDR) students are given the opportunity to learn the skills necessary to repair and refinish collision-damaged automobiles following nationally recognized training standards.\nThe cosmetology high school operator dual credit certificate of technology program is a course of study designed to meet the needs of high school students who desire to enter the beauty industry in a minimum amount of time.\nStudents who complete this program go to work as law enforcement professionals, in emergency Management, homeland security, and in various positions in social work, corrections, probation, and parole.\nThe Culinary Arts program offered at the San Jacinto North campus will prepare you for a career as a pastry chef.\nThis program will prepare you for a career as a diesel mechanic. Every student who has completed this program has gotten a position upon completion.\nThis program will prepare you to do household wiring, as well as cooperate construction and electrical related fields in the petro chemical industry.\nThis program will teach you the skills that you need in order to be a successful welder in a manufacturing or industrial setting.\nThe Pharmacy Technician Program at San Jacinto College provides graduates with the skills and knowledge that will enable them to qualify for entry-level positions in pharmacies as well as prepare them for national certification.\nThis program will help you to develop the skills that are necessary to assist doctors in keeping their practices running smoothly.']"	['<urn:uuid:eaeab0b0-a4cd-4b19-8f4d-9188b5dfc8f2>', '<urn:uuid:24ae7461-f4b2-4d2f-9816-9b4ce27b1a8d>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T18:38:36.516973	21	72	2229
41	german clay bowls history make colors materials	German pottery production has a long history, particularly in the Eifel/Rhine area. The Mayen ware vessels typically came in khaki, brown, buff, or pale orange colors, with a vitrified clay matrix containing sandstone fragments, feldspar, and quartz. Later developments included Rhenish stoneware, which came in buff to dark gray with brown speckled slip, or light gray with cobalt blue and manganese purple decorations. The stoneware had a hard, impermeable body with low porosity and was salt-glazed. These wares were primarily wheel-thrown and often featured elaborate decorations including applied molded relief ornaments, incising, stamping, and rouletting.	"['A related group of coarse ware pottery was produced at a number of sites in the Eifel/Rhine area of Germany which in aggregate is frequently referred to as ‘Eifelkeramik.’ Included amongst the production areas are Speicher, Mayen and Urmitz, where pottery was produced during both the Roman and medieval periods. Samples from both Speicher and Mayen are included here, and the two fabrics are described separately below.\nBriefly, Mayen ware contains sedimentary rock inclusions, while Speicher ware is restricted to quartz. They are united however by being very hard with a harsh ‘pimply’ surface – due to protruding inclusions, hackly fracture and frequently mottled surfaces. In Britain, both typically occur as ledge-rim or bead-rim jars, and dishes. Roman mortaria, best known from early Tiberio-Claudian levels at Sheepen (Hartley 1985b), are in allied fabrics and have been associated with production in this region (Williams 1985, 1993), but at this time it has not been possible to include samples of these vessels in the collection. Richardson (1986, 110) has also allied a group of 3rd century mortaria to production at Speicher and Urmitz.\nThese wares are part of a long tradition of pottery making, continuing into the medieval period, and known from kilns throughout the region. Inclusions in the fabric are compatible with the regional geology, as summarised by Richardson (1986, 109–10).\nFulford, M G, & Bird, J, 1975 Imported pottery from Germany in late Roman Britain, Britannia 6, 171–81\nGose, E, 1976 Gefässtypen der römischen Keramik im Rheinland, Cologne (2nd ed)\nHartley, K F, 1985b The mortaria – discussion and conclusions, in Sheepen: an early Roman industrial site at Camulodunum (R Niblett), CBA Res Rep 57, 92 and microfiche 2E7-G5\nRedknap, M, 1988 Medieval pottery production at Mayen: recent advances, current problems, in Zur Keramik des Mittelalters und der beginnenden Neuzeit im Rheinland: Medieval and later pottery from the Rhineland and its markets (eds D R M Gaimster, M Redknap & H-H Wegner), BAR Int Ser 440, 3–37\nRedknap, M, 1995 Mayen ware from Canterbury, in Excavations in the Marlowe Car Park and surrounding areas. Part 2: the finds (K Blockley, M Blockley, P Blockley, S S Frere & S Stow), The archaeology of Canterbury 5, 737–40\nRichardson, B, 1986 The waterfront group: coarsewares and non-samian finewares, in The Roman quay at St Magnus House, London. Excavations at New Fresh Wharf, Lower Thames Street, London 1974–1978 (L Miller, J Schofield & M Rhodes), London Middlesex Archaeol Soc Spec Pap 8, 106–38\nWilliams, D F, 1985 Petrological analysis of the mortaria, in Sheepen: an early Roman industrial site at Camulodunum (R Niblett), CBA Res Rep 57, 93\nWilliams, D F, 1993 The petrology of mortarium fabrics 5, 6, 11 and 13, in Report on the excavations at Usk 1965–1976. The Roman pottery (ed W H Manning), 424–5\nThis is also known as German Mayener ware.\nMayen ware vessels are frequently khaki (2.5YR 5/3–5/4, 2.5YR 6/3–6/4) or brown (10YR 4/2), buff (10YR 8/3–8/4) to pale orange or cream (10YR 7/4, 7.5YR 7/6) both on the surface and break, occasionally with a light grey (7/0-6/0) core.\nA vitrified clay matrix, appearing very dense and clean, is typical of the fabric. It is characterised by abundant ill-sorted subrounded red-brown sandstone or finer rock fragments (0.2–3.0mm). The remaining inclusions range between 0.3–2.0mm, but normally do not exceed 1.0mm. Feldspar is abundant to common, frequently with visible cleavage planes, followed by fewer accessory minerals as common to sparse. Finally, sparse quartz is also present. Varying quantities of red iron-rich inclusions (0.1–0.5mm) are present, some of which may be sedimentary rocks but are too small to identify accurately in the hand specimen.\nThe clay contains common ill-sorted subangular fine to very coarse-grade sand, including fragments of a very fine quartz sandstone, elongate grains of siltstone/slate, twinned and untwinned feldspar, greenish/tawny pleochroic clinopyroxene, porphyritic lava and brown amphibole.\nCanterbury Museums; Museum of London\nCanterbury Museums; Department of Medieval & Later Antiquities, The British Museum (sherds in Medieval reference collection); Museum of London; Rheinisches Landesmuseum, Bonn (Germany); Landesamt fur Denkmalpflege, Abteilung der Archäologische Denkmalpflege Amt, Koblenz (Germany) (medieval); Eifeler Landschaftsmuseum, Genovevaburg, Mayen (Germany)\nEiden, H, 1982 Ausgrabungen an Mittelrhein und Mosel in 1963–1976, Trier\nRedknap, M, 1987 The Roman and medieval pottery industry of the West German Eifel, Unpublished PhD, Institute of Archaeology, University of London\nRedknap, M, 1999 Die römischen und mittelalterlichen Töpfereien in Mayen, Kreis Mayen-Koblenz, Berichte zur Archäologie an Mittelrhein und Mosel 6, 1-401\nUnverzagt, W, 1916 Die Keramik des Kastells Alzei, Materialen zur römisch-germanischen Keramik 2, Frankfurt-am-Main\nAlso known as German Speicherer ware.\nOur sample, from Speicher itself, is pale orange or cream (7.5YR 8/6) with darker (7.5YR 7/6, 7.5YR 6/3) slightly mottled surfaces. Although not all sherds are of this colour, it is typical of the ware.\nThis fabric is similar to Mayen products, but in the hand appears to be dominated by quartz rather than rock fragments. Inclusions are well sorted with abundant quartz set in a dense clean clay matrix. Most quartz, including polycrystalline grains, ranges between 0.3–2.2mm, but is usually 0.5–1.0mm, while sparse smaller grains measuring <0.2mm occur consistently. In addition, sparse red-brown iron-rich inclusions (0.2–1.0mm), white or pink clay pellets (0.6–5.0mm) and feldspar (0.8mm) can be seen in the hand specimen.\nIn thin section an isotropic clay matrix, containing sparse silt-grade quartz with common to abundant subrounded to rounded inclusions (frequently 0.5–1.0mm), sometimes metamorphoed, is visible. In the larger size range both monocrystalline and polycrystalline quartz, medium to coarse-grade sandstone and siltstone are common; quartzite and chert are rare. Opaques, in mixed sizes, and large clay pellets are rarely identified.\nDepartment of Greek & Roman Antiquities, The British Museum\nDepartment of Greek & Roman Antiquities, The British Museum; Museum of London; Rheinisches Landesmuseum, Trier (Germany)\nGoethert-Polaschek, K, 1977 Speicher-Herforst, in Führer zu Sudwestliche Eifel. Bitburg, Prüm, Daun, Wittlich, Führer zu Vor- und Frühgeschichtlichen Denkmälern 33, 200–7 (summary of excavations with sources)\nHussong, L, & Cüppers, H, 1972 Die Trier Kaiserthermen. Die spätrömischen und frühmittelalterliche Keramik, Trierer Grabungen und Forschungen 1, Mainz (key sites to regions)\nLoeschcke, S, 1922 Tonindustrie von Speicher und Umgebung, Trierer Heimatblätter 1, 5–13\nLoeschcke, S, 1923 Tonindustrie von Speicher und Umgebung, Trierer Heimatblätter 2, 11–22\nLoeschcke, S, 1931 Die römischen Ziegelöfen im Gemeindewald zu Speicher, Trierer Zeitschrift 6, 1–7', 'Colonial period Rhenish stoneware is salt-glazed stoneware that comes in two major varieties: a buff to dark gray bodied ware coated with a speckled brownish slip, and blue on gray, a light gray bodied ceramic often colored with cobalt blue and/or manganese purple pigments. Both varieties can be decorated with incised and/or applied molded relief decorations. An unpainted very light gray to off-white stoneware called Höhr ware, typically produced c. 1675 to 1750, forms a less commonly found variety of Rhenish stoneware.\nTrue stoneware was developed in Germany at the end of the 13th century, and was exported to England in the 14th century (Gaimster 1997:35, 79). This trade with England peaked in the 17th century (Gaimster 1997: 82). The towns of Cologne and Raeren were leading stoneware production centers in the first half of the 16th century, and the term ""Cologne ware"" was popularly applied to all mottled brown stonewares. By the mid-16th century, the nearby town of Frechen had replaced Cologne as a pottery center, and supplanted Raeren as the leading exporter of brown stoneware to England (Gaimster 1997:193, 209, 225). This trade began to decline in the mid-17th century, and the development of English brown stoneware in the late 17th century greatly diminished the demand for Rhenish brown stoneware. However, it was still imported in limited numbers into the 1770s, and production of brown stoneware continued in Frechen until the mid-19th century (Gaimster 1997:210-211; Noël Hume 2001:127).\nBlue on gray stoneware developed in Raeren in the mid-16th century, but primary production had shifted to the Westerwald region by the end of that century. Although Westerwald products were less common than the Frechen-type brown stonewares in England before ca. 1650, by the latter part of the 17th century, and throughout much of the 18th century, blue on gray stonewares dominated the English import market (Gaimster 1997:94, 226, 252). In the Chesapeake region, Rhenish brown and blue on gray stonewares were in use from the earliest days of colonial settlement. Although Rhenish brown use declined in the late 17th century, the blue on gray wares continued to be imported in large numbers until the 1770s (Gaimster 1997:253; Noël Hume 2001:107) and remained popular until the end of the 18th century (Skerry and Hood 2009:59).\nAround 1860, there was a revival of the Rhenish stoneware tradition in Germany, in which the vessels that had been produced centuries earlier were often imitated. This movement lasted into the 20th century and, in fact, stoneware is still being made in the Westerwald region today (Gaimster 1997:251, 325; Noël Hume 2001:110).\nRhenish stoneware has a hard, impermeable body with low porosity. It can be partly vitrified, particularly on the blue on gray wares. The paste of the brown wares is generally coarser than that of the blue on gray, and can have occasional inclusions. However, it is not as grainy as typical English Fulham-type brown stoneware and does not include the dark flecks of hematite characteristic of English brown stoneware (Skerry and Hood 2009:65). The paste colors of Rhenish stonewares range from off-white to various shades of brown and gray, and the interior vessel surface is often a different color than its exterior. A dark gray paste is common on Frechen vessels (as it was on Cologne products), but these pieces can range from light gray to buff to reddish brown. Blue on gray type wares are most often a light gray, but can also have a buff color. Rhenish stoneware vessels were primarily wheel thrown. Templates used to shape 18th- and 19th-century blue on gray vessels often left shallow chatter marks visible in on the vessel surface (Gaimster 1997:220, 256; Noël Hume 2001:106).\nRhenish stoneware was salt-glazed. This technique was gradually introduced in the 15th and 16th centuries. On Rhenish brown stoneware, a brown engobe or a wash thinner than the engobe was added to the surface before firing. On the finished vessels, this slip appeared in various shades of brown, and was mottled to differing degrees. This mottling led to the popular name of ""tiger ware."" On blue on gray wares, no slip was added (Gaimster 1997:40, 47).\nRhenish stonewares are notable for their often elaborate decorations, with applied molded relief ornaments (sprig molds) being perhaps the most characteristic element. Other common decorative techniques included incising (engraving), stamping, and rouletting, as well as cordoning around rims, necks, and bases. Diaper carving and piercing (on double-walled vessels) also occurred.\nApplied ornaments came in a wide variety of motifs. On brown stoneware, one of the most typical was the ""Bartmann"" face mask, also known as ""Bellarmine"" or ""graybeard” (Skerry and Hood 2009:9). Bartmann bottles, which were made from the early 16th century through the mid-18th century, are quite common on 17th-century Chesapeake sites. As a general rule, the face mask tended to become more ""debased,"" or grotesque or cartoonish, in appearance over the course of the 17th century. However, high quality and poor quality masks can be found on contemporary vessels, so this is not a reliable dating marker. Molds used to create the masks were sometimes dated; since these molds were used over long periods, the dates cannot be used as a reliable indicator of vessel manufacture date (Skerry and Hood 2009:9).\nAnother common applied ornament motif on Rhenish stoneware was the armorial or heraldic medallion. These were sometimes dated, but cannot be used as a totally reliable temporal indicator, as molds could remain in use for a number of years. Heraldic and portrait medallions continued to appear on blue on gray vessels throughout the 17th century and into the early 18th century (Gaimster 1997:148). On vessels intended for export to England or its colonies in the late 17th and 18th centuries, the royal initials ""WR,"" ""AR,"" and ""GR"" were often added. GR mugs, tankards and jugs continued production in the third quarter of the 18th century (Skerry and Hood 2009:47), commemorating King George I (1714-1727), King George II (1727-1760) and King George III (1760-1820).\nBeginning around 1630, blue on gray stoneware was often decorated with a combination of applied and incised elements (Noel Hume 2001:105). Common applied motifs—roundels, fleur-de-lis, and floral sprigs, including tulips and daisies—were used in conjunction with incised lines of foliage, often created with a two or three-tined rake tool (Noel Hume 2001:105). Cobalt blue and often manganese were used to highlight the incising and applied elements.\nBy the end of the 17th century, decorations on blue on gray stoneware vessels were becoming more schematic, relying on stamped and incised motifs, with the incised foliage and scrolls either outlined or filled with blue pigment (Noel Hume 2001:105). It also became more common to stamp designs, rather than apply them as sprigged elements. These stamped designs commonly included hearts, circles, triangles, and floral motifs. To determine when specific decorative elements were introduced and later declined, consult the more detailed references listed below (Reineking-von Bock 1971; Gaimster 1997; Noël Hume 1970:282; Noel Hume 2001:118, 126).\nCobalt blue painted under the glaze was a typical addition to blue on gray stoneware beginning in the second half of the 16th century, and occasionally appeared on Rhenish brown. Manganese purple appeared as early as the 1630s, but did not become common until the second half of the 17th century. The combined use of cobalt and manganese on a single vessel body is characteristic of the late 17th-century (Glenn 2002). These vessels, often decorated with applied floral sprigs with incised stems, seem to date to the last two decades of the century. The use, however, of manganese to decorate rilled necks of jugs continued into the first half of the 18th century.\nAround 1675, off-white or very light gray-bodied salt glazed stoneware with no paint was introduced, and remained in production through the first half of the 18th century. This is sometimes known as ""Höhr ware,"" after a village in the Westerwald (Gaimster 1997:225, 252; Noël Hume 2001:94).\nPewter or silver lids and ornaments were attached to some Rhenish stoneware vessels. Ghost images of these mounts, as well as the lug holes where they were attached, are sometimes visible on handle sherds (Gaimster 1997:108; Noël Hume 2001:109\nRhenish stoneware was used primarily for storage, service and consumption, and sanitary purposes. Among the Frechen-type brown stoneware, globular bottles and jugs in various sizes are the most common forms found in the Chesapeake region. Drinking vessels were also imported, as were simple lozenge-shaped mineral water bottles in the 18th and 19th centuries.\nA variety of Rhenish blue on gray stoneware vessel forms are found on American archaeological sites, with more variation in form and decoration apparent prior to the 18th century. Elaborate panel bottles and jugs, as well as ball-belly pitchers, occur on some of the earliest sites. Straight-sided tankards and round mugs (globular-bodied drinking vessels also known as gorges) were introduced after ca. 1650. Tankards, mugs, and jugs were produced to hold standard capacities, and by around 1715, capacity numbers ranging from 1 to 10 were sometimes incised or painted on jugs and drinking vessels (Noel Hume 2001:103; Skerry and Hood 2009:43).\nChamber pots and straight-sided tankards became popular at the beginning of the 18th century, and along with mugs, pitchers, and jugs, are the most common forms on Chesapeake sites. By the mid-18th century, the rise of English refined earthenware and molded white salt-glazed stoneware led to a decline in the popularity of Westerwald-type tablewares among wealthy Chesapeake households, but storage and sanitary vessels were still used. Other blue and gray stoneware forms include porringers, teapots and tea bowls, salts, terrines, plates, storage jars, jardinieres, and figurines (Gaimster 1997:34, 103-104, 252-253: Noël Hume 2001:125)\nGaimster 1997; Gawronski 2012; Glenn 2002; Gusset\nHume 1970; 2001; Reineking-von\nBock 1971; Skerry and Hood 2009\nRhenish stoneware was produced in the Rhine River valley of Germany and the Low Countries. Similar stoneware was also made in other parts of Germany and Central Europe, but few of these ceramics appear in North America.\nFrechen and the Westerwald were the most common sources of Rhenish stoneware found in the Chesapeake. Their products have been found on 16thand 17th century Native American\nsites. Rhenish vessels such as Frechen-type Bartmann bottles and Westerwald jugs have been found in early colonial contexts at sites like Jamestown and Martin’s Hundred. On most 17th century Chesapeake archaeological\nsites, Frechen-type stoneware constitutes a majority of the Rhenish recovered, at times making up to 80% of the stoneware assemblage. However, Westerwald-type vessels such as jugs, tankards, and pitchers are also found in significant numbers (Gaimster 1997:98-103).\nA great big thanks to Dr. Meta Janowitz, AECOM Senior Material Specialist, Burlington, New Jersey, for her assistance with the preparation of this essay.']"	['<urn:uuid:cf568e6b-d48c-4e33-b023-88f8dba92c93>', '<urn:uuid:188d74ae-4074-4ce7-a288-b5390a58f788>']	open-ended	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-12T18:38:36.516973	7	95	2814
42	what signal indicates conductor ready start performance opera house	The conductor raising the baton marks the start of the performance.	"['While many people enjoy dressing up for an opera, anything goes, from jeans to your favorite gown or tuxedo. Opening night and evening attire tends toward the formal, whereas matinée attire is a bit more relaxed. The best rule of thumb is to dress how YOU feel most comfortable.\nArrive Ahead of Time\nPerformances start promptly at the designated curtain time. Arrival 30 minutes ahead of curtain time allows you to obtain your ticket, if necessary, and locate your seat so that you are not rushed. Extra time will also allow you to people-watch in the lobby and peruse your playbill to familiarize yourself with the production.\nPlease allow for traffic, weather, and other factors in planning your transportation to the opera.\nLatecomers will be seated by an usher at an appropriate interval, such as between scenes or at the end of an act. It is important to arrive on time so that you may enjoy the opera in a relaxed manner.\nRemain Quiet During the Performance\nBecause of the acoustics in an opera theater, extraneous noise is very audible and distracting to performers and musicians, and to the other audience members. Please do not whisper or talk during the performance - save your comments for intermission. Please silence cell phones, pagers, and watches, and refrain from eating, or unwrapping candy or lozenges.\nCameras and recording equipment\nThese are expressly prohibited inside the performance venue.\nApplause - Let the performers know you are enjoying yourself!\nYou are attending a live performance, and the performers respond to your enthusiasm! Applause is customary when the conductor takes the podium, after the overture, after a big aria, at the end of each act, and when the singers come out to take a bow. You may applaud while sitting, or join other audience members in a standing ovation for an exceptional performance. If you are not sure about when to applaud, follow the lead of the rest of the audience.\nYou may show appreciation for a job well-done by yelling ""Bravo!"" for a male performer, ""Brava!"" for a female performer, or ""Bravi!"" for an ensemble.\nBringing Children to the Opera\nSome operas are appropriate for children, depending on their age, and some operas deal with more mature themes. Very small children often do not have the patience to sit quietly through a lengthy performance, and an adult-themed opera may be inappropriate or confusing. Some of our productions are family-friendly; some operas are more suitable for different ages of children than others. It is best to familiarize yourself with the opera beforehand and make a judgment as to whether it is suitable for your child. If you have a question about the suitability of one of our performances for your child, please explore our website, or contact us at (336) 273-9472.\nThe orchestra members enter the orchestra pit and tune their instruments a few minutes before the performance begins. They will become quiet when they are finished, and then the conductor enters the pit and takes his place on the podium. Applause from the audience is appropriate at this point. S/he will bow to the audience, and then turn to face the orchestra, which signifies that the performance is about to begin, and that the audience must now be quiet. S/he will raise the baton, which marks the start of the performance.\nIf You Must be Excused During the Performance\nPlease use the time before the opera and during intermission to take care of your personal needs. If you must leave the auditorium during the performance, please do so quietly and courteously. You may not be able to reenter the auditorium until the end of the act, however.']"	['<urn:uuid:57768588-b4c1-4dee-831a-a680441c3aae>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	9	11	613
43	What's the difference between how nitrogen works in regular soybeans compared to how it works in regular grass lawns?	In regular soybeans, nitrogen is obtained through root nodules that interact with soil bacteria (Bradyrhyizobium japonicum) to fix nitrogen from the air, eliminating the need for nitrogen fertilizer. In contrast, lawn grasses require nitrogen supplementation through fertilizers, as they cannot fix their own nitrogen. Nitrogen in grass fertilizers helps the grass grow fast, turn green, and better tolerate environmental stresses like heat, cold, and drought.	['FAYETTEVILLE, Ark. — Breeding a modern soybean that does not “fix” its nitrogen may initially seem counterintuitive. However, it is part of the grand plan to assess nitrogen fixation in new high-yielding varieties and potentially reverse the decline in grain protein content seen over the past 30 years.\nAlthough U.S. soybean yield increased by about 14 bushels per acre between 1986 and 2019, average grain protein concentration decreased by about 2 percent. In typical soybean plants, root nodules interact with microbes in the soil to fix nitrogen from the air, eliminating the need to apply nitrogen fertilizer.\nWith support from the United Soybean Board, researchers with the Arkansas Agricultural Experiment Station in collaboration with scientists at Kansas State and the University of Georgia have bred high-yielding soybeans in Maturity Groups 4 and 5 that do not create root nodules interacting with microbes and source nitrogen from the atmosphere.\nThe experiment station is the research arm of the University of Arkansas System Division of Agriculture.\nThe new “non-nod” soybeans will be released to the USDA Germplasm collection this winter and will be available to researchers soon after. The advance in agricultural science is that they are genetically similar to modern-day, high-yielding soybeans with nodules that do fix nitrogen.\n“Having non-nodulating soybean varieties with similar yield potential when fertilized with nitrogen as nodulating cultivars is key to understanding potential crop nitrogen limitations, developing management strategies to improve grain protein concentration, and identifying soybean varieties with high rates of nitrogen fixation,” said Larry Purcell, Distinguished Professor of crop physiology and Altheimer Chair for Soybean Research at the University of Arkansas.\nPurcell said he has worked with nitrogen fixation for most of his career but began research on breeding a modern “non-nod” soybean about five years ago with Kansas State University professor and soybean breeder William Schapaugh Jr. The United Soybean Board-supported program to breed the modern, high-yielding “non-nod” soybean in all maturity groups is in its third year.\n“From strictly a biological perspective, it’s a fascinating symbiosis,” Purcell said. “You have the soybean that needs the nitrogen, the Bradyrhyizobium japonicum that needs the sugars and everything else the plant provides, in addition to an enclosed environment in that nodule where the magic takes place. The close coordination is a beautiful symbiosis that really caught my attention.”\nBradyrhyizobium japonicum is a rhizobium, a soil bacterium that converts atmospheric nitrogen into organic forms the plant can use.\nSoybeans are a $1.5 billion crop in Arkansas, valued for their high oil and protein concentrations, according to the 2022 Arkansas Agriculture Profile. Purcell noted that about 35 to 40 percent of the bean is protein and about 20 percent is oil. Soybean plants take up mineral nitrogen from the soil when available to build high protein levels, but they also form nodules on roots in symbiosis with rhizobium bacteria to obtain atmospheric nitrogen, especially when soil nitrogen is inadequate.\nMost of the soybeans grown in Arkansas are in Maturity Group 4. Agricultural research stations in Georgia, Kansas, Minnesota, and Nebraska have worked to breed “non-nodulating” soybeans and their nodulating pairs in Maturity Groups 0, 1, 2, 3, 6, 7, 8 and 9.\nNon-nod varieties have been around since the 1950s in backgrounds of varieties including Chippewa (MG 0), Williams82 (MG 3), Lee (MG 6), and Hardee (MG 8), but prior to this research there were no non-nods in modern, high-yielding varieties. For example, non-nodulating Lee, when fertilized heavily with nitrogen has a yield potential of about 50 bushels per acre, Purcell noted. A modern, high-yielding genotype under optimum growing conditions has a yield potential of 75 to 80 bushels per acre.\n“The United Soybean Board saw that the lack of high-yielding non-nods was a real limitation for advancing genetic research that looks to improve the protein concentration in grain,” Purcell said. “They have generously funded this national program to develop ‘non-nod’ lines along with their nodulating isolines.”\nPurcell said that the isolines, or nearly identical twin lines, differ primarily by just one gene that is essential for soybean to fix nitrogen. The isoline allows researchers to determine the fraction of atmospheric nitrogen that comes from the interaction with rhizobium compared to the nitrogen from the soil.\n“We knew the potential was not there genetically for these old non-nods to be able to adequately represent high-yielding genotypes that we grow today,” Purcell said.\nNitrogen fixation potential differs among soybean varieties, and nitrogen is critical to both yield and protein concentration, Purcell said. He added that the key to maximum nitrogen uptake is having the “proper genetics and proper environment” to take advantage of the symbiosis between the plant and rhizobium.\nField trials on the “non-nod” soybeans and their high-yielding isolines were held at the Division of Agriculture’s Pine Tree Research Station in St. Francis County. To see if the “non-nod” varieties could produce yields as good as their nodulating twin if fertilized, researchers applied the equivalent of 300 pounds of nitrogen fertilizer to “non-nod” plots and no nitrogen on other “non-nod” plots of the same variety to serve as a control.\n“If it only differs for that nodulating trait, we would expect the yields to be similar when plenty of nitrogen was provided. And that’s been pretty much what we have seen,” Purcell said.\nLast year at Pine Tree, the modern “non-nod” plants with nitrogen fertilizer added produced about 65 bushels per acre, the same as their nodulating twins. Experiments at Kansas and Georgia had similar results.\nOf course, a farmer does not want to use a “non-nod” line because of the added expense of nitrogen fertilizer.\n“This is strictly a tool for being able to do large-scale field research to evaluate the contribution of nitrogen fixation to soybean and indirectly look to determine the importance of nitrogen fixation is to grain protein concentration,” Purcell said.\nPurcell’s research colleagues on the non-nodulating soybean project include Aaron Lorenz at the University of Minnesota, George Graef at the University of Nebraska, William Schapaugh Jr. at Kansas State University, Zenglu Li at the University of Georgia and Andrea Acuna-Galindo with the Arkansas Agricultural Experiment Station.', 'All plants, including grasses, need a steady supply of nutrients to grow properly. Some nutrients come directly from air and water, but we generally supplement some or all of the chemical elements that are essential to healthy grass plants, just as we often supplement minerals and vitamins in our own diets.\nGenerally a lawn that is not fertilized regularly will look pale and growth may be sparse. If the grass becomes dormant or it is damaged in any way it will be more susceptible to disease or weed attack, and it will take longer to recover.\nWhat you need to know about fertilizing lawns\nThere are 16 chemical elements that grasses need and these may be divided into four groups. These are:\n- The natural elements found in the air or in water: carbon, hydrogen and oxygen. We, of course inhale oxygen and exhale carbon dioxide which plants then take in.\n- Vital macronutrients, specifically nitrogen, phosphorus and potassium that make up between 6 and 11% of the dry weight of lawn grasses.\n- Secondary elements which are also essential to the life of grass, namely calcium, magnesium and sulfur.\n- Micronutrients or trace elements that are present in minute amounts, but are also very important. These include iron, manganese, zinc, copper, molybdenum, boron and chloride.\nThe various types of fertilizer contain certain quantities of these nutrients and elements in various proportions, depending on the formulation and what it is intended to achieve. Always look for an indication of the ratio in which nitrogen, phosphorus and potassium occur. For example if you see 25-10-30 you know that there is a quarter pound of nitrogen for each pound of fertilizer, but only 10 lb. of phosphorous and 30 lb. of potassium included in the fertilizer mix.\nNitrogen makes grass grow fast and helps it turn a lush, green color. It also helps the grass tolerate heat, cold, drought and general wear and tear by keeping it healthy.\nPhosphorus helps to establish a strong, deep root system that will help make the grass resistant to general stress, including spells of dry weather. It also aids seed germination.\nPotassium contributes to photosynthesis which is the process plants use to turn sunlight into energy. It also toughens grass and, like nitrogen, helps it tolerate heat, cold, drought and general wear and tear.\nDifferent kinds of fertilizer for lawn\nThere are so many different types of fertilizer it can be mind boggling, especially if you don’t know exactly what it is your lawn needs. In general terms we can categorize fertilizers as either being liquid or granular, but there are also quick-release fertilizers, slow-release fertilizers and controlled release fertilizers. Then there are also fertilizers that contain herbicide as well as both organic and natural fertilizers.\nLiquid fertilizers take more time to apply than other types, and they are generally more difficult to spread.\nGranular fertilizers, which are dry, are popular because they can be measured out easily, quickly and accurately and they are easy to spread effectively using a broadcast spreader. These spreaders come in various sizes and different models, including those that you can attach to a rider mower if you have one.\nWhen a granular fertilizer states that it is ‘homogeneous’ this means that each fertilizer pellet or grain contains all the nutrients that are specified on the label. This also means that the proportions of nitrogen, phosphorus and potassium are exactly the same in every single pellet. Cheaper fertilizers that are not homogeneous are not accurately manufactured and so you won’t be sure that the different ingredients are being evenly distributed over your lawn.\nFertilizers with herbicide are designed to kill off certain kinds of broadleaf weeds. There are also fertilizers with that contain pesticides. Read the label information carefully and make sure you follow the guidelines. For example you need to be aware of how many dry days you need for some products to be effective. Also make sure you are using the right herbicide for the weeds in your grass. Most products are quite specific in this way.\nYou should also check your local regulations because in some areas only licensed applicators are allowed to spread fertilizers that contain herbicides, while others require you to get a special permit.\nIf you go here you will find out all about preventing pollution problems that can occur if you use lawn and garden fertilizers.\nOrganic fertilizers (or more correctly synthetic organic fertilizers) are made up of a combination of organic compounds, carbon for example. Truly organic fertilizers are natural fertilizers including alfalfa meal (3-3-2), bat guano (10-4-1), blood meal (10-1-0), bone meal (0-4-1), castor oil meal (5-1-1), composted cow manure (1-1-1), cottonseed meal (7-2-1), feather meal (11-0-0), fish meal (10-4-4), hoof and horn meal (12-2-0), poultry manure (4-4-1), rock phosphate (0-3-0), sea weed (1-0-4 plus more than 50 important elements), and wood ash that is high in potassium and various micronutrients.\nCommercial mushroom compost as well as homemade compost can also be used to fertilize lawns, as can horse manure that has been left to stand and rot naturally for several months. All types make an excellent organic fertilizer.\nTiming is not as important with natural fertilizers as it is when you use chemical fertilizers and they generally don’t ‘burn’ the grass. They are also completely environmentally friendly.\nFertilizing new lawns\nBefore you plant a new lawn you should always ensure that the soil is the best quality possible and suitable for the particular grass you are going to plant. This involves testing the pH of the soil to make sure it is neither too acid nor too alkaline. Most plants prefer slightly acid soil – most grasses like it to be either neutral (7.0 on the pH scale) or slightly acid (6.0).\nOrganic matter (compost for example) will improve any type of soil, and it is best to add this before you seed or plant lawns. Also add a fertilizer recommended for the grass you are going to plant. This should be done prior to seeding or planting.\nAllow seed to germinate and the plants to establish themselves before you even consider fertilizing a new lawn. If you have planted sods, don’t fertilize for at least the first six weeks.\nFertilizing established lawns\nWhile grass type as well as region and the maintenance level required will all help determine the schedule and frequency required for fertilizing, it is usually a good idea to fertilize once or twice in the fall. Then fertilize cool-season grasses again in early spring and warm-season grasses again mid-spring.\nThere are various different options, but you could use a high-nitrogen lawn fertilizer in which at least half of the nitrogen is slow release.\nLawn fertilizing tips\n- Most grasses benefit from being fertilized three or four times a year.\n- Fertilizing in fall helps to prepare lawn for the winter.\n- All grasses need a certain amount of nitrogen every year. Use the requirements shown in the table below to work out your fertilizer program.\n|Type of grass||Variety of grass||Lbs nitrogen required *|\n*The number of pounds of actual nitrogen that should be applied to 1000 square feet over one entire year.']	['<urn:uuid:c1a799c4-7b96-419c-82a0-7aa3a8626f27>', '<urn:uuid:6627af77-d04a-4a91-8062-5414d1c1bd06>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T18:38:36.516973	19	65	2209
44	How do modern DIY cloud makers compare to Victorian weather studies?	Victorian weather studies involved complex institutional efforts, with observatories collecting meteorological data and an organized network of amateur observers. In contrast, modern DIY cloud creation experiments using a glass container, warm water, and a cold metal dish can demonstrate the same basic principle that Victorian scientists studied - that clouds form when warm, moist air is cooled and condenses into water droplets. While the Victorian approach was more comprehensive and scientific, the simple DIY method effectively illustrates the fundamental process they were studying.	"['In 1873, a speaker at the Royal Dublin Society declared that ‘many persons are inclined to deny altogether the existence of a weather-science; the triteness of the subject, viewed as a break-ice topic of every-day conversation, having … tended to conceal from them the scientific aspect of the study of weather and climate’. Here, the speaker alluded to the weather as an ever-present but under-investigated topic. Interest in the weather had a long history – the keeping of weather journals was a popular pursuit in the eighteenth century, and from 1751 the Gentleman’s Magazine published a monthly weather report within its pages. The nineteenth century, though, saw more concerted efforts to chart the weather. As travel increased, the impact of the weather on both leisure and trade became an important factor, and monitoring conditions across the globe became easier with technologies such as telegraphy. Observatories collecting meteorological data including rainfall, wind direction, and temperature were set up at Greenwich and Kew in the 1840s, and the precursor of the Met Office established in the 1850s. Alongside these government-level endeavours was an army of amateur observers, organised and advised by the Meteorological Society.\nAlongside intricate measurements using a variety of instruments, there was also some appeal to ‘natural’ weather indicators, particularly in almanacs and gardener’s diaries. An unusual suggestion for a ‘natural barometer’ was offered in 1897’s The History of the Weather, which claimed that blackcurrant lozenges imbibed moisture and could ‘predict’ rain. There were also said to be ‘human barometers’. Oft recounted was the case of Captain Catlin, who was apparently able to foretell rain by the pain experienced at the site of his leg amputation. The effect of weather on the body had long been recognised. The impulse to quantification exhibited by nineteenth-century observers, though, opened up the possibility of charting such effects more carefully. From 1838, the General Register Office collected weather information alongside mortality statistics, and renewed impetus was given to such studies following mid-century cholera epidemics. Some doctors began to argue for the necessity of a national network of medical observers of the weather. In this way, harmful atmospheric influences might be recognised and eradicated, or artificial environments constructed for ailing patients.\nSeveral doctors saw the value in such an enterprise, and some were already volunteering observations to the Meteorological Society, particularly if they had a large site available to set up meteorological equipment. Hospitals or asylums often possessed open grounds where phenomena such as wind speed could be measured. A Meteorological Society inspector, checking the station set up on the Superintendent’s lawn at Caterham Asylum, was not impressed with what he saw in the 1870s, though: ‘I inspected Dr Adam’s instruments … [and] they were almost all bad’. Dr Adam was given a ticking off and a set of new, Society-approved, instruments for his observations.\nIn 1867, the management of Sussex Asylum decided that meteorological observations should be a regular exercise. The task fell to asylum chaplain Thomas Crallan, whose yearly meteorological reports were included in the asylum’s annual reports. Admissions and deaths were charted alongside fits and episodes of mania or melancholia. Crallan found high rates of admission in winter and summer that he attributed to the difficult circumstances that a cold winter caused for poor families, and increased sun exposure amongst farm labourers respectively. Though fits peaked in the winter and summer, Crallan noted that they occurred throughout the year. Therefore, he reasoned that the atmospheric force affecting them must be a less perceptible one than simple temperature. He turned to the lunar cycle. John Haslam had tried to chart the effect of lunar cycles on the excitement of Bethlem patients, but dismissed the idea as mere folklore. Crallan persevered in his study, however. He discovered that of 212 fits, all but five were ‘preceded or accompanied by considerable alteration in atmospheric pressure or solar radiation, or both’. Thus, he said, it was not the change of moon that was responsible for the fits, but a change in other weather conditions. It was the frequent coincidence of a changed moon and weather conditions, he said, that had led to the popular linkage of the moon and madness. He concluded: ‘so far as my own observations go, any marked change of atmospheric pressure, solar radiation, or both … is almost certain to be followed by [an] increased number of fits among the epileptics, or by a development of mania or melancholia.’\nCrallan’s concern for determining a ‘cycle’ of mental distress fitted – as Niall McCrae observes in The Moon and Madness – with the late nineteenth-century reconfiguration of mania and melancholia as one cyclical illness, rather than two separate ones. Though he was undertaking his investigations in a self-consciously ‘scientific’ way, Crallan also appeared to be imagining the asylum patient as a kind of ‘natural barometer’. In the eighteenth century, an awareness of approaching weather tended to be seen as a marker of sensibility, with one’s affinity with the environment a sign of sensitivity. During the nineteenth century, a good deal of attention was paid to the ‘natural barometers’ of the animal kingdom, which tended to be the lower animals – frogs and insects. This is rather wonderfully illustrated by the ‘Tempest Prognosticator’ of George Merryweather, which predicted storms based on the movement of leeches. In imagining the asylum patient as a kind of human barometer, keen to atmospheric changes, doctors were (consciously or unconsciously) aligning them with the animals of weather folklore – a link that fitted into wider discourses about insanity, degeneration and ‘de-evolution’.\nWhat became of medical meteorology, then? The attempts of bodies such as the Association Medical Journal to institute a nationwide network of medical observers proved difficult, as doctors disagreed over the specifics of how to record illness in tabular form. Nevertheless, attempts to correlate weather and mental distress continue to appear sporadically – several researchers have linked suicide and the advent of spring, for example, while moon lore continues to circulate amongst hospital emergency departments, with trepidation before a full moon and an anticipated influx of patients. Our Royal Dublin Society speaker would no doubt be pleased by this continued research into the weather’s effects, even if it does also remain the stereotypical ‘break-ice topic of every-day conversation’.\nKatharine Anderson, Predicting the Weather:Victorians and the Science of Meteorology (2005).\nJames Rodger Fleming, Meteorology in America, 1800-1870 (1999).\nNiall McCrae, The Moon and Madness (2011).\nJ.W. Moore, Royal Dublin Society. Afternoon Scientific Lectures on Public Health, 1873. Lecture III. Meteorology in its bearing on health and disease (1873).\nR.E. Scoresby-Jackson, On the Influence of Weather upon Disease and Mortality (1863).', ""A selection of ideas for creating your own weather instruments in the classroom or at home\nClouds in the real world form in warm, moist air. You can make something similar happen using things you can find around your home.\nWhat you will need:\nA clear glass or similar see-through container\nMetal or foil dish\nWhat to do:\n1. Place the ice into the metal dish.\n2. Pour a small amount of warm water into the bottom of the glass.\n3. Wait until the dish is really cold. Then place it on top of the glass.\n4. Watch the inside carefully. You should see a 'cloud' form near the top of the glass.\nThis is the way clouds form in the real world. Warm, moist air like that in your glass, is cooled (your ice). When it is cooled it condenses into tiny water droplets, which appear as clouds.\nWe get different sorts of weather because of the way warm air and cold air move around us in the atmosphere.\nA large, clear tank (an empty fish tank would be ideal)\nWarm and cold water\nTwo plastic cups\nSmall stones (pebbles)\nWhat to do:\n1. Fill the tank with normal water and leave this for a few hours to come to room temperature.\n2. Place a few small pebbles in each of the two cups to act as weights.\n3. Pour some hot water and a few drops of red food colouring in one of the cups.\n4. Fill the other cup with very cold water and add some blue food colouring.\n5. Place the two cups into the water at the same time, one at either end of the tank - the pebbles should hold the cups at the bottom.\n6. Carefully watch how the two different coloured waters move. You should see that the warmer red water should rise to the top, and the cooler blue water should sink to the bottom.\nWarm air (shown by your warm red water) is less dense than cold air (cold blue water), so warm air rises and pushes down with less pressure than cold air. As air cools, it becomes denser, so it sinks and also pushes down with greater pressure.\nWhen rain gathers on the ground it sometimes forms a puddle. This can change size during the day, which you can measure.\nA piece of chalk\nWhat to do:\n1. Choose a day when there has been rain overnight and the rest of the day is expected to be dry, warm and sunny.\n2. Choose a puddle and first thing in the morning use a piece of chalk to draw around the puddle.\n3. A few hours later go back and draw around your puddle again. Repeat this several times during the day.\nYou should find that the puddle slowly shrinks during the day as the water evaporates.\nA sundial used to be used to tell the time. The sun makes a shadow on the dial. As the sun moves through the day so does the shadow.\nA direction compass (to find north and south)\nA drawing compass (to draw a circle with)\nWhat to do:\n1. Print out the Sundial template and cut out the shapes.\n2. Draw round the shapes on card and cut them out.\n3. Fold out the two long straight flaps so that they are in opposite directions.\n4. Glue together the two triangles.\n5. Draw a semicircle on the cardboard base.\n6. In the centre of the semicircle, with the thin point of the triangle at the edge of the card, glue the sundial to the base.\nHow to use your sundial:\n1. Place your new sundial in the sunshine, making sure it is on something level.\n2. Turn the sundial until it is facing south, use your compass to do this part.\n3. Now you can use the sun to tell what the time is. Every hour mark the time and the position of the shadowing on the card. Do not worry if you miss an hour, you can estimate where that hour is by seeing how far there is between the shadows of the other hours.\n4. Once you have marked down where all the daytime hours are, you will have a fully working sundial.\nA tornado is air which is spinning very fast. It is made by a special type of thunderstorm called a supercell.\nA clear jam jar or similar see-through container with a screw-on lid\nWashing-up liquid or liquid soap\nWhat to do:\n1. Fill up the container with water then add a few drops of the washing-up liquid and a few drops of the food colouring. Tightly screw on the lid.\n2. Swirl the container around in a circle lots of times and then stop. Inside you should see what looks like a tornado. It will slowly disappear as it reaches the top of the container.\nTornadoes in the real world are made in the same way.\nOur rain originally came from oceans, seas and lakes. It became clouds and eventually fell on us as rain.\nA large bowl\nA small yogurt pot or plastic cup\nSmall weight or a few coins\nSunny window sill\nWhat to do:\n1. Take the large bowl and fill it with several centimetres of water.\n2. Place your small pot in the centre of the bowl of water, making sure not to get any water inside it.\n3. Cover the large bowl with clingfilm and fasten this down securely to the side of the bowl.\n4. Put a weight on top of the clingfilm, over the centre of the small pot to push the clingfilm down into it slightly.\n5. Place your experiment on a warm sunny window sill and leave for a few days.\nYou should find that the heat of the sun evaporates the water, which rises, condenses on the cool plastic, and falls into the small container. This is a small version of what happens in the real water cycle.\nWind is used to power things, you might see large windmills in the countryside.\nPencil or knitting needle\nBlu-Tack (or similar)\nA paper fastener\nA small bead that can go through the paper fastener\nA cardboard tube (from kitchen towels would be ideal)\nWhat to do: (you might need some adult help)\n1. Print the windmill template and cut it out.\n2. Decorate with colourful patterns.\n3. Put the lump of Blu-Tack under a little circle where you need to make a hole.\n4. Use a pencil or knitting needle to make a hole. You need to make five holes.\n5. Fold towards the centre and push a paper fastener through all five holes.\n6. Thread a small bead on the back of the paper fastener.\n7. Make a hole near the top of your cardboard tube.\n8. Push the paper fastener in the hole and fold the ends back to stop it from coming out again.\nYou can decorate the cardboard tube to make it more colourful.\nYou can find out how much rain falls where you live by making your own rain measure. This involves cutting, so you will need an adult to help.\nAn empty plastic bottle (2 litre fizzy drink bottle would be ideal)\nWhat to do:\n1. Cut around the plastic bottle about two thirds of the way up.\n2. Turn the top part of the bottle upside down and place it inside the bottom part - fix it in place using the tape.\n3. Make a scale in centimetres on a piece of tape, using a ruler, and fix it to the side of your bottle.\n4. Find a place outside to put your rain gauge. It must be open and away from trees.\n5. Dig a hole and bury your rain gauge so that the top is sticking out about 5cm out of the ground. This will stop the wind gauge from blowing down on windy days.\n6. Check the rain gauge every day at the same time, measure the amount of rain collected, and empty the bottle.\nDo not forget to write down the amount of rain collected in your weather diary (see below).\nThe instrument used for measuring wind direction is called a wind vane.\nA pen top\nA plastic fizzy drink bottle\nA knitting needle\nWhat to do:\n1. Draw an arrow 25 cm long on the card and cut it out.\n2. Make another arrow by drawing around the first arrow and cutting it out.\n3. Place the pen top between the arrows, in the centre, and glue together.\n4. Push four matchsticks into the long edge of the cork at right angles to each other.\n5. Cut out four small squares of card and label with the four main points of the compass; N, E, S, W. Attach these to the end of each matchstick with Blu-tack.\n6. Fill the bottle with sand.\n7. Push the knitting needle into the cork and push the cork in the top of the bottle. Now balance the wind vane on top of the needle.\n8. Choose an open area, perhaps near your rain gauge, to place your wind vane. Ask an adult or use a compass to point the N label on the bottle towards North.\nThe arrow always shows the direction the wind is blowing from.\nThermometers are kept in special boxes (Stevenson screen) to protect them from sunshine, wind and the surroundings. You can make your own special box.\nA sturdy plastic or wooden box that can stand on its side\nWhat to do:\n1. Paint the outside of your box white and wait until it's dry.\n2. Stick the thermometer inside the box using Blu-Tack.\n3. Take the box outside and find a safe, shady place to keep it.\n4. Stand the box on its side so that the thermometer is at the back of the box and protected from direct weather conditions.\n5. Now you can take temperature readings at the same time every day.\nYou can make your own simple barometer to show changes in air pressure.\nA small plastic drink bottle\nA piece of wood about 3cm wide\nSmall marker pen\nWhat to do:\n1. Attach the piece of wood to the dish using sticky tape.\n2. Fill the bottle about three quarters full of water, and also half fill the dish. Put your fingers over the end and turn the bottle upside down into the dish of water. Do not remove your fingers until the neck of the bottle is under the water.\n3. Fix the bottle to the piece of wood so that it is straight.\n4. Mark the level of water on the side of the bottle.\n5. Keep your barometer inside. Note in your weather diary whether the level of the water has risen, fallen or not changed.\nWhen air pressure rises, the water level in the bottle rises too. If the air pressure is high, the weather is usually fine.\nWhen air pressure falls, the water level in the bottle falls too. If the air pressure is low, then the weather is usually cloudy and wet.\nYou need a weather diary to record the measurements from your weather station at least once a day. The measurements must be done at the same times each day.\nBy placing a booking, you are permitting us to store and use your (and any other attendees) details in order to fulfil the booking.\nWe will not use your details for marketing purposes without your explicit consent.\nYou must be a member holding a valid Society membership to view the content you are trying to access. Please login to continue.\nJoin us today, Society membership is open to anyone with a passion for geography\nCookies on the RGS website""]"	['<urn:uuid:0dc86018-57fb-45d5-940a-b877166404b8>', '<urn:uuid:856cdac6-2d30-415e-a07c-cfe9139564cb>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T18:38:36.516973	11	83	3066
45	corn starter fertilizer benefits risks water quality impact farming	Starter fertilizer in corn provides better early-season growth, especially when planted in cool soil temperatures that may reduce phosphate uptake. While treatments like 10-30-40 in the furrow are safe for corn due to its hardy seeds, these fertilizers contain nitrogen and phosphorus which can contribute to water pollution if they enter water bodies. When these nutrients reach water systems, they can cause rapid eutrophication, leading to excessive algae growth and eventual collapse of aquatic ecosystems.	['What is in this article?:\n- Fertilizer recommendations proving valid even with increasing corn yields\n- Starter fertilizer\n• The latest buzz for those who work with fertilizer is not only rate but also source, timing and placement, which all come into play.\n• You can’t think that if you put out 360 pounds of nitrogen, you’ll automatically make 300 bushels of corn.\nUNIVERSITY OF GEORGIA soil scientist Glen Harris, right, discussing corn fertilization with a producer at the recent Georgia Corn Short Course held in Tifton.\nTurning to starter fertilizer in corn, Harris says he’s looking in his research at potassium in the furrow. Growers traditionally use small amounts of nitrogen and phosphorus as starter fertilizer.\n“Our data showed that we actually did get more potassium into the plant when we put it in the starter, but it didn’t translate into yield,” he says.\n“We’re pretty comfortable now with 10 gallons of 10-30-40 in the furrow on corn, after two years of looking at it, although it might not always translate into yield. Even with no nitrogen in the furrow and potassium alone, we still did okay.\n“As long as you account for other nutrients in your program, things like 10-30-40 and some nitrogen solution is not a bad way to go.\n“It didn’t hurt the stand in corn. It’s one of the concerns we have with putting stuff in the furrow. I’m still not comfortable with putting anything in the furrow with cotton. Corn is a hardier seed, and the in-furrow treatment did not hurt stand.”\nThe main advantage of starter fertilizer is better early-season growth, according to University of Georgia Extension recommendations.\nCorn planted in February, March or early April is exposed to cool soil temperatures, which may reduce phosphate uptake.\nBanding a starter fertilizer 2 inches to the side and 2 inches below the seed increases the chances of roots penetrating the fertilizer band and taking up needed nitrogen and phosphorus.\nHarris says he also conducted a simple phosphorus rate study this past year.\n“I found a really low soil-testing phosphorus patch on the experiment station that had a phosphorus level comparable to what you’d normally see when you’re going into new ground.\n“The recommendation for 200-bushel corn is actually 140. We say that if you have medium-testing phosphorus soils, you only have a 50-percent chance of seeing yield response from adding it. But you don’t need to take that chance — go ahead and put it out there.”\nOf the 16 essential plant nutrients, there’s a lot of focus on N, P and K, but micronutrients can limit yield as well, says Harris. “They’re needed in very small amounts by the plant, and they also have a very narrow window of sufficiency range in the plant. In corn, we concentrate on boron, manganese and zinc.”\nZinc deficiency can be prevented by using thee pounds per acre of actual zinc. Do not use zinc unless soil test levels are low. If needed, apply preplant or at planting.\nBoron deficiencies can occur on sandy soil low in organic matter. Generally, use one to two pounds per acre of boron applied in split applications. It is best to apply boron with the nitrogen applications.\nThe application of other essential nutrients should be based on plant analysis results.', 'What Is Nutrient Pollution of Water?\nA common way that humans pollute water is through the addition of nutrients (fertilizers and sewage) to water as nonpoint source pollution. These added materials are full of nitrogen and phosphorus, two nutrients that encourage the growth of aquatic producers, such as algae. The excessive growth of algae dramatically disrupts the entire ecosystem. Because this type of pollution results from excess nutrients, it’s called nutrient pollution.\nBodies of water naturally exist with different nutrient levels:\nWaters that have low levels of nutrients and, therefore, low numbers of producers and low biological productivity are called oligotrophic. Oligotrophic waters are clear without many microscopic algae plants.\nWaters that have high levels of nutrients and, therefore, high biological productivity with large algae populations are called eutrophic.\nThe transition from oligotrophic to eutrophic — called eutrophication —occurs naturally over time as a type of community succession in lakes and ponds. Additional sediment and nutrients enter the ecosystem via streams, rainwater runoff, and other sources. Eventually, the biological activity and plant growth in the lake or pond increase to a degree that it may no longer be a lake or pond, becoming instead a wetland.\nWhen humans pollute water with nutrients, this normally long, slow process of eutrophication occurs very rapidly and is called cultural eutrophication. The rate of cultural eutrophication is much faster than natural rates of eutrophication; as a result, the organisms in the ecosystem don’t have time to adapt to the changes and die. Here’s what the cultural eutrophication process looks like:\nPollution enters the water, adding sediment, nitrogen, and phosphorus (the nutrients).\nThe added nutrients fuel algae growth; algae begin to reproduce in huge numbers, creating what’s often called an algae bloom.\nThe algae population continues to grow until it runs out of either nutrients or space — covering the surface of the water completely.\nExtensive algae across the surface of the water blocks sunlight from reaching organisms below the surface of the water, such as aquatic plants.\nUnderwater plants begin to die from lack of sunlight, while the large algae population, having reached its limit, also begins to die, sinking into the water where bacteria decompose it.\nDecomposition of dead plants and algae by bacteria consumes the dissolved oxygen in the water, creating oxygen sag conditions.\nFish and other oxygen-breathing organisms suffocate from lack of oxygen, and the entire aquatic ecosystem collapses, creating one big dead zone.\nCultural eutrophication doesn’t occur only in lakes. Coastal waters also experience the effects of nutrient pollution and cultural eutrophication. As a result, highly polluted areas of water become dead zones that don’t have enough oxygen to support a healthy aquatic ecosystem.\nOne of the most studied dead zones from nutrient pollution occurs in the Gulf of Mexico. The Mississippi River flows into the gulf and brings with it nutrient runoff from agricultural fields in the Midwestern United States. For a long time, scientists didn’t understand what was causing huge numbers of fish to die off and whole regions of the Gulf of Mexico to display low levels of dissolved oxygen.\nAfter observing the ecosystem for a number of years, scientists realized that the worst dead zones occur after spring rains along the Mississippi River. These rains wash fertilizers and manure from agricultural fields into the river, which then carries these nutrients into the gulf. When these nutrients reach the gulf, the cycle of cultural eutrophication begins with an algae bloom and subsequent ecosystem disruption.\nThis pollution scenario isn’t unique to the Gulf of Mexico. It’s also common in other regions, such as the Mediterranean Sea and the Black Sea. In most cases, the dead zones are a recurring event; after the ecosystem recovers, a fresh flow of nutrients occurs and the cycle repeats itself.\nMany cities have begun to monitor and limit the amount of nitrogen and other nutrients that enter nearby aquatic ecosystems in an effort to control nutrient pollution, oxygen sag, and cultural eutrophication. Another way to solve this type of water pollution problem is by finding ways to clean sewage and wastewater before it enters and damages an ecosystem.']	['<urn:uuid:b330c322-9453-44f2-bcce-560487bd6d64>', '<urn:uuid:59cb45f9-b7bc-4997-9944-bc70e867aaf1>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	9	75	1234
46	explain how criminals can be identified using hair samples and what happens when skin touches contaminated materials	Criminals can be identified through forensic hair analysis by examining both the hair structure and DNA. For DNA testing, the hair must include the root and be plucked rather than cut. The hair samples are treated with specific chemicals and examined under a microscope. When it comes to skin contact with contaminated materials, the exposure process involves two steps: first, the initial contact between the contaminant and skin, and second, the absorption into the body. The amount of contamination absorbed depends on various factors including the concentration of the contaminant, duration of contact, skin surface area exposed, and how well the substance adheres to skin. The absorbed contaminants can then enter the bloodstream and interact with internal organs and tissues. This exposure can occur through contact with contaminated water, soil, dust, or other materials that have settled on various surfaces.	"[""Hair analysis can be used to check if people are blood relatives. Forensic hair analysis can be done to help identify a criminal by evaluating hair structure and DNA. Hair samples are tested with specific chemicals and looked at under a microscope. Hair analysis can also be used to check for poisoning caused by metals such as lead or mercury. But hair analysis alone usually is not used for this type of testing.\nHair is a protein that grows out of hair follicles in the skin. Normally, a hair grows in the hair follicle for many months, stops growing, and falls out. A new hair then grows in the follicle. It takes weeks for a hair sample to show changes in the body, because hair grows slowly. Hair samples do not show recent changes in the body, such as drug use within the past few days. But a hair analysis may show drug use or exposure to chemicals that occurred over the last few months.\nWhy It Is Done\nHair analysis is used to provide DNA evidence for criminal and paternity cases. For DNA testing, the root of one hair is needed to analyze DNA and to establish a person's genetic makeup.\nHair analysis is less commonly used to test for heavy metals in the body, such as lead, mercury, and arsenic.\nHow To Prepare\nIf you have a hair analysis done, the laboratory will give you specific instructions on how to prepare your hair. Hair preparation and the part of your body from which the hair is taken varies. In general, your hair should be washed and free of any hair care products.\nYou may be asked to sign a consent form that says you understand the risks of the test and agree to have it done.\nTalk to your doctor about any concerns you have regarding the need for the test, its risks, how it will be done, or what the results may mean. To help you understand the importance of this test, fill out the medical test information form(What is a PDF document?).\nHow It Is Done\nHair analysis is done by collecting a hair sample and sending it to a laboratory.\nHair samples are taken from a specific part of the body, such as from the back of the scalp by the neck or from the pubic area. Hair samples are generally collected from the section of the hair closest to the skin. Hair close to the skin or scalp includes the most recent growth, which provides the most accurate information about what has occurred recently in the body.\nHair samples are washed in special chemicals before testing.\nHair samples for DNA analysis will be gathered by laboratory personnel or law enforcement officials or a forensic examiner. For DNA testing, the hair must include the root. This requires that the hair be plucked, not cut.\nHeavy metal analysis\nIf you are collecting your own hair sample, follow the instructions given to you by the laboratory. In general, you will be asked to:\nHow It Feels\nThere is generally no pain or discomfort associated with this test. But if you have to pluck a hair for a DNA test, this may cause some minor discomfort.\nHair analysis itself has no risks or complications.\nHair analysis uses samples of hair for DNA analysis or to look for heavy metals, such as lead or arsenic.\nHeavy metal analysis\nThe results of hair analysis are usually complete within 3 weeks. You or your doctor will receive a report listing the levels of minerals and heavy metals in your hair. Several things need to be considered before testing for heavy metal exposure.\nThere is no standard procedure for cutting, washing, and analyzing hair. Different labs may report different results from the same hair sample. In fact, the same lab may report different results for separate hairs from a common sample. Standards for testing do not exist. Any hair analysis to detect the presence or absence of minerals, nutrients, or toxic metals in the body should be confirmed by testing blood and urine samples.\nWhat the hair sample contains is determined not only by nutrition and internal metabolism but also by external substances. Air pollution, mineral content of the water supply, exposure to industrial waste, shampoos, hair dyes, hair sprays, permanents, and bleaches may raise or lower the levels of certain minerals in the hair. Also, the use of medicines such as birth control pills can change the mineral concentration of hair.\nWhat Affects the Test\nThings that can interfere with your test and the accuracy of the results include:\nWhat To Think About\neMedicineHealth Medical Reference from Healthwise\nTo learn more visit Healthwise.org\n© 1995-2014 Healthwise, Incorporated. Healthwise, Healthwise for every health decision, and the Healthwise logo are trademarks of Healthwise, Incorporated."", ""Exposure Assessment Tools by Routes - Dermal\nAn exposure routeexposure routeThe way a chemical pollutant enters an organism after contact, e.g., by ingestion, inhalation, or dermal absorption. is the way that a contaminant enters an individual or population after contact (IPCS, 2004). Typically, exposure occurs by one of three exposure routes-inhalation, ingestion, or dermal.\nDermal exposure can result from skin contact with contaminated environmental media, including:\n- water (e.g., during bathing, washing, swimming);\n- sediment (e.g., while wading, fishing);\n- outdoor soil or dust (e.g., during recreational, gardening, or construction-related activities); and\n- indoor dust that has settled on carpets, floors, clothing, counter tops, or other surfaces.\nWater can become contaminated via direct discharges to surface water (i.e., point sources), indirect releases, and migration to groundwater or other water sources to which people may be exposed dermally.\nSoil, house dust, or other materials might become contaminated as a result of intentional application (e.g., pesticide use), deposition of particulate matter (e.g., from atmospheric pollutants), or transport from other media (e.g., water).\nHuman receptors could also come into contact with chemicals from consumer products directly (e.g. during use) or indirectly (e.g., chemicals that have settled on indoor or outdoor surfaces).\nEstimating exposure from dermal contact requires information on:\n- the concentration of the contaminant in the medium that is contacted;\n- timeframe of exposure (contact frequency and duration); and\n- other factors that affect dermal exposure—for example, skin surface area, dermal adherence of solids to skin, film thickness of liquids on skin, and/or residue transfer factors.\nCalculations to estimate dose from dermal contact are provided in this module as are various tools available for evaluating potential exposure concentrations. Exposure scenarios, exposure factors, and guidance for assessing exposure via dermal contact are also provided.\nDermal exposure assessment is a two-step process that considers the contact between contaminant and receptor as well as absorption of the contaminant into the body through the skin. The amount of contaminant absorbed represents what is available for interaction with target tissues or organs.\nThe magnitude of exposure is a function of media-specific contaminant concentration, timeframe of exposure (e.g., acute, chronic), and other factors that affect dermal exposure such as skin surface area.\nThere are a few different ways to characterize dermal exposure and dose (U.S. EPA, 1992b):\n- Potential dose is the amount of contaminant applied to skin, not all of which is actually absorbed.\n- Applied dose is the amount of contaminant at the absorption barrier (e.g., skin) that can be absorbed by the body.\n- Internal dose is the amount of contaminant absorbed and available for interaction with biological receptors (e.g., organs, tissues).\n- Biologically effective dose is the amount of contaminant that interacts with the internal target tissue or organ.\nAn illustration of dermal exposure and dose is provided below.\nThe potential dermal dose involves the contaminant and the matrix in which it is suspended (e.g., soil) that could come into contact with the skin. It is unlikely that all of the potential dose will come into contact with the skin. The amount of contaminant contained in a medium, such as soil, that actually comes into contact with the skin is the applied dose.\nThe internal dose is the amount of contaminant that gets past the exchange boundary (e.g., skin) and into the blood, or the amount of the contaminant that can interact with organs and tissues to cause biological effects (U.S. EPA, 1992b).\nThe internal (or absorbed) dermal dose is determined by the rate at which the contaminant is absorbed, and this will be a function of contaminant properties.\nAlgorithms for specific dermal pathways are provided in the Calculations tab of this module.\nFlux, or the amount of contaminant that penetrates per unit of time, has been mathematically described by Fick’s Law of Diffusion for membranes. It states that the flux of a contaminant across a homogeneous membrane is proportional to the concentration difference between the membrane outer surface and the membrane inner surface and inversely proportional to the thickness of the membrane. The key assumptions inherent in a Fick's Law based approach are:\n- The skin is a homogeneous membrane.\n- The flux of the contaminant across the skin is solely due to passive diffusion.\n- The permeability coefficient and the flux are specific to the vehicle contacting the skin and the animal species in which they are measured.\nAs shown in U.S. EPA (2004), internal (absorbed) dose for inorganics in water can be calculated as follows:\nDAevent = Kp x C x t\nDAevent = Absorbed dose (mg/cm2-event)\nKp = Permeability coefficient (cm/hr)\nC = Concentration of chemical in vehicle contacting skin (mg/cm3)\nt = Time of contact (hours/event)\nAll parameters must be expressed in consistent units; in some cases, unit conversion factors may be necessary. This absorbed dose is generally expressed as mass of contaminant per unit surface area of skin per event (e.g., mg/cm2 event).\nAccording to U.S. EPA (2004) the equation above may be used to estimate the absorbed dose per event (DAevent) when the inorganic contaminant contacting the skin is in the form of a liquid or aqueous mixture and the amount of contaminant on the surface is expressed as a concentration.\nU.S. EPA (2004) provides alternate forms of this equation for estimating dermal exposure to organics in water. Permeability coefficients have been measured for many contaminants and can also be estimated based on correlations that use molecular weight and octanol-water partition coefficient (Kow) (U.S. EPA, 2004, 1992a).\nFor dermal contact with solids (e.g., soil, sediment), the concentration of contaminant contacting the skin is a function of the concentration of contaminant in the solid material (e.g., soil) and the amount of solid that adheres to the skin per unit surface area. For surface residues, the amount of chemical contacting the skin is a function of the amount of chemical on the contaminated surface and the fraction of chemical transferred to the skin.\nFor example, the concentration of contaminant in soil (mg/g) may be multiplied by the adherence rate (Adh) of soil to skin (g/cm2-event) to estimate the concentration of contaminant on the skin (g/cm2-event). Likewise, the concentration of contaminant on a contaminated surface (mg/cm2) may be multiplied by the transfer efficiency fraction (TF) (%/100%-event) to estimate the amount of contaminant on the surface of the skin (g/cm2-event). Absorption factors are then applied to estimate the dermally-absorbed dose per event.\nOnce the absorbed dose per event has been estimated, the average daily dose (ADD) can be calculated. The ADD is calculated as follows and is generally expressed as mass of contaminant per unit body weight over time (e.g., mg/kg-day).\nADDabs = DAevent x SA x EF x ED / BW x AT\nADDabs = Average daily dose (mg/kg-day)\nDAevent = Absorbed dose (mg/cm2-event)\nSA = Skin surface area available for contact (cm2)\nEF = Exposure frequency (events/year)\nED = Exposure duration (years)\nBW = Body weight (kg)\nAT = Averaging time (days)\n- Surface area (SA) represents the surface of the skin that is exposed\n- Body weight (BW) of an individual, typically expressed in kilograms (kg), is included so that the dose is normalized to that value.\n- Temporal parameters in the dose equation include:\n- Exposure frequency (EF) refers to the frequency with which the exposure occurs and might be provided in events per year or events per day.\n- Exposure duration (ED) is the amount of time that an individual or population is exposed to the contaminant being evaluated and is typically provided in years.\n- Averaging time (AT) is the amount of time over which exposure is averaged and is equal to ED for assessing non-cancer risks. For chronic assessments (e.g., cancer), potential lifetime average daily dose (LADD) is calculated in which lifetime (LT, in days) is substituted for AT.\n- Exposure parameter inputs (e.g., dermal contact factors, body weights, and relevant patterns of behavior such as timeframe of exposure) can be obtained from the Handbook.\nAdditional information on exposure scenarios involving the dermal route can be found in the Indirect Estimation Module of the Approaches Tool Set.\nSources of contamination to soil, dust, water, and sediment are discussed in the Media Tool Set. Media-specific concentrations are needed for estimating exposure via the dermal route. Data and tools that can be used to estimate concentrations are also discussed in the Media Tool Set.\nInformation on sampling techniques and analytical methods is available to support the measurement of contaminants in environmental media to which people may be exposed via dermal contact.\nIn the absence of measurement data, a variety of models can be used to estimate contaminant concentrations in soil, dust, water, sediment, or other materials to which human receptors may be exposed via dermal contact. A number of resources are available to assist in these modeling efforts.\nThere are a number of information sources that provide monitoring data on contaminant concentrations in environmental media to which individuals might be exposed via dermal contact.\nDermal exposure can be estimated by first defining the exposure scenarioexposure scenarioA set of facts, assumptions, and inferences about how exposure takes place that aids the exposure assessor in evaluating or quantifying exposure. of interest. Exposure scenarios typically include information on the sources and pathways of exposure, contaminants of concern, and receptor populations. They might also describe a receptor population’s activities that may affect exposure and the timeframe over which exposure occurs.\nDermal exposure could occur from contact with contaminated environmental media-soil, dust, water, sediment-or contaminants from consumer products. Dermal contact with soil could occur during outdoor recreational, gardening, or construction-related activities.\nReceptors could come into contact with contaminants in soil and dust that have settled on carpets, floors, clothing, counter tops, or other surfaces. Children are particularly susceptible via this indirect pathway because certain activities (e.g., crawling or playing on the floor indoors) may increase their contact with contaminant-laden dust.\nDermal absorption of water contaminants could occur during activities such as hand washing, bathing, and swimming in a pool or surface water body. Receptors could contact contaminants in sediment while fishing, clamming, wading, or swimming in a lake, river, or other water body.\nExposure to contaminants in consumer products can occur from direct or indirect contact. For example, intentional direct contact could occur from applying cosmetics or other personal care products to the skin, or unintentional direct contact could occur from contact with laundry detergent or surface cleaner while in use. An example of indirect contact is contact with cleaning product or pesticide residues on surfaces).\nThe table below provides some examples of dermal exposure scenarios. The list of examples is not meant to be exhaustive. There are numerous other dermal exposure scenarios that may be constructed based on the specific needs of the assessment. There are also numerous variations of the examples provided in the table.\nAdditional information on exposure scenarios involving dermal exposure may be found in the Indirect Estimation Module of the Approaches Tool Set.\n|Medium||Receptor Population||Activity/Timeframe||Contact Rate||Exposure Period|\n|Tap water||General population; adults||Showering; lifetime||Total surface area\n|Surface water; localized||Children||Wading||Surface area of legs and feet\n|Swimming pool water||Recreating adults and children||Swimming over the short-term\n|Total skin surface area\n|Soil from contaminated site||Occupational; adults||Site-specific construction occupational tenure\n|Surface area of selected body parts\n|House dust||Young children||Contact during time spent indoors; less than lifetime\n|Surface area of selected body parts\n|Residues on indoor surfaces||Residential children||Contact during time spent indoors; less than lifetime\n|Surface area of selected body parts\n|a Information on dermal absorption may be found in U.S. EPA’s Risk assessment guidance for Superfund: Volume I: Human Health Evaluation Manual (Part E, supplemental guidance for dermal risk assessment) (U.S. EPA, 2004).|\nSeveral resources are available that illustrate dermal exposure scenarios.\nChapter 7 of EPA’s Exposure Factors Handbook: 2011 Edition (U.S. EPA, 2011) provides information on factors that affect dermal exposure, including skin surface area, dermal adherence of solids to the skin, film thickness of liquids on the skin, transfer of contaminant residues from surfaces to the skin, and other factors. Contaminant-specific factors related to dermal absorption and internal dose, however, are not provided in Chapter 7.\nSkin surface area (SA) is an estimate of the amount of skin (cm2 or m2) that can be exposed to contaminants. The more skin exposed the greater potential for dermal absorption. Depending on the scenario, total body surface area or surface areas of specific body parts may be appropriate.\nAnother dermal factor is the adherence factor (AF; mg/cm2), which represents the quantity of soil or solids that may adhere to the skin surface after contact. This varies with activity. Recommended values for SA and AF are provided in the Handbook.\nThe film thickness of liquids on skin (cm) is a measure of the amount of material that remains on the skin after contact with a liquid (e.g., consumer product such as cleaning solution or soap). Residue transfer coefficients (cm2/hour) of contaminants to human skin will vary based on exposure conditions such as activity, contact surface, and age. Recommended values for these dermal factors are not provided in the Handbook because data are limited; however, data from relevant studies are described.\nThe Exposure Factors Tab of the Indirect Estimation Module provides links to data on dermal contact rates.\nOther activity-specific factors that might be relevant for assessing exposures with contaminants via dermal contact are available in Chapter 16 of the Handbook. These might include time spent indoors, time spent outdoors, event frequency (showers per week, swimming events per month), or time spent doing the specific activities (e.g., playing on dirt or grass, gardening, participating in outdoor recreation).\n- Body weight (Chapter 8)\n- Consumer product use information (Chapter 17)\n- Life expectancy values, specifically when evaluating cancer risk (Chapter 18)\nThe Exposure Factors Tab of the Indirect Estimation Module provides links to data on other factors.\nDermal exposure factors should be selected to represent the age, gender (if appropriate), timeframe/activity level, and population group (e.g., resident, occupational worker) for the exposure scenario of interest.\nEPA’s Risk Assessment Guidance for Superfund (Part E, Supplemental Guidance for Dermal Risk Assessment) (U.S. EPA, 2004) and Dermal Exposure Assessment: Principles and Applications (U.S. EPA, 1992a) provide additional guidance on the use of dermal contact parameters. Exposure factors related to dermal exposures are also described in the Soil and Dust, Water and Sediment, and Consumer Products Modules of the Media Tool Set.\nThe following sources provide helpful information for conducting exposure assessments involving the dermal route of exposure.\n- IPCS (International Programme on Chemical Safety). (2004). IPCS Risk Assessment Terminology: Part 2: IPCS Glossary of Key Exposure Assessment Terminology (122 pp, 951 K, About PDF) Exit. Geneva, Switzerland: World Health Organization.\n- U.S. EPA. (1992a). Dermal Exposure Assessment: Principles and Applications (Interim Report). (EPA/600/8-91/011B). Washington, DC.\n- U.S. EPA. (1992b). Guidelines for Exposure Assessment. (EPA/600/Z-92/001). Washington, DC.\n- U.S. EPA. (2004). Risk Assessment Guidance for Superfund (RAGS), Volume I: Human Health Evaluation Manual, (Part E, Supplemental Guidance for Dermal Risk Assessment) Interim. (EPA/540/R/99/005). Washington, DC.\n- U.S. EPA. (2011). Exposure Factors Handbook 2011 Edition. (EPA/600/R-09/052F). Washington, DC.""]"	['<urn:uuid:95b28c6d-53b2-4600-b01a-1e219714b0f6>', '<urn:uuid:4a23ae85-3987-406c-9ee5-6d8546391653>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	17	140	3297
47	What makes these local food and nutrition training programs successful in helping communities?	The success of Farmer Nutrition Schools relies on 11 key elements. These include: learning by doing through practical sessions, using minimal inputs, implementing no cost/low cost solutions, promoting year-round production of diverse foods including animal source food, providing consistent messaging, using simple evidence-based innovations, taking a whole family approach, proper enrollment and orientation, following clear guidelines and fixed curriculum, delivering regular season-based lessons, and maintaining a trained staff for support. These elements ensure sustainable approaches and greater community buy-in for better chances of success.	"['What Are Farmer Nutrition Schools?\nFarmer nutrition schools (FNS) improve food security, empower women, and create social and behavior change for better nutrition by—\n- equipping community members with the practical knowledge and skills needed to produce nutrient-dense fruits, vegetables, and animal source foods at the household level\n- promoting improved hygiene practices.\nIn early 2012, the U.S. Agency for International Development asked SPRING to develop an approach for improving nutrition and food security that could be quickly scaled up across its Feed the Future zone of influence along the coastal belt of Bangladesh. We built our approach on three well-established, widely respected, and evidence-based strategies:\nAs of 2016, there are 6,421 SPRING-implemented farmer nutrition schools across 40 upazilas in Barisal and Khulna divisions that have trained more than 126,000 pregnant and lactating women since May 2012. Analyzing results from the 2014 and 2015 rounds of a study on the effects of the FNS, we found that women who participated in a SPRING FNS had a mean dietary diversity score increase from 3.9 to 6.0, meaning that the women’s diet was nearly twice as diverse as it was when they started the FNS. Further, the consumption of animal source foods went up dramatically, with egg consumption, for example, jumping from 28 percent to 62 percent in just nine months. Dietary inadequacy dropped from 72 percent to only 16 percent over this same period. These results surfaced among other positive changes, including increased knowledge of child feeding practices and handwashing practices.1\n- the essential nutrition actions and essential hygiene actions (ENA/EHA)\n- homestead food production (HFP)\n- farmer field schools (FFS).\nWe called this hybrid approach ""Farmer nutrition schools.""\nHow Do Farmer Nutrition Schools Reduce Malnutrition?\nFarmer nutrition schools conduct practical training sessions over nine months within small rural communities, teaching lessons on topics such as—\n- the significance of nutrition and hygiene practices in households\n- how to improve vegetable farming\n- how to raise local chickens and cultivate fish.\nAn FNS participant learns techniques for—\n- better food production\n- improving ENA/EHA practices\n- linking food production with improved nutrition and hygiene practices.\nTraining methods include—\n- hands-on participatory sessions on improved techniques for better food production\n- participatory discussions and practicing improved techniques immediately after the sessions\n- observations on production, analyzing the results, and making decisions about improved techniques.\nThe ""learning by doing"" format fosters decision-making skills, strengthens community investment in nutrition, and encourages greater involvement in the program.\nWhat Elements Are Necessary for a Successful Farmer Nutrition School Program?\nSPRING/Bangladesh focuses on the promotion and support of ENA/EHA into each session of our food production training modules, which include vegetable gardening, poultry rearing, and fish production. By incorporating messages based on small ""doable"" actions, we ensure that FNS participants receive adequate knowledge to link their food production with improved nutrition and hygiene practices, while also promoting their ability to consume a diversified diet.\nThe agriculture-nutrition pathways are an important conceptual framework when designing and planning interventions that include both nutrition-sensitive and nutrition-specific activities. Our FNS work integrates both types of interventions and the agriculture-nutrition pathways serve as an important roadmap in tracking the input activities to the outputs and ultimate outcomes of the FNS work. They show, for example, how food production influences nutrition outcomes, how agricultural income can affect diet, and how women’s empowerment affects caring practices.\nThe FNS Approach: 11 Key Elements\n|1.||Learning by doing – practical sessions and practice immediately after the sessions||Focus on skills building – more dynamic learning style|\n|2.||Minimal inputs||Focus on skills building instead of incentives|\n|3.||No cost/Low cost solutions||Simple approaches that are sustainable|\n|4.||Year-round production of diversified foods, including animal source food||Food and nutrition security|\n|5.||Consistent and repeated messaging||Reinforcement of ideas for behavior change|\n|6.||Simple, evidence-based innovations||Simple, sustainable approaches|\n|7.||Whole family approach||Responsibility of all, greater buy-in, better chance for success|\n|8.||Proper enrollment and orientation for participants||Communities are properly sensitized to SPRING’s approach before activities begin.|\n|9.||Clear guidelines and fixed curriculum||Well-trained staff for consistent and clear messaging|\n|10.||Regular, comprehensive, and season-based lessons||Make lessons relevant, useful, and routine to ensure greatest possible impact|\n|11.||Trained cadre of staff to train and provide follow-up support||Important for scale-up of project|\nWhat Resources and Tools Are Available?\nFrom 2012-2017, SPRING/Bangladesh has continually improved and refined the FNS model. We also piloted, tested, refined, and published the FNS curriculum.\nTo download or view these documents, please go here.\n- Farmer Nutrition School Technical Guide - Vegetable Gardening, Native Chicken Rearing, and Pond Fish Culture\n- Farmer Nutrition School Session Guide - Essential Nutrition Actions, Essential Hygiene Actions, and Homestead Food Production\n- Farmer Nutrition School flashcards\n- Monitoring and evaluation tools and household register book\nThe goal of SPRING/Bangladesh is to improve the nutritional status of pregnant and lactating women (PLW) and children under the age of two years in Barisal and Khulna divisions by promoting and supporting the adoption of ENA/EHA and the consumption of nutritious and diverse diets through a multichannel and integrated approach, providing wider coverage and reinforcing nutrition-specific and nutrition-sensitive messages.\nOur work targets the 1,000-day window of opportunity among PLW and children under two years of age, using a social and behavior change strategy within the health and agriculture sectors to reduce and prevent stunting in young children. SPRING/Bangladesh is working in 40 upazilas, or sub-districts, in the USAID Feed the Future zones of influence of Barisal and Khulna divisions. In addition to its FNS work, SPRING works with health and agricultural workers within the Government of Bangladesh and other NGOs to develop the confidence and capacity to counsel households with pregnant and lactating women and children under two on nutrition and hygiene at a variety of contact points, from community clinics to home visits.']"	['<urn:uuid:4b7716b5-fbe6-42f3-9728-224aa803a6f1>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	13	84	954
48	I'm curious about new AI systems - is there any way to make them less prejudiced and unfair in how they judge people?	Scientists at MIT CSAIL found a promising approach by training language models to use logical reasoning rather than stereotypes. They created a model that predicts relationships between sentences based on context and semantic meaning, using natural language inference data. This logic-trained model showed significantly less bias than conventional models - for example, it would recognize that being a doctor has no logical connection to gender, while traditional models often incorrectly associate doctors with being male. Testing showed these logic-based models achieved over 90% fairness scores, while other models scored only 40-80%.	['Credit: MIT Computer Science & Artificial Intelligence Lab\nTurns out, even language models “think” they’re biased. When prompted in ChatGPT, the response was as follows: “Yes, language models can have biases, because the training data reflects the biases present in society from which that data was collected. For example, gender and racial biases are prevalent in many real-world datasets, and if a language model is trained on that, it can perpetuate and amplify these biases in its predictions.” A well-known but dangerous problem.\nHumans (typically) can dabble with both logical and stereotypical reasoning when learning. Still, language models mainly mimic the latter, an unfortunate narrative we’ve seen play out ad nauseam when the ability to employ reasoning and critical thinking is absent. So would injecting logic into the fray be enough to mitigate such behavior?\nScientists from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) had an inkling that it might, so they set off to examine if logic-aware language models could significantly avoid more harmful stereotypes. They trained a language model to predict the relationship between two sentences, based on context and semantic meaning, using a dataset with labels for text snippets detailing if a second phrase “entails,” “contradicts,” or is neutral with respect to the first one. Using this dataset—natural language inference—they found that the newly trained models were significantly less biased than other baselines, without any extra data, data editing, or additional training algorithms.\nFor example, with the premise “the person is a doctor” and the hypothesis “the person is masculine,” using these logic-trained models, the relationship would be classified as “neutral,” since there’s no logic that says the person is a man. With more common language models, two sentences might seem to be correlated due to some bias in training data, like “doctor” might be pinged with “masculine,” even when there’s no evidence that the statement is true.\nAt this point, the omnipresent nature of language models is well-known: Applications in natural language processing, speech recognition, conversational AI, and generative tasks abound. While not a nascent field of research, growing pains can take a front seat as they increase in complexity and capability.\n“Current language models suffer from issues with fairness, computational resources, and privacy,” says MIT CSAIL postdoc Hongyin Luo, the lead author of a new paper about the work. “Many estimates say that the CO2 emission of training a language model can be higher than the lifelong emission of a car. Running these large language models is also very expensive because of the amount of parameters and the computational resources they need. With privacy, state-of-the-art language models developed by places like ChatGPT or GPT-3 have their APIs where you must upload your language, but there’s no place for sensitive information regarding things like health care or finance.\n“To solve these challenges, we proposed a logical language model that we qualitatively measured as fair, is 500 times smaller than the state-of-the-art models, can be deployed locally, and with no human-annotated training samples for downstream tasks. Our model uses 1/400 the parameters compared with the largest language models, has better performance on some tasks, and significantly saves computation resources.”\nThis model, which has 350 million parameters, outperformed some very large-scale language models with 100 billion parameters on logic-language understanding tasks. The team evaluated, for example, popular BERT pretrained language models with their “textual entailment” ones on stereotype, profession, and emotion bias tests. The latter outperformed other models with significantly lower bias, while preserving the language modeling ability. The “fairness” was evaluated with something called ideal context association (iCAT) tests, where higher iCAT scores mean fewer stereotypes. The model had higher than 90% iCAT scores, while other strong language understanding models ranged between 40% to 80%.\nLuo wrote the paper alongside MIT Senior Research Scientist James Glass. They will present the work at the Conference of the European Chapter of the Association for Computational Linguistics in Croatia.\nUnsurprisingly, the original pretrained language models the team examined were teeming with bias, confirmed by a slew of reasoning tests demonstrating how professional and emotion terms are significantly biased to the feminine or masculine words in the gender vocabulary.\nWith professions, a language model (which is biased) thinks that “flight attendant,” “secretary,” and “physician’s assistant” are feminine jobs, while “fisherman,” “lawyer,” and “judge” are masculine. Concerning emotions, a language model thinks that “anxious,” “depressed,” and “devastated” are feminine.\nWhile we may still be far away from a neutral language model utopia, this research is ongoing in that pursuit. Currently, the model is just for language understanding, so it’s based on reasoning among existing sentences. Unfortunately, it can’t generate sentences for now, so the next step for the researchers would be targeting the uber-popular generative models built with logical learning to ensure more fairness with computational efficiency.\n“Although stereotypical reasoning is a natural part of human recognition, fairness-aware people conduct reasoning with logic rather than stereotypes when necessary,” says Luo. “We show that language models have similar properties. A language model without explicit logic learning makes plenty of biased reasoning, but adding logic learning can significantly mitigate such behavior. Furthermore, with demonstrated robust zero-shot adaptation ability, the model can be directly deployed to different tasks with more fairness, privacy, and better speed.”']	['<urn:uuid:f9d20f57-8e76-41d1-a3b0-98053fa259ef>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	23	91	871
49	I'm trying to understand what happens in a bank account when multiple people try to withdraw money at the same time. Can you explain what might go wrong in this situation?	When multiple people withdraw money simultaneously from the same account, they might both check the balance and assume it's valid before completing their withdrawals. For example, if an account has $100 and two people try to withdraw ($10 and $25), one person might check the balance and before they complete their withdrawal, the other person withdraws their amount. This can lead to incorrect final balances and even catastrophic failures where the total money in the system isn't conserved.	"['On the surface, time seems straightforward. It is an ordering imposed on events. For any events A and B, either A occurs before B, A and B are simultaneous, or A occurs after B. For instance, returning to the bank account example, suppose that Peter withdraws $10 and Paul withdraws $25 from a joint account that initially contains $100, leaving $65 in the account. Depending on the order of the two withdrawals, the sequence of balances in the account is either or . In a computer implementation of the banking system, this changing sequence of balances could be modeled by successive assignments to a variable balance.\nIn complex situations, however, such a view can be problematic. Suppose that Peter and Paul, and other people besides, are accessing the same bank account through a network of banking machines distributed all over the world. The actual sequence of balances in the account will depend critically on the detailed timing of the accesses and the details of the communication among the machines.\nThis indeterminacy in the order of events can pose serious problems in the design of concurrent systems. For instance, suppose that the withdrawals made by Peter and Paul are implemented as two separate processes sharing a common variable balance, each process specified by the procedure given in section :\n(define (withdraw amount) (if (>= balance amount) (begin (set! balance (- balance amount)) balance) ""Insufficient funds""))If the two processes operate independently, then Peter might test the balance and attempt to withdraw a legitimate amount. However, Paul might withdraw some funds in between the time that Peter checks the balance and the time Peter completes the withdrawal, thus invalidating Peter\'s test.\nThings can be worse still. Consider the expression\n(set! balance (- balance amount))executed as part of each withdrawal process. This consists of three steps: (1) accessing the value of the balance variable; (2) computing the new balance; (3) setting balance to this new value. If Peter and Paul\'s withdrawals execute this statement concurrently, then the two withdrawals might interleave the order in which they access balance and set it to the new value.\nThe timing diagram in figure depicts an order of events where balance starts at 100, Peter withdraws 10, Paul withdraws 25, and yet the final value of balance is 75. As shown in the diagram, the reason for this anomaly is that Paul\'s assignment of 75 to balance is made under the assumption that the value of balance to be decremented is 100. That assumption, however, became invalid when Peter changed balance to 90. This is a catastrophic failure for the banking system, because the total amount of money in the system is not conserved. Before the transactions, the total amount of money was $100. Afterwards, Peter has $10, Paul has $25, and the bank has $75.\nThe general phenomenon illustrated here is that several processes may share a common state variable. What makes this complicated is that more than one process may be trying to manipulate the shared state at the same time. For the bank account example, during each transaction, each customer should be able to act as if the other customers did not exist. When a customer changes the balance in a way that depends on the balance, he must be able to assume that, just before the moment of change, the balance is still what he thought it was.\nCorrect behavior of concurrent programs\nThe above example typifies the subtle bugs that can creep into concurrent programs. The root of this complexity lies in the assignments to variables that are shared among the different processes. We already know that we must be careful in writing programs that use set!, because the results of a computation depend on the order in which the assignments occur. With concurrent processes we must be especially careful about assignments, because we may not be able to control the order of the assignments made by the different processes. If several such changes might be made concurrently (as with two depositors accessing a joint account) we need some way to ensure that our system behaves correctly. For example, in the case of withdrawals from a joint bank account, we must ensure that money is conserved. To make concurrent programs behave correctly, we may have to place some restrictions on concurrent execution.\nOne possible restriction on concurrency would stipulate that no two operations that change any shared state variables can occur at the same time. This is an extremely stringent requirement. For distributed banking, it would require the system designer to ensure that only one transaction could proceed at a time. This would be both inefficient and overly conservative. Figure shows Peter and Paul sharing a bank account, where Paul has a private account as well. The diagram illustrates two withdrawals from the shared account (one by Peter and one by Paul) and a deposit to Paul\'s private account. The two withdrawals from the shared account must not be concurrent (since both access and update the same account), and Paul\'s deposit and withdrawal must not be concurrent (since both access and update the amount in Paul\'s wallet). But there should be no problem permitting Paul\'s deposit to his private account to proceed concurrently with Peter\'s withdrawal from the shared account.\nA less stringent restriction on concurrency would ensure that a concurrent system produces the same result as if the processes had run sequentially in some order. There are two important aspects to this requirement. First, it does not require the processes to actually run sequentially, but only to produce results that are the same as if they had run sequentially. For the example in figure , the designer of the bank account system can safely allow Paul\'s deposit and Peter\'s withdrawal to happen concurrently, because the net result will be the same as if the two operations had happened sequentially. Second, there may be more than one possible ``correct\'\' result produced by a concurrent program, because we require only that the result be the same as for some sequential order. For example, suppose that Peter and Paul\'s joint account starts out with $100, and Peter deposits $40 while Paul concurrently withdraws half the money in the account. Then sequential execution could result in the account balance being either $70 or $90 (see exercise ).\nThere are still weaker requirements for correct execution of concurrent programs. A program for simulating diffusion (say, the flow of heat in an object) might consist of a large number of processes, each one representing a small volume of space, that update their values concurrently. Each process repeatedly changes its value to the average of its own value and its neighbors\' values. This algorithm converges to the right answer independent of the order in which the operations are done; there is no need for any restrictions on concurrent use of the shared values.\nExercise. Suppose that Peter, Paul, and Mary share a joint bank account that initially contains $100. Concurrently, Peter deposits $10, Paul withdraws $20, and Mary withdraws half the money in the account, by executing the following commands:\n|Peter:||(set! balance (+ balance 10))|\n|Paul:||(set! balance (- balance 20))|\n|Mary:||(set! balance (- balance (/ balance 2)))|\na. List all the different possible values for balance after these three transactions have been completed, assuming that the banking system forces the three processes to run sequentially in some order.\nb. What are some other values that could be produced if the system allows the processes to be interleaved? Draw timing diagrams like the one in figure to explain how these values can occur.']"	['<urn:uuid:2389a06c-05be-4f6f-8e30-628ca8f6694c>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	31	78	1263
50	experienced cyclist seeking optimal tire setup comparison between cyclocross winter road conditions traction durability	For cyclocross and winter conditions, tire choice depends on several factors. In cyclocross, riders are restricted to 33mm wide tires for UCI regulated races, simplifying the choice to mainly tread pattern and pressure. For winter conditions, both Bridgestone Blizzak and Michelin X-Ice provide excellent options. The Blizzak line shows superior performance in ice and snow with better wet and dry braking, while Michelin X-Ice offers better fuel efficiency, reduced noise, and longer tread life with a 40,000-mile warranty. Michelin X-Ice Snow specifically outperforms competitors in snow and ice conditions, though with slightly lower performance in wet and dry conditions. For durability, Michelin tires wear out slower due to their EverGrip Technology and MaxTouch Construction.	['“Hey… what’s the best tire for this event?”\nThat’s something I’ve been hearing a lot over the last few months as more and more mixed surface and gravel events come on the calendar. I remember back to my full time cyclo-cross (CX) days riding with the KCCX team and all the conversations we had about tire choice for this course with grass or that course with mud. The question was the same as then and the discussion to find the answer is very similar.\nEver since there were knobby tires, there has been a selection of knobby tires to choose from. Each tread pattern has an intended purpose and an actual purpose depending on the rider. Choosing the “best” knobby tire has become more of an artform than anything as was evident at the 2019 UCI Cyclocross World Championships where the Elite Men and Women both opted for mud tread on a non-muddy course. I’ve always envisioned a bunch of old Belgian guys sitting around smoking and drinking and dreaming up CX tread patterns to be used in the next World Cup or Super Prestige.\nGravel and mixed surface riding seems to be much newer and more trendy. Sure we’ve all forgotten about that random Roubaix race we did back in the day. This is the now and gravel is in. The bikes are amazing and the tire choice is intimidating. Relative to CX, gravel tire choice is probably more complex, simply because there are no restrictions. Fat bike? Go for it. Road bike and tires? That’s fine too. That’s also a range of about a million tread and width combinations… seemingly.\nCX rules basically restrict you to a 33mm wide tire for any UCI regulated races. For USAC events, other than nationals, that rule does not apply. 33c is not particularly wide, and wider tires tend to do better on uneven surface. The result is that every racer picks a 33mm wide tire. The only question would be about tread and pressure. Not easy, but not too complex. Coach Matt writes some good stuff about CX tires here.\nHowever, once you begin the choice of picking the best tires for gravel and mixed surface you get another variable – width. Now days we have all the complexity of tread pattern and we add tire width from 25c- 122c (4.8” fat bike). But which one is the best for you? Well, that’s the $150 question and we can’t tell you what the best tire is for you in any given situation.\nBUT what we can do is provide a guideline for how you choose your tire for the next event.\nFirst and foremost is to set your goal for how you want to perform at the event. Our friends at Enve did a great job of discussion that concept here. If you want to make it through the event and not have any tire issues or flats, it’s best to go bigger and beefier than needed. The sacrifice is speed and agility. Big tires are heavy and heavy tires aren’t fast. Spare me that “get it up to speed” chatter. Tires are always accelerating due to tangential vectors and centripetal acceleration. If your goal is to race the event and rip legs off on your way to gravel glory then you’ll be striking that balance between lightness, performance, and toughness. Narrower tires are (usually) lighter but the smaller tire offers less traction. Note that “less traction” and “enough traction” are different and can overlap. Once you have your personal goal, we can move onto the variables that should impact your tire decision.\nModern tires are trending tubeless (better puncture resistance), wider and because of physics, faster. 25c now defines as a skinny road/ pave based tire with 28c the new norm. 33c while a CX maximum is also an exceptional tire for road and mixed surface events. 40c tires are the gravel standard, which is trending toward 42c. And the 650b wider tires are reserved for anything that’s not rideable on a road bike but you still want drop bars (so, lots of stuff). With the death of rim brakes and rise of disc brakes, the frame is the limiter.\nTerrain is probably the single most influential variable on tire choice. Skinny road tires and CX tires just simply will never be able to ride well in loose sand because it doesn’t have the volume and displacement to stay on top of the sand. Meanwhile, a 47mm wide tire with knobs like the Boken Plus in IRC Tire quiver will grab any trail by the horns but with the weight penalty on pavement given the higher mass and more knobs. Doing some sort of pre ride or consulting with a Gravel Coach or local guru would be prudent.\nThe next variable impacting your decision should be skill level. Skill level is divided into two things, speed/ fitness and bike driving ability. Speed and fitness are key determinants to tire choice because speed changes the way tires ride and what they can do. Any course, no matter how mundane will become technical if there’s enough speed, even NASCAR. Faster speeds mean the tire has less time to deform in order to conform to terrain so maybe wider and bigger is better. But then bigger tires tend to have more tread. However, fitness enables you to use throttle to control the back end of the bike in a similar fashion to dirt bikes and feathering throttle in turns. This could enable you to properly use a smaller tire.\nBike driving ability is essential in gravel riding. Take the midwest plains roads – They’re arranged in 1 mile x 1 mile grids. Not much turning and even the fast downhills are usually in a straight line. However, compare that to the Belgian Waffle Ride which has fast, twisty, technical gravel descents and, all of a sudden, your driving skills are put to the test. Those speeds and your skill could demand dramatically different tire choices. Maybe wider is better, maybe you want some sidewall knobs, but maybe you have a background in downhill racing and a narrower is fine for your particular skillset.\nFinally, how comfortable do you want to be on the road? What are you willing to sacrifice to have that comfort? If you don’t want to feel all the bumps or jostles, wider is better especially when coupled with lower pressure. However, if you want to win the day, or just go faster and not take all day, then you’ll want a different setup. Also, how comfortable are you when tires lose contact with the road? More speed typically means more skipping or bouncing and that means you’re in the air a bit more. How comfortable are you being a projectile, even if it’s for a short amount of time?\n“For tire choices you weigh all the factors that this article lays out above. We know Kanza is in Kansas and therefore, that the course will be a grid system with the exception of a few “B” roads or animal trails. Whatever the course is, even if you have never done it before, you have to do some research on the terrain, and roads styles. Become the child geologist you left behind at age 9. Talk with your coach and others who have done the event, and use this article above as a checklist of sorts to plan your tire choice around trying to not flat on the most technical section of the ride, and not getting dropped on the fastest section.\nFor Kanza that means a somewhat light tubeless tire (everything is tubeless now) with a tread pattern to push some rocks out of the way and maintain traction. The average speed was around 18 miles an hour, pretty dang fast for 214 miles. If you go too big on the tire you have plenty of time to get tired while thinking about how you never flatted on the route. If you go for a gravel racing tire, narrow, fast, light, you will go from off the front on the fast straight smooth sections to waiting because it’s 214 miles and you need people to help, or flatting on the baby head rocks you hit right before the 3 foot ridable creek crossings. As far as mud, there’s really no such thing as a mud clearing gravel bike or tire. Even “mud” tires will have build up and require subsequent actions to fix that, like stopping to clear it, or looking around in the corn field next to you for a new rear derailleur after yours snaps off (you won’t find one). My tire choice was a fast tread pattern, good flat protection but relatively light for a 40mm tire. While rolling fast it was wide enough to be comfortable and rolling in for a top 10 at my first Dirty Kanza 200.”\nThanks for reading. We hope this was able to help you choose wisely and have a great ride!\nAbout the Author: Adam Mills is currently the Performance Director of the Elevate- KHS Pro Cycling Team. He has raced at the elite level since 2002 and graduated with a Masters in Exercise Physiology from the University of Kansas in 2005. His true talent comes with his ability to combine his vast experience with his knowledge of sport. He is indeed a student of science, sport, athletic performance, strategy, and tactics. He continuously educates himself by keeping up to date with current trends and methods in sport and his clients have reaped the benefits from this work with over 13 national championships in 8 disciplines on two continents. Adam is able to incorporate these attributes on a daily basis to help his clients reach and exceed their goals whether they are a beginner or a seasoned professional. Learn more about Adam and Source Endurance here.', 'Bridgestone Blizzak dominates ice and snow performance with seven tire models while Michelin X Ice has three. With so many products in each line, you may find it hard to differentiate Bridgestone Blizzak vs Michelin X-Ice. This article will help you do just that.\n|Michelin X Ice\n|Number of models\n|The Blizzak performs better, but not if we factor in the X Ice Snow\n|Wet and dry performance\n|The Blizzak is better in braking, though both lines have good handling and traction\n|Michelin wins this category\n|Lower by $5-30\nTable of Contents\nWhat is Bridgestone Blizzak? Michelin X-Ice?\n1. Bridgestone Blizzak\nBlizzak Bridgestone winter tires started in 1988. In 34 years, these winter tires have become known for exceptional ice traction and durability, although in general, Bridgestone does not provide an exact mileage warranty on its winter tires.\nWho makes Blizzak tires?\nBlizzak tires are manufactured by Tokyo-based tire maker, Bridgestone Corporation. Blizzak DM-V2 tires are made in Japan. Bridgestone Americas produces the Blizzak LT in Joliette, Quebec.\n2. Michelin X Ice Family\nMichelin has been producing winter tires since 1930. They currently have 8 winter tires, and the snow tire line, Michelin X Ice, consistently places as top competitors in the market.\nX-Ice snow tires are manufactured in\n- Nova Scotia, Canada for North American market\n- France, Germany, Hungary and Poland for European consumers\n- China for Asian market\n- Russia for Eurasia\nDifferences: Michelin X-Ice vs Bridgestone Blizzak\n- Tread Design\nIn general, Bridgestone Blizzak and Michelin X Ice can tackle ice and snow-covered paths because they are equipped with the following components:\n|make the tires soft and flexible even in lower temperatures\n|Flex Ice Compound (for Xi3 and X Ice Snow)\n|increase vehicle traction for stability or prevent slipping in snow and slush\n|Maximized Tread Width\n|enables larger contact area for smooth wet handling\n|Provide stability by having more biting edges on the road\n|Cross Z sipes (for Xi2 and Xi3)\nInterlocking 3D sipes (for the X-Ice Snow)\nKey differences in tread design:\n- Bridgestone Blizzak consumes more fuel than Michelin to roll over because it has a larger contact patch to support.\n- Blizzak grooves are deeper, thus creating more noise and more sensitivity to impact on the road.\n- Blizzak tires wear out faster, unlike Michelin, which is protected by its unique EverGrip Technology and MaxTouch Construction.\n- Michelin X-Ice tires are more fuel-efficient because the new Flex-Ice 2.0 compound and tread design reduced the tire’s rolling resistance by 9%. This is also an advantage of the Michelin X Ice Snow vs Xi3.\n- X Ice tires have a mileage warranty of 40,000 miles, equivalent to around nine winter driving seasons.\nTo put things in perspective, the Blizzak WS80 and Blizzak DM-V2 are in direct competition with the Michelin X Ice Xi3 and Michelin Latitude X-Ice Xi2, while the Blizzak LT competes with the Michelin LTX.\nBridgestone Blizzak tires outperformed the first two Michelin X Ice tires in ice, snow, wet and dry performances. On the other hand, Michelin maintained its lead in noise, comfort and wear categories.\nAs an illustration, between the Blizzak WS80 vs X-Ice Xi3, the former took the lead in ice and snow performance. Other than WS80 vs Xi3 models, the Blizzak WS90 and Blizzak DM V2 followed suit in getting better performance scores in ice and snow than the Michelin X-Ice Xi3.\nThings changed when Michelin introduced the X Ice Snow to replace the X Ice Xi3 and Latitude X Ice Xi2.\nWhen comparing Michelin X-Ice Snow vs Blizzak WS90, their fate in snow was reversed. The X Ice Snow has better acceleration, braking and handling in snow than Michelin’s WS90. The Michelin snow tire also outscored the Blizzak one in ride quality and noise.\nHowever, Bridgestone’s WS90 had an advantage in wet braking by 7.5 feet and dry braking by 3.8 feet from a speed of 50 mph to 0.\nIn short, Michelin X Ice ruled by capturing the overall driving experience. Its latest snow tire outperformed the competitor’s tire lineup in snow and ice but with slightly lower performance in wet and dry conditions.\nBetween the Bridgestone WS90 vs Michelin X Ice snow, the $5 price difference seemed insignificant. The latter is cheaper by almost $5 on Discount Tire, but on Simple Tire, it is the other way around.\nBetween the Blizzak DM-v2 vs Michelin X-ice Snow, Michelin has a lower price tag by $28 on Discount Tire and $30 on Simple Tire.\nNormally, when tires have competitive pricing like this, browsing a forum like Reddit may provide some additional insights.\n|Michelin X Ice Snow\nPros and Cons: X Ice vs Blizzak\nBased on the comparison above, both brands have tires that provides exceptional ice and snow traction. They are the Blizzak WS90 for Bridgestone and Michelin X Ice Snow for Michelin.\nAs you can see from the above comparison, each line’s advantages are where the other brand could improve on.\nMichelin X Ice Advantages\n- Cheaper in some tire dealers\n- More fuel efficient due to lower rolling resistance\n- Reduced noise level\n- More comfortable\n- Promises longer tread life\nBridgestone Blizzak Advantages\n- Superior wet and dry braking\n- Better ice and snow performance\nFrequently Asked Questions\nWhich is a better winter tire Bridgestone or Michelin?\nWhen we compare Michelin and Bridgestone tires, Bridgestone wins the winter category with 7 tire models that cater to extreme winter conditions. Highly-rated for affordability and excellent grip on ice and snow is the Blizzak WS90.\nWhen it comes to snow performance, Michelin LTX Winter was the best tire overall while three of Bridgestone Blizzak tires were recognized as best for highway use, durability and thick ice.\nAre Blizzak tires good on ice?\nYes, they are. In fact, the Blizzak WS90 and DM-V2 achieved third and sixth place respectively in an evaluation of winter tires.\nIn a separate review, the Blizzak DM-V2 was recommended for thick ice conditions because this model feels natural when cornering and has good acceleration and braking in snow.\nWhat tires are best for ice and snow?\nAmong the top tires that are excellent for ice and snow are Nokian Hakkapeliitta 10 SUV, Nokian Hakkapeliitta R3, Bridgestone Blizzak WS90, Toyo Observe GSI-6, and Michelin X-Ice Snow.\nHow many miles do Bridgestone Blizzaks last?\nBridgestone Blizzak tires are reliable for about 3 to 4 winter driving seasons. The mileage you can expect is 12,000 – 15,000.\nWhich is better: X Ice Snow vs Continental Viking 7?\nViking 7’s strengths include excellent braking and handling in dry and icy conditions. It has a 1% lead in snow braking and snow handling. The Continental winter tire is also extremely quiet and comfortable.\nOn the other hand, X Ice Snow’s main advantage is being more fuel-efficient due to its lower rolling resistance. It also performs better in wet handling and aquaplaning. Also, it is 5% more affordable than the Viking 7.\nComparing Bridgestone Blizzak vs Michelin X-Ice is interesting because there are more Blizzak tires to depend on, like Blizzak DM-V2 for thick ice conditions and Blizzak WS90 for light and heavy snow.\nHowever, when the Michelin X Ice Snow appeared, Blizzak tires faced a strong rival which was capable of delivering a quieter, more comfortable and fuel efficient ride.\nIn other words, for specific winter needs, Bridgestone has 7 tire options to choose from while Michelin has a reliable Michelin X Ice Snow.\nIf you want to know more about winter tires, don’t hesitate to ask us questions.']	['<urn:uuid:c9d97a2d-b7c2-476a-832d-4dcd9e3c6445>', '<urn:uuid:c42680ae-cab5-41c7-a74a-08578f7ac83d>']	open-ended	with-premise	long-search-query	distant-from-document	three-doc	expert	2025-05-12T18:38:36.516973	14	114	2898
51	Do natural ventilation and solar water heaters both help save energy?	Yes, both systems contribute to energy efficiency. Natural ventilation is utilized with strategically located openable windows in the Glasnevin Museum, while solar water heaters use free sunshine as fuel and can reduce energy demand on building heating systems.	"['Glasnevin Trust Museum and Glasnevin Heritage ProjectEdit profile\nGLASNEVIN NATIONAL HERITAGE PROJECT & GLASNEVIN TRUST MUSEUM for THE GLASNEVIN TRUST CONTEXT: • Glasnevin Cemetery is run by the Glasnevin Trust, the cemetery first opened in 1834, and from the beginning was completely open to people of all faiths. • Very little space was available for any new buildings within the cemetery walls; the site for the new building was the area immediately east of the former Superintendant’s Lodge, located between the calp boundary wall of the cemetery and the historic Republican graves. • The O’Connell Memorial Tower (1861) and the specimen Wellingtonia tree visually dominate this part of the cemetery: the Tower is visible from large areas of north and west Dublin. • The project included the conservation and refurbishment of the 19th century former Superintendent’s Lodge, - a protected structure – for continued use as executive offices, and also extensive areas of stone paving and landscaping, in addition to the new Museum. • Glasnevin Trust are engaged in extensive ongoing works of conservation and repair of buildings including Protected Structures, memorials, railings, and features throughout the cemetery. CONCEPT: The concept was to house the Museum and to provide for other required accommodation in the Museum and in the lodge, and to enhance the public precincts of the cemetery, while acknowledging the visual primacy of the O’Connell Memorial Tower and respecting the importance within the cemetery of the historic graves, and enhancing the visual dialogue between the Tower, the Museum, and the Wellingtonia tree. DELIVERY: Building Form: • The plan of the building curves outwards at the west end/entrance of the museum, and the roof curves outwards and upwards forming a cantilevered canopy and at the same time provides a sheltered meeting place for visitors. • This curving roof points directionally towards the Tower, creating a dialogue between the Tower, the curving roof of the Museum, and the adjacent Wellingtonia tree. The cylindrical form of the Tower and the large Wellingtonia tree is echoed in turn by the semi-circular north west corner of the Museum. • The new building and the free-standing stone screen wall north of the new building curves on plan to embrace and shelter the historic Republican graves, and the wall changes direction to point towards the Tower while the wall also decreases in height as it approaches the Tower. • This screen wall is composed of stone pillars of varying modular sizes and provides elements of screening between the historic graves and the new building, thus maintaining a calm contemplative atmosphere within the graves area. Paving layout/Landscaping: New paving throughout the precinct radiates outwards from the O’Connell Memorial Tower, thus visually and symbolically reinforcing the importance of the Tower by means of ; • Concentric circles of paving, each circle centred on the Tower, and each circle utilises two different stone types, namely dark grey Irish limestone and silver grey Wicklow granite, • ’Rays’ or radials of pink Kerry Red limestone, radiating outwards from the Tower to the edges of the precinct. Axes: • The north south axis between the main entrance gate and the Tower is reinforced by the geometrical device of spacing the radials equally from the axis, and also by other hard and soft landscaping. • The second axis is the axis located between Cardinal Cullen’s monument and the Mortuary Chapel, and this axis is recognized by means of a common paving area between these two features. New Entrance: • Demolition of the c.1960 extension to the Lodge building enabled the construction of a new entrance gate between the Lodge and the Museum, and also enabled the appearance and form of the19th cent Lodge to revert to it’s original appearance. Materials: Materials used externally in the new building are limited to stone, powder coated aluminium, glass, and Irish oak. • Walls are generally of dark grey powder coated aluminium curtain walling with extensive glass areas. • The overhanging roof fascias and soffits are of silver coloured powder-coated aluminium. • The south elevation to Finglas Road consists largely of louvres of Irish oak, visually hovering between the historic 19th cent. calp boundary wall and the projecting silver roof fascia. • The north elevation facing in towards the cemetery is of curtain walling, curving on plan so as to embrace the historic graves. The boundary between the new building and the graves is further defined by a freestanding dark granite wall. Sustainability: Sustainability provisions included in the Building Design include the following: • Heating : The use of High Efficiency Gas Condensing Boilers was incorporated in the design. A complete building management system (BMES) is provided to control and monitor all building systems. • Ventilation : Natural Ventilation is utilized to the maximum extent, with strategically located openable windows, while a Mixed Mode system complete with heat recovery serves the Milestone Gallery. • Water Services : Rainwater Harvesting System is provided , with a Rainwater Harvesting Tank located at roof level, while Solar Panels are also located at roof level to generate the hot water for the building and this will reduce energy demand on the building’s heating system. • Glass Services : The building design incorporates Low E glass, double glazing with solar control properties. The south elevation facing Finglas Road is provided with Brise Soleil to reduce heat gain: openable windows – openable to facilitate cross ventilation and cleaning - are located behind the Brise Soleil. • Use of Energy Efficient Light Sources are maximized throughout the design.', ""Solar Water heaters| Bayshore Plumbing\nSolar water heaters\nalso called solar domestic hot water systems—can be a cost-effective way to generate hot water for your home. They can be used in any climate, and the fuel they use—sunshine—is free.\nHow They Work\nSolar water heating systems include storage tanks and solar collectors. There are two types of solar water heating systems: active, which have circulating pumps and controls, and passive, which don't.\nMost solar water heaters require a well-insulated storage tank. Solar storage tanks have an additional outlet and inlet connected to and from the collector. In two-tank systems, the solar water heater preheats water before it enters the conventional water heater. In one-tank systems, the back-up heater is combined with the solar storage in one tank.\nThree types of solar collectors are used for residential applications:\nGlazed flat-plate collectors are insulated, weatherproofed boxes that contain a dark absorber plate under one or more glass or plastic (polymer) covers. Unglazed flat-plate collectors—typically used for solar pool heating—have a dark absorber plate, made of metal or polymer, without a cover or enclosure.\nIntegral collector-storage systems\nAlso known as ICS or batch systems, they feature one or more black tanks or tubes in an insulated, glazed box. Cold water first passes through the solar collector, which preheats the water. The water then continues on to the conventional backup water heater, providing a reliable source of hot water. They should be installed only in mild-freeze climates because the outdoor pipes could freeze in severe, cold weather.\nEvacuated-tube solar collectors\nThey feature parallel rows of transparent glass tubes. Each tube contains a glass outer tube and metal absorber tube attached to a fin. The fin's coating absorbs solar energy but inhibits radiative heat loss. These collectors are used more frequently for U.S. commercial applications.\nThere are two types of active solar water heating systems:\nDirect circulation systems\nPumps circulate household water through the collectors and into the home. They work well in climates where it rarely freezes.\nIndirect circulation systems\nPumps circulate a non-freezing, heat-transfer fluid through the collectors and a heat exchanger. This heats the water that then flows into the home. They are popular in climates prone to freezing temperatures.\nPassive solar water heating systems are typically less expensive than active systems, but they're usually not as efficient. However, passive systems can be more reliable and may last longer. There are two basic types of passive systems:\nIntegral collector-storage passive systems\nThese work best in areas where temperatures rarely fall below freezing. They also work well in households with significant daytime and evening hot-water needs.\nWater flows through the system when warm water rises as cooler water sinks. The collector must be installed below the storage tank so that warm water will rise into the tank. These systems are reliable, but contractors must pay careful attention to the roof design because of the heavy storage tank. They are usually more expensive than integral collector-storage passive systems.\nSolar water heating systems almost always require a backup system for cloudy days and times of increased demand. Conventional storage water heaters usually provide backup and may already be part of the solar system package. A backup system may also be part of the solar collector, such as rooftop tanks with thermosyphon systems. Since an integral-collector storage system already stores hot water in addition to collecting solar heat, it may be packaged with a demand (tankless or instantaneous) water heater for backup.\nFor more information about solar water heating system components, see the following information:\nSelecting a Solar Water Heater\nBefore you purchase and install a solar water heating system, you want to do the following:\n- Consider the economics of a solar water heating system\n- Evaluate your site's solar resource\n- Determine the correct system size\n- Determine the system's energy efficiency\n- Estimate and compare system costs\n- Investigate local codes, covenants, and regulations.\nFor information about specific solar water heater models and systems, see the Product Information resources listed on the right side of this page (or below if you've printed the page).\nInstalling and Maintaining the System\nThe proper installation of solar water heaters depends on many factors. These factors include solar resource, climate, local building code requirements, and safety issues; therefore, it's best to have a qualified, solar thermal systems contractor install your system.\nAfter installation, properly maintaining your system will keep it running smoothly. Passive systems don't require much maintenance. For active systems, discuss the maintenance requirements with your system provider, and consult the system's owner's manual. Plumbing and other conventional water heating components require the same maintenance as conventional systems. Glazing may need to be cleaned in dry climates where rainwater doesn't provide a natural rinse.\nRegular maintenance on simple systems can be as infrequent as every 3–5 years, preferably by a solar contractor. Systems with electrical components usually require a replacement part after or two after 10 years. For more information about system maintenance, see the following:\n- Solar Water Heating System Maintenance and Repair\n- Solar Water Heating System Freeze Protection\n- Scaling and Corrosion in Solar Water Heating Systems\nWhen screening potential contractors for installation and/or maintenance, ask the following questions:\nDoes your company have experience installing and maintaining solar water heating systems?\nChoose a company that has experience installing the type of system you want and servicing the applications you select.\nHow many years of experience does your company have with solar heating installation and maintenance?\nThe more experience the better. Request a list of past customers who can provide references.\nIs your company licensed or certified?\nHaving a valid plumber's and/or solar contractor's license is required in some states. Contact your city and county for more information. Confirm licensing with your state's contractor licensing board. The licensing board can also tell you about any complaints against state-licensed contractors.\nFor contractor information, see the Professional Services resources listed on the right side of this page (or below if you've printed it out).\nImproving Energy Efficiency\nAfter your water heater is properly installed and maintained, try some additional energy-saving strategies to help lower your water heating bills, especially if you require a back-up system. Some energy-saving devices and systems are more cost-effective to install with the water heater.\nOther Water Heater Options\n- Conventional storage water heaters\n- Demand water heaters\n- Heat pump water heaters\n- Tankless coil and indirect water heaters""]"	['<urn:uuid:e1ef2809-5547-4a2b-a91b-2af16e07fad4>', '<urn:uuid:e4ba8164-a78a-4907-8c90-8b99e05f426a>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T18:38:36.516973	11	38	1983
52	As a military historian, I'm curious whether Roman armies in field battles and at Alesia used similar defensive equipment?	The defensive equipment differed significantly. In field battles, the defending army could only use spike traps and barricades, and these were only available when having 2 or more non-mounted units. At the Siege of Alesia, Caesar's army employed much more elaborate defenses, including trenches with sharpened branches to impale enemies, hidden holes fitted with protruding branches covered by brushwood, and extensive fortification walls both facing inward toward Alesia and outward to protect against relief forces. The outer fortifications stretched fourteen miles long and included eight connected camps with twenty-three redoubts.	"['As with Rome: Total War, Rome II is set in the classical antiquity and focus on the Roman Republic, allowing players to transform it into the Roman Empire if they so choose.\nThe game has a larger campaign map than Rome: Total War, and apart from encompassing the extent of the Roman Empire and its environments, features new territories ""going further East"". A new graphics engine powers the visuals of the game, and new unit cameras allow players to focus on individual units in the battlefield, which in itself may contain thousands of combatants at a time.\nArmies and navies also have changeable stances on the campaign map. These stances determine many things, among them total movement points per turn or the ability to deploy traps for an ambush. These stances are called ""Forced March"" which enables an army to march further, but which will also tire out its men and reduce their fighting ability; ""Defensive Stance"" that enables the player to place stakes and build temporary forts; ""Ambush Stance"", which enable the army to remain hidden and attack the enemy with traps such as rolling balls of flaming hay; and finally ""Raiding Stance"" which lowers the range that the army can march but greatly lowers its upkeep cost. Armies and navies in Rome II can be made up of a maximum of 20 units and must have a general or admiral to lead them. There is also a cap on the number of armies and navies a faction can have at any time, based on the faction\'s power. A faction can gain more power by conquering more regions and filling its coffers with gold. This system has been implemented to make battles more decisive and for them to have a bigger impact on a war between two factions. Players have the ability to name their armies and navies, and to change their emblems.\nWhen the player creates a general and begins training his troops, the army enters a muster mode and it cannot move while in this stance. Generals acquire skills and traits independently from the army they command. Skills can be chosen by the player as their general levels up while traits are based on what a general does in game. Furthermore, if an army loses its general a new one will be immediately appointed by the player.\nAs with Total War: Shogun 2, the player will be prompted with decisions. The Creative Assembly is expanding on this mechanic, with each decision leading the player down a particular \'decision path\' based on the player\'s previous decisions. These decisions will then affect the way the campaign plays out, such as turning the Roman Republic into the Roman Empire. Additionally, rather than solely assigning traits to generals and family members as with previous Total War games, the player can now assign traits to armies and navies as they gain combat experience through their years of conquest.\nNavies play an even more important role in Total War: Rome II than in its predecessor. Creative Assembly introduced mixed naval and land combat for land battles and city sieges for the first time in the company\'s history. This reflects the naval strategies of the classical era, where coastal cities were conquered and destroyed in great invasions of infantry disembarking from warships. Legions can now attack the enemy\'s ground forces and cities, while naval units provide supporting fire or ram each other far away in the seas. Navies can also conquer poorly guarded coastal cities by themselves. Naval regions have made a return from Medieval: Total War. Controlling all the ports on one sea grants a player lower penalties for piracy on those ports.\nThere are three core types of agents in Rome II; the dignitary, the champion and the spy, but each culture will have its own variants for these. When spawned, each agent will have a ""profession"" that is determined by its supposed background or ethnicity, for example. A player can invest points to an agents profession in addition to its skill tree as the agent levels up. Each agent is able to assassinate other characters and convert them to join the cause of their faction. This is to make each agent type as useful as possible, but naturally different agents will have different skills and purposes only they can fulfill. When an agent is asked to perform a certain task, there is a deeper set of choices on how to complete said task. For example, when getting rid of an enemy agent, one could bribe him off, convert him, or murder him.\nCreative Assembly also seeks to capture the uniqueness of different cultures and fighting forces in the ancient times. Jack Lusted, the Creative Assembly\'s lead unit designer, has stated that instead of the ""rebel nation"" of the original Rome: Total War there are a large number of smaller nations and city states. Each ethnic group has a unique playstyle. A tribe of British barbarians looks and feels completely different from that of a disciplined Roman legion, for example. Different agents and technologies are also implemented for different factions. After all, inland barbarian tribes were not able to research the technology for polyremes or the ability to manufacture advanced ballistae, historically speaking. It was originally stated that there would be over 500 different land units in the game, including mercenaries, who would make a return from Rome II\'s prequel. They also said that over 30 different city variants would be implemented, to avoid siege battles feeling and playing out the same every time.\nIn addition to the traditional sieges and field battles, a myriad of new battle types are available in Rome II. These battle types include the following:\n- Combined naval/land battles: These battles will occur when assaulting a coastal city, or when two armies are near the coastline. In the case of the latter, navies can arrive by sea to bolster the land forces with their marines.\n- Land Battles: These are the most common battles when 2 armies or more approach each other. The defending army receives equipment. Which are spike traps to baracades, this gives this army the option to stake their claim on where the battle will be fought. Equipment only occurs when you have 2 or more melee/ranged non-mounted units. The maximum equipment is 20. It is unknown currently if other factions can receive more.\n- Settlement outskirt battles: These battles are fought near regional capitals, which are too small to have walls.\n- Siege battles: These battles occur when an army assaults a provincial capital or a fortified settlement. In these battles, the cities include multiple capture points which the defender has to defend in order to win the fight, as seen in the Siege of Carthage trailer. The attacker can build siege equipment in preparation of the assault.\n- Encampment battles: These battles are triggered when an army attacks another that is in defensive stance. The defending army has had time to build fortifications around its perimeter, including wooden palisades or small forts. All in all, the battle resembles a small scale siege.\n- River battles: River battles are fought when an army tries to cross a major, navigable river, and another army tries to stop it from doing so. Navies can aid in this fight, although armies are able to build transport ships of their own when crossing rivers.\n- Ambushes: Ambushes have been revamped in Rome II, and feel completely different than before. The ambushing army has the ability to place traps, such as flaming boulders, spikes and so on. The defending army must find a way to escape the ambush area to win, although it can also attempt to destroy the ambushing army. These battles are similar to the battle of Teutoburg Forest historical battle. Although they are no longer unique in the factor of traps.\n- Supply train battles: Supply train battles were included in the initial launch but was removed in the third patch. They occured whenever an army attempted to retreat and was chased down by the attacking army.\n- Port sieges: Another combined land and naval battle type, port sieges are triggered when a navy sails into an enemy coastal city with a port. The navy will attempt to land its marines in the city, while heavier ships intercept any enemy vessels and provide supporting fire to the marines using catapults and other projectiles, like in the Siege of Carthage historical battle.\nThe political system of Rome II has been completely remade and improved. The Roman and Carthagian factions will have three political entities that vie for power inside the faction. Players choose to be part of one of the entities once he selects the faction he wants to play. The political standing of different entities is based on a new resource system, that is in turn based on the deeds and actions of generals and characters belonging to a certain political entity. If one\'s standing drops too low, he may find himself powerless to affect his nation\'s affairs, or if one becomes too powerful, rivals might unite against him. In certain cases, a player can attempt to take all power for himself, thus becoming emperor or king. This requires a civil war, however, another part of the game completely redesigned by the Creative Assembly.\nPlayable Factions Edit\nThe Roman Republic\n- House of Julia\n- House of Cornelia\n- House of Junia\n- Lepidus\'s Rome (Imperator Augustus)\n- Octavian\'s Rome (Imperator Augustus)\n- Antony\'s Rome (Imperator Augustus)\n- Pompey\' Rome (Imperator Augustus)\nGreek States (DLC)\nNomadic Tribes (DLC)\nCaesar In Gaul (DLC)\n- Rome, paragon from civilization\n- Arverni, champions of Gaul\n- Nervii, bravest of the Belgae\n- Suebi, invaders from Germania\nHannibal At The Gates (DLC)\nBlack Sea Colonies (DLC)\nWrath of Sparta\n- Boiotain League\n|This page uses content from Wikipedia. The original article was at Total War: Rome II. The list of authors can be seen in the page history. As with Total War Wiki, the text of Wikipedia is available under the Creative Commons License.|\n|The Total War series|\n|Shogun (Mongol Invasion) • Medieval (Viking Invasion) • Rome (Barbarian Invasion • Alexander) • Spartan • Medieval II (Kingdoms) • Empire • Napoleon • Total War: Shogun 2 • Total War: Rome II • Total War: Attila • Total War: Warhammer • Total War: Arena • Total War: Warhammer II|', 'War History Online presents this Guest Article from by Joseph M. Durante\n“Gaul comprises three areas…” and “the whole of Gaul was now conquered.”\nIn 58 B.C. the military commander, politician, and aristocrat Gaius Julius Caesar waged war in Gaul and for six years tried to conquer the region and submit them to Roman control. By September of 52 B.C., Caesar was about to conquer of all of Gaul with one final siege and battle, the Siege of Alesia. Julius Caesar is easily one of the most well-known Romans and it was his military conquests that established his name as one of the greatest military commanders in history. His siege of Alesia, know as the circumvallation of Alesia, is what really set the mark on how intelligent he was regarding military stratagems. With his victory at Alesia, all of Gaul became under Roman control and paved Caesar’s path to glory.\nThe most important detail of this battle is to explain exactly what circumvallation is. Circumvallation is a military strategy used to siege a fortified position–cities, towns, villages, forts, and camps. This requires the army laying siege on a fortified position to construct siege equipment that covers the area of the fortified position. Usually, placing barracks with ramparts assigned to infantry with enough supplies to outlast the people inside the fortified position is key to a successful siege. Of course, the army laying siege must not allow anyone or anything to enter or leave the fortified position, because there can be no relief aid to the people inside. Some might wonder, though–why not just storm the city? Caesar knew a full frontal assault would produce heavy casualties and compromise victory over the Gauls, writing, “it was clearly impregnable except by blockade,”and that the city was on a hill of high altitude, with streams to the north and south, surrounded by hills on the north, east, and south sides.\nThis strategy is common among many sieges because it is a necessary factor in achieving victory. The circumvallation siege is supplied with infantry to defend if the enemy leaves the fortified area to try and break the siege. Artillery is used to help siege the area, such as ballistae, onagers, catapults, archers, all of which are to help suppress the actions and protect the siege machines.\nBut Julius Caesar’s siege was a little different than the definition discussed above. Caesar surrounded the city of Alesia with ramparts and infantry, but also built another set of walls and ramparts facing outward, behind his legions. During the construction of the siege, the Gauls from inside the city attacked Caesar’s troops. Their leader, Vercingetorix, then Chieftain of all the Gauls, took command of the Gallic tribes in the same year.\nThe Gauls sent their cavalry to attack the Romans constructing the siege, and in response “Caesar committed his reserve of German horseman and formed up some of the legionaries in support,” which caused the Gauls to retreat back into the city. Vercingetorix, fearing entrapment within the city for good, “sent his cavalry away before the blockade was closed, telling them to return to their tribes and raise a relief army.” Once the wall facing inward was complete, Caesar immediately instructed his men to work on the wall facing outward, knowing that the cavalry who slipped through would come back with a relief force. In response to the siege being completed, Vercingetorix released all the women, children, elderly and ill from the city. By doing so, Vercingetorix now had fewer mouths to feed and ensured that the army’s food supply could sustainably feed his warriors.\nCaesar, however, was not content to let the unarmed civilians through the siege, and so they were left to die in a kind of ancient no man’s land, stuck between both fortified armies. With the food now reserved for fighting men, Vercingetorix waited to attack Caesar so he could unite with the Gallic relief force. With the relief force in sight, Vercingetorix attacked the inner fortifications and the relief force attacked the outside fortifications. In response, Caesar “rush[ed] reinforcements to threatened sectors and at times led counterattacks against the enemy flanks,” leading the Romans to victory and pushing the Gallic force to surrender. Vercingetorix himself surrendered, warriors became war captives, and all of Gaul was soon under Roman control. Vercingetorix was captured, “kept in chains reserved for Caesar’s eventual triumphal procession, for six long years,” and “in 46 B.C. his shrunken frame was dressed once more in his best armor; and after being paraded in Caesar’s triumph Vercingetorix, a prince of Gaul, was ritually strangled.”\nAccording to Caesar himself, he had his men build eight camps that were connected by fortifications along with twenty-three redoubts. As mentioned earlier, Vercingetorix sent messengers to call for relief before the Romans could finish their fortifications. To help fight against surprise attacks, Caesar ordered his men to dig trenches all over the land in between Alesia and Caesar’s fortifications. The Romans also installed sharpened branches inside those trenches to impale any enemies that fell into them. Caesar also had his men dig smaller holes in the ground, fitted for one person with sharpened branches protruding towards the middle of the hole and covered the hole with branches and brushwood to hide the trap. As a result, when someone would fall into it, the branches would impale their legs and lower body and when they would try to climb out of the hole, the branches would shred their bodies. When these fortifications were finished, Caesar constructed the outside walls and fortifications to defend against the relief aid. These fortifications were fourteen miles long, located on the flattest ground around Alesia. With the inner wall now finished, they were able to protect the Roman siege workers working on the outside wall from Vercingetorix’s cavalry.\nThe importance of this siege is to show that ancient warfare was extremely tactical and that stratagems had to be deployed to obtain victory in war. This siege demonstrated the level of intelligence that Caesar had regarding military leaderships. Caesar would later go on to fight a civil war against his long time friend, ally, and co-consul Gnaeus Pompeius Magnus (Pompey the Great). With a victory against Pompey and the Senate, Caesar became sole political leader of the Roman Republic and became Dictator for ten years, and before his assassination, he was named dictator for life.\nGallic Forces Inside Alesia:\n- Vercingetorix – 80,000\nGallic Military Relief Forces:\n- Aedui (included, Segusiavi, Ambivareti, Aulerci, Brannovices, Blannovii) – 35,000\n- Arverni (included, Eleuteti, Cadurci, Gabali, Vellavii) – 35,000\n- Seguani – 12,000\n- Senones – 12,000\n- Bituriges – 12,000\n- Santoni – 12,000\n- Ruteni – 12,000\n- Carnutes – 12,000\n- Bellovaci – 10,000\n- Lemovices – 10,000\n- Pictones – 8,000\n- Turoni – 8,000\n- Parisii – 8,000\n- Helvetii – 8,000\n- Suessiones – 5,000\n- Ambiani – 5,000\n- Mediomatrici – 5,000\n- Petrocorii – 5,000\n- Nervii – 5,000\n- Morini – 5,000\n- Nitiobroges – 5,000\n- Aulerci Cenomani – 5,000\n- Atrebates – 4,000\n- Veliocasses – 3,000\n- Aulerci Eburovices – 3,000\n- Rauraci – 1,000\n- Boii – 1,000\n- Aremorican (included, Coriosolites, Redones, Ambibarii, Caleti, Osismi, Veneti, Lexovii, Vanelli) – 20,000\n- Bellovaci – 2,000\n- Army Strength – around 256,000 infantry and 8,000 cavalry strength\nJulius Caesar and Roman Army Strength:\n- Roman legions – 50,000\n- 5th Alaudae Legion\n- 8th Augusta Legion\n- 9th Hispania Legion\n- 10th Fretensis Legion\n- 11th Claudia Legion\n- 12th Fulminata Legion\n- 13th Gemina Legion\n- 14th Gemina Martia Victrix\n- Auxiliary infantry and cavalry – 30,000\n- Caesar, Julius. The Conquest of Gaul. Translated by S.A. Handford. England: Penguin Books, 1951.\n- Penrose, Jane, ed. Rome and Her Enemies: An Empire Created and Destroyed by War. Oxford: Osprey Publishing, 2008.\n- Goldsworthy, Adrian. Caesar: Life of a Colossus. United States: Yale University Press, 2006.\n- Goldsworthy, Adrian. The Complete Roman Army. London: Thames and Hudson Ltd, 2004.\n- Dando-Collins, Stephen. Caesar’s Legion: The Epic Saga of Julius Caesar’s Elite Tenth Legion and the Armies of Rome. Hoboken: John Wiley and Sons, Inc., 2002.\n- Dando-Collins, Stephen. Legions of Rome: The Definitive History of Every Imperial Roman Legion. New York City: St. Martin’s Press, 2010.']"	['<urn:uuid:183c3bfd-061d-4941-a652-3c01b9a0a32b>', '<urn:uuid:e2ef88f0-5ef4-4051-8bf0-1a92fd703682>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T18:38:36.516973	19	90	3111
53	what are the different felony classes and jail time for each in south carolina	In South Carolina, felonies are organized into six categories from Class A through Class F, with different maximum prison sentences for each: Class A felonies carry up to 30 years, Class B up to 25 years, Class C up to 20 years, Class D up to 15 years, Class E up to 10 years, and Class F up to 5 years. Some felonies are exempted from this classification system and have their own specific penalties.	"['South Carolina Felony Crimes by Class and Sentences\nThe state of South Carolina organizes felony crimes into six different categories: Class A through Class F. Class A felonies represent the most significant offenses, while Class F crimes are the least serious felonies.\nFor information on misdemeanors, see South Carolina Misdemeanor Crimes by Class and Sentences.\nSouth Carolina identifies some felonies that do not fall into the categorization system. Each of these ""exempted"" crimes has its own potential penalties. There are many such crimes exempted from the classification system, including crimes such as murder, incest, first-degree burglary, and failure to register as a sex offender.\nAdditionally, South Carolina law allows courts to impose a range of penalties for any crime not specifically exempted from the felony classifications or given a specified felony class.\nSentence and Sentence Range for Each Level\nAnyone convicted of a felony offense in South Carolina faces a potential prison sentence that depends on the class of the felony committed. The sentences associated with each class are maximum incarceration penalties, and individual sentences in any given case could be less.\n- Class A. Imprisonment for not more than 30 years.\n- Class B. Imprisonment for not more than 25 years.\n- Class C. Imprisonment for not more than 20 years.\n- Class D. Imprisonment for not more than 15 years.\n- Class E. Imprisonment for not more than 10 years.\n- Class F. Imprisonment for not more than 5 years.\nIn addition to incarceration, someone convicted of a felony in South Carolina may also be sentenced to pay a fine. Unlike prison terms, each class of felonies does not have an associated fine. Instead, each criminal statute states a fine, range of fines, or leaves it up to the court to impose a fine it deems appropriate.\nFor example, someone convicted of threatening the life, person, or family of a public official, a Class F felony, faces a fine of up to $5,000. On the other hand, someone convicted of stealing livestock valued at $10,000 or more, a Class E felony, faces a fine of up to $2,500.\nThe penalties associated with each exempted crime differ significantly. For example, someone convicted of murder faces a penalty of a minimum of 30 years to life in prison, or death. Someone convicted of incest faces a minimum $500 fine and a minimum of one year in prison.\nFor any non-specified felony, a court can impose an incarceration sentence of between three months and 10 years.\nExamples of Crimes at Each Level\nSouth Carolina felony statutes encompass a number of crimes. The following is a list of crimes at each felony level, though it is not representative of all felonies in South Carolina.\n- Voluntary manslaughter\n- Attempted murder\n- Failure to stop a vehicle in an accident when death occurs\n- Second-degree arson\n- Second-degree assault and battery by mob\n- Sexual exploitation of a minor\n- Attempted armed robbery\n- Manufacturing or distributing methamphetamine or cocaine, first offense\n- Third-degree arson\n- Second degree burglary\n- Causing harm to a child\n- Reckless homicide\n- First-degree sexual misconduct\n- Illegal conduct at elections\n- Sale or possession of counterfeit cigarettes\nSouth Carolina Statute of Limitations\nA statute of limitations is a law that requires prosecutors to file criminal charges in a specified timeframe. If prosecutors fail to file charges before time runs out, they cannot file charges later.\nSouth Carolina is one of the few states that have no criminal statute of limitations. This means prosecutors can file criminal charges against a person no matter how long it has been since the crime took place. You can learn more about the state’s law by reading South Carolina Criminal Statute of Limitations.\nTalk to a Lawyer Near You For Legal Advice\nA local criminal defense attorney who has represented clients in local courts and who has negotiated with prosecutors in your area is the only person who is capable of giving you legal advice about your case. South Carolina felony charges are very serious and have significant potential penalties associated with them. You need to speak to a lawyer as soon as you are arrested, charged, or approached by investigators. Not consulting an experienced criminal lawyer can significantly damage your case and hurt your chance to present the best possible legal defense.']"	['<urn:uuid:68ce8536-c8f0-497b-afc3-264acb6699d3>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	14	75	722
54	How do dams and lakes keep water from leaking out?	Both dams and lakes rely on clay materials for water retention. Lakes need a water-holding clay layer at least 12 inches thick that's carefully compacted, while dams use an impervious core (usually clay) or an impervious facing. However, it's important to note that all man-made lakes will leak to some degree - the goal is to minimize leakage through proper construction techniques and material selection.	['Basic Lake Construction Techniques For Building A Lake That Holds Water…… The Best It Can\nFocusing On Treating The Best Clay Material You Have The Best You Can\nBy: Brad Vollmar\nVollmar Pond & Lake Management\nLet’s start by saying all lakes man made or not will leak. The goal of any lake construction project should be to have a lake that leaks the least the site and site material allow.\nWe are not going to focus on lake site selection at this point, but it is fair to say lake site selection is EXTREMELY important and can make or break a lake construction project. Vollmar Pond and Lake Management can perform complete lake site assessments for you, they are money well spent, Benjamin Franklin said “ By Failing to Prepare, You Are Preparing to Fail”. Some basic lake site assessment questions are but not limited to:\n- How deep to bedrock?\n- Is the bedrock fractured ? to what extent?\n- How deep to sand or gravel layers?\n- Is there enough suitable material to make a water tight layer of clay? The lining material should be at least 20% clay\n- How much rain do you get?\n- How much rain will runoff into the lake?\n- How big is the watershed?\nAssuming you have a decent lake site and you know how big the lake can be (not how many acres but how many acre-feet, a volume measurement), lets start digging.\nAs you begin digging you need to know which material or combination of materials will be used to make the water holding clay layer (field test and send to geotech lab). The most suitable material(s) need to be sorted from the less suitable material. The shape and depth profile of the lake should be taking shape as the material is excavated away and sorted. The best clay on-site should be reserved for the dam core the second best for the water holding clay layer that is applied over the entire lake surface and upstream dam face. A dam core is the inner water holding structure of an earthen dam. The dam core should extend into a hard substrate, for instance rock, clay, or shale. More on the dam core later. The excavated hole should be about 2-3 feet deeper than the final depths of the planned lake because there will be two layers added to this excavated hole: 1) the water holding clay layer and 2) the protective layer applied over the water holding clay layer.\nTwo Pieces of Equipment Imperative to a Successful Lake Construction Project:\nSo the lake has taken shape, and the dam is being built and cored simultaneously. Before bringing in the material for the water holding clay layer spread over the entire lake surface, the sub-grade of the lake needs to be watered and compacted (sub-grade layer is the ground in which the clay will be placed, should be 2-3 feet below final lake depths). The amount of water needed for compaction is determined by the material you are working with, but you don’t want it to dry or to wet. Think back when you were a kid and where making mud balls…..to much water you have mud and it can not be formed….to little water you can’t get the dirt to stick. The sub-grade needs be watered and rolled with a vibratory sheep’s foot roller 4-6 times.\nAfter the sub-grade is watered and compacted it should be ran over with a disc (plow) about 2” deep. This shallow plowing gives the suitable water holding clay material which will be added to the sub-grade something to hold on to. The suitable water holding clay material needs to laid down in 6-8” lifts and then watered and compacted. When using a sheep’s foot roller estimating proper compaction can be done by looking at the feet on the roller, if the feet barely sink in the material, proper compaction is close. The water holding clay layer needs to be at least 12” thick once compacted, but final compacted thickness of the water holding clay layer is a function of the proposed depth of the lake ie. the deeper the water the thicker the layer need to be.\nNow that the water holding clay layer is in place it needs to be protected with non-select excavated material that was sorted earlier in the project. The non-select excavated material will help blanket the select excavated water holding clay material you placed and compacted on the sub-grade The water holding layer should be ran over with a disc (plow) about 2” deep. This shallow plowing gives the non-select excavated material something to hold on to. Add the non-select excavated material in 6-8” lifts as done earlier, these lifts need to be watered and compacted in the same manner as earlier. This protective blanket should be at least 12” thick. This protective blanket keeps the water holding clay layer from being exposed to drying and cracking as well as erosional forces.\nThe above information mostly addresses the reservoir pooling area of the lake, but very special attention needs to be paid to the dam if your lake has a dam (some lakes do not, they are just “dug-out” holes). If a dam is constructed the base trench of the dam needs to start at least 2-3 feet into solid suitable material (clay, shale, or bedrock). This trench is sometimes called a cut-off trench or a key-way. The idea is this key-way doesn’t allow water to go under the dam. The key-way is the base of what is called the dam core. The dam core and key-way is usually the center section of the dam and is constructed with the best water holding clay material available. The dam cores are at least 8 feet wide. The dam is constructed in lifts of 6-8”, but instead of just laying down one type of material, you lay down select material (makes up the core) and flank it with non-select material. These lifts are watered and compacted as described above. The core needs to be as high as the full water level. See Schematic below:\nThings You Should Consider:\nAll material has a maximum compaction, and in lake building you want to hit at least 95% of the materials maximum compaction. A geotecnical engineer can help determine this maximum compaction and tell you if you met it.\nWatering and compacting can improve most soils ability to hold water, that is why we recommend watering and compaction at all stages of construction.\nLake spillways are critical to safety and longevity of your lake, make sure the spillway is large enough to handle excess run-off in a manner that reduces the water’s erosive forces (slow the velocity). For larger lakes an engineer should be involved in spillway sizing.\nThis information listed above is basic and may not apply to very large lakes, however any size lake project always benefits from water and compaction.\nIf permeable material (fractured rock, gravel, sand) is found while excavating the hole special attention needs to be paid in these locations. Generally you cut these materials out as far as possible and back-fill and compact with suitable material.\nDo not hire a contractor unwilling to water and compact as much as described here. By no means hire a contractor who says they will compact with their bulldozer. When building a lake, you are paying for time and equipment, if the contractor is unwilling to compact at each stage of the process, they should not be considered.\nBy: Brad Vollmar\nVollmar Pond & Lake Management', 'MAJOR RISKS ASSOCIATED WITH DAM CONSTRUCTION\nA dam is a Civil Engineering construction which serves to contain the water of a natural course with two different aims, either alternative or simultaneous, according to the cases:\n- To increase the level so that it can be diverted through a spillway in order to regulate the volume of water.\n- To create a reservoir in order to retain the excess of water, allowing the supply of the liquid in moments of shortage.\nDams must fulfill a double function:\n- they must be impervious and\n- stable to resist the water pushing behind it.\nConde Guadalhorce Dam\nDams can be grouped into two major categories on the basis of their composition:\n- concrete dams and\n- embankment dams.\nThe selection of one or the other type depends on the topography and conditions of the ground.\n- Concrete dams are fundamentally constructed with concrete, with or without steel frameworks.\nThere are three different subcategories:\n- Gravity dams: The water is held back because of the own weight of the dam, without the intervention of the arch shape.\nGravity dam scheme\n- Arch dams: Arch dams transmit the thrust (vertical and horizontal loads) towards its foundation and abutments, due to the “arch” effect. They may have horizontal or double -horizontal and vertical- curvature, which is the most normal.\nDouble curvature dam\n- Arch-Gravity dams: A combination of the two previous dam types. The single action of the curvature is insufficient to withstand the thrust and the dam must be provided with a certain weight to compensate this defect.\n- Embankment dams are constituted by materials provided by nature which do not suffer any chemical process of transformation, being treated and laid through compaction procedures of soil mechanics, acting by gravity. They are composed of stone, gravel, earth, silt and clay, in accordance with the following classification:\n- Rockfill dams. When more than 50% of the material is composed of rock.\n- Earthfill dams. Also known as “rolled earth dams”. For materials with lower grain size.\nAccording to the distribution of materials:\n- Homogeneous: When all the material that the embankment dam is made of has the same characteristics. It may deal with more or less impervious materials (clay or silt). It requires very smooth slopes for the haunches to be stable under the different actions which may show up.\n- Zoned: This is the most common type of dam, with employment of different materials, with an impervious core (usually clay) and more permeable materials as we get further from the dam core. So, one part of the dam fulfills the impermeability criterion and the rest has enough resistance to provide the construction with overall stability. They allow for higher slope inclination, so that the dam occupies a lower surface area.\nZoned dams may have two different construction typologies:\n- An impervious core (made from clay or silt), either in the interior of the dam, in the centre (vertical or almost vertical) or near the upstream facing (inclined).\nHeterogeneous embankment dam with central core\n- An impervious facing covering the upstream slope area. This can be achieved through concrete or asphalt fabric screens.\nHeterogeneous embankment dam with impervious screen\nDam construction follows the following process:\n- River diversion\nFor the construction of the dam side in the course of the river, it is necessary to divert the river to create a dry work area at the dam site.\nThis dry area can be created in two different ways:\n- By totally diverting the river through an artificial course. It consists of the construction of an artificial course, for which it is needed to construct a temporary dam, called cofferdam (or diversion dam), in order to make a pool for the water to enter through the new course. It is a conduit (open channel or pipe) which transports the diverted water to a spot downstream. Moreover, the cofferdam shall contain the waters which cannot be held by the conduit.\nDiversion tunnel at Ralco dam (Chile)\nStone cofferdam at Three Gorges Dam (China)\n- By leaving only a portion of the waterway dry and channelling the passage of current through the rest, thus successively carrying out the work.\n- Auxiliary works\nIt is necessary for dam construction to transport large amounts of construction material, in many cases across very irregular or mountainous land. Therefore, it becomes necessary to construct provisional access roads and bridges, as well as tunnels (in occasions) or even to carry out slope filling works, etc.\nMoreover, depending on the function to be fulfilled by the dam, different water pathways must also be built -most of them underground- including the respective water inlet and outlet structures.\nThese works include:\n- Water intake towers and supply tunnels for water supply projects.\n- Upstream and downstream galleries.\n- Surge shafts.\n- Galleries for hydroelectric plant turbine room and transformers.\n- Construction of the dam itself\nThe construction method varies depending on the type of dam:\n- Concrete dams. These are built basically through two different methods:\n- Conventional method. Individual sections are constructed as blocks and concreted forming “towers”, the size of which depends on considerations in relation to the construction project and the setting heat to be expected. In this case, the decisive factors are: the temperature during the on-site concrete casting, the cement percentage, the concrete cooling system and the weather conditions at the dam site, as the setting and cooling temperatures may be affected.\nConstruction with blocks\nThere must be interruptions during the construction, so that construction joints are produced, which are practically horizontal shaped. Besides, the so called functional joints are constructed in order to prevent cracks from occurring in the concrete expansion and contraction processes -with possible seepage and resistance reduction-, causing the division of the dam into blocks with certain independence from each other. After the project completion, when the dam’s own weight acts, and prior to filling the reservoir, cement suspension is injected in the block cracks in order to achieve the monolithic structure.\n- RCC (Roller Compacted Concrete) Method. RCC is also known as “Rollcrete”. In this construction technique, the concrete is transported to the dam surface by trucks or conveyor belts. Then, it is distributed with bulldozers and compacted with vibratory rollers. The thickness of the concrete layer ranges between 30 and 50 cm. In order to ensure watertightness on both sides (dam facings), these are built of concrete according to the conventional method. Additionally, cement mortar is laid between the horizontal layers of Roller Compacted Concrete.\nConcrete is saved by using this method, together with a lower setting heat, thus avoiding the functional joints. Therefore, it is more economical than the previous technique.\nRalco dam construction (Chile)\n- Embankment dams:\n- Impervious inner core. Embankment dams are built by placing selected layers (30 to 50 cm thick) of fill material with different grain size, pre-treated if necessary. This material is compacted, important operation to prevent settlements and obtain the maximum resistance. During compaction, as the optimum degree of density depends on the water content of the material, work must stop during rainfall. In isolate cases, hard rainfall may make necessary the removal of material already placed and compacted before placing the next layer.\n- Rockfill dams with impervious face\n- Concrete-faced rockfill dams (CFRD): The impervious element in this type of dam is a concrete slab on the upstream slope in contact with the reservoir water. The concrete face is made up by triangle-shaped or quadrilateral-shaped slabs joined by means of impervious joints (corrugated copper plates or some bituminous substance), and allowing the independent movement of each slab.\nAt the dam toe, on the upstream side, the slab is connected to a concrete retaining wall which forms the transition between the rock subsoil and the impervious concrete slab.\nThe fundamental defect of the concrete face is the great difference in deformability compared to the material of the dam, which makes it very sensitive to possible dam settlements, which may leave the wall unsupported, and therefore they are reinforced in two directions.\n- Asphalt facing: The impervious element is a bituminous layer placed on the upstream slope of the dam.\nThe advantage lies in the fact that, besides being impervious, the material is also flexible, so that it adapts very well to dam settlements. Additionally, the construction and repair are easy and fast, and in the event of cracks occurring, they may close on their own thanks to the plasticity feature of the material.\nExecution of asphalt layer at Ralco dam\nThe construction method is similar to concrete facing rockfill dams, with the difference that the asphalt layer is laid once the dam body has reached its definitive height. The face is generally built in several layers (30-50 cm thick), and it must be supported on a flat surface made of porous or asphaltic concrete. The tamping is done with rollers moved from the crest.\n- The project and construction of a dam present special problems demanding high knowledge of several sciences and techniques: Geology, foundations, Hydraulics, properties and treatment of materials, etc. Perhaps, that is the reason why it requires, more than any other construction project, the collaboration of several specialists and team work. As a conclusion, the plan and the project have special importance in dam construction, for the corresponding works and for the river diversion system.\n- River diversion. The maximum flood to be diverted is the major problem in designing and determining the dimensions of the diversion elements. It must be determined based on the following dilemma:\n- If we fall short, floods higher than the diversion capacity will be spilt over the diversion works and will flow through the old natural course, flooding the works that are being built.\n- If, in order to avoid this, a very wide diversion is done, it may be excessively costly.\nThe drainage capacity of diversion tunnels is generally designed to have a 30 to 50-year return period flood event, in order to prevent flooding during the construction. Therefore, the two major risks associated with dam construction during the construction stage are: 1) The possible flooding of the works due to rain-induced floods (they are very sensitive to natural hazards), as such flows are not contained by the diversion elements; and 2) The risks derived from the necessary explosions to demolish the temporary cofferdams.\nTwo moments of a cofferdam demolition (with explosives) at Three Gorges Dam (China)\n- Auxiliary works. Due to the fact that access roads are provisional, no coating layer is applied on them, meaning that they are exposed to rain and possible floods.\nIn general, temporary works (roads, bridges, fillings, etc.) are very affected by landslides induced, for example, by hard rainfall; as well as natural hazards (earthquakes, etc.).\n- Materials storage warehouses and construction machinery are highly exposed to landslides, if they are located in the proximities of slopes susceptible to landsliding.\n- Embankment dams. These dams do not withstand being overtaken by floods, both during construction and once under operation, as the little cohesion of the materials they are composed of could be reduced, which might cause the collapse of part of the wall, as well as filtration areas. That is why it is important to carry out a good study on historical rainfall on the river during the project stage.\nThe dam toe is one of the most important elements in the construction of concrete facing dams. Its correct formation is essential to assure the watertightness of the dam.\nInternal erosion may develop in the joint zones between the impervious material of the dam core and the integrated concrete structures (bottom drain, etc.). In order to prevent this phenomenon, they must be planned and constructed with special care.\n- Double curvature dams. They are very slender and may adopt very daring and complex shapes, which allows them to be very high and with small thickness. This complexity requires great skillfulness and experience from the constructors, who must resort to uncommon construction systems and the use of expensive construction equipment as well. Due to this, any mistake made either during the planning or the project stage may lead to future accidents.\nGenerally, because of the large loads transmitted to the ground due to the “arch” effect, it must be resorted to extensive foundation works.\nConcrete may be altered by the presence of meteorological or environmental agents, by increasing its resistance in direct proportion to its thickness. Increase in thickness may lead to being less demanding with the quality of the materials in order to save costs.\n- A double curvature dam must be monolithic, so that the dam site is required to be resistant and little deformable, resorting if necessary to ground improvement techniques (injections, etc.).\n- Although damage by fire is not one of the major risks for these construction projects, material warehouses, workshops and construction equipment must be equipped with their corresponding fire extinguishing equipment.\nAs for the basin (bowl-shaped piece of land which serves for containing water) and the dam site (piece of land where the dam itself is located), they are required to be impervious, so that it is necessary to prevent seepage insofar as possible. Due to this, it is vital to conduct a proper ground study in order to get to know its characteristics and mechanical behavior. For example, limestone soil which may have caverns would be bad soil, whereas marl or granite grounds (provided they are not jointed) show a good behavior.\nPelatihan terkait risiko Konstruksi Bendungan:\n(Untuk info lebih detail & konsultasi gratis tentang Risk Engineering)\n+628129992037 (Russel Effandy)\n+6281293617419 (Cepa Jasa Indonesia Center – CJIC)']	['<urn:uuid:7805d312-e7b6-4856-85b8-4efc508a4b92>', '<urn:uuid:5e285c46-fbbe-45ac-93e8-957d987b071c>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	10	65	3537
55	underwater mountains pacific ocean size compare	The mountains beneath the Pacific Ocean rise 10,000 feet above the seafloor, creating escarpments, ridges and valleys that rival the peaks of the Rocky Mountains in size.	['Through the Looking Glass: Peering Through the Bottom of the Ocean\nWe are in the fifth day of our research cruise to the Line Islands and shipboard life is beginning to settle into a routine. Most people have their ‘sea legs’ and our sleep schedules are adjusting to the midnight to noon or noon to midnight work shifts. Meals are a time to catch up with scientist and crew, and the motivated scientists have begun regular exercise schedules in the ship’s gym.\nAs we steam over the incredibly wide expanse of the Pacific Ocean, the waves seem endless and monotonous, and the wind blows steadily from the same direction for days on end. However, beneath us the seafloor is far from monotonous. Huge mountains rise 10,000 feet above the seafloor and create escarpments, ridges and valleys that would rival the peaks of the Rocky Mountains. It is along these mountains that we hope to find sediment for our research.\nUsing scientific instruments we peer ‘through the looking glass’ to learn what the seafloor and sediments look like. The analogy to the looking glass is apt: Alice stepped through the mirror to see the world beyond and we peer through the bottom of the ocean to see what is below. However, unlike Alice, we use our ears. Short pulses of sound from the ship are focused on the seafloor and we listen to the echo and reverberations that return to the ship. Depending upon the pitch and intensity of the sound we can look at the top layer of the sediment or much deeper.\nThe most basic echo we listen to comes from the very top of the sediments. This echo travels down through ocean, bounces off the top of the sediments and returns back to the ship. We measure the time it takes to go down and come back up, and knowing how fast sound travels through seawater (~one mile per second or 3,400 miles per hour!) we can determine the distance to the bottom of the ocean. The times are very short, about two seconds for water a mile deep. We use these distances to construct a detailed map of the bottom of the ocean. This map shows the mountains and valleys on the seafloor where we will take our sediment samples. We also listen to how loud the echo is when it comes back to the ship. Hard surfaces like rock have a loud echo while soft sediment gives a quiet echo. This is an additional way to determine where there are ocean sediments to sample.\nIf we turn up the sound volume and use a lower pitch we can look beyond the seafloor into the sediments below. Now rather than just one echo from the seafloor, we begin to hear many echos as sound reflects off the different layers in the sediments. These echos allow us peer beneath the seafloor to know how thick the sediment is and whether it is nicely layered or jumbled and distorted.\nWhen we find the right sediments—not too deep, smooth, with nice layers—we will take cores of the sediment to study the climate history preserved in the layers.']	['<urn:uuid:62c5ce2f-9028-4e46-9508-c9c27158c5a7>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	6	27	527
56	How does money influence joy, and which brain chemicals control happiness?	Research shows that money does matter for happiness up to a point - we need enough to live well, but additional wealth beyond that doesn't significantly increase happiness. Specifically, experiences like holidays bring more happiness than buying material things. As for the biological basis, four key 'feel-good hormones' control happiness: serotonin, which produces feelings of joy and ease; dopamine, which creates pleasure through the brain's reward system; oxytocin, known as the 'love hormone'; and endorphins, which naturally ease pain and create well-being. These hormones are produced in different parts of the brain like the brainstem, hypothalamus, and pituitary gland.	"['What is happiness and how can we get more of it? For many years scientists have been trying to define the joy and well-being of happiness, how people come to experience it and how to help them get a bit more of it in their lives.\nWhat we do know from research is that happiness is infectious. We\'re all more likely to be happier if we have a happy friend nearby.\nAnd we know that money does matter. We need enough of it to live well - that makes us happier than not having money. But after a point, more money doesn\'t make a lot of difference.\nResearch in the journal Psychological Science shows that money spent on doing, rather than by buying things, works. Apparently an experience, such as a holiday or going skydiving, brings more happiness than a good shopping spree.\nThere have been many attempts to judge the happiness levels of countries and some have even suggested that this is a better measure of a nation\'s progress than looking at an economy through the lens of GDP (Gross Domestic Product).\nAustralia generally does well in the rankings. The World Happiness Report 2015 puts Australia in 10th place out of 158 countries.\nThe world happiness rankings are based on a range of metrics including life expectancy, social support, freedom to make choices and generosity of the general population. There are many others trying to measure well-being including the UN\'s Human Development Index, the Happy Planet Index, the Legatum Prosperity Index and the Gallup/Healthways Well-Being Index.\nNow researchers have, using data from a major study, determined that happiness depends on whether things are better or worse than expected.\nRobb Rutledge and colleagues, of the Max Planck University College London Centre for Computational Psychiatry and Ageing Research, designed a study looking at the relationship between rewards and happiness.\n""Based on the data, we developed a mathematical equation to predict how self-reported happiness depends on past events,"" says Rutledge, writing about it for the OECD. ""We found that happiness depends not on how well things are going, but whether things are going better or worse than expected.""\nFor those who like mathematics, here\'s the formula the researchers came up with for happiness:\nThe formula explained in brief:\n""Happiness depends on safe choices (certain rewards, CR), expectations associated with risky choices (expected value, EV), and whether the outcomes of risky choices were better or worse than expected. This final variable is called a reward prediction error (RPE), the difference between the experienced outcome and the expectation.""\nTo get a detailed explanation go HERE.\nThis all means, according to Rutledge, that your happiness should increase in anticipation as soon as you make a plan to meet a friend.\n""If you manage to get a last-minute reservation at a popular new restaurant, your happiness might increase even more,"" he says.\n""If the meal is good, but not quite as good as expected, your happiness should actually decrease.""\nThe study shows how important expectations are but this doesn\'t mean low expectations (and being pleasantly surprised when events exceed your expectations) are a key to happiness.\n""We often don\'t know the outcome of major life decisions for a long time, whether taking a new job or getting married, but our results suggest that positive expectations about those decisions will increase happiness,"" he says.\nThe equation has been used to predict the happiness of more than 18,000 people round the world playing a game on a smartphone app called the Great Brain Experiment.\nThe free app asks: ""What makes me happy?"" Players then choose between safe and risky options to win as many points as they can.\nThis article was originally published by Business Insider.\nMore from Business Insider:', 'Feel-Good Hormones - They Impact Your Mood, Mind, & Body\nYour body\'s chemical messengers, hormones, are secreted by glands and flow throughout your bloodstream. These hormones then target the various tissues and organs within your body, regulating everything from your body\'s functions to how you feel each day.\nHormones are broken down into subcategories, one of which is known as ""feel-good hormones"" due to their ability to produce happy and occasional euphoric feelings. Four feel-good hormones exist - serotonin, Dopamine, Oxytocin, and endorphins.\nYou experience the effects of serotonin when you feel joyful and generally at ease regarding life. Serotonin is the critical element for improved moods and various other functions. Not only can serotonin fight off depression, but it may also induce euphoric feelings.\nWhere does Serotonin production occur?\nSerotonin is produced in a centralized region of the brainstem, which then takes effect on various other areas of the brain that influence a multitude of functions and behaviors, such as:\n- The stress response\n- Body temperature\nExperiencing pleasure as part of the reward system within the brain is directly attributed to Dopamine. A Dopamine release, or ""rush,"" can be triggered by various scenarios - exercising, shopping sprees, and indulging in junk food.\nDopamine also functions in parallel with reinforcement. That\'s why it\'s hard to resist further temptation when eating just one donut or buying a product on sale. However, Dopamine is also rooted in a much darker truth; it is the overwhelming reward people experience during substance abuse such as meth or heroin, leading them to addiction.\nDopamine is also crucial in the function of:\n- Heart rate\n- Learning and attention\n- Blood vessel function\n- Kidney function\n- Pain processing\nWhere does Dopamine production occur?\nDopamine production takes place in a two-step process thanks to neurons within the base of the brain. First, tyrosine, an amino acid, is converted into a secondary amino acid called L-dopa. Enzymes then convert L-dopa into Dopamine.\nThe primary role of Oxytocin is to facilitate childbirth, a key reason why it\'s nicknamed the ""love drug"" or ""love hormone."" Not only does it stimulate uterus muscle contraction, it further increases these contractions by encouraging prostaglandin production. As a result, women experiencing slow labor may be given Oxytocin for a quicker delivery. In addition, after a baby is born, Oxytocin assists in moving milk from breast ducts to the nipple, fostering a mother and baby bond.\nWhere does Oxytocin production occur?\nThe hypothalamus produces Oxytocin, which the pituitary gland then releases into the bloodstream.\nThanks to Endorphins, our bodies are capable of naturally easing pain. These hormones\' names derive from the term ""endogenous morphine."" Endogenous"" refers to their production within our bodies. Morphine is an opiate painkiller whose actions have similar effects.\nThe most prominently studied Endorphin is beta-endorphin, linked to the ""runner\'s high."" Endorphins are also released during laughter, physical intimacy, and delicious food.\nWhere does Endorphin production occur?\nThe hypothalamus and pituitary gland release Endorphins as a response to physical pain or stress. Pain relief and an overall sense of well-being happen from this group of peptide hormones.']"	['<urn:uuid:8f5dc7bc-f54d-4561-aab1-305baad538a1>', '<urn:uuid:b5648304-3074-4931-9c0d-b62d01f6d4ee>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	11	99	1139
57	Which reaction involves IgE: anaphylaxis or type 2 hypersensitivity?	Anaphylaxis involves IgE-mediated reactions through crosslinking of IgE and activation of mast cells and basophils, while type 2 hypersensitivity is a cytotoxic reaction mediated by IgM and IgG antibodies that causes cell death through membrane lysis and macrophage action. Therefore, IgE is involved in anaphylaxis but not in type 2 hypersensitivity.	['Presentation provides basic information on the HLA-complex and transplantation immunology, with focus on kidney and haematopoetic stem cell transplantations. Most important HLA-typing methods are explained as well as other techniques used in donor-recipient pair selection.\nSerological methods are basic diagnostic methods used to identify antibodies and antigens in patient sample (serum, plasma). Agglutination and precipitation belong to classical serological methods that are used in diagnosis of infectious diseases (antibodies screening), serotyping of microorganisms and human blood group typing. Agglutination is based on reaction of particular (insoluble) antigen with antibodies, whereas precipitation involves reaction of colloidal (soluble) antigen and antibodies. In the immunodiagnostics an agglutination method as Coombs test is used to determine anti Rh antibodies in pregnant women (diagnosis of hemolytic disease of newborn). Another agglutination method involves latex agglutination that is used to determine, i.e. rheumatoid factor (diagnosis of autoimmune diseases) and hemagglutination method that can be used in, i.e. antibodies screening against Treponema pallidum, the main cause of syphilis. Radial immunodiffusion assay belongs to the precipitation method. This quantitative assay is based on immunodiffusion of antigens and antibodies in the gel and can be used in level analysis of complement components (C3, C4), inflammation proteins (CRP) and immunoglobulins (IgG, IgM and IgA). Modern precipitation methods performed in fluid involve nephelometry and turbidimetry.\nA principal biological role of the immune system is an eradication of both external as well internal violators of integrity of the organism. External „enemies“ are represented mainly by germs; those of internal origin belong especially to potentially malignant cells that appear in our organisms as the results of a breakdown of their replication mechanisms.Under certain circumstances, however, the immune response can have deleterious effects, resulting in significant tissue damage or even death. This inappropriate immune response is termed hypersensitivity. Although the word hypersensitivity implies an increased response, the response is not always heightened but may, instead, be an inappropriate immune response to an antigen.\nSeveral forms of hypersensitive reaction can be distinguished, reflecting differences in the effector molecules generated in the course of the reaction. In immediate hypersensitive reactions different antibody isotypes induce different immune effector molecules. IgE antibodies, for example, induce mast cell degranulation with release of histamine and other biologically active molecules. IgG and IgM antibodies, on the other hand, induce hypersensitive reactions by activating complement. The effector molecules in these reactions are the membrane-attack complex and such complement split products as C3a, C4a and C5a. In delayed-type hypersensitivity reactions, the effector molecules are various cytokines secreted by T helper cells and macrophages. As it became clear that different immune mechanisms can give rise to hypersensitive reactions, P. G. H. Gell and R. R. A. Coombs proposed a classification scheme in which hypersensitive reactions are divided into four types, I, II, III, and IV, each involving distinct mechanisms; later type V was added. Antibodies mediate four types of hypersensitive reactions: IgE-mediated (type I), cytotoxic (type II), immune complex (type III), and stimulatory/inhibitory (type V) hypersensitivity, respectively. T cells initiate the last type of hypersensitivity (type IV) and clinical symptoms appear more days after exposure; it is therefore referred to as delayed-type hypersensitivity (DTH). However, a great deal of com¬plexity exists within each type of reactions that blurs the boundaries between them.\nThe principal role of the immune system is to protect the organism from principally two the most dangerous events potentially threatening our life, i.e. infection and malignancy. However, sometimes the immune system instead of reacting against foreign and aberrant self-antigens can attack self-molecules. This inappropriate response of the immune system against self-components is termed autoimmunity.\nThere are 70 - 80 autoimmune disorders known till now and app. 5% of Caucasoid population suffers from them. Our understanding of autoimmunity has improved greatly during the last two decades, mainly because of the development of a variety of animal models of these diseases and the identification of genes that may predispose to autoimmunity. Nevertheless, the aetiology of most human autoimmune diseases remains still obscure.\nThe term “autoimmunity” is often erroneously used for a disease in which immune reactions accompany tissue injury; they are “a by-product” of a release of self-antigens to circulation without causing any damage; moreover, these “autoimmune reactions” help to degrade them and to remove them from the body.\nA success of pregnancy depends on a proper implantation and induction of immune tolerance. The immune system secures it by various mechanisms – special cells, cytokines, HLA molecules, peripheral tolerance take part in.\nThe immune system of the newborn has also its own specifics as it matures relatively long time till it reaches the same protective ability as characteristic for adults.\nThe lecture deals with primary and secondary immunodefeciencies. It gives an overview on general clinical manifestations and their divisions according to the type of the immune functions defects. Must of the lecture devotes to AIDS.\nType I hypersensitivity belongs to the most common disorder mediated by immune reactions; it affects app. more than 30% of all individuals in Caucasoid population. Type I hypersensitivity is commonly called allergy. It is characterised by rapid onset (hence the term immediate hypersensitivity), within minutes of antigen challenge, and results in conspicuous clinical symptoms.\nIncidence of toxocarosis in Slovakia 1977-1990 showed an exponential increase with an accessory cycling, having 7-8 years period length.\nAnnual incidence rates of 3157 cases of leptospiroses recorded within 1949 and 2004, caused by the main serovars ificant or borderly significant periodicities, resembling those known from solar and geomagnetic activity, were identified. By comparing them with the time course of Wolf numbers, an almost exact reciprocity of maxima and minima of Wolf numbers and leptospiroses was disclosed. It is hypothetized that the epizoonotic character of these diseases supports the putative dependence of its manifestation upon the nature including the sun. Surprisingly, no similar results were found, so far, in the world scientific registers.\nRussian scientist Tchijevskij investigated almost 100 years ago (suggested for Nobel prize) relationships between solar activity, cycling with the period around 11 years, and the status of mankind. He found „social excitation“, as revolutions and wars, at the time of solar maxima and peaceful activities like flourishing of arts at minima.The present paper explores whether such a parallelism applies also to the 500 years´ (semimillenial) cycling of solar activity. The present author obtained, using the cosinor analysis, significant positive anwer. It is hypothesized that one of the responsible factors could be solar impact on geomagnetic field.', 'Allergy: What is it?\nAllergy is a hypersensitivity (hyperreactivity) of the body’s immune system against a xenobiotic substance that can act as an allergen or antigen. This type of reaction by our immune system is undesirable since it predisposes the affected tissue to damage.\nThe allergic response is mediated by different mechanisms that may be due to an antigen-antibody reaction, or cell-mediated. The predisposition of an individual to different allergens is not predictable; hence, antiallergic drugs become essential. The drugs are used to decrease the reactivity of the cells to allergens.\nIf not taken proper care, the allergic response may become fatal.\n- How do they work?\nThe reaction of cells to an allergen can be classified into four broad categories –\n- Type 1 reaction – this allergic reaction is the IgE antibody-mediated. The response is immediate (within seconds – minutes). IgE sensitizes basophils in the peripheral blood or mast cells and causes them to produce anaphylactic mediators like histamine, serotonin, intestinal peptides, prostaglandins, leukotrienes upon degranulation. These mediators get released in high amounts that show the pharmacological effect.\n- Type 2 reaction – this reaction is cytotoxic, i.e., causes cell death. The result of such a response is tissue damage due to the binding of antibody (IgM, IgG) to the antigen. The damage is caused by cell membrane lysis, phagocytic action of macrophages on the target cell, and direct damage to the affected cell.\n- Type 3 reaction – this reaction results from the antigen-antibody complex, causing an immune complex formation. The complement system gets activated, and complex-mediated tissue injury by exogenous antigens (microbes), or endogenous antigens (blood components).\n- Type 4 reaction – it is a cell-mediated reaction, involving T cells, antigens, and macrophages. The macrophages present an antigen to T cells, by which cell-mediated immune memory develops. Upon subsequent exposure, T cells cause direct cytotoxicity, lymphokine, and inflammatory mediator recruitment.\nThe drugs, therefore, are used to inhibit these types of responses. The treatment initially begins with allergen elimination or avoidance, followed by drug interventions to prevent mediators’ exaggerated release.\n- Different classes of these drugs\nThe major types of antiallergic drugs are –\n- Antihistaminic – dimedrol, diprazine, loratadine, cetirizine, levocetirizine, fexofenadine\n- Mast cell stabilizers – cromolyn, nedocromil, lodoxamide, pemirolast\n- NSAIDs – ketorolac, flurbiprofen, tromethamine\n- Corticosteroids – fluoromethanol, betamethasone, hydrocortisone, prednisolone\nDrugs used to relieve generalized symptoms of immediate allergic response –\n- Adrenomimetics – adrenaline\n- Methylxanthines – euphylline, theophylline\n- Ca+2 preparations – calcium chloride, calcium gluconate\n- Indications in which they are preferred\n- Antihistaminic used in histamine-mediated allergic reactions – allergic rhinitis, allergic conjunctivitis\n- Mast cell stabilizers used in vernal keratoconjunctivitis, keratitis, allergic keratoconjunctivitis, and giant papillary conjunctivitis, itching\n- NSAIDs are used to inhibit inflammatory response mediated by prostaglandins and in seasonal allergic reactions\n- Corticosteroids are used in extreme allergic reactions, to suppress the overall immune response and inflammatory response throughout the body.\n- Combinations of drugs used to treat drug-mediated allergic response (e.g., penicillin)\n- Side effects\n- Antihistaminic – sedation, peripheral anti-secretory effects, CNS dysregulation, palpitations\n- Mast cell stabilizers – burning and stinging pain at the site of administration, watery, puffy eyes (ophthalmic preparations)\n- NSAID – transient stinging and burning sensation, allergic reaction & keratitis in rare conditions\n- Corticosteroids – overall decrease in immune response, secondary infection, anti-prostaglandin effects\n- Adrenomimetics – increased sympathetic effects\n- Methylxanthines – hypotension, arrhythmias, tremors, insomnia, GI disturbances\n- Ca+2 preparations – bradycardia, vasodilation, GI disturbance, hypercalcemia, arrhythmias\nv Antihistaminic – Sensitivity to specific antiallergic drugs, pregnancy, and lactation\nv Mast cell stabilizers – sensitivity of the patient to drug, and formulation\nv NSAIDs – sensitivity to NSAID, corneal epithelial defect (ophthalmic preparation), pregnancy\nv Corticosteroids – pregnancy, immunodeficient patients\nv Adrenomimetics – pheochromocytoma, patients on sympathomimetic therapy\nv Methylxanthines – sensitivity to specific drugs\nv Ca+2 preparations – patients with the risk of cardiovascular events\nDrugs used in anaphylaxis\n- What are they\nAnaphylaxis can be explained as a severe allergic reaction that has a rapid onset and may be fatal. The hyperactivation of mast cells and basophils in the blood involved crosslinking IgE and aggregation of other mediators. Activation of mast cells or basophils rapidly respond and release preformed mediators like histamine, peptides, and proteoglycans. This is further accompanied by phospholipase A2 activation, followed by prostaglandins, leukotrienes, and PAF (platelet-activating factor).\nA release of inflammatory cytokine and chemokines is also mediated that include tumor necrosis factor (TNF), interleukins. Many of these mediators are thus believed to be responsible for the pathophysiology of anaphylaxis. Anaphylaxis can thus be a response to multiple mechanisms of allergic responses. The drugs used to suppress such an exaggerated body response are known as anaphylactic drugs.\n- How do they work\nSeveral mediators have variable effects on different organs that precipitate serious implications. The table below comprises the key mediators and their multiple effects on organs.\nTable 1 – Mediators and their physiological effect\n|Chemical mediator||Physiological effect|\n|Histamine||Vasodilation, increase in vascular permeability, tachycardia, increased cardiac contractility, hypersecretion|\n|Prostaglandin D2||Bronchial, pulmonary, and coronary vasoconstriction, peripheral vasodilator|\n|Leukotrienes||Bronchoconstriction, increased vascular permeability, airway remodeling|\n|Platelet-activating factors||Potent bronchoconstriction and increased vascular permeability|\n|Tumor necrosis factor (TNF)||Neutrophils activation, chemokine synthesis, recruitment of other immune cells|\nThe drugs that are used for management, either act on the chemical mediator release, or they are used to relieve the physiological effects elicited by these mediators.\n- Different classes of these drugs\nSince anaphylaxis is a life-threatening condition, the management is focused on emergency stabilization, followed by a combination of drug therapy.\nv Emergency management – Airway, breathing, and circulation resuscitation to ensure the survival\nv Vital stats monitoring – oxygen administration with cardiac monitoring and oxygen saturation.\nv Airway management: in case of severe respiratory distress, tracheal intubation should be immediately performed. Investigation for angioedema and following fluid removal should be done immediately to reduce the chance of respiratory failure.\nv Decontamination: the patient should be physically examined for any causative agent, and removed if found, followed by removal of the endogenous agents by chelation or mechanical removal.\nDrug of choice – Epinephrine – 0.3 to 0.5 mg (0.3 to 0.5 mL of the 1:1000 dilutions) I.M. repeated every 5 to 10 minutes as per response/relapse.\nAfter controlling the emergency crisis successfully, a drug regimen specific for anti anaphylaxis action is used.\nSecond-line therapy –\nv Corticosteroids – methylprednisolone, hydrocortisone, prednisolone\nv Antihistaminic – cimetidine, ranitidine, diphenhydramine\nv Anti-asthmatic medications –albuterol, ipratropium bromide, magnesium sulfate\nv Glucagon – for patients on β blockers, to manage refractory hypotension\n- Indications in which they are preferred?\nBefore the management therapy is initiated, a patient needs to be examined for proper diagnostic criteria. The investigation for clinical criteria is essential since the treatment protocol includes heavy doses of drugs, which may precipitate further complications.\nClinical criteria for anaphylaxis include –\n- Acute onset of illness lasting minutes to several hours, affecting the skin and/or mucosal tissue (pruritus hives/urticaria, flushing, edema). The onset shows associations with one or more than one of the conditions – i) Respiratory insufficiency – breathlessness, difficulty breathing, wheezing ii) Hypotension\n- Symptoms of organ dysfunction – loss of body control, pain, hypotonia\n- If a patient shows signs after exposure to an antigen (minutes-hours) as – skin, mucosal tissue reaction, respiratory compromise, fall in blood pressure with associated symptoms and GI symptoms of abdominal cramps, and vomiting.\n- Patients get exposed to a known allergen and develop hypotension rapidly.\nThe use of treatment is indicated as soon as these symptoms develop. The early symptoms for anaphylaxis include –\n- cutaneous flushing\n- chest tightness\n- life-threatening laryngeal edema (symptom – hoarseness in voice)\n- Side Effects\nThe side effects of these medications are as same as when used as an antiallergic drug. The side effect of epinephrine includes tachycardia, arrhythmias, and tremors.\nSince anaphylaxis is a life-threatening condition, the treatment protocol cannot be contraindicated as it may result in morbidity.\nHowever, caution should be taken with the patients using adrenergic blockers since epinephrine administration may precipitate a severe hypertensive crisis, upon high adrenergic stimulation.']	['<urn:uuid:5a20d8e1-bc5d-4700-8e93-3c09ca8692ef>', '<urn:uuid:0cbe35c6-0112-4f63-99f2-fa872191987b>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T18:38:36.516973	9	51	2413
58	how evaluate software quality costs types	Software can be evaluated through hands-on testing, studying user reports, compatibility checks, and reading reviews. These evaluation methods are part of quality cost management, which has two main categories: conformance costs (including prevention and appraisal costs like project management and testing) and non-conformance costs (including internal and external failure costs like design rework and customer support).	['Software evaluation is the process of determining if a piece of software meets the needs of its users and whether or not it has any potential for improvements. It’s important to note that this evaluation should be done before purchasing software, as there are many things you can’t tell until you test it out.\nTable of Contents\nBenefits of Software Evaluation to a Business\nSoftware evaluations can help companies make more informed decisions about their buying habits and which applications they should invest in.\nFor example, if a company wants to improve customer service, but doesn’t know anything about what type of software would best fit those needs, then they could run an application through an evaluation process to find out how well it might work for them.\nSoftware evaluations also provide insights into problems with certain programs which may need updating or fixing before release. This allows companies to avoid slowdowns in their operations when they are using faulty software.\nBenefits of Software Evaluation to an Individual\nFor individuals, evaluations are important because they help people decide which programs are the best for their needs without having to spend a ton of money or waste time by using applications that contain bugs or aren’t designed well.\nIf an individual is looking for a specific type of program but doesn’t know where to find it, then he or she can read reviews to see if it’s something that’s worth buying, or just head over to an evaluation site and have all of his or her questions answered before making a purchase.\nWays to Evaluate Software\nThere are many ways in which software can be evaluated, including hands-on testing, studying reports from other users, checking compatibility across different programs, and reading reviews written by others.\nHands-on testing means that a product is actually used to see how well it works.\nThe idea behind this type of evaluation is that no matter how good reports, compatibility checks, and reviews sound, the only way to know if an application works well or not is to try it out yourself.\nThis can be done through trial versions, which allow for a very abbreviated version of the program to be tested out. Other ways of doing hands-on testing include purchasing a full license key and downloading the program directly from its website.\nStudying reports from other users involves looking through customer feedback and seeing what problems people report and whether or not they have suggestions on how the product could be improved upon in the future.\nCompatibility checks are an important part of this type of evaluation because they show whether or not the software is compatible with different operating systems, hardware devices, and other programs that are currently being used.\nFor example, if a company is working on iPads their employees use, but wants to switch to iPhones instead, compatibility checks will help them make sure the new software works well across both types of devices before making any changes.\nReading reviews written by others can be helpful because people share what specific things they do and don’t like about certain applications. This allows for companies to get a better idea of how useful certain programs maybe without having to spend money first just to see if it’s worth buying.\nReviews can also highlight problems that might arise when using certain types of software, which may need fixing before its full release.\nThere are many ways in which applications can be evaluated. Those who are curious about what types of evaluations are used to determine if a piece of software is worth buying should check out some online reviews or read through customer feedback.', 'Can you put a price-tag on developing, deploying, and maintaining high-quality computer systems in your organization? How about estimating the cost of poor-quality of your systems? In this post, we will explore the cost of quality of Laboratory Information Management Systems and related software for scientific data management.\nQuality costs have been studied since at least the 1950s. Before it was first formally characterized, people in business anecdotally understood that higher quality products and services meant higher costs, but also had the potential to lead to better sales and higher profits. Naturally, trade-offs must be made to ensure that an organization can control costs while being able to deliver products and services that meet the demands of their customers. The problem was there were no concrete ways to measure the cost differences between good and poor quality. Academics and accountants soon began developing models for measuring these costs based on empirical evidence from different industries.\nQuality Costs in Software\nCost of Quality accounting has been proven to be a useful practice in measuring the effectiveness of quality management systems for the manufacturing industry, and has adapted and modeled for Software development as well. However, it has not been widely incorporated into most Software Quality Assurance groups in the informatics industry. In this article, we will explore some of the common sources of quality costs in software and how proactive measures can be taken to reduce them.\nWhen examining Quality Costs, there are two general categories:\n- Conformance costs\n- Non-conformance costs\nConformance Costs, sometimes referred to as Achievement Costs or the Costs of Good Quality, are costs associated with maintaining good quality. Conformance Costs can be further categorized into appraisal and prevention costs. Appraisal costs are the costs associated with measuring, evaluating or auditing products or services to assure conformance to quality standards and performance requirements. Prevention costs are the costs of all activities specifically designed to prevent poor quality in products and services.\nThe table below shows examples of common appraisal and prevention costs in software quality.\n|Prevention Costs||Appraisal Costs|\n|Project Management||Unit Testing|\n|Requirements Management||Integration Testing|\n|Continuous Integration||External Audits|\n|Functional Testing||Quality Assurance|\nNon-conformance costs, sometimes described as the costs of poor quality, are costs associated with remediating the affects of poor quality. These costs have two general sources:\n- Internal failure costs\n- External failure costs\nThe table below shows examples of internal and external failure costs.\n|Internal Failure Costs||External Failure Costs|\n|Design Change Rework||Customer Support|\n|Defect Management||Warranty Rework/Repayment|\n|Requirements rework||Market Loss|\nExamples of internal and external costs of failure are obvious for any software development organization or department, but determining the best way to measure these non-conformance cost is not as apparent. Also, even if the costs can be meaningfully quantified, the more challenging problem is determining how to balance conformance costs with non-conformance costs. Anecdotal evidence tells us that it is usually easier to justify non-conformance costs to senior management, especially in young companies. It is human nature to not consider how to avoid an issue until after it has already affected you, and no business person wants to spend money on items that does not add value to the business.\nIt is the responsibility of the quality department in these organizations to clearly and effectively communicate the value of good quality, and how it is cheaper in the long run than bad quality.\nCost of quality program\nDeveloping a quality cost program is the best way to account for all costs of quality, and the best way to show the value of good quality to the organization. For the program to be effective, it is important to choose the most appropriate metrics to gather, and to present the results to senior management and other stakeholders in a thoughtful and engaging way.\nIf your organization manages quality software, consider how the costs of quality are accounted and managed.\n- Knox, Stephen T. Modeling the Cost of Software Quality\n- Ali Elbireer, MBA, MT(ASCP), CPHQ, Alicia R. Gable, MPH, J. Brooks Jackson, MD, MBA. Lab Medicine.com\n- Douglas C. Wood. Principles of Quality Costs, Fourth Edition. 2012\n- ASQ Quality Costs']	['<urn:uuid:9bd4ce04-c56d-43d0-beff-ddac4d5d64a7>', '<urn:uuid:a13044a3-7d4b-4e1d-8b1e-31604556c597>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	6	56	1293
59	How does the size of 3D printed objects relate to their complexity, and what challenges arise when trying to create both large and detailed items?	The relationship between size and complexity in 3D printing presents significant challenges. While 3D printers can handle either large objects or intricate objects, they typically cannot manage both simultaneously. Even the largest 3D printer in the world, with a 100-foot print bed, faces limitations with detailed work because larger printers require larger nozzles and thicker filaments, making it harder to achieve accuracy in fine details. For complex designs, particularly those involving overhanging areas or spiral shapes, the printing process becomes extremely challenging as each layer needs proper support to build upon. In the turbomachinery industry, this limitation means that while additive manufacturing can create complex shapes, the parts often require additional finishing processes. The cost and quality of finished products remain major concerns, particularly when it comes to surface finishes, which are usually too rough for primary flow path components and require conventional finishing methods.	['The turbomachinery and software industries have evolved, in tandem, to high levels of maturity. On the engineering side, improved modeling capability and access to more powerful computational resources have revolutionized the design process, reducing costs and significantly improving performance and reliability. On the manufacturing side, additive manufacturing is starting to open up exciting new possibilities by allowing component shapes that simply were not possible before.\nLike all technology, the evolution of these methods may seem clear enough for the near future, but it can be much more hazy over the long-term. Therefore, this article will take a look at what the future of turbomachinery software may hold.\nThe most advanced turbomachinery applications in the most performance-driven areas (i.e., propulsion and large scale energy production) have already reached the upper limits of what can be achieved by optimizing the primary flow path. For these industries, the most significant future improvements will come through upgrades to the secondary flow paths. This is particularly true in cooled gas turbine designs, where the extraction and re-injection of air has a major impact on cycle efficiency and local component performance.\nFigure 1 - Idealized representation of a high pressure turbine in AxCent®1 (left), and a more detailed figure of an actual turbine (right)\nMore complex shapes require more complex solution methods. The most obvious evolution of this in CFD and FEA has been the shift toward unstructured grids. As shapes become increasingly complex, they become progressively more difficult to capture with structured grid methods, whereas unstructured grids can effectively depict complexity. Figure 2 shows some of the grids that are possible today, in Pushbutton FEA™2 and in third-party CFD generators that can be imported from AxCent.\nFigure 2 - Unstructured girds in PushButton FEA and in CFD from a volute shape exported from AxCent.\nLower order solvers, such as meanline and streamline curvature, will also evolve to capture these more- complex shapes and flow paths. The components in the secondary flow paths can be parameterized, just like the primary flow path, and their performance can be modeled accordingly. Some complicated iterations are often necessary, in order to ensure an overall balance of mass and energy. Eventually, the calculations begin to resemble more of a system analysis, rather than just a component analysis.\nThe other technology on the horizon that has the potential to powerfully impact the turbomachinery industry is additive manufacturing (AM). In many respects, the impact of AM will be felt more immediately in the design realm than in manufacturing; in fact, most industry experts do not expect AM to replace traditional manufacturing methods anytime soon. This is because the cost and quality of finished products are still major concerns in AM, and it is unlikely those issues can be quickly resolved.\nTherefore, the primary benefit AM offers, at the present time, is the ability to make parts that simply cannot be made any other way. The flexibility of shapes made possible by AM opens up new design options that have never been considered before. AM’s potential is most significant in more complex, cooled parts, and some of these designs are already starting to enter the field.\nAM’s ability to generate complex lattice structures with very high strength-to-weight ratios is another immediate appeal of the technology. Figure 3 shows an idealized rotor with a blend of lattice and solid cross-sections; it also shows an actual part, taken from a missile nose cone, which illustrates a similar application. A design like this would have significantly lower weight and rotational inertia than a conventional design, and also would have very different vibrational characteristics. Techniques like laser metal sintering and electron beam melting are giving rise to new shapes and options such as these, in a surprising variety of metals—including stainless steel, titanium, and high temperature nickel alloys.\nFigure 3 - Example of a rotor generated with additive manufacturing using a combination of solid and lattice structures (left). An actual nose cone of a missile using a similar method (right) Courtesy of Pratt & Whitney Additive Manufacturing Innovation Center, Univ. Connecticut\nIn order for the full potential of AM to be realized, the software industry needs to provide the next generation of tools to take advantage of this new technology. Such software will include more complex geometry generation, to lay out the interlocking structures, and will also include improved modeling methods.\nMany of the more complex, smaller-scale structures possible in AM are too fine to be discretized directly using standard numerical methods, and must be solved as a macrostructure with the appropriate bulk properties defined. A lot of research is now being done to try to quantify these bulk properties for practical analysis. Residual stress levels in the parts produced, and defects in the structures that result from the more limited tolerances in AM, are major sources of uncertainty today. Surface finishes from AM-produced parts are also generally too rough for primary flow path components, and usually require finishing with a more conventional subtractive process.\nInevitably, advanced shaping will lead to advanced optimization processes. There are so many possibilities, and the potential is so open-ended, that it is hard to visualize exactly where AM might go in the future. Some tantalizing hints of the possibilities are starting emerge, and some of the designs are so unique and unconventional that they bear little resemblance to the items they are designed to replace. The potential of AM is limited as much by imagination as by technology. (One intriguing trend is the resemblance of these optimized designs to biological systems. For example, the lattice/solid structures often show a strong similarity to bone, and optimized flow passages resemble arteries and veins.)\nFigure 4 - Left—Fluid duct optimization using AM technology [courtesy of Ford Motor Company]; Right—Advanced bracket design [“Topology Optimization for Additive Manufacturing”, Brackett et al.; Solid Freeform Fabrication Symposium, 2015]\nIn summary, there are still many unresolved issues that need to be worked out in the next generation of turbomachinery modeling and manufacturing. A lot of work remains to be done before new methods can enter everyday service across the broader industry. But the potential for these methods is already starting to show up in the most demanding applications, and their presence will increasingly be felt in new areas. The ability to lay out, analyze, and optimize these new options in the next generation of software will be the key to their success.\n1 AxCent is a registered trademark of Concepts NREC, LLC\n2 Pushbutton FEA is a trademark of Concepts NREC, LLC', 'With all the parts, models, and designs available today, you’d think that 3D printers can pretty much do anything. After all, they’re being used by everyone, from hobbyists to prosthetics experts. Still, there are some things that just don’t work – well, not yet, anyway.\nHere are 7 things that cannot be 3D printed:\n- Perfect spheres without supports\n- Flammable natural materials\n- Hard natural materials, such as stone\n- Certain metals, including gold and silver\n- Large and intricate objects\n- Complicated designs\n- Completely finished products\nLet’s get into the details and talk about why these materials and shapes aren’t always compatible with 3D printing. I’ll also discuss the exceptions to these rules and tell you about some of the newest ways engineers work around the limitations of 3D printing.\n1. Perfect Spheres Without Supports\nSpheres and perfectly round objects are among the most challenging things to 3D print. This is because, as with every design and model, there must be a base onto which you print.\nIn most cases, there is a flat base that filament is built upon. Without it, anything round would roll away, interrupting the print.\nThat said, it is possible, provided you use some kind of support. Unfortunately, these supports will leave residue behind, which needs to be sanded away once the print is finished.\nCheck out this video to see how it can be done:\nSo, if you want to print a sphere, you will need to ensure that you use proper support structures and prepare for quite a bit of sanding and processing to even out the shape.\nAs of right now, it doesn’t seem like this problem with spheres will ever go away since you will almost certainly need a print bed to print anything, and to keep a globe on the print bed, you will need something flat to stabilize it.\n2. Flammable Natural Materials\n3D printers use heat to melt down materials and shape them into objects. As a result, they cannot print using materials that don’t tolerate heat.\nThese materials include:\n- Organic textile fibers\nStill, engineers have developed some filaments that contain small amounts of these organic materials. Although they will never compare to the real thing, they often look similar to the original material.\nWood filament usually includes about 30% wood particles (sawdust), an exception to the rule of 3D printing with flammable materials. However, this wood usually burns a bit during the printing process.\nThe wood particles are also always mixed with another filament such as PLA, making it less durable and lighter-feeling than natural wood. Wood filaments also usually contain something like cork, which is fire-retardant, to keep your printer from going up in flames.\nFabrics, too, are possible to 3D print, but materials such as cotton, hemp, and linen are far too flammable to be used.\nInstead, textile printing experimenters use materials such as flexible TPU and TPE.\nStill, this intelligent option for fabric production hasn’t made it off the runway yet. That’s because the processing time and materials necessary to print these designs aren’t any better than the traditional processes used to make cotton and other natural fabrics.\nIn addition, most 3D printed fabrics aren’t tightly knit, which usually leaves TPU and TPE fabrics looking like a holey fishnet. So, right now, making clothing and other fabrics with 3D printing methods isn’t possible.\nPaper is far too flammable to print like wood and other natural fibers.\nSo, 3D printing is not the way to go for natural materials. Although 3D printing can make convincing dupes to materials such as wood, fabric, and paper, it can’t manufacture these items efficiently or competently. As a result, the finished product doesn’t often compare to the original.\n3. Hard Natural Materials, Such As Stone\nLike the other natural materials on this list, stone is impossible to 3D print. Rock is far too hard and heat-resistant to turn into a filament, so, by and large, you can’t use it in 3D designs.\nHowever, there are two workarounds:\nStone-filled filament usually contains PLA and stone powder, making your models look a bit like concrete. These are generally made with softer stone varieties such as:\n- Terra cotta\nThese filaments are much like wood ones because they only contain small quantities of natural materials, and the end products still have a plastic-like feel. Likewise, the durability of stone-filled filament prints does not compare to raw, pure stone.\nStone-mimic filaments don’t contain any stone particles, but they can give your 3D prints a stone look. Such prints are not fireproof, which makes 3D printing with stone-inclusive and stone-look filaments more of an aesthetic choice than anything.\nAs of right now, it is frankly impossible to 3D print something as durable and heat-resistant as stone, and it will likely be a very long time before we can make convincing stone objects with fire resistance and durability using a 3D printer.\n4. Certain Metals, Including Gold and Silver\nWhile 3D printers recently added metal filaments such as aluminum, stainless steel, cobalt, titanium, and nickel alloys to their repertoire, you can’t 3D print with all metals. Instead, you will have to use metal clay or metal powders.\nMetal clay is just what it sounds like – clay with metal powders incorporated into it. Metal powders are usually pure metal fragments.\nPrinting With Metal Filament\nWhen you make a metal 3D print, a specialized printer will deposit a layer of hot metal powder or clay onto the print bed. Then, it will use a laser to heat and melt the metal.\nIn another printing process called metal binder jetting, the 3D printer deposits chemicals onto the metal powder or clay, which causes a chemical reaction that bonds and hardens the metal to itself.\nMetals That Won’t Print\nHowever, not all metals are compatible with 3D printing. According to metal manufacturing experts at Equisphere, when 3D printing metal objects, you need “a metal with high fluidity, high distribution density, low moisture absorption and high chemical stability for when the laser heats up.”\nBecause of these limitations, it is difficult and often impossible to print with materials such as gold, silver, tungsten, molybdenum, and several other common metals.\n3D printed metals also often have issues with structural integrity that make them unsuitable for use in heavy-duty machinery such as manufacturing equipment, automobiles, and airplanes. In addition, metal 3D printed parts usually have high porosity, making them prone to crumbling, cracking, and warping.\nHowever, you can likely expect metal 3D printing to get much more advanced in the coming years since it is a relatively new invention in itself, with the first metal 3D printer coming out in 1994.\n5. Large and Intricate Objects\n3D printers are small, and even the largest of them cannot tackle a project like an extensive and intricate car or a big piece of machinery.\nWhile they can print large or complex objects, they cannot usually do both.\nLarge Printers Need Larger Nozzles\nFor example, let’s look at the largest 3D printer in the world, a $2.5 million printer at the University of Maine. This printer is massive, with a 100-foot (30.48 m) print bed that is suited for making items such as boats and architectural components. But it is not ideal for creating a working machine.\nA larger printer comes with a larger nozzle, and it will accordingly use thicker filaments, making tiny details harder to print with accuracy.\nPrinting a finished product, complete with fine-tuned mechanisms, is impossible right now. With a printer this immense, you could create the casing for a machine, but it’s almost impossible to ensure finer details.\nInstead, you’d need to use a large printer for bigger pieces and several smaller printers for the more intricate parts.\nHowever, the investment in machinery usually isn’t worth it. That’s because the processing time will be longer, the cost of materials will be higher, and the parts won’t be as durable as molded or cast parts.\nStill, with this vast 3D printer comes promise for a more advanced future in 3D printing, and one day, likely soon, we will see printers that can produce seamless, fully-functional machines.\n6. Complicated Designs\nAlthough some complicated designs, from small, humanistic 3D figurines to scale models of ancient artifacts, are possible to 3D print, not every design is feasible.\nIntricate Spirals Need More Structure\nFor example, I recently read this article, which discusses a specific design for drone propellers. Unfortunately, they found that the design was far too complicated for even professional 3D printing services to attempt.\nThis small, round, spiral-shaped propeller, conceived after Leonardo Davinci’s propeller designs from the renaissance, wasn’t anything special at first glance. It had smooth surfaces and a coiling seashell-shaped design. However, the perimeter’s width-to-height ratio, the spiraling shape, and the lack of scaffolding were too much for a 3D printer to produce accurately.\nThis model is a perfect example of the limitations of 3D printing. 3D printers often have difficulty printing round objects since each layer of the print builds upon the previous layer. Therefore, the printer will likely struggle to make the layers meet up properly when printing a spiraling tower.\nPieces With Overhanging Areas Often Won’t Print Properly\nIn addition, the overhanging edges of the print, which had no support towers or bridges, created areas where the 3D printer could not connect each perimeter. Without any scaffolding, infill, or support, the 3D printer would have failed. If anyone tried to make it, the print would have looked like a shapeless lump on the bottom of the print bed.\nIn this case, the drone enthusiast had to make a mold and cast the propeller themself, but they were able to use a 3D printed prototype, complete with infill and scaffolding, to make the mold.\nSo, even if something looks simple, that doesn’t mean that it will be possible to 3D print.\nSometimes, even the most miniature, smoothest design is too intricate for a 3D printer, especially when it comes to overhanging parts, spiral designs, and hollow interiors.\n7. Completely Finished Products\nWhen you 3D print a design, nine times out of ten, you’ll need to finish it by hand. This might include:\n- Sanding off layer lines\n- Waterproofing the object\n- Clipping off support structures\n- Removing rafts depending on your design and the desired use for your product\nPost-processing is something that goes hand-in-hand with 3D printing, and very few printers can create seamless, smooth, airtight models without some manual labor.\nSo, when you want to make something once and be done with it without refining your product, 3D printing isn’t your best option.\n3D Printing Isn’t Always Consistent\nIn addition, though 3D printing is ideal for small parts and homemade objects, when it comes to making reliable, high-quality prints every time, 3D printing isn’t as efficient as other methods.\nSince each part that you 3D print needs some processing before using it, printing large machines, cars, phones, laptops, and other completed items with one design is impossible with a 3D printer.\nFor example, if you printed a small gear mechanism with two gears and a crank handle straight from the printer, you wouldn’t be able to sand off the gears. Without sanding, the layer lines could interfere with the smoothness of your mechanism, making it challenging to turn and use.\nFor now, we will have to be satisfied printing small parts, smoothing them out, then assembling them later.\n3D Printing Isn’t Always Precise Enough for Manufacturing\nThis limitation to 3D printing significantly impacts manufacturing, where efficiency is everything. However, when you are printing small parts at home, it likely won’t stop you from making your own designs.\nIn a factory setting, finishing a 3D-printed object takes care and time. Unfortunately, in industrial applications, that care and time mean more money, which makes the roughness and unreliability of 3D printed objects undesirable.\nSo, in a factory setting, most manufacturers still use tried and true methods such as injection molding to ensure that their products have a seamless, uniform surface without any need for sanding or refinement.']	['<urn:uuid:e31500e0-b4e3-46a8-a29d-7fd3d0316382>', '<urn:uuid:4c4aafc1-7b0a-426f-b00d-9cd79159df1a>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	25	145	3097
60	I'm researching regional competitions - who originally backed the Kandahar races?	The Kandahar name comes from the original sponsor of the race, Chip Armstrong. The name of the shop was Chip's Kandahar in Cortland, NY. The race began in 1974 with about 80 racers, and was created as an opportunity for kids who did not make state teams to continue racing throughout the season.	['March 9th and 10th 2019\nOpportunities for Sponsorship\nPlease view the pdf below with information on sponsorship opportunities for our upcoming Kandahar Festival in 2019.\nIf you have any questions or are interested in sponsoring please see the form above and contact the information provided. Thank you!\nOur Kandahar Sponsors\nThe Hammerich Family\nVeterinary Medical Center of CNY\n4 Seasons Golf and Ski\nThe Great New York State Fair\nYMCA - Manlius\nSweet Frog Cicero\nBlue Cross Blue Shield\nPalace Commons Cafe\nSki Center Ltd.\nPalace Commons Cafe\nThe Smith Family\nBeak & Skiff Apple Cider\nThe Palace Theatre\nSeabreeze Amusement Park\nBull & Bear Roadhouse\nTrappers Pizza Pub\nHistory of the Kandahar Festival\nThis piece was originally created as part of a Ski Racing Magazine article through the combined efforts of David Wenn, NSA Headmaster and Head Coach at Greek Peak in the early ’70s, and Paul VanSlyke, owner of Twitchell Sportswear.\nKandahar began in 1974. It is estimated there were about 80 racers the first year. Now we have over 600 racers and families each year who participate state-wide. It was created as an opportunity for kids who did not make state teams to continue racing throughout the season. The Kandahar name comes from the original sponsor of the race, Chip Armstrong. The name of the shop was Chip’s Kandahar in Cortland, NY.\nSince the first year of this Jr III, IV, V race NYSSRA has sent a state team to Gunstock, NH for the Buddy Werner Championships – now called the Piche Invitational. Development level ski racers gather from all over the east to clinic together on Friday and race on Saturday in March. Fun is abundant at both our Kandahar Series and in Gunstock.\nFor years Kandahar was run at Greek Peak. These wonderful events were prior to sophisticated snow making equipment. No matter what, the Greek Peak Ski Club got this event off. The race has always been a two day event. Families from around the state became acquainted and all present pulled together to make it happen. This event has always embodied some of the best aspects of ski racing, including friendly, healthy competition in a fun, family atmosphere. The foundation of the original event was based on the idea of athlete development and outdoor fun. Kandahar continues to this day to promote the same purpose.\nKandahar was held at Greek Peak until about 1991 and then began to move around the state. It has been held at Brantling, Holimon, West Mtn, and Gore Mtn. Due to the huge success and participation of this event, the Kandahar became a regional festival event in 2002, with Kandahar Festivals being held in the eastern, central, and western regions of the state. The top finishers from each of the festival event then compete at a state-wide Kandahar Championship held at a different location each year.\nNote: The history of the Kandahar dates back to the early 1900’s. Sir Henry Lunn organized what can be called the first formal Alpine Downhill in Switzerland in 1911. At stake was the Kandahar Trophy (donated by Lord Roberts of Kandahar, India). Sir Henry’s son, Arnold is credited with inventing the modern slalom in 1922 in a competition at Murren, Switzerland as well as with championing the push to include alpine skiing in the Winter Olympics. After a combines event at the 1936 Winter Olympics, Alpine Skiing emerged in its own right in the next Winter Games in 1948 with downhill and slalom events.']	['<urn:uuid:b4bbc262-e470-4a36-a75d-343b1462e279>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	11	53	582
61	farming challenges solutions government technology investments worldwide	Current farming faces multiple challenges but is seeing solutions through technology investments globally. The UK government reports being largely self-sufficient in grains, producing over 90% of its wheat needs, though climate impacts have reduced yields. The government claims a 'highly resilient food supply chain' despite farmers struggling with rising costs of fertilizer, feed and diesel. Meanwhile, 98% of large agribusinesses are investing in AI technologies, with the global AI farming market expected to reach $3.62 billion by 2028. These investments are showing promising results - AI in plant breeding can reduce crop development cycle by 50%, while predictive analytics can improve farm yields by 30%. Additionally, 29% of surveyed farmers show interest in AI and robotics for real-time soil health insights, indicating growing acceptance of technological solutions to agricultural challenges.	['It seems a touch absurd to have focussed our January magazine’s cover story on food after so many people undoubtedly over-indulged during the festive break, but I suspect it will be one of the hottest topics of 2023.\nMost people buy their goods without giving much thought to how it got from the field to the freezer section of the supermarket. It is a process that has been hugely sanitised. When people do talk about food production the conversation tends to focus on climate change, and for good reason.\nA December 2021 UK government report concluded that domestic wheat yields fell by 40% in 2020 due to a combination of heavy rain followed by drought. That same report states that the UK is largely self-sufficient when it comes to producing grains, harvesting the equivalent of more than 100% of the oats and barley we consume. When it comes to wheat, we grow over 90% of what we use.\nBut a 40% reduction in yield suddenly makes us more dependent on imports. Anybody with a wheat allergy knows how ubiquitous an ingredient it is. If global production is unaffected, prices remain relatively stable. But what about when the global supply is affected?\nWhen Russian troops crossed the border into Ukraine on 24 February 2022, the invasion was the catalyst for a series of shocks that will impact on food production into 2023 and beyond.\nFew in the west would have been aware of quite how much food in the global supply chain is grown in that part of the world. But the war has ravaged huge swathes of Ukraine, with land bombarded and crops set ablaze. Agricultural workers responsible for planting and tending to the next harvest have been conscripted into the army, many tragically losing their lives.\nIn a bitterly ironic twist, 2022 was a bumper year for Russian wheat. While the post-invasion sanctions did not affect food exports, getting the crop out through the blockaded Black Sea ports has been problematic for both nations. And wheat is only one example, Ukraine is also one of the world’s biggest producers of sunflowers, which has put a considerable dent in the edible oil supply.\nWhile the instinct in the UK may be to take comfort in our levels of domestic production and alternatives such as rapeseed oil, the export issues impacting Russian goods goes above and beyond food stuffs.\nAlongside the dawning realisation of how much food Ukraine produces for the world came the equally startling insight about how reliant farmers are on the supply of fertilisers from Russia.\n“The effects of the Russia-Ukraine war may reverberate through the global fertiliser industry for years to come,” warns an S&P Global report. “Post-Covid supply-chain disruptions had already pushed fertiliser prices to cyclical highs in 2021. Russia’s invasion of Ukraine in February, and the sanctions and trade supply disruptions that followed, then pushed prices even higher.”\nIt adds: “Russia is a major producer of the three main types of fertilisers – nitrogen, phosphate and potash, and a major exporter of key raw materials for fertiliser production elsewhere in the world.”\nThe government claims the UK has “a highly resilient food supply chain”, but the NFU says energy-intensive crops such as tomatoes, cucumbers and pears will hit their lowest yield levels since records began in 1985. A pint of milk is meanwhile set to fall below the price of production and beef farmers are considering reducing the size of their herds.\nAccording to the union, the rising cost of fertiliser, feed and diesel are to blame.\nHot and dry conditions\nBut the war was not the first domino to fall, according to Barings investment manager James Govan, who winds the clock further back to the emergence of a regular oceanic and atmospheric phenomenon at the start of 2021.\n“La Niña basically created hot and dry weather in southern parts of Brazil and northern parts of Argentina,” says the co-manager of the Barings Global Agriculture Fund. “Brazil is the biggest soybean exporter in the world and the second biggest in corn, so that led to a disappointing harvest at the start of last year.\n“And we are still living through La Niña weather conditions, continuing through the start of 2023,”\nGovan adds. “It doesn’t impact us much here in the UK and Europe, but the effects are felt in places such as southeast Asia, Australia, South America and even the US.”\nAs a result of myriad weather and supply chain difficulties, “global grain inventories, excluding China, are at their lowest levels in more than a quarter of a century”.\n“Inventories are low, but we are not expecting food shortages, especially in this country,” says Govan.\n“We should have high grain and edible oil prices for at least two years. If you look at the futures curve on commodities it indicates at least two years of good pricing to get us back to more comfortable levels.”\nAnd that would require a minimum of back-to-back global bumper harvests, he says.\nAnother gremlin in the food-price inflation machine is when countries feel they have no choice but to take protectionist measures to dampen both local inflation and local unrest. Recent examples include Indonesia temporarily restricting the export of palm oil and India doing the same with rice.\n“Unfortunately, we have begun to see pockets of protectionism,” says Govan. “Indonesia is the biggest palm oil exporter in the world and the month-long restriction on exports had a material impact and increased volatility. The price of palm oil shot up on the back of the restrictions. After they were largely removed, that led to an oversupplied market for two or three months.”\nGovan adds: “So, yes, things are tight. It’s not super acute but prices will remain high. It’s not quite stacking tins in cupboards time, but inventories are lower than comfortable levels.”\nAmid a cost-of-living crisis, paying more for food is the last thing anyone needs. The latest CPI figures show a cooling in the inflation rate, which was 10.7% in the year to November 2022, down from 11.1% in the 12 months to October.\nThe largest downward contribution between October and November “came from transport, particularly motor fuels, with rising prices in restaurants, cafes and pubs making the largest, partially offsetting, upward contribution”, according to the Office for National Statistics (ONS).\nYear-on-year inflation rates: 2021 versus 2022\nBut, after housing and housing services, food and non-alcoholic beverages was the second-biggest component of CPI – increasing to 16.4% from 16.2% in October. In November 2022, food inflation hit its highest level since at least 1977, according to the ONS. The largest upward effect came from breads and cereals, partially offset by a small downward effect from fruit.\nIt was interesting, however, just how few industry commentators chose to mention food in the views they shared with the market. Among those who did was Axa Investment Managers’ economist Modupe Adegbembo, who wrote: “Food inflation also picked up again in November, marking 16 consecutive months of rises.”\n“Shopping for food is taking a bigger bite out of household budgets,” notes Myron Jobson, senior personal finance analyst at Interactive Investor. “Part of what is fuelling this are price jumps in everyday larder products, such as bread, which makes this type of inflation sticky because consumers are resigned to paying it as they form part of essential expenditure for many.”\nBut Melanie Baker, senior economist at Royal London Asset Management, says what caught the attention of industry commentators and economists was the surprise fall in inflation. “Food wasn’t part of that particular story, as it increased about 1% on the month, pretty similar to the increase we saw this time last year.”\nAnd it is that focus on annual rather than monthly change that is important, Baker says. “In that first year, you have really significant jumps and big increases in inflation. But after 12 months, even if pricing is still at that really high level, year-on-year inflation technically drops to zero.”\nShe is expecting inflation to come down “quite substantially” in 2023, but a large part of that will likely come from energy prices. “If we don’t see the same scale of price increases this year that we saw last year, that will really start bringing the overall level of inflation down.”\nBaker adds: “The big uncertainty for economists and central banks is what is going to happen to the core services that are more reflective of underlying domestic inflation, so things like wages.”\nInvesting in ‘better’ food\nCircling back to agriculture, Barings’ Govan expects the average price of grains and edible oil, “assuming the harvest is as expected”, will be lower in 2023 “because we are unlikely to get the same price jump from when Russia invaded Ukraine”.\nWhile punchy by historical standards, in light of low inventory levels, he predicts prices will not be as high as 2022.\nWhereas farmers in certain sectors, such as those mentioned above, are struggling with costs, Govan says the higher prices have benefited food producers more generally.\n“The US Department of Agriculture expects US farmer profitability, on an inflation adjusted basis, to be the highest since records began in 1929. There are always exceptions, and potentially some bad luck if the weather is not very good, but economically 2022 was a good year to be a farmer, particularly on the arable side.”\nAnd that is expected to continue in 2023.\nThese additional profits have also given farmers firepower to improve their machinery and deploy more modern technological advancements to the betterment of the produce and the land. As such, from a fund perspective, Govan says “the outlook is positive for the agriculture equity asset class”.\n“The technology side is getting better,” he adds. “We have more drought-resistant seeds, the ability to track the weather and determine optimum planting conditions, more targeted use of water, fertilisers and pesticides. There are lots of interesting and exciting technologies that make this sector more resilient and robust, and which can drive productivity.\n“Yes, weather patterns, like La Niña or El Niño, can certainly be challenging. But that means we need to keep investing in the technologies which help us to better withstand that.”\nThis article first appeared in the January edition of Portfolio Adviser Magazine', 'Demystifying the facades of traditional agriculture, artificial intelligence (AI) has made an indomitable mark in modern farming. As we delve deeper into the world of AI-enhanced agriculture, we sail across fascinating trends and impactful statistics, illuminating the dawn of a new revolution. This blog post aims to take you on a comprehensive exploration of AI in farming, driven by compelling data and intriguing statistics, revealing the significant transformation AI has brought about in the agricultural sector.\nThe Latest Ai In Farming Statistics Unveiled\nArtificial intelligence in the agriculture market is expected to grow at a CAGR of 27.2% from 2019 to 2026.\nA testament to the burgeoning dominance of artificial intelligence in farming, the exceptional growth rate – a Compound Annual Growth Rate (CAGR) of 27.2% from 2019 to 2026 – serves as a beacon, illuminating the trajectory of this technology, in our case, within the agriculture market. When contextualized within a blog post about AI in farming statistics, these figures light a fuse of expectation, igniting interest and sparking discussion. They strike a chord in the readers, hammering home the ever-increasing relevance, impact, and potential of integrating AI in agricultural practices, painting a future agriculturally prolific and sustainable. A rate as robust as 27.2% promises radical transformation, and accompanying this data point with evocative discussions and analyses in such blog posts can help audiences grasp the magnitude of this AI-led agricultural revolution.\nPrecision farming, powered by AI and IoT, could increase crop yields up to 60%.\nIn the symphony of modern agriculture, Precision farming, supercharged by AI and IoT themes, is like a soaring high note that tilts the balance towards a fertile future. Imagine this: an impressive leap in crop yields of up to 60%, a feat not confined to one-off experiments but a promising reality for fields worldwide. Interwoven in the narrative of AI in farming, this bit of data doodles a vivid, hopeful canvas. It empowers you to understand the potent role of technology, playing the dual tunes of boosting yield while possibly curbing environmental impact. As you meander through the winding lanes of agricultural stats, let this statistic be your guiding lodestar, illuminating the transformative power and potential of AI and IoT in farming.\nBy 2022, more than 75% of farms are predicted to use some form of AI in their operations.\nHighlighting the projected 75% adoption of artificial intelligence (AI) in farming operations by 2022 fascinatingly unveils a trend of digital revolution sweeping across the agricultural sector. It narrates a nuanced story about the fusion of technology with agronomy, indicating an increasing reliance on automated and data-driven processes to manage crop growth, animal health, and farm productivity. A future-oriented blog post on AI in farming statistics would acquire a dynamic perspective from this crucial statistic, amplifying the rapid strides of agricultural technology. Further, it emphasizes the pivotal role that technology plays in modern farming, thereby setting a robust context for discussions about the potential benefits, challenges, and implications of AI in agriculture.\nAI global farming market size is estimated to be worth USD 3.62 billion by 2028.\nTo comprehend the profound influence AI wields on the farming industry, take a closer look at the projected value of USD 3.62 billion for the global AI farming market by 2028. This figure not only signifies a significant financial stake, but also implies an escalating reliance and trust on AI applications in farming operations. The forecasted marketplace ascendancy serves as a solid testament to the fact that AI is not merely an ephemeral tech fad, but rather a seismic shift, capable of overhauling the farming sector globally, sketching a future where nature meets technology for sustainable harmony.\nBy using AI technologies, the agriculture sector could see productivity improvements of $7.6 billion annually.\nIllustrating the magnitude of AI’s impact in agriculture, an impressive annual productivity boost of $7.6 billion showcases the immense economic value of integrating AI technologies into farming operations. In the ever-evolving narrative of AI in farming statistics, this figure stands as a towering testament to the fusion of tradition and innovation. It reveals a transformative potential that’s not merely hypothetical, but quantifiable and substantial. Through the lens of this statistic, readers can visualize a future where AI and agriculture go hand in hand, taking multiple strides forward in productivity, efficiency, and overall yield. Making farming smarter, more predictable and profitable, the $7.6 billion potential increase offers a convincing argument that advocates the intersection of AI and farming.\nAs of 2020, 14% of growers in the United States use AI on their farms.\nUnveiling the surprising revelation from 2020: A slice of American greenery, precisely 14%, has already begun sowing the seeds of technological revolution in their farms in the form of Artificial Intelligence (AI). This pivotal fact not only underlines the growing adoption of AI technology in the agricultural sector but also hints towards the initial stages of a transformative wave, an agritechnical evolution of sorts. This sphere of percentage illuminates the scope as well as the pace at which this AI-driven farming phenomenon is making inroads into traditional farming methodologies. Picturing a future, where this percentage only grows, we can envision a tech-integrated agriculture landscape distinctly different from today. Moreover, the mention of this statistic implicitly challenges the other 86% of growers to foster innovations and tune into the rhythm of progress. These numbers certainly paint an intriguing portrait of where we stand today, and where, as a tech-infused agricultural society, we could be headed tomorrow in the intriguing world of AI and farming statistics.\nA study shows farmers adopting AI increase their crop yield by 10% and reduced the use of chemicals by 20%.\nDelving into the riveting world of Artificial Intelligence (AI) in agriculture, this study unfurls one of its most promising revelations. The crescendo in crop yield by 10%, discovered among farmers who welcome AI into their routines, narrates a story of revolutionary efficiency. Even more captivating is the significant decrease in chemical dependency by 20%.\nEnvision a future where abundance does not necessarily cash a check on our planet’s health. That’s the allure of AI in farming. Through smarter, data-driven strategies, it cultivates not only bountiful harvests but also nurtures a more sustainable and greener agrarian landscape. This transforms agriculture from a traditionally instinctive craft into a futuristic, precision-guided practice.\nSo, when we recount this statistic, we shed light on a double-edged triumph – bolstered output and conservation of our ecosystem, echoing the essence and promise of AI in agriculture. This transformative blend of technology and farming swells into a profound message. It illustrates how embracing change can guide us towards a sustainable path, reinforcing innovation, and the rural economy, and simultaneously aligning with environmental consciousness.\nMore than a quarter (29%) of the surveyed farmers indicated an interest in AI and robotics for “real-time soil health insights.”\nUnveiling a fresh perspective on the future of farming, this crucial 29% showcases the burgeoning intrigue farmers hold towards AI and robotics for real-time soil health insights. This number manifests the potential shift in the agriculture sector towards more technologically advanced measures – a transformation fueled by the promise of precise, real-time data about soil health. This growing interest underlines the optimistic openness of farmers to intertwining traditional practices with modern technology for potentially enhanced yield and efficiency. Hence, it paints a progressive picture for the future of agriculture, underpinned by the influence of AI and robotics, making this statistic a cornerstone of any discourse about AI in farming.\nPredictive analytics driven by AI can improve farm yield up to 30%.\nIn the realms of a blog post analyzing the revolutionary role of AI in farming, imagine this statistic as a digitized seed. Envision it sprouting into a tree of opportunity. The substantial ‘30% improvement in farm yield’ reflects not just numbers but the green shoots of growth, efficiency, and prosperity for our farmers. The statistic provides empirical proof that predictive analytics driven by AI isn’t merely a technological buzzword, but a game-changer that could revitalize agriculture. It can enhance productivity and sustainability, marking a paradigm shift in how we approach farming. Think of this statistic as the harbinger of a future where agriculture and AI intertwine, sprouting fields of possibilities and higher yields.\nBy 2025, the AI in agriculture market is projected to reach $4 billion.\nSuch a riveting projection of AI in agriculture market reaching $4 billion by 2025 underscores the colossal impact artificial intelligence is anticipated to have on the farming sector. Poised for such substantial growth, AI becomes a key player in redefining farming practices globally. This prediction not only demonstrates the rapidly growing acceptance of technology in traditionally hands-on sectors but also hints at the yet untapped potential of AI-driven innovations in maximizing efficiency and productivity in agriculture. In the grand scheme of our blog narrative on AI in farming statistics, this massive financial forecast is a testament to the pivotal role of AI technology in shaping the future of farming.\nAI in plant breeding can reduce the crop development cycle by 50%.\nDelving into the domain of AI in farming, a fact reiterates just how profound the impact of technology can be. The capacity of AI to slash the crop development cycle by an astounding 50% is a revelation. In the quest to feed an ever-growing global population, this statistic emerges like a beacon of hope. It illustrates the potential of technology-fortified farming to leapfrog the traditional drudgery, propelling us towards an era of faster, more efficient food production. This transformative impact on plant breeding heralds a radical change in agricultural productivity and sustainability, reverberating across the farming landscape. Imagine the surge in farm output, the time saved, and the consequent economic acceleration. Thus, this dramatic reduction in the crop development cycle, triggered by AI, underscores the seismic shift that technology is poised to inflict on traditional farming practices, reshaping the agro-sector’s future and squarely positioning AI as a potent tool in the arsenal of the 21st-century farmer.\n98% of large agribusinesses are investing in AI, according to a Microsoft Agri-Food Survey.\nAs we delve into the fascinating world of artificial intelligence and agriculture, a recent Microsoft Agri-Food survey provides a startling revelation. Almost like a testament to the radical shift in farm management strategies, the survey shows that a staggering 98% of large agribusinesses are actively investing in AI. This intriguing piece of information drives home the realization that we are not merely speculating about a potential AI revolution in farming. Instead, we are living it, with most big players already embracing this technology. Furthermore, it underscores the message that if this phenomenon hasn’t yet arrived at smaller farms and agribusinesses, it is only a matter of time before it does. AI is no longer a distant, futuristic concept but a reality altering the landscape of farming, as evidenced by this compelling statistic.\nIt is estimated that AI technologies will make an overall impact of $14.6tn on the world’s economy by 2035.\nGazing into the crystal ball of economic prediction illuminated by that staggering $14.6tn figure attributed to AI by 2035, one can’t help but wonder how this massive influence spills over into the agricultural sector. Indeed, ploughing this fertile intersection, between the world of artificial intelligence and the soil of our farms, unearths expansive potential for growth and refinement.\nJust imagine, for a moment, the future of farming in this landscape of AI-sculpted innovation—technologies that micro-manage crops for optimal growth or drones that autonomously patrol fields, diagnosing and treating pest invasions at their earliest stages. Such advancements could drastically increase crop yield, improve resource utilization, and reduce wastage, monuments of economic gain contributing to that astounding $14.6tn.\nThus, standing on the cusp of this tech-infused era of agriculture, this statistic not only signifies economic progress on a global scale, but also paints a picture of tremendous transformation within the agrarian sphere itself—an evolution unraveled through statistical stories that sprinkle seeds of inspiration onto this digital farming frontier.\nIn summary, the advanced evolution of AI in farming and its integration into the agricultural sector paints a promising future. Agriculture + AI, as supported by the stimulating statistics we’ve dissected, represents a powerful duo that’s revolutionizing farming operations around the globe. From streamlining operations, improving productivity, to ensuring sustainability, AI’s robust potential in farming is virtually limitless. These statistics, without a doubt, support a scenario where both the farmer and technology coexist, driving a revolution aimed at smarter, more efficient, and successful farming. To keep up-to-date, agriculturists should continuously embrace these emerging AI technologies. Future farming is indeed getting smarter and greener with AI.\n0. – https://www.emerj.com\n1. – https://www.www.agriculture.com\n2. – https://www.reportsanddata.com\n3. – https://www.medium.com\n4. – https://www.financesonline.com\n5. – https://www.www.alliedmarketresearch.com\n6. – https://www.www.grandviewresearch.com\n7. – https://www.www.springer.com\n8. – https://www.azure.microsoft.com\n9. – https://www.analyticsindiamag.com\n10. – https://www.www.marketsandmarkets.com\n11. – https://www.www.nature.com\n12. – https://www.www.datainnovation.org']	['<urn:uuid:4cb12e5c-88de-4e8c-93d2-3ece07caada0>', '<urn:uuid:fabf1c36-787f-4dce-907e-8e2ecad64a4a>']	open-ended	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-12T18:38:36.516973	7	130	3843
62	What makes the wood so important when it comes to storing and aging whiskey, and how does it affect the final product?	The wooden barrels, specifically made from oak, play a crucial role in whiskey aging. When whiskey is sealed in aged oak barrels for years, the fermented grain sugars interact with the wood, which gives the whiskey distinctive flavors and depth. The charring process of the barrel can also add various flavors to the stored whiskey, with the depth of charring affecting the final taste profile.	"['Much of the art in the taste of whiskey comes from the wooden barrels it is aged in. Sealed tight for years in aged oak planks, the fermented grain sugars take on distinctive flavours and depth. Preparation of your own barrels offers an at-home way to add a new flavour profile to already aged whiskey, or you can use the barrel for a variety of decorating and storage solutions from table tops to flower beds. Barrel making is an art however, and coopers spend years in training learning to craft the wooden containers.\nFind the oak by locating oak auctioneers or by asking a cooper. Oak approved for official barrel making is from trees aged between 50 and 180 years old. Cut logs for whiskey barrels are dried in a kiln.\nCut the logs into staves by hand by quartering them, sawing the quarters, and planning the quarters into long strips several inches wide and as tall as you desire for your barrel. Prepare between 24 and 36 staves, depending on how large you want your barrel to be.\nTaper the ends of the strips so once they are fitted together the barrel will be curved. This enables the barrel to be rolled along the ground.\nAssemble the staves within an assembly jig, or a large iron hoop. This process is tricky but the tapered ends of the staves together with the tension from the jig will keep the staves in place.\nSlip a slightly larger hoop, one of the four you have on hand, onto the barrel until it is 1/4 of the way down the barrel. This will be one of the hoops that holds the barrel together.\nPlace the two smallest hoops onto the staves at either ends.\nHammer the hoops and the ends of the staves into place to ensure proper tension before removing the assembly jig. This partially prepared barrel is called the rose.\nWet the surface of the staves with a wet sponge to prepare the barrel to be charred without catching fire.\nHold the barrel above a fire for a half hour so the heat rising into the barrel can seal the edges together. This charring process can add a variety of flavours to the whiskey stored in the barrels, depending on the depth of charring.\nShape an arch into the hot, moist staves with a winch. This tension action works on the malleable staves only when still heated. The winch is stored in the bottom hoop of the barrel and may need to be pounded in.\nPlace the larger hoops onto the middle of the barrel to hold the arched shape into place and hammer them in place with the mallet.\nEven out the edges of the staves with a saw.\nNotch a groove, called a ""croze"", into the inside ends of each stave. This will be where the lids rest.\nShape the lids out of a rot-proof material-traditionally river reeds with dowels but you can cut a wooden lid if you prefer.\nPlace the lids into the grooves that have been coated with a wheat flower paste for security.\nSmooth the surfaces with sandpaper and hammer all staves into place one more time.\nFill the barrel with a litre of hot water to test its integrity. Check for leaks by swishing the water along all sides and applying pressure to the staves.\nThis is an eight hour process for an experienced cooper. It will take longer if you are inexperienced. Take care using fire, saws and other machinery. Completed oak barrels are heavy and designed for rolling, not lifting.\nTips and warnings\n- This is an eight hour process for an experienced cooper. It will take longer if you are inexperienced.\n- Take care using fire, saws and other machinery.\n- Completed oak barrels are heavy and designed for rolling, not lifting.\nThings you need\n- Staves of aged wood, 24 to 36\n- Rubber or iron mallet\n- Iron hoops, 4 of gradual sizes\n- Assembly jig']"	['<urn:uuid:16b6c1c5-a1ec-4ba4-8c2f-a7448548f568>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	22	65	667
63	how airbus a220 and traditional fly by wire planes improve flight efficiency	Both systems enhance flight efficiency in different ways. Traditional fly-by-wire aircraft improve economy by being lighter due to eliminating mechanical flight-control mechanisms, requiring less space, and offering optimized trim settings for drag reduction. The A220 contributes to efficiency by being specifically designed for challenging environments like hot and high city airports, where other aircraft might have operational limitations, particularly in extreme temperatures that often restrict arrivals and departures.	"['Fly By Wire\nFly-by-wire is a system that replaces the conventional manual-mechanical flight controls of an aircraft with an electronic interface. A computer system is interposed between the pilots and the final actuators-surfaces. The term “Fly-By-Wire” implies a purely electrically-signalled control system..\nThe movements of flight controls(which commanded by Pilots) are converted to electronic signals and these signals are transmitted by wires to the flight control computer, and flight control computers determine how to move actuators at each control surface.\nFly by wire aircraft incorporate flight-envelope protection system into its flight control software(inside flight control computer), It is used in all modern commercial fly-by-wire aircraft. Flight envelope protection system prevents the pilot of an aircraft from making control commands that would force the aircraft to exceed its structural and aerodynamic operating limits.\nThe flight data which used by flight computers for prevent dangerous actions-maneuras, stabilize the plane and control the plane are;\n- Pitch, yaw, roll rate and linear acceleration\n- Angle of attact and sideslip\n- Airspeed, pressure, altitude and radio altimeter indications\n- Sidestick-Yoke and pedal demands\n- Other cabin commands such as landing gear condition, thrust lever position etc.\nFly By Wire – Basic Operation\n- When the pilot move the sidestick to command the flight control computer to make a certain action such as pitch the aircraft up or roll to one side, this demand is first of all transduced into electrical signal in the cabin and sent to flight control computer, the signal is sent through multiple wires (channels) to ensure that the signal reaches the computers.\n- The flight control computer also receives the data signals concerning the flight conditions and servo-valves and actuators positions . Then the flight control computer evaluates and calculates (in accordance with the position of aircraft, airspeed, actuator positions, pressure, altitude, landing gear condition and signals come from the pilot’s sidestick-yoke and pedal demands) what control surface movements will cause the plane to perform that action, and then the computer chose the best action and convert it to electrical signals.\n- These signals are then sent to command the electronic controllers for each surface. The controllers at each surface receive these commands and then move the actuators attached to the control surfaces until it has moved to where the flight control computer commanded it to.\n- The controllers measure the position of the flight control surface with potentiometers in the actuators and then send a signal back to the flight computer (usually a negative voltage) reporting the position of the actuator. When the actuator reaches the desired position the two signals (incoming and outgoing) cancel each other and the actuator stops moving (completing a feedback loop).\nThis process is repeated continuously as the aircraft is flying.\nFly By Wire – Advantages\n- Flight-Envelope Protection software automatically help to stabilize the aircraft and prevent the unsafe actions.\n- Turbulence suppression and consequent decrease of fatigue loads and increase of passenger comfort\n- Drag reduction by an optimised trim setting\n- Easier interfacing to auto-pilot and other automatic flight control systems\n- Maintenance reduction\n- Reduction of airlines’ pilot training costs (flight handling becomes very similar in an whole aircraft family)\n- Flight-control computers continuously “fly” the aircraft, pilot’s workloads can be reduced\n- Fly-by-wire control systems also improve economy in flight because Fly By Wire Aircraft can be lighter due to they eliminate the need for many mechanical, and heavy, flight-control mechanisms and wires except of hydrolic systems cover less space, less complex and reliable.\nDifferences between Airbus and Boeing in using Fly By Wire system\nThe Airbus A320 was the first commercial aircraft to incorporate full flight-envelope protection into its flight-control software.Airbus give full-authority to FBW controls, the flight envelope protection cannot be overridden completely, although the crew can fly beyond flight envelope limits by selecting an alternate “control law”. But in the Boeing 777 has taken a different approach by allowing the crew to override flight envelope limits using excessive force on the flight controls. Airbus used full-authority FBW controls. Boeing followed with their 777 and later designs. (Just B777 and B787) Boeing uses yoke on the B777, with fly by wire primary flight controls. Airbus does the same with a sidestick. Both provide full envelope protection.', ""Atmospheric Turbulence and the Evolution of Aviation Technology in the A220 (Credit: Airbus, CBS News, Meteorologist Jessica Olsen)\nDISCUSSION: Often many do not see the connection between aviation and meteorology, there can be a basic assumption that aircraft can withstand a wide variety of weather phenomena, which is often true but some are lacking in overall capability. In addition to specialized instruments needed for aircraft, basic meteorology is on the forefront of a major phenomena that most airline passengers typically ask meteorologists about, turbulence.\nAccording to the American Meteorological Society, aircraft turbulence is, “ irregular motion of an aircraft in flight, especially when characterized by rapid up-and-down motion, caused by a rapid variation of atmospheric wind velocities.” Which, “this can occur in cloudy areas (particularly towering cumulus and lenticular clouds) and in clear air. Turbulence is the leading cause of nonfatal passenger and flight attendant injuries. The U.S. Federal Aviation Administration (FAA) classifies aircraft turbulence as follows:\nWhile there are a variety of aircraft that do not have operational limitations, some aircraft such as the Canadair (CRJ) 200 or Embraer (ERJ) 140/45, often must remain grounded (until weather should pass) due to lack of ACARS (Aircraft Communications Addressing and Reporting System) (CRJ 200), and weight and balance issues. Aircraft and its flight are also limited to the use of visual flight rules (VFR) or instrument flight rules (IFR) based on aircraft and pilot ratings.\nTurbulence is by far one of the most noticeable connections between aviation and meteorology, of course behind the use of air as a fluid, the forces of flight and Bernoulli’s Principle. As we investigate turbulence, we can often just deduce that turbulence is the flow of atmospheric winds due to the rising and sinking of air in the atmosphere. Should we further explore wind in turbulent flow we can consider that all turbulence is, is atmospheric motion, and as height increased, pressure typically decreases, with wind increasing, creating various components of vertical motion going on while moving horizontally in the atmosphere within an aircraft.\nAirbus has created the A220-100, built as the smallest jetliner in the Airbus family, designed to serve the 100-135 seat market, in addition to the A220-300 to serve the 130-160 market. These new aircraft are attempting to put down the days of the MD-80, and B717 while providing an economical and efficient aircraft, compared to previous iterations. What is of note is its turbulence avoidance technology which according to CBS News, “shows pilots where the smooth air is by crowdsourcing data from all of Delta's aircraft.” Pilots are hailing this new technology, rather than attempting to maneuver the aircraft to find it, it can be found on the instrument panel. In addition to this valuable technology the aircraft was designed for hot, and high, city-like environments, an ideal aircraft for places like Phoenix’s Sky Harbor International Airport where extreme temperatures often limit aircraft arrivals and departures in the summer months.\nFor further information on aviation and weather visit the Global Weather and Climate Center!\n© 2019 Meteorologist Jessica Olsen\n“A220-100.” Airbus, Airbus, www.airbus.com/aircraft/passenger-aircraft/a220-family/a220-100.html.\nVan Cleave, Kris. “Delta's New Airbus A220 Features Wider Coach Seats and Turbulence Avoidance Technology.” CBS News, CBS News, 28 Dec. 2018, www.cbsnews.com/news/new-delta-airbus-a220-features-wider-coach-seats-turbulence-avoidance-tech/.""]"	['<urn:uuid:22baeb56-2952-4e49-ad05-08da5f06aeda>', '<urn:uuid:dc4578c3-b978-4b98-86bc-d3aa6a4296d4>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T18:38:36.516973	12	68	1243
64	curious know did anyone develop method evaluate newborn health condition right after birth	Virginia Apgar was an obstetrician who developed the Apgar Score, a five-item assessment tool used at births all over the world that helps nurses and physicians determine if a newborn requires emergency care.	['25 Women in History We Love\nOn the last day of Women’s History Month, Every Mother Counts honors some of the women who’ve made the biggest impact on the lives of all.\nWomen have always played an essential role in history, but their stories rarely made it into history books. Still, some women’s contributions were so important their stories could not be edited out. In honor of Women’s History Month, we’ve created a timeline of women we simply couldn’t live without.\n1) Abigail Adams’ (1744–1818) words demonstrate that while we’ve come a long way, in some ways, very little has changed. As her husband, President John Adams helped draft the Declaration of Independence, Abigail reminded him, “If particular care and attention is not paid to the ladies, we are determined to foment a rebellion, and will not hold ourselves bound by any laws in which we have no voice or representation.”\n2) Sojourner Truth (around 1797–1883) was born into slavery in New York. Upon discovering one of her children was sold into slavery after emancipation, she sued for his freedom and won. She became an important voice for all women’s rights with her speech, Ain’t I a Woman.\n3) Elizabeth Cady Stanton (1815–1902) and 4) Susan B Anthony (1820–1906) were team-leaders of the women’s rights movement, working for suffrage, women’s parental, property, employment, divorce and income rights.\n5) Elizabeth Blackwell (1821–1920) was the first woman doctor. We love the spirit of her quote: “It is not easy to be a pioneer — but oh, it is fascinating! I would not trade one moment, even the worst moment, for all the riches in the world.”\n6) Clara Barton (1821–1912) was a Civil War nurse, educator and humanitarian who founded the American Red Cross and continues to represent the essential medical care provided by nurses.\n7) Ida Tarbell (1857–1944), among the first female journalists, wrote exposes about corporate America. While she didn’t profess to be pro-suffrage, her journalistic prestige opened up publishing opportunities for women.\n8) Margaret Sanger (1879–1966) Her nursing career made her painfully aware of the tragic consequences of too many unplanned pregnancies, inspiring her to become the mother of modern contraception and women’s health.\n9) Jeannette Rankin (1880–1973) the first woman elected to Congress said: “Men and women are like right and left hands; it doesn’t make sense not to use both.” And, “We’re half the people; we should be half the Congress,”- a statement that continues to ring true today.\n10) Eleanor Roosevelt (1884–1962) dedicated her life to social equality. Her connection with people in all walks of life made her one of the most influential First Ladies. Among the first delegates to the United Nations, she helped write the Universal Declaration of Human Rights.\n11) Amelia Earhart (1897–1937) broke aviatrix records and gave women wings. Amelia’s quote “Women must try to do things as men have tried. When they fail their failure must be but a challenge to others,” was a powerful statement about women’s rights to try, regardless of whether success is guaranteed.\n12) Virgina Apgar (1909–1974) was an obstetrician who developed the Apgar Score, a five-item assessment tool used at births all over the world that helps nurses and physicians determine if a newborn requires emergency care.\n13) Mother Teresa’s (1910–1997) impact on poverty eradication and her views that poverty isn’t just a matter of money, but also one of social isolation are modeled throughout the world today.\n14) Rosalyn Sussman Yalow (1921–2011) was an American medical physicist and co-winner of the 1977 Nobel Prize for helping develop a technique to measure minute quantities of insulin in the blood.\n15) Betty Freidan (1921–2006) wrote, The Feminine Mystique, permanently changed women’s conversations about whether their place really was in the home. Credited for sparking the second wave of feminism, Freidan became the first president for the National Organization of Women.\n16) Ruth Bader Ginzberg — (1933) was the first Jewish woman on the Supreme Court and remains a strong advocate for women’s rights and civil rights in general.\n17) Gloria Steinem (1934) is recognized as the media spokesperson for women’s liberation and co-founder of Ms Magazine. Her statement, “Most women are one man away from welfare,” woke a generation of women to their own economic vulnerability.\n18) Billy Jean King (1943) won 20 titles at Wimbledon and created the Women’s Sports Foundation, which helped create Title IX, guaranteeing equal opportunities for boys and girls in all educational activities including sports.\n19) Sally Ride (1951–2012) became the first woman in Space. She is on the record saying “I did not come to NASA to make history,” but she did it anyway.\n20) Beverly Johnson (1952) was the first African American model on the cover of Vogue magazine, which led to wider acceptance of African American women in all media spotlights.\n21) Oprah Winfrey (1954) is the first African American to have a syndicated talk show host. Her words, “For everyone of us that succeeds, it’s because there’s somebody there to show you the way out,” speak for all women looking for a better life.\n22–25) Sandra Day O’Connor became America’s first female Supreme Court Justice, who paved the way for Madeleine Albright (first female Secretary of State), and Condoleezza Rice (first African American Secretary of State) and arguably the most powerful woman in politics today — Hillary Rodham Clinton.']	['<urn:uuid:5a1c34df-3a88-4bd6-bdd8-885a13eb6277>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	13	33	892
65	I'm researching institutional responses to crises - how do museums and mental health researchers adapt their traditional approaches when dealing with unprecedented situations like Banksy's art market disruption and COVID-19's impact?	In both cases, institutions showed adaptability to unprecedented situations. Museums demonstrated flexibility by giving artists like Banksy access to galleries (as seen in the 'Banksy v Bristol Museum' show in 2009), while mental health researchers responded to COVID-19 by rapidly developing new research methods, such as the STARC lab's online longitudinal psychological wellbeing survey to understand the pandemic's impact. Both situations required institutions to move beyond traditional approaches to address new challenges.	['What is the relationship between art and the market? This thorny question, which regularly worries students of art and art history, was one of those also considered by Grayson Perry in his first BBC Reith Lecture. In the series as a whole he has promised to answer some of the ‘big questions’ that ordinary people might have about art. Under the title ‘Democracy has bad taste’ his first lecture considered how different works end up in the museum or gallery, and one of his big answers was the role of the market – auction houses and dealers – and their customers both public and private, in establishing the ‘value’ of pieces of art. The art that is ‘in’ with dealers and collectors is what is agreed as ‘art’ and has value.\nYet, this relationship, has increasingly been called into question in recent years, most notably by artists themselves. In 2008, YBA Damien Hirst infamously chose to hold a one-man sale of mostly new work at Sotheby’s, by-passing the usual route through the commercial gallery or other collector. He set a record, making £111 million in two days. As Maev Kennedy memorably commented in the Guardian, this was more than all the artists in the National Gallery made in a lifetime. The sale began on the very day that Lehman Brothers went bankrupt, and art has become an increasingly popular investment item in the subsequent years. Its value as an investment is only established by considering what it would fetch within the market.\nIn Perry’s second lecture ‘Beating the Bounds,’ he established a series of ‘tests’ for deciding what is and isn’t art, two of which were ‘Is it in an art space?’ (a museum or gallery), and ‘is it by an artist? He gave the striking example of the graffiti artist Banksy, who created a piece for the Olympics in 2012 showing a child sweatshop worker making Union Jack bunting. Perry recalled that, when the owner of the wall on which this was painted tried to sell the work, the Banksy argued that that the piece was no longer a ‘Banksy’ and was unsaleable as one. He had done his work for the general public, and thus it would, essentially, cease to be a Banksy if removed from that setting. So, its presence on the market would make it cease to be (his) art.\nBanksy has made another intervention in the art market more recently. A lot of media attention has been paid to his recent ‘residency’ in New York, entitled ‘Better Out than In’, during which he posted almost daily pictures on his website of new art works, whether graffiti, installation or performance. On 12th October, Banksy put a number of signed works on sale on a stall in Central Park for only $60 each. He told the New York magazine Village Voice ‘I wanted to make some art without the price tag attached. There is no gallery show or book or film. It’s pointless. Which hopefully means something.’\nSo, where does this leave art, the market, and museums? Undoubtedly, Banksy’s visibility and value were hugely increased when a brave curator at Bristol Museum gave him access to the galleries for the enormously popular show ‘Banksy v Bristol Museum’ in 2009. Is an artist such as Banksy better out than in when it comes to the market? Perhaps the real value is in the tensions between the one and the other.', 'The mental health impact of COVID-19\nHow can quarantine affect your mental health? How can we better manage the anxiety that lockdown can provoke? Professor Cherie Armour from the Stress, Trauma, and Related Conditions (STARC) lab at Queen’s is working to find out.\nAs isolation, lockdown and quarantine measures are enforced throughout the UK and Ireland, a team of researchers from the Stress, Trauma, and Related Conditions (STARC) lab at Queen’s University School of Psychology are conducting a study which aims to better understand the impact that COVID19 is having on the psychological wellbeing of the people in Northern Ireland and the Republic of Ireland.\nProfessor Cherie Armour, who is leading the project says: “COVID-19 and the restrictions it places on the population as a whole, such as the need to self-isolate, is and will continue to be a stressful life event for many people across the country.\n“In Northern Ireland, Universities have moved their academic activities off campuses to online environments, schools and childcare settings have seen closures, as have pubs, restaurants, leisure venues, and shops selling non-essential goods. Businesses across many sectors are supporting their employees to work from home. With government rules stating you must stay at home, the daily escalation of the seriousness of the situation will of course be anxiety-provoking for many people.”\nProfessor Cherie Armour\nCOVID-19 and mental health\nProfessor Armour’s research focusses on adverse and/or traumatic life events, including those that occur because of someone’s occupational role (e.g. Police, Military & Ambulance services), and how those impact on psychological well-being. She is particularly interested in the psychological disorders of Posttraumatic Stress Disorder, Depression, Anxiety and Dissociation.\n“My focus is on biopsychosocial factors that exist pre-during- and post-trauma and how they predict someone’s risk of developing, or resilience against developing, mental ill health outcomes,” she explains.\nAs part of her research, Professor Armour examines the role of a number of things that might impact on the trauma exposure and mental health outcome relationship such as sleep, social support and emotion regulation.\nShe is now applying her research skills to examine the impact of the COVID-19 pandemic on the overall psychological wellbeing of the population and what people can do to ensure that this impact is minimised for themselves and their families?\nThe impact of quarantine\nThe project follows research from Kings College London relating to the psychological impact of quarantine.\n“The research found that those quarantined reported heighted psychological distress including confusion, fear, anger, anxiety and difficulties with sleeping,” says Professor Armour. “One study reported that some longer lasting behavioural changes were seen such as vigilant hand washing and the avoidance of large crowds.”\nShe adds: “The research tells us that some of the key stressors during self-isolation / quarantine relate to fears about becoming infected, having inadequate supplies and inadequate information, a sense of loneliness through isolation, and feelings of boredom and frustration. Research has also found that longer durations of self-isolation / quarantine have a more adverse impact on psychological wellbeing.”\nWhile the Kings College project reviewed a relatively small number of studies (26 in total), it provided the research community with a good evidence base for further research. Professor Armour says it is therefore now vital that the research community mobilise to collect robust and reliable data that will allow us to understand what the psychological impact of COVID-19 is but also how that impact may change over time.\nGathering vital data\nTo that end, researchers at STARC have designed a longitudinal psychological wellbeing survey that is administered online.\n“This survey will ask people a wide variety of questions about their life experiences, physical health, living environments, exposures and worries related to COVID-19, sleeping habits, what social support networks they have, whether they are experiencing loneliness, how they are regulating their emotions, whether they can see meaning and purpose to their life, and whether they are experiencing any symptoms of depression, anxiety, or PTSD in response to the COVID19 situation,” says Professor Armour.\nThe first survey takes just 20 minutes to complete and then subsequent surveys will take approximately 10 minutes to complete.\nThe survey will allow researchers to examine the psychological wellbeing of subgroups, such as healthcare professionals and other key workers.\nProfessor Armour says “Previous research tells us that when people experience adverse life events those with better health, fewer past trauma experiences, more social support, a feeling of connectedness (i.e., not feeling lonely), those who have a sense of purpose and meaning in their life and those who can regulate their own emotions tend to experience fewer adverse impacts on their psychological wellbeing.\n“The survey will allow us to understand if this is indeed the case for COVID-19 and for all those currently self-isolating from within our Queen’s University community and from across the wider community of Northern Ireland. Ultimately, this will inform us in how to best offer pragmatic help, with a view to alleviating some of the pressure that services face during these unprecedented times.”']	['<urn:uuid:497e6110-ad42-450b-bbb7-42c73a346211>', '<urn:uuid:ef55af05-b093-467b-b6c2-64ebce94f5fa>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	31	72	1401
66	What progress and challenges exist for women's visibility in sports?	Women's visibility in sports is increasing, with Ronnie Gajownik becoming a pioneering female baseball manager at the High-A level, providing important representation for young girls. In broader sports, 2023 saw record attendances and viewing figures for women's sports, with increased visibility on free-to-air TV. However, significant challenges persist - while 86% of sports-playing girls believe they can become like a Lioness, there remains a substantial 'dream deficit' with only one-third of girls overall dreaming of becoming professional athletes compared to half of boys. Additionally, 72% of girls who watched the FIFA World Cup reported that it highlighted inequality at the top of sport.	"['D-backs name Gajownik as 1st woman skipper at High-A level\nPHOENIX -- When Ronnie Gajownik was growing up in Orlando, Fla., her dad would sometimes take her out of school to go to a Spring Training game with him. Baseball was a passion for her, but as she would stare out onto the field, there was something she didn\'t see -- people who looked like her. Instead, it was a game played, coached and managed by men.\nThanks to women like Gajownik, that is no longer the case.\nAfter serving as a coach in the Arizona D-backs\' Minor League system in 2022, Gajownik (pronounced Gah-jow-nick) was named manager of the High-A Hillsboro Hops on Friday as the club released its Minor League coaching staffs for 2023.\n""I know that if my dad took me out of school on a Wednesday and we went to a baseball game and I saw a female coach on the field, I know Thursday my life would have been changing in the trajectory of where I want it to go,"" Gajownik said. ""So the visibility aspect of it is huge, because, again, it\'s showing little girls and showing women that we\'re breaking the glass ceiling and we\'re leaving breadcrumbs for everybody behind us for us just to keep adding on to it to see how far we can go.""\nLast year, Rachel Balkovec became the first female manager in professional baseball when she led the Yankees\' Class A team, the Tampa Tarpons. This year, Gajownik will become the first woman to manage at the High-A level. The two recently spoke by phone after Balkovec learned of Gajownik\'s promotion.\n""It was definitely beneficial, and I know that her and I are going to have a really good friendship,"" Gajownik said. ""I know that her and I can lean on one another, if needed.""\nGajownik, who won a gold medal in the 2015 Pan-American Games as part of Team USA\'s women\'s baseball team, started with the D-backs in 2021 as a video assistant at Hillsboro. It\'s a job that has proven to be a great pipeline into the organization for coaches, scouts and analysts.\nWhen Arizona farm director Josh Barfield visited Hillsboro during the season and interacted with Gajownik, he came away from the experience thinking that she would make an excellent coach.\n""It was just her knowledge of the game,"" Barfield said. ""She had a playing background. And the way she\'s able to communicate -- the hunger she possessed, the desire to learn and grow -- it just all stood out.""\nAs other members of the organization made stops in Hillsboro, they came to the same conclusion: Gajownik belonged in coaching.\nSo when they were making out the Minor League staff assignments for 2022, Gajownik was ticketed to be a coach on one of the D-backs\' Rookie Arizona Complex League teams.\nHowever, a line drive off the bat of top prospect Corbin Carroll changed all of that.\nCarroll was taking front toss from Double-A Amarillo coach Javier Colina, and he smoked a ball that shattered Colina\'s foot. Colina would need surgery and pins inserted, and Amarillo therefore needed a coach, so the D-backs promoted Gajownik, who spent the season coaching first base.\n""She went out there and it took her about two seconds to fit right into the group,"" Barfield said. ""She did a great job working with the guys in Amarillo. She\'s just got such an ease about her, the way she connects with people. I think some of the best coaches -- obviously they have good content and she has that. But I also think they\'re incredible connectors and she\'s definitely one of those. She shows leadership, she shows initiative.""\nGajownik also served as a coach last fall for the Salt River Rafters in the Arizona Fall League, and when the D-backs were putting together their coaching staffs for this year, she was, in Barfield\'s words, ""an easy choice.""\n“On behalf of the entire organization, we congratulate Ronnie on this significant milestone as she becomes the second female team manager in Minor League Baseball and at the highest developmental level,” said D-backs team president and CEO Derrick Hall. “We look forward to witnessing her continued growth within player development as she shapes and develops the future stars of the D-backs, while continuing to be a tremendous point of pride for the enterprise.”\nGajownik returns to Hillsboro, where her professional baseball career started and where she already has relationships. She knows the owner, the general manager, the people in the front office and her wife also works in the organization.\n""It feels great to get this opportunity, especially with the Diamondbacks,"" Gajownik said. ""They saw my potential in me as a coach and as a person, and then obviously taking that chance with me last year. So this opportunity means a lot to me. Hopefully, everybody who\'s been involved with me and also hopefully all the little girls and the women who are seeing this know that if you\'re qualified, you can do it. And ... that\'s all you need to be. If you\'re qualified, you can go ahead and do whatever it is you\'re passionate about.""', 'Women in Sport is thrilled to see some prominent figures, including Mary Earps MBE, Millie Bright OBE and Helen Housby MBE, recognised in the King’s New Year Honours list 2024.\n2023 has been an exceptional year for female athletes and teams. From the history-making performances of the Lionesses and Vitality Roses in their respective World Cups, to remarkable achievements in cricket, golf and athletics which have continued to propel women’s sport forward. Record attendances and viewing figures have demonstrated that people love to watch great women’s sport and more brands are recognising the opportunities that women’s sport presents.\nView this post on Instagram\nThe increased visibility and profile on free-to-air TV and in the press is shifting. Women in Sport research shows girls who already love and play sport can now dream about reaching the top of sport, with 86% of them believing they can be like a Lioness. But overall, a significant dream deficit remains overall with a third of girls about to dream of being a professional athlete, compared to half of boys.\nIt is high time society acknowledged the inherent value of women’s sport and its significance in the lives of women and girls. The New Year Honours serves as a powerful symbol of that.\nNew Year Honours recipients include:\n- Millie Bright, who captained England Lionesses to the World Cup final, receives an OBE\n- Lauren Hemp, England footballer, MBE\n- Mary Earps, England goalkeeper, MBE\n- Helen Housby, named player of the tournament at this year’s Netball World Cup, MBE\n- Olympic Gymnast Beckie Downie, and her sister Ellie Downie, former Olympic Gymnast, are both made MBEs\n- Maddie Hinch, GB Olympic gold medal winning hockey goalkeeper, OBE\n- Sports presenter Hazel Irvine, MBE\n- Ruth Shaw, CEO of the Premier League Charitable Fund, OBE\n- Janie Frampton, former football referee and referee manager, OBE\n- Christine Windmill (Tennis Scotland honorary vice-president), for services to tennis, OBE\n- Lynda Bennett, for services to hockey in Wales, MBE\n- Joslyn Hoyte-Smith (GB Olympians Association chair), for services to athletics, MBE\n- Miriam Luke (Henley Rowing Club president and Olympic silver medallist), for services to women’s rowing, MBE\n- Elinor Middlemiss (Scotland Commonwealth Games chef de mission), for services to sport, MBE\n- Kelly Fisher (professional pool, snooker and English billiards player) for services to sport, MBE\n- Bronwin Carter (Paralympic coach and 11 times weightlifting world champion) for services to sport, BEM\nStephanie Hilborne OBE, Chief Executive of Women in Sport said:\n“There has been so much to celebrate in 2023 and it’s wonderful to see such incredible sportswomen recognised in the New Year Honours List. These are women who have dedicated their lives to achieving excellence and are at the forefront of the campaign for women’s sport to be given the credit it deserves. They have made sacrifices they should never have had to make and overcome barriers they should not have even existed. Their courage and resilience will ensure future generations of girls and women can feel they truly belong in sport.\n“But alongside great success and moments of pride this year, we have seen misogyny cast its ugly shadow time and time again. 72% of girls who watched the FIFA World Cup told us it opened their eyes to the inequality at the top of sport. What message is that giving to young girls?\n“Our research shows that success of elite women’s sport has allowed more girls to dream of reaching the top, far fewer girls than boys are still able to relate to this classic dream. Stubborn stereotypes, a lack of opportunity and limited funding is continuing to hold girls back. Having more women shine at all levels from athlete to broadcast to leadership can start to redress the balance. We know sport can build resilience, courage and self-belief but too many girls and women are missing out. The joy and sense of freedom that sport can provide is even more important than ever as so many people struggle to make ends meet.\nView this post on Instagram\n“Congratulations to all the women who have pushed boundaries in 2023. As we move into 2024 and our charity’s 40th birthday, we are optimistic about the continued progress of women’s sport remain steadfast in our commitment to expose injustice and break down barriers to create a world where all women and girls not only feel they belong in sport but can thrive.”']"	['<urn:uuid:1a0af8c4-6a45-41ea-89fb-515f2efb4922>', '<urn:uuid:a677434b-4cad-4272-8a9b-a9a6ca8f9b4d>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	10	103	1600
67	Between the traditional stainless steel external ring fixator and the new carbon fiber composite one being developed, which one is better at resisting chemical effects and why?	The carbon fiber composite ring fixator has better chemical resistance because it uses polyether sulphone (PES) as the matrix material, which exhibits excellent resistance to chemicals. While 316L surgical steel is corrosion-resistant, it's specifically vulnerable to chlorides which can have an adverse effect on its corrosion resistance, even though it contains molybdenum to enhance this property.	"[""|Carbon Fibre Composites in Orthopaedics|\nSangeeta Nangia, Soumitra Biswas\nThe Department of Science and Technology (DST), Govt. of India in association with the Defence Research & Development Organization (DRDO) has launched the Advanced Composites Mission aimed at promoting the technology of composite materials & products in Indian industry.\nThe Technology Information, Forecasting & Assessment Council (TIFAC), an autonomous organization under DST has been entrusted with the implementation of the mission-mode programme.\nThe success of the Advanced Composites Mission of TIFAC may possibly be gauged by the large array of projects initiated by the Mission across the country. The Mission initiatives cover a wide spectrum of developmental projects ranging from hi-tech brake-discs for aircraft to down-to-earth societal applications such as orthopaedics to alleviate human suffering.\nThe Mission has recently launched a project in collaboration with M/s. S H Pitkar Orthotools Pvt. Ltd., Pune for developing orthopaedic appliances to enhance surgical implants to international standards and thereby offering improved rehabilitation systems.\nThe project aims at developing components for orthopaedic applications based on polyether sulphone matrix reinforced with carbon fibre. This new material is being developed as a substitute for stainless steel currently being used for the external ring fixator for the bone-healing system.\nWhat’s a Ring Fixator?\nIt is an exceptionally versatile circular external fixator. The system has good range of fasteners and other supporting devices of various sizes & lengths, which produce an excellent combination of bone segments including angulation, rotation, translation, lengthening and compression.\nThe basic principle behind the use of this apparatus is tissue regeneration through the principles of distraction histogenesis of bone and tissues. The apparatus is quite stable permitting weight bearing and joint functioning during the treatment. This technique of fitting external fixator on human body is commonly known as llizarov technique.\nWhat is Ilizarov Technique?\nThe surgery of Poliomyelitis, Osteomyelitis etc. has always been considered unglamorous. Limb lengthening being considered esoteric, unsafe and unpredictable, deformity correction was considered uninteresting and unrewarding. There was simply no elegant or effective solution in sight and hence a large number of patients languished.\nThe conventional premise of orthopaedic surgery that the generation of new bones, muscles and nerve tissue is not possible has been belied by the Ilizarov technique. It is in this scenario that Prof. Ilizarov’s 'nuts and bolts' surgery came in like a whiff of fresh air from the Siberian wilderness.\nThe technique, first developed by Russian surgeon G. A. Ilizarov in 1951 but which remained little known till recent times, is being used to correct both congenital bone deformities and those acquired due to polio, bone infection and accidents.\nIt has been used with great success for a wide range of limb disorders such as fresh fractures, disunited and infected fractures with gaps & deformities. This technique is able to fix bony fragments with minimal invasion of the tissues and retained function.\nUsing the law of tension, stress transforms the tissues to their embryonic potential. The surgeon can lengthen and reshape limbs, heal the most recalcitrant of non-unions, straighten & align bones and stabilize joints with the ability of a craftsman. This technique has already gained immense popularity in Mumbai, Bangalore and Chennai & is now becoming popular in Delhi as well.\nHow is it performed?\nIlizarov ring fixator is a set of around 20 simple mechanical devices including rings, threaded rods, nuts, bolts and hinges. The stainless steel pins are jointed to steel rings/carbon composite rings with bolts and put under ideal tension. The two sets of rings are fixed at the two ends of a deformed bone.\nThese rings are supported by special 2 mm thick pins those are drilled into the bone as well as manually adjustable threaded rods that connect the rings at the two ends. By gradually strengthening the two rings apart, pressure is exerted on the bone below, stimulating cell growth and elongation.\nGrowth of the associated muscle, nerve, skin and blood vessel tissues takes place simultaneously. The patients are advised to distract the rods at the rate of 0.25 mm every six hours. The distraction is initiated at the recommended rate of 1 mm daily. On an average, it usually takes one month to increase the length by 1 cm\nThere is perfect lengthening without complications and every deformity can be corrected. The wires slightly larger than hypodermic needles cause minimal damage and have a low risk of infection.\nThe elongation of bones makes them weaker and without proper physiotherapy, it is difficult to regain the muscle powers. Physiotherapy plays a crucial role to ensure joint movement and proper blood supply.\nThis technique could also be used for cosmetic treatment. It would be a boon for the people who want to increase their height. Lengthening can be done in legs, thighs and arms leading to a possible increase in the height by about 30 cms. This is a long procedure requiring about seven to eight months for each leg.\nWhy Ring Fixator?\nThe benefits associated with the ring fixators are the following:\nWhile the former is rare and preventable by a thorough knowledge of anatomy, the latter can be treated with antibiotics. The versatility of the fixator permits fine-tuning and precise control over the process. The cost of this technique works out to be Rs.30,000 to over Rs.100,000 depending on the treatment duration.\nThe present project aims at developing the external ring fixator based on Carbon Fibre and Polyther Sulphone matrix. The importance of advanced composites for future light-weight materials is derived from the high stiffness combined with high strength & low density of fibre backbone. In this respect, carbon fibre surpasses the more economic glass fibre.\nMore than 90% of the carbon fibres used today are high-tension fibres with a tensile strength of above 3500 MPa. The Young’s Modulus of this type of fibres with values around 250 GPa is much higher than the values of the aramid and glass fibres.\nCarbon fibre has negative coefficient of expansion in axial direction, which makes them resistant to thermal shock. They also possess low coefficient of friction and very good fatigue characteristics. The dimensional stability and very low coefficient for thermal expansion are properties instrumental for its use in tooling. The reduced coefficient of friction than glass makes carbon fibre more effective against steel.\nThe ability of Carbon fibre composites to resist chemical environment depends principally on the matrix systems. With the suitable selection of the matrix resin, it is possible to manufacture composites that exhibit excellent resistance to chemicals. Therefore, using polyether sulphone as the matrix for carbon fibres makes an excellent combination of high chemical resistance.\nIn addition to heat deformation resistance, the polymer is also resistant to chemical change on heating. It is therefore, capable of absorbing a high degree of thermal and ionizing radiation without cross-linking.\nThe principal features of the polyhether sulphone are their exceptional resistance to creep, good high temperature resistance, rigidity and above all transparency and self extinguishing characteristics. The transparency characteristic of PES enables the composites to be radiolucent.\nThese excellent properties of carbon fibre and polyether sulphone and moreover, the compatibility of the fibre with PES matrix makes them suitable for such orthopaedic applications.\nThe stainless steel external fixators are found heavy for the patients. Composite fibre ring is not only lighter but also due to its radiolucency, it enables the surgeons read the X-rays with better accuracy. Moreover, the high strength to weight ratio of composite made of carbon fibre and engineering polymers makes it suitable for high impact strength, as needed for the surgical procedures.\nIt has been planned to develop the following items under this project:\nAs of now, the imported fixators, by way of rehabilitation aids, have limited reach due to their high costs. This project would aim, in a self-reliant fashion, to ensure that such rehabilitation aids reach the commoners."", 'What are 316L, 904L, Surgical Grade Stainless Steel\nChlorides in particular can have an adverse effect on the corrosion resistance of stainless steel. Grades high in Chromium, Molybdenum and Nickel are the most resistant to corrosion. Grade 316 is the standard molybdenum-bearing grade, second in importance to 304 amongst the austenitic stainless steels.\nSep 26, 2016 · Made with first class, hypoallergenic 316L Surgical Grade Stainless Steel that only the largest jewelry manufacturers use. You can be sure that this is an aromatherapy locket necklace that will last for many years, providing perfect aromatherapy every single day!\nSurgical Stainless Steel - an overview ScienceDirect Topics\nThe most common medical steel grades being used instruments and implants (e.g.fracture fixation plates, rods, and screws) are austenitic stainless steels, most of the form 316, 316, and 304 L. There are many compositions that would qualify as stainless steels though, and the American Society for Materials (ASM) has standards based on microstructure that would qualify as stainless steels.\nSurgical Steel Basic Body - Surgical steel basic body jewelry. Here we have 316L high grade surgical steel industrial barbells, captive bead rings, horse shoe rings, nose screws, twister rings, eyebrow rings and more. Variety of gauge sizes to choose\n316L SURGICAL STEEL contains between 6-13% nickel and it does not conform to the first part of the Directive and it cannot, therefore, be used for fresh piercings. It can only be used after the healing process (also known as ‘epithelization’) has taken place, as replacement jewelry, since it\nZEEWELY Tree of Life Essential Oil Diffuser Necklace – Aromatherapy - Hypoallergenic 316L Surgical Grade Stainless Steel 20 inch Adjustable Chain, and 7 Colours Lava Stone Beads. Only 7 left in stock - order soon.\nThe two most common stainless steel grades are 304 and 316. The key difference is the addition of molybdenum, an alloy which drastically enhances corrosion resistance, especially for more saline or chloride-exposed environments. 316 stainless steel contains molybdenum, but 304 doesn’t.\nSep 15, 2016 · The only body-friendly grades are 316L and 316LVM Surgical Steel. These are low-carbon Surgical Steel materials. They do contain alloys, however, the alloys are trapped inside the material through special process so they cannot be released from the metal. Allergic reactions, when they occur, are rarely due to the Stainless Steel but from other factors (most commonly from mechanical\n316LS / 316LVM Surgical Stainless Steel Bar - ASTM F\n316LS/316LVM surgical steel is an electro-slag remelted (ESR) or vacuum arc remelted (VAR), low carbon, high nickel and molybdenum version of 316 stainless. The secondary premium melting step imparts improved cleanliness.\nSome companies use 316 Surgical Stainless Steel which costs more but has no add value when cooking. Type 304 stainless steel is an austenitic steel possessing a minimum of, Type 304 is the most common of austenitic grades, containing normally, 20% chromium or more and 10% nickel, combined with a maximum of 0.08% carbon.\nIn order to achieve this, it’s vital to only use jewelry made of body-friendly Surgical Steel3 16L and 316LVM Surgical Steel. Out of the two, 316LVM is a superior material and recommended for body jewelry. It offers more protection and safety than the cheaper 316L Surgical Steel. That’s why 316LVM\nType 316L stainless steel is popular for surgical practices as it is the most corrosion resistant when in direct contact with biological fluid. It is important that a surgical implant is not susceptible to corrosion when placed inside the human body to prevent the chances of infection occurring.\nLet’s take a look at two alloys that are typically considered ideal for most food grade applications, grade 316 stainless steel and grade 430 stainless steel. Food Grade Stainless Steel 316 Grade 316 stainless is an austenitic stainless steel alloy with a high chromium and nickel content.\nSep 15, 2016 · What is colloquially known as Surgical Steel is a material with specific alloys and properties. There are many different grades of Surgical Steel but only a few are body-friendly and suitable for body jewelry. The only body-friendly grades are 316L and 316LVM Surgical Steel. These are low-carbon Surgical Steel materials.']"	['<urn:uuid:6bdf7e44-139f-4569-96c6-0de4e526c48a>', '<urn:uuid:e911f390-9a53-4ba7-a953-a3181b94f73e>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T18:38:36.516973	27	56	1989
68	relationship between pathogen exposure immunogenetic properties fitness wild sheep populations	In wildlife populations, both innate and adaptive immune responses act as defense mechanisms against pathogens. The effectiveness of these immunologic responses, and consequently the fitness of individuals, populations and species, is determined by pathogen exposure history and the immunogenetic repertoire of major histocompatibility complex (MHC) genes and non-MHC genes. Novel highly virulent pathogens can overwhelm host immune responses that aren't primed to their exposure, potentially reducing species distribution and abundance over short timeframes through effects on survival and reproductive success.	['Background A key problem for conservation biologists is to look for the best suited demographic and genetic administration approaches for wildlife populations threatened by disease. history considerably inspired success or cause of death. The presence of antibodies to a respiratory virus known to cause pneumonia was associated with increased survival but there was no correlation between genetic heterozygosity and the presence of antibodies to this computer virus. Conclusions Although genetic theory predicts normally increased heterozygosity was not associated with increased fitness (survival) among translocated animals. While heterosis or genetic rescue GSK1120212 (JTP-74057, Trametinib) effects may occur in F1 and later generations as the two source populations interbreed we conclude GSK1120212 (JTP-74057, Trametinib) that previous pathogen exposure was a more important marker than genetic heterozygosity for predicting survival of translocated animals. Every wildlife translocation is an experiment and whenever possible translocations should be designed and evaluated to test hypotheses that will further improve our understanding of how pathogen GSK1120212 (JTP-74057, Trametinib) exposure and genetic variability influence fitness. Background Innate and adaptive immune responses developed in vertebrates as a first and secondary line of defense respectively against a diverse and changing array of pathogenic organisms. The effectiveness of these immunologic responses and hence the fitness of individuals populations and species is driven by pathogen publicity history as well as the immunogenetic repertoire of main histocompatibility complicated (MHC) genes and non-MHC genes [1 2 Book Mouse monoclonal to CD29.4As216 reacts with 130 kDa integrin b1, which has a broad tissue distribution. It is expressed on lympnocytes, monocytes and weakly on granulovytes, but not on erythrocytes. On T cells, CD29 is more highly expressed on memory cells than naive cells. Integrin chain b asociated with integrin a subunits 1-6 ( CD49a-f) to form CD49/CD29 heterodimers that are involved in cell-cell and cell-matrix adhesion.It has been reported that CD29 is a critical molecule for embryogenesis and development. It also essential to the differentiation of hematopoietic stem cells and associated with tumor progression and metastasis.This clone is cross reactive with non-human primate. extremely virulent pathogens is able to overwhelm web host immune replies not primed with their publicity and such pathogens could be a solid selective drive reducing the distribution and plethora of a types over brief timeframes (1-2 years) through results on success and reproductive achievement . More than multiple generations a brief history of ongoing pathogen publicity theoretically should go for to get more resistant immunogenotypes that limit fitness influences by responding successfully upon initial publicity (innate immunity) or re-exposure (adaptive immunity). Bighorn sheep (Ovis canadensis) certainly are a useful model for evaluating this interplay between disease demography and genetics. They certainly are a polygynous extremely philopatric species within little fragmented populations in the mountainous parts of western THE UNITED STATES . These are extremely vunerable to infectious disease and outbreaks of disease frequently trigger high morbidity and mortality [3 4 The annals of people die-offs goes back to Western european settlement from the western USA >200 yrs ago  indicating that book pathogens were most likely introduced by connection with local sheep (Ovis aries). Pneumonia epizootics seem to be powered by density-dependence GSK1120212 (JTP-74057, Trametinib) portion to constrain people size  and presumably choosing for one of the most suit genotypes. However little populations of bighorn sheep are also susceptible to inbreeding and hereditary drift rendering it difficult to comprehend the relative need for pathogen-mediated selection drift and inbreeding on hereditary variability and fitness. We contacted this issue by examining whether hereditary background and prior pathogen publicity influenced success when pets from two different creator populations were concurrently translocated in to the San Andres Mountains (SAM) in New Mexico USA. The SAM once backed the biggest people of indigenous bighorn sheep in the state. However from the late 1990’s a combination of disease mountain lion (Puma concolor) predation and drought experienced reduced this populace to the point of extinction and translocation from captive and/or free-ranging herds was necessary to reestablish a self-sustaining populace in the SAM. The two GSK1120212 (JTP-74057, Trametinib) founder populations chosen for reintroduction were a genetically varied free-ranging herd in the Kofa National Wildlife Refuge (KNWR) Arizona and a less varied captive herd in the Red Rock Wildlife Area (RRWA) that was originally derived from the native SAM populace. In November 2002 51 bighorn sheep were translocated into the SAM from your KNWR (n = 20) and the GSK1120212 (JTP-74057, Trametinib) RRWA (n = 31) and 30 more bighorn were translocated from KNWR in November 2005. We examined genetic variance at 33 microsatellite loci to compare genetic variability and we carried out.']	['<urn:uuid:560f6239-481c-414c-86d3-14444493c6ae>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	10	80	760
69	what factors consider before installing automated water systems farming land requirements expertise needed	When installing automated irrigation systems, several key factors must be considered. These include: the crop type and its water requirements; field shape and size; topography and land slopes; water source availability and quality; soil profile and infiltration rates; energy requirements and power availability; availability of skilled labor for installation and maintenance; and social aspects like local preferences and capacity to maintain the system. For drip systems specifically, technical expertise is crucial for design, installation, and operation, with high skill requirements for managing filters, chemical treatments, and maintenance. Both sprinkler and drip systems need clean water sources to avoid blockages, and proper filtration systems must be installed. Environmental impact assessments should be conducted to understand potential effects on water resources. Additionally, institutional conditions such as land tenure, water rights, and government policies must be considered before implementation.	['Systems of pressurised irrigation, sprinkler or drip, can improve water efficiency and contribute substantially to improved food production. Sprinkler irrigation is a type of pressurised irrigation that consists of applying water to the soil surface using mechanical and hydraulic devices that simulate natural rainfall (see Figure 1). These devices replenish the water consumed by crops or provide water required for softening the soil to make it workable for agricultural activities. The goal of irrigation is to supply each plant with just the right amount of water it needs. Sprinkler irrigation is a method by which water is distributed from overhead by high-pressure sprinklers, sprays or guns mounted on risers or moving platforms. Today a variety of sprinkler systems ranging from simple hand-move to large self-propelled systems are used worldwide. Global use of sprinkler irrigation is: the Americas (13.3 million hectares (Mha)), Europe (10.1 Mha), Asia (6.8 Mha), Africa (1.9 Mha), and Oceania (0.9 Mha) (Kulkarni et al, 2006).\nA sprinkler irrigation system typically consists of:\ni) A pump unit which takes water from the source and provides pressure for delivery into the pipe system. The pump must be set to supply water at an adequate pressure so that the water is applied at rate and volume adequate to the crop and soil types\nii) Main pipes and secondary pipes which deliver water from the pump to the laterals. In some cases these pipelines are permanently installed on the soil surface or buried below ground. In other cases they are temporary, and can be moved from field to field. The main pipe materials used include asbestos cement, plastic or aluminium alloy\niii) The laterals deliver water from the pipes to the sprinklers. They can be permanent but more often they are portable and made of aluminium alloy or plastic so that they can be moved easily\niv) Sprinklers, water-emitting devices which convert the water jet into droplets. The distribution of sprinklers should be arranged so as to wet the soil surface in the plot as evenly as possible.\nA wide range of sprinkler systems is available for small and large-scale application. Set systems operate with sprinklers in a fixed position. These sprinklers can be moved to water different areas of the field, either by hand or with machinery. Hand-move systems are more labour intensive and may be more suited where labour is available and cheap. On the other hand, mechanically operated systems require a greater capital investment in equipment. Mobile systems minimise labour inputs by operating with motorised laterals or sprinklers, which irrigate and move continuously at the same time (Savva and Frenken, 2002).\nSprinkler irrigation efficiency is highly dependent on climatic conditions. FAO (1982) proposed the figures of farm irrigation efficiencies provided in Table 1 on the basis of climate.\nTable 1: Farm irrigation efficiencies for Sprinkler Irrigation in different climates (the overall efficiency comprises conveyance efficiency, field canal efficiency, and field application efficiency)\n|Climate/Temperature||Farm irrigation efficiency|\nSource: adapted from FAO (1982)\nSprinkler irrigation technology can support farmers to adapt to climate change by making more efficient use of their water supply. This is particularly appropriate where there is (or is expected to be) limited or irregular water supply for agricultural use. The sprinkler technology uses less water than irrigation by gravity, and provides a more even application of water to the cultivated plot. Additionally, sprinkler irrigation can reduce the risk of crops freezing due to colder than usual temperatures. More frequent and intense frosts are already impacting on crops as a result of climate change. During the night, the motion of the sprinklers and the application of rain-like water droplets can reduce the stress on crops caused by a sharp decrease in temperature (Snyder and Melo-Abreu, 2005).\nOne of the main advantages of the sprinkler irrigation technology is more efficient use of water for irrigation in agriculture. Sprinkler systems eliminate water conveyance channels, thereby reducing water loss. Water is also distributed more evenly across crops helping to avoid wastage. The sprinkler irrigation system has also been shown to increased crop yields (Table 2) and is suited for most row, field and tree crops that are grown closely together, such as cereals, pulses, wheat, sugarcane, groundnut, cotton, vegetables, fruits, flowers, spices and condiments (Narayanmoorthy, no date) and for cultivating paddy crop (Kundu et al, 1998).\nTable 2: Response of different crops to Sprinkler Irrigation Systems\n|Water saving %||Yield increase %|\nSource: adapted from INCID (1998)\nSprinkler irrigation technology is well adapted to a range of topographies and is suitable in all types of soil, except heavy clay. Sprinkler systems can be installed as either permanent or mobile fixtures. Sprinklers provide a more even application of water to agricultural land, promoting steady crop growth. Likewise, soluble fertilisers can be channelled through the system for easy and even application. The risk of soil erosion can be reduced because the sprinkler system limits soil disturbance, which can occur when using irrigation by gravity. In addition, sprinkler irrigation can provide additional protection for plants against freezing at low temperatures. Secondary benefits from improved crop productivity include income generation, employment opportunities and food security.\nThe main disadvantages associated with sprinkler systems are related to climatic conditions, water resources and cost. Even moderate winds can seriously reduce the effectiveness of sprinkler systems by altering the distribution pattern of the water droplets. Likewise, when operating under high temperatures, water can evaporate at a fast rate reducing the effectiveness of the irrigation. Although sprinkler irrigation can help farmers to use water resources more efficiently, this technology relies on a clean source of water and therefore may not be suited to areas where rainfall is becoming less predictable. Implementation costs are higher than that of gravity-fed irrigation systems and large labour force is needed to move pipes and sprinklers in a non-permanent system. In some places such labour may not be available and may also be costly. Mechanised sprinkler irrigation systems have a relatively high energy demand (Savva and Frenken, 2002).\nThe cost of installing a sprinkler system suitable for a family production unit ranges from US$ 600 to US$ 2500 per hectare, depending on the type of materials used and the amount of labour contributed by rural producers. Affordable Micro Irrigation Technologies (AMITs) are low cost and low pressure systems with the same technical advantages as conventional micro-irrigation system, however the technology is packaged and marketed as kits suitable for small fields (25 m2 to 4000 m2). The AMIT has the specific advantage of being affordable, and easy to understand; they also have rapid pay back, divisibility and expandability.\nWhen planning to install a sprinkler irrigation system, information should be obtained regarding the following key factors:\n- The crop or crops to be cultivated and their water requirements throughout the growing season\n- The shape and size of the field. This will determine the range of suitable technologies, investment and labour requirements\n- Topography, in particular the location and elevation of the water source relative to the field, land slopes and uniformity\n- The water source. The source of irrigation water can be surface water, groundwater or non-conventional water (such as desalinated water and treated wastewater) (Savva and Frenken, 2002). Water must be available in sufficient quantity from a locally accessible source. A clean supply of water free of sediment is required to avoid blockage in sprinkler nozzles and crop spoilage (FAO, 1988)\n- Available labour force. Where skilled labourers are not available on location, local farmers will require training to install, maintain and repair the various components of the sprinkler system\n- The soil profile. Sprinkler irrigation technology is best suited to soils with high infiltration rates so that ponding and surface runoff can be avoided. The application rate of the sprinkler system must therefore be matched to the infiltration rate of the most restrictive soil in the field.\n- Energy requirements of different systems, including the manufacturing, transportation and installation of the various systems. The location of the water source will also affect the need for energy for pumping (Savva and Frenken, 2002)\n- Social aspects such as local preferences, capacity to maintain the system, implications for labour requirements and how these may affect different members of the community (Savva and Frenken, 2002)\n- An understanding of existing health risks is crucial to avoid schemes that may promote water borne diseases (Savva and Frenken, 2002)\n- An environmental impact assessment should be conducted to fully understand potential impacts of drainage and diverting water resources, amongst others (Savva and Frenken, 2002).\nMaintenance of the system mainly relates to regular cleaning of the component parts. Seals on pipes and sprinkler nozzles should be checked to avoid water seepage. During periods when the equipment is not being used, it is recommended to store component parts in a cool, dark place.\nAccording to Savva and Frenken (2002), a whole range of institutional conditions must be understood before sprinkler irrigation technology selection can be made. These include land tenure issues, water rights, and financial incentives by government and taxation. Large-scale irrigation schemes will usually form part of national policy and could be harnessed to support national employment initiatives. Where the sprinkler irrigation type is not available nationally, foreign imports or government-supported stimulation of national manufacture will be required alongside investment in training for design, installation and maintenance. Coordination with public or private authorities in charge of water management will be crucial and could be facilitated through the establishment of a committee of irrigation users. At a local level, social organisation for the participatory monitoring of water resources and quality could provide a key monitoring tool. Whichever method is selected, developing regulations for the distribution and allocation of water would provide an important mechanism for conflict resolution.\nWhether a large or small-scale intervention, farmer involvement in the development stages of a sprinkler irrigation project is recommended to help ensure social acceptance and technical success (Box 1).\nBox 1: Sprinkler irrigation in Zimbabwe\n“The Hama Mavhaire irrigation scheme in Zimbabwe is a 96 hectare drag-hose sprinkler irrigation project. The scheme is apportioned equally to 96 farmers, of which 70 per cent are women. It is located in a dry agro-ecological area that receives about 450 mm of rainfall per year. Dryland cropping fails 3 to 4 years out of 5. The development of the scheme was initiated in 1989, following strong farmer requests to the government for irrigation development.\nParticipation of Farmers in Planning and Design\nThe government dispatched a team of experts, comprising engineers, agronomists and economists, to the project site to carry out a feasibility study. Several meetings were held in order for planners to understand the farmers’ expectations and to explain to the farmers the potential of and requirements for the proposed development. This was followed by a baseline socio-economic survey. The land chosen consisted of about 80 per cent of non-cultivated bush, while the remaining 20 per cent was arable land owned by the farmers who were selected for the scheme. The farmer group was to be the partner in irrigation development. It elected its own committee, which was tasked with liaising with the planners on all matters related to the new development.\nTo facilitate a process of making informed decisions, arrangements were made for farmers to visit different types of irrigation systems, surface and sprinkler. This exposure proved useful to farmers when they eventually decided on the type of irrigation system they preferred and the crops to be grown. This process took one full year.\nParticipation of Farmers in Construction\nWhen the design was adopted, tender documents were written to include the condition that the farmers would provide all unskilled labour required for construction. During construction the group provided labour for trenching and back-filling and assisted pipe fitters by carrying and placing pipes and fittings in position. As a result of their participation, the farmers were trained in pipefitting and other general repairs to their system. Additionally, the contractor trained one farmer per irrigation block on the repair of sprinklers. The irrigation engineers and extension staff trained the farmers in leadership, bookkeeping, scheme operation, improved agronomic practices and irrigation scheduling. This process took six months for the first 48 hectares and three months for the remaining 48 hectares.\nSocio-economic Impact of Scheme Development\nOn average, the net income per plot-holder quadrupled due to the introduction of irrigation, from a gross margin assessed at US$ 650 annually on 2.5 hectares of dryland crop production to a gross margin of US$ 2,775 for one hectare irrigated. There are other indicators of a substantial rise in the standard of living of the irrigators. About 29 per cent of the plot-holders are reported to have purchased between one and four head of cattle from the income earned through irrigation during the first five to six years of scheme operation. In addition, 13 per cent of the plot-holders had put up brick under corrugated iron houses and 10% had installed solar panels during the same period. Women, who constitute the majority of the plot-holders and are represented at all committees, also confirmed that the other major benefit of irrigation was that they are able to pay for the costs of educating their children.\nThe success of the Hama Mavhaire irrigation scheme is largely attributed to the participatory approaches adopted for the development of the scheme provided the opportunity to the group, planners and implementers to jointly plan and implement a scheme, making it both technically feasible and socially acceptable.”Source: Savva and Frenken, 2002\nPossible barriers to implementation include lack of access to finance for the purchase of equipment, lack of local skills for design, installation and maintenance of the system and lack of nationally/locally available component parts. A low level of public awareness of or concern for the importance of sustainable water management and use could also be a barrier to the exploration of sprinkler irrigation technology as a climate change adaptation option.\nSprinkler irrigation requires a suitable source of fresh water to be identified in close enough proximity to the farmland. This ensures that costs are kept at a reasonable level. Water availability will be highly dependent not only on current resources but also on future climate conditions. Where knowledge of potential climate change impacts on water resources does not exist, installing a sprinkler irrigation system could lead to conflicts over local water use.\nSprinkler irrigation is a versatile technology suitable for application in a wide range of contexts, can be implemented at small or large scale and with either low-cost or more sophisticated components. This technology can be employed in conjunction with other adaptation measures such as the establishment of water user boards, multi-cropping and fertiliser management.\nFAO (1982) Mechanised sprinkler irrigation. FAO Irrigation and Drainage Paper No. 35. Rome.\nFAO (1988) Irrigation Water Management: Irrigation methods, FAO. Rome.\nINCID (Indian National Committee on Irrigation and Drainage) ( 1998) Sprinkler Irrigation in India. INCID, New Delhi.\nKulkarni, S.A., F.B. Reinders and F. Ligetvari (2006) Global Scenario of Sprinkler in Micro-Irrigated Areas. Sept 10 – 16 2006, PWTC, Kuala Lumpur 7th International Micro Irrigation Congress\nKundu, D. K., H. U. Neue, R. Singh (1998) Comparative Effects of Flooding and Sprinkler Irrigation on Growth and Mineral Composition of Rice in an Alfisol. Proceedings of the National Seminar on Micro- Irrigation Research in India: Status and Perspective for the 21st Century. Bhubaneswar, July 27-28.\nNarayanmoorthy, A. (no date) Drip and Sprinkler Irrigation in India: Benefits, Potential and Future Directions. Available: http://www.iwmi.cgiar.org/Publications/Other/PDF/Paper%2015%20of%20NRLP%...\nSavva, A. P. and K. Frenken (2002) Irrigation Manual Planning, Development Monitoring and Evaluation of Irrigated Agriculture with Farmer Participation. Volume I Modules 1 – 6.\nSnyder, R. L. and J. P. Melo-Abreu (2005) Frost protection: fundamentals, practice, and economics – Volume 1. FAO, Rome.', 'This Guide will teach you everything you need to know about Drip Irrigation.\nComponents Of Drip irrigation system\nCost of installation\nAnd Guidelines For Maintenance and much other valuable information that I’ve never shared anywhere else before.\nLet’s get started…\nThe available water for agriculture decreases day by day due to the increase in population, industrialization, and short rainfall. It has become essential to use modern irrigation technologies like Drip irrigation, sprinkler irrigation in agriculture.\nDrip irrigation means providing the required quantity of water directly to Crop plants’ root zone through a network of small pipes; this is also called micro-irrigation or trickle irrigation.\nThis is the most efficient irrigation technique.\nIn the Drip irrigation system, water supplies to plant roots through a collection of plastic pipes, lateral tubes, and valves. These components, controlled with the help of a dripper and water pump. With the help drip irrigation system, it is easy to provide liquid fertilizer to the plant root system.\nDrip Irrigation Advantages\n- Drip irrigation Saves water around 30% – 60% as compared to flood irrigation.\n- It is observed that crop yield increase up to 230%.\n- Fertilizer use efficiency increase by 30 percent\n- Reduce Weed growth\n- Save Labour and electricity cost is much significantly less than other irrigation methods.\n- For drip irrigation, Field leveling isn’t vital.\n- Easy to maintain Moisture near the root zone\n- Water supply is extremely optional, controlled by way of each nozzle.\n- Help to reduce Soil erosion.\n- Enable the use of saline water for irrigation\nComponents Of Drip irrigation system\nThe drip irrigation system comprises the water pump, filter unit, mainline, sub-mainline, lateral pipes, dripper, and other accessories such as control valves, pressure gauge, Fertilizer tank/venturi, end cap, etc.\n1) Water Pump\nA pump of a suitable capacity Water pump is used to supply water through the drip irrigation system components at a specific level of pressure.\nIf the source of a water supply is a bore well, open well, or a Canal, there is the possibility of organic and inorganic foreign bodies in the water. In this case, use the suction filter to get relatively clean water.\nThe electric motors or diesel engines are the common prime mover of the pump. Recently the solar pump is being used to popularize it for drip irrigation purposes.\n2) Filter unit\nThere should be a good quality filter in the control head installation of a drip system. The filter uses to clean the suspended impurities of water supplied by the pump before it reaches to drippers. Impurities in irrigation water may cause blockage of the holes and passage of drippers.\nThe success of drippers greatly depends on the performance of the filter.\nA filter unit cleans irrigation water impurities that are suspended and prevent blockage of holes. There is various type of filter available in the market.\nThere are Three Types of filters: Hydrocyclone Filter, Media filter, and Screen or Disk filter.\nSuitable filters are installed according to the impurities found in the source of water.\nIf the water source is a bore well with less physical impurities, you can install only a screen filter.\nIf the water source is an open well or a Canal install screen or disc filter along with sand filter and Hydrocyclone Filter.\nTo work the drip irrigation system properly two-stage filter unit is used.\na) Media (Sand/ Garval) Filters\nThese filters come as the primary filter unit, and this is powerful against inorganic suspended solids, biological materials, and other organic substances.\nMedia Filter consists of fine gravel and sand of selected sizes placed in a pressurized tank. It helps to remove organic substances such as algae and other vegetative substances present in the water.\nThe filters are made up of a circular tank filled with layers of coarse sand and different gravel sizes with a provision of valves or flushing the filter assembly in case of clogging.\nThe media filters are available in different sizes ranging from 500 to 900 mm diameter with an output of 15 to 50 Cu.M., respectively.\nA gravel filter or sand filter is vital for the open reservoir, even where algae growth occurs in the water source.\nb) Hydrocyclone Filter\nIf the irrigation water contains more sand particles, hydro-cyclone type filters are removed this sand particle by creating centrifugal force and spins out sand from water.\nThe Hydro-cyclones Filter generates a spinning activity causing the sand particles to separate the water and be trapped in a storage tank at the bottom of this device.\nHydro-cyclone type filters are produced in various sizes for different discharge capacities.\nc) Screen Filter:\nIn general, the screen filter consists of a single or double perforated cylinder placed in a plastic or metallic container\nfor removing the impurities.\nGenerally, 100 to 200 mesh screens are used in this type of filter. It must be cleaned and inspected periodically for satisfactory operation of any drip system.\nScreen filer is installed with or without a gravel filter, depend upon the quality of irrigation water. The screen filter is made up of non-corrosive plastic or metal substance.\nc) Disc Filter:\nDisc filter comes as part of the secondary filter unit. Multiple circular discs filter water. it is made up of high-quality plastic material\nThe mainline transfers the total amount of water for the irrigation system. It connects the different sub-mains to a water source. The main pipes are commonly made of flexible materials such as PVC (polyvinyl chloride) or plastics.\nThe mainline pipe passes water from the filtration unit to the sub-main pipe. This pipe’s diameter depends upon drips irrigation system flow capacity, normally 2.5 – 4 Inch diameter PVC pipe used as main-line.\nMainline & sub-main should be installed in a Telescopic manner; that is, the pipe with a larger diameter should be connected first, followed by pipes with a smaller diameter. This arrangement helps to maintain uniform pressure in the system.\nThe Mainline should be buried at least 45 centimeters to prevent them from getting damaged during cultural operation.\nThe sub-main feed to the laterals on one or both sides. It is made of either medium-density polyethylene (PE) or PVC. There should be a balance between the diameter of the main and sub-mains.\nThese are determined in consideration of the rate of discharge, number of sub-mains, and friction losses in pipes\nLaterals are made up of low-density polyethylene (LDP) or linear low-density polyethylene (LLDPE) material and are available in different sizes, 12 mm, 16mm, and 20 mm.\nBased on the availability of water, crop, and spacing, 12 millimeter 16-millimeter laterals install.\nDrippers are also called emitters. The dripper is discharging water from the lateral pipe to the soil.\nDrippers are generally manufactured from polypropylene materials.\nMainly, two types of drippers are available in the market. Online Dripper and Inline Dripper\na) Online Dripper:\nIn this type of dripper, they are placed on the lateral, called an online dripper. these drippers are fixed on the lateral by punching suitable size holes in the pipe\nThis type of dripper uses mostly for Orchard Crops like coconut, pomegranate, guava, etc., the capacity to discharge water available is 2L/hour, 4L/hour, and 8L /hour.\nb) Inline Dripper:\nIn this type, the dripper is placed inside the lateral pipe. The distance between the two drippers is the same. There are three types of available inline drippers.\nNon-Pressure Compensating Drippers (NPC): This is a very simple dripper, which does not maintain uniform pressure\nPressure Compensating Drippers (PC): This dripper is more advanced; it maintains uniform pressure in all drippers. It mostly uses for the greenhouse crop, the price of this dripper is slightly high.\nNon-Draining Drippers: This dripper is mainly used in a soilless-like mediums like cocopits, Perlite, and Vermiculite.\n7) Fertilizing Unit\nThe direct application of fertilizer through drip irrigation has increased the efficient use of fertilizer and saving in labour and money.\nWith this fertilizing unit’s help, liquid fertilizer is provided to the plant through a drip irrigation system.\nApplication of fertilizer into irrigation system is made by either a by-pass pressure tank or by venture pump or direct\nIt is used to determine water pressure in the drip irrigation system.\n8) Controls Valves\nThis value is used to control water flow. They made up of plastic and iron material\n9) Flush Valve:\nThe flush valve is placed at the end of the sub-Maine pipe it uses to flush out dirt.\n10) Non-Return Valve\nNon -return valve is useful to stop return water toward the water pump.\n11) Air valve\nIt helps prevent the sucking of mud by the drippers and release air into the drip irrigation system.\nThe end cap is used to close one end of the lateral pipe; they remove it at the cleaning time.\nTypes of Drip Irrigation System\nTheir many types are available in drip irrigation here. Only two popular types are explained.\n1) Surface drip system\nIn the surface drip system, the emitter and lateral pipe are placed on the soil surface. This is the most common and popular type of drip system.\nIt is suitable for wide-spaced plants as well as for row crops. It is easy to use a surface drip to observe and inspect, change, clean the emitters, observe surface moisture patterns, and measure individual emitter discharge rates.\n2) Sub-surface drip system\nin a subsurface drip irrigation system, the lateral is placed below the ground and near the plant root zone area. In this system, water is slowly applied below the surface through the emitters.\nSub-surface drip systems have gained wider acceptance due to removing earlier problems of clogging to a large extent.\nDue to the sub-surface drip method, there is less intervention in agriculture or any cultural practices and possibly more operational life.\nSub-surface drip system provides little interference with cultivation or any cultural practices and possibly longer operational life.\nDrip Irrigation System suitable for this crop\n|Vegetables||Tomato, Capsicum, Cabbage, Chilly,|\nCauliflower, Onion, Brinjal, Bitter Gourd,\nRidge Gourd, Peas, Cucumber, Pumpkin,\n|Cash Crops||Sugarcane, Tobacco, Cotton|\n|Poyhouse crop||Gerbera, Dutch rose, Carnation, Anthurium,|\nLily, Orchids, Strawberry, etc\n|Plantation Crop||Coffee, Coconut, Tea, Rubber, etc|\n|Orchard Crops||Banana, Grapes, Citrus, Orange,|\nPomegranate, Mango, Guava,\nPineapple, Cashewnut, Coconut,\nPapaya, Watermelon, Muskmelon,\nLitchi, Lemon, etc\nDrip Irrigation System Cost\nThe Cost of drip irrigation system installation depends upon various factors, like what kind of crop you are sowing, type of terrain, quality of soil, sowing pattern, water quality, quality of drip material, manufacture company of drip irrigation system, and design of the drip irrigation system.\nDrip irrigation system cost per acre For vegetable crop will come around Rs. 50,000–65,000 per acre approximately and For fruits crop, if planted in the 3X3 pattern cost per acre for drip irrigation system come around 35,000- 40,000 approximately.\nGovernment Suggestive cost of drip irrigation system:\nSuppose you are using non-ISI material; your initial cost for one acre comes around 20,000-25,000 rupee for the vegetable crop, but the life of non-ISI material for 2-3 years with high maintenance cost. At the same time, the life of ISI material is 7-10 years with low maintenance.\nGovernment subsidy for drip irrigation\nThe subsidy is available for Drip irrigation in India under Pradhan Mantri Krishi Sinchayee Yojana (PMKSY). The government of India gives subsidies per beneficiary up to 5 hectares. There has been financial assistance under different categories and depending upon States’ categories for dessert, drought, hilly, and other regions of the country. For more detail, check the PMKSY website or contact the nearest agriculture office.\nList of Top Drip irrigation companies\nThese are the Top Drip irrigation manufacturer company in India.\nGuidelines For Maintenance Of Drip irrigation system\nDrip irrigation is a mechanical system, which performs at a specific pressure of water; if the system has to perform well for a long time, the pressure should be properly maintained.\nEven if good quality materials are used and installed scientifically, there is a possibility of failure. If the field maintenance is not proper and regular, therefore, the drip irrigation system’s management is very important.\nFollow these guidelines to maintain drip irrigation system –\nA. General care\n- Check all Working emitters or Dripper, Inspect the water leakage, and make sure that water is uniformly distributing all of the corners of this plot/field.\n- Check the water dispensation area. If their dry patches are found, increase drip irrigation operation time.\n- If a twist, fold, cut, puncher is observed on the lateral, sub-main, valve correct it immediately.\nB. Clean the filters.\nThe filter is the main part of the drip irrigation set. If the filter is not working properly, there is a major chance of complete drip irrigation system Failure.\n1. Sand filter:\nClean sand filter after every week. The backwashing facility available in the sand filter uses this by allowing water to flow through the lid instead of the water valve and stir the sand in the filter. So, the waste which is sitting at the bottom will come and go out of the water\n2. Screen Filters/ dice filter:\nOpen the lid and remove the mud and garbage trapped. Open the filter and remove the filings and rubber seals and clean it on both sides.\n3. Submain pipe and Laterals tubes:\nSometimes a fine particle, dirt passes through the filter and store in the main and sub-main tubs, Laterals; therefore, to clear these tubes, remove the end cap, Flush Valve and let the water flow. Do this until the clean water come.\nC. Chemical Treatment:\nThe drippers are stopping working due to various alkali\nIf the amount of carbonate, bicarbonate, chlorides, sulfur, manganese, and calcium, sulfur sulfate is higher in water, yellowish dots appear, and the reddish dots appear dripper, and if the amount of iron is high red spot appear on the dripper following treatment carried out.\n1. Acid Treatment:\nAcid treatment is useful for though dripper and laterals blocked by various chemical impurities, including fertilizer residue. This impurity can be removed by treating the system either with hydrochloric acid or sulphuric acid or nitric acid. This hydrochloric acid at 25% is best for acid treatment.\nTake the required amount of hydrochloric acid in water. Inject it into the system through the Venturi or fertigation tank. The system is filled with water allows acid solution into the system until the pH of 4. A check pH with litmus paper for both at the starting and the last dripper. Close the system for 24 hours.\nThe acidified water in the system reacts with the salts deposited in the system and dissolves it. After 24 hours, they acidify water along with the dissolved solids coming out of the system by flashing.\n2. Chlorine treatment:\nChlorination is useful to remove the biological impurities collected in the drip irrigation system. Chlorination can be done either using calcium hypochlorite, sodium hypochlorite, chlorine or calcium hydrochloride, or bleaching powder.\nDissolve the required quantity of bleaching powder in water one day before treatment. This solution is let into the system through Venturi or fertigation tank and allows it to stay in the system for 24 hours. after that, open the end caps of the lateral and run the system for about an hour so that the impurities are thrown out of the system\nDrip Irrigation Disadvantages/ Limitation\nDespite observed successes, some problems have been encountered in applying water with drip equipment for some soils, water qualities, and environmental conditions. Some important limitations are described below:\n1) Persistent Maintenance Requirements\nDripper clogging is considered the most serious problem in Drip irrigation unless preventive measures are taken. It is, therefore, necessary that water should be filtered properly.\nBesides this, salts and chemical deposits can be deposited in an emitter or lateral pipe. This leads to Clogging. It will adversely affect the rate and uniformity of water application, increase maintenance costs, and result in crop damage and decreased yield if not detected early and corrected timely.\nOther maintenance problems include pipeline leaks and cracking of the tubes. Rodents, coyotes, rabbits, and dogs can chew and damage dripline, and ants and other insects occasionally have enlarged openings in drip tubings.\n2) Economical – Initial cost\nAs the equipment requirements are numerous with drip irrigation, initial investment and annual costs may be high compared to surface or portable sprinkler irrigation systems.\nThe actual prices of drip irrigation system equipment will vary greatly depending on the types of crops, grades of pipelines, filtration equipment, and fertilization equipment, etc.,\n3) Salinity Hazards\nIf the drip system is used under saline conditions, it should be taken extra care to manage drip irrigation work properly.\n4) Technical knowledge\nHigh skill is required for drip irrigation system design, installation, and subsequent operation.\nTechnical improvements in the designs of emitters, fittings, filters, etc., have been necessary;\nThe development procedures for preventing or correcting emitter clogging and equipment failure have been difficult, and the development of the proper methods for injection of fertilizers and other chemicals has sometimes been a\nA higher level of design management and maintenance is required with drip than other irrigation methods.\nImage source :']	['<urn:uuid:937d4deb-a6e2-471c-b5a4-b6b79d000177>', '<urn:uuid:985313b8-574b-4533-abaa-a71d7519e156>']	open-ended	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-12T18:38:36.516973	13	136	5451
70	How do the engagement expectations differ between mindfulness therapy for anxiety and building school belonging - does one require more active participation than the other?	Building school belonging requires more active participation, as it involves students engaging in classroom activities, participating in extracurricular activities, socializing with peers, and interacting with teachers. In contrast, mindfulness therapy for anxiety focuses more on passive observation and awareness, teaching clients to notice their thoughts and feelings without necessarily taking action, primarily through meditation and breathing exercises.	"['What Goes Into School Belonging?\nTo feel a sense of belonging at school, children should believe that they’re supported, respected, and accepted by their teachers and peers. They should feel connected to the school community and believe that their teachers care about them both as individuals and as learners.\nWhat are the Benefits?\nChildren who experience school belonging will benefit in many ways, such as:\n- More likely to stay engaged in schoolwork\n- Better academic performance\n- Feeling happy and self-confident\n- Less likely to experience loneliness, anxiety, and depression\n- Less likely to engage in risky behaviors (like substance abuse or violence)\n6 Ways Teachers Can Help\nTeachers play a huge role in helping children foster a sense of school belonging. Research has shown that when teachers demonstrate that they’re caring and supportive, children feel more connected to the school.\n- Connect individually. Taking time to work one-on-one or by discussing a child’s personal interests helps. This can just be a quick greeting, remembering something that they mentioned in class or asking about something they’ve shared. We have classroom conversation starters that may help you get to know your students.\n- Encourage peer connections. Children who feel close with classmates feel more engaged at school and are more likely to believe that they belong. Grow stronger peer relationships in your classroom by giving students opportunities to learn about each other during certain lessons and by letting students work together on some class activities.\n- Empower students. Students who have a say in how they spend parts of the school day are more likely to have a greater sense of school belonging, and they’re less likely to be disruptive during class. Consider seeking student input while creating classroom rules or when deciding what topics to focus on during certain lessons.\n- Be aware of social dynamics. Kids have a greater sense of school belonging when their teachers know who their friends are and which students need social support to avoid bullying or social isolation.\n- Share positive expectations. Let students know you believe that they’ll succeed academically. When teachers expect them to excel, students are more likely to feel that they belong.\n- Set clear expectations. Outline acceptable classroom behaviors, explain the consequences of breaking rules, and then discipline students consistently, without playing favorites. The sense of fairness helps students feel connected to their school community.\nHow Can Parents Help?\nParents can also bolster a sense of school belonging. Take an interest in their life at school by having conversations. You might ask:\n- What are you learning at school?\n- Who do you spend time with at school? What kinds of things do you do together?\n- What do you like about your teacher?\n- What do you wish your teacher knew about you?\nYou can also help your child grow connected to classmates by encouraging your children to participate in extracurricular activities with peers and helping them socialize with friends after school. Parents can also print the school calendar and attend school events and activities, when possible, to show their kids they are interested in their school environment.', 'March/April 2017 Issue\nBe Here Now: Easing Anxiety With Mindfulness-Based Therapies\nSocial workers are using several mindfulness-based approaches to teach clients to be with their anxiety, notice it, and move toward reducing it.\nStop and notice. How many thoughts are cascading through your mind right now? Maybe you\'re trying to catch up on professional journals but can\'t stop thinking about a meeting you had earlier with your boss. You can\'t shake the sense that she was disappointed in your performance, and you\'re struck by the impression that she talks to you in the same dismissive way your father did when you didn\'t meet his expectations. Before you can think that through, it dawns on you that you forgot (again!) to defrost the roast for tonight\'s dinner with your husband\'s business partner and his wife. You\'ll have to stop for take-out, so you won\'t get home until after they\'ve arrived. Your guests will feel awkward, your children will be cranky, and your husband will be embarrassed. No wonder that tension is moving from your shoulders up your neck. In just a moment or two, you\'ve lost track of what you started out to do (read an article) and have spiraled into stress, self-doubt, and self-recrimination. If only you had a moment to stop the thoughts. Now, imagine how moments like this may feel to your clients with anxiety disorders.\nFor people overwhelmed by stress and for those who suffer from anxiety disorders, the present moment may be relatively unfamiliar territory. With minds that alternately fixate on the past and ruminate about the future, they likely find the notion of living in the present an impossibility, a Catch-22. If they could remain in the moment, their symptoms might be relieved, but it\'s their symptoms that stand in the way of being in the present moment. Relief might be possible, they may think, if only the past and the future could no longer dominate their thoughts and trigger the symptoms of anxiety. According to experts, however, not only can they learn to ""be here now,"" but evidence suggests doing so can significantly alleviate their anxiety. Training in these practices can also be helpful self-care practices for social workers to mitigate daily stresses and help combat compassion fatigue.\nWhat Is Mindfulness?\n""From a theoretical perspective, mindfulness is about staying in the present moment,"" says Courtney Miller, MSW, LICSW, a clinical social worker in Mountlake Terrace, WA. ""People who struggle with anxiety often worry about the past or the future. Staying in the present moment may alleviate some of these symptoms because people are not focused on the worries that cause anxiety."" From a scientific perspective, she says, mindfulness-based approaches have been shown in several meta-analyses to reduce the symptoms of anxiety as well as other strategies, such as cognitive-based approaches.\nWhile it may be all that some patients need, mindfulness-based therapy alone can\'t always quell the symptoms of anxiety; some clients also will need medication. ""Mindfulness often can stand on its own,"" Miller says, ""but it depends on the person, their treatment preferences, and what their doctor recommends.""\nWhen people have anxiety, ""it\'s so painful that they do their best to avoid it,"" says Kate Kitchen, MSW, RSW, a private practitioner and teacher of mindfulness-based group practice at Sunnybrook Hospital in Toronto, Ontario, Canada. Thus, she says, they never get to figure out how to take care of themselves when anxiety arises. What mindfulness does, she says, is allow people to notice anxiety and sit with it, learning to realize that it\'s not as devastating as it feels. A client\'s heart rate may go up and they may have trouble catching their breath. ""People with anxiety often say that the symptoms feel as bad as if they were having a heart attack,"" she says. They learn how to breathe into it, to notice what it feels like to breathe, and come to see that anxiety doesn\'t have to control them."" Kitchen observes that results can come quickly, as early as the midpoint of an eight-week program. Annabella Hagen, MSW, LCSW, a private practitioner in Provo, UT, agrees. She typically uses acceptance and commitment therapy (ACT) and doesn\'t introduce a mindfulness component until the fourth or fifth session, but notices significant progress by the sixth session. ""Once people notice their anxiety reducing,"" Kitchen says, ""mindfulness begins to creep into their everyday lives and they find ways to apply it that even I hadn\'t thought of.""\nTherapists work against the common misperceptions, even held sometimes by other therapists, that the purpose of mindfulness is relaxation and that the practice is outcome-oriented. ""That might be a side benefit,"" Hagen says, ""but it\'s not the purpose. It\'s not a Band-Aid approach for anxiety, she explains, but a lifestyle that happens to benefit people with anxiety. ""It\'s something clients need to learn to use for the rest of their lives, not just for anxiety but [also] to learn to see their thoughts and not judge everything that\'s happening in the present moment."" Being mindful, she adds, means you\'re going to ""be in the present moment, be curious, and see what happens.""\nRange of Mindfulness-Based Therapy\nMSBR, devised to help manage pain, chronic health conditions, and anxiety, is an eight-week training program that includes guided instruction in mindful yoga and mindfulness meditation practices. Although there are many ways to practice meditation, each method requires the practitioner to assume a comfortable position (which might be sitting, walking, or reclining); adopt a nonjudgmental, open attitude; and use one of several methods to focus awareness, for example a mantra or attention to the physical sensations of breathing.\nMBCT yokes aspects of MBSR with cognitive therapy strategies to help individuals detach from negative thought processes. Developed by Zindel Segal, PhD; John Teasdale, PhD; and Mark Williams, DPhil, DSc, FMedSci, FBA, it\'s primarily used in the treatment of relapse of depression, but research suggests it\'s effective against anxiety and numerous other disorders as well.\nMSC, like MBSR, is an eight-week program with exercises and discussion that help participants increase self-kindness, reduce crippling self-criticism, cope with suffering and difficult emotions, enhance resilience, and integrate these skills into daily living. Research suggests that self-compassionate individuals experience less anxiety and reduced rumination.\nMindfulness may be incorporated into still more therapeutic modalities for clients with anxiety. Miller observes that approaches such as ACT or dialectical behavior therapy (DBT) also incorporate mindfulness into their approaches to treating anxiety. ACT doesn\'t aim for clients to eliminate negative emotions but rather to change behavior in accordance with their values and in the presence of their difficult feelings. Thus, mindfulness, with its emphasis on nonjudgmental awareness, dovetails well with ACT. DBT, developed by Marsha Linehan, PhD, and originally used as an intervention for suicidal patients, merges cognitive behavioral strategies with mindfulness, which is one of four core skills emphasized in DBT, the others being distress tolerance, interpersonal effectiveness, and emotional regulation.\nFurthermore, Miller says, ""I often use mindfulness in conjunction with eye movement desensitization and reprocessing (EMDR). In EMDR, I begin by teaching skills that increase a client\'s ability to stay grounded and to feel safe. It\'s important that clients have these skills to reground themselves if they get overwhelmed in session or to cope with memories that may arise outside of session. I will teach skills such as mindfulness of breath as a grounding practice.""\nAccording to Miller, ""Mindfulness-based approaches have been shown to be equally effective as cognitive-based approaches, so people may get the same results when choosing between either of these approaches."" What\'s more, the various mindfulness-based therapies are complementary; each may offer something different, so clients may benefit from engaging in more than one type of therapy.\n""Nothing is really pure anymore,"" Kitchen says. ""You\'re putting together and learning from all these approaches."" It\'s a strategy, she says, that fits with social work values. ""We think about how to start where the clients are and what to use that is helpful to them. I use whatever works, incorporating what might be helpful for each individual.""\nThere\'s lots of research, Hagen says, ""but the evidence is when you see what happens with your clients.""\nKitchen draws a line at using mindfulness with individuals who self-harm, regardless of their diagnosis. ""I feel they should not be learning mindfulness until they\'re more stable, because in fact one\'s anxiety goes up before it goes down, and that would be true of whatever is making them feel so bad that they have to harm themselves to feel better."" For anyone who is actively suicidal, she says, ""I would want them to do other things first to help them feel safe.""\nHagen believes it\'s less black and white. Although she agrees that she wouldn\'t use a ""be with the feeling"" approach with a client who is suicidal, she wouldn\'t rule out mindfulness training for all clients who self-harm—for example, for teenagers who cut. Therapists, she suggests, must be client-centered and determine when and if individuals are good candidates for mindfulness-based therapy. It may not be appropriate for some clients at the beginning of therapy, she says, but eventually may be useful.\nSocial Workers Getting Involved\nNot for Clients Only\n— Kate Jackson is an editor and freelance writer based in Milford, PA, and a frequent contributor to Social Work Today.\nRESOURCES AND TRAINING\nCenter for Mindful Self-Compassion\nMindfulness-Based Training Institute\nAssociation for Contextual Behavioral Science\nThe Omega Institute']"	['<urn:uuid:eab80c00-71af-489d-9e7e-ff6a3703020e>', '<urn:uuid:15337894-56a6-46d7-94c3-e7319c0c0714>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T18:38:36.516973	25	57	2084
71	modern aviation education facilities requirements costs considerations versus traditional pilot instruction	Modern aviation education involves complex considerations spanning facilities and costs. The STEM Innovation Academy demonstrates how modern facilities integrate advanced technology like drones, with students able to pursue drone pilot certification and earn money right after high school. Regarding costs, flight schools emphasize that helicopter training requires careful financial planning, typically needing more than 50 flight hours despite FAA's 40-hour minimum requirement. Students must consider factors like aircraft availability, instructor scheduling, and school stability. Training in smaller aircraft like the Robinson R-22 can reduce costs compared to larger helicopters like the Bell Jet Ranger, with the option for transition training later.	"['Robert Tolomeo – Broken Arrow Public Schools\n- Written by: Jody Robbins\n- Produced by: Zachary Brann & Nolan Quinn\n- Estimated reading time: 4 mins\nFrom farm animals and drones to striking architecture and on-site input from students, the new STEM facility at Broken Arrow Public Schools in Broken Arrow, Oklahoma, is designed to educate—for life.\nBuilt in the Oklahoma soil and surrounded by 80 acres of usable land, the main building is a split-level design constructed to make the best use of outdoor space. Most of the first floor is buried in a hill, while the second floor juts off at an angle, overhanging a long boulevard-style open-air space.\nAccording to Bob Tolomeo, the district’s executive director of construction services, the facility will house programs ranging from vocational agriculture to drone mapping. Tolomeo expects to be moving furniture and farm animals in by the spring of 2021, with students arriving in August.\nThe state-of-the-art STEM facility will serve as a flagship of BAPS and firmly cement the district’s commitment to STEM education.\n“We’re making everything a learning experience, from the construction of the building to ongoing management, and will be continually coming up with fresh ways to innovate and educate,” Tolomeo says. “From building signage to greenhouses, we’re looking to stretch the physical and mental boundaries of education with this facility.”\nBIM to STEM\nThere are a lot of moving parts to what BAPS is currently calling the STEM Innovation Academy, Oklahoma’s first public school facility to integrate building information modeling from the design process all the way through construction and as-builts.\nKnown as BIM, the tool enhances coordination and reduces errors by creating a holistic picture of the facility—helpful on a property that will use the latest in tech and for students looking to understand the process behind building and managing a large-scale facility.\nTolomeo is accustomed to managing multiple projects, so it’s all in a day’s work—though there’s plenty to set the SIA apart. This includes the dramatic and environmentally responsible design, the integration of drone piloting into the curriculum and a well-designed vocational agriculture facility.\nIn addition to agriculture, students will learn about architecture, construction, engineering, life science and much more. Each student determines the scope of their studies. The goal, he says, is to give students a voice in their own academic and professional destiny.\n“Technology is always changing and so is the job market, so you have to trust the people around you,” Tolomeo says. “In this case that includes district administration, teachers, students, contractors and the list goes on. If this becomes just a building, I’ve failed. This project is the underpinning of a new kind of community within BAPS.”\nThe facility is part of a twelve-year bond issue to drive innovation across the district, including renewable energy initiatives and other learning experiences for 19,000-plus students. The bond has also funded projects at other schools in the district, like wind turbines and solar panels at a new elementary school.\nLet loose the drones\nClasses at SIA will be part of the district’s high school programming, with students shuttling between the new facility and the high school.\n“Project-based learning is a boon for some students,” Tolomeo says. “This facility will give students the ability to chart their own educational course to achieve their professional goals sooner rather than later.”\nOne example of the interplay between the facility, students and future careers: an on-staff drone pilot has been mapping the property down to the last water pipe while also creating educational content and a drone pilot curriculum.\nStudents, or anyone, will be able to take a virtual tour of the building while wearing a headset. BAPS has partnered with Flintco Construction to train students on the synergy between drone photography and construction work.\n“We’re creating curriculum based on real-world experiences,” Tolomeo says. “If you start flying a drone and get in enough hours and training, you can make money on that straight out of high school.”\nIn addition, young architects, engineers and construction managers will be able to follow their vocation, if they so choose, and get a head start in fields that are becoming increasingly complex and technical.\nAnimal farm of the future\nStill, the school hasn’t strayed far from its agricultural roots. Vocational agriculture has long been a major area of focus in Oklahoma, and SIA is no exception.\nNot only is the new facility only a quarter mile from the high school; the space—which houses three barns for different types of animals—allows students to participate in a working farm.\nStudents will also learn different methods of agricultural management. Teachers can rope off certain pieces of land, for instance, allowing students to study it and make recommendations, be it growing corn or grazing animals.\n“We talked to the ag students and staff and teachers, what they needed and wanted, where the program is going in the next few years,” Tolomeo says. “The architects listened to what we wanted and how we function, then worked with us to design the building around our wants and needs.”\nUltimate project-based project\nPulling off the project took a lot of one specific ingredient: collaboration. For Tolomeo, it’s a continuation of his mission to make construction about more than bricks and mortar.\nA graduate of BAPS himself, Tolomeo went on to gain a bachelor’s degree in construction management technology at Oklahoma State University and started working for the school district in 2011, only a couple of years after college.\nHe went on to earn a Master of Engineering Technology degree from Pittsburg State University and is using his educational experience these days to both give back to his hometown and look forward when it comes to facilities and community.\n“When I left Broken Arrow for college, I didn’t think I was coming back, but then all the pieces fit into place and you realize home is a good place to be,” Tolomeo says. “I face new challenges every day, and that’s exactly why I love my job.”\nShowcase your feature on your website with a custom “As Featured in Blueprint” badge that links directly to your article!\nCopy and paste this script into your page coding (ideally right before the closing', ""NEW! - GI-Bill TrainingFind VA-Approved Schools\nFind Aviation Schools Flight SchoolsAircraft Maintenance TrainingHelicopter SchoolsFlight Dispatcher CoursesAir Traffic Controller SchoolsAviation Management DegreesAvionics Technician TrainingCertified Flight Instructor TrainingFlight Instructor TrainingInternational Aviation SchoolsInstrument Rating CoursesMulti Engine TrainingSeaplane Rating CoursesSport Pilot SchoolsTime Building SchoolsTurbine & Jet Transition CoursesType Rating CoursesUnmanned Aircraft Systems\nHelicopter Flight TrainingFive Things to Consider When Looking at Helicopter Flight Schools\nBy Kyle Garrett\nLearning to fly helicopters is an exciting and fulfilling experience, but decisions made early in your flight training have a noticeable effect on your enjoyment of flight training. In order to have the best possible experience learning to fly helicopters, consider these five things: eligibility requirements, training cost, training aircraft and their availability, instructor availability, and flight school stability.\nAre you eligible?\nThe very first thing you need to know is whether you are eligible for at least a student pilot certificate and what it will take to get your private pilot certificate. Thankfully, the FAA has strict requirements that must be met and they don't keep them a secret. It is important that you understand any possible limitations you may experience early in the process. In general, you must be at least 16 years old, be able to read, speak and understand English, and hold at least a current third-class medical certificate. A medical examination at a qualified aviation medical examiner will provide you with both a medical certificate and student pilot certificate. While a student pilot certificate is not required for training with an instructor, you must have one for solo training flights and it is recommended you see the medical examiner as early in your training as possible in case any complications should arise.\nMore so than fixed wing flight training, the complexities of helicopters make flight training a potentially costly endeavour. It is important that you look at the costs and take stock of how you plan to finance your flight training. One important note, the FAA may requires only a minimum of forty flight hours, but you will, like most students, probably require more than 50 hours. Ask your school for a detailed cost estimate, including any additional costs such as insurance, ground school, etc. You want to make sure your training isn't interrupted by financial issues.\nTalking of training costs, perhaps the easiest way to save money is consider the training aircraft available and choose something less expensive. For example, a small two-place helicopter, like a Robinson R-22 will typically rent for less than a larger more-powerful helicopter, like a Bell Jet Ranger. Even though you may not plan to fly this type of helicopter after training, learning in a smaller, lighter helicopter will save money and be simpler. You can always get transition training later.\nAircraft and Instructor Availability\nOnce a budget is nailed down and you've got an idea of what helicopters are available, you should consider the scheduling availability of both instructors and helicopters. Whether a particular flight school can accommodate your schedule is key to how well your training goes. If you can only fly on the weekends, but instructors, helicopters, or both are only available mid-day during the week, you will probably have trouble sticking to a schedule.\nFlight School Stability\nThis may be a sign of the times, but flight schools are like any business and sometimes they have to close. While this is a pretty rare occurrence, you'll want to be aware of any potential issues before starting your training. You don't want to have your training interrupted by having to find a new school. Infinitely more likely is that your instructor may leave. Before you get too invested, see if there are several instructors on staff and how this transition might be handled. An instructor leaving is not the end of the world, but it can be a pretty big speed bump.\nHelicopter flight training is one of the most exciting and fulfilling experiences you can have, but it is important you build your training on a solid foundation. Doing your research ahead of time and ensuring that you are eligible, able to afford it and choose a stable, well-staffed, and well equipped school will provide you the best possible helicopter flight training experience.\nMore Helicopter Pilot Career And Training Info\nHelicopter Pilot Articles\n- Five Things To Consider Before Starting Helicopter Lessons\n- Helicopter Pilot Careers\n- Helicopter Pilot Training\n- Helicopter Pilot Salary\n- Helicopter Pilot Licenses and Ratings\n- Helicopter Pilot Jobs\n- Get the Most From Your Helicopter Flight School Experience\n- Helicopter Schools - Three Ways to Maximize Your Training\n- Helicopter Schools - Five Things to Consider\nGI-Bill and Veterans Benefits for Helicopter Pilot Training\nHelicopter Financial Aid & Scholarship Links\n- HelicopterScholarships.com - Helicopter training for UK pilots\n- AOPA Online - AOPA Air Safety Foundation Scholarships\n- EEA Young Eagles - Scholarships and Internships\n- The Ninety-Nines, Inc. - Chapter & Section Scholarships\n- Landings: Scholarships\n- Helicopter Scholarships - Whirly Girls\n- Vertical Flight Foundation - Scholarships\n- Northeast Helicopters - Helicopter Training Scholarships\n- Helicopter Foundation International - Scholarships\n- Griffin Helicopters (UK) - Dennis Kenyon Jr Scholarship\n- Aero Club of New England - Scholarships\n- University Aviation Association - UAA Scholarships""]"	['<urn:uuid:61420b65-5c23-46ac-8ac4-dca523ae2fb3>', '<urn:uuid:11c9e226-cc44-484d-82c4-d1045db90f70>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T18:38:36.516973	11	101	1904
72	Do both correlation and axle weight need visual checks first?	Yes, both require visual verification before proceeding. For correlation, you must create a scatterplot to verify a linear relationship between variables. For axle weight adjustments, you need to visually check that the locks have disengaged and will slide freely before attempting to move the axles.	"[""Two variables can be considered to correlate when there is a systematic change in their scores. The purpose of correlation is to improve estimates and/or make predictions about a population. Simple linear correlation is particularly useful for improving best estimates of a dependent variable by accounting for its relationship with the independent variable using the straight-line formula.\nIn SPSS we are able to run correlations between two interval/ratio variables. To do so, click on “Analyze” → “Correlate”. You will be shown three options. If you have two interval/ratio variables, choose “Bivariate….”\nThe pop up window will ask you to select two variables to move into the “variable” box to the right of the window. Select the variables of interest. In this example, we will use a religiosity scale and a spirituality scale as our two variables. Make sure that the box next to Pearson’s r is checked.\nThen Click “OK.”\nYou'll get the following table:\nThis table is called a correlation table. On the diagonal each variable is correlated with itself, thus they are perfect correlations of 1.0. The table is also symmetrical, meaning the cells above the diagonal are identical to those below the diagonal, thus you really only need to examine one set of cells – either those above or below the diagonal.\nAs far as what the cells tell us... The first value in the cell correlating religiosity with spirituality reports a correlation coefficient of .585. Correlation coefficients range from -1.0 to +1.0. An absolute value of 1.0 indicates a perfect correlation, which is rare in the social sciences. A negative correlation means that as one variable goes up, the other goes down. A positive correlations mean that as one variable goes up, the other also goes up. A correlation of .585 is positive, meaning as religiosity increases, so, too, does spirituality. But the correlation is not perfect, meaning some of the variation in spirituality is not explained by the variation in religiosity.\nThe asterisks at the end of the correlation indicate that the correlation is significant. The p-value of the significance is indicated in two places – at the very bottom of the table where the asterisks are defined and just below the correlation coefficient, where the p-value is provided. In this case, the p-value is .000, which means the odds of finding this relationship between these two variables just due to chance is less than .001, or less than 1 in a thousand. This indicates there is a significant relationship between religiosity and spirituality.\nThe last value in the cell is the sample size for the correlation – 646. If you were to correlate more than two variables simultaneously, which you can do in SPSS by simply adding more than two variables in the correlation dialogue, the sample sizes being compared can vary due to things like missing values or non-response.\nTechnically, before conducting a correlation, one should always run a scatterplot to insure that the relationship between the two variables is linear. This is done in SPSS by clicking on “Graphs” → “Chart Builder”.\nOnce the Chart Builder window is open, click on Scatter Plot. Select the first display example of a scatter plot graph and drag the display into the large box located towards the right of the pop up window. Select your independent variable from the side scroll bar and drag it to the X-axis on the box to the right. Select your dependent variable from the side scroll bar and drag it to the Y-axis on the box to the right.\nOnce this is done, click “OK.”\nYou will get something similar to the graph below:\nThis scatter plot illustrates that there is something like a linear relationship between the two variables. Basically, no one who scores high on religiosity scores low on spirituality. However, there are some people who score low on religiosity but who score high on spirituality. Even so, the general trend is a linear relationship, which means these two variables are suitable for correlation and regression analysis.\nChapter contributed by Brittany Harder."", ""Cargo weight must be distributed evenly across all your axles. And sometimes you have to slide your axles or 5th wheel to redistribute the weight in the trailer. Here’s a guide to basic axle weight rules across North America, and a how-to on repositioning axles to get the weight to where it needs to be.\nAs you pull out from the loading dock, you can’t help but notice that you have a good load on. The engine labors more than usual, and the truck just feels heavy.\nAccording to the bill of lading, you have 45,500 lb. in the box. You wonder if that weight includes the 24 pallets. That would be an additional 500 lb. or so. Either way, you’re going to be close to maximum gross weight, and that means you’ll be tight on the axle weights, too.\nOn the way to the truck stop to axle weigh the truck, you’re thinking it was a good thing you went in to load with just a third of a tank of fuel. That’s 750 lb. you won’t need to worry about.\nWeighing the truck, you discover the drive axles are overloaded, but you still have some room on the steer axle and the trailer axles. Let’s say your scale ticket looks like this: The steer is 12,000 lb.; drive 35,000 lb.; trailer 32,000 lb.; and gross, 79,000 lb.\nOptions to adjust weight\nYou have a couple of options. Slide the trailer axles forward so they will pick up some of the excess drive axle weight. Or slide the 5th wheel ahead to transfer 1,000 lb. from the drives to the steer axle.\nIf you have a 13.2K steer axle with the appropriate tires, you have room for an additional 1,200 lb., but doing that leaves little room for additional fuel.\nThe drive axles can handle the weight from sliding the trailer axles forward. The steer axle weight won’t change, so you’ll have room for more fuel.\nWhat happens when you move axles?\nMoving the trailer axles forward changes the way the weight is distributed over the center point of the axle. It acts as a fulcrum, like the center bar on a playground’s teeter-totter.\nIf more of the weight is placed further behind the fulcrum, the center point will bear more of the weight, lightening the load at front (the drive axles).\nSo how far forward should you slide the trailer axle? In this case, you need to move at least 1,000 lb. off the drive axles, but you have room on the trailer axles for an additional 2,000 lb.\nDane Arcand, a truck driving instructor at Transport Training Centres of Canada’s Ottawa campus, uses a rule of thumb that suggests about 100 pounds per inch of travel on the slider.\n“If your slider has locking holes at six-inch centers, you will shift about 500 pounds per hole,” he says. “If the hole spacing is three inches, they are good for about 250 pounds.”\nIf you make notes each time you slide your axles, you soon have a better picture of how much weight is transferred per inch of travel.\nIn this case, you probably need to slide the axle forward between 12 and 18 inches. Depending on the slider on your trailer that could two to four holes.\nAnother example of overloaded axles\nLet’s look at another example. This time the trailer axles are overloaded by 1,000 lb. The steer is at 12,000 lb.; drive at 32,000 lb., trailer at 35,000 lb.; and gross at 79,000 lb.\nOptions are limited in this case. You will have to slide the trailer axles back.\nSome of the weight transferred to the drive axles will shift to the steer axle. And sliding the trailer axle back too far could violate the kingpin-to-rear-axle restrictions.\nSince you want to take weight off the trailer axles, you need to move them further away from the center of the load, so you will need to slide them back 12-18 inches, or two to four holes on the slider rail.\nBut there’s a limit to how far back you can go. To maintain maneuverability and reduce off-tracking of 53-foot trailers while turning, states and provinces limit the allowable distance between the trailer kingpin and the center of the rear axle (the hub) in the tandem group.\nThat distance varies across different jurisdictions (what a surprise), with California being the most restrictive at 40 feet (12.2 m). Signatories to the Canadian MOU on Vehicle Weights and Dimensions allow 41 feet (12.5 m).\nAs a word of caution. If, staying with this example, you have a 12K steer axle loaded to 12,000 lb., and you slide some weight from the trailer axles forward onto the drive axles, the weight of the steer axle is likely to increase slightly as well. You may have to slide the 5th wheel back slightly to keep that weight off the steer axle.\nMoving the weight around\nBefore sliding anything, mark your starting point, Arcand suggests.\n“If you know you need to move it 12 inches, you’ll need to know where you’re starting from and where you want to wind up,” he says. “It’s difficult to move the axle or the 5th wheel precisely when you can’t really see what you’re doing from the cab. It helps to have someone spotting for you, but if you’re alone you may have to keep cinching it back and forth until you get close, then engage the lock and carefully slide it until the lock catches in the frame.”\nBefore attempting to slide, look to see the locks have disengaged and will slide freely. You may have to shuttle the truck forward and backward with the trailer brakes engaged to free up the mechanism. And since you’re loaded, that may take some deft clutch work with a manual transmission or careful throttle application with an automated transmission.\n“You don’t want to use a lot of throttle because, when it breaks free, it will keep on going until you stop it,” he says. “I have seen photos online where drivers slid the trailer tandems right out from under the trailer.”\nIf they seem stuck, driving around on some uneven ground can help, or try rolling the truck forward or backward and then sharply applying the trailer brakes.\nWill locks work and axles slide?\nIt helps to make sure the locks work and the axles will slide before you load — while the trailer is empty — just for peace of mind that the mechanism is working. Arcan stressed the importance of “exercising” the slider locks periodically.\n“They can get rusted into place or can get jammed up with grease from the 5th wheel or grit and debris from the road,” he says. “Lock and unlock them occasionally and move the slider back and forth once in a while so you know they will work when you need them to.”\nWhen sliding the 5th wheel, it helps to take the weight off the tractor frame by lowering the landing gear. Arcand suggests setting the trailer parking brakes and then lowering the landing gear until the pads touch the ground. Then crank the legs down a few more turns until you can hear a hissing sound from the suspension. Give the landing gear crank a few more turns and you will have lifted much of the weight off the tractor suspension and the 5th wheel will slide more easily.\nRelease the tractor parking brakes (not the trailer brakes) and the 5th-wheel lock, and gently rock the truck back and forth until the locks release. Slide the 5th wheel to the desired position. Get out and look to ensure it’s in the right place, then re-engage the locks. Shuttle the truck back and forth gently until you hear the locks clank into place. Get out and look again to ensure the locks have returned to the locked position.\nSet the tractor parking brakes and crank up the landing gear and go re-weigh the truck.\nArcand advises against deflating the tractor air suspension, because doing so drops the frame height by 4-6 inches. That could leave the tractor dangling from the kingpin, which could damage the 5th wheel.\nTips to achieve accurate axle weights\nWhile every load is different, and weights will vary each time you put something in the box, getting the truck into a typical or normal configuration prior to loading will improve your chances of driving away without having to redistribute the weight.\nIf the load is light, you probably won’t have to worry at all about weight distribution, unless the loader stuffs everything into the front of the box.\nStart with the tanks full of fuel and put the 5th wheel and trailer tandems into a “normal” position as a starting point.\n- If you have a sliding 5th wheel, position it about two to four inches (5-10 cm) ahead of the center of the drive axles with a setback-axle aero truck. With a long-nose truck, position the 5th wheel about 8-10 inches (20-25 cm) ahead of center.\n- With a 53-foot trailer, position the tandems so that distance between the trailer kingpin (the pivot point of the 5th wheel) and hub of the tandem’s rear axle is 40 feet (12.2 m). That should position the rear axle about eight-10 feet from the back of the trailer — and keep you under the maximum allowable kingpin-to-tandem distance in most states and Canada.\n- Most trailers weigh about the same, but they many use four-inch or six-inch holes in the slider. Rather than count the holes, measure the distance from the back of the trailer to the center of the rear axle hub.\n- Take pictures or makes notes of these positions so you have a record of your starting point.\nIf you configure the truck like this each time before you load, you’ll have a consistent starting point.\nHow does fuel affect axle weights?\nIt’s much easier to slide things when the trailer is empty, and if you discover a problem with the slider mechanism, you’ll be able to deal with it before you load.\nSometime when you are empty and preparing to refuel, axle weigh the truck before you fuel, record how much fuel you took on, and reweigh the truck. This will tell you how the weight of the fuel distributes itself between the steer and the drive axles.\nThat’s useful knowledge when taking on a heavy load that could put you over on your tractor axle weights if your tanks are full. It also helps to know how much fuel you can take on if you have a heavy load and your axles are close to maximum.\nFor an aero truck with set-forward fuel tanks, about 80% of the fuel’s weight will go on the steer axle, and 20% to the drives. With a long nose and rear-mounted tanks, about 70% will go to the drive axles and 30% will show up on the steer axle. Your own results may vary.\nDiesel fuel weighs about 7.5 lb. per U.S. gallon (0.85 kg/L or 1.8 lb./L). If you take on 100 U.S. gallons, your weight will increase by about 750 lb. (340 kg). The tank locations will determine where the weight goes. As per the aero-truck example above, about 600 lb. will wind up on the steer axle, 150 lb. on the drives.\nKeep a record of your previous load’s axle weights so you’ll get to know how the weight is distributed on the axles. This will reduce future guesswork and save you a lot of manual labor getting legal.\nUnderstanding axle weight regulations\nCommercial vehicle size and weight regulations are very complex, and they can vary from one jurisdiction to another. It’s almost impossible to make a single statement about weights and dimensions that applies universally across North America.\nThat said, this article speaks in generalities that apply to a typical highway truck, the five-axle tractor trailer combination.\nThe weight allowances we speak of that apply to tractor drive axles and trailer axles are common denominators. That is, they apply broadly to trucks operating on U.S. interstate highways, and on highways designated as part of Canada’s national highway system (NHS).\nFront or steer axles are a different story (see the Understanding Steer Axle Weights sidebar shown above). States and provinces have different regulations and policies relating to steer axles, and the weights we discuss here are generally the lowest common denominator. In some cases, the allowable weights can be higher.\nSo, to the question of the day: why do we have axle weight limits to contend with as well as gross weight limits?\nWhy are there axle weight limits?\nAxle weight limits prevent pavement and bridge damage due to high concentrations of weight distributed over a small area. They also protect the equipment from damage caused by overloading. A truck may be designed to carry 80,000 lb. (36,287 kg) but that weight must be distributed evenly over several axles or axle groups.\nPermissible axle loads vary across different jurisdictions. Some jurisdictions apply different weight limits based on the type of road (i.e., municipal roads, arterial roads, and highways). This is usually determined by the underlying construction of the road and pavement design.\nFortunately, both the U.S. and Canada have standard sets of allowable vehicle and axle weights that apply nationally. Some states and provinces may allow higher or lower weights based on vehicle configurations and local regulations.\nIn the U.S., weight limits on Interstate highways are determined by the Federal Bridge Formula. It relies on a calculation based on the distance between the axles and/or axle groups to determine the weight for the group and allowable gross vehicle weight.\nFor example, to haul a full load of 80,000 lb. (36,287 kg) on a U.S. interstate highway, there must be at least 39 feet (11.89 m) between the extremities of the drive and trailer axle groups. That’s easy to meet with a 53-foot trailer. But a 20-foot container chassis with an axle spacing of something like 28 feet (8.5 m) would be restricted to a lower weight.\nThe U.S. DOT Freight Management and Operations website offers a comprehensive view of these matters.\nEach Canadian province or territory has its own engineering standards for determining axle weight and spacing, but agreements called memorandums of understanding have harmonized the rules to some extent. We now have a common set of axle weights covering the most common vehicle configurations in Canada.\nThe MOU is based on the lowest common denominator across all provinces and territories. Each province enforces its own allowable axle and gross weights (some of which may be higher), but the MOU sets out the weights you would operate to remain legal from Newfoundland and Labrador to British Columbia.\nHigher weights in Canada\nCanada typically allows higher axle weights and gross vehicle weights than the U.S. For example, on roads designated as part of the national highway system, and per the MOU, the allowable gross weight for a typical five-axle tractor trailer combination is 39,500 kg (87,082 lb.).\nOn U.S. Interstate highways, the weight limit for that same truck is 80,000 lb. (36,287 kg).\nCanada’s MOU allows 17,000 kg (37,478 lb.) for a tandem axle grouping — that’s two adjacent axles spaced between 1.2 m (48 inches) and 1.85 m (72 inches) apart on designated highways.\nThe U.S. allows 34,000 lb. (15,422 kg) on a tandem axle grouping on interstate highways and many federal highways. States may impose lower or higher limits on state or county highways.\nProvincial weights vary\nEach province and territory has its own weights and dimensions , and while some regulations are similar, the difference can be significant. Ontario’s Vehicle Weight and Dimensions regulations are probably the most complex.\nThe Canadian Transportation Equipment Association (CTEA) offers an unofficial public guide to Ontario’s rules. It runs 232 pages. But to illustrate the differences that can exist, Ontario allows up to 18,000 kg (39,700 lb.) on a tandem axle in a SPIF configuration. SPIF stands for Safe, Productive & Infrastructure Friendly.\nDrivers should become familiar with the weight regulations for any jurisdiction in which they operate. And remember if you are traveling between jurisdictions, the lowest allowable weight in any jurisdiction you travel becomes the highest weight you can carry.\n- Driver’s Education is an ongoing series of articles that focuses on things drivers need to know, but may not have learned during entry-level driver training.\nHave your say\nThis is a moderated forum. Comments will no longer be published unless they are accompanied by a first and last name and a verifiable email address. (Today's Trucking will not publish or share the email address.) Profane language and content deemed to be libelous, racist, or threatening in nature will not be published under any circumstances.""]"	['<urn:uuid:f4e5e5fa-77cf-4697-b0ba-0d0e8b131a48>', '<urn:uuid:b2dd6a6c-e84c-49fc-a26d-cf2dcf0d6864>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T18:38:36.516973	10	45	3476
73	How many identical symbols are found on the mysterious clay tablet discovered in the palace of Phaistos, and what is the overall count of impressions on it?	The Phaistos Disc carries 45 different signs, with a total text of 221 signs altogether.	"['inaugeral date: 4 December 2005; last update: 5 November 2014\nComments, corrections, questions: John Younger (email@example.com)\nThe Phaistos Disc is unique (= it is one of a kind with no parallels). It is a clay disc (D. 15.8-16.5, Th 1.6-2.1 cm), like a large cookie in shape and color. On both sides there is a spiral band containing impressions made by metal stamps, each quite small and each giving just the profile of what it depicts (head, bird, boat) much like a modern cookie-cutter. The disc was found on July 3, 1908, in one of the eastern brick cists (compartment 8 in building 101) in the north-east angle of the palace at Phaistos; along with the disc was a Linear A tablet (PH 1) and Kamares ware pottery (MM II).\nThe disc\'s spiral band was first outlined with a stylus from the circumference to the middle; first tracing the outermost spiral as a circle, then stopping to shift direction and completing the rest of the spiral to the center, with the central loop being shaped with the inscription there in mind. Then the signs were stamped, again from the periphery in to the center (as overlaps indicate [Bradshaw 1977]: sign on sign [e.g., A XIV 3 over 2, XVII 4 over 3, XXIX 4 over 3; cf. A V 1 over 2 in an erasure], and sign on the upper spiral line [e.g., A XX 1, III 2, XIV 3, XX 3, XXVI 3, 4; B XII 2, XXII 4]). As each group of signs was stamped, a vertical line was drawn to the left to mark that sign-group off from the one to follow; here again, some signs that begin a sign-group overlap the vertical line separating it from the preceding sign-group (A XXI 1, XXIX 1; B XXVIII 2).\nThat the sign-groups are arranged in phrases may be indicated by small oblique strokes below final signs (e.g., A I, A III, etc.).\nThe Phaistos Disc carries 45 different signs. And a total text of 221 signs altogether.\nA bronze ax from the Arkalochori cave carries an inscription of 15 signs in three columns; four of these signs are new and the rest may duplicate signs on the Disc. And a clay sealing from the Phaistos sealing deposit archive (CMS II 5.246, below) also duplicates one of the signs on the Disc (sign 21); the sealing was excavated in 1955.\nThe Phaistos Disc syllabary (if that is the nature of the script) should have some 75-100 signs, of which some 55-59 are now known.\nAlthough the Disc is unique as a stamped inscription (often likened to a movable font inscription, like that produced by a printing press), there is one more clay object impressed with similar cookie-cutter stamps: a clay libation table found on the floor of vano VIII at Phaistos, below the deposit archive of sealings, again MM IIB context. This table was impressed by two metal ""cookie cutter"" stamps, one in the form of a large S-spiral (CMS II 6.253) that impressed most of the rim, the other a bovine standing to left (CMS II 6.252) that impressed two sections of the rim and the corners. Since all signs that have a natural (a facing) direction on the Disc face right (even if they were stamped upside down), the bovine to left is probably not part of the PH Disc syllabary.\nAnd as a spiral inscription the Disc has one Linear A parallel, KN Zf 13, the gold ring from Mavrospelio Tb IX.E1 (cf. the inked inscriptions in two conical cups, KN Zc 6 and 7): Linear A miscellaneous inscriptions.\nWith only 3 documents and only some 55-59 different signs, the entire signary is still unknown -- if a syllabary, as seems likely, there should be some 75-100 signs.\nFor a list of the PH Disc signs (including those on the Arkalokhori Ax and Phaistos sealing, compared with Hieroglyphic signs), click here.\nIn October 2014, Gareth Owens gave a TEDx-Talk at the Technical Education Institute (TEI) in Iraklion, in which he claims to have identified the phonetic values of 90% of the PhD signs, two words and a phrase that are similar to the Linear A ""Libation Formula,"" and the nature of the language of the Disc and Linear A: both scripts write the same language and it is Indo-European. Some of this has been published on the website.\nJGY has written a response (available as a PDF) and has incorporated the more plausible of Owens\'s sign identifications in his Phaistos Disc sign list.\nPhaistos Disc, side A\nPhaistos Disc, side b\nThe Phaistos Disc syllabary (the names are conventional, Godart 1993/1995)\n|01 walking man||02 plumed head||03 tatooed head||04 bound man||05 child|\n|06 woman||07 helmet||08 glove||09 plow||10 arrow|\n|11 bow||12 shield||13 club||14 yoke||15 ax|\n|16 saw||17 lid||18 boomerang||19 plane||20 triton|\n|21 comb||22 frond||23 column||24 hut||25 boat|\n|26 horn||27 hide||28 hoof||29 cat||30 ram|\n|31 bird||32 pigeon||33 tuna||34 fly||35 tree|\n|36 vine||37 papyrus||38 rosette||39 lily||40 bull hindquarters|\n|41 double flute||42 rasp||43 filter||44 cleaver||45 wave|\nThe Phaistos Disc inscription\n|/||A I: 02-12-13-01-18/|\n|, /||A II: 24-40-12, A III: 29-45-07/|\n|, , , , , , , , /||A IV: 29-29-34, A V: 02-12-04-40-33, A VI: 27-45-07-12, A VII: 27-44-08, A VIII: 02-12-06-18-, A IX: 31-26-35, A X: 02-12-41-19-35, A XI: 01-41-40-07, A XII: 02-12-32-23-38/|\n|, , /||A XIII: 39-11, A XIV: 02-27-25-10-23-18, A XV: 28-01/|\n|/||A XVI: 02-12-31-26/|\n|, , /||A XVII: 02-12-27-27-35-37-21, A XVIII: 33-23, A XIX: 02-12-31-26/|\n|, /||A XX: 02-27-25-10-23-18, A XXI: 28-01/|\n|/||A XXII: 02-12-31-26/|\n|, , , , /||A XXIII: 02-12-27-14-32-18-27, A XXIV: 06-18-17-19, A XXV: 31-26-12, A XXVI: 02-12-13-01, A XXVII: 23-19-35/|\n|, , , , , , /||A XXVIII: 10-03-38, A XXIX: 02-12-27-27-35-37-21, A XXX: 13-01, A XXXI: 10-03-38 B I: 02-12-22-40-07, B II: 27-45-07-35, B III: 02-37-23-05/|\n|, , /||B IV: 22-25-27, B V: 33-24-20-12, B VI: 16-23-18-43/|\n|, , , , , , , , , , , /||B VII: 13-01-39-33, B VIII: 15-07-13-01-18, B IX: 22-37-42-25, B X: 07-24-40-35, B XI: 02-26-36-40, B XII: 27-25-38-01, B XIII: 29-24-24-20-35, B XIV: 16-14-18, B XV: 29-33-01, B XVI: 06-35-32-39-33, B XVII: 02-09-27-01, B XVIII: 29-36-07-08/|\n|, /||B XIX: 29-08-13, B XX: 29-45-07/|\n|/||B XXI: 22-29-36-07-08/|\n|, , /||B XXII: 27-34-23-25, B XXIII: 07-18-35, B XXIV: 07-45-07/|\n|, /||B XXV: 07-23-18-24, B XXVI: 22-29-36-07-08/|\n|, , , /||B XXVII: 09-30-39-18-07, B XXVIII: 02-06-35-23-07. B XXIX: 29-34-23-25, B XXX: 45-07/|\nThe Arkalokhori Ax is a large, bronze double ax found in one of the caves on Prophetis Elias near the village of Arkalokhori in central Crete.\nAfter pillaging in the late 19th and early 20th century, and some excavation in the late 19th century, Spyridon Marinatos responded to more pillaging in 1935 with further excavation of the site and, among other finds, discovered a gold ax and silver ax inscribed in Linear A (""I-DA-MA-TE,"" AR Zf 1 & 2), and a bronze ax inscribed with some signs from the Phaistos Disc syllabary (Marinatos 1935: 250-259). Marinatos assumes a context date of MM III (-LM I), but there was little to no pottery. Most scholars assume a date either contemporary with Neopalatial axes of similar type or with the Phaistos Disc (late Protopalatial).\nSince most of the sign groups on the Phaistos Disc are short (mostly 3 and 4 signs long), it is tempting to regard the signs in the third column as one sign group and columns one and two containing two sign groups each, separated by a divider (cf. the dotted vertical line that starts both sides of the Disc).\nArkalokhori Ax, drawing of inscription\nThe Arkalokhori Ax inscription\n|Phaistos signs: 02-19-Face |\n|Phaistos signs: 02-Curved\n|Col. 3: 09-10-02||Phaistos signs: 10-Double Branch-19|\nIn signgroups (assuming sign begins a signgroup and each column contains two signgroups)\n|()||Signgroup 1: 01-02-03 (04)||Phaistos signs: 02-19-Face-23?|\n|Signgroup 2: 05-06||Phaistos signs: 39?-22?|\n|()||Signgroup 3: 01-07 (04b)||Phaistos signs: 02-Curved Branch-23?|\n|Signgroup 4: 01-03-08||Phaistos signs: 02-Face-39?|\n|Signgroup 5: 09-10-02||Phaistos signs: 10-Double Branch-19|\nThe third document is an impressed sealing from Phaistos (MM II late), CMS II 5, no. 246, whose seal carried Phaistos sign 21; the sealing was excavated in 1955.\nBibliography of selected archaeological works on the Phaistos Disc\nMackay. 1964. ""On the Type-fount of the Phaistos Disk,"" Statistical Methods in Linguistics 4, pp. 15-25.\nMarinatos, S. 1935. Marinatos, S. 1935. ""Ausgrabungen und Funde auf Kreta 1934-1935,"" Archäologischer Anzeiger 1935, 244-259.\nNeumann, Günter. 1968. ""Zum Forshungsstand beim ""Diskos von Phaestos,"" Kadmos 7, pp. 27-44.\nOliver, Jean-Pierre. 1973. ""Encore les corrections du disque de Phaistos,"" Antichità Cretesi 1, pp. 182-185 (Cantania).\nOwens, Gareth. 2007. Labyrinth: Scripts and Languages of Minoan and Mycenaean Crete. Heraklion, Centre for Cretan Literature with the support of Heraklion Prefecture (ISBN 978-960-86847-5-1), esp. pp. 185-203, “The Phaistos Disk: The Enigma of the Minoan Script.""\nPernier, Luigi. 1908. ""Il disco di Phaestos con caratteri pittografici,"" Ausonia 3, pp. 255-302.\nYounger, John G. 2007. ""The Aegean Bard: Evidence for Sound and Song."" In: EPOS. Reconsidering Greek Epic and Aegean Bronze Age Archaeology (Aegaeum 28), edited by S. Morris and R. Laffineur, 1-7, esp. 7. Liège & Austin.\nComments, corrections, questions: John Younger (firstname.lastname@example.org)']"	['<urn:uuid:631839d8-7f93-4259-a0ee-f9f034d6ea14>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	27	15	1514
74	As someone who loves making sandwiches at home, I'd like to know how both Japanese mixed sandwiches and classic American grilled cheese handle the problem of keeping the bread from getting soggy?	For Japanese Mixed Sando, butter is used as a waterproof shield/barrier to prevent the bread from becoming soggy. Similarly, for American grilled cheese sandwiches, butter is considered essential - experts specifically recommend buttering the outside of the bread rather than the pan to ensure the entire surface of the bread will turn golden and crisp. They also emphasize using real butter exclusively, making it a critical component for both sandwich styles.	"[""Mixed Sando (short for “sandwiches”) is a classic set menu item for Japanese sandwiches made with soft and tender Japanese milk bread. They’re found at cafes, restaurants, bakeries, and konbini (convenience stores) all across Japan. Home cooks make them for their children’s bentos and family picnics, too. Here, I show you three popular types of fillings: Tuna salad, tamago (egg salad), and ham and cheese with lettuce.\nJapanese sandwiches like Katsu Sando and Tamago Sando have found their spots in today’s food world and pop culture. But as you may have guessed, there’s more to discover! Today I’m introducing one of the most classic sandwiches that should be on your radar: Mixed Sando (ミックスサンド).\nTucked inside fluffy slices of crustless bread and served as a set, these sandwiches are perfectly simple yet unmistakably comforting and nostalgic. Let’s make mixed sando at home!\nTable of Contents\nWhat is Japanese Sandwich ‘Mixed Sando’?\nMixed sando (ミックスサンド) typically refers to three different sandwiches served as a set. Nothing fancy, but it is a standard style of sandwich in Japan. The fillings include:\n- Ham and cheese with lettuce\n- Tuna salad\n- Egg salad (tamago)\nBoth tuna and egg sandwiches are soft and creamy. The ham, cheese, and lettuce sandwiches add a nice crisp contrast. Together, they make a tasty combo that is well-balanced in texture, taste, and visual.\nYou can find mixed sandos everywhere: at cafes, restaurants, grocery stores, bakeries, and konbini (convenience stores). Because they are satisfying and nutritious, Japanese home cooks also love making these mixed sandwiches for their children’s bentos and picnics.\nHow to Make Mixed Sando\nIngredients You’ll Need\n- Shokupan (Japanese milk bread) – The key component that makes the sandwich uniquely Japanese. I’ll talk about this more below.\n- Butter – You used spreadable salted butter\n- Kewpie Japanese mayonnaise – You can buy it on Amazon or make it yourself\n- Mustard – Optional; I skipped it as my family is not fond of mustard.\n- Salt and black pepper\n- For Ham and Cheese Sandwich – Sliced ham, sliced cheese (I use Swiss), iceberg lettuce, and sliced Japanese/Persian cucumber\n- For Tuna Sandwich – Canned tuna and minced onion\n- For Egg Sandwich – Hard-boiled eggs and milk\nOverview: Cooking Steps\n- Make tuna salad and egg salad.\n- Prepare the rest of the ingredients (rinse the lettuce, slice the cucumber, etc.).\n- Cut the shokupan (Japanese milk bread) into thin slices (about 1-1.2 cm) and spread the butter, mayonnaise, and mustard (optional) on one side of the slice.\n- Add the sandwich fillings for each type of sandwich and press it down by placing a plate on top.\n- Slice off the crust of the sandwiches and cut them into smaller rectangular shapes.\nTypes of Sandwich Bread You Need\nOne distinct characteristic of Japanese sandwiches is the use of Shokupan (Japanese Milk Bread).\nHow is it different from the western style loaf of sandwich bread? Shokupan is known for its milky-sweet taste, incredibly fluffy texture, and pillowy softness. There is a lightness to the bread, which I think defines the essence of Japanese sandwiches.\nYou can either make your own milk bread or pick up a loaf from your local Asian grocery store. The pre-sliced shokupan for sandwiches comes with a 1 cm or 1.2 cm thickness, so you don’t have to cut the bread yourself. However, if you plan to make the shokupan for sandwiches, it’s not easy to hand slice the bread to an even 1-1.2 cm thickness.\nI used Black Sesame Shokupan for this mixed sando recipe, but you can certainly use plain white shokupan.\nIf you really don’t have access to Japanese milk bread but still wish to make the recipe, go for the fluffiest, softest white bread you can find.\nWhen I was growing up in Yokohama, Japan, the popular local bakery called Pompadour had mixed sandos made with black sesame milk bread. I have major cravings for sandwiches, so I made one for this recipe.\nThe black sesame seeds add a lovely nutty and earthy taste to the soft milk bread. If you have tried the classic plain shokupan, you need to make this black sesame version today as it is the next-level bread for Japanese sandwiches.\nTips to Make Japanese Sandwich ‘Mixed Sando’\n- Use butter to shield the bread from moisture. I highly recommend spreading the butter as it acts as a waterproof shield/barrier so the bread stays dry, not soggy.\n- Use Kewpie Japanese mayonnaise. For Japanese sandwiches, Japanese mayo is a must. It has a distinctly sweet, sour, umami-rich flavor that is crucial for the sandwiches.\n- Press down the sandwich before slicing. Place a plate on top of the sandwiches. Weighing the sandwich helps to compress everything so the bread and the ingredients are bound together better.\n- Cut off the crust of the bread. This is for the classic Japanese sandwich look. I do eat the crust when I make the sandwiches… no food waste. 🙂\nWhat to Serve with Japanese Sandwiches\n- Royal Milk Tea\n- Crispy Baked Salmon Nuggets\n- Shrimp and Broccoli Salad\n- Seafood Salad with Vinaigrette\n- Miso Butter Cookies\nOther Classic Japanese Sandwiches\nJapanese Sandwich (Mixed Sando)\nFor the Sandwiches\n- shokupan (Japanese milk bread) (you'll need 6 thin slices total; I used Black Sesame Shokupan for this recipe; you can also use regular Shokupan)\n- salted butter (at room temperature; I use spreadable butter)\n- Japanese mayonnaise (store bought or make my homemade Japanese mayonnaise)\n- mustard (optional; I skipped it as my family is not fond of mustard)\nFor the Tuna Salad\n- ⅛ onion\n- ½ (5-ounce) can albacore tuna (preferably packed in olive oil) (I used a whole 2.8-ounce (80 gram) can of Japanese brand canned tuna; well drained)\n- freshly ground black pepper\n- kosher salt (Diamond Crystal; use half for table salt)\nFor the Egg Salad\nFor the Ham and Cheese Filling\n- 2 leaves iceberg lettuce (trimmed to the size of a shokupan slice)\n- 2 slices ham\n- 2 slices Swiss cheese\n- 1 Japanese or Persian cucumber (thinly sliced diagonally; I used 6 slices)\nTo Make the Tuna Salad\n- Gather the tuna salad ingredients. Then, mince the onion and soak the pieces in water for 5 minutes to remove the bitterness and astringency.\n- Squeeze the water out and add to a bowl along with the drained tuna.\n- Add the mayonnaise, salt, and pepper. Mix with a fork. Flake the tuna chunks into smaller pieces and incorporate the seasonings well. Set aside. You can cover it and store in the refrigerator while you make the egg salad.\nTo Make the Egg Salad\n- Gather the egg salad ingredients. This recipe makes enough filling for two Tamago Sando. You can store the leftover egg salad in an airtight container for up to 2 days in the refrigerator. Put the eggs in a medium saucepan and add enough water to cover by one inch (2.5 cm). Bring it to a boil on medium heat. Once boiling, cook for 12 minutes.\n- Once it’s finished cooking, transfer the eggs to iced water to stop the cooking. Let them cool completely and peel the eggshells.\n- Transfer the peeled eggs to a bowl and mash with a fork. Try to mash the egg whites into small, same-sized pieces.\n- Add the sugar and salt.\n- Add the freshly ground black pepper and milk. Taste and add more salt and pepper if needed.\n- Add the mayonnaise and mix very well.\nTo Assemble the Sandwiches\n- Gather the filling ingredients for the three types of sandwiches.\n- If your bread is not pre-sliced, cut the shokupan into six thin slices, each about ⅓ to ½ inch (1 to 1.2 cm) thick. Lay out the six bread slices for the three sandwiches.\n- Using a butter knife, spread a thin layer of butter on all six slices. For the Tamago Sando, scoop one portion of egg salad onto one of the slices and spread to distribute evenly.\n- For the Ham and Cheese Sando, spread the Japanese mayo (and mustard, if desired) onto two of the buttered slices. On one of those slices, layer the iceberg lettuce, ham, cheese, and the cucumber slices on top.\n- For the Tuna Sando, scoop the tuna salad onto one of the remaining sandwich slices and spread evenly. Put the other slice of bread on top, buttered side down. For the Ham and Cheese Sando, put the slice on top that has both the butter and mayonnaise, spread side down. Finally, put the remaining slice of bread on top of the egg salad, buttered side down. Then, place plates on top of the sandwiches to compress the ingredients slightly. Set aside for 5 minutes.\n- After 5 minutes, remove the plates. If you'd like, cut the crusts off the sandwiches.\n- Cut each sandwich horizontally into thirds for three equal rectangular pieces. They're now ready to serve. For each serving, arrange one piece of each type of sandwich onto a plate for a total of three pieces. In the photo to the right, I show how to present two servings in a carry-out box.\n- You can keep any leftovers in an airtight container and store them in the refrigerator for up to 24 hours."", 'Susan Russo is a food writer in San Diego. She publishes stories, recipes and photos on her cooking blog, Food Blogga. She is working on a cookbook, Field Guide to Sandwiches (Quirk Books), which will be released in the fall of 2010. When she isn\'t writing about her Italian family back in Rhode Island or life with her husband in Southern California, she can be found milling around a local farmers market buying a lot more food than two people could possibly eat.\nThe grilled cheese sandwich – a classic American comfort food – has grown up. Traditionally most comfortable at home on the kitchen table alongside a bowl of tomato soup, the grilled cheese sandwich now finds itself in places it never thought it would be: chic bistros, posh restaurants and exclusive hotels.\nFor decades, the grilled cheese remained relatively unchanged. One bite of warm, gooey cheese nestled between two crunchy, buttery slices of white toast was all it took to transport you back to the better days of childhood.\nThen creative chefs, such as Nancy Silverton of La Brea Bakery in Los Angeles, took a new look at the classic model. About 10 years ago, Silverton started Thursday Grilled Cheese Night at her L.A. restaurant Campanile. The plain Jane grilled cheese got a sexy makeover. Silverton offered high-priced, high-style grilled cheese sandwiches such as gorgonzola with roasted radicchio, walnuts and honey and burrata with prosciutto, marinated cherry tomatoes and saffron-braised cabbage and salsa verde. This helped usher in a whole new world of designer grilled cheese sandwiches.\nWhile some people relish the thought of caramelized grilled onions and creamy Gruyere on artisanal olive bread, others view anything other than sliced American cheese on Wonder Bread an affront to the iconic grilled cheese.\nThe grilled cheese sandwich is nothing new. Various bread and cooked cheese combinations have existed since ancient times. However, the quintessential American grilled cheese sandwich – white bread with processed cheese – surfaced sometime in the 1920s.\nThat\'s when Wonder Bread began selling whole loaves, and a decade later it introduced pre-sliced bread. In 1917, Kraft Foods sold its first processed canned cheese to the U.S. Army. Then, much to Americans\' delight, Kraft introduced Velveeta in 1928 and individually wrapped slices in 1965.\nWith the widespread availability of affordable pre-sliced bread and processed cheese, the grilled cheese became an instant American classic. It began appearing on the menus of lunch counters and diners alongside other popular sandwiches, such as the club and the BLT. It was even included in government-issue cookbooks during World War II. Nowhere was it more prevalent, however, than in American homes, where it was both a lunchtime staple and a quick dinner favorite.\nThe love affair continues. Americans eat a whopping 2.2 billion grilled cheese sandwiches per year, according to the NDP Group Inc., a consumer research firm.\nThough there are no set rules to making a great grilled cheese, everyone seems to have her or his opinion on the right way to make one. A Google search on ""how to make the perfect grilled cheese sandwich,"" produces 275,000 hits. Here\'s what some of them say: Always use a cast iron pan. Use only a nonstick skillet. Use a panini press. Butter the bread only. Butter the pan only. Butter both the bread and the pan. Never use olive oil instead of butter. (I\'m glad my Italian-American mother didn\'t have Google when I was a kid.) Remove the crusts. Squish the sandwich with a heavy-bottomed skillet until the crusts crack and the cheese oozes out. Use only Wonder Bread. Always add a thin layer of mayo. Skip all condiments. Got it?\nHere\'s what is important when it comes to making great grilled cheese. Start with good, thinly sliced bread so the heat can penetrate easily to melt the cheese. While white bread still reigns supreme, whole wheat, sourdough and rye are also popular. More expensive hand-crafted artisanal breads rarely disappoint.\nNext, use a good melting cheese, which should be high in fat. Americans\' No. 1 cheese of choice is American, followed by cheddar and Swiss. Gouda, Gruyere, havarti, Jarlsberg and Muenster are other favorites. If cost isn\'t an issue, hand-crafted farmhouse cheeses are more widely available than ever before.\nNow decide if you want to include condiments such as Dijon, honey or pesto, or add-ins such as grilled onions, bacon or sliced fruits.\nFinally, choose a cooking method: the stovetop, the broiler, a panini press, a toaster oven or an indoor grill.\nBut, you ask, how do you make the perfect grilled cheese? That\'s easy. Make the one your mom made for you when you were a kid, because let\'s face it: A grilled cheese tastes only as good as the childhood memories associated with it.\nTips For Making A Great Grilled Cheese\nUse thinly sliced bread so the heat will penetrate easily and melt the cheese.\nA good melting cheese should be high in fat and not too hard or dry.\nShredded cheese melts more quickly than sliced cheese. If you prefer sliced cheese, make sure it\'s very thinly sliced. Two ounces is a good amount for one sandwich.\nButter the outside of the bread rather than the pan. This ensures that the entire surface of the bread will turn golden and crisp.\nUse real butter. Period.\nUse whichever cooking method you prefer. For stovetop cooking, which is the most common method: Use a nonstick pan for easy flipping; briefly preheat the pan and keep the heat on medium-low so the bread won\'t burn before the cheese is properly melted.\nIf you like your grilled cheese pressed, then either place a heavy pan on top of the sandwich while it\'s cooking or use a panini press.\nThis is a no-frills grilled cheese sandwich – the one that children love and that makes an adult feel like a kid again.\nMakes 1 sandwich\n2 slices favorite soft white bread\n1 tablespoon real butter\n2 slices American cheese\nButter one side of each slice of bread. Place slices, buttered side down, on a clean work surface. Place both slices of cheese on one slice of bread, slightly overlapping them. Top with the second slice of bread, buttered side up. Close the sandwich.\nPlace sandwich in a preheated nonstick skillet over medium heat. Cook 2 minutes, or until the underside is golden brown and the cheese has begun to melt. Flip the sandwich with a spatula, pressing lightly to flatten. Cook 1 minute, or until it is golden brown and the cheese is completely melted. Slice in half on the diagonal. Eat it while it\'s hot.\nI don\'t remember the first time my mom made me this breaded and fried Italian grilled cheese sandwich, but I do know that it\'s my all-time favorite. When you break the sandwich open with your hands, hot melted mozzarella comes oozing out all over the crunchy breadcrumb-coated bread. Though it\'s wonderful as is, take my advice and dunk it in your favorite marinara sauce.\nMakes 2 sandwiches\n4 thin slices Italian bread\n4 ounces shredded mozzarella or very thinly sliced fresh buffalo mozzarella\n4 large fresh basil leaves\n1 large egg\n1/8 teaspoon salt\n1/4 cup plain breadcrumbs\n1/8 teaspoon crushed red pepper flakes\n2 tablespoons olive oil\n1/2 cup marinara sauce of your choice\nPlace 2 bread slices on a clean work surface. Place half the mozzarella cheese on each slice, then top each with two basil leaves. Place a second slice of bread on each, forming two sandwiches.\nIn a wide, shallow bowl, lightly beat 1 egg with salt. In a separate wide, shallow bowl, mix breadcrumbs and red pepper flakes.\nIn a large skillet over medium-low heat, warm the olive oil. Carefully place one sandwich at a time in the egg mixture, making sure to coat the sandwich completely. Allow excess egg to drip back into the bowl. Dredge the sandwich in the bread- crumb mixture until completely coated. Place in the skillet. Cook 2 to 3 minutes or until the underside is golden brown. Flip the sandwich with a spatula, pressing lightly to flatten. Cook 1 to 2 minutes or until it is sizzling and golden brown. Serve hot with a side of your favorite marinara sauce for dunking.\nThis Southwest grilled cheese highlights the interplay of flavors and textures. Crunchy jalapeno cheddar bread is topped with a spicy chipotle mayo, sweet caramelized onions, creamy avocado and smooth, sharp cheddar.\nMakes 2 sandwiches\n3 tablespoons butter, divided\n1/4 cup thinly sliced red onion\n3 tablespoons mayonnaise\n2 teaspoons chipotle chilies in adobo sauce, finely chopped (or 1/4 teaspoon chipotle chili powder)*\n2 tablespoons fresh cilantro, coarsely chopped\n4 slices jalapeno cheddar bread (sourdough bread is a delicious alternative)\n4 ounces shredded or very thinly sliced sharp cheddar cheese\n1/2 large avocado, thinly sliced and lightly sprinkled with lime juice\nIn a small skillet over medium heat, melt 1 tablespoon butter. Add red onions and cook, stirring occasionally, for about 5 minutes or until lightly caramelized. Remove from heat.\nIn a small bowl, whisk mayonnaise, chipotle chilies in adobo sauce and cilantro. Set aside.\nSpread the remaining 2 tablespoons of butter on one side of each of the 4 slices of bread. Place slices, buttered side down, on a clean work surface. Spread the cilantro-mayo mixture evenly over the 4 slices of bread. Pile shredded cheese evenly over two slices of bread. Top each with half of the avocado slices and grilled onions. Place the second slice of bread, buttered side up, on each, creating two sandwiches.\nPlace sandwiches in a large preheated skillet. Cook 2 to 3 minutes, or until the undersides are golden brown and the cheese has begun to melt. Flip the sandwiches with a spatula, pressing lightly to flatten. Cook 1 to 2 minutes or until the undersides are golden brown and the cheese is completely melted. Eat immediately.\n*Both chilies in adobo sauce and chipotle chili powder can be found in the Mexican food section of most major supermarkets as well as in Latin American markets.\nGrilled Jarlsberg And Granny Smith Apple On Raisin Walnut Bread\nSliced fruit such as fresh apples, pears, figs and grapes are ideal add-ins for grilled cheese. In this sandwich, Granny Smith apples add tartness and texture to balance the sharp sliced onion and smooth, mild Jarlsberg cheese. Both Gouda and Gruyere are delicious cheese alternatives. Fruit jellies and chutneys are good alternatives to fresh fruit.\nMakes 2 sandwiches\n1 tablespoon plus 1 teaspoon Dijon mustard\n2 teaspoons apple cider vinegar\n2 teaspoons honey\n2 tablespoons butter\n4 thin slices hard-crust raisin walnut bread or similar fruit and nut bread\n4 ounces grated or very thinly sliced Jarlsberg cheese\n1/2 medium Granny Smith apple, sliced very thin\n1/4 cup yellow onion, sliced paper thin\n7 to 8 shakes of salt and cranks of freshly ground black pepper\nIn a small bowl, whisk Dijon, apple cider vinegar and honey. Set aside.\nButter one side of each slice of bread. Place slices, buttered side down, on a clean work surface. Top with Dijon-honey mixture. Pile shredded cheese evenly over two slices of bread. Top each with three slices of apple and half of the sliced onions. Season with salt and freshly ground black pepper. Top with the second slice of bread, buttered side up. Close the sandwiches.\nPlace sandwiches in a preheated skillet. Cook 2 to 3 minutes or until the undersides are golden brown and the cheese has begun to melt. Flip sandwiches with a spatula, pressing lightly to flatten. Cook 1 to 2 minutes or until they are golden brown and the cheese is completely melted. Eat immediately.']"	['<urn:uuid:5251534e-fb07-4145-98af-f6e771ad5f1b>', '<urn:uuid:136f69e6-c878-4080-803d-eb4a57233b44>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T18:38:36.516973	32	71	3504
75	What makes product design both functional and inclusive?	Product design combines functionality and inclusivity through several key principles. From Rams' perspective, this meant creating simple, understandable designs like colored buttons and optimized handle positions that anyone could use without complex manuals. From an accessibility standpoint, it means designing for people of all abilities - using features like larger buttons, clear labels, and high-contrast colors that work for everyone, not just those with disabilities. This universal approach to design ensures products are both functional and usable by the widest possible audience.	['10 rules for a good product from Braun designer Dieter Rams\nIn the 1960s, the Braun technique, designed by Rams, was an innovation. Then the bulkiness and complexity in the design of household appliances were replaced by minimalism, compactness, functionality.\nAs a world renowned designer, Rams founded the Dister and Ingerborg Rams Foundation with his wife to popularize his own design philosophy. Rams began promoting his vision in 1980. His concept was ten simple rules for a good product that he follows throughout his life.\nDieter Rams was born in the German city of Wiesbaden in 1932. At the age of 15 he entered an art school, studied architecture and design. He completed an internship in a carpentry workshop in Kelkheim, after which he continued his studies and received a diploma.\nAfter training, he entered the service of a local architect, from 1953 to 1955 he helped develop projects for American consulates in West Germany. But the architecture did not greatly captivate young Rams.\nIn 1955, Rams changed his specialization: he went through a competition to become in-house designers at Braun. He subsequently took over the design department and worked for the company for 40 years.\nIn 1956, he worked with instructor Hans Gugelot on the SK4 turntable for Braun. Hans Gugelot is an industrial designer and one of the founders of the Ulm School of Design. Rams and Gugelot were the first to decide to abandon the bulky wooden case, the “chest” format, and the retro design.\n“During this time, it was normal for electrical goods to look like furniture. I wanted a more technical view of these things, “- said Rams in an interview.\nThe turntable Rams created was metal with two wooden sides in light colors. The control panel was located on top, near the disk, and not on the side. Therefore, buttons and toggle switches have become more visible and accessible.\nThe first thing the owner of the turntable saw was the control room. He didn’t have to bend over and look at the buttons located at the end.\nRams replaced the original metal cover with a transparent plastic one. Unlike metal, it did not vibrate and opened the mechanical part of the player to users. Customers liked the solution and soon became an industry standard. For the plastic cover, the device is popularly nicknamed “Snow White’s coffin”.\n2. Ease of use\nRams brought simple form and functionality to the design. He was so immersed in his work that he spent 18 weeks looking for the convenient location of the handles on the transistor. Making the buttons on the calculator multi-colored is also his idea. As well as place the handle of the hair dryer at an angle, so that it is more convenient to dry your hair from the back.\nI believed the gauges were supposed to be lightweight, not overloaded with functions that only technicians could understand. In addition, products must be designed to be understood.\nWe knew that most people don’t read manuals, and we had the idea to assign different functions to different colors. Small strokes of color make the thing brighter.', 'THE IMPORTANCE OF ACCESSIBILITY IN PRODUCT DESIGN\nIn today’s world, product design is not just about creating something that looks good or functions well. It’s about creating something that is accessible to everyone, regardless of their abilities. The importance of accessibility in product design cannot be overstated. It’s not just a matter of making products that are easy to use for people with disabilities, but it’s also about creating products that are inclusive and welcoming to everyone. In this blog post, we’ll explore why accessibility is so important in product design, and how it can benefit both the users and the creators of the products.\nFrom improving user experience to increasing market reach, accessibility is a crucial aspect of product design that cannot be ignored. So, let’s dive in and discover the importance of accessibility in product design!\nTHE IMPORTANCE OF ACCESSIBILITY IN PRODUCT DESIGN\nProduct design is a crucial aspect of any invention or product creation process. It involves the creation of a product that is not only aesthetically pleasing but also functional and user-friendly. One of the most important aspects of product design is accessibility. Accessibility refers to the ease with which a product can be used by people with disabilities. In this article, we will discuss the importance of accessibility in product design and why it should be a top priority for designers.\n1. Accessibility is a moral obligation and a legal requirement.\nPeople with disabilities make up a significant portion of the population, and they deserve to have access to products that are designed with their needs in mind. In fact, according to the World Health Organization, there are over 1 billion people with disabilities worldwide. This means that if a product is not designed with accessibility in mind, it is excluding a significant portion of the population.\n2. Accessibility in product design is not just about making a product usable for people with disabilities; it is also about creating a product that is easy to use for everyone.\nFor example, a product that is designed with larger buttons and clearer labels will be easier to use for people with visual impairments, but it will also be easier to use for people without disabilities who may have trouble seeing small buttons or reading small labels.\n3. Usability is a crucial aspect of accessibility in product design.\nA product that is not usable is not accessible. Usability refers to the ease with which a product can be used by its intended audience. This includes factors such as the layout of the product, the size and placement of buttons and controls, and the clarity of instructions and labels. A product that is not usable will not be successful, regardless of how aesthetically pleasing it may be.\n4. Inclusivity is another important aspect of accessibility in product design.\nInclusivity refers to the ability of a product to be used by people with a wide range of abilities and disabilities. This includes people with physical disabilities, visual impairments, hearing impairments, and cognitive disabilities. Inclusivity is important because it ensures that a product can be used by as many people as possible, regardless of their abilities.\n5. Involving people with disabilities in the design process is crucial for ensuring accessibility in product design.\nThis can include consulting with disability advocacy groups, conducting user testing with people with disabilities, and incorporating feedback from people with disabilities into the design process. By involving people with disabilities in the design process, designers can ensure that their products are truly accessible and usable for everyone.\n6. Universal design principles can also help ensure accessibility in product design.\nUniversal design refers to the design of products that can be used by as many people as possible, regardless of their abilities. This includes things like designing products with larger buttons and clearer labels, using high-contrast colors, and incorporating tactile feedback into products. By using universal design principles, designers can create products that are accessible and usable for everyone, not just people with disabilities.\n7. Accessibility in product design makes good business sense.\nBy designing products that are accessible and usable for everyone, designers can tap into a larger market and increase their customer base. This is especially true in industries such as healthcare and assistive technology, where accessibility is not just a nice-to-have feature, but a necessity.\nIn conclusion, accessibility is a crucial aspect of product design that should be a top priority for designers. It is not just a legal requirement or a moral obligation; it is also a business imperative. By designing products that are accessible and usable for everyone, designers can tap into a larger market and increase their customer base. To ensure accessibility in product design, designers should involve people with disabilities in the design process, use universal design principles, and prioritize usability and inclusivity. By doing so, they can create products that are truly accessible and usable for everyone.\n- A guide to accessible design for connected products and services\nJun 20, 2021 … When designing with accessibility in mind, we maximize the chances of everyone being able to properly use a product – regardless of ability, …\n- 3 Reasons Why Accessible Design Is Good for All | IxDF\nMay 19, 2021 … One of the primary reasons that accessibility has wider benefits than allowing people with disabilities to use your products is the curb-cut …\n- Accessible design means better design | McKinsey & Company\nMay 21, 2020 … Accessible design means better design … As digital evolves and becomes the primary channel for products and services, there’s a growing …\n- Why Usability And Accessibility Are So Important In Design\nDec 21, 2022 … The aim of accessibility is to remove barriers for perceiving, understanding and navigating your interface and ensure that nobody is excluded.\n- Drive innovation with accessible product design | McKinsey …\nOctober 19, 2020 Thirty years ago, the United States passed the Americans with Disabilities Act (ADA) of 1990, which set accessible design standards for …\n- The Ultimate Guide to Product Design Accessibility\nTo me as a product designer, it was unacceptable. Nevertheless, it got me thinking that awareness of the accessibility of digital products is still in its …\n- Accessible Design: Why And How To Make It A Priority In 2021\nJan 27, 2021 … The case for accessible design is similarly strong on the revenue side of the ledger. Most notably, it can unlock new customer markets.\n- Here’s What You Need to Know About Color Accessibility in Product …\nOct 11, 2018 … Accessibility in digital product design is the practice of crafting experiences for all people, including those of us with visual, speech, …\n- Designing accessible products. A comprehensive visual guide to …\nApr 14, 2017 … That’s when accessibility becomes a crucial element of the design process that has inclusion and diversity in mind. An accessible product — in …\n- Why It’s Important To Build A Better Product Through Accessibility …\nMar 18, 2019 … “Accessible products are better because, as the social model reminds us, they allow more people to participate in society, act in the world more …\nThe lesser-known side of The Importance of Accessibility in Product Design\n- The first recorded patent was granted in Venice, Italy in 1474 for a device that improved the efficiency of canal locks.\n- The concept of user-centered design originated in the 1940s with the work of industrial designer Henry Dreyfuss.\n- In 1956, IBM introduced its first computer with a keyboard and monitor, revolutionizing how people interacted with technology.\n- The term “ergonomics” was coined by British psychologist K.F.H Murrell in 1949 to describe the study of human factors and their relationship to design.\n- Apple’s iconic iPod music player was designed by Jonathan Ive and his team at Apple’s Industrial Design Group using principles such as simplicity, elegance, and intuitive use.\n- Inclusive design is an approach that seeks to create products or environments that are accessible to all users regardless of ability or disability status.\n- Universal design is another approach focused on creating products or environments that can be used by as many people as possible without requiring adaptation or specialized features\nAre you ready to become an inventor?\nGetting your idea out of your head and into your hands is only the first in a long set of steps towards becoming a successful inventor.']	['<urn:uuid:3e10bcfc-f508-4450-8376-c5c3664d5e62>', '<urn:uuid:f8f4f1c1-f083-41cf-85de-532da03a7deb>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	8	82	1923
76	chacchoben mayan site plaster temple markings still visible today dr peter harrison first exploration archeologist	Yes, at Chacchoben Mayan site, you can still see some of the original plaster at the back of Edifice 24, which is a medium to large stepped pyramid. The site was first explored by American Archaeologist Dr. Peter Harrison in 1972, who discovered it by chance during a helicopter flight and subsequently made the first maps and reported the ruins to the Mexican Government.	['Chacchoben Mayan “The Place of Red Corn”\n200 BC – first inhabitants lived in small villages near the water. Population grew.\n700 AD – Cultural peak. Most of the construction of buildings and temples.\n1942 – Mayan Serviliano Cohuo settles on property and raises family near the ruins.\n1972 – Dr. Peter Harrison, American Archeologist, made the first exploration and maps and reported Chacchoben ruins to the Mexican Government after a fluke spotting of the site from a helicopter.\n1978 – Serviliano Cohuo, was designated honorary guard of Chacchoben and was granted the right to stay at the site1994 – Chacchoben restoration project under INAH (National Institute of Anthropology and History) started. Complexes identified as Group I-A and Group I-B were excavated and restored.\n2002 – Opened to the public.\nLOCATION & DESCRIPTION\nChacchoben is 165 KMs south of Tulum on the Yucatan Peninsula in Mexico. It is located in bush/jungle not far from the main highway 186, the north / south corridor from Cancun to Chetumal on the Caribbean coast.\nMAJOR GROUPINGS & STRUCTURE DESCRIPTIONS\nThere is a circular path that leads you to the restored temples and other buildings. There is continued research in the site and you may see ongoing work.\nNote that INAH has set up palapa to prevent erosion and degradation from the natural elements.\nThe surrounding area has an abundant wildlife such as armadillo, deer, gray fox, peccary and spider monkey. Often you can hear howler monkeys at the site. Larger canines live deeper in the jungle\nEdifice 24 is a medium to large stepped pyramid with a central stairway leading to the top. There are three separate levels and the top level with the remains of a temple.\nAt the back of the temple you can see some of the original plaster.\nPlaza B Las Vias\nOpen Daily from 8 am to 5 pm.\nAdmission 2013 46 pesos. Free on Sundays for nationals and Mexican residents.\nAllow at least 2 hours, especially if you like to climb and take pictures.\nYou can stroll safely undisturbed throughout the site – often you will be the only ones there. Take good shoes as there is plenty of walking and climbing- and a fair amount memory and batteries for your photographs.\nAs this is site is in a somewhat remote jungle area, take water and insect repellant. There are no restaurants or food vendors.\nDress according to the season you are visiting.\nGPS – N19.00.02 W88.13.57\nTake a bus to Limones or Pedro Santos and then a private Taxi to the site – approximately 200 pesos. Ask the taxi driver to return.\nTours and tour buses go frequently to Chacchoben and can be arranged on the internet. Often tour buses pick people up from the Costa Maya cruise terminal or Majahual.\nDrive from Cancun:\nTake Highway 307 and drive south past Tulum and Felipe Carrillo Puerto. Take the first right after KM 71 towards the ruins. Takes approximately 3.5 hours\nDrive from Chetumal:\nTake Highway 307 North to KM 170. Turn Left towards the ruins.']	['<urn:uuid:233481a9-bf3e-4a1a-a5e4-705435742ef3>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	15	64	509
77	I specialize in family dispute resolution and would like to understand how different states handle mandatory mediation - what are the key differences between California's and Virginia's approaches to mediation services and their associated costs?	In Virginia, courts may refer parties to dispute resolution at no additional cost, with the state paying the mediator's fee as an incentive for participation. Any unresolved issues after mediation will be handled by the court. In California, through the Department of Child Support Services (DCSS), each county has its own agency that provides no-cost services to help obtain, modify or collect support. Additionally, California offers free services through the Family Law Facilitator's Office, which helps self-represented individuals navigate the court system, explaining procedures and assisting with judicial forms for divorce, custody, and support matters.	['Family Law Attorney Paul Lozada Family Resources\nDepartment OF Child Support Services\nEach county has their own child support agency that can help you obtain, modify or collect support. There is no charge for this service through the Department of Child Support Services also known as “DCSS”. For more information please visit their website.\nFamily Law Facilitator’s Office\nThe Family Law Facilitator’s Office helps self-represented people navigate their own case through the court system by explaining court procedure, and helping with the preparation of Judicial forms for divorce, custody, parentage, visitation and calculate support.\nThis a no-cost service available at the civil/family law courthouse located on 3055 Cleveland Ave, Santa Rosa, 95403. Hours are limited, please see the Family Law Facilitator for more information.\nParent Guide to Separation and Divorce\nParents, children, and teens often have a difficult time dealing with changes that come along with divorce. A fantastic resource for many going through a divorce with children is offered through the California Courts website. See “Families Change” by clicking here.\nThis commendable Family Change website includes a Parent’s Guide, Kid’s Guide, and Teen’s Guide with an interactive game for the younger children.\nWe encourage you to familiarize yourself with this website BEFORE asking your teen to view it. Within the teen guide, click on “Rights & Responsibilities”. This insightful article consists of a type of “contract” that in simplest terms assigns rights and responsibilities among parent and child. This gives the child a sense of control that he/she can express his/her feelings safely, while opening up dialogue that you may not otherwise have known about.\nHave you heard the word “gatekeeping” tossed around and wondered what the fuss is? “Gatekeeping” refers to attitudes or behaviors by either parent that affect the quality of the other parent-child relationship in a negative way. Some examples of gatekeeping are:\n- Restricting access to phone calls or Skype\n- Withholding information regarding school functions, homework, and events\n- Being disruptive of the other parent’s custodial time by trying to control the activity or excessive calling, etc.\n- Communicating critical or disrespectful remarks to the child regarding the other parent.\n- Trying to replace someone else, such as your new partner, as child’s parent\nChildren do not like to hear their parents say bad things about the other. It causes them great anxiety and could eventually lead to self-destructive behavior. Try to do everything in your power to avoid hostility between the parent of your child and yourself. Research shows that children who have a healthy relationship with both parents show best long-term adjustment post-divorce. Counseling is available to help with this extremely difficult but important time in your life. Psychology Today provides a list of therapists in the Sonoma County, California area here.\nLegal Lingo & Definitions\nAlimony/Spousal Support: A payment of support provided by one spouse to the other.\nAnnulment: a marriage can be dissolved in a legal proceeding in which the marriage is declared void, as though it never took place. In the eyes of the law, the parties were never married. It is available only under certain limited circumstances.\nAppeal: A legal action where the losing party requests that a higher court reviews the decision.\nBest Interests of the Child: Legal standard used to determine child custody, visitation, and support.\nCustodial Parent: The spouse who has physical custody the spouses’ child or children.\nCustody-Sole & Joint refers to the legal arrangements for whom a child will live with and how decisions about the child will be made. Custody has two parts: legal and physical. Legal custody is the decision-making part: physical custody refers to where the child lives on a regular basis. Generally, the parent the child does not live with will be allowed to have regular visits with the child. Parents can make any custodial arrangement that is in the best interest of their children. The standard for custody is “best interest of the child.”\nDefault: A party’s failure to answer a complaint, motion, or petition.\nDissolution: The legal end of a marriage.\nEquitable Distribution: The division of property between the spouses, and usually only that property bought or acquired by one or both spouses during the marriage.\nEx Parte Orders: Orders issued without formal hearings. They are generally issued on an emergency basis because there will be irreparable harm to a party if the order is not granted.\nFiduciary Duties: Throughout the marriage and after separation, spouses have a fiduciary duty to act in each other’s best interests in financial transactions.\nFiling: Giving the Clerk of Court your legal papers.\nHome State: The state where a child or children of the marriage lived with a parent for at least six months before a child custody, support or visitation action was filed in court.\nJudgment: A court’s decision.\nJurisdiction: The authority of the court to hear a case.\nLegal Separation: A court order allowing spouses to live separate and apart while remaining legally married.\nMaintenance: One spouse’s payment to the other for financial support; the same as alimony or spousal support.\nMarital Property: Includes all property acquired during the marriage.\nMotion: A request to the court.\nNon-Custodial Parent: The spouse who doesn’t have physical custody of the spouses’ child or children.\nNo-Fault Divorce: A divorce that doesn’t require one spouse to prove the other spouse’s fault or misconduct before being entitled to a divorce.\nNon-Marital Property: Property that belongs to only one spouse and won’t be included in any equitable distribution of property.\nNotice: The formal legal process of informing one spouse about a legal action or proceeding involving that spouse.\nOrder: A court’s ruling or decision on a certain matter or legal issue, usually a decision on a motion filed by one spouse.\nPaternity Test: Proving the identity of a child’s biological father through scientific methods.\nPendente Lite: Temporary arrangements for custody, child support, child visitation, alimony, and possession of the family home, etc., until a final hearing.\nPetitioner: The spouse who filed the divorce petition.\nPro Per or Propria Persona: Acting on one’s own behalf, generally used to identify a person who is acting as his/her own attorney in a lawsuit.\nQualified Domestic Relations Order (QDRO): A court order giving one spouse a share of the other spouse’s pension or retirement funds.\nResidency Requirement: The amount of time a spouse must live within a state or county before that spouse may file a divorce action in that state or county.\nSeparate Property: Property or assets that belong to one spouse and usually won’t be included in the property distribution or division.\nService: Providing a copy of the papers being filed to the other side.\nStipulation and Order: A binding agreement of the parties signed by both parties and the judge. This eliminates the need to ask the judge to decide the issue.\nSubpoena: A form issued by the court requiring someone to appear in court and/or bring documents.\nTemporary Support: Payments made by one spouse to the other for financial support while the divorce action is pending.\nUncontested Divorce: When the Respondent is not going to try to stop the divorce, and there are no issues for the court to decide about the children, money, or property.\nVenue: The county where the case is heard.\nVisitation: The non-custodial parent’s right to spend time with the spouse’s child or children.\nContact Our Family Resource Attorneys Now\nAt the Law Office of Paul Lozada, you can reach out to a skilled Santa Rosa Family Law Attorney. We have all the resources you will need in the case of legal separation or other family law situations. Our experience together with our knowledge of California’s family laws will give you the edge in an already difficult situation. Don’t hesitate, call today at (707) 636-3272.', 'Virginia Divorce & Separation\nDivorce and legal separations are of course common across the country, but each state has its own unique laws governing the procedures, and Virginia is no exception. What are the legal requirements for getting a separation or divorce in Virginia? Are there simplified procedures available? Is mediation a requirement before you can get divorced in Virginia? What about annulments in Virginia? Find the answers to your Virginia divorce and Virginia separation questions here.\nVirginia Legal Separation:\nVirginia courts will recognize and consider separation agreements in adjudicating divorce suits, but there is no legal separation per se, whether as a precursor to divorce or a separate entity. For more information about the differences between divorce, separation and annulment, see Ending a Marriage or Taking a Break.\nGrounds for Divorce/Fault – No Fault:\nVirginia has both fault and no-fault grounds for divorce. The sphere of fault-based reasons includes: adultery, sodomy committed outside the marriage, a felony conviction, sentencing to confinement for more than one year and failure of a return to cohabitation after that year. Cruelty, reasonable apprehension of bodily hurt, willful desertion or abandonment are also grounds for fault-based divorce, but only when at least one year has passed from the date of the act itself.\nDivorce will also be granted when the parties have lived separately and apart for one year. Where separation agreements exist and no minor children are party to the divorce, a divorce may be granted upon only six months of living separately and apart.\nResidency/Where to File for Divorce:\nSuits for divorce or annulment are not valid unless one of the parties has been a resident for at least six months before the suit is filed. The Circuit Courts have jurisdiction over all annulment and divorce lawsuits, along with claims for separate maintenance. Judges automatically hear the suits as “equitable claims.”\nAvailability of Simplified or Special Divorce Procedures:\nWhen the grounds for divorce are willful desertion or abandonment, there is no requirement to allege or prove an offer of reconciliation. When there are no specific alleged grounds (in other words, an uncontested, “no-fault” divorce), then the divorce is much more likely to proceed through the system quickly and smoothly. If both parties can work together on most of their issues, especially those concerning property and children, they will save themselves frustration as well as excess legal fees. However, these agreements are not necessarily binding on the court.\nVirginia Divorce/Child Support/Child Custody Lawyers:\nFind an experienced Virginia Divorce Attorney at AttorneyPages.com\nFind an experienced Virginia Child Support/Custody Lawyer at AttorneyPages.com\nHow a Family Lawyer Can Help\nDivorce Mediation in Virginia:\nThe court may refer the parties to a dispute resolution session at no extra cost, but only as circumstances allow. Any unresolved issues between the parties after the mediation session (or after several mediation sessions, if the parties agree to it) will be resolved by the court. As an incentive for the parties to mediate, the fee for the mediator is paid by the state of Virginia.\nAn annulment is a court declaration that the marriage is legally invalid. In other words, rather than ending a marriage via divorce, an annulment decrees the marriage was never valid to begin with, for reasons including: fraud, duress, incurable impotency at the time of the marriage, undisclosed prior felony conviction, undisclosed extramarital pregnancy or undisclosed extramarital child born within 10 months of the marriage, and either party’s having been a prostitute before marriage (also undisclosed at the time of the marriage contract).\nVirginia Online Divorce Services:\nVirginia Divorce Laws: Click below to find the Virginia Divorce laws you’re looking for:\nVirginia Divorce Law, Lawyers & Attorneys\nVirginia Divorce & Finances\nVirginia Child Custody & Virginia Child Support\nVirginia Divorce Laws & Resources']	['<urn:uuid:457c1b1b-6bed-4316-8958-716f89531d6c>', '<urn:uuid:022ce2df-7f4b-41e5-9135-c8d388b5529d>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:38:36.516973	35	95	1919
78	what makes someone develop narcissistic personality	The exact cause of narcissistic personality disorder is not known, but it's believed to result from a combination of biological vulnerabilities, social interactions with early caregivers, and psychological factors. It may develop when children experience either excessively pampering parenting styles or neglect and abuse during childhood. The disorder usually becomes evident by adolescence or early adulthood.	"['Narcissistic Personality Disorder\n- What are the symptoms of narcissistic personality disorder?\n- What causes narcissistic personality disorder?\n- How is narcissistic personality disorder diagnosed?\n- What is the treatment for narcissistic personality disorder?\n- What complications are associated with narcissistic personality disorder?\n- What is the prognosis for people with narcissistic personality disorder?\n- Can narcissistic personality disorder be prevented?\n- Find a local Psychiatrist in your town\nNarcissism is a term used to describe a focus on the self and self-admiration that is taken to an extreme. The word ""narcissism"" comes from a Greek myth in which a handsome young man named Narcissus sees his reflection in a pool of water and falls in love with it.\nNarcissistic personality disorder is one of a group of conditions called ""Cluster B"" or ""dramatic"" personality disorders. People with these disorders have intense, unstable emotions and a distorted self-image. Narcissistic personality disorder is further characterized by an abnormal love of self, an exaggerated sense of superiority and importance, and a preoccupation with success and power. However, these attitudes and behaviors do not reflect true self-confidence. Instead, the attitudes conceal a deep sense of insecurity and a fragile self-esteem. People with narcissistic personality disorders also often have a complete lack of empathy for others.\nWhat Are the Symptoms of Narcissistic Personality Disorder?\nIn many cases, people with narcissistic personality disorder:\n- Are self-centered and boastful\n- Seek constant attention and admiration\n- Consider themselves better than others\n- Exaggerate their talents and achievements\n- Believe that they are entitled to special treatment\n- Are easily hurt but may not show it\n- Set unrealistic goals\n- May take advantage of others to achieve their goals\nOther common traits of narcissistic personality disorder include the following:\n- Preoccupation with fantasies that focus on unlimited success, power, intelligence, beauty, or love\n- Belief that he or she is ""special"" and unique, and can only be understood by other special people\n- Expectation that others will automatically go along with what he or she wants\n- Inability to recognize or identify with the feelings, needs, and viewpoints of others\n- Envy of others or a belief that others are envious of him or her\n- Hypersensitivity to insults (real or imagined), criticism, or defeat, possibly reacting with rage, shame and humiliation\n- Arrogant behavior and/or attitude\nWhat Causes Narcissistic Personality Disorder?\nThe exact cause of narcissistic personality disorder is not known. However, many mental health professionals believe it results from a combination of factors that may include biological vulnerabilities, social interactions with early caregivers, and psychological factors that involve temperament and the ability to manage stresses. Some researchers believe that narcissistic personality disorder may be more likely to develop when children experience parenting styles that are excessively pampering, or when parents have a need for their children to be talented or special in order to maintain their own self-esteem. On the other end of the spectrum, narcissistic personality disorder might develop as the result of neglect or abuse and trauma inflicted by parents or other authority figures during childhood. The disorder usually is evident by adolescence or early adulthood when personality traits have become consolidated.\nViewers share their comments\nGet tips on therapy and treatment.']"	['<urn:uuid:d1e44c85-942b-4742-a959-a9fa83bcf5ab>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	6	56	540
79	I want to share my music on streaming platforms but I've heard the audio file needs to be in a specific format. Can you help me understand what settings I should use when saving my final song?	For releasing music on streaming platforms, the standard format is a stereo interleaved WAV file with a 44.1kHz sample rate and 16-bit depth. While some platforms now support higher quality settings, this standard format is more than adequate since it can reproduce frequencies up to 22.05kHz (beyond human hearing capability) and provides a 96dB dynamic range. When bouncing the final track, you'll need to apply dither as the last processing step. This standard originally came from CD requirements but remains widely used for digital distribution.	"[""What does it mean to bounce audio?\nIn modern day digital audio, the term ‘bouncing’ or ‘exporting’ refers to the process of turning either some or all of the things that are going on in your session into an audio file. This could be a bounce of your finished song that you can then release on iTunes or stream on Spotify. Or it could be a bounce of your mix to send off for mastering.\nThe correct way to bounce audio can seem tricky at first. Which format, which sample rate and which bit depth to use are all common questions. In this lesson, we’ll cover all of this as well as explaining which file types to use, online vs offline bouncing and some of the other reasons you might want to bounce down the audio in your sessions.\nBefore we go any further, if you want to make better music in your home studio, then be sure to download your free home studio bundle here (EQ cheat sheet, compression cheat sheet, & vocal guide).\nAudio bounce settings:\nWhen you’re ready to bounce your track, open your DAW’s bounce dialogue window. Here, you will be given different options to set different parameters for the file you’re about to create. These parameters include the file type, sample rate, bit depth, format and more. Let’s break down what each of these mean.\nFile type lets you determine what the format of the audio file that you create will be. Options usually include WAV, AIFF, MP3 etc.\nSample rate refers to the number of samples per second that a piece of digital audio is made up of. This dictates the maximum frequency that can be represented.\nBit depth defines the number of values available to represent the amplitude of the samples, and in turn dictates the dynamic range.\nFor the format, you will be given options such as mono, multiple-mono, and stereo-interleaved. Stereo-interleaved gives you a stereo file. Multiple-mono gives you two separate mono files, one for the left channel and one for the right channel. Mono summed will give you a summed version of both the left and right channels in a single file.\nOnline (real time) / Offline Bounce:\nAn online bounce will export the song in real time i.e. it will take as long to bounce the audio as it would to listen to it from start to finish. Offline bouncing is much quicker as it does not happen in real time. Generally speaking, online bouncing is a more reliable way to bounce your audio. That’s because, when bouncing in real-time, the audio will play for you to listen to as the bounce happens. This gives you the opportunity to listen to what’s being bounced to make sure everything is as it should be. It gives you the opportunity to catch an error before it is too late.\nNow, let’s look at how to set these parameters for some of the most common bouncing applications. Let’s start with the bouncing of your final mastered track.\nHow to bounce the final mastered track:\nWhen deciding on the settings for the bounce of the final track, the options you choose are largely determined by what it is that you’re going to do with your finished songs. You may want to upload them to a digital distributor like Landr or CD Baby to make them available on platforms like iTunes and Spotify. You may also want to upload them directly to sites like Sound Cloud for people to download and stream. Or you may want to burn them to CD.\nFor a long time the standard for the bounce of the final track was a stereo interleaved, WAV format file with a 44.1kHz sample rate and a bit depth of 16 bit. The reason for this was that, for a long time, CDs were the most common way to distribute your music. To write a digital audio file to an audio CD, the file must be a PCM format (i.e. WAV or AIFF), stereo file with a sample rate of 44.1kHz and a bit depth of 16 bits.\nLargely speaking, this remains the standard for the final bounce. However, as we move away from CD releases and towards streaming and digital downloads, you are now often able to release your songs to the world with a higher sample rate and a higher bit depth setting than the CD quality standard. Various platforms have their own recommendations for the sample rate and bit depth of the files that you upload.\nSo, should you bounce to a higher sample rate and bit depth if the platform you’re uploading to supports it?\nBouncing the final master above CD quality:\nWell, there is plenty of debate as to whether bouncing a file with a sample rate higher than 44.1kHz and a bit depth higher than 16 bit produces a better sounding file. First, let’s look at what you’re getting with a CD quality file. A sample rate of 44.1kHz can reproduce frequencies up to 22.05kHz. A bit depth of 16 bit gives you a dynamic range of 96dB. Let’s put that into context… The highest frequency that human beings can hear is 20kHz, and most of us can only hear up to about 15 – 18kHz. As for dynamic range, even really dynamic music like classical generally only has a dynamic range of about 50dB at most. Rock and pop songs have far less.\nNow, there are definitely reasons as to why you would want to record with higher sample rates and bit depths. There are reasons why you would want to stay at that higher resolution when you’re bouncing files that will be imported into another session for further processing as well. But when it comes to bouncing the final track, it’s evident that the frequency range and dynamic range that the 44.1kHz, 16 bit standard is capable of producing is more than adequate. That said, there are those who believe that they can hear an improvement with a higher resolution audio file. For me, CD quality is perfect for the final bounce. But if you’re not convinced, then why not run some tests for your self. Bounce a CD quality file and a higher resolution version and then have a friend blind test you to see if you can hear a difference.\nUsing dither when you bounce audio:\nBefore we move on to some of the other common audio bouncing applications, there’s something else that you need to be aware of when bouncing your final track.\nBecause your final bounce will have a bit depth of either 16 or 24 bit, you’ll need to apply something called ‘dither‘. In some DAW’s, the bounce dialogue will give you the option to apply dither when you bounce your audio. If not, you will need to add it via a plugin. This plugin should be the very last plugin that your audio passes through. As a rule, dither is required whenever you bounce either a 16 or 24 bit file. The only time you don’t need to apply dither is when you bounce to 32 bit floating point. Keep in mind that bouncing to 32 bit float is only appropriate when bouncing audio which will be imported into a DAW. It is not to be used as the bit depth for the final mastered track that you will release.\nNow, let’s look at some other occasions when you’ll need to bounce the audio in your session…\nHow do you bounce audio for mastering?\nThe final step in the production of a track, which comes after mixing, is mastering. Once you’ve mixed your track, you’ll need to bounce your mix so that it can be imported into a mastering session (or sent off to a mastering engineer). The file types most commonly used for this are WAV and AIFF as these are both lossless formats.\nTo bounce your track for mastering, keep your bit depth and sample rate the same as they currently are. You can convert to a lower sample rate and bit depth later when you bounce the final mastered track if you need to. Or alternatively, you can bounce a 32 bit floating point file (providing your mastering engineer accepts this). The benefit of bouncing to 32 bit floating point is that if there is any clipping in the track, the track can be turned down in the mastering session and can still be used. That said, best practice dictates that you should leave about 6dB of headroom in your mix for the mastering stage. So, providing you’ve done this, bouncing to your original bit depth should be fine.\nIf you’re bouncing to either 16 or 24 bit, you need to apply dither. If you’re bouncing to 32 bit floating point, you don’t. As previously mentioned, real time bouncing is the most reliable process. You’ll also want to set your format to stereo-interleaved.\nBouncing audio for stem mastering:\nA variation on the traditional mastering process, which involves the mastering of your mix as a single stereo file, is a process called stem mastering. Once your track is mixed and ready to be mastered, rather than bouncing the entire mix, you instead bounce each different group of instruments to individual tracks or ‘stems’. This gives you more control over the processing that is applied to each group of instruments at the mastering stage.\nTypical groups of instruments that would form the stems would be: drums, bass, acoustic guitars, electric guitars, synths, vocals etc. Some mastering engineers request certain things like the lead vocal or the kick drum as separate stems too. All automation, processing and effects should be printed into the stems when you bounce. That way, when all of the stems are imported into a session, they form your exact mix. Keep your sample rate and bit depth the same, or bounce to 32 bit floating point. Apply dither if you’re saving to 16 or 24 bit. Don’t use dither if you’re saving to 32 bit float. Bounce WAV or AIFF files in stereo interleaved format, and remember to leave 6dB of headroom.\nFinally, if you’re sending the stems to a mastering engineer, check which file type they prefer and whether they accept 32 bit float files if that’s what you intend to send.\nOther bouncing applications:\nThere are times when you may want to bounce audio for reasons other than the ones covered so far. A common process in analogue recording studios was to bounce down multiple tracks, like all of the drum tracks for example, to free up space for more tracks to be recorded on the tape machine which had only a limited number of tracks available. This could also be done to capture the sound of tracks being processed through a piece of outboard gear like a compressor or a reverb, so that it could be freed up for use with a different track.\nThat said, with huge track counts and the ability to use endless amounts of plugins in modern DAW’s, these factors are now less of a consideration. But there are still times when this process could come in handy. An example of this would be to free up CPU power by replacing several tracks and their plugins with a single track with the processing or effects baked in. Of course, the downside is that individual levels of those multiple tracks can no longer be altered after the bounce.\nThe correct settings to use when you bounce audio can seem daunting at first. Things are made even trickier when you consider the fact that the correct settings for you to use change depending on why it is that you’re bouncing your audio. Just refer to this lesson anytime you need to and you’ll get a perfect bounce every time.\nAt what stages in the production process do you typically bounce down your audio? Leave your thoughts in the comment section below.\nGet the best results from EQ, compression, vocals & drums with the FREE 'Home Studio Bundle'\nGet all 4 guides sent straight to your inbox when you subscribe to our mailing list here:""]"	['<urn:uuid:54b5bc0a-fc39-495c-acff-6bccfe4f1a12>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	37	85	2024
80	natural organic food labels marketing difference understand explain	While many assume 'natural' and 'organic' mean the same thing, they are very different in terms of regulation. The FDA only requires that 'natural' products contain no synthetic colors, flavors, or ingredients - they can still contain GMOs, high-fructose corn syrup, and be high in sugar and sodium. The USDA has specific requirements for meat labeled 'natural,' meaning no artificial ingredients or colors and minimal processing. In contrast, 'organic' is strictly regulated - certified organic products must be free of synthetic chemicals, pesticides, antibiotics, irradiation, and chemical fertilizers, and cannot use genetically modified seeds. There are different organic designations: '100% organic' means completely free of non-organic ingredients, 'organic' requires 95% or more organic ingredients, and 'made with organic ingredients' means 70% or more must be organic.	"['Written by Alexa Cortese, web intern\nMany of us don\'t consciously consider packaging and marketing when it comes to food shopping, but it can be a huge factor in what we decide to pile into our carts. Food companies make health claims so often that it\'s hard to know which ones are true and which ones are simply trying to sell more items! Even more important than understanding these health claims, however, is understanding exactly how to decipher the nutrition labels. That’s right, that panel on the back or side of the box you’d rather not look at (or maybe you look but don’t fully understand). Here are some helpful tips to keep in mind when grocery shopping, courtesy of Jill Marie Hively, R.D.\n- The most important thing to keep in mind when reading nutrition labels? Serving size and servings per container, Hively says. This gives you a clear label of the nutrients included relative to the amount of food you are consuming. In other words, a food that’s only 100 calories may sound like a smart option until you realize that a serving size is only ¼ cup (or not enough to really fill you up!). Use MyPlate as a good example of proportions, Hively recommends, and familiarize yourself with visual cues—a three-ounce serving of protein should be about the size of the palm of your hand.\n- Percent daily values (DV) can also be helpful. Generally, aim for foods that contain less than 5 percent of the daily value for cholesterol or saturated fat. If the label says one serving contains 20 percent or more of your DV of a vitamin or mineral, that a food is a ""good source"" of that particular nutrient.\nFor the scoop about ""natural,"" ""made with whole grains"" and more, keep reading.\n- Look for specific claims that link an nutrient to the prevention of a certain disease (for example, calcium is proven to prevent osteoporosis). These claims have been approved by the FDA. More abstract claims are not as closely regulated.\n- The USDA has stipulations about meat that can be labeled with the word ""natural."" “This means meat and poultry that contains no artificial ingredients or color, and are minimally processed,” Hively says. Unfortunately, the FDA, which regulates many other foods, does not have such regulations for the use of the word natural. Generally, foods without synthetic colors or flavors can be considered natural, but since this is not strictly regulated, labels can be misleading. Check the ingredients!\n- Speaking of ingredients, Hively says that fewer are generally better. ""Anything that contains more than five ingredients is probably more processed,"" she adds. Skip products with unpronounceable ingredients, since they\'re most likely chemicals you should not be ingesting on a daily basis—or at all. Focus on nutrient dense foods that are in their natural form, or as they would be found in nature, Hively suggests.\n- Whole-grain wheat and whole-wheat are the smartest grain options. Enriched wheat flour means that the wheat has been processed, bleached, with nutrients added back in later. For the most nutrition, look for ingredients that say ""100 percent whole-wheat."" (Hint: If there are more than five grams of fiber per serving, the wheat being used is probably whole). This is tricky territory: Some wheat products are made using 25 percent whole grain, while the other 75 percent is enriched and claims to be “made with whole grains.” The company is allowed to say the product is made with whole grains on the front of the package, and if you don’t read the ingredients more in depth, you may not be aware you were getting ¾ enriched flour.\nNow tell us: What confuses you most about nutrition labels or claims? We can tackle your burning questions in a future post!', 'No doubt about it – the best foods have no labels unless it happens to be an organic sticker. These foods, in their natural state, are usually located along the perimeter of the store and have one ingredient – the food itself. It would be ideal if those were the only foods we ate. However, sometimes we’re in such a rush that we have to grab something quick, convenient, and wrapped in a package.\nHere’s the good news. Not every food you buy in a package is bad for your health. In fact, frozen fruits and vegetables are one of the best options when you’re pressed for time. The trick is to avoid frozen produce that contains too much added sodium or an unhealthy sauce.\nYou also have to be aware that the labels on packaged products are deceptive. After all, a manufacturer is out to sell a product and portray it in the most favorable light. So, be ready to do a little detective work. Here are five ways you can get fooled at the grocery store.\nCalling Not So Healthy Ingredients by Another Name\nIf there’s one ingredient manufacturers want to downplay it’s sugar. So, rather than calling it sugar on the ingredient list, they give it a more cryptic name. They have a lot to choose from. In fact, there are more than 60 names for what is essentially sugar. Fortunately, life recently got a little easier. Manufacturers now have to state how much ADDED sugar is in a product on a separate line on the label. The key is to make it a habit to read the nutritional information and the line that lists added sugar.\nWhat does added sugar include? According to Sugar Science, a website devoted to scientific information pertaining to sugar:\n“added sugars are any sugars – including table fructose, sugar, and even honey – either used in processing and preparing foods or beverages, added to foods at the table or eaten separately”\nSugar isn’t the only ingredient that assumes disguises. If you’re sensitive to MSG (monosodium glutamate), it probably won’t be listed as such on the label. Instead, it’ll be under the guise of hydrolyzed vegetable protein or yeast extract.\nCertain ingredients have a “health halo.” For example, when you see flaxseed, green tea, or spirulina in a product, you think you’re getting a product designed with health in mind. Not always. In fact, manufacturers often add a tiny bit of a “healthy” ingredient to a product just so they can say it’s in there, even if the amount is too small to be of benefit. This gives the impression their product has health benefits even if the rest of the ingredients include high-fructose corn syrup, salt, MSG, and synthetic colorings.\nHow can avoid being a victim of label padding? Look at where the healthy ingredient is listed on the ingredient list. If it’s at the bottom of the list, there’s probably not enough to impact your health one way or the other. It certainly won’t compensate for the other unhealthy ingredients.\nUsing the Word Natural Too Loosely\nWho doesn’t feel good when they hear the word “natural?” Don’t take it too literally. In the world of product labeling, natural is a term that manufacturers can freely apply to labels even if the product contains GMOs or high-fructose corn syrup and even if the ingredients were grown or raised with pesticides or antibiotics. The only requirement the FDA has for products labeled natural is that the product contains no synthetic colors, flavors, or ingredients. They can still be high in sugar and sodium. Natural says nothing about how healthy a product is for you.\nSome people think natural and organic are synonymous but they’re not. Certified organic products conform to a strict set of standards. Food grown or raised organically must be free of synthetic chemicals, including pesticides, antibiotics, irradiation, and chemical fertilizers and cannot be the product of genetically modified seeds. Organic is regulated while natural is not.\nThere are several different organic designations. If a product is labeled as 100% organic it must be free of non-organic ingredients. If it’s simply stamped “organic,” 95% or more of the ingredients must be organic, whereas “made with organic ingredients,” means 70% or more must be organic. Remember, organic may mean lower in chemicals and pesticides but it doesn’t always equate with healthy.\nAlthough the FDA is committed to eliminating trans-fat from all foods, with a few exceptions, it may still be a few years before it’s universal. Fortunately, more manufacturers are reducing or eliminating trans-fat from their offerings. One thing you can’t depend on though is reading the nutritional information. Trans-fat doesn’t always mean zero trans-fat. That’s because the FDA defines “zero” as 0.5 grams or less of trans-fat per serving. That means the zero trans-fat product you’re holding in your hand may STILL contain trans-fat. The way to know is to read the ingredient list. If it lists hydrogenated oils, like hydrogenated soybean oil, it’s not trans-fat free.\nThe Whole Grain Deception\nResearch shows people who eat more whole grains have a lower risk of premature death. Therefore, manufacturers want you to think their product is made with whole grains. Where they get sneaky is by mixing whole grains with refined grains and then labeling the product as “multi-grain” or “made with whole grains.” Neither of these terms means the item is 100% whole grain. If it truly is a whole-grain product, whole wheat or whole grain should top the ingredient list, not be an afterthought at the bottom of the list.\nThe Bottom Line\nThese are just a few of the “tricks” manufacturers use to make you think their product is healthful and nutritious, or at least not as bad as you think. Don’t get deceived. Ignore the marketing speak and buzzwords like “natural” and “wholesome” and make your own judgment. You’ll only get the truth by reading the ingredient list carefully. There’s a reason they list those ingredients with such a small font. They’re hoping you left your glasses at home and will make your decision based on the front of the package. Don’t do it.\nU.S. Food and Drug Administration. “Food Labeling Guide”\nSugar Science. “Hidden Sugars may have serious effects on children’s heart health”\nUnited States Department of Agriculture “Organic Standards”\nHarvard T.H. Chan School of Public Health. “Eating more whole grains linked with lower mortality rates”']"	['<urn:uuid:d96f541a-6d27-4cdb-ab58-557479cf08ee>', '<urn:uuid:cb816971-9a97-4c51-bb1e-0f9cf87072df>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	8	126	1703
81	galaxy clusters dark matter distribution measurement	Galaxy clusters serve as both tools to study dark matter distribution and evidence of potential gaps in our understanding. Studies using the Hubble Space Telescope and VLT found that dark matter concentrations in massive galaxy clusters produce lensing effects 10 times stronger than expected, particularly near cluster cores. These clusters behave like 'Russian dolls' with self-similar shapes across different sizes, but this pattern only holds true if the universe expands at the correct rate. By studying 320 clusters at various distances (from 760 million to 8.7 billion light years), scientists can measure both the universe's expansion rate and create detailed maps of dark matter distribution.	"['""Russian Doll"" Galaxy Clusters Reveal Information About Dark Energy\nWe are happy to welcome Dr. Andrea Morandi as our guest blogger, who is giving us insight into his recent work on using galaxy clusters to investigate the nature of dark energy. Originally from Italy, Dr. Morandi received his Ph.D. in astronomy from the University of Bologna. Prior to his current position as a research assistant at the University of Alabama in Huntsville, Dr. Morandi was a post-doctoral fellow at the DARK Cosmology Center in Copenhagen and Tel Aviv University, followed by time as a research associate at Purdue University.\nIn 1998 and 1999 astronomers discovered the accelerating expansion of the Universe, providing evidence for the existence of the mysterious dark energy driving this acceleration. The same year I started to study astronomy at the Bologna University, fascinated by this major breakthrough in cosmology. I guess my interest for cosmology started from here.\nThree or four years later, I got my first-hand impact with cosmology in a lab exercise where the students were supposed to do what radio astronomers call the log(N)-log(S) test. The professor of the course told us the result should show that we lived in a static Euclidean Universe, that is roughly a Universe that exist in a flat two-dimensional plane, like in our immediate surroundings.\nI came back to him a few days later and I told him: “Professor, I cannot reproduce your result, either the data are wrong or I made a major discovery.” He replied, “There is nothing wrong with the data, but you didn’t make any new discovery. Somebody else already figured out before you that we do not live in a static Euclidean Universe.” So I replied, “Well, I am just a few years late!” With the tail between my legs I went back to study and give exams. Hence, later on I became more an astronomer than cosmologist at heart.\nOne day during the summer of 2015, I was looking at a figure in a scientific paper where the authors defined galaxy clusters -- the largest structures in the Universe held together by gravity -- as “cosmic lighthouses.” However, I thought that if clusters are lighthouses, they should not feel the physical properties of the underlying Universe (and we know they do) where they are embedded. What about calling clusters “cosmic buoys floating on the sea of the Universe?” What a beautiful and poetic way to define an astronomical object!\nThe skeptical scientist would, of course, wrinkle his nose at my weakness for poetry, and ask, “Wait, but what about dark energy?” All right, let’s start from the beginning and try to understand what dark energy is. Once the age of the Universe is old enough, the once-decelerating expansion of the Universe starts to speed up. To explain that acceleration, around 70% of the Universe has to be in a form of mass and energy that\'s able to create the acceleration, with a kind of anti-gravitational effect. Ordinary material would indeed just attract things creating a deceleration of expansion. So here is our dark energy, the name scientists gave to the mysterious force that is driving this acceleration.\nWe actually don\'t know what dark energy is, in the sense that we can\'t identify it yet as being a particular type of matter that is predicted by particle physics. The evidence for its existence is only indirect and comes from the accelerated expansion of the Universe. But we know what it does – it drives away the Universe -- and there are many possibilities for what it could be. One proposed form for dark energy is the “cosmological constant,” a constant-with-time energy density filling space homogeneously. The other possibility is an energy field such as “quintessence”, whose energy density can vary in time and space.\nThe cosmological constant, also referred to as “Einstein’s cosmological constant” since he first postulated it, can be formulated to be equivalent to the energy associated with empty space. Energy fields that do change with time and space can be difficult to distinguish from a cosmological constant because they may slowly change.\nBut how do we study dark energy if we don’t know what it is? Well, with high-precision measurements of the expansion of the Universe we can understand how the expansion rate changes over time and space, and assess whether dark energy is truly the cosmological constant or quintessence.\nNow, let’s go back to our poetic definition, where we have some cosmic buoys – our galaxy clusters – anchored to the evolution of the Universe. You can figure out where the sea is agitated or calm – or how fast the Universe is expanding driven by dark energy – by observing these buoys. At the same time, we just published a paper based on data from the NASA’s Chandra X-ray Observatory, where we proved that the outer reaches of galaxy clusters are self-similar objects, that is behaving like Russian dolls, with smaller ones having a similar shape to the larger ones. I guess my earlier cosmology passion emerged and prompted the question: “What would happen if I changed the cosmology in which our clusters are embedded?” The short answer is that clusters wouldn’t be self-similar any longer, with very distant clusters being intrinsically too small and bright compared to nearby ones if the Universe is expanding too slowly. No more Russian dolls!\nSo we thought to play the reverse game to constrain the cosmology by assuming the self-similar Russian-doll model. We could do this because we had data on 320 galaxy clusters with distances from Earth that ranged from about 760 million light years to about 8.7 billion light years, overlapping the era where dark energy caused the once-decelerating Universe to accelerate. Thus, we could measure the rate of the Universe’s expansion by studying how dark energy drifts away our galaxy clusters acting as cosmic buoys. The process to unveil the right cosmology is akin to determining the unknown weight of an object by adding or subtracting known weights to a balance scale until the two sides – nearby and far away clusters – balance.\nWe further combined the X-ray data from galaxy clusters with information on the expansion rate of the Universe from optical observations of supernovas, and the cosmic microwave background, the leftover radiation from the Big Bang. Our findings support with great accuracy (about 2-3 percent) the idea that dark energy is best explained by the Einstein’s cosmological constant.\nSo the moral of the story after studying these clusters/cosmic buoys/Russian dolls is that: (i) the mysterious dark energy is now a bit less mysterious; and (ii) I may have a gift… for poetry!\nPlease note this is a moderated blog. No pornography, spam, profanity or discriminatory remarks are allowed. No personal attacks are allowed. Users should stay on topic to keep it relevant for the readers.\nRead the privacy statement', '“One new theory says that dark matter may be ordinary matter in a parallel universe. If a galaxy is hovering above in another dimension, we would not be able to see it. It would be invisible, yet we would feel its gravity,” conjectured physicist Michio Kaku, about the mysterious, invisible phenomena that glues stars, dust, and gas together in a galaxy–its mass– and forms the foundation of our universe’s large-scale structure. Because dark matter does not emit, absorb, or reflect light, its presence is only known through its gravitational pull on visible matter in space. Astronomers and physicists are still trying to pin down what it is.\n“With increasing distance, our knowledge fades, and fades rapidly. Eventually, we reach the dim boundary—the utmost limits of our telescopes. There, we measure shadows,” said astronomer Edwin Hubble, a quote that syncs perfectly with the image above captured by his namesake NASA/ESA Hubble Space Telescope and the European Southern Observatory’s Very Large Telescope (VLT) in Chile that unvelied that something may be missing from the theories of how dark matter behaves in the massive galaxy cluster MACSJ 1206. Embedded within the cluster are the distorted images of distant background galaxies, seen as arcs and smeared features. These distortions are caused by the invisible dark matter in the cluster, whose gravity bends and magnifies the light from faraway galaxies, the effect of gravitational lensing.\nGravitational lensing allows astronomers to study remote galaxy clusters, the most massive and recently assembled structures in the Universe, and the largest repositories of dark matter. Clusters are composed of individual member galaxies that are held together largely by the gravity of dark matter. Astronomers measured the amount of gravitational lensing caused by this cluster to produce a detailed map of the distribution of dark matter — unveiling an unexpected discrepancy between observations of the dark matter concentrations in a sample of massive galaxy clusters and theoretical computer simulations of how dark matter should be distributed in clusters. The new findings indicate that some small-scale concentrations of dark matter produce lensing effects that are 10 times stronger than expected.\nGalaxy Clusters -“Ideal Laboratories”\n“Galaxy clusters are ideal laboratories in which to study whether the numerical simulations of the Universe that are currently available reproduce well what we can infer from gravitational lensing,” said Massimo Meneghetti of the INAF-Observatory of Astrophysics and Space Science of Bologna in Italy, the study’s lead author.\n“We have done a lot of testing of the data in this study, and we are sure that this mismatch indicates that some physical ingredient is missing either from the simulations or from our understanding of the nature of dark matter,” added Meneghetti.\nThe video shows an artist’s impression of small-scale concentrations of dark matter in MACSJ 1206 (represented in this video in blue). These blue halos reflect how the galaxy cluster’s dark matter is distributed. Credit: NASA, ESA, G. Caminha (University of Groningen), M. Meneghetti (Observatory of Astrophysics and Space Science of Bologna), P. Natarajan (Yale University), the CLASH team, and M. Kornmesser (ESA/Hubble)\nThe Missing Feature\n“There’s a feature of the real Universe that we are simply not capturing in our current theoretical models,” added theoretical astrophysicist Priyamvada Natarajan of Yale University in Connecticut, U.S., one of the senior theorists on the team. “This could signal a gap in our current understanding of the nature of dark matter and its properties, as these exquisite data have permitted us to probe the detailed distribution of dark matter on the smallest scales.”\nThe higher the concentration of dark matter in a cluster, the more dramatic its light-bending effect. The presence of smaller-scale clumps of dark matter associated with individual cluster galaxies enhances the level of distortions. In some sense, the galaxy cluster acts as a large-scale lens that has many smaller lenses embedded within it.\nCluster Cores Harbor the Unexpected\nTo the team’s surprise, in addition to the dramatic arcs and elongated features of distant galaxies produced by each cluster’s gravitational lensing, the Hubble images also revealed an unexpected number of smaller-scale arcs and distorted images nested near each cluster’s core, where the most massive galaxies reside. The researchers believe the nested lenses are produced by the gravity of dense concentrations of matter inside the individual cluster galaxies. Follow-up spectroscopic observations measured the velocity of the stars orbiting inside several of the cluster galaxies to pin down their masses.\n“The data from Hubble and the VLT provided excellent synergy,” shared team member Piero Rosati of the Università degli Studi di Ferrara in Italy, who led the spectroscopic campaign. “We were able to associate the galaxies with each cluster and estimate their distances.”\n“The speed of the stars gave us an estimate of each individual galaxy’s mass, including the amount of dark matter,” added team member Pietro Bergamini of the INAF-Observatory of Astrophysics and Space Science in Bologna, Italy.\nBy combining Hubble imaging and VLT spectroscopy, the astronomers were able to identify dozens of multiply imaged, lensed, background galaxies. This allowed them to assemble a well-calibrated, high-resolution map of the mass distribution of dark matter in each cluster.\nThe team compared the dark-matter maps with samples of simulated galaxy clusters with similar masses, located at roughly the same distances. The clusters in the computer model did not show any of the same level of dark-matter concentration on the smallest scales—the scales associated with individual cluster galaxies.\n“With high-resolution simulations, we can match the quality of observations analyzed in our paper, permitting detailed comparisons like never before,” said Stefano Borgani of the Università degli Studi di Trieste, Italy.\nSource: Massimo Meneghetti et al, “An excess of small-scale gravitational lenses observed in galaxy clusters” Science 11 Sep 2020: Vol. 369, Issue 6509, pp. 1347-1351, science.sciencemag.org/cgi/doi … 1126/science.aax5164\nImage credit top of page: The Hubble image is a combination of visible- and infrared-light observations taken in 2011 by the Advanced Camera for Surveys and Wide Field Camera 3. Credit: NASA, ESA, G. Caminha (University of Groningen), M. Meneghetti (Observatory of Astrophysics and Space Science of Bologna), P. Natarajan (Yale University), and the CLASH team.']"	['<urn:uuid:0a7d8aa7-a1b2-4411-bd78-d8e23135cb4d>', '<urn:uuid:224a9bb0-6cb4-4909-9c47-b0ec536813f3>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	6	105	2156
82	Looking at both the TNG telescope and the Gemini observatory, how do they compare in their ability to observe different types of light from space?	The Gemini telescopes can collect and focus both optical and infrared radiation from space. Similarly, the TNG has multiple instruments that can observe in both visible light (through HARPS-N, DOLoRes, and SARG) and near-infrared (through NICS and GIANO).	"['Telescopio Nazionale Galileo\nThe dome of the Telescopio Nazionale Galileo near time of sunset.\n|Organization||Italian National Institute of Astrophysics (INAF)|\n|Location||Roque de los Muchachos Observatory, La Palma (Canary Islands)|\n|Built||1998 (first light)|\n|Collecting area||~12 m2|\nThe Telescopio Nazionale Galileo, or TNG, is a 3.58m Italian telescope located on the island of San Miguel de La Palma (or, more simply, La Palma), in the Canary Islands archipelago. It is one of the largest telescopes hosted by the Roque de los Muchachos Observatory, a very important observing site in the northern hemisphere. It is now operated by the ""Fundación Galileo Galilei, Fundación Canaria"", a no-profit institution which manages the telescope on behalf of INAF, the Italian National Institute of Astrophysics. The telescope saw its first light in 1998.\nObservations at the TNG can be proposed through the Italian Time Allocation Committee (TAC) which assigns, based on the scientific merit of the proposals, 75% of the available time. The rest of the time is at disposal of the Spanish and international astronomical communities. The TNG is open to new proposals two times a year, typically in March–April and September–October.\nThe TNG is an altazimuthal reflecting telescope with a Ritchey-Chretien optical configuration and a flat tertiary mirror feeding two opposite Nasmyth foci. It has a design derived from the New Technology Telescope (NTT), an ESO 4-meters class telescope located in La Silla (Chile). Therefore, the optical quality of the telescope is ensured by an active optics system performing real-time corrections of the optical components and compensating, in particular, for the deformations of the primary mirror, which is too thin to be completely rigid.\nThe interface between the telescope fork and the instruments at both Nasmyth foci is provided by two rotator/adapters. Their main function is to compensate for the field rotation by a mechanical counter rotation. The best quality of the TNG is that all the available instruments are permanently mounted at the telescope. This guarantees flexibility during an observing session, since it is possible to change instrument during the night with a loss of time limited to a few minutes.\nThe science based on observational data from the TNG is varied. Proposed observing programs go from the study of the planets and minor bodies of the solar system up to researches of cosmological interest (e.g. large-scale structure of the Universe and systems of galaxies).\nThe TNG is equipped with four instruments:\n- HARPS-N (""High Accuracy Radial velocity Planet Searcher""), echelle spectrograph dedicated to the discovery of extrasolar planets;\n- DOLoRes (""Device Optimized for the Low Resolution""), CCD camera and low-resolution spectrograph for observations in the visible band;\n- NICS (""Near Infrared Camera and Spectrometer""), camera and spectrograph for observations in the near-infrared;\n- GIANO, high-resolution echelle spectrograph for observations in the near-infrared.\n- SARG (""Spettrografo ad Alta Risoluzione del Galileo""), high-resolution spectrograph for observations in the visible band;\n- OIG (""Optical Imager Galileo""), CCD camera dedicated to optical images at high resolution;\n- Speckle camera, dedicated to observations in the visible band at the diffraction limit of the telescope;\n- AdOpt@TNG, adaptive optics system working in the near-infrared.', ""by Staff Writers\nLos Angeles CA (SPX) Feb 15, 2010\nA dramatic new image from the Gemini North telescope illustrates the dynamic and sometimes violent process of star birth. It also demonstrates the capabilities of new filters available to researchers using the Gemini Multi-Object Spectrograph (GMOS).\nKnown as Sharpless 2-106 (Sh2-106), the hourglass-shaped nebula in the new Gemini image is a stellar nursery made up of glowing gas and light-scattering dust.\nThe material shrouds a natal high-mass star thought to be mostly responsible for the hourglass (bipolar) shape of the nebula due to high-speed winds (at more than 200 kilometers/second) which eject material from the forming star deep within. Research also indicates that many sub-stellar objects are forming within the cloud and may someday result in a cluster of 50 to 150 stars in this region.\nThe nebula is located about 2,000 light-years away in the direction of the constellation Cygnus. Its physical dimensions are about 2 light-years long by 1/2 light-year across. It is thought that its central star could be up to 15 times the mass of our Sun.\nThe star's formation likely began no more than 100,000 years ago, and eventually its light will break free of the enveloping cloud as it begins the relatively short life of a massive star.\nThe new filters provide valuable insights by transmitting very specific colors of visible light emitted by excited hydrogen, helium, oxygen, and sulfur as radiation from hot young birthing stars energize clouds of gas and dust. The filters are also used to study planetary nebulae and excited gas in other galaxies.\nFor this image four colors were combined as follows: violet - helium II filter; blue - sulfur II filter; green - oxygen III filter; and red - hydrogen-alpha filter. Each filter was integrated for a total of 900 seconds. The seeing was 0.4 arcsecond RMS.\nThe rotation of the image is 125 degrees CCW from north up, east to the left and the field of view is 3.87 arcminutes on each side. The original calibrated, science-grade data for this image is soon to be available to the astronomical community on the Gemini Science Archive.\nThe image was obtained using the Gemini Multi-Object Spectrograph on the Gemini North telescope. An identical instrument and filter set is available at the Gemini South telescope in Chile for parallel capabilities on the southern sky. The new filters on GMOS are described and characterized for Gemini users.\nThe Gemini Observatory is an international collaboration with two identical 8-meter telescopes. The Frederick C. Gillett Gemini Telescope is located at Mauna Kea, Hawai'i (Gemini North) and the other telescope at Cerro Pachon in central Chile (Gemini South), and hence provide full coverage of both hemispheres of the sky.\nBoth telescopes incorporate new technologies that allow large, relatively thin mirrors under active control to collect and focus both optical and infrared radiation from space.\nStellar Chemistry, The Universe And All Within It\n|The content herein, unless otherwise known to be public domain, are Copyright 1995-2014 - Space Media Network. AFP, UPI and IANS news wire stories are copyright Agence France-Presse, United Press International and Indo-Asia News Service. ESA Portal Reports are copyright European Space Agency. All NASA sourced material is public domain. Additional copyrights may apply in whole or part to other bona fide parties. Advertising does not imply endorsement,agreement or approval of any opinions, statements or information provided by Space Media Network on any Web page published or hosted by Space Media Network. Privacy Statement|""]"	['<urn:uuid:261a8940-d8fb-443b-a2b5-085e25d9efbf>', '<urn:uuid:71db42b9-c2cd-4e18-8100-d5a3a86b52c7>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T18:38:36.516973	25	38	1097
83	What are the leadership expectations and career advancement paths for incident responders and combat medics, and how does their performance impact their potential growth?	Incident responders can advance from entry-level positions like Security Administrator to CSIRT Manager or Director of Incident Response, with salary ranges increasing from $50,000-70,000 to $100,000-150,000. For combat medics, leadership qualities include being inspiring, caring, skilled, personable, and serving as role models. Both roles require building trust within their units, demonstrating technical competence, and showing strong problem-solving abilities. Their advancement depends on their ability to handle stress, maintain high performance standards, and effectively communicate with team members and management. Leadership success is measured by their ability to inspire others, manage operations effectively, and maintain both technical skills and emotional resilience.	"['An incident responder is considered as a cyber firefighter within any organization. This job is often referred to as a Computer Science Incident Responder Team (CSIRT) Engineer or an Intrusion Analyst. The main function of this person within the organization is to react immediately to any occurrences of security incidents or any threats within the organization. An incident responder will rely heavily on several forensics tools that could be used in the industry such that he could be able to identify the reasons behind the incidents and know the problems to try to avoid them in the future for the sake of improving the level security for the entire organization. You should be expecting to spend some time educating other colleagues for better security practices and some other time to help prevent any potential upcoming threats.\nYou should always put in your mind that your main goal is to prevent any occurrences of cyber-attacks or to immediately limit their effects on the systems. The following list attempts to mention the mostly common responsibilities that you should expect to take whilst working as an Incident Responder.\n- It is your responsibility to keep checking and monitoring the organization’s networks and systems to spot any intrusion and prevent it.\n- You have to look for and define all the security flaws and holes within these systems and networks.\n- One of your main roles is to conduct several security audits on the systems, perform penetration testing tasks, network forensics, and risk assessment and analysis.\n- You will have to conduct many tasks related to malware analysis and reverse engineering.\n- It is one of your main responsibilities to define and undertake a defined course of actions in order to respond to any security problems that happen to exist in the system.\n- You should also define and make use of protocols for the sake of communication within the organization and dealings with law enforcement when a security incident occurs.\n- You should design and set a development plan which targets evaluations of security gaps inside the organization in terms of policies and procedures. Your development plan should also include scheduled training and tabletop testing.\n- You have to explain in a detailed technical report what happened and the reasons why this incident occurred in the first place. You should then submit such report to your administrator, upper management, or end users.\n- You should always build relations with other entities that are responsible for conducting cyber threat analyses.\nI want to point out that you may work as an independent consultant if you choose to become an incident responder. On the other hand, if you choose to have this job in a large scale organization, then you may become a part of CSIRT, and you will typically report your findings and analyses to the CSIRT Manager.\nIt is also worth mentioning that the fields of incident response and forensics are gradually merging, yet we preferred to create a separate job page for the Forensics Specialist job position.\nYou could begin your cybersecurity career with an entry level job that could provide you with the necessary information and experience for your career ahead of you. Some of these jobs are given by the following list.\n- Security Administrator\n- Network Administrator\n- System Administrator\nAfter proving yourself successful in one of these jobs, you can consider moving into the Incident Responder job position. However, you can get to the same job from experience as a Forensics Expert First.\nDuring your job time period as an Incident Responder, try to gain experience as much as possible, dealing with stresses and security problems. After that, you may consider becoming a CSIRT Manager or even the Director of Incident Response.\nMany job postings attach one more word to the Incident Responder job. Most of the times, the title and its associated responsibilities are perfectly the same as those of a normal Incident Responder. However, a job like Computer Network Defense incident Responder, the candidate should be dealing more with Networking issues in case of any incident occurrences.\n- Computer Security Incident Response Team (CSIRT) Engineer\n- Cyber Incident Responder\n- Incident Response Engineer\n- Cyber Security Incident Responder\n- Computer Network Defense (CND) Incident Responder\nIt is a common case that the required qualifications for job posting such as Intrusion Detection Specialist, Network Intrusion Analyst and Forensics Intrusion Analyst are quite the same.\nDue to the high specialization of this job, there are no clear salary figures for such job on the BLS or PayScale. This should even be unsurprising since even the job timings are really weird. You may work for 2 consecutive days for 48 hours in the workplace in order to deal with a specific security incident. You may also find yourself taking the rest of the week off after these two days. However, imagine the amount of stress associated with these two days!\nSearching for the term “Incident Response” on Indeed, we were able to get two popular salary ranges: first between $50,000 and $70,000, while the second goes from $70,000 to $90,000. Furthermore, when searching for the term “Incident Response Analyst” instead of the previous term the two popular ranges went up dramatically. The two ranges became from $70,000 to $90,000 and from $90,000 to $110,000. Finally, it comes with no surprise that a CSIRT Manager or Leader is paid a higher salary than all of these mentioned salary figures. He should expect a range between $100,000 and $150,000 on an average base.\nI would say that a bachelor’s degree in Computer Science or any equivalent field such as Math, Electrical Engineering, Cyber Security, etc. would work quite well. However, employers usually do not require a certain degree from their candidate for this job. In addition, you can consider a master’s degree in Information Assurance or Information Security with a focus on incident response. This should provide you with a great push in your incident response career where you may become a CSIRT manager or get a high salary while being a CSIRT member. If you are still into academics, I could also recommend considering a university which offers an incident response management track. This would be really great for you for the best opportunities.\nMost employers require from their candidates to spend 2 to 3 years in the field of security or preferably incident response in specific. On the other hand, if you are considering a senior level job such as Senior Incident Responder and Senior Intrusion Analyst, you should have at least 5 years of experience in the respected fields.\nThe main thing that employers expect you to know is to understand and get to know their systems from inside out and being able to identify all the potential security flaws. They will also look carefully for advanced forensics skills, especially when it comes to a target-rich environment in large scale organizations such as Google. The following list attempts to give you an overview of the main technical skills that you should have before considering this amazing job.\n- A perfect candidate has to be experienced in both Windows and Unix like systems (like Linux)\n- Programming languages such as C, C++, C#, Java, ASM, PHP, and PERL should be absorbed well by a candidate.\n- You should be comfortable when dealing with computer networking and communications especially those based on TCP/IP protocols.\n- You should have a solid knowledge of both the hardware aspect and software aspect of computers.\n- You should be able to install different operating systems on the organization’s different machine and system and further apply and configure any needed patches on them.\n- You should be experienced with technologies used for backing up information and archiving.\n- You have to be knowledgeable of main security concepts and practices when it comes to web-based applications used by the organization.\n- You should be aware of eDiscovery tools such as NUIX, Relativity, Clearwell, etc.\n- You should also be comfortable using different software applications designed for forensics purposes. These applications are like EnCase, FTK, Helix, Cellebrite, XRY, etc.\n- You should be confident when using enterprise tools for system monitoring and when using Security Information and Event Management (SIEM) which is really useful and desirable for an incident responder to function perfectly in his position.\nYou should have knowledge of the main concepts and practices of cloud computing.\nFirst things first, if you are a procrastinator, it is now time to work on evading this trait before applying for this job. Employers will love to see that their candidate is really flexible person who is willing to get adapted to any changes and who can work under extended stress. Remember that you never know when a security incident is going to happen. You should not be that type of person who easily panics when faced with hard or unpleasant situations.\nMoreover, an Incident Responder is expected to be a creative thinker who has great problem-solving skills as well as analytical skills. Also, you should work on your ability to convey technical concepts and approaches –either orally or through technical reports– to your management or to other colleagues who may be of a lower technical background that you. You will face these situations a lot in your corporate life generally. So finally, your oral and communication skills are really important to become the perfect Incident Responder in an organization.\nThere are several certifications that you should consider gaining by the time you become an Incident Responder. The following list provides you with most of these suggested certifications that we think would help you a lot along the way in your Incident Response job functions. Nevertheless, it is always advisable to check with potential employers or other senior level employees to check the required certificates and the most powerful ones as they perceive.\n- CCE: Certified Computer Examiner\n- CEH: Certified Ethical Hacker\n- GCFE: GIAC Certified Forensic Examiner\n- GCFA: GIAC Certified Forensic Analyst\n- GCIH: GIAC Certified Incident Handler\n- GCIA: GIAC Certified Intrusion Analyst\n- CCFE: Certified Computer Forensics Examiner\n- CPT: Certified Penetration Tester\n- CREA: Certified Reverse Engineering Analyst', 'Published on February 23, 2014\nCharles R. Figley, Ph.D. Tulane University Kurzweg Chair in Disaster Mental Health\nPresentation Dedication This presentation is dedicated to the thousands of combat medics who served their country and must anticipate and adopt to the deaths of all their clients the rest of their lives\nAcknowledgements ! Ted Rynearson and his team for choosing Tulane and New Orleans for this training conference ! Dean Ron Marks and the Tulane University School of Social for their support of this conference and my work represented here\nAcknowledgement My Research Team Joseph Boscarino Joia Speciale Kathy Figley Jeff Nagy\nPurpose of the Presentation Suggest that since combat medics are among the most resilient in adopting to violent death in combat what may be the reasons and what can we learn from them.\nObjectives After this presentation, participants will: 1. Be more familiar with the special circumstances of modern combat medics as caregivers vulnerable including the same challenges all caregivers face. 2. Be familiar with the combat stress injury model and the four types of injuries including a grief/loss injury.\nObjectives (cont.) After this presentation, participants will: 3. appreciate the connection emotionally and socially between what combat medics do and the importance of building secondary stress resilience capacity 4. understand and apply knowledge about the ""Spectrum of Compassion Response as an indicator of the level of thriving as human beings\nThesis ! Combat medics adapt to violent death by focusing on their job, apart from the emotional reactivity experienced by non-medic soldiers ! Thus, combat medics adapt to violent death through effective self regulation that includes focusing on the mechanics of their job as caregivers. ! Medics avoid stress injuries by a set of strategies that include displays of leadership, soldiering, and medical care.\nStudies of combat vets responses to death ! 1. Death was a central and profound experience for vets the studied ! 2. Pre-military service predicts, like personality, religious atmosphere at home, and contact with death, reactions ! 3. Investment in life – family, life generally, and education – increase death anxiety.\nOther factors that influence death anxiety ! Stress management generally manages any anxiety ! Strategic use of denial, desensitization, and compartmentalization to manage death anxiety ! The value of life increases with each exposure to death ! Being near death is most often traumatizing but offers important lessons that can make it worth it (Posttraumatic Growth)\nSimilarities of Disaster and War Deployment Characteristics War Deployment Disaster Deployment Lots of Training Preparation X X Away from home and family X X Exposed to a range of traumatic stressors X X Work long hours in difficult conditions for an extended period X X Expectations are to withhold self care and endure the conditions X X Varying levels of danger and uncertainty, and periods of X X\nWho are Medics and Navy Corpsmen? 1. Revered military role in all service branches 2. Critical to combat operations 3. Responsible for treating wounded Soldiers and allies civilians and enemy combatants 4. Potentially conflicting Dual Role (Soldier/ Medic)\nCombat Medics (cont.) 5. No studies specifically on combat medics 6. Focus on resilience or combat mettle to enable them to survive and thrive mentally and physically\nCombat Medics ! Required to fight battles ! provide front line trauma care, often in the heat of a battle, with limited resources and under enormous stress. ! In modern warfare, however, they must be able to transition from a soldier role to a medic role quickly and decisively in accordance with the tactical situation and rules of engagement.\nCombat Medics ! They must not only understand the nature of war, but also the nature of war-related injuries and ! the implications for medical procedures that will be effective given the tactical environment, current location, resources available, and capabilities. ! must not only cope with the emotional burden medic duties but also must be prepared to die and to participate in a killing (Mazurek and Burgess, 2006)\nCombat Medics ! All this plus endure war in the middle east ! changing rules of engagement, ! the stop loss and other war service-related rules and regulations, ! The media limited slants on the wars efforts,\nCombat Medics ! the changing and increasing deployment schedule that prevents sufficient down time to reach a healthy re-boot and ! more prepared psychologically for the next deployment, and many other realities of these post-9-11 wars.\nFrom the Combat Medic s Prayer ! If I am called to the battlefield, give me the courage to conserve and protect our fighting forces by providing medical care to all that are in need. ! If I am called to a mission of peace and mercy, give me the strength to lead by caring for those who need my assistance. ! Finally, Lord give me the strength and insight to take care of my own spiritual, physical, and emotional needs.\nCombat Medic Mettle: Our teams focus ! Dictionary definition of mettle: vigor and strength of spirit or temperament ! Having “medic mettle” means possessing the right stuff to adapt and thrive as a leader, healer, soldier, and person. ! Ours is part of a larger group conducting a longitudinal survey of 848 combat medics in two continents since late 2009. ! This is our final year of the 3-year study.\nRecruitment Numbers !""#$%! &%\'""()%*! &%\'""()%*+ ,-.+/(01234! 5(06+ &%\'""()%*! ""#$%&\'! !""#$%%&! #\'(\'! #()! #*((! ""#$! +,-./01! #22! #3! #\'45! $%&! 6%""/78! #9:3! ()#! :(\'! ()(!\nPreliminary Findings ! First study to confirm medics experience secondary trauma, like other medical health care providers. ! Though witnessing significantly more combat stress, medics scored better in behavioral health measures\nCombat Experiences ""#$%&\'!()*(+,(-""(.!/012!3453673183!79!5:97;7<3=! ! ! ""0BB71C!71!A763!:1!;D3!313E2! L363!2:>!?7638;B2!6395:197MB3!A:6!;D3!?30;D!:A!01!313E2!8:EM0;01;N! .D::;71C!:6!?7638;71C!A763!0;!;D3!313E2! .3371C!7BBQ71R>63?!S:E31!:6!8D7B?631!SD:!2:>!S363!>10MB3!;:!D3B5! .3371C!?30?!:6!9367:>9B2!71R>63?!&E3678019! .3371C!?30?!M:?739!:6!D>E01!63E0719! "">6631;!.;>?2! ! @! /A=! FGHI! /JK=! OGHK! /II=! IHGII! /JPI=! OIGOK! /IPF=! OOGTU! /IJV=! HVGIV! /IHJ=!\nPreliminary Findings ! Consistent with compassion fatigue theory, they experienced higher levels of depression than other soldiers.\nGetting Behind the Statistics ! The Tulane Research Team was responsible for the qualitative elements of the three-year study ! Our team conducted intensive video interviews with 17 named by the group as the best representation of combat mettle. ! The first measure of CMM that will help build a model of combat medic mettle that will assist us in measuring the essence of what is required to thrive.\nRecruits 17 combat medic interviews Initial Survey (n=848) Nominates Medics with Mettle (n=40) Successfully Interviews 17\nPurpose of the Interviews ! First identify key truisms about combat medics that may contribute to medic mettle. ! Second, convert these observations into items in the first draft of a Medic Mettle Scale ! Third, administer the scale to all 848 combat medics and investigate the predictive power of the Scale in predicting behavioral health markers.\nInterview Process Pre-Interview Paperwork Semi-structured Video Interview Post-Interview Team Discussion\nPost-Interview Team Discussion\nVGA Procedure for each Video Interview using Quick Time (video) Markers Primary Reviewer Secondary Reviewer Tertiary Reviewer\nExample of Reviewer Assignments for VGA Analysis Subject # A Reviewer B Reviewer C Reviewer 1 2 3 4 5 Secondary Tertiary Primary Secondary Tertiary Primary Secondary Tertiary Primary Secondary Tertiary Primary Secondary Tertiary Primary\nVGA Methodology to Generate Scale Items Identify items for the scale that may be truisms about being a combat medic: ! Personal experiences and observations ! Methods of coping with adversity and change ! Words to live by in coping and resilience\nVariable Generating Activity s Five Variable Domains P te tiv ro c e F c rs a to (P ) F 9 B h v ra e a io l Ha e lth In ic to s d a r (B ) H 8 1 0 2 S so tre s rs (S ) 6 3 5 7 1 4 In rv n n te e tio s (I) C lla ra o te l Ss m y te ic Im a t pc (C ) S\nVideo Data Analysis: Video Generating Activity (VGA) Assign Data Analysis Roles VGA 138-item Scale\nMedic Mettle Scale Face Validity Analysis 138-item Scale 104-items 4 Factors\nCOMBAT MEDIC METTLE SCALE (v1) Instructions: ! Thanks for helping medics. This Scale is composed of statements by active duty medics interviewed in 2010 near their base. ! The purpose of the Scale is to help understand military resilience and how to help combat medics be as resilient as possible in their important work. The Scale is divided into four, color-coded sections (Leadership, Personal Beliefs/Experiences, Technical Skill/Medical Training, and Soldiering Skill/Training) that represent medic skills sets.\nCOMBAT MEDIC METTLE SCALE (v1) Instructions (cont.): ! Please consider each statement as it applies to you and your opinions and rate how the statement describes you during the last week by selecting one of 4 options: ! 0= Not like me or NA; 1= Sort of like me; 2= Like me; 3=Very much like me; 4= That s totally me\nExamples of LEADERSHIP Items *%+,- .$/0\' 12342*56.7 \'(!)!*+!,-./*0123!*45!642.7235!64!+8!9.7563- 9!76239( :(!)!;*23!4.!,-.<73+!*9=64>!?.-!;37,!@;34!433535( $(!)!*+!*!<6>!<376323-!64!+34A.-9;6,(\nExample of PERSONAL BELIEF/EXPERIENCE Items *%+,- .$/0\' PERSONAL BELIEF/EXPERIENCE 27. I like to keep my mind occupied. 28. have taken prescription drugs to help with sleep but they had no effect. 29. I have taken prescription drugs to help with sleep and they worked well.\nExamples of Technical/ Medical Training Items *%+,- .$/0\' PERSONAL BELIEF/EXPERIENCE 69. I value being called doc. . 70. I maintain a strong relationship with my chaplain. 71. My training helps me overcome any fears.\nExamples of Soldiering Skills/ Training Items *%+,- .$/0\' SOLDIERING SKILL/TRAINING 92. I work hard, train hard, and am prepared. 93. I believe that my soldiers are like family to me. 94. I allow negative events to affect me.\nMedic Mettle Scale (MMS) Psychometrics MMS Completed by 838 Medics MMS-2\nMedic Mettle v2 ! Strong psychometric properties ! Correlated with measures of readiness, self confidence, resilience, thriving ! Help build the statistics model of medic resilience (mettle)\n9. Combat Stress Injuries and Resilience The following figure was originally developed to identify the process by which combatstress injuries could be predicted and prevented.\n9. Combat Stress Resilience Most deployed do well in both the short and long-term. However, some develop some form of psychosocial stress injuries that require attention. The following describes a roadmap for appreciating the psychosocial variables in predicting the stressors and stress reactions during deployments.\n""&! TRAIT RESILIENCE FACTORS "" Intelligence "" Trait Resilience (ER-89) "" Stress Adaptation Competence "" Self Confidence and Self Confidence Occupational Hazards "" Individual Demands "" Unit Demands "" Environmental Demands "" Family Demands Trauma-RELATED STRESS INJURIES AND RESILIENCES "" Physical Fatigue Injury and Resilience "" Grief Injury and Resilience "" Belief Injury and Resilience "" Trauma Injury and Resilience 5$%$/!*/\'8&8/,9/!:;<#$/9+=/>! ?%9$#<\'! @@!A<8/,$%+#,!%,B!""<%8,8,-!$#! CD8&BE! "" ""<D\'$!8,!$F/!G,8$! "" ""<%D0%+9!G,8$!H#F/\'8#,! "" H#,$/I$@C%\'/B!5$</\'\'@!! </BD98,-!H#0;/$/,98/\'! WORKER STRESS REACTIONS "" Biological Markers "" Psychological Markers "" Social Relationship Markers "" Behavioral Markers "" Spiritual Markers Stress Injuries and Resilience of Disaster\nTRAIT RESILIENCE FACTORS "" Intelligence - i.e., logic, critical thinking "" Trait Resilience (ER-89) – adaptive, adventurous "" Stress Adaptation Competence "" Self Confidence "" Sense of Humor\n5$%$/!*/\'8&8/,9/!:;<#$/9+=/>!?%9$#<\' B-634A*1.4!*45!C-*6464>!A.!640-3*93! "" C-D9A!64!A;3!E46A!F,3-036235!0.+,3A3403G!9D,,.-AG!9D00399! "" C-*D+*10!E46A!H.;396.4!F!73237!.?!<.464>! *45!+DAD*7!9.06*7!9D,,.-A! "" H.4A3IA/<*935!JA-399/-35D064>! H.+,3A340639!//<D67564>!.4!A;3!A-*6A9K!\nOccupational Hazards/Stressors "" Individual Demands – e.g., physical "" Unit Demands – e.g., frequent deployments "" Environmental Demands – e.g., internal politics, weather "" Family Demands – e.g., pressures to be home, financial stressors\nWORKER STRESS REACTION MARKERS "" Biological Markers – i.e., indicators of immune suppression "" Psychological Markers -- e.g. emotional indicators of stress, sleep dysfunction AND\nWORKER STRESS REACTION MARKERS "" Social Relationship Markers –social support and colleague care "" Behavioral Markers –job competence "" Spiritual Markers –sense of direction, hope, and fulfillment\nTrauma-RELATED STRESS INJURIES AND RESILIENCE "" Trauma Injury and Resilience – i.e., memory management and re-establishing safety "" Physical Fatigue Injury and Resilience – i.e., wear and tear "" Belief Injury and Resilience –moral and ethical challenges "" Grief Injury and Resilience -- adaptation to loss of person, place, thing\nInner Conflict ! Most recent and controversial (guilt, shame that can lead to suicide) ! Most often due to conflict between moral/ethical beliefs and ! current experiences such as ! taking action outside of the rules of engagement and ! where there is harm to an innocent life; ! not preventing harm to a buddy.\nGrief Injury and Resilience -nner Conflict — or beliefs injury is most often due to conflict between moral/ethical beliefs and current experiences such as taking action outside of the rules of engagement and where there is harm to an innocent life; not preventing harm to a buddy.\nEnabling Medics to Recover from Stress Injuries ! Estimating Functioning ! Using the Spectrum of Combat Resilience to determine the level of functioning. How do we know to refer for professional help?\nAccording to the Spectrum Model (vs psychopathology) ! It takes a system to coordinate care for those who require help ! versus those who deserve praise and respect ! without negatively affecting those who are dysfunctional ! Functioning is defined by the presence of five capabilities.\nCombat Medic Functioning: 1. Physically capable (measured by level of energy due to sleep, health) capable (measured by level of enthusiasm, intellectual capability, morale, spiritual support) 2. Psychologically . Interpersonally capable (measured by level of social 3 support and cohesion with group) 4. Technical and administratively capable (measured by standard productivity, client satisfaction, and competence scales) (Care) Regulation capable (measured by the existence of an EB self care plan and following it). EB self care plan (see Greencross.org) 5. Self\nSpectrum of Caregiver Stress Resilience ! FIVE LEVELS OF FUNCTIONING ! Useful for caregivers to determine the effectiveness of their self care plan and for ! Leaders and role models in stress resilience\nSpectrum of Caregiver Resilience Level 5 Level 4 Level 3 Level 2 Level 1 Highly Resilient Resilient Challenged Resilience Supported Resilience Failed Resilience Exceptional Role Model Good Functioning Acceptable Functioning Unacceptable Functioning Dysfunctional No challenges in functioning Challenged in 1 provider function Challenged in 2 functions Challenged in 4-5 Functions Failing in 1 or more functions Train and Coach others on the team Maintain Provide Coaching and Peer Support Explicit Plan Implemented for Resilience Immediate behavioral health services\nLevel 5: Highly Resilient ! Many people are in this category and are thriving in their career. ! They score high on thriving and human development ! Tend to score high on trait resilience and the other protective factors\nLevel 4: Resilient ! Most people are in this category and are challenged but meet the challenge in their career. ! They score moderately high on thriving and human development and on trait resilience and the other protective factors ! May have one of the five\nLevel 3: Challenged Resilience ! Many are in this category and require attention ! Acceptable Functioning ! Challenged in 1-3 Functions ! Provide Coaching and Peer Support\nLevel 2: Supported Resilience # Unacceptable Functioning # Challenged in 4-5 # Functions # Explicit Plan Implemented for Resilience\nLevel 1: Ineffective Resilience # Failing Resilience # Dysfunctional # Failing in 1 or more functions # Need aggressive behavioral health services # Benefit from the MASTERS Transformative approach to establishing and sustaining an appropriate self care plan\nConclusions Combat medics are caregivers like social workers, nurses, child welfare workers, and others trying to help others\nConclusions Stress and stress regulation are among the biggest challenges in war and are vulnerable to stress injuries that may lead to mental disorders but can also lead to growth.\nConclusion ! Medics and other caregivers secondary stress reactions must be closely monitored and given proper positive attention\nConclusion (cont.) # Caregivers should utilize good self care, practice colleague (buddy) care, and; # Encourage supervisory support for caregivers\nDedication: To the memory, life, and contributions of LtCol Dave Cabrera, PhD, killed in action October 29, 2011\nCharles Figley s Contact Information: ! Figley@Tulane.edu ! 504-862-3473 ! Charlesfigley.com\nThe role of leadership in promoting post-trauma resilience Leadership characteristics: ! 1. Inspiring: Sense of mission and history ! 2. Caring: Perceived to have the best interests of the deployed and the deployment teams in mind ! AND\nThe role of leadership in promoting post-trauma resilience (cont.) ! 3. Skilled: Knows operations, experienced in a variety of disasters and disaster mitigation ! 4. Personable: Knows team members by name and duties ! 5. Role Model: Others see their efforts at self care, sense of humor, and being ethical and humane.\nOcv nola may2012_final-pdf ... Listado de miembros de la O.C.V LIBERTADORES DE AMERICA DEL SIGLO XXI MIEMBROS DE LA OCV LIBERTADORES DE AMERICA DEL ...\nCurrent Production Planner at Ocv Italia Srl, Planning, inventory and account leader of alloy and subcontractor production at OCV ITALIA Past IMPIEGATO ...\nOcv nola may2012_final-pdf Discusses the challenges of grief and traumatic stress injury using the case of the combat medic as an example of resilience ...']"	['<urn:uuid:23d10321-7c06-40f1-9f9b-5cf86f8a59b6>', '<urn:uuid:38c2620b-8589-4873-a65b-ad8119cb93d9>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	24	100	4503
84	I'm fascinated by ancient trade routes. How did Bronze Age coastal communities in West Frisia conduct trade across the North Sea, and what do we know about Dover's role as an ancient trading port?	Bronze Age coastal communities in West Frisia participated in extensive trade networks, with evidence showing they obtained materials like Danish stone for making sickles. They likely used paddled vessels for sea transport, with their routes influenced by North Sea tidal streams and coastal patterns. The Rhine-Meuse estuary served as a border between Nordic and Atlantic trade networks. As for Dover, it played a crucial role as possibly the oldest port in the British Isles. Evidence of its importance in Bronze Age trade comes from the discovery of the world's oldest known sea-faring vessel (dated 1550 BC) in Dover. This boat, which contained shale from Dorset, proves that established trading routes existed along Britain's southern coast, connecting to areas like Devon and Cornwall, where tin was exported to Gaul around 1600 BC.	"['Venture 4 Social character and correspondence systems\nReason: Seaside cultivating groups are not yet an essential piece of investigations of the Bronze Age. Yet we know from the dissemination of material society (e.g. bronzes) that seaside groups framed provincially particular gatherings that took an interest in broad systems in which merchandise and thoughts were traded. In this manner we intend to talk about in sub-venture 4 both components of provincial social way of life and additionally interest in European trade systems.\n1. Could we infer a local social character in the west-Frisian information?\n2. With which locales did the beach front groups of West Frisia have their contacts?\n3. By what means can North Ocean tidal streams and beach front designs have affected seaside contacts?\nQuestion 1 is replied through the investigation of material society styles. In the Neolithic wetland groups created, similar to the Hazendonk, Swifterbant and Vlaardingen bunches, with settlement and internment customs that were not quite the same as the uplands. We consider them bunches with an unmistakable social character, in contact with, however not the same as similar upland groups (cf. Louwe Kooijmans 2005). In a matter of seconds this is being examined in the NWO-Odyssee venture Opening North Holland’s’ Late Neolithic fortune mid-section (2010-2014), with which there will be contacts through Prof. Van Gijn. Part of this undertaking is additionally actualized in Leiden (V. Garcia Diaz).\nIn the Bronze Age these customs most likely proceeded. Brandt (1988), for case, perceived a local earthenware style for West Frisia, the Hoogkarspel style. There are various different signs for a sort of provincial social personality, for example in house-working, in ancient rarities such as rock ‘sickles’, in pit-hovers, and in the entombment and accumulating conventions (or rather the nonappearance of them). Part of the information for noting this inquiry are created by sub-venture 2 and 3, however an import perspective will be the examination of ceramics collections. The first information of Brandt are still accessible, yet should be re-considered and contrasted and aftereffects of later unearthings.\nQuestion 2 requires an examination of the appropriation of material society and social conventions. Plotting the dispersion of material society and of social conventions is a device that archeologists use to decide local gatherings and connection circles. We will utilize this instrument to make a system for the examination about regionality and trade systems. Essential in this appreciation is the provenance of antiques. For example, it is realized that west-Frisian sickles were made of Danish stone (Van Gijn 1992). Examinations of different components of material society (metal, golden) might uncover comparative contact circles, both with the inland and along the coast or even over the North Ocean.\nThe examination of material society appropriations will likewise give a handle to talking about the part of beach front groups in ocean faring along the coast and crosswise over to Britain (venture 4). System examination might uncover this (cf. Freeman 2006).\nQuestion 3 is replied by the examination of North Ocean tidal streams and beach front setup. In the Netherlands two substantial Bronze Age communication circles meet up, the Nordic system the Atlantic system (cf. Fokkens 2009). The Rhine-Meuse estuary constitutes the surmised outskirt between the two. Both systems created distinctive conventions and traded diverse style curios, for occurrence bronze antiques (Fontijn 2003). There is most likely particularly bronze antiques have been transported over oceans, however the center was by and large of vessels, not on ocean faring (cf. Van de Noort 2006; Ling 2008).\nOur theory is that the ordinary beach front structures along the southern North Ocean coast, and the Channel, decide tidal streams and through that ocean faring. These examples might uncover both plausible and impossible courses for the generally paddled or paddled open vessels (cf. Clark 2009; Van de Noort 2009). This speculation has never been tried, yet will most likely contribute significantly to a comprehension of beach front zones as “Maritories” rather than “Regions” (Needham 2009).', ""Albion is the ancient Celtic name for Britain, and is believed to refer to Dover, home to the Bronze Age Boat\nDover during the Bronze Age\nThe name Dover is generally accepted as being of Celtic origin, deriving from Dubra, meaning 'the waters'. The name of Dover's river Dour stems from the same Celtic word, and is considered to mean literally 'the river'. Likewise, the name Kent is of Celtic origin; according to the Cambridge Dictionary of English Place-Names, the name comes from the Brythonic Kantion, meaning probably 'corner land'.\nThe ancestors of all the populations that settled in the British Isles are likely to have passed either near or through Dover. The White cliffs of Dover are the only part of Britain visible from the European continent, and so, in prehistoric times, any families attempting to cross the stretch of sea dividing Europe from Britain would have directed their small boats towards Dover's cliffs. An inlet between the cliffs formed a natural haven for sea-faring vessels, and the various arms of Dover's river Dour running into the sea made it possible for boats to move sufficiently inland to take up a safe mooring position, thus making Dover an ideal port.\nTaking into account the proximity to the continent, it is reasonable to imagine Dover as being the oldest port in the British Isles, and maybe also home to the oldest settlement. Pottery dating back as far as 1800 BC has been found on the Eastern Heights near Dover Castle, and can be seen on display in Dover Museum together with numerous other Bronze Age items.\nAlbion and the White Cliffs of Dover\nThe ancient Celtic name for Britain is Albion, and is believed to refer initially to the white cliffs of Dover. Historical references to Britain prior to the first century BC are both rare and vague, and stem mainly from the ancient Greek colony of Marseilles in modern France.\nThe idea that Albion derives from the Latin albus, meaning white, is misleading, as the Celtic name Albion was mentioned in Greek scripts from Marseilles possibly as far back as the sixth century BC, whereas the Romans first came into contact with Britain during the middle of the first century BC. The original meaning of the Celtic name Albion remains surrounded in mystery, with various interpretations being given. It is far from clear whether the name was used only during the Iron Age, or already in the earlier Bronze Age.\nThe Bronze Age Boat\nThe Bronze Age is considered to start in Britain in the latter half of the third millennium BC, and followed the Neolithic Age. It continued to around 800 BC, when iron was introduced. At first, copper was used on its own, but within time people discovered that bronze could be obtained by adding a small quantity of tin to the copper. It was used to manufacture tools, replacing the older flint implements of the Neolithic Stone Age. Tin reserves were abundant in Cornwall and Devon, and around 1600 BC tin was being exported from Britain to Gaul.\nThe oldest known sea-faring vessel in the world was discovered in Dover in 1992. Dating to 1550 BC, the Bronze Age Boat was conserved owing to a protective mud case that had formed around the wood, concealing it from contact with the air. The larger part of the vessel was retrieved and is preserved in Dover Museum.\nA piece of shale from Dorset was found in the boat, revealing that the vessel had travelled along the English Channel in the direction of Devon and Cornwall. The evidence shows that long before the Iron Age, cargo vessels were transiting along Britain's southern coast and crossing the Strait of Dover, operating within the context of established trading links that were vital to the Bronze Age civilisation in Britain and Europe.\n- Cambridge Dictionary of English Place-Names, edited by Victor Watts, Cambridge University Press, 2004.\n- Dover Museum.\nWritten by D. Alexander\nThe British Isles in the Neolithic Stone Age:\nCeltic Origins in the British Isles:\nPhoto 1: Albion, the White Cliffs of Dover\nPhoto 2: Light Tower Church on Eastern Heights, above the Port of Dover\nPhoto 3: The Union Jack In High, Albion, Dover""]"	['<urn:uuid:a7a9e87b-0d18-49ae-bba8-5576bf69599e>', '<urn:uuid:cf97ab7b-d6ae-4d65-8a5e-925477272def>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	34	131	1361
85	What are the limitations of spatial hashing in 3D versus 2D?	In 3D, spatial hashing becomes more complex and costly compared to 2D: hashing becomes 150% more complex due to three components, neighboring cell checks increase 300% (26 cells instead of 8), and the hit rate drops more than half for uniformly distributed particles, making potential neighbor testing at least 225% more complex.	['So let’s say you’ve got some particles in 2-dimensional space.\nThere they are. Now let’s say you want any two particles which are very close to each other to interact. You need to know which pairs are close enough. You could take all possible pairs of particles, calculate their distances, and compare those to the minimum distance\nd. That’s fine if you have 10 particles, but if you have 5,000 that’s going to add up to over 12,000,000 pairs. That’s too many pairs.\nThat complexity can be alleviated if for each particle\nP we can efficiently find a subset of all particles which includes all those within\nP and which includes as few as possible which aren’t within\nd. Then we can calculate the distances from\nP to the few particles in this subset and determine which ones are actually within distance\nEnter: spatial hashing. I’ll show how\nboost::unordered_multimap can be used to do this.\nunordered_multimap is a key-value storage where multiple values are allowed to have the same key. Under the hood of this object is a hash table. This means that the type used for keys must have an equality operator and a hash function yielding an unsigned integer which is “very likely” to be unique.\nSince each key in the multimap is ultimately reduced to a single integer, and what we’re trying to do is look up particles based on their positions, creating a hash function amounts to defining a mapping from continuous 2D space into discrete 1D space. A fine way to do this is to break down the continuous space by a grid and then combine the discrete grid coordinates into a single value with a generic hash function.\nThere’s your grid. The resolution of this grid i.e. the size of the cells is not arbitrary. More on that later.\nIt doesn’t take very much code to implement the concepts so far:\ninline vec_2i cell ( const vec_2f & p, float cellsize )\nreturn vec_2i( boost::math::iround<float>( p.x / cellsize ),\nboost::math::iround<float>( p.y / cellsize ) );\ninline size_t hash_value ( const vec_2i & v )\nsize_t h = 0xdeadbeef;\nboost::hash_combine( h, v.x );\nboost::hash_combine( h, v.y );\ntypedef boost::unordered_multimap< vec_2i, particle_t > positionhash_t;\n- The multimap value type could instead be a pointer or it could be an array index; it’s a data structure in this case for the sake of clarity. What’s best depends on what you’re up to.\ncellsizeis the width and height of the grid cells in continuous space. The reason for keeping the real position to grid position conversion in a separate function rather than using the real position itself as the multimap key will become apparent.\nLet’s focus on the particle\nP at the center of the blue circle. The circle has radius\nd (the distance within which we want to know about other particles). If the grid cells are at least\nd on a side then all particles within the circle will be found in the nine shaded cells (the cell containing\nP and the eight surrounding cells). Now it becomes clear why we want to use grid coordinates as keys rather than real coordinates: the keys for neighboring cells can be determined easily by adding/subtracting 1 to/from the x and y components. So,\ncellsize must be at least\nd for this to be correct, but it need not be larger than\ncellsize = d, this is what I’m getting at.\nIn this case the subset of particles we find includes two which are within\nd and one which is not; so we’ve got a 66% hit rate here. In general, if the particles are uniformly distributed, the hit rate of this spatial hashing process will be the circle’s area divided by the area of nine cells; with\ncellsize = d that’s roughly 35%.\nOK then, let’s move on to determining the neighboring pairs of particles over the entire set of particles. If you insert all particles into the multimap and then examine each particle as we just did you’ll end up calculating all distances twice, and you’ll end up with two copies of each pairing. To refine this such that we don’t run redundant calculations, don’t produce redundant pairings, and don’t use any additional data structures, we can simply intermix the processes of inserting particles in the multimap and searching for neighboring particles.\nTo find each pair only once:\n- Start with empty multimap\n- For each particle\n- Determine which cell will contain\n- Retrieve all particles in\nP‘s cell and the eight neighboring cells\n- Calculate distance from\nPto each of these and compare to\n- Determine which cell will contain\nThe code would look a little something like this:\nvec_2i offsets[ 9 ];\nfor( int dx = -1; dx <= 1; ++dx )\nfor( int dy = -1; dy <= 1; ++dy )\noffsets[ (dx+1) + 3 * (dy+1) ] = vec_2i( dx, dy );\nBOOST_FOREACH( const particle_t & P, particles )\nfor( size_t oi = 0; oi < 9; ++oi )\nstd::pair< positionhash_t::const_iterator, positionhash_t::const_iterator >\nr = positionhash.equal_range( cell(P.pos, d) + offsets[oi] );\nfor( positionhash_t::const_iterator n = r.first; n != r.second; ++n )\n// if distance from P to n->second is less than d:\n// note P and n->second as a neighboring pair\npositionhash.insert( positionhash_t::value_type( cell(P.pos, d), P ) );\nAnd there you have it. I applied this technique in a toy particle simulation, and I was able to go from simulating around 300 particles to simulating 7,000 or more. Because the asymptotic complexities are different, the difference in efficiency between this spatial hashing approach and the naive quadratic approach grows larger as the number of particles increases.\nI have not tried applying this to 3 dimensions, and I have no plans to. It would not be difficult to adapt, but cost would increase:\n- Real positions would have three components as would the grid positions. Hashing would be 150% as complex.\n- Instead of 8 neighboring cells, there would be 26. Querying for potential neighboring particles would be 300% as complex.\n- For uniformly distributed particles, hit rate would drop more than half. Testing potential neighboring particles would be at least 225% as complex.']	['<urn:uuid:fbbcaec5-5cdd-45a0-ba28-4e5ab36b47d0>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	11	52	1031
86	german u boat commanders atlantic coast 1942 battle successes and final fates details	Two notable U-boat commanders operated on the Atlantic coast in 1942: Heinz-Otto Schultze of U-432 successfully sank five allied ships in less than two weeks, including the Miraflores, but his submarine was later sunk in March 1943 by the Free French corvette Aconit, resulting in 26 crew deaths and 20 captures. Captain Helmut Rathke of U-352 was less successful, failing in his attack on the USCG Icarus, which led to his capture along with 32 other survivors while 15 of his crew went down with the ship.	"[""Registered: 1123644261 Posts: 322\nReply with quote #1\nNinety years ago today the U-151 attacked over seven defenseless fishing vessels and commercial ships off the New Jersey, Delaware, and Maryland coast. The first loss of lives occurred when a life boat carrying passengers from the passenger steamer Carolina overturned while making way to the New Jersey coast. Less than three decades later German U-boats would again wage war on innocent merchant fleets cruising the Atlantic Coast. Lost without a Trace The mystery of the S.S. Miraflores is solved. It is February 19, 1942. In the darkness of the cold winter morning, the stealthy German submarine U-432 follows the silhouette of a lone freighter steaming north fifty-five miles east of Cape May, New Jersey. The order to man battle stations is relayed to the crew. On board the British steamer, most of the crew members are off duty, settled in their bunks sleeping in the early morning hours. The drone of the pounding steam engine is abruptly interrupted by the detonation of two torpedoes that slam into the targeted ship. Within minutes the force of the sea draws the tortured hull to the sandy bottom. Thirty-four merchant sailors are lost without a word relating the circumstances of their deaths. Only a short entry in the German U-boat’s log gives a vague description of the victim with no name. The next day the S.S. Miraflores is reported missing after failing to reach her final destination, New York. Walter Autry considers himself a fortunate man. He was 17, when he served in the Merchant Marine as a fireman on the ill fated Standard Fruit Ship. The S.S. Miraflores made her home port in Kingston, Jamaica and carried her cargo of bananas to New Orleans where Walt signed on as crew. Walt is the only known surviving former crew member of the Miraflores. He spent a year on board gaining experience on the steamer then made a decision to join the Navy a year prior to the sinking. The 2,755 ton freighter went missing with all 34 of his fellow crew members after leaving Port de Paix, Haiti on February 14, 1942. The exact location and details of her loss remained a mystery until the year 2007. In December 1940, Walt was discharged from the Miraflores and joined the Navy where he served in the Asiatic-Pacific Theater of Operations from 1941 to 1945 during WWII. In the summer of 1941 he attended diesel engineering school at the University of Houston. His luck continued for he met his wife to be with whom he has been married to for more than 65 years. Owned by Standard Fruit Company, Miraflores normally carried a cargo of assorted tropical fruits such as bananas, coconuts and cashews between New Orleans and Central America. On February 6, 1942, Captain Charles Thompson of the S.S. Miraflores left port on a change of orders. These new orders were to dramatically affect the fortune of Captain Thompson and his crew. The S.S. Miraflores was destined to become one of the hundreds of victims lost during the start of World War II. Thompson was to sail to Haiti to load the vessel with a cargo of assorted fruits then sail for New York. On February 14, the vessel departed and was sighted the next day en route by a passing ship. S.S. Miraflores was not seen again, until the freighter was discovered and dived nearly fifty years later. By the 20th of February 1942, the S.S. Miraflores failed to report in New York and it was soon evident that her loss was to remain a mystery until investigations by the U.S. government after the war. Even then it was deducted that the ship was torpedoed but the exact location was based on speculation. It remained that way for several decades. Families would find few answers if any from the shipping company or from any government agencies. Soon the S.S. Miraflores was to become buried beneath the sands of the Atlantic Ocean and her memory was to be muttered only by an occasional word from the faded memories of those that were directly affected by her sinking. The German submarine U-432 was prowling the New Jersey coast, under the command of Heinz-Otto Schultze. This was the German submarine’s sixth successful patrol, lasting more than seven weeks while sinking five allied ships within a short time period of less than two weeks. Schultze torpedoed the Buarque off North Carolina on February 15, then Olinda on the 18th. After sinking the Olinda off the Virginia Capes the U-432 was credited with sinking the freighter Miraflores the next day; although his log book entry has no named vessel, only an estimated tonnage. At 3:18 a.m. Schultze positioned his submarine perpendicular to the little fruit ship steaming north and fired two torpedoes. Both struck the freighter with a united explosive power. The first torpedo struck forward of the wheelhouse cutting the ship in two which left the bow intact. The second followed striking amidship obliterating the stern forward of the aft deck house. It is probable that this enormous explosion caused the S.S. Miraflores had to sink within a few minutes. The possibility of the crew escaping the doomed sinking ship was nil. Had anyone survived the tremendous blast, the icy cold water and confusion in darkness diminished all hopes of escape too less than a few minutes. Hypothermia would spare no one in such an inhospitable frigid sea so far from land. No distress call could be made due to the direct and devastating blast near the bridge that probably stunned or killed all officers and crew in that proximity instantly. For Captain Thompson and his crew their luck had run out. There was a no hope of a rescue this night in the bitter cold North Atlantic. After sinking the Miraflores, Schultze and U-432 continued to plunder the American shipping route. The U-boat torpedoed the Azalea City farther off the New Jersey coast on February 21 and then the Marore off North Carolina before crossing the Atlantic to La Pallice to resupply. Three more successful patrols under Schultze were made by the U-432 on convoys in the north Atlantic. On March 11, 1943, the U-432 was detected by ASDIC of the Free French corvette Aconit. The U-432 success was soon to end. The crew under the new command of Hermann Eckhardt was celebrating the sinking of HMS Harvester of convoy HX-228. Failing to observe the corvette, the U-432 was taken off guard and machine gunned killing several crew including Eckhardt. U-432 was then was accidently rammed during a boarding attempt and sank. Twenty-six crew members were lost and 20 were captured, interrogated and then spent the remainder of the war as POWs. Looking back 66 years ago Walter Autry considers his survival a miraculous twist of fate. His decision at the age of 18 to join the Navy and take an offensive part in World War II actually saved his life. His patronage earned him, but several distinguished metals and citations for his service during World War II and during the Korean Conflict. After his retirement from the service Walt continued to serve his community, creating two successful businesses, providing for his daughter and son and remaining faithful throughout that time to his wife Lorraine. Walt now resides on a hundred acres, his cattle ranch that is on the outskirts of Livingston, Texas. Today, he enjoys giving talks for his local V.F. W. and weekly square dancing. He has fond memories of his youthful adventures with the crew of the lost ship. He is grateful to have been blessed with such a full life. Some people spend a lifetime not knowing answers to what may have been. He is satisfied knowing he was spared and was able to make a difference in the world. Knowing the true fate of the S. S. Miraflores has offered some answers to questions for Walt and the families of that ship. How might one’s life have been changed by a twist of fate or by a decision to serve their country? Walter Autry may be one patriot who can realize that answer. Today we placed flowers over the site, offering a moment of silence for those that perished off the coast on that February night in 1942. It was beautiful day of diving on a spectacular and historic wreck. May we never forget the privileges we have given to us by those that suffered and died for our freedom. Attached are Paul Whittaker's pictures of the dive. Also my birthday dive: http://gallery.mac.com/p.whittaker#100043 http://www.websitetoolbox.com/tool/post/atlanticdivers/vpost?id=2316161 __________________ Good Wreck Diving!"", '(originally published in Avenue magazine, June 2005)\nBy Joel Mangum\nCape Lookout – Many people may not realize how close the fight of WWII came to our land. The second sub to be sunk in U.S. waters and the first to be sunk by the US Coast Guard occurred just 30 miles offshore from Topsail Beach, south of Cape Lookout. Another landmark event occurred when 33 German POWs were taken from that same sunken sub and became the first foreign troops to be interned on U.S. soil since the War of 1812.\nIn early 1942, German U-boats were patrolling the eastern U.S. coast from Maine to Florida, and wreaking havoc on American supply ships. During January and February of that year, more than 60 allied ships were sunk by U-boats, some in broad daylight and using only the deck guns of the submarine. Some German sailors dubbed their missions “American Hunting Season” because of the easy pickings.\nWhat’s even more shocking is the U-boats were sometimes within 10 miles of shore, able to see cities and ships silhouetted in shore lights. U-boats were close enough to listen to jazz radio stations being broadcast from the U.S. mainland. During daylight the sub would sit on the ocean bottom, conserving fuel and batteries and resting the crew. At night, the U-boat would surface and look for targets.\nCaptain Helmut Rathke of U-352 was assigned to patrol the shipping routes of North Carolina from Cape Hatteras to Cape Lookout and was under orders to sink any supply ships they sighted. After two weeks of no definite targets, Rathke and his crew were desperate to sink a ship and spotted what they thought was a small freighter at 4:25 p.m. on May 9, 1942. They fired a torpedo they initially thought to be a direct hit, but moments later realized the target was not a freighter, but actually the fully armed USCG warship Icarus. The U-352’s torpedo was off the mark and exploded prematurely 200 yards off the port quarter. The crew of Icarus immediately went into battle mode and dropped five depth charges where they thought the U-boat to be. Depth charges are canisters of explosives dropped from ships that detonate when reaching a certain point under water.\nBecause the U-352 was operating in only 100 feet of water, they could not crash-dive far enough to evade their attackers. After three more depth charges were dropped by Icarus, the U-352 broke apart and surfaced at a latitude of 34.12 degrees north and longitude of 76.35 degrees west, with 33 men spilling out of the conning tower. Fifteen other Germans went down with the ship.\nAccording to U.S. Navy Department records, 45 minutes went by before the German survivors were rescued. One Icarus crewman even told the Germans bobbing in the water, “Gentlemen, I wish you a good evening-down with the sharks!”\nBecause there were no regulations or precedents on picking up prisoners of war, Icarus radioed headquarters three times before getting a response. After motoring away from the men in the water, the Icarus crew was ordered to turn back, pick up the survivors and land them at Charleston, S.C. for interrogation. Prisoners were brought on board and searched, then given sandwiches and coffee. Most prisoners were in good spirits because for them it meant the war was over. Many commented on the good treatment by their American captors.\nAccording to interrogation reports of the U.S. Navy, Captain Rathke had coached his crew well in the importance of not giving valuable information, even warning his men in the water before being rescued the punishments for revealing intelligence. U.S. and British investigators learned much about where the best bars and brothels were in Germany from the prisoners, but found out little about war operations and the U-boat fleets. The prisoners remained in Charleston for several days and then transferred to Fort Bragg, N.C. They did not return to Germany until 1947.\nCaptain Rathke made some of his opinions on the war known to interrogators such as disliking the Japanese but having to accept them as allies against Russia. He also dined with Hitler and would note how attentively the ruler would listen to conversations and his “amazing knowledge of detail.” Rathke also gave the view that England could not be invaded and refused to believe that German soldiers had committed atrocities upon Poland and Poles.\nU-352 remains a dive site today. The commander of the Icarus, Lieutenant Maurice D. Jester was awarded the Navy Cross and promoted. Gerd Reussel, a young German sailor who died of his wounds aboard the Icarus, was buried with full military honors in the National Cemetery at Beaufort, S.C. and his grave remains there today. News of other U-boats sank made news normally but for unknown reasons, no word of the U-352 was reported until months following the event. So goes another chapter of the rich history of the coastal waters of Southeastern North Carolina.']"	['<urn:uuid:f65edb9f-3321-4d21-bcc3-cd3fb1b9835e>', '<urn:uuid:902f0170-b5f3-412b-b654-81b2d06b7dbe>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T18:38:36.516973	13	87	2262
87	What strategies does P. aeruginosa use to spread and survive in hospitals?	P. aeruginosa uses a two-part strategy: First, it creates two cell types during division - 'stickers' that attach to host tissue and 'spreaders' that move to new sites using a rotating propeller. Later, it shifts to producing only surface-attached cells to establish long-term colonies and forms protective biofilms that help it resist antibiotics and the host's immune system.	"['The bacterium Pseudomonas aeruginosa is one of the most common and dangerous pathogens in hospitals causing severe infections in patients, such as wound infections, pneumonia or meningitis. During the first stage of infection, the pathogen needs to effectively attach to host tissue but at the same time spread to distant sites. At later stages of infection, the pathogen needs to re-adjust its behavior to permanently colonize the host and hide from its immune system, for example, by forming protective biofilms.\nFrom spreading to adhering cells\nThe key to success for P. aeruginosa is its perfected colonization strategy. ""During each division, the pathogen generates two distinct cell types, an attacking sticker and a motile spreader,"" explains Prof. Urs Jenal, research group leader at the Biozentrum of the University of Basel. ""The balance between these two cell types is crucial for the pathogen in order to successfully dwell on tissues and to disseminate rapidly.""\nIn the initial phase of an infection, the spreader actively moves towards tissue surfaces of the host with a rotating cell propeller, called flagellum. The researchers have now discovered that the flagellum also serves as a mechanical sensor. ""When the flagellum encounters a surface, bacteria rapidly respond by producing adhesive protrusions that firmly anchor the bacterium to the tissue,"" says Jenal. ""We were most surprised by how quickly this happens. Within seconds, the bacteria change their program to stick to the surface."" The tactile stimulus triggers the production of the bacterial signaling molecule cyclic-di-GMP, which in turn initiates the formation of the cell protrusions.\nFrom adhering to spreading cells\nFurthermore, the researchers have been able to show that upon division, surface attached bacteria generate two different offspring: one daughter cell remains a sticker that can damage the underlying host tissue, the other one becomes a spreader disseminating to distant sites. This process is also known as asymmetric cell division. ""This is due to the asymmetrical distribution of cyclic-di-GMP in the dividing mother cell,"" explains first author Benoît Laventie. ""While the daughter cell with high cyclic-di-GMP level remains attached to the landing site, the cell with low cyclic-di-GMP concentrations leaves the surface to colonize distant sites. Thus, P. aeruginosa exploits a simple business model: first establish, then grow and finally expand.\nA smart tactic: conquer and hide\nHowever, there is a limited window of opportunity for P. aeruginosa. After a few asymmetric divisions, the bacteria revert to dividing symmetrically producing exclusively virulent surface-attached offspring, thereby increasing the population at local sites exponentially. The pathogen has changed its strategy and is aiming for long-term establishment in the host and escaping the host\'s defense systems.\nThe researchers in the Jenal lab speculate that this colonization strategy is of a general nature and can be found in a wide variety of bacteria that effectively colonize different surfaces such as stones, shower curtains, coffee cups or human organs.\nBenoît-Joseph Laventie, Matteo Sangermani, Fabienne Estermann, Pablo Manfredi, Rémi Planes, Isabelle Hug, Tina Jaeger, Etienne Meunier, Petr Broz, Urs Jenal. A Surface-Induced Asymmetric Program Promotes Tissue Colonization by Pseudomonas aeruginosa. Cell Host & Microbe, published online 20 December 2018\nContact: Communications, Katrin Bühler', 'Pseudomonas aeruginosa is an opportunistic pathogen associated with a variety of life-threatening diseases. Infections caused by P. aeruginosa can be nearly untreatable because of its multidrug-resistance. One of the characteristics of P. aeruginosa that helps it survive in high drug concentrations is its ability to form biofilms–large communities of cells encompassed by extracellular polymeric substances that defend against many antibiotics. In fact, sub-minimum inhibitory concentrations of antibiotics stimulate biofilm production. This project aims to identify genes associated with biofilm induction in P. aeruginosa by screening a transposon mutant library for mutants that fail to show increased biofilm production when exposed to sub-minimum inhibitory concentrations of cefixime, tobramycin, and thiostrepton. So far, we have identified one gene, PA2714, that encodes a predicted molybdopterin oxidoreductase required for biofilm production. Because of the strong association between biofilm production and antibiotic tolerance in P. aeruginosa, the gene identified in this screen may be a useful therapeutic target for novel antimicrobials that can disrupt biofilm formation.\nPseudomonas aeruginosa, a gram-negative, rod- shaped bacteria, is a leading cause of hospital acquired infections; it remains difficult to treat because of its multidrug-resistance, especially in susceptible cystic fibrosis (CF) patients.1 CF is a genetic disease caused by mutations in the cystic fibrosis transmembrane conductance regulator (CFTR) gene.2 One characteristic of CF, increased viscosity of mucoid secretions, leads to environments conducive to the growth of P. aeruginosa biofilms in the lungs, resulting in 3 The prevalence of P. aeruginosa infection in CF patients is 60%, with acquired infections almost always becoming chronic.4 In CF patients, de novo emergence of antibiotic resistance has been observed after continual exposure to antimicrobials.2 In combination with patient-to-patient transmission of resistant strains in healthcare settings, this leads to the emergence of multidrug- resistant strains.2 It is important to continue studying the development of antibiotic resistance in P. aeruginosa to inform potential interventions.\nThe severity of P. aeruginosa infections is driven by its ability to form biofilms.5 A biofilm is a complex community of microorganisms embedded in an extracellular polymeric substance (EPS) that adheres to a surface. The EPS consists of a variety of macromolecules, including polysaccharides that act as a protective barrier against antimicrobials, proteins that provide a nutrient source via digestion of exogenous macromolecules, and extracellular DNA that facilitates horizontal gene transfer and provides structural integrity to the biofilm matrix.6 The benefits conferred by biofilms offer a more suitable environment, compared to planktonic growth, for bacteria.5\nSub-minimum inhibitory concentrations of antibiotics (sub-MIC), or concentrations below the lowest concentration that prevents growth, stimulate biofilm production in P. aeruginosa.7 Although the protective effect of the EPS is primarily structurally-related, the efficacy of antibiotics that can diffuse through this barrier is reduced against biofilms. One possible explanation for this phenomenon is that antimicrobial activity in biofilms can be quenched through diffusion-reaction inhibition via chelation of metal antimicrobials or enzymatic degradation of anti-microbials by proteins in the EPS.8 By lowering the concentration of antimicrobials to sub-lethal levels, cell survival within the biofilm community is promoted. In addition, there is a high degree of physiological heterogeneity within these cellular communities; in response to their local micro-environment some cells are more resistant than others due to their growth state or patterns of gene expression. The diverse set of genetic traits found in biofilm communities can be disseminated through horizontal gene transfer in order to increase overall antibiotic tolerance within the community. The acquisition of resistant genes can occur through uptake of extracellular DNA.5 It would be useful to investigate the genes associated with biofilm production in antibiotic conditions to gain insight into molecular mechanisms behind the antibiotic tolerance conferred by bio-films and to determine novel therapeutic targets for antimicrobials. We aim to identify genes that are essential to biofilm production by screening a library of transposon mutants for their ability to form biofilms in sub-MIC levels of the antibiotics cefixime, tobramycin, and thiostrepton. Genes associated with biofilm production can be elucidated by comparing biofilm production in mutants relative to wild-type P. aeruginosa. Furthermore, we will investigate the link between biofilms and antimicrobial activity by demonstrating that sub-MIC levels of antibiotics can stimulate biofilm production in wild type P. aeruginosa.\nA transposon mutant library of P. aeruginosa PAO1 KP containing the Himar1 Mariner trans- poson element was generated. A transposon is a mobile genetic element that can create mutations within the genome. The mutants in this library were created by mating PAO1 KP with Escherichia coli containing a plasmid carrying the Himar1 Mariner transposon, which inserts itself between TA dinucleotides, and a transposase that excises the transposon element from the plasmid.9 Containing a gene that encodes for gentamicin resistance, the transposon is inserted into a suicide vector unable to replicate in P. aeruginosa.9 Mutants that contained the transposon element in their genomic DNA were then selected by gentamicin screening, yielding ~13,500 mutants.\nThe library is currently being screened for mutants demonstrating a lack of biofilm stimulation in the presence of sub-MIC (5 μM) cefixime. The cut-off for increased biofilm production in wild-type PAO1 KP was established to be a >200% increase in biofilm in the presence of sub-MIC cefixime, compared to the vehicle control (dimethyl sulfoxide). Thus, hits from the screen were defined as all mutants that did not meet this cut-off (i.e. produced less than 200% of biofilm when exposed to the antibiotic, compared to DMSO). The bacteria were grown overnight in 150 μL of 10% lysogeny broth (LB) in a 96-well plate and subcultured for 2 hours in a 1:25 dilution. They were then grown overnight in a 96-well plate with a 96-peg lid with 5 μM cefixime or DMSO added. Biofilm was allowed to form on the pegs. After rinsing with 1X PBS to remove non-adherent bacteria, the biofilms on the pegs were stained with 150 μL of 0.1% (w/v) crystal violet for 15 minutes and de-stained into 200 μL of 33% (v/v) acetic acid for 5 minutes. Absorbance of the crystal violet in acetic acid was measured at 600 nm to quantify the relative amounts of biofilm.10\nFalse-positives were eliminated by retesting mutants identified from the initial screen through a dose-response experiment, with a concentration range spanning the MIC of cefixime, tobramycin, and thiostrepton to confirm that the mutants showed inhibited biofilm formation when compared with PAO1 KP in varying concentrations of three different antibiotics. Reduced biofilm formation would suggest that the gene disrupted by the Himar1 Mariner transposable element is essential for biofilm stimulation in the presence of sub-MIC antibiotic. The transposon insertion sites were then identified using two rounds of polymerase chain reaction (PCR). The first round was a touchdown PCR performed with a Himar1 Mariner-specific primer and an arbitrary primer. The second round of PCR was performed using the Himar1 Mariner-specific primer and a primer specific to the arbitrary primer used in the first round. PCR products were gel-purified and sequenced.\nOf the 4,200 mutants screened, one displayed reduced biofilm production in the presence of cefixime, tobramycin, and thiostrepton compared to the PAO1 KP wild-type control, hereafter referred to as BBTn41_G1. We also demonstrated that the highest levels of biofilm were observed at the highest concentration below the MIC of cefixime (5 μM) for both wild-type and the mutant. The results from the second round of screening (false positive elimination) for BBTn41_G1 is shown in Figure 1.\nTouchdown PCR determines that the transposable element in BBTn41_G1 is located at the start of the gene PA2714. Although this gene has yet to be characterized, the nucleotide sequence suggests that it encodes a molybdopterin oxido-reductase. Activated by binding to the cofactor molybdopterin, this family of enzymes catalyzes electron transfer reactions.11\nSo far, we have identified one possible gene that may contribute to biofilm formation. Based on its nucleotide sequence, PA2714 is likely to be gene encoding a molybdopterin oxidoreductase.11 This improves our understanding of the mechanism of molybdopterin-induced biofilm production. Loss-of-function mutations in molybdopterin synthase–an enzyme family that synthesizes the molybdopterin cofactor–increase biofilm formation.12 A possible explanation for this is that molybdopterin synthases consume guanine triphosphate (GTP), thereby reducing the amount of intracellular GTP available to form c-di-GMP, an important component of bio-films.12 Thus, decreased molybdopterin synthesis caused by mutations in molybdopterin synthases could increase intracellular GTP and c-di-GMP production, ultimately increasing biofilm production. In our experiment, we observed a decrease in biofilm formation with a molybdopterin oxidoreductase mutated by a transposon element.\nMoving forward, we will continue screening the rest of the library for genes that affect bio-film formation at sub-MIC antibiotic levels. Our findings may be useful in designing novel antimicrobials. With a better understanding of genetic factors affecting biofilm production, we can design drugs that inhibit biofilm formation and subsequently reduce antibiotic resistance.\nIn addition, many of the current high-throughput screens for new antimicrobials involve testing a huge library of drugs at arbitrary concentrations for indications of cell death.13 These screens could yield false negatives because MICs of new molecules are unknown; therefore, many potential antimicrobials are missed simply because they were screened at a concentration that was too low. Using biofilm production as an indicator of antimicrobial activity may be a more useful screening method, as we have demonstrated that relative amounts of biofilm production can be observed and measured at concentrations above and below the MIC of antibiotics.\nAntibiotic resistance represents a major global health threat. The deteriorating effectiveness of antibiotics renders both common and life-threatening diseases more difficult to treat.14 Accordingly, it is of paramount importance for the scientific community to better understand the molecular mechanisms behind antibiotic tolerance, so that novel and reliable antimicrobial treatments can be developed.\nThe World Health Organization has classified P. aeruginosa in the critical category of “priority pathogens” to target because of its multidrug-resistance and its threat in healthcare settings.15 In this project, we have demonstrated that P. aeruginosa has the ability to form biofilms in sub-MIC levels of antibiotics. In addition to identifying genes associated with biofilm production, we hope this project can serve as a proof-of-concept that biofilm production is a reliable indicator of antimicrobial activity in P. aeruginosa, a possibility that may be useful for drug screening. With the dearth of effective antibiotics, scientists may consider novel antimicrobials that target biofilm production as their mechanism of action.\nFigure 1: Effects of cefixime on PAO1 KP and BBTn41_G1 growth and biofilm production.\nA. Effects of cefixime on PAO1 KP planktonic growth and biofilm formation as a percentage of PAO1 KP in DMSO (n=1, 3 technical replicates).\nB. Effects of cefixime on BBTn41_G1 planktonic growth and biofilm formation as a percentage of BBTn41_G1 in DMSO (n=1, 3 technical replicates).\nBiofilm production increased for BBTn41_G1 and PAO1 KP with increasing concentration of drug until the lethal dose (10 μM). Both BBTn41_G1 and PAO1 KP showed maximum biofilm production at 5 μM of cefixime. However, at this concentration, BBTn41_G1 produced less than 600% more biofilm than the mutant in no drug, while PAO1 KP produced around 800% more biofilm than with no drug. Similar reduction in biofilm production was seen in tobramycin and thiostrepton.\nI would like to thank all the members of the Burrows Lab, especially Michael Ranieri, who created the transposon mutant library and aided me in labwork and manuscript preparation. I would also like to thank my supervisor, Dr. Lori Burrows, for her guidance.\nWritten by Jessica Chee\nReviewed by Dr. Lori Burrows, Professor, Department of Biochemistry and Biomedical Sciences & Department of Pathology and Molecular Medicine, McMaster University.\nReferences may be found in the journal.']"	['<urn:uuid:d80381a7-2099-498a-8705-7bf668310dee>', '<urn:uuid:20652181-ad04-4e92-a24d-c054817b22ed>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:38:36.516973	12	58	2423
88	oregano plant characteristics growing zones woody stems perennial annual	Oregano is a perennial herb in USDA zones 5 and higher, while in lower zones it grows as an annual. It has soft woody stems, similar to mint, unlike herbs like lavender or thyme which have harder woody stems. The plant grows very fast once the weather warms up and loves heat. Over time, the old stems will become woody and the plant becomes quite dense.	"[""Pruning An Oregano Plant: A Perennial Herb With Soft Woody Stems\nOregano is a perennial herb (in many climates) which grows very fast once the weather warms up. Pruning an oregano plant is important because it grows densely & you want to cut away the old growth to make way for the tender, tasty new growth. Here's how to do it.\nWhen my neighbor asked me to prune her oregano cascading out of a large terra cotta pot, I said “heck yeah”. Not only do I love pruning (my nickname was “Prunella” years ago!) but I was also lending Mary a hand. She hadn’t pruned this herb since it was planted over 2 years ago – oh my. Time for the overdue pruning of an oregano plant so all that tender new growth can appear as the weather warms.\nHerbs like lavender or thyme have harder woody stems. Oregano is like mint in that it has softer woody stems. Those old stems will eventually get woody over time and the plant becomes quite dense making it harder for the new growth to appear in spring and summer. I prune my mojito mint in the same way – out with the old and in with the new.\nPruning an Overgrown Oregano Plant:\nWhen To Prune\nOregano is a perennial herb in USDA zones 5 and higher. In lower zones, it’s grown as an annual.\nI live in Tucson, AZ & did this pruning towards the end of January. In colder climates it’s best to wait until spring when the danger of a freeze has passed. You don’t want to force out all that new growth & then have it hit.\nI grew up in Connecticut where we left the oregano be in fall & threw some hay over it for protection. We removed the hay in spring & did the big pruning.\nOregano grows fast & benefits from a mid-season pruning right after flowering. That’ll stimulate even more of that tasty new growth.\nThis is how the oregano looked before pruning – dense as can be with straggly stems way underneath.\nHow To Prune\nThis isn’t 1 of those finesse pruning jobs. You’re basically chopping off all the old growth to make way for the new.\nMake sure your pruners are clean & sharp. It’ll make the job so much easier. I used 2 pruners – Felcos for the big pruning & my Fiskar Floral Snips for the “finesse” work at the end.\nI made my way around the plant cutting it all the way back to 2-3″. That’s when you can really start to see the new growth that’s hiding underneath if your oregano is as dense as this 1. Yes, it’s what you call a hard pruning but this is what your oregano really needs.\nThe 2nd much lighter pruning was to remove some of those dead woody stems & leggy softer stems. I like to do this because it makes the plant look a little better. You can skip this step if you’d like but I prefer to clean things up as much as possible. Let’s be real, your oregano isn’t pretty at this point & will look like it’s been scalped for a month or so!\nThe poor parsley was trying to come back but is having a hard time!\nGood To Know\nYou prune oregano in this same manner whether it’s in a pot or in the ground.\nOregano loves the heat & will come back fast once the weather starts to warm.\nYou want to do this pruning (especially if your oregano is overgrown like this 1) to encourage all that new growth. The newer leaves have a better flavor than the older, tougher ones.\nDon’t be timid about this. You want to expose the new growth to sunlight & air. The oregano you see here was so thick that the new growth would have had a very hard time growing through it.\nThis is how it looks post-pruning. Not a sight to behold but just you wait – that new growth will spring forth in no time.\nLike my mint, I’ll apply about 1″ of worm compost (this is my favorite amendment, which I use sparingly because it’s rich) & 2-3″ of compost on the very top in a month or so. This will be the feeding of these herbs for the entire season.\nThe pile of twisted stems. Oregano anyone??\nI must admit, I’ll reap the benefits of this pruning as much as Mary. She lets me harvest stems of her oregano and basil whenever I like. With all that oregano I pruned off, I see lots of batches of marinara sauce coming right up!\nThis post may contain affiliate links. Please check our policies here.""]"	['<urn:uuid:01d722c7-2b92-4ab6-a5f3-99718f9dd1d5>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T18:38:36.516973	9	66	793
89	How can someone lead successful projects while running their business?	Successfully leading projects while running a business requires multiple key skills and proper planning. First, you need a strong business plan that includes an executive summary, market analysis, and financial forecasts. The plan should outline your business objectives, organizational structure, and future development goals. For project success, essential skills include effective communication with team members and stakeholders, strong organizational abilities, and time management. You'll need to be adept at problem-solving, decision-making, and conflict resolution. Leadership is crucial - you must be able to inspire and motivate teams while delegating tasks appropriately. Additionally, maintaining clear documentation, managing risks, and ensuring proper resource allocation are vital for both business and project success.	['Small businesses need a strong business plan, whether for financial resources or new growth opportunities. A business plan template can be helpful here and help entrepreneurs plan their businesses.\nWhat should you include in your business plan?\nNot all businesses invest much time creating a comprehensive program in the early stages. But most business owners find it helpful to take a step back, define their business idea and target market, identify their point of difference, and develop a successful strategy.\nA business plan template is made up of several elements. Whether it is a salon business plan or a farm business plan, it is essential to address these various elements; And while it is not necessary to include all the details, accuracy is critical to creating a successful business plan. So let’s take a closer look at these sections with Damavand Business.\nAn executive summary forms the basis of your business plan. This provides an overview of your business, products, distinctions, and financial forecasts.\nMost investors spend most of their time digging in this area. The challenge is to provide answers without going into too much detail.\nOverview and business objectives\nIn this section, your business plan explains who you are and what you plan to do with your business. An overview of the industry helps to explain what makes you different. This section is also helpful in providing an overview of your company. Who are the key members of your team? What value do they add? These are the questions that will be answered in this section.\nThis section describes the values that drive your business. This includes a mission statement that states why your company exists. This statement should be a short sentence that sufficiently expresses the basic tenet of your business. For example, LinkedIn’s mission statement connects the world’s experts to make them more productive and successful.\nAlso read: Types of business strategies\nThe success of your company depends on your ability to understand the market. Where is your target market? What are their main concerns? What motivates them to buy from you? These are some of the fundamental questions answered in this section.\nOrganization and management\nThis section provides information about the organizational structure of your company. From an operational point of view, it is essential to have a legal system that can help you in the future. Enter information about your team and management team.\nProducts and services provided\nAlthough you’ve included information about your products and services in the executive summary, this section will help you add more details. If you offer many products and services, you may want to provide more general information about your product lines. If you give only a few products and services, a detailed description may be helpful.\nYour marketing and sales plan is crucial for many stakeholders: investors, employees, and partners – to name a few. This should reflect your marketing strategy to promote your business and ensure that your products and services reach your target audience. Focus on highlighting your marketing efforts by emphasizing the four critical elements of marketing: product, pricing, advertising, and location.\nAnalysis of competitors\nNo matter how carefully you analyze your market, you can not succeed unless you are careful about other businesses. This is important because it helps you identify your differentiator and gain a competitive edge in the long run. Given the importance of competitive analytics, it is an excellent idea to delve deeper and identify your competitors’ leadership, history, and products and services.\nWhen you apply for a business loan, they will look for your financial plan to determine how much you can make in the future. The business finance plan section provides an overview of all the financial affairs involved in running your company. Three main components should be included in this section: profit and loss statement, balance sheet, and cash flow statement.\nDo not forget to read the valuable articles of Harvard Business Review to better understand these concepts\nFuture business plans for development\nWhile working on parts of a business plan requires keeping in mind current business goals, it is just as important to consider the future.\nWhat are your future work plans?\nWhere would you like your company to go?\nHow are you going to get there?\nThese are essential questions that you should try to answer to create a plan for the future of your business.\nSummary of logistics and operations\nBringing your products and services to the finish line is the key to your success. To achieve this, you must rely on many shareholders, such as suppliers and distributors. This section will help you identify the key partners essential in setting up your business. It also builds a robust supply chain that can work well when ready to sell.\nThe management summary outlines how your business is structured, the people involved, and the external resources. The idea here is to show your team’s expertise and foreign business partners.\nBecause your business plan is supposed to be concise, it may be difficult for you to include additional information. This is where including the appendix may come in handy. You can add your team player’s resume, market research, legal documents, and product information here. For example, if you have a business plan for kindergarten, be sure to include certifications and medical experience.\nWhat is a business plan?\nA business plan is a document that describes a company’s business goals and highlights its strategy for achieving those goals. In addition, it represents the sales plan, the financial and operational model of a business, and other vital elements necessary for its success.\nTips for writing a fantastic business plan\nStructuring information is vital to creating a good business plan. Here are some tips to help you get started:\nKeep it short: Although there is a lot of information that you may think is essential, the program should be concise and readable, preferably no longer than 20 pages.\nProvide supporting evidence: If you have documents that can make your business plan more compelling, add them as attachments.\nStay consistent: The document should have a consistent tone, message, and style; So it’s good to prove it right before finalizing it.\nAvoid Vocabulary: Do not enter industry-specific terms that make the document illegible. Instead, be honest and try to explain yourself as simply as possible.\nBe clear about your purpose: Where do you think this design will be used? Is this what you want to create for your team, or do you want to bring it to potential investors? Then, based to develop a structure that makes the most value for your audience based on your needs.\nDo your research: Your business plan should show the time and effort you put into it. This is especially important when working on specific areas such as competitive analysis and target markets.\nBe clear: When it comes to your company’s financial health, do not hide the problems you may face. Providing a clear picture will help you gain the trust and confidence of your investor.\nUpdate it: A business plan evolves over time. Your market realities may have changed, or new competition may have arisen; So it’s good to keep the document up to date to reflect your current business.\nAlso, you can read:', 'Project Management Skills are some of the most important skills that you need to possess to be a successful project manager.\nIn addition, it’s good to know there are a variety of project management skills that you will need to master to be successful.\nIn this article, we will take a look at some of the most important project management skills that you need to consider, allowing you to use these in both your personal and professional life.\nWhat are Project Management Skills?\nProject management skills encompass all of the abilities and knowledge needed to plan, execute, and close a project successfully. They include everything from scheduling and budgeting to risk management and stakeholder communications.\nTo be a successful project manager, it is important to have a well-rounded set of skills that can be applied in a variety of situations.\nMoreover, project management skills are not only for project managers. In today’s business environment, many employees are expected to have at least some project management skills.\nWhether you are working on a small team or managing a large organization, these skills will help you to get the job done efficiently and effectively.\nWhy Is it Important To Recognize Project Management Skills & Their Benefits?\nProject management skills are important because they provide individuals with the ability to successfully manage projects from start to finish.\nProject management skills can also help to improve communication and collaboration among team members, as well as increase the overall success rate of projects.\nThe following are some of the key benefits that can be gained from possessing strong project management skills:\n- Increased Efficiency\n- Improved Communication\n- Enhanced Collaboration\n- More Effective Planning\n- Greater Control Over Projects\n- Decreased Risk of Failure\n- Increased ROI\n- More Satisfied Customers\n- More Profitable Projects\n- Stronger Career Prospects\n10 Examples of Project Management Skills\nHere’s a list of project management skills that can help you succeed in any role.\nThe first and most important skill on this list is communication. As a project manager, you need to be able to communicate clearly with your team, clients, and other stakeholders. This means being able to explain complex concepts in simple terms, writing clear and concise emails, and giving effective presentations.\nA good project manager is organized and can keep track of all the different aspects of their project. This includes creating timelines, setting deadlines, and keeping all the different balls they are juggling in the air (at the same time).\nPlanning is a critical component of any successful project. A good project manager will create detailed plans that consider all the different risks and potential problems that could occur. They will also have contingency plans in place in case something does go wrong.\nA project manager needs to be good at time management to keep their project on track. This means being able to prioritize tasks, set deadlines, and make sure everyone on the team is working efficiently.\nProblems will inevitably arise during any project. A good project manager will be able to quickly identify the root cause of the problem and come up with a solution. They will also be able to communicate the problem and solution to the team and other stakeholders effectively.\nA big part of a project manager’s job is making decisions. This could be anything from deciding which vendor to use for a particular service to how to handle a conflict within the team. A good project manager can weigh all the different options and make a decision based on what is best for the project.\nA good project manager knows when to delegate tasks to other members of the team. They will assign tasks based on each person’s strengths and abilities and make sure that everyone has a clear understanding of their role in the project.\nConflicts will inevitably arise during any project. A good project manager will be able to quickly identify the root cause of the conflict and come up with a solution that satisfies all parties involved. They will also be able to effectively communicate the resolution to the team and other stakeholders.\nA good project manager needs to be a strong leader. This means being able to inspire and motivate the team, provide clear direction, and make tough decisions when necessary.\nA successful project requires a team of people working together towards a common goal. A good project manager knows how to get the best out of each member of the team and how to build a cohesive unit that can work effectively together.\nHow To Identify & Master Project Management Skills\nProject management skills are essential for any individual who wants to be successful in managing projects.\nBy definition, project management is the application of knowledge, skills, tools, and techniques to project activities to meet project requirements. To acquire these skills, one must first identify what they are.\nHere’s how to do so.\nUnderstand The Different Types Of Project Management Skills\nThere are many different types of project management skills, and it’s important to understand each one. The most common skill sets include technical skills, interpersonal skills, and organizational skills.\nTechnical skills are those related to the actual implementation of a project. This might include knowledge of specific software or hardware, as well as an understanding of project management methodologies.\nInterpersonal skills are those that help you interact with other people involved in the project. This includes communication and negotiation skills.\nOrganizational skills are those that help you keep the project organized and on track. This might include time management and task prioritization skills.\nAssess Your Own Skill Set\nOnce you understand the different types of project management skills, it’s time to assess your own skill set. Ask yourself which skills you already possess and which ones you need to work on.\nIf you’re not sure where to start, consider taking a project management certification course. This will help you identify your strengths and weaknesses, as well as allow you to learn new skills.\nDevelop a Plan To Improve Your Skills\nOnce you’ve identified the project management skills you need to work on, it’s time to develop a plan to improve them. Knowing where you’re going is key to getting better at anything, and project management is no different.\nPractice, Practice, Practice\nThe best way to improve your project management skills is to practice them. Find opportunities to use your skills in real-world situations. This will not only help you become more proficient in using them, but it will also help you build confidence in your abilities.\nSeek Feedback From Others\nDon’t forget to seek feedback from others. Ask your colleagues, friends, or family members for honest feedback on your performance. This can be an invaluable source of information for identifying areas where you need improvement.\nHow To Improve Project Management Skills in The Workplace\nThe workplace is one of the key places where people learn and develop essential skills. Whether it’s learning how to use a new software program or gaining an understanding of customer service, the workplace is where we build our professional toolkits.\nExpanding further on this, project management skills are needed in nearly every workplace. From small businesses to global enterprises, being able to manage projects effectively is a key ingredient for success.\nConsider the following situations that can help you improve your project management skills in the workplace.\n- You’re in charge of a project that is behind schedule\n- One of your team members is not meeting their deadlines\n- There is a conflict between two team members who are working on the same project\n- Your team is struggling to stay organized and on track\n- You’ve been asked to take on a new project that is outside of your comfort zone\nHow To Highlight Project Management Skills In Resume & Job Interview\nWhen creating a resume or sitting for a job interview, it’s important to be able to show off your project management skills. You want to demonstrate to potential employers that you can successfully lead and oversee projects from start to finish.\nWhen creating your resume, make sure to include any relevant experience you have in managing projects. This can include both paid and volunteer work. Be sure to list specific details about each project you oversaw, such as the size of the team, the budget, and the timeline.\nIn a job interview, be prepared to talk about times when you had to manage a challenging project. This is a great opportunity to showcase your problem-solving skills and how you were able to overcome any obstacles.\nWhen discussing your project management skills, use specific examples and avoid generalities. This will help the interviewer get a better sense of your abilities and how you would be able to handle managing a project at their company.\nWhat is the Main Difference Between Soft & Hard Project Management Skills\nThe main difference between soft and hard project management skills is that soft skills are more interpersonal, while hard skills are more technical.\nSoft skills involve things like communication, teamwork, and problem-solving, while hard skills are focused on specific tasks such as budgeting, scheduling, and engineering. Hard skills are usually easier to quantify and measure than soft skills.\nThere are many debates among project managers about which kind of skills are more important. Some people believe that hard skills are the most essential since they directly contribute to the success of a project.\nOthers argue that soft skills are equally important since they can make or break a team’s ability to work together effectively. Ultimately, the best approach is to focus on developing both types of skills.\nProject Management Skills FAQs\nWhat Are The 3 Critical Skills Of A Project Manager?\n3 critical skills of a project manager include:\n- Being able to effectively communicate with all members of the project team, including those who may have different levels of experience or expertise\n- Being able to create and maintain a detailed project schedule\n- Being able to identify and resolve potential conflicts within the project team\nWhat Skills Do You Need For Project Work?\nTo be successful at project work, it is important to have strong problem-solving skills, as well as be organized and detail-oriented. Additionally, being able to effectively communicate with all members of the team is essential.\nIs Project Management A Hard Skill?\nWhile some people may view project management as a soft skill, it is generally considered to be a hard skill. This is because it requires specific knowledge and experience to be successful. This can be acquired through formal education or training, as well as on-the-job experience.']	['<urn:uuid:5443b38b-8e9e-4a40-8de3-5a6ce2699f3b>', '<urn:uuid:730ddf6b-a737-4021-b766-a3452b43ad5f>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T18:38:36.516973	10	110	2983
90	ways protect home extreme temperatures health	Home weatherization and energy efficiency improvements can protect health during extreme temperatures. These include using materials that are poor heat conductors, like wool instead of linen for clothing and wooden handles instead of metal ones for teapots. Modern weatherization programs can reduce health impacts through better sleep, cognitive function, and decreased financial stress while also reducing air pollution exposure.	"[""The Montgomery Family\nTO JOHN LINING AT CHARLESTON, SOUTH CAROLINA.\nNew York, 14 April, 1757.\nIt is a long time since I had the pleasure of a line from you ; and, indeed, the troubles of our country, with the hurry of business I have been engaged in on that account, have made me so bad a correspondent, that I ought not to expect punctuality in others.\nBut, being about to embark for England, I could not quit the continent without paying my respects to you, and, at the same time, taking leave to introduce to your acquaintance a gentleman of learning and merit, Colonel Henry Bouquet, who does me the favor to present you this letter, and with whom I am sure you will be much pleased.\nProfessor Simpson, of Glasgow, lately communicated to me some curious experiments of a physician of his acquaintance, by which it appeared, that an extraordinary degree of cold, even to freezing, might be produced by evaporation. I have not had leisure to repeat and examine more than the first and easiest of them, viz. Wet the ball of a thermometer by a feather dipped in spirit of wine, which has been kept in the same room, and has, of course, the same degree of heat or cold. The mercury sinks presently three or four degrees, and the quicker, if, during the evaporation, you blow on the ball with bellows; a second wetting and blowing, when the mercury is down, carries it yet lower. I think I did not get it lower than five or six degrees from where it naturally stood, which was, at that time, sixty. But it is said, that a vessel of water being placed in another somewhat larger, containing spirit, in such a manner that the vessel of water is surrounded with the spirit, and both placed under the receiver of an air-pump; on exhausting the air, the spirit, evaporating, leaves such a degree of cold as to freeze the water, though the thermometer, in the open air, stands many degrees above the freezing point.\nI know not how this phenomenon is to be accounted for; but it gives me occasion to mention some loose notions relating to heat and cold, which I have for some time entertained, but not yet reduced into any form. Allowing common fire, as well as electrical, to be a fluid capable of permeating other bodies, and seeking an equilibrium, I imagine some bodies are better fitted by nature to be conductors of that fluid than others; and that, generally, those which are the best conductors of the electrical fluid, are also the best conductors of this; and e contra. Thus a body which is a good conductor of fire readily receives it into its substance, and conducts it through the whole to all the parts, as metals and water do; and if two bodies, both good conductors, one heated, the other in its common state, are brought into contact with each other, the body which has most fire readily communicates of it to that which had least, and that which had least readily receives it, till an equilibrium is produced. Thus, if you take a dollar between your fingers with one hand, and a piece of wood, of the same dimensions, with the other, and bring both at the same time to the flame of a candle, you will find yourself obliged to drop the dollar before you drop the wood, because it conducts the heat of the candle sooner to your flesh. Thus, if a silver tea-pot had a handle of the same metal, it would conduct the heat from the water to the hand, and become too hot to be used; we therefore give to a metal tea-pot a handle of wood, which is not so good a conductor as metal. But a china or stone tea-pot being in some degree of the nature of glass, which is not a good conductor of heat, may have a handle of the same stuff'. Thus, also, a damp moist air shall make a man more sensible of cold, or chill him more, than a dry air that is colder, because a moist air is fitter to receive and conduct away the heat of his body. This fluid, entering bodies\nin great quantity first expands them by separating their parts a little, afterwards, by farther separating their parts, it renders solids iluid, and at length dissipates their parts in air. Take this fluid from melted lead, or from water, the parts cohere again; the first grows solid, the latter becomes ice; and this is sooner done by the means of good conductors. Thus, if you take, as I have done, a square bar of lead, four inches long, and one inch thick, together with three pieces of wood planed to the same dimensions, and lay them, as in the margin, on a smooth board, fixed so as not to be easily separated or moved, and pour into the cavity they form, as much melted lead as will fill it, you will see the melted lead chill, and become firm, on the side next the leaden bar, some time before it chills on the other three sides in contact with the wooden bars, though, before the lead was poured in, they might all be supposed to have the same degree of heat or coldness, as they had been exposed in the same room to the same air. You will likewise observe, that the leaden bar, as it has cooled the melted lead more than the wooden bars have done, so it is itself more heated by the melted lead. There is a certain quantity of this fluid, called fire, in every living human body, which fluid, being in due proportion, keeps the parts of the flesh and blood at such a just distance from each other, as that the flesh and nerves are supple, and the blood fit for circulation. If part of this due proportion of fire be conducted away, by means of a contact with other bodies, as air, water, or metals, the parts of our skin and flesh that come into such contact first draw more near together than is agreeable, and give that sensation which we call cold; and if too much be conveyed away, the body stiffens, the blood ceases to flow, and death ensues. On the other hand, if too much of this fluid be communicated to the flesh, the parts are separated too far, and pain ensues, as when they are separated by a pin or lancet. The sensation, that the separation by fire occasions, we call heat, or burning. My desk on which I now write, and the lock of my desk, are both exposed to the same temperature of the air, and have therefore the same degree of heat or cold; yet if I lay my hand successively on the wood and on the metal, the latter feels much the coldest, not that it is really so, but, being a better conductor, it more readily than the wood takes away and draws into itself the fire that was in my skin. Accordingly if I lay one hand, part on the lock, and part on the wood, and, after it has lain so some time, I feel both parts with my other hand, I find the part that has been in contact with the lock, very sensibly colder to the touch, than the part that lay on the wood. How a living animal obtains its quantity of this fluid, called fire, is a curious question. I have shown, that some bodies (as metals) have a power of attracting it stronger than others; and I have sometimes suspected, that a living body had some power of attracting out of the air, or other bodies, the heat it wanted. Thus metals hammered, or repeatedly bent, grow hot in the bent or hammered part . But when I consider that air, in contact with the body, cools it; that the surrounding air is rather heated by its contact with the body; that every breath of cooler air, drawn in, carries oft' part of the body's heat when it passes out again; that therefore there must be in the body a fund for producing it, or otherwise the animal would soon grow cold; I have been rather inclined to think, that the fluid fire, as well as the fluid car, is attracted by plants in their growth, and becomes consolidated with the other materials of which they are formed, and makes a great part of their substance ; that, when they come to be digested, and to suffer in the vessels a kind of fermentation, part of the fire, as well as part of the air, recovers its fluid, active state again, and diffuses itself in the body digesting and separating it; that the fire, so reproduced by digestion and separation, continually leaving the body, its place is supplied by fresh quantities, arising from the continual separation; that whatever quickens the motion of the fluids in an animal quickens the separation, and reproduces more of the fire, as exercise; that all the fire emitted by wood, and other combustibles, when burning, existed in them before, in a solid state, being only discovered when separating; that some fossils, as sulphur, seacoal, &c., contain a great deal of solid fire; and that, in short, what escapes and is dissipated In the burning of bodies, besides water and earth, is generally the air and fire that before made parts of the solid. Thus I imagine, that animal heat arises by or from a kind of fermentation in the juices of the body, in the same manner as heat arises in the liquors preparing for distillation, wherein there is a separation of the spirituous, from the watery and earthy parts. And it is remarkable, that the liquor in a distiller's vat, when in its highest and best state of fermentation, as I have been informed, has the same degree of heat with the human body, that is, about 94 or 96.\nThus, as by a constant supply of fuel in a chimney, you keep a warm room, so, by a constant supply of food in the stomach, you keep a warm body; only, where little exercise is used, the heat may possibly be conducted away too fast; in which case such materials are to be used for clothing and bedding, against the effects of an immediate contact of the air, as are, in themselves, bad conductors of heat, and, consequently, prevent its being communicated through their substance to the air. Hence what is called warmth in wool, and its preference, on that account, to linen; wool not being so good a conductor; and hence all the natural coverings of animals, to keep them warm, are such as retain and confine the natural heat in the. body, by being bad conductors, such as wool, hair, feathers, and the silk by which the silk-worm, in its tender embryo state, is first clothed. Clothing, thus considered, does not make a man warm by giving warmth, but by preventing the too quick dissipation of the heat produced in his body, and so occasioning an accumulation.\nThere is another curious question I will just venture to touch upon, viz. Whence arises the sudden extraordinary degree of cold, perceptible on mixing some chemical liquors, and even on mixing salt and snow, where the composition appears colder than the coldest of the ingredients ? I have never seen the chemical mixtures made; but salt and snow I have often mixed myself, and am fully satisfied that the composition feels much colder to the touch, and lowers the mercury in the thermometer more, than either ingredient would do separately. I suppose, with others, that cold is nothing more than the absence of heat or fire. Now, if the quantity of fire before contained or diffused in the snow and salt was expelled in the uniting of the two matters, it must be driven away either through the air or the vessel containing them. If it is driven off through the air, it must warm the air; and a thermometer held over the mixture, without touching it, would discover the heat, by the rising of the mercury, as it must, and always does, in warm air.\nThis, indeed, I have not tried, but I should guess it\nVOL. VI. 27 R*\nwould rather be driven off through the vessel, especially if the vessel be metal, as being a better conductor than air; and so one should find the basin warmer after such mixture. But, on the contrary, the vessel grows cold, and even water, in which the vessel is sometimes placed for the experiment, freezes into hard ice on the basin. Now I know not how to account for this, otherwise than by supposing, that the composition is a better conductor of fire than the ingredients separately, and, like the lock compared with the wood, has a stronger power of attracting fire, and does accordingly attract it suddenly from the fingers, or a thermometer put into it, from the basin that contains it, and from the water in contact with the outside of the basin; so that the fingers have the sensation of extreme cold, by being deprived of much of their natural fire; the thermometer sinks, by having part of its fire drawn out of the mercury; the basin grows colder to the touch, as, by having its fire drawn into the mixture, it is become more capable of drawing and receiving it from the hand; and, through the basin, the water loses its fire that kept it fluid ; so it becomes ice. One would expect, that from all this attracted acquisition of fire to the composition, it should become warmer; and, in fact, the snow and salt dissolve at the same time into water, without freezing.\nI am, Sir,\nPlaque on the home of Dr. John Lining, Charleston, South Carolina, USA\nPhoto courtesy Michael Sean Nix, Feb 13, 2010\nDr. John Lining, a young physician who immigrated to Charleston from Scotland in 1730, was married to Sarah Hill in 1739. It is not known if the couple actually set up housekeeping in the town house of the bride’s family or not, but it is John Lining’s name that will forever be associated with the house. Here’s why: About the time of his marriage, Dr. Lining was frustrated by his inability to offer any medical remedy for the terrible epidemics of smallpox, whooping cough, and yellow fever which frequently swept through the colony with deadly consequences for many sufferers. As a man interested in science, he was convinced that a logical connection existed between South Carolina’s semi-tropical weather and these bouts of dreaded disease. Thus, he began to record scientific observations of the local climate from a make-shift “laboratory” in the house at 106 Broad Street. In particular, he was concerned with measuring the affects of the elements on the human body. He shared his meticulous findings through regular correspondence with a fellow scientist in Philadelphia – the young Benjamin Franklin. Eventually, the impressive weather observations he measured and recorded were published in England. The studies appeared in the Philosophical Transactions of the Royal Society (of London) in 1742-43. Dr. Lining’s work at 106 Broad Street was one of the important steps forward in the 18th century’s “Age of Reason” – contributing to the growing body of knowledge that would eventually save countless lives not only in Charleston but around the world. - from Preservation Progress for the Preservation Society of Charleston, 2006"", 'Research Assistant Professor, Social Environment and Health Program\nAs an environmental epidemiologist and with funding from the National Institute of Environmental Health Sciences, the National Science Foundation, and Detroit community input, I study how social, economic, health, and built environment characteristics and/or air quality affect vulnerability to extreme heat and extreme precipitation. This research will help cities understand how to adapt to heat, heat waves, higher pollen levels, and heavy rainfall in a changing climate.\nI received my BA in Biology from the University of Chicago, with a specialization in Ecology and Evolution. Subsequently, I worked as a research assistant in the Clinical Trials Office at the Karmanos Cancer Institute in Detroit, MI before pursing a Masters in Public Health at the University of Michigan. I completed my MPH in 2008 and then completed my PhD in 2013 in the University of Michigan Department of Environmental Health Sciences, where I was a National Institute on Aging Public Health and Aging trainee. My dissertation focused on associations between high temperatures and hospital admissions and mortality among the elderly as well as sociodemographic and land cover characteristics that modify these associations. In my postdoctoral fellowships, I studied how social and environmental characteristics influence vulnerability to heat-related health effects, using data from longitudinal studies of cardiovascular health in seven U.S. cities as well as state and national records of severe health events.\nAssessing the Neuropsychological Benefits of Weatherization Programs\nModifications to homes that increase energy efficiency and improve indoor temperatures may reduce the health impacts of extreme temperatures and financial stress. We will characterize and monetize the health impacts of low-cost and higher-cost weatherization measures, which may improve the health of older adults through improvements to sleep, cognitive function, and financial worry as well as community-wide reductions in air pollution exposure. Weatherization interventions may allow older adults to both mitigate and adapt to extreme weather.\nClimate Change and Allergies (Institute for Global Change Biology)\nWith climate change, pollen levels are rising in many parts of the country. Using a new daily speciated pollen model, we are estimating the onset and time course of allergy and asthma symptoms in people with allergies as well as associations between asthma emergency department visits and different species of pollen in the Industrial Midwest.\nEnergy Justice, Housing and Health in a Changing Climate (M-Cubed, with Tony Reames and Marie O’Neill)\nLow and moderate income households face increasing challenges regarding household energy insecurity. Additionally, a changing climate with increased extreme weather may exacerbate weather-related health effects, This project aims to characterize and map the health and economic burdens of housing quality, air pollution, and energy insecurity\nCausal Mechanisms for Sustainable Adaptation to Adverse Heat and Precipitation Health Effects\nWide socioeconomic disparities exist in weather-associated morbidity and mortality. Using novel data linkages while working with local officials and community leaders to understand the housing and health characteristics that increase vulnerability to heat-and precipitation-associated health effects will help identify optimal and sustainable strategies for adapting to increasing extreme weather events\n- Gronlund CJ, Cameron L, Shea C, O’Neill MS. Assessing the magnitude and uncertainties of the burden of selected diseases attributable to extreme heat and extreme precipitation under a climate change scenario in Michigan for the period 2041-2070. Environ Health, Accepted.\n- Gronlund CJ, Sullivan K. Kefelgn Y, Cameron L, O’Neill MS. Climate change and temperature extremes: a review of heat- and cold-related morbidity and mortality concerns of municipalities. Maturitas, 2018, 114:54-59. DOI: 10.1016/j.maturitas.2018.06.002\n- Gronlund CJ, Sheppard L, O’Neill MS, Adar S, Auchincloss A, Kaufman J, Lima JA, Madrigano J, Diez Roux AV. Vulnerability to the cardiovascular effects of summer heat in six U.S. cities: Results from the Multi-Ethnic Study of Atherosclerosis (MESA). Epidemiology, 2018. Epidemiology. 2018 Nov;29(6):756-764. doi: 10.1097/EDE.0000000000000910.\n- Gronlund CJ, Zanobetti A, Wellenius GA, Schwartz JD, O’Neill MS. Vulnerability to renal, heat and respiratory hospitalizations during extreme heat among U.S. elderly. Climatic Change, 2016, 136(3):631-645. DOI: 10.1007/s10584-016-1638-9.\n- Gronlund CJ, Berrocal VJ, White-Newsome JL, Conlon KC, O’Neill MS. Vulnerability to extreme heat by individual characteristics and area green space and socio-demographic characteristics among the elderly in Michigan, 1990-2007. Environ Res, 2014, 136:449-461. DOI: 10.1016/j.envres.2014.08.042.\n- Gronlund CJ, Zanobetti A, Schwartz JD, Wellenius GA, O’Neill MS. Heat, heat waves and hospital admissions among the elderly in the United States, 1992-2006. Environ Health Perspect, 2014, 122(11): 1187-1192. DOI:10.1289/ehp.1206132.']"	['<urn:uuid:dc17ba3a-b752-4d9d-a910-a25bb552a564>', '<urn:uuid:fa32e2ee-b124-4dfa-a002-cd9d245b37db>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T18:38:36.516973	6	59	3330
91	brain dead requirements transplantation legal usa	According to the Uniform Determination of Death Act (UDDA), which has been adopted in every state, death is defined as either irreversible cessation of cardiorespiratory activity or irreversible cessation of all brain activity, including activity of the brain stem. This legislation was developed in response to the dilemmas generated by organ transplantation technology.	"['Address correspondence to Mary Z. Pelias, PhD, JD, Dept. of Biometry and Genetics, Louisiana State University, Medical Center, 1901 Perdido Street, New Orleans, LA 70112-1393. Ph: 504-568-6151, Fax: 504-568-8500.\nRecent advances in medical and surgical technologies now permit successful organ transplantations for persons whose lives are in peril because of failure of one or more of their own vital organs. While organ transplantations have brought dramatic benefits to many grateful persons, the use of these procedures has also generated intense debate about ethical and legal propriety of removing organs from individuals who still have some vital functions or who are being maintained on machines. One very troublesome area of vital organ transplantation is the use of anencephalic infants as sources of these organs. Although both the proponents and opponents of this sad but potentially life-saving application of organ transplantation argue their positions with sincerity, their arguments sometimes lack clarity.\nAnencephalics lack recognizable cerebral hemispheres as well as overlying cranium and scalp11. The vital functions of live-born anencephalic infants are controlled by their brain stems, and they do show some reflexive responses during their brief lives5. They usually have adequate cardiorespiratory activity to permit survival for at least a short time. Because anencephaly is usually lethal within hours or days after birth, anencephalics fall into a class of infants who are born dying; they are permanently unconscious, with no hope of growth, mental development, cognition, or interaction with others.\nThe Uniform Determination of Death Act\nA cornerstone in the debate about anencephalics is the Uniform Determination of Death Act (UDDA)20, which has been adopted in every state in response to the dilemmas generated by organ transplantation technology2. This legislation defines death as irreversible cessation of cardiorespiratory activity or irreversible cessation of all brain activity, including activity of the brain stem. Authors who oppose the use of anencephalics as organ donors note that anencephalics do have brain stem activity and are, indeed,biologically alive. On the other hand, authors who support the use of anencephalics as organ donors have suggested that the UDDA could be modified to include anencephalic infants as a third category of the legally dead, so that healthy vital organs may be removed from these infants before the decline or cessation of vital functions18. Rewriting the UDDA to define anencephalics as legally dead would not, however, alter the fact that these infants are alive. No change of language would change the biological status of these infants or the fact that anencephalics are human infants, born alive, but born dying6. To create such a legal fiction would be nothing more than an error in semantics and, as Mearns et al 9 note, would carry the ominous tones of legalized social experiments that have occurred in the past.\nThe critical concerns of diagnosis\nThe suggestion to modify the UDDA has elicited heated debate, including the comments by Mearns et al in this journal. These authors, as well as most others, acknowledge that most anencephalics are aborted, or are stillborn, or are otherwise unsuitable for consideration as organ donors2,10. However, the fact that very few anencephalics could be considered as potential organ donors does not nullify their potential value in organ transplantation. Nor does the paucity of potential recipients nullify the value of anencephalics as potential donors. Numbers should not be an issue. Concerns about the accuracy of diagnosis are, however, especially relevant, because of the broad spectrum of neural tube defects and the association of anencephaly with syndromes that preclude transplant donation. To avoid the possible perils of the slippery slope, careful and conservative diagnosis of isolated anencephaly must adhere to a strict medical standard10,11,14,16. Mearns et al. also urge caution because of our meager knowledge of the dying process, although this lack of knowledge does not change the consistent observation of the deaths of anencephalics within a short time after birth.\nNew aplications of old values\nMearns et al argue further that allowing donations of vital organs from anencephalics would undermine regulations that have emerged from time-honored cultural values and priorities. These values and priorities presumably include respect for persons and an abhorrence of killing. Among physicians, of paramount value is the Hippocratic admonition to avoid doing harm. In recent times, however, we have found a need to examine our new technological powers in the light of older values as we try to reconcile the discrepancies between the two. Indeed, the UDDA was written in response to a need to define precisely what we believed we always knew about death. We are not abandoning our time-honored values as much as we are still trying to find a harmony between older ethics and newly developed powers in the medical arts. The question of using anencephalics as vital organ donors focuses on the specific ethical conflict between the utilitarian concept of using one person to benefit another and the Kantian concept of valuing each person as an end in himself, not as a means to another end, such as the well-being of someone else2. Thus far, no one has formulated a satisfactory philosophical compromise that would settle this basic ethical conflict.\nA classification based on brain function\nAnother fundamental conundrum with anencephalics is the value of the higher brain as a measure of personhood. Some authors have suggested that anencephalics be assigned the legal status of non-persons, on the theory that severe neurological impairment is incompatible with personhood7,16,17. As non-persons, these infants would then be available for organ removalømuch as one would remove porcine or primate organs, with no moral obligation to consider the implications of shortening the life of the donor. This suggestion, undoubtedly well intended, also side-steps the fact that anencephalics are living human infants, who become persons in the eyes of the law at the moment of live birth. The use of neurological impairment as a measure of personhood creates the difficult dilemma of basing a categorical social classification on the anatomic continuum of anterior neural tube defectsøa dilemma that so far has defied resolution.\nThe parents"" decision\nWhat is remarkably absent from the debate about anencephalics as organ donors is thoughtful consideration of the parents of these dying infants. Perhaps we should first acknowledge the tragedy that strikes the family when an anencephalic infant is born. How the parents cope with the situation may range from requests for aggressive life support to requests for minimal, if any, intervention. Now that some anencephalics may become suitable organ donors, some parents are faced with another option, and for some parents the prospect of donating the organs of their dying infant offers solace in a futile and bleak circumstanceøa chance, as some parents have stated, to give some import to the brief life of their infant8. In these situations, the grief of the parents is profound, and their decisions are made in an atmosphere that is charged with complex emotions. Yet our society has consistently stood behind parents as the primary decision makers for their own children3,12,15,21. With the expanding power of medical technology to manipulate death, parents are increasingly faced with the terrible decisions of letting their dying children dieødecisions that only the parents will have to live with for the rest of their lives. What may be appropriate in the arduous circumstances of the birth of an anencephalic infant would be thoughtful and careful counseling with the parents about their options in coping with an event that is, at best, immensely difficult. This counseling should acknowledge the grief of the parents but should also include careful discussion about the implications of organ donation, including the possibility that the parents could experience residual guilt long after the event16. If the parents elect to donate their dying infant""s organs, and if this decision means that the infant""s short life is shortened by hours, the parents may find some comfort and peace in knowing that their infant""s life was not utterly futile. If the parents are firm in their decision to donate their infant""s organs, then physicians will be in a position to assist in realizing some good from the hopeless circumstances that surround the anencephalic infant. We would do well to listen carefully to the parents of Theresa Ann8.\nDefinition of eligible donors\nArticles on anencephalic organ donations, in both the medical and the legal literature, indicate clearly that a consensus on these issues is unlikely. The Supreme Court of Florida speculated about whether any right or wrong would ever be found8. One study noted astonishing disparities in the definition of death from one state to the next, even though all states have adopted the UDDA4. Other studies have tentatively considered amending the Uniform Anatomical Gift Act19 to permit the parents of anencephalics to donate the vital organs of their infants before cessation of all vital functions, but with great caution about the slippery slope16. Such an amendment would require careful construction, with provisions for protecting infants with other neural tube defects, individuals in persistent vegetative states, inmates on death row, and any other persons who might be considered as potential organ donors. Ultimately, we must deal with anencephalics as human infants, not as legal concepts1. We must honor the values of the parents and the value of the infants as we try to realize some good from their short, sad lives.\n1. Annas GJ: From Canada with love: Anencephalic newborns as organ donors? Hastings Cent Rep 17:36-38 1987.\n2. Botkin JR: Anencephalic infants as organ donors. Pediatrics 82:250-259 1988.\n3. Bowen v. American Hospital Association, 476 U.S. 610 (1986).\n4. Bryne PA, Evers JC, Nilges RG: Anencephaly - organ transplantation? Iss Law Med 9:23-33 1993.\n5. Bryne PA, Nilges RG: The brain stem in brain death: A critical review. Iss Law Med 9:3-21 1993.\n6. Capron AM: Anencephalic donors: separate the dead from the dying. Hastings Cent Rep 17:5-9 1987.\n7. Fletcher J: Indicators of humanhood: a tentative profile of man. Hastings Cent Rep 2(5):1-4 1972.\n8. In re T.A.C.P., 609 So.2d 588 (Fla. 1992).\n9. Mearns EA, Lebel RR, Gold RL, et al: Anencephalics as organ donors. The Fetus 8500-19-20, 1994.\n10. Medearis DN, Holmes LB: On the use of anencephalic infants as organ donors. Ne Engl J Med 321:391-393, 1989.\n11. Medical Task Force on Anencephaly The infant with anencephaly. Ne Engl J Med 322:669-74 1990.\n12. Meyer v. Nebraska, 262 U.S. 390, 1923.\n13. Moore KL, Persaud TV: The Developing Human: Clinically Oriented Embryology, 2nd edition, Philadelphia: WB Saunders Company,\n14. Peabody JL, Emery JR, Aswal S: Experience with anencephalic infants as prospective organ donors. Ne Engl J Med 321:344-350, 1989.\n15. Pierce v. Society of Sisters, 268 U.S. 510, 1925.\n16. Shewmon DA, Capron AM, Peacock WJ, et al: The use of anencephalic infants as organ sources: A critique. JAMA 261:1773-81, 1989.\n17. Singer P: Sanctity of life or quality of life? Pediatrics 72:128-129, 1983.\n18. Truog RD, Fletcher JC: Anencephalic newborns: Can organs be transplanted before brain death? Ne Engl J Med 321:388-91, 1989.\n19. Unif. Anatomical Gift Act §1, 8A U.L.A. 5 Suppl. 1987.\n20. Unif. Determination of Death Act §1, 12 U.L.A. 340 Suppl. 1991.\n21. Wisconsin v. Yoder, 406 U.S. 205, 1972.']"	['<urn:uuid:69b20d18-6b10-426c-862c-031a8ec185e1>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T18:38:36.516973	6	53	1849
92	As a beginner farmer, I'd like to know about wheat density and protection. How many seeds should I plant per square foot, and what steps can I take to prevent diseases from spreading?	You should plant 30 to 35 seeds per square foot to achieve 22 to 25 vigorous seedlings per square foot. To prevent disease spread, you should implement several management practices: select resistant varieties, avoid planting wheat after wheat or other cereals, avoid heavy nitrogen fertilization which encourages leaf diseases, and consider fungicide application when there is severe disease pressure. Additionally, you should scout your fields by checking about 30 to 50 tillers randomly to assess disease presence.	"[""Cultivating winter wheat\nSoft red winter wheat is grown in the U.S eastern Corn Belt, along the Atlantic coast, and in the northern Mississippi Delta. Soft red winter wheat is planted from mid August to the end of October and is generally harvested from mid May to the end of June.\nWhy winter wheat?\nWinter wheat can be more profitable as it requires fewer inputs than spring wheat and it has a higher yield potential. Other advantages of winter wheat include:\n- Labor and machinery efficiencies as the crop can be planted and harvested at low peak agricultural months\n- Winter wheat establishes a ground cover and thus suppresses weeds, provides cover for wildlife, and reduces erosion from wind and water\n- The crop is well suited for no-till cropping systems and may qualify for incentive payments from programs such as the Environmental Quality Incentive Program (EQIP) for direct seeding or providing a winter cover crop\nYou should be aiming for a soil Ph range of 5.4 to 5.8. Regular testing will indicate whether you need to bring the Ph up or down; talk to your seed professional as to which varieties are best suited to your farm. Winter wheat generally grows in plant hardiness zone three through to zone eight. Planting for plant hardiness zones three to seven inclusive should be from late summer to early fall and in plant hardiness zone eight you can wait until late fall to early winter to plant your crop. You should be aiming to achieve 22 to 25 vigorous seedlings per square foot, which means planting at a density of 30 to 35 seeds per square foot. The seed rows can be set to four, six, seven, or eight inches apart and from one to one and a half inches deep; check your seed manufacturer's guide for the recommended seeded rate for differently spaced rows. Finally, timely planting is important too and delays may cost you more for seed as seeding rates for planting two weeks and one month later than optimum can mean sowing up to 10 per cent and up to 20 per cent more seed respectively.\nSoil sampling will reveal the nutrients of your soil and whether supplements are needed. For winter wheat it is most likely to be nitrogen, phosphorous, and potassium that will be required. Test your soil and talk to your Southern State Agronomy Professional for advice on a solution that suits your fields and crop.\nWeeds, pests, and disease\nWinter wheat can provide good competition against weeds. However, you may need to choose to take a helping hand through the use of insecticide and fungicide treatments. In terms of insects, aphids and Hessian flies are a particular problem for winter wheat. Diseases to be prepared for include:\n- Fusarium, a widely distributed fungus\n- Leaf rust, also known as brown rust that is especially damaging to wheat\n- Powdery mildew, a common fungal disease\n- Pythium, also known as cottony blight or grease spot, is a fungal disease affecting the roots of the plant\n- Septoria, a fungal leaf spot disease\n- Stripe rust, a fungal leaf disease\nIn essence, better preparation usually equals a better yield from your winter wheat. Pay attention to your combining equipment, a little money spent on repairs and fine tuning may save a lot of money when it comes to harvesting. Winter wheat can be either straight cut (direct combined), or, if your variety is a taller one, which does not lend itself well to straight cutting, it may be more suitable to cut and swath. Winter wheat grain at a moisture content of 12 percent to 13 percent will generally keep in storage for up to 12 months if the grain's temperature is kept below 55 degrees F with the aid of aeration; having a grain moisture tester is often a valuable extra tool to ensure your wheat is at its optimum moisture. Before harvesting, it is also a good idea to thoroughly clean both your equipment and all of your grain bins; treat the floors and walls of your grain bins with insecticide up to the point of runoff after cleaning, as insect larvae may have been present in any of the old grain left over from the last harvest."", 'Management of foliar wheat diseases, Part 1\nEditor’s note: This article is from the archives of the MSU Crop Advisory Team Alerts. Check the label of any pesticide referenced to ensure your use is included.\nThe most important leaf diseases of wheat in Michigan include powdery mildew, Stagonospora leaf and glume blotch, Septoria leaf blotch, and leaf rust. Depending on conditions, a serious level of any one of these fungal diseases can cause a significant yield loss. Part 1 of this article on wheat foliar diseases covers powdery mildew, Septoria leaf blotch, and Stagonospora leaf and glume blotch. Part 2, which will appear in the April 26 issue of the Field Crop CAT Alert, will cover leaf rust, stripe rust and fungicides for managing diseases. A fact sheet, Management of Foliar Wheat Diseases, is available at: http://fieldcrop.msu.edu/documents/wheat%20foliar%20disease-final.pdf\nLeaf diseases affect yield by reducing the amount of green leaf area available to capture sunlight and make carbohydrates used during grain filling. The flag leaf is the most important in determining grain yield and quality, but heavy infections as early as the fully-tillered stage have been shown to lead to modest yield declines.\nVarietal resistance is the first line of defense in managing foliar disease of wheat. Michigan State University’s annual variety trial results for red and white winter wheat (back to 1997) include disease resistance ratings, and can be found at the website: www.css.msu.edu/varietytrials/wheat/.\nUse the tables to view field trial ratings and determine variety performance with respect to susceptibility or resistance to a particular disease.\nCultural practices, particularly crop rotation, can serve to prevent or minimize disease development. Wheat should never follow wheat or other cereals, if at all possible. In addition, growers should avoid heavy fertilization with nitrogen as overly dense, lush stands tend to encourage leaf diseases. Planting wheat prior to the Hessian Fly Free Date may encourage the establishment of diseases such as powdery mildew and Septoria, and is an additional reason to delay planting.\nFungicide application may be profitable when there is severe disease pressure, and a susceptible variety has been planted. Fungicide research trials have shown that under low to moderate disease pressure, many wheat varieties may not show an economic return when treated with fungicides. Foliar fungicides protect yield when disease pressure is severe. The best candidates for fungicide treatment are fields with the highest yield potential. Before using a foliar fungicide, growers should determine the yield potential of the wheat crop, and the straw, if it is to be marketed, and determine whether applying a fungicide is likely to be profitable. Management practices targeted toward specific diseases are listed under the specific disease headings.\nScouting individual fields is critical to making sound management decisions. Inspect fields for disease before making a decision to apply fungicides. Check about 30 to 50 tillers randomly in the field to get a clear picture of the extent of disease present; avoid looking in just one or two locations.\nPowdery mildew (view images)\nCause: On wheat- Erysiphe graminis f.s tritici (fungus). On barley- Erysiphe graminis f.s. hordei (fungus). These fungi are host specific.\nSymptoms: Powdery, white to light gray patches may appear on leaves and stems (especially upper leaf surfaces) any time after seedlings emerge. Black specks containing spores may form in the patches of mildew as the season progresses.\nPowdery mildew overwinters as resting spores on straw, stubble, volunteer or overwintering wheat.\nConditions favoring the disease: Cool temperatures (59 to 72ºF) and high humidity (greater than 85%) are optimal for the development of the disease. Heavy nitrogen fertilization also enhances disease development.\nManagement: Select resistant varieties. Avoid heavy amounts of nitrogen, which can stimulate rapid growth. Determine the need for fungicide treatment by scouting for powdery mildew at flag leaf emergence and the boot stage. The threshold is an average two to three spots per leaf (averaged over 30 to 50 leaves) on the leaf below the flag leaf.\nSeptoria leaf blotch, Stagonospora leaf and glume blotch (view images)\nCauses: Septoria tritici and Stagonospora nodorum. (fungi).\nSymptoms: The first symptoms are tiny yellow flecks on the lower leaves. Septoria expands to angular, tan to brown lesions containing black, pinpoint specks (pycnidia, which produce spores). It does not infect the glumes. Stagonospora lesions are lens-shaped with yellow halos and may contain brown pinpoint specks (pycnidia) within the lesions. Stagonospora affects both leaves and glumes. On wheat heads, it starts as gray-brown spots on the chaff that become dark brown blotches with grayish-white centers on the glume.\nConditions favoring the disease: These fungi overwinter on straw, living plants or seed. Spores are present in late summer and fall, and can germinate over a wide temperature range. Spores are produced during periods of wet weather and can cause infections throughout the growing season.\nBarley is generally less susceptible than wheat. Wet and windy weather favors the development of the disease. Septoria is more prevalent earlier in the season (at temperatures around 50-68°F), during the period from stem elongation to flag leaf emergence.Stagonospora tends to appear around heading (temperatures are in the 68-81°F range). Cool, wet weather during flag leaf emergence provides favorable conditions for severe outbreaks of this disease. Planting small grains as successive crops allows inoculum to build up in the field, especially under no till or minimum tillage.\nManagement: Select varieties with resistance. Use certified seed. Seed treatment may help limit seedborne disease. Rotate out of small grains for two years. Avoid planting into wheat stubble. Determine the need for fungicide treatment by scouting at flag leaf emergence and the boot stage. The threshold is an average one to two lesions per leaf (averaged over 30-50 leaves) on the leaf below the flag leaf.']"	['<urn:uuid:518119bc-aee0-4edc-8cf5-2fc6d0d62052>', '<urn:uuid:3a270e40-b51c-4742-acd8-f089aea5afe0>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	33	77	1659
93	vogmask industrial chlorine facility worker protection certification standards safety requirements how effective	For workers in chlorine facilities, Vogmask provides certified protection meeting several standards. It has European CE mark with FFP1 R and FFP2 R ratings, conforms to NIOSH N99 filtering efficiency (tested by FDA certified labs), and is PPE approved for workplace use. However, Vogmask is not suitable for chemical protection, specifically stating it should not be used around chemicals like chlorine. For chlorine facility workers, specialized equipment using materials like HastelloyC and CF8M stainless steel would be required for handling chlorine, as these materials are specifically designed to resist chlorine corrosion and contamination. The mask's particle filtering capability of 99% for 0.3 micron particles would not provide adequate protection in chlorine environments.	"['What tests and certifications does Vogmask have?\nVogmask is the first reusable mask awarded the European CE mark with a certified FFP1 R and FFP2 R rating (The ""R"" in these standards stands for ""reusable""). This classification is defined in the EN 149:2001 + A1:2009 standard: ""Respiratory protective devices - Filtering half masks to protect against particles"". Vogmask is PPE (Personal protective equipment) approved for use at the workplace.\nVogmask conforms to NIOSH N99 filtering efficiency (N99 requirements are far stricter than those of N95) as stated in 42 CFR Part 84.181. N99 conformity has been tested by US FDA certified test labs Nelson Labs. Further testing has been conducted by FROST and several other leading institutions in the US. We meet all of the requirements for NIOSH and OSHA as well as US FDA. Vogmask is in the process of FDA approval for NIOSH listing.\nVogmask is currently being tested in Taiwan by TTRI (Taiwan Textile Research Institute) against the new national standard on respirators, CNS 15980.\nVogmask is certified KF-94 by the Korean Ministry of Food and Drug Safety. Vogmask is also certified KN95 in China, as set in GB2626-2006 Respiratory Protective Equipment regulation.\nThe organic cotton used in our N99 Organic C2V and N99 Organic masks, is certified according to the GOTS standards for the processing of fibres from certified organic agriculture.\nWhat is the difference between the available Vogmask models?\nAll our masks achieve the same N99 filtering efficiency, protecting the user from 99% of airborne particles such as PM 2.5, pollen, dust, bacteria, virus, mold and other contaminants as small as 0.3 microns. All Vogmasks are also reusable for months without requiring filter changes and can be washed.\nDifferences between our models affect the textile material of the external layers, the number of exhale valves and the presence of a carbon filter layer. The exhale one-way valve helps reduce moisture and remove CO2 by letting them exit freely. It is indicated for warm weather and physical activities. The carbon layer helps reducing bad odors and blocks ground level ozone.\nVogmask N99 CV\nMicrofiber inner and outer layers. Carbon filter layer and exhale valve.\nBrowse Vogmask N99 CV styles\nVogmask N99 Organic C2V\nOrganic cotton inner and outer layers. Carbon filter layer and 2 exhale valves.\nBrowse Vogmask N99 Organic C2V styes\nVogmask N99 Organic\nOrganic cotton inner and outer layers.\nBrowse Vogmask N99 Organic styles\nHow long does Vogmask last before it needs replacing?\nUnlike disposable respirators, Vogmask may be reused many times. Vogmask is certified FFP1 R and FFP2 R, the ""R"" meaning ""reusable"". The recommended life of a Vogmask varies according to the air quality of the environment. The number of recommended hours of usage range from 410 in light pollution to 80 in severe pollution. There are no expensive filters to replace. Being reusable also means that Vogmask is more environmentally friendly than disposable masks.\nAs you use your Vogmask, the pores of the filtering nanofiber textile in the middle layer of the mask become filled. Replace the mask when the breathing resistance noticeably increases.\nIn Taiwan, you can expect an average life for your Vogmask of 4 ~ 6 months of daily usage.\nYou can refer to this table based on the Air Quality Index (US EPA standard) as an approximate guide of the optimal duration of your Vogmask:\n|AQI||Air Pollution Level||Health Implications||Vogmask optimum life|\n|0 ~ 50||Good||Air quality is considered satisfactory. Air pollution poses little or no risk.||Not required|\n|51 ~ 100||Moderate||Air quality is acceptable. Moderate health concern for some sensitive people.||410 hours|\n|101 ~ 150||Unhealthy for Sensitive Groups||Sensitive groups will experience health effects.||370 hours|\n|151 ~ 200||Unhealthy||Everyone may begin to experience health effects. These will be more severe for sensitive groups.||210 hours|\n|201 ~ 300||Very Unhealthy||Health warning. Everyone is more likely to be affected.||160 hours|\n|300+||Hazardous||Health emergency. Everyone may experience serious health effects.||80 hours|\nIf any part of your mask has been damaged and it does not provide a good seal any longer, replace the mask. A good seal is essential for an effective protection.\nDoes wearing Vogmask prevent infectious diseases?\nBecause Vogmask prevents certain infectious agents from reaching your respiratory system, it can help prevent disease in some cases, but it can never guarantee it. Vogmask filters on average 99% of particles as small as 0.3 microns. This includes most bacteria and some viruses. Even so, no mask can guarantee complete protection from disease. Always exercise good hygiene to reduce risk of infection, and wash your hands after touching your Vogmask if you suspect you have been in contact with an infectious agent.\nDoes Vogmask protect me from chemicals and oils?\nNo. Vogmask is not recommended for use around chemicals or oil based particles such as paint spray or pesticides.\nContaminants cannot multiply on textile without moisture. Always let your Vogmask fully dry after use and wash. When you take off your mask, avoid placing it inside poorly ventilated places such as tight pockets. Hang it in a well-ventilated indoor area instead.\nCan I wash my Vogmask?\nYes, you can keep your Vogmask clean and fresh by washing it from time to time. It is also advisable to wash your Vogmask after activities that cause a lot of sweating, such as intense exercise. We do not recommend excesive, too frequent washing, as this can reduce the life of your Vogmask.\nTo wash your Vogmask, follow this procedure:\nRinse your mask lightly. Apply a small drop of liquid soap and gently rub. Rinse again to remove the soap and hang to dry. Avoid submerging the mask in water, as this will affect the filtering efficiency of your mask in the long term.\nWhere is the filter? Can it be replaced?\nThe particle filtering textile (and the carbon filter in those masks that have it) are sewn into the middle layers of the mask and span the whole area of the mask. In other words, the whole mask is the filter. The filter is not replaceable. When you feel the breathing resistance increases noticeably, the mask has to be replaced.\nWhat is the disk on the side(s) of the mask?\nThis is a one-way exhalation valve placed there to add comfort to the wearer by allowing moisture and CO2 produced during breathing to exit the mask freely. Air only flows outwards through the valve, never in. Vogmask is available with 1, 2 or no exhale valves. Models with exhale valve are well suited for warm weather and physical activities.\nDoes the exhale valve filter the exhalation of the wearer?\nNo. The exhale valve opens during exhalation allowing the wearer\'s exhalation to freely exit the mask. If protection from the wearer\'s exhaled particles is necessary, choose a Vogmask without an exhalation valve.\nCan the valve be taken off for washing?\nNo. The valve is permanently fixed to the mask. Taking it apart will permanently damage your mask. Don\'t worry about getting the valve wet. This is fine and will clear it of microscopic dust and particles.\nDoes washing clear the filter media of microscopic particles?\nNo. Washing only keeps the outer and inner layers clean and fresh for your comfort when wearing the mask.\nWhat sizes of mask do you have? What size is right for me?\nIt is essential you wear the right sized Vogmask to achieve both proper air filtration and comfort.\nNew Vogmasks are available in 3 sizes:\nS (10-20 kg / 22-45 lbs)\nM (21-47 kg / 46-104 lbs)\nL (48-90 kg / 105-200 lbs)\nWe also have a limited edition in XL size (90+ kg / 200+ lbs).\nSee *XL size* Vegan Leather N99 CV.\nAs each person\'s face is different, use the above weight recommendations only as a guidance. They are based on the average Taiwanese person. If in doubt, measure your face following the instructions in this image:\nPlease contact us if you have doubts about the right size for you, or if the mask you ordered does not fit properly.\nMy Vogmask fits tightly around my ears. Will it stretch?\nWhat should I do if the ear loops tug on my ears and bend my ears forward?\nDon\'t worry if you feel your brand new Vogmask a bit tight on your ears at first. The ear loops will stretch after a few uses. Most people find their Vogmask becomes more comfortable after being worn a few times. You can also try giving the ear loops a gentle stretch with your hands to maximum length 2 or 3 times.\nIf none of that worked, you might be using the wrong sized mask. Contact us for replacement with the proper size.\nIf you are using the right sized Vogmask and the ear loops still make you feel uncomfortable, try the optional head strap.\nWhat if the mask does not extend beneath my chin?\nOnce you position the mask with the aluminum noseband adapted to the shape of your nose, a properly fitting mask should reach below your chin. If it does not comfortably reach below your chin, try positioning the noseband lower on your nose bridge. If that does not work, the mask is too small to get a good seal. Please contact us for replacement with a bigger size.\nWhat should I do if the mask fogs my glasses?\nThis is indicative of a poor seal. Follow fitting instructions carefully. Carefully form the aluminum noseband on the gaps between the nose bridge and the cheeks. Your exhale should exit the mask through the exhalation valve and not out the gaps between the cheeks and nose bridge.\nIf you still have any questions about Vogmask, please contacts us.', 'Dry chlorine gas is found within chlorine production, storage and transfer facilities, as well as in downstream processes such as production of vinyl chloride monomer (used in making polymer polyvinyl chloride) or phosgene (used in applications such as pharmaceuticals and organic compounds). Dry chlorine is generally understood to have less than 150 parts per million (PPM) water containment.1 Moisture contamination in any of these dry chlorine service areas, however, can result in the formation of dendrites, crystal-like masses that can cause problems.\nFor example, research shows that dendrite formation on critical sealing surfaces, such as valve seats and packings exposed to extremely harsh operating conditions, leads to a significant increase in potential leak paths because of erosion of seal surfaces that come into contact with the dendrites. Dendrites can also form on contacting or sliding surfaces, causing valves to freeze or to become abrasive to the soft seat, which can lead to severe seat damage. In fact, any amount of moisture can lead to dendrite formation and rapid corrosion of moving valve parts.\nRecent collaboration between valve manufacturers and end users has led to a clearer understanding of the factors that cause valve failure under these circumstances and factors that can increase service life of those valves. As part of that collaboration, researchers investigated construction materials that could combat moisture contamination and provide enhanced performance in harsh operating conditions, looking at each component of the valve for dendrite susceptibility. Suitable materials were identified, leading to designs that incorporate the appropriate combination of super alloys and stainless steel to allow for a cost-effective solution to these issues.\nThis article discusses the relationship between dendrite formation and valve life, and provides an overview of shortcomings that are now being overcome with these new designs and materials. Coincidentally, the improvements in design that resulted, combined with best practices for specifying features, has contributed to lower total life-cycle costs in the critical applications in dry chlorine services.\nTHE ISSUES INVOLVED\nTo understand the newer solutions and how they were derived, it is important to look at critical valve performance and maintenance issues that end users encounter in applications involving dry chlorine. Understanding the factors that can contribute to valve failure enables valve manufacturers to make corresponding design improvements, and the end user can use this knowledge to extend valve service life.\nFor these dry chlorine applications, end users articulated that real world applications were less than ideal—that these applications posed challenges that were too often severe and unforeseen, and that the end users could save money if they had access to valves that would tolerate the varied and harsh conditions involved. The consensus vision for an optimal design was that it would take into account demanding material requirements, ease of cleaning and packaging, and that external seals would limit emissions under severe conditions. Double packing and a monitoring port also were considered preferable.\nBy working through both typical and more unusual critical pain points, it became evident that dendrites often formed on the surface of Monel discs, causing damage to the soft seats. When such crystals form on frictional surfaces, they cause binding and seizing of mating parts. An example is simple components such as disc spacers that keep the disc centered. The shaft turns inside the bearings and packing so any crystal formation quickly deteriorates these critical components. When these crystals form on hard surfaces that make contact with soft seals, they erode the seals (working like sandpaper).\nFurthermore, while Monel performs well in dry chlorine service (when temperatures stay high enough to preserve chlorine’s “dry” form), any amount of moisture can lead to rapid corrosion of moving parts. For these two reasons, engineers settled on HastelloyC as the preferred material for disc and stem construction.\nFor the valve body, CF8M stainless steel has been identified as the new standard, but all other valve components (pin, gland, bolts, etc.) were also studied individually to determine the optimal choice of material. Using Hastelloy for key parts enables the valve to tolerate moisture. Using CF8M stainless steel for the body, meanwhile, also makes the exterior visibly cleaner and provides visual confirmation that the device is robust.2\nPamphlet 6 of the Chlorine Institute (page 5 of the May 2005 edition), recommends the use of carbon steel for piping for handling dry chlorine. Monel bodies and internals are recommended for dry chlorine, but on page 20, it suggests that consideration be given to materials suited for both wet and dry applications if there is a chance that moisture contamination could be present. For wet chlorine, HastelloyC internals are specified by The Chlorine Institute (note that dry chlorine can become “wet” through moisture contamination, or by sufficiently reduced temperatures).\nResearch indicates that even in dry chlorine service, systems are prone to moisture contamination during installation or whenever connections are made. Such contamination reacts with the chlorine and forms hard, microscopic metallic chloride crystals on metal surfaces. This occurs even on Monel surfaces, which are considered acceptable with perfectly dry chlorine.\nTo combat corrosion, the use of HastelloyC is the better option for dynamic surfaces that form between metal and soft parts. This is a very hard, resilient material that is more costly and more expensive to machine. However, it is indispensable for longer product life and reduced total lifecycle costs—reduced in-service operating costs (replacement, maintenance, spill/leak cleanup, emission/environmental hazards, etc.) more than compensate for higher up-front costs.\nWCB valve bodies are normally recommended for dry chlorine since stainless steel is susceptible to chloride stress cracking. CF8M bodies have demonstrated exceptional endurance in corrosive environments, while being structurally resilient against stress-cracking.\nEnd users also showed interest in valves featuring live-loading, double-packing with monitoring ports and Inconel spring (Belleville washers). Ideally, the packing gland is made from HastelloyC to prevent freezing in the packing bore, which would disable live loading. The packing gland is a simple part on the outside of a valve that might not appear imperiled by possible contamination. Experience indicates, however, that it can be exposed to considerable moisture and minute traces of chlorine vapor emanating or permeating from the system, often causing the gland to seize. Once this occurs, it no longer serves its function of pressurizing the packing, and leakage may result.\nBecause of these risks, Alloy 20 bolting can be used for packing gland studs and nuts to minimize corrosion on these stressed parts. At the same time, this mitigates stress cracking concerns that arise when stainless steel is used.\nA lantern ring between the series of packing allows leakage from the flow path to accumulate in an area adjacent to the second packing. A small sampling valve can be threaded to a monitoring port channeled to this area so it can be periodically checked for early signs of weakening. The lantern ring should be made from HastelloyC to ensure long term operation.\nFor seating, the use of Teflon provides a compliant, inert seal that provides bubble-tight shutoff. An axially pliant seat allows the use of reinforced PTFE seats to maximize resistance to abrasion while maintaining the chemical compatibility of the Teflon. This axially pliant seat flexes but is not subject to hoop stretch. Since Teflon does not have “memory,” it is important to avoid stretching the material.\nChlorine systems often encounter rust or scale so valve seats must be impurity-tolerant. Molecularly enhanced PTFE (TFM) has the disadvantage of performing poorly at low temperatures whereas the R-PTFE ensures valves perform well during temperature cycling and in cold environments.\nSystems often require bidirectional sealing as well as the ability to be used in end-of-line service with pressure contained behind a closed valve without the concern of which side will remain exposed to the medium being processed (which is accompanied by pressure from the piping system).\nBecause minimizing the potential for leak paths is essential in controlling fugitive emissions, using a closed bottom design is a good choice for inhibiting leakage that might occur if the shaft bore protruded through the bottom of the valve. This is the acknowledged best design for valves 12 inches and under. For larger valves, where it is not practical to bore the bottom shaft hole from the top of the valve, a bolted plate with a static PTFE seal is preferred over a threaded plug because a threaded plug tends to grab a seal and twist it during assembly.\nWith the foregoing realities in mind, valve manufacturers are either developing solutions or have already succeeded. The culmination of those efforts is a valve made with materials designed to withstand and be impervious to the effects of dendrite formation. Such is the case with some high-performance butterfly valves, which have incorporated the advances mentioned here in efforts to meet the needs of dry chlorine service. Such valves represent a breakthrough in design, as they specifically address the requirement of preventing valve damage from both moisture contamination and abrasion or erosion.\n1 The qualifying reading may not always be 150 PPM because chlorine at this and lower readings can become “wet” in sufficiently cool or cold conditions.\n2 For reference about recommended materials for valve components, please see the Valve Materials Selection Guide provided by The Chlorine Institute: www.cl2.com/index.php/.../139-pamphlet-6-edition-15-may-2005 table 4.8).\n- The Chlorine Institute, Inc. Pamphlet 6 – Piping Systems for Dry Chlorine (Edition 15)\n- Xomox Manufacturing Control Manual – Level III Cleanliness Requirements: Chlorine, Food, Std. Oxygen, and Vacuum Service Valves']"	['<urn:uuid:598a47ae-c1a5-4c5d-b0fb-359ab38289d9>', '<urn:uuid:3239124a-92eb-421d-bd65-dcc5664a7002>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-12T18:38:36.516973	12	112	3165
94	Which helps more with water management: charcoal-based soil or stone-based soil?	Biochar (charcoal-based soil amendment) and CU-structural soil (stone-based system) both aid in water management differently. Biochar can store five times more water than its own weight due to its microscopic holes and large surface area, while also helping soil retain water and release it during dry periods. CU-structural soil manages water through interconnected voids between stones that allow water movement and drainage, with clay soil components providing water-holding capacity. The system requires positive drainage below the root system since the sub-grade may be compacted and impermeable, typically using a perforated drain connected to storm drainage. While both systems help with water management, they serve different purposes - biochar focuses on water retention and slow release, while CU-structural soil prioritizes proper drainage in urban paved environments.	['The Case for CU-Structural Soil™: Why do we need it, what is it, and how is it used? Dr Nina Bassuk of the School of Integrative Plant Science explains\nUrban trees experience a litany of environmental insults: soil and air pollution, heat loads, deicing salts, and impacts from utilities, vehicles, and buildings. The most significant problem that urban trees face, however, is the lack of usable soil volume for root growth, since trees are often an afterthought in city planning and streetscape design. (Fig. 1.1)\nOngoing construction, including sidewalk and road repair, disturbs and compacts soil (Fig. 1.2), crushing macropores (Fig. 1.3). Loss of macropores has three negative consequences: restricted aeration, diminished water drainage, and creating a dense soil that is difficult for roots to penetrate. These effects limit usable rooting space.\nWhat happens when roots encounter dense, compacted soil?\nWhen roots encounter dense soil, they change direction, stop growing, (Fig 1.5) or adapt by remaining abnormally close to the surface (Fig. 1.4) This superficial rooting makes urban trees more vulnerable to drought and can cause pavement heaving. However, if a dense soil is waterlogged, tree roots can also rot from lack of oxygen.\nThe role of soil volume on tree growth\nThe soil in urban tree lawns or parks can be improved by amendment or soil replacement. Where soil volume is limited by pavement, tree roots suffer (Fig 1.6). The highly compacted soils required for constructing pavements do not allow root penetration, resulting in declining trees which are all too common in cities. Yet it is precisely these paved areas such as parking lots and streets that most need the mitigating effects of shade trees.\nHealthy trees need a large volume of non-compacted soil with adequate drainage and aeration and reasonable fertility. CU-Structural Soil™ meets these needs while also fulfilling engineers’ load-bearing requirements for base courses under pavement.\nCU-Structural Soil™ Basics\nCU-Structural Soil™ (U.S. Patent # 5,849,069) is a two-part system comprised of a rigid stone “lattice” to meet engineering requirements for a load-bearing soil, and a quantity of soil, to meet tree requirements for root growth. The lattice of load-bearing stones provides stability as well as interconnected voids for root penetration, air and water movement (Fig. 1.7). The uniformly graded 3/4”-1 1/2” angular crushed stone specified for CU-Structural Soil™ is designed to ensure the greatest porosity. Crushed or angular stone provides more compaction and structural interface of stone-to-stone than round stone. Because stone is the load-bearing component of structural soil, the aggregates used should meet regional or state department of transportation standards for pavement base courses.\nSince among soil textures, clay has the most water and nutrient-holding capacity, a heavy clay loam or loam, with a minimum of 20% clay, is selected for the CU-Structural Soil™ system. CU-Structural Soil™ should also have organic matter content ranging from 2%-5% to ensure nutrient and water holding while encouraging beneficial microbial activity. A minimum of 20% clay is also essential for an adequate cation exchange capacity.\nWith carefully chosen uniformly-graded stone and the proper stone to soil ratio, a medium for healthy root growth is created that also can be compacted to meet engineers’ load-bearing specifications (Fig. 1.8). The intention is to “suspend” the clay soil between the stones without over-filling the voids, which would compromise aeration and bearing capacity. CU-Structural Soil™ utilizes Gelscape® hydrogel as a non-toxic, non-phytotoxic tackifier, in addition to stone and soil components.\nUsing CU-Structural Soil™ for Street Trees\nCU-Structural Soil™ is intended for paved sites to provide adequate soil volumes for tree roots under pavements (Fig. 1.9). It can and should be used under pedestrian mall paving, sidewalks, parking lots, and low-use access roads. The Urban Horticulture Institute is currently conducting trials of its use under turf and porous asphalt to provide more porous parking areas. Research at Cornell has shown that tree roots in CU-Structural Soil™ profiles grow deep into the base course material, away from the fluctuating temperatures at the pavement surface. One benefit of this is that roots are less likely to heave and crack pavement than with conventional paving systems (Fig. 1.10).\nPlanting a tree into CU-Structural Soil™ is much like conventional planting. If possible, the pavement opening should be expandable (via removable pavers or using a mulched area) for the sake of the anticipated buttress roots of maturing trees (Fig. 1.11). CU-Structural Soil™ should be used at a depth of at least 24” but preferably 36” (Fig.1.12). CU-Structural Soil™ can be used right up to the surface grade where there is a pavement opening that is large enough to allow for tree installation.\nTrees in Parking Lots and Plazas\nCU-Structural Soil™ may also be used to enlarge a ‘tree island’ within a parking lot. With a large tree planting area, good, well draining topsoil can be used in the island and CU-Structural Soil™ added as an unseen rooting medium under the asphalt (Figs. 1.13 – 1.15).\nTrees in parking lots, as well as paved plazas, benefit from the use of CU-Structural Soil™ (Fig.1.16 – 1.17). Whether there is a curb or not, good, well-drained topsoil may be used around the tree where the opening is at least 5’ x 5’. If the opening is smaller, CU-Structural Soil™ may be used right up to the tree ball. Although it is not necessary to use\nan additional base course on top of CU-Structural Soil™, some engineers may want to do this, immediately under the pavement.\nGiven the large volume of CU-Structural Soil™ for tree roots to explore, irrigation may not be necessary after tree establishment—the decision depends on the region of the country and on site management. While there is less moisture in CU-Structural Soil™ on a per-volume basis than in conventional soil, the root system in structural soil has more room for expansion, allowing for increased water absorption. Supplemental water should be provided during the first growing season as would be expected for any newly planted tree. In regions where irrigation is necessary to grow trees, low-volume, under-pavement irrigation systems have been used successfully. Fertilizer can be dissolved into the irrigation water if necessary, although to date, nutrient deficiencies have not been noted, probably due to the large volume of rooting media.\nPositive drainage below the root system is\nnecessary in this system, since the sub-grade\nbelow the CU-Structural Soil™ may be\ncompacted and impermeable. A perforated\nand wrapped drain, connected to storm\ndrainage, should be placed between the CUStructural Soil™ and the compacted subgrade (Fig.1.18).\nWhere the curb footer goes to greater depth for a planter, a 6”- 8” PVC sleeve filled with uncompacted soil should be used to give tree roots access to the CU-Structural Soil™ beyond the planter wall (Figs 1.18-1.19).\nCreating break-out zones for trees in narrow tree lawns\nWhere there is an adjacent green space, whether a park or front lawn, CU-Structural Soil™ may be used as a channel for roots to safely grow under pavement into this green space (Figs. 1.20 – 1.23). Generally two 5’ concrete flags are removed, then the area is excavated to 24”- 36” and CU-Structural Soil™ is backfilled into them. Paving slabs are then replaced in a conventional manner.\nCU-Structural Soil ™ use with permeable pavers\nIf non-mortared pavers are used, a setting bed of uniformly-graded coarse sand should be used, to a depth specified by paver manufacturer specifications. To discourage rooting in this layer, a geo-textile—one that does not restrict water movement—can be used between this material and the CU-Structural Soil™ (Figs. 1.24 – 1.25).', 'Biochar – A Carrier of Hope and Nutrients\nWhat is Biochar and how is it different from charcoal?\nYou may know charcoal from firing BBQs. Biochar, however, is a special type of charcoal produced by a thermal process called pyrolysis (from the Greek, ‘pyro’, meaning fire and ‘lysis’, meaning separation). During pyrolysis organic matter such as wood waste, organic kitchen waste, rice husks, grass cuttings etc. is thermos-chemically disintegrated in an oxygen-free environment at high temperatures of between 400 °C and 900 °C. Biochar starts out as organic material and becomes more mineral-like with the heating (Wilson K 2014). The carbon sequestration achieved in the process is 489 kg CO2 per 1,000 kg of organic material (Gerber 2009), which means almost half of plant wastes’ total carbon will be permanently stored in the biochar for more than 1,000 years (Schmidt HP 2011). When incorporated into the soil CO2 is actively taken out of our atmosphere, creating a so-called ‘carbon sink’ which is able to slow down climate change.\nHans-Peter Schmidt, a biochar expert, said in an article published in the Ithaka journal (1/2012) (http://www.ithaka-journal.net):\n“The current imbalance in the world’s carbon and nitrogen cycle is not just the main cause of climate change, but also a direct threat to ecosystems through eutrophication, desertification and a decline in biodiversity. Re-balancing through regularly recycling organic material with its carbon, nitrogen and phosphor content is needed. Biochar has the potential to play a key role, as it not only converts the carbon found in a wide range of biomasses into a stable form, but also binds volatile nutrients from biomass residues, thereby recycling them for agricultural use. Though still “early days” for biochar, the prospects for its use are good, whether in crop or livestock farming, or in industry.”\nSoil and Biochar\nHealthy soil can be pictured as a living being that consists of innumerable small organisms, inorganic minerals, water, roots of plants and organic matter. Almost 90 % of all organisms on our planet are living in the soil (Schmidt HP 2010). Once natural vegetation has been removed microorganisms disappear, the soil slowly loses its natural fertility and gets depleted in nutrients. In order to still be able to grow crops, non-organic farmers add tons and tons of inorganic fertilizer, not knowing that the inorganic fertilizer kills the soil life. In order to compensate for that loss even more inorganic fertilizer is added, at high cost to the farmer and the planet – most of these fertilizers are oil- and gas-based. This is a deadly spiral, which ends in contaminated soil and groundwater often resulting in ‘badlands’ – biodiversity deserts. There is an urgent need to maintain and build healthy, living soils which keep fertility.\nSince ancient times it is known that poor soils can be significantly improved by adding biochar. In South America biochar amended soil is known as “terra preta” black soil. Scientific experiments in the Brazilian Amazon have shown that a thousand years after application of the biochar crops still grow better on biochar amended soil than on freshly cleared rainforest soil.\nBefore applying biochar to the soil it has to be “activated”, meaning it has to be loaded with nutrients and microorganisms, which can e.g. be achieved by mixing it with compost or organic fertilizer. The pyrolysis process creates thousands of microscopic holes in each piece of Biochar. One gram of Biochar could, theoretically, unfold to be the size of a soccer pitch. This extremely high surface area means that lots of microorganisms can colonise the Biochar and later colonise depleted soil; the Biochar also helps soil to retain water. Once activated/loaded Biochar is mixed into depleted top soil, or simply put on the soil surface as a top dressing ideally covered with mulch. Over time the soil will become more alive, restoring itself and getting back into its healthy natural cycle.\nSEM images of the popular bio-char (y sectional surface) after the partial gasification\nHow does Biochar modify soil?\nBiochar serves as a carrier for nutrients, water and habitat for microorganisms, all crucial for a healthy soil and healthy plants. Due to its big sponge-like surface (300 m2 per 1 g) biochar is able to store a five times higher amount of nutrients and water than its own weight (Schmidt HP 2014). All the stored nutrients are easily available for plants.\nKey facts about biochar (according to Schmidt HP 2010, 2011, 2012):\n• Biochar stores nutrients like nitrogen and prevents them from being washed away\n• Stored nutrients are easily available for plants and microorganisms. Through the stimulation of microbial symbiosis, the plant takes up the nutrients from the porous carbon structure.\n• Biochar provides habitat for microorganisms, which are crucial in processing nutrients and building new fertile soil\n• Plant growth and plant health is improved\n• Increase of myccorhiza, so plants can access nutrients more easily\n• Biochar improves water retention and stabilizes soils\n• Biochar helps to prevent erosion and stagnant moisture and releases water through dry periods\n• Biochar improves aeration and reduces emission of climate-wrecking gases like methane or nitrous oxide\n• Biochar is capable of binding toxic substances (heavy metal, pesticides, etc) which is not only important for healthy plants but also for clean water and ground water protection\n• Biochar is raising the soil’s pH-value, again making nutrients more accessible to plants\n• Biochar reduces waste problems by recycling organic materials such as arboricultural waste, old bamboo scaffolding, old palettes, rice husks, coconut fibres, organic kitchen waste and so on – keeping them out of the landfill where they decay and release carbon into the atmosphere.\nHow is Biochar used in KFBG\nBiochar is used in KFBG to improve poor soils for organic farming, gardens and forest restoration. It helps us to reduce and recycle organic wastes.\nWe have had a small Biochar machine for several years. In July 2015 we have installed a new state-of-the-art machine, custom built in Australia, which has almost no emissions and, as it is the size of a shipping container, can handle a large volume of wood everyday if needed. As we change our abandoned, unproductive mono-crop orchards in the middle areas of KFBG, over the next two decades, we will convert all the cut wood to biochar and put this back onto the old orchard terraces, with mulch to enrich the depleted soil, ready for planting a wide range of native tree seedlings to create a healthy native forest and seed nursery.\nFurther information and links:\nSchmidt HP: Terra Preta – model of a cultural technique\nSchmidt HP: Climate Farming – A Master Plan for Sustainable Agriculture\nSchmidt HP: Biochar – a key technology for the planet\nWilson K: How biochar works in soil\nGerber H (2009): CO2-Bilanz des Pyregreaktors, Ithaka-Journal 2009, www.ithaka-journal.net/60, ISSN 1663-0521\nSchmidt HP (2010): Climate Farming – A Master Plan for Sustainable Agriculture, 1/2010, S.314–317, www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz; www.delinat-institut.org, www.ithaka-journal.net. ISN 1663-0521\nSchmidt HP (2011): Pflanzenkohle, Ithaka Journal 1/2011: 75–82 (2011); www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz; www.delinat-institut.org, www.ithaka-journal.net. ISSN 1663-0521\nSchmidt HP (2011): Pflanzenkohle – Landwirtschaft als Klimaretter – ein Jahresbericht. Ithaka Journal 1/2011: 9–13 (2011), www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz; www.delinat-institut.org, www.ithaka-journal.net\nSchmidt HP (2012): Pflanzenkohle, eine Schlüsseltechnologie zur Schließung der Stoffkreisläufe, Ithaka Journal 1/2012: 75–79 (2012); www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz, www.delinat-institut.org, www.ithaka-journal.net. ISSN 1663-0521\nSchmidt HP (2014): Terra Preta – model of a cultural technique, the Biochar Journal 2014, Arbaz, Switzerland.\nISSN 2297-1114; www.biochar-journal.org/en/ct/4\nWilson K (2014): How biochar works in soil, the Biochar Journal 2014, Arbaz, Switzerland. ISSN 2297-1114']	['<urn:uuid:395ee543-523b-44e1-98bb-a47b91410c54>', '<urn:uuid:911ac082-e677-4038-978f-60d9b8244cab>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T18:38:36.516973	11	125	2517
95	compare combat effectiveness success rate tt250 rocket igla missile	The combat effectiveness of the TT-250 rocket and Igla missile shows significant differences. The TT-250, while extremely powerful, had limited combat success due to its poor accuracy and was ultimately cancelled after testing from 1931-1935 due to low hit probability. The Igla, on the other hand, has demonstrated considerable combat effectiveness with documented successful engagements in multiple conflicts - including downing aircraft during the Gulf War, Iraq War, Bosnia and various other conflicts. According to manufacturer data, the Igla has a probability of kill between 0.30-0.48 against unprotected targets, reducing to 0.24 when countermeasures are employed. The TT-250's combat effectiveness was severely limited by having only two rockets and very difficult aiming, while the Igla's sophisticated guidance system made it a much more practical weapon.	"['The TT-250 is a large caliber high explosive rocket that is used on the RBT-5 event vehicle. The TT-250 is one of the most dangerous weapons in War Thunder with the ability to cause major damage or destroy almost any armoured vehicle that is hit by it. It is limited by only having two of them present on a BT-5 hull, as well as having very awkward aiming and flight characteristics. Although they are a rare sight, TT-250 can bring great rewards to those who are able to use it wisely and a swift end to those on the receiving end.\nThe rocket, or ""tank torpedo,"" was developed in the early 1930s as a potential armament for armoured vehicles. It was selected to be placed on a BT-5 and the arrangement of having two placed into launchers on the turret was selected, creating the RBT-5. Tests were conducted from around 1933 to 1935 but the project was ultimately cancelled to the limited use of the vehicle. This would not be the last time that the Soviet Union attempted to create a ""rocket tank,"" although it was certainly one of the earliest pursuits.\nVehicles equipped with this weapon\nTell us about the tactical and technical characteristics of the bomb.\nThe TT-250 has the capability to destroy any tank in the game, even those that are large and very heavily armoured. The direct high explosive damage dealt on impact is enough to instantly destroy a Maus, so any other tank stands little chance of survival. There is a degree of splash damage, although it is generally only effective against small and lightly armoured vehicles. Any direct hit to a tank is very likely to destroy whatever it hits.\nComparison with analogues\nThe TT-250 is largely unlike any other rocket in the game, and thus effectively has no direct analogues. However, there are other rockets that are used in a similar manner, only not to the same degree of effectiveness.\n- M13 Rocket - The M13, used on the BM-13N, has similar constraints as the TT-250 such as a minimum elevation that is a positive angle. The BM-13N does have 16 rockets although they only have around 5 kg of explosive mass, which can still devastate lightly armoured vehicles, but will struggle to defeat large tanks and may need multiple rockets to disable or destroy them.\n- S-24 Rocket - The S-24, found on several Soviet fixed and rotor-wing aircraft, has a similar weight (235 kg) to the TT-250. However, it has far less explosive mass (25.5 kg) which requires a direct hit on a vehicle to inflict damage.\n- Tiny Tim Rocket - The Tiny Tim, used on American fighters and attackers, is an example of a large diameter rocket (298 mm) intended for demolition / anti-shipping use. While the explosive power of the Tiny Tim is only about half (63.5 kg) of the TT-250, it has a semi-armour piercing warhead which does allow for significant damage or total destruction against a target if directly hit.\nUsage in battles\nDescribe situations when you would utilize this bomb in game (vehicle, pillbox, base, etc)\nPros and cons\n- Incredible destructive power which can destroy nearly any vehicle with a direct hit\n- Splash damage capable of inflicting severe damage on lightly armoured vehicles with a near miss\n- Minimum elevation of launcher prohibits direct fire, giving a minimum range of nearly 0.25 km\n- No guidance system and only two rockets available, meaning that aiming is both very difficult and missing is extremely punishing\nDuring the development of the BT-5 high speed tank, there were various other projects aimed at modifying and improving the BT-5 in different ways. Some were implementing underwater and swimming devices to the tank, switching out the engine for a diesel with 300 HP, and there was an idea to create a ""rocket tank."" Carried out under the direction of the Office of Motorization and Mechanization of Red Army (УММ РККА), these tests were often just single designs testing out a new concept and observing the successes and failures of each idea. Work on the BT-5 experiments began in October 1933 and an engineer was tasked with fitting a 250 kg ""tank torpedo"" (TT) to the BT-5. The main idea was to be an effective weapon against enemy heavy tanks and breaking through heavily fortified areas.\nThe TT-250 was first tested from 1931 to 1932, where short range tests between 300 and 1800 meters were conducted at a scientific research test site in Leningrad and at a training ground near Kuzminki. Following the testing, it was decided to mount them onto a tank. By the end of 1933, the idea of having two of these mounted as an additional armament to the BT-5 was realized and the new name of RBT-5 was given to this design. The rockets were placed into launching bins on either of the turret, with vertical guidance being performed by a geared mechanism from inside of the turret and horizontal guidance being provided by the rotation of the turret. Maximum elevation of the launcher was 48° which allowed targets to be engaged up to 1500 meters away with an electric firing mechanism. The rocket was 420 mm in diameter and 1805 mm in length and consisted of a 130 kg warhead with a secondary charge of 13.5 kg weight in the tail. Fuses from aerial bombs were used to detonate the rocket on impact or at a time delay, and the head of the rocket had an armoured cap to protect it.\nThe project was ultimately cancelled due to the limited range and low chance to hit a target, along with the added vulnerability of the RBT-5. Work was performed on 100 kg and 300 kg variants although these were also eventually cancelled. Out of the failure of the TT-250 and the RBT-5 came the idea of mounting RS-132 rockets to the BT-5, but this too was only an experiment.\nAn excellent addition to the article would be a video guide, as well as screenshots from the game and photos.\nLinks to the articles on the War Thunder Wiki that you think will be useful for the reader, for example:\n- reference to the article about the variant of the weapon;\n- references to approximate analogues by other nations and research trees.\n- Pavlov, M, Zheltov I., and I. Pavlov. BT Tank. Moscow: Eksprint, 2001. ISBN: 5-94038-019-0\n- M. Pavlov, I. Zheltov and I. Pavlov, BT Tank (Moscow: Eksprint, 2001), p. 63.\n- Ibid., p. 64.\n|70 mm||FFAR Mighty Mouse · Hydra-70 M247|\n|127 mm||HVAR · Zuni Mk32 Mod 0 ATAP|\n|298 mm||Tiny Tim|\n|150 mm||Wgr.41 Spr|\n|55 mm||S-5K · S-5KP · S-5M|\n|80 mm||S-8KO · S-8M|\n|82 mm||M-8 · RS-82 · RBS-82|\n|132 mm||M13 · RS-132 · RBS-132|\n|212 mm||S-1of · S-21|\n|240 mm||S-24 · S-24B|\n|70 mm||CRV7 M247|\n|87 mm||AP Mk I · AP Mk II|\n|183 mm||Triplex R.P.|\n|292 mm||Uncle Tom · Red Angel|\n|100 mm||Type 5 No.1 Mod.9|\n|120 mm||Type 3 No.1 Mod.28 Mk.1|\n|130 mm||Type 75|\n|210 mm||Type 3 No.6 Mod.27 Mk.1 · Type 5 No.6 Mod.9|\n|90 mm||Type 90-1|\n|130 mm||Type 130-2|\n|68 mm||SNEB type 23 · TDA|\n|100 mm||TBA ECC · TBA Multi-Dart 100 AB|\n|120 mm||T10 140 · T10 151|\n|75 mm||srak m/55 Frida · srak m/57B|\n|81 mm||Oerlikon Typ 3Z 8Dla|\n|145 mm||psrak m/49A|\n|150 mm||srak m/51|\n|180 mm||hprak m/49|', 'Igla missile and launch tube.\n|Type||Man-portable air-defense systems (MANPADS)|\n|Place of origin||Soviet Union|\n|Used by||See Operators|\n|Manufacturer||KB Mashinostroyeniya – developer of the system|\n|Weight||10.8 kg (24 lb)|\n|Length||1.574 m (5.16 ft)|\n|Warhead||1.17 kg (2.6 lb) with 390 g (14 oz) explosive|\n|contact and grazing fuzes|\n|Engine||solid fuel rocket motor|\n|5.2 km (3.2 mi)|\n|Flight ceiling||3.5 km (11,000 ft)|\n|Speed||570m/s (peak), about Mach 1.9|\n|dual waveband infra-red (S-version)|\nThe 9K38 Igla (Russian: Игла́, ""needle"") is a Russian/Soviet man-portable infrared homing surface-to-air missile (SAM). ""9K38"" is the Russian GRAU designation of the system. Its US DoD designation is SA-18 and its NATO reporting name is Grouse; a simplified, earlier version is known as the 9K310 Igla-1, or SA-16 Gimlet. The latest variant is the 9K338 Igla-S, with NATO reporting name SA-24 Grinch. It has been fielded by the Russian Army since 2004.\nThere is also a two-barrel 9K38 missile launcher called Djigit.\nThe development of the Igla short-range man-portable air defense system (MANPADS) began in the Kolomna OKB in 1972. Contrary to what is commonly reported, the Igla is not an improved version of the earlier Strela family (Strela-2 and Strela-3), but an all new project. The main goals were to create a missile with better resistance to countermeasures and wider engagement envelope than the earlier Strela series MANPADS systems.\nTechnical difficulties in the development quickly made it obvious that the development would take far longer than anticipated, however, and in 1978 the program split in two: while the development of the full-capability Igla would continue, a simplified version (Igla-1) with a simpler IR seeker based on that of the earlier Strela-3 would be developed to enter service earlier than the full-capability version could be finished.\nThe 9K310 Igla-1 system and its 9M313 missile were accepted into service in the Soviet army on 11 March 1981. The main differences from the Strela-3 included an optional Identification Friend or Foe system to prevent firing on friendly aircraft, an automatic lead and super elevation to simplify shooting and reduce minimum firing range, a slightly larger rocket, reduced drag and better guidance system extend maximum range and improve performance against fast and maneuverable targets, an improved lethality on target achieved by a combination of delayed impact fuzing, terminal maneuver to hit the fuselage rather than jet nozzle, an additional charge to set off the remaining rocket fuel (if any) on impact, an improved resistance to infrared countermeasures (both decoy flares and ALQ-144 series jamming emitters), and slightly improved seeker sensitivity.\nThe seeker has two detectors – a cooled MWIR InSb detector for detection of the target and uncooled PbS SWIR detector for detection of IR decoys (flares). The built-in logic determines whether the detected object is a target or a decoy. The latest version (Igla-S) is reported to have additional detectors around the main seeker to provide further resistance against pulsed IRCM devices commonly used on helicopters.\nThe 9M313 missile features an aerospike mounted on a tripod (Igla\'s 9M39 missile has aerospike attached directly to the seeker dome), which reduces a shock wave, thus providing less dome heating and greater range. The name Igla is derived from these devices.\nLike many other MANPADS, Igla-1 and Igla feature so-called rolling airframe missiles. These missiles roll in flight (900 – 1200 rpm) so steering the missile requires just a single pair of control surfaces, unlike roll-stabilized missiles, which require separate control surfaces for pitch and yaw. Both 9M313 and 9M39 missiles contain a gas generator, which drives a small gas turbine to provide electrical power, and the pistons, which move the canards used to steer the missile in a bang-bang mode. In addition to that, two exhaust tubes of the gas generator are placed perpendicular to the steering canards to provide maneuverability immediately after launch when the missile airspeed is too low for canards to be effective. Later versions of Igla are reported to use proportional control to drive the canards, which enables greater precision and less oscillation of the flightpath.\nAccording to the manufacturer, South African tests have shown the Igla\'s superiority over the contemporary (1982 service entry) but smaller and lighter American FIM-92A Stinger missile. However, other tests in Croatia did not support any clear superiority, but effectively equal seeker performance and only marginally shorter time of flight and longer range for the Igla.\nAccording to Kolomna OKB, the Igla-1 has a Pk (probability of kill) of 0.30 to 0.48 against unprotected targets which is reduced to 0.24 in the presence of decoy flares and jamming. In another report, the manufacturer claimed a Pk of 0.59 against an approaching and 0.44 against receding F-4 Phantom II fighter not employing infrared countermeasures or evasive maneuvers.\nThe full-capability 9K38 Igla with its 9M39 missile was finally accepted into service in the Soviet Army in 1983. The main improvements over the Igla-1 included much improved resistance against flares and jamming, a more sensitive seeker, expanding forward-hemisphere engagement capability to include straight-approaching fighters (all-aspect capability) under favourable circumstances, a slightly longer range, a higher-impulse, shorter-burning rocket with higher peak velocity (but approximately same time of flight to maximum range).\nThe naval variant of 9K38 Igla has the NATO reporting name SA-N-10 Grouse.\nThe Igla–1M missile consists of a Ground Power Supply Source (GPSS), Launching Tube, Launching Mechanism & Missile (9M 313–1).\nThe Igla is being replaced in Russian service by the 9K333 Verba (Willow) MANPADS. The Verba\'s primary feature is its multispectral optical seeker, using three sensors as opposed to the Igla-S\' two. Cross-checking sensors against one another better discriminates between relevant targets and decoys, and decreases the chance of disruption from countermeasures, including lasers that attempt to blind missiles.\nThe first combat use of the Igla-1E was during the Gulf War. On 17 January 1991, a Panavia Tornado bomber of the British Royal Air Force was shot down by an Iraqi MANPADS that may have been an Igla-1E (or Strela-3) after an unsuccessful bombing mission. The crew, Flt Lts J G Peters and A J Nichol, were both captured and held as prisoners of war (POWs) until the cessation of hostilities. \nIt is uncertain if an AC-130H lost was hit by a \'Strela\' missile or a more recent Igla since Iraq had SA-7, SA-14 and SA-16 missiles at the time, according to the SIPRI database.\nDuring the Iraq War, American and coalition forces suffered a relevant number of helicopter losses. Around a third of them, around 40 aircraft were due to hostile fire, including losses to small arms fire, Anti Aircraft guns, Rocket-propelled grenades and MANPADS; any kind of combat helicopter was shot down from small observation helicopters to armoured Apache gunships. Among the losses to MANPADS, some were reported as losses to older Strela-2 (SA-7) or Strela-3 (SA-14) while others were due to more modern Igla-1E (SA-16) missiles.\nPrivate intelligence company Stratfor asserts that Igla-1E missiles were used in the 1994 shoot down of a Rwandan government flight, killing the presidents of Rwanda and Burundi and sparking the Rwandan Genocide.\nA Peruvian Air Force Mi-25 attack helicopter was shot down on 7 February 1995 around Base del Sur, killing the 3 crewmen, while an Ecuadorian Air Force A-37 Dragonfly was hit but managed to land on 11 February. Hits on additional Ecuadorian aircraft were claimed but could not be confirmed.\nDuring Operation Deliberate Force, on 30 August 1995; a French Mirage 2000D was shot down over Pale by an Igla fired by air defence units of the Army of Republika Srpska. The pilots, Lt. Jose-Manuel Souvignet (pilot) and Capt. Frederic Chiffot (back-seater), were captured and freed in December 1995.\nThe 2002 Khankala Mi-26 crash occurred on 19 August 2002 when a team of Chechen separatists brought down a Russian Mil Mi-26 helicopter in a minefield with an Igla; this resulted in the death of 127 Russian soldiers in the greatest loss of life in the history of helicopter aviation. It was also the most deadly aviation disaster ever suffered by the Russian armed forces, as well as their worst loss of life in a single day since 1999.\nOn 26 January 2014, the militant group Ansar Bait al-Maqdis shot down an Egyptian Mi-17 over the northern Sinai peninsula using a suspected Igla-1E or Igla. How the group came to obtain the weapon is currently unknown.\nDuring the 2011 military intervention in Libya, Libyan loyalist forces engaged coalition aircraft with a certain number of Igla-S. Three Igla-S were fired against British Apache attack helicopters of the 656 Squadron Army Air Corps operating from the HMS Ocean amphibious assault ship. According to the squadron commander at the time, they were all dodged by insistent use of decoy flares by the gunships who in exchange successfully engaged the shooters.\nOn 23 March 2015, a Libya Dawn-operated MiG-23UB was shot down with an Igla-S (reportedly a truck-mounted Strelets variant) while bombing Al Watiya airbase (near Zintan), controlled by forces from the internationally recognized Council of Deputies. Both pilots were killed.\nVideo has surfaced showing rebels using an Igla-1E on a Syrian government helicopter. Such weapons were believed to have been looted from a Syrian army base in Aleppo in February 2013. In 2014, a member of the rebel group Harakat Hazm was filmed aiming an Igla-1E into the air on the same day that the group was filmed operating BGM-71 TOW missiles. Whether these weapons were raided from regime stockpiles or supplied via overseas is unknown.\nOn 14 June 2014, rebel forces near Luhansk International Airport in Eastern Ukraine shot down an IL-76 of the Ukrainian Airforce probably using an Igla MANPADS, killing all 49 Ukrainian service personnel on board.\nOn 12 November 2014, Azerbaijani forces shot down an Armenian Army Mi-24 of a formation of two which were flying near the Azerbaijani border. All three on board died when the helicopter crashed while flying at low altitude and was hit by an Igla-S MANPADS fired by Azerbaijani soldiers.\nOn 13 May 2016, PKK militants shot down a Turkish Army Bell AH-1W SuperCobra attack helicopter using 9K38 Igla (SA-18 Grouse) version of this missile system. The missile severed the tail section from the rest of the helicopter, causing it to fragment in midair and crash, killing the two pilots on board. The Turkish government first claimed that it fell due to technical failure before it became clear that it was shot down. The PKK later released video footage of the rocket being fired and striking the helicopter.\nSeveral variants of the Igla were developed for specific applications:\n|9K34 Strela-3 /SA-14||9K38 Igla /SA-18||9K310 Igla-1 /SA-16||9K338 Igla-S /SA-24||FIM-92C Stinger||Grom||Starstreak|\nready to shoot\n|16.0 kg (35.3 lb)||17.9 kg (39 lb)||17.9 kg (39 lb)||19 kg (42 lb)||14.3 kg (32 lb)||16.5 kg (36 lb)||20.00 kg (44.09 lb)|\n|Weight, missile||10.3 kg (23 lb)||10.8 kg (24 lb)||10.8 kg (24 lb)||11.7 kg (26 lb)||10.1 kg (22 lb)||10.5 kg (23 lb)||14.00 kg (30.86 lb)|\n|Weight, warhead||1.17 kg (2.6 lb),\n390 g (14 oz) HMX\n|1.17 kg (2.6 lb),\n390 g (14 oz) HMX\n|1.17 kg (2.6 lb),\n390 g (14 oz) HMX\n|5.5 lb (2.5 kg),\n20.6 oz (585 g) HMX\n|6.6 lb (3 kg) HE||1.27 kg (2.8 lb)||3x0.90 kg (2.0 lb) tungsten alloy darts,\n3x450 g (16 oz) PBX-98\n|Annular blast fragmentation||Directed-energy||Directed-energy|\n|Fuze type||Impact and grazing fuze.||Delayed impact,\nmagnetic and grazing.\nmagnetic and grazing.\nmagnetic and grazing.\n|Delayed impact.||Impact.||Delayed impact, armour-piercing.|\n|Flight speed, average / peak||470 m/s (1,100 mph) sustained||600 m/s (1,300 mph)\n/ 800 m/s (1,800 mph)\n|570 m/s (1,300 mph) sustained\n(in + temperature)\n|?||700 m/s (1,600 mph)\n/ 750 m/s (1,700 mph)\n|580 m/s (1,300 mph)\n/ 650 m/s (1,500 mph)\n|1,190 m/s (2,700 mph)\n/ 1,360 m/s (3,000 mph)\n|Maximum range||4,100 m (13,500 ft)||5,200 m (17,100 ft)||5,000 m (16,000 ft)||6,000 m (20,000 ft)||4,500 m (14,800 ft)||5,500 m (18,000 ft)||7,000 m (23,000 ft)+|\n|Maximum target speed, receding||260 m/s (580 mph)||360 m/s (810 mph)||360 m/s (810 mph)||400 m/s (890 mph)||?||320 m/s (720 mph)||?|\n|Maximum target speed, approaching||310 m/s (690 mph)||320 m/s (720 mph)||320 m/s (720 mph)||320 m/s (720 mph)||?||360 m/s (810 mph)||?|\n|Seeker head type||Nitrogen-cooled,\nlead sulfide (PbS)\nIndium antimonide (InSb)\nuncooled lead sulfide (PbS)\nIndium antimonide (InSb)\nIndium antimonide (InSb)\n|?||SACLOS and SALH|\n|Seeker scanning||FM-modulated||FM-modulated||FM-modulated||FM-modulated||FM-modulated||FM-modulated||Low intensity modulated-laser-homing darts|\n|Seeker notes||Aerospike to reduce\nsupersonic wave drag\nto reduce supersonic wave drag\n|Low laser beam energy levels ensuring no\nwarning to target\nOn 12 August 2003, as a result of a sting operation arranged as a result of cooperation between the American, British and Russian intelligence agencies, Hemant Lakhani, a British national, was intercepted attempting to bring what he had thought was an older-generation Igla into the United States. He is said to have intended the missile to be used in an attack on Air Force One, the American presidential plane, or on a commercial US airliner, and is understood to have planned to buy 50 more of these weapons.\nAfter the Federalnaya Sluzhba Bezopasnosti (FSB) detected the dealer in Russia, he was approached by US undercover agents posing as terrorists wanting to shoot down a commercial plane. He was then provided with an inert Igla by undercover Russian agents, and arrested in Newark, New Jersey, when making the delivery to the undercover US agent. An Indian citizen residing in Malaysia, Moinuddeen Ahmed Hameed and an American Yehuda Abraham who allegedly provided money to buy the missile were also arrested. Yehuda Abraham is President and CEO of Ambuy Gem Corp. Lakhani was convicted by jury in April 2005, and was sentenced to 47 years in prison.\nIgla and Igla-1 SAMs have been exported from the former Soviet Union to over 30 countries, including Angola, Bosnia and Herzegovina, Botswana, Brazil, Bulgaria (former producer), Croatia, Cuba, East Germany, Egypt, Ecuador, Eritrea, Finland, Hungary, India, Iran, Iraq, the Republic of Macedonia, Malaysia, Mexico, Morocco, North Korea, Peru, Poland, Serbia, Singapore, Slovakia, Slovenia, South Korea, Sri Lanka, Syria, Thailand, Turkey, United Arab Emirates, Vietnam and Zimbabwe. Several guerrilla and terrorist organizations are also known to have Iglas. Alleged Operatives of the Liberation Tigers of Tamil Eelam a rebel organization fighting for a homeland for Tamils in the island of Sri Lanka were arrested in August 2006 by undercover agents of the FBI posing as arms dealers, while trying to purchase the Igla. In 2003 the unit cost was approximately US$60,000–80,000.\n|Wikimedia Commons has media related to 9K38 Igla.|']"	['<urn:uuid:3a77e97e-f0aa-4d1a-9253-4f9c3fcb53ce>', '<urn:uuid:8f927b77-1a03-4f41-aba5-b5855779eac8>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-12T18:38:36.516973	9	125	3610
96	what steps needed plan successful reverse lottery event audience costs prizes	A successful reverse lottery event requires several key steps: First, gather basic materials including raffle tickets, a container for drawing, and prize envelopes. Next, analyze your audience's demographics and spending habits to set appropriate ticket prices. Consider costs including venue rental, catering, and prize money - for example, a 100-person event at $50 per ticket can net $2,500 after expenses with a $1,000 grand prize. Finally, implement targeted marketing strategies and consider offering tiered pricing options to maximize participation.	"['- slide 1 of 5\nWhat is a Reverse Raffle?\nReverse raffle fundraising gives the grand prize to the last ticket to be drawn as opposed to the first ticket winning in a standard raffle. Another difference in reverse raffle fundraising is that the grand prize is often cash. By using the raffle ticket sales proceeds to provide the prize, the fundraiser organizer don\'t have the upfront monetary outlay for a valuable grand prize. As the grand prize is won by the last ticket drawn in a reverse raffle, there are several opportunities through the course of the drawing to earn more money for the charity.\nThe Upside of Downs in Cleveland, Ohio used reverse raffle fundraising in conjunction with their spring fundraiser in March, 2010. The grand prize at that event was $4,000. The University of Toledo Athletic Department offered a $7,500 grand prize during their reverse raffle in early 2010.\nIt is important to verify the laws concerning raffles in each state, usually with the Attorney General\'s office. Registered charities are permitted to conduct raffles as a fundraiser in most states.\n- slide 2 of 5\nHow to Plan a Reverse Raffle\nThe basic fundamentals of reverse raffle fundraising are raffle tickets, a large container to mix them in and draw them from, envelopes for the cash prize(s) and a location to do the actual drawing. With these few elements in place, a reverse raffle fundraiser can be simple or as elaborate as the non-profit cares to make it.\nA reverse raffle does not have to involve a dinner or even require the ticket-buyer to be present. A youth sports league could hold the reverse raffle drawing on the last day of the regular season at the ball field while a symphony orchestra could conduct its reverse raffle as part of its annual fundraising gala. The more elaborate the setting, the higher the profits. However, an elaborate setting can involve rental fees of banquet halls as well as catering fees.\nA simple setting for reverse raffle fundraising is a dinner held in a school cafeteria or a church kitchen. Fraternal organizations such as Amvets or the Elks can benefit from a reverse raffle by taking advantage of the facilities they already own. An inexpensive meal to serve 100 people for reverse raffle fundraising can consist of lasagna, salad, rolls, dessert and non-alcoholic beverages such as coffee, tea and lemonade. Featuring a grand prize of $1000 and an additional $1000 in cash prizes distributed throughout the drawing, a small event of 100 people paying $50 per ticket can easily net more than $2,500 after expenses.\nEach ticket should contain all of the necessary information including the time and place of the drawing. The reverse of the stub or ""receipt"" portion of the raffle ticket should explain the drawing process and any additional prizes to be awarded.\nDownload a sample reverse raffle ticket here.\n- slide 3 of 5\nHow to Run a Reverse Raffle\nThe actual drawing process during a reverse raffle fundraising event will usually take around two hours with 100 tickets sold. If a non-profit decides to give away a total of $2,000 in prizes, they should allocate which tickets drawn will win the additional prizes. Attendees would appreciate having several opportunities throughout the drawing to win back their initial ticket fee of $50. A consolation prize of $100 could be awarded to the first ticketholder drawn and the remaining $900 given in $50 increments spread evenly through the drawing, such as every fifth ticket drawn.\nThe committee members should keep an accurate list of the exact order the tickets were drawn.\n- slide 4 of 5\nAdditional Moneymaking Opportunities During a Reverse Raffle\nShort games can be played at intervals throughout the reverse raffle drawing to give attendees an opportunity to win their way back into the drawing. Anyone wishing to try their luck at winning an extra ticket will pay a small fee ($5 or $10) to participate. Each person can draw a chip out of a bag containing several white chips and one black chip. The person drawing the black chip wins a new ticket back into the drawing. Another method is drawing cards, with the highest or lowest winning the re-entry.\nReverse raffle fundraising can be a challenge, but the rewards can also be highly beneficial.', ""Determining the optimal pricing for raffle tickets requires a delicate balance between ensuring affordability for participants and achieving fundraising targets. A comprehensive approach takes into account not only the direct costs associated with prizes and marketing but also the perceived value of the prizes to the audience.\nEmploying strategic pricing, such as offering discounts for bulk purchases or leveraging high-value prizes to justify higher ticket prices, can significantly impact the success of a raffle. As we explore these considerations, it becomes evident that a nuanced understanding of both financial objectives and participant psychology is crucial.\nDiscover how these elements interplay to formulate an effective pricing strategy that meets your fundraising goals while captivating potential participants.\n- Analyze your audience's demographics and preferences to set appropriate ticket prices.\n- Consider the value of prizes and costs involved to balance ticket pricing effectively.\n- Implement tiered pricing and bulk discounts to cater to different budgets and increase sales.\n- Use targeted marketing strategies to enhance ticket sales and meet fundraising goals.\nUnderstanding Your Audience\nTo effectively price raffle tickets, it is crucial to first understand the demographics, spending habits, and preferences of your target audience. This foundational step ensures that the pricing strategy aligns with the financial capabilities and interests of potential participants, thereby maximizing engagement and profitability. Analyzing past raffle participation data provides insights into which price points have historically attracted buyers, offering a baseline for setting ticket prices. Additionally, conducting surveys or focus groups can yield direct feedback from your audience about what they consider to be acceptable pricing, further fine-tuning your strategy.\nUnderstanding the economic status of your community plays a pivotal role in determining ticket affordability. Prices must strike a balance between being accessible to the majority of the community while still generating the necessary funds. Tailoring your ticket pricing strategy to match the unique interests and preferences of your target audience can significantly enhance participation rates. By integrating these considerations, organizers can establish a pricing structure that appeals to their audience, ensuring the success of the raffle event.\nEvaluating Prize Values\nIn the context of raffle ticket pricing, evaluating the value of prizes is crucial for aligning ticket prices with participant expectations and maximizing revenue.\nThis involves a thorough cost analysis of each prize and an understanding of its appeal to the target audience.\nPrize Cost Analysis\nEvaluating the market value of prizes is a critical step in setting appropriate raffle ticket prices. Understanding the actual costs of the prizes allows organizers to price tickets in a way that reflects the value of these rewards.\nFor high-value prizes, typically valued at $1,000 or more, ticket prices should be set higher to correspond with the perceived value. Conversely, lower-value prizes should have more affordably priced tickets to ensure wide participation.\nImplementing quantity discounts can encourage the purchase of multiple tickets, enhancing the value proposition for participants. Additionally, aligning ticket prices with the perceived value of prizes and offering a range of prices based on prize tiers can attract a diverse group of participants, ensuring the raffle's success.\nDetermining Prize Appeal\nAfter understanding the market value of prizes, it becomes essential to assess their appeal by evaluating prize values through the lens of uniqueness, quality, and relevance.\nThis critical step involves a careful consideration of how distinctive, high-quality, and pertinent the prizes are to the potential ticket buyers. The appeal of prizes significantly influences the success of a raffle, as it directly affects ticket sales.\nCalculating the total value of all prizes and ensuring it aligns with the overall attractiveness of the raffle is vital. Additionally, researching similar raffles provides insights into successful prize values that attract participants.\nCustomizing the prize selection to match the target audience's preferences and interests further maximizes ticket appeal and sales, making the raffle more enticing and likely to succeed.\nSetting Ticket Quantities\nDetermining the optimal quantity of raffle tickets to print requires careful analysis of your target market and anticipated demand. The success of your raffle hinges not just on the appeal of the prizes but also on setting an achievable sales target. This involves understanding the size of your potential market, the level of interest in your raffle, and the mechanisms you have in place to reach this audience effectively.\nTo ensure you set an appropriate number of tickets for your raffle, consider the following strategies:\nAre You Destined for Wealth? Score Over 50% If you Think Like a Millionaire!!!\n- Estimate Potential Ticket Buyers: Look at your community size and gauge interest levels. This will give you a baseline for how many tickets you could realistically sell.\n- Form a Selling Team: A dedicated team can help expand your reach and sell more tickets, potentially increasing the number you can put up for sale.\n- Leverage Grand Prizes: High-value prizes can drive interest and sales, allowing you to increase the number of tickets available.\n- Offer Quantity Discounts: Encouraging bulk purchases through discounts can boost total sales, making it feasible to offer a larger quantity of tickets.\nConsidering Costs and Goals\nHaving established the optimal quantity of raffle tickets, it is crucial to next consider both the costs involved and the fundraising goals to accurately set ticket prices.\nTo begin, the pricing strategy must account for production expenses. This includes not only the cost of printing the tickets but also any design fees if professional services were used. Additionally, the cost of the prizes, unless donated, significantly influences the ticket price. These direct costs must be carefully balanced against the anticipated revenue to ensure the fundraiser is profitable.\nFurthermore, the total expected ticket sales, which hinge on the set price and ticket availability, play a pivotal role. A price too high may deter potential buyers, while a price too low might not cover the expenses or meet the fundraising objectives. Therefore, determining the price involves a strategic calculation that aligns the ticket cost with the net proceeds needed to reach the fundraising target.\nDistribution costs, including marketing and selling the tickets, also impact the final pricing. To effectively finalize ticket prices, organizers must calculate the total costs, estimate the number of tickets they expect to sell, and consider rounding up the prices for simplicity, ensuring the goals of the fundraiser are met efficiently.\nExploring various pricing strategies is essential for optimizing raffle ticket sales and ensuring the success of your fundraising event. By carefully considering how you price your tickets, you can significantly influence the overall appeal and profitability of your raffle. The goal is to balance affordability for participants with the need to raise funds effectively.\nHere are four effective pricing strategies to consider:\n- Tiered Pricing Options: Offer tickets at varying price points, such as $1 for 1 ticket, $5 for 6 tickets, and $10 for 15 tickets, to cater to different budget levels and encourage more significant purchases.\n- Structured Bulk Purchase Discounts: Implement structured pricing to promote bulk purchases, e.g., $2 for 1 ticket, $8 for 5 tickets, or $15 for 10 tickets. This strategy can increase the volume of ticket sales.\n- Early Bird Discounts: Incentivize early purchases by offering discounts for those who buy tickets soon after they become available. This creates a sense of urgency and can boost initial sales momentum.\n- Value Perception through Unique Prizes: Align the prizes with your target audience's interests to enhance the perceived value of the raffle tickets, making them more appealing and justifying the ticket price.\nMarketing Your Raffle\nEffectively marketing your raffle is crucial for maximizing ticket sales and ensuring the success of your fundraising event. In today's digital age, utilizing social media platforms is a powerful way to promote your raffle to a larger audience. By creating engaging content and sharing it across Facebook, Twitter, Instagram, and other networks, you can significantly increase your raffle's visibility and attract more potential ticket buyers.\nAdditionally, traditional marketing methods still hold value. Visually appealing flyers and posters placed in strategic locations can capture the attention of passersby and generate interest in your event. These tangible pieces of marketing can complement your online efforts and reach segments of your audience who may not be as active on social media.\nWord-of-mouth marketing, driven by enthusiastic supporters, can also play a vital role in spreading the word about your raffle. Encouraging your most engaged participants to share their excitement with friends, family, and colleagues can lead to a ripple effect, greatly expanding your reach.\nPartnering with influencers or local celebrities can further boost your promotional efforts. Their endorsement can lend credibility to your raffle and attract a broader audience. Additionally, utilizing email newsletters and website announcements ensures that your existing supporters are well-informed and reminds them to participate, thereby driving ticket sales.\nUnderstanding the psychology behind decision-making can be instrumental in optimizing raffle ticket pricing strategies. The mental processes that guide consumers' choices can significantly influence the effectiveness of various pricing models in a raffle. By tapping into these psychological underpinnings, organizers can craft pricing strategies that not only appeal to potential participants but also maximize the revenue generated from ticket sales. This approach is particularly crucial when dealing with raffles offering exceptional prizes, such as Superbowl tickets, where the perceived value can justify higher ticket prices. Additionally, understanding and adjusting to market demand is essential for setting prices that participants are willing to pay while ensuring the raffle remains enticing.\nHere are four key points to leverage psychology in raffle ticket pricing:\n- Adjust ticket prices based on demand and prize value: Higher demand and more valuable prizes can justify increased prices.\n- Understand psychology's role: Recognize how psychological factors influence purchasing decisions.\n- Impact on revenue: Use psychology-informed strategies to enhance revenue potential.\n- Market demand awareness: Tailor ticket prices to reflect the market's willingness to pay, ensuring prices are attractive yet profitable.\nIn conclusion, the pricing of raffle tickets demands a comprehensive analysis encompassing audience understanding, prize valuation, cost assessments, and strategic marketing endeavors.\nEffective pricing strategies, grounded in psychological principles and market insight, play a pivotal role in ensuring the raffle's success.\nBy striking a balance between affordability and the imperatives of revenue generation, and by tailoring offerings to the preferences and expectations of diverse participant groups, organizers can significantly enhance the appeal and profitability of raffle events.""]"	['<urn:uuid:7be144cf-b38a-4b3e-b63c-8f4ebf4de19e>', '<urn:uuid:9327cd4c-285e-494e-b256-a48060780cf3>']	factoid	with-premise	long-search-query	distant-from-document	three-doc	expert	2025-05-12T18:38:36.516973	11	79	2434
97	What are the key advantages of mushroom mycelium when used as a base ingredient for plant-based food products?	Mushroom mycelium offers several key advantages: it is rich in umami flavor, has low off-flavors, and features a naturally fibrous texture. It also has a balanced nutrient profile, containing complete protein with all essential amino acids, prebiotic fiber, vitamins and minerals.	"['Mushlabs leverages submerged fermentation to produce mushroom mycelium - the network of threads from which mushrooms grow. This base ingredient offers a number of advantages to product formulators working in the plant-based space. It is ‘rich in umami’, low in off-flavours and boasts a naturally fibrous texture.\n“Mycelium from edible mushrooms offers ideal natural properties for product development, like a highly adaptable rich umami flavour, a naturally fibrous and juicy texture and minimal off-flavours - ideal requirements to produce tasty, natural and minimally-processed food products,” Co-Founder and CEO Mazen Rizk told FoodNavigator.\n“Our goal is to provide more options for a tasty, sustainable and healthy diet. Thanks to the special properties of our fermented edible mushroom mycelium, there are hardly any limits to what we can do in terms of product development. It enables us to play across the consumer basket in meat, dairy and fish alternatives and even create a totally new and unique food category,"" he said, noting that the group is currently working on a number of prototype products.\nMycelium also boasts a ‘balanced nutrient profile’, containing complete protein with all essential amino acids, prebiotic fibre, vitamins and minerals.\nFermentation: A sustainable solution to the ‘challenges of our times’\nMushlabs’ Rizk maintains that fermenting protein ingredients for use in food production offers a solution to the ‘various pressing challenges of our times’.\nFermenting mycelium in a nutrient liquid, Mushlabs is able to control growth parameters such as temperature and pH levels. This means resources can be used more efficiently and the growth rate of the mycelium can be increased.\n“Within a year, we managed to have the growth time for our mycelium to a few days,” Rizk said. “Mycelium also grows incredibly fast, around 25 times faster than soy, and needs relatively little resources.”\nProperties like aroma and texture can be altered in a ‘targeted manner’ by tweaking the conditions and growth parameters the fungus is exposed to, we were told. “In the future, we will be able to produce raw products that are precisely tailored to different requirements,” Rizk added.\nMushrooms are ‘masters’ of upcycling\nRizk explained that mushrooms are ‘masters of upcycling’. This means the start-up is able to leverage side streams from food and agriculture to ferment mycelium, increasing circularity and resource efficiency.\n“Mycelium produces highly active enzymes that can break down a large variety of nutrients, which means it can tap significantly more energy sources than other microorganisms,” Rizk elaborated.\nThis approach delivers more than sustainability gains. It provides Mushlabs with a ‘significant edge’ in terms of product versatility and building relationships with multinational food companies, Rizk explained.\nThis is important because Mushlabs, which was founded in 2018, plans to grow its business through partnerships with major F&B companies as well as in the B2C space. 2022 will be ‘all about commercialisation’, the biotech expert explained.\n“We are currently in our commercialisation phase, finalizing the upscaling of our production processes to an industrial level, establishing partnerships with major companies in Europe, the USA and Asia and preparing to announce the launch of our first product.\n“Currently we plan to develop a B2C business, we will closely cooperate with renowned food companies all over the world. We believe that our story is captivating and our product is absolutely unique - that\'s why we will launch our own product brand and build up a strong supporting community,” FoodNavigator heard.\nThe group is now working to build commercial capabilities and the upscaling of the production process to an industrial level is ‘almost complete’. Potential partners for sourcing, co-development and sales are being discussed worldwide.\nBitburger Brewery Group tie-up to upcycle brewery by-products\nIn this vein, Mushlabs has just revealed it is launching a collaboration alongside Bitburger Brewery Group - one of Germany\'s largest private breweries.\nBitburger Brewery will provide production capacity and by-products from beer production as raw materials that will be valorised by Mushlabs, who will use their fermentation process to produce mycelium that will be used as an ingredient in \'nutrient-rich, minimally processed\' foods.\nThe collaboration builds on a long-term relationship between Mushlabs and Bitburger, with the brewer\'s venture investment arm having invested in Mushlabs in 2019.\nIt also furthers Bitburger\'s own sustainability work. The brewer has been \'working intensively\' to develop circular economy solutions within its own operations and already recycles its residual materials to the agricultural sector. Bringing the upcycling process on site to produce value-added ingredients for human consumption will be more efficient and reduce emissions linked to areas like transportation, the company noted.\n""Through the interaction between the fermentation know-how of Mushlabs and the technical brewing know-how of the Bitburger Brewery Group, we can offer real added value,"" said Dr Friedrich Droste, MD of Bitburger Ventures.\n""Using by-products and existing infrastructure from the brewing process to produce alternative proteins in the brewery environment is no longer a future fantasy with this cooperation, but lived practice that creates sustainable value for all parties involved and the environment.""\nUnlocking the possibility of local production for \'fairer\' food\nThis hyper-local approach that sees by-products upcycled on-site highlights another advantage of the Mushlabs fermentation tech. The production is independent from environmental influences and can be carried out locally for shorter supply chains. There is no need for soil, pesticides or fertilisers and the production can be established ‘anywhere in the world’, regardless of factors like climate.\nThis meets food safety and security objectives and could help develop a ‘fairer’ and ‘decentralised’ future food system, Rizk believes.\n“We follow a so-called ‘multilocal’ approach. Since fermentation is independent from environmental conditions and can be implemented everywhere, we want to cooperate with partners worldwide to source local side streams and produce products, that are adapted to local taste and culture. Our vision is to help create a decentralized, fairer and more sustainable food system.”\nMushlabs recently secured fresh funding to help it accelerate towards commercial scale. The company will receive an eight-figure figure capital injection from the EU\'s EIC Accelerator program, it revealed. It is also amoung the start-ups to join EIT Food\'s RisingFoodStars, whose cohort receive industry support, networking opportunities and access to additional investment to help them grow and scale.\nWhile Rizk said that it is \'still early days\' and the start-up is evaluating next steps to build scale, it is clear that the company wants to grow rapidly with a focus on R&D. Over the last year it has doubled its team, bringing in additional expertise to support future growth.\n“We believe that mushroom mycelium is the food of the future.”']"	['<urn:uuid:4d8e4511-7ee7-4f1d-a01b-98c207c24e0e>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:38:36.516973	18	41	1087
98	How can doctors use patient reviews for advertising while staying legal?	Healthcare providers can leverage patient reviews for marketing by implementing effective feedback collection methods like surveys and patient interviews, and sharing positive reviews on their websites and social media channels. However, they must ensure HIPAA compliance by obtaining written patient authorization before using any protected health information for marketing purposes. Additionally, when using patient testimonials, providers must follow FTC guidelines - the testimonials must reflect honest opinions and typical patient experiences, with clear disclosure if results are not typical. Healthcare organizations can highlight patient satisfaction rates and improvement statistics in marketing materials, but must maintain patient privacy and confidentiality while ensuring all marketing activities comply with federal and state regulations.	"[""Healthcare marketing is the process of promoting and advertising healthcare services and facilities to the public. With the growing importance of patient-centered care, it is crucial for healthcare providers to listen to and incorporate patient feedback in their marketing strategies. Patient feedback can be collected through various channels, such as surveys, online reviews, and social media, and can provide valuable insights into patient experiences and perceptions of healthcare services. In this article, we will explore the importance of healthcare marketing based on patient feedback and reviews and strategies for incorporating patient feedback into healthcare marketing.\nThe Impact of Patient Feedback and Reviews on Healthcare Marketing\nPatient feedback and reviews are essential to creating a successful healthcare marketing strategy. Positive feedback and reviews can build trust and credibility with potential patients, while negative feedback can harm a healthcare provider's reputation. Incorporating patient feedback can help healthcare providers improve the quality of their services and patient experience, which can, in turn, improve their marketing efforts. Online patient reviews have become increasingly important, as patients often consult online reviews before choosing a healthcare provider. The impact of patient feedback and reviews on healthcare marketing is significant, as it can influence marketing messaging and strategies, website design and content, and social media marketing. In many cases, patient feedback and reviews can also influence the development of new healthcare services and offerings to meet patient needs.\nBest Practices for Leveraging Patient Feedback and Reviews in Healthcare Marketing\n- Implementing effective patient feedback collection methods: Collecting and analyzing feedback can help healthcare providers understand patient needs and identify areas for improvement. Effective feedback collection methods include surveys, focus groups, and patient interviews.\n- Analyzing and utilizing patient feedback to improve healthcare services: After collecting feedback, healthcare providers should analyze and use the insights to make meaningful improvements to the services they offer. Providers can use the feedback to identify pain points, prioritize areas of focus, and develop solutions that address patient concerns.\n- Leveraging patient reviews on online platforms for marketing purposes: Patient reviews on online platforms like Google, Yelp, and Healthgrades can influence prospective patients and their decisions. Healthcare providers can leverage these reviews by responding to them, addressing concerns, and sharing positive reviews on their websites and social media channels.\n- Encouraging patient reviews and managing negative feedback: Providers can encourage patients to leave reviews by making it easy and convenient for them to do so. Providers should also have a plan in place to manage negative feedback, respond promptly, and work to resolve issues. Turning negative feedback into an opportunity to improve can build trust and improve the provider’s reputation.\nHealthcare Marketing Strategies Based on Patient Feedback and Reviews\n- Using patient testimonials and success stories in marketing efforts: Patient testimonials and success stories are a powerful tool for healthcare marketing. They can help potential patients understand the positive impact your healthcare services have had on other patients and can help build trust in your organization. When using patient testimonials and success stories in marketing efforts, it is essential to obtain patient consent and ensure that the stories are representative of the patient experience.\n- Highlighting patient satisfaction rates and improvement statistics in marketing materials: Healthcare organizations can use patient satisfaction rates and improvement statistics in their marketing materials to demonstrate the quality of their services. This can include sharing data on patient outcomes, satisfaction rates, and other relevant metrics. When highlighting these statistics, it is essential to be transparent about the data and provide context to help potential patients understand what the numbers mean.\n- Utilizing patient feedback to identify target patient populations and tailor marketing messages to specific audiences: Patient feedback can be used to identify target patient populations and tailor marketing messages to specific audiences. For example, suppose patients consistently provide positive feedback about the quality of care they receive for a specific condition. In that case, healthcare organizations can use this information to tailor marketing messages to patients who have that condition.\n- Incorporating patient feedback into healthcare branding and messaging: Patient feedback can also be used to inform healthcare branding and messaging. By incorporating patient feedback into marketing messages, healthcare organizations can demonstrate their commitment to patient-centered care and build trust with potential patients. For example, if patients consistently provide positive feedback about the compassionate care they receive from healthcare providers, a healthcare organization can use this information to develop messaging that highlights their commitment to compassionate care.\nChallenges and Considerations in Healthcare Marketing Based on Patient Feedback and Reviews\n- Ensuring patient privacy and confidentiality in feedback collection and use: While collecting and using patient feedback is crucial for improving healthcare services and building trust, it is equally important to maintain patient privacy and confidentiality. Healthcare marketers must ensure that patient feedback collection and use complies with relevant privacy and confidentiality regulations and guidelines.\n- Managing negative feedback and online reputation management: While positive patient feedback can be a valuable marketing asset, negative feedback can have a significant impact on a healthcare provider's online reputation. Healthcare marketers must have a strategy in place to manage negative feedback, respond to patient complaints, and address concerns in a timely and effective manner.\n- Avoiding biases and ensuring that patient feedback is representative of the patient population: Patient feedback collection methods must be designed to avoid biases and ensure that feedback is representative of the patient population. This includes considering factors such as language barriers, cultural differences, and the accessibility of feedback collection methods.\n- Ensuring that marketing efforts based on patient feedback align with healthcare regulations and ethical considerations: Healthcare marketers must ensure that marketing efforts based on patient feedback are compliant with relevant healthcare regulations and ethical considerations. This includes avoiding false or misleading claims, ensuring that marketing messages are backed by evidence-based research, and avoiding practices that could be considered exploitative or unethical.\nIn conclusion, patient feedback and reviews can be powerful tools for healthcare marketing, helping to build trust, improve services, and reach specific patient populations. By implementing best practices and leveraging patient feedback, healthcare providers and organizations can enhance their marketing efforts and connect with patients more effectively. However, there are also several challenges and considerations to keep in mind. Working with a healthcare marketing consultancy like Compendious Med Works can help providers navigate these complexities and develop effective marketing strategies that are grounded in patient feedback and reviews."", 'Marketing activities may help to bolster revenue, but they must also be HIPAA compliant. Here are the legal issues to consider when marketing your practice.\nAs a result of increased competition within the healthcare industry, many providers are considering, or have pursued, marketing activities to bolster their practices. The most common question posed by healthcare providers when contemplating marketing is ensuring compliance with the Health Insurance Portability and Accountability Act (HIPAA).\nHealthcare providers must ensure that they are not violating HIPAA through the impermissible use or disclosure of a patient’s protected health information (PHI). In addition, healthcare providers need to be aware of other federal and state laws when developing their marketing strategies.\nHIPAA defines marketing as any oral or written communication about a product or service that encourages the recipient of the communication to purchase or use the product or service. With limited exceptions, the HIPAA Privacy Rule requires that a healthcare provider, as a covered entity, obtain the written authorization of the patient prior to any use or disclosure of the patient’s PHI for marketing purposes.\nThe two stated exceptions to the HIPAA Privacy Rule are:\nAbsent an exception, the healthcare provider would need to obtain the written authorization of the patient.\nWhat information must be included in the written authorization? The HIPAA Privacy Rule details a list of core elements and required statements that need to be included in the written authorization for it to be effective.\nNext: Paying patients for marketing\nThere is a key additional requirement in connection with marketing practices. If the marketing involves financial remuneration (i.e., direct or indirect payment from or on behalf of a third party whose product or service is being described) to the healthcare provider from a third party, the authorization must state that such remuneration is involved.\nObtaining a HIPAA-compliant authorization is merely the first step. The healthcare provider must also ensure that any marketing activities are in accordance with all federal and state statutes and regulations.\nBecause there has been an exponential increase in the utilization of patient testimonials as a marketing tool, it is worth spending a moment to discuss the Federal Trade Commission Act (FTCA).\nThe FTCA aims to prevent unfair competition methods and unfair or deceptive acts that may affect business commerce.\nIn 2009, the Federal Trade Commission (FTC) released “Guides Concerning the Use of Testimonials and Endorsements” to provide direction to advertisers, including healthcare providers, on how to ensure that testimonial and endorsement advertisements are in accordance with the FTCA. The 2009 release of the Guides was the first re-interpretation of the testimonial and endorsement regulations in nearly 30 years, as the FTC had not updated the guide since 1980.\nThe guidelines define an endorsement (which includes a patient testimonial) as any advertising message that consumers are likely to believe reflects the opinions, beliefs, findings, or experiences of a party other than the sponsoring advertiser, even if the views expressed by that party are identical to those of the sponsoring advertiser.\nThe advertisement must reflect the honest opinions, findings, beliefs, or experience of the patient and should be reflective of a typical experience for patients using a similar product or service.\nIf the testimonial is not reflective of the typical experience of most patients, the healthcare provider must clearly and conspicuously disclose the generally expected experience of similar patients.\nThis is a departure from the guide promulgated by the FTC in 1980, which permitted an advertiser to describe unusual results in an advertisement, provided that a disclaimer was included which stated that “results were not typical.”\nThe use of marketing by healthcare providers will continue to grow and evolve. It is essential that providers develop and implement a marketing strategy that is compliant with HIPAA, the FTCA, and all other federal and state laws.\nMatthew Colongeli, JD, is an associate at Garfunkel Wild, P.C., in Great Neck, New York. Send your legal questions to firstname.lastname@example.org.']"	['<urn:uuid:02cfb7a1-ff81-45fa-bb1d-1c870ebad1b4>', '<urn:uuid:85037c4b-e7f1-4629-9f46-dee0d823b276>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	11	110	1716
99	best rod weight recommendations catch king salmon fly fishing	For single-handed rods, nine and ten weight rods that are nine feet long work well. For two-handed (Spey) rods, those no longer than 14 feet are preferred, especially when fishing from a small boat. Eleven weight rods are too heavy and exhausting to cast.	['Jay Nicholas knows how to catch Chinook salmon on the fly. He spent the last 30 years studying and supporting wild salmon as a fisheries biologist with ODFW and has been fly fishing since 1954. He was a longtime gear fisherman, but after catching his first king salmon on the fly rod, it was over. There was no going back.\nIn this Q&A, Nicholas talks about techniques for catching salmon on the fly and the important conservation issues facing our salmon populations.\nBe sure to sign up for Jay Nicholas’s salmon seminar at The Caddis Fly on Thursday Dec. 11th. Details at the bottom of this post.\nSo why fish for salmon with a fly rod?\nFishing – any kind of fishing – is deeply personal. There is no objectivity to it. Eggs, spoons, plugs, jigs – these all require skill and technique. I fished all kinds of gear for Chinook from the mid 1960s until 2003. That year, on a whim, I decided to try the fly.\nI stopped at the Caddis Fly (honestly) to buy my shooting heads, knotted braided loops on the lines, applied PlioBond to the knots, and hung them over my windshield visor to dry on the trip from Eugene to Gold Beach. I put a second coat of PlioBond on the knots at Elkton, where I bought a diet Coke and got a giant case of hiccups.\nI arrived on the Rogue in the dark and stumbled into a campground where I met some old guys from California who were fly fishing regulars on the Rogue. They shared their beer and hot dogs. I shared my Hershey bars. They were up and on the water in the dark, but I hung back, hesitant in my ignorance. I didn’t know where to fish, how to anchor, how close I could fish to the regulars, what line or fly to use. The first day I fished I had one grab, a complete accident, and broke-off on the grab. The second day I never had a grab. My hands were swollen because I didn’t know how to double haul and I had never fished over an 8-weight fly rod. The sun seared my eyes. The wind howled in from the coast and threw every cast back in my face. I was exhausted. I was confused by the chit-chat overheard among the fly anglers around me. And then, on day-four, I hooked a Chinook on ten-pound leader and a size 6 Chartreuse Scud (a fly given me by a friend and mentor). This fish was maybe 12 or 14 pounds. Chrome on Chrome. Powerful. Primitive. My heart was in my throat every second I fought the fish, and I shook with joy and excitement long after I beached the fish. I had caught many, many Chinook on gear over the previous four decades. I even caught a #50-plus fish back in the 1980s. Not one of those salmon inspired me like this smallish king salmon did. Not one, and not all of them together. This one king salmon changed my life. For me, fly fishing for Chinook is a passion. An obsession. A lifestyle. Whatever you call it. It’s also a little crazy. It is what it is.\nDo you have a rule of thumb for fly selection for salmon?\nThe lower, the clearer, the more pressure from anglers on the water, the smaller the fly. Those are conditions where I’d go to a size eight Comet, sparsely dressed. If you get murkier water or real early or late in the day, I like larger flies that present a bigger silhouette.\nEven in murky water, most of my flies are size twos, but sometimes I larger hooks and longer shanks even though they’re out of vogue. In lower visibility I like my fly to be larger. And I like to have some black in these big flies because it shows up well.\nDo you notice a difference in what flies salmon prefer from river to river?\nI think the salmon like bigger flies on Oregon’s North Coast. I don’t know why. But on the South Coast, fishing the Rogue I would never consider using a big # 2 Boss. But of course, when I’m fishing the Rogue, it’s 65 degrees, clear water and that’s where I’m using size sixes and eights. On Elk River, if the water is clear, I’m using size 6 comets and smallish (under 2-inch) Clousers. If I get up to the Nestucca I’m going to want a 4-6 inch Clouser on a size 2 hook or a big bushy Boss fly.\nAre larger flies really more effective on the North Coast? Don’t know. Fact is, I haven’t done the true experiment of consistently fishing small flies in North Coast.\nWhat’s the biggest conservation threat to salmon on the coast?\nYou, me, our neighbors, and our society that wants everything from our streams, woods, and lands, thinking that we can continue to “use” these natural resources without consequences to fish, to clean water, and our children’s future. Salmon must have a suitable stream where they can spawn and young fish must have good rearing habitat. Salmon are different than migratory birds that can fly from one good patch of habitat to another. Salmon are different from animals that can survive in an isolated patch of suitable habitat. Salmon spawn in the headwaters, migrate to the ocean and back again. If something is broken in the river/estuary/ocean ecosystem anywhere along the line, you’re going to lose these great fish. Any break in the life-cycle chain can lead to the salmon’s demise.\nAnd I worry about predators, too, seals and some of the pisciverous birds that are mostly protected these days. I’m speaking from emotion here, not from data, but I really believe that marine mammals and birds are taking a significant chunk out of the number of salmon that could be returning to our coastal streams. Time will tell.\nThe broader fishing community has been apathetic about protecting salmon habitat. How can we get more people involved?\nI don’t know. I wonder if anglers, organized or not, tend to focus most of their energy on learning how to catch more fish, and on promoting hatcheries. I wonder if anglers just don’t want to get as involved in habitat protection as they do in the social aspects of fishing. Of course this isn’t universally true. I know anglers who participate in STEP (Salmon and Trout Enhancement Program) who are as passionate about protecting habitat as they are about supporting hatcheries and going fishing. I just wish that the watershed councils were full of anglers, and landowners, and conservation advocates.\nThere are a lot of coastal rivers where wild fish are the foundation of an opportunity to catch fish. In Tillamook Bay, probably 90% of the fall-run fish run is wild. On the Nehalem, Siletz, Alsea, and Siuslaw, all the Chinook are wild. The Rogue fall-run is probably over 98% wild.\nNone of these remarks are meant to diminish the importance of hatchery fish. Tillamook Bay, for instance, depends on hatchery spring Chinook to support a fishery. Same goes with the upper Rogue spring Chinook fishery – it really depends on hatchery fish. I just try to remind people that we have some really important and productive wild fish runs and emphasize the importance of protecting the ecosystem processes that these fish need.\nHow do Oregon’s coastal salmon populations look today?\nI want to be able to catch wild salmon and steelhead in our streams in 100 years. Well, you know, not me, really, but my son’s kids – I want them to be able to catch wild salmon in our coastal streams a hundred years from now. Whether salmon are reasonably abundant or scarce, a hundred years from now, that’s the real question. I don’t think extinction is on the table, especially on our coastal streams. I think on most coastal rivers, the habitat and fishery management decisions we make over the next 10-20 years will determine whether wild salmon are common or rare in the next century.\nYou asked how salmon are doing on the coast these days. OK – I would say OK. We are in a Chinook slump right now, after seeing great returns in the late 1980s and early 2000s. But we’ve seen more Chinook jacks on the coast this fall than I’ve heard of for years. That indicates good survival for the 2007 brood. The ocean off Oregon this summer was full of salmon food. If we could put together two-to-three good years of ocean conditions, we could have 100,000 kings come back to the Rogue and 50,000 to the Tillamook area. That’s what I’m hoping for. Three or four good survival years in a row. Then we need to maintain moderate harvest rates in ocean fisheries to pass through good numbers of spawners. There is hope – real hope – if we do our part protecting habitat and the ocean cooperates too.\nHow should someone start fly fishing for salmon?\nPlan A: Hire a good Guide. A fly fishing Guide who knows Chinook. That is the most direct route to a shorter learning curve.\nPlan B: Go to your local fly shop, listen to people you trust. Get out on the river and see what other people are doing. Go fishing, go fishing, and go fishing some more. Listen and observe. Be patient. Be tenacious. Be stubborn.\nPlan C: Execute both Plan A and Plan B!\nYour time on the water is valuable beyond price. You need to see the lines people are using in specific locations, and under specific flow and tide conditions. You need to see the leaders and flies and tackle they are using. You need to see where they stand, where they anchor, and where they cast. This is how you will learn when to use a 3-foot leader versus a 12-foot leader; when to choose small flies or big flies; when to use fast sinking lines versus dry lines. If you want to flyfish for Chinook, by yourself, you have to immerse yourself heart and soul in it.\nWhat are some of the misconceptions about salmon on the fly?\nPart of getting people introduced to Chinook fly fishing involves expanding their consciousness and understanding about the fish. You can catch big kings on small flies, ten-pound leader, and have a ball. For the angler getting into fly fishing for Chinook, there is no simple formula; it is a journey, a pilgrimage.\nA lot of the Salmon fly fishing articles promote heavy sinking heads, say, 400-500 grain lines. These lines can be very effective when you’re fishing swift deep water. In contrast, I tend to fish slower water and use lighter, slower sinking lines. The water might be 12 feet deep, but I want to have my fly at 6 feet.\nYou have to understand the fish. Very often in slow deep water Chinook won’t be on the bottom. If it’s a rip-roaring current, yes they will be, because that’s the only place they have velocity shelter. This is what Back-bouncers refer to as 3- or 4-ounce water, because it takes 3-4 ounces of lead to get their bait down to the fish. But if it’s slow moving water, and Chinook are fresh from the ocean, they’re pelagic. They’re swimming around 4-5 feet off the bottom, cruising around. So if you use a fast sinking fly line, you’re out of the zone by dredging the bottom. Another misconception is that salmon flies have to be tied on giant heavy hooks, that’s mostly nonsense.\nWhat is your preference on hook selection?\nMy first choice is usually a straight-eye hook, my second choice is a down eye, and my last choice is up-eye. That’s my theory. In practice, I use all of these hooks and they all work. I have even used 3XL standard trout hooks – like you would use for tying Wooly Buggers – and had good results. I tend to use a Uni-loop knot and I think that my hook-setting ability is better with either a ring-eye or down-eye hook. But I can’t provide proof, just preference.\nI like the Uni loop because I tend to fish fairly slow water. If I’m fishing slower water, I don’t want to take a chance that my knot is off to the side or my fly is hanging funny. You may get a little more wiggle with a loop knot.\nAny advice on rod selection for salmon?\nI started out fishing single handed rods for kings. I tried an eleven weight and it almost killed me. I had to triple dose on ibuprofen to cast the thing. Then I went to nine and ten weight rods – much nicer. Then I saw a guy fishing a Spey rod. I watched him, decided to give it a try, just overhand casting with traditional shooting heads or integrated shooting head lines.\nNow I carry both one- and two-hand rods whenever I fish kings. Tidewater Spey rod casting is strictly overhead casting. Fishing Spey rods upriver calls for Skagit lines and genuine spey casting with sinking tips.\nMy one-handers are all nine and ten weights, and are typically nine-footers. As far as two handers go, I prefer rods no longer than 14’, simply because a long rod can get in the way in an eleven foot pram. Spey rods have great fish-fighting ability. When you’ve got a 35lb king that wants to hang under the boat, with a 9-weight single hand rod, it’s really hard to pull them out. Males tend to do that to you. They will get under the boat and just stay there. An 8- or 9-weight Spey rod has a lot more butt to it. The awkward part is finding the line that will load the two-hander, without fishing too deep.\nDo you have any recommendations for prospective salmon-chasers to read?\nA lot of the glossy articles about Chinook fly fishing show photos of pristine rivers in Canada or Alaska. They show the most beautiful giant fish and talk about 20-fish days. Unless you can afford to travel to exotic locations, that’s not what you’re likely to experience here in Oregon. It is just plain hard work here in Oregon, and it is rare to find solitude and numbers of willing Chinook in the same place and time here. Chinook fly fishing here in Oregon includes boats anchored six feet apart, or 30 guys standing in a riffle. Chinook fly fishing here in Oregon often includes small fish, or dark fish – they aren’t all giant chromers. Here in Oregon, you may fish 6 days straight, dawn to dark, and only getting a single grab. And that’s when you have it good. It may take you 15 days or more, to get a single grab. But if you love the quest, the art, the passion of fly fishing for Chinook, it is a sweet joy when the grab does come.\nThe classic book on fly fishing for Chinook salmon in California and Oregon is The Angler’s Coast by Russell Chatham. It’s good to know that there were fanatics fly fishing for Chinook decades ago, using tackle we would now consider primitive. The Angler’s Coast is a fun book to read. It will tell you something about the roots of the sport, the personalities that you will still meet today, and the techniques — and the book will give you encouragement, perhaps, to give this crazy pursuit a try yourself.\nJay Nicholas is coming to The Caddis Fly for a one-evening fly fishing for salmon seminar. Jay will share his tips for catching big chrome fish on flies, presenting his PowerPoint tour of salmon fishing in Oregon, demonstrating how to tie his most successful salmon fly patterns, and explaining gear and line selection for handling these sea-run bruisers. This is going to be an amazing class, so come sign up at the shop, seating is limited.When: Thursday Dec 11th, 6-9pm\nWhere: The Caddis Fly 168 West 6th Ave., Eugene, OR 97401']	['<urn:uuid:973aaff7-7904-42f7-bcf2-0b556066a441>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T18:38:36.516973	9	44	2682
100	Is honey good for wounds and what makes it dangerous?	Honey is beneficial for wounds as a natural antiseptic - it prevents infections, reduces odors, swelling, and scarring, and helps tissue regrowth. However, there are risks to consider - honey can contain botulism spores, which are found in about 2.1% of honey samples. While these spores can be dangerous, especially to infants, the botulinum toxin itself can be destroyed by heating to temperatures greater than 80°C.	['By Henrylito D. Tacio\nA teaspoon of honey before bed seems to calm children’s coughs and help them sleep better, according to a new American study that relied on parents’ reports of their children’s symptoms.\nWhat is remarkable is the fact that the folk remedy did better than cough medicine or no treatment in a three-way comparison. Honey may work by coating and soothing an irritated throat, the study said. “Many families are going to relate to these findings and say that grandma was right,” noted Dr. Ian Paul of Pennsylvania State University’s College of Medicine who headed the study.\nHoney has been used as a folk remedy for ages for ailments ranging from allergies to indigestion. The body of Alexander the Great is said to have been preserved in honey. John the Baptist lived for a long period of time in the wilderness on a diet consisting of locusts and wild honey (Matthew 3:4).\nHoney is composed of sugars like glucose and fructose and minerals like magnesium, potassium, calcium, sodium chlorine, sulphur, iron and phosphate. It contains vitamins B1, B2, C, B6, B5 and B3 all of which change according to the qualities of the nectar and pollen. In addition, copper, iodine, and zinc exist in it in small quantities. Several kinds of hormones are also present in it.\n“Honey is a treat, and is man’s oldest sweetener,” someone once wrote. “It is an excellent substitute for sugar in our drinks and food. It is also good for many medicinal uses and treating certain conditions.”\nFor at least 2700 years, honey has been used to treat a variety of ailments. But it’s only in recent times that scientists have been investigating the use of honey in modern medicine.\nIn Hamilton, New Zealand, scientists at the honey research unit of the University of Waikato have found that honey kills a wide range of bacteria. Part of honey’s antibacterial activity can be explained by what’s known as an “osmotic” or “water-withdrawing” effect. Honey reportedly has a density of about 1.36 kilogram per liter (that’s 40% denser than water!).\n“Honey has very little water – that’s what makes it thick and gooey – whereas bacteria are made mostly of water,” explained Dr. Peter Molan, professor of biological sciences and the unit’s director. “So when certain kinds of bacteria come into contact with honey, the honey basically sucks the water out of the bacteria like a sponge, and the bacteria die.”\nEven though honey tastes sweet, it’s actually quite acidic. The pH of honey is commonly between 3.2 and 4.5. This relatively acidic pH level prevents the growth of many bacteria.\nDo you have a cut? Honey is therefore a natural antiseptic. Medical journals cite more than 600 cases in which honey was employed to treat wounds. By applying honey to your wounds, you prevent infections. Honey contains antimicrobial agents, which prevents infections by killing the bacteria in and around your wounds.\nInstructs ‘The Folk Remedy Encyclopedia : Olive Oil, Vinegar, Honey and 1,001 Other Home Remedies’: “When using honey it may help to heat it up before putting it on your wound (caution test the heat before you place it on the wound). Many types of bacteria can’t survive in honey, so wounds heal, swelling eases, and tissue can grow back.”\nSome studies suggest that the topical use of honey may reduce odors, swelling, and scarring when used to treat wounds; it may also prevent the dressing from sticking to the healing wound.\nHoney may also be good for your skin. It has the ability to attract water. It is also safe for sensitive skin. Health experts recommend: Just mix one teaspoon of honey with one teaspoon of olive oil and one-half teaspoon of lemon juice. Apply to hands, elbows, heels of your foot, among others, and wash off after 15 minutes.\nYou can also use honey as mouthwash by mixing one tablespoon of honey with a cup of warm water. Honey reportedly cleans teeth and dentures, and kills germs in the mouth. Also, honey works well on chapped lips and for acne because it has antibacterial properties.\nHoney may also be effective in the treatment of ulcers. In Europe, honey has been used internally to help cure ulcers, particularly stomach ulcers. Experts recommend taking 1-2 teaspoons of honey on an empty stomach (at least half an hour before meals). This should be done one to four times a day “to assist in healing and provide pain relief.” Before doing so, however, be sure you have no other ailments.\nBurns, too, heal better with honey, some studies show. The advantage of honey is that it not only prevents infections from occurring, it actually accelerates skin healing. Since the sugar in honey absorbs water it helps to trap some of the moisture so that the bacteria and other microbes can’t grow as easily as in other food.\nFrom ‘Honey: The Gourmet Medicine’ comes this information: “Honey provides an important part of the energy needed by the body for blood formation. In addition, it helps in cleansing the blood. It has some positive effects in regulating and facilitating blood circulation. It also functions as a protection against capillary problems and arteriosclerosis.”\nHere’s a word of caution: Due to the natural presence of some spores which cannot be destroyed by their digestive tracts, children under one year of age should not be given honey. Infants, on the other hand, can contract botulism from honey.\nHoney produced from some flowers may cause honey intoxication. Symptoms include dizziness, weakness, excessive perspiration, nausea and vomiting. Less commonly, low blood pressure, shock, heart rhythm irregularities and convulsions may occur, with rare cases resulting in death. Honey intoxication is more likely when using “natural” unprocessed honey and honey from farmers who may have a small number of hives. Commercial processing, with pooling of honey from numerous sources generally dilutes any toxins.', '- What temperature kills botulism spores?\n- How do I know if my home canned food has botulism?\n- Does all honey contain botulism?\n- Where is botulism found?\n- Who is most at risk for botulism?\n- Can you survive botulism?\n- Can botulism survive alcohol?\n- How long does it take for botulism to grow in canned food?\n- Does salt kill botulism?\n- Can botulism spores be killed by boiling?\n- Does microwaving kill botulism?\n- Can you see botulism?\n- Does botulism die when exposed to air?\n- How long before you know you have botulism?\n- How can you tell if food has botulism?\n- Can botulism grow in vinegar?\n- Does botulism die when cooked?\n- Does boiling tomatoes kill botulism?\n- Does frying bacon kill botulism?\nWhat temperature kills botulism spores?\nTo kill the spores of Cl.\nbotulinum a sterilisation process equivalent to 121°C for 3 min is required.\nThe botulinum toxin itself is inactivated (denatured) rapidly at temperatures greater than 80°C ..\nHow do I know if my home canned food has botulism?\nHome-canned and store-bought food might be contaminated with toxin or other harmful germs if:the container is leaking, bulging, or swollen;the container looks damaged, cracked, or abnormal;the container spurts liquid or foam when opened; or.the food is discolored, moldy, or smells bad.\nDoes all honey contain botulism?\nHoney is one of the most common sources of botulism. About 20 percent of botulism cases involve honey or corn syrup. One 2018 study looked at 240 multifloral honey samples from Poland. The researchers found that 2.1 percent of the samples contained the bacteria responsible for producing the botulinum neurotoxin.\nWhere is botulism found?\nCauses and types of botulism Clostridium botulinum bacteria are found in soil, dust and river or sea sediments. The bacteria themselves aren’t harmful, but they can produce highly poisonous toxins when deprived of oxygen, such as in closed cans or bottles, stagnant soil or mud, or occasionally, the human body.\nWho is most at risk for botulism?\nWho is at risk for botulism? Everyone is at risk for foodborne botulism, especially those who eat home-canned, low-acid foods. Drug users, especially those who use black-tar heroin, may be at risk of wound botulism. Infants younger than 12 months who are fed honey are at risk of infant botulism.\nCan you survive botulism?\nAlthough botulism can cause severe and prolonged symptoms, most people recover completely from the illness. Early treatment reduces the risk of permanent disability and death. However, even with treatment botulism can be fatal. Without treatment, more than 50% of people with botulism would die.\nCan botulism survive alcohol?\nThe alcohol in your drink won’t destroy the toxin (make it harmless). The only way to be sure you don’t get botulism from pruno is to not drink it.\nHow long does it take for botulism to grow in canned food?\nThe onset of botulism is usually 18 to 36 hours after eating the contaminated food, although it can be as soon as four hours and as long as eight days.\nDoes salt kill botulism?\nA concentration of about 10% salt will effectively prevent germination of Botulism spores in your canned food. … Botulinum in low-acid foods is through pressure canning. Pressure canning allows you to achieve temperatures high enough to kill the spores in a smaller amount of time.\nCan botulism spores be killed by boiling?\nThough spores of C. botulinum are heat-resistant, the toxin produced by bacteria growing out of the spores under anaerobic conditions is destroyed by boiling (for example, at internal temperature greater than 85 °C for 5 minutes or longer).\nDoes microwaving kill botulism?\nbotulinum, and anti-toxin is not useful for prevention. Heating to high temperatures will kill the spores. … The toxin is heat-labile though and can be destroyed at > 185°F after five minutes or longer, or at > 176°F for 10 minutes or longer.\nCan you see botulism?\nYou cannot see, smell, or taste botulinum toxin – but taking even a small taste of food containing this toxin can be deadly. Click on the following tips for details on how to protect yourself and the people you feed. When it doubt, throw it out!\nDoes botulism die when exposed to air?\nNaturally acquired botulism is contracted by ingesting food that has been contaminated with the spores; however, the spores can be aerosolized for use as a potential weapon. The toxin is absorbed readily by gastrointestinal or respiratory epithelium. … Simple exposure to open air can deactivate toxin within 12 hours.\nHow long before you know you have botulism?\nIn foodborne botulism, symptoms generally begin 18 to 36 hours after eating a contaminated food. If you or someone you know has symptoms of botulism, immediately see your doctor or go to the emergency room.\nHow can you tell if food has botulism?\nBotulism symptoms usually begin about 18 to 36 hours after eating contaminated food. Signs of botulism include weak muscles, drooping eyelids, and double vision. In rare cases, you may also experience nausea and vomiting.\nCan botulism grow in vinegar?\nCathy also pointed out that vinegar-pickled vegetables are also not likely to host the botulism bacterium. Because pickled vegetables are covered in an acidified brine, the process creates a high enough acidity to prevent the risk of botulism.\nDoes botulism die when cooked?\nDespite its extreme potency, botulinum toxin is easily destroyed. Heating to an internal temperature of 85°C for at least 5 minutes will decontaminate affected food or drink.\nDoes boiling tomatoes kill botulism?\nNew Mexico says, “To prevent the risk of botulism, chiles and other low-acid and tomato foods not canned according to the 1994 USDA recommendations in this guide should be boiled even if no signs of spoilage are detected. … Boiling destroys botulism toxin. If in doubt, always boil foods before tasting.”\nDoes frying bacon kill botulism?\nThe main ingredient in a cure, salt, functions primarily to kill bacteria and thus act as a preservative. … But because bacon is fried before eating, botulism isn’t an issue, so the use of curing salt is considered optional.']	['<urn:uuid:d91d1cfd-d963-4ca4-bffc-d37c84160813>', '<urn:uuid:fbccfe76-dac1-4b05-91c7-1547f2304064>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:38:36.516973	10	66	1999
