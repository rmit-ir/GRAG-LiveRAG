qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	water efficiency increase crop yields irrigation	Both sprinkler and micro-irrigation systems significantly improve water efficiency and crop yields compared to traditional irrigation methods. Sprinkler systems can achieve water savings of 30-50% for crops like wheat and groundnut, with yield increases of 20-25%. Meanwhile, drip irrigation systems are even more efficient, operating at 90-95% efficiency compared to traditional irrigation's 40-50% efficiency. The improvements come from reducing water losses through evaporation, conveyance, and distribution, as both systems deliver water more precisely to the crop's root zone. This efficient water use not only conserves resources but also enables farmers to irrigate previously unirrigated areas with the saved water.	['Systems of pressurised irrigation, sprinkler or drip, can improve water efficiency and contribute substantially to improved food production. Sprinkler irrigation is a type of pressurised irrigation that consists of applying water to the soil surface using mechanical and hydraulic devices that simulate natural rainfall (see Figure 1). These devices replenish the water consumed by crops or provide water required for softening the soil to make it workable for agricultural activities. The goal of irrigation is to supply each plant with just the right amount of water it needs. Sprinkler irrigation is a method by which water is distributed from overhead by high-pressure sprinklers, sprays or guns mounted on risers or moving platforms. Today a variety of sprinkler systems ranging from simple hand-move to large self-propelled systems are used worldwide. Global use of sprinkler irrigation is: the Americas (13.3 million hectares (Mha)), Europe (10.1 Mha), Asia (6.8 Mha), Africa (1.9 Mha), and Oceania (0.9 Mha) (Kulkarni et al, 2006).\nA sprinkler irrigation system typically consists of:\ni) A pump unit which takes water from the source and provides pressure for delivery into the pipe system. The pump must be set to supply water at an adequate pressure so that the water is applied at rate and volume adequate to the crop and soil types\nii) Main pipes and secondary pipes which deliver water from the pump to the laterals. In some cases these pipelines are permanently installed on the soil surface or buried below ground. In other cases they are temporary, and can be moved from field to field. The main pipe materials used include asbestos cement, plastic or aluminium alloy\niii) The laterals deliver water from the pipes to the sprinklers. They can be permanent but more often they are portable and made of aluminium alloy or plastic so that they can be moved easily\niv) Sprinklers, water-emitting devices which convert the water jet into droplets. The distribution of sprinklers should be arranged so as to wet the soil surface in the plot as evenly as possible.\nA wide range of sprinkler systems is available for small and large-scale application. Set systems operate with sprinklers in a fixed position. These sprinklers can be moved to water different areas of the field, either by hand or with machinery. Hand-move systems are more labour intensive and may be more suited where labour is available and cheap. On the other hand, mechanically operated systems require a greater capital investment in equipment. Mobile systems minimise labour inputs by operating with motorised laterals or sprinklers, which irrigate and move continuously at the same time (Savva and Frenken, 2002).\nSprinkler irrigation efficiency is highly dependent on climatic conditions. FAO (1982) proposed the figures of farm irrigation efficiencies provided in Table 1 on the basis of climate.\nTable 1: Farm irrigation efficiencies for Sprinkler Irrigation in different climates (the overall efficiency comprises conveyance efficiency, field canal efficiency, and field application efficiency)\n|Climate/Temperature||Farm irrigation efficiency|\nSource: adapted from FAO (1982)\nSprinkler irrigation technology can support farmers to adapt to climate change by making more efficient use of their water supply. This is particularly appropriate where there is (or is expected to be) limited or irregular water supply for agricultural use. The sprinkler technology uses less water than irrigation by gravity, and provides a more even application of water to the cultivated plot. Additionally, sprinkler irrigation can reduce the risk of crops freezing due to colder than usual temperatures. More frequent and intense frosts are already impacting on crops as a result of climate change. During the night, the motion of the sprinklers and the application of rain-like water droplets can reduce the stress on crops caused by a sharp decrease in temperature (Snyder and Melo-Abreu, 2005).\nOne of the main advantages of the sprinkler irrigation technology is more efficient use of water for irrigation in agriculture. Sprinkler systems eliminate water conveyance channels, thereby reducing water loss. Water is also distributed more evenly across crops helping to avoid wastage. The sprinkler irrigation system has also been shown to increased crop yields (Table 2) and is suited for most row, field and tree crops that are grown closely together, such as cereals, pulses, wheat, sugarcane, groundnut, cotton, vegetables, fruits, flowers, spices and condiments (Narayanmoorthy, no date) and for cultivating paddy crop (Kundu et al, 1998).\nTable 2: Response of different crops to Sprinkler Irrigation Systems\n|Water saving %||Yield increase %|\nSource: adapted from INCID (1998)\nSprinkler irrigation technology is well adapted to a range of topographies and is suitable in all types of soil, except heavy clay. Sprinkler systems can be installed as either permanent or mobile fixtures. Sprinklers provide a more even application of water to agricultural land, promoting steady crop growth. Likewise, soluble fertilisers can be channelled through the system for easy and even application. The risk of soil erosion can be reduced because the sprinkler system limits soil disturbance, which can occur when using irrigation by gravity. In addition, sprinkler irrigation can provide additional protection for plants against freezing at low temperatures. Secondary benefits from improved crop productivity include income generation, employment opportunities and food security.\nThe main disadvantages associated with sprinkler systems are related to climatic conditions, water resources and cost. Even moderate winds can seriously reduce the effectiveness of sprinkler systems by altering the distribution pattern of the water droplets. Likewise, when operating under high temperatures, water can evaporate at a fast rate reducing the effectiveness of the irrigation. Although sprinkler irrigation can help farmers to use water resources more efficiently, this technology relies on a clean source of water and therefore may not be suited to areas where rainfall is becoming less predictable. Implementation costs are higher than that of gravity-fed irrigation systems and large labour force is needed to move pipes and sprinklers in a non-permanent system. In some places such labour may not be available and may also be costly. Mechanised sprinkler irrigation systems have a relatively high energy demand (Savva and Frenken, 2002).\nThe cost of installing a sprinkler system suitable for a family production unit ranges from US$ 600 to US$ 2500 per hectare, depending on the type of materials used and the amount of labour contributed by rural producers. Affordable Micro Irrigation Technologies (AMITs) are low cost and low pressure systems with the same technical advantages as conventional micro-irrigation system, however the technology is packaged and marketed as kits suitable for small fields (25 m2 to 4000 m2). The AMIT has the specific advantage of being affordable, and easy to understand; they also have rapid pay back, divisibility and expandability.\nWhen planning to install a sprinkler irrigation system, information should be obtained regarding the following key factors:\n- The crop or crops to be cultivated and their water requirements throughout the growing season\n- The shape and size of the field. This will determine the range of suitable technologies, investment and labour requirements\n- Topography, in particular the location and elevation of the water source relative to the field, land slopes and uniformity\n- The water source. The source of irrigation water can be surface water, groundwater or non-conventional water (such as desalinated water and treated wastewater) (Savva and Frenken, 2002). Water must be available in sufficient quantity from a locally accessible source. A clean supply of water free of sediment is required to avoid blockage in sprinkler nozzles and crop spoilage (FAO, 1988)\n- Available labour force. Where skilled labourers are not available on location, local farmers will require training to install, maintain and repair the various components of the sprinkler system\n- The soil profile. Sprinkler irrigation technology is best suited to soils with high infiltration rates so that ponding and surface runoff can be avoided. The application rate of the sprinkler system must therefore be matched to the infiltration rate of the most restrictive soil in the field.\n- Energy requirements of different systems, including the manufacturing, transportation and installation of the various systems. The location of the water source will also affect the need for energy for pumping (Savva and Frenken, 2002)\n- Social aspects such as local preferences, capacity to maintain the system, implications for labour requirements and how these may affect different members of the community (Savva and Frenken, 2002)\n- An understanding of existing health risks is crucial to avoid schemes that may promote water borne diseases (Savva and Frenken, 2002)\n- An environmental impact assessment should be conducted to fully understand potential impacts of drainage and diverting water resources, amongst others (Savva and Frenken, 2002).\nMaintenance of the system mainly relates to regular cleaning of the component parts. Seals on pipes and sprinkler nozzles should be checked to avoid water seepage. During periods when the equipment is not being used, it is recommended to store component parts in a cool, dark place.\nAccording to Savva and Frenken (2002), a whole range of institutional conditions must be understood before sprinkler irrigation technology selection can be made. These include land tenure issues, water rights, and financial incentives by government and taxation. Large-scale irrigation schemes will usually form part of national policy and could be harnessed to support national employment initiatives. Where the sprinkler irrigation type is not available nationally, foreign imports or government-supported stimulation of national manufacture will be required alongside investment in training for design, installation and maintenance. Coordination with public or private authorities in charge of water management will be crucial and could be facilitated through the establishment of a committee of irrigation users. At a local level, social organisation for the participatory monitoring of water resources and quality could provide a key monitoring tool. Whichever method is selected, developing regulations for the distribution and allocation of water would provide an important mechanism for conflict resolution.\nWhether a large or small-scale intervention, farmer involvement in the development stages of a sprinkler irrigation project is recommended to help ensure social acceptance and technical success (Box 1).\nBox 1: Sprinkler irrigation in Zimbabwe\n“The Hama Mavhaire irrigation scheme in Zimbabwe is a 96 hectare drag-hose sprinkler irrigation project. The scheme is apportioned equally to 96 farmers, of which 70 per cent are women. It is located in a dry agro-ecological area that receives about 450 mm of rainfall per year. Dryland cropping fails 3 to 4 years out of 5. The development of the scheme was initiated in 1989, following strong farmer requests to the government for irrigation development.\nParticipation of Farmers in Planning and Design\nThe government dispatched a team of experts, comprising engineers, agronomists and economists, to the project site to carry out a feasibility study. Several meetings were held in order for planners to understand the farmers’ expectations and to explain to the farmers the potential of and requirements for the proposed development. This was followed by a baseline socio-economic survey. The land chosen consisted of about 80 per cent of non-cultivated bush, while the remaining 20 per cent was arable land owned by the farmers who were selected for the scheme. The farmer group was to be the partner in irrigation development. It elected its own committee, which was tasked with liaising with the planners on all matters related to the new development.\nTo facilitate a process of making informed decisions, arrangements were made for farmers to visit different types of irrigation systems, surface and sprinkler. This exposure proved useful to farmers when they eventually decided on the type of irrigation system they preferred and the crops to be grown. This process took one full year.\nParticipation of Farmers in Construction\nWhen the design was adopted, tender documents were written to include the condition that the farmers would provide all unskilled labour required for construction. During construction the group provided labour for trenching and back-filling and assisted pipe fitters by carrying and placing pipes and fittings in position. As a result of their participation, the farmers were trained in pipefitting and other general repairs to their system. Additionally, the contractor trained one farmer per irrigation block on the repair of sprinklers. The irrigation engineers and extension staff trained the farmers in leadership, bookkeeping, scheme operation, improved agronomic practices and irrigation scheduling. This process took six months for the first 48 hectares and three months for the remaining 48 hectares.\nSocio-economic Impact of Scheme Development\nOn average, the net income per plot-holder quadrupled due to the introduction of irrigation, from a gross margin assessed at US$ 650 annually on 2.5 hectares of dryland crop production to a gross margin of US$ 2,775 for one hectare irrigated. There are other indicators of a substantial rise in the standard of living of the irrigators. About 29 per cent of the plot-holders are reported to have purchased between one and four head of cattle from the income earned through irrigation during the first five to six years of scheme operation. In addition, 13 per cent of the plot-holders had put up brick under corrugated iron houses and 10% had installed solar panels during the same period. Women, who constitute the majority of the plot-holders and are represented at all committees, also confirmed that the other major benefit of irrigation was that they are able to pay for the costs of educating their children.\nThe success of the Hama Mavhaire irrigation scheme is largely attributed to the participatory approaches adopted for the development of the scheme provided the opportunity to the group, planners and implementers to jointly plan and implement a scheme, making it both technically feasible and socially acceptable.”Source: Savva and Frenken, 2002\nPossible barriers to implementation include lack of access to finance for the purchase of equipment, lack of local skills for design, installation and maintenance of the system and lack of nationally/locally available component parts. A low level of public awareness of or concern for the importance of sustainable water management and use could also be a barrier to the exploration of sprinkler irrigation technology as a climate change adaptation option.\nSprinkler irrigation requires a suitable source of fresh water to be identified in close enough proximity to the farmland. This ensures that costs are kept at a reasonable level. Water availability will be highly dependent not only on current resources but also on future climate conditions. Where knowledge of potential climate change impacts on water resources does not exist, installing a sprinkler irrigation system could lead to conflicts over local water use.\nSprinkler irrigation is a versatile technology suitable for application in a wide range of contexts, can be implemented at small or large scale and with either low-cost or more sophisticated components. This technology can be employed in conjunction with other adaptation measures such as the establishment of water user boards, multi-cropping and fertiliser management.\nFAO (1982) Mechanised sprinkler irrigation. FAO Irrigation and Drainage Paper No. 35. Rome.\nFAO (1988) Irrigation Water Management: Irrigation methods, FAO. Rome.\nINCID (Indian National Committee on Irrigation and Drainage) ( 1998) Sprinkler Irrigation in India. INCID, New Delhi.\nKulkarni, S.A., F.B. Reinders and F. Ligetvari (2006) Global Scenario of Sprinkler in Micro-Irrigated Areas. Sept 10 – 16 2006, PWTC, Kuala Lumpur 7th International Micro Irrigation Congress\nKundu, D. K., H. U. Neue, R. Singh (1998) Comparative Effects of Flooding and Sprinkler Irrigation on Growth and Mineral Composition of Rice in an Alfisol. Proceedings of the National Seminar on Micro- Irrigation Research in India: Status and Perspective for the 21st Century. Bhubaneswar, July 27-28.\nNarayanmoorthy, A. (no date) Drip and Sprinkler Irrigation in India: Benefits, Potential and Future Directions. Available: http://www.iwmi.cgiar.org/Publications/Other/PDF/Paper%2015%20of%20NRLP%...\nSavva, A. P. and K. Frenken (2002) Irrigation Manual Planning, Development Monitoring and Evaluation of Irrigated Agriculture with Farmer Participation. Volume I Modules 1 – 6.\nSnyder, R. L. and J. P. Melo-Abreu (2005) Frost protection: fundamentals, practice, and economics – Volume 1. FAO, Rome.', 'माइक्रो सिंचाई प्रणाली से सिंचाई क्रांति\nIndia is predominantly an agricultural country and even with current orientation towards services, still agriculture contributes ¼th of total GDP of the country, 15 percent of total export and 65 % of total population’s livelihood. After independence, India has made remarkable progress in increasing food production and productivity, credit goes to concerted efforts made under various Agri revolutions. For agriculture Land and Water are two most important resources. Of which, water (irrigation) becomes lifeline of agriculture. It is a truth in agriculture “if we fail in irrigation, we will fail in agriculture”.\nWater is required for agriculture as well as for other sectors (Domestic, Industries, etc) and the demand of water is increasing alarmingly. At present level, agriculture consumes over 80 per cent of total water consumption in India. The country is endowed with many perennial and seasonal rivers. The river system which constitute 71 per cent of water resources is concentrated in 36 % of geographic area. Most of agricultural fields are irrigated by use of underground water for assured irrigation, however, erratic, mansoon based rainfall is source for water for rainfed agriculture. Though water is a renewable resource, the recharge is ultimately limited to rain. Drought like situation in Indian agriculture is more common and occurs frequently in some of the part of vast geography of the country almost every year. Excessive and unbalanced use of water became a common practice to grow more & more to earn more & more. In other words, the water resources are being depleted by current practice of farming and we will be devoid of sufficient irrigation water if the trend continues in years to come. All these factors are focusing the need of judicious and efficient use of water for gricultural use.\nVarious type of flood method of irrigation is commonly and traditionally followed in almost whole India. This system offers liable to loss of water conveyance, distribution and evaporation. Therefore, about 30-40 % of applied water is being utilized by the crop rest is leached out; evaporated, or lost through surface run off.\nMicro Irrigation System is panacea in irrigation related problems. In this technology, field is irrigated in the close vicinity of root zone of crop. It reduces water loss occurring through evaporation, conveyance and distribution. Therefore high water use efficiency can be achieved (Table 1). The unirrigated rainfed crop area, could be irrigated with the water saved with this technology become a potential source of food production for the benefit of country’s food security.\nTable 1: Irrigation efficiency under different methods of irrigation\n|Methods of Irrigation|\n|Conveyance efficiency||40-50 (canal)\n|Surface water moisture evaporation||30-40||30-40||20-25|\nMicro-irrigation system is the best available way to utilize water and fertilizer efficiently under farm conditions. The type of Microirrigation system may very with the type of crop selected and amount of water available for irrigation (Table 2). However, modern technology was developed in Israel. Since MIS is a well planned and scientifically designed way of farming, it also provides option for Crop diversification. Unlike surface irrigation, drip irrigation is more suitable and economical if it is introduced in water scarce areas having undulated topography, shallow and sandy soils barren and for wide spaced high value crops. It reduces cost of cultivation, increases productivity and reduces energy (electricity) consumption.\nTable 2: Crop group wise advisable Micro Irrigation System\n|Crop||Crop Spacing||Adjustable Micro Irrigation System|\n|Horticulture Crop||12 m to 3 m between crop raw. (wide spaced)||Drip Irrigation System / Pours Pipe|\n|Crops fruit part under ground like Potato, Groundnut, Turmeric, Ginger, Vegetables, Medicinal Crops etc.||Less then 1 m between crop raw. (Narrow)||Drip Irrigation / Sprinkler Irrigation / Raingun|\n|Field Crops like Cotton, Castor, Tobacco, Pulses, Sugarcane, Banana, Vegetables etc.||Less then 3 m between two crops||Drip Irrigation|\n|Fodder Crops / Nursery Raising of Vegetables, Ornamental Crops etc.||-||Sprinkler Irrigation / Raingun|\nThe advantages of drip irrigation are:\n- Minimized fertilizer/nutrient loss due to localized application and reduced leaching.\n- High water application efficiency.\n- Leveling of the field not necessary.\n- Ability to irrigate irregular shaped fields.\n- Allows safe use of recycled water.\n- Moisture within the root zone can be maintained at field capacity.\n- Soil type plays less important role in frequency of irrigation.\n- Minimized soil erosion.\n- Highly uniform distribution of water i.e., controlled by output of each nozzle.\n- Lower labour cost.\n- Variation in supply can be regulated by regulating the valves and drippers.\n- Fertigation can easily be included with minimal waste of fertilizers.\n- Foliage remains dry thus reducing the risk of disease.\n- Usually operated at lower pressure than other types of pressurised irrigation, reducing energy costs.\nCrop-wise water saving over surface irrigation method and increase in yield is presented in table (Table 3).\nTable 3: Crop-wise water saving and increase in yield\n|Drip||% Increase||Surface||Drip||% Saving|\nEven having many benefits the reach of MIS among the farmers restricted. Though, the government is trying to promote the technology through part financial support to offset its high initial cost syndrome. Few adoptions were observed in the decade of eighties and nineties (Table 4). Putting all together efforts of all machineries under one, total coverage of land under MIS is less than 1 per cent, which underlines the need of integrated efforts to be made by all stake holders. The rural electrification is another major constraint for the popularization of drip systems among farmers. The high care as well as meager crop and soil specific technology are few major constraints for deeper reach of the technology among farmers.\nTable 4 : Decade wise development in the field of Micro-irrigation\n|Decade||Focus of Research/ extension|\n|Seventies||Comparisons of micro irrigation system with conventional systems in terms of water savings and yield enhancements.|\n|Eighties||Estimate water requirements, modifications of crop geometry and use of mulches in drip irrigated fields for realizing the potential benefits of the system|\n|Nineties||Develop hardware and software for cost reduction, design modifications and fertigation and chemigation|\n|Twenty first century||Precision farming, including the use and application of software and more efficient instruments in agriculture besides the use of simulation and modeling of moisture and nutrients movement under different soil and dripper characteristics|\nPromotion of adoption of Micro Irrigation System in India\n- Concerted efforts taken by the Government / NGO and the MIS companies for widespread awareness about the usefulness of the wonderful technology.\n- Efforts should be made to ensure the production and supply of good quality microirrigation system to the farmers by enforcing strict quality control measures.\n- Microirrigation should be made an integral part of all irrigation projects.\n- The microirrigation system manufacturers should also guide the farmers in adopting suitable agronomic practices along with microirrigation.\n- After sales service should be strengthen.\n- Technological intervention is required to cut down the cost of Micro-irrigation system,\nDr. Sarvesh Kumar Shah\nH-12, Komal Enclave, Shantivan, Paldi, Ahmedabad-380 007']	['<urn:uuid:937d4deb-a6e2-471c-b5a4-b6b79d000177>', '<urn:uuid:cd6fb782-9286-4f9e-8f6c-f1d39387db97>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:32:05.666290	6	99	3754
2	I've been cutting paper at work and noticed the blades get dull quickly. What's causing this wear, and what's the best way to maintain sharp blades like we do with garden tools?	Cutting coated stock, board, or plastic material will dull a blade more rapidly than cutting standard uncoated stock and fine paper - the more abrasive the material, the faster you need to change the blade. To maintain sharpness, like with garden tools, there's a three-step maintenance process: clean, sharpen, and lubricate. For paper cutter blades specifically, you should change them after cutting approximately 5,000 lifts or if you notice signs of dullness like rough edges or increased cutting pressure. Regular sharpening actually extends blade life - if you let a blade become extremely dull, up to 1/4 inch must be removed to restore the edge, whereas only about 1/32 inch is needed when sharpened at first signs of dullness.	"[""- Author: Hannah Meyer\n- Editor: Cindy Fake\nSadly, there are a lot of dull, rusted tools out there, even on productive farms. If you watch the videos at the end of this post, you will hear “Rust is always a sign of neglect” so let this be the end of neglect. Put your tools “to bed for the winter,” or prepare them for pruning season, right around the corner.\nThere are three main parts of tool maintenance: clean, sharpen, and lubricate. These three steps should simply be done, in that order, every single time a tool is put back in the shed. The more often you do it, the easier and more effective it is at improving the lifespan of your tools.\nClean: There are three steps - cleaning off debris, removing rust, and sanitizing to prevent the transfer of disease. Proper cleaning may require removing screws and partially dismantling the tool.\nClean – Pressurized water, or a wire brush and a little soapy water are effective when used to scrub off all dirt and debris from your tool.\nRust? – Sometimes, especially if your tools have not been constantly maintained properly, you may see a rusted tool that just isn't what it used to be. Don't worry, if you have this problem, there are some easy tips that can help take that rust off. Spray the tool with vinegar, wrap in a paper towel and cover in plastic for about three hours, up to 24 hours. Remove the paper towel and plastic. Use a brush, an old toothbrush works just fine on small projects, put some baking soda in water and use the brush to scrub off the rust. Turpentine and steel wool also work well. After you scrub the tool to remove the rust, rinse thoroughly with water.\nSanitize – To ensure your tools are not going to spread disease around your farm, sanitation is important. Wipe down the tool surface with a 10% bleach solution (10 parts water to 1 part bleach), leave it for 30 seconds and then rinse thoroughly with water. Be sure to dry and oil your tool after sanitizing to be sure it does not rust from the bleach! Learn how below.\nSharpen: Many tools, even though you may not think of them as having a blade, actually require frequent sharpening to ensure their consistent function. A shovel, for instance, needs sharpening on the edge, which helps cut through roots, make clean holes, among other things. The basic method of sharpening is pretty simple. Use caution when sharpening and be sure to wear the proper protective equipment, such as gloves and eye protection.\nStabilize – Small tools, such as hand pruning shears, may easily be held firmly with the non-dominant hand. Larger tools, such as mower blades, or loppers, may need to be held in a vise.\nSharpen - Locate the proper edges to sharpen. Remember that there is a cutting blade and a bypass edge on some tools like pruning shears. You only need to sharpen the blade. A file can be used and should only be pushed in one direction. Hold the file at an angle, usually 45 degrees to the blade. Be sure to lubricate the blade and properly tighten screws after sharpening.\nIf you would like to pay a professional to sharpen your tools, at our January 9th Farmer-to-Farmer Breakfast, Ruben with Sharpening Tech will available to sharpen tools. Watch for an announcement on the Foothill Farming Calendar.\nLubricate: It is very important to oil your tools, even if they do not need to be sharpened. Oil helps keep the rust at bay. If your tool comes in contact with food crops or soil, we recommend a plant-based oil, like linseed oil for wood handles, and vegetable oil for pruners and shovels. This should happen each time your tools are put away. A barrel or bucket with sand and a small amount of oil left near your tool shed will work. Simply stabbing the tool in and out of the sand mixture can remove debris and oil the tool at the same time.\nStore your tools standing upright or hanging, this also helps prevent rust. Develop a system to maintain your tools and always have them in their correct place so that anyone working in your operation can find the right tool anytime. Livestock Advisor Dan Macon has developed a system of sharpening his tools and oiling all the handles on New Year's Eve each year. He also prepared a lambing box, and outfits it with sufficient supplies and freshly sharpened tools each year before lambing season. What systems do you use, or need to use, to be prepared in your operation?\nHow to Easily Clean Rusted Gardening Tools – Organically! – Learn to use vinegar and baking soda to remove old rust from your hand tools. https://www.youtube.com/watch?v=jtdDotcHnl4\nSharpening Tools – Pruners, Loppers, Shovels and More! – GrowOrganic.com – Tips to properly clean, sharpen, and store your hand tools to improve their life and performance. https://www.youtube.com/watch?v=yn8npWqkCa8\nHow to Replace a Shovel Handle - Wranglerstar.com – A detailed, step by step demonstration on how to properly replace a broken shovel handle with a new one. https://www.youtube.com/watch?v=j5UH0Y4KurY"", ""May 21, 2019 [Updated on December 12, 2019]\nThe best results in paper cutting (as in anything else) begins with selection of the right equipment. This is especially true with the guillotine knife in a industrial paper cutting machine. An inferior quality paper cutter blade inaccurately ground with an incorrect bevel may not only result in a poor quality product being produced, but could increase costs and even damage a commercial paper cutter.\nSelection of a paper cutter blade is critical. Always buy from a reputable manufacturer or supplier, one who is aware of the latest developments in paper cutter blade sharpening and has enough experience to provide your shop with support.\nBlades for guillotine paper cutters are available in three major types. (While some manufactures have developed other variations of paper knives, they are beyond the scope of this article).\nThe difference in paper cutting knives is in the type of steel used in the inlays. While standard steel knives have been and will continue to be an acceptable type of knife, the popularity of high-speed steel knives have grown due to their improved performance, both in the quality of cut and increased life between grindings.\nHigh-speed knives cost about twice as much as standard knives, but as they can last 2-3 times longer between sharpening. The additional cost can easily be offset by the reduction in knife change time and knife sharpening costs.\nCarbide knives provide the best quality edge and last even longer than high speed steel knives, but they are the most expensive (often 3-4 times the cost of high-speed knives) and require special grinding equipment to sharpen (also 3-4 times the cost of sharpening high-speed knives).\nThey are also very susceptible to chipping and due to their high cost, are only recommended for operations that are consistently cutting the same type of material such as fine paper, where the chances of damage from material is minimal. Carbide knives should NEVER be used when cutting any type of recycled material.\nKnife manufacturers vary only slightly in their recommendations for a standard bevel (24° to 25°). There are times when a different bevel or even a double bevel is advisable, depending on the material to be cut.\nThe following chart illustrates some of the different bevels and will serve as a guide for obtaining the correct bevel for a particular material. Knife suppliers and machine manufactures all have information available to assist in determining the correct bevel to use with hard-to–cut materials.\n|Material to be cut||Paper Knife||a||b||h (mm)||Pressure||false clamp plate|\n|Bible paper||HSS, HM, UFK*||24°||1500-2000||false clamp plate|\n|Double waxed papers||HSS, HM, UFK||24°||3200||**|\n|Printing papers, regular||HSS, HM, UFK||24°||2500||false clamp plate|\n|Duplex papers||HSS, UFK||24°||26°||2,0||3000-3500||**|\n|Flimsy||HSS, HM, UFK||19°||3000-4000||false clamp plate|\n|Label papers||HSS, HM, UFK||24°||3500-4000||**|\n|Felt-cardboard||HSS, HM, UFK||24°||2000-2500||false clamp plate|\n|Photographic papers||HSS, HM, UFK||24°||2500-3000||**|\n|Gummed papers||HSS, HM, UFK||24°||2500-3500||**|\n|Carbonizing papers||HSS, HM, UFK||19°||400||false clamp plate|\n|Carbon papers||HSS, HM, UFK||22°||800-1000||**|\n|Art papers||HSS, HM, UFK||23°||25°||3,5||3000-4000||**|\n|Plastic fiber paper||HSS, UFK||26°||2500-3000|\n|Blotting paper||HSS, HM, UFK||19°||2000-2500||**|\n|Metallic papers||HSS, HM, UFK||24°||3000-3500||**|\n|Stencil duplicator paper||HSS, HM, UFK||19°||3000||false clamp plate|\n|Writing papers||HSS, HM, UFK||24°||2500-3000|\n|Tissue paper||HSS, HM, UFK||19°||2000||false clamp plate|\n|Autocopying paper||HSS, HM, UFK||24°||800-1000||**|\n|Transparent papers||HSS, HM, UFK||24°||3000-3800||**|\n|Velours papers||HSS, HM, UFK||19°||2500||**|\n|* HSS = high-speed steel knives\nHM = carbide-tipped knives (normal: 23/25°)\nUFK = ultra-finest grain hard metal (normal 22/25°)\n|** Compensation of differences in height is necessary|\nSource: Polar-Mohr® “Cutting in Practice”\nDownload Polar Mohr's knife bevel guide showing in detail the recommended knife angle for every type of material to be cut, from paper and cardboard to plastics and packing materials.\nAfter selecting a good quality knife, it is equally important that the knife be kept sharp at all times. A dull paper knife is an unnecessary waste, especially in a competitive market. It must push its way through the cutting material and when dull, requires up to three to four times more force than a sharp paper knife. A dull knife increases energy usage, wears the paper cutter prematurely and drives up other production costs due to inferior work.\nA lift cut with a dull knife has a rough, burry, at times “sealed” edge. This can be quite expensive, as it creates separating problems in subsequent operations like feeding on a folder.\nHow do you know when your paper cutter knife needs sharpening? It’s the responsibility of the paper cutter operator to know when a paper knife change is necessary—so he or she must be able to recognize the signs of a dull knife.\nA general reference is to change the blade:\nDespite these guidelines. it may be necessary to change the blade if the knife appears dull.\nSigns of a dull paper cutter knife include:\nAny of these signs should indicate to the experienced paper cutter operator that a knife change is in order. Cutting coated stock, board or plastic material will dull a blade more rapidly than cutting standard uncoated stock and other fine paper. The more abrasive the nature of the material being cut, the faster a knife change becomes necessary.\nSome job problems are related to inaccuracies in loading the paper or settings on the paper cutting machine. See our Commercial Paper Cutting Troubleshooting Guide.\nFrequent knife grinding will materially lengthen the life of the knife, saving you money in the long run.\nThe magnified cross section of a paper blade in Figure 2 shows that only about 1/32” of the metal is removed in order to re-establish a keen cutting edge—as long as it’s done when sharpening first becomes dull.\nIf the paper knife is permitted to become extremely dull, which occurs at an accelerated rate once the sharp edge is worn off, a great deal more (up to ¼”) must often be removed to restore a keen edge.\nIf the paper knife becomes very dull, you can be charged for the additional time required to restore its edge. If you permit the paper cutter knife to become extremely dull, only a few sharpenings can be made before the knife becomes too short for further use. You are literally grinding your knife investment away.\nThe cost of changing knives can be seen in this light too:\nLet’s assume a 45” paper cutter costs about $120.00 per hour to operate. It takes approximately 10 minutes to make a complete knife change cycle, so a knife change in this scenario would cost about $20.00. A 26” x 40” folder has an hourly cost of about the same as a paper cutter. Misfeeding the folder (resulting from “sealed” or burred edges caused by a dull knife) could cost more than a knife change.\nJust one bad feeder jam-up, resulting from misfeeding or feeding doubles, could certainly take more time to clear up than the cost of a knife change and a number of small jam-ups will, of course, have the same result. If you are cutting paper to feed a press, this situation becomes even more costly.\nAny way you figure, it pays to keep a sharp blade in your paper cutter.\nThe procedure for changing knives will vary depending on the make and model of the cutting machine. But no matter the cutter, the majority of operator accidents happen during the knife-changing operation. It’s extremely important that each operator develops good safety habits, follows the manufacture's procedures and be provided with a safe machine.\nThe knife on a 45” paper cutter weighs about forty pounds. If it slips and catches a hand or a finger between it and the table, no imagination is needed to figure out what will happen.\nModern paper cutters have safety devices built into them to reduce this danger. Even so, carelessness will negate even the best safety devices.\nFollowing these blade sharpening safety recommendations should reduce the risk of accidents, regardless of the age and make of the paper cutting machine:\nRemember, keep handling of the unprotected knife to a minimum. By insisting that your operators follow the correct procedures in caring for your paper cutter blades, you will minimize danger to the operator, save time and money and increase the quality of your work.\nSee our guide to Commercial Paper Cutter Safety: Standards and Accident Prevention for more safety tips.\nNo matter your level of experience with paper knives, reaching out to a pro can help. Contact us with any of your knife grinding or handling questions and we’ll get you in touch with the experts at Tri-State Knife Grinding. They’ve spent 65 years perfecting paper knife sharpening and use state-of-the-art sharpening and honing machines operated by only the most experienced knife grinders. It’s how they deliver the highest quality precision grinding and professional services every time.\nAsk about sharpening services or purchasing paper knives, slitters, sheeter blades, trimmers, cutting sticks, saw blades and paper drills.\nIf you permit the paper cutter blade to become extremely dull, only a few sharpenings can be made before the knife becomes too short for further use.\nShare this news story:""]"	['<urn:uuid:70234dfe-0fe8-460d-80f1-12b3ccbdbbff>', '<urn:uuid:5f1b5e80-66b0-4a5c-a200-cb74aa9a13ff>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:32:05.666290	32	119	2347
3	How do nonprofits show they're trustworthy with donations?	Nonprofits demonstrate trustworthiness through transparency and ethical practices. Charity Navigator rates organizations based on financial health, transparency, and accountability. They must follow ethical standards like honest communications, respecting donor privacy, fulfilling donor expectations, and avoiding lavish spending. However, administrative expense ratios alone don't indicate effectiveness - GiveWell's analysis shows that the best charities often have higher operational costs while achieving better results.	['By Norman B. Gildin\nIn his book “Lessons in Leadership,” Lord Rabbi Jonathan Sacks, touches on the matter of desecrating and sanctifying God’s name.\nIn Judaism Rabbi Sacks asserts, “When we behave in such a way to evoke admiration for Judaism as a faith and a way of life that is a kiddush Hashem, a sanctification of God’s name. When we do the opposite – when we betray that faith and way of life, causing people to have contempt for the God of Israel – that is a chillul Hashem.” Clearly, this has great application to fundraisers who need to abide by ethical principles when raising funds for their nonprofit organization.\nIn 2013, Helen La. Oyekanmi, PhD, wrote a paper entitled “Ethical Issues in Fund Raising and Biblical Response” giving the Christian point of view on fundraising. She discovered that a large number of Christian fundraisers had no formal or informal training as to how fundraising should be ethically approached. She quotes Matthew 6:1-4 that stresses that “charity should be handled with care and discretion and with a sense of privacy. God rewards every good deed done in secret and with a focus on Him.”\nRegardless of the religion you practice, it would seem that the same principles apply. Anyone involved in raising funds, has an obligation to act ethically in this noble work. When representing any nonprofit organization, how one performs in public will dictate how you are perceived by the world.\nI am a member of the Association of Fundraising Professionals (AFP), the trade association representing thousands of professional fundraisers. We must adhere to the national code of ethics promulgated by the AFP. For example, code #21 states that professional fundraisers:\n“…not accept compensation or enter into a contract that is based on a percentage of contributions; nor shall members accept finder’s fees or contingent fees.” Unfortunately, there are nonprofits that do not follow this rule.\nCharity Navigator objectively rates charitable organizations to trust and support. Inherent in the nonprofit’s financial health, transparency and accountability is a principal mandate to meet ethical standards. On its website Charity Navigator states:\n“Our ratings show givers how efficiently we believe a charity will use their support today, how well it has sustained its programs and services over time and their level of commitment to good governance, best practices and openness with information.”\nGood ethical conduct is, therefore, a condition in achieving Charity Navigator’s mission. Objective professional evaluators of charities mirror the same approach.\nI have compiled a short, albeit inexhaustible, list of ethical markers that fundraisers and organizations they represent should consider as part of an ethical fundraising approach. Some do’s and don’ts:\n- Commissions by fundraisers based on a percentage of what they raise are an inappropriate and unethical form of compensation. As a donor, I don’t want to find out that a sizable chunk of my donation went to the fundraiser. Appropriate forms of compensation include salary, project allowances and even bonuses for high level performance.\n- Do what you say you are going to do. Meet or exceed donor expectations and set high standards.\n- Be honest in your endeavors including all communications with the public.\n- Always follow the rule of law, respect donor privacy and intent, as well as acknowledge donor gifts in a timely fashion.\n- Run campaigns that are real and not “of the moment.” Some organizations run specious campaigns contending these will establish a program or a building. Time elapses and no program or buildings appear. “Of the moment” campaigns ruin credibility and future prospects for support.\n- Commemorative dedications are a sensitive topic. Some organizations remove plaques or naming gifts from buildings without seeking permission from the donors or families. Unless the donor did not fulfill the pledge, removing a commemoration is unethical and inappropriate.\n- Fundraising expenses should, at most, not exceed 35% of revenue collected. This yardstick assures that the majority of funds raised are used for their intended purpose. Obviously, the less spent on fundraising expense and the more allocated to services the better.\n- Don’t spend money lavishly for transportation, personal amenities or even gifts or premiums for donors. It’s simply unethical and wrong.\nThere are, of course, many more benchmarks we can cite. But, these are minimum expectations. The National Council of Nonprofit’s indicates:\n“Transparency inspires confidence. Beyond what the law requires, nonprofits can demonstrate their commitment to ethical practices by being entirely transparent with financial information and fundraising practices.”\nSo, I ask you as a fundraiser, or as a nonprofit organization raising funds, or anyone involved in raising funds that has an obligation to act ethically, are you interested in sanctifying God’s name?\nAbout the author: Norman B. Gildin has fundraised for nonprofits for more than three decades and has raised upwards of $93 million in the process. He lived in Teaneck for 34 years and now resides in Boynton Beach, Florida and currently is the President of his own company Strategic Fundraising Group. He can be reached at email@example.com.', 'Why Ranking Charities by Administrative Expenses is a Bad Idea\nHow does one know whether a charitable donation will make an impact? For this we need a simple formula (easy to write, hard to apply):\nIdea X Implementation = Bang for your buck\nWhen I give talks about aid effectiveness, people often comment that they too think this is important. And to make sure they are supporting good charities, they always hone in on the charities’ finances to see how much goes to administrative and fundraising expenses. Charity Navigator, for example, scrubs these numbers and doles out stars to charities that don’t spend “too” much on operations.\nGiven the title of my book with Jacob Appel, More Than Good Intentions, many assume that they are speaking my language, and that I admire such focus on those numbers too.\nBut I do not. Those numbers do not tell you what is really happening.\nFor assessments of organizations, I like a website called Givewell.org. I may not agree with every assessment they make, but they are the best I’m aware of at doing this. And I’m not sure I could do any better. It is always easier to criticize than to improve. Givewell scrubs for all sorts of information, both publicly available and also through direct communications with charities. They look to understand what the charity does, and how much room they have for growth. Last but not least, they assess whether the idea of what the organization does has been shown to be effective, i.e., the first part of the above formula. (Disclosure: Givewell uses research from Innovations for Poverty Action extensively to gather such evidence, so naturally I agree with many of their conclusions, but we have no formal relationship. Givewell is considering Innovations for Poverty Action for a recommendation.)\nSo for ratings of organizations—not just ideas—as far as I’m aware, Givewell comes closest to assessing the above formula. When people ask me where to send money to maximize its impact—aside from my beloved IPA of course—this is where I usually send them.\nThis past week we did a simple exercise. We looked up the fundraising and administrative expenses of each of the U.S. 501(c)3 charities that Givewell rated. We calculated whether those rated better by Givewell had lower administrative and fundraising expenses. They did not. In fact, the opposite is true. The higher rated charities by Givewell have higher administrative and fundraising expenses.\nThe mean of the fundraising expense ratio for charities (the money spent on fundraising divided by the total expenses of the entire organization) that earned a gold, silver, or notable rating by Givewell (41 charities) is 0.073, while that of the charities reviewed by Givewell but not ranked well (253 charities) is 0.054 (p-value of 0.05, i.e., statistically significant at the 95% level). The mean of the administrative expense ratio for charities that earned a gold, silver, or notable rating by Givewell is 0.102, while that of the charities reviewed by Givewell but not ranked well is 0.092 (p-value of 0.35). Adding administrative and fundraising together, the mean ratio for charities that earned a gold, silver, or notable rating by Givewell is 0.174, while that of the charities reviewed by Givewell but not ranked well is 0.147 (p-value of 0.11).\nSee below for three charts which show the distributions (the red line (“recommended” in some way) is consistently to the right of the blue line (“reviewed but not recommended”).\nDoes this mean ignore expenses entirely? No, I’d say that huge outliers—charities whose expenses are much higher than their peers’ doing similar work—should be approached with caution. But within the reasonable expense band that most charities are in, I simply ignore these numbers. There are many reasons why. To touch on one: they are really easy to manipulate. Just Google “Feed the Children” and see the accounting shenanigans behind gifts-in-kind (amongst other nasty stuff). In short, overvalue the gifts-in-kind you receive and voilà, your program services just got bigger, holding everything else constant (and the corporation gets a bigger tax write-off too). Garbage in, garbage out. For more on this, check out Uncharitable by Dan Pallotta.\nWhat sources do Freakonomics readers like to use to guide their decisions?']	['<urn:uuid:fd86c540-9f60-4481-92d2-37bd522803a0>', '<urn:uuid:5505d703-0d7c-45f0-af7f-058b06850d4b>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:32:05.666290	8	62	1535
4	minimum pressure safety nozzle blow gun	When using a blow gun without a built-in safety feature, the pressure should be reduced to 2 bar or 30 PSI gauge pressure.	['BCAS latest advice on working safely with compressed air\nTake the pressure. One of the common challenges that many health and safety managers face is that users can become complacent and overlook the fact most compressed air is pressurised to at least six times atmospheric pressure – and it is this pressurised energy that can create a hazard if not managed properly.\nIn addition, the noise created by unsilenced compressed air when exhausting to the atmosphere also needs careful consideration to ensure it remains with legal limits.\nAvoid contact. While health and safety practices continue to improve continually and employees understand their obligations to creating a safe working environment, intentional or accidental misuse of compressed air can occur.\nCompressed air injection into the body at any pressure can cause serious injury, or even death, so it is always worth reminding users not to use compressed air to clean off dust or debris from their clothing or skin and to avoid dislodging debris that could cause bodily harm.\nJoint responsibility. The Health and Safety at Work act states that an employee must take reasonable care and follow all procedures laid down by his/her employer to create a safe working environment for everyone.\nTo ensure this is achieved, every employee, through the Provision and Use of Work Equipment Regulations should receive training in order to use compressed air equipment in the correct manner.\nAdditionally, it is important that the appropriate personal protective equipment (PPE) is provided to all using compressed air tools, that it is worn correctly and fits properly. Depending on the application, this can include safety goggles/glasses, ear defenders, gloves and face masks.\nEquipment types and hazards. There is a wide range of handheld pneumatic equipment and each type has its own associated hazards. These fall into two groups; where the compressed air drives a motor, e.g. grinders, sanders, nut runners or where the compressed air is used directly through a nozzle, e.g. blow guns, spray guns.\nThese air powered tools give rise to a number of hazards requiring various items of personal protective equipment to be worn to help protect from noise and vibration, sparks and dust.\nSometimes, the connection between the hose and tool or the hose clamp can become loose, through wear of tampering. Attention should also be paid to quick-release couplings, which may not be pushed home properly or have worked loose through wear,\nTo help mitigate against this risk, users should check both the hose itself and its connection with the tool for signs of wear and tear before use.\nBlow gun safety. The end of a blow gun must never be blocked, nor pointed at anyone and should not be used for general cleaning purposes, such as cleaning clothing, especially while being worn, or benches. Vacuum cleaning is a far safer option.\nOne way to help minimise the risk of injury is to only use blow guns which have some form of safety feature, either a safety-pattern nozzle or a safety mechanism which cuts in if the nozzle becomes blocked. If the blow gun not have a built-in safety feature, then reduce the pressure to 2 bar or 30 PSI gauge pressure. And ensure that safety goggles or glasses are worn at all times.\nBCAS training. BCAS offers a dedicated ‘Working Safely with Compressed Air’ course via its new e-learning portal. The course covers the employer’s and employee’s responsibility for safety in the workplace and outlines why air users must know how to work safely and understand the risks involved if good practice is not followed.\nFor further information about the full range of BCAS training courses on offer, including classroom-based training, please visit the e-learning portal or call 0207 935 2464.']	['<urn:uuid:f6baa3b2-2504-4e44-a92c-135f5fef8d8b>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	6	23	618
5	ideal tilt angle and orientation solar panels australia maximum electricity generation	For maximum electricity generation in Australia, the ideal orientation for solar panels is due north, and the ideal tilt angle should match the angle of your latitude (for example, about 30 degrees in Sydney and Perth). However, small variations from these ideal angles will not significantly impact the power output of your solar energy system.	['Solar panel tilt angle and orientation are two of the most important factors in determining how much electricity your solar panel array will generate. If you live in Australia and have a grid-connected solar system, your solar panels will be installed at the angle and aspect of your roof. In most cases – provided you have an unshaded, north or west-facing roof that is has some incline – this is the most economical approach, and your solar power yield will not be significantly less than if the panels were installed at the ‘perfect’ angle and orientation. But what should you do if you have a flat roof? Is it ever worth it to have your solar panels installed horizontally?\nTo tilt or not to tilt: Solar panels on a flat roof\nInstalling your solar panels at the ideal tilt angle and orientation for your latitude ensures that your system generates as much electricity as possible for your location. The ideal orientation for a solar panel array is due north, and the ideal tilt angle is the angle of your latitude (e.g. about 30 degrees in Sydney and Perth). Small variations away from these ideals will not result in a significant difference in the power output of your solar energy system, which is why the natural tilt angle of your roof is usually fine (and keeping that tilt frames can add significantly to the cost of your system, putting a damper on the value of the solar power it produces).\nSolar panels installed on a nearly flat roof at Downfields Engineering in Toowoomba.\nSolar panels installed horizontally on a roof at the St George Hotel in St George, QLD.\nIn the past, panel manufacturers would not offer warranties on panels installed at an angle lower than 2 degrees, but these days most of the top manufacturers will give warranties even if their panels are installed at 0 degrees (completely flat).\nHowever, industry best practice in Australia is to ensure that panels are at least slightly tilted – so that the panels can better ‘self clean’ when it rains. It’s also worth noting that many roofs naturally have some degree of tilt – aluminium roofs, for example, usually have a pitch of about 3-4 degrees. Concrete or membrane roofs are the most common roofs that are truly horizontal – in which cases a tilt or ballast system is frequently used to give the panels at least a 10-degree pitch.\nIf the roof is truly flat, there are a limited number of situations in which installing your panels horizontally (or nearly horizontally) might be worth it for you. These are:\n- If the building on which you are installing the panels is located in the tropics – under about 23 degrees of latitude. This is because the sun’s movements through the tropical sky mean that the sun may actually be to the south of your roof during the summer months (late December – late March in the southern hemisphere). By contrast, the midday sun never goes south of a system located above 23 degrees latitude. If you are in the tropics, the closer to the equator that your system is, the more time the sun will spend ‘behind’ your roof. If your panels are laid horizontally (or close to it), the more of this winter sun you’ll be able to catch. (That being said, a slight pitch is still preferable to avoid dust accumulation.)\n- If you have limited roof space. Even if you’re not in the tropics, it may still make sense to install your panels horizontally if your roof is flat as opposed to paying more for tilt frames. Interestingly, the efficiency reduction in laying your panels flat in Sydney (instead of north-facing at a 33-degree angle, which would be ideal) is about 10-12%, while installing tilt frames could increase the cost of your system by about the same percentage. Make sure you do the maths before you make a decision – the higher your latitude (for example, if you live in Tasmania), the more likely tilt frames will be worth your while.\nA note about dust accumulation on flat solar panel arrays\nOne other thing to keep in mind if your panels are horizontal is that they will not self-clean as effectively as panels at a tilt – dust has a higher chance of accumulating and impeding electricity production. The drop in performance due to dust accumulation on panels is roughly between 5% and 10%. On the other hand, solar panel arrays tilted at an angle – even a slight angle – are more likely to be washed clean when it rains.\nCompare solar & battery storage prices instantly: Complete our Quote Comparison request form on the right of this page\n© 2016 Solar Choice Pty Ltd']	['<urn:uuid:71e171d1-f6ca-442c-8455-871d198b5da5>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T10:32:05.666290	11	55	797
6	compare dem digital elevation model and dsm digital surface model elevation differences	Digital Elevation Model (DEM) is a numerical representation of the Earth's surface containing height points representing topography, while Digital Surface Model (DSM) represents the elevations of both the ground and all features on it, including vegetation and man-made structures. The key difference is that DSM represents the highest elevation points - following forest canopy and building roofs in developed areas - while showing the Earth's surface only in open areas. DSM is often created using automatic extraction algorithms and can be visualized as laying a blanket over the terrain, capturing all surface objects. Both models are used for analyzing terrain and surface properties.	"['Digital Elevation Model (DEM) is a numerical representation of the Earth’s surface that contains actual height points representing the topography, as well as the method to calculate elevations between the height points. Typically, DEM is stored in a data system as a regular grid or a triangulated irregular network (TIN). In a discussion related to DEMs, common terms are Digital Terrain Model (DTM) and Digital Surface Model (DSM).\nDigital Terrain Model (DTM) is a model that represents the Earth’s surface together with other topographic information, such as data about land cover, slopes, and aspects of the terrain. The most important element of DTM is DEM.\nDigital Surface Model (DSM) is a concept that has become common due to widespread use of airborne laser scanning. It means a model that represents the highest elevation of the terrain. Thus, DSM represents the Earth’s surface only in open areas, while in other regions the model follows the forest canopy and the roofs of buildings.\nDigital Elevation Model (DEM) is an invention that has liberated gathering and storing of elevation data from the principles of traditional cartography. Before the diffusion of using DEMs, country-wide elevation data was stored on the contour lines in printing plates and paper maps. While contours are still a valid method for visualising topography, from the perspective of data storage, they have two deficiencies:\n- Contours are a non-continuous representation of the terrain, in which the surface forms between the selected contour interval are unknown.\n- Generation of contours is conditioned by visualisation, which enforces the use of cartographic generalisation rules. In this process, some details of the topography are removed, while other forms are intentionally over-emphasised.\nNowadays, since airborne laser scanning has become general in DEM production, storage and visualisation of elevation data are truly separated for the first time in the history of cartography.\nFigure 1. Glaciofluvial ice-marginal formation and littoral deposits at the First Salpausselkä, Lohja, as seen on the (a) topographic map, (b) DEM25, and (c) DEM2 by the National Land Survey of Finland.\nFigure 2. Ravines on clay deposits in Siuntio, as seen on the (a) topographic map, (b) DEM25, and (c) DEM2 by the National Land Survey of Finland.\nDEMs are considered as very significant geospatial datasets due to the versatile possibilities for using them. They are used for ortho-rectification of aerial photographs, cartographic representations, 3D visualisations, geomorphological, biogeographical, hydrological, and hydraulic analyses and models, water management, analysis of landscape dynamics, climate and climate impact studies, geological applications, agriculture and forestry applications, palaeogeographical mapping, visibility analyses, road and dam planning, cut-and-fill analysis, automatic drainage basin delineation, flood risk analysis, planning of telecommunication networks, and geophysical modelling, among many other uses and applications.\nDEMs available in Finland are DEM2 (2 m grid) based on airborne laser scanning, and DEM10 (10 m grid), DEM25 (25 m grid), and DEM200 (200 m grid) based on contours and other elevation related data of the Topographic database all created by the National Land Survey of Finland. The new generation country-wide DEMs are typically produced by using airborne laser scanning, but on a global scale, DEMs based on map data will still be used concurrently for the foreseeable future. However, the quality and level of detail in DEMs based on airborne laser scanning are superior compared with the previous generation DEMs.\nFigure 3. Terminal moraines in Siikala, Karkkila, as seen on the (a) topographic map, (b) DEM25, and (c) DEM2 by the National Land Survey of Finland.\nFigure 4. The bog Keihässuo, Loppi, as seen on the (a) topographic map, (b) DEM25, and (c) DEM2 by the National Land Survey of Finland. The typical circular structure of bog hollows is visible only in the most detailed model based on airborne laser scanning, DEM2 (c).\nMore about DEMs\nDEM error and error propagation in terrain analysis - http://urn.fi/URN:ISBN:952-10-3350-9\nDEM2 by the National Land Survey of Finland\nDEM10 by the National Land Survey of Finland\nGlobal ETOPO1 (topography and bathymetry) - http://www.ngdc.noaa.gov/mgg/global/global.html\nGlobal ASTER GDEM Version 2\nGlobal GTOPO30 - http://www1.gsi.go.jp/geowww/globalmap-gsi/gtopo30/gtopo30.html\nNear-Global SRTM (Shuttle Radar Topography Mission) DEM - http://eros.usgs.gov/#/Find_Data/Products_and_Data_Available/SRTM', ""Request a quote for\nNeed a Proposal or Have a Question? Please fill out the form below\nand one of our aerial experts will be in touch immediately.\nA rugged, built to last, professional multispectral sensor. It captures five discrete spectral bands, and is one of the most flexible solutions on the market.\nFor RedEdge-MX equipped drone packages, visit our Micasense Page\nNormalized Difference Vegetation Index\nAs plants become healthier, the intensity of reflectance increases in the NIR and decreases in the Red, which is the physical basis for most vegetation indices. NDVI values can be a maximum of 1, with lower values indicating lower plant vigor. Therefore, 0.5 typically indicates low vigor whereas 0.9 indicates very high vigor. NDVI is also effective for distinguishing vegetation from soil. NDVI is recommended when looking for differences in above-ground biomass in time or across space. NDVI is most effective at portraying variation in canopy density during early and mid development stages but tends to lose sensitivity at high levels of canopy density.\nThis layer is a color composite and not an Index. It is referred to as a Color Infrared Composite because instead of combining Red, Green, and Blue bands (which is the standard image display method you are accustomed to) we are combining NIR, Red, and Green bands. NIR light is displayed as red, red light is displayed as green, and green light is displayed as blue (R: NIR, G: RED, B: GREEN). This color composite highlights the response of the Near-infrared band to crop health and water bodies.\nHealthy vegetation reflects a high level of NIR and appears red in CIR layers. Unhealthy vegetation will reflect less in the NIR and appear as washed out pink tones, very sick or dormant vegetation is often green or tan, and man-made structures are light blue-green. Soils may also appear light blue, green, or tan depending on how sandy it is, with sandiest soil appearing light tan and clay soils as dark tan or bluish green. This is also highly useful in identifying water bodies in the imagery, which absorb NIR wavelengths and appear black when water is clear. Since this is not an index, as stated above, there is no color palette to select. The colors you see are a result of additive mixture of NIR, Red, and Green wavelengths at each image pixel.\nNormalized Difference Red Edge\nNDRE is an index that can only be formulated when the Red edge band is available in a sensor. It is sensitive to chlorophyll content in leaves (how green a leaf appears), variability in leaf area, and soil background effects. High values of NDRE represent higher levels of leaf chlorophyll content than lower values. Soil typically has the lowest values, unhealthy plants have intermediate values, and healthy plants have the highest values. Consider using NDRE if you are interested in mapping variability in fertilizer requirements or foliar Nitrogen, not necessarily Nitrogen availability in the soil.\nChlorophyll has maximum absorption in the red waveband and therefore red light does not penetrate very far past a few leaf layers. On the other hand, light in the green and red-edge edge can penetrate a leaf much more deeply than blue or red light so a pure red-edge waveband will be more sensitive to medium to high levels of chlorophyll content, and hence leaf nitrogen, than a broad waveband that encompasses blue light, red light, or a mixture of visible and NIR light (e.g. a modified single-imager camera).\nNDRE is a better indicator of vegetation health/vigor than NDVI for mid to late season crops that have accumulated high levels of chlorophyll in their leaves because red-edge light is more translucent to leaves than red light and so it is less likely to be completely absorbed by a canopy. It is more suitable than NDVI for intensive management applications throughout the growing season because NDVI often loses sensitivity after plants accumulate a critical level of leaf cover or chlorophyll content.\nThe Chlorophyll Map is a layer that is less sensitive to leaf area than NDRE. This layer isolates the chlorophyll signal from variability in leaf area as a function of changes in canopy cover. It has a physiological basis which takes into account the relationship between canopy cover and canopy nutrient content.\nThe Chlorophyll Map is especially sensitive to well gathered and well calibrated data. Non-plant pixels are excluded and shown as transparent, which in some cases results in plant pixels also being omitted. This layer is less useful for row crops and more useful for vineyards and orchards, as the dense canopy is better at differentiating the Chlorophyll signal.\nDigital surface Model\nDSM is a digital model representation of a terrain's surface. DSM represents the elevations above sea level of the ground and all features on it. A DSM is a gridded array of elevations. It is a layer symbolized by a gray color ramp, special effects such as hill-shading may be used to simulate relief. DSMs can be used to study surface properties and water flow.\nA digital surface model (DSM) is usually constructed using automatic extraction algorithms (i.e. image correlation in stereo photogrammetry). DSM resembles laying a blanket on your imagery. It represents top faces of all objects on the terrain, including vegetation and man-made features, and highlights the different elevations of the features.""]"	['<urn:uuid:a28a0496-d7cf-4fb2-8cff-8894655ce718>', '<urn:uuid:ae2f70f6-d07c-4075-89c8-484d66917803>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T10:32:05.666290	12	103	1566
7	Studying magnetic fields - what's unique about Uranus' magnetic setup?	Uranus has a strong magnetic field that is remarkably offset, with the magnetic axis tilted 59 degrees from the rotational axis and displaced from the planet's center by 30% of its radius. This unusual configuration was discovered by Voyager 2 and likely results from molten ice and lopsided rock lumps created by an ancient collision with an Earth-sized object.	"['Uranus was hit by a massive object roughly twice the size of Earth that caused the planet to tilt and could explain its freezing temperatures, according to new research.\nAstronomers at Durham University, UK, led an international team of experts to investigate how Uranus came to be tilted on its side and what consequences a giant impact would have had on the planet’s evolution.\nThe team ran the first high-resolution computer simulations of different massive collisions with the ice giant to try to work out how the planet evolved.\nThe research confirms a previous study which said that Uranus’ tilted position was caused by a collision with a massive object — most likely a young proto-planet made of rock and ice — during the formation of the solar system about 4 billion years ago.\nThe simulations also suggested that debris from the impactor could form a thin shell near the edge of the planet’s ice layer and trap the heat emanating from Uranus’ core. The trapping of this internal heat could in part help explain Uranus’ extremely cold temperature of the planet’s outer atmosphere (-216 degrees Celsius, -357 degrees Fahrenheit), the researchers said.\nThe findings are published in The Astrophysical Journal.\nLead author Jacob Kegerreis, PhD researcher in Durham University’s Institute for Computational Cosmology, said: “Uranus spins on its side, with its axis pointing almost at right angles to those of all the other planets in the solar system. This was almost certainly caused by a giant impact, but we know very little about how this actually happened and how else such a violent event affected the planet.\n“We ran more than 50 different impact scenarios using a high-powered super computer to see if we could recreate the conditions that shaped the planet’s evolution.\n“Our findings confirm that the most likely outcome was that the young Uranus was involved in a cataclysmic collision with an object twice the mass of Earth, if not larger, knocking it on to its side and setting in process the events that helped create the planet we see today.”\nThere has been a question mark over how Uranus managed to retain its atmosphere when a violent collision might have been expected to send it hurtling into space.\nAccording to the simulations, this can most likely be explained by the impact object striking a grazing blow on the planet. The collision was strong enough to affect Uranus’ tilt, but the planet was able to retain the majority of its atmosphere.\nThe research could also help explain the formation of Uranus’ rings and moons, with the simulations suggesting the impact could jettison rock and ice into orbit around the planet. This rock and ice could have then clumped together to form the planet’s inner satellites and perhaps altered the rotation of any pre-existing moons already orbiting Uranus.\nThe simulations show that the impact could have created molten ice and lopsided lumps of rock inside the planet. This could help explain Uranus’ tilted and off-centre magnetic field.\nUranus is similar to the most common type of exoplanets — planets found outside of our solar system — and the researchers hope their findings will help explain how these planets evolved and understand more about their chemical composition.\nCo-author Dr Luis Teodoro, of the BAER/NASA Ames Research Center, said: “All the evidence points to giant impacts being frequent during planet formation, and with this kind of research we are now gaining more insight into their effect on potentially habitable exoplanets.”\nThe research was funded by the Science and Technology Facilities Council, The Royal Society, NASA and Los Alamos National Laboratory.\nMaterials provided by Durham University. Note: Content may be edited for style and length.', '|Grand Tour of the Outer Planets|\nJanuary 24, 1986, marks an auspicious occasion, for it was then that Voyager 2 flew within 50,600 miles of the cloud tops of Uranus, thus becoming the only spacecraft to have visited this planet.\nImage of Uranus at a distance of 60,000 miles, taken by Voyager 2 about 21 minutes before its closest approach\nIt takes Uranus, the third largest planet, by density, 84 years to orbit the Sun, and Voyager 2 measured a day on Uranus at 17 hours, 14 minutes.\nUnlike the other planets in the solar system, Uranus is tipped on its side so that, during its orbit of the Sun, Uranus\'s north and south poles are alternately exposed to sunlight. Voyager discovered that the hemisphere exposed to sunlight emitted large amounts of ultraviolet light, or ""dayglow."" Surprisingly, Voyager measured temperatures at the poles and the equator to be similar, although the polar regions receive direct solar heat and there is no other significant internal heat.\nMagnetic field and axis of rotation of Uranus\nEarth\'s magnetic field is oriented at 12 degrees from the rotational axis. Before Voyager 2 explored Uranus, there was no direct evidence of its magnetic field. However, Voyager 2 discovered that not only does Uranus have a strong magnetic field, but that the magnetic axis is offset by 59 degrees from the rotational axis and is tilted from the center of the planet by 30% of its radius, so that the magnetic poles are closer to the equator. The orientation of Uranus causes its magnetotail to be wound like a corkscrew behind the planet. Furthermore, from the orientation, it appears that the magnetic field is generated somewhere in the middle of the interior of Uranus, where there is sufficient pressure for water to conduct electricity.\nAnimation of Uranus\'s magnetosphere. Courtesy of Ralph McNutt.\nUranus\'s atmosphere is composed mainly of hydrogen and helium. Some previous studies had suggested that the helium component might be as much as 40%, so it came as a surprise when Voyager determined that it was only about 15%. About 2% of the atmosphere is made up of methane, which absorbs red light; hence Uranus appears blue-green.\nLeft image is a composite using blue, green, and orange filters, showing Uranus as we would see it, with a clear atmosphere. The right image exaggerates the contrasts, by using ultraviolet, violet, and orange filters. The dark polar hood is over Uranus\'s south pole.\nBefore the Voyager encounter, we knew that Uranus had at least five moons: Miranda, Ariel, Umbriel, Titania, and Oberon. Voyager obtained high-resolution images of all these moons and, in addition, discovered 10 new moons, the largest of which was named Puck.\nMontage of Voyager 2 images showing the five original moons, which, traveling counterclockwise from the left, are Ariel, Miranda, Titania, Oberon, and Umbriel.\nThe smallest, innermost, and certainly the strangest of the 5 large moons is Miranda. Miranda\'s diameter is about 300 miles, only 1/7th the size of our Moon, yet it contains fault canyons 12 miles deep and cliffs up to 12½ miles high. Researchers were amazed to discover this much geologic activity on a moon whose temperature is only -355 degrees Fahrenheit. They surmise that the gravitational pull from Uranus may have caused some form of tidal heating.\nClose-up view of Miranda, from 19,000 miles, showing fractures, grooves, and craters\nUmbriel has the oldest, darkest surface, covered with craters and showing little evidence of tectonic activity. Oberon\'s surface is similar. Titania, on the other hand, has huge fault systems and canyons, possibly caused by subsurface water\'s expanding while the interior of the moon froze. Ariel has the brightest, youngest surface of the five original moons. Icy material appears to have flowed through its fault valleys, and it seems that the older, deeper craters have been destroyed, leaving only smaller, younger ones.\ni. Voyager 2 image of Uranus\' satellite Oberon from 663,000 km. The dark spot to the right of center is the 206 km diameter crater Hamlet, centered at 46 S, 44E. Oberon, the most distant major satellite from Uranus has a diameter of 1523 km. North is at 11:30.\nVoyager 2 measured and imaged all nine of the previously known rings of Uranus. In addition, Voyager discovered a new ring and a system of dust rings that is only visible when backlit by the Sun. It appears the rings are younger than Uranus and were possibly formed by the disintegration of a moon hit at high velocity or destroyed by the effects of gravity.\nThis image of Uranus\'s rings was taken while Voyager 2 was in the shadow of the planet.\nPage created by Center for Visual Computing']"	['<urn:uuid:53fc9369-08b4-4385-b141-d5db1299e78e>', '<urn:uuid:e861c43e-c3e2-4d2f-b108-a738f18301ef>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T10:32:05.666290	10	59	1393
8	scientific concerns unregulated arctic fishing	Over 2,000 scientists from 67 countries have signed an open letter calling for a precautionary moratorium on commercial fishing in the central Arctic Ocean's international waters. They argue this moratorium should remain until there's better understanding of fish species present, their populations, and sustainable management practices. This concern is supported by research showing that Canada, the USA, and Russia significantly underreported their Arctic fish catches between 1950 and 2006, taking 75 times more fish than reported to the UN's Food and Agriculture Organization.	['Our overseas correspondent Anthony Speca says NTI’s statement on virgin international fisheries in the Arctic Ocean raises a question about Inuit resistance to involving non-arctic states in arctic economic governance.\nThough few noticed at the time, on April 25 Nunavut Tunngavik Incorporated (NTI) seems to have suggested that aboriginal peoples should have preferential access to commercial fisheries in the international waters of the Arctic “beyond the Exclusive Economic Zones [EEZs] of the Arctic States.” This would be surprising, since it’s internationally acknowledged that international waters beyond any state’s EEZ are the common property of humanity. No one, whether aboriginal or non-aboriginal, adjacent or distant, has preferential access to such waters under international law.\nOften lost in the hype about the Arctic “scramble for resources” is the fact that a large part of the Arctic Ocean can’t be claimed by anyone, including even the arctic states. According to the United Nations Convention on the Law of the Sea, no state has exclusive rights to the open ocean beyond 200 nautical miles (370 kilometres) from its shorelines, which is the limit of its EEZ. Some of the seabed beyond this limit can be claimed, provided it’s a natural extension of the continental shelf. But the waters beyond it—often called the “high seas”—belong to the international community.\nThis means that there is a huge “donut hole” of high sea in the central Arctic Ocean, a bit bigger in area than the Mediterranean or Caribbean Sea. The EEZs of the arctic coastal states surround it, but it lies beyond them. All members of the international community have an equal right to exploit the waters of the donut hole, which are rapidly beginning to open up in summer as the permanent pack ice disappears. And those waters might just be full of commercially valuable fish—a potential bounty over which NTI believes Inuit and other arctic aboriginal peoples should have “preferential consideration” as permanent inhabitants in the Arctic living adjacent to those waters.\nBut NTI isn’t the only group with an interest. Alarmed by the prospect of an unregulated international fishing derby in the arctic donut hole, over 2,000 scientists from 67 countries have recently signed an open letter calling for a precautionary moratorium on commercial fishing there. They believe this moratorium should remain in place at least until it’s better understood what kinds of fish swim in the central Arctic Ocean, how many of them there are, and how to manage them sustainably. Considering that researchers from the University of British Columbia concluded just last year that Canada, the USA and—especially—Russia took 75 times more fish out of arctic waters between 1950 and 2006 than they reported to the UN’s Food and Agriculture Organization (FAO), a moratorium might seem only prudent.\nAnd the worry isn’t just overfishing by the arctic states. The Pew Environmental Group, which sponsored the scientists’ open letter through its Oceans North International campaign, points out that the donut hole is significantly closer to Chinese ports than the Southern Ocean around Antarctica, where Chinese vessels already fish. It’s also within reach of the ports of other major distant-water fishing states, such as Japan, South Korea, and Spain. According to the FAO, the world’s fishing states have either fully exploited or overexploited about 80% of global fish stocks already.\nIt would require multilateral action on the part of the international community, particularly states with fleets capable of reaching the central Arctic Ocean, to close the donut hole. That isn’t to say it can’t be done—but in the past it’s been like letting the fox guard the henhouse. In 1993, China, Japan, Poland, Russia, South Korea and the USA jointly applied a moratorium on the pollock fishery in a similar donut hole in the Aleutian Basin, but only after they’d completely demolished the once-massive pollock stock through overfishing. The moratorium remains in force, as the stock still hasn’t recovered.\nThe EU, one of the world’s biggest markets for arctic fish, has already come out in favour of a moratorium on new arctic fisheries until a robust fisheries-management regime can be set up. It’s far from clear, however, that others in the international community share this view. In 2011, a senior researcher from South Korea’s government-run Korea Maritime Institute proclaimed that “Arctic fisheries can become the center of world fisheries in the near future”, and he went on to extol their potential not only to meet Korea’s high demand for fish in the face of declining stocks elsewhere, but also to rescue the Korean fishing industry from its financial troubles.\nLittle wonder, then, that NTI might want to stake an early claim to the arctic donut hole. After all, NTI has found it repeatedly necessary to remind the Government of Canada to respect Inuit rights to preferential access to arctic fisheries within Canada’s own EEZ, even though these rights are guaranteed under the Nunavut Land Claims Agreement. And with its experience co-managing the allocation of fishing quotas for Baffin Bay and Davis Strait, NTI is aware of what it could lose to international interests.\nBefore 1977, much of the waters of Baffin Bay and Davis Strait lay beyond Canada’s EEZ, which then legally extended only to 12 nautical miles (22 kilometres) from shore. These waters hosted a commercial stock of roundnose grenadier, which unregulated fishing in the 1960s and 1970s, mostly by Soviet and German fleets, may have helped eliminate. Today, the roundnose grenadier is considered endangered in Canada, and the Baffin Bay and Davis Strait fisheries are limited to turbot and shrimp.\nSo does NTI support the call for a precautionary moratorium on fishing in the arctic donut hole? Nunatsiaq News reported that it categorically does not—but though NTI never corrected the report, it in fact neither opposed nor supported a moratorium explicitly in its April 25 statement. But either way, how could NTI, or even Canadian Inuit as a whole, expect to have any serious influence on the management of fisheries they’ve never traditionally exploited, and over which Canada has no authority?\nNormally, fisheries on the high seas are governed by dedicated international organizations, which conduct scientific surveys of fish stocks and regulate total allowable catches. An example is the Northwest Atlantic Fisheries Organization (NAFO), which manages international fisheries off Canada’s east coast. Their members are sovereign states—both adjacent states and states with distant-water fleets. Japan, South Korea and the EU, for example, are members of NAFO, alongside Canada, Greenland and the USA. Aboriginal peoples are not represented except indirectly through their states, since aboriginal maritime rights can extend no farther than the limits of their states’ EEZs.\nThere is currently no international fisheries organization like NAFO covering the arctic donut hole, which is precisely why some fear overfishing there. There is, however, an international body that considers sustainable development in the Arctic within its remit—and which moreover counts aboriginal peoples as permanent participants. This body is the Arctic Council, and one of those permanent participants is the Inuit Circumpolar Council (ICC), which represents the interests of all Inuit internationally.\nIf NTI intends to work through ICC and the Arctic Council to advance an agenda of preferential aboriginal access to international commercial fisheries in the Arctic, it should bear a couple of things in mind.\nFirst, ICC and the other arctic aboriginal organizations would need to clarify a common position on arctic commercial fishing. So far, no Inuit organization apart from NTI has publicly commented on fishing in the donut hole, but the Alaska branch of ICC supported a 2009 moratorium on expansion of commercial fishing in the American EEZ north of the Bering Strait, and the Inuvialuit Regional Corporation supported a 2011 moratorium on expansion of commercial fishing in the Canadian EEZ in the Beaufort Sea. In its “Circumpolar Inuit Declaration on Sovereignty in the Arctic”, ICC made a general claim for “the rights of indigenous peoples to self-determination and representation in intergovernmental matters”, but it’s unclear what this would mean for international fisheries management specifically.\nSecond, the Arctic Council in its current form almost certainly cannot take the lead on managing international fisheries in the donut hole—let alone provide aboriginal peoples preferential access to them on some as yet unrecognized legal or human-rights grounds. Its membership is too narrow, excluding such important distant-water fishing states as China, Japan, South Korea and the EU—in some cases even merely as observers. If the Arctic Council actually wished to take on the role of an international fisheries organization, or to create a new one from scratch, it doubtlessly would have to include all the relevant distant-water fishing states as members.\nThis would create something of a dilemma for the Inuit. So far, they’ve been very wary of allowing non-arctic states a voice in arctic affairs. They suspect that non-arctic states don’t really understand the Arctic and its peoples, and that their own voice would be drowned out. To take but one example, Inuit Tapiriit Kanatami opposes the EU’s bid for observer status on the Arctic Council because the EU’s seal-trade ban threatens Inuit economic rights over their traditional marine resources. The complaint is legitimate and the suspicions are justifiable. But all this hardly bodes well for winning special Inuit access to the international community’s own marine resources—or perhaps more importantly for obtaining a seat at the table managing international commercial fisheries in the Arctic.\nNTI and the Inuit have good reason to worry that powerful fishing states with large distant-water fleets might irreparably damage commercial fisheries in the Arctic, and that the international community might fail to prevent it. But it’s hard to see what the Inuit could do to protect their interests in those fisheries without engaging non-arctic states on the interests they have in them as well.\nEditor’s note: The views presented by NPA bloggers do not necessarily reflect the views of the magazine']	['<urn:uuid:cca89b31-99da-4bdc-8922-979bb5fc9d96>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	5	83	1632
9	looking for skin creams risks side effects first time user need info safety effectiveness	Topical medications, when used properly, can yield desired outcomes but may cause adverse skin reactions like itching, inflammation, and redness if taken in excess. Before using any topical medicine, you should consider potential allergic reactions to medicines, foods, dyes, preservatives, or animals. Special precautions apply to young children and elderly people due to increased skin absorption. The medicine should be kept away from eyes, and hands should be washed immediately after application. Additionally, you should avoid using other skin preparations simultaneously (like abrasive soaps, alcohol-containing preparations, or medicated cosmetics) as this may cause severe skin irritation.	"['The term epicutaneous refers to the route of topical drug administration by which the drug, poison, fluid, or other substance is taken into the body via direct skin application. The three major routes of administration include: topical, in which the effect is local; enteral, in which the effect is non-local; and parenteral, in which the effect is systemic. Epicutaneous medications range from lotions, creams and ointments to powders, patches, and tinctures. Other topical medications may be inhaled or applied to surfaces of tissues other than skin. Topical medications taken in excess can result in adverse skin reactions such as itching, inflammation and redness; however, if consumed properly can yield the desired outcome.\nPermeation Through SkinEdit\nThe skin acts as a barrier separating the internal body structures from the external environment, maintaining a slightly acidic pH as to prevent against pathogens and other foreign invaders. There are several layers of skin: the outer layer called the epidermis, followed with the dermis, followed with an innermost layer of fatty tissues. The two principal routes of drug administration through the skin are:\n1) Transepidermal absorption: This process outlines diffusion of substances across the stratum corneum. Diffusion can occur via the intercellular lipoidal route or through a microscopic route designed for polar compounds and ions. Because the epidermis structure is extremely compact leaving little intercellular space, permeation calls for the crossing of cell membranes. Yet the greatest struggle lies in penetration through the dermis, where diffusion occurs through the interlocking channels of the substance.\n2) Transfollicular (shunt pathway) absorption: Sebaceous and eccrine glands operate as the skin\'s appendages and channels of drug absorption by acting as shunts that bypass the stratum corneum. The overall mechanism progresses through partioning and diffusion through the sebrum, the driving force being a concentration gradient. An equation has been formulated to describe this process: R = h/FDK\nR =Resistance of diffusion resistor, F = Fractional area, H = Thickness, D = Diffusivity, K = Relative capacity,\n1) Allergy Testing: Antigen extracts are applied to the skin as puncture skin tests. This method measures the reactivity of the skin to the injected material. This can be carried out through the scratch test, prick test, or intradermal test.\n2) Local Anesthesia: Local anesthesia provides immediate relief to the region of application by numbing the sensation and causing a loss of nociception. These drugs can be classified either as aminoamide or aminoester and are given as an injection, spray, or ointment. These are often used for dental or dermal purposes.\n3) Transdermal Patches: Transdermal patches overlap under the parenteral category but some can fall under the epicutaneous category as well. They serve a wide array of purposes:\n- Prevention of motion sickness: Scopalamine was the first transdermal patch to be approved by the FDA, in 1979.\n- Birth control\n- Hormone replacement therapy: To alleviate symptoms of menopause.\n- Cessation of tobacco smoking\n- Allergy tests\n4) Arthritis Pain: These drugs exist mainly as creams that work to immediately and temporarily mitigate the sensations of arthritis in the hand. They come in a few forms:\n5) Wounds/Rashes: Skin conditions can be treated with the usage of creams, lotions, gels, ointments, or in some cases steroids.\n- ""Topical Drug Delivery Systems : A Review."" Drug Delivery Systems : Topical Drug Delivery |. N.p., n.d. Web. 29 Oct. 2012. <http://www.pharmainfo.net/reviews/topical-drug-delivery-systems-review>.\n- ""Allergy & Immunology."" University of Vermont, n.d. Web. 29 Oct. 2012. <http://www.uvm.edu/medicine/surgery/documents/AllergyandImmunology1.pdf>.\n- ""Topical Creams for Arthritis PainÂ Relief."" About.com Osteoarthritis. N.p., n.d. Web. 29 Oct. 2012. <http://osteoarthritis.about.com/od/painrelief/a/topical_creams.htm>.', 'Holevn Health share articles about :Thuốc SAStid (Topical) , side effects – dosage , Thuốc SAStid (Topical) what disease treatment.Other noted issues. Please refer to the details below.\nGeneric Name: salicylic acid and sulfur (Topical route)\nsal-i-SIL-ik AS-id, SUL-fur\nMedically reviewed by Holevn.org. Last updated on Dec 11, 2019.\nCommonly used brand name(s)\nIn the U.S.\n- Fostex Cream\n- Night Cast R\nAvailable Dosage Forms:\nTherapeutic Class: Antiacne Keratolytic\nPharmacologic Class: NSAID\nChemical Class: Salicylate, Non-Aspirin\nUses for SAStid\nSalicylic acid and sulfur combination is used to treat acne and other skin disorders and dandruff and other scalp disorders, such as seborrheic dermatitis.\nThis medicine is available without a prescription.\nBefore using SAStid\nIn deciding to use a medicine, the risks of taking the medicine must be weighed against the good it will do. This is a decision you and your doctor will make. For this medicine, the following should be considered:\nTell your doctor if you have ever had any unusual or allergic reaction to this medicine or any other medicines. Also tell your health care professional if you have any other types of allergies, such as to foods, dyes, preservatives, or animals. For non-prescription products, read the label or package ingredients carefully.\nYoung children may be at increased risk of unwanted effects because of increased absorption of salicylic acid through the skin. Products containing salicylic acid should not be applied to large areas of the body or used for long periods of time in infants and children.\nMany medicines have not been studied specifically in older people. Therefore, it may not be known whether they work exactly the same way they do in younger adults or if they cause different side effects or problems in older people. There is no specific information comparing use of salicylic acid and sulfur combination in the elderly with use in other age groups.\nThere are no adequate studies in women for determining infant risk when using this medication during breastfeeding. Weigh the potential benefits against the potential risks before taking this medication while breastfeeding.\nInteractions with medicines\nAlthough certain medicines should not be used together at all, in other cases two different medicines may be used together even if an interaction might occur. In these cases, your doctor may want to change the dose, or other precautions may be necessary. When you are taking this medicine, it is especially important that your healthcare professional know if you are taking any of the medicines listed below. The following interactions have been selected on the basis of their potential significance and are not necessarily all-inclusive.\nUsing this medicine with any of the following medicines is not recommended. Your doctor may decide not to treat you with this medication or change some of the other medicines you take.\nUsing this medicine with any of the following medicines is usually not recommended, but may be required in some cases. If both medicines are prescribed together, your doctor may change the dose or how often you use one or both of the medicines.\n- Amtolmetin Guacil\n- Bismuth Subsalicylate\n- Choline Magnesium Trisalicylate\n- Choline Salicylate\n- Ethacrynic Acid\n- Flufenamic Acid\n- Magnesium Salicylate\n- Mefenamic Acid\n- Niflumic Acid\n- Nimesulide Beta Cyclodextrin\n- Pentosan Polysulfate Sodium\n- Phenyl Salicylate\n- Salicylic Acid\n- Sodium Salicylate\n- Tenofovir Disoproxil Fumarate\n- Tiaprofenic Acid\n- Tolfenamic Acid\n- Trolamine Salicylate\nUsing this medicine with any of the following medicines may cause an increased risk of certain side effects, but using both drugs may be the best treatment for you. If both medicines are prescribed together, your doctor may change the dose or how often you use one or both of the medicines.\n- Azilsartan Medoxomil\n- Candesartan Cilexetil\n- Olmesartan Medoxomil\n- Perindopril Erbumine\nInteractions with food/tobacco/alcohol\nCertain medicines should not be used at or around the time of eating food or eating certain types of food since interactions may occur. Using alcohol or tobacco with certain medicines may also cause interactions to occur. Discuss with your healthcare professional the use of your medicine with food, alcohol, or tobacco.\nProper use of SAStid\nThis section provides information on the proper use of a number of products that contain salicylic acid and sulfur. It may not be specific to SAStid. Please read with care.\nUse this medicine only as directed. Do not use more of it and do not use it more often than recommended on the label, unless otherwise directed by your doctor.\nImmediately after using this medicine, wash your hands to remove any medicine that may be on them.\nKeep this medicine away from the eyes. If you should accidentally get some in your eyes, flush them thoroughly with water.\nTo use the skin cleansing lotion:\n- After wetting the skin, apply this medicine with your fingertips or a wet sponge and rub in gently to work up a lather. Then rinse thoroughly and pat dry.\nTo use the shampoo or bar as a shampoo:\n- Wet the hair and scalp with lukewarm water. Then apply enough medicine to work up a lather and rub into the scalp. Continue rubbing the lather into the scalp for several minutes or allow it to remain on the scalp for about 5 minutes, depending on the product being used, then rinse. Apply the medicine again and rinse thoroughly.\nTo use the bar as a soap:\n- After wetting the skin, use this medicine to wash the face and other affected areas. Then rinse thoroughly and pat dry.\nThe dose of this medicine will be different for different patients. Follow your doctor’s orders or the directions on the label. The following information includes only the average doses of this medicine. If your dose is different, do not change it unless your doctor tells you to do so.\nThe amount of medicine that you take depends on the strength of the medicine. Also, the number of doses you take each day, the time allowed between doses, and the length of time you take the medicine depend on the medical problem for which you are using the medicine.\n- For acne or oily skin:\n- For bar dosage form:\n- Adults and children—Use on the affected skin two or three times a day.\n- For cleansing lotion dosage form:\n- Adults and children—Use on wet skin one to three times a day. Rub lather into skin for one minute then rinse.\n- For bar dosage form:\n- For dandruff and seborrheic dermatitis of the scalp:\n- For bar and shampoo dosage forms:\n- Adults and children—Use on the scalp once a day at least two times a week or as directed by doctor. May be used each day if needed.\n- For bar and shampoo dosage forms:\nIf you miss a dose of this medicine, apply it as soon as possible. However, if it is almost time for your next dose, skip the missed dose and go back to your regular dosing schedule.\nStore the medicine in a closed container at room temperature, away from heat, moisture, and direct light. Keep from freezing.\nKeep out of the reach of children.\nDo not keep outdated medicine or medicine no longer needed.\nPrecautions while using SAStid\nWhen using salicylic acid and sulfur combination medicine, do not use any of the following preparations on the same affected area as this medicine, unless otherwise directed by your doctor:\n- Abrasive soaps or cleansers\n- Alcohol-containing preparations\n- Any other topical acne preparation or preparation containing a peeling agent (for example, benzoyl peroxide, resorcinol, or tretinoin [vitamin A acid])\n- Cosmetics or soaps that dry the skin\n- Medicated cosmetics\n- Other topical medicine for the skin\nTo use any of the above preparations on the same affected area as salicylic acid and sulfur combination medicine may cause severe irritation of the skin.\nDo not use any topical mercury-containing preparation, such as ammoniated mercury ointment, on the same affected area as this medicine . To do so may cause a foul odor, may be irritating to the skin, and may stain the skin black. If you have any questions about this, check with your health care professional.\nTaking large doses of aspirin or other salicylates (including diflunisal) while using topical salicylic acid (contained in this medicine) may lead to overdose. If you have any questions about this, check with your health care professional.\nSAStid side effects\nAlong with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention.\nCheck with your doctor as soon as possible if any of the following side effects occur:\n- Skin irritation not present before use of this medicine\nSome side effects may occur that usually do not need medical attention. These side effects may go away during treatment as your body adjusts to the medicine. Also, your health care professional may be able to tell you about ways to prevent or reduce some of these side effects. Check with your health care professional if any of the following side effects continue or are bothersome or if you have any questions about them:\n- Redness and peeling of skin (may occur after a few days)\n- unusual dryness of skin\nOther side effects not listed may also occur in some patients. If you notice any other effects, check with your healthcare professional.\nSeek emergency medical attention or call 115\nCopyright 2020 Truven Health Analytics, Inc. All Rights Reserved.\nThe content of Holevn is solely for the purpose of providing information about Thuốc SAStid (Topical) and is not intended to be a substitute for professional medical advice, diagnosis or treatment. Please contact your nearest doctor or clinic, hospital for advice. We do not accept liability if the patient arbitrarily uses the drug without following a doctor’s prescription.\nReference from: https://www.drugs.com/cons/sastid-topical.html']"	['<urn:uuid:1fbdc919-0270-427f-8162-b74388fab567>', '<urn:uuid:07e34a9d-e251-4c1b-9f8a-6a9facca99cc>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T10:32:05.666290	14	96	2232
10	What happened during the October 7, 1950 coup in Nepal?	On October 7, 1950, a political crisis erupted in Nepal when the Rana Prime Minister attempted to force King Tribhuvan to abdicate. After the King refused and sought protection in the Indian Embassy in Kathmandu, the Prime Minister tried to crown one of the King's grandsons as the new king. India refused to accept this new king and stood firmly behind King Tribhuvan. This coup coincided with the same day that Chinese forces crossed the Yangtze to advance towards Chamdo in Tibet.	"[""This document confirms that Tibet was an independent nation in 1949.\nInterestingly, many historians questioned why did Lhasa not request Nepal for military help in October 1950 when the PLA's Second Army entered Chamdo, in Eastern Tibet.\nAn extraordinary coincidence occurred, on October 7, 19050, the day the Chinese forces crossed the Yangtze to advance towards Chamdo, there was a coup in Nepal.\nHere is is what I wrote in my book, Tibet: The Lost Frontier:\nInstead of trying to look in the history books, one should perhaps be looking at the stars. What was going on this 7th day of October of the Fateful Year?\nPeng Dehuai was taking over the Chinese Army on the Korean front and leading the Chinese Dragon in one of the most devastating war.\nThousand of miles eastwards, Deng Xiaoping was invading a small peaceful country living in its Buddhist paradise and down South a revolution was brewing in another Himalayan kingdom of Nepal.\nKing Tribhuvan and his family had to ask asylum in the Indian Embassy in Kathmandu. In Nehru words: “There has been a great deal of friction for sometime past between the King and the Prime Minister”.\nNepal had been administrated for the past two hundred years by the Rana family who were known as Maharaja and holding the post of Prime Minister of Nepal. The Kings who belonged to another family were just a figurehead.\nBut lately, King Tribhuvan had been trying to introduce popular reforms which had been resisted by the more traditional Maharaja Mohan Shamsher, a Rana.\nA week before the ominous day, the tensions had grown worse between the King and his Prime Minister. The King had asked the Indian Embassy in Kathmandu if he and his family could be given protection as he was fearing for his life. The Rana Prime Minister was resenting more and more the reforms as a personal attack against his rule and as an interference of the democratic India at his door steps.\nNehru continued: “Efforts were made by the Prime Minister to get the King to abdicate, but the later refused to do so”.\nAt this point of time, CPN Singh, the Indian ambassador to Nepal convinced the Indian Prime Minister that the King should be given shelter and protection. Nehru agreed.\nThe Prime Minister’s son went to the embassy to try to pressurise the King to abdicate, but Tribhuvan sure of the Ambassador support reiterated that he was no question for him to abdicate.\nOn the same October 7, the Rana Prime Minister decided to crowned one of the King’s grand son (through Tribhuvan had not abdicated) and make him the next king of Nepal. In the midst of the Himalayan high drama, Delhi decided to act firm and refused to accept the new King and stand behind King Tribhuvan.\nWas the coup in Kathmandu linked with the attack on Chamdo or the crossing of the 38th paralell? It is difficult to say, but though there are apparently no links, it is sure that infiltration of the Communist forces had begun in Himalayas.\nRetrospectively, one understand that it was difficult for Nepal to intervene in Tibet, so legally the Himalayan Kingdom was bound to do so.\nThe Forgotten History of Tibet's Role in Nepal's 1949 UN Application\nTibetan Political Review\nOctober 12, 2011\nOn September 23, 2011, Palestinian President Mahmoud Abbas formally submitted Palestine’s application for United Nations membership to Secretary General Ban Ki Moon. At a minimum, Palestine appears set for recognition by the General Assembly as a nonmember state, on par with sovereign Vatican City, which will be an important step forward for Palestine from its current observer status. This drama, which is playing out at the UN headquarters in New York, highlights the UN’s role as a main source of legitimacy in interstate relations and international affairs. It also leads us to recall Nepal’s application for membership to that body 62 years ago, which prominently cited Nepal’s “diplomatic relations” with Tibet as proof of its sovereignty.\nAt the time, Nepal explicitly recognized Tibet as an independent country, including in its official application to the United Nations. This fact, which we explore below, recalls the strong historical ties between the peoples of Nepal and Tibet. Although Nepal currently has a “one-China” policy that recognizes Tibet as part of the People’s Republic of China (PRC), it is worth remembering that this is only recent history. The domineering presence of the PRC in Nepalese affairs is a relatively recent disruption that cannot erase from historical memory the enduring Nepali and Tibetan relations across the Himalayas.\nNEPAL’S EXPLICIT RECOGNITION OF TIBET’S SOVEREIGNTY\nWhen Nepal applied to join the United Nations, it was required to submit proof that it was a sovereign state with the capacity to enter into relations with other states. Tibet Justice Center has made available a copy of Nepal’s application package of July 22, 1949. (You can download the copy at http://sites.google.com/site/legalmaterialsontibet/home/nepal-un-app)This application clearly shows that Nepal considered Tibet a sovereign state.\nOn page 9, the application describes Nepal’s “Diplomatic Relations”. It states:\nThe Diplomatic Relations of Nepal with the United Kingdom have already been described in paragraph 4(e) above. In addition, Nepal has established diplomatic relations with the following countries:\n(a) Tibet. In 1856 Nepal established a Legation at Lhasa, maintained representatives at Gyangtse, Kuti and Kerung.\n(c) United States of America …\n(e) Burma …\nNepal apparently considered its relations with the “country” of Tibet to be second in significance only to its relations with Britain, and even more significant than its relations with the USA or India.\nNepal’s UN application also included a copy of the text of the 1856 peace treaty between Nepal and Tibet, as additional proof of Nepal’s foreign relations with other countries. Incidentally, this treaty committed Nepal to aid in Tibet’s defense. (Article 2: “[S]hould troops of any other Raja invade Tibet in future, [Nepal] will afford such assistance as it can.”)\nBased on the sufficiency of this application, the United Nations admitted Nepal as a member state on January 14, 1955.\nCHINA’S ATTEMPTED COVER-UP\nChina subsequently tried to cover up this inconvenient historical truth. On September 20, 1956, China and Nepal signed an “Agreement to Maintain the Friendly Relations Between China and Nepal and on Trade and Intercourse Between the Tibet Region of China and Nepal”. Article 3 of this agreement stated that “All treaties and documents which existed in the past between China and Nepal including those between the Tibet Region of China and Nepal are hereby abrogated.”\nThis provision was an attempt by China to cover its legal bases by getting Nepal to annul its prior treaty commitments with Tibet, including the defense obligation of the 1856 Nepal-Tibet treaty. As a practical and political matter, this cover-up worked. Subsequent Nepalese governments have restated their position that Tibet is part of the PRC.\nAs a legal matter, however, the 1956 treaty is shaky. Its purported abrogation begs the question because Nepal’s Tibetan treaty obligations were with the “country” of Tibet. There were no treaties between Nepal and the “Tibet Region of China” to be abrogated, because Nepal did not recognize such an entity to have existed until 1956. This legal argument will not have much practical effect now, but it might be revived at an opportune time in the future.\nNATIONAL DIGNITY IS A POWERFUL FORCE\nObviously, Tibetans will care about the historical fact that Nepal explicitly recognized Tibet as independent as recently as 1949, and indeed relied on its relations with Tibet to gain admission to the UN. But this fact is also a part of Nepal’s national history. Nepal’s relationship with Tibet deeply affects the social, political, cultural, and religious history of the Nepalese people themselves. It is a question of national dignity for both Nepalis and Tibetans.\nFor the Chinese government to try to change or cover over this history is an affront to the Nepalese people, and an act of historical imperialism. Because the justification for China’s occupation of Tibet depends on a historical fiction, China has sought to get others to play along with its revisionism. Yet despite China’s current success in bullying the Nepalese people into sometimes abrogating their own sovereignty and national history over the issue of Tibet, nothing can erase the documented fact that Nepal once recognized Tibet as an independent state at the United Nations in front of the global stage. The Nepalese people should not be bullied into denying their own national history.\nNepal has demonstrated some willingness to reject China’s interference in its affairs. On September 22, 2011 (the day before the Palestinian U.N. application), the Government of Nepal released a group of 23 mostly-teenaged Tibetan refugees into the care of the United Nations High Commission for Refugees. What made this act particularly notable was it was done despite the tremendous pressure China exerted on tiny Nepal. China wanted Nepal to return these refugees to Chinese-occupied Tibet, contrary to Nepal’s “Gentleman’s Agreement” concerning Tibetan refugees and the fundamental international law forbidding such refoulement.\nMuch has been written about how the Palestinian quest for statehood is in part a reaction to the indignities of life under occupation. In another more subtle sense, the Chinese attempt to dictate Nepal’s refugee policy was a matter of national dignity for Nepal. This was a Chinese intrusion into Nepal’s sovereignty, which Nepal repulsed. If we were Nepalese -- even if we cared nothing about the 23 Tibetans -- we would be happy that our government stood up for the dignity of its people and refused to be dictated to by Beijing.\nTibetans will continue to cross over the mountains in search of freedom from occupied Tibet. The next time another group of Tibetan refugees lands in Nepalese custody, we hope that Nepalese leaders will recall without fear or embarrassment that Nepal enjoys UN membership today based on a document in which it prominently highlighted its relations with the then-independent country of Tibet. Moreover, just as the Palestinian people are set to have their statehood finally recognized by the U.N., given the irrepressible desire of all peoples to be free, one day the Tibetan people may reach the same place.""]"	['<urn:uuid:11c2e2bc-0f7b-4923-9dea-f097b902ee72>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	10	82	1697
11	How can control valves be adjusted in jet pumps?	Control valves can be adjusted through three methods: using a pressure gauge to set 20 psi backpressure, using an amp meter to match nameplate amps, or manually adjusting until flow rate maximizes while avoiding cavitation.	['This is part 2 of our jet pump series. Last month, we said that a jet pump was a centrifugal pump with a jet assembly (also called an injector), either integrally attached, as in the case of shallow well jets, or remotely attached, as in the case of deep well jets. We further explained that a shallow well jet pump draws the water up to the surface and is limited by atmospheric pressure to a depth of about 25 feet. Deep well jets, with the jet assembly located down in the well below the water level, push the water to the surface and can pump from as deep as 200 feet.\nThe operation of a jet pump requires a certain amount of water to be circulated back through the jet assembly as drive water. In a shallow well jet pump, the amount of water diverted is governed by the internal construction of the pump and is not adjustable because the narrow range of pumping conditions can be met without having to adjust the amount of drive water. The performance of a deep well jet pump, on the other hand, can be optimized to specific pumping conditions through the use of a control valve to create the right amount of backpressure, and this article will explain their function and how to adjust them.\nTypes of Control ValvesControl valves can be as simple as a manually adjustable flow-restricting valve built into the outlet port of the pump (see Figure 1), or as complex as a pilot or manually operated spring-loaded diaphragm valve like a pressure regulator, attached to the outlet port of the pump (see Figure 2). Used manually, diaphragm valves operate just like any manually operated valve to restrict the pump’s discharge to create backpressure to force drive water to the jet. Their advantage over a simple gate or ball valve is that they can be adjusted more precisely and hold their adjustment better. Manually adjusted backpressure valves are set to a specific pumping level, and if the level changes, the pump’s performance will be affected.\nAt least one manufacturer offers an automatic backpressure valve that has a small line connected to it from the suction side of the pump. Vacuum regulates the valve and thus the output of the pump to provide optimum performance as the water level in the well goes up and down. When the water level is high, the vacuum on the suction side is low, and the valve is wide open. As the water level drops lower, the vacuum increases, and the valve begins to close to circulate more drive water down to the jet to provide the additional lift needed to pump from a deeper level. Automatic backpressure valves are factory-set and need no field adjustment.\nHow to Set Manual Control ValvesBefore making the final adjustments, the pump must be primed and the system purged of air. To do this, manually fill the pump and piping down to the jet with water and turn on the pump with the control valve open. You should see pressure at the outlet of the pump, indicating it is primed. Then, begin to close the control valve to purge air from the rest of the system.\nMethod one — Using a pressure gauge: Some manufacturers recommend adjusting the control valve in order to provide a specific amount of backpressure bet-ween the pump and the control valve, usually 20 psi. To set it this way, run the pump until the well has stabilized to establish the pumping level. Once there, set the control valve to give you 20 psi on a gage on the outlet side of the pump before the valve.\nMethod two — Using an amp meter: With the pump pumping at the stabilized pumping level, use an amp probe to set the control valve to give you an amp reading equal to the name plate amps of the pump. Don’t use service factor amps because you could overload the motor if the pumping level rises in the well.\nMethod three — Using your ears: This method is a little less orthodox but one used by many old-time jet pump installers. Open a faucet near the pump so there is no backpressure from the tank and so you can observe and estimate the discharge flow rate. Slowly open the valve until the flow rate appears to maximize. If you continue to open the bypass, you will reach a point where the pump starts to rattle, as if it is full of marbles, due to cavitation. If you continue to open the valve beyond this point, the pump will loose prime, and you’ll have to start over. Back off one turn on the backpressure by adjusting the screw and then tighten the lock nut. You’re done.\nYou can check your setting with a pressure gauge on the discharge and a bucket to measure flow. Watching the pressure gauge, restrict the discharge flow until the gauge reads 30 psi. Using the bucket and a stopwatch, measure the flow rate at this pressure and verify that it agrees with the manufacturer’s performance tables for the pump/injector package you have. If not, play with the control valve adjustment to maximize the flow.\nPumping from a Slow-producing WellSince air entering a jet pump system causes it to stop pumping and necessitates re-priming the system, it is important to take precautions to ensure the water level in the well does not drop below the inlet of the foot valve. To prevent air from entering the foot valve in a slow producing well, try the following. Because a pump cannot lift water more than 34 feet maximum due to the limitations of atmospheric pressure, installing a 34-foot section of suction pipe below the injector will preclude the possibility of the water level being drawn down below the foot valve. As the well draws down, increasing the suction head on the injector, its pumping capacity slows down until it matches that of the well. With the foot valve set at 34 feet below the injector, even in a well with no production, air cannot enter the foot valve because the injector is not able to lift the last foot of water to empty the well. A neat little trick.\nSumming up, shallow well jet pumps have the jet assembly built into or attached to the suction port of the pump and are designed to pull water out of a shallow well (less than 25 feet in depth). Deep well jet pumps have the jet assembly down in the well so they can push the water up out of the well and are not confined by the 25-foot limit. They can pump water from as deep as 200 feet. Convertible jet pumps are designed with the injector assembly removable and small enough to fit into 4-inch and larger well casing. They can be used as either a shallow or deep well jet pump, but can not pump from as deep as a dedicated deep well jet — 130 feet is about as deep as they go.\nProperly sized, installed and adjusted, a jet pump can be a real workhorse and provide years of trouble-free service. Millions of people around the world depend on jets for their water. Hopefully, this series has given you a better understanding of the inner workings of these thoroughbreds.\nNext month, we learn how to determine the pressure and flow requirements of a pumped water system.']	['<urn:uuid:b58f4ebc-f0d7-405d-87f3-7a7a0c973f90>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	9	35	1243
12	How does light quality affect plant growth and agricultural productivity?	Light quality significantly impacts plant growth and agricultural production in several ways. In diffuse light conditions, tomato plants show improved water-use efficiency and higher fruit production compared to direct light, possibly due to temperature differences. Various light spectra also affect plant morphology - far-red light (720-750nm) causes stem and leaf stretching as plants interpret it as shading, while green light creates taller plants with more open canopies. Studies have shown that specific light recipes, like providing blue light in the morning followed by red light, can increase tomato production by 8%. Additionally, far-red light can improve fruit production by 5-20% by favoring assimilate movement to fruits instead of leaves. Light quality can also be used to control plant height and flowering without chemicals, particularly important in bedding plant production.	"[""Check out the write-up on the project in Chapman's Newsroom.\nA big shout out to Andrew Felton, who will begin a USDA-NIFA postdoctoral fellowship with us on September 1st. Andrew will work on a project entitled: Optimizing rangeland decision making by unraveling geographic variation in the timing of forage sensitivity to weather.\nCheck out the write-up on the project in Chapman's Newsroom.\nA new paper out in the Journal of Microbiology and Biology Education in collaboration with the postdoctoral fellows, faculty and staff from the Grand Challenges Initiative:\nThe Goldsmith Lab has been awarded the 2019 Ignacio Rodriguez-Iturbe award for an outstanding publication from the journal Ecohydrology for their article quantifying the spatial variation in stable isotopes of water in a forest ecosystem. The research establishes best practices for using stable isotopes of water as a tool for understanding the source of water used by different species of trees in forest ecosystems:\nSpatial variation in throughfall, soil, and plant water isotopes in a temperate forest\nby Gregory R. Goldsmith, Scott T. Allen, Sabine Braun, Nadine Engbersen, Clara R. González‐Quijano, James W. Kirchner, Rolf T.W. Siegwolf\nThe results are part of a long-term collaboration designed to inform the sustainable management of Switzerland’s forests in the face of unprecedented rates of climate change.\nThe project was carried out in collaboration with researchers from the Institute for Applied Plant Biology, the Swiss Federal Institute for Forest, Snow and Landscape Research and ETH Zurich with funding from the Swiss Federal Office of the Environment.\nMeeting AbstractP30-2 Sat Jan 2 Effects of diffuse light on the physiology, growth, and fruit yield of tomato plants Ellertson, K*; Prakash, A; Goldsmith, G; Berry, ZC; Chapman University ; Chapman University ; Chapman University ; Chapman University\nIntroduction: We routinely study how the quantity of light affects rates of plant photosynthesis. However, what happens to photosynthesis when we change the angle of light? A growing body of research has demonstrated changes in leaf photosynthesis and net ecosystem exchange in diffuse light conditions caused by clouds or other aerosols. However, our understanding of the effects of diffuse light on physiological processes and the concomitant effects on growth and yield remain limited. Methods: We compared the physiology, growth and yield of tomatoes (Solanum lycopersicum) grown in direct compared to diffuse light conditions. Diffuse light conditions (ca. 50-60% diffuse) were created by a painted glass panel that leads to diffusion of light, but does not significantly reduce the quantity of light. Results: We observed significant differences in photosynthetic function, including water-use efficiency, of plants in diffuse light as compared to direct light conditions. However, there was increased leaf cupping in plants grown in direct light, which may be due to higher temperatures (ca. 3-5 °C) in that treatment. We also observed no differences in initial plant growth (height, leaf number, and stem diameter); however, fruiting in the diffuse light conditions was higher than in direct light conditions. Discussion: Diffuse light conditions may have their greatest impact on plant structure and function by increasing flowering and fruit production. This effect may be mediated not by the light quality, but in this case by the temperature change induced by diffusing the light. As the climate warms, these results suggest that simple modifications to greenhouse structure may benefit fruit yield for key crops.\nWe have a new preprint posted to Edarxiv from the teaching team in the Grand Challenges Initiative at Chapman University:\nDiscussion can be an important and powerful tool in efforts to build a more diverse, equitable, and inclusive future for STEM. However, facilitating discussions on complex and uncomfortable issues, like racism and sexism, can feel daunting. We outline a series of steps that can be used in offices, laboratories, and classrooms to facilitate productive discussions that empower everyone to listen, contribute, learn, and ultimately act to transform STEM.\nRead the full text here: FULL TEXT\nIt is feeling like the end of an era in our lab. But so many great things are happening:\nDr. Eleinis Àvila-Lovera has started an Earl S. Tupper Fellowship at Smithsonian Tropical Research Institute in Panama. We continue to collaborate on project funded by a USDA-AFRI grant designed to understand the effects of green stem photosynthesis on hydraulic functioning in avocados.\nDr. Scott T. Allen has started a faculty position at the University of Nevada, Reno. We continue to collaborate on a project funded by the Swiss Federal Office of the Environment designed to understand the future of Switzerland's forest ecosystems.\nDr. Carter Berry has moved back east to his old haunting grounds in North Carolina. We continue to collaborate on a project funded by a USDA-AFRI grant designed to understand the effects of diffuse light photosynthesis on plant gas exchange.\nDr. Andrew Felton joins us from Peter Adler's laboratory at Utah State University. Andrew will bring together our research on wood water storage and satellite remote sensing, funded by grants from USDA-AFRI and NASA. We are so excited to have Andrew as part of our community.\nIn our lab, we believe that Black Lives Matter, Love is Love, Women's Rights are Human Rights, Immigrants are Welcome, and of course, that Science is Real. We also believe that it all starts with Being Kind to one another.\nMy student Alex Drivas (Biochemistry '21) has been spending part of his summer working on printing aluminum parts using our 3D printer. I encouraged Alex to try this approach because, if successful, it would transform our ability to make new equipment in the lab. Generally, if you need something made of metal, you send a drawing out to a company that uses a CNC mill to remove material from a block of metal until you have the desired shape. This is referred to as subtractive manufacturing. Now, our desktop 3D printers have the ability to print in metal or in plastics supplemented with metal at the click of a mouse. Instead of sending something off to an outside manufacturer and paying hundreds or thousands of dollars per piece, we can print a design in the lab in the matter of hours for just the cost of the printer (< $1000) and the cost of the filament (~$165 per kilo). The print does need to be de-bound and sintered offsite, so it's not instant gratification, but it is a huge step forward.\nAlex is at the cutting edge of using this technology (a future employer will be very lucky) and I asked him to write up his specifications and thoughts on printing for the benefit of the 3D printing community. Here's what he has to say:\nHot end: 240 °C\nBed temp: 100 °C\nLayer thickness: 0.2mm\nPerimeters: 4 count\nInfill type: Concentric\nFirst layer print speed: 25mm/s\nPrint speed: 40mm/s\nTravel speed: 125mm/s\nCooling Fan: off\nExtrusion multiplayer: 90%\nFirst layer extrusion: 93%\nRetraction length: 6mm\nRetraction speed: 125mm/s\n4 count brim\nWhat I learned:\nAdhesion and extrusion settings were the most important variables in printing. I had to apply more Dimafix around the corners of the print than the center and introduce a small brim to achieve proper layer adhesion through the print. I had to lower the Z offset (beyond what I would for PLA for ABS) to get better first layer adhesion. In following layers, I manually increased the Z offset just slight. If the nozzle was too far from the plate, the layer would not stick properly, however, if the Z offset was too low, the nozzle would grab material from the print as it extrudes. Improper adhesion would cause the corners of the print to curl and destroy the progress of the print. Applying too much Dimafix however, would make it close to impossible to remove from the bed.\nOver-extrusion was a second variable adding to the debris build up around the nozzle. It was difficult at first to differentiate between the two. It will vary in printer type, but I narrowed ours down to the extrusion and retraction settings I had stated above. Decreasing the extrusion of the first layer however, made capturing the proper Z offset very important. Even with the successful print, I was still receiving a little over-extrusion and debris build up around the nozzle and had to pause the print to clean the nozzle. The debris did not build up over time, but rather varied based on contours and geometries at the specific stage of the print. The green part coming off the bed is unclean and needs to be sanded and filed to clean up the past troubles with extrusion.\nIn terms of the material and how it physically printed, I would compare it to trying to print clay. Coming out of the hot end, it does not stick to the bed as you would expect. Rather, it sticks to itself at times you don’t want it to and doesn’t when you do. Geometrically the material is very limited. Holes below a 10mm radius print unclean and inconsistent through layers. Thicker walls the better. Any internal fillets or contours are difficult and should be avoided. The optimal print for this material would be a gear or some small, geometrically uncomplex, internally uniform object.\nAlex's work was supported by a grant from the Center for Undergraduate Excellence at Chapman University.\nWe are fortunate to have a Picarro L2130i cavity ringdown spectrometer in our new lab here in the Keck Center for Science and Engineering here at Chapman University. The instrument can analyze stable isotopes of oxygen and hydrogen in water vapor or liquid samples via the autosampler (with a dry nitrogen carrier gas). The lab has a number of in-house standards that span the range of typical waters at natural abundance. Many labs are not running highly enriched or depleted liquid samples right now due to memory effects; these problems can be solved with a particularly rigorous approach to sample injection and careful choice of standards; we are willing to take the time to run those samples. We are happy to serve as a resource for the community - please reach out if you have analyses in mind.\nWe also have the ability to extract water from environmental samples (e.g. soil, plants, and other biological tissue) using the method proposed by Koeniger et al. (2011 in RCMS). The basic principle is that a sample under vacuum is heated in the dry block heater and the evaporated water is in turn condensed in liquid nitrogen. This approach has not been as widely applied as cryogenic vacuum distillation systems built with glass or metal manifolds; however, the principles of use are identical. Again, we are happy to serve as a resource for the community - please reach out if you have analyses in mind or if you would like the parts list that we developed for how to build this instrumentation.\nFolks in the lab are pursuing a number of different innovative projects right now, ranging from understanding how environment contributes to the evolution of new species in the Tropics to how water moves through landscapes. What is interesting is how many of those innovative projects are leaning on very classic methods - we are measuring soil texture, quantifying stem anatomical traits, and counting stomata. Looking forward to seeing the results!"", 'One of the projects of Dr. Youbin Zheng, associate professor in the University of Guelph’s School of Environmental Sciences, has the goal of determining the best light spectral combinations and intensities for indoor/warehouse and greenhouse microgreen production. This study, supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) and Greenbelt Microgreens, involves investigation of both yield and quality (for example, nutritional value and post-harvest shelf life).\nZheng is also developing a feedback control system to manage greenhouse lighting in order to save energy and maximize crop production. The work is supported by the Ontario Ministry of Agriculture, Food and Rural Affairs (OMAFRA) and Heliospectra.\n“Our recent trial compared the use of a conventional HPS control strategy with real-time feedback-controlled LEDs (which are dimmable) for the production of cut gerbera in Ontario during the normal supplemental lighting season of November to March,” notes Zheng.\n“Results showed that the LED treatment had the same or better crop growth, harvest and quality metrics compared with HPS, while also delivering 15 per cent less supplemental light overall to the crop throughout the season.”\nDYNAMICALLY CONTROLLED LEDS\nZheng says these results provide a compelling argument for the use of dynamically controlled LEDs as a supplemental light source for greenhouse production in the darker months in northern latitudes.\nAnother of his studies involves using LEDs to control plant height. Bedding plant growers often produce hanging baskets above the main lower crop level (bench or floor) in the same greenhouse to maximize use of production space, he notes. This practice reportedly can result in stretched plants at lower crop level, commonly considered a result of a combination of high-density plantings, competition for available light and changes in spectral composition from passing through an upper canopy.\n“Our group has been investigating how to use different light spectral combinations and timing of light treatment on the morphological changes of several bedding plant species (e.g., petunia, marigold, geranium),” he explains, “to investigate whether we can control bedding plant height and flowering without using chemicals.”\nCUT FLOWER PROJECTS\nSupported by the International Cut Flowers Growers Association and Lumigrow, Zheng has also recently investigated the use of LEDs for producing cut flowers during the darker months in Ontario over three growing seasons.\n“The first year we compared HPS and LED supplemental lighting treatments (at the same crop-level intensity and photoperiod) to produce three cultivars of cut gerbera,” he says. “The LED treatment provided similar or better production, harvest quality and postharvest shelf-life indices compared with HPS treatment.”\nIn the second year, a single cultivar of cut snapdragons was produced using four different canopy-level supplemental LED intensities, and in the third year, two cultivars of cut gerbera (one starting in plug-stage and the other already in flower) were grown under five different supplemental light treatments.\nThe results of both of these trials can be used to help growers determine the optimum lighting setup for their specific production systems.\nZheng and his colleagues are also actively investigating the responses of different plant species to different LED light spectra and spectral combinations to unlock some of the mechanisms involved. This will guide the development of different lighting recipes for specific functions such as increasing or decreasing height, branching, increasing biomass production, promoting flowering and increasing certain nutrients in edible crops.\nYEAR-ROUND VEGETABLE CROPS\nOne research project led by Dr. Xiuming Hao is focussed on economical year-round vegetable production.\n“In Ontario and most places in Canada, we are short on light from October to March,” says Hao, who is based at the Agriculture and Agri-Food Canada (AAFC) centre in Harrow, Ontario. “Growers want to plant tomatoes, cucumbers and peppers in September or October in order to harvest around Christmastime to get the best price. Supplemental lights can achieve this, but variety, nutrition and climate control are important factors affecting plant growth and fruit production and need to be adjusted.”\nThis and Hao’s other projects are being funded by AAFC Peer Review Program, AAFC’s AgriInnovation Program and the Ontario Greenhouse Vegetable Growers.\nHao and his colleagues have already found that lower intensity over a longer duration costs less in capital expenses with fewer light fixtures required per unit area. However, crops exposed to longer periods of light can be pushed beyond their limits; the yellow spots of chlorosis will appear on leaves and there can be little yield increase.\n“We’ve developed a solution where we lower the temperature right after the supplemental lighting was shut-off during the darkness so the crop has a better rest and is not so stressed by the increased duration of light,” Hao explains. “A temperature of 13.5 to 14 C is suitable as a minimum for tomatoes, 13 C for cucumbers and 15.5 to 16 C for peppers, to maintain quality. There are also small differences among varieties.”\nNot only does the temperature drop give the crop a good rest, but while it occurs, leaves cool faster (due to their large surface area in relation to volume) than the fruit (larger volume). This causes the plant to focus on growing the fruit, resulting in faster fruit growth and increased yields.\nHao notes that it takes one to two hours to lower the temperature depending on the temperature outside, and 30 minutes to one hour to bring it back up.\n“You only want the temperature to stay lowered for about a half-hour to one hour,” he says, “just giving the plants a little shock.” In addition to ensuring better yield, lowering the temperature also saves growers money on heating.\nView the embedded image gallery online at:\nFAR-RED LED LIGHTING\nHao is also studying far-red LED light, wavelengths in the 720 to 750 nm range out of the visible range of humans. Applying far-red light at the top of plants is interpreted by them to mean they are being shaded, and in response stems and leaves stretch to reach for more light.\n“Most greenhouses currently use HPS lights that produce lots of heat and the plants are very compact,” Hao notes. “However, with bushy plants, the light is being blocked by leaves and not utilized efficiently by the plant. With far-red LED, you get a longer plant that can intercept and absorb more light, and can achieve higher yields in the early production period in tomatoes and sweet peppers. Later on, the canopy is thicker and there is no effect.”\nCucumbers do not show any effect, he explains, because they grow very quickly in comparison.\nHao is also in the first year of a three-year comprehensive study looking at the use of LED lighting to improve crop growth, fruit yield, quality (including anti-oxidants) and pest control efficiency on tomatoes, peppers and cucumbers. Team members include Dr. Rose Labbé at Harrow (biocontrols and pests); Dr. Aiming Wong of AAFC London Research Centre (how LEDs might help greenhouse vegetable crops increase tolerance or resistance to virus disease); Dr. Rong Cao of AAFC’s Guelph Research Centre (antioxidant levels in fruit); and University of Guelph scientist Dr. Bernie Grodzinski (how LEDs can speed up the movement of starch from the leaves to the fruit for increased yield).\n“Red light is good for growth but blue light and UV-A light is better for fruit quality,” Hao explains. “With LED lighting, you can change the wavelengths at various heights. With red light and far-red light at the top and blue light in the middle, we try to increase both fruit yield, and quality and antioxidants at the same time.”\nFor fruiting greenhouse vegetables such as tomatoes, the translocation or movement of photo-assimilate (sugar etc.) from leaves to fruit is very important. Jason Lanoue (a PhD student at the University of Guelph under the supervision of Grodzinski and Hao) is studying various spectra of LED and developing LED light recipe to promote the translocation of photo-assimilates from leaves to fruit for increasing fruit yield.\nAt Wageningen University in the Netherlands, Dr. Anja Dieleman has been working on LED greenhouse lighting for five years.\n“Several years ago, one of my colleagues, Tom Dueck did multiple trials in which he compared LEDs with HPS and hybrid systems (HPS+LED inter-canopy lighting; LED + LED inter-canopy lighting) in tomatoes,” she notes, “with the result that these systems were comparable in crop production.”\nDuring the last few years, within the framework of the EU HI-LED Project (www. Hi-led.eu), Dieleman and colleague Esther Meinen have been studying the effects of light colours on young pepper and tomato plants. They found two hours of green light at various times of day all provided taller plants with more open canopy, with light better able to penetrate the crop. These results show that green light provides plant ‘stretching’ effects similar to far red light. The effect of other colours was similar to white light.\nDieleman and her colleagues have also developed lighting recipes in which plants received a few hours of blue or green light in the morning followed by red light during the rest of the day (in addition to normal solar greenhouse light). Several hours of blue light at the start of the day led to an eight per cent increase in tomato production.\n“In the last few years we have been working on adding far-red light in the cultivation of tomatoes, with striking results,” Dieleman adds. “Adding far-red light favours assimilate partitioning to the fruits, at the expense of the leaves, which led to a five to 20 per cent increase in tomato fruit production depending on the variety.”\nDieleman and her colleagues are also still working on the design of low carbon footprint production systems for tomato production, based on LED lighting and varieties that suit this lighting system best.\nTreena Hein is a freelance writer in Ontario and a frequent contributor to Greenhouse Canada.']"	['<urn:uuid:dca949e0-fed0-4863-9e2c-98aefee1fa77>', '<urn:uuid:dcb15ea8-8d52-4dce-95ee-6d6975abad49>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T10:32:05.666290	10	129	3491
13	What are the dangers that make teenage drivers so vulnerable to accidents, and what steps can parents take to help their children become safer drivers?	Traffic crashes occur disproportionately among newly driving young adults, with one in four crash fatalities involving 16-24 year olds. The first six months after licensure are the most dangerous, with crash fatality rates being highest for 16-17 year olds. Three out of four serious teen driver crashes are due to inexperience. To help address this, parents should ensure their teen receives at least 40 hours of additional behind-the-wheel instruction from a licensed adult under various driving conditions, including nighttime and inclement weather driving. Additionally, enrolling teens in an accredited driver's education program can help improve their confidence and skill level, teaching them about driving responsibilities and safety practices.	['Supplement Editor, Dr Flaura K Winston, and Co-Editor, Dr Teresa Senserrick, introduce 10 papers covering the current science of safe driving among adolescents from the varied viewpoints of an international panel of experts. This Expert Panel, convened by the Center for Injury Research and Prevention (formerly TraumaLink) at the Children’s Hospital of Philadelphia and State Farm Insurance Companies®, working jointly on the Youthful Driver Research Initiative, represents a wide range of expertise, thereby providing a broad understanding of driving, adolescence, and adolescent driving.\n- risk behaviors\n- traffic crashes\nStatistics from Altmetric.com\nIf you wish to reuse any or all of this article please use the link below which will take you to the Copyright Clearance Center’s RightsLink service. You will be able to get a quick price and instant permission to reuse the content in many different ways.\nAdolescence encompasses the exciting, but challenging, transition from childhood to adulthood. During this period, which is marked by dramatic psychological and social maturation, many adolescents learn to drive. Positive development requires adolescents to learn new skills and have the freedom to explore the limits of their abilities, negotiate relationships, and—at times to their detriment—experiment with risk behaviors. Unfortunately, driving has a small margin for error and the mix of adolescent development and driving too often leads to tragic outcomes.\nTraffic crashes occur disproportionately among newly driving young adults,1 with one in four crash fatalities in the United States involving 16–24 year olds (based on US Department of Transportation’s Fatality Analysis Reporting System data (see http://www.nhtsa.dot.gov/people/ncsa/fars.html. The crash fatality rate (crash fatalities/100 000 population) is the lowest for learners and the highest for 16–17 year olds; the first six months after licensure are the most dangerous, and the rate remains high through age 24. For the age group 16–24, the crash fatality rate in 2003 was nearly twice as high as other age groups: 27.9 deaths/100 000 population for 16–24 year olds, compared with 15.4 for 25–54 year olds and 16.3 for those 55 and older.\nThe goal of adolescence is the emergence of a healthy, competent, independent adult. How, then, do we achieve competent, independent driving during adolescence without tragic outcomes? If the current crash fatality frequency continues unabated, over the next 10 years 100 000 adolescents and young adults will die in the United States alone. The purpose of this supplement is to explore the intersection between research on adolescents and research on driving in order to make recommendations for future, synergistic approaches.\nThe papers in this supplement were first presented in September 2005 in Boston, Massachusetts, USA, as part of an international panel of experts convened by the Center for Injury Research and Prevention (formerly “TraumaLink”) at the Children’s Hospital of Philadelphia (http://traumalink.chop.edu), Philadelphia, PA, USA, and State Farm Insurance Companies® (http://www.statefarm.com), Bloomington, IL, USA, working jointly on the Youthful Driver Research Initiative. The Expert Panel members represent a wide range of areas of expertise, thereby providing a broad understanding of driving, adolescence, and adolescent driving. Each expert was asked to prepare a written summary report of state-of-the-art research and knowledge in their expertise area and apply it to the task of driving, and then to revise their text based on the discussions at the meeting and the most current research.\nIn the opening paper of the series, Allan Williams, PhD, formerly of the Insurance Institute for Highway Safety, Arlington, VA, USA, highlights the importance of a comprehensive approach built on the foundation of Graduated Driver Licensing laws to achieve meaningful young driver crash reductions. He provides an overview of the risky driving behaviors and situational driving risks of young beginning drivers, exploring the role of age, developmental, and inexperience factors.2 He reviews past education, training, and licensing and enforcement initiatives that have aimed to address these risk factors, and suggests ways to strengthen them.\nThis is followed by a comprehensive overview of young driver behaviors, factors that influence them, and the implications for interventions from a public health perspective by Jean Shope, MSPH, PhD, Research Professor and Associate Director at the Transportation Research Institute, School of Public health, University of Michigan, USA.3 Concurring with Dr Williams, Dr Shope asserts that comprehensive, multilevel, theoretically grounded interventions are needed. In particular, three theoretical models are highlighted as most applicable to behavior change in young drivers. She details a framework of six categories of influences on youthful driving behavior, comprising driving ability, developmental factors, personality factors, demographics, the perceived environment, and the driving environment. Shope duly notes that certain factors (such as certain demographics, personality, and developmental factors) are not amenable to change, and that therefore intervention efforts must focus primarily on those factors that can be changed, using those that cannot to guide and inform.\nHans-Yngve Berg, PhD, of the Swedish Road Administration (Vägverket), broadens the perspective to include a European viewpoint.4 Of note, in Europe, similar beginning driver risks to those in the US are reported, despite a later licensing age (generally 18 years). Dr Berg stresses the need to consider how lifestyle and other social and demographical factors influence driving behavior and safety in young people. Within a hierarchical model, he demonstrates goals and motivational factors for driving that at their lowest level involve achieving vehicle control and mastery of traffic situations. At the higher levels he explores the goals and context of driving, and, most broadly, how driving fits within life goals and skills.\nNext, John Groeger, PhD, Professor of Cognitive Psychology at the Department of Psychology and a founding member of the Surrey Sleep Research Centre, University of Surrey, in the United Kingdom, argues “the young driver problem” as not one but a variety of multifaceted problems that therefore require multifaceted interventions, and highlights recent learnings from brain development and personality research. He highlights promising new avenues for investigation that can capitalize on increasing technological capabilities.5 His paper provides a thoughtful exploration of why inexperience and young age so significantly contribute to crash risk. Particular attention is given to the similar power-law relationships that exist between hours of driving and skill development, and months of driving or distance traveled and crash involvement. He concludes by emphasizing the importance of the much neglected but potentially profound role of fatigue in teen driver risk.\nDonald Fisher, PhD, Professor of Mechanical and Industrial Engineering at the University of Massachusetts-Amherst, USA and his colleagues Alexander Pollatsek and Anuj Pradhan present a practical, skill building intervention that targets inexperience of new young drivers in hazard detection.6 They report on a series of studies to develop and evaluate their personal computer based risk awareness and perception training program. The program aims to identify hazards, and explain why they are hazards. They demonstrate the ability of young drivers to achieve both “near transfer” (demonstrated ability to recognize similar driving hazards to those in the training), and “far transfer” (demonstrated ability to generalize the training to new hazards not represented in the training).\nBruce Simons-Morton, EdD, MPH, and his colleague, Marie Claude Ouimet, PhD, of the National Institutes of Health, Bethesda, MD, USA, offer recommendations to improve parental management of the learning and early driving experience.7 They suggest parents are likely to remain the primary agents responsible for preparing teenagers for independent driving. They explore parental roles in supervising practice driving, deciding when their teen is ready for independent driving, and managing the teen’s initial independent driving experience, particularly via use of a parent-teen agreement. They identify several gaps in knowledge in relation to the role of parents in learner and early independent teen driving.\nA very different, large scale approach to changing teen driving behavior—that of social marketing—is suggested by William Smith, EdD, of the Academy for Educational Development, Washington, DC, USA.8 The paper opens with examples of successes and cautions from previous social marketing campaigns in transportation and other public health domains, as well as particular learnings regarding teens and marketing. The differences between commercial marketing and social marketing are detailed, and nine fundamental principles of social marketing are discussed. Included are a framework for measuring the success of a social marketing effort, and issues regarding sustainability of a campaign.\nMost of the experts agreed that strong Graduated Driver Licensing laws should form the basis for young driver campaigns. In the next paper, Jacqueline Gillan of Advocates for Highway and Auto Safety, Washington, DC, USA, provides an overview of tactics used by her organisation and others in promoting effective initiatives to state and federal governments and organisations in the United States.9 Her paper focuses on licensing reform, including the specific statistics underlying the need for reform. The benefits of complementary education, supporting public opinion polls, and the need for continued improvements in vehicle crashworthiness are also discussed.\nThe final expert panel paper addresses the issue of applying interventions to a large population and the importance of cultural appropriateness.10 Nathan Stinson, Jr, MD, PhD, MPH, of the National Center for Optimal Health and the Department of Family and Community Medicine at Meharry Medical College, Nashville, TN, USA, and his colleagues, Paul Juarez, David Schlundt, and Irwin Goldzweig, apply a conceptual framework for optimal health to reducing risky driver behaviors among teens. The multilevel framework incorporates characteristics of the individual; contextual factors of the motor vehicle; and physical, social/cultural, political, and economic environments of the family unit, peers, local neighborhood, broader community, and society at large. Particular attention is given to the crucial role of restraints and differences in usage rates, intervention approaches, and acceptability issues among minority youth.\nThe Center for Injury Research and Prevention at the Children’s Hospital of Philadelphia\nThe Center for Injury Research and Prevention at The Children’s Hospital of Philadelphia, formerly known as TraumaLink, is a comprehensive pediatric injury research center based at the Children’s Hospital of Philadelphia and the University of Pennsylvania. The center consists of a multidisciplinary team of experts who conduct and disseminate research on the causes of childhood, adolescent, and young adult injury and develop and evaluate interventions to prevent injury and its psychological effects. For more information on the Center, please visit http://www.chop.edu/injury.\nThe Children’s Hospital of Philadelphia was founded in 1855 as the nation’s first pediatric hospital. Through its long standing commitment to providing exceptional patient care, training new generations of pediatric healthcare professionals, and pioneering major research initiatives, the Children’s Hospital has fostered many discoveries that have benefited children worldwide. Its pediatric research program is among the largest in the country, ranking second in National Institutes of Health funding. In addition, its unique family centered care and public service programs have brought the 430 bed hospital recognition as a leading advocate for children and adolescents. For more information, visit http://www.chop.edu.\nAbout State Farm®\nState Farm insures more cars than any other insurer in North America and is the leading US home insurer. State Farm’s 17 000 agents and 76 000 employees serve nearly 73 million auto, fire, life, and health policies in the United States and Canada. State Farm also offers financial services products. State Farm Mutual Automobile Insurance Company is the parent of the State Farm family of companies. State Farm is ranked number 18 on the Fortune 500 list of largest companies. For more information, please visit http://www.statefarm.com or in Canada http://www.statefarm.ca\nThe concluding paper in the supplement by Teresa Senserrick, PhD, of the Center for Injury Research and Prevention at the Children’s Hospital of Philadelphia, Philadelphia, PA, USA,11 provides guidance for reducing the tremendous burden of crashes from young drivers. She suggests the highest priority young driver skill deficits and risk factors to address and how best to address them. She integrates the papers in the supplement and other research to recognize the complexity of the field, but she points to reasons for optimism. In particular, she highlights an emerging scientific foundation based on new research insights into adolescent development and driving, promising intervention directions, advanced safety technologies, and better understanding of effective policy and communication efforts.\nWe are in an exciting era of rapid advances in our understanding of both adolescent development and the risks and skills associated with driving. These two previously separate areas of inquiry are beginning to merge to address a leading cause of death and acquired disability for youth and young adults globally: traffic crashes. Technological advances in advanced safety technologies and monitoring as well as the collective will for policy and legislative action as demonstrated through Graduated Driver Licensing laws point to a promising platform in which research can be translated into meaningful action and impact. While it is important to address the high young driver crash issue as efficiently and rapidly as possible, it is important to proceed with cautious, measured optimism. Many stakeholders from manufacturers to insurers to educators to policymakers and advocates are committed to the cause of reducing young driver fatalities, but most of their efforts are delivered to the public without plans for careful evaluation. We hope that this supplement provides those interested in this topic with a healthy view that this is not a simple problem with a quick-fix solution. Evaluation of the success as well as any unintended consequences will be essential as we proceed. Only a comprehensive iterative process of research, informed by adolescent development and current and local cultural and societal contexts, and leading to informed comprehensive intervention development, evaluation, and diffusion will lead us to meaningful reductions in road traffic deaths. It is important to incorporate the wisdom of teenagers and young adults in intervention development to ensure that the interventions are not only efficacious but also acceptable. It may be helpful to think of novice driving as an archetypal task of adolescence. Here is the challenge before us: non-driving teens, dependent on their parents and others for mobility, must become independent individuals who learn to make safe decisions, manage their passengers and peers, avoid the hazards of the road, while continuing to develop new competencies and capabilities and explore new horizons.\nThis paper was written as part of the Youthful Driver Research Initiative, a collaborative research program between the Center for Injury Research and Prevention at the Children’s Hospital of Philadelphia (CHOP) and State Farm Insurance Companies® (State Farm). The views presented in this paper are those of the author(s) and are not necessarily the views of CHOP or State Farm. The authors would like to acknowledge the tireless dedication of their colleagues at CHOP, in particular, Lauren Hafner, MPH, D Alex Quistberg, Dennis Durbin, MD MSCE, and Kenneth Ginsburg, MD MSED, for their careful reviews, and at State Farm, Cynthia Garretson, CPCU, John Nepomuceno, MBA, and John Werner, MS.', 'Learning how to drive is a major milestone in anyone’s life. If your son or daughter is getting behind the wheel for the first time, you’re probably a very proud parent. And you know it’s important to teach your teen that driving is a privilege – not a right.\nDriving is a complex task that involves mastery of multiple skills. It requires a full understanding of the rules of the road and how to adjust to different driving situations and circumstances. Over time, your son or daughter may be able to predict actions other drivers may take.\nEnrolling your teen in an accredited driver’s education program can help improve their confidence and skill level – and can give you peace of mind when they’re on the road.\nIs driver’s ed required for your child?\nIn many states, teens aren’t required to complete a driver’s ed course to obtain their driver’s license. New York, for instance, doesn’t require driver’s ed; California and Texas do.\nGeorgia requires drivers to complete a certified driver’s education course to receive their license at 16. Otherwise, they must wait until 17. The law, Joshua’s Law, was passed after teenager Joshua Brown died following a car accident on a wet road.\nSince laws regarding driving and driver’s licensing vary by state, check your state’s department or bureau of motor vehicles website to find out about specific requirements.\nIs driver’s ed worth it?\nDriver’s education courses can offer very real benefits:\nYour child can become more confident.\nWhen your teen successfully completes a driver’s ed course, they may feel more self-assured about driving because they’ve learned best practices from a professional training instructor.\nYour child will learn about the responsibilities of driving.\nAfter completing a driver education class, your child will have a better understanding of the responsibilities all drivers share.\nYour child can become a safer driver.\nIn addition to teaching driving etiquette, certified driver’s ed courses address safety. Young drivers are taught the rules of the road and the importance of avoiding distractions while driving. Instruction may also include safety tips specific to your area – like tips about driving in different weather conditions and navigating busy roads.\nIs driver’s ed alone enough?\nWhile enrolling your child in a certified driver’s ed course is an important first start, it should only supplement the other driving lessons they receive. Teens are new to driving and they need as much experience as possible. Three out of four serious teen driver crashes are due to inexperience, according to the Children’s Hospital of Philadelphia Research Institute.\nWe recommend that your teen driver receives at least 40 hours of additional behind-the-wheel instruction from a licensed adult, preferably under a variety of different driving conditions, including nighttime and inclement weather driving. Most states require 40 to 50 hours of supervised driving by law, so check your state’s bureau of motor vehicles website for more information.\nTeens often emulate how their parents drive, so be sure to set a great example. It’s likely that your teen will make a few mistakes. You should always remain calm and provide clear instructions ahead of time.\nDuring this exciting journey, check out our teen driving resources for more useful information.']	['<urn:uuid:a7f89296-08e5-40fd-896b-4904017eeccd>', '<urn:uuid:00321df4-7697-4c18-a45a-870709e38195>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:32:05.666290	25	108	2952
14	As a wildlife researcher studying desert adaptations, I'm curious about the Thorny Devil's water collection mechanisms and how these might become more crucial given Australia's changing rainfall patterns. Could you explain how this lizard collects water and why this ability is particularly relevant given Australia's current climate trends?	The Thorny Devil has evolved a unique water collection system where narrow channels between its scaly spines draw droplets of rain or dew and transport it to its mouth through capillary action. This adaptation helps them survive in dry lands. This ability is becoming increasingly important as Australia faces significant climate-related water security challenges. Southeast Australia has experienced a 15% decline in late autumn and early winter rainfall, with a 25% decline in average rainfall in April and May over recent decades. With droughts becoming more severe due to drier, hotter conditions and increased water loss from plants and soils, such specialized adaptations for water collection are particularly valuable for desert species' survival.	['The Thorny Devil is a species of lizards found in Australia. It is known for its unique spiny appearance and chameleon-like color changing abilities. It is the only species belonging to the genus Moloch and is known by various names such as the Mountain Devil, Thorny Lizard, Thorny Dragon as well as Moloch.\nThorny Devil Scientific Name\nThe scientific name for Thorny devil is “Moloch horridus”.\nThorny Devil Description\nThe Thorny Devils have got the most unique appearance among all lizard species. Here is a brief description of these creatures.\nLength: Thorny Devils grow up to almost 20 centimeters in length.\nWeight: These lizards weigh around 70 to 95 grams.\nSpines: Their bodies are entirely covered with mostly uncalcified conical spines which even extend up to their tails. They have spiny horn-like structures above their eyes and a spiny bump behind their head that acts like a false-head.\nColor: The bodies of Thorny Devils are covered with camouflaging shades of reddish-brown, orange, white, yellowish tan and black. These colors vary with temperature and surroundings, changing from paler shades in warm weather to darker shades during cold weather.\nSexual Dimorphism: The females are larger in size than the males.\nThorny Devil Distribution\nThorny Devils mostly reside in Central and Western Australia.\nPicture 1 – Thorny Devil\nThorny Devil Habitat\nTheir preferred habitats include shallow burrows in deserts and arid scrubs.\nThorny Devil Behavior\nHere are some common behavioral traits of these species.\n- In spite of having a fearful appearance, these lizards are one of the most harmless creatures on earth, preferring to evade predation through camouflage and illusions.\n- While walking, they raise their tail and move slowly with jerky motions. They often tend to freeze themselves in mid stride.\n- They prefer to eat their meals slowly, consuming only one ant at a time.\n- During nights, these lizards dig into the soil and bury themselves to keep themselves warm. In times of extreme heat, they create shallow underground burrows to protect themselves from blazing temperatures.\n- They often hide themselves behind small shrubs to escape anything that they consider disturbing.\n- Thorny Devils lead mostly a solitary life except when they choose to mate.\nThorny Devil Diet\nThorny Devils mostly consume ants and termites in their diet. Occasionally when ants are scarce, they can also eat some other small insects.\nThorny Devil Predators\nThorny Devil is a prey animal. Its list of predators includes foxes, goannas, bobcats, coyotes, bustards, brown falcons and snakes.\nThorny Devil Adaptations\nThese lizards have developed some unique features to adapt themselves well to their environments as well as to save themselves from their predators.\n- Their bodies are covered with spines which make it difficult for their predators to swallow them.\n- They can change their body color and camouflage themselves by blending with their environment.\n- They have a unique way of collecting drinking water which helps them to survive on dry lands. Narrow channels between scaly spines draw droplets of rain or dew and transport it to their mouths through capillary action.\n- When predators are near, they hide their real head between their front legs and project their spiny false head towards them.\n- They can also inflate themselves and look bigger than their actual size. This they do to scare off predators.\nPicture 2 – Thorny Devil Image\nThorny Devil Mating Season\nTheir mating season lasts from September to December.\nThorny Devil Reproduction\nThorny Devils start mating at an age of 3 years. The males attract females with elaborate courtship rituals which include head-bobbing and leg-waving. The females lay a clutch of 3 to 10 eggs around the months of September to December in an underground nesting burrow.\nThe burrows are dug by the females around 30 centimeters below the ground. The females lose nearly 40% of their body weight while laying the eggs. The eggs are incubated for about 3 to 4 months, after which the hatchlings are born.\nThorny Devil Life Cycle\nAfter coming out of the eggs, the young Devils dig their own way up to the surface. The newly born lizards are also equipped with spines just like their parents. Both the male and female lizards grow at same rates during the first year, after which the females start growing at a significantly greater pace. The growth rate varies from season to season and is slower during the winter. It takes at least 5 to 6 years for young Thorny Devils to reach full maturity.\nThorny Devil Life Span\nThorny Devils live up to 15 to 20 years.\nThorny Devil As Pets\nHousing: These creatures should be housed in an aquarium big enough for them to move around freely. Artificial sand dunes should be made inside along with some thorny bushes to provide them with a natural environment feel. Temperature should be maintained a dry one like their natural habitat.\nFeed: They can be fed some commonly found small insects and ants.\nCare: They do not need much caring. Human handling should be checked as they are not accustomed naturally to be handled. While handling them their spines can even prick the human hands.\nThorny Devil Conservation Status\nThorny Devil is not yet classified by the IUCN. However, their numbers are gradually decreasing in present times.\nThorny Devil Interesting Facts\nHere are some interesting facts about these lizards.\n- These lizards can consume more than a thousand ants at a time.\n- Thorny Devils eat almost 45 ants in a minute.\n- They can run at a maximum speed of 60 kilometers per hour.\n- Although Thorny Devils look fierce, they are not aggressive and would rather make use of their unusual adaptations to defend themselves.\nThorny Devil Pictures\nHere are some images showing these wonderful and interesting lizards.\nPicture 4 – Thorny Devil Picture', 'Deluge & drought: Water security in a changing climate\nAustralia’s water security has already been significantly influenced by climate change. Rainfall patterns are shifting and the severity of floods and droughts has increased.\n- Droughts are becoming more severe due to drier, hotter conditions, leading to declines in soil moisture due to increased water loss from plants and soils.\n- Southeast Australia has experienced a 15% decline in late autumn and early winter rainfall, and a 25% decline in average rainfall in April and May over the past two to three decades. This area includes major population centres of Brisbane, Sydney, Canberra, Melbourne and Adelaide.\n- Hotter conditions and reduced rainfall have led to less runoff into streams, rivers, lakes and dams in the southwest and southeast of the continent. In southwest Western Australia, reductions in rainfall, due to climate change, have led to a more than 50 percent decline in streamflow. Across the Murray-Darling Basin, streamflows have declined by 41 percent since the mid-1990s.\n- A warmer atmosphere can hold more water vapour, contributing to an increase in heavy rainfall events and an increased risk of flash flooding.\nThe severe drought being experienced across Queensland, NSW and northern Victoria is being influenced by climate change.\n- The severity of the current drought is being increased by the long-term declines in rainfall and the hotter conditions associated with climate change.\n- Since the mid-20th century, the severity of droughts, such as the Millennium Drought, has also been increased by climate change.\nOn-going failure to reduce greenhouse gas emissions from coal, oil and gas, globally and here in Australia, has already negatively affected Australia’s water security and will increasingly affect it into the future.\n- Profound changes to Australia’s water cycle are projected, with increasing threats to our urban water supplies, the agriculture sector and natural ecosystems.\n- Severe droughts are expected to become more frequent, especially across southern Australia, while extreme rainfall events are expected to become more intense everywhere except, perhaps, for the southwest corner of Western Australia.\n- Across southern Australia, cool season rainfall is projected to continue decreasing and time spent in drought is projected to increase.http://ozewex.org/wp-admin/post.php?post=3646&action=edit\n- Less water is likely to be available for agriculture, urban water supplies and ecosystems in coming decades across southern Australia including regions surrounding Melbourne, Adelaide and Perth.\n- A 2°C rise in average global temperatures could lead to an 11–30 percent increase in extreme rain events (wettest day of the year and wettest day in 20 years) across Australia.\nSignificant impacts on and risks to our water security are already evident, and these risks will continue to escalate unless deep and rapid reductions in global greenhouse gas pollution can be achieved.\nHealth: Severe droughts, heavy rainfall and floods all affect our health in many ways – contaminating water supplies, increasing mosquito-borne diseases such as Dengue and Ross River virus, and increasing psychological stress in rural communities.\nAgriculture: Drought has a significant impact on agricultural industries and communities. Severe droughts kill livestock, destroy crops and increase soil erosion, leading to higher food prices and loss of livelihoods.\nWater supplies: Less water is likely to flow into dams in southern Australia as a result of human-driven climate change.\nWater infrastructure: Water related infrastructure, such as water supply reservoirs, dam spillways and river levees, have been designed for historic rainfall patterns. Upgrading this infrastructure to cope with increased flooding and drought, as well as building new infrastructure like desalination plants, is expensive. Over $10 billion has been spent recently on desalination plants to improve water security in our major cities.\nEnergy: Coal, gas and hydro power stations require significant amounts of water and can be negatively affected by drought.\nBushfires: Severe drought leads to higher bushfire risk as shown by the current bushfire season across the southeast of Australia. Changes in land cover due to fire can adversely affect catchment water supplies.\nFlooding: The economic consequences of floods and droughts are significant; the extensive Queensland floods of 2010-2011, for example, cost to the state more than $6 billion (directly).\nPlants, animals and ecosystems: Declining rainfall in southwest Western Australia has affected freshwater fish species. The Murray-Darling Basin has been under considerable pressure, further reductions in rainfall and runoff will make it even harder to rehabilitate degraded aquatic ecosystems, affecting bird and fish life. In 2016 warmer and drier conditions in Tasmania triggered bushfires that severely damaged over 70,000 hectares of western Tasmania’s World Heritage-listed forests and alpine areas.\nIncreasing global water insecurity is becoming a ‘threat multiplier’, with significant implications for Australia and other regions.\n- The worst drought in Syria’s history, likely influenced by climate change, was a factor in triggering conflict and instability in that region, leading to a surge of refugees into Europe.\n- Agricultural systems on the Indian sub-continent are vulnerable to the melting of Himalayan glaciers and instability in the Indian monsoon system, with implications for political and social stability in our region.\n- The global food trade system is vulnerable to prolonged and severe droughts in major food-producing regions, such as the central United States and southeast Australia.\nAustralia’s water security is dependent on action on climate change, particularly on the rapid phase-out of fossil fuels.\n- Australia’s water security is threatened by climate change. Coping with increased frequency and severity of drought and floods is costly and will become progressively more challenging into the future.\n- Continuing on our current trajectory of high emissions has enormous and growing risks.\n- Short-term drought solutions will ultimately be futile without concerted and rapid action to tackle climate change, both here in Australia and globally.\nOriginally published by the Climate Council, 13 November 2018.']	['<urn:uuid:76beea31-9a34-4939-9a0f-bae8f3e62c65>', '<urn:uuid:c894755f-5c77-425d-8e40-4bb167c694ac>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:32:05.666290	48	113	1916
15	How do pest detection methods differ between mites and nematodes?	Spider mites can be detected using a hand magnifying lens to spot the minute creatures (<1/60 inches long) that look like specks of dirt, and can be observed by shaking leaves onto white paper to see their movement. They appear cream or green when feeding, or orange to red in unfavorable conditions. For nematodes, detection is more complex and often requires transcriptomic analysis and structural imaging of feeding sites, as damage can occur without visible above-ground symptoms.	"['Twospotted spider mite is an occasional pest of both corn and soybean in Iowa that is exacerbated by dry conditions. Most parts of Iowa are considered abnormally dry right now, according to the National Drought Mitigation Center in Lincoln, Neb. Soil moisture conditions this spring are similar to the last drought that hit Iowa in 1988; the same year Iowa experienced a statewide outbreak of spider mites.\nSpider mites generally reach economically damaging levels in late July or early August when conditions are favorable for their growth. However, twospotted spider mite can start building populations in June during years with early-season temperatures greater than 85°F, humidity less than 90 percent, and low moisture levels. These are ideal conditions for the twospotted spider mite, and populations are capable of increasing very rapidly. Twospotted spider mites have been reported by several crop consultants in southwestern Iowa this week.\nA hand magnifying lens is recommended to scout for the minute (< 1/60 inches long) twospotted spider mites. They can be mistaken for specks of dirt to the naked eye (Photo 1). Twospotted spider mite larvae have six legs and nymphs and adults have eight legs. Mites can be removed from collected leaves by shaking the leaves onto a white piece of paper and then looking for moving mites. Twospotted spider mites are typically a cream or green color when feeding on corn or soybean. They can also be an orange to red color when conditions are unfavorable for their growth.\nTwospotted spider mites begin feeding on the bottom of the plant and move to the top as the plant’s health deteriorates. Although they lack wings, twospotted spider mites disperse with the wind to move from dying plants to areas with healthy plants. Therefore, it is important to scout healthy areas of an infested field that are downwind from damaged areas. Early symptoms of twospotted spider mite damage will appear as small yellow dots or stipples on the lower leaves of the plants. Prolonged feeding will cause the infested leaves to turn completely yellow, then brown, and eventually the leaf will die and fall from the plant. The webbing is visible on the edges and underside of leaves and is an indication of prolonged colony feeding (Photo 2).\nThere are not established economic thresholds for twospotted spider mites in corn and soybean, but scouting for initial infestations is very important to avoid yield loss. Twospotted spider mite is capable of reducing soybean yield by 40 to 60 percent when left untreated; drought-stressed plants could experience even more yield loss.\nOrganophosphates are the recommended insecticidal chemistry for twospotted spider mite control. Examples include dimethoate and chlorpyrifos. These products may not kill the eggs, thus a treated field should be scouted 7 to 10 days after application to determine if a second application is necessary. As always, refer to the label for the appropriate rates and re-entry intervals. Pyrethroid insecticides should not be used to control twospotted spider mites as they are not as effective and can actually flare populations. Under dry conditions, foliar treatments are recommended when plants have substantial stippling or leaf-yellowing and spider mites are active (Photo 3). Because a naturally-occurring fungus can control populations, treatment of twospotted spider mites may not be required when temperatures drop below 85°F and humidity levels are greater than 90 percent for an extended time. Mites that are infected by the fungus will appear brown and will not move on the piece of paper used for scouting.', ""Plant Parasitic Nematodes\nPlant-parasitic nematodes are among the most devastating crop pests in the U.S. The USDA’s Committee on National Needs and Priorities in Nematology estimated that the value of plant damage associated with nematodes can reach up to 7 to 9 billion dollars annually in the U.S. and over 100 billion dollars worldwide. Root-knot nematode and cyst nematodes are the biggest problems in crop plants today accounting for about 75% of the damage worldwide. Frequently, estimated damages caused by plant-parasitic nematodes are considered underestimates, as plants can often suffer approximately 35% yield loss without showing noticeable above-ground symptoms under field conditions. Plant-parasitic nematodes are well known for their ability to parasitize roots and remove nutrients normally used for plant growth.\nCurrent research in the laboratory focuses on several areas of study.\nRoot-Knot Nematode Research\n- Development and remodeling of cell architecture during feeding site development. Focus is on the structural imaging of root-knot nematode feeding sites.\n- Transcriptomic analysis of nematode feeding sites: Understanding what genes are expressed during nematode parasitism and in the nematode feeding site.\n- Transcriptomic analysis of adult root-knot nematodes. Examination of nematode genes active during adult feeding.\n- Understanding the role of amino acids during nematode parasitism. Examining the role of amino acids in chemotaxis and nutrition of the root-knot nematode.\nSoybean Cyst Nematode Research\n- Analysis of virulence of soybean cyst nematode in Ohio.\n- Transcriptomic analysis of adult soybean cyst nematodes. Examination of nematode genes active during adult feeding.\n- Development of nematode control strategies for soybean cyst nematode. Examining both biocontrol and activation of plant defenses for soybean cyst nematode control.\nWalsh, E; Elmore, JM; Taylor, CG. 2017. Root-Knot Nematode Parasitism Suppresses Host RNA Silencing. Molecular Plant-Microbe Interactions 30:295-300.\nTesten A, Walsh EK, Taylor CG, Miller SA, Lopez-Nicora HD. 2014. First report of bloat nematode (Ditylenchus dipsaci) infecting garlic in Ohio. Plant Disease 98: 859\nMarella, HH; Nielsen, E; Schachtman, DP; Taylor, CG. 2013. The Amino Acid Permeases AAP3 and AAP6 Are Involved in Root-Knot Nematode Parasitism of Arabidopsis. Molecular Plant-Microbe Interactions 26:44-54.\nMorse AM, Carballo V, Baldwin DA, Taylor CG, and McIntyre LM. 2010. Comparison between NuGEN's Ovation Pico and One-Direct Amplfication System. Journal of Biomolecular Techniques 21:141-147.\nBerg RH, Fester T, Taylor CG. 2008. Development of the root-knot nematode feeding cell. In: Cell-biology of plant nematode parasitism. (Eds. Berg RH, Taylor CG). pp. 115-152.\nLi Y, Fester R, Taylor CG. 2008. Transcriptomic analysis of nematode infestation. In: Cell-biology of plant nematode parasitism. (Eds. Berg RH, Taylor CG). pp. 189-220.\nFester T, Berg RH, Taylor CG. 2008. An easy method for the microscopic analysis of plant biotrophic interactions. Journal of Microscopy 231:342-348.\nHammes UZ, Nielsen E, Honaas LA, Taylor CG, Schachtman DP. 2006. AtCAT6, an amino acid transporter that is expressed in sink tissues of Arabidopsis. Plant Journal 48:414-426.\nYang Y, Hammes UZ, Taylor CG, Schachtman DP, Nielsen E. 2006. High-affinity auxin transport by the AUX1 influx carrier protein. Current Biology 16:1123-1127.\nHammes U, Schachtman DP, Berg RH, Nielsen E, Koch W, McIntyre L, Taylor CG. 2005. Nematode induced changes of transporter gene expression in Arabidopsis roots. MPMI 18:1247-1257.\nOpperman CH, Taylor CG, Conkling MA. 1994. Root-Knot Nematode-Directed Expression of a Plant Root-specific Gene. Science 263: 221-223. Front cover publication.\nYamamoto YT, Taylor CG, Acedo GN, Cheng CL, Conkling MA. 1991. Characterization of Cis-Acting Sequences Regulating Root-Specific Gene Expression in Tobacco. Plant Cell 3: 371-382.""]"	['<urn:uuid:1a353887-079a-48f5-be75-7bbad9bf8010>', '<urn:uuid:c0bbc9c5-4672-4863-99bf-c2e49aaea0f6>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T10:32:05.666290	10	77	1145
16	I need a quick overview - who created the first MST algorithm?	Czech scientist Otakar Borůvka was the first to design a Minimum Spanning Tree (MST) algorithm in 1926.	['1st Algorithmic Breakthrough in 40 years for solving the Minimum Spanning Tree (MST) Replacement Edges problem\nOne of the most studied algorithms in computer science is called “Minimum Spanning Tree” or MST. In this problem, one is given a graph comprised of vertices and weighted edges, and asked to find a subset of edges that connects all of the vertices, and the total sum of their weights is as small as possible. Many real-world optimization problems are solved by finding a minumum spanning tree, such as lowest cost for distribution on road networks where intersections are vertices and weights could be length of the road or time to drive that segment. In 1926, Czech scientist Otakar Borůvka was the first to design an MST algorithm. Other famous approaches to solving MST are often given by the name of the scientist who designed MST algorithm in the late 1950’s such as Prim, Kruskal, and Dijkstra.\nSeveral important variations of MST are also used in real applications, including the replacement problem. Imagine a use case when an edge in the MST degrades and either has a significanly increased cost or is removed entirely. One must quickly find the lowest cost “replacement edge” that reconnects the spanning tree. Several algorithms are known for this MST replacement edge problem. The first algorithm, due to Spira and Pan in 1975, took cubic time in the number of vertices. They presented an $O(n^2)$ algorithm to update the MST when new vertices are added, and could find all replacement edges in $O(n^3)$ time, where $n$ is the number of vertices in the graph. This was improved by Chin and Houck in 1978 to a quadratic time algorithm, or $O(n^2)$, using a more efficient approach to insert and delete vertices from the graph. The best approach to date is due to Tarjan in 1979, who gave an $O(m \\alpha(n, m))$ time algorithm using path compression, where $m$ is the number of edges in the graph and $\\alpha(n, m)$ is the inverse Ackermann’s function. $\\alpha()$ is a very slow growing function, usually a number around 3 or 4 in practice, but still a gap has remained if a better approach exists.\nFor the first time in 40 years, progress has been made on this important graph algorithm. With Paul Burkhardt, we’ve designed a simple algorithm that runs very fast in linear time and space, or $O(n+m)$ where $n$ and $m$ are the number of vertices and edges, respectively, in the graph. The paper entitled A Linear Time Algorithm for Finding Minimum Spanning Tree Replacement Edges is now available in Arxiv. The main result of this paper is the first linear-time algorithm for finding all replacement edges in the minimum spanning tree. Our linear time and space algorithm is an asymptotic improvement from all prior algorithms, uses only simple arrays and Gabow-Tarjan disjoint set union data structures, alleviates the need to use least common ancestor (LCA) algorithms, and is easy to implement.\nOther important graph algorithms need to find replacement edges, a step often considered their bottleneck in performance. For example, the most vital edge of a connected, weighted graph $G$ is the edge whose removal causes the largest increase in the weight of the minimum spanning tree. When the graph contains bridges, the most vital edge is undefined. Several algorithms were designed in the early 1990’s for the most vital edge, including $O(m \\log m)$ and $O(n^2)$ time from Hsu et al. in 1991, and improvements to this approach by Iwano and Katoh in 1993 with $O(m+n \\log n)$ and $O(m \\alpha(m,n))$ time algorithms. When using our new MST replacement edge algorithm, we now can find the most vital edge in linear time (or $O(n)$) by simply finding the tree edge with maximum difference in weight from its replacement edge. Thus, our approach is the first linear algorithm for finding the most vital edge of the minimum spanning tree.']	['<urn:uuid:e48a6f10-618a-4678-93ff-859c558edb18>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	12	17	650
17	What protects data better, IPsec tunnel mode or HTTPS?	IPsec tunnel mode provides better protection as it encrypts and encapsulates the entire original IP packet, while HTTPS only encrypts the message content but not the routing information, making IPsec more comprehensive for securing data transmission.	['The Internet Protocol (IP) was the first way of transferring datagram across users. However, it lacked the ability to authenticate the data being sent. This lead to the creation of IPsec, which is one of the major online security protocols used by organizations.\nIn this article, you will be introduced to what IPsec is all about and how it works. There are many technical bits and terms, but there will be short explanations to make you understand better as you read.\nInternet Protocol Security (IPsec) is a set of protocols (or set conditions/agreement) that provides security between two communicating hosts. The host represents users like you and I. IPsec helps to provide data authentication and encryption.\nDue to its ability to provide security, IPsec is sometimes used for setting up virtual private networks (VPNs). IPsec uses a different set of rules (protocols) to carry out its functions and duties. These protocols are the Authentication Header (AH) and Encapsulating Security Payloads (ESP).\nWe now know that IPsec uses a collection of techniques and protocols to work. This makes it impossible to make it operate by a single standard. Instead, a collection of standards set by different publications authored by computer engineers and scientists.\nThese standards define the architecture, services, and specific protocols used in IPsec. It also helps the use of IPsec to stay uniform across the world. These collections of publications are officially known as “Request For Comments” (RFC).\nIPsec mode of operation\nTunnel mode involves the total protection of the entire original IP packet by IPsec. IPsec covers the genuine packet (which is a set of data being transferred), encodes it, appends new information about the IP (this information is called the IP header) and sends it to the other side of the tunnel (IPsec peer) which is mostly a VPN tunnel which the other host is connected to. Tunnel mode is most regularly used between gateways, or at an end-station to a gateway.\nTunnel mode is used to secure the data exchange between secure IPsec gateways. In tunnel mode, an IPsec header which can be either AH or ESP is installed between the upper layer protocol and the IP header. ESP is most commonly used in the IPsec VPN tunnel configuration over AH. It is used if one of the Internet Key Exchange (a sort of basic IPsec rules and regulations) peers is a security gateway (a middleman) applying IPsec on behalf of another host.\nThis mode actually encloses just the IP payload (IP payloads) to ensure secure communication. Think of the information you send as a human, having a head, arms, torso, and legs. The same goes for your data. The tunnel model encloses the whole “human” while transport mode encloses just the body, leaving the head out. Therefore, when transport mode is used, the IP header reflects the original source and destination of the packet. Transport is most often used in a host-to-host scenario, where the data endpoints and the security endpoints are the same.\nThere are two main parts that do the heavy lifting when it comes to IPSec. These parts are referred to as the protocol, although they are not a standard standalone protocol as they cannot function on their own.\nThese protocols are:\n- Authentication Header (AH): it provides verification services for IPsec. It allows the recipient of a message to verify that the supposed sender of a message was actually, in fact, the one that sent it. It also helps to discover disguise.\nIn addition, it allows the recipient to authenticate that intermediary devices during transmission have not tampered with any of the data being transferred and also provides security against “replay attacks”, whereby information is read by an unauthorized host and resent. Ultimately, AH ensures the integrity of the data in the datagram, but not its privacy.\n- Encapsulating Security Payload (ESP): this protocol helps to encrypt or encode your data as it is exchanged over the public network. Its main focus is the “body,” the data itself (IP payload).\nSecurity Association (SA)\nThe Security Association (SA) concept works directly with the protocols discussed. An SA is a relationship between two or more hosts. This relationship describes how the hosts make use of security services (such as IPsec) to communicate securely.\nIPsec provides many options for performing network encryption and verification. IPsec uses the security association to track all the details concerning a given security communication session. The good thing about this association is that it gives a host or computer the flexibility to choose any security service it desires.\nPhases of IPsec\n- Determine interesting traffic: Interesting Traffic means a data exchange that is worth protecting. The first phase of IPSec is to determine whether a particular connection is worth protecting or not.\n- Internet Key Exchange (IKE) phase 1: in conjunction with the service of IPsec, a key management protocol standard is used. It creates a shared secret key, which is used to decipher the encrypted data. Imagine person A locking a book meant for person B in a box, A then sends B a key so only he can open.\n- IKE phase 2: After IKE has established a secure tunnel, the IPsec policy and security association are established. The hosts must agree on a set of security protocols to use so that each one sends data in a format the other can understand.\n- Transfer data: After all this has been done, each device must use all the parameters ( protocols, methods, and keys) previously agreed upon to encode, send and decode data across the network.\n- On completing the data exchange, the tunnel of communication is immediately “torn down”.\nImplementation of IPsec\nImplementation is done in different ways because some feel IPsec should be installed on all hosts connected to a network and others feel it should be limited to specific hosts/routers.\n- End-host implementation: This involves having IPsec on all devices and this provides the most flexibility and security, but it is tedious.\n- Router implementation: this involves installing IPsec on specific routers which is a lot easier as only a few routers instead of hundreds or thousands of users need to be configured.\nTo get IPsec into a router or device, it also requires different methods. It can be installed in the layer of connection (IP stack) of an operating system. This procedure actually requires a modification of the source code. To carry this out you need to use this method for both the security gateway and the host. If you do not want the OS modified, you can use the bump-in-the-stack (BITS) implementation. Here, the IPsec is implemented between the network drivers and the IP stack.\nKey functions or services of IPsec\nWe have been talking about the technicalities of IPsec, let us now properly consider the functions of IPsec:\n- Confidentiality: IPsec helps to encrypt (to make a data only readable to authorized hosts) data so that only the desired hosts can read it.\n- Data integrity: it helps to determine whether during transmission the data has been changed or modified in any way.\n- Data authentication: is the sender/receiver who they say they are.\n- Anti-replay: it helps to ensure each packet is unique.\nAlternatives to IPSec\nWhile IPSec is a very basic security protocol offering security at the network (IP) level which is the root of connection. There are also other security protocols, that can be used out there:\n- Security at the connection level: this type of security enables tunnels to be set up when the hosts are prepped to be connected, unlike the IPsec that depends on the IP before establishing security. An example of this is the Point to Point Tunneling Protocol (PPTP) and Layer 2 forwarding protocol (L2F). These two protocols were later merged into the Layer 2 Tunneling Protocol. A major disadvantage of this protocol is the lack of security at the IP (packet) level.\n- Security at higher levels: You can decide to go further and have your security at a higher level. This type of security usually does not require any change to your device. Example of the type of security in this category is Secure Sockets Layer (SSL), Secure/Multipurpose Internet Mail Extensions (S/MIME) and Pretty Good Privacy (PGP).\nVPN and tunneling\nA virtual private network (VPN) works by enabling users to exchange data across public networks as if the devices were directly linked to the virtual network. A VPN gives a user a different IP making them appear as if they are connected from another location.\nIPsec can be used seamlessly with VPNs. this incorporation forms IPsec VPN. This refers to the process of creating and managing VPN connections using an IPsec protocol suite. You can also refer to it as VPN over IPsec.\nAnother important aspect of a VPN is the tunnel used in transmitting data. A VPN tunnel is an encrypted connection linking a device to a server. It works by covering (encapsulating) data in an encrypted data packet. IPsec naturally uses the tunnel mode for establishing VPN tunnels. IPsec provides an enhanced level of security on VPN connections by providing encryption, authentication and compression services at the network level of VPN.\nMost times VPN service providers use IPsec in conjunction with other protocols to increase security.\nIPsec provides freedom in allowing different devices to decide how they want to implement security.\nThe IPsec method of administering security is more basic compared to other methods as it tackles security from the foundation, the IP itself.\nThis makes it still relevant even with more complex security protocols being developed regularly.\nOverall, IP level security is quite solid and worth considering.', 'Secure Sockets Layer (SSL) is the most widely used technology for providing a secure communication between the web client and the web server. Most of us are familiar with many sites such as Gmail, Yahoo etc. using https protocol in their login pages. When we see this, we may wonder what’s the difference between http and https.\nIn simple words, a HTTP protocol is used for standard communication between the Web server and the client. HTTPS is used for a “Secure communication”.\nHow Secure Sockets Layer Works?\nBefore we understand the concept of SSL, let us first learn what a “Secure Communication” means. Suppose there exists two communicating parties: Say A (client) and B (server).\nWorking of HTTP:\nWhen A sends a message to B, the message is sent as a plain text in an unencrypted manner. This is acceptable in normal situations where the messages exchanged are not confidential. But, imagine a situation where A sends a PASSWORD to B. In this case, the password is also sent as a plain text. This has a serious security problem because, if an intruder (hacker) can gain unauthorized access to the ongoing communication between A and B , he can easily obtain the PASSWORDS, since they remain unencrypted. This scenario is illustrated using the following diagram:\nNow let us see the working of HTTPS:\nWhen A sends a PASSWORD (say “mypass“) to B, the message is sent in an encrypted format. The encrypted message is decrypted on B‘s side. So, even if the Hacker manages to gain an unauthorised access to the ongoing communication between A and B he gets only the encrypted password (“xz54p6kd“) and not the original password. This is shown below:\nHow is HTTPS implemented?\nHTTPS is implemented using Secure Sockets Layer (SSL). A website can implement HTTPS by purchasing an SSL Certificate. Secure Sockets Layer (SSL) technology protects a Web site and makes it easy for the site visitors to trust it. It has the following uses:\nAn SSL Certificate enables encryption of sensitive information during online transactions.\nEach SSL Certificate contains unique and authenticated information about the certificate owner.\nA Certificate Authority verifies the identity of the certificate owner when it is issued.\nHow Encryption Works?\nThe whole concept of Secure Sockets Layer is implemented on the basis of RSA algorithm where each SSL Certificate consists of a Public key and a Private key. The public key is used to encrypt the information and the private key is used to decrypt it. When your browser connects to a secure domain, the server sends a Public key to the browser to perform the encryption. The public key is made available to everyone but the private key(used for decryption) is kept secret. So, during a secure communication, the browser encrypts the message using the public key and sends it to the server. This message is decrypted on the server side using the Private key(Secret key).\nHow to Identify a Secure Connection?\nIn the Internet Explorer and most other browser programs like Firefox or Google Chrome, you will see a lock icon in the Security Status bar. The Security Status bar is located on the right side of the Address bar. You can click the lock to view the identity of the website.\nIn high-security browsers, the authenticated organization name is prominently displayed and the address bar turns GREEN when an Extended Validation SSL Certificate is detected. If the information does not match or the certificate has expired, the browser displays an error message or warning and the status bar may turn RED.\nSo, the bottom line is, whenever you perform an online transaction such as Credit card payment, Bank login or Email login always ensure that you have a secure communication. A secure communication is a must in these situations. Otherwise there are chances of a Phishing attack using a fake login page.\nI Hope you like the information presented in this article. Please pass your comments.']	['<urn:uuid:1d768cfa-e892-48ab-bab7-90f462a9ecca>', '<urn:uuid:03982534-e35d-4df3-8ff2-0c2bf9ca1263>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T10:32:05.666290	9	36	2268
18	What kind of new business arrangement has emerged in recent years where contractors take full responsibility for engineering projects from start to finish?	In recent years, the turnkey or package contract has emerged, where the contractor undertakes to finance, design, specify, construct, and commission a project in its entirety. In this arrangement, the consulting engineer is engaged by the contractor rather than by the client.	"[""Civil Engineering Functions\nThe functions of the civil engineer can be divided into three categories: those performed before construction (feasibility studies, site investigations, and design), those performed during construction (dealing with clients, consulting engineers, and contractors), and those performed after construction (maintenance and research).\nNo major project today is started without an extensive study of the objective and without preliminary studies of possible plans leading to a recommended scheme, perhaps with alternatives. Feasibility studies may cover alternative methods—e.g., bridge versus tunnel, in the case of a water crossing—or, once the method is decided, the choice of route. Both economic and engineering problems must be considered.\nA preliminary site investigation is part of the feasibility study, but once a plan has been adopted a more extensive investigation is usually imperative. Money spent in a rigorous study of ground and substructure may save large sums later in remedial works or in changes made necessary in constructional methods.\nSince the load-bearing qualities and stability of the ground are such important factors in any large-scale construction, it is surprising that a serious study of soil mechanics did not develop until the mid-1930s. Karl von Terzaghi, the chief founder of the science, gives the date of its birth as 1936, when the First International Conference on Soil Mechanics and Foundation Engineering was held at Harvard University and an international society was formed. Today there are specialist societies and journals in many countries, and most universities that have a civil engineering faculty have courses in soil mechanics.\nThe design of engineering works may require the application of design theory from many fields—e.g., hydraulics, thermodynamics, or nuclear physics. Research in structural analysis and the technology of materials has opened the way for more rational designs, new design concepts, and greater economy of materials. The theory of structures and the study of materials have advanced together as more and more refined stress analysis of structures and systematic testing has been done. Modern designers not only have advanced theories and readily available design data, but structural designs can now be rigorously analyzed by computers.\nThe promotion of civil engineering works may be initiated by a private client, but most work is undertaken for large corporations, government authorities, and public boards and authorities. Many of these have their own engineering staffs, but for large specialized projects it is usual to employ consulting engineers.\nThe consulting engineer may be required first to undertake feasibility studies, then to recommend a scheme and quote an approximate cost. The engineer is responsible for the design of the works, supplying specifications, drawings, and legal documents in sufficient detail to seek competitive tender prices. The engineer must compare quotations and recommend acceptance of one of them. Although he is not a party to the contract, the engineer's duties are defined in it; the staff must supervise the construction and the engineer must certify completion of the work. Actions must be consistent with duty to the client; the professional organizations exercise disciplinary control over professional conduct. The consulting engineer's senior representative on the site is the resident engineer.\nA phenomenon of recent years has been the turnkey or package contract, in which the contractor undertakes to finance, design, specify, construct, and commission a project in its entirety. In this case, the consulting engineer is engaged by the contractor rather than by the client.\nThe contractor is usually an incorporated company, which secures the contract on the basis of the consulting engineer's specification and general drawings. The consulting engineer must agree to any variations introduced and must approve the detailed drawings.\nThe contractor maintains the works to the satisfaction of the consulting engineer. Responsibility for maintenance extends to ancillary and temporary works where these form part of the overall construction. After construction a period of maintenance is undertaken by the contractor, and the payment of the final installment of the contract price is held back until released by the consulting engineer. Central and local government engineering and public works departments are concerned primarily with maintenance, for which they employ direct labour.\nResearch in the civil engineering field is undertaken by government agencies, industrial foundations, the universities, and other institutions. Most countries have government-controlled agencies, such as the United States Bureau of Standards and the National Physical Laboratory of Great Britain, involved in a broad spectrum of research, and establishments in building research, roads and highways, hydraulic research, water pollution, and other areas. Many are government-aided but depend partly on income from research work promoted by industry.\nBranches of civil engineering\nIn 1828 Thomas Tredgold of England wrote:\nThe most important object of Civil Engineering is to improve the means of production and of traffic in states, both for external and internal trade. It is applied in the construction and management of roads, bridges, railroads, aqueducts, canals, river navigation, docks and storehouses, for the convenience of internal intercourse and exchange; and in the construction of ports, harbours, moles, breakwaters and lighthouses; and in the navigation by artificial power for the purposes of commerce.\nIt is applied to the protection of property where natural powers are the sources of injury, as by embankments for the defence of tracts of country from the encroachments of the sea, or the overflowing of rivers; it also directs the means of applying streams and rivers to use, either as powers to work machines, or as supplies for the use of cities and towns, or for irrigation; as well as the means of removing noxious accumulations, as by the drainage of towns and districts to . . . secure the public health.\nA modern description would include the production and distribution of energy, the development of aircraft and airports, the construction of chemical process plants and nuclear power stations, and water desalination. These aspects of civil engineering may be considered under the following headings: construction, transportation, maritime and hydraulic engineering, power, and public health.\nAlmost all civil engineering contracts include some element of construction work. The development of steel and concrete as building materials had the effect of placing design more in the hands of the civil engineer than the architect. The engineer's analysis of a building problem, based on function and economics, determines the building's structural design.\nRoman roads and bridges were products of military engineering, but the pavements of McAdam and the bridges of Perronet were the work of the civil engineer. So were the canals of the 18th century and the railways of the 19th, which, by providing bulk transport with speed and economy, lent a powerful impetus to the Industrial Revolution. The civil engineer today is concerned with an even larger transportation field—e.g., traffic studies, design of systems for road, rail, and air, and construction including pavements, embankments, bridges, and tunnels.\nMaritime and hydraulic engineering\nHarbour construction and shipbuilding are ancient arts. For many developing countries today the establishment of a large, efficient harbour is an early imperative, to serve as the inlet for industrial plant and needed raw materials and the outlet for finished goods. In developed countries the expansion of world trade, the use of larger ships, and the increase in total tonnage call for more rapid and efficient handling. Deeper berths and alongside-handling equipment (for example, for ore) and navigation improvements are the responsibility of the civil engineer.\nThe development of water supplies was a feature of the earliest civilizations, and the demand for water continues to rise today. In developed countries the demand is for industrial and domestic consumption, but in many parts of the world—e.g., the Indus basin—vast schemes are under construction, mainly for irrigation to help satisfy the food demand, and are often combined with hydroelectric power generation to promote industrial development.\nDams today are among the largest construction works, and design development is promoted by bodies like the International Commission on Large Dams. The design of large impounding dams in places with population centres close by requires the utmost in safety engineering, with emphasis on soil mechanics and stress analysis. Most governments exercise statutory control of engineers qualified to design and inspect dams.\nCivil engineers have always played an important part in mining for coal and metals; the driving of tunnels is a task common to many branches of civil engineering. In the 20th century the design and construction of power stations has advanced with the rapid rise in demand for electric power, and nuclear power stations have added a whole new field of design and construction, involving prestressed concrete pressure vessels for the reactor.\nThe exploitation of oil fields and the discoveries of natural gas in significant quantities have initiated a radical change in gas production. Shipment in liquid form from the Sahara and piping from the bed of the North Sea have been among the novel developments.\nDrainage and liquid-waste disposal are closely associated with antipollution measures and the re-use of water. The urban development of parts of water catchment areas can alter the nature of runoff, and the training and regulation of rivers produce changes in the pattern of events, resulting in floods and the need for flood prevention and control.\nModern civilization has created problems of solid-waste disposal, from the manufacture of durable goods, such as automobiles and refrigerators, produced in large numbers with a limited life, to the small package, previously disposable, now often indestructible. The civil engineer plays an important role in the preservation of the environment, principally through design of works to enhance rather than to damage or pollute.""]"	['<urn:uuid:a2edeae0-188c-4405-9ab4-b0cb2a7687b3>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	23	42	1572
19	what challenges do mining companies face when choosing locations to mine	Mining companies cannot choose locations that are logistically, socially, environmentally or politically optimal, as geology defines where mineral deposits occur. This leads to several challenges, including: managing relationships with local communities, dealing with landscape/environmental impacts, handling relationships with local and national governments, and addressing weak governance with increased risk of corruption and bribery.	['Modern societies are dependent on mineral-based products. Energy technology, Information and communications technology, consumer electronics, infrastructure, logistics and food production all increasingly rely on an ever-widening array of minerals and metals. For example, production of a personal computer or a smartphone needs over 40 elements. Rapid replacement of internal combustion engines by electricity based technology in the car industry and widening application of wind and solar energy may cause a massive demand for mining of metals such as, lithium, cobalt and rare earth elements. The use of many of these hi-tech metals will vastly increase quantities in the future, and mining of primary resources is the only way to produce them. Minerals also provide the materials to build homes, schools, hospitals and infrastructure. Minerals and metals are essential for generating and supplying “renewable” «green» energy and low-carbon production technology. Even wind generation requires huge amounts of traditional minerals and metals including aggregates for their concrete anchorage, copper for the motor windings and transmission cables and aluminum for their construction material. Minerals and metals are also fundamental to make societies more resilient to climate change because of their use in the technologies mentioned earlier.\nMineral and metal consumption strongly correlates with economic growth and urbanization. Three billion additional people will likely move to cities by 2050. Improved recycling, resource efficiency, better product design and new materials will reduce mineral and metal consumption per capita, but mining of primary resources will continue to play an important role in the future in building sustainable societies.\nGeology defines the occurrence of mineral deposits so mining is geographically constrained, but the use of the products of mining in down-stream industries or as final products often takes place in continents and countries different from the location of the mine. Therefore, mining communities do not necessarily appreciate the importance of mineral production for the welfare of people living in other countries, particularly if there is no tangible sharing of those benefits.\nMining cannot choose locations that are logistically, socially, environmentally or politically optimal, appropriate or ‘friendly’. This means that companies may have to deal with circumstances that could pose ethical challenges including: the relationship with local communities, position in the landscape/environment, relationship with local and national governments, weak governance and associated increased risk of corruption and bribery. It is necessary to deal with these challenges in a responsible way. This also means that geoscientists and engineers will need to build their capacity and skills on how to deal with local communities and related social issues.\nThere is no doubt that mining can bring positive benefits to the host countries but these can come at a cost to the environment and local communities if relationships, resources and operations are not managed properly. The fundamental aim must be equitable distribution of the benefits of development and minimization of the negative impacts on people and the environment. Responsibly navigating this field requires a strong ethical compass.\nArvanitidis N., Boon J., Nurmi P. and Di Capua G. (2017). The White Paper on Responsibile Mining. IAPG – International Association for Promoting Geoethics.\nThe paper will be also presented in the sessions on geoethics at EGU 2018 and RFG 2018.ACCESS WHITE PAPER ON RESPONSIBLE MINING\nComments are closed.']	['<urn:uuid:f7163aa3-ba3b-40ed-88ca-c34b78f09e55>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T10:32:05.666290	11	53	535
20	Could you explain in simple terms how animals can adapt to new environments before any genetic changes happen?	This process is called genetic accommodation, which involves three steps. First, organisms change their physical form by altering how their genes are expressed. Second, a gene emerges that helps lock in that physical change. Third, that gene spreads through the population. For example, if predators are forced to move from dense forest to open areas, they might start running to catch prey instead of ambushing it. Their bodies would adapt through gene expression, developing leaner torsos and stronger legs. Only later might a mutation occur that makes these changes easier to maintain, and this mutation would then spread through the population because it's now advantageous.	"['The truth makes for a bad meme. That seems to be the lesson we can take from the continued popularity of Richard Dawkins\' idea that the ""selfish gene"" controls evolution.\nImage by Sergey Nivens via Shutterstock\nCurrent evidence suggests evolution is guided by environment as much as genes, but most people still think genes are in the driver\'s seat.\nIn a terrific and controversial essay in Aeon magazine yesterday, science journalist David Dobbs has done a careful job unpacking how Dawkins\' brilliant book, The Selfish Gene, has outlived its usefulness. Published in 1976, The Selfish Gene was the most elegant summary of cutting-edge genetic theory at the time. More importantly, it was written for a popular audience, which ate the book up and made it a bestseller. Perhaps the most catchy part of the book was its title, a slightly hyperbolic twist on the book\'s main thesis: that evolution is driven by competition between genes, as well as between organisms and species.\nLike I said, at the time it was published, this was a notion that few people outside the scientific community understood. And Dawkins helped millions of people understand this crucial idea, which remains true. It\'s just not the full story, as Dobbs reveals in interview after interview with scientists who currently study genes and evolution.\n""We have a more complicated understanding of football than we do genetics and evolution. Nobody thinks just the quarterback wins the game,"" says biologist Gregory Wray, comparing the gene to the quarterback. ""We\'re stuck in an outmoded way of thinking that should have fallen long ago."" Evolutionary biologist Mary Jane West-Eberhard puts it more simply: ""The gene does not lead. It follows."" This is the same argument that MIT\'s Evelyn Fox Keller made 14 years ago in her book The Century of the Gene.\nTo be clear, biologists like West-Eberhard and others are not saying genes and their selfishness aren\'t important. But they are merely one part of a much larger and more complicated mechanism, which involves inputs from the environment which affect how genes express themselves.\nSo what replaces the selfish gene in this new model of evolutionary change? From his interviews with scientists, Dobbs believes a good contender would be the idea of ""genetic accommodation,"" which explains how the gene fits into this larger machinery (or, to continue Wray\'s metaphor, into the football game). It\'s worth quoting at length from Dobbs here:\nGenetic accommodation involves a three-step process.\nFirst, an organism (or a bunch of organisms, a population) changes its functional form — its phenotype — by making broad changes in gene expression. Second, a gene emerges that happens to help lock in that change in phenotype. Third, the gene spreads through the population.\nFor example, suppose you\'re a predator. You live with others of your ilk in dense forest. Your kind hunts by stealth: you hide among trees, then jump out and snag your meat. You needn\'t be fast, just quick and sneaky.\nThen a big event — maybe a forest fire, or a plague that kills all your normal prey — forces you into a new environment. This new place is more open, which nixes your jump-and-grab tactic, but it contains plump, juicy animals, the slowest of which you can outrun if you sprint hard. You start running down these critters. As you do, certain genes ramp up expression to build more muscle and fire the muscles more quickly. You get faster. You\'re becoming a different animal. You mate with another fast hunter, and your kids, hunting with you from early on, soon run faster than you ever did. Via gene expression, they develop leaner torsos and more muscular, powerful legs. By the time your grandchildren show up, they seem almost like different animals: stronger legs, leaner torsos, and they run way faster than you ever did. And all this has happened without taking on any new genes.\nThen a mutation occurs in one grandkid. This mutation happens to create stronger, faster muscle fibres. This grandchild of yours can naturally and easily run faster than her fastest siblings and cousins. She flies. Her children inherit the gene, and because their speed wows their mating prospects, they mate early and often, and bear lots of kids. Through the generations, this sprinter\'s gene thus spreads through the population.\nNow the thing is complete. Your descendants have a new gene that helps secure the adaptive trait you originally developed through gene expression alone. But the new gene didn\'t create the new trait. It just made it easier to keep a trait that a change in the environment made valuable. The gene didn\'t drive the train; it merely hopped aboard. Had the gene showed up earlier (either through mutation or mating with an outsider), back when you lived in the forest and speed didn\'t mean anything, it would have given no advantage. Instead of being selected for and spreading, the gene would have disappeared or remained in just a few animals. But because the gene was now of value, the population took it in, accommodated it, and spread it wide.\nThe problem is that this account is complicated — there is no easy causal relationship between gene and phenotype. Dawkins\' model has retained its grip on the public imagination because it\'s simple enough to go viral as an idea. Which was great back in the 1970s, when our understanding of genes was itself relatively simple. But today, in the wake of rapid genetic sequencing and advances in population genetics, most scientists include genes as one part of a much bigger picture.\nExcept, apparently, Dawkins himself. When Dobbs asked Dawkins what he thought of the accommodation model, Dawkins claimed that it didn\'t unseat the gene as the main driver of evolutionary change because the gene cemented the phenotypic changes brought about by environment. And when Dobbs published his article yesterday, Dawkins claimed on Twitter that he ""could have written"" Dobbs\' article because it contained nothing that he hadn\'t already covered in his 1976 book. Even though his book was written before any genomes had been sequenced, and before the explosion of discoveries in epigenetics, which is the study of how environment shapes gene expression. Later, Dawkins simply advised his followers:\nIf someone tells you The Selfish Gene is wrong, superseded, deserves to die etc, ask if they\'ve read the book itself or only the title\n— Richard Dawkins (@RichardDawkins) December 4, 2013\nNeedless to say, this is an excellent way to refuse to engage. Instead of responding to the questions that people are raising, simply claim that everything resides in the good book — if you\'ll only read it correctly.\nThe irony is that nobody is actually claiming that Dawkins is wrong. They\'re just saying that ""the selfish gene"" doesn\'t give us a complete picture of how evolution works. In the 38 years since its publication, evolutionary biologists have worked out that the whole thing is a lot more complicated than ""selfish genes."" As geneticist Michael Eisen puts it to Dobbs:\nEvolution is not even that simple. Anyone who\'s worked on systems sees that natural selection takes advantage of the most bizarre aspects of biology. When something has so many parts, evolution will act on all of them. It\'s not that genes don\'t sometimes drive evolutionary change. It\'s that this mutational model — a gene changes, therefore the organism changes — is just one way to get the job done. Other ways may actually do more.\nAnd this doesn\'t even get into the ways that organism evolution may be shaped by microbiomes, or the vast ecosystems of microbes that live in and on our bodies. The problem is that we don\'t yet have a meme that\'s as catchy as ""the selfish gene"" to explain the ""other ways"" model that Eisen and his contemporaries are working with. And this is ultimately what Dobbs\' essay is about.\nWe need a better way of explaining the genetics of evolution to people who don\'t work in labs and read scientific journals. The Selfish Gene was a pop science masterpiece of a bygone age. Now we need to build a new public understanding of evolution that puts the selfish gene in its place, as a player alongside many others in the ongoing drama of mutation and evolutionary change.\nWhat snappy phrase will sum up genetic accommodation and epigenetics? Maybe the answer is that we don\'t need one, but many. That would be in keeping with what we\'ve learned about evolution itself. There is no one unit of information that leads our transformation as a species. Instead there are a lot of factors affecting change, sometimes working together — and sometimes at odds.\nAnnalee Newitz is the editor-in-chief of io9, and the author of Scatter, Adapt and Remember: How Humans Will Survive a Mass Extinction.']"	['<urn:uuid:90e4b409-ef92-430c-8cbe-80768316828b>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	18	105	1465
21	genetics expert need compare dna sequence similarity assessment methods caloric restriction effects gene expression	DNA sequence comparison between species requires complex methodological considerations including handling insertions, deletions, duplications, and translocations, with the often-cited 99% human-chimp similarity being questionable due to these factors. In contrast, gene expression analysis is more straightforward - a study of caloric restriction effects on aging showed that of 6347 genes analyzed, only 58 (0.9%) showed significant expression changes, indicating aging isn't caused by widespread gene expression alterations but rather specific changes that caloric restriction can suppress.	"['I am often struck by how the topic of evolution in general, and chimp/human ancestry in particular, can be an immediate conversation opener that just as quickly becomes a conversation closer. Mind you, I don’t go around buttonholing people at, say, my favorite lounge (this music will conjure up the atmosphere) about some phylogenetic arcana — at least, I try not to do so. But for some strange reason, there exist individuals of good will who apparently feel called upon to “raise my consciousness” about some Darwinian facts that I’ve presumably gotten wrong. Not just a bit wrong, but astoundingly wrong. You see, to their way of thinking, I am in dire need of reeducation and they are there to charitably point the way to “help.”\nHere is an example of how “chats” like the one I’m talking about begin. After I have been formally introduced (though sometimes not) to an emissary of enlightenment, my just-made acquaintance proceeds to ask whether I’ve read a certain book (title withheld) that purportedly shows four things: We are 99% chimp; our chromosomes contain “scars” that are shared with those of our simian cousins; the DNA scars, like 98.5% of our genome, are simply junk; and these facts change everything we “know” about God. In response I invariably say, “How interesting,” with a wan smile followed by, “Oh, sure, I’ve read parts of it.” For me this is a taxing turn in the conversation for I must all at once feign attention, ask the bartender for another drink, and work to suppress my desire to bolt out the door. Sensing my unease, my new friend usually seems to read my restlessness as one of intellectual discomfort — possibly fear. Anyhow, seeing me as the quarry, he leans in and expounds on each of the topics, his eyes glinting throughout with the impression that he is surrounding me via a four-pronged conceptual assault, a two-pincer strategy. (All the while, I am praising the heavenly host for the warm irreducible complexity of scotch.)\nThen a lull in the barrage occurs. To his way of thinking, it is my guess, an opportunity is being provided for me to offer an unconditional surrender; or, at the very least, for me to acknowledge that pieces like the one just published in the Scientific American (Katherine S. Pollard, “What Makes Us Human? Comparisons of the genomes of humans and chimpanzees are revealing those rare stretches of DNA that are ours alone,” April 20, 2009) are right when they assert that “our DNA blueprints are nearly 99 percent identical” to the sequences of chimps. Awaiting the white flag, my conversation partner will now sometimes try to emphasize that I have been at the receiving end of a coup de grâce,\nBy the way, this is the apogee or climax of the conversation. It is strictly downward from here on. But at such a critical juncture I proffer no surrender and, indeed, I mount a counter-offensive. Yes, yes, I know: The audacity…the rudeness. Whether my attempts to make my case are ever successful is unknown for my responses sooner or later elicit an abrupt termination of discourse. Regardless, my turn at the conversation goes something like this…\nOne can seriously call into question the statement that human and chimp genomes are 99% identical. For one thing, it has been noted in the literature that the exact degree of identity between the two genomes is as yet unknown (Cohen, J., 2007. “Relative differences: The myth of 1%,” Science 316: 1836.). Part of the reason for this is if one decides to take into account the plethora of species-specific DNA insertions and deletions (“indels”) that are present along any segment compared between chimp and human, the percentage of identity drops. Another reason is that duplications, inversions, translocations, and transpositions at all scales uniquely characterize the two genome sequences — these have to be untangled before aligning the sequences in order to measure their similarity. Also, the 99% identity figure is often derived from protein-coding regions that only comprise about 1.5% of the two genomes. Many mammalian protein-coding regions are highly conserved, however. We also have to consider that a detailed comparison of certain “heterochromatic” chromosome regions between chimps and humans has yet to be made. In short, the figure of identity that one wants to use is dependent on various methodological factors.\nAs I continue in this vein, I notice that I am being given the universal gesture of “Wow, look at the time…it’s really getting late…I’d love to pursue this matter further but I have better things to do…” by my interlocutor: He keeps staring at his watch and asking the bartender for the time. Since I’m now getting warmed up, I lean in and suggest to him that he should try his own chimp-human alignments and not take so-and-so’s word for it — after all, the sequences are publicly available. Why trust authority? (I can tell from his sandals and ponytail that this late 1960s reference will appeal to him.) But he has to make a parting shot and so, after commenting that only creationists are as recalcitrant to logic as I seem to be, he presents to me the ultimate criterion of truth, the standard by which I have failed. That criterion, the one I missed in school, comes through in a single sentence he utters: “Everything you just said, well, I have never heard this before.” Taken aback and after I request that he repeat what I just heard, my now peeved acquaintance tells me (holding up his book) that since he has never read in his trusted sources that DNA sequence comparisons often require complicated alignments, that the data are filtered through software algorithms that in turn rest on a priori assumptions, etc., he must dismiss my first salvo.\nHe tallies the intellectual score as 4-0 in his favor.\nAt this break, three things happen. The bartender receives my nod that I want another drink and then, after he places it before me, I inquire as to whether he can play anything by Ethel Ennis — I now want to listen to something languorous, music that will soothe the feeling of ennui that has come over me. Next, or simultaneously, my sparring partner makes one of two moves. Either he places his book into his hand-woven Inca-nesque bag and leaves without so much as a farewell, or he decides to tarry a bit longer and says, “You have no answer for ITSs, do you?”\nITSs…interstitial telomeric sequences…the chromosome scars, the pieces of junk DNA he was lecturing me about earlier. As you know, telomeres are the ends of chromosomes. In many species, including chimps and humans, the DNA sequences that are found at these genomic tips are tandem repetitions of TTAGGG. That’s right…TTAGGGTTAGGGTTAGGG…over and over and over again. A notable exception to this rule is the fruit fly, an organism that in this regard has provided the junk DNA notion no succor, since its telomeres have complex combinations of three different retrotransposons instead of those six-basepair units. What is important to note, though, is that telomeric sequences are essential to the cell, and it seems that hardly a week does not pass without some new role being discovered for these elements.\nHow, precisely, are miles and miles of TTAGGG of significance? From the standpoint of chromosome architecture, the repetitive elements en masse have the propensity to form complicated topologies such as quadruplex DNA. These sequences or, rather, topographies are also bound by a host of chromatin proteins and particular RNAs to generate a unique “suborganelle” — for the lack of better term — at each end. As a matter of fact, the chromatin organization of telomeres can silence genes and has been linked to epigenetic modes of inheritance in yeast and fruit flies. Furthermore, different classes of transcripts emanate from telomeres and their flanking repetitive DNA regions, which are involved in various and sundry cellular and developmental operations.\nI try to outline all the functions of telomeric repeats, but my friend tells me that I am getting off the subject.\nHe wants to me to focus on the ITSs, the tracks of the hexamer TTAGGG that reside within chromosome arms or around the centromere, not at the ends. I tell him that I was just coming to that topic. The story, you see, is that in the lineage leading up (or down, I forget which) to chimps and humans, a fusion of chromosome ends occurred — two telomeres became stuck together, the DNA was stitched together, and now we find the remnants of this event on the inside of chromosomes. And to be fair, I concede at this point that the 2q13 ITS site shared by chimps and humans can be considered a synapomorphy, a five-dollar cladistic term meaning a genetic marker that the two species share. As this is said, it is apparent that the countenance of my acquaintance lightens a bit only to darken a second later. For I follow up by saying that of all the known ITSs, and there are many in the genomes of chimps and humans, as well as mice and rats and cows…, the 2q13 ITS is the only one that can be associated with an evolutionary breakpoint or fusion. The other ITSs, I hasten to add, do not square up with chromosomal breakpoints in primates (Farré M, Ponsà M, Bosch M. 2009. “Interstitial telomeric sequences (ITSs) are not located at the exact evolutionary breakpoints in primates,” Cytogenetic and Genome Research 124(2): 128-131.). In brief, to hone in on the 2q13 ITS as being typical of what we see in the human and chimp genomes seems almost like cherry-picking data. Most are not DNA scars in the way they have been portrayed.\nExasperated with my stubbornness, the caffeine from innumerable herbal teas having only enhanced his tension, he rises from the bar and asks: “How, then, do you account for such ITSs in the first place…everyone knows they are out-of-place junk.” I tell him that I do have an answer but that first I must be excused for a moment. While making my way back to the bar, I mentally rehearse so as to be as succinct as possible. My rejoinders are, simply, that ITSs reflect sites where TTAGGG repeats have been added to chromosomes by telomerases, that these repeats are moreover engineered — literally synthesized by the telomerase machinery, that ITSs have a telomere-like chromatin organization and are associated with distinct sets of proteins, and that many have been linked to roles such a recombination hotspots. And just as I begin to reflect on where the references are in my bag that supports those points I notice…he is gone.', 'Research on the biology of aging has led to a revolution in understanding the cellular and molecular changes that occur with aging. This new gerontology investigates the progressive biological and physiological changes that normally occur with advancing age and the abnormal changes that are risk factors for or accompany age-related disease states. Progress is being made in understanding the gradual changes in structure and function that occur in the brain and nerves, bone and muscle, heart and blood vessels, hormones, nutritional processes, immune responses, and other aspects of the body. Research has begun to reveal the biologic factors associated with extended longevity in humans and animal models. The ultimate goal of this effort is to develop interventions to reduce or delay age-related degenerative processes in humans.\nScience Advances—Biology of Aging\nMitochondrial DNA Mutations Increase With Aging. One hypothesis of the cause of aging is the accumulation of mutations in mitochondrial DNA (mtDNA). Although earlier research has shown that a particular deletion mutation of mitochondrial DNA increases with age, it appeared that this mutation only occurred in less than 4 percent of mtDNA molecules. However the methods used to quantitate the level of this mutation would not have detected other deletions, so it was argued by some that the common deletion mutation represented the ""tip of the iceberg"" of mitochondrial mutations. Skeptics responded that this unproven hypothesis represented wishful thinking. By use of a sensitive method to look at point mutations in mitochondrial DNA, researchers found hard evidence that mtDNA point mutations increase with aging and mitochondria deteriorate as people age. These scientists show that one particular point mutation in the control region of the mtDNA occurs in a high proportion of the mtDNA molecules of more than 50% of people over the age of 65, but is absent in younger individuals. Because the mitochondria are the cellular sites for energy metabolism, deterioration of mitochondria could deprive cells of the energy they need to function and ultimately could lead to premature cell death.\nCaloric Restriction Prevents Age-Associated Changes in Gene Expression. Most multicellular organisms exhibit a progressive and irreversible physiologic decline during the aging process. The only intervention known to slow the intrinsic rate of aging in mammals is caloric restriction. Given 30 to 40 percent fewer calories than in usual feeding schedules, but fed all the necessary nutrients, rodents and other nonprimate laboratory animals studied not only have lived far beyond their normal lifespans but have reduced rates of several diseases, especially cancers. In a new study, the gene expression profile of the aging process was analyzed in skeletal muscle of mice. Of the 6347 genes surveyed by new microarray techniques, only 58 (0.9%) displayed a greater than twofold decrease in expression. Thus, the aging process is unlikely to be due to large, widespread alterations in gene expression. The major effect of caloric restriction seems to be to heighten animals’ stress response in response to damage to proteins and other large molecules. Caloric restriction also completely or partially suppressed age-associated alterations in expression of a large proportion of genes. This is the first global assessment of the aging process in mammals at the molecular level. Potentially, gene expression profiles can be used to assess the biological age of mammalian tissues, providing a tool to evaluate experimental interventions.\nLink Established Between Telomeres and Mammalian Aging. Telomeres are highly repetitive DNA sequences located at the end of chromosomes. They are essential for the stability of chromosomes and cell survival in a wide variety of organisms. In human cells grown in culture, telomere length shortens with each cell division and the progressive telomere shortening ultimately limits the ability of cells to divide. To test the possibility of a link between telomere shortening and aging of an organism, investigators have created genetically altered mice lacking telomerase, an enzyme that adds new telomeric DNA sequences to existing telomeres. In this transgenic model, telomeres progressively shortened throughout the lifespan, providing a unique opportunity to understand the cellular consequences and aging significance of telomere shortening in the living animal. Although loss of telomeres did not elicit a full spectrum of the classical symptoms of aging, age-dependent telomere shortening was associated with a shortened lifespan, reduced capacity to respond to physiological stress, slow wound healing, and an increased incidence of spontaneous cancers. As individuals age, aged organs show a markedly diminished capacity to cope with acute and chronic stress. The telomerase-deficient mouse provides a valuable model to study the role of telomere maintenance in cellular stress responses in the aging organism.']"	['<urn:uuid:9eb49f88-df57-4209-8f43-a23d8bfbfc3a>', '<urn:uuid:8973ac7a-085c-4002-96b5-4d75bebda512>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T10:32:05.666290	14	76	2541
22	What mounting method provides the best frequency response for vibration sensors?	A stud mounting provides the best frequency response for vibration sensors, though this requires properly drilling the machine and is typically only used for permanent monitoring applications.	['Vibration Analysis Basics – Sensor Mounting\nSelect the right mounting methodology\nAs seen in our previous article, the settings of vibration measurement are crucial if you want to achieve the right detection and diagnostic performances. But, this is not the only thing to consider in order to achieve the desired objectives. For instance, the sensor mounting method to collect vibration data can greatly impact the quality of the time wave form. As a rule of thumb, the closer you put your sensor to the machine, the better it senses the vibrations. This means that the type of mounting accessory used affects the measurement quality, depending on its type and size, and how well vibrations can be propagated through it.\nAs we know, high frequencies are very low in amplitude (and energy) and are the first type of content to be affected by the mounting method, as there may be loss of energy during transmission to the vibration sensor.\nThere are several mounting methods used today, each with pros and cons, so you need to make sure that the mounting technique you are using is in line with your strategy. For example, a probe tip may make it possible to reach a small or narrow area ̶ to obtain a measurement that nothing else can ̶ but is not be suitable to monitor high frequency problems, such as bearings, cavitation and gear problems. On the other hand, the best frequency response is achieved through a stud, meaning the machine must be properly drilled, which is often only the case for permanent monitoring applications.\nPlease see below the illustration of the different options and their frequency response.\nAt the end of the day, it is often a compromise between how early you want to detect problems ̶ depending on the machine’s use and failure modes ̶ and the solution’s “user-friendliness”.\nFor example, in order to capture high frequencies above 15 KHz, you may need to use an adhesive mounting pad or directly stud-mount the sensor. Practically, you will notice that all high-power rating gearboxes use antifriction bearings and rotate at slow speed. Also, monitoring this type of gearbox is a challenge. A Roller Press or a VRM gearbox in a cement mill, as well as a rolling mill gearbox in a steel plant, are examples of this.\nMeasurement repeatability is key\nBeside the quality of the measurement in terms of frequency content, the mounting accessory can also affect the trending and diagnostic reliability. One of the common issues when running a portable vibration program (where data are collected manually on a periodic basis), is ensuring that the sensors are placed in the same location from time to time, so that measurements can be compared by the vibration analyst. Although magnets are often preferred by users, because of their ease of use, they run some measurement repeatability risks. Hopefully, different techniques or tips can be implemented when using magnet-based accessories, such as marking the machine with a permanent marker or taking pictures of the sensor position on the machine; the OneProd FALCON vibration data collector allows you to both take photos and display them. As a matter of fact, despite the additional time and effort needed, installing cementing studs remains the safest choice to avoid any measurement errors, and guarantees the best possible results for the analyst in charge. Note that to keep the data collection time as fast as possible when using a magnet base, you can install cementing studs compatible with the use of magnets on the machines. Just bear in mind that the metrological performances achieved are impacted by the magnet, as explained in the first part of this article.\nIn conclusion, keep in mind that the condition monitoring leader has to establish a methodology for monitoring to ensure that:\n- The measurement location (place where the sensor is installed) is a good position, where you have good vibration transmission;\n- Data are being collected at the same location every time, despite potential staff turnover;\n- The frequency content captured matches with both the failure modes you are trying to detect and how early you want to get warned, according to your application.\nAs illustrated, some types of machines may not leave any room for compromise.']	['<urn:uuid:f97f5a76-db57-483a-924e-35a76081b9e3>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	11	27	706
23	What inspired Nicole Richie's TV series?	Nicole Richie's series is based on her irreverent twitter feed. The show follows the outspoken celebrity as she shares her perspective on style, parenting, relationships and her journey to adulthood.	"[""Authentic voices. Remarkable stories. AOL On Originals showcase the passions that make the world a more interesting place.\nHank Azaria’s touching, humorous, and often enlightening journey from a man who is not even sure he wants to have kids, to a father going through the joys, trials and tribulations of being a dad.\nGwyneth Paltrow and Tracy Anderson spend time with women who've overcome hardship, injury, and setbacks to triumph in the face of adversity.\nEnter the graceful but competitive world of ballet through the eyes of executive producer, Sarah Jessica Parker. This behind-the-scenes docudrama reveals what it takes to perform on the ultimate stage, the New York City Ballet. Catch NYCB on stage at Lincoln Center.\nACTING DISRUPTIVE takes viewers inside the businesses and passion projects of Hollywood’s top celebrities.\nExplore what it means to be human as we rush head first into the future through the eyes, creativity, and mind of Tiffany Shlain, acclaimed filmmaker and speaker, founder of The Webby Awards, mother, constant pusher of boundaries and one of Newsweek’s “women shaping the 21st Century.”\nThey say every picture tells a story and AOL On's new original series My Ink proves it. Travel along as some of the world's greatest athletes bring their tattoos to life through exclusive interviews and visits to their favorite tattoo parlors.\nDiscover crowdfunded small business success stories with author, comedian, and entrepreneur Baratunde Thurston.\nGo behind-the-scenes with racing's hottest, young talent, 17-year-old Dylan Kwasniewski, as he aspires to make it in the #1 motorsport in America – NASCAR\nFollow Scott Schuman, the Sartorialist, from the streets of NYC to the capitals of Europe on his quest to photograph and document the best in culture and fashion.\nIconic potter, designer, author and personality Jonathan Adler shares his unique perspective on creativity. Showcasing the inspiration Jonathan finds in the most unlikely people and places, Inspiration Point will add style, craft and joy to your life.\nServing Innovation gives a fresh look into the stories and passions that motivate some of the most innovative tastemakers in America.\nA documentary directed by Alex Winter exploring the Napster downloading revolution; the kids who created it, the bands and businesses that were affected and its impact on the world at large.\nNicole Richie brings her unfiltered sense of humor and unique perspective to life in a new series based on her irreverent twitter feed. The show follows the outspoken celebrity as she shares her perspective on style, parenting, relationships and her journey to adulthood.\nTravel with Bennett-Watt and learn about Wayne Henderson, guitar maker and musician, whose guitar workshop is located in ...\nTags:Wayne Henderson's Guitar Workshop in Rugby,Local Musicians Rugby Virginia,Rugby Tourist Attractions,Rugby Virginia,Rugby Virginia Music Shop,Rugby Virginia Music Store,Wayne Henderson,Wayne Henderson Workshop,What to do in Rugby,bennett watt\nGrab video code:\nWayne Henderson's Guitar Workshop in Rugby Virginia Wayne Henderson My dad was an all time musician. He liked to play the fiddle and I guess most of my grandparents were old time musicians who played banjos and this has always been interesting then. It’s also a sort of way of laughter around this area too. Almost stay like it is. It’s not too wily. It is a place and the cheapest and the best form of entertainment was to get together and play music. That sounds so nice and that still goes on.\nThen I think it will definitely work. It takes me probably I guess a couple of weeks or something to make my luck in. Maybe shorter about concentrated on one instrument and just kept busy at one thing and at work you know, pretty long days of work later than that and during the day time I have used a lot of kind of pent house and company getting most of the work done at the night time.\nI have to start out with a board like this one right here, like this mahogany board, it’s so thick. I’ll take this one big outside and we got jack all over but I cleaned it out before I’m going to use it. This really saw it and the pieces somewhat this thick and the thickness of this right here which is about 3 or 30 seconds, something like it then I take it over on other side of the shop and sand it, run it through the sander.\nThen when I first started making guitars, back in the 60s, I used to take a piece of glass and get it down in the floor and scrape it out. I would spend the days to get a piece of wood downtown but I’m not to do that now. I’ve got a nice sander. I can write it through in a few minutes.\nThen I fit it in this pot of water here. It says it still got some water and it would have been in scent in the other day. And you just turn the hot, lie down, you could keep like you used to picking yourself or you know laying through sand and keep it for about 15 minutes and then that piece of wood is still scrape but it’s wet and hot and soak through. And you just fit in this homemade contraption. This is real and I used to be under this over a piece of hot pot and you know a little torch running and something real hot. But some fellow out in California like invented this thing and it is really a neat trick. You can put different shapes and guitar. You can see that shape like this guitar I just build it there.\nWhen you get this done, you take it and fit it in another one. I’ll show you can, you can walk right over here. I've put it in this old form I've had for years and this might add a piece of sheet board or plywood and then you clump the sides in here with the spreader and clip the ends of the model. Then when you spread this thing it forces it out against this form and that gets it to right shape.\nNext thing you do is start making a top and back. Like here’s a top and a back, I have ready to go along this guitar right here and put the braces on. The back of the guitar has to have an arch build in it especially the back because to deal humidity changes of wood. Over the years I’ve learned the step. Most, I’ll have it the hard way because you know what and I thought back of the guitar was flat and then make it winner time and it starts sinking in and humidity lives in the wood, it shrinks. And then the next thing you know you got a crack or it pulls the joint apart.\nThis is bad stuff that happen and that’s the a little bit of arch built in and that gives a little bit of leeway to shrink so much and that kind of stuff was always the hardest to learn and you have to do it by some of your guitars blowing it and cracking or getting cracks and you have to fix them and then you discover what powers it then you can build in things to prevent it. And then if you look at the old ones, they already knew that 100 years ago and figure that kind of stuff out.\nIt’s just a box I’m doing repair work on it. This was made I think in 1968. It had been through an awful lot of abuse. You know, it has been played out. Somebody glued some kind of a big square bridge, ripped it of and brought pop and all. Now I think somebody left handed has played it and for years without a pick guard because you got the wire over here and off it sagged where it would normally but then have made left handed had a pick guard over.\nWith all these inlays or head and with the power and that kind of thing and all the channels where you run around it or rather this is all within the pocket knife. Always inlays around here and this I made out of the best piece of rosewood and had it blowing. I don’t know how it got there with this but thanks to some pretty wreath table had this guitar on time. So that’s your main channels that live over the other side of the gala somewhere and I don’t know what was in some kind of—I hope nobody was playing it when it got shot what they are made of. It make somebody said, they are mad to say that that kind of it could shoot with it somehow it wouldn’t hurt. The thing didn’t know any better.\nHere is the one that I made I think when I was 10 years old something like that. I think I had to put a new box on it until the thinking day on those boxes, 59 looks like maybe. You got to its length that sort of thing but I used to play a thing on it a little bit. I mean use that thing. But anyway, here’s that first one and like this little piece around here. I wouldn’t allow each one of those individually with the pocketknife. I wouldn’t have any idea how long it took me everyday.\nThere we have the guitar of everybody, this had the neck. This instrument steps my therapy when I’m dancing and everything. I can totally relax to sit down and by the time I get a guitar and that one make carve there. It has my favorite thing to do. People are always asking me. You know how do you make these guitars and I used to tell them the same thing. You get some mass mahogany like this and screws and ebony and the thing about bridges and that kind of thing and good sharp wood in that and get up everything, it does not look like a guitar.\nSo now, you can look around the flow here and see all things that don’t look like guitar, isn’t it?\nMale: Wayne Henderson has to put to mind is the heart, intelligence, wisdom soul and spirit of the mountain people in Virginia. He’s the famous talented artist. He has 6 CD recordings released, a National Heritage award winner who has played in more countries representing the United States than we can count on. A well traveled, brilliantly talented, humble man who loves his art, life and those who surround him which really adds up to his music.""]"	['<urn:uuid:b5981c00-249f-4252-94ad-d67b34fa165c>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	6	30	1778
24	day of dead celebration traditions alter decorations meanings	Day of the Dead celebrations involve several traditional altar decorations with specific meanings. The altars include four elements: earth (symbolized by fruit and nuts), wind (represented by stenciled cut paper), water (left for the dead from their long walks), and fire (represented by candles). Cempazuchitl (marigold) flowers are the most popular decorations, believed to guide spirits back with their distinct smell when paired with candles. Other flowers like baby's breath, gladiolus, and chrysanthemums are also used for their symbolic meanings. Altars typically feature photos of deceased loved ones, religious symbols, personal items, and sometimes toys for deceased children. These decorations help create a space for spirits to return and enjoy life's pleasures once more during the November 1st and 2nd celebration.	['Day of the Dead is no ordinary celebration.\nIn fact, it’s an extraordinary one that celebrates loved ones who have passed on by showing appreciation and respect for the dearly departed.\nFor the past four years, La Vista Memorial Park in National City has put on a fiesta like no other.\nLast Friday evening, it once again hosted Dia de los Muertos, inviting the National City community to not only enjoy, but also participate in celebrating the memories of those who have passed on.\nLa Vista Memorial general manager Luisa McCarthy came up with the idea for the event after her father passed away.\n“I thought why don’t we do a Dia de los Muertos celebration?” McCarthy said. “Many in the community are Latino and Filipino.”\nMcCarthy said the event is for those who want to keep the tradition close to their heart.\n“The purpose is keeping tradition because everybody has a loved one who has passed away and this is one day where we call their spirit here,” McCarthy said. “A cemetery is not just a grounds to be sad but also to celebrate life.”\nIn the last four years the event has grown significantly from 50 people to nearly 4,000.\nHundreds of people covered the cemetery grounds Friday from 5 to 9 p.m., partaking in the festivities which included face painting, crafting colorful sugar skulls, eating food, watching traditional dance and listening to music.\nA nine-foot authentic Michoacán alter greeted guests toward the front of the event, while men to its left and right donned in traditional dress and painted masks celebrated the “dance of the little old men.”\n“That dance reminds you of your grandparents and your father — the first ones who would cultivate the land and bring back the corn for the women to do the tortillas,” McCarthy said. “They dance because they’re happy that even though they’re old they can still be the providers of the family and they do it very proudly.”\nMcCarthy said the event is one highly respected by those who attend.\n“The atmosphere, if you walked around — people smiled at each other, it was very loving,” she said. “It was just so peaceful and harmonious.”\nAt least a dozen families participated in an alter-building contest, all of which included aspects of the four elements — earth, symbolized with fruit and nuts; wind, with stenciled cut paper; water, left for the dead from their long walks to their alter; and fire, represented by the candles.\nFor Grace Acosta, it was her family’s first time participating.\nAcosta, 42, grew up in National City and has lost many family members she and relatives celebrate each year.\n“The tradition of altars is important to leave something behind for the family,” Acosta said. “My mom used to do it. I want my kids to see it and enjoy it. I think it’s too beautiful of a tradition to be forgotten.”\nThe Acosta family altar, “angels of paradise,” featured photos of loved ones who had passed, including her brother, grandparents, great-grandparents and uncles.\n“It’s a very beautiful family event,” Acosta’s mother, Lupe Garcia said. “It helps us conserve our traditions.”\nNational City council member Louie Natividad has attended the event each year.\n“I think that because of our history, because of our culture, that it’s a great idea because we were taught that our ancestors are spirits and that if we go down to the cemetery during that day that they come and join us,” Natividad said.\nNatividad added that the city is fortunate to have a cemetery close to National City and that the owners take the time to put on the event.\n“It’s good for the children to know about it and to not be afraid and to know that there’s a purpose for it … we need to continue this cultural tradition,” he said.', 'What Is Day of the Dead?\nDay of the Dead (Día de los Muertos), is a holiday that is celebrated annually on November 1st and 2nd to honor and remember those that have passed on. The holiday resembles All Saints Day and All Souls Day, which are celebrated after Halloween on the Christian calendar.\nFirst developed in Mexico, the Day of the Dead is most notably celebrated in Latin American regions, although it is becoming increasingly popular throughout the United States, too!\nWhy Is Día de los Muertos Celebrated?\nWhen the Aztecs first began observing this occasion over 3,000 years ago, their ultimate goal was to honor and celebrate the deceased as their spirits returned, not necessarily to mourn those who are no longer with us. While it is definitely still a time of reflection and remembrance, the main focus is to celebrate the lives that our loved ones lived.\nDecorating with Day of the Dead Flowers\nThe most popular Day of the Dead flowers used for celebrations are cempazuchitl flowers, also known as “marigolds.” Decorating for Day of the Dead is one of the most beautiful things about the holiday, and flowers are often a big part of these decorations. Day of the Dead flowers are often left as an offer from family members to include on their altar and at the grave of their loved ones.\nCempazuchitl flowers, or marigolds, have a very distinct smell, and when paired with the right kind of candle, believers say that the souls of the departed return for a brief period of time to enjoy the pleasures of life once more.\nLaying cempazuchitl flowers on the altar or at a gravesite isn’t the only way they’re used on the Day of the Dead. On occasion, the cempazuchitl petals are also laid out strategically to guide the spirits back to our world (think of them as being used as a spiritual path or walkway). Since these flowers are said to represent the fragility of life, it’s only natural that they are used to honor those that have passed on to the next chapter of life.\nAdditional Flowers Used for Day of the Dead Decorations\nAlthough marigolds are the most closely associated flower with the Day of the Dead, other blooms are often used or found at the altars or near the resting pace of the deceased as well to honor deceased loved ones. Other popular Day of the Dead flowers include but are not limited to baby’s breath, hoary stock, cockscomb, gladiolus and chrysanthemums. Similar to marigolds, these flowers are chosen for a reason such as their colors, scents and meaning. For example, gladiolus often symbolize remembrance, which makes them an ideal choice for this type of observance.\nHow Is Day of the Dead Celebrated?\nWhile it may sound like it could be a rather solemn day, the truth is that the Day of the Dead is actually quite the opposite. If you ever have the chance to visit an area where Day of the Dead is observed, you’ll likely be surrounded by several parties and festivals where there is always tons of food, especially at the “altar.”\nSpeaking of the altar, for Día de los Muertos, living relatives of the deceased typically build an altar in their home where they present the spirits of their loved ones with gifts, such as fresh fruits for example. Furthermore, Day of the Dead celebrators often enjoy snacking on delicious sugar skulls – yum!\nOne of the most popular ways to celebrate Day of the Dead is by wearing skeleton or skull masks. In fact, this tradition has caused for skeletons or skulls (or calacas and calaveras) to be the most widely recognized symbols of this holiday. But skeletons, food and flowers aren’t the only decorations used on the Day of the Dead (though they are the most popular). When it comes to decor for the altar, it’s also common to see collections of candles, photos, images of religious figures or religious symbols, and potentially even some toys for those who were still children or babies at the time of their passing. Chances are you’ll also see items that were valued by the person who has passed placed on the altar too.\nIn terms of attire and costumes, you may find celebrators wearing various types of noisemakers, which is a tradition that has stemmed from the idea that music is essential to create a celebratory atmosphere. Additionally, for those celebrations that take place in the United States, you can bet that those participating will have their face painted with the sugar skull as the foundation for their design. However, don’t be surprised if you only see half of their faces painted with skull and skeleton characteristics. This is largely due to the fact that Día de los Muertos recognizes that death is a natural part of life, and as a result, some celebrators choose to only paint half of their face to represent both life and death and their connection to one another.']	['<urn:uuid:042f41a2-56fc-4cb2-8ec1-76788d6a37ee>', '<urn:uuid:168293af-c038-4d7d-bfc5-713d85eafc7a>']	open-ended	direct	short-search-query	similar-to-document	three-doc	expert	2025-05-12T10:32:05.666290	8	121	1479
25	As a healthcare researcher studying cross-cultural communication, I'm curious about how communication styles differ between African and American cultures, and how these differences impact healthcare delivery for indigenous populations?	African and American cultures have distinct communication patterns that significantly influence healthcare interactions. In African culture, which is high-context, patients use more words to explain their conditions and avoid direct eye contact with healthcare providers. In contrast, American culture is individualistic, with patients being direct and maintaining constant eye contact during medical consultations. These cultural differences in communication extend to healthcare delivery for indigenous populations, where studies show that culturally inappropriate clinical encounters can generate mistrust and dissatisfaction. For instance, research in Aboriginal healthcare services reveals that despite healthcare providers receiving cultural awareness training, Aboriginal people often don't perceive their healthcare services as culturally appropriate, highlighting the need for culturally respectful associations between providers and consumers.	"[""What is culture?\nAccording to Hofstede (2003), culture refers to the way of life of a society that is considered as the right by the members of the particular society. Culture is mostly attributable to people in a certain geographical area. For instance, the African culture is different from the American culture. Therefore, a culture depicts actions, behaviors and words that certain groups of people, who subscribe to the culture, deem to be right. These normal and beliefs come in to play a role in promoting challenges between\nThe African culture is a high-context culture as outlined by Hall & Hall, (1989). Communication in the African culture is indirect and the person will end up saying many words to make the point clear. Sometimes the extra information given might not be necessary. Furthermore, handshakes and moments of introduction are not long. In addition, conversations last longer than they ought to last.\nHall & Hall (1989) classify the American culture as individualist and thus during conversations, individuals seek to give out only the necessary information and direct to the point. Greetings and introductions are part of the communication. Most communications start and end with a handshake, hug or a kiss on the cheek.\nExample: In Africa, patients are expected to use more words when explaining to physicians how they are feeling. Eye contact is not maintained in these circumstances.\nExample: In US, patients will be direct to the point as regards their problems. A constant eye contact is maintained in the conversation.\nThe American TV series called the Blacklist presents a case of cross-cultural communication challenges in one of its African characters called Dembe. Though the storyline is not on cross-culture communication, it is quite possible to identify the struggles of the character Dembe in communicating.\nInability to communicate clearly\nGiven the big contrast between communication in the US and African cultures, Dembe is often speechless and sometimes passes the wrong message to his boss. He often uses many words to explain a small issue. For instance, in one occasion he is surprised why people were not removing their shoes will entering a persons house. This is considered as a show of disrespect in African culture.\nThe character often feels that some actions and behaviors are not well with him given his cultural orientation. There is a feeling of lack of authenticity that could trouble a person from the inside after seeing people practicing what would be considered wrong in their culture.\nBeing accommodative and open to trying new things is the key principle to overcoming cross-culture communication challenges. The person should find a way of becoming comfortable by appreciating diversity and being open to try out new things.\nAlthough it might be difficult to make an adjustment, the person could try to make necessary adjustments mostly if he/she is likely to live in the conflicting culture for a longer period (Hall & Hall, 1989). There is a great challenge making adjustments though since people will always want to maintain their cultural orientation.\nIn every culture, there are key phrases, behaviors and actions that if committed or omitted may be disrespectful. It is important for the person to learn these key issues in the new culture and internalize them. To avoid cases of conflicts and the feeling of being left out, the person should seek to belong.\nHall, E. T., & Hall, M. R. (1989). Understanding cultural differences. Intercultural press.\nHofstede, G. (2003). What is culture? A reply to Baskerville. Accounting, Organizations and Society, 28(7), 811-813.\nIf you are the original author of this essay and no longer wish to have it published on the thesishelpers.org website, please click below to request its removal:\n- The Influence of Popular Culture on Violence - Essay Example\n- The Aging Process and Society's View - Essay Example\n- Case Study Example: The Cobra Effect\n- Essay Sample: Unique Cultural Practices of Chinese Americans\n- In the Mood for Love - Research Paper on Melodrama\n- Education Essay Example: Lesson on Cultural Diversity\n- Paper Example on Transcultural Healthcare"", 'Are primary healthcare services culturally appropriate for Aboriginal people? Findings from a remote communityKaye Smith A , Yaqoot Fatima A B and Sabina Knight A\nA Mount Isa Centre for Rural and Remote Health, James Cook University, 100 Joan Street, Mount Isa, Qld 4825, Australia.\nB Corresponding author. Email: firstname.lastname@example.org\nAustralian Journal of Primary Health 23(3) 236-242 https://doi.org/10.1071/PY16110\nSubmitted: 1 August 2016 Accepted: 13 January 2017 Published: 13 April 2017\nThis study explored the views of key stakeholders on cultural appropriateness of primary health care (PHC) services for Aboriginal people. A total of 78 participants, including healthcare providers, administrative team members (n = 24, ~30% of study sample) and Aboriginal community members (n = 54, ~70% of study sample) living in remote North West Queensland participated in the study. Outcome measures were assessed by administering survey questionnaires comprising qualitative questions and various subscales (e.g. provider behaviours and attitudes, communication, physical environment and facilities, and support from administrative staff). Descriptive statistics were used to present quantitative findings, whereas inductive thematic analysis was used for qualitative data. In contrast to the views of PHC providers, a significant number of Aboriginal people did not perceive that they were receiving culturally appropriate services. Although PHC providers acknowledged cultural awareness training for familiarising themselves with Aboriginal culture, they found the training to be general, superficial and lacking prospective evaluation. PHC providers should understand that culturally inappropriate clinical encounters generate mistrust and dissatisfaction. Therefore, a broad approach involving culturally respectful association between PHC providers, Aboriginal consumers and administrative staff is required to bring sustainable changes at the practice level to improve the health of Aboriginal people.\nReferencesAndrews B, Simmons P, Long I, Wilson R (2002) Identifying and overcoming the barriers to Aboriginal access to general practitioner services in Rural New South Wales. The Australian Journal of Rural Health 10, 196–201.\n| Identifying and overcoming the barriers to Aboriginal access to general practitioner services in Rural New South Wales.CrossRef |\nAustralian Bureau of Statistics (2006) National regional profile Mount Isa 2002 to 2006. Australian Bureau of Statistics, Canberra.\nBainbridge R, McCalman J, Clifford A, Tsey K (2015) ‘Cultural competency in the delivery of health services for Indigenous people.’ (Australian Institute of Health and Welfare: Canberra, ACT, Australia)\nBelfrage M (2007) Why ‘culturally safe’ health care? The Medical Journal of Australia 186, 537–538.\nBywood P, Katterl R, Lunnay B (2011) ‘Disparities in Primary Health Care Utilisation: Who are the Disadvantaged Groups? How are They Disadvantaged? What Interventions Work? PHCRIS Policy Issue Review.’ (Primary Health Care Research & Information Service: Adelaide, SA, Australia)\nCook CT, Kosoko-Lasaki O, O’Brien R (2005) Satisfaction with and perceived cultural competency of healthcare providers: the minority experience. Journal of the National Medical Association 97, 1078–1087.\nDurey A (2010) Reducing racism in Aboriginal health care in Australia: where does cultural education fit? Australian and New Zealand Journal of Public Health 34, S87–S92.\n| Reducing racism in Aboriginal health care in Australia: where does cultural education fit?CrossRef |\nEggington D (2012) Aboriginal health equity: the key is culture. Australian and New Zealand Journal of Public Health 36, 516\n| Aboriginal health equity: the key is culture.CrossRef |\nFarnbach S, Eades A-M, Hackett ML (2015) Australian Aboriginal and Torres Strait Islander-focused primary healthcare social and emotional wellbeing research: a systematic review protocol. Systematic Reviews 4, 189\n| Australian Aboriginal and Torres Strait Islander-focused primary healthcare social and emotional wellbeing research: a systematic review protocol.CrossRef |\nFreeman T, Edwards T, Baum F, Lawless A, Jolley G, Javanparast S, Francis T (2014) Cultural respect strategies in Australian Aboriginal primary health care services: beyond education and training of practitioners. Australian and New Zealand Journal of Public Health 38, 355–361.\n| Cultural respect strategies in Australian Aboriginal primary health care services: beyond education and training of practitioners.CrossRef |\nGill GK, Babacan H (2012) Developing a cultural responsiveness framework in health care systems: an Australian example. Diversity and Equality in Health and Care 1, 45–55.\nGozu A, Beach MC, Price EG, Gary TL, Robinson K, Palacio A, Smarth C, Jenckes M, Feuerstein C, Bass EB, Powe NR, Cooper LA (2007) Self-administered instruments to measure cultural competence of health professionals: a systematic review. Teaching and Learning in Medicine 19, 180–190.\n| Self-administered instruments to measure cultural competence of health professionals: a systematic review.CrossRef |\nHooper K, Thomas Y, Clarke M (2007) Health professional partnerships and their impact on Aboriginal health: an occupational therapist’s and Aboriginal health worker’s perspective. The Australian Journal of Rural Health 15, 46–51.\n| Health professional partnerships and their impact on Aboriginal health: an occupational therapist’s and Aboriginal health worker’s perspective.CrossRef |\nKeast K, Dragon N (2015) Indigenous health stepping into the gap. Australian Nursing & Midwifery Journal 22, 18–22.\nMcMurray A, Param R (2008) Culture-specific care for indigenous people: a primary health care perspective. Contemporary Nurse 28, 165–172.\n| Culture-specific care for indigenous people: a primary health care perspective.CrossRef |\nOng LM, de Haes JC, Hoos AM, Lammes FB (1995) Doctor–patient communication: a review of the literature. Social Science & Medicine 40, 903–918.\n| Doctor–patient communication: a review of the literature.CrossRef | 1:STN:280:DyaK2Mzht1yquw%3D%3D&md5=e40e8fb2e6d2500306b67e65e963f278CAS |\nSi D, Bailie R, Cunningham J, Robinson G, Dowden M, Stewart A, Connors C, Weeramanthri T (2008) Describing and analysing primary health care system support for chronic illness care in Indigenous communities in Australia’s Northern Territory – use of the Chronic Care Model. BMC Health Services Research 8, 112\n| Describing and analysing primary health care system support for chronic illness care in Indigenous communities in Australia’s Northern Territory – use of the Chronic Care Model.CrossRef |\nStephens C, Parkes MW, Chang H (2007) Indigenous perspectives on ecosystem sustainability and health. EcoHealth 4, 369–370.\n| Indigenous perspectives on ecosystem sustainability and health.CrossRef |\nThe Association of Faculties of Medicine of Canada (2013) AFMC Primer on Population Health. An AFMC Public Health Educators’ Network resource. (AFMC) Available at https://afmc.ca/pdf/AFMC-Primer-on-Population-Health-2013-08-14.pdf [Verified 4 April 2017]\nThomson N (2005) Cultural respect and related concepts: a brief summary of the literature. Australian Indigenous Health Bulletin 5, 1–11.\nTucker CM, Moradi B, Wall W, Nghiem K (2014) Roles of perceived provider cultural sensitivity and health care justice in African American/Black patients’ satisfaction with provider. Journal of Clinical Psychology in Medical Settings 21, 282–290.\n| Roles of perceived provider cultural sensitivity and health care justice in African American/Black patients’ satisfaction with provider.CrossRef |\nTucker CM, Wall W, Marsiske M, Nghiem K, Roncoroni J (2015) Validation of a patient-centered culturally sensitive health care office staff inventory. Primary Health Care Research and Development 16, 506–512.\n| Validation of a patient-centered culturally sensitive health care office staff inventory.CrossRef |\nvan der Geest S (2004) Forgetting compliance: Aboriginal health and medical culture: Kim Humphery and Tarun Weeramanthri with Joseph Fitz; Northern Territory University Press, Darwin NT, 2001, 122pp. Social Science & Medicine 58(1), 220.\nWestwood B, Westwood G (2010) Aboriginal cultural awareness training: policy v. accountability – failure in reality. Australian Health Review 34, 423–429.\n| Aboriginal cultural awareness training: policy v. accountability – failure in reality.CrossRef |']"	['<urn:uuid:ed984e2d-6864-4127-8fb5-5d96ebd2a636>', '<urn:uuid:dd591238-ade8-4e37-ad5c-b397d5aa5e65>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:32:05.666290	29	116	1837
26	impact energy efficiency farm waste digester permit	Regarding energy efficiency, the study of a farm-based anaerobic digestion (AD) reactor in Norway showed that energetic consumption (1592 kWh/year) exceeded production (482 kWh/year). Radiation losses were the main energy consumer, but could be reduced by up to 18% through improved insulation and reactor shape modifications. Regarding permits, most AD installations require planning permission, though small-scale digesters using only on-farm waste may qualify as Permitted Development. An Environmental Impact Assessment is mandatory for facilities processing over 50,000 tonnes of waste annually or in sensitive locations, and many councils request EIAs even for smaller installations.	"['Department of Process, Energy and Environment, Faculty of Technology, Telemark University College, Porgsrunn, Norway\nReceived date: 30/01/2016; Accepted date: 01/04/2016; Published date: 10/06/2016\nVisit for more related articles at Research & Reviews: Journal of Engineering and Technology\nAn anaerobic digestion (AD) process is studied. The AD reactor belongs to UASB (up-flow anaerobic sludge blanket) typology and it was constructed in a typical Norwegian farm (Foss farm) localized in Skien, Norway. The reactor is fed with cattle manure. The process is divided into two different parts: pre-treatment and AD, nitrification and post-treatment. Biogas and compost are outputs of the first section, liquid fertilizer output of the second one. This paper has two core parts: energy/mass flows determination and CO2 emissions-reduction assessment. The study has indicated that energetic consumption is bigger than production. Radiation losses represent the most important terms of consumed energy, but is possible to diminish them by changing insulation and/or reactor shape. The pilot plant has an average energy consumption and thermal production of 1592 kWh/year and 482 kWh/ year, respectively. The equivalent CO2 emissions of open-air-standing cow’s manure are noteworthy. Two different probable situations are analyzed: processed manure and not processed manure. The reduction of carbon dioxide releases in the first case, compared to the second, is significant and goes from 32% to 83%. It could be possible to reduce energetic consumption up to 18% halving radiation losses and raise energetic production up to 250% by increasing the methane yield from 20% to 50% of the maximum value.\nBiogas, Anaerobic digestion, Energy, Radiation.\nAnaerobic digestion (AD) is a green-house gases (GHG) saving tool and a renewable energy carrier . This work focuses on a small-scale biogas plant built in Foss farm in Skien, Norway. The substrate is cow’s manure. The process is divided into AD, in an UASB (up-flow anaerobic sludge blanket) reactor typology, and nitrification. The latter allows achieving high nutrient’s level in the output fertilizer . The input of the process is the manure (with 25% of added water), while the output are biogas from AD, compost from the initial separation and the final fertilizer after the nitrification (solid and liquid part). The present reactor in FossLab has been in operation since April 2012. Main goals of this work are to evaluate the energetic flows (production and consumption) and to demonstrate that processing manure means less CO2,equ emissions.\nThe initial substrate’s flow is 60 L/day. It is pumped (P1) from the reservoir open-air tank (R) to a buffer tank (B). Later, it is mixed (M1) and sieved (S) separating 30% of solid (vermi-composting) and 70% of liquid. The latter is pumped (P2) in the AD reactor (AD). This UASB AD reactor has kept at an average value of 32°C (operational range 25°C / 35°C). Average biogas production is 230 L/day containing 70% of CH4. The AD effluent has sent to the nitrification reactor (N) in which ammonia is converted into nitrate thanks to oxygen , entering in reaction through the air pumped in (P4). Nitrified effluent is mixed (M2) and separated in two different stages: a funnel-shaped tank, in which half of the influent is removed as solid and half, as liquid, is pumped (P3) in an oozing bag. Here 70% of the total is the filtrate-part and the rest part is solid-foam. Output of the overall process will be biogas and fertilizers (the vermin-composting in the first stage, the liquid and solid fertilizers after the nitrification). In the next table are summarized all components data (Table 1).\n|Entering flow [L/day]||60||/||/||/||/||42.5||/||/||41||/||/|\nTable 1: Data of reservoir open-air tank (R), pumped (P1), buffer tank (B), mixed (M1), sieved (S), pumped (P2), reactor (AD), pumped in (P4), nitrification reactor (N), mixed (M2), pumped (P3).\nFor the following work were used: online data monitored through sensors-network 8 times per hour (Data 1), offline data hand-measured through sample-survey about twice a week (Data 2), data measured in the lab (Data 3) and data taken from literature (Data 4). The first two types are present in a database implemented for a previous work  in the same lab, starting from 19th April 2012 and ending on 20th February 2014. Below all data are summarized (Table 2).\n|Data 1||Data 2||Data 3||Data 4|\n|Biogas flow [L/d]||COD in substrate [gCOD/Lfeed]||Pumps capacities [W], working time [h] and dimensions [m]||(4) Calorific value of methane [MJ/Nm3]|\n|Methane concentration [%]||VS in substrate [gVS/Lfeed]||Mixers capacities [W] , working time [h] and dimensions [m]||(5), (6) Substrate heat capacity and density [kJ/kg/K]|\n|CO2 concentration [%]||TS in substrate [gTS/Lfeed]||Sieve capacity [W] , working time [h] and dimension [m]||(7), (8), (9) Steel, insulation and ground thermal conductivities [W/m/K]|\n|Feeding AD flow [L/d]||/||Lamps capacities [W] and working time [h]||(10) Air heat transfer coefficients [W/m2/K]|\n|Inlet AD reactor temperature [°C]||/||AD reactor dimension [m]||Thermal and electricity energy efficiency [%]|\n|Room temperature [°C]||/||Nitrification reactor dimension [m]||(11) Sensors and computer capacities [W]|\n|Reservoir tank temperature [°C]||/||Reservoir and buffer tank dimensions [m]||/|\nTable 2: Online data monitored through sensors-network 8 times per hour (Data 1), offline data hand-measured through sample-survey about twice a week (Data 2), data measured in the lab (Data 3) and data taken from literature (Data 4).\nIn order to understand if this lab is energetically free-standing an energy balance was implemented. The net energy was calculated according to (1).\nE= Eproduced - Elosses- Econsumed (1)\nTwo producing cases were taken in account: thermic or electrical energy produced. Assuming respectively 60% efficiency (gas/steam turbine combined cycle) and 30% efficiency (advanced gas-turbine engine). These equations terms are evaluated in the appendix according to (2) and (3).\nIn the histogram (Figure 1) above it is possible observe many terms: the three red producing-energy-terms need to be considered originating from the zero-value-ordinate with the last one which represent the energy produced with the biogas with a 100% efficiency. The consuming-energy-terms are cumulative with the radiation losses, represented in a lighter blue to highlight their variation with the outlet temperature. Reactor temperature and feed flow are present too to clarify the seasonal energy flow variations.\nEnergy losses are divided into radiation losses (4) of the AD reactor, because the inlet reactor temperature is always above the room temperature, and heat needed for the substrate (8), to bring the substrate from the outlet temperature to above 30°C for the biological process.\nEnergy consumed in FossLab is due to the pumps (P1, P2, P3 and P4), to the sieve, to the mixers (M1 and M2), to the lightning system (one LED lamp) and to the controlling system (sensors and computer). It was calculated using the maximum power capacity and the working time of each component according to (9), (10), (11), (12) and (13) respectively.\nTwo different cases were taken in account: processed manure (Case 1) and not processed manure (Case 2). In the first case the emissions would be due to the CH4 reacting in the combustion (Case 1.1), to the CO2 present in biogas and no reacting in combustion (Case 1.2) and to the CO2 resulting by the production of energy consumed in the lab – assuming it is produced by natural gas (Case 1.3). In the second case, emissions would be due to the outside standing manure producing CH4 (Case 2.1) and to the emission of CO2 resulting by the production of energy of the previous case (Case 2.2).\nMass flow analysis was implemented using COD, VS and TS concentration [g/Lfeed] database-values in order to determinate the biological efficiency represented by the yield [LCH4/g] calculated by (14) and (15). Stoichiometric value of the latter yield is 0.35 LCH4/gCOD according to (17). Fixing the COD yield (15) with values equal to 50%, 75% and 100% of the stoichiometric one, were evaluated biological efficiencies (14). If the latter increase, the produced energy will grow as well. Results are tabled below with mean values for the yields and summed (during database period) values for energy (Table 3).\n|Case||COD yield||Yield percentage||VS yield||Electric energy||Thermic energy||Methane equivalent energy||Energy percentage|\n|[L CH4 / g COD]||[%]||[L CH4 / g VS]||[kWh]||[kWh]||[kWh]||[%]|\nTable 3: Mass flow otpimization.\nAnother analysis has been realized, studying radiation losses. In the real case (Case A), radiation losses represent 28% of consumed energy (4). This term was reduced by several procedures:\n• changing AD reactor shape minimizing surface’s area in contact with the room temperature (Case B),\n• decreasing the insulation’s thermal conductivity from 0.06 W/m/K to 0.04 W/m/K  (Case C),\n• increasing insulation’s thickness from 5 cm to 8 cm (Case D),\n• considering the best case as a ‘sum’ of all previous cases (Case E).\nResults are summarized following (Table 4). In the case E radiation losses characterize 12% of the overall consumption.\n|Mean radiation losses [kWh/d]||Percentage in respect to real case [%]||Mean total consumed energy [kWh/d]||Percentage in respect to real case [%]|\n|Case A||Real case||1.23||/||4.36||/|\n|Case B||Changing reactor shape||0.88||-28%||4.02||-8%|\n|Case C||Decreasing insulation thermal conductivity||0.85||-30%||3.99||-9%|\n|Case D||Increasing insulation thickness||0.81||-34%||3.94||-10%|\n|Case E||Best case||0.44||-64%||3.57||-18%|\nTable 4: Energy flow optimization.\nThe AD process in FossLab consumes more energy than it produces. This gap may be reduced through some expedients in order to increase the energy production such as increasing the COD yield (and the biological efficiency) or some procedures in order to decrease the energy consumption (radiation losses). On the other hand, this technological process reveals itself to be one of the best ways to avoid outstanding cattle manure emissions of CH4, which is a strong GHG  and influences the global warming.\nI am thankful to the Department of Process, Energy and Environment at the Telemark University College represented by professors Carlos Dinamarca, Finn Haugen and Rune Bakke for providing data and for the guidance.', ""Planning permission is necessary for most anaerobic digestion installations. Small scale digesters using only on-farm waste may be passed as Permitted Development, but it is recommended you speak to your local authority in the early stages to confirm this. Any installation accepting third party waste will need full planning permission.\nThe planning authorities’ role in dealing with proposals for AD plants in Scotland has been clearly set out in Scotland’s Zero Waste Plan (ZWP) 2010. Advice on the development of AD facilities can be found on the Scottish Government Website. Planning applications in Scotland are dealt with by the local planning authority.\nIn Northern Ireland planning permission falls under Northern Ireland planning policy, PPS 18: Renewable Energy.\nThere is nothing that can guarantee any planning application is approved, but the following steps lower the risk of a refusal and save time and money by avoiding resubmissions and planning appeals.\n- Initiate a pre-application enquiry with your local minerals and waste development control team at an early stage in your feasibility process. You can find your minerals and waste development control team through your local council here.\n- Inform the local community at an early stage. Many successful AD projects have begun their consultation before formal plans have been submitted – this minimises the risk of misinformation being circulated.\n- Prepare for a possible Environmental Impact Assessment (EIA). This is a must if your proposal is large (accepting over 50,000 tonnes waste per year) or in a sensitive location e.g. Conservation area, Green Belt, close proximity to residential development. However, many councils are adopting a precautionary approach and requesting EIAs for installations well under the 50,000 tonne threshold.\n- When submitting a plan, it can be helpful to refer to Planning Policy Statements, which state the Government's principles towards certain aspects of planning. Two particularly relevant documents are Planning Policy Statement 22: Renewable Energy and Planning Policy Statement 7: Sustainable Development in Rural Areas. Planning Policy Statement 22 states that a planning application for an anaerobic digestion plant could usefully include the following:\n- site plan and elevation drawings to help determine visual impact\n- photomontage of digester, plant building(s) and chimney stack with clear indication of building material\n- information on grid connection works, including transformer and transmission lines\n- details of emissions to air and an assessment of their impact\n- details of vehicular access and vehicular movement\n- landscaping provisions\n- site management measures during the construction phase\n- model of emissions dispersion\n- community consultation plans\nIt is important to communicate and engage with the local population from an early stage in development to ensure they fully understand the proposals, process and terminology. Surveys have shown people are generally positive about renewable energy. However, this doesn't necessarily translate into support for a local AD project.\nCommon concerns with AD applications are noise, odour, visual impact and transport – ensure communications around these areas are clear , to eliminate local speculation and reduce the risk of objection.\nThis case study of Merevale and Blyth Estates Biomass plant, produced as part of the Beyond Nimbyism project, covers general planning issues such as trust and engagement.\nDetailed designs are required before submitting a planning application, and outline feedstock agreements may also be necessary. A useful indicative list of the documents that may be required to develop an AD project are available here.\nWho can benefit from anaerobic digestion?\n- Potential for food waste collections\n- Energy from a local, renewable source\n- Potential for heat to harness for local use\n- New jobs\n- Cleaner environment\n- Reduced odours from slurry\n- Reduces emissions of methane, a potent greenhouse gas\n- Produces renewable energy\n- Reduces the amount of biodegradable waste sent to landfill\n- Produces fertiliser, reducing the demand for petrochemical-derived fertilisers\n- Can reduce nitrate pollution by decreasing run-off\nFarmers and Energy Entrepreneur\n- Can be profitable\n- Supports livestock sector\n- Digestate provides more nitrogen to crops than slurry\n- Helps to deal with Nitrate Directives\n- Kills pathogens in the feedstock, reducing potential diseases\n- Kills seeds in the feedstock, preventing the spread of weeds\n- Helps to meet environmental targets\n- Contributes to the renewable energy targets in the Renewable Energy Strategy\n- Helps to meet the Landfill Directive\n- Can help meet Nitrates Directive implementation requirements\n- Can help Local waste authorities with the Landfill Allowance Trading Scheme\n- Better organics recycling is a good message for customers\n- Easier waste recycling compliance\n- Easier rodent and vermin control for food processing industry\n- Cheaper than incineration or landfill""]"	['<urn:uuid:96f71573-0110-498b-bd1b-c1ff51578b8d>', '<urn:uuid:81cbd42a-3ab6-4c12-875e-5cb59854bd28>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T10:32:05.666290	7	94	2353
27	space station manual micro satellite release method	Russia's method of releasing satellites from the International Space Station involves a spacewalking crew member manually throwing the small satellites away. This ensures a positive separation from ISS, with the throw direction carefully chosen to prevent any re-contact with the satellite on subsequent orbits. This is different from the U.S. approach, which uses sophisticated deployers and the Kibo Robotic Arm.	['Two Russian Cosmonauts are set to venture outside the International Space Station on Thursday on a planned spacewalk of six hours and five minutes dedicated to a variety of tasks on the exterior of the Russian Segment. Veteran spacewalkers Fyodor Yurchikhin and Sergey Ryazansky will test a new version of Russia’s Orlan space suit, deploy five small satellites, install external experiments on the Russian segment and obtain microbial samples from the outer hull of ISS.\nThursday’s Russian Stage EVA is the seventh spacewalk taking place outside the International Space Station in 2017, coming after six U.S.-based spacewalks completed over the course of the year so far. Russian EVA-43 is the first Russian-Segment spacewalk since February 2016 when Yuri Malenchenko and Sergei Volkov retrieved and installed various external experiments on the Russian Segment.\nThursday’s EVA will be carried out by two experienced spacewalkers and the only two Russian crew members of ISS after Roscosmos reduced its permanent Station crew to only two members as a cost-saving measure and to respond to current workload requirements on the Russian Segment.\nFyodor Yurchikhin is a veteran of eight previous spacewalks, having performed his first EVA in 2007 and raking up a total EVA time of 51 hours and 53 minutes, ranking seventh on the all-time record list with prospects of moving into fourth if Thursday’s EVA runs the planned duration. Sergey Ryazansky conducted three previous EVAs for a total of 20 hours and 5 minutes including the highly publicized November 2013 spacewalk that took the Olympic Torch into the vacuum of space before being flown back to Earth to light the Olympic Flame for the 2014 Winter Games held in Sochi.\nYurchikhin will be the lead spacewalker on Thursday wearing Orlan-MKS No. 4 with blue stripes while Ryazansky, designated EV-2, will wear Orlan-MK No. 6, also with blue stripes. The duo is scheduled to open the external hatch of the Pirs airlock at 14:41 UTC after several hours of EVA preparations that include suiting up and sealing off the Pirs module and PKhO Transfer Compartment that acts as a backup airlock for Russian spacewalks.\nThursday’s EVA will begin with the release of five small satellites flown up to the Space Station by several recent Progress missions, set for a collective release during the spacewalk. Unlike the U.S. CubeSat deployment infrastructure with sophisticated deployers and the Kibo Robotic Arm, Russia’s method of releasing satellites from ISS is rather low-key and involves a spacewalking crew member literally throwing the small satellites away to ensure a positive separation from ISS with a direction chosen to rule out any re-contact with the satellite on subsequent orbits.\nThe EVA procedure calls for Sergey Ryazansky egressing the airlock and setting up on the EVA ladder in front of Pirs. His first task will be the collection of photos and video of the “Restavratsiya” (Restoration) Experiment that was performed on the previous Russian EVA by applying a thermal protection foil to a plate which was then left exposed to the space environment to study how the material fares over time. Ryazansky will demate the plates and hand them to Yurchikhin for transfer into ISS and eventual return to the ground.\nNext will be the release of the five satellites, also to be completed by Ryazansky while Yurchikhin remains in Pirs to hand out the satellites with a total of 40 minutes budgeted for the satellite deployment, to be captured by a GoPro 360 camera.\nThe five satellites to be released during Thursday’s EVA are:\nTOMSK-TPU-120 is a 3U CubeSat from Tomsk Polytechnic University launched in March 2016 to test new satellite materials, featuring an all 3D-printed structure plus a handle to assist with the manual deployment. The 5-Kilogram satellite hosts an amateur radio payload and was first activated in May 2016 while still on board ISS to commemorate the 120th anniversary of Tomsk Polytechnic University.\nTNS-0 №2 is the second Tekhnologicesky Nanosputnik orbited under a program of Russia’s government-industry complex in cooperation with the Scientific Institute of Space Device Engineering. The first TNS satellite was released from ISS in March 2005 and No.2 will feature a number of upgrades, measuring 20 by 65cm in size and weighing 5.1 Kilograms. Its primary purpose is testing out new small satellite systems for power generation, attitude control and communications using the Globalstar satellite network and a UHF transceiver.\nRadioscaf RS-6 and RS-7, also known as Tanyusha SWSU №1 & №2, are two small satellites developed by Southwestern State University, Kursk. Both weigh in at around 4.8 Kilograms and use 3D printed structures, complying with the 3U CubeSat form factor but adding handles and fixed antennas as they do not need to fit into a CubeSat deployer. The two satellites feature communications systems that will be joined in a peer-to-peer type data network that could be used for the self-organization of large satellite constellations with a high degree of autonomy for adding new satellites to the network and removing failed ones. The network will support retransmission and parallel transmission to a ground monitoring station.\nRS-6 and 7 also carry SWSU-developed vacuum gauges to measure the density of neutral and charged particles in Earth’s upper atmosphere as a function of altitude. The amateur radio community can engage in the missions by receiving voice greetings from the satellites in four different languages.\nSfera-53 №2 (ТС530-Зеркало) is a follow-on to the first Sfera-53 released from ISS in August 2012. It is a passive spherical satellite measuring 53 centimeters in diameter and weighing 13 Kilograms for use in the calibration of ground-tracking equipment (either optical or radar) and to measure atmospheric density as a function of altitude by tracking the satellite’s orbital decay. Sfera-53 remained in orbit for around three months.\nWhen all satellites are deployed, the EVA clock should be reading PET+63 minutes at which point Yurchikhin will also be out of the airlock and the two spacewalkers will translate aft to the large diameter of the Zvezda Service Module. There, they will collect photos of the SKK No.9 exposure experiment before starting the installation of the “Impakt” experiment package.\nImpakt, to be installed on Plane I of Zvezda, will expose a number of material and coating samples to the space environment to study how different materials degrade in this challenging environment while also being exposed to exhaust from the Space Station’s thrusters, aiming to examine the scope of contamination and corrosion caused by thruster exhaust and propellant residue.\nThe two spacewalkers will put in place a hand rail and retrieve their GoPro camera before departing Zvezda and moving inboard – going through the standard procedure of wiping down their gloves and suits before throwing the towels overboard to remove potentially harmful contamination due to thruster residue and unburnt propellants accumulating around the aft section of Zvezda. Both EV crew members will stop by the airlock to retrieve equipment for the second half of the EVA before moving up to the Poisk module, installing a hand rails between MRM-2 and the Service Module along their way.\nAt Poisk, the crew members will get stated with the “Test” Experiment involving a series of samplers to collect surface samples from the outer protection layer of ISS and the windows. The Test Experiment aims to study how different materials behave in the challenging space environment over a long period of time characterized by harsh thermal variations, high doses of radiation and atomic oxygen causing corrosion. Test also looks at the distribution of thruster residue and potential microbial activity on the outside of the space station which has proven to be an interesting area of research based on the previous ten Test samples that have been returned so far.\nThe Test Samplers consist of pairs of collection devices for the acquisition of swab samples and containers to protect the samplers. Yurchikhin and Ryazansky will take samples from the BL-1 and 2 hatch areas on Poisk and also collect photos of another SKK experiment (No. 3) and install an Expose experiment unit.\nAlso at Poisk, the crew will change the positioning of the БКДО Plume Impingement and Deposit Monitoring Unit to view a different sector in its ongoing study of of the plumes created by the thrusters of visiting vehicles and the Space Station itself to learn more about the impingement characteristics of the thruster plumes and processes causing the deposition of combustion products on the external hull of the space station.\nYurchikhin and Ryazansky will install a sensor unit on the BL-2 hatch of Poisk and put in place hand rails between ports 6016 and 1505 before moving back down to the Pirs module where they will collect another pair of test samples and put in place exposure devices. Finally, the spacewalkers will go through a tool inventory and move back into the Pirs module to end the EVA in orderly fashion with repressurization.\nThe entire EVA will be used as a technology demonstration of the new Orlan-MKS space suit that offers greater capabilities and is geared to providing greater safety and comfort to spacewalking crew members.\nOrlan-MKS has been designed to be more robust than its predecessor while being much easier to use and operate, cutting time from the typical preparations flow for each spacewalk carried out on ISS. The suit has been built for up to 20 uses over a service life of six to seven years, capable of supporting extended spacewalks of up to nine hours. Changes to the older suits include the introduction of an automated thermal control system, a suit management computer that controls the temperature & monitors the suit’s life support functions, and a new, more durable material in the suit’s protective outer layers.\nThe more robust pressurization layer has an internal capability of withstanding failure, allowing a backup layer to be eliminated which makes the suit lighter and more flexible.\nFyodor Yurchikhin will evaluate the performance of Orlan-MKS for its first in-space test that follows extensive testing performed on the ground over the last five years.']	['<urn:uuid:55da32e4-9396-4b6d-9f50-92eead67a2c3>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	7	60	1661
28	What are the main things I should do to get ready before putting my piano up for sale, to make sure I can get a fair price for it?	To prepare for selling a piano, you should first get it professionally tuned and address any necessary repairs, as buyers may be deterred by an out-of-tune instrument. Keep all maintenance receipts as they prove the care you've taken. Then gather documentation including: year, make, model, size, color; current condition details of surface, legs, keyboard, pedals, strings, hammers and soundboard; extra features; reasons for selling; serial number; and service history. To determine a fair price, you can use the free Blue Book of Pianos online resource or hire a piano technician for an evaluation, though this may cost up to $250.	['The only way to make sense out of change is to plunge into it, move with it, and join the dance.Alan Watts, from The Wisdom of Insecurity [ 2 ]\nParting with any of your possessions can be difficult, as nostalgia seems to creep into every box, memento or broken chair. That piano, on which you first learned Für Elise, or watched your children pounding out chopsticks, taps directly into your sense memory. These charged emotions are not to be trusted when selling your piano.\nThat’s where this guide comes in, to help navigate through the logistics during this seemingly difficult task; and avoid rash, uninformed decisions. You want to approach selling your piano with as much zeal as you did in buying it—so be prepared.\nThe first step is to get your piano tuned professionally and make sure there aren’t any necessary repairs. Buyers may be turned off if the piano is too out of tune. And showing your instrument in the best condition is an important selling tool. Keep the receipt, and while you’re at it, dig up any old receipts; an often-overlooked step that authenticates the care you’ve taken to preserve your piano (maintenance & cleaning).\nThe best way to inform is to be informed. Prepare as much documentation about your piano to ensure you can communicate its value effectively and answer any questions the buyer may pose. This will also help you succeed in getting an appropriate price for your piano.\nThe following is a simple list of things to prepare in advance of selling:\n1. General information: year, make, model, size, color.\n2. Describe current condition: surface/finish, piano legs, keyboard, pedals, strings, hammers & soundboard.\n3. List extra features.\n4. Reasons for selling.\n5. Locate serial number.\n6. Last date tuned and serviced.\n7. Any repairs needed?\nThe next step is to figure out what your piano is worth. The Blue Book of Pianos is a free online resource that can help you get started. Or you can hire a piano technician to provide an evaluation. There is a fee associated with this path that could be up to $250. But it may be worth it to properly ensure you are getting the most you can.\nNow that you are fully armed with information, it’s time to decide how you will sell it. Things to consider: Do you want to place an ad in the paper, online or both and sell it yourself? You will probably get more money if you sold privately and can choose a fitting home for your beloved instrument. However, it may take a while. This involves a level of patience and negotiating for which you may not have the time or energy. Besides, you will have to contend with strangers in your private space.\nAn alternate possibility is to sell it through a dealer or merchant such as a piano store. As an incentive, they will offer a free valuation. It may be as simple as filling out a form and sending pictures, along with a serial number. Unlike a private sale, you won’t have to sell it yourself; you’ll receive immediate payment, and avoid strangers in your home. However, you may not get as much for it as you would on your own. But remember you have done all your research so you know what your piano is worth and can negotiate smartly.\nSo that’s all there is to selling your piano. Still feelin’ kinda blue? Perhaps knowing that you have informed yourself and the buyer as best you can, will help you say goodbye to an old friend.']	['<urn:uuid:0cb315ca-8dd5-4c39-8d6c-af35a12e8d14>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	29	100	600
29	How does storing alcohol in wooden barrels change its taste?	When alcohol is stored in charred oak barrels, several changes occur. The alcohol soaks into the wood, absorbing flavor and color from its vanillin and tannins. Additionally, oxygen enters through the barrel, which softens and mellows the alcohol. The charred wood's carbon also acts as a filter. This process is particularly important for whiskey, where the majority of its flavor comes from the barrel aging process.	['The popularity and demand for barrel-aged beer has grown a lot over the last several years. Our ever evolving palates crave more complexity in flavor and mouthfeel, which can be achieved by infusing beer with the essences of oak barrels previously occupied by other spirits like whiskey. Now distillers are completing that circle by using barrels formerly inhabited by beer to finish whiskey. Whiskey aged in beer barrels? It’s a mad, mad world.\nLet’s go back to before all of the action, when the barrel was just a virgin. The oak barrel is first charred to a specific degree desired by the distiller, then filled with clear alcohol fermented from a mash of grains and distilled to the proper proof. After spending any length of time in a charred (usually new) oak barrel, it becomes whiskey. Some of the flavor of the finished spirit can be attributed to its mash bill (the sum of all grains used) and the yeast used in fermentation, but the majority comes from the barrel. As it ages, the alcohol soaks into the wood, permitting a transfer of flavor and color from its vanillin and tannins. The barrel also allows oxygen in, which softens and mellows the whiskey from its harsher origins straight from the still. Carbon from the char acts as a filter and also aides in this.\nOnce the barrel is emptied, it’s journey might lead to a brewery (or homebrewer). Here, it becomes the vessel for a beer that will now benefit from many of the same qualities that the whiskey did. The beer will also mix with the residual spirit left in the barrel, adding yet another layer of complexity to its flavor. With each use, the wood becomes a little less potent and its characteristics more subtle. Breweries may start with a heavier beer like an Imperial stout, but graduate to somewhat lighter beers with each use.\nNow, let’s head back over to the distillery. They’ve made a fresh batch of whiskey and matured it in new charred oak barrels as usual. To make things a bit more interesting, the whiskey can be finished in a barrel that once held another spirit (like sherry, rum, or brandy), or it can go into a beer barrel! The very same barrel that started with whiskey and then contained craft beer, could again house whiskey. The whiskey will now mingle with the lingering beer and the flavors it left behind in the barrel. Is your head exploding yet?\nThe concept of whiskey finished in beer barrels isn’t all that new, but it hasn’t been widely done. Now that there are so many more breweries using barrels, the practice could become a trend. The Jameson Caskmate series includes versions of their Irish whiskey aged in barrels that held stout and IPA (India Pale Ale). Glenfiddich has also used IPA casks while Grant’s chose malty ale casks to finish Scotch whiskies. Salt Lake City’s Sugarhouse Distillery has now experimented with two different local beer barrels to finish bourbon.\nThe Boilermaker Series from Sugarhouse Distillery starts with its house bourbon made from a 75% corn/20% rye/5% malted barley mash bill. The bourbon is matured in a series of casks starting with the smaller quarter size, progressing to a half, then full-sized cask. Smaller barrels “age” the whiskey more quickly as the surface area of the wood is greater in proportion to the volume of the spirit inside. This technique allows the bourbon to gain a fuller flavor in a shorter amount of time. The Sugarhouse bourbon has notes of vanilla, sugar cookie, spice, light caramel, and grain. The finished whiskey spends an additional 8-9 months in a beer barrel to complete the process.\nIn the first Boilermaker offering, Sugarhouse obtained a barrel (of Buffalo Trace origins) from Uinta Brewing that had contained Cockeyed Cooper barley wine. This beer has rich flavors of vanilla, dark chocolate, dried fruit, and oak. Because barley wines use a good amount of malt to achieve the high alcohol content, they also have a lot of hops added to balance the sweetness. I’m told that Cockeyed Cooper actually had hops added to the barrel while it rested there. For this reason, hops are the main addition in both the aroma and flavor of the bourbon. A prominent floral, honeysuckle tone and some hints of the dark fruit malts give the bourbon a delightful complexity.\nThe second in the series utilizes a (Heaven Hill) barrel from Epic Brewing that held the Belgian style Smoked and Oaked. The smoked cherry wood malts add smokiness to sweet caramel and vanilla with traces of wood and dark fruits courtesy of the Belgian yeast rounding out the flavors of this beer. The bourbon benefits from the fruitiness of the Belgian yeast and develops a richer caramel flavor with traces of smoke after sharing a cask with Smoked and Oaked. I also noticed a slight floral aroma and flavor that I attribute to hops. Although whiskey essentially starts out as beer before being distilled, unlike beer there are no hops added. My guess is that any whiskey finished in beer barrels will end up with some suggestion of the hops used in the beer.\nIf you haven’t tried the Boilermaker Series from Sugarhouse Distillery, there are still bottles of the second release available for sale at the distillery. Or ask for it at your local whiskey bar. At Bar X and Beer Bar in downtown Salt Lake City, you can sip both the bourbon and the Smoked and Oaked beer side by side. To really get a sense of what the beer barrel adds to the whiskey, try the Boilermaker next to the original bourbon. The first release has sold out, but if you have a friend who’s a whiskey nerd, you may be able to get them to share with you. Also, The Owl Bar at Sundance still has some left. It’s definitely worth the effort to track it down. A third release in the Boilermaker Series is not currently in the works, but will likely happen sometime in the future. Ultimately, when their production increases, Sugarhouse Distillery would love to have their own barrels go to the brewery and then come back again for the Boilermaker Series.\nWhiskey aged in beer barrels is pretty awesome. I look forward to trying more versions, and from what I’ve tasted so far, it seems to be a success. Sometimes complicating things makes for a better, more interesting finish. When it comes to whiskey, beer, and barrels, this couldn’t be more true.']	['<urn:uuid:6a669194-75d0-42f9-b4a2-1ee0ddea1727>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	10	66	1092
30	function definition triple sets grade 8	A function is defined as an ordered triple set consisting of X (domain), Y (co-domain), and F (set of ordered pairs). In Grade 8, students learn that a function is a rule that assigns exactly one output to each input, where one quantity determines another. They understand functions through different representations like tables and graphs, and learn how these aspects are reflected in different representations.	['Difference Between Relations and Functions\nRelations vs Functions\nIn mathematics, relations and functions include the relation between two objects in a certain order. Both are different. Take, for instance, a function. A function is linked with a single quantity. It is also associated with the argument of the function, input, and value of the function, or otherwise known as the input. To put it in simple terms, a function is associated to one specific output for every input. The value could be real numbers or any elements from a provided set. A good example of a function would be f(x) =4x. A function would link to every number four times every number.\nOn the other hand, relations are a group of ordered pairs of elements. It could be a subset of the Cartesian product. Generally speaking, it is the relation between two sets. It could be coined as a dyadic relation or a two-place relation. Relations are utilized in different areas of mathematics just so model concepts are formed. Without relations, there wouldn’t be “greater than,” “is equal to” or even “divides.” In arithmetic, it can be congruent to geometry or adjacent to a graph theory.\nOn a more determined definition, function would pertain to an ordered triple set consisting of the X,Y,F. “X” would be the domain, “Y” as the co-domain, and the “F” would have to be the set of ordered pairs in both “a” and “b.” Each of the ordered pairs would contain a primary element from the “A” set. The second element would come from the co-domain, and it goes along with the necessary condition. It has to have a condition that each single element found in the domain will be the primary element in one ordered pair.\nIn the set “B” it would pertain to the image of the function. It doesn’t have to be the entire co-domain. It can be clearly known as the range. Do bear in mind that the domain and co-domain are both the set of real numbers. Relation, on the other hand, will be the certain properties of items. In a way, there are things that can be linked in some way so that’s why it’s called “relation.” Clearly, it doesn’t imply that there are no in-betweens. One thing good about it is the binary relation. It has all three sets. It includes the “X,” “Y” and “G.” “X” and “Y” are arbitrary classes, and the “G” would just have to be the subset of the Cartesian product, X * Y. They are also coined as the domain or perhaps the set of departure or even co-domain. “G” would simply be understood as a graph.\n“Function” would be the mathematical condition that links arguments to an appropriate output value. The domain has to be finite so that the function “F” can be defined to their respective function values. Oftentimes, the function could be characterized by a formula or any algorithm. The concept of a function could be stretched out to an item that takes a mixture of two argument values that can come up with a single outcome. All the more, the function should have a domain that results from the Cartesian product of two or more sets. Since the sets in a function are clearly understood, here’s what relations can do over a set. “X” is equal to “Y.” The relation would end over “X.” The Endorelations are through with “X.” The set would be the semi-group with involution. So, in return, the involution would be the mapping of a relation. So it is safe to say that relations would have to be spontaneous, congruent, and transitive making it equivalence relation.\n1. A function is linked to a single quantity. Relations are used to form mathematical concepts.\n2. By definition, a function is an ordered triple sets.\n3. Functions are mathematical conditions that connect arguments to an appropriate level.\nSearch DifferenceBetween.net :\nEmail This Post : If you like this article or our site. Please spread the word. Share it with your friends/family.\nLeave a Response', 'In Grade 8, instructional time should focus on three critical areas: (1) formulating and reasoning about expressions and equations, including modeling an association in bivariate data with a linear equation, and solving linear equations and systems of linear equations; (2) grasping the concept of a function and using functions to describe quantitative relationships; (3) analyzing two- and three-dimensional space and figures using distance, angle, similarity, and congruence, and understanding and applying the Pythagorean Theorem.\n1. Students use linear equations and systems of linear equations to represent, analyze, and solve a variety of problems. Students recognize equations for proportions (y/x = m or y = mx) as special linear equations (y = mx + b), understanding that the constant of proportionality (m) is the slope, and the graphs are lines through the origin. They understand that the slope (m) of a line is a constant rate of change, so that if the input or x-coordinate changes by an amount A, the output or y-coordinate changes by the amount m·A. Students also use a linear equation to describe the association between two quantities in bivariate data (such as arm span vs. height for students in a classroom). At this grade, fitting the model, and assessing its fit to the data are done informally. Interpreting the model in the context of the data requires students to express a relationship between the two quantities in question and to interpret components of the relationship (such as slope and y-intercept) in terms of the situation.\nStudents strategically choose and efficiently implement procedures to solve linear equations in one variable, understanding that when they use the properties of equality and the concept of logical equivalence, they maintain the solutions of the original equation. Students solve systems of two linear equations in two variables and relate the systems to pairs of lines in the plane; these intersect, are parallel, or are the same line. Students use linear equations, systems of linear equations, linear functions, and their understanding of slope of a line to analyze situations and solve problems.\n2. Students grasp the concept of a function as a rule that assigns to each input exactly one output. They understand that functions describe situations where one quantity determines another. They can translate among representations and partial representations of functions (noting that tabular and graphical representations may be partial representations), and they describe how aspects of the function are reflected in the different representations.\n3. Students use ideas about distance and angles, how they behave under translations, rotations, reflections, and dilations, and ideas about congruence and similarity to describe and analyze two-dimensional figures and to solve problems. Students show that the sum of the angles in a triangle is the angle formed by a straight line, and that various configurations of lines give rise to similar triangles because of the angles created when a transversal cuts parallel lines. Students understand the statement of the Pythagorean Theorem and its converse, and can explain why the Pythagorean Theorem holds, for example, by decomposing a square in two different ways. They apply the Pythagorean Theorem to find distances between points on the coordinate plane, to find lengths, and to analyze polygons. Students complete their work on volume by solving problems involving cones, cylinders, and spheres.\n||Common Core State Standard\n|The Number System\n||Know that there are numbers that are not rational, and approximate them by rational numbers.\n||Understand informally that every number has a decimal expansion; rational numbers have decimal expansions that terminate in 0s or eventually repeat, and conversely.\n||Use rational approximations of irrational numbers to compare the size of irrational numbers, locate them approximately on a number line diagram, and estimate the value of expressions (e.g., (pi)^2). For example, by truncating the decimal expansion of sqrt2 (square root of 2), show that sqrt2 is between 1 and 2, then between 1.4 and 1.5, and explain how to continue on to get better approximations.\n|Expressions and Equations\n||Work with radicals and integer exponents.\n||Know and apply the properties of integer exponents to generate equivalent numerical expressions. For example, 3^2 × 3^(-5) = 3^(-3) = 1/(3^3) = 1/27.\n||Use square root and cube root symbols to represent solutions to equations of the form x^2 = p and x^3 = p, where p is a positive rational number. Evaluate square roots of small perfect squares and cube roots of small perfect cubes. Know that sqrt2 is irrational.\n||Use numbers expressed in the form of a single digit times an integer power of 10 to estimate very large or very small quantities, and to express how many times as much one is than the other. For example, estimate the population of the United States as 3 × 10^8 and the population of the world as 7 × 10^9, and determine that the world population is more than 20 times larger.\n||Perform operations with numbers expressed in scientific notation, including problems where both decimal and scientific notation are used. Use scientific notation and choose units of appropriate size for measurements of very large or very small quantities (e.g., use millimeters per year for seafloor spreading). Interpret scientific notation that has been generated by technology.\n|Understand the connections between proportional relationships, lines, and linear equations.\n||Graph proportional relationships, interpreting the unit rate as the slope of the graph. Compare two different proportional relationships represented in different ways. For example, compare a distance-time graph to a distance-time equation to determine which of two moving objects has greater speed.\n||Use similar triangles to explain why the slope m is the same between any two distinct points on a non-vertical line in the coordinate plane; derive the equation y =mx for a line through the origin and the equation y = mx + b for a line intercepting the vertical axis at b.\n|Analyze and solve linear equations and pairs of simultaneous linear equations.\n||Solve linear equations in one variable.\na. Give examples of linear equations in one variable with one solution, infinitely many solutions, or no solutions. Show which of these possibilities is the\ncase by successively transforming the given equation into simpler forms, until an equivalent equation of the form x = a, a = a, or a = b results\n(where a and b are different numbers).\nb. Solve linear equations with rational number coefficients, including equations whose solutions require expanding expressions using the\ndistributive property and collecting like terms\n||Analyze and solve pairs of simultaneous linear equations.\na. Understand that solutions to a system of two linear equations in two variables correspond to points of intersection of their graphs,\nbecause points of intersection satisfy both equations simultaneously.\nb. Solve systems of two linear equations in two variables algebraically, and estimate solutions by graphing the equations. Solve simple\ncases by inspection. For example, 3x + 2y = 5 and 3x + 2y = 6 have no solution because 3x + 2y cannot simultaneously be 5 and 6.\nc. Solve real-world and mathematical problems leading to two linear equations in two variables. For example, given coordinates for two\npairs of points, determine whether the line through the first pair of points intersects the line through the second pair.\n||Define, evaluate, and compare functions.\n||Understand that a function is a rule that assigns to each input exactly one output. The graph of a function is the set of ordered pairs consisting of an input and the corresponding output. (Function notation is not required in Grade 8.)\n||Compare properties of two functions each represented in a different way (algebraically, graphically, numerically in tables, or by verbal descriptions). For example, given a linear function represented by a table of values and a linear function represented by an algebraic expression, determine which function has the greater rate of change.\n||Interpret the equation y = mx + b as defining a linear function, whose graph is a straight line; give examples of functions that are not linear. For example, the function A = s^2 giving the area of a square as a function of its side length is not linear because its graph contains the points (1,1), (2,4) and (3,9), which are not on a straight line.\n|Use functions to model relationships between quantities.\n||Construct a function to model a linear relationship between two quantities. Determine the rate of change and initial value of the function from a description of a relationship or from two (x, y) values, including reading these from a table or from a graph. Interpret the rate of change and initial value of a linear function in terms of the situation it models, and in terms of its graph or a table of values.\n||Describe qualitatively the functional relationship between two quantities by analyzing a graph (e.g., where the function is increasing or decreasing, linear or nonlinear). Sketch a graph that exhibits the qualitative features of a function that has been described verbally.\n||Understand congruence and similarity using physical models, transparencies, or geometry software.\n||Verify experimentally the properties of rotations, reflections, and translations:\na. Lines are taken to lines, and line segments to line segments of the same length.\nb. Angles are taken to angles of the same measure.\nc. Parallel lines are taken to parallel lines.\n||Understand that a two-dimensional figure is congruent to another if the second can be obtained from the first by a sequence of rotations, reflections, and translations; given two congruent figures, describe a sequence that exhibits the congruence between them.\n||Describe the effect of dilations, translations, rotations and reflections on two-dimensional figures using coordinates.\n||Understand that a two-dimensional figure is similar to another if the second can be obtained from the first by a sequence of rotations, reflections, translations, and dilations; given two similar two-dimensional figures, describe a sequence that exhibits the similarity between them.\n||Use informal arguments to establish facts about the angle sum and exterior angle of triangles, about the angles created when parallel lines are cut by a transversal, and the angle-angle criterion for similarity of triangles. For example, arrange three copies of the same triangle so that the three angles appear to form a line, and give an argument in terms of transversals why this is so.\n|Understand and apply the Pythagorean Theorem.\n||Explain a proof of the Pythagorean Theorem and its converse.\n||Apply the Pythagorean Theorem to determine unknown side lengths in right triangles in real-world and mathematical problems in two and three dimensions.\n||Apply the Pythagorean Theorem to find the distance between two points in a coordinate system.\n|Solve real-world and mathematical problems involving volume of cylinders, cones and spheres.\n||Know the formulas for the volume of cones, cylinders and spheres and use them to solve real-world and mathematical problems.\n|Statistics and Probability\n||Investigate patterns of association in bivariate data.\n||Construct and interpret scatter plots for bivariate measurement data to investigate patterns of association between two quantities. Describe patterns such as clustering, outliers, positive or negative association, linear association, and nonlinear association.\n||Know that straight lines are widely used to model relationships between two quantitative variables. For scatter plots that suggest a linear association, informally fit a straight line, and informally assess the model fit by judging the closeness of the data points to the line.\n||Use the equation of a linear model to solve problems in the context of bivariate measurement data, interpreting the slope and intercept. For example, in a linear model for a biology experiment, interpret a slope of 1.5 cm/hr as meaning that an additional hour of sunlight each day is associated with an additional 1.5 cm in mature plant height.\n||Understand that patterns of association can also be seen in bivariate categorical data by displaying frequencies and relative frequencies in a two-way table. Construct and interpret a two-way table summarizing data on two categorical variables collected from the same subjects. Use relative frequencies calculated for rows or columns to describe possible association between the two variables. For example, collect data from students in your class on whether or not they have a curfew on school nights and whether or not they have assigned chores at home. Is there evidence that those who have a curfew also tend to have chores?']	['<urn:uuid:b1b398e5-0ac8-415c-9b8a-a5d499925da6>', '<urn:uuid:a75c4a56-cce3-47d5-9993-395d3483a5e1>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:32:05.666290	6	65	2695
31	How do traditional hands-on therapy methods compare to newer high-tech approaches in treating stroke patients?	While there are exciting technological advances in stroke therapy, experts emphasize that simple hands-on, traditional approaches that mimic real-life activities are often the most effective. High-tech tools can help advance rehabilitation, but everyday functional activities like making lunch or brushing hair can be equally or more beneficial for recovery.	"[""Passive range of motion (pROM) helps prevent joint contractures and maintain joint flexibility for the paralyzed limb. It is done by passively moving a limb through its available range of motion. The stroke survivor can perform the exercises alone or have a caregiver help. Arm exercises can often be done by the patient without assistance, however leg passive range of motion exercises are best performed by a caregiver. Exercises can be performed for 5-10 repetitions each preferably several times a day.\nThe following passive range of motion exercises for the arms are ones that an individual can do for him/herself without a caregiver.\nClasp Hands - Lying down, clasp your hands together to prepare for doing exercises.\nShoulder Flexion Forward - Raise hands straight up toward ceiling using your strong arm to help lift the weak or paralyzed arm. Bring hands back down to chest.\nStraight Arm Punches - Keeping arms straight, try to lift shoulder blades off of the bed or mat. Do not lift the head.\nElbow Flexion and Extension - Bring hands to forehead, bending elbows, then straighten arms bringing hands back up toward ceiling.\nShoulder Flexion Overhead - Keeping elbows straight raise arms overhead as far as it is comfortable. If your shoulder is subluxed (dropping down from the socket), then only bring arms straight up and not overhead (arms at 90 degree angle with body). Check with your occupational or physical therapist to see how to perform safe range of motion if your shoulder is subluxed. Also, if you feel pain, do not raise arm any higher. Stop at the point where you start to feel any discomfort in the shoulder. Perform in a pain-free range only.\nShoulder External Rotation - If you are able to bring arms overhead comfortably and your physical or occupational therapist gives you the okay, try to place your hands behind your head and relax your elbows to the side(keeping hands clasped). This exercise is difficult for many stroke patients. Do not attempt if you have pain or have been advised not to do this exercise by your therapist.\nWrist Flexion and Extension - Holding your arms straight up, bend your wrist slowly side to side. This exercise may work better in a seated position with the hands in front of you instead of having the arms raised.\nWrist Supination and Pronation - Turn weak hand palm up and then palm down.\nFinger Flexion and Extension - Bend fingers of weak hand into palm then straighten. If hands are already fisted then only work on straightening fingers.\nLeg passive range of motion exercises are more difficult for stroke patients to perform on their own and will not be demonstrated on this page. However, if you are interested in knowing hot to do self range of motion for the legs, you can visit: https://patienteducation.osumc.edu/Documents/leg-self.pdf\nThe video below shows how each joint is moved through passive range of motion by a helper. The subject in the video does not have a stroke and has full range of motion. When performing passive range of motion on a stroke patient with limited range of motion, the exercises would only be done in a pain-free range, performed more slowly, and the helper should be aware of the patient's available movement or range of motion. Never force movement if you feel resistance or the patient expresses pain. A licensed occupational or physical therapist can teach caregivers how to do passive range of motion on a stroke patient.\nDec 09, 18 09:23 AM\nQuestion: My husband had several mini strokes only problem is he has short term memory loss but he feels he doesn’t have to help do anything around the\nOct 24, 18 07:35 AM\nRead stories and get inspiration from stroke survivors.\nJun 18, 18 01:07 PM\nQuestion: I recently suffered a stroke on left side of brain, so it has effected my right leg. I have numbness in my right leg and foot. I had a car\nShare your stroke\ntreatment or exercise\nShare your stroke survival story - CLICK HERE!"", ""Genius for strokes\nBundled payments and dual-eligibles bring opportunities to long-term care operators, experts say\nIt is a long-term care resident stream that does not figure to be soon diminished.\nDetermining the best therapy approach isn't easy because each stroke incident, degree of impairment, rate of recovery and overall outcome is as unique as the residents themselves. Treatment approaches, too, are anything but one-size-fits-all. The good news is an ever-growing array of stroke-recovery therapies and supplemental — and, in some cases, experimental — practices are allowing therapists to add some new and promising treatment approaches to their arsenal.\n“We know that the brain has a remarkable ability to regain lost function, and we also know that the route each person can take toward recovery can be very different. What works for one person might not provide the same result in another, and the goals and priorities will also differ from one person to the next, which is why therapy must be tailored to each individual,” says Jan Davis, MS, OTR/L, president and founder of International Clinical Educators Inc. She's also the spokeswoman for the American Occupational Therapy Association on issues related to stroke rehabilitation.\nOne thing that does apply to virtually every stroke recovery patient is that the sooner treatment begins, the better. Not only does early and aggressive intervention give the greatest opportunity for maximum functional recovery, experts agree that it also helps reduce the risk for other potentially debilitating and even life-threatening conditions.\n“Patients who have suffered a stroke are usually at increased risk for development of multiple comorbidities,” notes Mary Van de Kamp, senior vice president of clinical operations for RehabCare Inc. Those conditions include pressure ulcers, dehydration, malnutrition, deep vein thrombosis, seizures and falls. Muscle atrophy and depression can compound matters further, she notes.\n“With medical intervention advancements, we as rehabilitation professionals are now able to intervene with patients much [earlier] in the recovery process,” Van de Kamp adds. She notes that patients who just five to 10 years ago might have languished in a hospital bed for eight to 10 days before receiving any meaningful rehabilitation are now coming to the skilled nursing facility around five to seven days post-stroke.\nNo pause for recovery\nAttitudes and perceptions surrounding stroke recovery also have evolved, which has further contributed to faster, more targeted therapy for even the most compromised patients.\n“Stroke patients are now treated more aggressively rather than with kid gloves. They are pushed hard to recover function sooner,” says Dede Tsuruoka, OT, vice president of clinical services for Hallmark Rehabilitation LLC.\nExperts largely agree that stroke recovery often hinges on the marriage of nursing services and physical, occupational and speech-language therapies. Ideally, the practitioners will work closely together (and alongside social workers and recreational therapists, as appropriate) to tailor a rehab program around the patient's personal interests, hobbies, daily routines, occupations and individual recovery goals.\nWhile spontaneous recovery occurs quickly and statistics show that the window of opportunity for best post-stroke outcomes is within the first six months, it's common for stroke patients to experience alternating bouts of progress and plateaus, and a recovery process that can take years.\nIn light of the often-lengthy recovery — and therapy caps that limit treatments for some — therapists must work quickly and methodically to coordinate care and make the most of the therapy provided during a skilled stay.\nMore than ever, therapy is based on the principle of neuroplasticity, the brain's ability to adapt and relearn certain essential functions after stroke, often through the use of repetition and other techniques that engage and strengthen areas of the body and brain most affected by the stroke.\n“It's up to us as clinicians in skilled nursing facilities to provide our patients with the most effective treatments in the short amount of time we have with them to achieve the best results,” says Megan Harper, PT, outpatient rehabilitation manager for Life Care Center of Littleton, a skilled nursing facility in Littleton, CO.\nWith such limitations in mind, experts stress that functional, relevant therapy that meets the fundamental needs of each individual — such as basic communication, mobility and the ability to perform some activities of daily living — is critical. For the many stroke recovery patients with speech or swallowing impairment, for example, a simple communication system that helps patients communicate hunger, thirst, pain or comfort will go a long way toward motivating and engaging them, and advancing their recovery, according to Martha Schram, president of Aegis Therapies.\n“I can't overemphasize the need to keep therapy functional in accordance with the communication lifestyle the person is used to,” she says, noting that when patients can communicate their basic needs, their confidence and self-esteem improve, which helps motivate them to recapture an active lifestyle.\nThe way patients are communicating and engaging in therapy sessions has indeed evolved in recent years.\nIncreasingly, traditional neuromuscular re-education and neurodevelopmental techniques, such as the Bobath method and proprioceptive neuromuscular facilitation (PNF), are being blended with supplemental (and, in some cases, high-tech) treatment approaches to further advance therapy progress.\nOn the speech language pathology side, for example, newly emerging therapies, such as the Madison Oral Strengthening Therapeutic (MOST) device, an assistive intra-oral tool that provides resistance for tongue exercises that aid communication and swallowing, are showing progress.\nTherapy approaches that tap into the brain's internal timing also are catching on in stroke therapy. At Cascade Park Care Center in Vancouver, WA, therapists are employing a computer-based system, called the Interactive Metronome, which uses an auditory guidance system that guides patients through a series of coordinated, repetitious and task-oriented movements, progressively challenging them to improve their timing by matching the computer's rhythm.\nThe community also uses a complementary intervention called Melodic Intonation Therapy, a type of “singing therapy” that uses melodic and rhythmic components to tap into undamaged areas of the brain and increase fluent production of speech.\n“With these interventions, our stroke patients have made remarkable progress,” says Cascade speech language pathologist Cassandra Kimble, MS, CCC-SLP. One patient, she says, had such slurred speech at the beginning of treatment that caregivers were unable to understand her basic wants and needs. “Within a few months, she was able to speak with family members over the phone and computer.”\nBased on the assumption that the brain processes information better with a combination of sight, sound and touch, some therapists are also incorporating virtual reality technology, using sign language and VR to generate a synthesized voice in speech therapy sessions, and VR and repetitive movement to increase eye-hand coordination in OT/PT sessions.\n“The idea is to create brain action whether the body moves or not. In this way, we begin to trick the brain into creating pathways to create movement,” says Kathleen Weissberg, MS, ORT/L, education director for Select Rehabilitation.\nOT/PT therapy is increasingly incorporating functional electrical stimulation devices that mimic the signal that nerves usually send to arm and leg muscles, and work to facilitate movement in the hand and normalize gait patterns.\n“New waveforms allow improved retraining of the brain, thus prompting the development of new motor pathways,” says Pat Hoskin, PT, TOPAZ director, Hallmark Rehabilitation.\nAnd the benefits of the technology don't end there.\n“We have taken advantage of electrical stimulation that can stimulate the muscles that support swallowing functions so often compromised in patients who have suffered a stroke. These methods of tricking the muscles into believing they are receiving neural signals are examples of the technologies that supplement the hands-on approaches of therapists at the bedside,” says Van de Kamp.\nStill, electrical stimulation therapy isn't for everyone, including those with pacemakers.\nEarly ambulation is another big step forward in stroke rehab, thanks to the availability of body-weight supporting treadmill and track systems.\n“The move is away from passive care to more active and challenging therapeutic approaches,” adds Life Care Center's Harper.\nLow-tech, high impact\nWhile technology and supplemental rehabilitation tools have helped advance the field of stroke therapy and rehabilitation research, experts caution that high-tech approaches don't always translate into better outcomes.\nSensory-rich therapy environments filled with “normalized” experiences that work to rebuild motor skills through familiar actions and applications often deliver the best results, they say.\n“There are many tools a therapist has at their disposal to facilitate muscle and nerve development,” notes Sharlaina Kramer, PT, Cascade Park Care Center. “Part of the challenge is finding activities that engage the patients' interests.”\nSome of the most effective, engaging approaches often don't appear scientific or advanced. Therefore, they might have their value questioned by stroke-recovery patients and their families. But their benefits are very real, various experts assure.\n“I've been in this profession for around three decades and I can tell you that some of the least scientific-seeming therapy approaches that engage the patient, that are meaningful for them and are taken from real-life are what work best,” Kramer says. “You don't need a robot or high-tech piece of equipment to help people regain function.”\nConstraint-induced movement therapy (CIMT), which essentially works by restraining the functional arm to force use on the impaired side, is one relatively new and promising treatment approach in the area of stroke rehabilitation. The downside: CIMT requires patients to have some arm movement and some ability to move the hand, fingers and wrist, which means only a small percentage of stroke patients are good candidates.\nThat's not to say modified CIMT principles can't be applied on a broader scale, however. In speech therapy, for example, constraint refers to the avoidance of compensatory strategies, such as gesturing, drawing or writing, Weissberg explains, and “forced use” means communicating only by talking.\nSimply working impaired limbs into therapy sessions and other activities also has been shown to speed recovery.\nDavis never misses an opportunity to incorporate a weak (or even paralyzed) arm into therapy, for example — even if it just means placing an arm on the table instead of letting it drop to the side. She's also a staunch supporter of “guiding,” an easy-to-apply practice that has the therapist's or caregiver's hand guiding the patient's hand through various tasks — even before movement returns.\n“This isn't a form of exercise,” she explains. “You place your hand over the patient's and then use your hand to power the movement in a comfortable, normal way.”\nShe notes that when the patient's hand is moved, such as with the action of hair brushing, the brain and neural impulses begin to kick into gear. Over time, and through repetition, notable progress is often made.\n“Making a lunch, cutting up fruit or even brushing hair might not seem like big progress, but those everyday [tasks] are very important to a person who has had a stroke,” Davis reasons, adding that those seemingly small steps then pave the way for further progress and recovery.\n“In therapy, people often want to latch on to that next big thing because they want that one answer that will give them better and faster results. There are some exciting advancements, but we need to remember that some of the least high-tech therapy approaches that mimic what patients do in real life are still the most effective.”""]"	['<urn:uuid:e715fd3d-177e-4900-9fe3-cc798e880b28>', '<urn:uuid:1c02a112-7c0c-4b8d-8614-bea3476759e0>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T10:32:05.666290	15	49	2532
32	I'm planning a trip to Tibet and I'm really interested in Buddhist monasteries. Can you tell me about the Tashilhunpo Monastery and what makes it special?	The Tashilhunpo Monastery is a vast complex located in Shigatse that was founded in 1447 by Gedun Drup, who was later recognized as the first Dalai Lama. The monastery features its own streets, housing sectors, plazas, and temples. One of its most impressive attractions is a giant 26-meter tall statue of Maitreya (Future Buddha), built in 1914 by the 9th Panchen Lama. This statue took 4 years to complete and contains 275 kg of solid gold, along with precious materials like pearls, turquoises, corals, and ambers. The monastery also includes the Ngagpa College, which served as the residence of the Panchen lamas.	"['Zhangmu, Khasa in Nepalese language is a border town, a small settlement situated into 10 km inside from the Friendship Bridge across the Bhotekosi river and a very important trade center between China and Nepal after closure of borders between India and China from Gangtok. Nowadays Khasa is one of the most important entry point for the tourists to visit Tibet (Mt. Everest, Mt. Kailash / Mansaravar Lake, Lhasa etc.). So it has a bank, a post office, a government store, and many more.\nThe Climate is quite different from the surrounding area. The giant hills around Zhangmu are heavily wooded with frozen ""icicles"" during the winter and beautiful waterfall in summer.\nNyalam a small town but used to be an important trade post placed into a fertile valley is situated in an altitude of 12,200 feet but the vegetation is sparse. About an hour drive (28 km) uphill from the border town Khasa you\'ll reach Nyalam. Few basic lodges and flat roofed mud-brick houses introduces you the sign of touch of Tibetan plateau.\nA beautiful small town situated at the altitude of 13,800 feet, Shegar is the place from where the Himalayan range to the South including Mt. Everest can be seen very close in the clear day. This is also the basement place forexpeditions to Mt. Everest and other peaks around. Another place of interest in Shegar is a new Chinese commune with the population of about 3000 just at the 7 kilometers from the main road.\nAnother beautiful Place, cultivation with fields of barley and mustard especially in the summer, Lhatze is in an altitude of 13100 feet. It is the place of more prosperous villages with well ordered houses of good appearance. A much more Chinese presence & feel, both in terms of people and signs, which are almost entirely in Chinese script. The town is more important due to the junction to turn Kathmandu to the South and to the western Tibet towards Mt. Kailash and Mansarovar Lake. Another attraction of Lhatze is a hot spring located in few kilometers from the town for the explorers.\nSakya monastery, the center of the Sakyapa Sect in Tibetan Buddhism is located in Sakya Country, about 30 km of the Xegar - Shigatse main road. The monastery stands in two parts (Northern and Southern) on the side of Dongchu River. The northern part of the monastery was built in 1079 and the southern founded in 1268 by Pagpa, a ruler of whole Tibet under the Yuan Dynasty\'s supervision. The monastery has great influence over the Tibetan history and cultural development. So the State Council of China has classified it as the national level protected monuments.\nShigatse, the second biggest city in Tibet is situated at the altitude of 12,600 ft. It is the famous city for its great Tashilhunpo Monastery - the seat of the Panchen Lama, who is regarded as the reincarnation of the Buddha of Endless Enlightenment. Items of interest inside this monastery built in 1447 by the First Dalai Lama are: the relics of Sakyamuni, the Hall of Maitreya, and a incredible collection of thanks, frescoes and statues. There is also a dynamic \'free\' market where one can buy local handicraft embedded with coral and turquoise, Tibetan Daggers, Chinese pottery and yak butter. Since it is the second biggest city of Tibet, it is the center for transportation and distribution of agriculture and other products.\nThe Tashilhunpo Monastery\nTashilhumpo is a vast monastery with its own streets, housing sectors, plazas, back alleys and complex of temples and halls. It is located in the town of Sigatse and was founded by Gedun Drup, a disciple of Tsongkapa, the founder of the Gelungpa Sect in Tibetan Buddhism. Gedun Drup was later recognized as the first Dalai Lama. The monastery was built in 1447 and continuously expanded by the subsequent Panchen Lamas. The Ngagpa College (Tantric College), one of its four monastic colleges, was the residence of the Panchen lamas. One of the most attraction of this monastery is the giant Maitreya (Future Buddha) erected by the 9th Panchen Lama in 1914 which took 4 years to complete. This twenty six meters tall statue is very big where lots of precious things like pearls, turquoises, corals and ambers were used with its 275 Kg. of solid gold.\nA 100 yeard old Shalu monastery is in the Salu village near Shigatse. The monastery is built by the Chinese Yuan Dynasty. The main feature of the monastery is the combination Tibetan and Chinese art and architecture. The monastery has lots of religious objects like books related to Buddhism etc.\nGyantse, a small agricultural town famous for its wool carpets and the Palkhor Choide Chorten is situated in an altitude of 13000 feet. The main attractions of this place is 13 circiled 5 storeyed unique structure built in 1414 and the fort. The historic town of Gyantse is on the way to Lhasa that need to cross three mountain passes: Simala-4380m, Karola-5045m, Ghampala-4794 m and the turquoise Yamdrok Lake.\nPalcho Monastery and Kumbum Stupa\nAn unique and typical combination of three different sects of Tibetan Buddhism (the Gelungpa, the Sakyapa and Bhuton Sect), you can see in Palcho monastery in Gyantse. The popular pagoda style Kumbum has nine storyed 108 doors and 77 chapels that contains clay sculptures and thousands of wall paintings.\nYamdrok Lake and Nagarje\nOne of the threebiggest lakes in Tibet, Yamdrok Lake is in between of the capital city Lhasa and the historic city Gyantse. This turquoise green and freshwater lake is a countless depth and is spread in 624 square kilometers making about ten independent hilly islands inside. It is the home of fish and migratory birds. Nagarje is situated at the coast of this huge lake.\nLhasa and its Surrounding\nLhasa, culturally rich city with its 1300 years history is the capital of immediate Tibetan kingdom. Having the magnificent Potala Palace, former palaces of the Dalai Lamas Lhasa is more popular in the world. The palace was built in 1645 at the top of a hill. It has 1,000 rooms, 10,000 shrines and 200,000 religious statues.\nThe Jhokhang temple\n(Built in 7th century) is another holiest Buddhist shrine in Tibet, the quaint Barkhor market, Norbulingka (three palaces once used by the Dalai Lamas as a summer retreat), that surrounds it. The Jhokhang, about 700 years old giant Drepung monastery and the Sera (another important center of Buddhist learning) are the main focuses of this historic city. Besides these heritage sites you can see modern city life in its concrete buildings, fancy department stores, night clubs and wide roads.\nThe historic city Tsedang represents the Tibetan civilization. Tsedang is situated in 195 km southeast of Lhasa which offers historic tours as it is known as the valley of the kings (the capital of the Yarlung kings who established the Tibetan kingdom). The main attraction of this place are the Yumbu Lagang Palace (built by the first Yarlung king) and the Tandruk Monastery (one of the three royal Buddhist temples).\nMt. Kailash and Lake Manasarovar\nMt. Kailash is striking in the way it rises high above the surrounding range and always remains snow-capped, thus its name in Tibetan ""Gang Rinpoche"", which means ""supreme mountain"". Traditional Buddhist cosmology has often connected Kailash with Mt. Meru, the great mythological mountain that forms the axis of this world system. The power of this strange, domed peak has gripped the imagination of the people of Nepal, India and Tibet since thousands of years and more peoples from all over the world are willing to see it in reality. Mansarovar, situated in West Tibet and in distance of 55 kilometers to Mt. Kailash. The Lake is biggest in Tibet and is considered as holy as Mt. Kailash. The Hindu religious people believes Mansarovar as a swimming pool of Lord Shiva.\nIn the history of Tibetan Monasteries Samye is the first built monastery founded by the Tibetan King Trisiong Detsen and belongs to the Nyingmapa and Sakyapa sects. The monastery is located beside the Yarlung Tsangpo (Brahmaputra river) in the middle of the sandy Samye Valley. It is said that the monastery was destroyed by fire and was rebuilt for three hundred years as the residence of the 6th Dalai Lama and was completed by Rating Rimpoche. It is the only monastery that represents Nyingmapa sect (red hat) in Tibetan Buddhism among the visits of monasteries in and around Lhasa. One can start trekking from Ganden to Samye or take a drive from Lhasa to visit the monstery crossing over the Yarlung Tsangpo by a ferry.\nMt. Everest (Qomolangma)\nQomolangma means "" Goddess"" in Tibetan and is the highest mountain on earth with an altitude of 8848m. Mount Qomolangma, known to the western world as Mt. Everest, stands at the south of Tingri in the southern Tibet, on the border land of the central Himalayas, between China and Nepal, capped with accumulated eternal snow. The optimum weather on Mount Qomolangma is from April to June, a golden period for mountaineers. Each year a great number of brave robust mountaineers come from all over the world to climb Mount Qomolangma, hoping to fulfil a life-long wish by climbing and looking out the highest peak on earth.']"	['<urn:uuid:a052dc14-a55c-471d-9ce0-675fcef09b8c>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T10:32:05.666290	26	102	1542
33	need help tracking suspicious activity on my cloud system what should i monitor	A proper security system should monitor for concurrent log-ins, suspicious activity, and repeated failed log-in attempts. This includes detecting suspicious behavior like logging in at unusual hours and downloading questionable items and data. Additionally, it's important to monitor and record user permissions and file activities to check for unauthorized access, changes, downloads, and uploads. Data activity monitoring systems should also record all users who have viewed a file.	['A Cloud Computing Service System (PaaS) makes it possible for clients to construct, safeguard, run, and handle internet applications. It allows groups to create and deploy apps without acquiring or managing the IT facilities that support them.\nOn the whole, the platform supports the complete software application advancement and use life cycle while simultaneously providing developers and individuals with Net gain access to. PaaS benefits include simplicity of use, expense savings, flexibility, and also scalability.\nHow to Safeguard System as a Service (PaaS) Settings\nA PaaS is regularly not protected the same way an on-premises data facility is.\nProtection is included in PaaS environments. PaaS clients protect their system accounts, applications, and also information. In an excellent globe, premise safety and security transfers to identity boundary safety.\nSo the PaaS customer must focus on identification as the primary safety and security boundary. Verification, operations, tracking, and logging will undoubtedly be necessary to safeguarding code, data, and configurations.\nDefend applications against unidentified as well as frequent hazards\nUndoubtedly, the most effective strategy is to utilize a real-time computerized safety system to find and stop an attack immediately. Furthermore, PaaS individuals may use the system’s protection functions or third-party remedies.\nUnauthorized gain access to attacks or breaches should be detected and protected against right away.\nGood Read: Learn more about hygine and mental health.\nYou ought to be able to detect aggressive customers, weird log-ins, destructive robots, as well as take-overs, to name a few anomalies. Together with technology, the application should have safety.\nSafeguard customer and application resources\nEveryone gets in touch with is a feasible attack surface. The best method to prevent strikes is to restrict or restrict untrustworthy individuals’ access to susceptibilities and sources. To minimize vulnerabilities, protection systems should be automatically patched and also updated.\nEven if the service provider safeguards the platform, the customer is ultimately responsible for safety and security. Integrated system safety and security features, attachments, third-party services, and safety and security approach significantly enhance account, application, and information protection. It likewise assures that just licensed individuals or employees might access the system.\nAnother approach is to restrict administrative accessibility while creating an audit system to spot possibly dangerous interior groups and exterior customer actions.\nAdministrators should additionally limit customers’ authorizations as high as possible. To guarantee that programs or other actions are effectively carried out, customers need to have as marginal approvals as possible. The strike surface area is reducing, and also blessed sources are being exposed.\nApp to look for safety vulnerabilities\nEvaluate safety and security threats and also susceptibilities in applications as well as their libraries. Make use of the results to enhance total part security. For example, daily scanning would be scheduled immediately in a suitable scenario based on the app’s sensitivity and possible safety risks. Include an option that can incorporate right into other devices, such as interactive software applications, or inform the appropriate people when a protection risk or attack is recognized.\nAnalyze and deal with addiction-related safety and security problems\nApplications usually count on both direct and indirect open resource requirements. If these weak points are not dealt with, the application may come to be insecure.\nExamining APIs and validating third-party networks needs assessing the program’s interior as well as external elements. Patching, upgrading, or changing a secure variation of the dependence are all practical mitigating approaches.\nPentesting and also hazard modelling\nPenetration testing helps identify and solve safety and security troubles before aggressors discover as well as manipulate them. Nevertheless, penetration testing is aggressive and also might seem like DDoS assaults. To stop false alarms, safety workers must interact.\nDanger modelling entails simulating attacks from reliable borders. These assists identify weak design points that aggressors may exploit. Consequently, IT groups may enhance protection and produce treatments for any recognized weak points or threats.\nTrack customer as well as file gain access to\nManaging fortunate accounts enables protection groups to see just how users engage with the system. On top of that, it allows security teams to assess if they choose individual actions to pose a risk to security or compliance.\nDisplay and document customer approvals and data activities. This checks for unapproved access, changes, downloads, and uploads. Data activity checking systems must, in addition, record all individuals that have viewed a file.\nAn appropriate service needs to discover completing the log-ins, suspicious activity and duplicated not successful log-in attempts. For example, visiting at uncomfortable hrs, downloading dubious products and also information, etc.\nLimited information access\nEncrypting information during transport as well as storage is the best technique. On top of that, human assaults are protected by safeguarding Internet interaction links.\nIf not, establish HTTPS to use the TLS certification to encrypt and shield the channel and, therefore, the information.\nGood Read: Tips to receive best Search Engine Optimization Mississauga\nValidate the information regularly.\nThis guarantees the input information is risk-free and also in the correct style.\nWhether it originates from interior users or external protection groups, all data should be considered risky. If done correctly, client-side recognition and safety mechanisms must stop compromised or virus-infected files from being submitted.\nAnalyze the susceptibility code during development. Till the protected code is verified, programmers should not release the program right into manufacturing.\nMulti-factor verification guarantees only accredited individuals may access apps, data, and systems. As an example, a password, OTP, TEXT, or mobile application may be utilized.\nApply password security\nMost people select weak passwords that are conveniently recalled as well as never updating them. Therefore, administrators may minimize this safety threat by utilizing robust password plans.\nThis demands the use of solid passwords that end. Preferably, encrypted verification tokens, credentials, and passwords are conserved and transferred as opposed to plain text qualifications.\nAuthentication and also permission\nVerification, as well as permission approaches and protocols like OAuth2 and Kerberos, are suitable. Nonetheless, while unique authentication codes are unlikely to reveal systems to assailants, they are not error-free.\nGood Read: Learn more about kid-safe moving methods.\nAvoid using foreseeable cryptographic keys. Instead, use safe and secure necessary distribution techniques, revolve secrets regularly, restore keys on time, and stay clear of hardcoding keys into applications.']	['<urn:uuid:bcf7cb76-0caa-481e-88bc-1264524ed7ca>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	13	68	1024
34	What's the best way to add nutrients to crops, and what environmental damage can they cause?	The best approach is to use highly soluble nitrogen and potassium fertilizers through drip irrigation systems, with starter fertilizer applied before planting. However, these nutrients can cause serious environmental damage when excess amounts leach into water bodies - they increase microorganism growth which depletes oxygen in water, threatening fish and plant life, and can lead to the death of aquatic species that require unpolluted water.	"['Fertigation of Vegetable Crops\nPosted: July 5, 2012\nBill Lamont (email@example.com), Penn State Extension Vegetable Specialist\nSoluble fertilizers can be added to the drip irrigation water to provide uniform crop fertilization. A simple ""hozon brass siphon mixer"" venturi injector draws soluble fertilizer from a bucket or jug into the line at a preset ratio (usually 1:16 or 1 gallon for every 16 gallons of water flowing through the line).\nHowever, the hozon injection system, is suitable only for one-third to one-half acre plantings or less. Other venturi units are available in sizes up to two inches in diameter. More expensive injectors with greater capacity and accuracy use an electric or hydraulic ""pump"" to inject fertilizer solutions from a stock tank into the line. A hydraulic device, called a Dosatron, placed in the mainline can be set at various dilution rates and operates with water flowing directly through it. Use only high quality, soluble fertilizers that completely dissolve. All fertilizer injections should be made ahead of either the main filters on the line or the secondary filters if placed closer to the field, so that any contaminants are filtered out.\nFertigation is used most commonly to supply nitrogen and potassium, because they are highly soluble and move easily through soils to roots. Phosphate and micronutrients are best applied prior to planting and not injected through the irrigation system. Other chemigation applications may include pest control measures, but check label restrictions on use in chemigation applications. If any fertilizer or chemicals are applied through the system, a check valve or proper back-flow prevention devices are required to ensure that no contamination of the water source is possible.\nTake a soil test to know what level of fertility is in your soil. Use a starter fertilizer, a small amount of fertilizer, either liquid or dry, that is applied in the bed in drip irrigated crops. This fertilizer would contain all of the phosphorus (P) and micronutrients and up to 20-30 percent of the nitrogen (N) and potassium (K). On soils testing very low in P and K, the starter can be broadcast or banded in the bed. If only small amounts of P and micronutrients are required, then it would probably be better to band these materials two to four inches below the bed surface and to the side of the plant row, but not between the drip tube and the row. In most cropping situations, approximately 20 to 30 pounds per acre of N and K would be sufficient in the starter fertilizer mixture. In situations where the soil test index for P is high or very high, then no P would be added to the soil.\nNote on phosphorous and micronutrients. In general, simultaneous application of P and micronutrients is not recommended in drip irrigation systems. This is because of precipitation events that can happen between the fertilizers or between the P and the calcium or magnesium in the well water. If application of P is required during the season (such as during cold periods), it should be injected as phosphoric acid alone, in separate applications. Acidification of the irrigation water to pH 4.0 to 5.0 might be needed to keep the P in solution during this fertilizer application. Acidification can be achieved by using phosphoric, sulfuric, hydrochloric, or other acids to reduce the pH of the water. Concentrated acids always must be added to water, never the reverse.\nSimilar problems also occur for micronutrient injection. The key is to avoid precipitation. If micronutrients must be injected, then soluble forms, less subject to precipitation, such as chelates, should be used. Like P, micronutrients should be injected alone.\nRates. In most situations, injected fertilizers will consist only of N and K. The amount of N to use is determined basically by the N requirement of the particular crop. This amount of N is recommended for each crop for each season. The current recommendations for open-soil culture can serve as ""starting points"" for developing local plasticulture recommendations.\nThe amount of K to be injected is based on the soil-test predicted requirement of K for the crop minus the portion of this requirement that is applied in the bed as a starter. For example, if the soil tested medium in K, perhaps only 100 pounds per acre of fertilizer would be required for the season. If 20 percent of this K, i.e., 20 pounds, were applied in the bed as starter fertilizer, then 80 pounds would be injected through the season.\nSources. Several sources of N and K can be used for drip irrigation injection, but all sources must be highly water soluble to be effective. Nitrogen sources include ammonium nitrate, calcium nitrate, various N solutions, and urea. Potassium can come from potassium nitrate, potassium chloride, or potassium sulfate.\nFrequencies. It is most convenient to think of rate of injection in terms of pounds of a particular nutrient per acre per day or week. For example, the recommended schedule of N injection for a particular crop might be to start out early in the season with l pound N per acre per day and finally inject 2.5 pounds per acre per day when the crop is at its peak growth rate. The general rule is that the amount of N and K injected per day or week starts out low and peaks with the crop demand for the nutrients. It is tied to the stage of crop growth or development.\nNutrients can be injected into the system in various frequencies. Basically, the frequency of injection, whether once a day or once every two days or even once a week, depends on system design constraints, on soil type, and on grower preference. Research has shown that the frequency, even up to once per week, is not as important as achieving a correct rate of application of nutrients to the crop during a specified period of time. With computer control of drip irrigation systems, some growers find it easy to inject more frequently, such as once every day. This may have a slight advantage logistically. For example, injecting fertilizer on a more frequent basis would reduce the chances that nutrients were leached from the beds during a heavy rain storm or excessive irrigation compared to injecting larger amounts on a less frequent basis. If the chances for leaching losses are extremely low for any particular field, then injection once per week would be satisfactory. In any case, it is extremely important that the nutrients applied in any irrigation event are not subject to leaching either during that same irrigation event or by subsequent irrigation events. This is why knowledge of the crop root zone is important for optimum fertilizer management. It is critical to monitor the application of water and to realize that fertilizer application is linked closely to water application. To be a good fertigator, a grower first needs to be a good irrigator.\nWhen injecting fertilizer in noncontinuous (bulk) fashion, such as once per day or once per week, it is important to keep in mind a few pointers about the operational sequences for the injection events. The drip irrigation systems always should be brought up to operating pressure prior to injecting any fertilizer or chemical. After the system has been pressurized fully, the fertilizer can be injected. Following the completion of the fertilizer injection, the drip irrigation system should be operated for a period of time to ensure flushing of the nutrients out of the tubes and into the soil. This period might be the next irrigation cycle of the day, if that water will not contain fertilizer. With these operation constraints in mind, it becomes very important to design the drip irrigation system so that fertilizer injection can be achieved in a reasonable amount of time without running the risk of overwatering the crop to get the fertilizer applied. This means that injection pumps, pipe sizes, and injection rates must be adjusted properly to apply the nutrients in the desired amount of time, so that the system can still be flushed without applying excess water during the injection and subsequent cycles.\nIn some systems, fertilizer is injected continuously (concentration injection) so that all irrigation water applied contains nutrients. This system is acceptable as long as no irrigation cycle is excessive, causing nutrients to be leached below the root zone.\nIt should be apparent from the above discussions that water application and fertilizer application are linked inextricably.', 'Nitrates are found in fertilisers, manures, soils, meat preservation, and some industrial operations. The allowable nitrate concentration in water causes nitrate contamination. Nitrate water pollution occurs when excessive amounts of nitrates are observed in various waterbodies.\nNitrates have long-lasting harmful consequences on the environment. With global nitrate levels rising, it is critical to take all essential precautions and measures to avert further harm. Water is crucial to life on Earth but without it, life will cease to exist.\nWater Nitrate Contamination\nExcess nitrate contamination degrades water quality. It is present in various water sources including groundwater, bigger aquifers, wells, and others.\nFlooding and leaching of agricultural compost animal manures & nitrogen-rich fertilisers into groundwater occurs when heavy rains cause irrigation and leaching. Precipitation washes away fertilisers from adjacent agricultural fields and industrial industries, causing nitrates to accumulate in larger bodies of water like aquifers.\nContamination of bodies of water is influenced by depth and kind of water body, soil type, land use, and age of groundwater. It is critical to comprehend the impact of these diverse variables on water contamination. For example, shallow aquifers contain more nitrate than deep aquifers. Because nitrate content declines with depth, surface water is more sensitive to nitrate pollution. Nitrate toxicity levels are likewise lower in bigger bodies of water, but higher in smaller & shallower waterbodies due to simpler nitrate concentration buildup.\nIncreased nitrate levels in water supply are typically linked to shallow private wells with porous soils. Drinking water from shallow, drilled or excavated wells in unconfined aquifers is extremely risky. Wells near agricultural area are much more likely to contain high nitrate contents because to nitrogen contamination from agricultural lands. Water nitrate contamination is a serious concern, as many sources of drinking water are affected.\nNitrate Water Pollution Effects on Biosphere\nNitrate pollution has various harmful consequences on the ecosystem and many living organisms. Nitrates in soil, such as fertiliser runoff, increase the proliferation of microorganisms in water sources. Because these microbes dissolve oxygen from water, oxygen availability in such bodies decreases. Plants and fish, for example, are threatened by a lack of oxygen and may die from asphyxia. Excess nitrates in water have been related to the demise of various macrophytes. Some macrophytes, like charophytes, require pure, unpolluted water to survive, hence why high nitrate pollution is a major hazard.\nFurthermore, nitrates can change oxygen-transporting pigments into pigments that cannot transport oxygen, endangering freshwater invertebrates, fish, and amphibians. As nitrate concentrations rise, so does nitrate toxicity to freshwater aquatic creatures, which are particularly vulnerable to contaminants. Long-term exposed to elevated nitrate levels can degrade ecosystems and cause species extinction.\nNitrate levels not only affect aquatic life, but also human health. Humans are not equipped to eat large doses of nitrates, which are typically ingested by drinking water.\nMethemoglobinemia, often termed as blue baby syndrome, is a serious blood condition that typically affects infants & pregnant women. It is called methemoglobinemia when a haemoglobin molecule could no longer carry oxygen, converting it to methemoglobin. Because methemoglobin cannot carry oxygen, excessive amounts of methemoglobin diminish the oxygen delivered to the body. Excessive nitrate consumption can cause this because nitrates are transformed to nitrites, which combine with oxygen & begin oxidising numerous necessary components for survival and proper physiological function.\nMethemoglobinemia can cause unconsciousness or death in the affected person, generally a newborn. Methemoglobinemia is frequently induced by drinking contaminated private well water containing high nitrate-nitrogen. Symptoms include dark brown blood, tiredness, headaches, and other symptoms. If an infant’s blood concentration has more than 2% oxyhaemoglobin, they have this condition.']"	['<urn:uuid:ad776053-95f5-4374-97b9-54a1f324193d>', '<urn:uuid:26e79b64-5890-436c-9602-15a7e06dc613>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T10:32:05.666290	16	65	2000
35	What kind of support and guidance can students expect to receive during their music studies at the Chichester Conservatoire?	Students receive comprehensive support through weekly individual tuition in their instrument or voice, weekly group skills development sessions, and masterclasses at Chichester and other institutions. They also get ensemble training and a consultation with an external specialist each semester. The teaching is delivered by a core team of experienced and highly qualified tutors, alongside more than 60 specialist instrumental and vocal teachers. Around 140 professional tutors visit campus regularly throughout the semester to deliver practical and contextual modules. Students are supported by their one-to-one tutor, module tutors, and Academic Advisor.	['The University of Chichester Conservatoire has one of the largest and liveliest music departments in the UK with a community of over 1000 student performers. Our facilities include computerised recording and media studios, well equipped practice rooms and an acoustically superb performance venue.\nThis MA Music Performance degree will support you to enhance and perfect your performance. You will explore the work of specialists in your own performance field, develop a lecture recital and research relevant areas of repertoire and performance practice. Your dissertation is a recital presented at the beginning of the following academic year.\nYou can choose to study this MA online.\nTeaching and Assessment\nHow you will learn\nThis course requires a fluent level of technical and expressive skill. You will attend weekly individual tuition in your instrument or voice, weekly group skills development sessions, masterclasses at Chichester and other institutions, ensemble training, and a consultation with an external specialist each semester. The support you receive from University lecturers is complemented by invaluable exposure to other professional views and experience of different institutions throughout your studies.\nWhat you will study\nYou will study a selection of core and optional modules in each year. Each module is worth a number of credits is delivered differently, depending on its content and focus of study.\nThis list is indicative and subject to change.\nYou will identify an appropriate professional performance context and demonstrate the particular strategies it demands of the player, singer or conductor. You are encouraged to identify your own practice with the work of individual artists who are active in a relevant field.\nIn this module, you will explore a range of community and professional performance contexts and the varied strategies that these demand of the player, singer or conductor, using the module criteria to evaluate observed approaches to programming and repertoire. Case studies of current performance practice will be introduced by tutors, and as the module progresses you will be encouraged to identify your own practice with the work of individual artists active in a relevant field.\nThis module’s content will depend on the choice of performed material of the individual students, but will focus on three main areas of personal development:\n- the achievement of technical and expressive maturity\n- the identification of a demonstrably appropriate repertoire or a mix of repertoire which is able to foreground individual strengths\n- the acquisition and consolidation of strong performance and communication skills.\nYou will select an area of your own repertoire to research theoretically and in terms of its broader context, presenting results either in written form or as a linked sequence of performance recordings.\nWritten Exercise (Performance Practice)\nThis module builds on the previous module and is an opportunity to reflect on the specialist practice that exists within your own discipline. You will examine individual elements of performance as well as more innovative performance techniques, which must be translated into discrete terms for the instrument or voice.\nUse industry standard equipment\nLearning Resource Centre\nLearn from experienced performers, musicians and tutors\nYou will be taught by a core team of experienced and highly qualified tutors alongside a wide-ranging team of more than 60 specialist instrumental and vocal teachers.\nAs well as supporting student development and the student experience our staff are active, practicing professional musicians and researchers who regularly perform and record. We have around 140 professional tutors who visit campus regularly throughout the semester to deliver our practical and contextual modules. You will be supported by your one-to-one tutor and your module tutors, as well as your Academic Advisor.\nCourse Fees 2023/24\nFor further details about fees, please see our Tuition Fees page.\nFor further details about international scholarships, please see our Scholarships page.\nUniversity of Chichester alumni who have completed a full undergraduate degree at the University will receive a 15% discount on their postgraduate fees.\nTypical offers (individual offers may vary):\nYou will need to interview for this course.\nFrequently asked questions\nHow do I apply?\nClick the ‘Apply now’ button to go to our postgraduate application form.']	['<urn:uuid:2af35881-5754-42b5-aa00-094b0fc00af7>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	19	90	674
36	vehicle employee safety tripping hazards prevention tips	To prevent tripping hazards in vehicle operations, you should: find and address tripping hazards in front office, maintenance shop, and parking lot using visual cues like fluorescent paint or cones; ensure sufficient lighting in employee parking lots and vehicle areas; check all stairs for hazards like worn friction material, frayed carpeting, or loose handrails; and keep shop working surfaces clear of oil, grease, and debris.	['Take the employee safety challenge\nBy Matthew A. Daecher\nChances are I have never been to your facility, stepped foot onto one of your vehicles, or know anything about your overall safety program. Despite this, I’d be willing to place two wagers about your passenger transportation operation:\n1. You focus more on passenger safety and DOT compliance than worker safety and workplace safety compliance.\n2. Your most frequent employee injuries are due to overexertion or slips, trips and falls.\nI can’t predict the winning Powerball numbers, but if my bets about your operation ever paid off, you may take solace in the fact that you are not alone. My above assessment comes from many years of visiting with hundreds of transportation operations, reviewing their safety practices and looking at collision and injury issues and costs.\nI would hope that you to aspire to better your operations and separate yourself from the rest of the pack. You can get started by accepting my challenge to complete at least three tasks that will positively affect employee safety within three weeks of reading this article.\nIf you’re still reading, I assume you’ve decided to accept the challenge — or at least entertain the idea to alleviate some of the risk lurking within your company.\nIf you’re on the fence with this, let me sweeten the deal. Being keenly aware that the probability of success relates directly to the amount of effort expended to meet the challenge, I am offering eight options from which you have to select only three to accomplish. This allows the best chance of success and ensures that everyone participating can choose the tasks that are most applicable to their operations. Overachievers, feel free to take on more than three. Here we go.\nTASK 1: Find three tripping hazards. Yes, you have them. Tripping hazards can be anywhere from the front office and maintenance shop to the parking lot. If you can’t eradicate them, do something to call attention to them and make them more visible using visual cues like fluorescent paint, cones, etc.\nTASK 2: Find three oversized items, or those that weigh more than 75 pounds that employees are expected to move, lift or carry. Determine if you have equipment such as hoists and wheel dollies available or have a policy in place that requires workers to get assistance for handling these items. If not, do something about it.\nBonus points: If you do have such equipment or policy, ask an employee to actually relocate one of the items to see if the process meets all expectations — if not, conduct an on-the-spot safety talk with all the workers.\nTASK 3: Wait until dark. Go out to the employee parking lot. Is there sufficient lighting to illuminate the path between that lot and the area where the revenue vehicles are parked? Is there a danger of a trip hazard? If so, replace all burnt out bulbs or make lighting improvement a top-five capital expenditure item.\nRemember, everyone wants more vehicles, but worker compensation injuries can chew a big chunk of out of the bottom line through lost productivity, partial salary payments and insurance costs.\nA word to renters: If you rent or lease your operational facilities, suggest to your landlord about your performing improvements for a percentage reduction in rent, i.e., $2,000 in capital investment for a $1,000 rent deduction.\nBy the way, additional lighting doubles as a deterrent for other break-ins and crime.\nTASK 4: Check every set of stairs for hazards. Look for and fix absent or worn friction material, frayed carpeting, missing or loose handrails, or items stacked on edges of stairs.\nTASK 5. Inspect and address shop working surfaces. Instruct the shop crew to inspect for oil and grease areas that have not been cleaned up, or maybe oil-dry that has not been swept up. Asking the staff to stop what they are doing and clean up their areas on the spot will emphasize expectations and responsibility.\nRepeat this every three days until they understand the policy to immediately handle such hazards.\nTASK 6. Stage Operation Sure-Step. During this operation, scout for employees getting on and off the vehicle without taking necessary precautions. Most injuries occur when employees are stepping off the vehicle. Watch for this either during pre-trips where they should they should have to get on and off during the process. Better yet, watch when they return from a trip. They are more likely to be exiting with their hands full — and without three points of contact to help prevent falls.\nTASK 7. Beware of walking zombies. Humans are becoming increasingly tied socially to their smartphones, while the industry is asking drivers to not use their personal cell phones while driving the company vehicle.\nAs a result, smartphone timeouts are creating a nation of walking zombies when the timeout is over. I see it weekly when planes land and passengers empty into the terminal staring at their small screens. The same thing could be happening in your facilities.\nWith restrictions in place — and hopefully followed — drivers are turning on the personal phone once they park the vehicle and while catching up on the walk back to the building — making them oblivious to potential hazards. Watch for walking zombies in the lot, and talk about this at your next safety meeting.\nTASK 8. Require chocks and jack stands. This task requires trips through the garage on three separate days during peak operating hours. If a vehicle in for service has any wheels on the ground, at least one should be chocked. Any vehicle on a lift in the area, either partially or entirely, should have jack stands in use for emergency support. If you don’t have enough chocks or jack stands, get more. If they are not being used, have a toolbox meeting and make it mandatory.\nWhile the injuries that result from mishaps that that these practices can prevent may seem few and far between, they are usually very serious and very costly when they do occur.\nIt doesn’t count if you choose one of these tasks and find nothing wrong or anyone to counsel. Choose another. Somehow I’m guessing you know exactly which of these tasks will be fruitful before you even start.\nOn a last note, do me a favor. For those of you who accept my challenge, drop me an email at email@example.com and let me know which three or more tasks you chose and what you discovered.']	['<urn:uuid:92f63479-5a52-40a4-b6d8-50701e05b766>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	7	65	1083
37	How is argon produced naturally and used in radiometric dating?	Argon is naturally produced when potassium-40 in Earth's crust undergoes radioactive decay to argon-40. This process is utilized in potassium-argon and argon-argon dating methods to date rocks between 4.3 billion and 100,000 years old by measuring the ratio of radioactive argon to radioactive potassium within rocks. When volcanic rocks form and cool, all argon is released, and any subsequent argon presence must come from potassium decay, allowing age calculation.	"['|Classification:||Argon is a noble gas and a nonmetal|\n|Melting point:||-189.3 oC, 83.85 K|\n|Boiling point:||-185.8 oC, 87.3 K|\n|Neutrons in most abundant isotope:||22|\n|Electron configuration:||1s2 2s2 2p6 3s2 3p6|\n|Density @ 20oC:||0.001784 g/cm3|\n|Atomic volume:||22.4 cm3/mol|\n|Structure:||fcc: face-centered cubic when solid|\n|Specific heat capacity||0.520 J g-1 K-1|\n|Heat of fusion||1.188 kJ mol-1|\n|Heat of atomization||0 kJ mol-1|\n|Heat of vaporization||6.447 kJ mol-1|\n|1st ionization energy||1520.5 kJ mol-1|\n|2nd ionization energy||2665.8 kJ mol-1|\n|3rd ionization energy||3930.8 kJ mol-1|\n|Minimum oxidation number||0|\n|Min. common oxidation no.||0|\n|Maximum oxidation number||0|\n|Max. common oxidation no.||0|\n|Electronegativity (Pauling Scale)||–|\n|Polarizability volume||1.586 Å3|\n|Reaction with air||none|\n|Reaction with 15 M HNO3||none|\n|Reaction with 6 M HCl||none|\n|Reaction with 6 M NaOH||none|\n|Atomic radius||71 pm (measured)|\n|Ionic radius (1+ ion)||–|\n|Ionic radius (2+ ion)||–|\n|Ionic radius (3+ ion)||–|\n|Ionic radius (1- ion)||–|\n|Ionic radius (2- ion)||–|\n|Ionic radius (3- ion)||–|\n|Thermal conductivity||1.77 x 10-2 W m-1 K-1|\n|Electrical conductivity||0 mS cm-1|\n|Freezing/Melting point:||-189.3 oC, 83.85 K|\nDiscovery of Argon\nArgon was the first noble gas to be discovered.\nThe first hint of its existence came from English scientist Sir Henry Cavendish as far back as 1785. Cavendish was unhappy that so little was known about air. He was particularly unhappy about the lack of information about the fraction of air (the majority) which was not oxygen. (1)\nHe knew the nitrogen in air could be reacted with oxygen to form, ultimately, nitrous acid. He aimed to find out if ALL of the air that was not oxygen or carbon dioxide could be converted to nitrous acid. If it could, he would know that air was entirely oxygen, carbon dioxide and nitrogen.\nCavendish used an electric spark in air to react the oxygen and nitrogen to form nitrogen oxides. He then added additional oxygen until all the nitrogen had reacted.\nNitrogen oxides are acidic. Cavendish used aqueous sodium hydroxide to remove them from the apparatus. [This would also, of course, have removed any carbon dioxide that was present.] He removed the remaining oxygen using potassium polysulfides.\nA small bubble of gas remained [mostly argon]. Cavendish wrote that this bubble “was not more than one hundred and twentieth of the bulk of the phlostigated air [nitrogen].” (1) So, Cavendish is saying that air is at least 99.3 percent nitrogen/oxygen/carbon dioxide with a maximum 0.7 percent of something else. We now know that the ‘something else’, argon, is very unreactive; this enabled Cavendish to find it, but it also prevented him finding out more about it. (The giant advances in spectroscopy made by Gustav Kirchhoff and Robert Bunsen lay 85 years in the future.)\nIn hindsight, we can say Cavendish slightly underestimated the part of air that isn’t oxygen, nitrogen, or carbon dioxide. Despite this, he was ahead of his time. After his experiment, more than 100 years passed until scientists again began to think that something about air didn’t quite add up.\nIn 1892 English physicist John William Strutt (better known as Lord Rayleigh) announced that no matter how it was prepared, oxygen was always 15.882 times denser than hydrogen. This very precise work had taken ten years to complete.\nContinuing to work with great attention to detail, he found that the ‘nitrogen’ in air was always denser by about 0.5 percent than nitrogen sourced from nitrogen compounds. (2), (3) How could this be explained? In 1893 he wrote to Nature, announcing the problem to the world. Any scientist who responded to that challenge actually had the chance of discovering a new element. None did!\nIn April 1894 Rayleigh wrote an academic paper about the nitrogen problem. Funnily enough, Rayleigh viewed pure nitrogen, containing no argon, as ‘abnormally light nitrogen.’ He stored it for eight months and retested it to see whether its density would increase. (4)\nRayleigh’s paper awakened the serious interest of Scottish chemist William Ramsay, who had already been aware of the problem.\nRayleigh and Ramsay carried out further experiments, keeping in touch with one another about their progress.\nIn August 1894 Ramsay took air and removed its components – oxygen, carbon dioxide and nitrogen. He removed the nitrogen by reacting it with magnesium. After removing all the known gases from air, he found gas remaining that occupied one-eightieth of the original volume. Its spectrum matched no known gas.\nRayleigh and Ramsay wrote a joint paper in 1895 notifying the world of their discovery. The new gas wouldn’t react with anything, so they named it argon, from the Greek ‘argos’, meaning inactive or lazy. (5)\nIn his Nobel Prize winning address, Rayleigh said: “Argon must not be deemed rare. A large hall may easily contain a greater weight of it than a man can carry.” (6) William Ramsay discovered or codiscovered most of the other noble gases: helium, neon, krypton and xenon.\nHe was responsible for adding an entire new group to the periodic table. Radon was the only noble gas he didn’t discover.\nInteresting Facts about Argon\n- Lord Rayleigh said: “Argon must not be deemed rare. A large hall may easily contain a greater weight of it than a man can carry.” On a planetary scale, we can calculate that Earth’s atmosphere holds 65 trillion metric tons of argon. That’s more than 9 metric tons of argon per person on Earth.\n- Until 1957, argon’s chemical symbol was A. In 1957, IUPAC agreed that the symbol should change to Ar. Argon was not the only element whose symbol changed in 1957. IUPAC also changed mendelevium from Mv to Md.\n- Most people are familiar with carbon dating, which uses the decay of the radioactive carbon-14 isotope to find the ages of things that were once alive. Carbon-14′s half-life is about 5730 years and the technique is not useful for material more than about 60 thousand years old. Potassium-argon and argon-argon dating allow us to date rocks that are much older than this. Potassium-40 decays to argon-40 and calcium-40, with a half-life of 1.25 billion years. The ratio of potassium-40 to argon-40 trapped in rock can be used to determine how long it is since the rock has solidified. More recently, the ratio of argon-39 to argon-40 has been used in precision dating.\n- The vast majority of argon on Earth comes from the radioactive decay of potassium-40, producing stable argon-40. Over 99% of Earth’s argon is argon-40.\n- Away from Earth, argon-36 is the most abundant isotope, synthesized in the silicon burning phase of stars with a mass of about 11 or more Earth suns. During silicon burning, an alpha-particle adds to a silicon-32 nucleus to make sulfur-36, which can add another alpha-particle to become argon-36, some of which can become calcium-40, etc.\nAppearance and Characteristics\nArgon is considered to be non-toxic.\nArgon is a noble gas. It is colorless, odorless and extremely unreactive.\nIt is, however, not completely inert – photolysis of hydrogen fluoride in a solid argon matrix at 7.5 kelvin yields argon fluorohydride, HArF.\nUses of Argon\nAs a result of its unreactiveness, argon is used in light bulbs to protect the filament and to provide an unreactive atmosphere in the vicinity of welding.\nIt is also used in the semi-conductor industry to provide an inert atmosphere for silicon and germanium crystal growth.\nArgon is used in medical lasers, in ophthalmology for example to correct eye defects such as blood vessel leakage, retinal detachment, glaucoma and macular degeneration.\nAbundance and Isotopes\nAbundance earth’s crust: 3.5 parts per million by weight, 1.8 parts per million by moles\nAbundance solar system: 0.01 percent by weight, 3.3 parts per million by moles\nCost, pure: $0.5 per 100g\nCost, bulk: $ per 100g\nSource: Argon is produced when 40K present naturally in the earth’s crust undergoes radioactive decay to 40Ar. The argon makes its way into the atmosphere. Argon is produced commercially by fractional distillation of liquefied air with (for high purity argon) catalytic burning of left over traces of oxygen.\n1. Encyclopaedia Perthensis, or, Universal dictionary of the arts, Sciences, Literature, &c., 1816, vol 1, p231-232, John Brown.\n2. John H. Wolfenden, The Noble Gases and the Periodic Table: Telling it like it was., J. Chem. Educ., 1969, 46 (9), p569.\n3. Mary Elvira Weeks, The Discovery of the Elements. XVIII. The Inert Gases., J. Chem. Educ., 1932, 9 (12), p2065.\n4. Lord Rayleigh, On an Anomaly Encountered in Determinations of the Density of Nitrogen Gas., Proc. Roy. Soc. London, 1894, 55, p340.\n5. Vivi Ringnes, Origin of the Names of Chemical Elements, J. Chem. Educ., 1989, 66 (9), p731.\n6. Lord Rayleigh, The Density of Gases in the Air and the Discovery of Argon, Nobel Lecture, December 12, 1904. (pdf download.)\n7. Robert L. Kelly, David Hurst Thomas, Archaeology., Sixth Edition, 2012, Wadsworth, p137.\n8. Image by Deglr6328.\nCite this Page\nFor online linking, please copy and paste one of the following:\n<a href=""http://www.chemicool.com/elements/argon.html"">Argon Element Facts</a>\nTo cite this page in an academic document, please use the following MLA compliant citation:\n""Argon."" Chemicool Periodic Table. Chemicool.com. 15 Oct. 2012. Web. <http://www.chemicool.com/elements/argon.html>.', 'But how is it dated? What does radiometric dating actually mean? And what methods of dating can be used to date which kinds of items?\nWhat is radiometric dating?\nRadiometric dating is a method of establishing how old something is – perhaps a wooden artefact, a rock, or a fossil – based on the presence of a radioactive isotope within it.\nThe basic logic behind radiometric dating is that if you compare the presence of a radioactive isotope within a sample to its known abundance on Earth, and its known half-life (its rate of decay), you can calculate the age of the sample.\nRadiometric dating is useful for finding the age of ancient things, because many radioactive materials decay at a slow rate.\nWhat is radioactive decay?\nRadioactive atoms are unstable, meaning they decay into “daughter” products. The number of protons or neutrons in the atom changes, leading to a different isotope or element. The time it takes for one half of the atoms to have decayed is referred to as a “half-life”.\nWe know the half-lives of the radioactive isotopes found on Earth, and so we can trace how long a radioactive material within an object has been decaying for, and therefore how long (within a range of error) it’s been since the object was formed.\nSome radioactive materials decay into daughter products that are also radioactive, and have their own half-life: the result is called a “decay-chain”, which eventually decays into a non-radioactive substance.\nTypes of radiometric dating\nRadiocarbon (14C) dating\nYou’ve almost definitely heard of “carbon dating”. It’s a very common method used mostly by archaeologists, because it can only date relatively recent materials.\nRadiocarbon dating is possible because all living things take in carbon from their environment, which includes a small amount of the radioactive isotope 14C, formed from cosmic rays bombarding nitrogen-14.\nWhen an animal or plant dies, it will not take in any more carbon, and the 14C present will begin to decay. We can thus measure how long it’s been since the animal or plant died by comparing the presence of 14C with the known half-life.\nThis can raise complexities in archaeology when, for example, a society uses a piece of wood that was felled hundreds of years prior. There are also issues because the rate of cosmic ray bombardment of the planet over time has not always been stable: but this problem is largely redressed by a calibration factor.\nRadiocarbon dating is not suitable for dating anything older than around 50,000 years, because 14C decays quickly (its half-life is 5,730 years) and so will not be present in significant enough amounts in older objects to be measurable.\nRadiocarbon dating identified Ötzi, the Italian-Alps Iceman, as a 5,300-year-old traveller. More recently, Australian scientists used radiocarbon dating to figure out the age of wasp nests in rock art, and thereby establishing a date range for the art.\nPotassium-argon and argon-argon dating\nPotassium-argon dating is a method that allows us to calculate the age of a rock, or how long ago it was formed, by measuring the ratio of radioactive argon to radioactive potassium within it.\nRadioactive potassium (40K – a solid) decays to radioactive argon (40Ar – a gas), at a known rate. When volcanic rocks are formed and cooled, all argon within the rock is released into the atmosphere, and when the rock hardens, none can re-enter.\nThis means that any argon present in a volcanic rock must have been produced by the decay of radioactive potassium, so measuring the ratio can enable a scientist to date the sample.\nThis method is limited, because it’s only applicable to volcanic rocks, but is useful for older archaeology because it has a date range of about 4.3 billion to 100,000 years ago.\nHowever, there are potential issues with potassium-argon dating. For example, deep-sea basalts retain some argon after formation due to high hydrostatic pressure, and other rocks may incorporate older “argon-rich” material during formation.\nArgon-argon dating is an updated method, based on the original K-Ar dating technique, that uses neutron irradiation from a nuclear reactor to convert a stable form of potassium into the argon isotope 39Ar, and then measures the ratio of 40Ar to 39Ar.\nArgon-argon dating was used to determine that the Australopithecus Lucy, who rewrote our understanding of early hominin evolution, lived around 3.18 million years ago.\nThis technique involves measuring the ratio of uranium isotopes (238U or 235U) to stable lead isotopes 206Pb, 207Pb and 208Pb. It can be used to determine ages from 4.5 billion years old to 1 million years old. This method is thought to be particularly accurate, with an error-margin that can be less than two million years – not bad in a time span of billions.\nU-Pb dating can be used to date very old rocks, and has its own in-built cross-checking system, since the ratio of 235U to 207Pb and 238U to 206Pb can be compared using a “concordia diagram”, in which samples are plotted along a straight line that intersects the curve at the age of the sample.\nU-Pb dating is most often done on igneous rocks containing zircon. It’s been used to determine the age of ancient hominids, along with fission-track dating.\nThis method involves examining the polished surface of a slice of rock, and calculating the density of markings – or “tracks” – left in it by the spontaneous fission of 238U impurities.\nThe uranium content of the sample must be known; this can be determined by placing a plastic film over the polished slice and bombarding it with slow neutrons – neutrons with low kinetic energy. This bombardment produces new tracks, the quantity of which can be compared with the quantity of original tracks to determine the age.\nThis method can date naturally occurring minerals and man-made glasses. It can thus be used for very old samples, like meteorites, and very young samples, like archaeological artefacts.\nFission-track dating identified that the Brahin Pallasite, a meteorite found in the 19th century in Belarus – slabs of which have become a collectors item – underwent its last intensive thermal event 4.26–4.2 billion years ago.\nThis method involves calculating the prevalence of the very rare isotope chlorine-36 (36Cl), which can be produced in the atmosphere through cosmic rays bombarding argon atoms. It’s used to date very old groundwater, from between around 100,000 and 1 million years old.\nChlorine-36 was also released in abundance during the detonation of nuclear weapons between 1952 and 1958. It stays in the atmosphere for about a week, and so can mark young groundwater from the 1950s onwards as well.\nLuminescence dating methods are not technically radiometric, since they don’t involve calculating ratios of radioactive isotopes. However, they do use radioactive material.\nThese methods date crystalline materials to the last time they were heated – whether by human-made fires or sunlight.\nThis is possible because mineral grains in sediments absorb ionising radiation over time, which charges the grains in “electron traps”. Exposure to sunlight or heat releases these, removing the charges from the sample.\nThe material is stimulated using light (optically stimulated luminescence) or heat (thermoluninescence), which causes a signal to be released from the object, the intensity of which can provide a measure of how much radiation was absorbed after the burial of the material – if you know the amount of background radiation at the burial site.\nThis method can date archaeological materials, such as ceramics, and minerals, like lava flows and limestones. It has a normal range of a few decades to 100,000 years old, but some studies have used it to identify much older things.\nOther types of radiometric dating\nThere are several other radioactive isotopes whose ratios can be measured to date rocks, including samarium-neodymium, rubidium-strontium, and uranium-thorium. Each of these have their own advantages and idiosyncrasies, but they rely on the same logic of radioactivity to work.\nThe Royal Institution of Australia has an Education resource based on this article. You can access it here.\nAmalyah Hart has a BA (Hons) in Archaeology and Anthropology from the University of Oxford and an MA in Journalism from the University of Melbourne.\nRead science facts, not fiction...\nThere’s never been a more important time to explain the facts, cherish evidence-based knowledge and to showcase the latest scientific, technological and engineering breakthroughs. Cosmos is published by The Royal Institution of Australia, a charity dedicated to connecting people with the world of science. Financial contributions, however big or small, help us provide access to trusted science information at a time when the world needs it most. Please support us by making a donation or purchasing a subscription today.']"	['<urn:uuid:76d0945a-42a0-4300-b89a-0b4c184f4069>', '<urn:uuid:14aa7cac-c5d6-4d9d-b31f-461799cb2586>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T10:32:05.666290	10	69	2938
38	Where can I find groups that need environmental volunteers?	Several organizations can connect you with volunteer opportunities. These include AmeriCorps, the Sierra Club which provides volunteers for multi-day service trips, Break Away which organizes alternative spring break groups, and Volunteer Match which helps recruit qualified volunteers for nonprofits. The Points of Light Foundation also provides a list of service centers in California offering volunteer support.	['What would weed work be without volunteers? Many programs rely on volunteers, from one-time classes of 25 fourth-graders to weekly groups of three retirees. See resources below for volunteers and volunteer organizers.\nWildland Volunteer Network\nCal-IPC is working with land management partners in the San Francisco Bay Area to grow the Wildland Volunteer Network. The network organizes trainings for volunteers and hosts an online map of volunteer projects. Please see our Wildland Volunteer Network page for more information.\nCal-IPC Resources for Volunteers\n- Cal-IPC Inventory categorizes plants that threaten California’s natural areas. The Inventory includes plants that currently cause damage in California (invasive plants) as well as “Watch” plants that are a high risk of becoming invasive in the future.\n- Cal-IPC Plant Profiles: Clearinghouse of information on plants listed in our inventory, including assessment of invasiveness, links to control information, and more. Select species by scientific or common name.\n- Weed Worker Handbook: Designed for volunteers, this small, field-ready book explains how to manage more than 35 of California’s most invasive plants. Hard copies are out of print, but the pdf is available for free download.\n- Cal-IPC Store: A place to order boot brushes and books like Weed Control in Natural Areas in the Western United States and Weeds of California and Other Western States, and more.\n- Training Videos: Cal-IPC’s library of training videos includes basics on prevention, best management, mechanical controls, and safe herbicide use.\n- Connect to the Wildland Volunteer Network\nLearning how to identify plants\nBOOK: Weeds of California and Other Western States. This two-volume set contains detailed descriptions and hundreds of photographs of plants, including life stages from seed to adult.\n- Weed ID Cards:\n- Cal-IPC species ID Cards for many plants that can be downloaded, printed double-sided, and trimmed to size.\n- Golden Gate National Recreation Area uses weed cards with their volunteers. There are eight cards with three species each.\n- Save the Bay ID cards A 38-page pdf that can be printed with Identification information on a set of weed species.\n- National Park Service ID cards Two other pdfs of weed ID cards.\n- UC Integrated Pest Management Program Weed Photo Gallery: This photo gallery includes many, includes many, but not all, weed species commonly found in California farms and landscapes, separated by type.\n- Calflora – This online mapping database tracks plant locations across the state. Create an account and download the Observer app to your smartphone to submit your observations from the field. You can see “What Grows Here?” for a given site, and you can set up an Email Alert to let you know about new observations of weeds in places you want to track.\n- Weed Research & Information Center – Information from UC Davis, including treatment options for many plants and a plant ID tool.\n- University of California, Integrated Pest Management – The UC IPM program puts out peer-reviewed Pest Notes for various pests, including some weeds.\n- East Bay Regional Park District plant lists\n- Phytosphere Research – Information about Sudden Oak Death and other topics.\nVolunteer recruitment, safety and management\nWEBSITE: Developing and Managing Volunteer Programs: this free management library provides a comprehensive list of management tips and resources, including screening, selection, and management\n- PDF: Managing Volunteers: Balancing Risks and Rewards. An insurance-industry pamphlet that addresses risk management with volunteers.\n- BOOK/PDF: The Weed Workers’ Handbook. Check out the following chapters:\n- PDF: Breaking up with Difficult Volunteers: an article from the Charity Channel.\n- WEBSITE: TNC’s Global Invasive Species Initiative volunteer coordination and outreach tools. Brochures, videos, and how-to.\n- Organizations: finding volunteers. Try these organizations to bulk out your program:\n- AmeriCorps: Application/Grant-based volunteer support\n- Points of Light Foundation: provides a list of service centers in California offering volunteer support\n- Break Away: A list of alternative spring break groups\n- Sierra Club: Environmental organization that provides volunteers for multi-day service trips.\n- Volunteer Match: An effective tool to recruit highly qualified volunteers for your nonprofit.\n- Event Management: Resources to post event information and manage event registration and attendance:']	['<urn:uuid:08a052d9-7dc0-46a1-b581-58a6fe26cf60>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	9	56	684
39	nissen surgery vs heart cath kids pain difference	In pediatric laparoscopic Nissen fundoplication surgery, pain may occur after the procedure and is managed with medicines as needed. For cardiac catheterization, most procedures are performed under general anesthesia so children sleep through without pain or anxiety, though there may be slight discomfort at insertion sites afterward. Both procedures prioritize pain management during the hospital stay.	"[""Pediatric Laparoscopic Nissen Fundoplication\nWHAT YOU SHOULD KNOW:\n- A pediatric laparoscopic Nissen fundoplication is surgery to treat your child's gastroesophageal reflux disease (GERD). GERD occurs when the lower muscle of the esophagus, called the lower esophageal sphincter, does not close properly. The esophagus is the tube that carries food and liquid from the throat to the stomach (belly). GERD causes normal stomach acid and food to reflux (move back up) from his stomach into his esophagus. When the acid irritates and damages his esophagus, it causes a burning feeling called heartburn. Nissen fundoplication may be done when GERD does not get better after medicines have been tried, or if other medical problems occur. It may be done if your child gets lung or breathing problems, fails to grow or gain weight, or has trouble swallowing.\n- With this surgery, the top part of your childs stomach is wrapped around the lower part of his esophagus. This puts pressure on the lower esophageal sphincter, making food go down, but not letting it come back up. The surgery is done using a laparoscope. A laparoscope is a long metal tube with a light and tiny video camera on the end. With a laparoscopic Nissen fundoplication, symptoms of GERD such as throwing up may stop, and damage to your child's esophagus may be prevented.\n- Keep a current list of your child's medicines: Include the amounts, and when, how, and why they are taken. Bring the list and the medicines in their containers to follow-up visits. Carry your child's medicine list with you in case of an emergency. Throw away old medicine lists. Give vitamins, herbs, or food supplements only as directed.\n- Give your child's medicine as directed: Call your child's primary healthcare provider if you think the medicine is not working as expected. Tell him if your child is allergic to any medicine. Ask before you change or stop giving your child his medicines.\n- Do not give aspirin to children under 18 years of age: Your child could develop Reye syndrome if he takes aspirin. Reye syndrome can cause life-threatening brain and liver damage. Check your child's medicine labels for aspirin, salicylates, or oil of wintergreen.\nAsk for more information about where and when to take your child for follow-up visits:\nFor continuing care, treatments, or home services for your child, ask for information.\nYour child may need to have a soft diet for several weeks after surgery. Soft foods include puddings, applesauce, and mashed potatoes. Ask your caregiver for more information about a soft food diet.\nYour child may have a gastrostomy tube. This tube is used to give liquids, food, or medicine, and let air or fluids out of your child's stomach. Ask caregivers for more information about feeding your child through, and caring for the tube.\nCONTACT A CAREGIVER IF:\n- Your child has a fever.\n- Your child has chills, a cough, or feels weak and achy.\n- Your child has nausea (upset stomach) or vomiting (throwing up).\n- Your child is irritable (fussy) and crying more than usual.\n- Your child's bandage becomes soaked with blood.\n- Your child's skin is itchy, swollen, or has a rash.\n- You have questions or concerns about your child's condition, surgery, medicine, or care.\nSEEK CARE IMMEDIATELY IF:\n- Your child feels very full and cannot burp or vomit (throw up).\n- Your child has trouble swallowing or cannot eat or drink.\n- Your child cannot have bowel movements.\n- Your child has severe (very bad) chest or shoulder pain, or trouble breathing all of a sudden.\n- Your child's vomit is greenish in color, looks like coffee grounds, or has blood in it.\n- Your child's wound has pus or a bad smell coming from it.\nCopyright © 2012. Thomson Reuters. All rights reserved. Information is for End User's use only and may not be sold, redistributed or otherwise used for commercial purposes.\nThe above information is an educational aid only. It is not intended as medical advice for individual conditions or treatments. Talk to your doctor, nurse or pharmacist before following any medical regimen to see if it is safe and effective for you."", ""Pediatric Cardiac Catheterization and Electrophysiology Studies\nFor Referring Physicians\nThe Pediatric Heart Program is proud to join the tradition of outstanding clinical programs that you expect from University of Wisconsin Hospital and Clinics and American Family Children's Hospital.\nStaff of the American Family Children's Hospital Pediatric Cardiology Clinic specialize in the care of children and young adults with heart-related conditions. During a cardiac catheterization, parents can be with their child through much of the process. Here are some frequently asked questions to help parents:\nWhy does your child need a cardiac catheterization?\nA cardiac catheterization is a procedure used to both diagnose and treat congenital heart disease. It provides information used to decide if there is a need for medicines or surgery. Interventions can also be done during a cardiac catheterization to treat problems that may have needed surgery. The cardiologist doing the procedure will go over the plan in detail and answer any questions you may have. If you are not sure about what is going to happen during the catheterization, please ask questions. It is our job to make sure you are comfortable with the care plan for your child before the procedure.\nWhat is the Cath Lab?\nAlso known as the Catheterization Lab, it is a place where cardiac catheterizations are performed.\nHow is a catheterization done?\nA cardiac catheterization is a procedure which involves threading a thin flexible tube (catheter) through the arteries and veins of the heart and lungs, often from the groin (see illustration). With the use of X-ray and contrast dye, the procedure team can define structure and function of the heart and lungs. This information is then used to determine the best therapies for treating congenital heart disease. We call that a diagnostic catheterization. If the doctor is also going to treat a type of congenital heart disease during the procedure, it is called an interventional catheterization. A number of interventions can be performed in the cath lab.\nIs it painful?\nMost cardiac catheterizations done in children with congenital heart disease are performed with general anesthesia. You will meet with a dedicated pediatric cardiac anesthesiologist before the procedure who will review your child’s medical history to make sure anesthesia is safe and effective. The anesthesiologist will monitor your child during the entire procedure. Your child will sleep through the procedure without pain or anxiety. After the procedure, there may be slight discomfort at the insertion sites. Pain medicines are given as needed. Pain management is a priority at American Family Children’s Hospital, and you can expect pain to be treated and controlled during the hospital stay. We also have a dedicated team of Child Life specialists experienced in supporting and distracting children during hospital visits. If support from a Child Life specialist would be helpful for your child, please let us know so it can be arranged ahead of time.\nHow long does a catheterization take?\nUsually, there is at least a 2- to 3-hour wait from the time your child leaves for the procedure until you are reunited in the recovery area or in an inpatient room at American Family Children's Hospital. The cath lab team, along with the recovery room or inpatient nurse, will let you know where to wait and when you can come into the room, once your child is settled after the procedure.\nWhere do I wait during the procedure?\nAt the start of the catheterization, staff will bring family members and guests to the Cath Lab Waiting Room. Cath lab staff will be available during this time to assist you, and food and restrooms are either in or near these areas while you wait. During the procedure, cath lab team members will give you frequent updates about the progress of the catheterization.\n- Luke Lamers, MD\n- Nicholas Von Bergen, MD\n- Jenna Torgeson, APNP\n- Martine Moran, RN""]"	['<urn:uuid:4951b2b4-9b41-4a86-9085-d2de1d84f1ab>', '<urn:uuid:9e465787-5611-45bc-b2b0-4840705c968c>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T10:32:05.666290	8	56	1342
40	As a truck driver, what's the safe distance to scan ahead?	Professional drivers should scan ahead about 15 seconds (which equals a quarter mile on interstates, or one to two blocks in cities) to detect traffic issues, work zones, and other potential hazards.	"[""Awesome Hughson motorcycleAccident Legal Services\nMake motorbike riding safety your leading precedence!\nRunning a motorbike can take various expertise than driving an automobile; nonetheless, the legal guidelines of the highway implement to every driver just the same. A mix of constant education and learning, regard for website traffic legislation and simple frequent perception can go a great distance in assisting minimize the quantity of fatalities associated with bike mishaps on a annually foundation.\nObserve the following tips for safe Using:\nOften wear a helmet which has a face protect or protective eye have on.\nPutting on a helmet is The easiest way to shield versus significant head accidents. A motorbike rider not carrying a helmet is five instances more more likely to sustain a critical head damage.\nDon proper equipment.\nBe sure to don protective equipment and garments which will limit the amount of injuries in the event of a mishap or maybe a skid. Wearing leather outfits, boots with nonskid soles, and gloves can safeguard Your whole body from serious injuries. Take into consideration attaching reflective tape in your garments to really make it easier for other drivers to check out you.\nAdhere to targeted traffic regulations.\nObey the velocity Restrict; the more quickly you go the extended it will eventually take you to stop. Pay attention to regional website traffic guidelines and procedures of your highway.\nLeading Huge Rig Security Ideas\n1. Defense! Protection!\nProfessional motorists ought to be frequently vigilant to detect unexpected highway conditions, distracted motorists, and motorists who don’t understand how industrial autos function.\nScan ahead about fifteen seconds (1 / 4 mile on interstates, or a person to 2 blocks in metropolitan areas) for targeted traffic concerns, function zones, as well as other potential risks.\nLook at mirrors each individual 8-ten seconds to pay attention to automobiles moving into your blind places.\n2. Sign for Basic safety\nSignal and brake to present other drivers a lot of time to note your intent.\nIf you will need to pull off the highway, use flashers, reflective triangles, and street flares to notify approaching drivers.\nthree. Know When to Sluggish\nDriving as well quick for temperature or road conditions or failing to decelerate for curves or ramps develop hazards for spills and rollovers, in addition to crashes.\nfour. Preserve Your Auto\nMake certain that pre-trip protection inspections are concluded especially for tires and brakes. Your daily life is determined by them. Be certain your load is properly well balanced and secure, for a shifting load could cause a rollover or lack of Management. Unfastened resources create street dangers.\n5. Buckle Up\nUse your protection belt each and every time. Protection belts conserve lives, lessen accidents, and allow drivers to remain inside of and in charge of their cars in the event of a crash. In 2014, 30% of truck drivers involved with lethal crashes had been partly or entirely ejected from their cars.\nsix. Stay Sharp\nGet ample relaxation; don’t drive once you’re fatigued, far too unwell to target, or on prescription drugs (including above-the-counter medication) which make you drowsy or dizzy.\nseven. Get the ideal Info for Vacation Planning\nStay updated on climate and street ailments, detours, and mountainous routes in order to strategy driving time.\nBe aware that non-commercial navigation methods and apps may not offer warning of height and fat restrictions and also other business car or truck limitations.\n8. Follow Operate Zone Basic safety\nOperate zones current quite a few dangers, like lane shifts, sudden stops, uneven road surfaces, shifting workers and tools, and puzzled find more info passenger automobile drivers. In 2014, thirty% of deadly get the job done zone crashes included not less than 1 large truck compared to only eleven% of all deadly crashes – so it’s essential to get perform zone safety seriously.\nDecelerate, keep added adhering to space, also to be ready to cease.\nObey all perform zone indicators and alerts.\nScan forward for transforming targeted traffic designs, and become notify to automobiles moving into your blind spots.\nKeep a pointy eye out for road staff and flag crews.\n9. Under no circumstances Generate Distracted\nTexting is Amongst the worst driving distractions. The percentages of currently being involved in a crash, in the vicinity of-crash, or unintentional lane deviation are 23.2 situations bigger for truck and bus motorists who are texting when driving.\nAnalysis reveals that drivers texting though driving took their eyes off the ahead road for four.six seconds on typical. At fifty five mph, this equates to touring 371 feet (in excess of the length of a soccer field) devoid of taking a look at the street.\nIt is prohibited to get a business driver to textual content though driving, and you'll find limits on making use of cellphones (devices have to be arms totally free, and dialed working with no multiple button).\nEating, drinking, interacting by using a navigational machine, map reading through, managing a pet, or almost every other action that usually takes focus from the highway can also be a deadly distraction.\nIf you need to go to to an action in addition to driving, Homepage get off at the next exit or pullover – it’s not worth the danger.""]"	['<urn:uuid:2fd9016c-d63a-41a0-9c2e-611b36bba822>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	11	32	867
41	I'm new to gardening and noticed insects eating my plants. I've heard brown mustard and carrots can both have pest problems - how do the protection methods differ for these vegetables?	Both plants require different approaches to pest protection. Brown mustard's strong aroma attracts pests that can damage and devour the crop, so it needs to be grown in a polytunnel to reduce the chance of insect invasion. Carrots, on the other hand, are specifically vulnerable to carrot fly, which flies close to the ground. Carrot protection methods include using fine insect netting, growing in raised beds (since the pest only flies low), and planting spring onions around the edges as a deterrent.	"['Brown mustard is an oriental plant with an exquisite taste. It is the main ingredient for mustard sauce, and gourmets will surely appreciate mustard leaves. It is also a popular ingredient in Indian and Far Eastern cuisine, representing a key feature in these tasty dishes. Its cultivation is very simple, with low requirements, and care consist of watering and looking after it to make sure it develops properly.\nThe polytunnel will protect the crop against parasites, which are the main problem with its cultivation. The plant has a strong aroma, which attracts pests and becomes a nuisance in open field production. They can damage and even devour the crop. Thanks to the cover, the chance of an insect invasion that could feed on them is reduced.\nThis time in the Krosagro crops under cover guide, we will explain how to plant, cultivate and collect mustard.\nWhat is brown mustard ?\nIt is a deciduous plant with an intense aroma, spicy taste and a large amount of health properties. It can be used in any dish, both raw and after a cooking process. In addition, high concentrations of vitamins A, C and K, numerous micronutrients. Folic acid and sulforaphane are more reasons to find a spot in the tunnel for this particular plant.\nConditions for growing mustard\nSoil for its cultivation is not really relevant as brown mustard can grow on every type, although sandy – loamy will be the optimal substrate.\nThe plant needs a sunny place for proper development, but it also works well in the shades. This makes it possible to plant it close to a higher crop that will shadow it when growing up. This represents an opportunity to fill empty spaces and get a crop from those unused areas.\nThe plant will grow at both low and high temperatures. Optimal growth occurs with heat in the open, when temperatures reach 17 to 20 ° C. While in a garden tunnel, if soil and air are monitored, it is possible to cultivate it throughout the year.\nMustard from seedlings grow even faster than seeds placed straight into the ground. It is worth mentioning that the growing period of this plant is three weeks. So if the soil is not deprived of its elements, you can repeat the cultivation process. Although mustard grows quickly, leaf growth is much slower. Thanks to the short maturation period, brown mustard is an ideal forecrop and aftercrop. To prepare seedling, 6 grains are sown in each multi tray. When they rise, they tend to be weak. After 2-3 weeks, the plant should be strong enough to be permanently placed inside the polytunnel.\nSowing and spacing:\nThe seeds are sown at a depth of 0.5 cm and with a spacing of 30 cm, and the same distance between the rows. You can reduce the spacing if the crops are taken straight from garden, then half of the distance mentioned above will be enough. In open field cultivation, the whole process begins in the second half of April while this can be implemented in March under cover.\nBrown mustard can be harvested directly from the plot, cutting off the leaves from the main stem. The plant then begins to release new shoots. It is important to leave at least 5 cm of stem, from which the next leaf will develop. Of course, you can wait for it to fully develop and then collect the whole plant, but the first option is a much better solution, bearing in mind that we then avail of a permanent source of fresh mustard.', ""Growing carrots from seed\nBefore we go any further though, please look at the photos which both show carrots being grown well above ground. This is to deter the carrot fly pest which only flies at just a few inches above ground.\n1) Carrots grow best in soil which has not been recently composted, or if compost has been used it should be very well rotted otherwise the end result will look like something which has been spawned from a nuclear reactor site with lots of roots forking from the main one.\n2) Carrots are grown best in a light sandy soil, so if your soil is heavy or clay based then dig in plenty of fine builder's sand, the light coloured sand is best we find. All Purpose compose which you can buy from any garden centre is perfectly OK to use, either on it's own or mixed with soil and sand. Sharp sand is excellent too.\n3) You will find that some of your crop will show above ground and that part may turn green. Greening of the top of the carrot is caused by sunlight. Heavy rain can wash away the soil from carrot tops exposing them to the sun. The green colour is the chlorophyll pigment. Mound up the soil around the shoulders of the carrots to prevent exposure to the sun.\nSowing carrot seeds:\nA typical packet of carrot seeds contains in the region of 2000 seeds and these can be sown in your garden in one of several ways.\n1) By broadcasting: basically this means you get a handful of seed and scatter them onto your already prepared and pre-watered patch of soil. Cover them with a light coating of new soil and leave them to grow. This method means that you will have a great many shoots appearing very close together and they need to be thinned out to leave the strongest growing on to become part of your salad.\n2) The more time consuming method is to dib 1 CM deep holes in the soil and drop in an individual seed. Cover and leave to grow as normal. You will still have to do some thinning out because it is a virtual impossibility to drop just 1 seed into every hole. But at least with this method most of your seeds will grow at predetermined distances away from each other.\nWell, all the above should get you a good crop but there are other little things such as carrot fly which can ruin a whole crop, but we tackle that problem on one of our pages in the pests/diseases section. However, if you follow the tips below you should avoid the problem\n1) Carrot flies fly close to the ground and lay their eggs near to the top of you carrots. To overcome this you need to buy some very fine insect netting which will keep them away.\n2) Make a raised bed as high as you can and plant your seeds in there. As the pests only fly low they will not be able to infect your crop.\n3) When you plant your carrot seeds then also plant plenty of spring onion seeds around the edges of your plot and these will help to deter the pests.""]"	['<urn:uuid:d6013db1-68ce-4232-84f3-a876f7feef03>', '<urn:uuid:e5790b0d-9115-4cd8-8efb-0995b2855b32>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T10:32:05.666290	31	82	1141
42	Why do body fat control mechanisms fail in humans?	Fat levels going outside a narrow range isn't lethal, unlike blood pH which must stay within 7.35 to 7.45. Additionally, something about the human lifestyle interferes with our innate appetite signaling system, unlike in most other living organisms.	['What is leptin?\nLeptin is a hormone produced by body fat.\nLeptin acts on the brain to make you feel full.\nThe more fat a person has, the more leptin they produce.\nIn an ideal world, this should keep one’s body fat levels constant and within a narrow range.\nSo why are some people obese and others suffer from anorexia?\nIf I knew the full answer to this question, I wouldn’t be here…I would be dealing with the queue of people outside my door…\nWhat I can tell you is that there is light at the end of the tunnel.\nWhy don’t our biological mechanisms to maintain a constant weight work well?\nMainly because our fat levels going outside a narrow range isn’t lethal.\nThese biological control mechanisms are also known as homeostasis.\n(Homeostasis = biological mechanisms to maintain an equilibrium).\nAn example of homeostasis:\nWe have exquisite homeostatic mechanisms to keep our blood pH within a very narrow range of 7.35 to 7.45 – in this case homeostasis works extremely well; if your blood is a touch too acid or a teensy bit too alkaline the consequences could be fatal. Consequently, our elegant biochemical and physiological machinery is very finely tuned to make sure pH stays within this range.\nA little, or even a lot, of extra fat clearly isn’t an immediate life or death situation, unlike blood pH.\nWhat we do know about energy homeostasis\nWe still have a long way to go as far as fully elucidating energy homeostasis is concerned, but we do know that the ventromedial hypothalamus (VMH) in the brain plays a significant part.\nWhat else is involved?\nThe ob/ob gene – mutations of the ob/ob gene in mice are associated with massive obesity.\nHow long have we known about all of this?\nNot all that long.\nTo be precise, since 1994 – a seminal year for me – the discovery of leptin, the ob/ob gene and birth of my eldest son.\nLeptin is part of the appetite signalling system – leptin converses between fat cells and the brain and informs the brain how much fat we have.\nThe brain then switches hunger on and off accordingly.\nWhy does energy homeostasis go awry?\nWe are clearly doing something to upset our energy homeostatic mechanisms – they function well in most other living organisms (domesticated animals being the main exception).\nThere’s evidently something about the human lifestyle that’s mucking with our own, and our pets, innate appetite signalling system.\nWhat changes the fat-o-stat?\nA study was conducted on rats that were made obese by feeding them ‘industrial’ type diets of refined ingredients – probably not dissimilar to junk and fast food diets in humans.\nAfter making the rats obese, they were then fed exactly the same number of calories as lean rodents who were previously eating a normal diet.\nA surprising outcome\nEven though the obese and lean rats were fed exactly the same number of calories, the obese rats continued to gain fat, whereas the lean rats didn’t, regardless of the type of diet or even if the lean rats were given free access to food.\nWhat’s going on?\nThe obese rats reduced their energy expenditure more than the lean rats did.\nThe most logical explanation for this behaviour is that the “set point” of the energy homeostasis system was amended. The industrial diet causes the rodents’ bodies to “want” to accumulate more fat, therefore they will accomplish that by any means necessary, whether it means eating more, or if that’s not possible, then by expending less energy.\nThis shows that poor diets can, in principle, dysregulate the systems that control energy homeostasis (in rodents at least).\nDoes this rat study have analogies in humans?\nProbably yes, but one can’t know for sure.\nOne could hypothesise that ‘junk food’ and abnormal lifestyles lead to gradual fat gain by dysregulating the energy homeostasis system.\nThis system is not under our conscious control, and most likely has nothing to do with willpower.\nI suspect that if one were to put a group of children on a junk food diet for many years, and then compare them to a group of children on a healthy diet with exactly the same number of calories, the junk food group would end up fatter as adults, even though calorie intake might be identical in the two groups (as in the rodent example).\nDo calories matter?\nIt appears that the type of food one eats is as important as the quantity – so calories probably aren’t as important as previously thought.\nOne could speculate that when homeostasis is working fine and dandy our brain does all the calorie maths automatically, and energy balance requires no conscious effort.\nProblems arise when our energy balance control mechanisms aren’t working well and we are constantly surrounded by cheap unhealthy food…all that’s left then is for our conscious will power to stop us from over eating.\nAnd as I’m sure most of us know, it’s hard to resist that helping of chocolate cake even when we are feeling full.\nSufficient sleep makes a difference\nSeveral studies have shown that insufficient sleep has detrimental effects on leptin.\nFish and omega 3 fatty acids\nIn cultures where fish is eaten daily, leptin resistance and leptin levels are low. Specifically it seems to be the omega-3 fatty acids in fish which help regulate leptin.\nHow to keep your energy control mechanisms functioning well\nEasy to say, but not so easy to follow – get plenty of sleep, keep active, feast on vegetables and fruit, eat a little bit of fish, meat, eggs, and nuts and not much else…']	['<urn:uuid:dc7459b2-f066-44b5-ba6e-1f3070e916e0>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	9	38	940
43	What historical connection exists between the astronomical cycles developed by ancient Greek astronomers like Callippus and the development of the modern Iranian calendar system, particularly in terms of their attempts to reconcile solar and lunar measurements?	While Callippus in 330 BC developed his 76-year cycle as an improvement on the 19-year Metonic cycle to better align the tropical year with synodic months, the Iranian calendar took a different approach. The Iranian system, which was revised in the 11th century by scientists, abandoned the lunar approach entirely in favor of a purely solar calendar. Unlike the Callippic cycle which tried to reconcile both solar and lunar measurements, the Iranian calendar focused exclusively on solar observations, specifically using the vernal equinox as its reference point, making it more accurate but also more complex in determining leap years.	"[""|This page uses content from the English Wikipedia. The original article was at Callippic cycle. The list of authors can be seen in the page history. As with the Calendar Wikia, the text of Wikipedia is available under Creative Commons License. See Wikia:Licensing.|\nIn astronomy and calendar studies, the Callippic cycle (or Calippic) is a particular approximate common multiple of the year (specifically the tropical year) and the synodic month, that was proposed by Callippus in 330 BC. It is a period of 76 years, as an improvement on the 19-year Metonic cycle.\nA century before Callippus, Meton invented the cycle of 19 years that counted 6,940 days, which exceeds 235 lunations by almost a third of a day, and 19 tropical years by four tenths of a day. It implicitly gave the solar year a length of 6940/19 = 365 + 1/4 + 1/76 days = 365 d 6 h 18 min 56 s. But Callippus knew that the length of the year was more closely 365 + 1/4 day (= 365d 6h 00m 00s), so he multiplied the 19-year cycle by 4 to reach an integer number of days, and then dropped 1 day from the last 19-year cycle. Thus he constructed a cycle of 76 years that contains 940 lunations and 27,759 days, and has been called the Callippic after him.\nAlthough the cycle's error has been calculated as one full day in 553 years, or 4.95 parts per million., in actuality 27,759 days in 76 years has a mean year of exactly 3651 days, which relative to the mean northward equinoctial year is about 11 minutes too long per year, in other words the cycle drifts another day late per 13010 years, which is considerably worse than the drift of the unrounded Metonic cycle. If the Callippic cycle is considered as closer to its unrounded length of 27,758+3/4 days (based on 940 lunations) then its accuracy is essentially the same as the unrounded Metonic cycle (within a few seconds per year). If it is taken as 940 lunations less one day then the Callippic mean year will be shortened by 1/76 day (18 minutes 57 seconds), making it grossly too short, and it will also grossly drift ahead with respect to the mean lunar cycle at the rate of 1/940 of a day (1 minute 31 seconds) per lunar month. If the cycle length is truncated to 27,758 days then the mean year is 365 days 5 hours 41 minutes 3 seconds, or almost 8 minutes too short per year, and it will drift ahead of the mean lunar cycle by about (3/4)/940 day (1 minute 9 seconds) per lunar month. Altogether, the purported accuracy of this cycle is not impressive, but it is of historical interest.\nThe first year of the first Callippic cycle began at the summer solstice of 330 BC (June 28 in the proleptic Julian calendar), and was subsequently used by later astronomers. In Ptolemy's Almagest, for example, he cites (Almagest VII 3, H25) observations by Timocharis in the 47th year of the first Callippic cycle (283 BC), when on the eighth of Anthesterion, the Pleiades were occulted by the Moon.\nThe Callippic calendar originally used the names of months from the Attic calendar, although later astronomers, such as Hipparchus, preferred other calendars, including the Egyptian calendar. Also Hipparchus invented his own Hipparchic calendar cycle as an improvement upon the Callippic cycle. Ptolemy's Almagest provided some conversions between the Callippic and Egyptian calendars, such as that Anthesterion 8, 47th year of the first Callippic period was equivalent to day 29 in the month of Athyr, in year 465 of Nabonassar. However, the original, complete form of the Callippic calendar is no longer known."", '|This page uses content from the English Wikipedia. The original article was at Iranian calendar. The list of authors can be seen in the page history. As with the Calendar Wikia, the text of Wikipedia is available under Creative Commons License. See Wikia:Licensing.|\nThe Iranian calendar (گاهشماری هجری خورشيدى) also known as Persian calendar or the Jalāli Calendar is a solar calendar currently used in Iran and Afghanistan. It is observation-based, rather than rule-based, beginning each year on the vernal equinox as precisely determined by astronomical observations from Tehran (or the 52.5°E meridian) and Kabul. This makes it more accurate than Gregorian calendar, but harder to work out which years are leap years.\nBackground[edit | edit source]\nPersians have been keen on the idea and importance of having a calendar system throughout their recorded history. They were among the first cultures to employ a solar calendar, and have long favored a solar approach rather than lunar or lunisolar models. In general, the sun has always had an important symbolic significance in the Iranian culture.\nAfter a first try by the second Persian parliament on February 21, 1911 which mandated the use of the solar years and months for official governmental use, the present Iranian calendar was legally adopted by the Persian parliament on March 31, 1925, specifying the origin on the calendar (Hegira of Muhammad from Mecca to Medina in 622 CE), mentioning that the beginning of the year is the first day of spring, that the year is the ""true solar"" year ""as it has been"" (کماکان), and specifying the month names and the number of days in each month. The law goes further and officially deprecates the 12 year cycles of the Chinese-Uighur calendar which were unofficially but commonly used.\nAfghanistan legally adopted the new Iranian calendar in 1957, using the same number of days in each month but different month names. In Afghan Persian (also known as Dari), the Arabic language names of the zodiac signs for the months are used instead of the names adopted in Iran in 1925. (These zodiac names were also used in Iran before 1925.) In Afghan Pashto, native Pashto names of the zodiac signs are used.\nHistory of calendars in Persia[edit | edit source]\nThe first calendars based on Zoroastrian cosmology appeared during the later Achaemenian period (650 to 330 BCE) and though they have evolved and changed over the centuries, the names of the months have remained more or less the same till now. Before this period, old Persian inscriptions and tablets indicate that early Iranians used a 360-day calendar based on the Babylonian system modified according to their own beliefs, and their own named days. Months were divided into two or three divisions depending on the phase of the moon. Twelve months were named for various festivals or activities of the pastoral year with 30 days in each month. A thirteenth month every six years was added to keep the 360-day calendar in harmony with the seasons.\nUnder the unified empire of the Achaemenians it was necessary to create a distinctive Iranian calendar, and one was devised based on the Egyptian tradition, with twelve months of thirty days, each dedicated to a yazata (Eyzad), and with four divisions resembling the Semitic week. Four of the days in the month were dedicated to Ahura Mazda and seven days were named after the six Amesha Spentas. Thirteen days were named after Fire, the Waters, Sun, Moon, Tiri and Geush Urvan (the soul of all animals), Mithra, Sraosha (Soroush, yazata of prayer), Rashnu (the Judge), Fravashi, Bahram (yazata of victory), Raman (Ramesh meaning peace), and Vata, the divinity of the wind. Three were dedicated to the female divinities, Daena (yazata of religion and personified conscious), Ashi (yazata of fortune) and Arshtat (justice). The remaining four were dedicated to Asman (lord of sky or Heaven), Zam (earth), Manthra Spenta (the Bounteous Sacred Word) and Anaghra Raocha (the \'Endless Light\' of paradise).\nThe calendar had a significant impact on religious observations. Not only did it fix the pantheon of major divinities, but ensured that their names were continually uttered, since at every Zoroastrian act of worship the yazatas of both day and month were invoked. With the new system, the pattern of festivities became clear as well. For example, Mitrakanna or Mehregan was celebrated on Mithra day of Mithra month, and the Tiri festival (Tiragan) was celebrated on Tiri day of the Tiri month.\nAfter the conquests of Alexander the Great and his subsequent death, the Persian territories fell to one of his generals Seleucus (312 BCE) and the Seleucid dynasty of Iran was formed. Based on the Greek tradition, they introduced the practice of dating by era rather than dating by the reign of the individual kings. Their era became known as that of Alexander, or later the Seleucid era. The Zoroastrian priests, having lost their function at the royal courts since the new rulers were not Zoroastrians, resented the Seleucids. Although they followed the new trend of dating by eras, they established their own era of Zoroaster.\nThis marked the first serious attempt to determine the dates associated with the prophet Zoroaster\'s life. With no Zoroastrian historical sources they turned to Babylonian archives famous throughout the ancient world. From these records they learned that a great event in Persian history took place 228 years before the era of Alexander. The date was 539 BCE, which was in fact the conquest of Babylon by Cyrus the Great. The Zoroastrian priests, however, misinterpreted this date to be the time the ""true faith"" was revealed to their prophet, and since Avestan literature indicates that revelation happened when Zoroaster was thirty years old, 568 BCE was taken as his year of birth. The date entered written records as the beginning of the era of Zoroaster, and indeed, the Persian Empire. This incorrect date is still mentioned in many current encyclopedias as Zoroaster’s birth date.\nThe Parthians (Arsacid dynasty) adopted the same calendar system with minor modifications, and dated their era from 248 BCE, the date they succeeded the Seleucids. Their names for the months and days are Parthian equivalents of the Avestan ones used previously, differing slightly from the Middle Persian names used by the Sassanians. For example in Achaemenian times the modern Persian month ‘Day’ was called Dadvah (Creator), in Parthian it was Datush and the Sassanians named it Dadv/Dai (Dadar in Pahlavi).\nThe next major calendar change happened during the reign of Ardashir I the founder of the Sassanid dynasty in 224 CE. In 46 BCE, Julius Caesar adopted the Egyptian solar calendar system of 365 days with some modifications. Iranians had known about the Egyptian system for centuries but never used it. Ardashir, however, changed the length of the calendar year to 365 days by adding five extra days at the end, and named these ‘Gatha’ or ‘Gah’ days, after the ancient Zoroastrian hymns of the same name. The new system created confusion and was met with resistance, and many Zoroastrian feasts and celebrations have two dates, to this day. Many rites were practiced over many days instead of one day and duplication of observances was continued to make sure no holy days were missed.\nThe situation became so complicated that another calendar reform had to be implemented by Ardeshir’s grandson Hormizd I. The new and old holy days were linked together to form continual six-day feasts. No Ruz was an exception as the first and the sixth day of the month were celebrated separately, with the sixth becoming more significant as Zoroasters’ birthday rather than a continuation of No Ruz itself. The reform however did not solve all the problems, and Yazdgerd III, the last ruler, introduced the final changes. The year 631 CE was chosen as the beginning of a new era, and this last imperial Persian calendar is known as the Yazdgerdi calendar. However, before work on the new calendar was completed, Muslim Arabs overthrew the dynasty in the 7th century and with their victory, a new lunar calendar based on Islamic principles replaced the old solar calendar of the Sassanid period.\nThe Islamic calendar was outlined in the prophet Muhammad\'s revelation, the Qu\'ran, and in his last sermon during his farewell pilgrimage to Mecca. It was the same as the old pagan Meccan calendar except that the intercalary month was eliminated, effective at the end of AH 10 (March 632 CE). Umar, the second caliph, began numbering its years in AH 17 (638 CE), regarding its first year as the year during which Muhammad\'s Hijra (emmigration) from Mecca to Medina occurred, in September 622 CE. The first day of the year was not changed—it continued to be the first day of Muharram. Years of the Islamic calendar are designated AH from the Latin Anno Hegirae (in the year of the Hijra).\nThe Iranian calendar was revised in the 11th century by a panel of scientists, allegedly including Omar Khayyám. The recalibration was completed during the reign of Jalaal ad-Din Malik Shah Seljuki, one of the Seljuk sultans, and named in his honor.\nThe Islamic lunar calendar was widely used till the end of the 19th century. During the early Pahlavi era in 1925, the lunar calendar was officially replaced by the modern Iranian calendar. The act of 1925 mentioned that ""the true solar year"" should be used for computing the first day of the year, and also fixed the number of days in each month (which was previously different in each year, corresponding with the tropical zodiac). It also revived the ancient Persian names, which are still in use today.\nDetails[edit | edit source]\nThe Iranian calendar year begins on the midnight between the two consecutive solar noons which include the instant of the Northern spring equinox, when the sun enters the northern hemisphere; in other words, the start of Spring in the northern hemisphere. The calendar consists of twelve months with Persian names. The first six months are 31 days each, the next five 30 days, and the last month has 29 days but 30 days in leap years. The reason the first six months have 31 days and the rest 30 was not a random decision by the designers – it has to do with the fact that the sun moves slightly more slowly along ecliptic in the northern spring and summer than in the northern autumn and winter. It should also be noted that before the adaptation of the modern Persian calendar in 1925 (1304 AP), the length of the months were different each year, and a month could also consist of 32 days. For example, the length of the Persian months in the year 1303 AP were respectively 30, 31, 32, 31, 32, 30, 31, 30, 29, 30, 29, and 30 days, while the length of the months in 1302 AP were 30, 31, 32, 31, 31, 31, 31, 29, 30, 29, 30, and 30 days.\nIn other words, the Persian new year is determined by noon-time observation of the Northern spring equinox. If between two consecutive noons the sun\'s altitude rises through its equinoctial altitude then the first noon is on the last day of one calendar year and the second noon is on the first day (Norouz) of the next calendar year.\nTypically leap years are devised and used by various solar calendar systems, usually every four years. Four-year leap years add 0.25 day to each year in the period, but that is a slight overcompensation compared to the actual behaviour of the sun. To remedy this overcompensation, after about every seven four-year leap year intervals, the Persian solar calendar produces a five-year leap year interval. It usually follows a thirty-three year cycle with occasional interruptions by single twenty-nine year or thirty-seven year subcycles.\nThis general picture of the Persian calendar\'s leap-year behaviour contrasts with less accurate predictive algorithms which are based on confusion between the astronomers average tropical year (365.2422 days, approximated with near 128-year cycles or 2820-year great cycles) and the mean interval between spring equinoxes (365.2424 days, approximated with a near 33-year cycle).\nMonth Names[edit | edit source]\n|Order||Days||Persian name||Kurdish name||Afghan name\n(Arabic translation of Zodiac signs)\n|Afghan Pashto name|\n|4||31||Tir||تیر||Poshper||پووش په ر||Saratan||سرطان||tʃungaʂ||چنګاښ|\n|7||30||Mehr||مهر||Rezber||ره زه به ر||Mizan||میزان||Təla||تله|\n|8||30||Aban||آبان||Gelarêzan||گه لا ريژان||Aqrab||عقرب||Laɻam||لړم|\n|9||30||Azar||آذر||Sermawez||سه ر ما وه ز||Qaws||قوس||lindəy||لیندۍ|\n|12||29/30||Esfand||اسفند||Resheme||ره شه مه||Hout||حوت||kab||کب|\nThe first day of the calendar year is also the day of the greatest festival of the year in Iran and its surrounding regions, called Norouz (a single word made up of two parts, no (new) and rouz (day), meaning ""new day"").\nDays of the week in Iranian calendar[edit | edit source]\nIn Iranian Calendar, every week begins on Saturday and ends on Friday. Names of the days of the week are as follows:\nShanbeh (شنبه in Persian) equivalent to Saturday.\nYekshanbeh (یکشنبه in Persian) equivalent to Sunday.\nDoshanbeh (دوشنبه in Persian) equivalent to Monday.\nSeshanbeh (سه شنبه in Persian) equivalent to Tuesday.\nChaharshanbeh (چهارشنبه in Persian) equivalent to Wednesday.\nPanjshanbeh (پنجشنبه in Persian) equivalent to Thursday.\nJom\'eh (جمعه in Persian, originally Arabic) or Adineh (آدینه in Persian) equivalent to Friday.\nIn most Islamic countries, Jom\'eh is the holiday.\nCalendar seasonal error[edit | edit source]\nIn the 11th century, a team of astronomers, allegedly including Omar Khayyam, proposed certain rules. While the details of the exact rule is debated, some claim that it inserted 8 leap days in every cycle of 33 years (different rules, such as the 2820-year cycle have also been accredited to Omar Khayyam). This replaced a previously common calendar that had a leap day every four years, and was adopted by Jalaal ad-Din Malik Shah Seljuki and became known as the Jalaali calendar.\nThis image shows the difference between the Iranian calendar (using the 33-year arithmetic approximation) and the seasons. The Y axis is ""days error"" and the X axis is Gregorian calendar years.\nEach point represents a single date on a given year. The error shifts by about 1/4 day per year, and is corrected by a leap year every 4th year regularly, and one 5 year leap period to complete a 33-year cycle. One can notice a gradual shift upwards over the 500 years shown.\nCalculating the day of the week[edit | edit source]\nCalculating the day of the week is easy. You just need an anchor date to start with. One good day to choose is Sunday, 1 Farvardin 1372, which equals 21 March 1993.\nAssuming the 33-year cycle approximation, to jump ahead by one 33-year cycle: move back by one weekday. Similarly, to jump back by one 33-year cycle, move ahead by one weekday.\nAs in the Gregorian calendar, dates move forward exactly one day of the week with each passing year, except if there is an intervening leap day. The leap day will make the date move an additional day forward. The chosen anchor date (1 Farvardin 1372) is chosen so that its 4th, 8th, ..., 32nd anniversaries come immediately after leap days, yet the anchor date itself does not immediately follow a leap day.\nSee also[edit | edit source]\n[edit | edit source]\n- An online Jalali(shamsi)/Gregorian/Islamic(hijri) Date Convertor\n- The Persian Calendar : A simple explanation\n- The Persian Calendar : How the leap years are calculated\n- System.Globalization.PersianCalendar class documentation in MSDN Library (Microsoft .Net 2.0 class for Persian Calendar calculation and conversions from/to Gregorian Calendar.)\n- An Interactive Iranian Calendar\n- The Zoroastrian Calendar']"	['<urn:uuid:d563e522-bafb-49da-acb2-48fc24fbe1c7>', '<urn:uuid:d5ac10ec-1081-400e-941e-cd2a2fd63dbf>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	36	99	3186
44	german harvest beer origins and modern yeast strains needed make authentic	Traditional German harvest beer (Erntebier) was a top-fermenting beer from northern Germany, particularly Lower Saxony. It was very dark in color, highly hopped, and lagered for several months at cool temperatures, with an original gravity of 1049 to 1057. For an authentic recreation, you would need a yeast strain that produces clean, malty characteristics with some ester production. Based on the geographic proximity to Düsseldorf, where similar beers were produced, a good choice would be yeast strains suitable for German-style ales like Kölsch and Alt beers, which produce a clean profile with a slight sulfur component that reduces with aging.	"['It’s Time for Harvest Beer!\nFor millennia, we were primarily an agricultural people. The ebb and flow of the seasons determined much of the rhythm of human culture. In our developed world, agriculture still occupies a lot of land, but the space it occupies in our collective psyche has shrunk almost to nothing.\nIn the urban world, it’s easy to forget that agriculture is a dicey, exhausting and often tedious business. Success is never assured, and no matter what the effort, disaster always looms. Getting the last kernel of grain into the barn is the triumphal culmination of a whole year’s worth of worry and sweat. Such a supremely elating moment deserves celebration, and what could be better than a well-brewed beer to preside over the harvest festivities. As one old poem goes: “And free beer clearly/ like no other removes the thirst.”\nOriginally, all beer was seasonal. Technology had yet to triumph over nature, and brewers were forced to adapt their beers and brewing practices to suit the season at hand. In many times and places, the load of bacteria in the warm air prohibited brewing in summer. This meant that the autumn’s beer must be brewed in the spring and stored until needed. Fortuitously, brewers found they could use the remainder of last year’s malt and hops to make their beer beefy enough to stand up to a summer’s worth of aging.\nIn the fall, a number of factors come into play: the well-earned need to celebrate the harvest; the crisp, cool days; and a general feeling of plenty, so well symbolized by the cornucopia. A harvest beer filled the bill, as well as the mug. Such beers were iconic and much beloved, as evidenced in an East Prussian poem by Ernst Gardey:\nO wonderful harvest beer,\nYou full celebration of freedom and desire!\nBecause of your beer tap, humans\nAre weaned as babies from the breast!\nToday, we are all familiar with Oktoberfest, or märzen beer, brewed in March for celebratory consumption in the fall. There is, however, another brewing tradition that has now all but faded—Erntebier, which translates simply as “harvest beer.” This is undoubtedly a very old and highly localized product, with brewers each producing their own variant.\nBy the end of the 19th century in northern Germany, erntebier had a somewhat cohesive stylistic identity, at least in Lower Saxony. Max Delbrück, in his 1910 Illustriertes Brauerei-Lexicon, describes erntebier as being a top-fermenting beer of 1049 to 1057 original gravity (12 to 14 degrees Plato), “very dark” in color, highly hopped, and lagered for several months at cool temperatures.\nThis sounds a lot like Düsseldorf altbier or the famous seasonal specialty, Sticke. The latter is simply a stronger version of regular altbier, brewed and served twice a year as a thank you to the breweries’ regular customers.\nIt is possible that erntebier is similarly just a dolled-up version of the regular small beer. We may never know for sure, but references to erntebier invariably describe it as “strong,” even at the gravities shown above, so that seems like a reasonable possibility. Geographically, Lower Saxony is just to the north of the North Rhine-Westphalia state where Düsseldorf is situated, so it would not be surprising to find similarities between alt and erntebier.\nHowever beloved this beer was at one time, a German book on top-fermenting beers (Schönfeld) in 1938 makes no mention of erntebier, so we can assume it had pretty much vanished by that time. Or at least the name had.\nModern-day interpretations of erntebier have popped up in several places in Germany, but interesting as this trend is, the beers have little to do with the historical style described by Professor Delbrück. In Franconia, northern Bavaria, Braugasthof Grosch brews a light version at 3.3 percent alcohol by volume. The Alte Klosterbrauerei of Vierzehnheilegen (also in Franconia) makes a “three-grain” pale lager of similar weight; Schlössbrauerei Maxlrainer in southeast Bavaria brews an even lighter one at 2.3 percent ABV. Surely these weak beers are nobody’s muse. Vogel, in Karlsruhe, in Baden-Württemberg brews a 5.3 percent golden lager under the erntebier name.\nSo, with no help from the modern erntebier brewers, we’re left with having to make an informed guess as far as a 19th century recipe goes. In general, dark beers of old tended to use a larger amount of colored malt rather than all pale (or pils) malt, tinted by a small amount of very dark malt.\nPersonally, I find that the old way gives a fuller, more complex flavor and aroma, so we’ll be using a good deal of Munich malt. We will also be using a small amount of German röstmalz, a very smooth type of chocolate malt, just to give it a slight roasty kick. Hops in the 19th century would have been the traditional sort, so let’s employ the soft, smooth bitterness of Hallertau Hersbrucker hops.', 'You will find descriptions of all strains available to professional/craft brewers on this page. To refine the listings, please use the strain finder bar to the right.\nLooking for Homebrew Strains? Go here.\nGood for Kölsch, Alt, and German style Pale Ales. Strong sulfur component will reduce with aging. Clean, but with more ester production than WLP029.\nFerments dry and flocculates very well. Produces a distinctive ester profile. Good choice for most English style ales including bitter, pale ale, porter, and brown ale.\nProduces a clean, malty beer. Pleasant ester character, can be described as ""bready."" Can ferment successfully, and clean, at higher temperatures. This yeast combines good flocculation with good attenuation.\nTraditional mixed yeast culture. British style character, slightly fruity, with a hint of sulfur production. This yeast can be used for many different beer styles. The most traditional choices would be English style ales including milds, bitters, porters, and English style stouts. North American style ales will also benefit from fermentation with WLP017. The beer will clear easily.\nFlavorful British style yeast. Drier finish than many British ale yeast. Produces slightly fruity and bready character. Good top fermenting yeast strain, is well suited for top cropping (collecting). This yeast is well suited for classic British milds, pale ales, bitters, and stouts. Does not flocculate as much as WLP002 and WLP005.\nThis yeast produces a beer that is malty, but well-balanced. Expect flavors that are toasty with malt-driven esters. Highly flocculent and good choice for English pale ales, English brown ales, and mild ales.\nBritish style ale yeast with a very dry finish. Medium to low fruit and fusel alcohol production. Good top fermenting yeast strain, is well suited for top cropping (collecting). This yeast is well suited for pale ales, ambers, porters, and stouts.\nClean strain that complements malt flavor. Low to moderate esters, when fermentation temperature is below 70°F. Moderate plus ester character over 70°F. Low diacetyl production. Good yeast strain for Biere de Garde, blond, amber, brown ales, and specialty beers.\nLess phenolic than WLP400, and more spicy. Will leave a bit more sweetness, and flocculation is higher than WLP400. Use to produce Belgian Wit, spiced Ales, wheat Ales, and specialty Beers.\nNotice to brewers: Tends to take a long time to start; brewers should plan this into their brewing schedule. Needs heavy aeration and nutrients. Allow temperature to free rise.\nClean, almost lager like Belgian type ale yeast. Good for Belgian type pales ales and amber ales, or with blends to combine with other Belgian type yeast strains. Biscuity, ale like aroma present. Hop flavors and bitterness are accentuated. Slight sulfur will be produced during fermentation, which can give the yeast a lager like flavor profile.\nAn authentic Trappist style yeast. Use for Belgian style ales, dubbels, tripples, and specialty beers. Fruit character is medium, in between WLP500 (high) and WLP530 (low).\nNote: This strain benefits from extra oxygenation\nProduces beer with a high fruit ester characteristic, as well as some slight tartness. Finishes slightly malty, which balances out the esters. Also produces low levels of clovey phenolics. Great yeast choice for a summer Saison that is light and easy-drinking.\nThis yeast is famous for its clean flavors, balance and ability to be used in almost any style of ale. It accentuates the hop flavors and is extremely versatile.\nFor use in rice-based fermentations. Traditional strain used in Ginjo-shu production because of the yeast’s development of high fragrance components. Also a fairly strong fermenter, but producing a foamless fermentation.\nClassic yeast from a famous Bavarian monastery. This strain develops a creamy, malty beer profile with low sulfur production and low esters. It is a great choice for styles like traditional Helles, Oktoberfest, Bock, and Dunkel.\nThis yeast helps to produce a malty, but balanced traditional Munich-style lager. Clean and strong fermenter, it\'s great for a variety of lager styles ranging from Helles to Rauchbier.\nSwiss style lager yeast. With proper care, this yeast can be used to produce lager beer over 11% ABV. Sulfur and diacetyl production is minimal. Original culture provided to White Labs by Marc Sedam.\nFrom Southern Germany, this yeast finishes malty with a slight ester profile. Use in beers such as Oktoberfest, Bock, and Dark Lagers.']"	['<urn:uuid:17a29147-c221-4c01-b74c-cfeea02dc9cc>', '<urn:uuid:f0584259-97b2-433d-abac-602933ed471d>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:32:05.666290	11	100	1521
45	raising sheep cattle together difference profit grazing habits fencing requirements multispecies management	When raising sheep and cattle together, there are several key aspects to consider. In terms of profit, sheep are generally more profitable per acre than cattle, with potential income differences of over $1,000 more per animal unit for sheep. Regarding grazing habits, sheep prefer a diet of 40% grass, 40% forbs, and 20% browse, while cattle prefer 60% grass, 20% forbs, and 20% browse. For fencing, cattle can be contained with non-electrified barbed or woven wire, while sheep typically require woven wire perimeter fencing or electric nets. Multi-species grazing can improve pasture utilization and enhance parasite control, potentially eliminating the need for chemical dewormers in some areas.	['If you have grazing available on your farm, you are probably thinking about getting into sheep or cattle. Both grow well on grass, are popular choices for both small and large farms and have the potential to produce an income.\nWhich one should you pick, will sheep or cattle will bring you more money for your farm?\nGenerally, sheep are significantly more profitable per acre than cattle. This depends upon the prices of both sheep and cattle in your area, if your area is appropriate or too harsh for sheep and if you are willing to put in more management time for the sheep, as compared with cattle.\nIn my area, north central Ohio, sheep are easily more profitable than cattle. Here are some numbers to show you why:\n|Market Animal Sold|\n(one year’s worth of production)\n|Price per animal unit|\n(1,000 pounds mom equivalent)\n|500 pound steer @ $1.25 per pound||$625||$625 (one calf)|\n|79 pound lamb @ $2.97 per pound||$234.63||$1,642.41 (7 lambs)|\n|income difference||$1,017.41 more for sheep per animal unit|\nThese prices are from this week’s (as of writing) market report from Mt. Hope Auction.\nThe lamb prices are what we got for the larger group of lambs that we sent. The price for the steer is from the market report linked in the table. Both prices are middle of the price range for that weight group.\nRaising Sheep For Profit is an article I wrote to show you the budget for raising sheep, with current prices (for my area), including how to find prices for your area, so you can see what sheep can do for you.\nSheep are more profitable than cattle, per animal unit (1,000 pounds)\nSheep are more profitable than cattle, if you are comparing by animal unit.\nIf you are just comparing money from sheep per acre to money from cattle per acre then the answer is: raise sheep. You get more income from the pounds of lamb raised on the same acreage as the pounds of calf raised.\nThis is what the chart is showing. One animal unit is 1,000 pounds of grazing animal, which in this case is one cow or 5 sheep.\nAnimal units are a way to make things even (or close to even) so you can compare likely results between species (sheep vs cattle) or age groups (feeders vs brood cows), based off of forage per acre.\nThe cow has one calf per year, this is where the 500 pound steer comes in. The 5 sheep have an average of 7.5 lambs, 150% of number of ewes, but since you can’t have .5 of a lamb, I went with 7 lambs per animal unit for the sheep.\nIf all things are equal, you get $1,017.41 more per animal unit from sheep than you do from cattle.\nBut…all things are never equal, so let’s get into a few of the things you should consider before making a decision on sheep vs cattle for your farm.\nManagement is more demanding for sheep than cattle\nSheep take more management than cattle, by that I mean observation time by the shepherd. Observation is far more important with sheep, since sheep seem to have less ability to rally back to health than cattle.\nThe key with sheep is timeliness, you must be “on top of it” with sheep, lagging behind in care, like deworming, seems to lead to problems in sheep faster than in cattle.\nI don’t mean to say sheep are overly needy, they are not, but they do tend to require more attention from you than the cattle herd that would be on the same amount of land.\nThe other aspect of management that is often overlooked is that sheep are just less well known than cattle, which means you have less vet and medicinal support with sheep (or goats) than the vet knowledge and meds for cattle.\nOn the plus side, interest in sheep (and goats) is growing rapidly, since folks with smaller acreages are realizing that they can raise livestock, too.\nSheep knowledge and support businesses are growing, but as of today, it is still easier to get help with cattle than sheep, since both neighbors and vets are more likely to have experience with cattle rather than sheep.\nSelling price for lambs or calves is based on your area\nThis is one that may come as a surprise to you, especially if you are new to livestock. Not all areas will have the same demand for what you raise. This applies to cattle or sheep.\nHow Much Will My Lambs Sell For? will show you how to figure up the likely prices for lambs in your area based on auction prices. Even if you are selling privately, you should know the current market prices for your area.\nIn our area, I would say there is actually a higher demand for sheep rather than cattle, despite this being a predominantly dairy cattle raising area up until the past decade or so.\nOur area has lots of smaller Amish farmers who are looking for something to eat the grass then sell for a bit of extra income, this is where the sheep come in and why sheep are doing well at our local auction.\nSheep are just more small acreage friendly than cattle.\nWhat is the trend in your area? If most folks selling at the auction are commercial beef farmers with tons of cattle and acreage, chances are your local market for sheep isn’t so great.\nWhy? Not many sheep are showing up, so the sheep buyers don’t show up either. If this is the case in your area, you’ll need to either figure out how to privately sell lamb to customers or ship your stock to a more sheep friendly auction.\nIf transport to an auction is an unreasonable distance, you need to have a plan for those sheep before you get them or be willing to sell custom raised lambs or ship packaged lamb directly to customers.\nPlease take some time to figure this out! Raising sheep without a reliable sales outlet is a recipe for disaster, not to mention a huge drain on your wallet!\nHow To Read A Cattle Market Report shows you how to figure up cattle prices for your area.\nFacilities are less demanding for sheep than cattle\nAfter the biggies of management and selling your sheep or cattle, working facilities will be a minor point, but it’s worth mentioning.\nIf you just have a few sheep, they can be corralled and worked in the corner of the barn with a hog panel or a couple of small wooden gates. Sure sheep can be hard on facilities, at times, but they are nothing compared to cattle.\nIf you are going to raise cattle, you need a sturdy working area with a few pens, at the very least.\nEven if you do near no vet type work with the cattle, you still need a cattle tough place to sort off a steer or two, or pen up a cow for the vet.\nOf course, a working area is simple to set up, but will need to be tough, so it will be more costly than a working area that will be fine for sheep.\nSheep and cattle can be raised together\nSheep and cattle can be raised together, if you want to have both, and have the grass to do it.\nThis will not change the profit per acre of sheep or cattle, but you can quite often “double up” by multi species grazing with little increase in costs, which will get you more total animals raised per acre for your farm.\n5 Animals That Can Be Raised With Cattle goes over some good combinations, including sheep, and what you need to take into consideration for all (you, the cattle and the sheep) to be happy.', 'How are paddock systems designed for sheep different from those for cows?\nAnswer: Most of the differences between paddock systems for sheep and cattle will be based on diet preferences, pasture composition, fencing, and grazing habits. Sheep tend to browse, preferring forbs over grass while cattle diets consist primarily of grass. Sheep have a higher preference for leafy forage over stemmy ones, when compared to cattle. For both species, the best pastures usually contain a mixture of grasses and legumes. It is recommended to have more than one species of grass and legumes in your pasture. It is important to remember that each paddock needs water and shelter.Fencing for the two species is accomplished differently. Perimeter fencing is most commonly permanent fence, either electric or unpowered. Cattle can be fenced with non-electrified barbed or woven wire. Perimeter fencing for sheep or multi-species normally requires woven wire. Temporary fencing of pasture paddocks for cattle can be accomplished through the use of a single line of polywire and “tread in” temporary posts. While some graziers are successful in fencing sheep by this method, most are not. It all depends on the stocking density (number of animals per acre) and how well the sheep are trained to the electric fence. Alternatively, electric nets effectively keep sheep and goats in and predators out. The electric nets can also be moved very quickly. As with all powered fence systems, an adequately sized fence energizer and a well-constructed fence are paramount to your success. More information on fencing techniques and stock-watering systems can be found in the ATTRA publications Paddock Design, Fencing and Water Systems for Controlled Grazing, available at https://attra.ncat.org/attra-pub/summaries/summary.php?pub=249, and Rotational Grazing, available at https://attra.ncat.org/attra-pub/summaries/summary.php?pub=245.Diet Preferences of Sheep vs. Cattle Sheep prefer a forage diet of 40% grass, 40% forbs, and 20% browse, while cattle prefer a forage diet of 60% grass, 20% forbs, and 20% browse.The stocking rate is the number of a specific kind and class of animals grazing a unit of land for a specified time period. The carrying capacity is the maximum stocking rate possible while maintaining or improving vegetation or related sources. Both are often expressed as Animal Unit Months (AUM). Definition of Animal Unit (AU): 1,000 pounds of body weight Definition of Animal Unit Month (AUM): Amount of forage that an animal unit will consume in one month. The stocking rate for your paddock will depend on animal species, quality and quantity of forage (total available forage), and animal demand for forage. Therefore, the stocking rate for sheep and cattle will differ. Multi-species grazing (cattle and sheep) is an excellent management strategy. Not only is pasture utilization improved, parasite control is enhanced to the point that in many areas of the country, chemical dewormers are not neccessary. This is especially true if the grazing management techniques of adequate pasture rest, residual management, and short paddock grazing periods are employed.For additional information on pasture design and utilization, consult the following ATTRA publications: Pasture, Rangeland and Grazing Managementhttps://attra.ncat.org/attra-pub/summaries/summary.php?pub=246Irrigated Pastures: Setting Up an Intensive Grazing System that Workshttps://attra.ncat.org/attra-pub/summaries/summary.php?pub=449']	['<urn:uuid:5692f399-1a23-4f11-be24-43cef77e3934>', '<urn:uuid:71ac4d64-9b82-4a8a-9e6f-af865bfc6a9b>']	open-ended	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-12T10:32:05.666290	12	107	1818
46	method build theater audience before launch	Instead of opening shows and waiting months for public awareness, the recommended approach is to build an audience early on before opening, similar to Hollywood's strategy. This helps ensure seats are filled rather than having half-full houses in the initial months.	"['""I have a sense that Broadway hasn\'t entered into the 21st century,"" Star Trek legend George Takei, whose musical Allegiance is now Broadway-bound, said on stage at TEDxBroadway. ""Broadway hasn\'t boldly gone where it needs to.""\nLike many presenters at the event, held for the second year at New World Stages on January 28, Takei focused on how technology can improve Broadway. The epic Twitter and Facebook user, who said his following grows by ""40, 000 new friends a week,"" sold out the premiere of Allegiance, at San Diego\'s Old Globe Theatre, by growing his social media audience long before the production opened. The show broke every box office record in The Old Globe\'s 77-year history, and Takei now has more than 558, 000 Twitter followers and 3.4 million Facebook page ""likes."" He argued that Broadway needs to get with the times, and follow suit.\n""Broadway [will be] at its best when it embraces all of the technological advances of the times, especially on social media,"" Takei said. ""Then Broadway will live long and prosper.""\nAfter Takei left the stage, took photos with adoring fans, and entertained a group of visiting college students, TheaterMania cut in to chat about how viral cat photos relate to Broadway sales.\nHow can the rest of Broadway cultivate the kind of social media audience you have?\nWell, [I] started from scratch too. Well, mostly from scratch: I had a core base of sci-fi geeks and nerds. You find whatever you have that can engage a group of people in a conversation. You know, Star Trek and sci-fi are totally unrelated to what we were putting together [Allegiance, which is set during the Japanese American internment], but you find what it is that you do have, and establish that. Then you start growing by expanding the topics you\'re talking about. That\'s the amazing thing about social media. You begin anywhere, with whatever base you have… and eventually you start embracing and absorbing and swallowing other groups, until you have a large enough group to talk about what you really want to talk about. I now have a huge audience that knows about Allegiance.\nSo is it more about personality than the subject of the show?\nYou work with what you have, even if it\'s totally unrelated to what you\'re trying to sell. But it is [about] content, what you talk about. And I talked about Star Trek, to begin with, and then broadened it to science fiction. Then I discovered that funny posts, especially with cats, get a lot of likes and shares. I started talking about GLBT rights, and then the dark chapter in American history where [Allegiance] takes place, when my family was shipped across the country to Arkansas from Los Angeles and locked in a barbed wire prison for four long years for looking like the people that bombed Pearl Harbor. I now have a huge audience that knows about the internment, and that there is a show coming to Broadway about the internment. I mix that [serious content] with funny photos of cats.\nYou said in your talk that Broadway has not entered the 21st Century…\nI use the contrast between Hollywood and Broadway. Hollywood has a movie, but before they release it, they start building an audience for it, early on, so that when it is released, the audience is there. How many musicals do you go to – not West Side Story, but new musicals – that open and it is months before the general public becomes aware of it? They have it backwards. What you need to do is start getting and building an audience before you open…You try to get your audience eager, waiting, excited and anticipating, and then you open the show, so that you are filling your seats, rather than staggering through the first few months with a half-full house.\nA common theme presenters addressed today was accessibility, and the need to make Broadway available to more people, people who cannot afford current ticket prices. Do you have any ideas about how to achieve this?\nWhen I was a student, and absolutely enamored with the theater, living in Los Angeles, I made friends with the house manager at the Biltmore Theater, which was the place where all the Broadway shows stopped, and I asked [him], ‘Do you need an usher today?\' And he\'d say, ‘Yeah, and bring a couple of your friends with you.\' We\'d go and usher and see the show for free. But with the young people here, while you do have…these last-minute sales that they squat on the sidewalk for, there should be a more organized plan to make theater accessible for young people. One of the speakers today [David Sabel] talked about making a theater performance at the National Theatre something you can watch on YouTube, or right here"" [Points to phone]. So there are many schemes that could make it more accessible to all.\nNational Theatre Live is an interesting example because it was actually met with a lot of criticism. Critics even said that live broadcasting would be the death of theater.\nYou know, there are always complainers and snipers, but then there\'s the people who haven\'t complained, who got access to it, and were thrilled by it. You can\'t make your decisions based on the naysayers. Think of the large group of people that benefit and put the naysayers in perspective with that.\nShare via Email\nDon\'t show this again.']"	['<urn:uuid:930bb96f-1ceb-435b-84f0-b30094362724>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	6	41	912
47	How did Jean-Pierre Dorléac and Irene Lentz impact costume design in films?	Jean-Pierre Dorléac and Irene Lentz both made significant contributions to costume design, though in different eras. Dorléac won an Emmy for Battlestar Galactica and was nominated for four Emmys for Quantum Leap, where he masterfully recreated four decades of fashion and created futuristic styles. His costume work was so effective it is now studied in university classes. Irene Lentz, working earlier in Hollywood, designed for major studios like MGM and earned two Academy Award nominations. She was known for creating the dressmaker suit in the late 1930s and designed costumes for numerous leading ladies including Ginger Rogers, Carole Lombard, and Doris Day.	"['Tripping the Couture Fantastic with Jean-Pierre Dorléac\nHe imaginatively dressed up Lorne Greene in TV’s Battlestar Galactica and artfully dressed down Brooke Shields in the cult classic film The Blue Lagoon. But Quantum Leap fans will best remember costume designer Jean-Pierre Dorléac for giving their favorite show its most iconic image, and for giving their favorite hologram his flamboyant style.\nIn our post Mad Men, nostalgia-driven media culture, it’s hard to remember a time when costumes weren’t so much wistful distractions to be blogged about as they were tools to transport viewers into another time and place. And when it comes to embodying eras gone by, Don Draper ain’t got nothing on Sam Beckett.\nAnd that’s thanks to Dorléac. In his series-long tenure as costume designer for Quantum Leap, Dorléac had one of the most important jobs on show. When Sam Leapt into a new time and place, his first clue as to who and where he was was often his outfit. It was a crucial, visual shorthand for both the character and the viewers—one that required Dorléac to reliably recreate four decades worth of fashions.\nHe rose to the challenge brilliantly; whether tracking a dropper named Clapper as noirish Nick Allen in “Play it Again, Seymour,” fighting sexism as a stylishly-dressed Samantha Stormer in “What Price Gloria” or romancing on the high seas as wealthy playboy Philip Dumont in “Sea Bride,” Sam convincingly inhabited any era a story required. And Dorléac made it look so natural and effortless that he was nominated for four Emmys for his work on Quantum Leap (including for the aforementioned “Sea Bride”). In fact, his QL period costuming was so effective that it is now studied in university classes.\nBut not only did Dorléac have to faithfully recreate the past. He also had to give viewers a glimpse of the future. And his fashion choices have given Quantum Leap its signature genre style.\nThe designer’s futuristic flourishes take front and center in the show’s very first scenes. Al’s neon star lapel pin and matching shoe appliques are prominently featured in the tease for “Genesis,” as are Tina’s LED high heels and earrings. And while we’re still grinning over this retro-future chic, Dorléac hits us with Quantum Leap’s most iconic image: Sam, in his clean, white Fermi suit, arms outstretched, being buffeted by quantum energy in the Accelerator Chamber as he prepares for his first Leap.\nFashion, both real and imagined, was critical to the success of Quantum Leap. And fashion would remain the show’s primary tool to differentiate the past from the future, embodied mainly by Al.\nAl’s flamboyant fashion choices were a brilliant counterpoint to Sam’s historic mien. And while the metallic fabrics, funky cuts and garish hues may have started out as a visual gimmick to make the hologram an anachronistic standout in Sam’s pedestrian surroundings, they evolved into an abiding character statement for Al. For a man who spent five years in filthy black rags as a POW, Al’s colorful clothes proclaim that he remains unbroken, and that he’s full of joy and lust and a zest for life.\nQuantum Leap wasn’t the first time Dorléac recreated historical fashions for show creator Donald P. Bellisario. They started working together on Bellisario’s 1930’s era series Tales of the Gold Monkey. And Dorléac lent his futuristic vision to a parade of genre shows through the 1970s and 80s: Buck Rodgers in the 25th Century, Automan, The Greatest American Hero, Max Headroom, Knight Rider and the aforementioned Battlestar Galactica, for which he won his first Emmy. And his impressive roster of film work includes another time travel classic, Somewhere in Time, starring the late Christopher Reeve.\nDorléac has chronicled his storied career in his new memoir, The Naked Truth: An Irreverent Chronicle of Delirious Escapades. In it, the costume designer relates never-before-told, behind-the-scenes stories about working in theater, couture, television and film.\nAnd in the vein of those never-before-told stories, The Quantum Leap Podcast is proud to bring you Jean-Pierre Dorléac’s first-ever interview about his work on Quantum Leap. Listen as he recounts his time with Scott Bakula, Dean Stockwell and the unique challenges presented by Sam’s trips through time. Along the way he tells us more about The Naked Truth, and his passion to preserve historic Hollywood fashions.\nJoin us as we take this couturistic leap!\nSigned copies of The Naked Truth: An Irreverent Chronicle of Delirious Escapades, are available at Mr. Dorléac’s website jean-pierredorleac.com\nThe Naked Truth: An Irreverent Chronicle of Delirious Escapades is also available on Amazon.com and wherever good books are sold.', 'Irene (costume designer)\n|This article needs additional citations for verification. (April 2009) (Learn how and when to remove this template message)|\nDecember 8, 1901|\nBaker, Montana, U.S.\n|Died||November 15, 1962\nLos Angeles, California, U.S.\n|Cause of death||Suicide|\n|Resting place||Forest Lawn Memorial Park, Glendale|\n|Other names||Irene Gibbons\n|Known for||Designing costumes for motion picture actors|\n|Spouse(s)||F. Richard Jones, Elliot Gibbons|\nIrene Maud Lentz (December 8, 1901 – November 15, 1962)  also known mononymously and professionally as Irene, was an American fashion designer and costume designer. Her work as a clothing designer in Los Angeles led to her career as a costume designer for films in the 1930s. Lentz also worked under the name Irene Gibbons.\nBorn in Baker, Montana, to Emil Lents and Maud Walters, Lentz started out as an actress under her birth name, appearing in secondary roles in silent films beginning with Mack Sennett in 1921. She played ingenue parts opposite Sennett\'s leading comedians, Ben Turpin and Billy Bevan. Lentz was directed in her first film by Sennett\'s production chief, F. Richard Jones; their professional relationship matured into a personal one. They had been married for less than a year when Jones succumbed to tuberculosis in 1930.\nLentz had been taught sewing as a child and with a flair for style, she decided to open a small dress shop. The success of her designs in her tiny store eventually led to an offer from the Bullocks Wilshire luxury department store to design for their Ladies Custom Salon which catered to a wealthy clientele including a number of Hollywood stars.\nLentz\'s designs at Bullocks gained her much attention in the film community and she was contracted by independent production companies to design the wardrobe for some of their productions. Billing herself simply as ""Irene,"" her first work came in 1933 on the film Goldie Gets Along featuring her designs for star Lily Damita. However, her big break came when she was hired to create the gowns for Ginger Rogers for her 1937 film Shall We Dance with Fred Astaire. This was followed by more designs in another Ginger Rogers film as well as work for other independents such as Walter Wanger Productions, Hal Roach Studios as well as majors such as RKO, Paramount Pictures and Columbia Pictures. During the 1930s, Irene Lentz designed the film wardrobe for leading ladies such as Constance Bennett, Hedy Lamarr, Joan Bennett, Claudette Colbert, Carole Lombard, Ingrid Bergman, and Loretta Young among others. She ""is generally regarded as the originator of the dressmaker suit""  that was popular in the late 1930s.\nThrough her work, Lentz met and married short story author and screenwriter Eliot Gibbons, brother of multi-Academy Award winning Cedric Gibbons, head of art direction at MGM Studios. Despite her success, working under the powerful set designer Cedric while being married to his brother Eliot was not easy. Irene confided to her close friend Doris Day that the marriage to Eliot was not a happy one. Generally regarded as the most important and influential production designer in the history of American films, Cedric Gibbons hired Lentz when gown designer Adrian left MGM in 1941 to open his own fashion house. By 1943 she was a leading costume supervisor at MGM, earning international recognition for her ""soufflé creations"" and is remembered for her avant-garde wardrobe for Lana Turner in 1946\'s The Postman Always Rings Twice. In 1948, she was nominated for the Academy Award for Best Costume Design, Black-and-White for B.F.\'s Daughter.\nIn 1950, Lentz left MGM to open her own fashion house. After being out of the film industry for nearly ten years, in 1960, Doris Day requested her talents for the Universal Studios production Midnight Lace for which Lentz earned a second Academy Award nomination. The following year she did the costume design for another Doris Day film and during 1962 worked on her last production, A Gathering of Eagles.\nIn 1962, after Doris Day noticed that Lentz seemed upset and nervous, Lentz confided in her that she was in love with actor Gary Cooper and that he was the only man that she had ever loved.[page needed] Cooper had died in 1961.\nOn November 15, 1962, three weeks short of her sixty-first birthday, Lentz took room 1129 at the Knickerbocker Hotel, checking in under an assumed name. She jumped to her death from her bathroom window.\nShe had left suicide notes for friends and family, for her ailing husband, and for the hotel residents, apologizing for any inconvenience her death might cause. Per her wishes, she is interred next to her first husband, director F. Richard Jones, at the Forest Lawn Memorial Park Cemetery in Glendale, California.\nIn 2005, Irene Lentz was inducted into the Costume Designers Guild\'s Anne Cole Hall of Fame.\n|1933||Goldie Gets Along||Costume designer, uncredited|\n|1933||Flying Down to Rio||Costume designer, uncredited|\n|1937||Shall We Dance||Gowns for Ginger Rogers|\n|1937||Vogues of 1938||Gowns for Joan Bennett|\n|1938||You Can\'t Take It With You||Gowns for Jean Arthur|\n|1938||Topper Takes a Trip||Gowns for Constance Bennett|\n|1938||Vivacious Lady||Gowns for Ginger Rogers|\n|1939||In Name Only||Gowns for Carole Lombard|\n|1939||Intermezzo: A Love Story||Costume designer for Ingrid Bergman|\n|1939||Midnight||Gowns for Claudette Colbert|\n|1940||Green Hell||Gowns for Joan Bennett|\n|1940||Seven Sinners||Gowns for Marlene Dietrich|\n|1941||That Uncertain Feelings||Gowns for Merle Oberon|\n|1941||Mr. & Mrs. Smith||Gowns for Carole Lombard|\n|1941||To Be or Not to Be||Gowns for Carole Lombard|\n|1942||Take a Letter, Darling||Gowns for Rosalind Russell|\n|1942||You Were Never Lovelier||Gowns for Rita Hayworth|\n|1943||No Time for Love||Gowns for Claudette Colbert|\n|1943||Girl Crazy||Costume supervisor|\n|1944||Meet Me in St. Louis||Costume supervisor|\n|1944||Bathing Beauty||Costume supervisor|\n|1945||The Picture of Dorian Grey||Costume supervisor|\n|1945||Week-End at the Waldorf||Costume supervisor|\n|1946||Harvey Girls, TheThe Harvey Girls||Costume supervisor|\n|1946||Ziegfeld Follies||Costume designer/supervisor, uncredited|\n|1947||Lady in the Lake||Costume supervisor|\n|1947||Cass Timberlane||Costume designer|\n|1948||Easter Parade||Costume designer (women)|\n|1948||Pirate, TheThe Pirate||Costume supervisor|\n|1949||Barkleys of Broadway, TheThe Barkleys of Broadway||Costume designer|\n|1949||Neptune\'s Daughter||Costume designer|\n|1950||Shadow on the Wall||Costume designer|\n|1960||Midnight Lace||Gowns for Doris Day|\n|1961||Lover Come Back||Gown for Doris Day|\n|1963||Gathering of Eagles, AA Gathering of Eagles||Costume designer|\n- ""Irene"", in Suicide in the Entertainment Industry: An Encyclopedia of 840 Twentieth Century Cases, by David K. Frasier (McFarland, 2005) p156-157\n- Hall, Mary (March 23, 2009). ""Angelina Jolie\'s Costumes in The Tourist Pay Homage to MGM Fashion Designer Irene Lentz"". Huffington Post. Retrieved August 7, 2011.\n- In Day\'s autobiography, she wrote that in 1962, Irene ""had an unhappy marriage to a man who lived out of the state and only occasionally came to visit her.""\n- Day, Doris; Hotchner, A.E. (Oct 1976) . Doris Day: Her Own Story (Bantam mass market paperback) (6th printing ed.). New York: William Morrow. p. 237. ISBN 0-553-02888-X.\n- Day later wrote that she got the feeling that she was the first person to whom Irene had confided this information. She also wrote: ""Thinking about it now, I cannot honestly say whether Irene\'s love was one-sided or whether she and Cooper had actually had or were having an affair.""\n- Michelle Vogel (2012). McFarland, ed. Lupe V\'Lez: The Life and Career of Hollywood\'s Mexican Spitfire. p. 47. ISBN 978-0786461394.']"	['<urn:uuid:f50843b6-d322-40fc-998b-88990a735cbb>', '<urn:uuid:14ba064b-395c-47f5-bbee-49b222b6be6e>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	12	102	1928
48	manufactured glass silicate composition sand production process commercial plant	In a commercial glass plant, glass is manufactured by mixing sand with waste glass (from recycling), soda ash (sodium carbonate), and limestone (calcium carbonate), which is then heated in a furnace. The most common type is silicate glass, which is based on silica (silicon dioxide or quartz) - the primary constituent of sand. This results in a non-crystalline, amorphous solid that is often transparent and widely used in applications like window panes, tableware, and optoelectronics.	"['M-Sand (Manufactured Sand) :-. M-sand is manufactured. M Sand is an alternative to River sand. M Sand (manufactured sand) produced by crushing of hard granite stone. MSand is manufactured through the process of shaping cubically, grading and cleaning by using VSI machine. The size of Manufactured sand is less than 4.75mm.\nThe properties of engineering materials can be classified into the following main groups: physical and chemical. The physical properties can also be further grouped into categories: mechanical, thermal, electrical, magnetic, optical etc. The chemical properties include: environmental and chemical stability.\nSand Samples\' Preparation Using Mobile Pluviator. Authors; Authors and affiliations; Mahdy Khari ... Sand sample preparation—the slurry deposition method. Soils Found. 28(4), 107–118 ... A sample preparation method and its effect on static and cyclic deformation-strength properties of sand. Soils Found. 22(1), 61–77 (1982) Google ...\nUser Guidelines for Waste and Byproduct Materials in Pavement Construction [ Asphalt Concrete] [ Flowable Fill] ... Chemical Properties. Spent foundry sand consists primarily of silica sand, coated with a thin film of burnt carbon, residual binder (bentonite, sea coal, resins) and dust. ...\nTitanium processing, the extraction of titanium from its ores and the preparation of titanium alloys or compounds for use in various products. Titanium (Ti) is a soft, ductile, silvery gray metal with a melting point of 1,675 °C (3,047 °F).\npreparation of manufactured sand and its properties, process .... preparation of manufactured sand and its properties 38 Views. The Zenith is the professional mining equipments manufacturer in the world, located in China,India ...\nSULPHUR CONCRETE\'S TECHNOLOGY AND ITS APPLICATION TO THE BUILDING INDUSTRY ... – sand, C – additives, D – sulphur, 1 – compounds storehouse, 2, 3, 4 ... Process of sulphur concrete production is based on the sulphur\'s properties of changing its .\nSilica sand products are marketed in a wide range of grades, including extremely fine grades known as flours. The chemical compound silicon dioxide, also known as silica (from the Latin silex), is an oxide of silicon with a chemical formula of SiO2 and has been known for its hardness since antiquity.\nUses and properties. Titanium metal connects well with bone, so it has found surgical applications such as in joint replacements (especially hip joints) and tooth implants. The largest use of titanium is in the form of titanium (IV) oxide. It is extensively used as a pigment in house paint, artists\' paint, plastics, enamels and paper.\nIn fact, sand casting is one of the few processes that can be used for metals with high melting temperatures such as steels, nickel, and titanium. Usually sand used to manufacture a mold for the casting process is held together by a mixture of water and clay. A typical mixture by volume could be 89% sand, 4% water, 7% clay.\nDEPARTMENT OF HOUSING AND URBAN DEVELOPMENT Series and Series Number: Housing - Federal Housing Commissioner MATERIALS RELEASE NO: ... or sand for hot mop or Cold Adhesive applications. ... When requested, provide the FHA Standards, Office of Manufactured Housing Programs, HUD Headquarters, with a representative list of properties, in which the ...\nSalt manufacture. Commercial salt is manufactured from rock salt, as well as from seawater and other natural and artificial brines. Most of the artificial brines are obtained by pumping water into underground salt beds. A considerable amount of brine itself is used directly in industrial countries.\nGUIDE TO FOUNDATION AND SUPPORT SYSTEMS FOR MANUFACTURED HOMES Excellence in Design, Manufacturing and Installation Series ... GUIDE TO FOUNDATION AND SUPPORT SYSTEMS FOR MANUFACTURED HOMES. Excellence in Design, Manufacturing and Installation Series . .\nas fine aggregate. The proportion of lateritic sand was varied from 0% to against quarry dust at intervals of 25%, using concrete mix of 1:1.5:3 and water/cement ratio of 0.65. Concrete samples were prepared, cured for 28 days, and tested in the laboratory to destruction in order to determine their flexural and tensile strength properties.\nGlass is a non-crystalline, amorphous solid that is often transparent and has widespread practical, technological, and decorative usage in, for example, window panes, tableware, and optoelectronics.The most familiar, and historically the oldest, types of manufactured glass are ""silicate glasses"" based on the chemical compound silica (silicon dioxide, or quartz), the primary constituent of sand.\nContents - Previous - Next. The flour produced from the cassava plant, which on account of its low content of noncarbohydrate constituents might well be called a starch, is known in world trade as tapioca flour. It is used directly, made into a group of baked or gelatinized products or manufactured into glucose, dextrins and other products.\nFortunately, there are easier and less extreme ways of making glass—but all of them need immense amounts of heat. In a commercial glass plant, sand is mixed with waste glass (from recycling collections), soda ash (sodium carbonate), and limestone (calcium carbonate) and heated in a furnace.\nSand is used to provide bulk, strength, and other properties to construction materials like asphalt and concrete. It is also used as a decorative material in landscaping. Specific types of sand are used in the manufacture of glass and as a molding material for metal casting. Other sand is used as an abrasive in sandblasting and to make sandpaper.\nBecause of its small size and sheet-like structure, clay has a large amount of surface area per unit mass, and its surface charge attracts ions and water. Because of this, clay is the ""active"" portion of the soil matrix. For all mineral soils, the proportion of sand, silt, and clay always adds up to 100 percent.\nPolyethylene Maufacturing and its Properties. HDPE is the strongest four times that of low density polyethylene, toughest, most chemical resistant, and least flexible of these four types of polyethylene.HDPE is used in products and packaging such as milk jugs, detergent bottles, margarine tubs, garbage containers and water pipes.\nCoffee, tea and other beverages made with water will contain some amount of silicon dioxide. Beer is an excellent source of the mineral in addition to being a tasty drink. The silicon dioxide in beer is in the form of orthosilicic acid, which is important for bone health and to prevent osteoporosis.\nsand by manufactured sand with 0%, 50% and on hardened properties of cement mortar. The experimental work includes the casting, curing and testing of specimens.\nPYrogeniC siliCa. Chemical Structure and Properties Pyrogenic silica consists of SiO 4/2 tetra- hedra, each of which is linked to adjacent tetrahedra by means of a common oxygen atom. Pyrogenic silica is pro - duced at temperatures over 1,000 °C by introducing volatile chlorosilane into an oxyhydrogen flame.\nSand & Gravel (~900,000,000 tons per year) Crushed stone (~1,200,000,000 tons per ... other properties dependent on parent rock ... Aggregates in Concrete Concrete Technology 10 Rocks are classified according to origin into three major groups: 1.\n2.9 Sand—Sand consists of fine aggregate particles that are retained on the No. 200 (75 μm) sieve, either as natural sand resulting from natural disintegration and abrasion of rock, or as manufactured sand, which is produced by the crushing of rock, gravel, slag, etc.\nQuartz is a chemical compound consisting of one part silicon and two parts oxygen. It is silicon dioxide (SiO2). It is the most abundant mineral found at Earth\'s surface, and its unique properties make it one of the most useful natural substances. Rock crystal quartz: Transparent ""rock crystal"" quartz.\npreparation of manufactured sand and its properties 4.9 - 4364 Ratings ] The Gulin product line, consisting of more than 30 machines, sets the standard for our industry.\nWood is the oldest material used by humans for construction after stone. Despite its complex chemical nature, wood has excellent properties which lend themselves to human use.\nSUSTAINABLE SAND MINING MANAGEMENT GUIDELINE SEPTEMBER 2015 . Page 2 of 54 S USTAINABLE SAND MINING ... (Manufactured Sand) & construction ... causing loss of properties and degradation of landscape, it can also undermine bridge supports, pipe lines or other structures. ...\nManufactured Sand (M Sand) for Concrete Properties and ... The crushed sand is of cubical shape with grounded edges, washed and graded to as a construction material. The size of manufactured sand (M Sand) is less...']"	['<urn:uuid:5f1b89ac-024f-4f1d-af24-b78af79ee079>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	9	75	1358
49	What was found in the A tower during the Bursa wall excavations?	A rectangular basilica structure from the early Roman era was found, featuring marble columns and wall decorations. The basilica served both as a court and religious structure.	['Excavations unearth basilica in Bursa\nBURSA - Anadolu Agency\nThe basilica unearthed in Bursa served both as a court and a religious structure in the early Roman era. AA PhotoExcavations at a tower in the Tophane portion of Bursa’s city walls have revealed a basilica from the early Roman era that could be one of the oldest structures ever discovered in the northwestern province.\nArchitect İbrahim Yılmaz, who has been conducting the restoration projects on Bursa’s city walls, said the Tophane city walls restoration project included an area of 1,200 square meters from the north of the Saltanat Gate to the Kaplıca Gate.\nHe said all restoration projects in the area had been approved by the Bursa Cultural and Natural Heritage Preservation Board and that the restoration had been divided into two phases because of long distances. The first phase includes two big towers, A and B.\nYılmaz said that as part of the restorations of the towers, the A tower had been excavated. “During these excavations works, we have found the remains of some walls in the lower levels of the tower.\nWe tought that the remains were from an early Roman-era basilica and decided to deepen the excavations considering that the remains would shed light on Bursa’s history of architecture. This is why a single-floor structure on the remains has been expropriated and ruined. When the excavations ended, a rectangular basilica structure with marble columns and wall decorations was revealed. This basilica served both as a court and a religious structure in the early Roman era. It is possibly the oldest structure in the city after the walls.”\nTechnical features of the basilica\nSpeaking about the technical features of the basilica, Yılmaz said: “There is a round apse [the place for religious ceremonies] and a window bay in front of it. In the middle of the basilica is a nave and two rooms on its right and left sides. One of these rooms is in the northeast of the apse and the other is in its southeast.\nThe name of the first room is the diokonikon, which is home to holy objects, and the other is the prosthesis, where gifts are accepted and kept. In the northwest of the apse there is a burial chamber, in which there is the skeleton of a priest.”\nYılmaz said the ongoing excavations in the basilica aimed to unearth its entrance, namely the narthex and the atrium.\n“The emergence of the basilica, which is very unique and had not been known so\nfar, will increase the position of Bursa in the history of culture. Restoration works that will\nbe carried out after excavations will boost the city’s tourism,” he said.\nYılmaz said cleaning works in the B Tower had revealed the remains of a casemate, an underground armored structure. “It is understood that the B Tower was the first tower protecting the Bey Palace inside the walls. This tower has been restored on its remains using traditional materials and methods, and regained its glory. Although the tower is two-storied, one of its floors was rebuilt and the other floor was organized as a view terrace,” Yılmaz said.']	['<urn:uuid:e92a8a12-4d54-4678-845f-d5340dcf1306>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	12	27	525
50	I'm planning a Mars mission. Where could tiny rovers explore?	PUFFERs could be deployed to explore confined spaces like overhung rocks and caves on Mars. They can be integrated into a parent spacecraft, such as a larger rover or lander, and then deployed to access terrain features that the parent vehicle cannot reach.	['Pop-Up Robots Enable Extreme Terrain Science\nThis blog post originated in the 2015 Science Mission Directorate Technology Highlights Report (2 MB PDF).\nTechnology Development: A NASA-led team is designing an extremely compact origami rover for new extreme terrain applications in both the planetary and Earth science domains. PUFFERs (Pop-Up Flat Folding Explorer Robots) utilize a folding printed circuit board (PCB) as the rover chassis, which enables the platform to fold into a minuscule, palm-sized volume. With this feature, many PUFFERs can be integrated into future spacecraft or packed into Earth science experiments at low cost.\nThe multitude of PUFFERs would then be used to carry out science investigations that specifically require a distributed, multi-unit approach, such as entering cave formations on Mars or conducting spatially-distributed topographic mapping of ice on Earth. In addition to small packing volume, PUFFER’s folding chassis provides unique mobility benefits; PUFFERs can collapse into a lowprofile “crouch” to crawl beneath tight terrain features, such as overhung rocks, and to lower their center of gravity for ascending steep inclines. The highly-flexible origami-inspired chassis also provides impact-absorbing capabilities, allowing PUFFER to survive great falls.\nImpact: The PUFFER technology will provide low-cost access to new science-rich terrains both on Earth and beyond. Here on Earth, scientists hope to deploy swarms of PUFFERs to track ice fluctuations in the polar regions. Teams of researchers could deploy PUFFERs over ice sheets of interest from the air, dropping the impact-resistant rovers from helicopters. The PUFFERs would then remain behind and autonomously rove over the ice while making measurements. The units would recharge themselves using solar energy, allowing them to operate for months and possibly years at a time. Work is currently underway to develop and test PUFFERs for snow and ice mobility, and the prototypes were recently tested at Mt. Erebus in Antarctica. Beyond our own planet, PUFFERs will provide future NASA missions with a low-cost add-on technology for accessing new extreme terrains. A swarm of PUFFERs could be folded up into a parent spacecraft, such as a larger rover or lander, and then be deployed into terrain features that the parent itself cannot reach. Example features of interest could include steep inclines and confined spaces, such as overhung rocks and caves on Mars, or “chaos terrains” on the surface of Europa.\nStatus and Future Plans: PUFFER has successfully completed initial field tests in both the Mojave Desert and Mt. Erebus in Antarctica. The Mojave Desert tests evaluated mobility on Mars-analog terrains, while the Antarctica testing evaluated snow and ice mobility. The team is currently preparing next-generation prototypes for expanded field tests, which will integrate new instruments such as cameras and a microscope.\nSponsoring Organization: The PUFFER technology is jointly funded by SMD’s Planetary Science and Earth Science Divisions, and by the Space Technology Mission Directorate (STMD) as part of the Game Changing Development (GCD) Program. In addition, the project’s small business partner, Distant Focus Corporation, has received funding through the NASA Small Business Innovation Research (SBIR) office for development of a novel folded optic microscope for PUFFER. The PUFFER effort is being led out of NASA Jet Propulsion Lab, with collaborators at the University of California, Berkeley; Distant Focus Corporation (Champaign, IL); and Pioneer Circuits Inc. (Santa Ana, CA).\nRead more Technology Stories']	['<urn:uuid:c63226ed-3b54-4c08-a8df-4fbe0d1c79f4>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	10	43	542
51	core responsibilities duties museum artifacts collection administrator daily work	Curators plan research projects, authenticate and examine acquisitions, attend meetings and civic events, negotiate purchases and loans of collections, confer with boards of directors on policies and budgets, and supervise staff including technical, research, and clerical personnel.	"[""What is a Curator?\nCurator Job Description Administer collections, such as artwork, collectibles, historic items, or scientific specimens of museums or other institutions. May conduct instructional, research, or public service activities of institution.\nLife As a Curator: What Do They Do?\n- Plan and conduct special research projects in area of interest or expertise.\n- Study, examine, and test acquisitions to authenticate their origin, composition, history, and to assess their current value.\n- Attend meetings, conventions, and civic events to promote use of institution’s services, to seek financing, and to maintain community alliances.\n- Confer with the board of directors to formulate and interpret policies, to determine budget requirements, and to plan overall operations.\n- Negotiate and authorize purchase, sale, exchange, or loan of collections.\n- Train and supervise curatorial, fiscal, technical, research, and clerical staff, as well as volunteers or interns.\nWhen polled, Curators say the following skills are most frequently used in their jobs:\nSpeaking: Talking to others to convey information effectively.\nReading Comprehension: Understanding written sentences and paragraphs in work related documents.\nActive Listening: Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.\nWriting: Communicating effectively in writing as appropriate for the needs of the audience.\nCritical Thinking: Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.\nComplex Problem Solving: Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.\nTypes of Curator Jobs\n- Gallery Director\n- Educational Resource Coordinator\n- Collections Curator\n- Stamp Collector\n- Educational Institution Curator\nJob Demand for Curators\nThere were about 12,400 jobs for Curator in 2016 (in the United States). New jobs are being produced at a rate of 13.7% which is above the national average. The Bureau of Labor Statistics predicts 1,700 new jobs for Curator by 2026. The BLS estimates 1,500 yearly job openings in this field.\nThe states with the most job growth for Curator are Colorado, Washington, and Nevada. Watch out if you plan on working in Mississippi, Maine, or Idaho. These states have the worst job growth for this type of profession.\nCurator Average Salary\nThe salary for Curators ranges between about $29,010 and $94,330 a year.\nCurators who work in District of Columbia, Connecticut, or New York, make the highest salaries.\nHow much do Curators make in different U.S. states?\n|State||Annual Mean Salary|\n|District of Columbia||$86,080|\nTools & Technologies Used by Curators\nBelow is a list of the types of tools and technologies that Curators may use on a daily basis:\n- Microsoft Excel\n- Microsoft Word\n- Microsoft Office\n- Microsoft PowerPoint\n- Microsoft Outlook\n- Web browser software\n- Microsoft Access\n- Autodesk AutoCAD\n- Adobe Systems Adobe Photoshop\n- FileMaker Pro\n- Adobe Systems Adobe InDesign\n- Scheduling software\n- Graphics software\n- Corel WordPerfect\n- Microsoft Visual Studio\n- Desktop publishing software\n- Adobe Systems Adobe Freehand\n- Microsoft Paint\n- PastPerfect Software PastPerfect\n- Questor Systems ARGUS\nBecoming a Curator\nLearn what Curator education requirements there are.\nHow many years of work experience do I need?\nWho Employs Curators?\nCurators work in the following industries:\nThose interested in being a Curator may also be interested in:\nCareer changers with experience as a Curator sometimes find work in one of the following fields:\nImage Credit: Jorge Royan via Creative Commons Attribution-Share Alike 3.0 Unported\nMore about our data sources and methodologies.\n|Request Info||Southern New Hampshire University You have goals. Southern New Hampshire University can help you get there. Whether you need a bachelor's degree to get into a career or want a master's degree to move up in your current career, SNHU has an online program for you. Find your degree from over 200 online programs. Learn More >|""]"	['<urn:uuid:5945c3ab-82f6-4691-ac55-05e4bb41df28>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	9	37	642
52	medicine induced movement problems early symptoms	Drug-induced movement disorders can be classified into acute (transient) and chronic (persistent) categories. Acute symptoms occur during the early phase of drug therapy and are typically short-lived. These can include acute dystonia, which involves abnormal and prolonged contraction of the muscles of the eye, head, neck, limbs, or trunk, and acute akathisia, characterized by a sensation of restlessness, a constant urge to move, and anxiety. Chronic symptoms commonly develop with prolonged use of the inciting drug. While some believe permanent movement disorders may arise after a single dose of a dopamine receptor antagonist, the general consensus supports the chronic-use concept.	['US Pharm. 2014;39(1):HS13-HS16.\nABSTRACT: Tardive dyskinesia (TD), a drug-induced movement disorder, is a serious side effect resulting primarily from the prolonged use of dopamine-blocking agents. TD is distressing because this adverse effect is likely to be permanent. Age is a consistent risk factor for TD, and the disorder occurs more frequently in women. Most treatments for TD have not proven to be successful, and therefore the best treatment option is prevention of the disorder. If a drug known to cause TD is prescribed, the clinician should monitor the patient for symptoms. Early detection may improve the likelihood of remission.\nTardive dyskinesia (TD) is a drug-induced movement disorder (DIMD) characterized by the presence of abnormal involuntary movements.1-3 TD was named and classified in the early 1960s, a number of years after chlorpromazine was marketed in the United States as an antipsychotic agent.4 Chlorpromazine revolutionized the treatment of schizophrenia.4 Neurologic side effects such as involuntary movements and tics began to be reported.2,3 Initially, these neuromuscular disorders (i.e., movement disorders) were attributed to the psychiatric disease, rather than to incitement by a drug. However, based on the prevalence of TD in psychiatric patients, it was eventually established that these movement disorders were linked to the use of antipsychotics that block dopamine receptors.3\nThese early agents used in the treatment of psychosis were termed first-generation antipsychotics (FGAs), or typical antipsychotics. TD has been reported with all FGAs. When the second-generation antipsychotics (SGAs), or atypical antipsychotics, were developed, researchers expected lower rates of TD based on the weaker affinity of these drugs for blocking dopamine receptors.5\nWhile the risk of TD may be lower with SGAs, it still does occur. TD is listed in all of these agents’ package inserts, although clozapine has had only rare or unconvincing reports of TD.6-10 Quetiapine also appears to have a lower tendency to cause TD; however, for both quetiapine and clozapine, data have been conflicting.2,7-9\nAntipsychotics are the primary drugs associated with TD, but there are nonpsychiatric drugs that also block dopamine receptors and are associated with TD (TABLE 1).2,8,9,11 One of these agents, metoclopramide, has been on the U.S. market since 1979, and its use has increased over the past decade. In 2009, the FDA issued a warning about the risk of TD associated with metoclopramide use. At that time, more than 2 million patients were taking metoclopramide. Failure to recognize the importance of the warning became a concern because of the possibility that nonpsychiatric clinicians prescribing metoclopramide were less familiar with the side effect than were psychiatric clinicians, who were aware of the connection between antipsychotics and TD.12\nTD develops in approximately 20% of patients on long-term antipsychotic drugs, although there is some debate over a lower incidence with SGAs.4,13,14 Prevalence rates are widely variable, depending upon the study design, participants, duration of use of previous inciting agents, and diagnostic criteria.4 The incidence of TD with metoclopramide (a gastrointestinal prokinetic agent used to treat symptomatic gastroesophageal reflux disease, diabetic gastroparesis, and nausea and vomiting) has been reported to range from 1% to 10%.15 A more recent review of TD associated with metoclopramide stated that the risk may be closer to less than 1%.3,12\nDIMD Classification and Symptoms\nThe various movement disorders—which can be confusing to patients and clinicians alike—are classified based on timing and specific symptomatology. There are two major categories of DIMD: acute (transient) and chronic (persistent). Acute symptoms occur during the early phase of drug therapy and are frequently short-lived. Chronic symptoms commonly arise with prolonged use of the inciting drug. Some thought leaders believe that permanent movement disorders may arise after a single dose of a dopamine receptor antagonist, but the general consensus supports the chronic-use concept.13\nThe Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), developed and published by the American Psychiatric Association, includes classifications of all currently recognized mental health disorders. In DSM-5, DIMDs are termed medication-induced movement disorders (MIMDs). The MIMDs listed in DSM-5 include neuroleptic-induced parkinsonism and other medication-induced parkinsonism; neuroleptic malignant syndrome (NMS); medication-induced acute dystonia; medication-induced acute akathisia; TD; tardive dystonia and tardive akathisia; medication-induced postural tremor; and other medication-induced movement disorders (TABLE 2).16\nMedication-induced parkinsonism can be misdiagnosed as Parkinson disease (PD) because of the similarity of symptoms, including tremor, bradykinesia, and abnormal gait. Sudden transient freezing, one of the most distressing symptoms of PD, is not seen in medication-induced parkinsonism.2 NMS, which can be life-threatening, occurs in 0.01% to 0.02% of patients treated with antipsychotics.16 Symptoms of NMS include altered mental status, hyperthermia, altered blood pressure, tachycardia, tachypnea, and muscle stiffness.16 Withdrawal of the inciting drug, fluid and electrolyte replacement, lowering of the body temperature, and management of cardiac and renal complications are essential.3\nAcute dystonia and akathisia can occur very quickly after the drug is taken. Acute dystonia involves abnormal and prolonged contraction of the muscles of the eye, head, neck, limbs, or trunk.2,16 In tardive dystonia, a variant of TD, the abnormal movements are distinct from those of classic TD. TD more often develops in older women; tardive dystonia more often manifests in younger patients and has no affinity for gender.2 TD can occur after brief exposure to a dopamine-blocking agent, but it usually manifests after months or years of therapy.3 Acute akathisia is characterized by a sensation of restlessness, a constant urge to move, and anxiety.16 Tardive akathisia, a subtype of TD, occurs after prolonged use of dopamine-blocking therapy and is clinically similar to acute akathisia.2\nMedication-induced postural tremor is expressed as a fine tremor that occurs when the patient attempts to maintain a posture. The tremor is similar to that seen with anxiety and the use of caffeine and other stimulants.16\nTD is a DIMD of particular concern for the clinician, patient, and family because of the severe and potentially irreversible motor effects. TD is characterized by persistent, involuntary, rapid, and repetitive stereotypical movements that involve the oral, buccal, and lingual areas (tongue, cheeks, lips, and jaw). The patient may experience twisting and protrusion of the tongue, smacking of the lips, and chewing or puckering of the mouth. Some involuntary movements, such as the tongue pushing food out of the mouth, can be particularly problematic.4,13,17,18 Frequently, the continual movement of the tongue causes sores to develop inside the cheeks. This can lead to considerable difficulty for patients with dentures.13\nTD can cause other facial movements, such as grimacing, eyebrow raising or furrowing, and eye closing. If the limbs are involved, quick movements of the fingers or toes occur, and nonrhythmic movements of the arms and legs also take place. The patient may extend the toes and tap the foot while sitting.4 The larynx and diaphragm may be involved, although they rarely cause vocal or breathing problems.18 The patient may be unaware of the movements or underrate their magnitude.13 Symptoms are more pronounced when the patient is awake and/or excited, and they tend to resolve during sleep. At times, it may be possible for the patient to contain the movements with a strong, concentrated effort.4,19 The major risk factors associated with TD are advanced age, female sex, mood disorders, organic brain dysfunction, greater total drug exposure, and early extrapyramidal symptoms (EPS).2,4\nThe exact pathophysiology of DIMDs is unknown, but several mechanisms of action have been proposed. The chronic blockade of dopamine receptors by these drugs, leading to an escalation in receptor sensitivity, is one of the most frequently postulated causes.3,20\nTo determine whether a patient’s movement disorder is TD, the clinician first must exclude other movement disorders, and the diagnosis is usually validated by a physical examination, along with a neuropsychiatric and medication history.4,7 Several scales are available to help the clinician accurately assess TD. The Abnormal Involuntary Movement Scale (AIMS) is widely used to detect TD and track its severity over a period of time.21 The AIMS includes 12 items that assess orofacial movements, extremity and truncal dyskinesia, and global severity from the examiner’s evaluation, along with the patient’s awareness of the movements. The last section of the AIMS contains questions on problems with teeth and dentures.\nThe optimal treatment path for TD is to prevent the disorder from occurring.11 Many studies have evaluated treatments for TD, with a few showing slight or modest improvements for patients. In July 2013, the American Academy of Neurology (AAN) published evidence-based guidelines for the treatment of tardive syndromes (TDS), including TD. The panel defined TDS as including lingual-facial-buccal dyskinesia, as well as the variant forms. TDS encompasses all types of persistent dyskinesia caused by dopamine-blocking agents.22\nTDS are disorders that meet the following criteria: a history of at least 3 months of neuroleptic exposure; the presence of at least moderate abnormal involuntary movements in one or more body areas, or at least mild movements in two or more body areas; and the absence of other conditions.\nThe AAN panel recommended that five questions be addressed to determine the management of TDS, including TD. The questions are as follows: 1) Is withdrawal of the dopamine receptor blocker an effective treatment for TDS? 2) Does switching from typical to atypical dopamine-blocking agents reduce TDS symptoms? 3) What is the efficacy of pharmacologic agents in treating TDS? 4) Do patients with TDS benefit from chemodenervation with botulinum toxin? and 5) Do patients with TDS benefit from surgical therapy?22\nThe guidelines state that data are insufficient to support or refute withdrawal of the agent as treatment; however, from a clinical context, if TDS is present, it is recommended that the clinician withdraw the antipsychotic. This applies only to patients who can tolerate this, however. Although evidence is limited, the guidelines note that short-term withdrawal may worsen TDS, whereas adding an antipsychotic with stronger EPS can reduce it. Data were insufficient to support or refute switching from a typical dopamine receptor blocking antagonist to an atypical agent to reduce TDS symptoms.22\nThe AAN panel reviewed studies involving several pharmacologic agents for the treatment of TDS. In a randomized, controlled study, amantadine (300 mg/day) used conjointly with neuroleptics reduced TDS during the first 7 weeks. The panel suggested consideration of treatment with amantadine plus neuroleptics for short-term use, based on weak evidence.22\nTetrabenazine, a dopamine-depleting agent, is licensed in other countries for treatment of TD. In the U.S., tetrabenazine (Xenazine) is approved to treat chorea associated with Huntington disease (HD), but not to treat TD. The initial dosing for HD-associated chorea is 12.5 mg/day.20 The AAN panel found weak evidence to support the use of tetrabenazine for TDS. In a trial involving haloperidol, tetrabenazine was dosed at 100 mg/day for 14 weeks. A second nonrandomized study had participants discontinue the neuroleptic and other TDS treatments at least 30 days before baseline. Reductions in symptoms were seen posttreatment with tetrabenazine at a mean dose of 57.9 mg/day.22\nBenzodiazepine trials have yielded limited results. In a Cochrane Review, one small study provided preliminary evidence that benzodiazepines may have an effect in the treatment of TD.23 Clonazepam was the benzodiazepine used in this study, and the AAN stated that this drug may be helpful for short-term (3 months) use in decreasing TD symptoms, with moderate evidence supporting its use.22\nTrials utilizing melatonin, ginkgo biloba, vitamin E, and vitamin B6 have had mixed results.22,24 Melatonin dosed at 2 mg/day was “possibly ineffective,” but a dosage of 10 mg/day for a longer period of time was purported to be feasibly effective in treating TDS, although the AAN panel deemed that the evidence was insufficient. There was moderate evidence supporting the use of ginkgo biloba in inpatients with schizophrenia who had TD. Vitamin E, which was used to neutralize free radicals, generated some improvement in newly diagnosed TD present for less than 5 years. Another study, which used vitamin E dosages of 1,200-1,600 IU/day for 4 to 12 weeks, reported reduced TD severity.22,25 However, the AAN panel determined that the data were conflicting and insufficient to determine efficacy.\nData also were insufficient to support or refute the efficacy of TDS treatment with acetazolamide, bromocriptine, thiamine, baclofen, vitamins B12 and B22, selegiline, clozapine, olanzapine, nifedipine, fluperlapine, sulpiride, flupenthixol, thiopropazate, haloperidol, levetiracetam, quetiapine, ziprasidone, sertindole, aripiprazole, buspirone, yi-gan san, botulinum, alpha-methyldopa, reserpine, electroconvulsive therapy, or biperiden discontinuation. Diltiazem, galantamine, and eicosapentaenoic acid should not be considered treatment options, according to the AAN panel.22\nSurgical interventions have been explored as treatment for TD. Deep brain stimulation (DBS), currently used in many PD patients, may be a potential treatment option for TD.26 A systematic review of studies (many of which were case studies) in which DBS was used to treat medication-induced TD and/or dystonia found improvement in these treatment-resistant patients. Results were not reported for TD and dystonia separately, although the authors stated that this was not an issue since most patients experience both conditions. The mean improvement 3 to 76 months after DBS was 77.5% according to the Burke-Fahn-Marsden Dystonia Rating Scale.27 However, the AAN deemed that the evidence was insufficient to determine benefit.22\nTD can significantly alter the patient’s quality of life. The pharmacist can actively educate patients about the risk of DIMDs when the prescribed medications are associated with these side effects. The pharmacist can also provide patients with information about the initial signs and symptoms of TD. Remission rates are inversely correlated with the severity and length of time of TD.17 Early detection may improve the chances of minimizing the disorder or achieving remission.2,21,25,28\n1. Merrill RM, Lyon JL, Matiaco PM. Tardive and spontaneous dyskinesia incidence in the general population. BMC Psychiatry. 2013;13:152.\n2. Sethi KD. Movement disorders induced by dopamine blocking agents. Semin Neurol. 2001;21:59-68.\n3. Jankovic J. Tardive syndromes and other drug-induced movement disorders. Clin Neuropharmacol. 1995;18:197-214.\n4. Soares-Weiser K, Fernandez HH. Tardive dyskinesia. Semin Neurol. 2007;27:159-169.\n5. Gershanik OS, Gómez Arévalo GJ. Typical and atypical neuroleptics. Handb Clin Neurol. 2011;100:579-599.\n6. Woods SW, Morgenstern H, Saksa JR, et al. Incidence of tardive dyskinesia with atypical and conventional medications: a prospective cohort study. J Clin Psychiatry. 2010;71:463-474.\n7. Tarsy D. Tardive dyskinesia. UpToDate. www.uptodate.com. Accessed October 24, 2013.\n8. Parkinson’s Disease Center and Movement Disorder Clinic, Baylor College of Medicine. Tardive dyskinesia. www.bcm.edu/departments/neurology/parkinsons/?pmid=14198. Accessed October 26, 2013.\n9. Pierre JM. Extrapyramidal symptoms with atypical antipsychotics: incidence, prevention, and management. Drug Saf. 2005;28:191-208.\n10. Kane JM, Correll CU. Pharmacologic treatment of schizophrenia. Dialogues Clin Neurosci. 2010;12:345-357.\n11. Rich SS. Drug-induced movement disorders. R I Med. 1993;76:556-562.\n12. Rao AS, Camilleri M. Review article: metoclopramide and tardive dyskinesia. Aliment Pharmacol Ther. 2010;31:11-19.\n13. Chou KL, Friedman JH. Tardive syndromes in the elderly. Clin Geriatr Med. 2006;22:915-933.\n14. Rosenheck RA. Evaluating the cost-effectiveness of reduced tardive dyskinesia with second-generation antipsychotics. Br J Psychiatry. 2007;191:238-245.\n15. Ganzini L, Casey DE, Hoffman WF, McCall AL. The prevalence of metoclopramide-induced tardive dyskinesia and acute extrapyramidal movement disorders. Arch Intern Med. 1993;153:1469-1475.\n16. American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5). Arlington, VA: American Psychiatric Association; 2013.\n17. Chen JJ. Drug-induced movement disorders: a primer. US Pharm. 2007;32(11):HS16-HS32.\n18. Dayalu P, Chou KL. Antipsychotic-induced extrapyramidal symptoms and their management. Expert Opin Pharmacother. 2008;9:1451-1462.\n19. Haddad PM, Dursun SM. Neurological complications of psychiatric drugs: clinical features and management. Hum Psychopharmacol. 2008;23(suppl 1):15-26.\n20. Leung JG, Breden EL. Tetrabenazine for the treatment of tardive dyskinesia. Ann Pharmacother. 2011;45:525-531.\n21. Mathews M, Gratz S, Adetunji B, et al. Antipsychotic-induced movement disorders: evaluation and treatment. Psychiatry (Edgmont). 2005;2:36-41.\n22. Bhidayasiri R, Fahn S, Weiner WJ, et al. Evidence-based guideline: treatment of tardive syndromes: report of the Guideline Development Subcommittee of the American Academy of Neurology. Neurology. 2013;81:463-469.\n23. Bhoopathi PS, Soares-Weiser K. Benzodiazepines for neuroleptic-induced tardive dyskinesia. Cochrane Database Syst Rev. 2006;(3):CD000205.\n24. Margolese HC, Chouinard G, Kolivakis TT, et al. Tardive dyskinesia in the era of typical and atypical antipsychotics. Part 2: incidence and management strategies in patients with schizophrenia. Can J Psychiatry. 2005;50:703-714.\n25. Bhidayasiri R, Boonyawairoj S. Spectrum of tardive syndromes: clinical recognition and management. Postgrad Med J. 2011;87:132-141.\n26. Kefalopoulou Z, Paschali A, Markaki E, et al. A double-blind study on a patient with tardive dyskinesia treated with pallidal deep brain stimulation. Acta Neurol Scand. 2009;119:269-273.\n27. Mentzel CL, Tenback DE, Tijssen MA, et al. Efficacy and safety of deep brain stimulation in patients with medication-induced tardive dyskinesia and/or dystonia: a systematic review. J Clin Psychiatry. 2012;73:1434-1438.\n28. Aia PG, Revuelta GJ, Cloud LJ, Factor SA. Tardive dyskinesia. Movement disorders. Curr Treat Options Neurol. 2011;13:231-241.\nTo comment on this article, contact email@example.com.']	['<urn:uuid:d07c3263-7abb-4e5f-820a-49b7c34c601f>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	6	100	2728
53	How much does the Raman peak shift per GPa in silicon?	The Raman-active Si phonon peak shifts by approximately 1.95 cm-1/GPa.	['Driven by commercial considerations as well as technical needs, two trends presently dominate R&D in modern power semiconductor (SC) manufacturing: increasing wafer diameters and decreasing substrate thicknesses, down to a few 10 µm. With this, mechanical stresses in the material increasingly become a tangible problem, with consequences ranging from wafer deformation, which poses a manufacturing and handling problem, to the in-use failure of the devices in the worst case.\nWith stresses being introduced all along the processing chain, e.g. in layer deposition and structuring, mechanical or chemical wafer thinning, or the final chip separation process, this creates an urgent and concrete need for a method capable of directly and rapidly measuring absolute mechanical stress in silicon - preferably in a contact-free mode.\nFrom theory to the real world scenario\nThe competence centre ASSIC’s research team around DI Martin De Biasio and Dr. Martin Kraft from CTR and Dr. Michael Roesner from In-fineon Technologies Austria could identify micro Raman spectroscopy (RS) as an optical meas-urement method meeting all these requirements.\nRS is known to be capable of measuring stress in various materials, including silicon, with a wealth of related literature on re-search applications investigating e.g. local stress introduced by underlying structures. Still, despite the inherent advantages of this fast, non-destructive, optical and hence contact-free method, RS has not (yet) been accepted as a reliable method in industrial SC fabs.\nThe key purpose of this work was hence to investigate and validate RS as a dependable tool for spatially resolved quantification of absolute stress levels for the use in SC manufacturing process development, optimisation and control. The prime measurand in the determination of stress(es) in silicon by RS is the relative shift of the triply degenerated Raman-active Si phonon peak at 520.5 cm-1 by ~ 1.95 cm-1/GPa.\nTensile strain shifts the centre wavelength to lower frequencies, while compressive stress results in a higher centre frequency. Concerns regarding the method included i) stability and sensitivity of the measurements, ii) the impact of the excitation laser wavelength on the penetration depth, and thus the readout, and the iii) reliability of measurements, especially when applied to entire production-scale wafers.\nThe laboratory Raman micro-spectrometry system used (Renishaw InVia) was hence first applied to a stress-free silicon sample held under perfectly controlled conditions in a specially developed bending device. With an analytical accuracy of ±25MPa and an excellent agreement of the measured values with stress levels derived from analytical calculations and FEM simulations, the method exceeded expectations.\nThe one major interferent is the temperature, which has to be held possibly constant for reliable and sensitive measurements. The optimised and thus validated method was subsequently used as a (standard) tool to support a range of SC processing research activities. One current key R&D interest is the impact of surface processing methods on residual stress-es in the material.\nOnly indirectly accessible until now, the evaluation of the directly measured, absolute stress levels by Raman spectroscopy showed a clear impact of the processing method and parameters. This process step may introduce surface stresses of up to 250 MPa, even with process parameters deemed standard and safe. Furthermore, the type and the levels of dopant(s) present in the material have a surprisingly strong impact, a finding that has now initiated related materials research.\nAnother interesting effect could be observed in the Raman spectra of mechanically diced samples. Raman bands relating to silicon metaphas-es were found, indicating a substantial change in the crystalline structure. This effect could be related to the high pressures occurring during silicon dicing with cutting diamonds, which cause the observed metamorphosis of the top-layer Silicon.\nImpact and effects\nWith the introduction of Raman spectroscopy as a non-destructive, reliable, fast and contact-free method to R&D in semiconductor processing, CTR and Infineon have established a metrology tool capably of reliably measure mechanical stresses, both on small scales like dicing edges and across entire silicon wafers. The measurement system can distinguish and quantify tensile and compressive stresses from 0 MPa up to the material’s breakdown strength with an analytical accuracy of ±25MPa.\nThe established measurement tool has a direct practical impact for industrial process and quality control, as well as providing vital, dependable information to researchers and engineers for semiconductor manufacturing process optimization. Based on this, subsequent activities now target other seminal semiconductor materials, including silicon carbide (SiC).']	['<urn:uuid:15f633aa-a9eb-4594-9cd0-f19aef85051c>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	11	10	717
54	Do mirror neurons fire when observing mime gestures?	No, mirror neurons do not fire when observing mime gestures or pantomimes. While mirror neurons fire when observing actual grasping, they do not respond to the observation of pantomimed actions.	"['Gesture. 1. A position or movement of the hands used to depict the shape, motion, or location of a person, place or thing. 2. A speaking gesture in which the hands and fingers mimic physical, spatial, and temporal relationships among objects, activities, and events. 3. A hand gesture with neurological circuits as complex as those for speech.\nUsage: Because they reveal the presence of conceptual\nthought, mime cues are our most intellectual gestures. Unlike palm-down, palm-up, and self-touch cues, which convey mainly emotion, mime cues also express narrative thinking,\nrelationships among objects, and the association of ideas. In this regard, mime\ncues resemble the spoken words\nthey so often accompany.\nApplication point. Used sparingly, mime cues lend authority, contribute to visual understanding, and add drama to key speaking points.\nEvolution. Mimicking complex sequences of acts--demonstrating the body movements used, e.g., to make stone tools, build brush shelters, and topple trees--mime cues represent an advanced, conceptual form of nonverbal communication. Given in serial order, miming may have been our species\' first step on the intellectual path leading to nonverbal narrative, the precursor of the verbal sign and vocal languages used today.\nSemantics. 1. In a conversation about throwing a\nbaseball, we may mime the motion with our hands. 2.\nMime cues depict a. relationships among objects (e.g.,\n""closer than,"" ""as big as,"" ""heavier""), b. attributes\n(e.g., ""flat,"" ""long,"" ""rounded""), and c. action\nsequences (e.g., ""I pick up snow,"" ""form a snowball,"" and ""throw it at\nyou""). 3. A typical mime sign is the walking-figure,\nused to mimic the body\'s rhythmic, strolling gait.\nRESEARCH REPORTS: 1. In the literature on nonverbal communication, mime cues have been called illustrators (Ekman and Friesen 1969). 2. Of the eight kinds of illustrator gestures defined by Ekman and Friesen (1972), pictographs (i.e., drawing a picture in space with the hands) most closely resemble mime cues.\nNeuro-notes I. To mimic an act such as, e.g., changing a lightbulb, mime cues use the same brain modules to move the same muscles as the physical activity itself. Thus, neurologically, swinging a bat is nearly the same as gesturing the act of batting without using the bat itself. Computer imaging studies show that mentally rehearsing an activity involves the same brain areas, as well (Sirigu, et al. 1996:1564). 1. Mime cues engage many areas of our cerebral neocortex, as well as evolved sub-regions of our basal ganglia and cerebellum. 2. Asked to pantomime the use of an object (e.g., a screwdriver), we orient our hand toward the imagined object\'s target (i.e., the screw). Important in the ability of right-handers to use such transitive mime cues is the left supplementary motor neocortex (Watson et al. 1992:685-86). 3. Increased regional cerebral blood flow (rCBF) in this region "". . . occurs only when movements have an extrapersonal [i.e., transitive, rather than intrapersonal (as in giving a military salute)] frame of reference"" (Watson et al. 1992:686).\nNeuro-notes II. Miming in temporal order and tracing shapes in space involve a highly evolved area of our neocortex\'s parietal lobe. The posterior parietal\'s left side is specialized for language. Its right side helps us process relationships among objects in space, along with information about the position of our hands and our motivational state, all at the same time. 1. The right posterior parietal helps us perform and perceive complex gestures, and recognize complex objects placed in our hand, unaided by vision (Ghez 1991B:623). 2. ""The right parietal lobe is specially concerned in the handling of spatial data and in a non-verbalized form of relationship between the body and space"" (Eccles 1989:197). 3. As it integrates arriving visual, spatial, auditory, and tactile information, our parietal cortex receives emotional input from the cingulate gyrus of the mammalian brain. The parietal lobe then directs our body movements for gesture (and our tongue movements for speech) through fiber links to premotor areas of our brain\'s frontal cortex and lateral cerebellum (Ghez 1991B:623). 4. Mime cues are produced by nerve impulses traveling down the lateral corticospinal tract. This evolutionary recent pathway channels the fine-motor control of our finger and wrist muscles required by the mime gesture.Neuro-notes III. Mirror neurons: Mime cues may not register in mirror neurons: ""In two baseline conditions, the firing of the cells was measured for observation of grasping and of grasp pantomime. As expected, mirror neurons fired for grasping observation but not for observation of the pantomime."" (Source: Iacoboni, Marco (2009). ""Imitation, Empathy, and Mirror Neurons,"" Annual Review of Psychology, Vol. 60, pp. 653-70.)\nSee also APRAXIA, POINT, STEEPLE.\nYouTube Video: Can you spot the mime cues in this three minute video?\nCopyright 1998 - 2016 (David B. Givens/Center for Nonverbal Studies)']"	['<urn:uuid:76ef7389-cde5-44df-9cd3-34f826250463>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T10:32:05.666290	8	30	775
55	How does data privacy protection differ between regular encrypted messaging and end-to-end encrypted platforms in terms of server vulnerability and data access?	Regular encrypted messaging and end-to-end encrypted platforms offer different levels of data privacy protection. In regular encryption, like that used by Facebook Messenger, messages can be vulnerable at the server level because they are decrypted and re-encrypted there, making them accessible to the company, hackers, internet service providers, and governments if served with a warrant. In contrast, end-to-end encrypted platforms like Signal, WhatsApp, and iMessage provide superior privacy protection because messages can only be decrypted by the sender and receiver - the servers cannot access message content even if they wanted to. This difference is significant for privacy, as servers in regular encryption could be hacked or compelled to reveal message contents, while end-to-end encrypted services technically cannot provide this access since they don't have the necessary decryption keys.	"['How to protect your privacy online\nElizabeth Barber delves into online privacy – sharing some helpful tips to remain incognito on the internet.\nAt a time where many companies’ business models are built around collecting as much of our data as possible, it can feel overwhelming to try and control who has access to our information. It often feels as though we are powerless to do anything about this. No one wants to read privacy policies that often run to thousands of pages, and it is not realistic for the majority of people to completely quit social media (but if you can – good for you!).\nHowever, there are steps you can take to increase your privacy online. Often these are quick fixes, such as changing a preference in your phone settings or selecting ‘opt out’ when asked if you want to share your data. Here are 3 tangible steps you can take to help protect your online privacy.\nClear your cookies\nCookies, which you often need to consent to before accessing any website, are files created by websites you visit and which save browsing information. Persistent cookies do not expire when you quit your browser. They may be removed after a few days or may be coded to automatically delete only after a few thousand years! This information can be used to create a digital persona to show you targeted ads.\nSee details here on how to clear cookies for different browsers. Doing so periodically will make it harder for companies to create a detailed persona with your data.\nReview the apps that have access to your location\nYou might be surprised by how many apps have requested access to your location data. Do they need access to your location at all?\nIf apps have access to your location, do they always need to know it, or just when you’re using the app? Do they need your precise location, or is it enough to have your approximate location?\nTo review these on\n- iPhone: Settings → Privacy & Security → Location Services.\n- Android: Settings → Location → App Permissions.\nConsider the messaging platforms you’re using\nFacebook messenger and Twitter or Instagram DMs may not be the place to discuss anything sensitive or private – while Meta offers end-to-end encryption, it isn’t the default setting. Without end-to-end encryption, these companies, outside parties including hackers, internet service providers and governments could access your data. This isn’t conjecture – in Nebraska, Facebook turned over the messages of a mother and her daughter to police after they were served with a warrant as part of an investigation into an illegal abortion.\nUsing iMessage, Whatsapp and Signal mean that your messages are all end-to-end encrypted as a default – no one can access the messages except the sender and receiver.\nHegemonic companies will continue to use our data for their purposes, commercial and otherwise unless there are adequately-enforced restrictions that force them to stop. While we are a long way off from meaningful legislation until then we can all take small steps to empower ourselves to exercise some degree of control over the privacy of our data.', ""We've talked before about encryption, which is an essential part of modern digital life to keep your data secured. But there's another type of communication encryption you've probably heard about too: end-to-end encryption.\nLet's find out how end-to-end encryption differs from other forms of encrypted conversations, and what it means for you when you use apps like Signal.\nA Refresher on Encryption\nIn our overview of encryption, we discussed that encryption is essentially the process of taking data and making it unreadable to everyone except for the parties who are supposed to read it. It's akin to scrambling a note with a secret code that only you and the recipient know, though it's much more mathematically complex than this.\nEncryption is present in many forms in the digital world. When you connect to a website that offers HTTPS, as most websites do today, nobody outside of your computer and the website's server can view your activity. Protecting your Wi-Fi network with a password also encrypts the traffic on it.\nDefining End-to-End Encryption\nEncryption is used in a lot of cases, but it's not used the same way in every instance. This is where end-to-end encryption, which is typically used in communication, comes in. End-to-end encryption is a system where only the people communicating can decrypt and read the messages. Nobody else, whether they're part of the process or not, can see the messages at any point.\nIn particular, this means that the server handling the messages cannot access the messages, even if it wanted to. This is important, as we'll see next.\nAn Example Without End-to-End Encryption\nLet's take an example with a service like Facebook Messenger. Facebook Messenger offers what's called client-server encryption, but messages you send on the service are not end-to-end encrypted.\nClient-server encryption means that when you send a message, your message is encrypted when it goes from your device to Messenger's servers. It's protected with a key that only your device and the server know.\nHowever, the message is decrypted on Facebook's server, then re-encrypted when it's sent from the server to the other person's device. When the server re-encrypts the message, it uses a key that only it and the recipient know.\nThis protects your message while in transit, which is typically when a message is at its most vulnerable. It's also convenient, as it allows you to access your Messenger conversations from anywhere that you're signed in to your account. When you sign in to a new device, Messenger syncs all of the conversations it has stored for you to your new phone or computer.\nHowever, this setup also poses a privacy risk. Because Facebook's servers decrypt your message as part of the communication process, your message is vulnerable at that point. If the server was hacked, the attacker would likely be able to read all of your messages. And if Facebook wanted to, it could read your messages too.\nHow End-to-End Encryption Differs\nNow, end-to-end encryption has a key difference. On an end-to-end encrypted service, such as Signal, only you and the person you're messaging have the ability to decrypt the message. The message still has to go through Signal's servers, but those servers do not have the ability to decrypt the message.\nEven if the government tried to force it to decrypt the messages, Signal wouldn't be able to. In end-to-end encryption, decrypting a message requires your private key, which only your device has.\nThis is why apps that are end-to-end encrypted (including WhatsApp and Signal) only let you use them on one device at a time. Each device generates a private key, which your device uses to decrypt messages meant for it. While Signal and WhatsApp offer web apps, these are just mirrors of the app on your phone. You can't use them unless your phone is connected to your PC.\nEnd-to-end encryption also makes it more difficult for an attacker to tamper with the message, which is known as a man-in-the-middle attack. With the client-server model example, Facebook could take your message, change it, and deliver a modified version of what you said to the recipient. Neither of you would know this had happened.\nWith end-to-end encryption, any attempts to change the encrypted message will result in it in arriving as complete gibberish. This would alert the recipient that the message was tampered with in transit.\nEnd-to-End Encryption Isn't Perfect\nWhile end-to-end encryption provides great security for messaging and other purposes, it still has some potential pitfalls. One of them is that while it protects the contents of your communication, the fact that you communicated with someone is still visible. Thus, it would still be possible to prove that you talked to a shady individual using an end-to-end encrypted service.\nThe more relevant factor is that end-to-end encrypted messages are only as secure as the device you access them on. This is because once messages arrive on your device, they are decrypted so you can read them.\nSay you use Signal for end-to-end encrypted messages, but don't have a passcode on your phone. Anyone who picked up your device would be able to open Signal and read all your messages, which obviously isn't secure.\nThis means that you must be sure to keep your devices properly secured. Intruders, malware, or other forms of attack could allow your messages to be compromised. The same goes for whomever you're talking to. If someone stole their phone and were able to unlock it, they could read all your communications.\nFinally, end-to-end encryption could also be broken by a backdoor, which is a secret way of bypassing encryption implemented by the owner. You might not even be aware that the company behind a supposedly secure service is able to break the encryption and view your messages whenever it wants.\nEnd-to-End Encryption Keeps Communication Private\nNow you know the basics of how end-to-end encryption protects your communication. By only allowing the two devices communicating to decrypt the messages, you cut out the chance of a server in the middle compromising your privacy.\nWhen you message someone and want to keep it private, you should stick with a service that offers end-to-end encryption. Compared to insecure methods like SMS and apps that aren't end-to-end encrypted, they're a much smarter choice. If you use an iPhone, the built-in iMessage is an end-to-end encrypted service already available to you.""]"	['<urn:uuid:238359c2-f89a-40e0-8211-e88b4f96753c>', '<urn:uuid:42cb6870-db85-40b7-a417-930d09e6f560>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	22	129	1586
56	when can corn worms damage plants most	Pest damage to corn plants varies by timing and conditions. For corn borers, extremely early or late plantings face higher damage risk. Early plantings are vulnerable to first generation attacks, while late planted fields are prone to second generation damage. Bt corn shows greater yield advantages with later planting dates due to increased activity of corn borers, corn earworms, fall armyworms and black cutworms. For lesser cornstalk borers, damage risk increases during hot, dry weather - specifically when there are more than 10 days with temperatures of at least 95 degrees Fahrenheit and precipitation less than 2.5mm. These warm conditions accelerate egg laying and can lead to pest outbreaks.	"['Bt Corn Refuges for Corn Borer Management\nENTFACT-128: Bt Corn Refuges for Corn Borer Management | Download PDF\nby Ric Bessin, Extension Entomologist\nUniversity of Kentucky College of Agriculture\nThe EPA regulates resistance management policies to be used by commercial corn producers when using Bt‑corn seed. Initially, Bt-corn technologies required the use of a structured refuge to delay the development of pest resistance to the Bt toxins. However, some newly approved Bt-corn technologies offer other resistance management strategies. These include the standard 20% structured refuge, a reduced 5% structured refuge, and refuge in the bag (a one bag option). The elements of these are:\n20% Structured refuge\n- At least 20% of the corn grown on all farms must be non-Bt (the refuge for corn pests). This can be planted as strips in the Bt corn field, as a block, or as separate fields. In cotton-growing counties, the structured refuge is increased to 50%.\n- The refuge be within 1/4 mile to ½ mile of the Bt corn if it only has traits to control corn borers, and immediately adjacent to Bt corn if it contains traits for corn rootworm control.\n5% Structured refuge (for SmartStax and Intrasect technologies only)\n- The structured refuge is reduced to just 5% with the SmartStax and Intrasect technologies. These can use reduced refuges because they use gene pyramiding (multiple independent toxins are used to control pests).\n- The refuge be within 1/4 mile to ½ mile of the Bt corn if it only has traits to control corn borers (Intrasect), and immediately adjacent to Bt corn if it contains traits for corn rootworm control (SmartStax).\nRefuge in the Bag (AcreMax1 and AcreMax RW only)\n- With refuge in the bag (RIB), a small amount of non-Bt seed (10%) is preblended with the Bt seed. With AcreMax RW, no additional refuge is needed. With Acremax 1, a 20% structured refuge for corn borers is still needed, the AcreMax RW can serve as the corn borer refuge for AcreMax 1 fields.\nWhy Use a Refuge?\nUsing refuges with Bt corn is important and it benefits all growers directly. This article addresses the some of the key reasons why growers need to plant a corn borer refuge when using Bt corn and discusses how the refuges should be deployed and managed.\nSpread Your Risk\nEuropean and southwestern corn borer levels are highly variable from year to year. While a producer may have substantial loses to corn borers one year, that does not necessarily mean that the problem will be as bad or worse the following year. Several factors influence the likelihood of corn borer problems. These include corn borer levels in the fall, parasitism rates, overwintering survival, spring weather conditions during moth flight, and corn planting dates.\nBecause the Bt‑corn seed is an added expense, growers want to use it where it will be economically advantageous. In years when corn borers are not a problem, there is no advantage in using Bt‑corn and the added cost of the seed is not recovered. Growers who have had a history of corn borer problems can spread their risk by only planting a portion of their crop to Bt corn. This reduces seed costs across the entire farm while protecting a substantial portion of their crop from corn borers. In addition, this acreage of non‑Bt corn helps to reduce the potential of ECB developing resistance to Bt‑corn.\nEvaluate the Need For and Return of Bt Corn\nPlanting the entire farm with Bt corn does not allow the grower to compare its performance to standard hybrids. The infestation level in the standard hybrids in the refuge area provide an estimate of the corn borer levels on the farm. The corn borer economic scale (see ENT‑49) can be used to determine the economic loss due corn borers. This value can be compared to the premium cost of the Bt seed. The results of this comparison can be used to make future planting decisions.\nEffective Resistance Management\nFollowing the approved refuge plan for each type of Bt corn technology is the only strategy to prevent pest resistance. In the unfortunate event that corn borers did develop resistance to Bt corn, growers in that area would undoubtedly lose a tremendously valuable corn borer management tool.\nWhile the possibility of pests developing resistance to Bt corn is only a theory, insect pests have a long history of developing resistance to any pest management tactic that is used for a long period of time over a wide area. The examples pests being able to overcome pest management strategies are too numerous to list! Consider the western corn rootworm beetle in Illinois and Indiana. For more than 20 years it was effectively controlled through the use of a corn‑soybean rotation. The eggs that were laid one summer in a corn field would hatch the following year in what has become a soybean field. This pest had been a problem only with continuous corn. But it adapted. Now a portion of the female beetles lay their eggs in soybean fields and rootworms are now a serious problem in first‑year corn in this area. Don=t underestimate the ability of insect pests to adapt!\nIn order for refuges to effectively delay corn borer resistance, they must be designed to take advantage of a few key elements. The first element of a refuge is that if a resistant corn borer is able to develop on Bt‑corn, we want to make sure that it is most likely to mate with a >normal= corn borer so that these offspring will still be susceptible to the Bt toxin. To do this, there must be a reservoir of susceptible moths around each Bt corn field. At least 20% (5% for SmartStax and Intrasect Bt corn) of the corn acreage on each farm must be non‑Bt corn in order to provide a sufficient reservoir.\nAnother important element is timing. It is important to have the Bt and non‑Bt corn in about the same stage of development throughout the season. This is because pests often select fields for egg laying based on development stages of the corn. To ensure the effectiveness of the refuge, the Bt and non‑Bt corn need to be planted at about the same time and have similar maturity. It is not necessary to have a refuge each day when Bt corn is planted on consecutive planting dates. However, there should be at least one refuge for Bt corn planted over a 4 to 7 day period.\nMixing of Bt and Non-Bt seed in Hopper Boxes: This is not approved. This results in a mixture of Bt and non‑Bt corn in the same row and may favor, rather than delay the development of resistance. While this may appear to be the same as refuge in the bag technology, the Bt corn used with refuge in the bag uses multiple toxins against each pest targeted.\nPlanting Bt and Non-Bt Corn in Strips: Growers fill some of the hopper boxes on the planter with Bt‑seed and some with non‑Bt seed. This is one possible arrangement. Growers need to read the seed label to determine the minimum number of rows that can be used with this refuge arrangement.\nPlanting the Bt and Non-Bt Corn in Large Blocks: This is the recommended method of planting the refuge for most types of Bt corn. One portion of a large field is designated for Bt‑corn and the other for the non‑Bt refuge. In this situation, insecticide sprays or early harvest can be used to minimize corn borer losses in the refuge, and the grower can manage weed control according to the needs of the particular hybrids. If Bt and non‑Bt corn cannot be planted in the same field, then adjacent fields or fields within one‑quarter to one-half mile of the Bt corn can be used as refuges for Bt corn using corn borer traits only and immediately adjacent for Bt corn with rootworm traits.\nManaging Corn Borers in the Non-Bt Refuge\nWhile the reason for planting a refuge is to maintain a population of Bt‑ susceptible pests, growers should still manage those refuges to avoid serious losses. When using a 20 percent refuge with Bt corn plantings, growers may consider spraying for corn borers or other pests if scouting indicates it is economical.\nPlanting Dates and the Potential for Corn Borer Losses. Extremely early or late planting will increase the potential for borer damage. The first generation can be very damaging to early plantings, while late planted fields are prone to attack by the second. Generally, Bt corn has a greater yield advantage with later planting dates because of increased corn borer, corn earworm, fall armyworm, and black cutworm activity. It is the late planted refuges that are most likely to be damaged by corn borers.\nEarly Harvest. This option can reduce corn borer losses due to broken or lodged plants or dropped ears. Second generation larvae feed primarily in the plant\'s ear zone and below. This damage is the primary cause of harvest loss. Early harvest of heavily infested fields, can be an effective strategy with corn borer refuges.\nDetermining the Need to Spray Corn Borers in the Refuge. Careful scouting of fields is the most effective means of detecting economic infestations of the European corn borer and other pests. With the first generation in late May and early June, examine the whorl leaves of 20 consecutive plants in at least 5 areas of the field. Look at the leaves for the ""window‑pane"" type of feeding damage caused by the larvae. Pull the whorl from one damaged plant at each stop and unroll it carefully to look for live larvae and their size. The percentage of infested plants in the field and size of the larvae present can help you determine the need for an insecticide application. Sprays should be considered if 50% of the plants show ""shot hole"" damage and live larvae are present in the whorl. Once larvae bore into the stalks treatment is not effective.\nThe same general procedure is followed with the second generation; however, the leaf axils, leaf sheaths, and ear shank areas should be examined for live larvae. Again, the percentage of infested plants and the size of larvae are the keys to determining the need for treatment. Treatment is suggested if egg masses average one per plant and egg hatch has begun or if 50% of plants inspected have live larvae feeding on the leaves or tassels in leaf axil or behind sheaths. If your examination indicates that half of the larvae have entered the stalk, treatment is not recommended.\nCAUTION! Pesticide recommendations in this publication are registered for use in Kentucky, USA ONLY! The use of some products may not be legal in your state or country. Please check with your local county agent or regulatory official before using any pesticide mentioned in this publication.\nOf course, ALWAYS READ AND FOLLOW LABEL DIRECTIONS FOR SAFE USE OF ANY PESTICIDE!', 'About the Lesser Cornstalk Borer\nReproduction and Life Cycle\nThe lesser cornstalk borer has 2 generations per year. This pest overwinters as larvae or pupae in the ground. When they emerge as adults in the spring, they will mate and the female will lay her eggs underground beside host plants. Sometimes; however, they are placed on leaves, stems, or on the soils surface. A female can lay around 200 eggs over her lifespan, sometimes more. The eggs will hatch within 3 days. Larvae live in the ground, creating silken tunnels that they remain in, except for when they feed off of the plants stalk and stems. The larvae will mature within 20 days. On average they go through 6 instars, but this can be modified to 5 or up to 9, contingent on climate conditions. The mature larvae will then enter the pupal stage of development, which takes on average 10 days to complete, but can range anywhere from 7 to 13 days. When the adult emerges, their lifespan is approximately 10 days.\nLesser Cornstalk Borer Identification and Habitat\nThis pest as an adult will develop a wingspan of about 17 to 22mm in length. You can easily distinguish the males from the females. The male’s forewing is generally yellow-brown with a band that is darker than the body on the inner margin and has purple scales. The females are black with purple or red scales at the forewing’s base. The thorax is also darker on the females. At rest, wings are held close to the body on their back. The adults tend to be the most active during the evening as long as temperatures are above 80 degrees Fahrenheit, and especially when it is humid, as these conditions are ideal for oviposition and mating. The eggs are only about 0.6mm long and 0.4mm wide. They are oval and start off as a green colour, but will later change to pink and then red. Once they have matured, the larvae are typically 16mm in length. During this stage, they are extremely active and will wiggle aggressively when disrupted. When they first hatch, larvae are yellow-green with red pigmentation on their back. Later in their development they will have white stripes running lengthwise down their body (mainly noticed during the fifth instar). Fully mature larvae will be a blue-green or red-brown with yellow-white stripes on its back. Their heads are black. When they enter their pupal cells, they will be about 8mm long. They construct their pupae out of silk and sand, creating the cocoon which is 16mm long. They start as a yellowish colour and will then turn brown and black right before the adult emerges.\nThe lesser cornstalk borer is common in the western hemisphere, mainly in the southern areas of the United States. They tend to prefer plants that grow in poor soil types, especially those that are sandy. They are attracted to grasses, and a variety of crops, mainly corn. They thrive in hot, dry weather. These warmer temperatures can lead to higher cornstalk borer populations since eggs are laid at a quicker rate in that type of condition. If there are more than 10 days in the summer that have temperatures of at least 95 degrees Fahrenheit with precipitation less than 2.5mm, an outbreak is very possible. If there are 5 to 9 days with the above mentioned conditions, there is still a possibility of an outbreak and lesser cornstalk borers should be scouted for in your fields. The larvae will tunnel into the plants stem and feed from within, whether above or below ground. The plant will often be stunted, will wilt, and buds sometimes wither. This can eventually lead to the plants death. Another way to tell that this pest has infested your crops is silken webbing that forms a tube by the stalks base in the soil.\nLesser Cornstalk Borer Management and Control Methods\nFor preventative measures, ensure you are practicing clean cultivation. Keep weeds along the fields edges to a minimum. Plowing in the fall can help kill any borers who are overwintering. Early planting can also keep their population in check. Always make sure weeds are removed from the field before planting. However, it is important to note that conservation tillage can also lessen plant injury because when crop residue remains at the soils surface, the larvae will sometimes feed on this instead of the newly seeded plants. Keeping the soil moist through irrigation can also be a useful deterrent for the females from laying their eggs. You can plant RIB (refuge in a bag) corn seed; this seed helps in the protection from corn pests while still having 5% seed that has not been treated with Bt in order to keep this method effective, as that 5% refuge seed does not allow the pest to grow immune to this treatment plan. Not a lot of natural enemies have been overly successful at controlling the lesser cornstalk borer. Note that having satisfactory control is often dependent on using an integrated management system that includes the use of cultural and chemical methods.\nGranular insecticide types are often used to control this pest. They are typically applied in the seed furrow. Liquid pesticides can work as well, but must be aimed at the plants roots. In the past, chlorpyrifos, chlorantraniliprote and bifentrhin have been helpful in suppressing the cornstalk borer. Be sure to carefully read the label for cautions and proper application. It is important to never spray on days that are windy. After applying insecticides, it is important to irrigate sprayed area to increase the insect control. That being said, a large rainfall or irrigation soon after the application can reduce the concentration insecticides.\nLatin / Alternative Lesser Cornstalk Borer Names\n- - Elasmopalpus lignosellus\n- - Jumping borer']"	['<urn:uuid:c0d1e3d4-f32d-4fbf-a226-998a138ec86d>', '<urn:uuid:3945e810-b920-4c64-a3e5-e2d23fc5b12a>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T10:32:05.666290	7	109	2796
57	workplace slip fall accidents insurance guidelines	For slip, trip and fall accidents, which account for over 8.7 million ER visits annually, follow these steps: 1) Ask about the cause of the fall and check for serious injury or underlying medical conditions. 2) Secure and clean the area to prevent further injuries, document the hazard with photographs and witness reports. 3) Contact your insurer, as customers and employees are typically covered under different policies. Gather all information including records, videos, and names of those involved. For security camera footage, secure it immediately as it may be overwritten within 24 hours.	['Worst-Case Scenarios: How Would You Handle These Emergencies?\nby Margot Carmichael Lester, Staples® Contributing Writer\nYou dont have to be a Boy Scout to benefit from their motto: Be prepared. Whether youre a small business owner or employee, your response to workplace accidents has tremendous impact.\nThe biggest mistake I see small business owners making is not preparing ahead of time, laments Elizabeth Lewis, a Denver-based small business lawyer. I get calls from a lot of people saying, This happened! What I do? Of course I can help, but what you should have done is call me six months ago, before this even happened.\nHow can you be better prepared? We offer the following advice for information only. Consult state regulations, your attorney and your insurance broker before enacting or executing any emergency response policy or procedure.\nAllergic reactions can range from simple skin reactions to serious, life-threatening reactions, explains Charles Cairns, professor and chair of the Department of Emergency Medicine at the University of North Carolina. The key is to determine whether a more serious reaction is occurring. When someone has an allergic reaction:\n1. Ask if theyre having trouble breathing or swallowing, or are feeling faint. If yes, then they need to go the emergency room, Cairns says. Call 911 to getEMS on the scene to provide medication and oxygen immediately. Also ask if they have a history of allergies, and if so, what medications they take including if they carry an epinephrine device or other anaphylaxis treatment. If so, ask if they have injected themselves with it, Cairns adds. Even if they have, they should go to the emergency room.\n2. Determine if this was an isolated incident (one persons allergy) or a larger environmental issue but only after the medical situation is resolved, according to Lewis. If its the latter, you may need to secure the area with flagging tape or crowd control fixtures to restrict access and avoid additional incidents.\n3. Reach out to your insurance broker and/or agent sometime that day to explain the situation, says Scott Johnson, owner/broker of Marindependent Insurance Services in Mill Valley, CA. Then I would suggest you begin to collect all the documentation of the situation, such as records and videos, or whatever you may have. Whenever someone leaves your premises by ambulance, its a good idea to call your attorney, too.\nSlips, Trips and Falls\nAccording to data from the National Safety Council, slips, trips and falls account for more than 8.7 million emergency room visits each year. These are common injuries, Lewis notes. The best-case scenario is that youve already talked to your staff and have guidelines in place for dealing with these incidents. When this kind of accident happens:\n1. Ask why they fell and determine if there is evidence of a serious injury or serious underlying medical condition. We like to distinguish rapidly between people whove had a simple mechanical problem a misstep with otherwise normal function versus those who have other serious problems that contributed to the fall, such as abnormal heart, vascular and neurological function, and so on, Cairns says. Then determine their condition, which is extremely helpful when calling 9-1-1.\n2. Secure the area to prevent further injuries and/or damage. In other words, clean up the water people are slipping on, etc., Johnson explains. You also may need to secure the area, as noted above. Be sure to document the hazard with photographs and witness reports.\n3. Call your insurer. Customers and employees are likely covered under different policies. A call to your insurance agent lets you know your need for workers compensation or liability insurance. Be prepared with policy numbers when you make the phone calls, Johnson explains. I would then begin gathering information about the accident, including, but not limited to, records, videos and the names of people involved. Security cameras may be set to record over every 24 hours, so make arrangements to secure that footage immediately.\nBefore you let people drive for your business, you have to have some kind of policy in place and have checked with your insurance company, Lewis says. When an auto accident occurs:\n1. Call 9-1-1 to get police (andEMS, if necessary) to the scene. This is critical for safety and liability reasons. Exchange contact and insurance information with the other driver, but do not admit fault or discuss coverages. Be polite, but discuss the details of the accident only with the police,EMS and your insurance agent.\n2. Move the vehicles out of travel lanes if that practice is legal in your state and if its safe enough to do so. Set up traffic cones, warning triangles or emergency flares to alert oncoming traffic. If possible, take photos of license plates, damage and landmarks/road signs.\n3. Call your insurance agent, broker or insurance company immediately after law enforcement andEMS have stabilized the situation, Johnson says. There are insurance coverages you may elect to use immediately, such as towing.\nThis general advice gives you an idea of what to do in the case of common work-related accidents and emergencies. Consult with your own attorney and insurance agent before creating policies and procedures for your office. Laws and regulations differ widely from state to state and you want to have the correct response plan in place to ensure safety and decrease liability.\nMargot Carmichael Lester is a business writer who grew up in her familys gourmet grocery. Shes run her own creative agency, The Word Factory, for 21 years, and frequently advises start-ups and emerging enterprises on everything from communications to operations. She lives and works in Carrboro, NC. Follow Margot on Google+.']	['<urn:uuid:c57e67aa-4f91-4192-be0c-accf9f188ae6>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	6	93	938
58	How can mobile phones support disabled students' learning?	Mobile phones can support disabled students in several ways: students who cannot speak can express ideas rapidly or ask questions via text messages, deaf pupils can read instructions or messages sent by SMS, and for visually impaired students, phones with SD card slots can be used to record instructions or texts that other classmates need to read.	['7.1 Support for pupils.\nThere are many ways to provide support for pupils to access learning.\na. Types of support integrated in the teaching:\nWhen teachers are preparing lessons, it is important that they decide in advance on the type of support the pupils will need in order to do the expected task. They might, for instance, consider the choice and/or the adaptation of activities and resources, or the questioning technique. Once teachers have established the necessary support for pupils, they then have to plan carefully.\nCase study 12: The advice from Mrs Dalok’s school board\nMrs Dalok is the headteacher of a state primary school at Adétikopé in Togo. This school year, the school is welcoming children with disabilities: a child in a wheelchair who also has a lack of visual acuity, and a child with a hearing impairment. The headteacher checks her school with her team of five teachers to establish what needs to be modified. The third item on the agenda is:\n3. Support during class lessons\nMrs Dalok:Mrs Laban, the pupil with a physical disability will be in your class. Have you thought of what you could do for him? Remember, that he is also a pupil with a visual disability. He does not see well. However, you can all give Mrs Laban ideas on how to include this child better.\nMrs Laban: I was thinking of placing him where he can see the blackboard well.\nMr Adji: It is also imperative to write legibly and in big font and to read what is written on the blackboard aloud. All materials also need to be prepared to accommodate his needs: materials printed in big font, bigger pictures …\nMrs Dalok: Thank you. We will meet his parents to have more information on his condition and to ask whether he might need notebooks with thicker lines for writing.\nMrs Laban: I was thinking that we could use objects for him to handle/manipulate to learn better. I also intend to involve his classmates; he will work in pairs or in small groups and thus will have his friends’ support.\nMs Karim: If his eyesight is greatly affected then we have to help him, through games, to learn the names and voices of his friends by hearing or touch.\nMr Eglo: If his eyes are greatly impaired, then, we will need the help of teachers from specialised schools for blind children to be able to manage, as he will need specific materials for science and mathematics. He will need materials written in Braille, and it might also be helpful to record some lessons for him.\nMrs Dalok: We’ll see. Now, let’s talk about the pupil with a hearing disability, Mr Adji. Most of the arrangements we have talked about will also benefit her. But more specifically, how do you plan to meet the needs of that particular pupil who will be in your class? Please, the rest of you, feel all free to contribute.\nMr Adji: First of all, I will explain to her friends the difficulties that she faces, the precautions they have to take when they will talk to her. In class, I will place her facing away from the light, not far from the blackboard so that she can see my face and also the other pupils’ faces when we are talking and we have to articulate clearly and at a slower pace. I intend to seat her near a good and caring pupil who can help her if needs be. She will also benefit from duplicated lesson notes.\nMr Eglo: For spelling exercises, she can be given texts with mistakes to detect while her friends are writing the text. But if she can do the spelling test, then you must talk at a slower pace and never talk with your back turned. You must also check if she has understood the questions and reframe them in a simpler and different way.\nMrs Dalok: You must also make sure that the teaching aids include objects, toys, games and pictures that will facilitate understanding and assimilation of knowledge to acquire. All the other pupils will benefit from the arrangements that should facilitate learning for all. But in this case, we’ll have to see whether the hearing impairment is acute, how to work with the parents and specialised schools, and how to improve what we do. We may have to learn sign language to be able to communicate with her.\nActivity 31: Pupil support strategies integrated to the teaching\nThis activity will enable teachers to start collecting strategies to vary and adapt activities and resources and to use questioning in order to support pupils.\nSupport integration in the activities\n- Prepare a sheet entitled ‘How to integrate support into the learning activities’.\n- As you read the following documents, write down the ideas you could use to support pupils in doing all suggested activities. If other ideas come to mind, add them. Organise your notes in such a way that they are really clear and useful when you prepare your lessons.\n- Documents to read: Click on the link A directory of activities or look for it in the chapter ‘Planning and preparing lessons to include all pupils’ and read it. Then, read Case study 12: The advice from Mrs Dalok’s School Board above.\nSupport through using varied and adapted resources\n- Prepare a sheet with the title ‘Support by resources’.\n- As you read the following documents, write down the ideas you find on varied and adapted resources. If other ideas come to mind, add them. Organise your notes well so that they are truly useful when preparing your lesson.\n- Documents to read: Resources for all, click on the link or look for it in the chapter ‘Planning and preparing lessons to include all pupils’ from this Toolkit. Then, read Case study 12: The advice from Mrs Dalok’s School Board above.\nSupport using appropriate questions\n- Download the key resource ‘Using questioning to promote thinking’ from the TESSA website\n- While reading this resource, annotate it.\n- Think of the stages of learning in the topic and the level in development of reflection reached by your pupils.\n- Think of your pupils. Which type(s) of question(s) will you use to enable weaker pupils to find the right answer? Which type(s) of question(s) will you ask to promote further thinking for the gifted pupils and further develop their problem-solving skills?\n- If you wish, you could also prepare a sheet Questioning techniques to enable pupil support. This may help you prepare your lessons.\nIf you are working with a colleague, share and discuss your answers and your list of strategies.\nKeep these lists at hand. When you encounter new strategies for assessment of learning, add them to the appropriate list.\nb. Providing support by using group work in the classroom\nActivity 32: Advantages of using group work\nThis activity will allow teachers to think about the best way to use group work to provide different pupils with different support.\n- Alone or with colleagues, brainstorm on the theme ‘Providing support by using group work in the classroom’. (See TESSA key resource ‘Using mind maps and brainstorming to explore ideas’ on the TESSA website.)\n- After brainstorming, read your list of ideas. Would you like to organise them? Would you like to add anything? Feel free!\n- Collect other ideas from the TESSA key resource ‘Using group work in your classroom’ (download it from the TESSA website) and Encouraging collaboration in the chapter ‘A classroom for all in a school for all’.\n- Now, compare your list to the one created by a group of teachers during a session for professional development in Case study 13 below. Which list seems more comprehensive? Don’t hesitate to add other elements to yours if you wish.\nCase study 13: The results of a group of teachers’ brainstorming session on the advantages of using group work in the classroom\n- Pupils learn from each other.\n- More pupils will have the opportunity to talk in the same space of time.\n- Shy pupils will feel safe enough to express themselves.\n- The ideas of pupils who do not dare talk in front of the whole class will be listened to and validated.\n- My deaf pupil can write her contributions and will be given a voice through the group spokesperson.\n- The pupils can make mistakes without feeling threatened in front of the whole class.\n- When well managed, group work promotes collaborative work and the participation of all children.\n- There is less room for teacher talk and thus pupils have greater opportunities to express themselves.\n- When pupils are working in groups, I can circulate among groups, listen to them, take note of the needs of pupils’ individual needs, intervene to provide them with individual support or ask questions that will push them to find more complex solutions.\n- If, as the teacher, I think ahead on the composition of the groups and the type of work I will give to each group, I can work with different groups at different points in times so as to provide the appropriate support to the weakest or to the gifted ones and allow everyone to try the type of work that will stimulate them most.\n- One can give different activities to different groups in order to address any gaps or put in place the activities for progress, according to notes I took in previous sessions.\n- I can distribute different support cards to different groups.\n- I can ask one of the parents helping me in class to work on something specific with specific pupils.\nc. Pupils supporting pupils\nCase study 14: Alassane, a profoundly deaf young man from Senegal, shares some of his experiences at school\n‘There is not a secondary school for children with hearing disabilities in Senegal. When I was in standard 6, on the first day of school, the headteacher made me sit on the front bench. The girl next to me started writing on a piece of paper for me. It was then that I realised the importance of making progress in French.\nFortunately some teachers as well as some classmates started to learn sign language and the little they knew made them become more interested in me and more willing to help me.\nAfter the high-school certificate, I took part in a competitive examination for admission at the Higher Institute of Technology and I was accepted. When we started the new academic year, I was not feeling at ease when the people started to talk. Fortunately, there was a student, MouhamedLamine, who knew American Sign Language and had completed his studies the previous year. He came to sign the first day’s instructions and also the following days’. He also explained to me some of the difficult topics.’\nCase study 15: Idrissa, a child with albinism, talks of the support received during his school years in Zambia.\n… Others supported me in class, as my eyesight is not good. My classmates took notes for me or read from the blackboard; others let me borrow their notebooks so that I could copy the notes at home. Even though I was seated in the first row I had problems seeing. As far as the teachers’ support is concerned, there was a teacher who wrote to the authorities to advise them that there was a pupil who could not see well at school and that all the assessments given should be written in bigger font. And it did happen: they used a bigger font for all the exam papers.\nCase study 16: Lélé has a slight mental deficiency. He is in CE2 in a state primary school in Boukoki in Niger. Sani, his brother, talks about what he does in his free time.\n‘I am in Form 2 and we don’t have lessons in the afternoon during week days. Every week, for one or two afternoons, I go to Lélé’s school. When I arrive, the teacher tells me exactly what he is going to do and what he wants me to do for my brother. Actually, I mostly repeat the instructions to make sure that Lélé understands properly. I observe and guide him when he is doing the activities. I follow the teacher’s recommendations and am very careful not to do the work for Lélé. What I prefer when I go to my brother’s school is to read stories to a small group, and to show them pictures. Seeing and understanding what goes on at my brother’s school is really helpful: I can help him at home and I can also explain to my parents how the teacher helps Lélé.’\nActivity 33: My very special friends, my allies\nThis activity will allow teachers to consider how pupils can help each other and what the consequences are for all.\n- Download the resource ‘Keep your mouth shut!’ from the section Equal opportunities in the Audio resources area on the TESSA website.\n- Listen to this short play.What role does Efe play in relation to Ada?\nNow look back on what Alassane, Idrissa and Sani wrote about their experiences at school in Case studies 14, 15 et 16.\n- Make a list of the roles that other people can play in relation to pupils with special needs.\n- If you have other examples of the roles that other people can play in class, write them down and describe or explain them.\n- Which advantages and disadvantages for the teacher, for those receiving the support or for those providing it and for the whole class, do you see to another person working in the classroom alongside the teacher?\nBefore considering the part played by children, let’s comment once again on the initiative taken by Idrissa’s teacher who, in this particular case, made it possible for the pupil to have access to the exam papers by contacting the examination board. All the children, Efe, Mouhamed Lamine, the girl sitting next to Alassane, Idrissa’s, and Sani’s friends, all acted as an interpreter or intermediary between the teacher and the pupil who has special or specific needs. They too are great friends who help provide access to learning. This role is very important for the child with special or specific needs, but it is paramount to remember that support-children do have the right to education too and the teacher should check that the support they are giving to their classmates or close relatives does not interfere with their own learning and progress.\nd.Support using innovative technology\nNew technologies can provide important support for pupils in the classroom.\nActivity 34: Brainstorming new technologies\nThis activity will allow teachers to identify technologies available in class.\n- Organise a brainstorm (See TESSA key resource ‘Using mind maps and brainstorming to explore ideas’ on the TESSA website) and make a list of the new technologies that are or could be available in class.\nActivity 35: Inventive and creative use of new technologies\nIn this activity teachers will use their knowledge of the pupils with special and specific needs and their own imagination to determine to what extent new technologies can be used as a support.\nFor each new technology you have chosen:\n- write a minimum of one and a maximum of three examples of how this particular technology would support a range of pupils\n- if there are certain specific challenges (excluding the cost) that require special arrangements, note down these challenges and suggestions to overcome them.\nIf you are working with other colleagues from or out of your educational establishment, share your ideas on ways to provide support to all pupils. Are they all realistic?\nIt is impossible to have an exhaustive list of existing possibilities, all the more because of the rapid evolution in technologies. But let’s consider one or two examples.\nMost of us possess a cellphone. While using text messages, a pupil who cannot speak can express his ideas rapidly or ask questions, and a deaf pupil can read instructions or any other short messages that a friend may send by SMS. If the cellphone has an SD card slot, it might be possible to use it to record instructions or texts for pupils with visual deficiencies, which the other classmates will have to read. However, it is likely that you will need to seek the school headteacher’s permission and a discussion will need to take place with the whole class, to agree a Pupil Code of conduct concerning the use of cellphones in class.\nPresumably less frequent among pupils, a laptop that can be shared in class is a precious tool. Word-processing software packages allow them to write, save, recall, correct and improve documents (with or without the teacher feedback): this makes it possible for all children to improve what they create. A laptop can help to present documents neatly and allow pupils with motor difficulties or who find it difficult to hand-write to be proud of their work. Some software packages allow users to write with images or to transform written text to an audio output. With some imagination and creativity, a teacher can use a laptop for a vast array of support at different levels.\nThere are other technologies – such as school radio – that offer a lot of sources that enable activities to be varied and differentiated and to motivate pupils.\nOne should be careful though! New technologies are not the universal panacea: remember that like any other resource, they should be used with good judgment.\ne. Managing support strategies as a way to differentiate\nAll forms of support mentioned above entail pedagogical scaffolding that help construct learning. As pupils gain confidence and develop their own learning strategies, that is, when pupils become more secure in their learning, the scaffolding can be removed progressively.']	['<urn:uuid:3599ae59-0ea7-4341-a18f-e4ba03ded475>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	8	57	2975
59	which came first mounting art or collage technique history origin	Japanese mounting art is historically older, dating back to the Heian Court Period (857-1185 A.D.) when it was used for Buddhist paintings and scrolls. Modern collage technique, while having earlier origins, was formally established much later by Georges Braque and Pablo Picasso in the early modern period as part of the Cubist movement.	['Samuel Peralta here…\nA collage is an artistic technique, whereby a piece of artwork is assembled from fragments of art from numerous sources, creating a new whole.\nWhile its origins may be traced back hundreds of years, modern collage is said to have begun with Georges Braque and Pablo Picasso, who led the Cubist school of painting. Braque is said to have applied the technique first to his charcoal drawings, applying cut swatches of textured wallpaper to his drawings; almost simultaneously, Picasso began pasting materials to his oil paintings.\nBoth Braque and Picasso used the term collage – stemming from the French word ‘coller’, or ‘to glue’ – in discussing this new, modernist technique.\nTheir example led to an explosion of works using this new technique, and today it has become a highly-developed form, rather than a novelty.\nCollages now utilize a plethora of sources – newspapers, photographs, handmade papers, cloth, ribbons. Sometimes artists rip up their own canvases and glue them back together in new ways, creating a new whole.\nIn terms of artistic meaning, collage is a unique medium for commentary on the sources that it assembles. Juxtaposing disparate images or sources produce emotional content that serve to either underscore the original meaning, or provide ironic subtext.\nIn poetry, there is an equivalent structure to that of collage for the visual arts, known as the cento.\nThe cento – derived from the Latin for ‘patchwork’ – is a poetic form composed of passages taken from one or more other authors, but arranged in a new structure or order. Essentially, the cento is a collage of lines from poems by other poets, rearranged to form a new whole.\nPoets ordinarily will borrow lines from other poets, using them as epigraphs or homage quotations. However, a true cento is composed entirely of lines emanating from other sources.\nThe sources for the cento may come from many authors – a line each from different poets, the titles of different novels, quotations from different politicians.\nAlternatively, the sources may come from one author – the poems of a single poet, the stories of a single writer, the daily entries of a diarist.\nIn my case, I have fashioned a homage to a young girl, in my “Cento from the Diary of Anne Frank”.\nHomer and Virgil have written classical centos, and some critics argue that Dante Alighieri’s entire Divine Comedy cycle is a cento, being a repository of numerous classical allusions.\n“Wolf Cento” by Simone Muench, is a representative modern cento that references Anne Sexton, Dylan Thomas, Larry Levis, Ingeborg Bachmann, Octavio Paz, Henri Michaux, Agnes Nemes Nagy, Joyce Mansour, William Burroughs, Meret Oppenheim, Mary Low, Adrienne Rich, and Carl Sandburg.\nThe cento should take care to provide its sources, if they are not obvious or in the public domain – either in the title of the cento, or in footnotes. Additionally, one should be mindful of fair-use; one should do some research, but in general – and I’m not a lawyer, but have done some research – this depends on\n– Purpose and character of the use – Is the use “transformative”, such as a parody, or for that matter, an assemblage?\n– Substantiality of the portion used compared to the work as a whole – Does one use only a small proportion of a work (one line from a sestina is much less substantial than from a haiku)?\n– Market effects – Does the new work displace the original in the marketplace?\nIn general, centos using short lines from numerous sources are not problematic. In the cento above, I’ve used my own adaptations of Frank’s lines rather than using available translations from the original, but have preserved the tenor of the young girl’s voice.\nFinally, we should note that it isn’t enough to simply create an assemblage. The cento should stand on its own. T.S. Eliot has been quoted in this context to great effect: “Immature poets imitate; mature poets steal; bad poets deface what they take, and good poets make it into something better, or at least something different.”\nTonight, let’s see what we can do in terms of something different – I’d love to see what people can do with the form of the cento.\nAs usual, share your work via the link button below, and please, visit your fellow writers to see what they’ve come up with. I know I’m looking forward to it!\nSamuel Peralta – on Twitter as @Semaphore – is the author of four titles in The Semaphore Collection – Sonata Vampirica, Sonnets from the Labrador, How More Beautiful You Are, and Tango Desolado – all of which hit #1 on the Amazon Kindle List of Hot New Releases in Poetry on their debut. His next book in the series, due in October, is War and Ablution.\nCopyright (c) Samuel Peralta. All rights reserved.\nImages public domain / via WikiMedia Commons or as attributed.', 'By mounting is meant the attaching of a sheet of paper to a piece of cloth or perhaps another sheet of paper, on which some Chinese calligraphy or painting has been drawn and then either rolled up (when not in use) or hung on a wall. Another type of mounting is one in which the paper is fixed to a wooden frame and used as a of screen, as in the case of Byobu, or as one type of sliding door-the Fusuma.\nGenerally speaking, however, mountings, called Hyogu or sometimes Hyoso, refer to scrolls or painting hung on a wall, and the classical weaving or mounting cloth used to accent the picture is called Hyosogire.\nHistory Of Mounting\nMounting date back to the Heian Court Period (857-1185 A.D.) when Esoteric Buddhism was being introduced into Japan from China. Paintings of the Buddha and other Buddhist saints which were used as items of reverence in the dissemination of the religion throughout the country, along which Kakejiku or scroll painting called Mandala, which were symbolic depictions of the Buddhist cosmos, formed the basic models for mountings.\nFrom the Kamakura into the Muromachi period the scroll paintings called Shoga (actually they were pictures painted alongside some capsule Buddhist phrase) of the Zen school became widespread. It was during this period that the Tokonoma or alcove became established as part of the unique architecture of the Japanese room. It was their use in the Tokonoma that the form of the mountings came into completion. They were used as decorations and frequently changed to harmonize with the season or the occasion.\nThe Edo Period (1603-1868) saw the appearance of Chagake Hyogu which were mountings hung in the tea-rooms and served as appropriate conversation pieces during the tea-ceremony. The subject matter of the paintings were either calligraphic, called Bokuseki, or Southern Chinese style landscapes painting by famous men of letters. These were called Bunjinga.\nOther types of mountings developed along with the popularization of Buddhism and the tea-ceremony becoming a part of the culture of all levels of the society. Today , with the Westernization of Japanese homes, mountings are no longer found only in the Tokonoma. We can now find them hanging on almost any wall inside picture frames or serving as dividers called Tsuitate, to set off certain areas or parts of a room.\nMounting Materials And Techniques\nThe basic materials used in making mountings are paper and cloth, in addition to some sort of adhesive, and wood. The most important process in the making of mountings is the ancient technique of backing or mounting the painting onto either a piece of cloth or onto some other paper with an adhesive. Lining up and fixing the two pieces requires a highly skilled hand. Even though the same person may use the same materials and the same techniques, the consistency of the adhesive or a slight difference in humidity could produce a totally different result.\nTherefore, painstaking care is an essential element. The cloth used for the backing and the paper on which we have the painting are cut and then joined together. The work of fitting together, adhering, and drying section by section is repeated over and over exposing the piece to various temperatures. Finally, horizontal rods are attached to the top and bottom and the mounting is complete.\nThe Shape of Mountings\nMountings do not exist as independent pieces, that is, as standardized frames. They are part of the painting or calligraphy itself and together form one art piece. A well-balanced mounting is one in which the backing paper or cloth best brings out the qualities of the painting or calligraphy. When preparing mountings, the paper or cloth used as a binding surrounding the picture gives overall shape and proper tone of color to the piece. There is a detailed procedure for deciding the shape, the measurements, the way of putting paper and cloth together, and even the color of the binding paper itself.\nFinally, when not in use, in order to preserve and protect it from damage, the mounting should be rolled up and kept in the paulownia wooden box made especially for this purpose. As the materials for the mounting must harmonize with the picture being mounted, it is natural for there to be various shapes depending on the size, use, or subject matter of the painting. Chagake Hyogu which are closely associated with the tea-ceremony and the unique mountings used for decorative purposes in Western style rooms serve as good examples of the breadth in the variety of mountings.']	['<urn:uuid:09bcdefb-52f5-4e26-a0a9-f58700e42161>', '<urn:uuid:659faec6-1012-4427-b967-aefc753fcbf1>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T10:32:05.666290	10	53	1579
60	research findings how flavivirus rna resists degradation cellular mechanisms exact process	When flavivirus RNAs undergo degradation, they produce a stable intermediate called sfRNA. This intermediate specifically inhibits the function of the host 5'-3' exonuclease, XRN1. The inhibition of XRN1 results in the stabilization of host cell mRNAs, which likely has broad effects on host cell gene expression. This mechanism may also help the viral genomic RNA persist in the host cell cytoplasm.	"[""Viral RNA Decay\nWe believe that the cellular mRNA decay machinery can act as an antiviral defense mechanism. After all, many viral RNAs have evolved to mimic cellular transcripts and should therefore be ideal targets for degradation. A degraded genome cannot be replicated or translated, and therefore presents no threat (Dickson and Wilusz, 2011; Moon, Barnhart and Wilusz, 2012). Several projects in the lab focus on characterizing the interactions between viral RNAs and cellular mRNA decay factors. One virus we have investigated extensively is the alphavirus Sindbis Virus (SINV). We have discovered that SINV RNAs are actually quite resistant to decay, and this is likely because they usurp cellular RNA-binding proteins that act as stabilizing factors (Garneau et al 2008, Sokoloski et al, 2010). Our recent results show that many alphaviruses induce relocalization of the cellular HuR protein from the nucleus to the cytoplasm which presumably interferes with its normal functions (Dickson et al, 2012). We have also shown that the rabies virus glycoprotein mRNA can interact with a poly(C) binding protein to enhance its stability (Palusa et al, 2012).\nInterestingly, another family of arboviruses, the flaviviruses, have evolved a completely different mechanism of interfering with mRNA decay. When their RNAs are degraded, they generate a stable intermediate called sfRNA, which inhibits the function of the host 5’-3’ exonuclease, XRN1. This results in stabilization of host cell mRNAs and likely has wide-ranging effects on host cell gene expression (Moon et al, 2012). It may also help the viral genomic RNA persist in the host cell cytoplasm.\nWe hypothesize that other RNA viruses (e.g. Dengue, Rabies, Ebola) will have evolved similar, or novel mechanisms to protect their genomes from the mRNA decay machinery. Moreover, by interfering with these interactions we should be able to induce viral RNA decay and prevent infection.\nMoon SL, Anderson JR, Kumagai Y, Wilusz CJ, Akira S, Khromykh AA, Wilusz J. A noncoding RNA produced by arthropod-borne flaviviruses inhibits the cellular exoribonuclease XRN1 and alters host mRNA stability. RNA. 2012 Epub ahead of print.\nDickson AM, Anderson JR, Barnhart MD, Sokoloski KJ, Oko L, Opyrchal M, Galanis E, Wilusz CJ, Morrison TE, Wilusz J. Dephosphorylation of HuR Protein During Alphavirus Infection Is Associated with HuR Relocalization to the Cytoplasm. J Biol Chem. 2012 Aug 22.\nPalusa S, Ndaluka C, Bowen RA, Wilusz CJ, Wilusz J. The 3' Untranslated Region of the Rabies Virus Glycoprotein mRNA Specifically Interacts with Cellular PCBP2 Protein and Promotes Transcript Stability. PLoS One. 2012;7(3):e33561.Dickson AM, Wilusz J. Strategies for viral RNA stability: live long and prosper. Trends Genet. 2011 Jul;27(7):286-93.\nSokoloski K.J., Dickson, A.M., Chaskey E.L., Garneau, N.L., Wilusz C.J. and Wilusz J. (2010) Sindbis Virus Usurps the Cellular HuR Protein to Stabilize Its Transcripts and Promote Productive Infections in Mammalian and Mosquito Cells. Cell Host Microbe. 8(2):196-207\nGarneau N.L., Sokoloski K.J., Opyrchal M., Neff C.P., Wilusz C.J. and Wilusz J. (2008) The 3' untranslated region of Sindbis virus represses deadenylation of viral transcripts in mosquito and mammalian cells. J Virol. 82:880-92\nOpyrchal M., Anderson J.R., Sokoloski K.J., Wilusz C.J. and Wilusz J. (2005) A cell-free mRNA stability assay reveals conservation of the enzymes and mechanisms of mRNA decay between mosquito and mammalian cell lines. Insect Biochem Mol Biol. 35:1321-34.\nOther Labs working on Viral RNA Decay\nKaren Beemon - Rous Sarcoma Virus - Johns Hopkins University""]"	['<urn:uuid:ada29c37-6c6f-4e8e-a9e3-10df093916ae>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	11	61	554
61	I'm curious about how people dealt with threats in different eras - how do modern cybersecurity measures compare with how European countries protected themselves from threats in 1914?	In 1914, European powers relied on military mobilization and strict censorship of media to protect against threats, while also using propaganda to create stereotypes of enemies and promote nationalist unity. In contrast, modern security protection focuses on technical measures like antivirus software, email encryption, two-factor authentication, and device management apps. While 1914 emphasized collective state-controlled protection, today's approach focuses on individual responsibility with specific guidelines for protecting devices, using secure networks, and maintaining password hygiene.	"['Chauvinistic rhetoric and martial sabre-rattling dominated the critical days of July 1914. The belligerent vocabulary of the political elites struck a chord in hearts and minds all over Europe.\n‘If anyone has something to say, let him come forward – and be silent!’\nQuoted from Christian Wagenknecht (ed.), Weltgericht I: In dieser großen Zeit (Frankfurt am Main, 1988), p. 9\n‘I know no party any more: I only know Germans.’\nQuoted from Wolfdieter Biehl, Der Erste Weltkrieg 1914–1918. Chronik – Daten – Fakten, (Vienna/Cologne /Weimar, 2010), p. 64\nAlthough the ardent martial fervour that overcame large parts of Europe in the summer of 1914 is hardly comprehensible to us today, there were several reasons for the positive image that war possessed in the popular mind of the time. Firstly, nobody in Europe had any experience of the kind of industrialized ‘war of the masses’ that the First and Second World Wars turned out to be. There was little knowledge of the horrors of war, especially now that the last large-scale European wars (such as the Franco-Prussian War of 1870/71) were long past. Although there had been wars in Europe in the decades around 1900, for the great powers these had ‘only’ been localized proxy wars in the colonies or on the periphery of Europe (the Balkan Wars of 1912 and 1913).\nAt the beginning of the war there was enormous patriotic euphoria in all the countries concerned. The capitals witnessed patriotic rallies at which the most significant participating group was the urban middle class. The streets resounded with cries for a national closing of ranks and for a truce in which internal tensions and conflicts were to be set aside in the interest of ‘Burgfriede’ (‘peace within the castle precincts’). Rich and poor, right-wingers and left-wingers: the whole ‘national community’ was now to pull together for a common goal. This well-orchestrated vision of the nation as a collective body is referred to by historians as the ‘August experience’.\nIn parallel with the military mobilization there was also a mobilization on the level of inner values. Existing prejudices were intensified and new stereotypes of the enemy created. The Habsburg Monarchy presented itself as a superior ‘bulwark of civilization’, whose ‘honour’ had to be defended against the ‘barbarian East’, meaning the Serbs and the Russians. The alliance with the German Empire was continually invoked, principally with calls for the blind faith summed up in the watchword ‘Nibelungentreue’ (‘Nibelung loyalty’); and the rest was done by phenomena typical of the time such as social Darwinism (‘Only the strongest survive!’), the glorification of force, and the cult of manliness (‘soldier’s honour’).\nMany of the actions described in the press as ‘spontaneous expressions of the people’s will’ were in fact engineered or directed by the government or the military. There was very much less questioning of state orders than there is today. In most Central European states a patriarchal authoritarian state meant that large parts of the population (women, the poor, and others) were totally excluded from the decision-making process. Hardly any critical voices were raised in the media, because censorship came into operation as soon as the war broke out, extending firm control over the newspapers and other channels for the expression of public opinion.\nNevertheless, there were clear signs of scepticism and of reservations concerning the war and its consequences. Peace initiatives were mounted to hold back the tide of war, with peace rallies taking place in various towns. Not everyone identified to the same extent with the war goals prescribed by the state authorities as necessary for the nation.\nBut the war was also seen as a catalyst in the fight against traditional social systems that were perceived to be unjust and in the battle against the decadence of the fin-de-siècle elites. The Social Democrats, for example, were in principle in favour of maintaining peace, but at the same time saw the outbreak of war as a chance for a ‘just struggle’ against anti-democratic regimes in Europe.\nThe special structure of the Habsburg ‘state of many peoples’ meant that there was another dimension to the whole scenario, that of nationalism, for it was among the two largest national groups, the Germans and the Magyars, that the war euphoria was at its greatest. This was in a sense paradoxical, as the war aims of the Habsburg Monarchy were in fact in opposition to German and Magyar national interests. The gaining of further Polish or southern Slav territories would have increased the proportion of Slavs in the Monarchy as a whole, thus posing a further threat to German and Magyar hegemony.\nNevertheless, the other nationalities were likewise seized by general wave of patriotic feeling. The official positions of the Czech, Polish and southern Slav national representatives showed that they were not really sure what to do. Although they initially hastened to make public statements of loyalty to the Habsburg Monarchy and its war aims, they gradually drifted into a state of political passivity. At the beginning of the war there was no active political resistance to the Monarchy’s war policy.\nTranslation: Peter John Nicholson\nBihl, Wolfdieter: Der Erste Weltkrieg 1914–1918. Chronik – Daten – Fakten, Wien/Köln/Weimar 2010\nHamann, Brigitte: Der Erste Weltkrieg. Wahrheit und Lüge in Bildern und Texten, 2. Aufl., München 2009\nHanisch, Ernst: Der lange Schatten des Staates. Österreichische Gesellschaftsgeschichte im 20. Jahrhundert [Österreichische Geschichte 1890–1990, hrsg. von Herwig Wolfram], Wien 2005\nHirschfeld, Gerhard/Krumeich, Gerd/Renz, Irina (Hrsg.): Enzyklopädie Erster Weltkrieg. Aktualisierte und erweiterte Studienausgabe, Paderborn/Wien [u.a.] 2009\nLeidinger Hannes/Moritz, Verena: Der Erste Weltkrieg, Wien [u.a.] 2011\nRauchensteiner, Manfried: Der Erste Weltkrieg und das Ende der Habsburgermonarchie 1914–1918, Wien u. a. 2013\n- The enthusiasm for the war\n- ‘Brothers in arms’: Austria-Hungary and Germany as partners and allies\n- Front lines – The course of the war 1914–16\n- Italy enters the war\n- The impact of the war on civilian society\n- The accession of Emperor Karl\n- The Sixtus Letters – Karl’s quest for a way out\n- Karl’s bid for freedom\n- The Russian Revolution and its consequences\n- 1917 – The turning point', ""Everyone's working remotely these days, yet security risks remain. Here are 10 ways you can combat online security threats.\nShare this article\nThere can’t be many businesses today that don’t use remote working to some extent throughout the working day. Even those without a culture or need to offer remote working will have employees or directors taking work home, or working from hotel suites, conference venues and public transport at times.\nThis more casual form of remote working, one that may not be accounted for when analysing how business IT networks are used, is often missed in cyber security policies and procedures. However, it is one important factor that can put organisations at risk of cyber attacks and data breaches.\nRemote working, whether a formalised arrangement between a business and an employee, or an ad hoc ‘needs must’ requirement to get work done, can leave your business IT network, systems and devices vulnerable.\nThe first step for managing security and remote workers is to understand where your business is at risk. This should be followed with an awareness raising campaign within the organisation so that all employees understand how their actions may compromise security and what steps they must take to protect company networks and systems.\nCyber security policies need to include the specific risks associated with remote working, with procedures and guidance in place for working away from the office. This will also need to explain what actions need to take place if a remote worker believes they have exposed the company to a cyber attack, and any disciplinary measures that may be taken.\nThe following top tips provide an excellent starting point:\n1. Keep mobile devices and laptops safe\nLost and stolen mobile devices and laptops are easy pickings for cyber criminals if insufficient security measures are in place. The first line of defence is to look after these business assets: keep them with you and in sight at all times, and never leave them in hotel safes, cars etc.\nNext up is securing the devices themselves with good password hygiene and encryption on laptops. Finally, installing mobile device management apps such as AirWatch and MaaS360 give employees a chance of securing and recovering lost mobiles or tablets.\nRemember: it's not just cyber crime that can disrupt your IT\n2. Excellent password hygiene\nStrong passwords will not only protect your devices and systems being accessed if a mobile or laptop is lost or stolen, they also protect businesses from hackers. Good password hygiene includes using long passwords with multi-characters, two-step authentication processes, and unique passwords for different systems and logins.\n3. Ensure up-to-date security protection is in place\nAny devices that are owned by the organisation should be properly protected with antivirus, web filtering, firewalls, device encryption and other preventative software, but so too should your employees’ own devices if they are using them for remote working.\nThis can be a difficult area to negotiate as your employee may feel this impinges on the personal use of their device: Your cyber security policies will need to address issues like these, either restricting staff from using their own devices for certain business critical activities, providing secure company owned devices, or making your cyber security protection mandatory.\n4. Use of public wifi\nPublic wifi can be vulnerable to malicious attack, presenting issues for those employees who may need to work from a hotel or conference. While it is good advice to only connect to trusted networks this is not always feasible.\nTherefore, your remote working / cyber security policy should stipulate that employees should not use public wifi for any sensitive, business critical activities. It is advisable to draw up some guidelines that explain what systems and activities staff can and cannot access when using public wifi.\n'Quick question Dave: where's all our money gone?\n5. Email encryption and best practice\nEmail is perhaps the most used digital technology by staff members who are away from the office, and one that can open a backdoor to cyber criminals. Encryption and robust management of corporate email is a must.\nThe installation of applications such as Mimecast is a no brainer, but raising awareness of the vulnerabilities of email will also help embed best practice in your organisation. This can include training in spotting cyber threats like phishing emails, and also policies on what information should not be communicated in an email – for example logins and passwords.\n6. Using public computers\nWhile the majority of people will have their own laptop or mobile device that they use for remote working, occasionally someone may need to use a public computer such as in a business suite in an airport.\nEmployees should be aware of the security implications of this and adhere to the following guidance: keep screens private (position them away from other people), don’t use public computers for any sensitive information, use ‘private browsing’ where possible, never use ‘remember me’ or ‘save information’, and clear your browsing history and delete any downloads before closing the browser.\n7. Using devices when out and about\nEmployees should also be aware of physical threats when using devices when in public places like cafes, hotels, airports etc. Just as you would hide your PIN when using an ATM, employees should be discreet when keying in passwords and logging into systems.\nThey should also be aware of the risk of snooping and eavesdropping, not just online, but also from other people in the vicinity. Can someone see and potentially grab a discreet photo of company sensitive information while they work in a public space?\n8. Removable devices\nUSB sticks and other removable devices can be a source of malware and should be checked first. Many conferences hand out USB sticks that may be infected, often unbeknown to the organisers. Also don’t allow anyone to plug in a USB device into your computer, for example to share information in a meeting. Always get your IT department to security check removable devices.\n9. Monitoring and policy enforcement\nTwenty four-seven network monitoring and security will help your organisation identify threats and monitor users on your networks. Remote workers and their mobile devices can be monitored using this solution to protect your organisation’s network.\n10. Negligence and accidental risks in the home\nEven when your employees are working from home using your secure VPN, VDI or remote desktop, there can be other risks that need to be considered. Children and pets can be a surprising threat.\nCats have a habit of jumping on computer keyboards and inquisitive minds might press a few keys when a laptop is unattended. These kinds of risks should be addressed in your remote working / security policies to ensure that your staff take every feasible step to protect your systems at all times.""]"	['<urn:uuid:f335277a-0128-43c5-816e-706a822b21f4>', '<urn:uuid:02598588-2ea6-4379-b2e0-6b47df581446>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:32:05.666290	28	75	2143
62	What equipment does John Scofield use to create his sound?	John Scofield mainly uses an Ibanez AS200 guitar and a Vox AC30 amp from the '90s. However, he believes that guitarists get their sound primarily through their fingers, with equipment choice having some influence.	"[""Guitarist John Scofield's new album Uberjam Deux (Decca) is a fascinating and energetic amalgam of jazz, R&B, soul and rock. Finding novel ways to merge music styles has long been a way of life for John. He was at the heart of the jazz-fusion movement in the early '80s, playing with Miles Davis, Pat Metheny, Billy Cobham and others. John is intensely curious, and his musical taste is eclectic and diverse. As a result, his compositions are often a dynamic braid of concepts, unified by his signature, metallic-meowing jazz-rock guitar style.\nUberjam Deux's beat-driven hybrids range from funk (Boogie Stupid), romantic soul (Al Green Song), Jamaican rocksteady (Camelus) and a Philly soul cover (Just Don't Want to Be Lonely). The beauty of John's music is that it combines influences and always stretches the form without losing the listener. In this regard, songs on his new album emerge as steel sculptures, with pieces affixed here and there—some rotating, some swinging.\nI asked John, 61, about his approach...\nJazzWax: Your new album is highly eclectic with a range of intriguing beats. How does it compare with your earlier jazz-R&B albums?\nJohn Scofield: It was so much fun to get together with these guys after 10 years. I think we all played well because of the lift of our reunion. On the previous records—Uberjam and Up All Night—the songs were either tunes I composed or tunes that came out of group jam sessions. [Photo above of Uberjam]\nJW: And on the new one?\nJS: On Uberjam Deux, most of the tunes are composed by Avi Bortnick (rhythm guitar and samples) and myself. When we decided to do the recording, Avi gave me free reign to take some things he'd composed and change them and add sections that I composed. I think the writing is stronger on the new CD than on the other ones. [Photo above of Avi Bortnick]\nJW: Do you hunt for beats and then play them back for the band?\nJS: A lot of these tunes started with grooves that Avi came up with and had demoed. The bass and drums work out parts to fit with the sampled percussion grooves. On the tunes I wrote alone, I made primitive home demos with my cheapo Zoom recorder just to give everybody an idea. I compose with the players in mind, and the band members always modify their parts to fit their own personal style.\nJW: Are you on a mission to jazzify today's musical language?\nJS: I like to think I'm still improving as an improviser, increasing my musical vocabulary. I continue to love the stuff I loved before I started studying modern jazz—blues, R&B and all the great stuff we heard on the radio in the ‘60's. I guess I “jazzify” stuff unconsciously because that's just what I do. I’m not on a mission, just always in a moment.\nJW: How do you strike a balance—not leaning too far into R&B and holding too firm to jazz?\nJS: I think it's all instinct. That's what tells me if I’ve crossed the line. I’ve seen the connections between R&B, soul and jazz since I started playing guitar. They’re all branches on the same tree. I saw in an interview that B.B. King [pictured] objected to being called a “blues” artist and preferred being called a “jazz” guitarist. That's interesting, coming from the King of the Blues, right? I continue to learn about and practice straight jazz as well as the dreaded back-beat music.\nJW: Did you introduce Miles to the contemporary pop tunes featured on You're Under Arrest in 1985?\nJS: Oh not at all. Miles was always very familiar with current pop stuff and checked it out on his own. I think he always had—even in the '50s and before. He actually discouraged us from listening to old music so we remained current and cutting edge.\nJW: Why did he choose Human Nature and Time After Time?\nJS: We recorded a bunch of pop tunes at the time, but Miles believed he could only do one take on them before he got stale. Human Nature and Time After Time were the only ones where he liked his playing, I guess. I remember he wanted to try Life Begins With You by DeBarge. He had me write up a lead sheet and he played it beautifully, but we never got around to it.\nJW: You've always had a bold, distinct sound that takes listeners back to the 70s and yet propels them forward. How is the Scofield sound created?\nJS: I think all guitarists get their sound through their fingers. Equipment choice matters some, too. I mostly use my trusty Ibanez AS200 guitar and a Vox AC30 amp from the ‘90s—the new ones are different. I like to bend notes and play with vibrato and expressive articulation. I think that can be missing in the orthodox jazz guitar approach.\nJazzWax tracks: Uberjam features John Scofield on guitars, Avi Bortnick on rhythm guitar and samples, Andy Hess on bass, Adam Deitch and Louis Cato on drums (different tracks) and special guest John Medeski on organ, Wurlitzer and Mellotron. You'll find John Scofield's Uberjam Deux (Decca) here.\nJazzWax clip: Here's John's Al Green Song...""]"	['<urn:uuid:8fddfcaa-bbb9-44a4-99cd-f13de97a6548>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T10:32:05.666290	10	34	872
63	How do the theoretical frameworks for capital obsolescence compare with the historical development of economics education at Brown University?	The theoretical framework shows that capital lifetime increases with disembodied technical progress but decreases with embodied technical progress, featuring complex interactions between obsolescence and depreciation rates. In contrast, Brown's economics education evolved from a simple framework under Diman in 1869 focusing on natural laws of wealth accumulation, to a more comprehensive approach by the 20th century that encompassed business courses, research bureaus, and specialized economic studies.	"['Technological Progress, Obsolescence and Depreciation\nWe construct a vintage capital model à la Whelan (2002) with both exogenous embodied and disembodied technical progress, and variable utilization of each vintage. The lifetime of capital goods is endogenous and it relies on the associated maintenance costs. We study the properties of the balanced growth paths. First, we show that the lifetime of capital is an increasing (resp. decreasing) function of the rate of disembodied (resp.embodied) technical progress. Second, we show that both the use-related depreciation rate and the scrapping rate incease when embodied technical progress accelerates. However, the latter drops when disembodied technical progress accelerates while the former remains unaffected. A key feature of our model is that the age-related depreciation rate does depend on the obsolescence rate in sharp contrast to the neoclassical model.\n|Date of creation:||01 Mar 2006|\n|Contact details of provider:|| Postal: Place Montesquieu 3, 1348 Louvain-la-Neuve (Belgium)|\nFax: +32 10473945\nWeb page: http://www.uclouvain.be/econ\nMore information through EDIRC\nReferences listed on IDEAS\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on ""citations"" and make appropriate adjustments.:\n- Boucekkine, Raouf & Germain, Marc & Licandro, Omar & Magnus, Alphonse, 1998.\n""Creative Destruction, Investment Volatility, and the Average Age of Capital,""\nJournal of Economic Growth,\nSpringer, vol. 3(4), pages 361-384, December.\n- R. Boucekkine & M. Germain & O. Licandro & A. Magnus, ""undated"". ""Creative destruction, investment volatility, and the average age of capital,"" Working Papers 97-08, FEDEA.\n- BOUCEKKINE, Raouf & GERMAIN, Marc & LICANDRO, Omar & MAGNUS, Alphonse, ""undated"". ""Creative destruction, investment volatility, and the average age of capital,"" CORE Discussion Papers RP 1376, Université catholique de Louvain, Center for Operations Research and Econometrics (CORE).\n- Epstein, L. & Denny, M., 1980. ""Endogenous capital utilization in a short-run production model : Theory and an empiral application,"" Journal of Econometrics, Elsevier, vol. 12(2), pages 189-207, February.\n- Ellen R. McGrattan & James A. Schmitz, 1999. ""Maintenance and repair: too big to ignore,"" Quarterly Review, Federal Reserve Bank of Minneapolis, issue Fall, pages 2-13.\n- Feldstein, Martin S & Rothschild, Michael, 1974. ""Towards an Economic Theory of Replacement Investment,"" Econometrica, Econometric Society, vol. 42(3), pages 393-423, May.\n- Beaudry, Paul & van Wincoop, Eric, 1996. ""The Intertemporal Elasticity of Substitution: An Exploration Using a US Panel of State Data,"" Economica, London School of Economics and Political Science, vol. 63(251), pages 495-512, August.\n- Gylfason, Thorvaldur & Zoega, Gylfi, 2001. ""Obsolescence,"" CEPR Discussion Papers 2833, C.E.P.R. Discussion Papers.\n- Greenwood, Jeremy & Hercowitz, Zvi & Huffman, Gregory W, 1988. ""Investment, Capacity Utilization, and the Real Business Cycle,"" American Economic Review, American Economic Association, vol. 78(3), pages 402-417, June.\n- BOUCEKKINE, Raouf & RUIZ-TAMARIT, Ramon, ""undated"". ""Capital maintenance and investment: complements or substitutes?,"" CORE Discussion Papers RP 1630, Université catholique de Louvain, Center for Operations Research and Econometrics (CORE).\n- BOUCEKKINE, Raouf & RUIZ-TAMARIT Ramon, 2001. ""Capital Maintenance and Investment : Complements or Substitutes ?,"" Discussion Papers (IRES - Institut de Recherches Economiques et Sociales) 2001012, Université catholique de Louvain, Institut de Recherches Economiques et Sociales (IRES).\n- Michael J. Geske & Valerie A. Ramey & Matthew D. Shapiro, 2007. ""Why Do Computers Depreciate?,"" NBER Chapters,in: Hard-to-Measure Goods and Services: Essays in Honor of Zvi Griliches, pages 121-150 National Bureau of Economic Research, Inc.\n- Michael J. Geske & Valerie A. Ramey & Matthew D. Shapiro, 2004. ""Why Do Computers Depreciate?,"" NBER Working Papers 10831, National Bureau of Economic Research, Inc.\n- Karl Whelan, 2002. ""Computers, Obsolescence, And Productivity,"" The Review of Economics and Statistics, MIT Press, vol. 84(3), pages 445-461, August.\n- Karl Whelan, 2000. ""Computers, obsolescence, and productivity,"" Finance and Economics Discussion Series 2000-06, Board of Governors of the Federal Reserve System (U.S.).\n- Karl Whelan, 2002. ""Computers, obsolescence, and productivity,"" Open Access publications 10197/204, School of Economics, University College Dublin.\n- Karl Whelan, 2000. ""Computers, obsolescence, and productivity,"" Open Access publications 10197/244, School of Economics, University College Dublin.\n- Greenwood, Jeremy & Hercowitz, Zvi & Krusell, Per, 1997. ""Long-Run Implications of Investment-Specific Technological Change,"" American Economic Review, American Economic Association, vol. 87(3), pages 342-362, June.\n- Greenwood, J. & Hercowitz, Z. & Krusell, P., 1995. ""Long-Run Implications of Investment-Specific Technological Change,"" UWO Department of Economics Working Papers 9510, University of Western Ontario, Department of Economics.\n- Greenwood, J. & Hercowitz, Z. & Krusell, P., 1996. ""Long-Run Implications of Investment-Specific Technological Change,"" RCER Working Papers 420, University of Rochester - Center for Economic Research (RCER).\n- Patrick Musso, 2004. ""Productivity Slowdown and Resurgence. The Role of Capital Obsolescence,"" Revue économique, Presses de Sciences-Po, vol. 55(6), pages 1215-1239.\n- Nickell, Stephen, 1975. ""A closer look at replacement investment,"" Journal of Economic Theory, Elsevier, vol. 10(1), pages 54-88, February.\n- Bruce W Hamilton & Molly Macauley, 1996. ""Competition and Car Longevity,"" Economics Working Paper Archive 361, The Johns Hopkins University,Department of Economics.', 'Office of University Communications\nFrom Martha Mitchell’s Encyclopedia Brunoniana:\nEconomics was introduced as “political economy” by Francis Wayland in 1828. William Gammell taught history and political economy from 1850 to 1864, followed by J. Lewis Diman from 1864 to 1881. The teaching of political economy continued as a subordinate duty of the history professor until late in the nineteenth century. When E. Benjamin Andrews resigned as professor of history, it was decided that the professorship of history and political economy should be divided into two parts. Henry Brayton Gardner was appointed associate professor of political economy in 1888, and a separate Department of Political Economy was first listed in the catalogue in 1889. The change of name to Department of Economics took place in 1906.\nWhen Professor Diman reported on his class in political economy in 1869, he stated that “Instruction has been by lectures, and has included a general introduction to the study with an examination of the laws of Production, Exchange, Distribution and Consumption. The general design of the course has been to demonstate that wealth accumulates strictly in accordance with natural laws; that the interests of society are harmonious; and that all restrictions upon exchange are injurious.” The subject matter of economics has enlarged considerably since that time. A more timely definition is that economics is the study of rational decision making in the allocation of scarce productive resources among alternative goals. In the twentieth century students were beginning to ask for business courses, and in 1916 the University responded, first, by the presentation of a series of lectures sponsored by Edgar L. Marston on South American business and trade, and, second, by entering into cooperation with the National Bank as one of a group of selected universities invited to recommend each year three seniors and three juniors or sophomores to be trained by the bank for future service in its foreign branches.\nIn the 1920s the department grew through the addition of faculty members James P. Adams in 1921, Hugh B. Killough, Harry E. Miller, and James H. Shoemaker in 1924, Albert F. Hinrichs in 1926, George E. Bigge in 1927, Williams Adams Brown in 1928, and Chelcie C. Bosland in 1929. The Brown Bureau of Business Research was formed in 1921 by the Providence Chamber of Commerce and the Brown Department of Economics for the purpose of effecting a close business contact between Rhode Island business and the department. This came about largely through the initiative of Professor Ralph E. Badger of the Economics Department, who interested several prominent business men in the project. Its articles of agreement defined its primary purpose as the maintenance and operation of a research laboratory to study problems relating to commerce and industry in metropolitan Providence. In 1924 the Bureau inaugurated its official publication, Brown Business Service, subtitled “Analyses of Economic Conditions in Southeastern New England,” for distribution of timely information to the business community monthly from September to July. In 1927-28 the Bureau cooperated with the Division of Industrial and Municipal Research at M.I.T. in the industrial survey of metropolitan Providence. The Bureau continued as an agency of the University until 1939. In 1932 a grant was received from the Rockefeller Foundation for a study of the international gold standard under the direction of William Adams Brown and Carel J. Smit, who came to Brown from Amsterdam in 1932.\nA textbook, Introduction to War Economics, written by members of the Department of Economics and edited by Professor Alfred C. Neal, was published in 1942. The authors, who began to write the book before the United States’ declaration of war on December 8, 1941, rushed to finish it in early January, and in six months 7,000 copies had been sold and were in use in more than one hundred universitites and colleges. Philip Taft, who became a noted labor historian, joined the faculty in 1937, and was chairman from 1949 to 1953. In 1952 Brown entered into cooperation with a newly appointed Business Executives’ Research Committee from the community to study the economic problems of Rhode Island with grants from the nationwide Committee for Economic Development and the Ford Foundation’s Fund for Adult Education. These studies were conducted by Professor Merton P. Stoltz, who had joined the department in 1940. Professor Stoltz later served as chairman from 1957 to 1964, and went on to become Provost and acting President of Brown. In the 1950s the department grew through the addition of faculty members George H. Borts in 1950, Jerome L. Stein in 1953, Michael J. Brennan in 1957, Phillip D. Cagan in 1958, and Martin J. Beckmann and Mark B. Schupack in 1959. In 1958 the department received a grant from the Ford Foundation to study regional economic development in the United States. The study was carried out jointly by Professors George H. Borts and Jerome Stein. A graduate program in regional economic development was established. In 1963 the department received a grant from the Ford Foundation to study the economics of aging. The study was carried out under the direction of Professor Philip Taft, together with Professors Michael J. Brennan and Mark B. Schupack.\nIn the 1960s the department grew through the addition of faculty members Herschel I. Grossman in 1964, James A. Hanson and Harl E. Ryder in 1965, Benjamin Chinitz in 1966, and Ryuzu Sato in 1967. New faculty members in the 1970s included Allan Feldman in 1971, J. Vernon Henderson and William Poole in 1974, and in the 1980s Louis Putterman in 1980, Rajiv Vohra in 1983, Oded Galor and Robert Moffitt in 1984, Peter Garber in 1985, Talbot Page in 1986, Anthony Lancaster in 1987, and Mark Pitt in 1989. From 1968 to 1980 the department housed the editorial offices of the American Economic Review, the main publication of the American Economic Association. During this period George H. Borts served as managing editor of the Review.\nAmong the chairmen of the department have been James P. Adams, George E. Bigge, Hugh B. Killough, Chelcie C. Bosland, Philip Taft, Merton P. Stoltz, George H. Borts, Benjamin Chinitz, Mark B. Schupack, Harl E. Ryder, William Poole, Herschel I. Grossman, and Rajiv Vohra. The department offers a variety of courses in micro- and macro-economics, labor markets, industrial organization, financial markets and institutions, public finance, international trade and finance, monetary theory and policy, econometrics, regulation, economic development, general equalibrium, game theory, welfare economics, and urban economics. In addition to the standard undergraduate concentration in economics, there is a concentration in business economics. The department also offers a bachelor of science and a bachelor of arts degree in Applied Mathematics and Economics, and a graduate program leading to the Ph.D. degree. There is no master’s degree program as such, but the degree is awarded to students who have successfully completed the first year of the Ph.D. program.\nThe above entry appears in Encyclopedia Brunoniana by Martha Mitchell, copyright ©1993 by the Brown University Library. It is used here by permission of the author and the University and may not be copied or further distributed without permission.']"	['<urn:uuid:f68c461a-e319-40f6-89cc-69f1b896bdc2>', '<urn:uuid:14f27b3a-8fe8-4de3-ba9f-fc10a74afefc>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T10:32:05.666290	19	66	1997
64	nurgle pestilaan light cruiser crew living conditions versus tau base creation workspace safety hazards differences	The working conditions for creating Tau bases are relatively safe, requiring only basic materials like petroleum jelly, Milliput, clay, and tools like textured rollers and cutters. In stark contrast, the Pestilaan Light Cruiser presents extremely hazardous conditions with rotting crew quarters, fungal respirators, and crews infected with horrific diseases. The ship's environment is so toxic that any boarding attempts automatically fail, and it can spread diseases to other ships with a 50% chance of causing long-term ship sickness, requiring Very Hard Medicae tests to contain.	"['This post is also available in: Español (Spanish)\nWould you like to give a special touch to your Tau army? In that case you will like this tutorial, we will see how to create custom bases for the Tau army of the Warhammer 40,000 wargame.\nTo create these custom bases for T’au, we will use the GreenStuffWorld Tau textured roller. Also, if we want to cut them in a circular way,we will need the circular base cutters. The silicone guide rings can help us too, but in this particular tutorial they have not been necessary.\nIn addition, we will use a mixture of Milliput and standard clay.\nWe will start by mixing 50-50 the milliput with the clay, (we will use the same amount of milliput and clay).\nWhy are we doing this? Firstly to spend less Milliput,since the clay is something cheaper, and on the other hand, because by adding some clay the textured roll will stick less to the mixture.\nTextured rolling pin\nOnce we have the mixture (take your time to become a homogeneous mass), we will flatten this mixture. To do this, we can even use the container in which the textured rolling pin comes. It is important that if we use this container, we have the roller pin inside so that it has the proper consistency.\nAfter that, we will apply petroleum jelly or lipstick over the entire flattened surface. We will do this so that in the next step when we pass the textured roller, the mass does not adhere to the roller.\nOnce we pass the roller slowly, the result will be the following:\nIt is possible that even if you have applied petroleum jelly or lipstick, some bits could stick to the roller. A trick to remove these leftovers easily is to use blutack to remove them.\nIf we want circular bases, we will use the circular cutters from GSW, choosing the measure that corresponds to our base:\nWe will cut them easily using those tools and with the help of a spatula we take off the bases from our working table:\nIrregular bases – Rock bases\nAnother way is to create some irregular stones or rocks,rather than circular ones. Those are my favorite ones. To do this, we will just pass the textured rolling pin and we will let the putty dry. Here are some examples:\nOnce the putty is dry, we will break different bits depending on the size we want. The idea is to put them over cork pieces.\nUsing a modeling blade,we will adapt these bits to the cork.\nTo finish with the creation of these bases, we will fill with some kind of putty the space between the cork and the milliput pieces. In this case I have used Desert Sand but you can use any putty or even sand directly using PVA glue. Here you can see the result after applying the sand:\nHere we would have the final result.\nAs you’ll see in the next tutorial, I’ll explain how to paint them step by step and how well they look on circular bases:', ""[ Models | Lore | Sources | Inspiration ]\nIn the Warhammer 40,000 setting, the Pestilaan Light Cruiser is a starship used by the followers of Nurgle, corrupted to suit their preferred environmental conditions.\nThe name is a reference to Pestilaan, a character created by Tim Huckelbery (then the head of Games Workshop USA Customer Services Department) and published in Citadel Journal #17 in 1996. By 2011, when Battlefleet Koronus was published, Tim was working at Fantasy Flight Games and (amongst other roles) was a writer/developer on Battlefleet Koronus, sneaking in a reference to his old character's name. It is explicable in that the Imperium's classification of the starship may relate to them first encountering such a vessel while it was under the command of Pestilaan.\nHull: Light Cruiser\nClass: Chaos Pestilaan-class light cruiser\nDimensions: 5 km long, .9 km abeam approx.\nMass: 39 megatonnes approx.\nCrew: 50,000 crew approx., plus countless lesser daemons\nAcceleration: 2 gravities max sustainable acceleration\nA disturbing predator in the Expanse, Pestilaan-class ships appear bloated and sluggish with tarnished, rusted metal, open sores scattered across the hull and odd organic protrusions. Such appearances belie the power of such a vessel, as its very diseased nature drives the energies that sustain both it and its crew. It is thought that the Pestilaan class is a long forgotten ancient light cruiser, upgunned and armoured at the price of speed. Most of the Pestilaans are thought to be these once-proud vessels, now debased into unclean servitude after mutiny or capture by the Archenemy.\nVoid Shields: 1\nHull Integrity: 65\nCrew Population: 100\nCrew Rating: Competent (30)\nTurret Rating: 1\nWeapon Capacity: 1 Dorsal, 1 Prow, 1 Port, 1 Starboard\nRetrofitted Saturine-pattern drive, Warp Drive, Malfunctioning Gellar Field, Void Shield Array, Shrine Bridge, Rotting Crew Quarters, Fungal Respirators\nPort and Starboard Hellus Macrocannon Broadsides: (Macrobattery; Strength 5; Damage 1d10+3; Crit Rating 4; Range 6)\nDorsal Melta-cannons: (Macrobattery; Strength 3; Damage 1d10+4; Crit Rating 3; Range 3)\nProw Torpedo Tubes: (Torpedo Tubes; Strength 4; Damage 2d10+10; Range 40; Terminal Penetration ) These torpedo tubes are loaded with virus torpedoes (see page 8) but can be loaded with boarding torpedoes. This Component has 32 torpedoes.\nPlagueship: These vessels are resplendent with the very finest of sicknesses and diseases, to the point where any attempt to enter it is unthinkable. Attempts to board or conduct Hit and Run attacks on this vessel automatically fail by 1d5+3 Degrees of Failure.\nNurgle's Rot: The crew of this ship are all infected with horrific diseases. Whenever this vessel conducts or is the target of a Hit and Run attack or Boarding Action, the opponent automatically loses an additional 1d5 Crew Population and Morale. There is also a 50% change that some disease will catch, and the ship will suffer a long-term ship sickness (Rogue Trader core rulebook page 227). In this case, the Medicae Test is Very Hard (-30), and the sickness does its damage once a week until contained.\n|Creatures||Beasts of Nurgle; Blight Drones; Foetid Bloat-drones; Great Unclean Ones; Hooktors; Lords of Contagion; Mabrothrax; Malignant Plaguecasters; Noxious Blightbringers; Nurglings; Pestigor; Plaguebearers; Plaguebulls; Plague Hulks; Plague Ogryns; Plague Toads; Plague Towers; Plague Zombies; Poxbringers; Poxwalkers; Rot Flies; Sloppity Bilepipers; Spoilpox Scriveners|\n|Characters||Abcellyoth; Aynthrexes; Bilerot Vomitflesh; Lothar Bubonicus; Bubonis; Vorxec Calvarius; The Carrier; The Entomancer; Epidemius; Ferrue Fayne; Tormus Fayne; Foulspawn; Nathaniel Garro; Ghulroth; The Glottkin; Goresqualor; Gulgoth; Horticulous Slimux; Jibberjaw; Ku'gath; Mamon; Adrius Meinloka; Mephidast; Mortarion; Mortius; Mulch; Necrosius; Nurgle; Pestilaan; Putricifex; Rotigus; Scabeiathrax; Septicus; Achkovas Spengh; The Thanator; Typhus; Ussax; Karloth Valois; Plaguestrangler Vilestench; Jonas Whitespore; Ystareth|\n|Groups||Apostles of Contagion; The Befouling Host; Blessed Flesh; Bringers of Decay; Brotherhood of Plague; Callers of Sorrow; Carnival of Chaos; The Cleaved; Company of Misery; Death Guard; Deathmongers; Death Priests; Flylords; The Grey Death; Inevitable Order; Legio Mortis; Legion of Festering Death; Lords of Decay; Mournful Song; Nurgle's Rotters; Plague Legions; Pox Tribes; The Purge; The Reborn; The Scourge; Septicus Legion; Sorcerer-Kings; The Tainted; Tainted Sons; Vile Savants; House Zegenda|\n|Things||An'garrach; Balesword; Blight Grenade; Bloodrot Rounds; Bone Maul; Corruption; Cursed Carillon; Death Head; Dolorous Knell; Doomsday Bell; Entropic Knell; Epidemia; Father of Blades; Foulswarm Grenade; Gem of Nurgle; Horn of Nurgle's Rot; Icon of Despair; Icon of Seeping Decay; Manreaper; Palanquin; Pandemic Staff; Pestilaan Light Cruiser; Pestilent Flail; Plague Banner; Plaguebringer; Plague Cauldron; Plague Chalice; Plague Claw; Plague Flail; Plague Knife; Plague Sceptre; Plague Skull of Glothila; Plaguesword; Poxwalker Hive; Puscleaver; Rot Giver; Scab; Scourge Shells; Staff of Nurgle; TP-III; Undead Heart; War Altar|""]"	['<urn:uuid:25460af5-708a-451c-b3e0-ac02891f5add>', '<urn:uuid:40fcdc79-63c0-44ae-948f-c55ba9f25319>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	15	85	1265
65	As someone interested in communication changes over time, I'd like to know how families stayed connected during WW1 versus today - how did the Wilson family learn about their soldier's fate, and what's happening with family conversations now?	The Wilson family only recently discovered details about their Great Great Uncle's service through research at the Essex Regiment Museum, as they previously knew nothing about his service in Gallipoli and had assumed he died on the Western Front. In contrast, modern family communication is increasingly screen-focused, with observations showing that when families go to restaurants, everyone immediately pulls out mobile devices instead of talking to each other, compromising valuable family interaction time.	"['One family has been on a journey of discovery. The good news – finally the truth of what happened to their Great, Great Uncle has now been revealed and his exact medal entitlement confirmed. The sad news – like hundreds of thousands of families around the world …. the war medals and Memorial Plaque are missing.\nPrivate Wallace Frederick WILSON, Service Number 201212 of the 4th Battalion, Essex Regiment lost his life during the Great War. Formerly 20799 Private Wallace Frederick WILSON of the Bedfordshire Regiment, his descendants are now armed with the knowledge that he was posthumously awarded the 1914-1915 Star, British War Medal and the Victory Medal. Until recently, they had no idea he had served in the Dardenelles campaign – commonly referred to as Gallipoli. As a result of his tragic death in another theatre of war, Wallace was commemorated with a Memorial Plaque, Scroll and letter from the King.\nThe 1914-1915 Star, British War Medal and Victory Medal, similar to that posthumously awarded to Private Wallace Frederick WILSON\nBorn Wallace Frederick WILSON, the only son of Berry Alfred and Mary Jane WILSON, of Broughton Huntingdonshire, he made the ultimate sacrifice during the first Battle of Gaza, Palestine. His great, great nephew Jamie said, “The last few years I have been researching my great great uncle. He is on the war memorial in Broughton Cemetery (Huntingdon) the village where I grew up . It was very strange because none of my relatives, even the older ones knew anything about him. We had all been assuming he had died during the Somme battles on the Western Front! Anyway after a big break I discovered his soldier number and discovered he died in the first battle of Gaza during 1917”.\nSuvla Bay, Gallipoli as seen from The Nek at dawn on 7 August 2015 during the Centenary Commemorations of the August Offensive. Photo taken by Medals Gone Missing.\nJamie quizzed his extended family as to the whereabouts of the missing war service medals however nobody in the family had any knowledge of their location. It was believed he was entitled only to the British War Medal and the Victory Medal, having began his service life with the Bedfordshire Regiment. Jamie explained, “Wallace first served with the 1/4th Battalion during August 1914 in Brentwood. Part of the Essex Brigade in the East Anglian Division. He moved to Norwich in late 1914 and on to Colchester in April of 1915. In May of 1915 the formation became the 161st Brigade in 54th (East Anglian) Division. From Saint Albans on 21 July 1915 they sailed from Devonport for the Gallipoli campaign, via Lemnos. His unit landed at Suvla Bay 12 August 1915 as a part of the offensive to push inland. On 4 December 1915 Wallace was evacuated from Gallipoli and moved to Mudros, then going on to Alexandria on 17 December 1915. Wallace Remained in the Egypt-Palestine theatre of operations thereafter and his Soldier Number was changed to 201212”. So this vital information revealed that Wallace was also entitled to the 1914-1915 Star.\nJamie credits much of this information which came as quite a surprise to himself and his relatives, to the Essex Regiment Museum . Sandra SMITH – Senior Research Officer with Medals Gone Missing says “It is wonderful when families such as this can unlock the past and finally discover what happened to their ancestors. That all of this information would come to light in time for the Centenary of the Gallipoli August Offensive is an extra bonus for them. We often associate the Sinai Palestine campaign with units such as the Light Horse and Imperial Camel Corps. However this story is a sad and timely reminder that the poor ol’ British infantrymen were scattered to the four corners of the world and lost their lives in many faraway lands”.\nIn terms of the missing war medals and Memorial Plaque, Jamie states “None of my family have his medals or know where they are. I would love to find them on behalf of my family before 2017, the Centenary of his death”.\nCommonwealth War Graves Commission photograph of the Jerusalem War Cemetery where Private Wallace Frederick WILSON was laid to rest. His Memorial Plaque is missing. Can you help?\nWallace Frederick WILSON lost his life on the 26th of March, 1917. If you have these medals or Memorial Plaque in your collection …. or know of their whereabouts, his descendants would be extremely grateful to have them returned. Can you help?', ""Hansa Bhargava MD FAAP\nStaff Physician, Children's Healthcare of Atlanta\nMedical Editor, WebMD\nPediatricians are seeing more and more teens suffering from stress. Whether they are complaining of it or having somatic symptoms such as headaches and stomach aches, it seems that stress and anxiety are on the rise. We know that over scheduling, homework, and the pressures of getting into college can contribute to this. But can media also affect it? Is screen time and media a stressor or a remedy for stress?\nIn a recent WebMD survey published in their Teens and Stress report, 54% of teens were stressed according to parents. Interestingly, 40% of parents turned to the screen for family stress relief while 58% of teens did. Social media and texting was used as stress relief by almost half the teens. This is on the heels of the Common Sense Media survey reporting that US teens were using media for 9 hours a day. Other recent reports have shown that 94% of teens with mobile devices are online daily with many online constantly.\nSo it seems that stress is on the rise and media use is on the rise. Although there may not be a direct relationship, some real issues impact stress and anxiety. Consider this: 23 % of teens report cyberbullying, especially girls. There have been reports of “Facebook depression” and loneliness, as kids who aren’t in social media conversations may feel left out. Other negative consequences can also have an impact: many teens are in front of a screen late at night or ‘sleep text’, both of which can contribute to lack of sleep, which in turn can decrease focus and potentially cause irritability and depression. And last but certainly not least, what about the time media consumes?\nTime spent on media is time often not spent communicating with family. Lately, when I’ve gone into a restaurant, I’ve observed that as soon as a family sits down, everyone pulls out a mobile device. No one is really talking. So even the short amount of time not doing homework, playing soccer, or at school is being compromised. Psychologists, community leaders and experts have long reported that family time can contribute to less depression, less anxiety, better academic performance and generally happier kids. But what if that family time is on media??\nAs the AAP reviews our screen time recommendations, I feel that we, as pediatricians should continue to advise parents about basic principles.\nParents need to lay down some parameters about when and how media is used. Media is a centerpiece of teens’ lives and is not going away, but just as we don’t give our kids a set of keys to our car and say “just drive”, we need to enforce appropriate media use. And good modeling is also critical: parents need to put down their mobile devices and simply communicate with their kids. Old fashioned parenting and just talking to your kids can build the foundation to a less stressful childhood and hopefully a happier life.""]"	['<urn:uuid:ff2c09ae-41bf-4b0b-b7f8-753aa269edd1>', '<urn:uuid:9ee5a44c-ac0e-4471-bc2a-f3b31e423c46>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T10:32:05.666290	38	73	1255
66	wondering origin indian curry madras chicken where did come from history	Chicken Madras is believed to have originated in Madras (now Chennai) in India back in 1640s' with the arrival of the British merchants. However, it's not known by the same name in India. The name of this recipe has its origin in the United Kingdom.	['Restaurant style Chicken Madras Recipe that’ll become your new favorite. It’s even better than takeout, I say this because it’s made with a special homemade spice mix known as Madras Curry Powder. The secret taste lies in the masala powder & how you cook the whole recipe. Don’t miss the Video! 3 methods of cooking shared – Instant Pot or pressure cooker, slow cooker & stove top.\nTable Of Contents\n- 1 Recipe Video\n- 2 Chicken Madras Restaurant Recipe\n- 3 What is Chicken Madras?\n- 4 Where does Chicken Madras come from?\n- 5 Chicken Madras Ingredients\n- 6 7 Tips to make the best Chicken Madras Recipe take out style\n- 7 How long can you store Madras Chicken Curry?\n- 8 Can you freeze Madras Chicken restaurant recipe?\n- 9 Reheating tips\n- 10 What to serve with Chicken Madras?\n- 11 Does Chicken Madras have dairy?\n- 12 How to make Chicken Madras? Step by Step in Instant Pot\n- 13 How to make Chicken Madras in slow cooker?\n- 14 Chicken Madras Recipe – FAQs\n- 15 Chicken Madras Recipe\nChicken Madras Restaurant Recipe\nChicken Madras – the very popular & one of the most ordered curry dishes in the curry houses of UK & also several other countries.\nIt’s chicken (on the bones or boneless) cooked in an onion tomato base gravy with a special spice mix that consists of a plethora of Indian spices & a special herb.\nDon’t worry because you can find each & every ingredient in Indian stores.\nIt’s the cooking technique & the Madras curry powder that combine together to give this dish an outrageous flavor.\nIf you love curries you won’t resist this one. No wonder that Madras dishes are one of the hottest take out food in the UK.\nAnd now you can easily make it at home with my easy recipe steps + Video.\nSo what are you waiting for, ditch your favorite curry house & make some today.\nWe love Madras recipes & since eons I have been lovingly making them in my kitchen.\nThe more I make the more I fall for it.\nA riot of soothing fragrant spices play a major role in making this dish so delectable.\nThe Madras Curry powder recipe is so well balanced, not a single spice in it tastes piquant.\nIt’s an extraordinary combination of flavors with a subtle kick from peppercorns & chilies.\nI am sure you are going to love today’s recipe of Chicken Madras curry.\nWhat is Chicken Madras?\nChicken Madras is a fairly spicy curry that’s made with chicken cooked in an extraordinarily fragrant spice mix along with onions & tomatoes. It may or may not contain yogurt or tamarind.\nMadras Chicken curry is a famous take out favorite back in the UK & almost every curry house carry it in their menu.\nThe specialty of Chicken Madras lies in the Madras curry powder & the way the whole recipe is cooked.\nWhere does Chicken Madras come from?\nChicken Madras is believed to have originated back in Madras (now Chennai) in India back in 1640s’ with the arrival of the British merchants.\nIt’s however not known by the same name back in India.\nBack in India Madras curries are typically more fiery & hot & also varies from region to region.\nThe name of this recipe has it’s origin back in the United Kingdom.\nChicken Madras Ingredients\nToday’s recipe is very simple.\nYou just need to stock on the recipe of Madras Curry Powder which I have already shared a few days back on the blog.\nDon’t miss it if you are after that restaurant taste.\nHere’s what you’ll need to cook a fantastic bowl of Chicken Madras:\n- Chicken marinated & on the bones: You can use boneless chicken too but chicken on the bones is highly recommended because it tastes the best. Skinless meat is a must. Marinate the chicken with ginger garlic, turmeric, chili powder & a bit of salt.\n- Onions: Roughly chopped. You may use red or white but avoid the sweeter varieties.\n- Tomatoes: Roughly chopped.\n- Tomato paste: This adds tons of color & a slightly sweeter note to the recipe. It’s better to say that it balances the flavors of the spices beautifully. We used store bought from the tube.\n- Ground spices: Turmeric, cumin, coriander, red chili – all of them in the powder form.\n- Special Spice: Homemade Madras Curry Powder. Don’t miss this one, this is the heart & soul of todays’ Chicken Madras recipe. The recipe is up on the blog & I have also shared the link here in this post below.\n- Whole Spices: Whole cumin seeds + dry red chili.\n- Marinated yogurt: Throw in some tandoori masala, Kashmiri red chili powder, salt to the yogurt & whisk well. A fantastic spiced up yogurt that boosts the flavors of today’s chicken Madras & also mellows down the heat from the chilies.\n- Fresh Curry Leaves: Available in every Indian grocery store. You may omit this if you want to.\nPro Tip: Want to cook the Chicken Madras with boneless chicken? Buy some good skinless chicken thigh meat. Avoid breast pieces. Chicken thighs will yield a tender juicy meat in the gravy whereas you’ll end up with a stiff cooked chicken meat if you choose chicken breasts.\n7 Tips to make the best Chicken Madras Recipe take out style\n- Do not Skip the homemade Madras Curry Powder recipe. That’ll add tons of flavor, texture & taste to your Chicken Madras recipe.\n- Do not sub the homemade Madras Curry powder with it’s store-bought counterpart. You won’t manage the best flavors. No store bought masala can beat the flavors of the homemade spice mix.\n- Sear the Chicken before starting the curry recipe: This is very important because searing helps to lock the juices of the chicken. It will remain ultra juicy & tender in the gravy & will taste the best.\n- Fry the Onions well: This will make the curry more tasty, the onion will not stand out & the Chicken Madras will have a lovely color too.\n- Fry the spices well: The taste lies in the frying method known as Bhuna. Do not miss out this technique even if you are in a hurry. Fry the spices until the raw smell disappears & oil begins to ooze out.\n- Spice up the yogurt: Spice it up before adding it to the curry base. It adds depth & helps develop the flavors.\n- Add the homemade Madras curry powder liberally. It contains the magic flavors. The complexity, depth & intensity of the Madras chicken recipe comes from this homemade masala powder.\nHow long can you store Madras Chicken Curry?\nYou can store Madras Chicken Curry in air tight containers for 10 days in the fridge.\nCan you freeze Madras Chicken restaurant recipe?\nYes you can freeze Madras Chicken restaurant recipe. It keeps good for 1 month in the freezer.\nStore it in air tight freezer safe bags or containers.\n- Stored it in fridge? Do this to reheat: Scoop out the curry on a skillet, sprinkle some water. Reheat over medium heat. Stir in between. It might take 2-3 mins approx. to warm up.\n- Freezed it? You can do this to reheat: You may or may not thaw the curry. Dump it in a saucepan, skillet or pot. Add about 1/2 cup water. Allow to warm up over medium heat. It should take you about 10-15 mins approx. If you choose to thaw the curry overnight in the fridge first than follow the reheating technique of No 1 shared above.\nWhat to serve with Chicken Madras?\nWe love to mop our restaurant style Chicken Madras with hot Naan breads, butter Naan, Garlic Naan or plain Naan.\nI specially love it with homemade parathas. You can even enjoy it with plain basmati rice & chapathi.\nDoes Chicken Madras have dairy?\nChicken Madras Recipe may or may not contain dairy. Recipes vary & people love to cook it in various ways.\nBut, today’s just-like-takeout Chicken Madras contains dairy in the form of yogurt.\nYogurt makes things better, it takes away the spicy-ness from the recipe & balances off the taste with a certain subtlety.\nPro Tip: Yogurt does not make it spice free or bland. It just balances off the heat, adds a mild tang & boosts the flavor quotient. Try cooking Madras Chicken with & without yogurt & see what you love 🙂\nDon’t miss these Chicken Recipes from Foodies Terminal.\nHow to make Chicken Madras? Step by Step in Instant Pot\nSteps in words below pictures\nHow to make Chicken Madras?\n- Marinate the chicken\nFlash marinate the chicken pieces with turmeric powder, ginger garlic paste, chili powder & some salt.\n- Sear the chicken\nSear the marinated chicken pieces until golden & set aside. Do this on SAUTE kept at NORMAL in an Instant Pot.\n- Sizzle the whole spices\nKeep the IP settings the same & add more oil. Throw in the whole cumin, fresh curry leaves & dry red chili & sizzle them for a few seconds.\n- Fry the onion\nThrow in the chopped onion & fry very well until the onion becomes golden brown & oil begins to leave the sides of the pot.\n- Sauté the tomatoes\nThrow in the fresh tomatoes & fry until they becomes completely mushy & oil begins to ooze out.\n- Add the dry spices\nAnd fry them very well until the raw smell completely disappears & oil begins to ooze out.\n- Throw in the special homemade Madras Curry Powder\nAdd the homemade Madras Curry Powder in generous amount & fry well.\n- Add the chicken\nThrow in the seared chicken pieces & give everything a very good mix. Fry for 3-4 mins until oil begins to leave the sides of the pot & the chicken pieces are well coated with the rest of the ingredients in the pot.\n- Add the tomato paste + spiced yogurt\nLower the heat to LOW on SAUTE. Throw in the tomato paste & the spiced up yogurt sauce & give everything a very good mix. Fry for sometime until you see oil leaving the sides of the pot.\n- Add warm water + pressure cook\nPour warm or hot water. Cancel the SAUTE mode & pressure cook with the lead on for just 5 mins on HIGH.\n- Simmer the sauce until desired consistency\nFollow a QPR (Quick pressure release). Open the lid & cancel the pressure cook mode, press the SAUTE & simmer the sauce until the desired consistency is reached. We love it on the thick side. Garnish with more fresh curry leaves & serve hot. Enjoy!\nHow to make Chicken Madras in slow cooker?\nIt’s very easy to cook chicken Madras in a slow cooker.\nI alternately do cook it in slow cooker as well as in a pressure cooker.\nThe recipe remains exactly the same. Follow the steps until step 9 mentioned above (the instant pot method) and than add enough water to just cover the chicken pieces.\nPut the lid of the slow cooker & cook it for 3-4 hrs until the meat falls off from the bones.\nServe hot. Enjoy!\nChicken Madras Recipe – FAQs\nChicken Madras is fairly hot. But again, every persons heat tolerance level varies.\nMadras Chicken recipe is fairly spicy. It’s not something that we cook every night. It’s spicy & it’s rich. It’s a dish to enjoy occasionally.\nNo, Chicken Madras is nut free.\nYes, Chicken Madras recipe is completely gluten free.\nYes, Chicken Madras recipe is fairly spicy.\nHere’s the Chicken Madras Curry Powder recipe. It’s exclusively made with best whole spices & at home. Don’t miss the informative post of Madras Curry Powder Recipe shared below.\nDon’t miss the MADRAS CURRY POWDER RECIPE to cook today’s recipe.\nChicken Madras Recipe\nFor the spiced yogurt\n- 3/4 cup Yogurt\n- 1/2 tsp tandoori powder (store bought)\n- 1/4 tsp Kashmiri Red Chili Powder\n- a pinch of turmeric powder\n- a pinch of salt\nSpices for sizzling & garnishing\n- 1/2 tsp Whole Cumin Seeds\n- 2 dry red chili\n- 10 fresh curry leaves (optional)\nHow to marinate the chicken?\n- Take the chicken on bones in a clean dry bowl. Add 1/4 tsp turmeric powder, 1/4 tsp red chili powder, 1 tbsp ginger garlic paste & some salt to taste. Massage the chicken pieces well so that all the pieces are uniformly coated with the ingredients. Set aside while you prepare the rest of the stuff.\nHow to make the spiced yogurt sauce?\n- Whick the yougurt with all the mentioned ingredients from “For the spiced yogurt”. Set aside.\nHow to cook the Chicken Madras in Instant Pot?\n- Set the Instant pot on SAUTE & keep it on NORMAL. Add about 1/2 the mentioned oil.\n- When the oil becomes moderately hot gently release the marinated chicken pieces & sear them until golden. Set aside once they are nicely seared.\n- In the same pot add the rest of the oil & sizzle the cumin, dry red chili and a few curry leaves for a few seconds or until aromatic.\n- Next. add the chopped onions & fry very well until the onions become light brown in color & oil begins to leave the sides of the pot.\n- Add the chopped tomatoes & fry them until the tomatoes become completely mushy & oil begins to ooze out. It should take you about 2-3 mins.\n- Next, add the dry spice powders except the Madras curry powder. Give eveything a good mix & fry the spices until the raw smell completely diasppears & oil begins to ooze out. If the spices stick to the bottom of the pot sprinkle some water & keep on frying.\n- Now, add the homemade madras curry powder & fry until aromatic.\n- Throw in the seared chicken pieces & give everything a good mix so that the chicken pieces are well coated with the bhuna masala. Fry for some time until the chicken pieces are well roasted in the masala. It should take you about 3-4 mins.\n- Lower the heat & keep it at LOW. Now, add the tomato paste + the spiced yogurt & mix everything well. After about 1 min up the heat to NORMAL in SAUTE mode. Keep on frying until the oil oozing stage is reached. The masala will become darker as you keep on frying them.\n- Add warm or hot water and give eveything a very good mix. Do not add too much water. The chicken pieces should be just 70% submerged in water.\n- CANCEL the SAUTE mode & close the lid of the IP & make sure that the vale is at the SEALING position. Now, press the PRESSURE COOK button & set it on HIGH for 5 mins.\n- After 5 mins when you hear the beep of the IP immeditely do a QUICK PRESSURE RELEASE. Open the lid of the pot.\n- CANCEL the PRESSURE COOK mode & turn on the SAUTE mode & set it on NORMAL. Simmer until the sauce thickens & the desired consistency as per your liking is reached.\n- Garnish with a few more fresh curry leaves if you want to & serve hot. Enjoy!\nHow to cook Chicken Madras on the stove top?\n- The whole recipe remains exactly the same. Just use a skillet or Kadai to cook the Madras chicken curry & use medium heat to cook it. After adding water to the skillet or kadai cover & cook over medium heat until the chicken pieces are well done. It should take you 15-20 mins. Serve hot.\n- Measuring units used, 1 cup = 240 ml & 1 teaspoon = 5 ml.\n- Instant Pot used = 6 quart IP DUO.\n- Don’t miss the homemade Madras curry powder recipe. Use it to make todays recipe. It’s this masala powder that’s the flavor bomb in today’s chicken Madras recipe.\n- Don’t miss the spiced yogurt. It adds tons of flavor & do flash marinate the chicken pieces before searing the meat.\nPin it for Later!45']	['<urn:uuid:d2a68436-bbe7-485d-aac3-0dde28750a37>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	11	45	2719
67	optimum temperature range thermophiles bacteria growth	Thermophiles have an optimum growth temperature of 60°C, with a temperature range of 45-70°C.	"[""Unformatted text preview: Chapter 6: Chapter 6: Microbial Growth Environmental Requirements Environmental Requirements Physical and Chemical Factors Temperature pH Osmotic pressure Oxygen availability Hydrostatic pressure Radiation Temperature Requirements Temperature Requirements\nType Psychrophiles Mesophiles Thermophiles Range 020°C Optimum 15°C 1545°C 2045°C 4570°C 60°C Hyperthermophiles 70120°C 90°C Temperature Requirements Temperature Requirements Temperature Requirements Temperature Requirements Psychrophiles: Cause spoilage of foods while refrigerated Include most human pathogens (body is 37°C) Found in compost heaps Hydrothermal vents, hot springs (above 104°C) Mesophiles: Thermophiles Hyperthermophiles: pH Requirements pH Requirements\nType Acidophiles Neutrophiles Alkalophiles Range pH 1.0 – 5.5 pH 5.5 – 8.0 pH 8.5 – 11.5 Acidotolerant and alkalotolerant microbes can persist for short periods under these conditions, but are unable to reproduce pH Values of Some Environments pH Values of Some Environments\nAcidic 1 2.5 3.54.5 6 8 9 10 Basic 11 ammonia gastric juices vinegar peaches, tomatoes peas, corn, shrimp seawater alkaline lakes/soils soap solutions household pH Requirements pH Requirements Large changes in [H+] can: Disrupt the cytoplasmic membrane Inhibit enzymes and transport proteins Most bacteria are neutrophiles Even in acido or alkalophiles, cytoplasmic pH remains neutral Keeping the Cytoplasmic pH Neutral Keeping the Cytoplasmic pH Neutral Antiport exchange of K+ for H+ in neutrophiles (Na+ for H+ in alkalophiles) Synthesize proteins under acidic conditions Acid shock proteins use ATP to actively transport H+ out of the cell Fermentation produces acids Putrefaction produces ammonias Export wastes to the environment Osmotic Pressure Osmotic Pressure Moderate halophile Require NaCl; found in marine habitats Require NaCl; found in hypersaline habitats like the Great Salt Lake Can grow in salty situations, but grow better without the NaCl; skin bacteria Requires sugars; yeasts & molds Extreme halophiles Osmotolerant Saccharophiles Osmotic Pressure Osmotic Pressure Effect of Osmotic Pressure Effect of Osmotic Pressure Osmotic pressure and water activity are inverse Solution has low water activity = high osmotic pressure Low water activity draws water out of cells via osmosis to dilute external [solute] Water is essential to macromolecule breakdown High water activity may lyse cells by drawing too much water into cell cytoplasm via osmosis Cell wall offers great protection from this Mechanosensitive channels in plasma membrane open Regulating Osmotic Pressure Regulating Osmotic Pressure To prevent growth, reduce water availability inside the cytoplasm Add solutes (sugars, salts) Lyophilization (freezedrying) Dessication (drying) Oxygen Availability Oxygen Availability Major groups based on O2 use and tolerance: Obligate aerobes Facultative anaerobes Obligate anaerobes Aerotolerant anaerobes Microaerophiles Obligate Aerobes Obligate Aerobes Undergo aerobic respiration: O2 required as a terminal electron acceptor No other respiratory paths available In a broth medium: Growth only near top of liquid Limited by the penetration of dissolved O2 from the atmosphere Facultative Anaerobes Facultative Anaerobes In the presence of O2: Undergo aerobic respiration using O2 as terminal electron acceptor Producesmore ATP = more growth capable Undergo anaerobic respiration or fermentation paths less ATP produced, and hence less growth than aerobic In the absence of O2: Facultative Anaerobes Facultative Anaerobes In a broth medium: Growth will be densest at top where O2 is available Growth will occur throughout the depth of the medium via anaerobic respiration or fermentation, or both Obligate Anaerobes Obligate Anaerobes Undergo anaerobic respiration: Ions such as NO3 are required as a terminal electron acceptor No other respiratory paths available In a broth medium: Limited by the penetration of dissolved O from the atmosphere very toxic Growth only near bottom of tube 2 Obligate Anaerobes Obligate Anaerobes O2 presence causes the formation of toxic compounds (superoxides, free radicals) Disrupt cytoplasmic membranes & other cell components Do not have enzymes capable of converting toxic compounds to harmless ones: Superoxide dismutase (SOD): converts superoxides to peroxides Catalase: breaks down hydrogen peroxide to H2O and O2 Peroxidase: converts peroxides to water and NAD+ Aerotolerant Anaerobes Aerotolerant Anaerobes Ignore O2 No toxic effects of O2 due to SOD presence Use fermentation or anaerobic respiration for ATP production In a broth medium: Grow throughout the depth of the medium no denser region at surface Microaerophiles Microaerophiles Too high of O2 content is damaging Low enzymes = inability to adequately prevent damage from toxic species Require 210% O2 concentration Normal atmosphere is 20% Microenvironments with aerobically respiring consortia reduce [O2] to tolerable range Many respiratory pathogens are microaerophiles Oxygen Use/Tolerance Groups Oxygen Use/Tolerance Groups Hydrostatic Pressure Hydrostatic Pressure Barophiles (piezophiles): optimal growth rate where pressure > atmospheric pressure Pressureadapted microbes growing at higher temperatures are mostly Archaea Pressureadapted microbes growing at moderate and cold temperatures are mostly Bacteria Adaptation to pressure is not too extreme Slight genomic differences between pressure adapted vs. normal atmosphere isolates Radiation Ultraviolet Rays Radiation Ultraviolet Rays Damages DNA base pair bindings to produce mutations like thymine dimers Mutations will result indirectly in cell death Inability to replicate chromosome Inability to correctly transcribe mRNA Radiation Ultraviolet Rays Radiation Ultraviolet Rays UV light DNA damage can be repaired: Photoreactivation Blue light energizes a specific enzyme which breaks the thymine dimers Allows normal crosshelix basepairing by hydrogen bonding Dark reactivation Thymine dimers are excised by endonucleases Missing bases in the DNA sequence are replaced by other endonucleases Ionizing Radiation Ionizing Radiation Cause atoms to lose electrons Include Xray and gamma radiation Low levels cause mutation and can indirectly cause cell death High level exposure causes direct cell death Breaks HH bonds, oxidizes double bonds, breaks ring structures, polymerize some molecules Often used as a sterilizing treatment What is Microbial Growth? What is Microbial Growth? Defined as an increase in number Achieved by: Budding Binary Fission Cell duplicates its components, then shares them between 2 daughter cells Daughter cells independent when septum forms betweencell ‘halves’ Bacterial Cellular Growth Cycle Bacterial Cellular Growth Cycle C phase = chromosome replication D phase = delay period Nucleoid partitioning Septation begins Cytokinesis = septation complete Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log vs. Arithmetic Scales Log vs. Arithmetic Scales Conversion to log scale compresses the distance between data points evenly. Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Culture Growth Phases: Lag Log (Exponential) Stationary Death (Decline) Easily measured during growth in liquid media by spectroscopy or densitometry Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Lag phase: Metabolically active but NO increase in number Adaptation: induce enzymes needed; synthesize new ribosomes, ATP, and cofactors; replicate chromosome Repair cellular components, increase in cell size Unbalanced growth rates of synthesis of cell components varies with one another Length of entire phase varies w/ species & environmental conditions Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log (Exponential) phase: Population doubles each generation Generation (doubling) time ranges from 7 min to 20 hr – average is 20 min Growth is asynchronous not all cells divide at exact same time Growth rates are saturable; limited to [celluar enzyme] Balanced growth all cellular constituents made at constant rates to one another Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log (Exponential) phase: Rapid expansion with 20min generation time: also called doubling time Population doubles in number every 20 minutes)\n0 m in 2 0 m in 4 0 m in 1 h r 2 h r 3 h r 4 h r 5 h r 6 h r 1 0 ce lls 2 0 ce lls 4 0 ce lls 8 0 ce lls 6 4 0 ce lls 5 , 1 2 0 ce lls 4 0 , 9 6 0 ce lls 3 2 7 , 6 8 0 ce lls 2 , 6 2 1 , 4 4 0 ce lls Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Stationary phase: Curve horizontal: population growth ceases New cells made at same rate as old cells die (growth rate = death rate) Reasons for stationary phase: Nutrient limitation or O2 limitation Accumulation of toxic wastes Cell density Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Stationary phase: Very common in nature (oligotrophic) Not simply a time when things run out and cell enters a stasis we see changes in: Gene expression: starvation proteins and other proteins, as well as antibiotics, are produced Peptidoglycan crosslinking Nuceloid condensation Endospores formed by certain species Changes make them more resistant to unfavorable conditions Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Death (Decline) phase: Number of viable cells decreases exponentially Constant number of cells die per hour Usually a logarithmic, but not always so clear Bacterial cell death is defined by the inability to grow (reproduce) Death (Decline) phase: Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle\nWhat causes cell death or loss of viability?\nBuild up wastes/toxins and poor environmental conditions for survival Survival of the fittest to reproduce Viable But Not Culturable (VNBC) Temporarily unable to grow under lab conditions but resuscitate upon entry into different environment Programmed Cell Death Certain % of cells that commit suicide to provide nutrients to survivors Bacterial Death/Lossof Viability Bacterial Death/Loss of Viability Continuous Culture Systems Continuous Culture Systems Continuous growth = a constant state of the growth curve; often exponential phase Achieved in a chemostat chamber: Fresh growth medium added at same rate as spent medium and cells are removed Removes pressure of limiting nutrient Also removes toxic waste buildups Growth rate is adjusted by exchange rate Model Chemostat Model Chemostat Population level and generation time are controlled by dilution rate Increase dilution = increase generation time because less of limiting nutrient is available; density kept low most energy used for maintenance, not reproduction Decrease dilution = decrease generation time because little limitation of nutrient; density also increase Measuring Microbial Growth Measuring Microbial Growth By cell number By cell mass Viable vs. Total Direct Microscopic Counts Coulter Cell Counters Viable Counts Calculations and conversions Total cell weight, or by individual chemical (carbon, protein, etc.) Turbidity Direct Microscopic Counts Direct Microscopic Counts PetroffHauser Counting Chamber Accepts fixed volume (0.1 ml) Count number of cells per volume Cannot distinguish live from dead cells Calculations done to determine original cells/ml Coulter Counter Coulter Counter Automated counting device Microbial suspension directed through a small hole the size of an individual cell Change in electrical resistance when cell passes through the hole = 1 cell counted Can’t distinguish live vs. dead cells or cells from small particles of debris Viable Counts Viable Counts Measures colonyforming units rather than cells (due to possible clumping) Seek statistically “countable” plate having 30 300 colonies Serial dilution, plate count, membrane filters Does NOT necessarily count all living bacteria present in the sample just those able to grow under certain conditions given Used with spread and pour plates, also membrane filtration Viable Counts Via Membrane Filtration Viable Counts Via Membrane Filtration Turbidity (Cell Mass) Turbidity (Cell Mass) Done by measuring the amount of light scattered by a cell More mass = more scatter (proportional) Uses a spectrophotometer to measure optical density (OD) of the cells Create standard curves to determine population density based on turbidity Turbidity (Cell Mass) Turbidity (Cell Mass) Low population of cells = low scatter = low OD High population of cells = high scatter = high OD Top scale = % transmittance Bottom scale = optical density Turbidity (Cell Mass) Turbidity (Cell Mass) Create standard curves to determine population density based on turbidity Done in conjunction with viable plate counts initially OD700nm is plotted against the number of viable cell counts taken at the same time points to produce the standard curve Future growth can be estimated from this established relationship Species and environmental conditions must be identical as when the original curve was produced Standard Curve Standard Curve ...\nView Full Document\n- Fall '09\n- Bacterial Culture Growth, Culture Growth Cycle""]"	['<urn:uuid:0ef80001-466f-4e36-95aa-42bee82f8185>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	6	14	1925
68	What are the main components that make up bones?	Bone is made up mostly of collagen, along with the mineral calcium. The collagen provides the soft framework, and calcium provides strength.	"[""Reviewed by John P. Cunha, DO, FACOEP\n- What is another medical term for osteoporosis?\n- Bones are composed of calcium and what other substance?\n- I may not be aware of my osteoporosis until I suffer a fracture. True or False?\n- What is the function of vitamin D?\n- How is bone density measured?\n- About 20% of people with osteoporosis are men. True or False?\n- Improve your Health I.Q. on Osteoporosis\n- Osteoporosis Related Slideshows\n- Osteoporosis Related Image Collections\nQ:What is another medical term for osteoporosis?\nA:Osteoporosis, or porous bones, is a bone disease characterized by bone loss, or the body's inability to make new bone. The bones lose mass and density, and a person with osteoporosis is more prone to fractures, especially of the hip, spine, and waist. About 54 million people in the U.S. have osteoporosis and low bone mass. Approximately half of all women and up to one quarter of men age 50 and older will fracture a bone due to the disease.\nQ:Bones are composed of calcium and what other substance?\nBone is made up mostly of collagen, along with the mineral calcium. The collagen provides the soft framework, and calcium provides strength. This combination makes bones flexible and strong at the same time so they can withstand stress.\nQ:I may not be aware of my osteoporosis until I suffer a fracture. True or False?\nMany people do not realize they have osteoporosis until a fracture occurs or spinal vertebrae collapse, which is why osteoporosis is often referred to as a “silent disease.” Bone loss can occur gradually over time with no symptoms. Osteoporosis causes bones to become so fragile that fractures can occur even after minor falls, or normal stresses and strains on bones.\nQ:What is the function of vitamin D?\nA:Vitamin D is required to absorb calcium from the intestine, so it is important in maintaining healthy bones and reducing the risk of developing osteoporosis.\nUnlike calcium that you can only get from food, your body makes vitamin D when you are exposed to sunlight. A few foods such as fatty fish, egg yolks, beef liver, and cheese have vitamin D, as do some foods that are fortified with the vitamin, but foods alone are not enough to get adequate amounts.\nVitamin D supplements can be taken to help you meet the recommended daily allowance (RDA) of 600 International Units (IU) for men and women up to age 70 and 800 IU for adults over age 70. Vitamin D can be found in multivitamins and in supplements in combination with calcium. Getting too much vitamin D, especially over 2,000 IU daily, is not advised unless prescribed by a doctor. Many dietary supplements contain vitamin D, so check the dose in each to make sure you do not take too much.\nQ:How is bone density measured?\nA:A bone mineral density (BMD) test is a type of low-dose X-ray that measures the amount of calcium and other minerals present in your bones. It is the only test that can diagnose osteoporosis.\nThe National Osteoporosis Foundation recommends bone mineral density tests for:\n- Women 65 and older and men 70 and older\n- Anyone who has broken a bone after age 50\n- Women of menopausal or postmenopausal age with risk factors\n- Men age 50-69 with risk factors\nQ:About 20% of people with osteoporosis are men. True or False?\nWhile osteoporosis affects women at a higher rate, about 20% of patients with osteoporosis are men. About 2 million men in the U.S have osteoporosis, and 12 million more are at risk of developing the condition. Risk factors that make women more prone to osteoporosis also apply to men, including:\n- Family history\n- Use of steroid medications\n- Sedentary lifestyle\n- Excess alcohol consumption\n- Low testosterone levels\n- Low estrogen levels in men\n- Medical problems such as chronic kidney, lung or gastrointestinal disease, prostate cancer and autoimmune disorders such as rheumatoid arthritis (RA)\nSource quiz on MedicineNet""]"	['<urn:uuid:c1e33ca5-693b-44ff-a30e-18a4f6a1ed54>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	9	22	668
69	As a family counselor, what guidance exists for household relationships?	Wives should submit to their husbands, as is fitting in the Lord. Husbands should love their wives and not be harsh with them. Children should obey their parents in everything, as this pleases the Lord. Fathers should not provoke their children, lest they become discouraged. Bondservants should obey their earthly masters with sincerity of heart, not as people-pleasers.	['Put On the New Self\n1 # ch. 2:12 If then you have been raised with Christ, seek #[Phil. 3:14] the things that are above, where Christ is, #See Eph. 1:20seated at the right hand of God. 2#See Matt. 16:23Set your minds on things that are above, not on things that are on earth. 3For #ch. 2:20; See Rom. 6:2you have died, and your life is hidden with Christ in God. 4When Christ #See John 11:25 who is your#3:4 Some manuscripts our life #[Phil. 3:21; 1 Pet. 1:1, 7, 13; 1 John 2:28; 3:2] appears, then you also will appear with him #1 Cor. 15:43in glory.\nRom. 8:13; [Gal. 5:24] Put to death therefore #Rom. 6:13 what is earthly in you:#3:5 Greek therefore your members that are on the earth #See Eph. 5:3, 5 sexual immorality, impurity, #Rom. 1:26 passion, evil desire, and covetousness, #[Job 31:25, 26]which is idolatry. 6#See Eph. 5:6On account of these the wrath of God is coming.#3:6 Some manuscripts add upon the sons of disobedience 7#See Eph. 2:2, 11In these you too once walked, when you were living in them. 8But now #See Eph. 4:22 you must put them all away: #Eph. 4:31 anger, wrath, malice, #See Eph. 4:29slander, and obscene talk from your mouth. 9#Lev. 19:11; See Eph. 4:25 Do not lie to one another, seeing that #ch. 2:11 you have put off #Rom. 6:6; Eph. 4:22the old self#3:9 Greek man; also as supplied in verse 10 with its practices 10and #Eph. 4:24 have put on #See Rom. 6:4 the new self, #See Rom. 12:2 which is being renewed in knowledge #See Rom. 8:29 after the image of #[Eph. 2:10]its creator. 11#[Rom. 10:12]; See 1 Cor. 12:13; Gal. 5:6 Here there is not Greek and Jew, circumcised and uncircumcised, barbarian, Scythian, slave,#3:11 For the contextual rendering of the Greek word doulos, see Preface; likewise for Bondservants in verse 22 free; but Christ is #Eph. 1:23all, and in all.\n12 # ver. 10 Put on then, as #Rom. 8:33 God’s chosen ones, holy and beloved, #Phil. 2:1 compassionate hearts, #Eph. 4:32 kindness, #See Eph. 4:2humility, meekness, and patience, 13#[See ver. 12 above] bearing with one another and, #Mark 11:25 if one has a complaint against another, #[See ver. 12 above]forgiving each other; #[See ver. 12 above]as the Lord has forgiven you, so you also must forgive. 14And above all these put on #[1 Thess. 5:8]; See Eph. 5:2 love, which #Eph. 4:3 binds everything together in #Heb. 6:1; [John 17:23]perfect harmony. 15And let #See Phil. 4:7 the peace of Christ rule in your hearts, to which indeed you were called #Eph. 2:16 in one body. And #ver. 17be thankful. 16Let #John 15:3 the word of Christ dwell in you richly, teaching and admonishing one another in all wisdom, #See Eph. 5:19 singing psalms and hymns and spiritual songs, #ch. 4:6with thankfulness in your hearts to God. 17And #ver. 23; 1 Cor. 10:31 whatever you do, in word or deed, do everything in the name of the Lord Jesus, #ch. 1:12; 4:2; See Eph. 5:20giving thanks to God the Father through him.\nRules for Christian Households\n18 # For ch. 3:18–4:1, see Eph. 5:22–6:9 Wives, submit to your husbands, as #Eph. 5:4; Philem. 8is fitting in the Lord. 19Husbands, love your wives, and #Eph. 4:31do not be harsh with them. 20Children, obey your parents #[Eph. 5:24; Titus 2:9]in everything, for this pleases the Lord. 21Fathers, do not provoke your children, lest they become discouraged. 22Bondservants, obey #[See ver. 20 above]in everything those who are your earthly masters,#3:22 Or your masters according to the flesh not by way of eye-service, as people-pleasers, but with sincerity of heart, fearing the Lord. 23#ver. 17 Whatever you do, work heartily, #[Philem. 16]as for the Lord and not for men, 24knowing that from the Lord #[Eph. 6:8] you will receive the inheritance as your reward. #[1 Cor. 7:22]You are serving the Lord Christ. 25For the wrongdoer will be paid back for the wrong he has done, and there is no partiality.']	['<urn:uuid:95388c5b-9a3e-4357-85c7-9768e668e201>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	10	58	669
70	I'm an industrial safety inspector and I'm worried about workers who assist in riveting - what percentage of power tool users typically develop hand-related health issues?	On average, 46% of workers who use vibrating power tools contract Hand Arm Vibration Syndrome (HAVS), which is a painful, potentially disabling condition affecting the fingers, hands, and arms.	"[""igh-impact aircraft riveting joins aluminum sheets, typically requiring two people who get exposed to repetitive, hammering forces. One person normally uses a riveting gun while a partner on the other side of the joined material holds a bucking bar – effectively a hand-held anvil that forms the end of the rivet, or bucktail.\nRepetitive impact and vibration conveyed to bucking bars can lead to health or ergonomic complaints. On average, 46% of workers who use vibrating power tools contract Hand Arm Vibration Syndrome (HAVS), a painful, potentially disabling condition of the fingers, hands, and arms.\n“People don't understand that the person on the receiving end [of rivet bucking] is taking highly damaging vibration to the hand,” says Richard Borcicky, a retired tool engineer and ergonomics manager who oversaw safety at the Department of Defense’s (DOD) Fleet Readiness Center East base in Cherry Point, North Carolina.\nBorcicky says the DOD continually seeks to improve safety and ergonomics in its facilities. Through implementing industry best practices, Borcicky notes the Fleet Readiness Center East base was able to reduce annual carpal tunnel syndrome cases from 50 to 0.\nHowever, Borcicky says people bucking rivets often experienced hand swelling during the week, and throughout time, this can develop into an incurable, crippling disease of the fingers and hands.\n“We couldn’t get rid of the bucking bar issues because there was no fix,” Borcicky says.\nBrian Lewis, lead engineer at the Tulsa, Oklahoma facility of Spirit AeroSystems, the world’s largest tier-one manufacturer and supplier of aerostructures, adds, “Without an ergonomically friendly bucking bar that absorbs impact and vibration, you have to continually switch out workers because they cannot rivet all day long, but that in itself can cause some quality issues.”\nHowever, rivets still must be reliably and evenly driven without marring airplane skins, or they must be drilled out, deburred, and redone – and such rework increases production costs. This can be a particular challenge for under-trained staff or new hires, who are often assigned riveting tasks.\n“Due to the force and impact of riveting, rivets and bucktails can be misaligned – but these need to be just right each time,” Lewis says. “So, having the right ergonomic equipment to facilitate fast, reliable production is critical.”\nWhile some bucking bars incorporate tungsten to absorb and dissipate vibration, it’s seldom sufficient to fully address impact/vibration related repetitive injury or ergonomic issues. If dropped, tungsten bucking bars can break, making them unusable.\nFortunately, to speed reliable aerospace riveting while minimizing injuries, the industry has developed safe, ergonomic, impact absorbing bucking bars that can be tailored for ease of use in aerospace riveting processes.\nFaster, safer riveting\nAccording to Lewis, Spirit AeroSystems’ Tulsa facility primarily builds new parts for wing structures, such as slats and flaps, which require large quantities of rivets.\n“With the traditional bucking bars, riveters can develop elbow or shoulder issues, so it isn’t prudent to leave people in that role for very long,” Lewis says. “Also, the rivet bucktails sometimes are not the same height; and the bucking bars can leave marks on sheet metal surfaces, which is not acceptable.”\nIn search of a solution, Lewis was receptive to the recommendation of an airline maintenance employee at a nearby facility, who had successfully used an advanced bucking bar called the Torpedo Guardian ISOVIB from Honsa, a Milan, Illinois-based manufacturer of ergonomic bucking bars and power tools to improve productivity and reduce injuries.\nThe advanced bucking bar, developed in collaboration with Richard Borcicky’s expertise in safety and ergonomics, provides three levels of vibration reduction: a wave spring, tungsten inserts, and a cushioned palm pad. Compared to traditional bucking bars, this reduces vibration up to 50%.\nLewis says the bucking bar manufacturer came to the Tulsa facility, demonstrated the bucking bars, and let the mechanics test them.\n“One of our mechanics had shoulder surgery after an unrelated injury, and was unable to rivet using typical bucking bars,” Lewis explains. “When we let her try the Honsa bucking bar, she was able to rivet without the impact and vibration hurting her shoulder. She spoke to our leadership team to get the first order pushed through, and later several more orders were placed for different areas in the plant.”\nThe advanced bucking bars include a precision non-mar height gage that can eliminate over-bucking and damage to metal and/or painted surfaces, ensuring that less-experienced riveters produce the same height bucktail on every rivet.\nExperienced riveters get a feel for properly set rivets,” Lewis says. More advanced bucking bars allow less experienced personnel to install rivets faster, with better feel.\nAerospace producers need bucking bars in potentially thousands of different shapes and sizes to suit specific applications, so customizing the modular bar with interchangeable end effectors can allow operators to reach a wide variety of difficult rivets.\n“Honsa was able to custom make a solution for pretty much every area we had – it was not one product for the whole plant,” Lewis says. He notes that the manufacturer was able to turn rough drawings from workers on the plant floor into fully engineered drawings. “We went back and forth with their design team to get a truly custom solution and they were very easy to work with.”\nUsing advanced bucking bars significantly improved production and safety in Spirit AeroSystems’ Tulsa facility, Lewis says.\n“The ergonomic bucking bars have definitely helped our production flow and reduced riveting redos by about 10% to 20%,” Lewis adds. “Anytime we can move the needle in terms of quality, it is a good thing for us and the customer.”\nUsing impact- and vibration-reducing tools has boosted crew morale as well. As a result, he has already recommended their use to other Spirit AeroSystems facilities in the U.S. and overseas.\n“Our work crew tends to do the same riveting tasks over and over,” Lewis concludes. “So, anytime we can help them do their job better and prolong their career, it is a win-win. For anyone doing aerospace riveting, switching over to advanced bucking bars is really a no brainer.”""]"	['<urn:uuid:f3bd5b5b-52bd-4b5d-9fdf-1e61fd943257>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	26	29	1005
71	wood decking durability comparison stone granite patio materials weather resistance	Stone and granite patio materials generally show better weather resistance and durability compared to wood decking. Wood decking, even when treated, is vulnerable to water damage, humidity, and sun exposure, requiring regular application of preservatives and sealers every 1-4 years. In contrast, stone and granite patio materials, while potentially porous, are naturally more weather-resistant - particularly slate which withstands cooler climates and temperature fluctuations well, and granite which tends to keep cleaner than stone.	['Seasonal Care and Maintenance for Your Decking in London\nDecking installation in London properties has been gaining enormous popularity lately and this is certainly not without a reason. Unlike a patio, it doesn’t require hard landscaping or the use of materials such as stone or cement; yet it can make the perfect quiet relaxation spot, a sitting area, a place for BBQ, al fresco dining, and entertaining. What’s best, its usage is not limited to properties with gardens only. Due to its light construction, it can be installed in roof terraces, balconies, and as extension to living areas.\nWith its clean-cut lines and natural look, a decking brings understated elegance to modern homes; softens the lines of more traditional properties, and even works great with sloped gardens. Levelling the base for decking installation is much easier than levelling a patio base. Its most popular shapes are square and rectangle but you can also experiment with circles, triangles, or trapezoids or even design your own decking to complement your home’s architecture. And for a truly unique look, you can also choose a colour that will make your London decking stand out. be it the natural colour of the wood, white for a Mediterranean feel, or a shade of grey, green, or taupe that will blend well with almost any garden.\nAfter you’ve chosen the perfect decking style to suit your needs, you will also need to consider the maintenance, so that your charming wooden oasis will look its best season after season.\nKeep a Close Eye on Your London Decking\nEven treated wood is not entirely impervious towards the elements. Water, humidity and sun exposure will affect your decking in a different way than they affect other wooden surfaces, such as walls or fences, causing a lot of damage even to a sturdy wood in just one season. Water stays on the flat surface of the decking longer, and with no shade or other overhead protection, sunlight can have a very detrimental effect on the wooden surface as well. Heavy traffic, moisture from plant holders, and restricted air circulation from furniture or other objects can also make your decking appear dingy in just one summer. And come autumn, piled dead leaves can lead to mildew growth and pest infestations. Other factors that will affect the appearance of your decking are dust, dirt, and bird droppings; while cracked boards, nails coming out from the floorboards, wobbly railings, and loose stairs can pose a serious risk of injury. That’s why, you need to keep a close eye on how your decking is ageing; take care of repairs and seasonal maintenance promptly.\nProvide Regular Maintenance\nSome very simple things can help you keep your wood decking looking good longer.\n- Take the time to sweep your deck on a regular basis. Be extremely careful that there’s no accumulation of dead, especially wet leaves.\n- Clear weeds and debris from the gaps between the floorboards regularly.\n- Clear spots and bird droppings upon noticing them to prevent permanent staining and discolouration.\n- Keep surrounding vegetation under control – prune hedges and shrubs to prevent them from growing within a couple of feet of your decking.\n- Move planters regularly if they’re not positioned in a way that allows proper airflow underneath them.\nLike other hardfloors, a decking will also suffer the consequences of foot and pet traffic. It is vulnerable to scuff marks, scratches from high heels, and dog nail scratches. That’s why, it is highly recommended that you lift chairs and other furnishings before moving them, keep your pets’ nails trimmed, and basically ensure the same amount of care for your decking as you do for any other section of flooring in your home.\nProtect Your Decking from London Weather\nTo ensure the best protection for your decking and prevent it from deteriorating quickly, apply a decking preservative. Use this even on pressure treated wood – although it is resistant to rotting and pest infestations. Floorboards can still crack and split from water exposure. You can find a variety of sealers and paints on the market especially formulated for use on decking. They can achieve a very different look; but overall their main function is to protect a wood or composite decking from moisture damage, mildew growth and sunlight. Some formulas can also act as fire retardants.\nOne thing you should consider when choosing decking protection products. Their protective properties fade over time and you will have to reapply them at one point, sooner or later. Although in general the more expensive the product, the longer its protection will last. There is really no such thing as lifetime protection for a wood decking. Reapply a wood sealer on a regular basis; best in autumn on a mild day when the temperature is stable and weather forecast doesn’t predict rain for the week to come. As highly unlikely as it may sound for London. Once a year should suffice and there are now new formulas that may last up to three or four years.', 'Picking the best patio paving material for your project can be challenging. Many factors must be considered, and the variety of paving materials available can make the decision even harder. Adding to the pressure is the fact that patio paving is costly and-labour-intensive, so you will likely be living with your choice for many years to come.\nFactors to Consider when Picking Patio Paving\nThere is really no “right” or “wrong” choice of patio paving material; only what is best for your situation.\n- What is the overall look and feel of your house and garden? Does it call for traditional or contemporary patio paving?\n- How will the patio be used? For informal or formal dining and entertaining? Relaxing? Pet kennels or sports practice?\n- Existing and future vegetation: perhaps your patio paving will be incorporating established trees, or you’d like to include flower beds, or even some raised herb or vegetable plantings.\n- Budget and personal preferences: do you or a family member have your heart set on a certain finish? Is local sourcing or a low carbon footprint an important factor?\nStone patio paving adds character, as no two pieces are exactly the same, and allows for a more natural-looking, informal paving layout. Soft stone can be porous and hard to clean, and tends to become mossy with age. Blue limestone, or Kilkenny limestone, is said to absorb less dirt than other soft stones. Indigenous stone can prove to be more expensive than imported, but it has a lower environmental impact and carbon footprint. Slate withstands cooler climates and temperature fluctuations well.\nActually a hard stone, granite patio paving looks luxurious but can also be porous, although it tends to keep cleaner than stone. Chinese granite tends to be softer, whereas Portuguese granite is said to be harder-wearing.\nConcrete can be poured on site, or purchased in precast pieces in a variety of shapes, sizes and colours. A polished eco-concrete, which can give a lovely natural look, is now also available. Concrete patio paving need not be boring: a mixture of different shapes and sizes can be most effective. If you are considering coloured concrete, check how long the colour is guaranteed for, as it can fade over time with exposure to light and the elements.\nGravel is often overlooked as a possible patio paving material, but it is excellent for introducing warmth and texture and invaluable when it comes to accommodating tricky shapes or existing trees. Gravel will need an edging, such a brick, to keep it in place, which can make for a nice contrast. Loose gravel can be prone to weeds, but self-binding gravel is now available, which is suitable for wheelchairs and prams.\nBrick patio paving looks warm and classical, is suitable for all climates, and lends itself to a variety of patterns and combinations. It is vital that the course on which the bricks will be laid is thoroughly prepared beforehand, or very soon the bricks will start lifting up and sprouting weeds between them.\nAvailable in an almost unlimited variety of shapes, sizes, colours and textures, tiles can be laid in regular or mosaic-like patterns to make bold visual statements. They can also be a slipping hazard when wet and the grouting between them will require maintenance.\nTraditional cobbles can look charming, but their uneven surface can make them impractical. They are slippery when damp and almost impossible for prams and wheelchairs to navigate. They need to be laid so that they won’t work loose from the concrete, which looks messy and creates a tripping hazard.\nMaking the Final Selection\nOnce you’ve drawn up a shortlist of options, it can be well worthwhile to spend some time walking around your neighbourhood and observing existing patios. If possible, talk to friends and relatives with patios about the pros and cons of their patio paving. Consider investing a couple of weekends in visiting showrooms and asking questions. Then you will be fully equipped to choose the best patio paving materials for your project.']	['<urn:uuid:5e0e90dd-cf22-41a5-8d6b-f5bbaf6cd95f>', '<urn:uuid:76244533-1ac3-4848-aa61-a72d8782128b>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	10	74	1509
72	As a religious scholar comparing dress codes, are there specific color restrictions in Islam vs Northern Insight Meditation Center?	While Islam has no specific color restrictions and allows yellow clothing as long as it's modest, the Northern Insight Meditation Center strictly requires white clothing only. Islam emphasizes modesty over color, while the meditation center has explicit rules mandating white attire.	"['There are numerous myths and misconceptions surrounding the topic of clothing in Islam. One such debate revolves around the question: Is it Haram (forbidden) to wear yellow in Islam? In this article, we will debunk these myths and explore the religious guidelines regarding clothing in Islam.\nThe Significance of Color in Islam\nIn Islam, clothing holds religious and cultural significance. The Prophet Muhammad (peace be upon him) emphasized the importance of modesty and simplicity in dressing. While Islam doesn’t prescribe specific colors to wear or avoid, it encourages Muslims to dress modestly and avoid clothes that draw unnecessary attention. The focus is on modesty rather than specific colors.\nYellow as a Color\nYellow is not inherently prohibited in Islam. It is a vibrant color that can symbolize happiness, optimism, and positivity. However, just like with any other color, wearing yellow should be done with modesty and in accordance with Islamic teachings.\nUnderstanding the Misconception\nThe myth that wearing yellow is Haram in Islam may have originated from cultural beliefs or personal opinions, rather than religious guidelines. Islam encourages individual expression through modest clothing choices, and as long as the clothing is not revealing or provocative, yellow can be worn without any religious prohibition.\nApplying Islamic Guidelines\nWhen choosing clothing, regardless of the color, Muslims should adhere to the following Islamic guidelines:\n- Dress modestly, covering the awrah (private parts) according to the teachings of Islam.\n- Avoid clothing that is transparent or form-fitting.\n- Avoid clothing that is extravagant or overly flashy.\nIn conclusion, wearing yellow is not Haram in Islam. Islam places emphasis on modesty and simplicity in clothing choices, rather than any specific color. Muslims should focus on adhering to the general guidelines of modesty and avoid clothing that is revealing or flamboyant. By understanding and debunking such myths, we can promote a better understanding of Islam’s teachings on clothing.\nFaqs about “is it haram to wear yellow in islam”\nIs it haram to wear yellow in Islam?\nWearing yellow clothing is not specifically prohibited in Islam. Islamic teachings do not explicitly mention any restrictions on the color yellow. Muslims are encouraged to dress modestly and avoid clothing that is overly flashy or attention-seeking, regardless of its color.\nCan I wear yellow clothing during Ramadan?\nYes, you can wear yellow clothing during Ramadan or any other time. The color of one’s clothing does not have any direct impact on the observance of Ramadan or any religious obligation. It is more important to focus on maintaining a sincere intention and observing the principles and obligations of Ramadan.\nIs there any significance of yellow in Islam?\nYellow holds no specific significance in Islam. Islamic teachings do not attribute any religious or symbolic meaning to the color yellow. However, colors like green hold significance in Islam, as they are associated with Prophet Muhammad and represent life and paradise.\nAre there any guidelines for dressing in Islam?\nIslam encourages modesty in dressing. Muslims are advised to dress modestly, covering the private areas and avoiding clothing that is tight, transparent, or revealing. The intention should be to dress in a way that demonstrates humility, respect, and adherence to Islamic principles.\nIs there a specific dress code in Islam?\nThere is no specific dress code that applies universally to all Muslims. However, Islamic teachings emphasize modesty and require covering the private areas. Different cultures and regions may have their own traditional styles of Islamic dress, such as the hijab for women.\nCan men wear yellow clothing in Islam?\nWearing yellow clothing is permissible for men in Islam. There are no specific restrictions on men wearing yellow. As mentioned earlier, the emphasis is on modesty, rather than the color of the clothing.\nWhat colors are recommended in Islam?\nThere are no specific colors recommended or required in Islam. However, colors like white and green hold positive connotations and are often associated with Islam. White symbolizes purity and cleanliness, while green is associated with Prophet Muhammad and represents life and paradise.\nIs it haram to wear bright-colored clothing in Islam?\nWearing bright-colored clothing is not inherently haram (prohibited) in Islam. The focus should be on dressing modestly and avoiding clothing that draws unnecessary attention or goes against the principles of humility and decency. It is important to prioritize modesty and intention while selecting clothing.\nCan yellow be worn as part of traditional Islamic clothing?\nYes, yellow can be worn as part of traditional Islamic clothing. Different cultures and regions may have their own traditional styles and colors of Islamic clothing, and yellow can be incorporated into those styles if it is culturally accepted. It is important to consider the cultural context and norms when wearing traditional Islamic attire.\nShould I avoid wearing yellow to a mosque?\nWearing yellow to a mosque is generally acceptable. There are no specific restrictions on wearing yellow in a mosque. However, it is good to consider the overall modesty and appropriateness of your attire when visiting a place of worship, including mosques. Dressing respectfully, modestly, and without causing distractions is advised.\n- Surah Yaseen Pdf download | Mp3 | Video | Images\n- New Ramadan Iftar and Sehri Time 2023 | Best Calender\n- Surah Yaseen Ayat 1 with Best Translation 2023\n- Surah Yaseen Ayat 20 Read online with translation (2023)\n- Is Smoking Haram or Halal? Why? Islamic Perspective 2023\n- Is Cineplex Poutine Haram or Halal? Religious Overview 2023\n- Taharat-o-Namaz ka SUNNAT Tarika | Saheh Ahkam-o-Masal\n- The Blessings of Tahajjud | Best Time | Rakat |Tahajjud 2023\n- Tahajjud Time in Gujranwala: Night Prayer in Pakistan\n- Meaning of “Allahumma Barik”: Understanding Its Importance\n- Iman e Mujmal: Understanding the Basic Tenets of Faith in Islam\n- The Sword of Imam Ali: Exploring the History, Significance, and Mystique of Islam’s Most Iconic Weapon\n- Sifat meaning in urdu | English |Arabic | Meaning of صفت\n- How to perform Eid-ul-Fitr? Eid-al-Fitr Mubarak – 2023\n- The Top 15 Most Important Islamic Worship Places in the World', 'The Northern Insight Meditation Center welcomes all who are\nwilling to learn the Vipassana-Meditation Practice to develop the mind. May you\nbe happy, free from suffering, diseases, grief, troubles, difficulties and\ndanger and be protected from all misfortune.\nINSIGHT MEDITATION AND THE TECHNIQUE PRACTICED\nMental Development is a personal experience. It does not matter if you are\nBuddhist, Christian, Jewish or Moslem. Nor is it important what nationality or\ncolor you are, as each person in the world is longing for a better life. The\nInsight Meditation Technique taught here is the way to prepare a path to a\nbetter life of peace through the right understanding about one self.\nMEDITATION is the English word for MENTAL-DEVELOPMENT.\nThere are two kinds of Meditation\nTranquil - or Samatha Meditation develops concentration (samadhi) on\none object to help calm the mind.\nVipassana - or Insight Meditation develops self understanding through\nThe Teaching and Practice at Wat Ram Poeng (Tapotaram) is based\non THE FOUR FOUNDATIONS OF MINDFULNESS.\nContemplation of the body\nContemplation of feelings\nContemplation of the mind\nContemplation of mind objects\nWat Ram Poeng offers a 26-day basic course in Vipassana\n(Insight) Meditation under the guidance of a Teacher on an on-going basis. For\nmeditators who have completed the basic-course, a 10-day Insight Meditation\nRetreat can be taken which builds on the 26-day basic course.\nFor those who do not have time to join the full basic course, the Monastery\noffers the chance to try out the practice, but for not less than 10 days.\nIf you wish to join the Insight-Meditation course offered here, please come to\nthe administration office with your intentions and further arrangements can be\nmade. Once you have received official permission, please prepare the following\nThe customary offering for the opening ceremony\neleven white lotus (or other white flowers)\neleven yellow or orange candles\neleven incense sticks\nA completed application form\nYour valid passport and visa\nTwo passport photos\nTwo photocopies each of a valid passport and visa with entry stamp\nA working alarm clock or timer. You must be able to set the timer to five\nAt least two sets of white clothing, which can be purchased from the Temple\nFor men: Loose, modest, non-transparent white trousers and shirt. White\nunderwear is a must.\nFor women: Loose, modest, non-transparent white sarong or trousers, a loose\nwhite shirt with sleeves and a white `sabhai\' ( a white scarf, which is worn\nover the breast and around the shoulder). White underwear (inc, bra) is a\nAll personal items, such as soap, shampoo, toothpaste, etc. should be\npurchased beforehand. However you can buy some essentials at the Temple\'s\nGENERAL RULES FOR MEDITATORS\nWhat does it means to stay in a MONASTERY?\nInside of the Monastery, the 8-EIGHT-PRECEPTS are followed. Please read these in\nthe ""Opening Ceremony"" notes. As well, the following points should be noted\nYou and your clothing must always be clean, proper and\nhygienic. You have to wear white clothing by day and night.\nPlease keep your room clean and tidy.\nThe Precept: to refrain from killing / destroying living creatures is in\nitself an order not to attract insects or animals to stored food. Stored food\nleads to killing, which makes the precept of not killing easier to break,\ninadvertently or otherwise.\nKeep the bathroom and toilet clean in order to prevent fungi\nMeditators are not allowed to talk about their personal\nmeditation practice or experience. Do not discuss or compare your meditation\npractice with each other. Your experience is your own and may not be the same\nYou are not allowed to mix the practice with other techniques.\nYou are only allowed to smoke cigarettes in your room, but\nsure it\'s better to avoid that burdensome habit.\nNo kissing, hugging, holding hands, massaging or any other\nphysical contact is allowed. No sun-bathing.\nWhile taking a bath or sleeping, make sure that the door, and\nwindow/curtains are closed and locked.\nThe Meditators rooms are for their privacy. Visitors are not\nallowed to enter the Meditators\' rooms.\nMeditators should not visit others in their rooms. Men are not\nallowed to enter women\'s rooms. Women are not allowed to enter men\'s rooms.\nNo socializing, gossiping etc.\nNo reading; this includes BUDDHIST BOOKS. No writing (letters,\ndairies etc.). No listening to radios - tapes - CD\'s etc. No telephone-calls\nduring your practice. Please leave mobile phone at the front office.\nMeditators are not allowed to leave the Monastery area without\nthe permission of the Teacher.\nIf you are tired during day-time, you may lay down and rest in\nyour own room, but meditators are not allowed to sleep during day-time.\nPlease unplug all electrical appliances when not in use and\nturn off all lights, fans, etc., when leaving your room.\nTemple Authorities reserve the right to refuse entry or expel\nin the case of disrespect regarding the rules or instructions.\nUpon completion of the course, meditators must pay respect to\nthe Teacher at the Closing Ceremony and gain further advice from him/her about\nMeditators are responsible for the loss or damage of Temple\nAll Temple properties must be returned and the meditator\'s\nroom cleaned before leaving. Consider yourself as the next person to use the\nroom. Don\'t forget to give back the key to the office.\nEach evening before WAN PHRA-BUDDHA DAY meditators must share\nthe ceremony in the VIHARN - main Temple - to renew the precepts.\nDonations are appreciated. All donations should be made at the\nTemple-office, where an official receipt can be obtained.\nThe bell or your alarm-clock should wake you up for practice.\nStarting with the mindful prostration, you continue with the mindful walking\npractice and then sitting practice.\nNOTE: Always start with \'mindful walking\', then when you stop walking, take\nyour place immediately for the sitting practice.\nThe bell rings for breakfast. It is your responsibility to\nfollow the bell and arrive in the dining hall on time, because prayers are\nchanted before each meal.\nAlms-food is considered sacred.\nTake only as much as you will eat.\nLate-comers will not be served.\nEat slowly, mindfully and preferable alone.\nTo EAT ALONE means no talking. Do not make any conversation during or after\nmeals as doing so is disruptive to mindfulness.\nWash your dish and glass immediately after eating. Take care of the rubbish\nand put it in its place near the sink.\nAfter breakfast, CLEANING - WASHING - BATHING MAY BE CARRIED OUT.\nThen, it is again time for practice, until the bell rings.\nThe bell rings for lunch.\nAfterwards, it is time for Practice until REPORTING\nSleeping time may begin while dressed in your white clothing.\nOnce a day you REPORT to the Teacher.\nHere an example\nSAWASDEE - KA, AJAHN SUPHAN (women)\nSAWASDEE - KRAP, AJAHN SUPHAN (men)\n""You told me to do the first walking step acknowledging RIGHT goes thus, LEFT\ngoes thus for 20 min. I was asked to do 20 min SITTING, observing and\nacknowledging the rising and falling of the belly as rising and falling. In\ntotal, I was told to do 7 hours. I did 8 hours.\nThe practice is both easy and difficult. Walking is easier than sitting, and\npain is a problem, with cramped legs when thinking. I never imagined, that 20\nminutes sitting could be so long. I have had doubt, anger, impatience and I was\nalways looking at the timer. Sometimes I felt a little bit tired, sometimes the\nthinking became like a movie.""\nIf the Teacher wants to know more, then he will ask you. Give short answers.\nSo no STORY about the pain, anger, or thinking is necessary. You do not have to\nfind excuses as to why you are not perfect. You must just learn to understand\nyour imperfection. Be patient with yourself.\nWhen you start the basic course, to learn the Vipassana Meditation, you are all\nYOGIES = Meditation students.\nAcknowledging is the heart of insight meditation. It is the\ncontinual work of mindfulness to be aware and acknowledge. Insight meditation\nthrough the four foundations of mindfulness focuses on the body, feeling, the\nmind and mind objects. The body postures are standing, sitting, walking and\nMindfulness of the body is the acknowledgement of the bodily\nmovement such as the rising/falling of the abdomen when breathing and the\nright or left foot when stepping.\nMindfulness of feeling is the acknowledgement of the happiness\nor suffering which appear while we are concentrating on the rising/falling.\nWhen happiness or suffering happens we stop acknowledging the rising/ falling\nto acknowledge the feelings. For example, when we feel the pain in any part of\nthe body, we acknowledge ""pain, pain, pain"". For a while before we resume\nMindfulness of the mind is the acknowledgement of our\nthoughts. While we are acknowledging rising/falling, our minds may think of\nwork or home. We have to stop acknowledging rising/falling and acknowledge\n""thinking, thinking, thinking"" for a while before we resume acknowledging\nMindfulness of mind objects is the acknowledgement of the five\nhindrances: like, dislike, drowsiness, anxiety and doubt. These exist in the\nminds of people of all nations. While we are concentrating on rising/falling,\none of the hindrances such as pleasure may occur in the mind. We have to stop\nacknowledging rising/falling and acknowledge "" pleased, pleased, pleased""\ninstead. If it is displeasure which occurs, we acknowledge ""displeased,\ndispleased, displeased."" If it is drowsiness that occurs, we acknowledge\n""drowsy, drowsy, drowsy."" If it is anxiety that occur, we acknowledge\n""anxiety, anxiety, anxiety."" If it is doubt that occur, we acknowledge ""doubt,\ndoubt, doubt."" After we acknowledge the hindrances for a while, we resume\nThe present moment is immensely important to meditation\npractice. Acknowledge the body/mind in the present moment develops and\nstrengthens momentary concentration. Without acknowledgement of the present\nmoment, meditation practice cannot progress because momentary concentration\nContinuity is also important. Be mindful from the moment you\nwake up till falling asleep at bedtime. We have to acknowledge our daily\nactivities. Once we rest after sitting we may do some work or talk without\nmindfulness. When that happens the mind, not acknowledging, wanders away, gets\ndistracted, and thus the momentary concentration which has been developed will\nThe goal of insight meditation is to gain a clear, complete\nunderstanding of the three obvious characteristics: impermanence, suffering\nand non-self. Having gained an insight into the three characteristics, the\nmeditator realizes that everything in this world is transient, subject to\nsuffering and uncontrollable because it is not-self. Thus the mind abandons\nthe desire to acquire, the desire to have and the desire to be.\nLord Buddha gave five purposes for insight meditations\nTo purify the mind.\nTo get rid of sorrows and lamentation.\nTo get rid of physical and mental sufferings.\nTo understand the truth of life.\nTo extinguish suffering and gain nibbana.\nTHE OPENING - CEREMONY\nThe following is the translation is of the Opening Ceremony.\nThe Teacher or assistant will speak the PALI-words one by one and you have to\nrepeat them in turn.\nIt would be a benefit to read the translation before the ceremony.\nHOMAGE TO THE TRIPLE GEM\nWith hands joined together in anjali, recite the passage in Pali:\nI undertake these Eight precepts.\nI undertake these Eight precepts.\nI undertake these Eight precepts.\nimāni attha sikkhāpadāni silena sugatim yanti silena bhogasampadā silena\nnibbutiivi yanti tasmā silam visodhaye These Eight Precepts have morality as a vehicle for happiness, good fortune,\nand liberation, Let morality therefore be purified.\nREQUESTING THE KAMMATHANA\nSitting in a kneeling position, with both hands together in anjali, recite the\nfollowing passages in Pali:\n1. Paying Homage and offering oneself to the Buddha imāham bhagavā attabhavam tumhākam pariccajāmi Holy Sir, the Lord Buddha, the Blessed One, may I humbly offer my body and\nmind to your for the purpose of practicing Insight Meditation.\n2. Paying Homage and offering oneself to the Teacher imāham ācariyā attabhavam tumhākam pariccajāmi Venerable Sir, Teacher, may I humbly offer my body and mind to you for the\npurpose of practicing Insight Meditation.\n3. Requesting the Meditation Exercise: nibbanāssa me bhante saccikaranathāya kammatthānam tehi Venerable Sir, please give me the instruction for Insight Meditation so that\nI may realize Nibbana.\n4. Extending friendship to yourself aham sukhito homi niddukkho homi avero homi abhayā pajjo homi anigho homi\nsukhiattānam parihārāmi May I be happy, free from suffering, free from enmity, diseases and grief,\nfree from troubles, difficulties and danger and be protected from all\n5. Extending friendship to all beings Sabbe sattā sukhitā hontu averā hontu abhayā pajjā hontu anighā hontu sukhi\nattānam pari hārāmi May all beings be happy, free from suffering, free from enmity, diseases and\ngrief, free from troubles and difficulties and be protected from all misfortune.\n6. Practice the exercise of mindfulness of death (maranasati) thus:\nOur lives are transient and death is certain. That being so, we are fortunate to\nhave entered upon the practice of Vipassana on this occasion, as now we have not\nbeen born in vain and missed the opportunity to practice the Dhamma.\n7. Resolving to the Buddha and his disciples: Yeneva Yanti Nibbānam Buddha Desanca Sāvaka Ekāyanena Maggena Satipatthānā\nSanninā Ti The path taken by all Buddha\'s and their Two Chief Disciples, their great\ndisciples and their Arahant disciples to Nibbana, the path which is known as the\nFour Foundations of Mindfullness and is the path comprehended by the wise, I\nsolemnly promise to practice to attain that Path, the Fruition and Nibbana,\naccording to my own initiative, from this occasion onwards.\n8. Imāya dhammānudhamma patipattiyā rattanatayam pūjemi\nWith this practice of Dhamma, worthy of Dhamma, I worship the THE TRIPLE GEM\nAsking for Forgiveness\nIt is advisable, in the Teachings of the Lord Buddha, that when\na person has done wrong to another - by thought, word or action - to ask for\nforgiveness of the wronged person. One should not have thoughts of revenge\nagainst the former, but should forgive the other for their wrong doing.\nIf a person undertakes the meditation practice with thoughts of anger, hatred or\nrevenge against the Teacher, he will not be able to progress in the meditation.\nTherefore, it is advisable to beg forgiveness from the Teacher before beginning\nthe meditation practice, during the course of the practice (when wrong doing has\noccurred) and upon completion of the practice, when taking the Closing-Ceremony.\nTo ask forgiveness, with hands joined together in anjali, recite the following\npassage in Pali\nForgive me (us), Venerable Sir, for all wrong doing done\ncarelessly to the Reverend One by way of the three doors -mind-speech-body.\nThe Teacher then responds\nAham Khamāmi Tayāpi (Tumhehipi) me Khamitabbam\nI forgive you (you all), you should forgive me.\nThe disciples then responds\nKhamāmi (Khamāma) Bhante I (We) forgive you, Venerable Sir. Then the disciples prostrate three times.\nThe steps of the Closing Ceremony is similar to the Opening\n- Homage to the Triple Gem.\n- Requesting the Three Refuges. Change ""ATTHA SILANI to PANCA SILANI"".\n- Change Eight Precepts to Five Precepts.\nTHE FIVE PRECEPTS\nTo undertake the Five Precepts, with the hands joined in anjali, repeat each\nprecept after the leader in Pali:\n1. Pānātipātā Veramanī Sikkhāpadang Samādiyāmi\nI undertake the precept to refrain from destroying living\n2. Adinnādānā Veramanī Sikkhāpadang Samādiyāmi\nI undertake the precept to refrain from taking what is not\nThese Five Precepts have morality as a vehicle for happiness,\ngood fortune, and liberation, Let morality therefore be purified.\nADDITIONAL ADVICE TO FOREIGN MEDITATORS\nIn Thailand, the feet are considered to be the lowest part of your body.\nTherefore, it is rude to point your feet at anyone. While sitting and during\nmeal times, please be aware of this and sit with your legs either crossed in\nfront of you or to the side.\nBe also aware that it is rude to point your feet at any Buddha images.\nThe mats in the temple are to be used by the monks only, please do not use or\nDo not touch the top of any Thai person\'s head, as this is also considered\nDon\'t hang your under wear and socks outside. Let them dry in your room.\nShould you wish to leave the meditation practice earlier than planned, you\nmust inform the office or the Teacher, so that a Closing Ceremony for you can\nbe arranged. It is very disrespectful to leave the Monastery without informing']"	['<urn:uuid:b3efa5d6-3c2e-4f70-a82b-52c890c6da00>', '<urn:uuid:43dbf26f-a4c8-4224-a76a-19bfbf693f39>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	19	41	3720
73	cost penalties unauthorized stock image use	Using stock images without proper licensing can result in legal penalties that can range into the thousands. Stock image companies actively pursue unauthorized usage, and images often contain coded tracking information that makes them easy to locate even if modified.	['Many graphic designers have a tool in their creative arsenal that aids in creating fantastic designs quickly: stock imagery. Stock images are all over the place – in online ads, corporate websites, highway billboards, you name it – although chances are, you haven’t really thought much about it. Think back to the last ad you saw featuring unusually attractive people who seem just a little too happy to be at work (or shopping, or eating dinner, or visiting a doctor)—probably stock!\nHow does stock imagery impact you, and your contest? Here we’ll teach you how to better identify stock images in your designs and how to properly secure the rights to use them. 99designs’ policy prohibits designers from using stock images in logo design contests, but they’re free to incorporate them into submissions for other types of design contests.\nWhile designers are required to notify you if stock imagery is used, it’s helpful to have a basic understanding of stock so you know what to look out for.\n1. What exactly is a stock image?\nAs you’ve likely guessed by now, stock images have nothing to do with investing, shares or Wall Street. But the creator is paid for them! Stock images are professionally designed or photographed images sold or licensed for use to the buyer or licensee.\nDesigners often incorporate stock images into their designs to provide clients with high-quality photos without having to put resources into creating their own images. If graphic designers had to line up photo shoots every time a client wanted to incorporate a shot of a couple guys in suits exiting a building or a city skyline on a blue-sky day, you can imagine how difficult it would be to get the actual design work done!\nThe stock images used on 99designs usually fall into one of two stock categories: photographs and vector images.\nSome of the larger stock photograph distribution companies include Getty Images, Corbis, ShutterStock, and iStockphoto. Designers are able to browse libraries of thousands of photos and select one that is appropriate for your design requirements.\n2. Will designers tell me when they’re using stock?\nAs we mentioned, our designers aren’t allowed to use stock images in logo design contests. When they use them in any other design contest categories they’re required to inform the contest holders. Our support team actively identifies and reprimands designers who fail to do this—we take it very seriously!\nThe notification you should receive will also include a link to the image so that you can easily navigate to its source and purchase the appropriate license, as seen above.\n3. The licensing process\nLicenses for stock images are generally divided into two types:\nRoyalty-free means that once someone has purchased a license to an image they can use that image multiple times without paying any additional fees to do so. That said, there may be some restrictions, so it’s important to review the terms and conditions of the license to be aware of any of its limitations.\nRights-managed images are generally restricted in terms of usage – limitations may include industry, geographic location or the duration for which the image can be used.\nLicense fees vary widely, but they’re not generally steep – usually you’ll dole out no more than the sum total of your weekly Starbucks habit—for the non-coffee drinkers out there: you can expect to pay in the $20-$50 range to license an image.\nNot having the appropriate license to use a stock image can cost you big time. Companies who own these images can and will pursue you legally if you’re found to be using an image that hasn’t been licensed properly, and penalties can range into the thousands. Many stock images contain coded tracking information that, even if the image is altered or modified, makes them quite easy to locate.\nIf you’re unsure what type of license you’ll need to purchase for a particular image, be sure to contact the company who owns it and ensure you’ll be covered.\nThe vast majority of companies who use stock images in their branding and marketing materials have no legal issues at all, so don’t be scared off. There’s absolutely no reason to avoid designs with stock images – they’re the basis of many extraordinary designs! Still, it’s good to have a basic grasp on what stock images look like and how to recognize them at a glance.\n4. How to spot stock\nThe easiest way to identify a stock image is to look for a watermark on the image itself. This watermark will often indicate the source of the image, as seen below:\nGenerally, any photographs included in your design are most likely stock photographs. Unless the designer actually took the photograph, chances are they obtained it from an internet resource and it may require a license. Ask the designer for the source of the image and for a direct link to the source as well.\nIt can be a bit more difficult to determine if non-photographic images are stock. One way to check is to use reverse image search platforms like Google Images or TinEye.com, which allow you to upload the image and search it against billions of others. If the image doesn’t turn up, chances are it’s original.\nIf you’re not sure, don’t hesitate to ask the designer where they obtained it or contact 99designs Customer Support to assist you. If a designer offers to secure the rights to a stock image for you, bear in mind that the license of stock images generally isn’t transferrable and the 99designs copyright contract does not provide for this transfer. Our designers should know that, but ultimately it’s your responsibility. Instead, you’ll want to directly purchase the license to the images on your own.\nThis may sound like a lot of work, but in reality, it all boils down to just a few extra minutes of your time. And with all of the effort you put day in and day out into making your business a success, it’s certainly worth it to ensure you’ll be able to use your new design, hassle-free, for years to come.']	['<urn:uuid:80922a3c-919a-407f-b8b9-6efa743ebba6>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	6	40	1021
74	Is organic farming different in Spain versus Canada?	Yes, organic farming practices differ between Spain and Canada. Spain's organic agriculture is regulated under European Union legislation and focuses on maintaining soil fertility and using natural resources rationally. Canada has specific requirements like prohibiting hydroponic production, sodium nitrate fertilizer use, and mandating specific living space requirements for livestock. Canada also allows antibiotic treatments for animals under certain conditions, while in Spain, as part of the EU, organic farming emphasizes environmental preservation and natural production methods.	['Organic, Nearby and Healthy\nOrganic Agriculture for quality products\nOrganic agriculture is a form of production which has as one of its main objectives to obtain products of the maximum nutritional and sensory quality, respecting the environment.\nTaking advantage of the natural resources in a rational form. Preserving and at the same time enriching the fertility of the soil. Organic products are vital foods that maintain all its organoleptic and nutritional properties, with a high fibre content, rich and balanced in vitamins and minerals; as well as free from artificial substances, and of complete confidence for the consumer.\nWhen you consume a product which comes from organic agriculture, you are contributing to the benefit of your own health as well as to the conservation of the natural environment. The environment is not damaged by chemical products, using nature without breaking its biological cycle, to obtain from the land what the land is able to produce not having to overuse it by using polluting substances.\nFruit of which is the production of food products free from undesirable chemical residues.\nBecause the health of the land is our own health.\nES-ECO-023-MA. Organic Agriculture of the Community of Madrid\nThis the code of the control organism that regulates our industry sector ES (Spain) ECO (Organic) 023-MA (Code for the Community de Madrid). AGRICULTURA UE (the raw materials used in the products come from the European Union). In our case, they only come from Spain.\nThe Organic Agriculture Committee of the Community of Madrid (CAEM) is the deconcentrated organism of the Community of Madrid designed to act as the Control Authority of the organic agriculture production and responsible of implementing the control system established by the European Union legislation on this matter in the Community of Madrid. Presently between producers, processors, importers the CAEM has more than 350 operators.\n* Decree 108/2018, of 19th of June, of the Cabinet Ministry, by which the decree 84/2018, of 5th of June, which established the organic structure of the regional environmental government (BOCM nº 146, of 20th of June). Error correction (BOCM nº 151, of 26th of June)\n* Decree 59/2018, of 24th of May, of the State Council, where the competences and organic structure of some of the Ministries of the Community of Madrid are partially modified. (BOCM nº 124, of 22nd of May)\n* Decree 58/2018, of 21 of May, from the President of the Community of Madrid, by which it is established the number and names of the ministries of the Community of Madrid (BOCM nº 121, of 22nd of May)\nM, certified product\nEThe trademark with which the Community of Madrid guarantees the origin and the quality of the agri-foods produced in its region are known as: M Producto Certificado.\nSince 2nd of May 2015, Conservas Cachopo, S.L. has been authorised the use of the Quality Trademark “M CERTIFIED PRODUCTS” since we meet all the demanded conditions and requirements. The objectives of this Trademark is to guaranty the differentiated quality of the food products produced and elaborated in the Community of Madrid and highlight the importance the professions linked to the agri-food sector as an essential part of the economic development of the regional territory. With this logo, the consumers can easily identify food products from Madrid, associated with values which the brand represents such as proximity, guarantee and confidence.\nNQA, auditing company specialised in the management system of food innocuousness has given Conservas Cachopo, S.L. since last April 2017 the certification in Fiven SSC22000, a quality standard comparable to other certifications such as IFS or BRC.\nAll the products produced in Conservas Cachopo, S.L follow strict quality and safety procedures and the whole company is highly committed with the quality, safety and development controls, because of which, in order to give more value to the products manufactured by the company and to improve the service given to the customers, Conservas Cachopo, S.L. has implemented the Management System of Food Innocuousness FSSC 22000. The quality and food innocuousness guide us to protect better the consumer through all the links of the food chain, with the commitment of the whole company to achieve the requisites which Conservas Cachopo, S.L. imposes in its processes and products, as well as the legal requirements and the applicable legislation, to manufacture products SAFE for consumption.\nTo support the follow up and the compliance of the quality and food innocuousness values, Conservas Cachopo, S.L. imposes every year a series of measurable objectives which will be periodically reviewed by the management of the company with the sole objective of carrying out a constant improvement in its production processes and products.\nOrganic agricultura in Spain\nOrganic agricultura, can be simply defined as the summary farming techniques that normally exclude the use of synthetic fertilizers, insecticides, pesticides, antibiotics, etc., with the objective of preserving the environment, maintain or increase the fertility of the soil and offer food products with all its natural properties.\nOrganic agriculture has been legally regulated in Spain since 1989, once the Regulation of the Generic Name “Organic Agriculture” was approved, which was applied until the use of the 2092/91 Regulation (CEE) related to organic agricultural production and its indications in the agricultural and food products.\nPresently, since the 1st of January 2009, the date that it was implemented, the Organic production is regulated by the Regulation (CE) 834/2007 the advice on production and labelling of organic products and the repeal of the Regulation (CEE) 2092/91 and the Regulations: R(CE) 889/2008 of the Commission, by which the orders of implementation are established of the R(CE) 834/2007 with respect to the organic production, its labelling and control and R(CE) 1235/2008 of the Commission by which the orders of implementation are established of the R(CE) 834/2007, in what is referred to the importation of organic products from third countries. (See section “Documents of interest/ Legislation”).\nIn Spain, the control and certification of the organic production farming is the competence of the Autonomous Communities and it is normally carried out by public control authorities, through territorial Boards or Committees of Organic Agriculture which are organisms which depend directly from the corresponding Ministries or Departments of Agriculture, or directly by the General Directorate assigned to them.\nNevertheless, the Autonomous Communities of Andalucia and Castilla La Mancha, have authorised private organisms to carry out these functions and, in the case of Aragon, the competent authorities have appointed a public control authority as well as private control organisms.\nA trademark so that the consumer can recognise products coming from organic agriculture, all the packaged units, besides having their own brand and the mention of some of the specifications of organic agriculture, require to have printed the authorisation code and the control organism or the specific logo, with the name and the code of the control organism. Also, the community logo of the AE mentioned at the beginning of this page can also be stamped, which will be an obligation, in the new design, the 1st July 2010, under the conditions of the legislation.\nAll of this means that the farm or industry where the product has been grown or produced, is submitted to the corresponding controls and inspections of the Authority or the established Organism depending on the Autonomous Community. It also constitutes the only official guarantee that the product meets the quality expected by the consumer and that it complies to the legislation established in the Regulation (CE) 834/2007 and the provisions of application.\nFor more Information about the authorities and organic production control organisms in each Autonomous Community, refer to the section “Documents of interest/general information of interest related to organic agriculture”.\nDevelopment and evolution of organic farming in Spain\nOur country gathers the conditions to develop this type of agriculture because of its favourable climate and the extensive production systems which are applied to many crops. In respect to animal production, the conservation of a patrimony of regional breeds of genetic importance, in general very rustic and adapted to the environment favouring the rearing in an extensive system. All of this, without forgetting the tradition and the development obtained in organic agriculture.', 'We’ve all seen it. Organic here, organic there, organic everywhere!\nThese days, it seems as though it’s hard to find a product that doesn’t have the USDA Organic label sealed someone amongst nutrition facts, ingredients lists, and dietary claims on food packages these days. With nearly 28,000 USDA certified organic operations in the United States alone, organic practices are certainly trending but why? Of course, it sounds nice, but what does organic really mean?\nWhat Makes Food Organic?\nWell there are many ways to look at it. A good place to start could be the always reliable Merriam-Webster which defines organic as:\n‘of, relating to, yielding, or involving the use of food produced with the use of feed or fertilizer of plant or animal origin without employment of chemically formulated fertilizers, growth stimulants, antibiotics, or pesticides’\nNot bad, Merriam-Webster, not bad.\nGenerally speaking, food is considered organic when it is produced by methods that comply with the standards of organic farming.\nWhat is Organic Farming?\nOf course, the standards vary from country to country, but organic farming normally features practices that cycle resources, promote ecological balance, and conserve biodiversity. Organizations regulating organic products may restrict the use of certain pesticides and fertilizers in the farming methods used to produce such products. Organic foods typically are not processed using irradiation, industrial solvents, or synthetic food additives.\nWho Inspects Our Food?\nTwo government agencies, the U.S. Department of Agriculture and the Food and Drug Administration, share most of the responsibility of food safety inspection. The rules that determine which agency is responsible for which food can be complex, and sometimes the division of labor defies categorization altogether.\nTake eggs, for example. The FDA inspects shelled eggs, while the USDA is responsible for egg products, including liquid, frozen and dehydrated eggs. The FDA regulates the feed chickens eat, but the laying facility falls under USDA jurisdiction. Therefore, In the United States, the USDA is the regulatory body that determines whether a food is ‘officially organic’ or not.\nCertified by the USDA… means?\nAccording to the USDA, “Certified organic foods are grown and processed according to federal guidelines addressing, among many factors, soil quality, animal raising practices, pest and weed control, and use of additives. Organic producers rely on natural substances and physical, mechanical, or biologically based farming methods to the fullest extent possible.\nProduce can be called organic if it’s certified to have grown on soil that had no prohibited substances applied for three years prior to harvest.\nWhat are prohibited substances?\nVirtually any synthetic fertilizers and pesticides. In instances when a grower has to use a synthetic substance to achieve a specific purpose, the substance must first be approved according to criteria that examine its effects on human health and the environment.\nHere is the official list, compiled by the The National Organic Standards Board (NOSB), made up of dedicated public volunteers appointed by the Secretary of Agriculture, board members include organic growers, handlers, retailers, environmentalists, scientists and consumer advocates.\nDangers of Synthetic Fertilizers…\nFertilizers are of two types: organic, or natural, and inorganic, or synthetic. Organic fertilizers are naturally occurring substances and include biofertilizers, green manure, organic manure and compost. They slowly leach essential nutrients into the soil and improve its overall vitality with time. Synthetic fertilizers are man-made combinations of chemicals and inorganic substances. They typically combine nitrogen, phosphorus, potassium, calcium, magnesium and other elements in different ratios. Synthetic fertilizers, unlike their organic counterparts, immediately supply essential nutrients to soil.\nSynthetic fertilizers have long-term negative effects such as beneficial microorganisms in the soil that convert dead human and plant remains into nutrient-rich organic matter, increasing groundwater toxicity, increasing the nitrate levels of soil.\n& Synthetic Pesticides…\nWell first, some of you may be wondering,”Do organic foods have pesticides?” The answer is yes, but these pesticides come from natural sources, such as certain types of plants, and they do not use synthetic pesticides. Organic farmers also tend to spray less pesticides on their produce than other farmers, and the pesticides are less dangerous for the environment.\nNot only are pesticides dangerous to the environment, but they are also hazardous to a person’s health. Pesticides are stored in your colon, where they slowly but surely poison the body. You may not realize this, but when you are eating a non-organic apple, you are also eating over 30 different pesticides that have been sprayed on the apple. Even if you wash a piece of fruit, such as an apple, there are still many pesticides lingering on it and they could have seeped into the fruit or vegetable.\nHow Do American Standards Differ From Other Countries?\nEuropean regulations against additives in food products are generally stricter than in the U.S. This difference is due mostly to the fact that Europe has chosen a precautionary approach in regulating, while the U.S. governing bodies tend to be more reactive. In other words, in the United States, food additives are innocent until proven guilty, while in Europe, only those additives proven not to be harmful are approved for use.\nAs a result, there many petrochemical-based food colorings and other artificial ingredients like brominated vegetable oil (BVO) and rBHG that are banned in Europe, but these and other additives are approved for use in the USA.\nHow about our organic standards compared to that of our trusty neighbor to the north?\n- Canada’s organic standard prohibits hydroponic and aeroponic production methods. USA allows both in organic.\n- Canada’s organic standard prohibits use of sodium nitrate fertilizer (a mined quick release fertilizer that kills soil organisms) (USA allows its use in organic).\n- Canada’s organic standard requires specific amounts of living space for each livestock type. USA does not have the same specific requirements.\n- USA’s organic standard prohibits livestock being treated with antibiotics (Canada allows antibiotic treatments to prevent suffering of animals under certain conditions.\nAre Community Gardens Organic Farming?\nOf course, just because you are not USDA certified, doesn’t mean that your products are not organic. There are countless locally-grown products that certainly would meet the organic standards but just have failed to go through the certification process.\nAt Small Axe Peppers, we like to say that the peppers we source in our local community gardens are ‘beyond organic’. Not only are they all grown in local gardens don’t use pesticides at all, neither synthetic nor organic.']	['<urn:uuid:9c9d8b8f-8035-4b0b-ae93-9255b26f4146>', '<urn:uuid:20e678d5-5f1f-4d03-bdb8-0b5e6fa87123>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T10:32:05.666290	8	76	2410
75	farm antibiotics resistance transmission coastal waters	Antibiotic resistance can spread from farms to humans through multiple pathways. Research has demonstrated that resistant bacteria can move from livestock to humans, with whole-genome sequencing studies showing clear evidence of bacterial transmission between farm animals and farmers. Additionally, coastal waters serve as an important transmission route, where resistant bacteria from agricultural runoff can affect water users. Studies have shown that surfers who swallow seawater are at greater risk of carrying antibiotic-resistant bacteria in their guts compared to non-water users, particularly after heavy rainfall when agricultural runoff enters coastal waters.	"['Gene Sequencing Pinpoints Antibiotic Resistance Moving From Livestock to Humans\n- 4:12 pm |\nThe antibiotic era was barely 20 years old when people started raising concerns about using the new “miracle drugs” in agriculture. Penicillin first entered use in 1943, streptomycin in 1944, tetracycline in 1948 — and by 1965, the United Kingdom’s Agricultural Research Council was hearing testimony that organisms common in food animals, especially Salmonella, were becoming resistant to the antibiotics being used on the animals while they were alive. By 1969, the UK government had compiled an official report outlining the danger, and by 1973, a task force of the US Food and Drug Administration had concurred, and concluded the only safe action was to withdraw approval to use antibiotics in animals. (At which, as we now know, they would never be successful.)\nThe policy difficulty regarding this long-recognized problem has never been the emergence of resistant bacteria on farms; no one seriously disputes that resistance emerges whenever antibiotics exert selective pressure on bacteria, killing the vulnerable and opening an ecological niche into which the surviving not-vulnerable can expand. The sticking point has been the difficulty of proving that those resistant bacteria depart from farms, cross to humans, and cause resistant illness in them. Stuart Levy demonstrated it in 1976, on an experimental farm plot he set up just to make the proof. Most of the rest of the research, though — and after decades, there are hundreds of pieces of research — has been observational and retrospective: Looking at the drugs administered to populations of animals (about which we have very little data), measuring the antibiotic-resistant illness that arises in the human population, and making increasingly sophisticated backward matches between the resistance factors that show up in humans and the drugs that are deployed primarily on farms.\nDemonstrating the bacterial traffic prospectively and experimentally, as Levy did, is challenging not just logistically but also ethically. It is difficult to imagine a study design that could trace specific animals, their meat, and their eaters in a large group of free-living humans; and unless you have volunteers, as Levy did, the study would push ethical boundaries as well. But having that lack of definition in the middle of the animal-to-human bacterial flow permits uncertainty — which proponents of continued ag antibiotic use exploit.\nA new study of Danish farmers and their livestock may have ended that uncertainty. It is still retrospective, but its observations — using whole-genome sequencing — are so fine-grained that their tracing of the bacterial traffic seems to me to be difficult to challenge.\nThe work, published in EMBO Molecular Medicine, is by Mark Holmes, senior lecturer in preventive veterinary medicine at the University of Cambridge, and collaborators from two other UK institutions and two in Denmark. It traces the connections between two farmers in Denmark, who each suffered a MRSA infection, and animals on their farms, which lie 28 miles apart.\nThere is very little MRSA, drug-resistant staph, in Denmark, so little that any occurrence is considered a notifiable disease — meaning that, when a case is diagnosed, public health authorities must be told. One farmer, a 53-year-old woman who kept two horses and two cows, was diagnosed with a MRSA blood infection and also had the organism in her nostrils. The other, a 69-year-old woman who had a flock of 10 sheep, had a wound that had become infected with MRSA. When their cases came to light — more on that later — they were recognized as a new MRSA strain that has been reported in cattle and so Danish researchers went out to check the animals on both farms. One cow, and three sheep, were carrying the new strain.\nHolmes, whose former PhD student Laura Garcia-Alvarez first identified the cattle strain, put the samples through a number of analyses. All seven (four animals, two from Farmer A, one from Farmer B) were identical on several different assays (MLS typing, ST130; spa typing, t843; fingerprinting, PFGE and MLVA) and had the same resistance pattern, susceptible to antibiotics that were not beta-lactams (penicillins and cephalosporins). Then he conducted whole-genome sequencing on them, and compared them to see how closely related they were. Across all seven there was a difference of 154 SNPs (single nucleotide polymorphisms — single-letter “copying errors” in the genetic code). Based on their relatedness, the samples made neat clusters that corresponded to the two farms: the first farmer and her cow, and the second farmer and her sheep.\nHere is what the researchers consider the critical part. The isolates from the farmer and her cow were functionally identical (5 SNPs), and so were the isolates from the other farmer and two of her three sheep (5 and 3 SNPs). So, first finding, the humans and their animals shared an infection. But it was the third sheep’s isolate that allowed them to conclude the infection had gone from animal to human: It was slightly different from both the woman’s infection (37 SNPs) and the other two sheeps’ (40 and 42 SNPs). That diversity indicates to them that the infection has been circulating in the flock for a while and then passed to the human via Sheep No. 1 or Sheep No. 2; if the traffic had gone the other way, from farmer to sheep, the relationship among the sheep’s isolates should have been tighter.\nNow, the caveats, which Holmes kindly pointed out to me in a phone call.\nFirst, the old criticism: This was not observed experimentally. Second, the sheep flock: It is possible that the diversity of the isolates between Sheep 1 and 2, and Sheep 3, represents a second introduction of MRSA into the flock, not one introduction followed by dissemination. If that happened, then the “distance” between the isolates would not represent the amount of time the infection was circulating among the sheep, and it would be easier to argue for a human-to-animal transmission being as likely as a zoonotic one.\nBut here is where we get to the identity of the strain, and to why these women were chosen for study at all. When Garcia-Alvarez identified the cattle-related strain in 2011, it was a perplexing finding: The isolates she was working with were demonstrably MRSA, but tested negative for the gene mecA, whose presence is a defining characteristic of MRSA. (It confers resistance to beta-lactam antibiotics, including methicillin, which supplies the M in MRSA.) This was troubling, because it indicated that there might be a class of MRSA that would not be detected by widely used diagnostic tests which look for that gene. Subsequently, it was determined that this strain possessed a variant gene, since dubbed mecC. Because of the group’s 2011 findings — that there might be another livestock-associated strain circulating that was different from our old friend, pig-to-human ST398 — the researchers involved doubled back to look for other evidence of mecC. This included a review of Denmark’s stored samples. The two women farmers were infected with the mecC strain.\nSo, that raises a further caveat. Because this is not standard MRSA, it would be possible to argue that these findings don’t actually illuminate the animal-to-human resistance-transmission debate. And because it is only two cases, it could be dismissed as a blip.\nBut if the analysis is correct, then it represents several kinds of potential trouble. First, it reinforces the argument for animal-to-human transmission of resistant bacteria. Second, it emphasizes that such bacteria can be picked up and transmitted even by animals that are not routinely receiving antibiotics; after all, this cow and these three sheep were living on small remote farms, not in a big American CAFO.\nAnd third, it raises the question of how much more resistant bacterial traffic is out there that we are not detecting. Holmes told me the “host range” — that is, the species detected to be carrying mecC MRSA, now mostly being called CC130 — so far include not just cows and sheep, but horses, rabbits, cats, dogs, deer, seals, rats and wild birds. That is a lot of friendly territory for resistant bacteria to occupy. And it kinda makes you wonder whether we shouldn’t think twice about making so many resistant bacteria available to those species in the first place.\nA footnote: On March 14, Rep. Louise Slaughter (D-NY) — Congress’s only microbiologist — reintroduced her bill PAMTA, the Preservation of Antibiotics for Medical Treatment Act, which seeks to reduce routine antibiotic use in agriculture. Yesterday, having seen this paper, she shot a letter off to FDA Commissioner Margaret Hamburg. Key quote: “The current strategy of voluntary guidance regarding ‘judicious use’ of antibiotics is doomed to fail, and it does not reflect the extreme urgency of this issue.”\n(And also don’t miss former FDA Commissioner David A. Kessler’s op-ed in the New York Times, denouncing the lack of data the FDA releases on agricultural antibiotic use — the issue explored in my posts on ADUFA. More on that soon.)\n- Harrison EM, Peterson GK, Holden MTG et al. Whole genome sequencing identifies zoonotic transmission of MRSA isolates with the novel mecA homologue mecC. EMBO Molecular Medicine published online 25 MAR 2013. DOI: 10.1002/emmm.201202413 (the paper discussed above)\n- Petersen A, Stegger M, Heltberg O et al. Epidemiology of methicillin-resistant Staphylococcus aureus carrying the novel mecC gene in Denmark corroborates a zoonotic reservoir with transmission to humans. Clinical Microbiology and Infection, published online 19 OCT 2012. DOI: 10.1111/1469-0691.12036 (the database review which identified the farmers)\n- Garcia-Alvarez L, Holden MTG, Lindsay H et al. Meticillin-resistant Staphylococcus aureus with a novel mecA homologue in human and bovine populations in the UK and Denmark: a descriptive study. The Lancet Infectious Diseases, Volume 11, Issue 8, Pages 595 – 603, August 2011. doi:10.1016/S1473-3099(11)70126-8 (the first identification of the mecC MRSA strain)', ""Hi Reddit, We are Dr Anne Leonard and Dr William Gaze from the European Centre for Environment and Human Health (http://www.ecehh.org/), based at the University of Exeter Medical School. We are here to answer your questions on antibiotic-resistance in coastal waters.\nBacteria that can survive in the presence of medicines (antibiotics) designed to kill them, are termed antibiotic-resistant bacteria, and are a growing threat to human wellbeing around the world. Infections caused by bacteria that survive treatment with antibiotics are difficult to cure, and can even kill people if effective antibiotics aren’t available (https://www.newscientist.com/article/2118046-woman-dies-from-infection-resistant-to-all-available-antibiotics/)\nUnderstanding the various ways people come into contact with resistant bacteria can help develop effective strategies to control the spread of resistance. We recently published a study (Beach Bums) on resistant bacteria in coastal waters and the potential for their spread to water users. Finding that surfers, who swallow a lot of seawater when they surf, are at a much greater risk of having antibiotic-resistant bacteria in their guts compared to people who don’t go in the sea indicates that coastal waters could be an important environment in which members of the community acquire resistant bacteria.\nWe are looking forward to reading your questions and comments about antibiotic-resistance in the environment.\nEDIT: hi! Thanks to everyone who got in touch to ask us thought-provoking questions about the issue of antibiotic-resistance in the environment. We’re going to sign out in a bit, but (time permitting) we will check back later to see if there are any more questions to answer.\nHow many years do you believe we have till antibiotics are useless in humans?\nWill Gaze - Hi William, some bacteria are already resistant to all known antibiotics and people are already dying from pan resistant infections. Thankfully this is still quite rare but is predicted to become more common. We don't know how long it will be until all antibiotics are useless but it could be a few decades if we cant find or make more new antibiotics. Hopefully we will never reach the point where all bacterial pathogens are pan resistant which is why a lot of research is focusing on drug discovery and the mechanisms by which bacteria acquire antibiotic resistance. Anne's work on environmental transmission in surfers is just one example of efforts to better understand this complex process.\nFirst, thanks for your awesome work!\nMy question is: Do you think surfers should change their behaviour in any way to mitigate the effect that you've shown? For instance, after a particularly heavy rainfall is the water likely to be worse with all the run-off, and should surfers avoid going in?\nThanks! We think that antibiotic-resistant bacteria are more common in seawater after heavy rainfall, as you rightly point out due to run-off, as well as untreated sewage being released from storm outfalls. Experts suggest that surfers and other water users avoid going in the sea about 2 days after heavy rainfall to reduce their exposure to antibiotic-resistant bacteria, as well as to other microorganisms carried in sewage that might make them unwell. Water users in England can look up water quality at their favourite beaches to see whether levels of bacteria are high, or if sewage pollution is likely to be a problem - this information is publically available via the Environment Agency bathing water quality website for over 400 beaches. The Safer Seas Service also provides live alerts on sewage spills for some beaches. Anne\nHi folks, is there a risk of this sort of exposure in other bodies of water - eg freshwater lakes and rivers?\nHi, we know that rivers are an important way by which microorganisms (including antibiotic-resistant bacteria) are transported from land to coastal waters. The type and abundance of resistant bacteria in the water will probably differ according to the sources of pollution upstream. Anne\nCan you please enlighten us regarding few most important measures that can be taken to prevent the spread of such resistant strains\nWill - in the context of water borne transmission there are several ways to prevent transmission. For risks associated with human sewage much depends on existing treatment infrastructure. For example 1 billion people in the world do not have access to a toilet so introducing sewage treatment of any kind would be beneficial. In countries where reasonably good treatment facilities exist additional tertiary treatment steps and UV and ozone treatment can be used but these are expensive. There may also be risks associated with antibiotic residues that humans excrete, alongside antimicrobial cleaning products used in the home, which may further increase resistance in the environment. For farm animal waste you can reduce antibiotic usage and faecal waste can be treated and/or prevented from entering streams and rivers.\nHow this is gonna affect agriculture?\nI think Anne is also answering this one but from my perspective ultimately we need to reduce reliance on antibiotics in farming particularly use as a growth promoter. Also antibiotic use enables intensive factory farming that is only possible because of preventative prophylactic usage of antibiotics, including those critical to treatment of disease in humans.\nHow this is gonna affect agriculture?\nHuman health, animal health and environmental health are all connected, and this is especially true for antibiotic resistance because some of the types of bacteria that live in the environment can also live in (and harm) humans and animals. Not only this, but bacteria can share the genes that confer resistance to antibiotics with other bacteria (a process called horizontal gene transfer). This means that antibiotic-resistant bacteria in humans or in the environment do not necessarily stay there: resistant bacteria in livestock have found their way into the clinic and vice versa. While livestock may not acquire resistant bacteria from the sea directly, they might drink from streams affected by sewage pollution and containing resistant bacteria. Antibiotic resistance undoubtedly affects agriculture because we are also concerned with treating and preventing infections in livestock and crops to maintain the health and productivity of agricultural systems.\nIf antibiotics cease being an effective way to kill bacteria, is there any other option or will preventing others from being exposed be the only recourse?\nWill - reducing transmission, so better infection control and hygiene, is always important but there are other possibilities. Vaccines can be developed so that infections by certain pathogens are reduced, this is already a focus of much effort. Viruses that infect bacteria (bacteriophage or phage) have long been used to treat infections such as wound infections in certain parts of the world and there is renewed interest in phage therapy. A further approach is to actually intervene at a molecular or genetic level. One possible approach is to use CRISPR systems (bacterial immune mechanisms currently being used in human gene editing) located on mobile genetic elements to actually get rid of antibiotic resistance genes. Because bacterial DNA can move between even unrelated bacteria by horizontal gene transfer other mechanisms to block this transfer process are also being investigated.\nI do both in silico design and vitro testing for new inhibitors for ESBL's that are mutated or we have no inhibitors for them. My PI said working on this is a waste of time since the products are not naturally found in nature and we have no hope of this drug going anywhere. I am still new to the academic research and drug development and was wondering if this is true for drug development? Assume this drug has good absorption , low toxicity and good selectivity. Thanks!\nThere are likely to be many natural products that remain undiscovered and resistance mechanisms in environmental bacteria that are also unknown. We know that the original ESBL genes or their progenitors have come from environmental bacteria so I'm not sure that it's a waste of time. Can you be more specific?\nDo you all see antibiotic resistance in vibrio in coastal waters becoming a significant threat to be both people and industry in the near future?\nThere is increasing concern about vibrios in coastal waters and although I personally don't know much about resistance in these organisms it is possible that environmental pollution could enable transfer of resistance genes or even select for resistance in polluted estuaries for example. One of my colleagues, Michiel Vos, has recently isolated an emerging vibrio pathogen species from coastal UK waters. We are currently looking at resistance determinants in this and other vibrio species.\nIt may not be part of your group's research, but have you seen/aware of any similar effects for other antimicrobials, especially antifungals? Perhaps not terribly relevant for coastal waters, but for other bodies of water, I'd imgaine there's quite a bit of dangerous fungi lying around?\nThe concern regarding fungal resistance lies with the huge amount of fungicides used in horticulture. The resistance mechansisms that these antifungals select for can be the same as those in human fungal pathogens.\nNot sure if this is so much your area but I was told (in a UEMS lecture, #bleedgreen) that a major reason behind the development of bacteria with such a wide variety of resistances is the differences between front line antibiotics in different countries, for example vancomycin is not commonly used as a first line treatment over here but is in Japan. Have there been any attempts at stopping this by international organisations (like the WHO maybe?).\nDifferent antibiotics are used in different amounts and for different purposes in different countries. For example colistin is used in animal production in China and India and resistance has emerged and spread to human pathogens in this way. There was supposed to be a division between antibiotics used in humans and animals but this has not really been implemented widely and sometimes a different compound may be used but still have the same mode of action so the resistance mechanisms are the same. There are coordinated efforts to reduce use of frontline human drugs in agriculture but this is hard to enforce. Now that AMR is on the international agenda hopefully regulation will continue to tighten, and we can already see this beginning to happen.\nWhat's the predominant resistance mechanism you see in coastal water pathogenic bacteria?\nAll different types. Anne has recently been looking at resistance in E. coli in coastal waters using genome sequencing methods and we see many different types of resistance genes in this single species.\nWhat part or system of the body is usually affected when antibiotic-resistance occur? What exactly happens to the person when there's an antibiotic resistance?\nIs this kind of bacteria present everywhere? What reason/s made this research focus on coastal waters? And why not on other parts of the environment?\nPS I really appreciate your time, effort and perseverance. Thank you for not giving up 😘\n- Infections caused by antibiotic-resistant bacteria can cause problems in any part of the body. They can affect the site of the original infection, and have the potential to spread if resistant bacteria get into the bloodstream. There are many factors at play that might affect how the infection presents and progresses (e.g. age, comorbidities, etc.), but the issue with infections caused by resistant bacteria is that they take longer to treat, are more expensive to cure, and can cause long-term damage (and even death) during their course.\n- Antibiotic-resistant bacteria have been found in many different environments (including in the home, hospitals, food, etc.), and it is really important that we study the various ways people might acquire resistant bacteria so that certain high-risk pathways can be identified for intervention. The reason we chose to focus on coastal waters for our recently published paper (the Beach Bum Survey) is that seawater is frequently contaminated by faecal waste from humans and animals, and faecal waste carries bacteria, including clinically important antibiotic-resistant bacteria. Also, millions of people visit the sea, go in the water and enjoy activities which involve high levels of exposure to seawater and the resistant bacteria in it.\nWill Gaze leads a research group which works on understanding the emergence and spread of antibiotic resistance in various natural environments: not just coastal waters.\n- t3_7uikfu_comments.json 82.3 KB\nThis article and its reviews are distributed under the terms of the Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, and redistribution in any medium, provided that the original author and source are credited.""]"	['<urn:uuid:6d0b9255-c108-4e3d-8847-169c3b53fe56>', '<urn:uuid:8c6dfe07-90f6-427e-afa8-a241022d7935>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T10:32:05.666290	6	90	3687
76	coastal maine archaeological site destruction	The slow rise of sea level is methodically destroying the coastal occupation sites of the Red Paint People, limiting our ability to learn about them. After 4,000-5,000 years, relatively few sites remain, with the Turner Farm site in Penobscot Bay being one of the few excavated locations.	"['By Brian Robinson\nThe so-called ""Red Paint People"" of nearby Maine provided one of New England\'s earliest archeological controversies. Evidence of the Red Paint People was found throughout the nineteenth century as groups of stone artifacts (for example, gouges or woodworking tools, fishing plummets, flaked stone knives and spearheads of ground slate) in deposits of bright red ocher (iron oxide powder). The first detailed excavations were conducted by Charles Willougby in 1892, for the Peabody Museum of Harvard University and his 1899 account remains a model of archeological reporting. Later work by Warren K. Moorehead (n.s. Peabody Museum, Andover, MA) from about 1912-1920 brought wider attention to the ""Red Paint People"" as well as controversy surrounding their identity.\nThe controversy surrounding the age and identity of the people originated with the character of red ocher deposits. Although they were generally considered to be graves, they were apparently so old that no bones were preserved and some of the stone artifacts were even badly decayed. This led Moorehead to declare in the pages of the preimmenent journal, American Anthropology:\n""It is our conviction that the graves represent an ancient and exceedingly primitive culture, totally diffferent from that of the later Algonquin tribes"" (Moorehead 1913).\nThe proposal of a ""new"" ancient culture could not go unchallenged and the challenge came in 1914 from none other than David T. Bushnell of the Smithsonian Institution, who suggested that the Red Paint People may indeed be quite recent. This controversy occured long before the radiocarbon dating (carbon 14) techniques had been invented, and with so many controversies, that of the Red Paint People faded away with new information. Moorehead was correct that the graves were quite ancient, usually dating between 2,000 to 6,000 years ago, but with rocks going as far back as 6,000 years. Bushnell\'s concern was not unfounded, however, as the Smithsonian was at the time waging a battle against other ""Satanic"" interpretations of Native American archeology, most notably the theory that the Mound Builders of Midwestern United States were an ancient race of people, perhaps one of the seven tribes of Isreal, that came before the Indian cultures of North America.\nArcheology has come a long way since the turn of the century and presumably has a long way to go, if reserach can keep pace with all the agents of destruction, both natural and man made. The ""Red Paint People"" were initially only recognized from their cemetery sites and these are now more commonly referred to as the ""Moorehead burial tradition"" in archeological journal. Recent research, conducted by the Maine State Museum and various campuses of the University of Maine has foucused on the occupation sites or living areas of the people and their maritime hunting and fishing culture. The regular hunting of swordfish in the Gulf of Maine provides an example of their hunting and boating skills.\nIronically, some of the old ""Mysteries of the Red Paint People"" persist but for different reasons. One of the early theories explaining their disappearance called upon the then recent (1930) evidence that the coast of Maine was subsiding below the ocean. This geological evidence was expanded upon with the suggestion that the ocean side habitation areas of the people may have succumbed to tidal waves that resulted from violent earthquakes that caused the land to sink. While such catastrophes did not likely wipe the people out, the slow rise of the sea level is methodically destroying the coastal occupation sites and thus our ability to learn about them. Relatively few such sites remain after 4,000-5,000 years, and the few that have been excavated, such as the Turner Farm site in Penobscot Bay (Bruce Bourque 1995), contain precious glimpses of what was once a typical lifestyle of the area.\nPhoto courtesy NH Div of Historical Resources.\n©1997 SeacoastNH.com. All rights reserved.\n[ New | Site Map | Talk | Store | Sponsors ]\n[ Themes | Experts | Historic Sites | Historic Houses | Historical Societies ]\n[ Prehistoric Era | Contact Era | Colonial Era | Revolution Era ]\n[ Guide To Artifacts | Indian Exhibits | SCRAP ]\n[ NH Arch. Society | Red Paint People | Poem ]']"	['<urn:uuid:3c763a1c-41bf-48a1-ae1d-f52a30f8eca1>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	5	47	697
77	major risks maritime port infrastructure today	The criminal and terrorist threat to ports and the maritime supply chain remains a global constant. Corruption poses serious risks to security plan integrity. Piracy interferes with the free flow and confidence of maritime trade. Additionally, poverty and greed can drive company and facility insiders to collaborate with criminal elements, leading to security breakdowns. This is evidenced by routine reports of container break-ins and theft of whole containers.	['Indonesia Port Security: International Standards for Success\nby Scott M. Bernat, a civilian Special Agent of the US Naval Criminal Investigative Service (NCIS), currently assigned to the US Embassy Jakarta, Indonesia.\nIndonesia, rich in natural and man-made resources, is dependent on its port and maritime industry to advance its economic interests and attract business development. The safety and security of Indonesia’s ports and maritime infrastructure remains paramount to achieving success as a leader in world trade. Destination countries need to be assured that maritime assets and associated cargo from Indonesia have been subjected to the highest security standards possible. Indonesia expects no less a commitment to security from countries involved in exporting goods and products there. The International Maritime Organization (IMO) headquartered in London, England sets the standard for improving and maintaining the safety and security of international shipping and associated facilities. Worldwide adherence to IMO standards ensures mutual safety and security standards are met, increases confidence in commer-cial trade and promotes business opportunity and growth.\nThe criminal and terrorist threat to ports and the maritime supply chain remains a global constant and is routinely highlighted in the media. Corruption serves to fuel the fire and raises serious questions regarding the overall integrity of even the best security plan. Piracy, in its current form is arguably nothing more than criminal activity, spurring the public imagination and ultimately interfering with the free flow and confidence of maritime trade. In many locations throughout the world, poverty and/or greed drives some company and facility insiders to link with outside criminal elements to further breakdown security efforts. Routine reports of container break-ins, as well as the theft of whole containers, continue to highlight the justification behind the adoption, implementation and maintenance of internation-ally recognized security standards. A cost effective security program begins with a clear understanding of the threat faced and the application of tailored and focused security practices and procedures.\nIMO / ISPS Code\nFollowing the 9/11 terrorist attacks in the United States, the IMO promulgated a set of standards and procedures to address and promote the global need for port and maritime security. These standards, known as the International Ship and Port Facility Security (ISPS) Code, were specifically designed, based on existing local threat conditions and environment, to enhance security practices and procedures for and between ships and port facilities involved in international trade. These include the routine conduct of ship and port security exercises and the partnering of private and government entities to ensure the sharing of threat information and security best practices. The ISPS Code utilizes sound risk management concepts to effectively address and mitigate potential threats to ships and ports, thereby protecting against potential criminal and terrorist threats. A ship or port facility adhering to the ISPS Code highlights a commitment to security excellence and is often awarded preferred treatment by the international community. Preferential action includes decreased port entry/exit delays for both ships and cargo.\nAn integral part of maritime trade confidence is the establishment, application and management of effective supply chain security procedures. In 2007, the International Organization for Standardization (ISO) published a series of supply chain security management standards. These standards, known collectively as ISO 28000, establish security best practices and procedures to protect against potential threats to the safety and security of the supply transportation and logistics system. Ships and port facilities are essential components of the international supply chain and their successful operations remain dependent on sound supply chain practices.\nResources are available to assist countries, companies and facilities with a vested interest and/or involved in interna-tional maritime trade, to develop, establish and maintain an effective security program. These include both private companies involved in fee-based ISPS Code and ISO 28000 standards training, as well as governmental organizations that provide similar services at no cost, dependent on eligibility. The US Coast Guard (USCG) International Port Security Program focuses on worldwide maritime trading nations and the implementation of the ISPS Code. The USCG International Training Division offers port safety and security training, often partnering with the Asia Pacific Economic Cooperation (APEC) and Organization of Ameri-can States (OAS) to provide port security subject matter experts familiar with the various port operating environ-ments. The US Naval Criminal Investigative Service (NCIS), through its Security Training, Assistance and Assessment Teams (STAAT), also provides port and facility security training, primarily focusing on locations and facilities frequented by and/or contracted with the US Navy.\nIndonesia’s maritime trade and economic growth depend heavily on its ability to secure and protect the supply chain and associated ship and port facilities. The fast-paced maritime commercial shipping industry, focused on the import and export of in-demand commodities, requires high confidence in the safety and security of its operations. The uninterrupted ability of a ship to deliver and a port to clear and account for discharged cargo is critical to this success and remains reliant on the successful implementation of ISPS Code and ISO 28000 standards.\nIMO / ISPS Code Information | Website: http://www.imo.org\nISO 28000 Information | Website: http://www.iso.org\nUSCG International Port Security Program: The Maritime Security Help Desk | Website: http://homeport.uscg.mil | Navigate to the IPS Program Tab\nUS Naval Criminal Investigative Service (NCIS) | Website: http://www.ncis.navy.mil\nBritish Ship photo - AMCHAM at: www.belgian-navy.be\nScott M. Bernat is a civilian Special Agent of the US Naval Criminal Investigative Service (NCIS), currently assigned to the US Embassy Jakarta, Indonesia Force Protection Detachment as the Resident Agent in Charge and Chief of US Military Security. During his 22 year career, he has worked as a security professional throughout Asia, Australia/Oceania, Central America, Europe, Middle East and the United States. He is a recognized expert in Maritime and Port Security.']	['<urn:uuid:f9464066-b364-41a4-9c38-be6e4f59477b>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	6	68	943
78	How does the brain maintain recognition of smells over time despite neural changes, and how does this relate to broader neurotransmission processes at synapses?	The brain maintains smell recognition through pattern stability despite neural turnover. While olfactory sensory neurons only live about 60 days, not all die simultaneously, and there are thousands responding to each scent. When new neurons expressing the same receptor gene mature, they connect to the same olfactory bulb neurons as their predecessors, maintaining stable patterns. Individual neurons respond to molecular features rather than specific smells, and the brain interprets the combined pattern as a particular scent. This relates to broader neurotransmission where neurons communicate through synapses using complex patterns. Neurotransmitters are released from axon terminals to receptors on dendrites of other neurons, with multiple inputs being integrated. The overall pattern of neural activity remains stable through regulated neurotransmitter release, receptor availability, and careful positioning of new neural connections.	"[""How do we remember smells for so long if olfactory sensory neurons only survive for about 60 days? —A. A. Bozorgi, Irvine, Calif.\nDonald A. Wilson, a zoology professor at the University of Oklahoma and co-author of Learning to Smell (Johns Hopkins University Press, 2006), replies:\nWe recognize an old scent, despite having replaced at least a subset of the olfactory sensory neurons that first interacted with that odor, because the overall pattern of activity within the olfactory system remains relatively constant over time.\nOlfactory sensory neurons, which sit in the mucus in the back of the nose and relay data to the brain via axons (fingerlike projections that transmit information out from the cell body), are one of an increasingly large number of neuron types that are known to die and be replaced throughout life. Fortunately, they do not all die at the same time, and there are many thousands of olfactory sensory neurons that respond to any given scent.\nThe 2004 Nobel Prize in Physiology or Medicine went to Linda B. Buck and Richard Axel for their 1991 research showing that there is a huge family of genes that encode proteins called olfactory receptors. One of their important observations was that individual olfactory sensory neurons typically express just one of those genes. That is, signals from a given neuron provide information about odors that activate the specific receptor protein expressed by that cell. In fact, when an olfactory sensory neuron expressing a particular receptor gene dies and a new neuron expressing that same gene matures, the new neuron's axons plug into the same group of olfactory bulb neurons that its predecessor did. This phenomenon results in remarkable pattern stability over years, despite continual rewiring.\nA single receptor protein, however, appears to bind (or recognize) many different odors. Thus, rather than having neurons that respond selectively to coffee or vanilla or Bordeaux, most individual cells respond (via their receptors) to submolecular features of the volatile chemicals coming from those objects. For example, an olfactory sensory receptor neuron may respond to a hydrocarbon chain of a particular length or to a specific functional group such as an alcohol or aldehyde.\nTherefore, any given sensory neuron will respond to many different odors as long as they share a common feature. The brain (specifically, the olfactory bulb and the olfactory cortex) then looks at the combination of sensory neurons activated at any given time and interprets that pattern; the brain's interpretation is what you perceive as smell. With so many inputs contributing to the formation of a scent pattern, the absence of a small number of constituents does not appreciably change the pattern or the brain's perception.\nWhy do migratory birds fly in a V formation? —J. F. Bowman, Corte Madera, Calif.\nBruce Batt, who retired in 2007 as chief biologist for Ducks Unlimited, a wetlands conservation group based in Memphis, Tenn., offers this explanation:\nThere are two well-supported and complementary explanations for why birds fly in formation. (Both V and J structures are typical and readily recognizable flight formations for migratory birds; studies have shown that a J formation is, in fact, more common than a true V-shaped structure.) One way to account for the phenomenon is that followers benefit from a supportive upwash of air created by the lead birds. The other is that regimented flight formation facilitates proper spacing, directional orientation and group communication.\nThe relative importance of each benefit undoubtedly shifts along with changes in various factors, such as the season of the year or the purpose of an individual flight. During local feeding flights, for example, energy conservation is probably much less important than careful orientation and collision avoidance are. During long-distance migration, on the other hand, each member of the flock gains a great deal by optimizing its position to conserve energy."", 'Neurotransmission (Latin: transmissio ""passage, crossing"" from transmittere ""send, let through"") is the process by which signaling molecules called neurotransmitters are released by the axon terminal of a neuron (the presynaptic neuron), and bind to and react with the receptors on the dendrites of another neuron (the postsynaptic neuron) a short distance away. A similar process occurs in retrograde neurotransmission, where the dendrites of the postsynaptic neuron release retrograde neurotransmitters (e.g., endocannabinoids; synthesized in response to a rise in intracellular calcium levels) that signal through receptors that are located on the axon terminal of the presynaptic neuron, mainly at GABAergic and glutamatergic synapses.\nNeurotransmission is regulated by several different factors: the availability and rate-of-synthesis of the neurotransmitter, the release of that neurotransmitter, the baseline activity of the postsynaptic cell, the number of available postsynaptic receptors for the neurotransmitter to bind to, and the subsequent removal or deactivation of the neurotransmitter by enzymes or presynaptic reuptake.\nIn response to a threshold action potential or graded electrical potential, a neurotransmitter is released at the presynaptic terminal. The released neurotransmitter may then move across the synapse to be detected by and bind with receptors in the postsynaptic neuron. Binding of neurotransmitters may influence the postsynaptic neuron in either an inhibitory or excitatory way. The binding of neurotransmitters to receptors in the postsynaptic neuron can trigger either short term changes, such as changes in the membrane potential called postsynaptic potentials, or longer term changes by the activation of signaling cascades.\nNeurons form complex biological neural networks through which nerve impulses (action potentials) travel. Neurons do not touch each other (except in the case of an electrical synapse through a gap junction); instead, neurons interact at close contact points called synapses. A neuron transports its information by way of an action potential. When the nerve impulse arrives at the synapse, it may cause the release of neurotransmitters, which influence another (postsynaptic) neuron. The postsynaptic neuron may receive inputs from many additional neurons, both excitatory and inhibitory. The excitatory and inhibitory influences are summed, and if the net effect is inhibitory, the neuron will be less likely to ""fire"" (i.e., generate an action potential), and if the net effect is excitatory, the neuron will be more likely to fire. How likely a neuron is to fire depends on how far its membrane potential is from the threshold potential, the voltage at which an action potential is triggered because enough voltage-dependent sodium channels are activated so that the net inward sodium current exceeds all outward currents. Excitatory inputs bring a neuron closer to threshold, while inhibitory inputs bring the neuron farther from threshold. An action potential is an ""all-or-none"" event; neurons whose membranes have not reached threshold will not fire, while those that do must fire. Once the action potential is initiated (traditionally at the axon hillock), it will propagate along the axon, leading to release of neurotransmitters at the synaptic bouton to pass along information to yet another adjacent neuron.\nStages in neurotransmission at the synapse\n- Synthesis of the neurotransmitter. This can take place in the cell body, in the axon, or in the axon terminal.\n- Storage of the neurotransmitter in storage granules or vesicles in the axon terminal.\n- Calcium enters the axon terminal during an action potential, causing release of the neurotransmitter into the synaptic cleft.\n- After its release, the transmitter binds to and activates a receptor in the postsynaptic membrane.\n- Deactivation of the neurotransmitter. The neurotransmitter is either destroyed enzymatically, or taken back into the terminal from which it came, where it can be reused, or degraded and removed.\nNeurotransmitters are spontaneously packed in vesicles and released in individual quanta-packets independently of presynaptic action potentials. This slow release is detectable and produces micro-inhibitory or micro-excitatory effects on the postsynaptic neuron. An action potential briefly amplifies this process. Neurotransmitter containing vesicles cluster around active sites, and after they have been released may be recycled by one of three proposed mechanisms. The first proposed mechanism involves partial opening and then re-closing of the vesicle. The second two involve the full fusion of the vesicle with the membrane, followed by recycling, or recycling into the endosome. Vesicular fusion is driven largely by the concentration of calcium in micro domains located near calcium channels, allowing for only microseconds of neurotransmitter release, while returning to normal calcium concentration takes a couple of hundred of microseconds. The vesicle exocytosis is thought to be driven by a protein complex called SNARE, that is the target for botulinum toxins. Once released, a neurotransmitter enters the synapse and encounters receptors. Neurotransmitters receptors can either be ionotropic or g protein coupled. Ionotropic receptors allow for ions to pass through when agonized by a ligand. The main model involves a receptor composed of multiple subunits that allow for coordination of ion preference. G protein coupled receptors, also called metabotropic receptors, when bound to by a ligand undergo conformational changes yielding in intracellular response. Termination of neurotransmitter activity is usually done by a transporter, however enzymatic deactivation is also plausible.\nEach neuron connects with numerous other neurons, receiving numerous impulses from them. Summation is the adding together of these impulses at the axon hillock. If the neuron only gets excitatory impulses, it will generate an action potential. If instead the neuron gets as many inhibitory as excitatory impulses, the inhibition cancels out the excitation and the nerve impulse will stop there. Action potential generation is proportionate to the probability and pattern of neurotransmitter release, and to postsynaptic receptor sensitization.\nSpatial summation means that the effects of impulses received at different places on the neuron add up, so that the neuron may fire when such impulses are received simultaneously, even if each impulse on its own would not be sufficient to cause firing.\nTemporal summation means that the effects of impulses received at the same place can add up if the impulses are received in close temporal succession. Thus the neuron may fire when multiple impulses are received, even if each impulse on its own would not be sufficient to cause firing.\nConvergence and divergence\nNeurotransmission implies both a convergence and a divergence of information. First one neuron is influenced by many others, resulting in a convergence of input. When the neuron fires, the signal is sent to many other neurons, resulting in a divergence of output. Many other neurons are influenced by this neuron.\nCotransmission is the release of several types of neurotransmitters from a single nerve terminal.\nAt the nerve terminal, neurotransmitters are present within 35–50 nm membrane-encased vesicles called synaptic vesicles. To release neurotransmitters, the synaptic vesicles transiently dock and fuse at the base of specialized 10–15 nm cup-shaped lipoprotein structures at the presynaptic membrane called porosomes. The neuronal porosome proteome has been solved, providing the molecular architecture and the complete composition of the machinery.\nRecent studies in a myriad of systems have shown that most, if not all, neurons release several different chemical messengers. Cotransmission allows for more complex effects at postsynaptic receptors, and thus allows for more complex communication to occur between neurons.\nSome neurons can release at least two neurotransmitters at the same time, the other being a cotransmitter, in order to provide the stabilizing negative feedback required for meaningful encoding, in the absence of inhibitory interneurons. Examples include:\n- GABA–glycine co-release.\n- Dopamine–glutamate co-release.\n- Acetylcholine (Ach)–glutamate co-release.\n- ACh–vasoactive intestinal peptide (VIP) co-release.\n- ACh–calcitonin gene-related peptide (CGRP) co-release.\n- Glutamate–dynorphin co-release (in hippocampus).\nNoradrenaline and ATP are sympathetic co-transmitters. It is found that the endocannabinoid anadamide and the cannabinoid WIN 55,212-2 can modify the overall response to sympathetic nerve stimulation, and indicate that prejunctional CB1 receptors mediate the sympatho-inhibitory action. Thus cannabinoids can inhibit both the noradrenergic and purinergic components of sympathetic neurotransmission.\nOne unusual pair of co-transmitters is GABA and glutamate which are released from the same axon terminals of neurons originating from the ventral tegmental area (VTA), internal globus pallidus, and supramammillary nucleus. The former two project to the habenula whereas the projections from the supramammillary nucleus are known to target the dentate gyrus of the hippocampus.\nNeurotransmission is genetically associated with other characteristics or features. For example, enrichment analyses of different signaling pathways led to the discovery of a genetic association with intracranial volume.\n- Biological neuron model § Synaptic transmission\n- G protein-coupled receptor\n- Molecular neuropharmacology\n- Neuromuscular transmission\n- Melis M, Pistis M (December 2007). ""Endocannabinoid signaling in midbrain dopamine neurons: more than physiology?"". Current Neuropharmacology. 5 (4): 268–77. doi:10.2174/157015907782793612. PMC 2644494. PMID 19305743.\nThus, it is conceivable that low levels of CB1 receptors are located on glutamatergic and GABAergic terminals impinging on DA neurons [127, 214], where they can fine-tune the release of inhibitory and excitatory neurotransmitter and regulate DA neuron firing.\nConsistently, in vitro electrophysiological experiments from independent laboratories have provided evidence of CB1 receptor localization on glutamatergic and GABAergic axon terminals in the VTA and SNc.\n- Flores A, Maldonado R, Berrendero F (December 2013). ""Cannabinoid-hypocretin cross-talk in the central nervous system: what we know so far"". Frontiers in Neuroscience. 7: 256. doi:10.3389/fnins.2013.00256. PMC 3868890. PMID 24391536.\nDirect CB1-HcrtR1 interaction was first proposed in 2003 (Hilairet et al., 2003). Indeed, a 100-fold increase in the potency of hypocretin-1 to activate the ERK signaling was observed when CB1 and HcrtR1 were co-expressed ... In this study, a higher potency of hypocretin-1 to regulate CB1-HcrtR1 heteromer compared with the HcrtR1-HcrtR1 homomer was reported (Ward et al., 2011b). These data provide unambiguous identification of CB1-HcrtR1 heteromerization, which has a substantial functional impact. ... The existence of a cross-talk between the hypocretinergic and endocannabinoid systems is strongly supported by their partially overlapping anatomical distribution and common role in several physiological and pathological processes. However, little is known about the mechanisms underlying this interaction. ... Acting as a retrograde messenger, endocannabinoids modulate the glutamatergic excitatory and GABAergic inhibitory synaptic inputs into the dopaminergic neurons of the VTA and the glutamate transmission in the NAc. Thus, the activation of CB1 receptors present on axon terminals of GABAergic neurons in the VTA inhibits GABA transmission, removing this inhibitory input on dopaminergic neurons (Riegel and Lupica, 2004). Glutamate synaptic transmission in the VTA and NAc, mainly from neurons of the PFC, is similarly modulated by the activation of CB1 receptors (Melis et al., 2004).\n• Figure 1: Schematic of brain CB1 expression and orexinergic neurons expressing OX1 (HcrtR1) or OX2 (HcrtR2)\n• Figure 2: Synaptic signaling mechanisms in cannabinoid and orexin systems\n• Figure 3: Schematic of brain pathways involved in food intake\n- Freund TF, Katona I, Piomelli D (July 2003). ""Role of endogenous cannabinoids in synaptic signaling"". Physiological Reviews. 83 (3): 1017–66. doi:10.1152/physrev.00004.2003. PMID 12843414.\n- Ayakannu, Thangesweran; Taylor, Anthony H.; Marczylo, Timothy H.; Willets, Jonathon M.; Konje, Justin C. (2013). ""The Endocannabinoid System and Sex Steroid Hormone-Dependent Cancers"". International Journal of Endocrinology. 2013: 259676. doi:10.1155/2013/259676. ISSN 1687-8337. PMC 3863507. PMID 24369462.\n- Nagatsu, T. (December 2000). ""[Molecular mechanisms of neurotransmission]"". Rinsho Shinkeigaku = Clinical Neurology. 40 (12): 1185–1188. ISSN 0009-918X. PMID 11464453.\n- Andreae, Laura C.; Burrone, Juan (March 2018). ""The role of spontaneous neurotransmission in synapse and circuit development"". Journal of Neuroscience Research. 96 (3): 354–359. doi:10.1002/jnr.24154. ISSN 0360-4012. PMC 5813191. PMID 29034487.\n- Holden A, Winlow W (1984). The Neurobiology of Pain: Symposium of the Northern Neurobiology Group Held at Leeds on 18 April 1983 (1st ed.). Manchester Univ Pr. p. 111. ISBN 978-0719010613.\n- Kolb B, Whishaw IQ (2003). Fundamentals of Human Neuropsychology (5th ed.). Worth. pp. 102–104. ISBN 978-0-7167-5300-1. (reference for all five stages)\n- Squire L, Berg D, Bloom FE, du Lac S, Ghosh A, Spitzer NC (2013). Fundamental neuroscience (4th ed.). Amsterdam: Elsevier/Academic Press. pp. 133–181. ISBN 978-0-12-385870-2.\n- Williams SM, McNamara JO, Lamantia A, Katz LC, Fitzpatrick D, Augustine GJ, Purves D (2001). Purves D, Augustine GJ, Fitzpatrick D, et al. (eds.). Summation of Synaptic Potentials. Neuroscience (2nd ed.). Sunderland (MA): Sinauer Associates.\n- Wang JH, Wei J, Chen X, Yu J, Chen N, Shi J (September 2008). ""Gain and fidelity of transmission patterns at cortical excitatory unitary synapses improve spike encoding"". Journal of Cell Science. 121 (Pt 17): 2951–60. doi:10.1242/jcs.025684. PMID 18697836.\n- Yu J, Qian H, Chen N, Wang JH (2011). ""Quantal glutamate release is essential for reliable neuronal encodings in cerebral networks"". PLOS ONE. 6 (9): e25219. Bibcode:2011PLoSO...625219Y. doi:10.1371/journal.pone.0025219. PMC 3176814. PMID 21949885.\n- Yu J, Qian H, Wang JH (August 2012). ""Upregulation of transmitter release probability improves a conversion of synaptic analogue signals into neuronal digital spikes"". Molecular Brain. 5 (26): 26. doi:10.1186/1756-6606-5-26. PMC 3497613. PMID 22852823.\n- Hevern VW. ""PSY 340 Brain and Behavior"". Archived from the original on February 19, 2006.\n- Anderson LL (2006). ""Discovery of the \'porosome\'; the universal secretory machinery in cells"". Journal of Cellular and Molecular Medicine. 10 (1): 126–31. doi:10.1111/j.1582-4934.2006.tb00294.x. PMC 3933105. PMID 16563225.\n- Lee JS, Jeremic A, Shin L, Cho WJ, Chen X, Jena BP (July 2012). ""Neuronal porosome proteome: Molecular dynamics and architecture"". Journal of Proteomics. 75 (13): 3952–62. doi:10.1016/j.jprot.2012.05.017. PMC 4580231. PMID 22659300.\n- Trudeau LE, Gutiérrez R (June 2007). ""On cotransmission & neurotransmitter phenotype plasticity"". Molecular Interventions. 7 (3): 138–46. doi:10.1124/mi.7.3.5. PMID 17609520.\n- Thomas EA, Bornstein JC (2003). ""Inhibitory cotransmission or after-hyperpolarizing potentials can regulate firing in recurrent networks with excitatory metabotropic transmission"". Neuroscience. 120 (2): 333–51. doi:10.1016/S0306-4522(03)00039-3. PMID 12890506. S2CID 26851745.\n- Pakdeechote P, Dunn WR, Ralevic V (November 2007). ""Cannabinoids inhibit noradrenergic and purinergic sympathetic cotransmission in the rat isolated mesenteric arterial bed"". British Journal of Pharmacology. 152 (5): 725–33. doi:10.1038/sj.bjp.0707397. PMC 2190027. PMID 17641668.\n- Dh, Root; S, Zhang; Dj, Barker; J, Miranda-Barrientos; B, Liu; Hl, Wang; M, Morales (2018-06-19). ""Selective Brain Distribution and Distinctive Synaptic Architecture of Dual Glutamatergic-GABAergic Neurons"". Cell Reports. 23 (12): 3465–3479. doi:10.1016/j.celrep.2018.05.063. PMC 7534802. PMID 29924991.\n- Adams HH, Hibar DP, Chouraki V, Stein JL, Nyquist PA, Rentería ME, et al. (December 2016). ""Novel genetic loci underlying human intracranial volume identified through genome-wide association"". Nature Neuroscience. 19 (12): 1569–1582. doi:10.1038/nn.4398. PMC 5227112. PMID 27694991.']"	['<urn:uuid:466827e1-56da-494e-8129-6f989b1cb4fd>', '<urn:uuid:e4105a28-95d6-4775-8a48-7e81b51e3ff3>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T10:32:05.666290	24	128	2962
79	what happens when reading entries in young tableaux from right left top down	When reading entries in a semi-standard Young tableaux from right to left, starting with the top row and working down, you obtain what is called the reverse row word. For example, for a particular tableau shown in the document, this process yields its reverse row word.	"['Schur functions in algebraic combinatorics\nThe Schur functions are a special basis for the algebra of symmetric functions . They are also intimately connected with representations of the symmetric and general linear groups (cf. also Representation of the symmetric groups). Standard references are [a3], [a6], [a8], [a9].\nLet be a set of variables and let be the algebra of symmetric functions in . Bases for this algebra are indexed by partitions , i.e., is a weakly decreasing sequence of non-negative integers called parts. Associated with any partition is an alternant, which is the determinant\nwhere addition of partitions is component-wise. It is clear from this equation that is a symmetric homogeneous polynomial of degree .\nThere is a more combinatorial definition of a Schur function. A partition can be viewed as a Ferrers shape, obtained by placing dots or cells in left-justified rows with boxes in row . One obtains a semi-standard Young tableaux, , of shape by replacing each dot by a positive integer so that rows weakly increase and columns strictly increase (cf. also Young tableau). For example, if , then its shape and a possible tableau are\nEach tableau determines a monomial , e.g., in the example above, . The second definition of the Schur function is then\nwhere the sum is over all semi-standard Young tableaux of shape with entries between and .\nChange of basis.\nThe Schur functions can also be written in terms of the other standard bases for . A monomial symmetric function is the sum of all monomials whose exponent sequence is some permutation of . Also, define the Kostka number [a5] as the number of semi-standard Young tableaux of shape and content , i.e., contains entries equal to for . The combinatorial definition of immediately gives the following rule, known as Young\'s rule:\nNow consider the complete homogeneous symmetric functions and the elementary symmetric functions , where (respectively, ) is the sum of all (respectively, all square-free) monomials of degree . Also, let denote the partition conjugate to , whose parts are the column lengths of \'s shape. In the preceding example, . For the two bases under consideration, the function can be described as a determinant (the Jacobi–Trudi identity [a2], [a12] and its dual):\nNote that this identity immediately implies\nwhere is the partition with parts all equal to . These specializations also follow directly from the combinatorial definition of .\nThe description of in terms of the power sum symmetric functions brings in the representation theory of the symmetric group . The irreducible representations of are indexed by partitions such that . Given a conjugacy class of corresponding to a partition , let denote its size and let be the value of the th irreducible character on the class. Now consider the power sum symmetric function , where .\nThe following now holds: If , then\nIn other words, is the cycle-indicator generating function (in the sense of Polyá–Redfield enumeration) for the irreducible character of corresponding to .\nNow, consider the complex general linear group . A representation is polynomial if, for every , the entries of are polynomials in the entries of . The polynomial representations of are indexed by the partitions with non-negative parts. Let be the character of a polynomial representation and let have eigenvalues . Then is a polynomial function of the (because this is true for diagonalizable and these are dense in ) and is symmetric (because is a class function). In fact, more is true: The irreducible polynomial characters of are precisely the for with non-negative parts.\nThe connection with representations of can be used to construct an isomorphism of algebras. Let denote the vector space of all class functions on and let . The irreducible characters form a basis for , and it can be endowed with a multiplication by induction of the tensor product. The characteristic or Frobenius mapping [a1] is defined on by\nwhere is the value of on the class corresponding to . The mapping is an isomorphism of algebras. In fact, there are natural inner products on and that make an isometry.\nA number of identities involving Schur functions have interesting bijective proofs using the combinatorial definition. Among the most famous are the following, in which it is assumed that is another set of variables.\nThe Cauchy identity and its dual are\nD. Knuth [a4] has given algorithmic bijections between matrices and semi-standard Young tableaux that prove these identities. It is a generalization of a mapping of C. Schensted [a10] for standard Young tableaux, i.e., semi- standard Young tableaux where the entries are precisely .\nOne can also describe the structure constants for the algebra in the basis combinatorially. If as Ferrers shapes, then one has a skew shape consisting of all dots or cells that are in but not in . Skew semi-standard Young tableaux are defined in the obvious way. The reverse row word for a semi-standard Young tableaux , , is obtained by reading the entries in each row from right to left, starting with the top row and working down. For the example tableau, . Also, a sequence of positive integers is a lattice permutation or ballot sequence if, in every prefix , the number of \'s is at least as big as the number of \'s for all . The Littlewood–Richardson rule [a7] states that if\nthen is equal to the number of semi-standard Young tableaux of shape and content such that is a ballot sequence. Via the characteristic mapping, the Littlewood–Richardson coefficients can also be viewed as giving the multiplicities of the character product when decomposed into irreducibles. Equivalently, one can consider the decomposition of the inner tensor product of two irreducible polynomial representations of .\nThere are many generalizations of Schur functions, one of the most notable being the Hall–Littlewood functions. See [a8] for more information.\n|[a1]||F.G. Frobenius, ""Über die Charactere der symmetrischen Gruppe"" Sitz. K. Preuss. Akad. Wiss (1900) pp. 516–534 (Also: Gesammelte Abh. 3 Springer, 1968, 148-166)|\n|[a2]||C. Jacobi, ""De functionibus alternantibus earumque divisione per productum e differentiis elementorum conflatum"" J. Reine Angew. Math. , 22 (1841) pp. 360–371 (Also: Math. Werke 3, Chelsea, 1969, 439-452)|\n|[a3]||G.D. James, A. Kerber, ""The representation theory of the symmetric group"" , Encycl. Math. Appl. , 16 , Addison-Wesley (1981)|\n|[a4]||D.E. Knuth, ""Permutations, matrices and generalized Young tableaux"" Pacific J. Math. , 34 (1970) pp. 709–727|\n|[a5]||C. Kostka, ""Über den Zusammenhang zwischen einigen Formen von symmetrischen Funktionen"" Crelle\'s J. , 93 (1882) pp. 89–123|\n|[a6]||D.E. Littlewood, ""The theory of group characters"" , Oxford Univ. Press (1950)|\n|[a7]||D.E. Littlewood, A.R. Richardson, ""Group characters and algebra"" Philos. Trans. R. Soc. London Ser. A , 233 (1934) pp. 99–142|\n|[a8]||I.G. Macdonald, ""Symmetric functions and Hall polynomials"" , Oxford Univ. Press (1995) (Edition: Second)|\n|[a9]||B.E. Sagan, ""The symmetric group: representations, combinatorial algorithms, and symmetric functions"" , Wadsworth&Brooks/Cole (1991) (Second ed.: Springer, to appear)|\n|[a10]||C. Schensted, ""Longest increasing and decreasing subsequences"" Canad. J. Math. , 13 (1961) pp. 179–191|\n|[a11]||I. Schur, ""Über eine Klasse von Matrizen die sich einer gegeben Matrix zuordnen lassen"" Inaugural Diss. Berlin (1901)|\n|[a12]||N. Trudi, ""Intorno un determinante piu generale di quello che suol dirsi determinante delle radici di una equazione, ed alle funzioni simmetriche complete di queste radici"" Rend. Accad. Sci. Fis. Mat. Napoli , 3 (1864) pp. 121–134 (Also: Giornale di Mat. 2 (1864), 152–158; 180–186)|\nSchur functions in algebraic combinatorics. Encyclopedia of Mathematics. URL: http://www.encyclopediaofmath.org/index.php?title=Schur_functions_in_algebraic_combinatorics&oldid=39344']"	['<urn:uuid:12c57f46-1746-4c28-81ff-ead72489c7af>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	13	46	1234
80	Which plastic making method is cheapest for small orders?	Thermoforming is typically a more reasonably priced way of making plastic parts for smaller production runs compared to mass-production methods. This process involves heating pre-extruded rigid plastic sheets and sucking them down into hollowed-out cavities, where they cool and take the shape of the mould.	['Injection moulding is a very popular method for producing structurally sound plastic parts and components en masse, using melted polymers and plastics. These are forced into a specially shaped mould cavity, left to solidify and then extracted from the mould once cool. Very effective for prototypes and mass-produced solid items such as toys, phone components, automotive parts and utensils.\nBlow moulding follows a similar process to injection moulding, but the melted plastic is poured vertically out of a barrel and down into the mould, via a special tube. From there, the action of the mould closing on it blows the plastic outwards to that it coats the inside of the mould to form the required shape. As it cools, the inside section hollows out, making it a great technique for bottles, food containers and tubes.\nStructural Foam Moulding\nThis is an oft-chosen method for items that need thicker walls for greater durability and tensile strength. A small quantity of nitrogen or chemical blow agent is added into the melted plastic to thicken its consistency. The mixture foams as it enters the mould, forming a plastic ‘skin’ that solidifies to make the wall of the item being constructed. This technique works with any type of thermoplastic that is capable of being injection moulded.\nThermoforming takes as different approach to injection moulding and other methods that use melted plastic. Sheets of pre-extruded, rigid plastics are heated and sucked down into hollowed-out cavities. As the sheets cool, they take on the shape of the mould to form the finished piece. This is normally a more reasonably priced way of making plastic parts that have a smaller run than anything that is being mass-produced.\nThis method uses powdered plastic that is fixed around spokes that extend out from a central hub inside a specially prepared mould. As the mould rotates, the hub moves into a furnace room, where the plastic powder melts and sticks to the inside of the mould. The hub then moves to a cooling area, where the hot plastic hardens into a hollow component that fits the exact shape of the mould. This is a slightly costlier process, but a good option for prototypes and pieces that need to be highly accurate, but not produced in high numbers.\nCompression moulding uses vertical presses as opposed to the horizontal ones found in injection moulding. The plastic material is pressed between the two halves of the heated mould to form the required shape. It is then air-cooled, extracted and passed on to quality control. This is a moderately costly process that yields excellent results and high levels of accuracy.\nGas Assist Moulding\nFinally, gas assist moulding uses gas injection moulding techniques to create hollow plastic parts that are needed by the customer. The hot plastic is partially added to the mould and immediately followed by a jet of high-pressure inert gas (e.g. nitrogen) to force it into the right place against the walls of the mould. The process is repeated until the required wall thickness is achieved.']	['<urn:uuid:03ed19b9-c8bf-4747-b77f-0549250574b0>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	9	45	504
81	waste processor magnetic separation uses industries examples	Magnetic separation is used across diverse industries including dairy, grain and milling, plastics, food, chemical, oils, textile, mining, pharmaceutical, feed and grain industries. It's also applied in pollution control, chemical processing, ore beneficiation, slag reclamation, automobile shredding, and municipal solid waste processing.	"['Separators, Classifiers, and Screeners Information\nScreeners, classifiers, shakers, and separators are used for classification of powders or other bulk materials. Classification is done by particle size, density, magnetic properties, or electrical characteristics. Round and rectangular screeners, magnetic separators, electrostatic separators, rotary sifters, wet or concentrating tables, rake classifiers, classifying hydrocyclones, floatation systems and trommels are included in the category.\nScreeners are sifting units that are rotated as powder is fed into their interior. The finer particles fall through the sieve opening and oversized particles are ejected off the end. Rotary sifters or drum screeners are often used for deagglomerating or delumping type operations. Screeners are available in three main types: drum sifter, rectangular deck, and round deck.\nAir classifiers use the spiral air flow action or acceleration within a chamber to separate or classify solid particles. Powders suspended in air or gas enter the cyclone and the heavier particles spiral out and down where they are collected. The air and finer particles flow up to the top where they may be passed to another cyclone with finer classification capability. A cyclone is essentially a settling chamber where the effects of gravity (acceleration) have been replaced with centrifugal acceleration.\nConcentrating tables or density separators screen bulk materials or minerals based on the density (specific gravity), size and shape of the particles. This group includes jigging equipment, shaking table, spiral concentrators, concentrating or wet tables, or specialized settling vessels. Most concentrating or density separation equipment are hydraulic or water-based, although pneumatic or air-based systems are also available.\nElectrostatic separators use preferential ionization or charging of particles to separate conductors from dielectrics (nonconductors). The charged dielectric particles are attracted to an oppositely charged electrode and collected. The particles may be charged through contact electrification, conductive induction, or high tension (ion bombardment).\nFloatation systems separate hydrophobic particulates from hydrophilic particulates by passing fine air bubbles up through a solid-liquid mixture. The fine bubbles attach to and lift or float the hydrophobic particles up where they are collected.\nMagnetic separators use powerful magnetic fields to separate iron, steel, ferrosilicon, or other ferromagnetic materials from non-magnetic bulk materials. The magnetic field may be generated by permanent magnets or electromagnets.\nRake, spiral, and bowl classifiers use mechanical action to dewater, deslime or separate coarse bulk materials from finer materials or liquids. Drag classifiers consist of a chain-link conveyor or endless belt that is dragged through a solid-liquid mixture. Rake classifiers lift solid-liquid mixtures up onto a plate with a screen or rake. Spiral classifiers use an Archimedes pump screw to lift solid-liquid mixtures up onto a screen for dewatering. Bowl classifiers, bowl desilters, hydroseparators or countercurrent classifiers are other types or mechanical classifiers.\nTrommels are large rotary drum shaped with a grate-like surface with large openings. Trommels are used to separate very coarse materials from bulk materials such as coarse plastics from finer aluminum recycled material, coarse inorganic materials from organic wastes or large ore chunks from finer minerals.\nWater classifiers such as elutriators and classifying hydrocyclones use settling or flow in water or a liquid to separate or classify powdered materials based on particle size or shape.\nAdditional considerations for selecting a screener or separator are the application and the material processed. These devices play an important role in many application including those in the mining, pharmaceutical, powders, plastics, and feed and grain industries.\nAICHE E-29 -- Methodology for conducting and interpreting performance evaluation tests on particle classification\nRead user Insights about Separators, Classifiers, and Screeners', '01-01-1979· Magnetic separation of ferrous metallics from municipal solid waste is based on technology developed for, and profitably applied to, ore beneficiation, slag reclamation, automobile shredding, and scrap processing industries.\nMagnetic separation is also used in situations where pollution needs to be controlled, in chemical processing, as well as during the benefaction of nonferrous low-grade ores. Magnetic separation is also used in the following industries: dairy, grain and milling, plastics, food, chemical, oils, textile, and more. Magnetic cell separation\nwaste minerals through processing, here illustrated with the behavior of cassiterite and iron oxides in magnetic separation. The application of SEM-based image analysis data to magnetic separation was inter alia carried out by Leißner et al. (2016b) . The magnetic susceptibility of different minerals is one major determining\n21-07-2017· The use of strong magnetic field gradients and high magnetic fields generated by permanent magnets or superconducting coils has found applications in many fields such as mining, solid state chemistry, biochemistry and medical research. Lab scale or industrial implementations involve separation of macro- and\nResults for magnetic separation equipment from Enerpat, Eriez, Mastermag and other leading brands for waste and recycling. ... Waste Wood Processing Product line Premium. Shred-Tech - Model STQ-75 - Four Shaft Shredder. The STQ-75 is designed to process bulk materials including ...\n15-08-2001· Today, magnetic separation still dominates the way processors remove ferrous from nonferrous material. While permanent magnets are popular choices, advances in electromagnets have made them competitive again. OVERHEAD MAGNETS. The first type of magnetic separation equipment is the overhead magnet.\nThe present invention provides a method of separating and recovering iron from a waste non-ferrous slag, generated in a process for smelting of non-ferrous metals, including copper, zinc and lead, in which a reducing agent and a reaction catalyst are added to the crushed waste non-ferrous slag, and the mixture is subjected to a reduction reaction, thereby converting amorphous iron oxides ...\nThese magnetic separators are used to remove tramp metal from conveyors, chutes and hoppers in dry bulk processing equipment and systems. Magnetic separators can be placed inside or outside of product flows and are used in industries such as food, chemical, plastic, recycling, aggregate, grain and more. Custom sizes are available for some magnets.\n04-05-2017· MAGNETIC SEPARATION Magnetic separation is a process used to segregate magnetic (i.e., ferrous) metal from a mixture of different types of materials, e.g., mixed waste or commingled metal, glass, and plastic containers. The process is technically simple and of relatively low cost. 9Department of Environmental Scieces, QAU Islamabad 10.\nA new method for separating different types of waste plastic has been developed by Turkish researchers. The \'flotation\' method recovers PVC and PET, among the most difficult plastics to separate from each other. It could help increase the amount of plastic waste that can be recycled and ease pressure on bulging landfill sites.\nBarry A. Wills, James A. Finch FRSC, FCIM, P.Eng., in Wills\' Mineral Processing Technology (Eighth Edition), 2016 13.4.3 Material Transport in Magnetic Separators. Commercial magnetic separators are continuous-process machines, and separation is carried out on a moving stream of particles passing into and through the magnetic field. Close control of the speed of passage of the particles ...\nV Waste processing, treatment and recycling 1 5 Waste processing, treatment and recycling Background ... Therefore magnetic separators, blowers, inclined conveyors and screens prepare the waste stream for the manual separation and help to raise the efficiency rating of sorting personnel.\n14-08-2018· Magnetic Separator is the most trusted machine used to recover metal from the waste materials. It is known for the easy separation process to detach fine particles which have poor magnetic properties. Magnetic separator provides the excellent separating effect, …\nseparation by the waste processing industry. Before incineration this residual waste often goes through a separation process. After arriving at the waste processing plant, useful components are removed from the waste flow. In this regard, ferrous metals such as iron, sheet metal, steel, etc. are of main consideration. For this purpose de-ironing\nMagnetic density separation (MDS) is such a technology. Early research showed that MDS has the potential to turn around the poor image of polyolefin recycling. However substantial research was needed to improve the separation process before high quality secondary polyolefins could be obtained from complex waste mixtures.\n4. Magnetic Separator: This machine will help you separate the magnetic and non-magnetic materials. That should help you understand the functions of every part of MSW sorting equipment. Now, the following is the whole municipal solid waste sorting process. It should also provide you with additional information on how the parts function together.\n> Blog: Evaporation Technology: A Unique Separation Process for Industrial Wastewater Treatment Evaporation Technology Evaporation, or distillation, is a separation process that takes advantage of the changing physical states of water, or other solvents, from liquid to vapor.\nFigure 1. Processing routes for extracting REE from concentrates for three major REE-bearing minerals. The ""REE purification"" stage would in most cases involve REE separation by e.g. solvent extraction. The ""Mountain Pass"" route refers to the process used before the mine closed in 2002. From (British Geological Survey, 2011)\nA magnetic separator to select must be suitable for the purpose of use and have a sufficient capacity. To select such a most suitable separator, when inquiring about separators, ... ※ This system is installed not only in wastes processing plants, materials\nMagnetic separators for the recycling and waste processing, scrap yards & recycling centers - SOLLAU s.r.o. - Magnetic separation Magnetic separators for the recycling and waste processing Overband …\n01-01-1979· Magnetic separation of ferrous metallics from municipal solid waste is based on technology developed for, and profitably applied to, ore beneficiation, slag reclamation, automobile shredding, and scrap processing industries. No one system or type of magnet can be used for all ferrous waste …\n- disadvantages of a magnetic drum separator Jul\n- magnetic separator double roll\n- magnetic separation kg\n- industrial magnetic drum separator\n- chromite spiral magnetic separator 45 basalt crusher\n- magnetic separation in chemistry\n- magnesite ore low gradient magnetic separator\n- Magnetic Separators Product\n- high quality magnetic separator for iron ore concentrating\n- iron sand river separation hi magnetic\n- example of low intensity magnetic separator\n- magnetic resonance considered\n- kolkata magnetic conveyor belt used for sale\n- magnetic field at machine wall\n- bar magnetic separator\n- magnetic separator stream\n- used iron ore magnetic for sale u s a\n- magnetic separation machine ls sand washing machine hj series jaw crusher\n- magnetic separator in iron sand processing project for sale\n- process of magnetic separation\n- magnetic separation of iron particles images\n- air lifter slat feeder magnetic separator\n- high intensity mining equipment wet magnetic ore separator\n- magnetic spiral water\n- cyclic high gradient magnetic separator inlet nozzle\n- nelson co eiez magnetic sparatora\n- appliions for magnetic sand\n- Crushing And Magnetic Seperator Cost Coal Russian\n- magnetic chuck surface grinding machine\n- high intensity and high gradient magnetic separators\n- crusher magnetic ball\n- magnetic separator working principle particle size minimum\n- magnetic separation garnet']"	['<urn:uuid:9477d028-58aa-4123-9b8f-6d29efbbf930>', '<urn:uuid:33af4b22-7f2b-4b71-b248-789bbbd03e94>']	factoid	with-premise	short-search-query	distant-from-document	three-doc	expert	2025-05-12T10:32:05.666290	7	42	1765
82	As a marine conservationist studying endangered species and climate change, I'm wondering what is currently threatening sawfish populations globally, and what role could their coastal habitats play in climate change mitigation?	Sawfish populations are threatened primarily by high catchability in fishing gear, their occurrence in shallow coastal waters where they interact with human activities, and the high value of their fins and saws which means they are rarely released unharmed. Habitat modification like damming and mangrove loss has also contributed to their decline. These same coastal habitats where sawfish live, including mangroves, are crucial for climate change mitigation as they act as carbon sinks, sequestering carbon dioxide up to 35 times faster than tropical rainforests.	['In order to protect the five species of endangered sawfishes, we need to know where they can still be found today. Colin will work with sawfish experts around the world to undertake a global sawfish survey using environmental DNA (eDNA).\nI grew up exploring tide pools, fishing and snorkelling along the west coast of Australia. At age nine I announced to my family that I was going to be a marine biologist. That started a life-long journey of discovery and adventure with the ocean. I studied at James Cook University in Queensland, first for my Bachelor’s degree and then PhD. Along the way, the opportunity to research sharks and rays presented itself Since then, elasmobranch have been the primary focus of my career for 30 years.\nIt was during my PhD in the late 1980s that I caught my first sawfish...\nTo resolve the current global distribution of sawfishes using eDNA survey techniques.\nSawfishes are considered the most threatened group of marine fishes, with all species on the IUCN Red List as Critically Endangered or Endangered. Sawfishes have disappeared from many countries that made up their historic range because of fishing and habitat loss. As a result of rapid population declines sawfishes are protected by many of their range states, and listed by CITES and CMS. The development of a global conservation strategy identified the need to clearly resolve where populations persist beyond the known refuges of northern Australia and Florida. The rarity of sawfishes, however, makes traditional fishing surveys largely ineffective, unreliable and cost-prohibitive, at the scales needed to locate now scarce individuals. Through a recent local test case in northern Australia eDNA has been demonstrated to effectively survey for sawfish. This successful proof of concept provides the opportunity to conduct broad-scale surveys to resolve the current distribution of all species.\nSawfishes (Family Pristidae) are considered to be globally the most threatened group of elasmobranchs with all five species listed either as Critically Endangered or Endangered by the IUCN Red List of Threatened Species. The decline of sawfish populations is largely a result of their high level of catchability in fishing gear, their occurrence in shallow coastal waters where they commonly interact with human activities, and the high value of their fins and saws (which mean they are rarely released unharmed). In addition, habitat modification such as damming of rivers and loss of mangroves have also played a role in the declines. At present sawfish populations appear to be relatively safe in only two locations – northern Australia and Florida. In these locations national protection, effective management of threats and ongoing monitoring have enabled populations to stabilise or even recover.\nA recent analysis of the distribution of sawfish species demonstrated that all are extinct in significant parts of their historic ranges (30 – 81% decline in area of occurrence, depending on species), with 20 countries having lost all of their sawfishes, and 43 countries having lost at least one sawfish species. This analysis also demonstrated that there are many countries where the status of sawfishes remains uncertain. The lack of data on their current occurrence and distribution makes their conservation significantly more difficult because it is hard to know where to focus limited conservation resources to ensure that sawfish are not lost from more countries and to start recovering them in areas where they remain.\nTraditional approaches to surveying and monitoring sawfish populations involve fishing surveys of potential habitats, and can be manually intensive and costly. This makes undertaking a global survey using these techniques prohibitively expensive and time consuming. New molecular approaches to surveying water bodies (rivers, estuaries and oceans), however, make broad global scale surveys more feasible. The most common approach currently being developed is environmental DNA (eDNA) where water samples are filtered and assayed for fragments of a specific species’ DNA. This technique is becoming more widely used for research and monitoring of pest, invasive and threatened species because of its cost- effectiveness and reliability. We recently demonstrated that this approach can be used to detect sawfish in the flood plain waterholes in northern Australia. This study has demonstrated the potential for this approach to be used to deliver greatly improved information on the occurrence and distribution of sawfish at large spatial scales.\nBy trawling the fish markets and landing sites of Ghana’s coastline, Issah is surveying the patterns in catch composition over time for sharks and rays in artisanal fisheries. In doing so, he is also raising awareness about the best fishing practices that safeguard sharks and rays and garnering fishers’ support for sharks and the conservation of ocean ecosystems in Ghana.\nJuan is collecting environmental DNA (eDNA) samples from the estuaries and mangroves of Colombia’s Chocó region. He is uncovering the presence and distribution of largetooth sawfishes on the Colombian Pacific coast by detecting traces of their DNA left behind as signatures in their environment. The Critically Endangered largetooth sawfish – known locally as ‘El Guacapa’ – is typically found in estuaries and thought to be resident in some of Central and South America’s freshwater systems. Knowing exactly where this sawfish occurs is critical to its conservation.', 'When we think about carbon and climate change, our minds often go to fossil fuels, deforestation, and greenhouse gas emissions. However, there’s another side to the carbon story that’s just as important – the role of coastal ecosystems in storing and sequestering carbon. This is where the concept of “Blue Carbon” comes into play.\nThe Role of Blue Carbon in Climate Change\nCarbon Sequestration in Coastal Ecosystems\nBlue carbon refers to the carbon captured and stored by the world’s coastal and marine ecosystems. These ecosystems, including mangroves, seagrasses, and salt marshes, act as carbon sinks, capturing and storing large amounts of carbon dioxide from the atmosphere. In fact, they can sequester carbon at a rate up to 35 times faster than tropical rainforests.\nThe Blue Carbon Initiative\nRecognizing the importance of these ecosystems, the Blue Carbon Initiative was launched to promote their conservation and restoration. The initiative brings together governments, research institutions, NGOs, and communities to protect and restore these vital ecosystems, which not only help in climate change mitigation but also provide numerous other ecosystem services such as biodiversity conservation, water purification, and coastal protection.\nCoastal Ecosystems and Blue Carbon\nMangroves are one of the most important blue carbon ecosystems. These unique trees, which grow in intertidal zones, have complex root systems that trap sediments, slowing down water flow and allowing organic material to settle. This creates a carbon-rich soil that can store significant amounts of carbon.\nSeagrasses, another vital blue carbon ecosystem, are flowering plants that grow in shallow coastal waters. They have the ability to capture and store large amounts of carbon in their roots and sediment, making them highly efficient carbon sinks.\nSalt marshes, found in the intertidal zone of coastal areas, are another important blue carbon ecosystem. These marshes are highly productive, with plants that capture and store carbon in their biomass and soil.\nThreats to Blue Carbon Ecosystems\nOne of the main threats to blue carbon ecosystems is coastal development. As coastal areas become more developed, these ecosystems are often destroyed or degraded, leading to the release of stored carbon back into the atmosphere.\nPollution, especially nutrient runoff from agriculture and sewage, can also harm blue carbon ecosystems. Excess nutrients can lead to algal blooms, which deplete oxygen levels in the water, harming seagrasses, mangroves, and other organisms.\nClimate change poses a significant threat to blue carbon ecosystems. Rising sea levels and temperatures can lead to the loss of these ecosystems, while increased storm intensity can cause physical damage.\nConservation Efforts for Blue Carbon Ecosystems\nTo counter these threats, various restoration projects are underway to bring back lost or degraded blue carbon ecosystems. These projects involve planting mangroves, seagrasses, and salt marshes, and are often done in collaboration with local communities.\nPolicy measures, such as designating protected areas and creating incentives for conservation and restoration, are also crucial in protecting blue carbon ecosystems. By recognizing the value of these ecosystems, we can ensure their conservation and restoration, leading to long-term benefits for the climate and biodiversity.\nThe Future of Blue Carbon\nThe concept of blue carbon is still relatively new, but it’s gaining recognition and momentum. As we continue to understand the importance of these ecosystems in climate change mitigation, we can expect more efforts to conserve and restore them. With collaboration between governments, researchers, NGOs, and communities, we can protect and enhance these vital ecosystems for future generations.\n- Biology Niche: Exploring the Fascinating World\n- Biofuels: The Future of Sustainable Energy\n- Biodiversity: Exploring Nature’s Tapestry\nIn conclusion, blue carbon is a crucial part of the global carbon cycle, with coastal ecosystems playing a key role in capturing and storing carbon. By conserving and restoring these ecosystems, we can mitigate climate change while also enjoying other ecosystem services. The future of blue carbon is promising, and with continued efforts, we can ensure these ecosystems are protected for generations to come.\nWhat is blue carbon?\nBlue carbon refers to the carbon captured and stored by the world’s coastal and marine ecosystems, including mangroves, seagrasses, and salt marshes. These ecosystems act as carbon sinks, helping to mitigate climate change by sequestering large amounts of carbon dioxide from the atmosphere.\nWhy are coastal ecosystems important in capturing carbon?\nCoastal ecosystems, such as mangroves, seagrasses, and salt marshes, are crucial for capturing carbon because they have unique properties that allow them to trap and store carbon in their biomass and sediments. These ecosystems can sequester carbon at a rate up to 35 times faster than tropical rainforests, making them highly efficient carbon sinks.\nWhat is the Blue Carbon Initiative?\nThe Blue Carbon Initiative is a global program that aims to promote the conservation and restoration of coastal and marine ecosystems to enhance their capacity to capture and store carbon. The initiative brings together governments, research institutions, NGOs, and communities to work towards protecting and restoring these vital ecosystems for climate change mitigation and other ecosystem services.\nWhat are some threats to blue carbon ecosystems?\nCoastal development, pollution, and climate change are the primary threats to blue carbon ecosystems. Coastal development can lead to the destruction or degradation of these ecosystems, while pollution, especially nutrient runoff, can harm their health. Climate change, with rising sea levels and temperatures, can also result in the loss of these ecosystems and increased storm intensity can cause physical damage.\nHow can we protect and restore blue carbon ecosystems?\nTo protect and restore blue carbon ecosystems, conservation efforts include restoration projects, policy measures, and community involvement. Restoration projects involve planting mangroves, seagrasses, and salt marshes, often in collaboration with local communities. Policy measures, such as designating protected areas and creating incentives for conservation and restoration, are crucial in protecting these ecosystems. Additionally, raising awareness and involving local communities in conservation efforts are essential for the long-term success of these initiatives.']	['<urn:uuid:333b391e-1000-47bf-92c4-4f2d2242c3e8>', '<urn:uuid:98890e9d-0424-4365-bfae-27add36ce21c>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T10:32:05.666290	31	84	1821
83	water pollution standards slaughterhouses fish farms compare	Both slaughterhouses and fish farms are subject to EPA water pollution standards, but with different approaches. For slaughterhouses, EPA set numerical standards last updated 15 years ago for direct discharge facilities, while never setting standards for those that discharge to sewage treatment plants first. In contrast, for fish farms, EPA established non-numerical best management practices rather than specific numeric limits, requiring facilities to prevent discharge of drugs and pesticides, minimize feed discharge, and maintain proper systems. The average slaughterhouse releases about 330 pounds of nitrogen daily, equivalent to untreated sewage from 14,000 people, while fish farms are expected to reduce total suspended solids by 500,000 pounds per year and biochemical oxygen demand and nutrients by 300,000 pounds annually through the implementation of their standards.	"['December 18, 2019\nThe federal Clean Water Act requires the Environmental Protection Agency to set industry-wide water pollution standards for slaughterhouses and to review those standards each year to decide whether updates are appropriate to keep pace with advances in pollution control technology. On Wednesday, 12 groups sued EPA for its decision not to update national standards restricting water pollution from slaughterhouses.\nOn Oct. 24, 2019, EPA announced its decision in the Federal Register that it would not revise the federal water pollution standards for slaughterhouses that discharge processed wastewater directly into waterways and that it would not create standards for plants that send their wastewater to sewage plants before discharging into rivers or streams.\nThe Environmental Integrity Project (EIP) and Earthjustice filed the lawsuit in the U.S. Court of Appeals for the Fourth Circuit in Richmond, Va., on behalf of Cape Fear River Watch, Rural Empowerment Association for Community Help, Waterkeepers Chesapeake, Animal Legal Defense Fund, Center for Biological Diversity, Comite Civico del Valle, Environment America, Food & Water Watch, The Humane Society of the United States and Waterkeeper Alliance.\nEPA last revised standards for slaughterhouses that discharge polluted water directly into waterways 15 years ago, the groups stated, noting that more than a third of these slaughterhouses are still operating under guidelines that date back to 1974 or 1975.\n“EPA’s national standards for water pollution from slaughterhouses are either weak and outdated or nonexistent,” EIP attorney Sylvia Lam said. “It is well past time for EPA to crack down on this public health hazard. Cleaner plants have already installed technology to lessen the pollution they send into their local rivers and streams. By not updating these nationwide standards, EPA is rewarding dirty slaughterhouses at the expense of the public.”\nEPA has never set standards for slaughterhouses that send their waste to sewage treatment plants before discharging into waterways, even though these slaughterhouses make up a substantial portion of the industry.\n“Some of EPA’s technological requirements for slaughterhouses date from the mid-1970s,” Earthjustice attorney Alexis Andiman said. “Technology has changed a lot since then, and EPA needs to catch up. EPA’s failure to update pollution standards for slaughterhouses is illegal, and it allows a major industry to continue cutting corners at the expense of communities and the environment.”\nIn a report released in October 2018, EIP found that the average slaughterhouse discharged more than 330 lb. of nitrogen per day in 2017 – equivalent to the amount of pollution in untreated sewage from a town of 14,000 people. About two-thirds of the 98 slaughterhouses studied by EIP discharge to waterways that are impaired by one or more pollutants found in slaughterhouse wastewater. At least 66 of the 98 plants surveyed by EIP are owned by companies that each reported more than $2 billion in annual revenues.\nMeat processing plants discharge water contaminated with blood, oil and grease and fats. This wastewater contains nitrogen and phosphorus pollution, pathogens and other contaminants. When released into waterways, pollution from slaughterhouses can cause algae blooms that suffocate aquatic life and turn waterways into bacteria-laden public health hazards, EIP noted.\nWaterkeeper Alliance senior attorney Kelly Hunter Foster added, “EPA acknowledges that harmful algal blooms from uncontrolled nitrogen and phosphorus pollution are a major problem in all 50 states – a problem that can sicken or kill people exposed to extremely dangerous toxins, destroy fisheries and decimate local economies. Slaughterhouses are a major source of this pollution; EPA must take action to protect the public from these dangerous discharges.”\nA release from the plaintiffs stated that updated pollution standards could lead to significant improvements in waterways across the country, especially in areas where slaughterhouses are concentrated, such as eastern North Carolina and portions of Arkansas, Delaware, Georgia, Illinois, Indiana, Iowa, Mississippi, and Pennsylvania. “The most technologically advanced slaughterhouses already release far less pollution than the dirtiest plants, proving that improved technology exists. Under the Clean Water Act, EPA must ensure that all slaughterhouses adopt up-to-date and effective technology,” the groups said.\nYou May Also Like\nIowa turkey flocks confirmed with HPAIOct 23, 2023\nCurrent Conditions for\nNew York, NY\nEnter a zip code to see the weather conditions for a different location.', ""Effluent Guidelines - Aquatic Animal Production Industry\nFinal Rule - Fact Sheet\nEPA is setting standards for the discharge of wastewater from concentrated aquatic animal production facilities (known as fish farms). This rule establishes effluent limitation guidelines and new source performance standards for specific types of commercial and non-commercial operations that produce aquatic animals for food, recreation and restoration of wild populations, pet trade, and other commercial products. Rather than setting numeric limits, we are requiring best management practices to control the discharge of pollutants in the wastewater from these facilities. We found that it is not necessary to establish pretreatment standards for existing or new facilities.\n- To which facilities does this rule apply?\n- What are the impacts of the regulation?\n- What does the rule require?\nOn June 30, 2004, EPA's Acting Deputy Administrator signed a final rule to establish wastewater controls for concentrated aquatic animal production facilities (known as fish farms). The regulation applies to about 245 facilities that generate wastewater from their operations and discharge that wastewater directly to waters of the United States. When these requirements are applied in NPDES permits, they will help reduce discharges of conventional pollutants (mainly Total Suspended Solids), non-conventional pollutants (such as nutrients, drugs and chemicals) and, to a lesser extent, toxic pollutants (metals and PCBs).\nIn October 1989, the Natural Resources Defense Council and others sued EPA claiming the Agency had failed to comply with the Section 304(m) planning process required by the Clean Water Act. In January 1992, plaintiffs and EPA agreed to a settlement that established a schedule for EPA to promulgate effluent limitation guidelines for 11 specific industrial categories and for eight other categories to be determined by the Agency. EPA selected the concentrated aquatic animal production industry as one of those 11 categories. The revised consent decree requires EPA to sign a proposed rule by August 14, 2002, and to take final action by June 30, 2004. This rule is the last of the 19 categorical rules to be issued and completes EPA's obligation under the 1992 consent decree.\nTo which facilities does this rule apply?\nThe final rule applies to direct discharges of wastewater from these existing and new facilities:\n- Facilities that produce at least 100,000 pounds a year in flow-through and recirculating systems that discharge wastewater at least 30 days a year (used primarily to raise trout, salmon, hybrid striped bass and tilapia).\n- Facilities that produce at least 100,000 pounds a year in net pens or submerged cage systems (used primarily to raise salmon).\nWhat are the impacts of the regulation?\nWe expect that, when the rule is implemented through NPDES permits the discharge of total suspended solids will be reduced by more than 500,000 pounds per year, and the discharge of biochemical oxygen demand and nutrients will be reduced by about 300,000 pounds per year. The resulting improvements in water quality will create more opportunities for swimming and fishing and reduce stress on ecosystems in those waters. We estimate it will cost about $1.4 million a year for the facilities to comply with this rule, and our analyses indicate that they can afford these costs.\nWhat does the rule require?\nThe rule requires that all applicable facilities:\n- Prevent discharge of drugs and pesticides that have been spilled and minimize discharges of excess feed.\n- Regularly maintain production and wastewater treatment systems.\n- Keep records on numbers and weights of animals, amounts of feed, and frequency of cleaning, inspections, maintenance, and repairs.\n- Train staff to prevent and respond to spills and to properly operate and maintain production and wastewater treatment systems.\n- Report the use of experimental animal drugs or drugs that are not used in accordance with label requirements.\n- Report failure of or damage to a containment system.\n- Develop, maintain, and certify a Best Management Practice plan that describes how the facility will meet the requirements.\nThe rule requires flow through and recirculating discharge facilities to minimize the discharge of solids such as uneaten feed, settled solids, and animal carcasses.\nThe rule requires open water system facilities to:\n- Use active feed monitoring and management strategies to allow only the least possible uneaten feed to accumulate beneath the nets.\n- Properly dispose of feed bags, packaging materials, waste rope, and netting.\n- Limit as much as possible wastewater discharges resulting from the transport or harvest of the animals.\n- Prevent the discharge of dead animals in the wastewater.""]"	['<urn:uuid:b7c1a28f-09b6-4861-863b-83c8a6c48c15>', '<urn:uuid:fad2b80a-5c1b-4a88-925d-490de95decd8>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	7	124	1440
84	mineral oil comedogenic evidence studies	A 2005 study titled 'Is mineral oil comedogenic?' examined this question and concluded that mineral oil is noncomedogenic in humans. This conclusion was based on both animal and human data, along with the AAD recommendation.	"['|Ingredient name||what-it-does||irr., com.||ID-Rating|\n|Paraffinum Liquidum (Mineral) Oil||emollient||0, 0-2|\n|C12-13 Pareth-3||emulsifying, surfactant/cleansing|\n|Melaleuca Alternifolia (Tea E) Leaf Oil||soothing, anti-acne, antimicrobial/antibacterial||goodie|\n|Salicylic Acid||exfoliant, anti-acne, soothing, preservative||superstar|\nEnviron SebuprepIngredients explained\nThe famous or maybe rather infamous mineral oil. The clear oily liquid that is the ""cheap by-product"" of refining crude oil and the one that gets a lot of heat for its poor provenance. It is a very controversial ingredient with pros and cons and plenty of myths around it. So let us see them:\nThe pros of mineral oil\nTrust us, if something is used for more than 100 years in cosmetic products, it has advantages. Chemically speaking, cosmetic grade mineral oil is a complex mixture of highly refined saturated hydrocarbons with C15-50 chain length. It is not merely a ""by-product"" but rather a specifically isolated part of petroleum that is very pure and inert.\nIt is a great emollient and moisturizer working mainly by occlusivity. Occlusivity is one of the basic mechanisms of how moisturizers work and it means that mineral oil sits on top of the skin and hinders so-called trans-epidermal water loss, i.e water evaporating out of your skin. When compared to heavy-duty plant oil, extra virgin coconut oil, the two of them were equally efficient and safe as moisturizers in treating xerosis, a skin condition connected to very dry skin.\nThe other thing that mineral oil is really good at is being non-irritating to the skin. The chemical composition of plant oils is more complex with many more possible allergens or irritating components, while mineral oil is simple, pure and sensitivity to it is extremely rare. If you check out the classic French pharmacy brands and their moisturizers for the most sensitive, allergy prone skin, they usually contain mineral oil. This is no coincidence.\nThe cons of mineral oil\nThe pros of mineral oil can be interpreted as cons if we look at them from another perspective. Not penetrating the skin but mostly just sitting on top of it and not containing biologically active components, like nice fatty acids and vitamins mean that mineral oil does not ""nourish"" the skin in the way plant oils do. Mineral oil does not give the skin any extra goodness, it is simply a non-irritating moisturizer working mainly by occlusivity.\nThe myths around mineral oil\nBadmouthing mineral oil is a favorite sport of many, it is a cheap material and being connected to petrolatum makes it fairly easy to demonize.\nWhile it is true that industrial grade mineral oil contains carcinogenic components (so-called polycyclic compounds), these are completely removed from cosmetic and food grade mineral oil and there is no scientific data showing that the pure, cosmetic grade version is carcinogenic.\nWhat is more, in terms of the general health effects of mineral oils used in cosmetics, a 2017 study reviewed the data on their skin penetration and concluded that ""the cosmetic use of mineral oils and waxes does not present a risk to consumers due to a lack of systemic exposure.""\nAnother super common myth surrounding mineral oil is that it is comedogenic. A 2005 study titled ""Is mineral oil comedogenic?"" examined this very question and guess what happened? The study concluded that ""based on the animal and human data reported, along with the AAD recommendation, it would appear reasonable to conclude that mineral oil is noncomedogenic in humans.""\nOverall, we feel that the scaremongering around mineral oil is not justified. For dry and super-sensitive skin types it is a great option. However, if you do not like its origin or its heavy feeling or anything else about it, avoiding it has never been easier. Mineral oil has such a bad reputation nowadays that cosmetic companies hardly dare to use it anymore.\nA hydrocarbon-based emollient that can come in different viscosities from silky-light through satiny-smooth to luxurious, rich. It forms a non-occlusive film on the surface of the skin and brings gloss without greasiness to the formula. It\'s a very pure and hypoallergenic emollient that\'s also ideal for baby care products.\nWe don\'t have description for this ingredient yet.\nA super common emollient that makes your skin feel nice and smooth. It comes from coconut oil and glycerin, it’s light-textured, clear, odorless and non-greasy. It’s a nice ingredient that just feels good on the skin, and it’s also easy to formulate with. No wonder it’s popular.\nThe famous tea tree oil. One of the best known essential oils which comes from Australia where it has been used for almost 100 years for its antiseptic and anti-inflammatory actions. Legend has it that the medicinal benefits of the oil were considered so important that Australian soldiers were supplied with some tea tree oil in their World War II military kit.\nSimilar to other essential oils, tea tree oil is a very complex chemical mixture consisting of about 100 components, the major ones being terpinen-4-ol (40%), γ-Terpinene (23%) and α-Terpinene (10%). Terpinen-4-ol is considered to be the main active component but as a great article in Clinical Microbiology Reviews states ""while some TTO components may be considered less active, none can be considered inactive"" and most components contribute to TTO\'s strong antibacterial, antiviral and antifungal effects.\nRegarding skincare and tea tree oil, its most well-known effect is probably being a well established anti-acne ingredient. Multiple studies confirm that TTO is effective against the evil acne-causing bacteria called P. acnes and the effectiveness of 5% TTO gel is comparable to the gold standard anti-acne treatment, 5% Benzoyl Peroxide lotion. You need to be a bit more patient with TTO, though, as its effects come slower but also with fewer side effects.\nRegarding TTO and sensitive skin, we say that you should be careful. Even if your skin is not sensitive you should never put undiluted TTO on your skin. Luckily, it contains only very small amounts of the common allergens (such as limonene), but irritant and allergic reactions still happen, especially by oxidation products that occur in older or not properly stored oil. So if you have some pure TTO at home, know that storage matters, keep it in a cool, dry, dark place and use it up in a reasonable amount of time.\nOverall, we do not often give a goodie status to essential oils, but we feel that TTO\'s unique antibacterial and anti-acne properties with its minimal allergen content warrant an exception. If your skin is acne-prone, TTO is something to experiment with.\n- It\'s one of the gold standard ingredients for treating problem skin\n- It can exfoliate skin both on the surface and in the pores\n- It\'s a potent anti-inflammatory agent\n- It\'s more effective for treating blackheads than acne\n- For acne combine it with antibacterial agents like benzoyl peroxide or azelaic acid\nGood old water, aka H2O. The most common skincare ingredient of all. You can usually find it right in the very first spot of the ingredient list, meaning it’s the biggest thing out of all the stuff that makes up the product.\nIt’s mainly a solvent for ingredients that do not like to dissolve in oils but rather in water.\nOnce inside the skin, it hydrates, but not from the outside - putting pure water on the skin (hello long baths!) is drying.\nOne more thing: the water used in cosmetics is purified and deionized (it means that almost all of the mineral ions inside it is removed). Like this, the products can stay more stable over time.\nA super common and cheap fragrance ingredient. It\'s in many plants, e.g. rosemary, eucalyptus, lavender, lemongrass, peppermint and it\'s the main component (about 50-90%) of the peel oil of citrus fruits.\nIt does smell nice but the problem is that it oxidizes on air exposure and the resulting stuff is not good for the skin. Oxidized limonene can cause allergic contact dermatitis and counts as a frequent skin sensitizer.\nLimonene\'s nr1 function is definitely being a fragrance component, but there are several studies showing that it\'s also a penetration enhancer, mainly for oil-loving components.\nAll in all, limonene has some pros and cons, but - especially if your skin is sensitive - the cons probably outweigh the pros.\nCopy and paste into your blog\n|irritancy, com.||0, 0-2|\n|what‑it‑does||emulsifying | surfactant/cleansing|\n|what‑it‑does||soothing | anti-acne | antimicrobial/antibacterial|\n|what‑it‑does||exfoliant | anti-acne | soothing | preservative|\n|what‑it‑does||perfuming | solvent|']"	['<urn:uuid:9e0fb9ad-383a-4dea-8028-650a855bf16b>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	5	35	1387
85	what happens during third man vienna sewer tour	During the Third Man sewer tour, visitors wear hard hats with LED headlamps and descend through a star-shaped hatch into the sewers. The tour includes three connected rooms: the first features a main sewer canal where a film about the sewer system is shown, the second is the original filming location of many sewer scenes from The Third Man movie, and the third is another sewer section. Visitors may also see the Wien river tunnel, depending on conditions. The tour combines movie history with real sewer system information and includes sound effects and film projections on the walls.	"['Third-Man-Tours of Vienna\'s sewers\nGuided tours that take visitors down into the ""underworld"" of Vienna\n\'s sewers, in particular to spots that were famously used for key scenes in the classic movie ""The Third Man"".\nMore background info:\n""The Third Man"", released in 1949, was one of the most surprisingly successful movies of the immediate post-WWII\nera. It\'s based on a novel/screenplay by Graham Greene who was directly inspired by post-war Vienna\n, and so the backdrop and thematic setting of the film have an almost documentary quality, though the cinematography offsets this with some decidedly experimental elements.\nThe plot, as briefly summarized as possible, involves a second-rate writer, Holly Martin, who comes to Vienna to see an old friend, Harry Lime, who Martin soon finds out is apparently dead. He learns that Lime had been a wheeler and dealer in substandard black-market penicillin, which left many an unfortunate patient damaged for life – apparently this was indeed a real-life problem in Vienna at that time. Martin, however, finds out that Lime\'s death was only staged, and that he still carries on with his illegal dealings, using the underground sewers to move from one part of the city to another, which above ground was heavily patrolled by the military.\nMartin eventually meets up with Lime – who is played by Hollywood star Orson Wells – who in one of the most famous scenes of the film delivers his infamous ""cuckoo-clock speech"" on Vienna\n\'s landmark Ferris wheel in the Prater amusement park. In this speech he plays down the noble morals Martin is trying to promote by cynically pointing out that 500 years of peace and democracy in Switzerland\nhave produced little more than the cuckoo clock.\nThe showdown of the film takes place in the sewers of Vienna, when police (and Martin) chase Lime. Intense dark scenes of eerie footsteps, reverberating voices and echoing gunshots make these scenes really stand out. They rank amongst the most captivating in movie history. In fact, though, parts of what purportedly is a sewer is actually the Wien river in its concreted over underground canal … but never mind, it certainly provided for better angles and light.\nThe enduring fame of the film is today still exploited in Vienna, even though initially, it hadn\'t even been all that successful in the city in which it was shot – that came later, more gradually. A cinema on the Ring, the Burgkino, still screens the film regularly (in the English original), there\'s a museum about the movie, and at least three regularly scheduled walking tours on the theme.\nOn one of these tours, visitors are taken down into the parts of Vienna\n\'s sewer system where crucial scenes of the film were shot – and given the element of darkness and undergroundness and the film\'s general dark tone, I thought this could be featured on these pages with an entry of its own too.\nWhat there is to see (and smell): I\'ve experienced this unusual tourist treat twice, once in October 2009 on a German language tour and again in May 2010 in English. Apart from the language there were also further differences.\nOne general one is that now you have to wear hard hats with little LED headlamps on them. The reason for this is not just enhanced safety (a good thing, especially if you\'re tall, since some of the corridors are indeed so low that you could easily bang your head). It\'s also because the underground passages are now less lit up – clearly to heighten the spooky atmosphere – which has furthermore been supplemented by a few sound effects that come from hidden tannoys. For real, on the other hand, are the sounds that cars rolling over manholes make – this produces strange clanking noises which reverberate around the underground passages, seemingly coming out of nowhere. Much eerier than the recorded sound effects.\nOnce you\'ve registered, paid for your ticket and have been provided with your hardhat, the tour guide will first give a brief introduction, still above ground: bits of information about the film and post-WWII Vienna\nin general. On the English-language tour this was much briefer. But that\'s not necessarily a bad thing as you get to what you\'re here for quicker: namely when the group descends through a star-shaped hatch into the actual sewers. This spot also featured in the ""The Third Man"" and may thus look familiar to those who\'ve seen the film.\nDown there you get to see three ""rooms"" connected by low corridors. The first room is at one of the main sewer canals with its fast-flowing dirty water. This is the smelliest section on the tour, but it\'s not actually all that bad. But if you\'re really, really sensitive, take a scarf or mask to cover your nose and mouth.\nHere a short film is shown. In 2009 this was in German with English subtitles and it was projected onto a screen suspended directly above the sewer. In May 2010 the film was in English and was projected directly onto the walls. While this may be an improvement in as much as it, again, somewhat enhances the spooky effect, it makes it more difficult to follow the film: it\'s harder to discern what it is that\'s being screened and the sound reverberates in such a way that it is also harder to understand what\'s being said (subtitles are of course unaffected by this).\nContent-wise, the film shown here has nothing to do with ""The Third Man"" but is a general introduction to the sewer system and the tasks of the people whose job it is to do the maintenance work down here.\nThen the ""Third Man"" theme is picked up again in the next room – which will look instantly familiar to all those who have seen the film. This is the main original location of many of the sewer shots of the movie – although others were actually shot in a studio mock-up in England. Orson Wells was famously reluctant to go down the sewer himself (allegedly he was afraid of rats), so that a double was needed. The location is real, though, even if it has been slightly altered: a new overflow canal inlet was added in recent years, only barely visible behind the old weir.\nOn the more recent English-language tour I did in May 2010 it was here that more film snippets were projected directly onto the wall opposite the elevated walkway you stand on – and colourful lighting effects were added as well. The German-language tour in 2009 was focused less on such haunted-house effects and more on factual information delivered by the guide. The new design is more effective in a visual sense, perhaps, but to be honest I preferred the more sober earlier approach. (It was also easier to take good photos of the room, as it looked in the film – which of course lacked colour effects, having been shot in black and white.) But others may well prefer the ""upgraded"" design – it\'s a matter of personal taste, I suppose.\nBy the way: at this point, the sewer odour is negligible, as the waters are actually fed by Vienna\'s many little rivulets coming down from the hills of the Wienerwald.\nThe next point on the tour is another sewer proper, and here you can see it too: grimy bits hanging of wall, lamps and lower handrails. More info on the history of the sewer is provided here.\nIn 2009 another film was shown here, again projected onto a screen suspended above the sewer. It too was more on the topic of ""The Third Man"" movie and its various connections with Vienna\n– and if I remember correctly was more traditional and informational rather than for effect.\nWhat I did not see in 2009 (allegedly for security concerns) but was again included in 2010 is the famous inside of the tunnel that the Wien river flows through and which provided one of the most iconic images of the entire film. It could well be that whether you\'re shown this or not is weather-dependent (obviously, if there\'s been too much rain, the water levels may suddenly rise to levels too dangerous for people to be down here.) If you get to see it it\'s quite something – although you don\'t actually see that much, since it is pretty dark. Still, even in the dim light of the headlamps on the hard hats and the guide\'s torch you get a feeling for the cavernous space of this covered river. On my tour in May 2010 there was even a car down here! It was the police (just as in the film!) – and their rear lights in the distance gave a good impression of distance. Then it turned round and came towards us. Quite a sight!\nFinally you are led back up into the open and there\'s a chance for some more questions and answers. And that\'s it.\nAll in all, the tour is only about one half to two thirds about the ""Third Man"", but that shouldn\'t stop dark tourists from doing it – as the other real-life bits of info about (and sensory experience of) the sewer are also worth taking in. It\'s not even as claustrophobic as you might expect – only the connecting corridors may feel a bit narrow, but the stations where the talking and film-showing takes place are quite spacious.\nIncidentally, you are very unlikely indeed to encounter any rats on the tour (even though the guide may joke about you not being allowed to take any away with you). Even if there are any about, they\'d disappear at the first sign of humans approaching.\n, more precisely: under Esperantopark and Friedrichstraße on the southern edge of the First District bordering the 4th – the meeting point for the tour is opposite No. 6 Friedrichstraße – and is clearly marked.\nAccess and costs: restricted, seasonal and by guided tour only; reasonably priced.\n: It is, quite predictably, only possible to go down to the sewers on guided tours. Before you\'re admitted you also have to sign a disclaimer (as the environment down there potentially poses a slight health & safety risk\nTours are conducted seasonally only, between 1 May and 31 October, and during the season take place Thursdays to Sundays, every hour on the hour between 10 a.m. and 9 p.m. – cost: 7 EUR (concessions and combination discounts apply). English-language tours may only be offered somewhat less frequently. Best check ahead.\nAnd also as the tours are generally very popular and numbers of participants for each tour are limited, it is highly recommended to book ahead: phone +43-1-4000-3033.\nThe meeting point for the tour is at the access hatch on a small green plot called Esperanto Park (for a small statue in honour of the inventor of this artificial language, Ludwig Zamenhof) – from the First District walk down Operngasse from opposite the Opera itself until you come to an open area where the busy street Friedrichstraße bends down into Wienzeile. The meeting point on the island in the street/square is marked by a large information panel next to a kind-of monument (and at times by a kind of oddly-shaped flag featuring the distinctive face of Orson Wells as Harry Lime, as well) and a single concrete sewer section. Here, you\'ll find the green van where you have to register for your tour, sign the disclaimer and leave your payment, before the group tour commences.\nTime required: The tour lasts about 45 to 55 minutes; you are advised to be at the meeting point 15-20 minutes before the start of the tour (for administrative reasons – you need to sign a disclaimer before you can join such a tour).\nCombinations with other dark destinations: There are further attractions on the same movie theme, esp. an above ground ""Third Man"" walking tour of some of the locations where scenes for the film were shot. Such walks are organized by the same company that also offers the sewer tours – and in fact if you book both you get a discount. Another ""Third Man"" walking tour of film locations is offered by viennawalks.com.\nThose who are really into the film could also go to the Third Man Private Collection, a kind of museum about the film, and in part also about Vienna\n). The most prized exhibit is the original zither that Anton Karas played/composed the movie\'s characteristic music on (a hit record back then that made Karas a star).\nThose not yet or no longer all that familiar with the film could go and watch it (in the English original) at the Burgkino cinema on Opernring 19 (showing the film usually on Sundays and Tuesdays at 4 in the afternoon, sometimes also on Friday night (best check ahead at www.burgkino.at).\nThe ""Third Man"" connection could also additionally justify a trip out to the Prater funfair with its iconic Ferris wheel, which also featured in the film. It\'s quite heavily touristic, but why not go on it all the same and when at the top spare a few thoughts about morals, penicillin, Italian bloodthirst and cultural icons and/or Swiss democracy and cuckoo clocks … for those seeking a more immediate adrenaline kick, the funfair also offers some real white-knuckle rides of the highest order!\nFor other dark sites not related to the ""Third Man"" in any way see under Vienna\nCombinations with non-dark destinations:\nThe access hatch to the sewer tour is only steps away from some of Vienna\n\'s best sights. One of them is the Naschmarkt. You may not immediately feel like it, perhaps, after the smells of the sewers, but a visit to the food market and bar/restaurant strip that is the Naschmarkt is a firm favourite amongst Vienna\'s visitors and locals alike.\nAt one end of the market, closest to the meeting point for the tour, stands the Secession building, arguably the finest example of Vienna\'s rich art nouveau (Jugendstil) architectural legacy.\nOn the more baroque front, the imposing domed church Karlskirche with its two unusual, almost minaret-like columns is certainly worth a look – and is only a few hundred yards east of the tour meeting point.\nJust north of it is one of the grandest stretches of Vienna\n\'s famous Ring street, by the Opera and some of the city\'s most famous hotels (e.g. the Sacher).\n- Third-Man-Tour A01 - meeting point on traffic island\n- Third-Man-Tour A02 - starting from here\n- Third-Man-Tour A03 - open-air objects\n- Third-Man-Tour A04 - Harry Lime marker\n- Third-Man-Tour A05 - descend down here\n- Third-Man-Tour A06 - sewer and screen\n- Third-Man-Tour A07 - famous sewer\n- Third-Man-Tour A08 - manhole from below\n- Third-Man-Tour A09 - sewer lamp\n- Third-Man-Tour A10 - tourist sewer\n- Third-Man-Tour A11 - another screen over sewer\n- Third-Man-Tour B1 - 2010\n- Third-Man-Tour B2 - new wall projections\n- Third-Man-Tour B3 - new colourful lighting\n- Third-Man-Tour B4 - it is still a sewer\n- Third-Man-Tour B5 - rose-tinted']"	['<urn:uuid:cd284ffc-77d0-4381-a7be-f567de77e240>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	8	98	2518
86	What steps are being taken to balance the free sharing of research papers with protecting the rights of publishers and authors in online platforms?	Several steps are being implemented to maintain this balance. ResearchGate has formed a coalition with major publishers like Springer Nature to educate users about their rights while protecting copyrights. The platform monitors and removes infringing content while notifying publishers when their content is shared. Additionally, authentication systems are used to control access, including login credentials and IP filtering, while digital watermarking and signatures help track and verify content ownership. These measures aim to enable knowledge sharing while protecting intellectual property rights.	"['Academic Publishers Form a Coalition With ResearchGate\nAfter months of conflict relating to copyright and open access infringement, ResearchGate appears to be on the road to recovery. It has recently signed an agreement to collaborate with giant publishers like Springer Nature, Cambridge University Press and Thieme. Such an alliance would ensure the scholarly article platform consistently protects the rights of publishers and authors. How exactly will this agreement, change or resolve the conflicting recent past of the platform? Let us read ahead to find out.\nCopyright Infringement and ResearchGate\nResearchGate is an academic community platform that allows researchers to upload their publications as published in academic journals. It allows broader engagement with the scholarly content while improving the citation index of researchers. However, most of these ResearchGate publications have copyright specifications preventing their open-access availability. As the research community engaged on the platform increased, the number of publications that infringed copyright reached a massive scale. These several issues of copyright infringement led to a backlash by the publishing industry. This, in turn, resulted in the establishment of the coalition for responsible sharing. However, attempted negotiations between the publishers, including Wiley and Elsevier with ResearchGate did not culminate in a productive outcome. The situation progressed towards legal action against the academic platform.\nTerms of Agreement Between Publishers and ResearchGate\nThe newly announced cooperation agreement between publishers and ResearchGate aims to educate all users of their rights as authors. Besides that, it also aims to make the publishers aware of their responsibility to secure copyrights. In accordance with the agreement, ResearchGate will remove content infringing copyrights from its site. Likewise, publishers will be notified when any publication from their journals is archived on ResearchGate.\nMoreover, this agreement will take a step in the right direction by creating an educated user-content platform, as opposed to asserting rules. Accordingly, ResearchGate will monitor the site for any article that violates this agreement, promptly taking down content to prevent disagreement.\nPush Notifications for the Publication Industry\nAs mentioned before, ResearchGate will notify publishers when their content becomes available on the site. Visibility to copyright content shared on the platform will provide transparency that was sorely lacking on ResearchGate to begin with. This agreement does not resolve all publication conflicts, including the rights to ensure open-access for all research publications. However, we see a rise from ongoing dispute to form plausible arrangements among the users, for scholarly research access.\nThis constructive effort comes on the heels of a few other such collaborative efforts among publishers for journal-based research access. Recently, Figshare an online repository for research data, collaborated with Springer Nature to launch a new tool supporting open research. This new tool allows authors and journals to follow good practice in data-sharing and research archival, resembling current collaborations with ResearchGate. Building up on the previous research data, the service provides a platform to submit/store private datasets securely. Authors are also able to provide open access to their research data, according to the Creative Commons attribution license.\nMoving in the Right Direction in Research\nIn academic publications, the authors (researchers) strive to ensure higher visibility for scholarly publications. The coalition with ResearchGate allows researchers and publishers to find common grounds to disseminate knowledge while increasing article-based citations for career advancement. Forming an alliance to share research-publications does not resolve all related conflicts, yet forms a first-step in the right direction. It shows that a common ground of shared responsibility can be found among the individuals involved, enabling strengthened research commitments.\nHow far do you think this agreement will be successful in its fight against copyrigt infringement? Please share your thoughts with us in the comments section below.', ""Authentication and Authorization: Security Issues for Institutional Digital Repositories\nIn this digital age and the development of Information and Communication Technology (ICT) many organizations have realized the benefits of sharing information within the organization as well within the community and globally. These organizations may be corporate company, research organization or academic institutions. In the academic institutions with the higher education, information capturing, dissemination and sharing is practiced most. In spite of Open Source Drive , in the highly competitive environment, many university or colleges raise a paradox between allowing information and knowledge to flow freely, and the need to keep certain information very secure. In restricted or closed-information environment secured information channel, authorization and authentication of both users and digital contents are a burning issue today. Digital contents are managed and stored in repository to share. Repository of an institution can support research, learning, and administrative processes as well as purposes. Standards are followed for the repositories which ensure that the contents contain is accessible in that and it can be searched and retrieved for later use. A wide variety of contents may be included in the digital repositories for the multiplicity of purposes and users. It is the technical ability and administrative policy decision that what kind of materials goes into a repository (Jones, et al 2006). A proper digital repository not only requires an organized collection of digitized content, it also requires that the content be accessed and distributed as widely as possible to legitimate users around the globe. Access management and control is one of the major concerns for content-providers on the Internet. Without a proper access management mechanism confidentiality and integrity of information cannot be guaranteed. Different conventional methods are practiced by the content-providers but not a single method is sufficient for access management (Ray and Chakraborty, 2006). However, the administrators of the digital content-providers mostly expect their preferences for the technology or the procedure to be available which may be best practiced globally.\nInitially, substantial amount of literatures have been reviewed to come up with an idea for formulating this paper which is a review by type. The researches, practices, progresses, development and successes for the access management specially authentication and authorization are reviewed to see the global practices by the repository administrators or managers. Even in Bangladesh there are very few repositories, and the repository managers are interviewed by the author though all are examined also to see the status. Though there are very common types of process or methods observed where traditional or built in securities of repository software or operating systems are adopted most. Based on review and local managers interview this paper gives idea about the current practices about authentication and authorization.\nAccess management typically is a combination of users' authentication and authorization, access permission operations, policies for license agreement and digital materials authentications or digital rights management. Authentication is the process of determining the validity of a user who claims to be, and authorization is the process of determining what resources a user is permitted to access. Digital Rights Management (DRM) is a system of solutions created or designed as a means to prevent unauthorized access, duplication and illegal distribution of copyrighted digital media. In online environment, the scope of DRM can be leveraged to control access to and usage of digital objects and to impose restrictions on their misuse (Functional Groups, 2009). Access Management ensures security of resources on servers but also during communication to ensure authenticity and integrity of data. It is possible for an unauthorized user to snoop on communication between a user's browser and a Web server and hack sensitive information. Occurrences of unauthorized user getting access to important Web sites and defacing them are not uncommon. Electronic content can be copied very easily, it is essential to impose measures to control misuse of digital content. IP authentication and password-based access, two most commonly used authentication methods, are not able to protect the content from being duplicated or shared. Access Management is necessary most for commercial digital contents because their access is restricted to its subscribers or licensed users. Even when access to digital collections is provided openly, access control is required for assigning responsibilities for operations such as, additions, updating, editing and deleting or with-drawing content, and other tasks related to digital collections. Other reasons to control access to materials in a repository may include confidentiality of resources. Tracking of all changes made so that the collections can be restored if any system error occurred. In access management as other matters are related to policy or administrative decisions the user authentication, authorization and digital material authentication are most necessary issues.\nUser Authentication: User Validity\nA user authenticates with his or her organizational or personal identification. The identity provider passes the minimal identity information necessary to the service manager for authentication to enable an authentication decision (ACM, 2009). Digital identities are increasingly being used to facilitate the execution of transactions in various domains. When developing and analyzing digital identity technologies, it is important to consider the type and objective of repository, type of digital content, security of the system, security of communication channel, diversity of users' platform, number of users, even the perceptions and responses of end users also. Different authentication processes are as follows:\nThe most common and familiar authentication process is Log-in ID and Password-based Access (Antón, 2007). Log-in is also called log on, sign in, or sign on which identifies oneself to the system in order to obtain access. The primary use of a computer login procedure is to authenticate the identity of any computer user or computer software attempting to access the computer's services (Logging, 2002). Another popular authentication process is IP Filtering or IP authentication. This process is a packet filter that analyzes TCP/IP packets. That is software routine that analyzes incoming data packets and forwards them or discards them based on one or more criteria such as address, range of addresses and types (IP filter, 2009). Institutions or organizations are encouraged to register for accessing digital contents using IP addresses (ranges) if they are static. This allows for: seamless access (no logon screen), usage statistics for the institution, greater security as no misuse of usernames and passwords, can allow for access for all computers on campus to resources and much more (INASP, 2009). Web Cookie is another process of user authentication, which can be used by a server to recognize previously-authenticated users and to personalize the web pages of a site depending on the preferences of a user (Cookies, 2009). A cookie is a token that the web browser stores on disk in the form of a small text file. Cookies provide a way to track individual users' usage of website. Web Proxy is another way to authenticate. Most proxy programs like Squid, NetCache provide a mechanism to deny access to certain URLs in a blacklist, thus providing content filtering. A content filtering proxy will often support user authentication, to control web access. EZproxy (2008) is also a web proxy server program that provides users with remote access to Web-based licensed content offered by libraries. It is middleware that authenticates library users against local authentication systems and provides remote access to licensed content based on the user's authorization (Proxy server, 2008). Another method Challenge-Response Authentication is used to prove the identity of a user logging into the network. When a user logs on, the network access server, wireless access point or authentication server creates a “challenge,” which is typically a random number sent to the client machine. The client software uses its password or a secret key to encrypt the challenge via an encryption algorithm or a one-way hash function and sends the result back to the network (the “response”). The authentication system also performs the same cryptographic process on the challenge and compares its result to the response from the client. If they match, the authentication system has verified that the user has the correct password (Challenge-response authentication, 2008). Referring URL is a way to authenticate users. From the point of view of a web page or resource, the referrer, or HTTP referrer, identifies the address of the webpage or URL, the more generic URI of the resource which links to it. By checking the referrer, the new page can see where the request came from. referrer logging is used to allow websites and web servers to identify where people are visiting them from, for promotional or security purposes. referrer is a popular tool to combat Cross-site request forgery, but such security mechanisms do not work when the referrer is disabled (HTTP referrer, 2008). referrer is widely used for statistical purposes.\nUser Authorization: Resource Access Permission\nAuthorization defines users' permissions in terms of access to digital resources and extent of its usage. Authorization is granted to the successfully authenticate users according to his/her rights information available in the Access Management System (AMS) (Lynch, 2009). Authorization also addresses the issue of responsibilities assigned to different personnel involved in development of a digital repository/library and their respective authorities in terms of addition, deletion, editing and uploading of records into a digital collection. Authorization is more challenging than authentication, especially for widely distributed digital content providers.\nConventional access control architecture denotes an access control policy as a subject (user) is authorized to exercise some permission on an object. This usual model implicitly assumes that the user population is known more or less. But in a digital content environment the user population is vast, dynamic and impossible to predict all the users. Thus conventional authorization or access control mechanisms that rely on knowing the user and associating permissions with them fail significantly in digital repositories. So, this digital environment demands some further challenge for access control (Bertino, 2002). The access control policies are often based on user qualifications and characteristics. In one of the early works on access control in digital repositories or libraries, Gladney (1997) proposes a scheme called DACM (Document Access Control Methods), where the basic idea is geared toward flexible access control with some extensions to handle mandatory access control. Blaze has proposed credential-based access control (Blaze, 1996), to address the problem of unknown users. In these models a user has to produce one or more testimonials that have been certified by one or more third parties. The credential provides information about the rights, qualifications, responsibilities and other characteristics attributable to its bearer by the third parties. These third parties need to be trusted by the service provider. Winslett (1997) developed a credential based security and privacy related system for enforcing access control in digital contents of repository or system. Access to systems containing protected information resources must be managed based on one or multiple selections of the alternative access control methods. However, different methods are based on, Users identity, Role, Policy, Content Dependency, Context, View, Time, Physical Location, Network Node, Mandatory, and Discretionary. In-addition, a risk assessment is needed to conduct to identify the data or resource risk and severity prior to establishing the level and selection of access controls or authorization to digital contents (Access control, 2009).\nDigital Materials Authentication\nDigital Watermarking and Digital Signature are very common in use to provide a range of solutions for identifying, securing, managing and tracking digital materials (Stallings, 2003). In different types of objects like audio, video, still images and printed documents Digital Watermarking technologies allow users to embed. This technology permits digital code that is imperceptible during normal use but readable by computers and software. The major purpose of digital watermarks is to provide protection for intellectual property that is in digital format. This system does not prevent copying, but ensures that any copies made of the media will be traceable to a particular copy and perhaps to a particular user. In this process, also referred to as data embedding, information hiding, or simply watermarking, a pattern of bits is inserted into a digital image, audio or video file that identifies the file's ownership and can convey additional information like copyright. Unlike printed watermarks, which are intended to be somewhat visible, digital watermarks are designed to be completely invisible, or in the case of audio clips, inaudible. Moreover, the actual bits representing the watermark must be scattered throughout the file in such a way that they cannot be identified and manipulated. And finally, the digital watermark must be robust enough so that it can withstand normal changes to the file, such as rotation, filtering or the application of compression algorithms such as JPEG that discard some of the original data (Watermarking, 2009). On the other hand Digital Signature is an electronic signature which authenticates to identify the sender of the message where original document has been sent remain unchanged. It is easily transportable, cannot be reproduce by someone else, and can be automatically time-stamped. In addition the message is sent, the sender cannot easily reject it later. A digital signature can be used with any kind of message, whether it is encrypted or not, simply so that the receiver can be sure of the sender's identity and that the message arrived intact. A digital certificate contains the digital signature of the certificate-issuing authority so that anyone can verify that the certificate is real (Digital Signature, 2009).\nA digital signature scheme typically consists of three algorithms:\nb. A signing algorithm which, given a message and a private key, produces a signature.\nc. A signature verifying algorithm which given a message, public key and a signature, either accepts or rejects.\nTwo main properties are required. First, a signature generated from a fixed message and fixed private key should verify on that message and the corresponding public key. Secondly, it should be computationally infeasible to generate a valid signature for a party who does not possess the private key (Brands, 2000) .\nAs institutions, information strategies which call for sharing and licensing access to information resources in the networked environment, authentication and access management have emerged as major issues which threaten to hinder progress. While considerable work has been done over the last two decades on authentication within institutions and, more recently, in support of digital repository, a series of new technical and policy issues emerge in the cross-organizational authentication and access management context. In this paper it has been illustrated the secured process digital repository which ultimate objective is information sharing and dissemination. Though it has not shown any model or architecture regarding any types of technical aspects whether it has been tried to introduce the terms and methods or even the factors for access management for digital content provider. A lot of work, however still remains to be done. In future the authors will work on few open source technology for access management of digital library by which the digital content provider can find a solution to gather the working process of those for implementation.\nAbout Digital Watermarking. Available: http://www.willamette.edu/wits/idc/mmcamp/watermarking.htm\nAccess control criteria for right to use automated information resources. Available: http://michigan.gov/documents/Policy_1350_157471_7.40_Access_Control_Final_PDF.pdf\nAntón, L., Jones, A., Earp, J.B. (2007). Towards understanding user perceptions of authenticationtechnologies. Proceedings of the 2007 ACM Workshop on Privacy in Electronic Society. Virginia : ACM.\nBertino, E., Ferrari, E., & Perego, E. (2002). Max: An access control system for digital libraries and the web. Oxford, UK: Proceedings of the 26th IEEE International Computer Software and Applications Conference.\nBlaze, M., Feigenbaum, J., & Lacy, J. (1996). Decentralized trust management. Oakland, CA: Proceedings of the 1996 IEEE Symposium on Security and Privacy.\nBrands, S.A. (2000). Rethinking public key infrastructures and digital certificates: Building in privacy. London: MIT Press.\nChallenge-response authentication definition. Available: http://encyclopedia2.thefreedictionary.com/Challenge-response+authentication\nEZproxy: OCLC – Web and Data Services. Available: http://www.oclc.org/us/en/ezproxy/default.htm\nFAQs on Information resources. Available: http://www.inasp.info/file/188/faqs-on-information-resources.html\nFunctional groups: Access management R & D. Available: http://www.inflibnet.ac.in/functionalgroup/openaccess.html\nGladney, H.M. (1997). Access control for large collections. ACM Transactions on Information Systems 15: 154–194.\nHTTP referrer. http://en.wikipedia.org/wiki/Referrer\nIP filter definition of IP filter in the free online encyclopedia. Available: ttp://encyclopedia2.thefreedictionary.com/IP+filter\nJones, R., et al. (2006). The Institutional Repository. Oxford: Chandos.\nLogging (computer security). Available: http://en.wikipedia.org/wiki/Logging_%28computer_security%29\nLynch, C. (n.d.). A white paper on authentication and access management issues in cross-organizational use of networked information resources. http://www.cni.org/projects/authentication/CNI_authentication.doc\nProxy server. http://en.wikipedia.org/wiki/Web_proxy\nRay, I., & Chakraborty. S. (2006). A framework for flexible access control in digital library systems. Data and Applications Security : 252–266.\nStallings, W. (2003). Cryptography and network security: Principles and practice . New Delhi: Pearson Education.\nTowards understanding user perceptions of authentication technologies. http://portal.acm.org/citation.cfm?doid=1314333.1314352\nWeb handbook – Cookies. Available: http://archive.cabinetoffice.gov.uk/e-government/resources/handbook/html/4-7.asp\nWinslett, M., et al. (1997). Assuring security and privacy for digital library transactions on the Web: Client and server security policies. Proceedings of the IEEE international forum on Research and Technology Advances in Digital Libraries, Washington , DC , USA , pp. 140-151.\nWhat is digital signature? – a definition from whatis.com. Available: http://searchsecurity.techtarget.com/sDefinition/0,,sid14_gci211953,00.html""]"	['<urn:uuid:fde063ce-245d-4f86-9426-ada93338f7ea>', '<urn:uuid:78455cbc-3b75-43c3-8b02-5b9a48f269be>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T10:32:05.666290	24	81	3401
87	marine life hazards conservation measures	Marine life faces multiple threats from plastic pollution, including direct ingestion of debris and entanglement in fishing gear. To combat these issues, various conservation measures are being implemented. On an individual level, sailors are advised to avoid plastic utensils, sort waste, use biodegradable detergents, and actively participate in beach cleaning. Conservation efforts for sea turtles in the United States have shown positive results over 40+ years. However, challenges remain - for instance, research has documented cases of sea turtles entangled in fishing lines and affected by plastic debris, while conservation programs continue working to protect various species, from radiated tortoises to green sea turtles, against both direct threats and habitat degradation.	"['Desert tortoises range from the Mojave and Sonoran deserts in California and can also be found in Arizona, Nevada, and areas in Mexico.\nAlligator snapping turtles in the state of Oklahoma are a species of special concern.\nThere have been fewer Blanding\'s turtle deaths after the fencing was installed in 2001.\nFibropapillomatosis is wreaking havoc in sea turtles around the world.\nA city council in Wodonga, Australia drew down water in a pond that inadvertently caused the death of nearly 25 broad-shelled and long-necked turtles.\nEven though the radiated tortoise is captive bred, the critically endangered species is still poached from the wild.\nElseya rhodini is of the side-neck turtle family Chelidae\nOnce in captivity, desert tortoises can no longer be released into the wild due to the potential for them to introduce disease to wild populations of tortoises.\nMore than 300 species rely on the network of tunnels the gopher tortoise creates.\nConservationists with the Fisheries Administration (FiA), WCS in the Southeast Asian country found 16 eggs in a nest of the critically endangered royal turtle.\nResearchers have found that dead cane toads placed on top of sea turtle nests deter monitor lizards from pilfering the eggs.\nGopher Tortoise Day is a fun informational day for the entire family.\nTurtle smuggler sent endangered reptiles via mail.\nThere is a thriving black market for sea turtle eggs in South Florida.\nSea turtles that were found cold-stunned in early January have been released by Clearwater Marine Aquarium in Florida.\nAlthough the decimation of some African chelonians has been well documented, many species are not listed on the IUCN Red List because there is almost no information about them\nScientists Use Drones To Count Olive Ridley Sea Turtles At Costa Rica’s Ostional National Wildlife Refuge\nDrone footage revealed that sea turtle density during peak nesting season was about 2,086 reptiles per square kilometer.\nEast Coast Population Of Leatherback Sea Turtles May Be Downgraded From Endangered To Threatened Under ESA\nThe leatherback sea turtle is currently listed as endangered.\nIn spite of the best efforts by staff at the Oregon Coast Aquarium, Turkey died January 8.\nThe woman was told several times to move off the sea turtle.\nThis sulcata tortoise was returned to its owner after a day gallivanting around the streets and canals of Mesa, Ariz.\nFemale green sea turtles on Raine Island outnumber males by 116 to 1.\nWhen turtles become cold-stunned, they are too cold to swim and float on the surface, where they can become dehydrated.\nQuang Ngai beaches on Ly Son Island have been breeding grounds in the past for green sea turtles), hawksbill turtles, and leatherback turtles.\nSpike was ran over by a car in June 2016. He has since healed and can walk more than three hours a day.\nMonitor Lizards Now No. 1 Predator Of Loggerhead Turtle Eggs On Second Most Popular Nesting Beach In Australia\nResearchers are unsure how the large lizards discover the nests, which are on the second most popular nesting beach for loggerhead turtles.\nTourists depicted in an Airbnb ad are shown touching a green turtle and got a magical experience, while Airbnb got chastised and was forced to pull the ad.\nResearchers had expected the disoriented turtles to expend more energy than those collected from the nests, but found that the hatchlings were very proficient in determining when to take a rest\nThe intermediate musk turtle sits morphologically between the stripe neck musk turtle and the loggerhead musk turtle.\nMalayemys isan, a species new to science, was discovered in a Thai fish market.\nThis sea turtle was entangled in bales of cocaine. It was freed by the U.S. Coast Guard.\nThe Asian giant softshell turtle is currently listed as endangered by the IUCN as it has largely disappeared in much of its natural range.\nA three year study turned up just 50 spotted turtles out of 2,000 turtles caught.\nThese highly aquatic turtles are found in both slow-flowing sections of stream and river habitats, as well as lakes and ponds. They move clumsily on land.\nThe stranded olive ridley sea turtle had a body temperature of 59 degrees Fahrenheit. A temperature of 75 degrees Fahrenheit is considered normal.\nFlorida recorded 39,000 green sea turtle nests during the 2017 nesting season.\nThe survey documented 72 mature tortoises in four hours.\nDrifter was found entangled in fishing line that was attached to a buoy in the Florida Keys.\nIt is hoped the unlimited collection of wild turtles is stopped or curtailed in Texas.\nThe last time an alligator snapping turtle was seen in the wild in the state of Illinois was back in 1984.\nThe dead turtles were already in a state of decomposition.\nWould there be a moral dilemma if these little guys were raised in captivity?\nJonathan and Frederic(a) are inseparable, even though they are both males.\nCarlos Argenis Hernandez was sentenced to 30 days in jail for knowingly disturbing a sea turtle, an endangered species.\nData models show that the state\'s turtle populations cannot sustain any more losses.\nConservation efforts over the last 40+ years of sea turtles in the United States are paying off.\nScientists hope to have a population of 250 adult gopher tortoises in the Aiken Gopher Tortoise Heritage Preserve.\nIt is estimated that there are about 100 Sonyota mud turtles in the wild.\nCurrently there are no limits to the number of turtles collected in the wild in Arkansas.\nA breeding program at the Galapagos National Park in the Galapagos Islands will hopefully result in Floreana giant tortoise offspring.\nHurricane Irma serves as a reminder that an entire nesting season could have been disrupted had the storm hit in July.\nWhen the person who shot the video confronted the resort staff, they said the turtles don\'t have enough food.\nMore than 1,040 western swamp tortoises have been successfully bred as part of Western Shield, a breeding program to bolster the wild populations of the tortoise.\nAlthough the population has reached 1,000, the Western pond turtle remains an endangered species in Washington state.\nThe chance of survival into adulthood for this little turtle is slim.\nOfficials have since removed the blue paint and released the tortoise back into its burrow.\nTwo Aussie blokes with beer stand on top of a sea turtle, post the photo to social media.\nThe find is significant because the Blanding’s turtle is endangered\nBlatant case of animal abuse occurred in Florida.\nThe natural urban lake went through a decade long cleanup process.\nOlga Jumenes was no stranger to law enforcement, the U.S. Attorney\'s office said.\n139 radiated tortoises and seven angonoka tortoises were smuggled from Madagascar and discovered in India.\nThe turtle was caught and released by a man fishing for catfish.\nAll sea turtles whether dead or alive are legally protected in Hawaii\nMore than 900 eggs were seized by the U.S. Fish and Wildlife Service in November 2014.\nDozens of turtles and countless tadpoles saved after a pond was drained in Tulsa.\nAlbino green sea turtles are rare.\nWheels spent 165 days in Captivity and was released all healed up\nJanuary 20, 2016\nPlease reduce your use of plastic items that are ""disposable.""\nAaron Culling buys sea turtles from meat markets and releases them.\nEmydoidea blandingii may get some help from the Friends of Cobus Creek in Indiana\nConcrete ditches help prevent turtles from getting stuck in the switch track on a Japanese train track.\nKai Xu had 51 live turtles in his possession when he was caught trying to cross into Canada.\nPlastic is literally choking the world’s ocean inhabitants.\nPlastic bags and plastic pollution kills and maims hundreds of thousands of reptiles and other animals every year.\nLeatherback sea turtles primarily eat jellyfish and plastic tends to look like jellyfish in the ocean.\nNovember 2, 2015\nThere are two species of Galapagos tortoise on Santa Cruz Island.\nThe suspects allegedly had Palawan forest turtles, Asian leaf turtles and Southeast Asian box turtles in their possession.\nThe hapless tortoise walks into the path of Enid\n27 of the 206 pond turtles have since died.\nAustralian researchers built a device out of a sun shirt, some velcro and a custom poop collector to collect poop samples.\nOne of the women allegedly rode the turtle all the way to the water.\nDivers were shooting biofluorescent corals when a Hawksbill sea turtle swam into the light and revealed it too was biofluorescent.\nCosta Rican Govt. upset that tourists disrupted sea turtle nesting with selfies and other bad behavior.\nThe species is nearly extinct in the wild, but is mass farmed for the food industry in China.\nAlbino green sea turtles are not common, though several have surfaced around the world over the years.\nIf suspects who destroyed the nests are found, fines of up to $100,000 and jail time is possible.', 'Virgin nature, crystal-clear sea and a rich marine life - these are the greatest attractions that truly capture our heart. The ocean is the most beautiful element for us sailors. But in just a matter of years, will our boats be accompanied by plastic bottles instead of adorable dolphins?\nThe problem, of course, is not just about us sailors. The oceans and seas cover more than 70% of our planet\n. They supply half our oxygen\nand absorb up to one third of the carbon dioxide.\nThey are home to millions of animals and a source of livelihood for countless people\n. Almost 2.4 billion people live within 100 kilometers of the coast.\nIn the future, will our planet still be blue or just full of plastic?\nCar tyres dumped on coral reef in sea\nIs plastic really a problem?\nEach year, more than 8 million tons of plastic\nreach the ocean. According to a study published in March 2015 in the magazine Science\n, this number may be higher - up to 14 million tons per year. This is equivalent to dumping a full truck of plastic into the sea every minute. To put a specific figure to the total, imagine over 200 billion plastic bottles per year.\nEvery single year, plastic waste kills a million seabirds\n, 100,000 marine mammals, sea turtles and untold numbers of fish. Plastics remain in the ecosystem for countless years and are harming marine animals every day.\nPlastics account for up to 80% of all waste in the seas. According to some estimates, at the rate plastic products are being discarded, by 2050 there will be more plastic in the sea than fish\nand an estimated 99 percent of seabirds will have fragments of plastic in their bowels.\nHow plastic waste reaches the sea\nGarbage and wastes on the beach\n- 80% of all sea pollution is as a result of land-based activities. Asia accounts for more than 63% of the plastics discarded - more than a quarter produced by China and to a large extent by Indonesia, the Philippines, Thailand and Vietnam. The US is the highest ranking developed country in the 2015 ranking in 20th place.\n- According to a German study, more than 90% of the plastics that end up in the sea are brought there by ten large rivers flowing through densely populated areas. Eight of them are in Asia and two more (the Nile and Niger) in Africa.3 The problem, albeit much smaller, is also from Europe. The Danube alone annually collects and then releases approximately 1,700 tons of plastic into the sea.\n- Plastic waste, however, can also get into the sea during a natural disaster. A study, published in the journal Scientific Reports, estimates that the 2011 earthquake in Japan swept plastic waste into the sea which made up 20 percent of the total.\nHow much plastic is actually floating in the ocean?\nSince the invention of plastic, over 8 billion tons\nof it have have been produced worldwide. At present, around 300 million tons of plastic\nis produced annually, 40% of which is packaging (the forecast for 2020 is 400 million tons). However, recycling systems cannot keep up with this expansion in demand and the problem is compounded by the current cut in recycling\nEstimates suggest that only 9% of plastic is recycled, another 12% burned, and the remaining 79% of plastic waste pollutes the environment\n. If the current trend continues, the Earth will have generated around 34 billion tons of plastic by 2050.4\nDetermining exactly how much plastic waste is floating in the oceans is not easy and not even exactly possible. One frequently cited study 20135\nestimated the total amount of plastic in the sea to be only 269,000 tons\nOceanographer Marcus Erikson, along with a group of scientists, had carried out 24 expeditions in 2007 and 2013. They collected data on key water streams and the amount and size of plastics\n. However, they only focused on plastics that float on the surface.\nAs a result, this number is far from accurate as it wasn’t possible to include waste that no longer floats\n. Such waste is even more dangerous. We now also know that every year there are 8 million more tons of plastics. Where does all this plastic disappear to?\nWhere plastic waste accumulates\nA total of 57% of plastic waste is floating in the northern hemisphere. And most of it is in the northern Pacific. There lies the so-called Great Pacific garbage patch\n. But it isn’t the only one.\nThere are six similar artificial islands floating in the world’s oceans. But mapping and chronicling them is almost impossible. Due to ocean currents their shape, size, density and even location are constantly changing.\nSimilar patches of waste can also be found in the Atlantic and Indian Ocean. They are also beginning to appear in smaller areas, such as the North Sea, Greenland and the Barents Sea.\nAccording to estimates, 300 billion tiny pieces of plastic are currently floating in the Arctic.\nThe Atlantic Ocean currents bring waste there mainly from North America and Europe.\nPlastic sea pollution\nEven the deepest depths and uninhabited virgin beaches are affected\nSome other studies estimate that up to 70% of plastic waste will end up on the seabed and in the ocean depths. Even at the deepest natural point on the planet, the Mariana Trench\n(nearly 11 km deep), researchers found a plastic bag.\nUnspoilt deserted bays are not immune to plastics. In 2015, Jennifer Lavers, along with researchers from the Institute for Marine and Antarctic Studies\n(IMAS), discovered 20 tons of plastic waste on the uninhabited Henderson Island in the Pacific Ocean\nIn fact, at one place, 672 pieces of waste per square metre were counted.\nHenderson Island is on the list of UNESCO protected areas, because of the lack of drinking water it is uninhabited, and so not directly impacted by human activity.\nWhat waste is most encountered at sea?\nThe authors of the study, Plastic Pollution in the World\'s Oceans, recorded what waste ends up in the seas\n. Often they encountered roll-on deodorant balls, toothbrushes, buckets, bouncy balls, plastic bottles and beach footwear.\nDisposable plastic packaging\nis the most common item found on beaches. Other items to be found included drink bottles, straws, disposable shopping bags, ladies\' sanitary pads, tampons, cotton earbuds, condoms, cigarette butts and disposable lighters.\nFishing equipment often ends up in the sea, the so-called ""ghost nets""\n. Forgotten, lost or otherwise discarded fishing equipment accounts for up to 10 percent (640 thousand tons) of all marine waste.\nIn 2004 members of the project GhostNets Australia found and collected over 13,000 lost fishing nets in the area to the north of Australia. A study published in the journal Conservative Biology\nshows that 4866 to 14,600 turtles\nwere caught in these ""ghost nets"".\nGreen sea turtle entangled in a discarded fishing net\nHow does waste kill animals?\nAccording to the United Nations, every single year, plastic waste kills up to a million seabirds, 100,000 marine mammals, sea turtles and countless fish.\nThe Internet has long been flooded with images of tortoises spewing up plastic bags, and seahorses holding onto cotton earbuds. And the evidence is mounting.\nIn the autumn of 2018, Cetaceans washed up dead on the beach of the Indonesian island of Sulawesi had nearly 6 kg of plastic in their stomachs\n. Among other things, flip-flops, plastic bottles, shopping bags, more than a hundred disposable cups and over thousands of plastic fragments.\n- It is reported that more than 40% of the existing whale, dolphin and porpoise species, all sea turtle species and approximately 36% of seabirds have ingested marine litter . Afflicted animals have their stomach filled with plastic debris and then literally die of starvation.\n- Fish, turtles, seabirds and mammals are captured in old fishing gear in so-called ""unwanted catches."" According to the non-profit organization World Animal Protection, this kills 100,000 whales, fish, seals, turtles and other sea life.\n- Plastics in the water also causes harm in another way. It acts as a magnet for oily and dangerous substances, which poison the fish and then the person whose plate they end up on.\n- There are some chemical substances contained in plastics that act as a poison, weakening or killing marine animals. It can be carcinogenic or negatively affect reproductive organs, which further threatens the population of fish, birds and other animals.\n- Floating waste can also serve to spread invasive species.\n- In many areas, plastic concentrations are up to seven times higher than zooplankton concentrations, as demonstrated by research from Algalita, an independent California-based marine research institute.\nThe plastics that most pollute the seas and oceans are not even visible. About 92%\nof the more than five trillion pieces of plastic waste floating on the surface form so-called microplastics\n. These are small particles with a diameter of up to 5 millimeters. They pose a serious problem not only at sea, as they can be found in large quantities in drinking water.\nThe toughest race in the world the Volvo Ocean Race is an agreeable combination of sport, science and ecology. A global map of microplastics\nwas created at the event itself. According to this map, the highest levels were measured in the South China Sea, with an incredible 349 particles per m3\n. Even European waters are problematic with 307 particles per m3\nMicroplastics are hidden in up to 80% of tap water.\nAnd how do they get into the water? One possible way is from washing clothes which contain microfibres. These are such materials as fleece, nylon, waterproof fabrics and fabrics used to produce sportswear. Microplastics are also contained in cosmetics, such as various skin peels and shower gels.\nTime to change\nThe oceans are also struggling with a host of other major problems\n. Numerous toxic substances\nin the water threaten to cause irreversible damage to the balance of the ecosystem. An urgent problem is unregulated fishing and overfishing\nof some species. But the biggest threat to the ocean is undoubtedly climate change\nA number of non-profit organizations\nand projects are dedicated to protecting the oceans, education and training. National authorities and the international community are aware of the problem and have been discussing possible measures.\nDream scene with beautiful white sand beach\nWhat can sailors do to help?\nCan sailors make change themselves? Surprisingly it’s very easy. Below we have handy tips for novices that won’t complicate life aboard the boat\n. Every step counts and these tips cost your almost nothing.\n- Do not leave any plastic utensils or bags loose on board. They can easily blow away. If it\'s possible, avoid plastic on the boat altogether.\n- Do not throw cigarette butts into the sea. Besides being plastic, the captured pollutants destroy corals.\n- Sort your waste. Plan stops at eco friendly marinas where they handle waste.\n- Ideally, of course, it best to not produce waste at all. Choose packaging that doesn’t use disposable plastic, use cloth bags and buy in markets locally.\n- Don’t use coral-destroying sunscreen. Replace with creams based on mineral filters, such as zinc.\n- Bring biodegradable, non-perfumed detergent on board. There are several on the market that can handle the greasiest of pots and pans.\n- Try to buy bigger containers rather than small plastic (PET) bottles. They will fit comfortably in the locker and each crew member can fill their own bottle with fresh water.\n- If you have children or love mixed drinks, bring a stainless steel straw instead of a plastic one.\n- Actively engage in beach and ocean cleaning. Whether within a wider group or just on your own.\nThere are dozens of tips and tricks for a wasteless and environmentally responsible behaviour, and most of them can be easily applied to boat voyages. Gradually try more ecologically responsible steps, even if there are still more fish in the ocean than plastic waste.\nNo matter what, you’ll be happy to see the impact of these efforts with your own eyes, whether it be clean white beaches or swimming and snorkelling in a crystal-clear turquoise sea.\n1) Plastic waste inputs from land into the ocean by Jenna R. Jambeck, Roland Geyer, Chris Wilcox, Theodore R. Siegler, Miriam Perryman, Anthony Andrady, Ramani Narayan, Kara Lavender Law dostupné online\n2) Ocean conference\n4) Production, use, and fate of all plastics ever made by Roland Geyer, Jenna R. Jambeck and Kara Lavender Law, dostupné online\n5) Plastic Pollution in the World\'s Oceans\n6) Exceptional and rapid accumulation of anthropogenic debris on one of the world’s most remote and pristine islands, by Jennifer L. Lavers and Alexander L. Bond, dostupné online']"	['<urn:uuid:8667a989-e3a1-4e26-8826-0c3363d5d08a>', '<urn:uuid:5aea5c4b-4ea4-4ac0-a176-3bcb043fd31c>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T10:32:05.666290	5	111	3595
88	I work with industrial control systems and need help understanding both the basic design elements of electric solenoids and their reliability issues - what are the core components that make up a solenoid, and what are the common maintenance problems that affect their performance?	Electric solenoids are built with several key components: a stator assembly (the coil winding), an armature assembly (the moving parts including the central shaft), and return springs for neutral position reset. The shaft is typically made of steel or iron and responds to the magnetic field created by the copper winding. Regarding maintenance issues, common problems include stuck or defective solenoids (which may need contact cleaning), insufficient power due to bad battery connections or corroded terminals, and water damage from submersion which requires thorough drying of windings. Performance can also be affected by long periods of operation causing overheating, and loose connections that need tightening. Regular maintenance should include keeping components free from contaminants and ensuring all electrical connections are clean and tight.	"[""Learn something new every day\nMore Info... by email\nAn electric solenoid is a type of switch that is used to control a wide variety of mechanical processes. It is similar in construction to an electric motor with the exception that the metallic central shaft of the solenoid is moved in and out of its enclosure by the application of a magnetic field that is created when electrical power is applied to a copper winding that surrounds the shaft. Types of solenoids are used in automobiles for various purposes, such as mechanical clutches to engage a starter motor assembly that meshes with a flywheel, as controls for vacuum and air valve systems, and in fuel injectors.\nMiniature versions of solenoids are built into doorbells to engage a small armature that rings a bell when the doorbell button is pushed, and in thousands of other micro-control systems. Other precise applications include for pinball machine flipper controls and for door locks and controls to close some doors automatically. Small electric solenoid components have very little force that they are capable of applying, so usually they act as an electric solenoid actuator which engages stronger mechanical systems to close doors or otherwise move heavy objects.\nThe purpose of an electric solenoid centers around any need for rapid mechanical control of a system though the application of electrical power. This gives it a diversity of uses in both heavy-duty machinery and fine electro-mechanical circuits. A solenoid's parts are essentially simple electromagnet parts consisting of a copper coil winding that produces a magnetic field when electrical power is applied to it, and a central movable shaft, usually made of a magnetic metal like steel or iron. Through the principle of induction discovered by Michael Faraday in 1831, the coil winding produces a magnetic field that pulls or pushes on the shaft. The shaft is usually spring-loaded to keep it in a non-contact position until power is applied to the electric solenoid, and, once it has done its job as an actuator, power is cut to it and it withdraws to a neutral position in the mechanical assembly.\nSince using an electric solenoid depends entirely on the application, they vary greatly in size and power needs. Solenoids can be powered either by alternating current (AC) or direct current (DC), as both will produce induction in a wire winding. The mechanical components that operate the shaft are usually referred to as the armature assembly and make up the moving parts in the electric solenoid. The axial stroke is the amount of distance that the shaft will move when activated, and usually ranges from between 0.022 to 0.1 inches (0.559 to 2.54 millimeters). Return springs act as a mechanical means for resetting the solenoid to a neutral position, and the coil winding that surrounds the armature assembly is known as the stator assembly.\nOne of our editors will review your suggestion and make changes if warranted. Note that depending on the number of suggestions we receive, this can take anywhere from a few hours to a few days. Thank you for helping to improve wiseGEEK!"", 'The following page lists many common problems encountered with 12 volt 4×4 and ATV winches that are used in off-road recovery situations. With each condition several possible causes are listed along with possible solutions to get your winch working again. Also be sure to check out the Winch Maintenance article at the bottom of the page.\n|MOTOR RUNS IN ONLY ONE DIRECTION||(1) Defective solenoid or stuck solenoid||(1) Jar solenoid to free contacts. Check by applying 12|\nvolts to coil terminal (it should make an audible click when\n|(2) Defective remote control switch||(2) Disengage winch clutch, remove remote control switch plug from the socket and jump pins at 8 and 4 o’clock. Motor should run. Jump pins at 8 and 10 o’clock. Motor|\n|MOTOR RUNS EXTREMELY HOT||(1) Long period of operation||(1) Cooling-off periods are essential to prevent overheating.|\n|MOTOR RUNS, BUT WITH INSUFFICIENT POWER, OR WITH|\nLOW LINE SPEED\n|(1) Insufficient battery||(1) Check battery terminal voltage under load. If 10 volts or less, replace or parallel another battery to it.|\n|(2) Bad connection||(2) Check battery cables for corrosion; clean and grease.|\n|(3) Insufficient charging system||(3) Replace with larger capacity charging system.|\n|MOTOR RUNS, BUT DRUM DOES NOT TURN||(1) Clutch not engaged||(1) If clutch engaged but symptom still exists, it will be|\nnecessary to disassemble winch to determine cause and\n|MOTOR WILL NOT OPERATE||(1) Defective solenoid or stuck solenoid||(1) Jar solenoid to free contacts. Check solenoid by|\napplying 12 volts to coil terminal (it should make an\naudible click when energized).\n|(2) Defective remote control switch||(2) Disengage winch clutch, remove remote control switch|\nplug from the socket and jump pins at 8 and 4 o’clock.\nMotor should run. Jump pins at 8 and 10 o’clock. Motor\n|(3) Defective motor||(3) If solenoids operate, check for voltage at armature|\npost; replace motor.\n|(4) Loose connections||(4) Tighten connections on bottom side of hood and on|\n|MOTOR WATER DAMAGED||(1) Submerged in water or water from|\nhigh pressure car wash\n|(1) Allow to drain and dry thoroughly, then run motor|\nwithout load in short bursts to dry windings.\n|CABLE DRUM WILL NOT FREESPOOL OR IS DIFFICULT TO FREESPOOL||(1) Clutch not disengaged||(1) Check clutch operation according to nameplate. Make|\nsure clutch shifter knob is fully at “OUT” position.\n|(2) Winch not mounted squarely causing end bearing to bind drum (model dependant)||(2) Check mounting to see that installation instructions have been followed.|\n|Ring gear retainer capscrews are too tight. (model dependant)||(3) Remove the gear housing cover, 413018, and all gears|\nfrom inside the gear housing. Disengage the clutch and\ncheck to see that the ring gear will rotate by hand. If it will\nnot, using a hex (allen) wrench, slightly loosen all the\ncapscrews and then snugly re-tighten them in criss-cross\npattern, but do not over-tighten. The ring gear must rotate by hand. Re-assemble winch.\nYou can test a winch solenoid by doing bypass test to determine if the problem lies in the switch or solenoid. Use a lead of 12 gauge wire and jump from the battery +12V and contact the green wire on the solenoid. The winch should operate in one direction. Then apply the +12V wire to the black wire on the solenoid. If the winch does not work in the other direction then the solenoid has a problem.\nThe direction of the winch movement, whether in or out, depends on which of the two two solenoid contacts is connected to the 12V power source with your lead wire. If the winch moves in only one direction, inspect the connections on that solenoid and ensure they are clear of corrosion and make good contact.\nTo test the motor, apply the +12V power wire to each of the two terminals on the motor. The motor should operate in both directions. If it does operate in both directions then you know the motor is ok and the problem probably lies in the switch or solenoid.\nInspect the wire rope before and after each winching operation. If the wire rope has become kinked or frayed, the wire rope needs to be replaced. Be sure to also inspect the winch hook and hook pin for signs of wear or damage. Replace if necessary.\nKeep winch, wire rope, and switch control free from contaminants. Use a clean rag or towel to remove any dirt and debris. If necessary, unwind winch completely (leaving a minimum of 5 wraps on spooling drum), wipe clean, and rewind properly before storage. Using a light oil on the wire rope and winch hook can keep rust and corrosion from forming.\nOperating your winch for a long period of time places an extra burden on your vehicle’s battery. Be sure to check and maintain your battery and battery cables according to manufacturer guidelines. Also inspect switch control and all electrical connections to be certain they are clean and tight fitting.\nInspect the remote control for damage, if so equipped. Be sure to cap the remote socket to prevent dirt and debris from entering the connections. Store remote control in a protected, clean, dry area.\nMany winches require no lubrication for the life of the winch. However check with your manufacturer’s instructional manual for specific maintenance and lubrication requirements.']"	['<urn:uuid:14e3bc6b-f0e6-4b23-a2aa-4660d04277da>', '<urn:uuid:ec0fe708-3b95-4088-aef9-3b0a0a7598d7>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T10:32:05.666290	44	123	1378
89	explain vix calculation methodology historical development and risk management implications	The VIX calculation methodology was developed over time, starting with Brenner and Galai's concept in 1986 and formally launched by CBOE in 1993. The current VIX uses a complex formula that considers prices of various S&P 500 index options to estimate expected 30-day volatility. From a risk management perspective, this mathematical approach has important implications. While VIX provides standardized volatility measurements, risk models using these metrics can be problematic during crises. Research shows that risk estimates would need about 50 years of daily price data to reach theoretical reliability, and different risk models tend to diverge during market turmoil. This becomes particularly critical as VIX-based risk assessments can trigger institutional selling pressure, contributing to market-wide feedback loops involving dynamic hedging, credit risk adjustments, and forced deleveraging.	"['Risk management relies on statistical metrics that converge on common standards. These metrics can change drastically alongside market conditions. A risk management shock is a large unanticipated market-wide change in statistical risk estimates. These shocks give rise to coerced or even distressed flows, typically subsequent to an initial large move in market prices. Risk management shocks and related flows can team up with other dynamics in the financial system to form feedback loops. Such reinforcing dynamics include dynamic hedging, market price-driven credit downgrades, popular fear of crisis, investment fund redemptions, and forced deleveraging. Feedback loops can trigger large and persistent price distortions and offer special trading opportunities.\nThe below is the third instalment of this site’s updated summary of key forces behind market price distortions, i.e. deviations of quoted prices from a level that would clear the market if all participants were trading for conventional risk-return optimization. Previous posts explained price distortions that arise from liquidity conditions and rebalancing rules.\nRisk management shocks\nThe risk management rules of most institutional investors follow commonly accepted standards. Alas, similar rules often coerce similar flows. And one-sided flows in markets with limited liquidity can push prices far from fundamental values. In this way, conventional risk management rules can be a cause of distortions and even set in motion self-reinforcing feedback loops.\nProminent risk metrics are value-at-risk (VaR), a statistical measure of expected maximum loss at a specific horizon within a specific range of probability, and expected shortfall, a measure of expected drawdown in a distress case. These statistical assessments of risk rely on historical variances and covariances, and can be subject to sudden major revisions.\n- The calculation of risk metrics depends on the lookback window, i.e. the history of the price return experiences used for its calculation and the weighting of recent versus distant observations. Lookback windows that rely on multi-year experience adapt poorly to a changing risk environment. Therefore, many risk metrics are short, with a half-time of lookbacks of no more than 11 days. This makes them susceptible to drastic reassessments based on market volatility alone. Such “statistical” reassessment would occur without any consideration of the underlying causes of changes in volatility.\n- Even with many years of data history, risk estimates are still vulnerable to event shocks. Small variations in assumptions can cause large changes in forecasts. Some research claims that it would take half a century of daily price data for VaR and expected shortfall models to reach their theoretical asymptotic properties. Intuitively, even long historical samples have only limited data on actual crises and hence are subject to revision with each new crisis experiences (view post here).\n- Risk models are prone to compounding uncertainty when they matter most: in financial crises. Research shows that different types of statistical risk models tend to diverge during market turmoil and hence become themselves a source of fears and confusion (view post here). Acceptable performance and convergence of risk models in normal times can lull the financial system into a false sense of reliability\nReliance on statistical metrics can give rise to so-called ‘VaR shocks’: If estimated risk metrics surge, VaR-sensitive institutions recalibrate the risk of their existing positions and subsequently reduce their positions (view post here). For example, if an institution has a fixed “statistical” risk budget a doubling of the estimated value-at-risk or expected shortfall requires it to liquidate half of its nominal positions. Importantly, this type of selling pressure typically arises after the initial price decline.\nAnalogously, many trading desks or asset management companies set “drawdown limits” for their managers. These are loss thresholds for a portfolio’s net asset value beyond which traders must liquidate part of all of their positions. Managers are typically under obligation to cut risk regardless of asset value and return prospects. Hence, once the common drawdown limits are broken additional flows ensue in the same direction of the original loss, accentuating price movements for no fundamental reason.\nInitial shocks to risk metrics and related flows can team up with other forces to form feedback loops:\n- Dynamic hedging: Many institutions run explicit or implicit “short volatility” positions. Indeed, such short-volatility strategies seem to have expanded strongly in the wake of declining fixed-income yields. They pay steady positive risk premia in normal times, just like a fixed income asset, but at the peril occasional outsized losses. Dynamic hedging refers to sales and purchases of underlying assets in order to contain the risk related to volatility. This gives rise to feedback loops in two ways.\n- From a macro perspective, there is reinforcement between volatility and the scale of short-volatility strategies (view post here). In particular, there is a plausible feedback loop between low interest rates, debt expansion, (low) asset volatility, and financial engineering that allocates risk based on that volatility.\n- From a micro or trading perspective, dynamic hedging is common practice for option books but is applied widely in other markets, including credit, rates and leveraged risk parity. For example, U.S. financial institutions have historically been “short volatility” with respect to long-term interest rates because of homeowners’ option to repay mortgages early (view post here). In times of declining yields delta and probability of execution of this implied option is increasing, forcing institutions to hedge by further extending duration exposure. The probability of severe “convexity events” has been reduced since the Federal Reserve has bought a sizable share of mortgage-backed securities from the market (view post here), but not eliminated.\n- Credit risk: Risk management can also form feedback loops with credit risk, particularly country risk and counterparty risk. A good illustration for this is the Credit Default Swaps (CDS) market. CDS are assumed to represent a measure of default risk. In practice, this (less liquid) market can gap in large moves, simply as a consequence of one-sided institutional order flows, which themselves could be motivated by risk management or regulatory considerations (view post here). As CDS spreads themselves are used as a measure of credit risk, institutional flows and spreads can reinforce each other to form escalatory dynamics.\n- Public fear: Financial market turbulences typically focus popular attention on crisis risk. Bouts of fear of extreme events, such as economic depressions or war, are more frequent than the actual occurrence of disasters (view post here). In normal or good times, people tend to pay little attention to extremes. As economic or political conditions deteriorate, people begin to contemplate the possibility and consequences of disasters. Such enhanced awareness plausibly changes subjective expectations and price of risk. This is called “salience theory”(view post here). If public fear of crisis is rising, financial risk managers experience pressure from investors, shareholders and even governments to position more defensively.\n- Redemptions: Significant declines in the net asset values of investment vehicles usually give rise to redemptions, often from investors that cannot afford or bear watching wealth dwindling beyond certain thresholds. This is supported theoretically and empirically for equity, bond and credit markets (view post here). In many cases, funds provide daily liquidity and costs of redemptions are effectively borne by investors that do not redeem or redeem late. This creates incentives for fire sales and causes price distortions (view post here). Indeed, the pro-cyclicality of redemptions is consistent with survey evidence of pro-cyclicality of equity return expectations of investors (view post here).\n- Forced deleveraging: Risk-reduction in banks and other financial intermediaries does not only constrain their own asset holdings but, indirectly, those of other market participants, particularly leveraged investors such as hedge funds. This creates both relative price distortions and high directional risk premia. Most obviously, limitations of arbitrage capital give rise to price differentials between contracts with similar risk profiles (view post here). Also, empirical analyses have found that the leverage provided by Broker-Dealers, i.e. their funding of others, is an important explanatory variable for the risk premium paid on equity and credit exposure (view post here). When credit supply is ample, risk premia and future excess returns are low. When credit supply is scarce, risk premia and future excess returns are high.', 'The CBOE\'s Volatility Index, known by its ticker symbol VIX, is a popular measure of the implied volatility of S&P 500 index options, calculated and published by the Chicago Board Options Exchange (CBOE). It is colloquially referred to as the uncertainty index, the fear index or the fear gauge.\nThe formulation of a volatility index, and financial instruments based on such an index, were developed by Menachem Brenner and Dan Galai in 1986 and described in academic papers. The authors stated the ""volatility index, to be named Sigma Index, would be updated frequently and used as the underlying asset for futures and options. ... A volatility index would play the same role as the market index play for options and futures on the index.""\nIn 1986, Brenner and Galai proposed to the American Stock Exchange the creation of a series of volatility indices, beginning with an index on stock market volatility, and moving to interest rate and foreign exchange rate volatility. In 1987, Brenner and Galai met with Joseph Levine and Deborah Clayworth at the Chicago Board of Options Exchange to propose various structures for a tradeable index on volatility; those discussions continued until 1991.\nThe current VIX concept formulates a theoretical expectation of stock market volatility in the near future. The current VIX index value quotes the expected annualized change in the S&P 500 index over the next 30 days, as computed from the options-based theory and current options-market data.\nThe CBOE retained consultant Robert Whaley in 1992 to develop a tradable volatility instrument based on index option prices. Since 1993, CBOE has published VIX real-time data. Based on historical index option prices, Whaley has computed a data series of retrospective daily VIX levels from January 1986 onward.\nThe VIX is calculated and disseminated in real-time by the Chicago Board Options Exchange. Theoretically it is a weighted blend of prices for a range of options on the S&P 500 index. On March 26, 2004, the first-ever trading in futures on the VIX began on CBOE Futures Exchange (CFE). As of February 24, 2006, it became possible to trade VIX options contracts. Several exchange-traded funds seek to track its performance. The formula uses a kernel-smoothed estimator that takes as inputs the current market prices for all out-of-the-money calls and puts for the front month and second month expirations. The goal is to estimate the implied volatility of the S&P 500 index over the next 30 days.\nThe VIX is calculated as the clarify] initiated today. Note that the VIX is the volatility of a variance swap and not that of a volatility swap (volatility being the square root of variance, or standard deviation). A variance swap can be perfectly statically replicated through vanilla puts and calls whereas a volatility swap requires dynamic hedging. The VIX is the square root of the risk-neutral expectation of the S&P 500 variance over the next 30 calendar days. The VIX is quoted as an annualized standard deviation.[\nThe VIX has replaced the older VXO as the preferred volatility index used by the media. VXO was a measure of implied volatility calculated using 30-day S&P 100 index at-the-money options.\nStatistician Salil Mehta of Statistical Ideas shows the distribution of the VIX.\nThe VIX is quoted in percentage points and represents the expected range of movement in the S&P 500 index over the next year, at a 68% confidence level (i.e. one standard deviation of the normal probability curve). For example, if the VIX is 15, this represents an expected annualized change, with a 68% probability, of less than 15% up or down. One can calculate the expected volatility range for a single month from this figure by dividing the VIX figure of 15 not by 12, but by √ which would imply a range of +/- 4.33% over the next 30-day period. Similarly, expected volatility for a week would be 15 divided by √, or +/- 2.08%.\nThe price of call and put options can be used to calculate implied volatility, because volatility is one of the factors used to calculate the value of these options. Higher (or lower) volatility of the underlying security makes an option more (or less) valuable, because there is a greater (or smaller) probability that the option will expire in the money (i.e., with a market value above zero). Thus, a higher option price implies greater volatility, other things being equal.\nEven though the VIX is quoted as a percentage rather than a dollar amount, there are a number of VIX-based derivative instruments in existence, including:\n- VIX futures contracts, which began trading in 2004\n- exchange-listed VIX options, which began trading in February 2006.\n- VIX futures based exchange-traded notes and exchange-traded funds, such as:\n- S&P 500 VIX Short-Term Futures ETN (NYSE: VXX) and S&P 500 VIX Mid-Term Futures ETN (NYSE: VXZ) launched by Barclays iPath in February 2009.\n- S&P 500 VIX ETF (LSE: VIXS) launched by Source UK Services in June 2010.\n- VIX Short-Term Futures ETF (NYSE: VIXY) and VIX Mid-Term Futures ETF (NYSE: VIXM) launched by ProShares in January 2011.\nSimilar indices for bonds include the MOVE, LBPX indices.\nAlthough the VIX is often called the ""fear index"", a high VIX is not necessarily bearish for stocks. Instead, the VIX is a measure of market perceived volatility in either direction, including to the upside. In practical terms, when investors anticipate large upside volatility, they are unwilling to sell upside call stock options unless they receive a large premium. Option buyers will be willing to pay such high premiums only if similarly anticipating a large upside move. The resulting aggregate of increases in upside stock option call prices raises the VIX just as does the aggregate growth in downside stock put option premiums that occurs when option buyers and sellers anticipate a likely sharp move to the downside. When the market is believed as likely to soar as to plummet, writing any option that will cost the writer in the event of a sudden large move in either direction may look equally risky.\nHence high VIX readings mean investors see significant risk that the market will move sharply, whether downward or upward. The highest VIX readings occur when investors anticipate that huge moves in either direction are likely. Only when investors perceive neither significant downside risk nor significant upside potential will the VIX be low.\nThe Black–Scholes formula uses a model of stock price dynamics to estimate how an option’s value depends on the volatility of the underlying assets.\nLimitation and GVIX\nChow, Jiang and Li (2014)  demonstrated that without imposing any structure on the underlying forcing process, the model-free CBOE volatility index (VIX) does not measure market expectation of volatility but that of a linear moment-combination. Particularly, VIX undervalues (overvalues) volatility when market return is expected to be negatively (positively) skewed. Alternatively, they develop a model-free generalized volatility index (GVIX). With no diffusion assumption, GVIX is formulated directly from the definition of log-return variance, and VIX is a special case of the GVIX. Empirically, VIX generally understates the true volatility, and the estimation errors considerably enlarge during volatile markets. In addition, the spread between GVIX and VIX (GV-Spread) follows a mean-reverting process.\n||The neutrality of this article is disputed. (August 2011) (Learn how and when to remove this template message)|\nVIX is sometimes criticized in terms of it being a prediction of future volatility. It is a measure of the current price of index options.\nDespite their sophisticated composition, critics claim the predictive power of most volatility forecasting models is similar to that of plain-vanilla measures, such as simple past volatility. However, other works have countered that these critiques failed to correctly implement the more complicated models.\nSome practitioners and portfolio managers seem to completely ignore or dismiss volatility forecasting models. For example, Nassim Taleb famously titled one of his Journal of Portfolio Management papers We Don\'t Quite Know What We are Talking About When We Talk About Volatility.\nIn a similar note, Emanuel Derman expressed his disillusion with the enormous supply of empirical models unsupported by theory. He argues that, while ""theories are attempts to uncover the hidden principles underpinning the world around us, as Albert Einstein did with his theory of relativity"", we should remember that ""models are metaphors -- analogies that describe one thing relative to another"".\nVIX should have predictive power as long as the prices computed by the Black-Scholes equation are valid assumptions about the volatility predicted for the future lead time (the remaining time to maturity). Robert J. Shiller argues that it would be circular reasoning to consider VIX to be proof of Black-Scholes, because they both express the same implied volatility. He also finds that calculating VIX retrospectively in 1929 does not predict the highest-ever volatility of the Great Depression, due to the anomalous conditions of the Great Depression itself, and we thus have no confidence in VIX to predict, even weakly, such severe events if they should occur in the future.\nHere is a timeline of some key events in the history of the VIX Index:\n- 1987 - The Sigma Index was introduced in an academic paper by Brenner and Galai, published in Financial Analysts Journal, July/August 1989. Brenner and Galai wrote, ""Our volatility index, to be named Sigma Index, would be updated frequently and used as the underlying asset for futures and options... A volatility index would play the same role as the market index play for options and futures on the index.""\n- 1989 - Brenner and Galai\'s paper is published in Financial Analysts Journal. Brenner and Galai develop their research further in graduate symposia at The Hebrew University of Jerusalem and the Leonard M. Stern School of Business at New York University.\n- 1992 - The American Stock Exchange announced it is conducting a feasibility study on a volatility index, proposed as the ""Sigma Index."" ""SI would be an underlying asset for futures and options that investors would use to hedge against the risk of volatility changes in the stock market.""\n- 1993 - On January 19, 1993, the Chicago Board Options Exchange held a press conference to announce the launch of real-time reporting of the CBOE Market Volatility Index or VIX. The formula that determines the VIX is tailored to the CBOE S&P 100 Index (OEX) option prices, and was developed by Robert Whaley.\n- 2003 - The CBOE introduced a more detailed methodology for the VIX. Working with Goldman Sachs, the CBOE developed further computational methodologies, and changed the underlying index the CBOE S&P 100 Index (OEX) to the CBOE S&P 500 Index (SPX).\n- 2004 - On March 26, 2004, the first-ever trading in futures on the VIX Index began on the CBOE Futures Exchange (CFE). Nowadays the VIX is proposed on different trading platforms, like XTB.\n- 2006 - VIX options were launched in February 2006.\n- 2008 - On October 24, 2008, the VIX reached an intraday high of 89.53.\nBetween 1990 and October 2008, the average value of VIX was 19.04.\nIn 2004 and 2006, VIX Futures and VIX Options, respectively, were named Most Innovative Index Product at the Super Bowl of Indexing Conference.\n- Greed and fear\n- Hindenburg Omen\n- IVX, volatility index\n- Market trend\n- S&P/ASX 200 VIX, volatility index\n- SKEW, a CBOE index complementary to VIX, which attempts to measure implied risk of black swan market events\n- Brenner, Menachem, and Galai, Dan. ""New Financial Instruments for Hedging Changes in Volatility,"" Financial Analysts Journal, July/August 1989.\n- Brenner, Menachem, and Galai, Dan. ""Hedging Volatility in Foreign Currencies,"" The Journal of Derivatives, Fall, 1993.\n- ""Amex Explores Volatility Options,"" International Financing Review, August 8, 1992.\n- Black, Keith H. ""Improving Hedge Fund Risk Exposures by Hedging Equity Market Volatility, or How the VIX Ate My Kurtosis."" The Journal of Trading. (Spring 2006).\n- Connors, Larry. ""A Volatile Idea."" Futures (July 1999): p. 36—37.\n- Connors, Larry. ""Timing Your S&P Trades with the VIX."" Futures (June 2002): pp. 46–47.\n- Copeland, Maggie. ""Market Timing: Style and Size Rotation Using the VIX."" Financial Analysts Journal, (Mar/Apr 1999); pp. 73–82.\n- Daigler, Robert T., and Laura Rossi. ""A Portfolio of Stocks and Volatility."" The Journal of Investing. (Summer 2006).\n- Fleming, Jeff, Barbara Ostdiek, and Robert E. Whaley, ""Predicting Stock Market Volatility: A New Measure,"" The Journal of Futures Markets 15 (May 1995), pp. 265–302.\n- Hulbert, Mark, ""The Misuse of the Stock Market\'s Fear Index,"" Barron\'s, October 7, 2011.\n- Moran, Matthew T., ""Review of the VIX Index and VIX Futures.,"" Journal of Indexes, (October/November 2004). pp. 16–19.\n- Moran, Matthew T. and Srikant Dash. ""VIX Futures and Options: Pricing and Using Volatility Products to Manage Downside Risk and Improve Efficiency in Equity Portfolios."" The Journal of Trading. (Summer 2007).\n- Szado, Ed. ""VIX Futures and Options—A Case Study of Portfolio Diversification During the 2008 Financial Crisis."" (June 2009).\n- Tan, Kopin. ""The ABCs of VIX."" Barron\'s (Mar 15, 2004): p. MW16.\n- Tracy, Tennille. ""Trading Soars on Financials As Volatility Index Hits Record."" Wall Street Journal. (Sept. 30, 2008) pg. C6.\n- Whaley, Robert E., ""Derivatives on Market Volatility: Hedging Tools Long Overdue,"" The Journal of Derivatives 1 (Fall 1993), pp. 71–84.\n- Whaley, Robert E., ""The Investor Fear Gauge,"" The Journal of Portfolio Management 26 (Spring 2000), pp. 12–17.\n- Whaley, Robert E., ""Understanding the VIX."" The Journal of Portfolio Management 35 (Spring 2009), pp. 98–105.\n- Brenner, Menachem, Fand Galai, Dan. ""New Financial Instruments for Hedging Changes in Volatility,"" Financial Analysts Journal, July/August 1989. http://people.stern.nyu.edu/mbrenner/research/FAJ_articleon_Volatility_Der.pdf\n- Brenner, Menachem, and Galai, Dan. ""Hedging Volatility in Foreign Currencies,"" The Journal of Derivatives, Fall, 1993. http://people.stern.nyu.edu/mbrenner/research/JOD_article_of_Vol_Index_Computation.pdf\n- CBOE (26 March 2004). ""Contract Specifications: CBOE Volatility Index (VX) Futures"". Retrieved 7 March 2017.\n- Robert E. Whaley, 1993, ""Derivatives on market volatility: Hedging tools long overdue,"" Journal of Derivatives 1 (Fall), 71-84. http://rewconsulting.files.wordpress.com/2012/09/jd93.pdf\n- ""VIX White Paper"" (PDF). Retrieved 2010-09-20.\n- Note that the divisor for a single month is √, and not 12. See the definition volatility for a discussion of computing inter-period volatility.\n- ""Does VIX Truly Measure Return Volatility?"". SSRN .\n- Cumby, R.; Figlewski, S.; Hasbrouck, J. (1993). ""Forecasting Volatility and Correlations with EGARCH models"". Journal of Derivatives. 1 (2): 51–63. doi:10.3905/jod.1993.407877.\n- Jorion, P. (1995). ""Predicting Volatility in Foreign Exchange Market"". Journal of Finance. 50 (2): 507–528. JSTOR 2329417. doi:10.1111/j.1540-6261.1995.tb04793.x.\n- Adhikari, B.; Hilliard, J. (2014). ""The VIX, VXO and realised volatility: a test of lagged and contemporaneous relationships"". Int. J. of Financial Markets and Derivatives. 3 (3): 222–240. doi:10.1504/IJFMD.2014.059637.\n- Andersen, Torben G.; Bollerslev, Tim (1998). ""Answering the Skeptics: Yes, Standard Volatility Models Do Provide Accurate Forecasts"". International Economic Review. 39 (4): 885–905. JSTOR 2527343.\n- http://papers.ssrn.com/sol3/papers.cfm?abstract_id=970480 We Don\'t Quite Know What We are Talking About When We Talk About Volatility\n- Derman, Emanuel (2011): Models.Behaving.Badly: Why Confusing Illusion With Reality Can Lead to Disaster, on Wall Street and in Life”, Ed. Free Press.\n- ""Index Product Awards"". Retrieved 2008-01-05.']"	['<urn:uuid:586fe5fa-fea0-4675-8a54-ad92e0fe558e>', '<urn:uuid:78c94b62-3861-4eda-96b2-a2af7a9a4121>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:32:05.666290	10	126	3831
90	difference between tukano ngoni marriage customs family structure	The Tukano practice tribal exogamy with marriages between language groups, operating within a system of sibs arranged in hierarchical order. Their local groups are connected through marriage alliances. In contrast, Ngoni marriage customs involve a man informing his uncle who then meets with the woman's uncle to arrange the marriage. Among the Ngoni in Mzimba, where dowry is practiced, children belong to the father's side, while in other Ngoni areas like Ntcheu, Mchinji, Dedza, Thyolo and Dowa, both sides discuss child custody. The Ngoni traditionally practice polygamy, though this is less common in the Central and Southern regions. In matters of inheritance, the eldest son of a Ngoni chief's wife inherits the kingdom.	"['The Tukano are a group of tribes speaking languages of the Eastern Tukanoan language family. They occupy the tropical forest areas of the Comisaría del Vaupés, a region roughly the size of New England within southeastern Colombia and northwestern Brazil. Tukano subsistence activities include fishing, hunting, collecting, and horticulture. First contact with Spanish fortune hunters in the mid-16th Century was followed by mission work from several different orders up to the 20th Century. The late 19th Century saw a flurry of nativistic movements.\nSelect the Culture Summary link above for a longer description of the culture.\nSouth America --Amazon and Orinoco\nNote: Select the Collection Documents tab above to browse documents.\nDocuments referred to in this section are included in the eHRAF collection and are referenced by author, date of publication, and eHRAF document number.\nThe Tukano file consists of seventeen documents. Three of these are translations from the Spanish (Fulop 1955, 1956, 1954, nos. 1-3), and one from the Portuguese (Silva 1962, no. 4). The remaining thirteen works are all in English. Fieldwork for these studies was conducted intermittently for a period of over forty years from 1939 to 1980. Probably the most comprehensive ethnographic account covering the Tukano as a whole is that found in Silva (1962, no. 4). This work, the result of the author\'s fieldwork, archival research, and interviews with Brazilian missionaries, contains quotations from early documents, a vast amount of linguistic material, descriptions of material culture, and a discussion of the identification of Tukano subgroups. The three Fulop publications, used in conjunction with those by Sorensen (1967, no. 7), and Reichel-Dolmatoff (1987, no. 9), provide supplemental data to that found in Silva. These works present information on kinship terminology, folktales and myths, cosmology, shamanism, agriculture (including the growing, processing, and use of coca), and multilingualism and tribal exogamy. The remaining documents in the file relate to specific subgroups of the Tukano. Goldman\'s two publications (1963, 1976, nos. 5 & 14), provide well-rounded ethnographic coverage on the Cubeo, including data on the economy, social structure, marriage, political organization, the ancestor cult, and religion. Similar data are provided on the Bará by Jackson (1983, no. 12); the Makuna by Århem (1981, no. 13); the Desana by ReichelDolmatoff (1971, no. 8); the Barasana by C. Hugh-Jones (1979, no. 11); and the Wanano by Chernela (1993, no. 18). Other documents deal with more specific ethnographic topics. For example Dufour\'s articles on the Tatuyo focus on the dietary needs of this group and the expenditure of energy by women in horticultural work (Dufour 1983, 1984, nos. 15-16), while Chernela discusses the concept of linguistic exogamy among the Arapaso and Makuna. Stephen Hugh-Jones who shared a common field site among the Barasana Indians with his wife Christine, whose work is mentioned above, provides an in-depth study of the YURUPARA (YURUPARÍ) initiation cult of this tribe, interspersed with lengthy discussions on religious symbolism (see S. Hugh-Jones 1979, no. 10).\nFor more detailed information on the content of the individual works in this file, see the abstracts in the citations preceding each document.\nThe culture summary was written by John Beierle in December, 1996, and supplemented with additional information from the article ""Tucano"" by Eleanor Swanson in Sixty Cultures: A Guide To The HRAF Probaability Files. 1977. Robert O. Lagacé, ed. New Haven, Conn.: Human Relations Area Files, Inc. The synopsis and indexing notes were written by John Beierle in December, 1996.\nalliance -- generally in terms of marriage between local groups -- categories 571, 581\nBALAY -- a flat circular basket tray -- category 285\nBOGÁ -- bio-cosmic energy -- category 822\nCACHIRÍ -- a social reunion during which CHICHA beer is consumed -- category 574\nCHAGRA -- a cultivated field or garden plot -- category 241 chanter -- reciter of myths and religious chants; a type of priest -- category 793\nCHICHA -- a slightly fermented beer made of maize, manioc or palm fruits -- category 273\nDABUCURÍ -- a ceremonial gathering during which gifts are distributed between allies (see also POOA) -- categories 796, 431, 430\ndancers/chanters -- categories 554, 535, 538\nexogamous groups -- a collection of sibs arranged in a hierarchical order -- category 614\ngourd of beeswax -- application in ceremonies -- categories 237, 415, 778\nKUMÚ -- a Tukano individual with priestly functions -category 793, 756\nKURUPÍra -- forest spirits -- category 776\nlanguage groups -- composed of sibs who share a common ancestor and speak one language -- categories 614, 619\nlanguage family -- composed of related member language groups -category 619\nlocal group -- categories 621, 628\nMacu-Tukano relationships -- categories 629, 563\nMAHSA WAMI -- the village headman -- category 622\nMALOCA -- a large communal house occupied by several nuclear families -- categories 342, 592\nMASA -- the sib -- category 614\nMASA BUTU -- any descent unit above the level of the sib; generally refers to a phratric segment -- category 614\nMATAPI -- see TIPITÍ\nMOJOJOI -- large, edible larvae -- category 262\nOHPË -- the household head -- category 592 order -- category 614\nPOOA -- a ceremony of exchange between sibs (see also DABUCURÍ) -- categories 796, 431, 614\nresidence group (WINGANA) -- category 592 territorial group -- category 619\nTIPITÍ (MATAPI) -- a sleeve-like elastic tube made of basketry, used in squeezing out the poisonous juices of grated manioc -- categories 285, 252\nVAÍ-MAHSË -- The Master of Animals -- category 776\nWATI (WAHTÍA) -- spirits in general or often spirits of the dead -- categories 776, 775\nwax, burning of -- as protective magic and for sending away disease -- categories 789, 751\nWI NGANA -- see residence group\nYAJÉ -- a hallucinogenic plant -- category 276\nYURUPARÍ (JURUPARI) --a ceremonial complex -- categories 796, 881 and/or 852', '(A TRIBUTE TO MY GREAT NGONI TRIBE: THANKS TO MR. PETER MITUNDA FOR HELPING US APPRECIATE OUR TRIBES. CULTURE: IT MATTERS!)\nBy Richard Chirombo\nWhen did Ngoni culture die? Is it when some of the tribe’s strongest members died at the beck and call of pure greed when, in that moment of temporary madness, they pounced on partially-cooked wild beans (locally known as Kalongonda) left by the Lhomwe in the latter’s fright for dear life, or the moment they abandoned their bows, arrows, and war-mongering and quickly adopted the traditions of the very tribes they conquered? For 98 year-old Donarta Maziya of Kauye Village in the area of Traditional Authority (T/A) Kamenyagwaza in Dedza, suggestions that Ngoni culture is dead are a fallacy: the eleven beauty marks marking the length and breadth of her wrinkled face being a mark of the invincibility of that culture. Just last year (2009), she lost her younger sister and mourned her for four months using the Ngoni sorrowful cry for women ‘Muyene wame muyene’. Soon, she takes that same path (to the grave), but is convinced that she will go in full Ngoni honours, moarned the Ngoni way; in fact, she is convinced that, to a large extent, inter-marriages and settlement among other tribes of Malawi has left Ngoni culture unscathed, and this is evident in such Ngoni strongholds as Ntcheu, Dedza and Mchinji in the Central region, Mzimba in the North and Bvumbwe, Thyolo, in the Southern region. Ngoni remnants pay strict observance to ceremonies signifying the progression of a child from one stage of life to another; stages still marked by colourful rites. A visit to T/A Bvumbwe’s court in Thyolo revealed that the Ngoni still attach strong meaning to birth, puberty, the post-puberty period, marriage, senility, death and ancestral worship, though contact with Western and Asian religions has drastically reduced their emotional attachment to their ancestors.\nBirth is not the beginning of life; it is the continuation of Ngoni culture. This principle applies everywhere; whether it is among the Ngoni of Zungendaba in Kasungu, or Mputa Ngonis at Domwe in Dowa, in Ntcheu or Dedza1- the child is more than just another life, and deserves maximum attention and protection. This begins with the maternity hut, which is ‘fortified’ by herbal medicine to prevent witches from casting a bad spell on the mother, leading to still-birth2.Access to the hut is restricted to elderly women and community birth attendants (even the husband is denied entry), until after discharge of the umbilical cord, which marks the official completion of the labour process. A female child is given a small pillow filled with herbal medicine and charms, and attached to a string the child wears around the waist, while the male child is given a small herbal medicine-saturated pillow wore around the neck for protection from spells of bad luck, misfortune and disease. The mother is given advice on how best to take care of the child, and her growing role in the family and society. In cultures where bride pricing (dowry) is still valued, such as Mzimba in the Northern region3, the husband’s side reserves the right to name the child, while in areas where dowry is out of date- either because of intermarriages with other tribes or lack of resources1, members from the paternal and maternal side consult each other on what name to christen a new born4. The child is ready to face the world.\nPuberty is the cross-roads of life. This is the most crucial time in the life of a Ngoni child, the time an individual progresses from fantasy to reality; in fact, Ngoni elders vouch that this is a time a child either makes or breaks in life, and that, because it is a defining moment, children be introduced to facets of culture in their entirety and wholeness5. The concoctions into responsible adulthood come in the forms of initiation, circumcision, advice, and introduction to basic Ngoni tools, including the arrow, small (thorn, because it was originally used for plucking out thorns lodged in the body) knife, axe, hoe and bow6. During initiation, the head of clan (mwinimbumba) invites elders (in case of boys) who gather in an initiation hut (often, one of the houses in the parents’ compound) and offer advice on wide-ranging issues. In the case of girls, the head of clan is merely informed by elderly women, who also gather in an initiation hut within the village and offer advice on life’s realities, including sex and relationships with members of the opposite sex. As for circumcision, the practice is not practiced among the Ngoni of Ntcheu, Dedza, Bvumbwe, Mzimba and Dowa, ostensibly because it has never been part of Ngoni culture (as traditional warriors) to circumcise the youth: what if enemies suddenly attack you?7 Generally, the words advice and initiation are used interchangeably, while circumcision is not regarded as Ngoni culture and is thus best dismissed.\nIf puberty introduces an individual to the ‘wholesome’ world7, then post-puberty activities determine the worthiness of an individual to remain a human being. Getting out of the initiation hut represents one’s readiness to face the world; a world where people sometimes cry (when a member of the family, distant relative or community dies), in which case the individual who has been accepted into the adult world is expected to play a contributory role according to established gender roles. For males, it means putting a hoe on the shoulder and heading for the grave yard to help others dig the final resting place. When burial has taken place, he is expected to quickly sweep the hoe over fire prepared for the task to ‘kill’ evil spirits that may cling to the grave-digging implement and ‘kill’ one more ‘innocent soul”8. Females have their roles, too, and these revolve around domestic chores like cooking, washing, fetching water and firewood, and comforting bereaved members of a family or clan. During times of cerebration, such as marriage, ceremonial beer, among others, females engage in their usual chores: cooking, fetching water, ululating, clapping hands for chiefs and other village elders, rearing children, nursing the sick, tendering crops, among others, while men are expected to become experts at beating the drum for such Ngoni dances as Ingoma, Ziwedewede (a jovial dance in Dedza and Ntcheu) and Mganda. It is during the post-puberty period that boys and girls are allocated their own small huts (called gowelo or mphala), located within their parents or guardians’ compound for loose monitoring of their behavior and sexual tendencies- for girls are advised to stay away from members of the opposite sex while, at the same time, being encouraged not to ‘keep their distance’ from their cousins (as these are treated as their husbands in districts such as Dedza, Thyolo, Dowa and Ntcheu. Thus sexual intercourse between cousins is indiscreetly sanctioned).\nMarriage is the concrete curtain that separates men from boys. In other words, Ngoni tribesmen and women believe that real honour, derived from the complete enjoyment of privacy and the ability to look after other people, is attained after marriage, and this explains why three-quarters of current Ngoni chiefs are married, while those who are not married are either acting in that capacity, are widowed, or have been installed while pursuing further education (such as T/A Gomani of Ntcheu, where Swazi, a boy of less than 20 years, was installed last year after his father’s death)9. Otherwise, the bushy path towards marriage is paved by the same processes, namely: courting, engagement, marriage, child bearing (if possible) and child rearing. A Ngoni tribesman will not propose (ask if a woman of his heart will marry him) someone he has been courting (the way Westerners do); he proposes someone he either falls in love with at first sight, or has been observing for a time. That is real love, the Ngoni way, not the way Westerners get it all wrong: to propose your own woman is a weakness9. When a man finds a woman, and seeks her hand in marriage, he informs his uncle, who goes to the woman’s side to meet her uncle, after which engagement (which until recently meant marriage) preparations take place and, subsequently, marriage. In Mzimba, the children belong to the father’s side because of the tradition of dowry- which means that, in case the husband, or wife, dies, the children remain in the hands of their father’s relations; in Ntcheu, Mchinji, Dedza, Thyolo and Dowa, both sides discuss and decide on who takes care of the children- meaning, inheritance and adoption issues are mostly amicably settled. In cases of chieftaincy, the eldest son to the deceased chief’s wife (for Ngoni are by tradition polygamous people, though that is not common in the Central and Southern region) inherits the kingdom.\nNgonis neither die, pass away nor pass on. This belief has sustained the culture of burying important Ngoni figures, including chiefs and senior counselors, in a life-like position (principally, in a seated position)- a tradition adopted from Zululand (Kwazulu Natal) in South Africa and Swaziland, the original homes of the Ngoni. Ngoni people burry their leaders in a seated position so that they may ‘observe’ (actually, spy on) enemy armies, perpetuating the belief that Ngonis never die but live on to play watchful roles over the living. When suspicious deaths occur, some Ngonis still consult witch-doctors to establish the ‘real’ cause of the death. Among other rituals still observed is the practice of throwing dust into the grave of the deceased (this applies to children, specifically) to prevent the problem of nightmares as well as speeding up the process of recovering from shock associated with the death, apart from the process of mourning the departed personality (kukhuza maliro- where female relatives of the dead wake up as early as 4 O’clock in the morning to perform ceremonial mourning) for a period of between two to four months 10. Relatives of the deceased are also encouraged to rub medicated water in their faces to wash away bad spells that might have been cast by ‘evil-minded’ individuals wishing the family bad luck, after which the head of clan and other senior members of the family sit down to discuss the fate of children, wife (if the deceased is a husband) and property; issues such as education and medical care for children are also taken care of, with one of the senior relations allocated the responsibility11.\nOld age (senility) is for women. A true Ngoni man will avoid two ‘womanly’ things: crying (though at funerals he screams ‘Mayi wawaye’; it is just a plastic cry because a real Ngoni man sheds no tears), and getting old. It must be noted that while it remains a fact that human beings reach a stage in life where their physical strength fails them, and they succumb to old age, what the Ngoni mean by claiming that men never get old applies to the mind’s abilities and experience; meaning, ageing Ngonis still maintain the will of a warrior and never lose their experiences and skills, accumulated through long days in the sun and many years of combat. This perception might have developed during their early years of war, combat, conquest, invasion and destruction of feeble tribes. However, the eventuality of old age has finally come to be accepted as normal, now that Ngonis have settled peacefully among other tribes, and tactics of warfare have changed with the advent of modern weaponry. Nevertheless, there are no specific rites attached to one’s passage into old age, though the accepted norm is for village elders or clan heads to take advantage of the death of a bread-winner, gather all relations over a gourdful of beer and remind them of the responsibility they have in taking care of ageing members- which generally means the provision of farm inputs and clothing. In Ntcheu, Dedza, Mchinji, Dowa, Thyolo and Mzimba, community members also take turns to assist the aged by organizing beer sessions (commonly referred to as mowa waudzu, mowa wamunda, mowa wa dzuwa, according to the purpose of the occasion: community members either contribute in cash, when they buy the beer, or in kind, when they provide the wanted materials like grass for thatching the house of an aged person, or supplying manual labour in the senile member’s garden). Old people have also established an enviable role for themselves in Ngoni society: they act as history and folklore tellers, advice givers, and custodians of culture with the effect that they rarely sleep alone in their huts as children (grandchildren) find a place for a night’s rest in their huts. Like all Malawian societies, however, ageing has its own problems, topping which being the fact that old people are often accused of practicing, or teaching children, witchcraft- an unfounded accusation that has occasioned into cases of victimization.\nAncestors are never barren. Even if individuals leave no children or property behind, they become rich, caring ‘guardian angels’ after their death, even endowed with the power to communicate with early ancestors who “sit on the rain clouds”12 and bring rains in times of drought; even if they were a paramount chief, they become voluntary watchmen, watching and warning the living of possible danger. If need be, the dead even pick up their war instruments and throw an arrow at invaders. To that end, dead people are regarded in high esteem. When a deceased individual appears to a close relation in a dream holding something (for instance, a curtain, or beckons with stretched hands), it is translated as a gesture of want. The individual who beheld the dream thus informs the head of the clan, and a party (accompanied by locally-brewed beer) is thrown with the aim of ‘settling’ the spirit of the deceased ‘down’ (locally known as Kugoneka mizimu pansi)13. Others also believe that when food falls down from a plate or an individual’s hands, it is a sign that ancestors’ spirits are hungry; members of the tribe are thus advised not to pick up, or gather, food pieces that have fallen down so that, when ants devour such food stuffs, it is widely regarded as an acceptance gesture by the spirits.\nIn the end, it is clear that various aspects of Ngoni culture remain intact. Though the Ngoni of Malawi have largely lost touch with their original language, they continue to value many aspects of their culture by still observing requisite rites during various stages of life. This is seen, to a large extent, during occasions marking the birth of children, puberty, marriage, senility, death and ancestral worship. Ngoni cultural displays, observed during the installation of chiefs and such events as Um’theto in Mzimba (Northern region), are a giant step towards maintaining crucial aspects of Ngoni culture, and stand to enrich Malawi’s cultural diversity.\nBvumbwe, Traditional Authority. In an interview. (4)\nGanya, Traditional Authority. In an interview. (12)\nKamenyagwaza, Traditional Authority. In an interview. (6)\nKanchindamoto, Traditional Authority. In an interview. (7)\nKanduku, Traditional Authority. In an interview. (5)\nKanjuli, Village Headman. In an interview. (10)\nKauye, Vilage Headman. In an interview. (2)\nKwataine, Traditional Authority. In an interview. (11)\nM’mbelwa, Imkosi ya Makosi. In an interview. (3)\nM’n’gona, Group Village Headman. In an interview. (13)\nMkanda, Traditional Authority. In an interview. (9)\nMtwalo, Imkosi ya Makosi. In an interview. (8)\nPhilip, K.,D,. (1955), Onani Angoni. Blantyre: McMillan Malawi Limited. Pp., 29-40 (1)\n1. Kamwaza, H.,J., Senior Comprehensive History of Central Africa,(2007) Blantyre: Claim Publications\n2. Philip, K., D., Onani Angoni, (1955, 1998). Blantyre: McMillan Malawi Limited\n3. Phiri, D., D., From Nguni to Ngoni, (1982).Lilongwe: Likuni Publishing House\n4. Shillington, K., History of Africa (Revised 2nd Edition), (2005). Blantyre: McMillan Education']"	['<urn:uuid:c73e616a-5e78-4cf7-9acf-3364e4dc12ac>', '<urn:uuid:a4d23060-0452-47e1-8045-c9693b5a7060>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T10:32:05.666290	8	113	3569
91	Working in a design agency, we're trying to optimize our software selection process. What's the relationship between initial sketching phases and the choice of design software for different project types?	The relationship between sketching and software selection depends on the specific project requirements. For logo design projects, after initial sketching, vector-based software like Adobe Illustrator, Freehand, or Corel Draw is recommended. For projects involving digital images, Photoshop becomes the right choice after the sketching phase. The transition from sketching to digital work should only happen after getting client approval on the initial sketches, as this prevents wasting time on digital refinements that might need major revisions. The sketching phase helps designers understand the basic concept and required components before choosing the appropriate software tools for final execution.	"['As a tool or skill, sketching has its role in the design process. That role will vary depending on the end-product being created, the size and scope of the project, the individual designer\'s style, experience, and workflow, and the client\'s expectations. Find out more about how sketching is used in the design process within multiple design disciplines.\nThe role of sketching in digital art varies depending on if your creating Web sites, identities, illustrations, product concepts, or other designs. An illustration or a logo is likely to need more sketching than a website.\nA large project with a significant client budget will benefit from sketching throughout the design process. This makes sure that before massive amounts of time are invested on refining a solution, a direction is first agreed upon with the client. Sketching can start loose, beginning with basic concepts. Then work on compositions or layouts. After those directions are chosen, the concepts can further be refined with detailed sketching.\n5 Uses for Sketching in Design\nThere are multiple uses for sketching in the design process. Below is a review of five categories of uses with examples and links.\n1. Rapid Concept Development\nSketching is an excellent way to quickly explore concepts. You can sketch for one or two hours and work out multiple possible solutions to the design problem at hand. This is an essential step in the design process. It will save you time to work through concepts on paper before going to the computer. While it is possible to build sketches on the computer, it\'s not as fast as sketching multiple concepts on paper.\nIn the article bioTrekker Logo Design Sketches, designer Karley Barrett shows us her vast use of rough sketches for logo design development. She explores over 60 possible solutions before narrowing the concepts down to just a handful of best ideas. It\'s interesting to see how she explores iconic imagery, typography, and layout.\nShe works through multiple ideas and searches for the best presentation of those ideas. Because she\'s making small sketches, she\'s able to work quickly and generate a multitude of ideas in a relatively short period of time.\nProduct designers spend a lot of time sketching. If you\'re going to design the next sport shoe, piece of furniture, or bike, the idea doesn\'t start in a computer, it starts on paper.\nJames over at the blog Bicycle Design has this to say about sketching, ""Putting ideas quickly on paper is the only way to evaluate them to see if they are worth exploring further. Computer renderings and modern CAD and modeling packages are great, but thinking on paper with a good old-fashioned pencil is always the place to start.""\n2. Basic Composition or Layout\nSketches are a quick way to create the basic composition of your illustration. They are also used in Web site design and graphic design to quickly evaluate layout choices. You can make a series of thumbnail sketches, or they can be larger. As long as your sketches are good enough that they capture the necessary elements, drawing skill is unnecessary.\nIn the tutorial Creating A Cool Vintage Collage Design In Photoshop, Fabio describes how it\'s faster to do some sketches before going to the computer. As you can see below, he captures the basic composition on the left in a sketch. Compare the sketch to the final Photoshop image on the right. You can see the basic layout was worked out on paper. The image of the woman is represented by a stick figure in the drawing. It doesn\'t require amazing, or even good, drawing skills to work out composition before opening up Photoshop.\nWeb Design from Scratch is a well-known Web site that offers practical advice on building Web sites. In the article The Complete No-Nonsense Guide to Designing Websites, the author has this to say about pencil sketching layouts: ""The quick pencil sketch just helps me quickly record the likeness of what I\'ve visualized in my head. Then I don\'t forget and can make it up quickly in Photoshop. I find this way of working a lot more efficient than starting off in Photoshop."" As you can see below, drawing skill isn\'t necessary to capture layout composition either. The left side below is the sketch, and the right side is the final design.\n3. Client Communication and Approval\nShowing sketched thumbnails or compositions to clients, will potentially save you an enormous amount of time. The more detailed the project will be the earlier you want client approval. If you\'re going to spend hours on an illustration, you want to make sure the client is in agreement with your choice of design before moving forward. Getting thumbnail approvals from clients is a common part of the illustration process. It is also common on large logo design projects and other projects as well.\nThe SOS Factory designs predominately mascot logos. Their workflow follows a methodology similar to a comic book design studio. The individual that sketches is often not the same as the one who does the line work. The designer, colorist, and art director are all different roles. They break each role apart into specialties.\nAt this studio, the sketcher works out concepts and client corrections with the art director and designer. The client approves artwork before it goes to the next stage of inking and coloring. This saves time by solidifying an idea before going on to more advanced stages in the process. The example below is a concept worked out based on initial client communication. This sketch is then sent to the customer for approval or for change requests. Once the sketch is finalized, the design moved to the next stage of inking the line work and then coloring the character.\nIn the article From Sketch to Vector Illustration, Bill at GoMedia explains how early in the process they get client approval. They send a series of rough compositional sketches to the client before drawing a more detailed sketch. Below left you can see the one the client chose. Then on the right a more detailed sketch is done before moving to the computer.\n4. Visual Exploration\nSketching can be used as a journaling activity to record and explore your interests. It can also be used to explore multiple options you could take in a particular design.\nSherrie Thai has a portfolio over at Coroflot. She has a section there dedicated to Sketches. These sketches show her visual explorations in multiple fields of design. In the sketch area of her portfolio, she visually explores topics such as patterning, identities, and tattoo styles.\nThe product design book Design Sketching explains the entire process of sketching for product design. It offers tutorials, explanations, and examples. The example below from the book shows how a designer might investigate a problem and explore potential solutions.\n5. Refining Visual Solutions\nThe process of creating a design or illustration at later stages involves refinement. The overall concept and direction of the piece may be working great, but one element isn\'t. Often, this can be tightened up and corrected in further rounds of sketching. Of course, at some point a digital artist moves to the computer. The process of sketching then moves into digital drafts.\nIn the article A Project with Angel Dâ€™Amico you get a feel for how important sketching was in this project, but also how seamlessly the artist moves to Photoshop. In some cases, the artist prefers digital solutions as further client corrections are requested. The artist decides which medium will get the job done faster as a deadline looms.\nI mentioned the article From Sketch to Vector Illustration a little earlier. It\'s an excellent reference on this subject. Bill discusses refining illustrations before going to the computer. There is a section titled, ""Often times some aspect of the illustration looks bad. A professional artist will re-work that part of the illustration on a separate piece of paper until they get it right."" He then explains his process.\nIn this situation, the artist has identified the need to rework a part of the sketch. In some cases, it may be based on a client request, like with Angel D\'Amico above. Regardless of the reason, you\'ll ultimately want a tight sketch for detailed work. Below is a section of one of Bill\'s tight sketches. After that he brought the image into the computer to complete the process.\nYou may feel the desire to skip sketching and jump straight to the computer or work out your solutions as digital sketches. There is nothing wrong with that, especially for your own experimental work. There is no quicker method for exploring multiple visual solutions than sketching though. Try to weigh the advantages of sketching in regards to the project at hand.\nHand-drawn sketching plays an important role in the digital arts. The larger a project is, and the more concepts a client will need to see, the more sketching will prove its worth in your design process. Consider using rough sketches for composition or layout options in your next project. Or push yourself to do another handful of thumbnail sketches before firing up Photoshop.\nLet us know what your experiences are with sketching before jumping to digital within your design process.\nEnvato Tuts+ tutorials are translated into other languages by our community members—you can be involved too!Translate this post', 'Most Important Phases of Graphic Designing – How The Final Design Comes Into Shape?\nGraphic designing covers many areas. Most of the times, a designer has to interact with people from many other fields. This profession requires creativity, adeptness in technical as well as communication and presentation skills. There is a common perception that a graphic designer just sketches a visual message from nowhere. This is actually not the case and like many other professions there is a set pattern of actions that need to be taken for getting an appealing design. As a graphic designer you need to complete many tasks using round about the same approach; which is as follows:\nUnderstanding the Design\nThe most important phase of graphic designing is to understand the basic idea behind what you are going to design. The whole design revolves around this “understanding”. Think, write, sketch whatever comes into your mind, never neglecting the “problem”. Get into the crux of the design, as you cannot get the desired solutions until you get the understanding of what you are designing.\nBefore embarking on any project the first action is to plan. Set priorities for projects. Finalize deadlines with the clients by looking at the amount of work that needs to be done. Always gauge the time requirements in worst case scenarios. Set a daily timetable to complete the different stages of the project.\nAgreement on Design Parameters\nConsider the target audience, the environment in which the design will be used and the priorities of the clients. Examine the dimensions of the design and visualize major components. Ask the client for a confirmation on all these parameters.\nHere comes the stage which will differentiate your design from the competitors. Every designer has a unique aesthetic sense and his own priorities. Try to add something different to your design. Get inspiration from your surroundings, the TV shows, commercials, internet, country side and even clothes… Anywhere you find that spark of uniqueness. But mind you it should not clash or over rule the real message you need to deliver. Sometimes, brainstorming technique works wonders. Sketch down as many ideas as you can, because in graphic designing, more is less!\nMake a Rough Sketch\nJot down the sketch which comes in your mind. Start studying it from the audience’s viewpoint. Consult the people around you for an opinion. It won’t be a bad idea to ask from client of his approval because after this you would start the arduous task of putting your design on software. And then you won’t have many options to modify your design.\nChoosing the Software\nThe next stage comes where you transform all that you have visualized into reality. You should be expert in using at least one of the graphic designing software. The choice of the tools you use depends on what area you will be working in. If you are going for a logo design then it is better to use vector based software like Adobe Illustrator/ Freehand or Corel Draw. If your work involves digital images then Photoshop is the right choice.\nSelecting Colors & Typography\nThe choice of colors is very important. If your final design will be in print then CMYK color model needs to be followed or else it will be used in electronic form, RGB model should be put into use. After choosing the right mode it is necessary to select the right color combination. Colors should not only satisfy the aesthetic sense but also other constraints such as the environment the design where it will be used and the demographics of the audience.\nFont size, shape and color are also very important in a number of graphic designs. For example you need to care about the size of the message you are conveying and whether there is enough contrast from the background and most important of all how easy it is for the viewer to read and memorize it. You can consider typography in the previous stage when you made a rough sketch but it is easy to modify and observe changes in the software.\nAs mentioned earlier, graphic designing is a very vast field. Some of the applications of graphic designing include logo, icon, business card, brochures, newsletters, flyers, monograms, websites or animation. Barring the last two all other applications will need to go through a print process. Although you would have worked on all the parameters of print designing earlier it is still essential to see the final outcome. In some cases like newsletters or magazines binding is also important. All your hard work can go in drain if this stage is not executed correctly.\nGraphic designing is by no means a piece of cake and all those designers are getting paid for a lot of hard work and struggle, not to forget many sleepless nights.']"	['<urn:uuid:5f8222e6-678d-4d43-b6d1-aa88cad10d86>', '<urn:uuid:7380a2f7-8145-4f07-9c7a-efe891ed2cc8>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T10:32:05.666290	30	97	2362
92	performance space size keystone vs playwrights	The Keystone Theatre's Dunn Center features a proscenium stage with an 18-foot arch and a performance space measuring 30ft x 20ft. In contrast, Playwrights Horizons deliberately maintained a tiny stage size in their new building to preserve their intimate production style, though the exact dimensions are not specified. The Playwrights stage floor is fully trapped and removable, with counterweight rigging extending into the auditorium.	"['A vibrant history and a bright future complement each other at the historic Keystone Theatre. Whether you are coming out for the latest movie or a live event, we provide memories which stick with you.\nTheatre Tech Specs\nKeystone Theatre – 2 Screens\nDunn Center for the Performing Arts\n- Proscenium Stage\n- Arch 18 feet\n- Performance Space 30ft x 20ft\n- Total Seats 407\n- Orchestra – 214 plus 10 wheelchair spaces\n- First Mezzanine – 110\n- Second Mezzanine -83\n- Stadium Seating\n- 3D Movie Capability\n- 106 seats\n- No Stage\nRehearsal Hall/Dance Studio\n- Available for community rental\nKeystone Theatre History\n(Formely Hale’s Opera House – 1886 Towanda, PA)\nIn 1988, the Bradford County Regional Arts Council (BCRAC), with the help of supporters, began an extensive restoration project which brought new life to the Keystone Theatre. The Keystone was the first major project of an effort that made the BCRAC owner of Bradford County’s three restored theatres. In 2012, the Keystone Theatre celebrated 125 years of movies, live performances and community events. As part of the celebrations, the following articles chronicling the Theatre’s history ran in The Daily Review:\nThe Ghost of Elias W. Hale\nRemembering the Keystone Theatres Early Years\nFrom Opera House to Modern Movies: Henry Decker & Half A Century at the Keystone Theatre\nTough Years for the Keystone Theatre: Beverly & Walter Buffington as Heroes\nKeystone Theatre at 125…and beyond\nNow in its 134th year of operation, the Keystone Theatre, originally built by Elias W. Hale as Hale’s Opera House, opened its doors for its first public performance on September 21st, 1887. Hale and his wife, Mary Hale, owned the opera house from 1886 to 1908. The theater was then purchased by Edward L. Smith and Charles P. Welles and was renamed the Keystone Opera House. That name stuck until 1921 when the building was sold to William Woodin. From that point on the theater became known by what it is today, the Keystone Theatre.\nThrough the years, the Keystone Theatre has undergone several physical transitions including, the addition of a smaller 106 seat stadium-style seating theatre, now named the Taylor Theatre, a major renovation to the Dunn Center for the Performing Arts (historic auditorium) and a complete transition from 35 mm film to a fully digital projection system in 2012. Although the theater may not look exactly as it did when it was built, it remains a central landmark in the community. Owned and operated by the Bradford County Regional Arts Council (BCRAC) since 1988, the Keystone Theatre continues to offer a wide variety of entertainment. The Nightlife Presents series, Schooltime Youth Series, National Theatre Live series and first run commercial movies are just a few of the offerings that can be seen at the Keystone Theatre.\nAs technology and cinematic experiences have changed, BCRAC has adjusted to keep up. In June of 2014, the BCRAC signed with a new movie buying agent, Jeffrey Jacobs of Jacobs Entertainment Inc. in Rye NY. As the buying agent for the BCRAC, Jeffrey negotiates with the movie studios (Fox, Sony, Warner Bros., etc…) on behalf of the organization to ensure we are able to offer the best programming to the patrons of all three BCRAC theatres.\nFor information on the Keystone Theatre movies and events, please visit our movie website at bradfordcountymovies.com.', 'Home · Email · List of Projects · Project Sites and Links · Articles · Robert Davis Biography\nPlaywrights Horizons - New York, New York\nPeter Jay Sharp Studio Theater\nEntrance on 42nd Street\nPlaywrights Horizons was founded in 1971 and has become one of the nation\'s premier venues for new work. Playwrights has presented new work by more than 300 writers, including Christopher Durang\'s Sister Mary Ignatius Explains It All For You, A.R. Gurney\'s Later Life, Scott McPherson\'s Marvin\'s Room, Wendy Wasserstein\'s The Heidi Chronicles, and Alfred Uhry\'s Driving Miss Daisy. Playwrights produced critically acclaimed musicals such as William Finn\'s Falsettos and Stephen Sondheim and James Lapine\'s Sunday in the Park With George.\nPlaywrights is one of six Off-Broadway theaters along west 42nd Street called ""Theater Row"". The six new and renovated theaters range from 99 to 199 seats. A 499-seat Shubert Organization theater, the ""Little Shubert"", is Playwrights\' next-door neighbor to the West.\nThe new Playwrights was built on the same lot as the old Playwrights building, which was vacated for a year during demolition and construction. The old Main Stage is duplicated almost exactly in the new building. The new building increases the seating capacity of the Main Stage auditorium, adds a Studio Theater, and provides a radical improvement in support space. The new facility includes administrative offices, a telemarketing office, one large and one small rehearsal room, dressing rooms, and spaces for stage departments such as carpentry, props, wardrobe, sound, and electrics. The ground floor lobby on 42nd Street includes a new home for Ticket Central, which is a cooperative box office for several theaters.\nThe building lot is very small so this project is an exercise in artful stacking. Mechanical equipment is located on the bottom and top floors for acoustical and other practical reasons. The left, right, and rear walls are lined with continuous mechanical chases the full height of the building to transfer ducts and conduits from floor to floor. The major spaces are separated acoustically by interstitial noise isolation floors. These noise buffer floors are filled with offices and other similar quiet activities that won\'t leak noise into the adjacent theaters and that don\'t mind a little noise leaking into them.\nPlaywrights was careful not to lose their unique style in the new building. Playwrights decided not to enlarge the stage, but instead kept the tiny stage the same size in the new building that it was in the old building so that their production budgets and their intense small production style would not have to expand or be diluted trying to fill more space in the new facility.\nThe entire stage floor is trapped, can be removed, and can open completely into the trap room below. The front edge of the stage and the proscenium walls left and right are removable to suit the needs of the performance. The counterweight rigging extends out past the front edge of the stage roughly half way back into the auditorium, so it is possible to stand at the locking rail and operate linesets over the auditorium chairs. The Main Stage is wired with the old data standard DMX, with the new data standard Ethernet, with Wybron data wires for scrollers, and anticipates dimmer-doubling on most of the 192 circuits.\nThe new Playwrights Horizons opened in 2003. The capital campaign to cover the cost of the building and an endowment is $32 million.\nArchitect: Mitchell Kurtz Architect PC']"	['<urn:uuid:a61b8a32-927d-46f0-b436-d66400810b1d>', '<urn:uuid:8aa143aa-b9ea-46f1-8626-5389cbe823ae>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T10:32:05.666290	6	64	1133
93	fermentation process temperature duration ruffino premium chianti production	The alcohol fermentation took place in thermo-controlled stainless-steel vats at an average temperature of 28 degrees for 10 days, followed by a post-fermentative maceration on the skins for another 8 days.	"[""Ruffino Chianti Classico Riserva Ducale Oro 2005\n- Harvest: Riserva Ducale Oro Chianti Classico is produced with grapes meticulously selected by hand during harvest. The picking started at the end of September. The vintage 2005 shows balance and elegance, with fruity and spicy notes. - Yield of grapes per hectare: 6 tons. - Fermentation: The alcohol fermentation, aided by racking and punching down, took place in thermo-controlled stainless-steel vats at an average temperature of 28 for 10 days, and was followed by a post-fermentative maceration on the skins for another 8 days. - Aging: After completing the malo-lactic fermentation, the wine was aged for about 36 months, first in vats for about 3 months, then in 35/75 hl. oak casks for about 24 months and, lastly - after completing before being bottled a period of about 3 months in vats - a minimum of further 6 months in the bottle.\nExternal Reviews for Ruffino Chianti Classico Riserva Ducale Oro\nThis shows a raisiny tar character with hints of brown sugar. Full-bodied and very rich, with velvety tannins. This will improve with age. Drink now.\nI prefer the 2001 Chianti Classico Riserva Ducale Oro with the gold label, however, very elegant in its aromas and flavors of cranberries and raspberries, tobacco, and violets, vigorous and sustained on the mid-palate and finish and with tasty, textured tannins on the close. It should continue to drink well for another dozen years.\nI preferred the 1993 Ducale Gold Label Chianti Classico Riserva as it reveals more aromatics, as well as sweeter, riper fruit without the high level of astringent tannin. A complex nose of smoke, dried herbs, sour cherries, grilled meats, and herbs is enticing. Rich, with medium to full body, good acidity, and sweet tannin, it is close to full maturity, and should last for 10-12 years.\nRuffino's most famous label is the gold-label Chianti Classico Riserva Ducale, first produced in 1947. The 2001 rendition capably maintains the tradition, combining coffee, prune and tobacco scents with flavors of ripe plums and dark earth. Tannins give a richly textured feel in the mouth, the drying finish balanced by ample fruit. Drink 2008-2015.\nMature and cedar in style but enjoyable. Pretty strawberry and cedar aromas follow through to a light- to medium-bodied palate, with light tannins and a crisp finish. At its peak. Drink now.\nThe excellent, dark ruby/garnet colored 1995 Ducale Gold Label Chianti Classico Riserva is still tannic, but it offers scents of animal fur, black cherries, tar, leather, and earth. Firm, structures, and muscular, but lacking charm and sweetness, it will last for 15-20 years, although I suspect the tannin will always dominate the wine's personality.\nThe 2004 Chianti Classico Riserva Ducale Oro remains the bellwether wine from this historic property. It is a layered, full-bodied wine with attractive notes of tobacco, sweet red cherries, wild herbs and toasted oak. The wine's density, persistence and complexity make it one of the better versions in recent memory. Anticipated maturity: 2010-2022.\nFood Pairings for Ruffino Chianti Classico Riserva Ducale Oro\nColor : ruby red with garnet Aroma : A bouquet typical of the Chianti Classico, with hints of violets, cherries, plums and a spicy sensation. Elegant, graceful and inviting displays all the characteristics of a great Chianti Classico. Palate : With great body, fruity, where the notes of cherries and small berries follow each nuance of Mediterranean herbs like rosemary and thyme. The alcohol component is well integrated into the structure, as well as tannins. The aftertaste is long and refers to violets, nutmeg, plums and sweet tobacco.\nNotes: After malolactic fermentation the wine was matured for about 36 months, first in the tank for 3 months, then in oak barrels of 35/75 h for about 24 months and, finally, after 3 months in tanks, a further minimum period of 6 months in bottle.\nBest Wine Deals\nSangiovese Top Lists\nFamega Portugal (NV)Reviewed\nSanta Carolina Reserva Carmenere Rapel Valley (2007)Wishlisted\nI have these wines to sell. Any interest?Replied\nDrinking old champagneReplied\nAttending to the Vinyasa at the VineyardReplied\nLeroux Blackberry Flavored Polish Style Brandy (2010)Reviewed\nWeekday Wines - Post Pictures of your SelectionsReplied\nWayne Gretzky Okanagan (2014)Reviewed\nSan Vigilio MerlotCellared\nStock & Stein Spatburgunder Weissherbst Trocken Rose (2012)Wishlisted""]"	['<urn:uuid:654b5cae-8674-4a86-98e4-bcd603da2f27>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T10:32:05.666290	8	31	704
94	queue recording capabilities data privacy implications	Enhanced queues support call recording through third-party providers like AWS and Google Cloud, allowing full call monitoring capabilities. This technical ability to record calls relates to broader ethical privacy concerns, as IT professionals must consider whether to inform users about monitoring, how to handle sensitive recorded information, and the appropriate boundaries of surveillance activities.	"['ACD queues (Automatic Call Distribution) are a more advanced version of Groups that allow your company\'s users to handle calls one at a time while new callers wait in line for the next available agent. Call queues are great for businesses that experience high volume customer calls, as they allow your representatives to effectively deal with each incoming call without losing other callers to busy signals or unanswered phones.\nYour users, or agents, which want to handle incoming calls from the queue must log in to the queue in order to begin receiving calls. When your agents have finished for the day, or no longer wish to continue receiving calls from the queue they must log out of the queue. Logging in and out of the queue is done by dialing the queue\'s login extension.\nSimple vs Enhanced Queues\nOnSIP now offers two (2) types of queues, Simple or Enhanced Queues. Pricing for queues is based on the available slots for agents. The default number of agents is set at 5, meaning 5 agents can log into the queue at one time.\nStandard queue functionality with HD voice, 5 ring strategies and the ability for callers waiting in the queue to failover to a voice mailbox at any time.\nCost per Agent seat = $9.95\nOffers the Simple Queue feature set plus a reporting dashboard accessed in the OnSIP app. Upon creation of a Enhanced Queue, the Account Admin will be granted a new role called ""Queue Supervisor"", which will make reports available in the OnSIP app. Account Admins will have the ability to grant this ""Queue Supervisor"" role to other users on their OnSIP Account. This dashboard offers in-depth statistics and reports on the following:\n- Real-time Overview - Showing a snapshot of the queue within the hour (how many calls are waiting in the queue, a list of agent(s) currently logged into the queue and their status (on a call, available, logged out)\n- Historical Reports - Customers will be able to run detailed reports on the queue by selecting the time period to review ranging from the last day to the last month(s), etc. (includes average call wait time, longest call wait time, busiest time of day) and download as CSV files.\nFor more information on the Enhanced Queue Dashboard, please see this Knowledgebase article.\nEnhanced Queues also offer call recording capabilities in conjunction with a 3rd party Storage Service Provider (SSP). OnSIP currently supports the following three SSPs for call recording:\n- Amazon Web Services (AWS)\n- Google Cloud\nCall recording is included in Enhanced Queues pricing, however there may be additional costs incurred by the external storage provider and due to the SSP directly- see their products for more details.\nCost per Agent seat = $14.95\nCreating an ACD Queue\nUnder ""Apps"" tab, click ""Create New Application"" at the top left of the ""Apps"" page, then select ""ACD Queue"" > ""Create a new ACD Queue"".\nEnter the name for your queue, something descriptive like ""Tech Support Queue"" or ""Billing Queue"" is generally best.\nAn extension is a 4-digit extension, created by you, to allow for blind transferring to your queue from other users.\nThis is the number of seats available for agents to log into. Default is set at 5 agents, meaning you can have up to 5 agents log into the queue at any given time.\nThe universal PIN for agents to use to enter the queue after entering the Login Extension.\nThis is the extension your agents use to log in and out of the queue.\nBy placing a checkmark in the box, you are enabling reporting data that lets you view real-time caller/agent events and historical aggregated statistics at an additional cost of $5.00 per agent seat configured for queue.\nBy placing a checkmark in the box, you are enabling call recording for all calls on your Enhanced Queue. Before enabling call recording on your queue, you must set up an account with one of the 3 supported SSPs. See the following articles for setup instructions:\nWhen you’ve created an account with a SSP, enter the credentials of that account in the Resources tab of your OnSIP account. Once that has been done, choose the name of the Storage Service (created in the Resources tab) here.\nSelecting a Recording Announcement will play a recording before every call, stating to the caller that their call may be monitored or recorded.\nBy placing a checkmark in the box, a tone will play periodically during a call, notifying the caller that the call is being recorded.\nThis determines the manner in which your agents will be delivered calls from the queue. OnSIP offers five (5) ring strategies:\n- Roundrobin Memory: each agent is rung in succession beginning with the last agent succeeding the last agent to answer a call. Example: Three agents logged into queue. First call to first agent logged into queue, second call to next agent logged in, third call to next agent logged into queue, fourth call will loop to ring first agent and ring specified ""Ring Each agent for"" time and then call will move to next agent.\n- Fewest Calls: rings the agents who has received the fewest calls from callers\n- Least Recent: rings the agents who has waited the longest since receiving a caller.\n- Ringall: rings all agents simultaneously\n- Random: randomly selects agents to send the next call\nRing each Agent for\nWhen the queue routes caller to an agent, if the agent(s) does/do not answer within the amount of time specified, the call will be routed to the next agent, based on the ""Ring Strategy"".\nChoose whether the caller will hear music or ringing:\n- Ringing - the queue will act like a ring Group and no announcements will be played\n- Music - MoH (music on hold) Choose from the music sources available. (Enhanced MoH not needed)\nYour callers in queue will be played status messages regarding their position in the queue.\nIf the Caller Hears music option is selected, you may optionally specify how often the caller will be played status messages regarding their position in the queue (On Pickup).\nUse this to allow an agent to accept a call from the queue, but still give them a few moments to prepare before connecting them with the caller.\nReport Hold Time\nThis announces to agent the length of time caller was on hold.\nPost Call WrapUp\nThis value, when set, will be sure that the agent does not begin receiving new calls from the queue for this amount of time. If agents require a few moments to record notes or other final tasks then this value is useful.\nFailover Voice Mailbox\nThis is the location to which timed-out callers will be automatically routed. Also, if no agent(s) happens to be logged into the queue, then this again serves as a failover destination.\nBy checking this box you are setting an email address to have the voicemail to be sent as a WAV file notification.\nMaximum Callers in Queue\nThis is the maximum amount of callers that can be in a queue at one time. All additional callers will immediately go to the queue failover.\nCaller\'s Max Wait Time\nThis is the maximum amount of time you would like any caller to stay in queue before being automatically routed to the Failover Voice Mailbox.\nEscape Voice Mailbox\nIf an escape address is enabled, the caller gets a menu option to manually leave the queue and go to the escape voice mailbox.\nBy checking this box you are setting an email address to have the voicemail be sent as a WAV file notification.\nThis is created by you to be used to directly call into the queue as a caller.\nLogin SIP Address:\nThis is the SIP Address that agents can directly call to reach the login/logout prompt.\nLogging Into the Queue:\nTo login to the queue, an agent must dial the login extension from a phone registered with OnSIP. Important: logging into the queue from an outside phone like a cell phone will NOT work. Once the agent has dialed the login extension they will be prompted to enter the login password, this may be found in the ACD queue details under ""Login Details"". Upon successfully entering the PIN the agent will be logged in and the queue will hangup the call. Now when callers begin entering the queue, the agent will receive calls.\nLogging Out of the Queue:\nSimply dial the queue login extension again from the phone that is logged in. If you are already logged in you will hear a message to the effect of \'agent logged off\', at which point the queue will hang up the call.\nImportant Notes and Suggestions:\n- Unless all of your agents log out of the queue at the end of the business day, all callers calling into the queue will be waiting in line for the full caller timeout duration before failing over to the timeout destination. In order to avoid this please be sure that all agents log out when they are not answering calls\n- Unexpected fail over - many people become surprised when queue calls begin ending up in their personal voice mailbox. This usually occurs when an agent is logged into the queue and has a setting to fail incoming calls over to another location after a certain period of time. If this length of time is less than the ""Callers Max Wait Time"" (see above) then the incoming queue call will fail over according to the user\'s incoming call preference. In order to avoid this either select a sufficiently short ""Callers Max Wait Time"" in order to subvert user timeout settings, or only log into the queue from ""special"" lines configured never to fail over in their incoming call preferences, this is the suggested approach. This will also occur with agents who are logged in but do not have a device registered.\n- What to do if the settings do not display\nRevised September 2019', 'Ethical Issues for IT Security Professionals\nPhysicians, attorneys and other professionals whose job duties affect others\' lives usually receive, as part of their formal training, courses that address ethical issues common to their professions. IT security personnel often have access to much confidential data and knowledge about individuals\' and companies\' networks and systems that give them a great deal of power. That power can be abused, either deliberately or inadvertently. But there are no standardized training requirements for hanging out your shingle as an IT security consultant or in-house security specialist. Associations and organizations for IT pros are beginning to address the ethical side of the job, but again, there is no requirement for IT security personnel to belong to those organizations.\nWhy are ethical guidelines needed?\nThe education and training of IT professionals, including security specialists, usually focuses on technical knowledge and skills. You learn how to perform tasks, but with little consideration of how those abilities can be misused. In fact, many IT professionals approach their work with a hacker\'s perspective: whatever you can do, you\'re entitled to do.\nNote that in this article, we\'re using the word ""hacker"" in the current common meaning, pertaining to ""black hat"" hackers who use their skills to break into systems and access data and programs without the permission of the owners. We\'re well aware that the term originally referred to anyone with advanced programming skills, and that there are ""white hat hackers"" who use their skills to help companies and individuals protect against the black hats.\nIn fact, many IT pros don\'t even realize that their jobs involve ethical issues. Yet we make decisions on a daily basis that raise ethical questions.\nWhat are the ethical issues?\nMany of the ethical issues that face IT professionals involve privacy. For example:\n- Should you read the private e-mail of your network users just ""because you can?"" Is it okay to read employees\' e-mail as a security measure, to ensure that sensitive company information isn\'t being disclosed? Is it okay to read employees\' e-mail to ensure that company rules (for instance, against personal use of the e-mail system) aren\'t being violated? If you do read employees\' e-mail, should you disclose that policy to them? Before or after the fact?\n- Is it okay to monitor the Web sites visited by your network users? Should you routinely keep logs of visited sites? Is it negligent to not monitor such Internet usage, to prevent the possibility of pornography in the workplace that could create a hostile work environment?\n- Is it okay to place key loggers on machines on the network to capture everything the user types? Screen capture programs so you can see everything that\'s displayed? Should users be informed that they\'re being watched in this way?\n- Is it okay to read the documents and look at the graphics files that are stored on users\' computers or in their directories on the file server?\nRemember that we\'re not talking about legal questions here. A company may very well have the legal right to monitor everything an employee does with its computer equipment. We\'re talking about the ethical aspects of having the ability to do so.\nAs a network administrator or security professional, you have rights and privileges that allow you to access most of the data on the systems on your network. You may even be able to access encrypted data if you have access to the recovery agent account. What you do with those abilities depend in part on your particular job duties (for example, if monitoring employee mail is a part of your official job description) and in part on your personal ethical beliefs about these issues.\nThe slippery slope\nA common concept in any ethics discussion is the ""slippery slope."" This pertains to the ease with which a person can go from doing something that doesn\'t really seem unethical (such as scanning employees\' e-mail ""just for fun"") to doing things that are increasingly unethical (such as making little changes in their mail messages or diverting messages to the wrong recipient).\nIn looking at the list of privacy issues above, it\'s easy to justify each of the actions described. But it\'s also easy to see how each of those actions could ""morph"" into much less justifiable actions. For example, the information you gained from reading someone\'s e-mail could be used to embarrass that person, to gain a political advantage within the company, to get him/her disciplined or fired, or even for blackmail.\nThe slippery slope concept can also go beyond using your IT skills. If it\'s okay to read other employees\' e-mail, is it also okay to go through their desk drawers when they aren\'t there? To open their briefcases or purses?\nReal world ethical dilemmas\nWhat if your perusal of random documents reveals company trade secrets? What if you later leave the company and go to work for a competitor? Is it wrong to use that knowledge in your new job? Would it be ""more wrong"" if you printed out those documents and took them with you, than if you just relied on your memory?\nWhat if the documents you read showed that the company was violating government regulations or laws? Do you have a moral obligation to turn them in, or are you ethically bound to respect your employer\'s privacy? Would it make a difference if you signed a non-disclosure agreement when you accepted the job?\nIT and security consultants who do work for multiple companies have even more ethical issues to deal with. If you learn things about one of your clients that might affect your other client(s), where does your loyalty lie?\nThen there are money issues. The proliferation of network attacks, hacks, viruses, and other threats to their IT infrastructures have caused many companies to ""be afraid, be very afraid."" As a security consultant, it may be very easy to play on that fear to convince companies to spend far more money than they really need to. Is it wrong for you to charge hundreds or even thousands of dollars per hour for your services, or is it a case of ""whatever the market will bear?"" Is it wrong for you to mark up the equipment and software that you get for the customer when you pass the cost through? What about kickbacks from equipment manufacturers? Is it wrong to accept ""commissions"" from them for convincing your clients to go with their products? Or what if the connection is more subtle? Is it wrong to steer your clients toward the products of companies in which you hold stock?\nAnother ethical issue involves promising more than you can deliver, or manipulating data to obtain higher fees. You can install technologies and configure settings to make a client\'s network more secure, but you can never make it completely secure. Is it wrong to talk a client into replacing their current firewalls with those of a different manufacturer, or switching to an open source operating system - which changes, coincidentally, will result in many more billable hours for you - on the premise that this is the answer to their security problems?\nHere\'s another scenario: what if a client asks you to save money by cutting out some of the security measures that you recommended, yet your analysis of the client\'s security needs show that sensitive information will be at risk if you do so? You try to explain this to the client, but he/she is adamant. Should you go ahead and configure the network in a less secure manner? Should you ""eat"" the cost and install the extra security measures at no cost to the client? Should you refuse to do the job? Would it make a difference if the client\'s business were in a regulated industry, and implementing the lower security standards would constitute a violation of HIPAA, GLB, SOX or other laws?\nThis article has raised a lot of questions, but has not attempted to provide set answers. That\'s because, ultimately, the answer to the question ""Is it ethical?"" must be answered by each individual IT professional. Unlike older, more established professions such as medicine and law, most ethical issues that IT and security professionals confront have not been codified into law, nor is there a standard mandatory oversight body (such as the national or state medical association or bar association) that has established a detailed code of ethics.\nHowever, the question of ethical behavior in the IT professions is beginning to be addressed. Voluntary professional associations such as the Association for Computing Machinery (ACM) have developed their own codes of ethics and professional conduct, which can serve as a guideline for individuals and other organizations.\nFor an excellent, detailed paper on how to use the ACM code of ethics in making decisions and discussion of many common scenarios, see http://www-cs.etsu.edu/gotterbarn/p98-anderson.pdf.\nFor very detailed discussion of both technological and non-technological ethical issues that face IT pros from systems admins to programmers to ISPs, see Stephen Northcutt\'s book IT Ethics Handbook, published by Syngress: http://www.syngress.com/catalog/?pid=2900']"	['<urn:uuid:e2db9817-f269-4aa1-8630-d361e36041ab>', '<urn:uuid:f4244df6-e208-4249-990e-edaab8660712>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T10:32:05.666290	6	54	3184
95	im learning about socialism what are the main beliefs and goals of socialism	Socialism believes in a controlled market and public ownership of the means of production. Socialists aimed to create a classless society with wealth being equally distributed, ensuring there would be no poor people. They argued that by having the state control the economy and means of production, problems like unemployment and financial crises could be avoided. Under socialism, the state safeguards individual interests by shielding people from unpredictable market forces.	['Capitalism vs Socialism\nBefore we try to find out the differences between capitalism and socialism, it is prudent to have a look at the turn of events that led the development of socialism and finally communism from capitalism that had played a vital role during the industrial revolution in England and later in France, Germany, Japan, and many other European countries. The invention of the steam engine, mass production, and the industrial revolution in Britain meant a large-scale displacement of people from rural settings to cities where industries were established, making them work as wage earners. Capitalists that owned industries and mines attracted men and women from villages to cities where they were asked to work for long hours at low wages.\nThese events had a drastic effect on growing inequalities with rich becoming richer and poor becoming poorer. The Great Depression in the thirties prompted many countries to look for alternatives to capitalism. Thinkers like Karl Marx proposed state ownership of means of production (resources) and equal share of all. This appealed to many countries, especially the Eastern Bloc countries that adopted socialism, which appeared to them as being superior to capitalism.\nWhat is Socialism?\nSocialism is a political and economic system that exists with a controlled market and public ownership of the means of production. The proponents of socialism suggested that the problems of unemployment and financial crises would not arise as economy would be planned with means of production, and distribution remaining concentrated in the hands of the state. This would safeguard the interests of the individual, as he would be shielded from the unpredictable forces of the market-dominated economy.\nSocialists dreamt of a classless society as against the extremely rich and poor divide in capitalism, which was inevitable with individual property and ownership of means of production remaining in the hands of private people. Socialists argued that with wealth being equally distributed, there would be no poor, and all will be equal.\nIt was in 1917 that Soviet Union adopted socialism as state instrument of controlling the economy under the leadership of Vladimir Lenin. The initial success of the policies of the communist government attracted many other countries with China, Cuba, and many others following suit.\nWhat is Capitalism?\nCapitalism is a political and economic system that exists with a free market and private ownership of the means of production. Capitalism that is based upon the belief that competition brings out the best in people evolved in 15th century, and ruled supreme in the world till the 20th century, with the industrial revolution taking place in countries with capitalism in place. Capitalism encourages individual enterprise with the incentive of earning more and rising up the social ladder working to motivate people. Private ownership of property means, wealth remains concentrated in the hands of capitalists, and they gobble up most of the margins with a very small share going to those who work in factories and mines, to produce goods and services.\nWhat is the difference between Capitalism and Socialism?\nThe world has seen the rise and fall of socialism and the loopholes in capitalism. No one system is perfect and can be installed discarding the other. While there is no doubt that capitalism has survived the onslaught of all other ideologies like communism, socialism, etc., it is a fact that the great bubble of communism has burst with the breakup of Soviet Union and failing of other communist economies. The time has come to evolve and put into practice a system that takes up salient points of both ideologies, not only to encourage private enterprise but also to implement government control in resources to work for the good of the poor and the oppressed in the society.\n• Definitions of Capitalism and Socialism:\n• Capitalism is a political and economic system that exists with a free market and private ownership of the means of production.\n• Socialism is a political and economic system that exists with a controlled market and public ownership of the means of production.\n• Ownership of Means of Production:\n• In capitalism, means of production were owned by individuals.\n• In socialism, means of production were owned by the state.\n• Social Classes:\n• A society that followed capitalism had classes in it.\n• A society that followed socialism dreamt of a classless society.\n• In capitalism, those who owned the means of production had more of a share of the earnings while the workers got only a little share.\n• In socialism, everyone was given equal earnings as the state owned the means of productions.\n• Capitalism had a free market system.\n• Socialism had a government controlled market system.\n• Government Interference:\n• In capitalism, government interference is minimal.\n• In socialism, government decides everything.']	['<urn:uuid:cf87bb54-c20b-4138-ab90-c3c336d5e64a>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T10:32:05.666290	13	70	795
96	cultural diversity research royal bc museum vs cultural museum taunggyi compare approaches	Both museums study cultural diversity but take different approaches. The Cultural Museum Taunggyi focuses on showcasing and documenting the cultures of over thirty Shan communities through traditional costumes, instruments, arts, crafts, and Buddhist beliefs in four exhibition rooms. The Royal BC Museum studies social economic and cultural diversity through multiple research programs, including the study of immigrant experiences, community histories, and biographical documentation, with a particular focus on how different communities have contributed to British Columbia's development over time.	['|English||The Cultural Museum (Taunggyi)|\nCultural Museum (Taung Gyi) is situated on the Bogyoke Aung San road at the forest ward in Taung Gyi, Shan State, Myanmar. At first, it was organized as the Shan state Cultural Department at the Shan state office, and it exhibited some cultural objects. According to the new administration system of 1974, it was transferred to the Fine Arts Department and it was inaugurated as the Cultural Museum at the present building on May 11, 1974. This museum is a cultural museum under the Department of Archaeology and National Museum, Ministry of Religious Affairs and Culture, Republic of the Union of Myanmar.\nIt is two-storied brick building with a display area of about 11,200 square feet. There are four exhibition rooms. Altogether 880 artifacts are showcased in the excavation rooms.\nIn the first exhibition room, traditional costumes and traditional instruments of the over thirty Shan communities still living in Shan State each with its own culture are displayed.\nIn the second exhibition room, intangible cultural heritage of Shan peoples, such as traditional arts and crafts, are displayed.\nIn the third exhibition room, the Pinlon agreement led by General Aung San on 12 February 1947 is shown. Moreover, colorful paintings illustrating the traditional cultures of the Shan peoples are displayed.\nThe fourth exhibition room is dedicated to the literature and Buddhist beliefs of Shan peoples. Moreover, ancient artifacts from various periods that flourished in the Shan state are exhibited.\nCultural Museum (Taunggyi) is situated in the Southern Shan State and is under the Department of Archaeology and National Museum (Taunggyi). According to the organizational chart, the state director manages the entire Shan State. The Shan State is divided into three parts: the Eastern Shan State, the Northern Shan State and the Southern Shan State. The eastern and northern parts each have an assistant director who explores and safeguards both tangible and intangible cultural heritage of the Shan communities. To be effective, they submit reports about the ICH and TCH of Shan communities directly to the head office situated in Nay Pyi Taw, although these two sections are under the state director. Hence, the Cultural Museum (Taunggyi) surveys, explores, records, and safeguards the ICH and TCH in the Southern Shan State under the management of the state director.\nIn the exploration steps, documentation is the most important process. Documentation is carried out by following the form of UNESCO. Most of the ICH recorded by Cultural Museum (Taunggyi) are (a) social practices, rituals, and festive events; and (b) traditional craftsmanship. Social practices, rituals, and festive events can be recorded only during the occasions, and therefore, care must be taken when documenting as there is only one chance to record. For traditional craftsmanship, the craftsmen can be asked to demonstrate their skills and photographs and videos can provide detail. Moreover, their history, the process of craftsmanship, and other necessary data can be requested in detail.\nThe staff has to travel to search and collect the data in various parts of the state. Even in the insurgent area, the staff eagerly travels for documentation because some tribes living there and some arts and crafts developing there are very rare. Documentation is done by taking photographs, videos, and recording. Sometimes, it is difficult to record ICH owing to the language barrier. The recorded data is kept in both soft copy and hard copy at the Department of Archaeology and National Museum (Taunggyi). ICH documentation records have been kept since 2000, and altogether fifty items of ICH have been recorded.\n|Contact Information (Organization)|\n|Address||Forest ward, Bogyoke Aung San Road, Taunggyi, Shan State.|', 'Research forms a key part of our mission at the Royal BC Museum. It is how we unlock the secrets to this magnificent land and the cultures that have flourished here. It helps us to be the best possible stewards of the human and natural histories of British Columbia. And it also guides us in sharing the story of this province, so rich in geography and culture, with the rest of the world.\nAbout Our Research\nTheme 1: Describing how government functions, including First Nations government\nProgram 1: History of public service\nGovernment plays a central role in British Columbia society and the documentation of government and the records it produces are critical to our understanding of British Columbia yesterday, today and tomorrow. By conducting research into the records arising out of the activities of government, and administrative histories of the bodies that create those records, we will document this history.\nProgram 2: The BC Government and other organizations\nThe BC Government interacts with a wide range of other governments and agencies. It is our intention to document the Province’s relationship to these organizations and examine how it has evolved since Confederation, be it as a Crown Corporation, a municipal government, the Canadian Government or a community or business association.\nTheme 2: Understanding social economic and cultural diversity\nStandards and practices toward digitization for preservation of sound and moving images\nProgram 3: History of communities in BC\nBritish Columbia is not only a community, it is also made up of many communities. It is our aim to understand the way these communities have contributed to the British Columbia we know today. Why, for example, did some coastal villages and towns appear at one point in time, only to disappear at a later date? Who were the people who lived here and what shaped their lives? These are some of the questions we seek to answer.\nProgram 4: Biographical and historical documentation\nOur work here consists of documenting British Columbia’s people and their histories through biographical and historical research that pertains directly to records. British Columbians will benefit from our contributions to the understanding of the people and events of our past, especially when undertaking their own historical research.\nProgram 5: The immigrant experience in BC\nMyriad people have moved to and reside in British Columbia. This ethnic diversity has shaped who we are – and what we are. Their stories are our stories and it is these stories that we use to develop our collection, and create new galleries and exhibitions.\nProgram 6: Early period archaeology of BC\nHuman history in BC reaches back in time to over 10,000 years ago. Our ongoing investigations into the archaeological record add depth to our current understanding of the human story in British Columbia. Research in this area expands the quality of the information about our collections and provides a better context for this information. Having a solid knowledge base for our collections allows us to go forward with enhanced public programs that involve everything from public inquiries to major exhibits.\nProgram 7: Energy\nEnergy is basic to nearly every activity. Understanding how energy use has changed over time and studying the sources of energy – be it animal power, steam power or electrical power – helps us see where we have been in the past and where we are going in the future.\nProgram 8: Resources and industry\nBritish Columbia’s economy has for hundreds of years been dependent on our natural resources. By linking our resources to the growth of industry we can explain much of the changing human and natural landscape of BC.\nTheme 4: Studying biodiversity\nProgram 9: Rare and endangered species\nResearch into the rare and endangered species of BC deepens our understanding of our rich biological heritage. Museum scientists contribute to this growing field by making and studying collections. Knowledge gained from this research can inform the management, protection, restoration and conservation of these special species.\nProgram 10: Non-native species\nDeveloping our knowledge base in invasive or exotic species helps us to understand distribution patterns, areas of particular vulnerability and impacts on indigenous organisms. By continually building upon this foundation of knowledge we can provide useful information to conservation officers and those in the field of species management. We’re also helping to inform members of the public so that in turn, they can know more about what belongs in BC – and what doesn’t. Our work here allows us to understand and raise awareness of the potential impacts of climate change on human communities and natural ecosystems.\nProgram 11: Paleo-environment of BC\nOur research into past environmental changes reveals ancient landscapes, how plant and animal distributions have changed and the reasons for them. The evolving environmental also influenced how and where people lived. Our study of the history of the landscape, climate and plants and animals informs our understanding of the present-day and the implications of climate change. Our insights help identify gaps in our natural and human history collections and develop strategies to fill them.\nProgram 12: Taxonomy and phylogenetics\nBasically, museum biologists study systematic biology (systematics), the field that (a) provides scientific names for organisms, (b) describes them, (c) preserves collections of them, (d) provides classifications for the organisms, (e) produces keys for their identification, (f) assembles data on their distributions (biogeography), (g) investigates their evolutionary histories and relationships (phylogenetics), and (h) considers their environmental adaptations. Taxonomy, narrowly defined, deals with functions a and b, but we can include classification and identification and other aspects here.\nProgram 13: Diversity\nMuseum biologists frequently assemble taxonomic, distributional, life-history, ecological, behavioral and other biological information on species and groups of species. This information goes into field guides, handbooks, annotated lists, and so on. Such products are important in the basic documentation of the province’s biological diversity. They are useful to, and popular with, scientists and the general public alike.']	['<urn:uuid:482d4e51-c417-4dad-af6e-f800ef4a10b5>', '<urn:uuid:1751113f-c433-4065-855d-762ffcbd4c91>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T10:32:05.666290	12	79	1579
97	golf course types links course characteristics location requirements	Links courses are a specific type of golf course that must be built on the coast and have sandy soil underneath. These courses are primarily found in England, Ireland, and Scotland. While many courses claim to be links courses, only a few are genuine links courses meeting these requirements.	['You may have heard of Golf but never played it. This article will help you understand what golf is about, its Rules, how it works, and what the different golf courses are. You will also learn the hand-eye coordination necessary to play golf. Get ready for a challenging and fun day of golf! Here are the basics:\nRules of golf\nTo play the game well, you will need to be familiar with the Rules of Golf. The rules for the greens are determined by the terrain on the course. The course is marked using yellow or red stakes. The tee box should be two club lengths from the golfer’s tee. If they miss the ball by more than two club lengths, they may be disqualified. They can use the penalty stroke if they miss the ball more than two club-lengths.\nRule 8 explains the central principle of golf. The central principle of golf is covered in Rule 8. The player may take reasonable measures to improve the conditions. However, in limited circumstances, these conditions can be restored with no penalty. Provisional balls can also be used under the Rules of Golf. These balls may be used as an emergency replacement for the original ball if it is not found. The provisional ball can be used if the original ball is not found within three minutes.\nThe next rule concerns etiquette. It is an important part of golfing. It determines how you interact and how the environment is perceived by other players. Respect others and play at a reasonable pace. You should also take care of the course and repair divots and bunkers. You could be disqualified from the tournament if you cause excessive damage. It is important that you remember the Rules of Golf and follow them.\nTypes of golf courses\nThere are many types of golf courses. There are also links and parkland golf courses. Inland parks are home to natural vegetation, while desert or parkland-style courses use artificial turf or carpet. Links courses are generally open to the public. However, there are some that are located in urban areas. A golf course can generally be divided into one or more of these categories. Listed below are the common types of golf courses.\nGolf courses can be categorized by access, size, type of holes, setting, and design. There are three types of golf courses: public, municipal, and daily-fee courses. Public courses, which are often open to the public at no cost, are generally more accessible. Privately-owned courses are more expensive and generally more accessible. Below is a table listing some of the most well-known golf courses along with their characteristics.\nLinks course: These courses are often the most famous. Links is an Old English word that means “sandy spot” in Old English. Many golf courses claim to have links, but only a few of them are actually links. Links courses are located in England, Ireland, Scotland, and Ireland, regardless of their title. Links courses must have sandy soil underneath and be built on the coast. Links golf is a name for golf.\nOrigins of golf\nThe game of golf may have its origins before the 15th century. Evidence suggests that it may have evolved from medieval soccer. Many historians have challenged the authenticity of Scottish sources regarding golf’s origins. Pictorial evidence also suggests that the game was invented somewhere in Europe. The earliest known picture of the game is found in a book called Hours, owned by Adelaide of Savoy. The book predates Scottish sources by almost 400 years.\nThe game of golf has a rich history. Its origins can be traced back at least to Moses, who played it during ancient times. Literature mentions it in ancient Rome. Later, the game traveled throughout Europe and was first played by shepherds. In medieval Germany, shepherds played golf by using sticks to launch stones. The Dutch also contributed greatly to the development and evolution of golf. The Dutch called the game “het kolven” (or kolf), which is similar to the modern name of golf.\nThere are many theories about how golf came to be. Some historians believe that the game was adapted from a national game of the Persians, chaugan. Other theories claim that it developed from a cross-country game, the chicane, which involved hitting a ball with a stick. According to the Chinese, chui-wan, a form of golf, is believed to have been played as far back 300 BC. The French game was influenced by the game of chui wan, which is also known as golf and was referred to as chole in Emil Zola’s novel.\nGolf requires hand-eye coordination\nHand-eye coordination is one of the most important skills for golfers. This is the ability to quickly switch your focus between near and far objects. You should practice near-far drills to improve your hand eye coordination. These drills can improve your golf swing, your reaction time, as well as your athleticism. A drill to improve hand-eye coordination is a great place to start. Read on to find out how to develop hand-eye coordination with this simple exercise.\nGolfers have been told for decades to keep their heads down when hitting the ball, but this has caused many golfers to change their swing and suffer poor club-ball contact. It has also increased injury risk. Golfers use their 13-million-bit processor to maintain correct posture, arm extension, and wrist action during their swing. Even though they cannot see the ball, golfers still swing correctly. This allows them hit the ball accurately.\nAside from a proper swing mechanic, a good player should also have perfect hand-eye coordination. Repetitive practice sessions are essential for a player to be able to make the right decisions about how to hit the ball. Golf is a sensory-motor activity. This information is sent to various parts of the body, including the muscles, joints, and ligaments. This means that practice must be done with a healthy brain and good coordination.\nCommon scoring methods in golf\nThe common scoring methods in golf are different depending on the type of golf game you are playing. The stroke play method is used in most golf games. This allows players to count the number of strokes they have played per hole. This determines who wins each hole. Match play determines who wins if the score of a player is lower than that of the other players for the round. Each hole a player ties will earn him half of a point.\nAnother popular scoring method is the Par system. This rates each hole according its difficulty. A par 3 hole is difficult to hit and experts would expect to reach it in three shots. To allow players with different skill levels to compete, handicap systems are used to grade players. The par system has been in use for centuries and has helped to ensure that the game is fair and competitive. Here are some examples:\nScores are often expressed as percentages of a score against a certain par. A player who shoots five strokes on the par 4 hole is considered to be one over par. A player who scores three strokes on the same hole is considered one under par. On some holes, a player can concede without penalty if he scores one stroke over par. This allows players score more points without penalty.\nEquipment required to play golf\nIf you’re planning on learning how to play golf, you’ll need the appropriate equipment. In general, golf clubs are the most important piece of equipment you’ll need. Every club has a unique purpose and a specific design. All golfers have a basic set. However, more players are opting for hybrid clubs, which combine elements of many golf clubs in one. This is a great way to learn the game while saving money.\nYou will also need appropriate clothing. The clothing should fit well, and not wrinkle. The shirt should not be pulled up. You’ll also need golf shoes, which should absorb moisture away from the foot. You can also borrow the divot repair tools of the course manager. They aren’t necessary but it is a good practice to use one. These tools can also be used for repairing dents in golf balls.\nGolf is often regarded as a sport for kings but it can be enjoyed by everyone. With the right tools and basic knowledge, you’ll be able to make the most of your game. Shopping for golfing gear can be intimidating for first-timers, especially if you’ve never played before. There are many options on the market and it can be overwhelming to choose which items to purchase.']	['<urn:uuid:377a0ef2-85ef-4593-828a-ca966db26f1f>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T10:32:05.666290	8	49	1435
98	I'm curious about eco-friendly farming - how long do the benefits of adding this special charcoal to soil actually last?	Scientific experiments in the Brazilian Amazon have shown that biochar's benefits in soil can last for over a thousand years. Even after this extremely long period, crops still grow better on biochar-amended soil than on freshly cleared rainforest soil.	['Biochar – A Carrier of Hope and Nutrients\nWhat is Biochar and how is it different from charcoal?\nYou may know charcoal from firing BBQs. Biochar, however, is a special type of charcoal produced by a thermal process called pyrolysis (from the Greek, ‘pyro’, meaning fire and ‘lysis’, meaning separation). During pyrolysis organic matter such as wood waste, organic kitchen waste, rice husks, grass cuttings etc. is thermos-chemically disintegrated in an oxygen-free environment at high temperatures of between 400 °C and 900 °C. Biochar starts out as organic material and becomes more mineral-like with the heating (Wilson K 2014). The carbon sequestration achieved in the process is 489 kg CO2 per 1,000 kg of organic material (Gerber 2009), which means almost half of plant wastes’ total carbon will be permanently stored in the biochar for more than 1,000 years (Schmidt HP 2011). When incorporated into the soil CO2 is actively taken out of our atmosphere, creating a so-called ‘carbon sink’ which is able to slow down climate change.\nHans-Peter Schmidt, a biochar expert, said in an article published in the Ithaka journal (1/2012) (http://www.ithaka-journal.net):\n“The current imbalance in the world’s carbon and nitrogen cycle is not just the main cause of climate change, but also a direct threat to ecosystems through eutrophication, desertification and a decline in biodiversity. Re-balancing through regularly recycling organic material with its carbon, nitrogen and phosphor content is needed. Biochar has the potential to play a key role, as it not only converts the carbon found in a wide range of biomasses into a stable form, but also binds volatile nutrients from biomass residues, thereby recycling them for agricultural use. Though still “early days” for biochar, the prospects for its use are good, whether in crop or livestock farming, or in industry.”\nSoil and Biochar\nHealthy soil can be pictured as a living being that consists of innumerable small organisms, inorganic minerals, water, roots of plants and organic matter. Almost 90 % of all organisms on our planet are living in the soil (Schmidt HP 2010). Once natural vegetation has been removed microorganisms disappear, the soil slowly loses its natural fertility and gets depleted in nutrients. In order to still be able to grow crops, non-organic farmers add tons and tons of inorganic fertilizer, not knowing that the inorganic fertilizer kills the soil life. In order to compensate for that loss even more inorganic fertilizer is added, at high cost to the farmer and the planet – most of these fertilizers are oil- and gas-based. This is a deadly spiral, which ends in contaminated soil and groundwater often resulting in ‘badlands’ – biodiversity deserts. There is an urgent need to maintain and build healthy, living soils which keep fertility.\nSince ancient times it is known that poor soils can be significantly improved by adding biochar. In South America biochar amended soil is known as “terra preta” black soil. Scientific experiments in the Brazilian Amazon have shown that a thousand years after application of the biochar crops still grow better on biochar amended soil than on freshly cleared rainforest soil.\nBefore applying biochar to the soil it has to be “activated”, meaning it has to be loaded with nutrients and microorganisms, which can e.g. be achieved by mixing it with compost or organic fertilizer. The pyrolysis process creates thousands of microscopic holes in each piece of Biochar. One gram of Biochar could, theoretically, unfold to be the size of a soccer pitch. This extremely high surface area means that lots of microorganisms can colonise the Biochar and later colonise depleted soil; the Biochar also helps soil to retain water. Once activated/loaded Biochar is mixed into depleted top soil, or simply put on the soil surface as a top dressing ideally covered with mulch. Over time the soil will become more alive, restoring itself and getting back into its healthy natural cycle.\nSEM images of the popular bio-char (y sectional surface) after the partial gasification\nHow does Biochar modify soil?\nBiochar serves as a carrier for nutrients, water and habitat for microorganisms, all crucial for a healthy soil and healthy plants. Due to its big sponge-like surface (300 m2 per 1 g) biochar is able to store a five times higher amount of nutrients and water than its own weight (Schmidt HP 2014). All the stored nutrients are easily available for plants.\nKey facts about biochar (according to Schmidt HP 2010, 2011, 2012):\n• Biochar stores nutrients like nitrogen and prevents them from being washed away\n• Stored nutrients are easily available for plants and microorganisms. Through the stimulation of microbial symbiosis, the plant takes up the nutrients from the porous carbon structure.\n• Biochar provides habitat for microorganisms, which are crucial in processing nutrients and building new fertile soil\n• Plant growth and plant health is improved\n• Increase of myccorhiza, so plants can access nutrients more easily\n• Biochar improves water retention and stabilizes soils\n• Biochar helps to prevent erosion and stagnant moisture and releases water through dry periods\n• Biochar improves aeration and reduces emission of climate-wrecking gases like methane or nitrous oxide\n• Biochar is capable of binding toxic substances (heavy metal, pesticides, etc) which is not only important for healthy plants but also for clean water and ground water protection\n• Biochar is raising the soil’s pH-value, again making nutrients more accessible to plants\n• Biochar reduces waste problems by recycling organic materials such as arboricultural waste, old bamboo scaffolding, old palettes, rice husks, coconut fibres, organic kitchen waste and so on – keeping them out of the landfill where they decay and release carbon into the atmosphere.\nHow is Biochar used in KFBG\nBiochar is used in KFBG to improve poor soils for organic farming, gardens and forest restoration. It helps us to reduce and recycle organic wastes.\nWe have had a small Biochar machine for several years. In July 2015 we have installed a new state-of-the-art machine, custom built in Australia, which has almost no emissions and, as it is the size of a shipping container, can handle a large volume of wood everyday if needed. As we change our abandoned, unproductive mono-crop orchards in the middle areas of KFBG, over the next two decades, we will convert all the cut wood to biochar and put this back onto the old orchard terraces, with mulch to enrich the depleted soil, ready for planting a wide range of native tree seedlings to create a healthy native forest and seed nursery.\nFurther information and links:\nSchmidt HP: Terra Preta – model of a cultural technique\nSchmidt HP: Climate Farming – A Master Plan for Sustainable Agriculture\nSchmidt HP: Biochar – a key technology for the planet\nWilson K: How biochar works in soil\nGerber H (2009): CO2-Bilanz des Pyregreaktors, Ithaka-Journal 2009, www.ithaka-journal.net/60, ISSN 1663-0521\nSchmidt HP (2010): Climate Farming – A Master Plan for Sustainable Agriculture, 1/2010, S.314–317, www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz; www.delinat-institut.org, www.ithaka-journal.net. ISN 1663-0521\nSchmidt HP (2011): Pflanzenkohle, Ithaka Journal 1/2011: 75–82 (2011); www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz; www.delinat-institut.org, www.ithaka-journal.net. ISSN 1663-0521\nSchmidt HP (2011): Pflanzenkohle – Landwirtschaft als Klimaretter – ein Jahresbericht. Ithaka Journal 1/2011: 9–13 (2011), www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz; www.delinat-institut.org, www.ithaka-journal.net\nSchmidt HP (2012): Pflanzenkohle, eine Schlüsseltechnologie zur Schließung der Stoffkreisläufe, Ithaka Journal 1/2012: 75–79 (2012); www.ithaka-journal.net; Publisher: Delinat-Institut für Ökologie und Klimafarming, CH-1974 Arbaz, www.delinat-institut.org, www.ithaka-journal.net. ISSN 1663-0521\nSchmidt HP (2014): Terra Preta – model of a cultural technique, the Biochar Journal 2014, Arbaz, Switzerland.\nISSN 2297-1114; www.biochar-journal.org/en/ct/4\nWilson K (2014): How biochar works in soil, the Biochar Journal 2014, Arbaz, Switzerland. ISSN 2297-1114']	['<urn:uuid:911ac082-e677-4038-978f-60d9b8244cab>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	20	39	1272
99	when moscow power plant culture center open	The GES-2 power station conversion was initially planned to open in September 2020, but was delayed due to the COVID-19 pandemic causing construction delays.	['In an effort to consider the varied impacts of COVID-19—a virus with a global reach—post has interviewed curators and directors from vital museums and galleries around the world about how the pandemic has affected their ideas regarding programming, civic engagement, and the role of the institution. This is an interview with Katerina Chuchalina, Chief Curator at the V—A—C Foundation (Moscow and Venice).\nInga Lāce: The V—A—C Foundation was planning to open in 2020 with a new building in the converted former site of GES-2, a power station right in the center of Moscow overlooking the Kremlin. How have you reshaped your current programs, themes, and the inner institutional workings in response to the global pandemic?\nKaterina Chuchalina: Indeed, the global pandemic caught us by surprise as we were finalizing the opening program for GES-2, which was supposed to launch in September 2020. By March 2020, however, it was clear that this would not happen as the construction was inevitably delayed by the crisis. Like everyone else in the world, we began to recognize the destructive and generative potential of the virus, and we used the delay to analyze the state of our community and the ways in which the institution might support it. We used this continuous momentum to rethink the opening season. Our rapidly growing team has spent several years planning, designing, and creating the range of directions and disciplines encompassed within GES-2: dance, cinema, theater, music, a publishing program, community-based artistic practices and events, the urban studios and residency program, inclusivity and public programs, and of course, the exhibition part. This sort of planning is schizophrenic and exhausting. You need to be thinking several years ahead—imagining and designing in detail projects that will take place in a space that has not yet been built for people unaware of your existence—and then travel back to now, and amend all the projects over and over again to adapt them to the present.\nThis process has never been easy, but when the crisis broke, we had the time and space to rethink. As a result, we came up with the program preceding the opening of the first season of GES-2 and instigated by our desire to support local living artists and musicians, to introduce GES-2 as both a new, local venue and institution, and to show Renzo Piano’s architectural project in a never-to-be-experienced-again bare state—before it is used for programs and exhibitions. A series of newly commissioned site-specific works by Russian contemporary composers created and recorded in the aftermath of the global lockdown will play for a limited period of time as a sound accompaniment to the meeting between the city and its new institution. I hope that the conditions of the pandemic and the construction of the building will allow this pre-opening program to come true.\nIL: Before undertaking a building plan for the V—A—C Foundation, your curatorial practice was more nomadic in that it involved working on-site within different museums. How would this reexamination of the existing museum collections and knowledge manifest in the work that is undertaken at GES-2?\nKC: Indeed, in the ten years since its creation, the V—A—CFoundation has been engaged in mapping artistic processes focused on contemporaneity, the synchronization of present and past through dialogue, and the location of codes and tools to investigate important themes. For a long time, we did not have a building of our own and so operated outside the existing art infrastructure—doing projects, for example, at the Central Museum of the Armed Forces, Museum of Contemporary Russian History, Institute for African Studies, and GULAG History Museum, among others. We were able to occupy an unconventional position, one that allowed us to make a break in the continuous and undifferentiated history while also establishing new historical narratives through interventions involving contemporary artistic research. GES-2 will undoubtedly inherit this attention to thresholds and connections, and as such, aim to function as a storytelling institution—one that is fully immersed in the narratives surrounding Russia’s history and contemporaneity.\nWe have already designed a cycle of five seasons of programming, each including many exhibitions, performative, cinema, music, theatre and public programs entitled Holy Barbarians: Both Are Worse. Unfolding over the next three years, this incremental narrative is intended to engage critically with clichés and cultural tropes associated with Russia, mainly those projected from the outside, though also present within: great Russian literature, tyranny, the mother archetype, a propensity for melodrama, moral relativism, cosmos as an emblem of geo-cosmo-political superiority, etc. Among the clichés that will be explored—and the one that has inspired the title of the series—is the contradictory notion of “holy barbarism,” which embodies opposed phantasmal projections. This dilemma embodies, on the one hand, the age-old myth of Russian savagery and backwardness; and on the other, the (equally orientalist) idea of Russia’s irreducible uniqueness, “chosen-ness,” or even holiness. The format of a narrative in five parts, like a five-volume novel, is in itself a performative acting-out of the stereotype of so-called Russian literature-centrism. The choice of themes is guided by the fact that the clichés, in spite of (or, perhaps, thanks to) being deeply entrenched, raise questions that are relevant in a global context.\nThe first exhibition in the cycle, Santa Barbara. How Not to Be Colonized?, will feature a large-scale commission by Ragnar Kjartansson (Icelandic, born 1976), who is known for his interest in the emotional power of music and drama, in combination with contemporary Russian works exploring the carnivalesque in Russian culture from the 1990s onward. An attempt to travel back in time thirty years, this exhibition will reimagine the foundational myths of post-Soviet Russia and look at the images, cultural values, and ideas that have ingrained themselves in its collective consciousness since then. The starting point of this conversation is Santa Barbara, the first Western soap opera to be broadcast in Russia and the most enduring on post-Soviet television. Airing from 1992 to 2002, the show not only presented different cultural models and inspired an urge for self-determination, it also sparked resistance to Western homogenization, a movement in which Russian artists played a role. The Santa Barbara decade was a time for the reinvention of the self—at once emancipatory and carnivalesque—with myriad consequences, both intended and unintended.\nNext, we will look at the conception of truth and realism, which translates in Russian as “istina,” or scientific truth or truth as a religious category, and “pravda,” which is an ethical concept related not only to theory, but also to actions and deeds. Mother: Why Motherland? continues the inquiry into cultural representations of Russia via questions related to motherhood, such as care, labor relations, family, gender dynamics, and kinship; Kosmos Is Ours will explore the universal cosmological impulse as well as the colonial drive behind it; while, finally, Barely Audible will focus on a shift in tonality rather than in the cultural landscape, and reflect on the possibility of an institutional space as one of genuine intimacy that is free of transactional uses.\nIL: Museums are also important as they are fostering communities centered on learning and discussion. Are you working on that aspect prior to the opening of the building?\nKC: We started to work on that the very moment we started to develop programming for GES-2, and in the process, one thing was fundamental: the program should be conceived together with educators and community builders, and not only by curators. Exhibitions, educational programs, discussions, and community programs are proposed and debated by the larger group of curators and educators in the framework of the season. Through this back-and-forth, we intend to break the hierarchical structure of exhibitions, concerts, and education. We want to make sure that the community-building and educational programs are conceived of and thus perceived as equal to and complementary components of the narrative—as opposed to accessory or merely illustrative of the main program of exhibitions and live events.\nIL: You mention that one part of the program will focus on the dynamic surrounding the Western cultural colonization of Russia in the post-Soviet period of the 1990s. Do you also envision examining the Soviet Union as a colonial project, and the historical and still present cultural, infrastructural, and political interconnections therein—that is, to think about how Russia relates to post-colonial and decolonial debates?\nKC: Absolutely. The fourth seasonal program Kosmos Is Ours will look at who we are if we continue to expand our presence in time and space, and identify the local and pluriversal cosmologies currently in place and now forced to unite in a seemingly possible universality by various geopolitical regimes. This conversation cannot happen without meticulous investigation of the Soviet colonial impulses and structures, which is of course not possible without inviting participation of artists and curators from the states within the former USSR.\nIL: Earlier this year, large-scale demonstrations broke out across Russia against the arrest of Alexey Navalny and the ongoing corruption of the current government, generating a lot of reactions in the local and international media, as well as from Western governments and Russia itself. Are the roots and objectives of these protests adequately represented locally and in the international media, in your opinion, given the complexity of the situation? How do you see the role of cultural institutions in the process?\nKC: Both the mainstream Western media channels and the official local ones have oversimplified the situation. Indeed, both sides broadcast news in a predictable way, reaffirming the information warfare in a rather old-fashioned manner. And to be honest, local media channels are not that scarce now and don’t sound unanimously; mostly online, some of them do offer nuanced consideration of the moment, addressing different possible futures and vectors, but this kind of analysis is outside that of the international mainstream and the official local mass media outlets.\nAs we all know, culture is a continuation of politics (in the broadest sense of the word) by other means, and I think an institution always aspires to contribute to creating a more nuanced portrait, to offer a mirror reflecting society’s fears, biases, and internalized clichés as well as its strength and common futures. But of course, an institution is not only aiming at representation but also at creating a platform for active engagement, polemics and debates.\nIL: Some of the artists you have commissioned through the V—A—C Foundation, such as Kirill Savchenkov (born 1987) and Arseniy Zhilyaev (born 1984), have been referencing visionary futures or science fiction from the past. What do you think we could draw from this sort of work in terms of thinking about the future of our museums, art ecosystem, and the planet?\nKC: Both artists that you mention indeed engage with different modalities—not necessarily referring to science fiction from the past, but rather different systems. Savchenkov deals with knowledge systems and practical skills—from various cosmological systems to paramilitary, meditative and new media practices—that human intelligence can navigate and use to survive in the world bombarded by crises, global instability and in the potential conditions of the posthuman future. Zhilyaev explores existing intersections, or creates new ones between art, philosophy, and science; often these encounters take place in an imaginary institutional space, a museum, but at a moment in time that is unreachably remote from the present. It is always a reinvention of an art institution, rearranging the system components of knowledge and practice to get to some common future beyond geographies and prescribed functions. I know how difficult it may be for an institution to follow and trust artistic intuition while building a new museum, but I believe that doing so is the only way for all of us to get there.']	['<urn:uuid:e215e34e-e6f8-4931-a1e8-937c4a41f39b>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T10:32:05.666290	7	24	1934
100	agile methodology benefits vs project management success factors	Both approaches focus on successful project implementation but have different emphases. Agile methodology benefits include flexible response to change, iterative design through sprints, continuous customer feedback, and tripled probability of project success according to analysis of 10,000 projects. Meanwhile, key project management success factors include employee involvement from the start, measurable objectives, proper communication, change culture management, and expert support. Both approaches emphasize the importance of team collaboration, customer focus, and adaptability to change.	"['You completed your audit 4.0. What do you have to do now to achieve your business’s digital transformation? Where do you begin?\nHere you are, recommendations in hand, facing a plan providing you with a clear portrait of your business’s level of digital maturity.\nYou now have to take action to develop this maturity in line with your strategic planning. But where do you start to achieve your ambitions?\nStep 1: Determine project priority\nThe 4.0 diagnosis includes a digital plan presenting an overview of the intermediate and digital transformation projects to be implemented.\nAt a certain point, the business will have to make a final selection of the projects to be implemented based on its budget and implementation plan. It is therefore necessary to determine the critical path for achieving the objectives of the various strategic and operational directions established.\nTo help you categorize your actions objectively, we suggest that you define some criteria and give them a summary score (between 1 and 5).\nYou could, for example, use the following criteria:\n- Criticality: Is your project critical? (5 = very critical, 1 = less critical);\n- Immediate realization: Does your project depend on the implementation of other projects?\n(5 = project without other prerequisite projects, 1 = project with several other prerequisite projects);\n- Budget and expected return: Will the return on investment on your project be quick or very significant (5 = project with high expected return, 1 = project with lower expected return);\n- Timeframe: Is the expected end date of your project fixed and achievable (5 = very achievable within expected timeframe, 1 = improbable or unrealistic timeframe);\n- Risk: Does your project involve risks that could negatively affect the company’s performance if they were poorly defined? (5 = low risk, 1 = high risk);\n- Impact: Will the completion or non-completion of your project have an impact on the achievement of your objectives? (5 = significant impact, 1 = no impact).\nThen, you will have to add up the scores to obtain an overall score. Higher scores will determine the priority of projects to be implemented.\nYou will therefore be able to better categorize your projects based on the following priorities:\n- Priority no. 1: important and urgent;\n- Priority no. 2: important and non-urgent;\n- Priority no. 3: not important and urgent;\n- Priority no. 4: (abandon): not important and non-urgent.\nStep 2: Analyze project feasibility\nOnce you have finished prioritizing your projects, some of them with a high number of variables will require more rigorous analysis to determine in greater depth the difference between what the 4.0 diagnosis predicts and the actual situation.\nThe feasibility analysis therefore confirms whether or not the optimistic view of the digital plan is valid in your situation. It is, to some extent, the ultimate test before making a definitive commitment to the project.\nConcretely, you will conduct:\nStep 3: Establish project charter\nNow that you know your priority projects and are aware that the gap between the 4.0 diagnosis your situation is narrowed, it is essential to finally define the project.\n- Draft a preliminary statement of the purpose of the project.\n- Confirm and specify its objectives.\n- Determine the project’s main actors.\n- Define the project’s authority (its champion).\nThis charter serves, among other things, to authorize the project and acts somewhat as a contract between the project champion, the various stakeholders and the project team. Of course, you must obtain senior management approval before starting your project.\nStep 4: Manage projects\nThe management steps of technology projects emerging from an industry 4.0 audit are not so different from those of another type of project, i.e. planning, execution, control and finalization.\nHowever, not everyone is comfortable with technologies. This is one of the reasons why projects relating to the digital shift bring their share of challenges.\nYou will have to adapt your management style, as the projects will be affected by new elements, that companies are not familiar with:\n- internet of objects,\n- additive manufacturing,\n- system integration,\n- autonomous systems,\n- augmented reality,\n- massive data,\n- cloud computing.\nKey factors for success:\nGet your employees involved in your digital transformation from the very beginning of the project\nAn employee who is engaged and committed to the success of your growth projects will become a positive leader and excellent ambassador for your employer brand.\nPut employee health and safety first\nNew technologies and robots will require special attention and handling. Standards, employee interaction, workspace reconfiguration and risk management are matters to consider in your transformation.\nNot everyone has the same digital maturity, and a technology project involves resources and expertise. Start on the right track with the support of business advisors.\nEstablish measurable objectives\nWithout a goal, it will be difficult for you to measure the impact of any change. Assess your results regularly, that way you can consider improvement measures quickly instead of lingering in a doomed situation.\nAim for small victories\nWhat pays off quickly is extremely motivating and instills the desire to surpass oneself and go even further.\nChallenge your business model\nAsk yourself: Does it contribute to or hamper your progress.\nInvest in training\nTrained and informed employees will be more productive and more likely to present new ideas and innovate. Training is an important factor when it comes to considering your future in a company.\nDon’t overlook the importance of proper communication\nTaking the time to explain the reason for the changes will promote a better understanding on the part of all staff and it will be easier to get your resources on board with on this new adventure.\nDevelop a change culture management plan and listen to your employees\nYou would be surprised to see how much they can contribute in ideas and suggestions for improvement.\nDon’t try to do it alone\nYour projects will require a good financial investment and will require much from your resources. Being supported by experts will make the task manageable for all.\nIn a context of digital transformation, the magnitude of the technological challenges facing organizations is relatively large. For most SMEs, a technology project can be a daunting adventure.\nBeyond competitiveness, you will have to take a look at your how you manage your human resources who are essential to the growth of your organization. A technological project can quickly become demanding for your employees in addition to taking them out of their comfort zone. That’s why it’s important to implement ways to retain your resources and keep them mobilized.\nFeel free to call on our experts in information technology, business strategies and models, organizational performance and human resources management to assist you in all stages of the implementation of 4.0 projects, such as:\n- the realization of an information technology master plan;\n- the management of technological projects;\n- guidance in selecting computer systems;\n- guidance in implementing computer systems;\n- business strategies and models;\n- organizational performance;\n- human resource management.', ""The cornerstone of business success in the digital world is the ability to adapt. For an organization to thrive in a fast-paced world, they must be able to respond to new opportunities and threats quickly to come out stronger on the other side. Around the world, more and more organizations are embracing agile methodologies. During the COVID-19 pandemic, many organizations have accelerated their shift to agile with the hopes of speeding up project delivery and enhancing customer experience.\nFor digital businesses, agile methodology is a compelling approach for a business that wants to succeed in the face of fierce competition and escalating customer expectations. Modern businesses use digital transformation as a tool for adaptation. Ongoing digital transformation enables an organization to stay at the forefront of technology trends and the cutting edge of user experience. Digital transformation is a relatively easy strategy to design, but the execution of the strategy is where many businesses run into barriers. This is where Agile comes into play.\nAgile for digital transformation\nAgile methodology is an iterative design process wherein requirements and solutions for a project constantly evolve. Cross-functional teams collaborate in “sprints” where a working concept is developed and shared with end-users. Feedback is then taken into consideration for the next sprint, and the cycle is repeated until the solution is finalized. The most important trait of agile development is that it encourages rapid and flexible response to change. This way, executives can first envision a strategy for digital transformation, then ensure it aligns with the realities of the business environment. Agile methodology enables organizations to derive relevant data and formulate adaptive plans for digital transformation. Agile methodology, when adopted, has the potential to promote flexibility, speed, and continuous improvement. An agile organization ultimately empowers its units to be more responsive to change and adapt to local customer needs, ensuring executive goals are met while retaining the ability to innovate and adapt.\nCreating an agile environment\nAgile is a way of thinking as much as it is a project management methodology. This means that businesses have to train themselves to think in the new mindset and employees must open themselves up to new ways of working. The methodology promotes adaptation and provides the business with the tools needed to achieve digital transformation. An organization that is agile needs to break down the business into units, capitalize on the skills of cross-functional teams and encourage collaboration. The responsibility of an agile team is to be flexible, focused and work around shorter planning cycles. A good IT leader should be a pillar of motivation to the team and encourage transparency especially when the team is working to achieve an objective.\nBenefitting from an agile culture\nAn organization that is agile orients itself to customer needs and is able to optimize customer experience on an ongoing basis. The backbone of an agile organization is the right technology ecosystem with access to the right data. This allows you to create a testing and learning environment where prototypes can be optimized. Using sprints, the organization can take advantage of this environment to continually adapt their products and customer experience to the changing environment.\nDespite the growing popularity of agile methodology, few organizations are implementing the process. About 36% of organizations report implementing a singular framework across teams with minimum management support and experience in agile methodology. The largest barrier to digital transformation in organizations is a culture where change is not encouraged.\nAgile methods act as a catalyst for innovation, but accepting and implementing these methods within an organization takes time. As digitization progresses, businesses need to respond quickly to changing market demands and develop new business models. The technology exists, yet product innovations and projects often continue to fail because companies are structured with too much rigidity, suppressing creativity and innovation. Which is why companies should focus on agile methods to drive innovation.\nAgile Values Explained:\nAt this point you may be wondering a bit more about the specifics of agile, so here is a short history lesson about agile and how these methods came to be. 17 methodologists defined a manifesto that encouraged better and more reliable ways to develop software. Based on this manifesto, a collection of values that define the criteria to be employed in agile software development processes was formulated. The four values defined in the manifesto formed the foundation of the agile movement. They define preferences rather than alternatives and encourage a focus on particular areas without eliminating others. The following are the values of the Agile Manifesto:\n1. Individuals and interactions rather than processes and tools\nSoftware systems are created by teams of people and in order to make a project successful, effective participation is mandatory for each team member. This includes, but is not limited to, the customers, modellers, project managers, testers and programmers. This value places more emphasis on people and how they are able to work together. If this is not a primary factor of consideration, even the best tools and processes will not be of any use. As much as tools and processes are important, they can’t yield the same results as working together effectively.\n2. Working software rather than comprehensive documentation\nIt makes more sense to work in a manner through which you are able to produce software much faster and are able to meet the needs of users. Users will most definitely have an easier time understanding the software you come up with than the complex diagrams that describe its internal functionality or abstractions of its usage. Documentation is an invaluable guide that helps people understand the reasons behind the creation of software and its functionality. Nevertheless, the primary goal of software development is creating software and not documents.\n3. Customer collaboration rather than contract negotiations\nOnly your customers can tell you what they need. They may not be equipped with the skills that can exactly specify the system and most likely, they won’t get it right the first time. It’s often hard to work together with your customer but it tends to build the foundation for better relations. Yes, you need to have a contract with your customer but of greater importance is to understand that everyone has their rights and responsibilities. This helps to formulate a contract, although the contract is not meant to be a substitute for effective communication. Working closely with customers is essential for every developer; you need to invest as much effort as possible to discover the needs of your customers and educate them along the way.\n4. Responding to change rather than following a plan\nEveryone is bound to change their priorities. This happens for several reasons. As the work progresses on systems, the understanding of project stakeholders on the problems being faced and the software being developed changes. This is also the case with the business environment and technology, but not always for the better. Change is inevitable and applies to software development of the same magnitude. This should be reflected in the process. It is also important to understand that there’s actually nothing wrong with using a project plan. As a matter of fact, every project needs one. However, this project plan should be malleable and allow room for change since situations change that might render the plan irrelevant.\nThese value statements were created to ensure better practices in the field of agile software development without isolating any member of the team. In each one of them, there’s something almost everyone instantly agrees to and all participants admitted that the creation of software is the primary goal of software development.\nDigital transformation is a continuous process and business leaders are tasked with the challenge of embracing it in order to drive change from the top down. Implementing agile methodology is necessary, not only as a project framework but as a culture shift within the organization.\nThe roots of agile methods lie in software development. Since software is the basis for all digital business models, agile methods are gaining increased acceptance. In addition, rapid technological change is forcing CIOs to rethink their digital transformation strategies. Agile methodology is no longer used only in IT. In a survey by the certification organization Scrum Alliance, more than half of those surveyed stated that other areas of the company use agile methods.\nInnovation, speed and team productivity\nAn analysis of more than 10 000 projects revealed that the use of agile methods more than triples the probability of success. Many companies have recognized that the challenges of digitization require an agile environment to stay ahead of changing trends. However, employees tend to have difficulty transferring newly acquired knowledge into their organizations. Why is that? Companies often have a vertical, hierarchical culture that has existed for many years, which prevents rapid innovation. In many cases, agile contradicts the existing corporate culture.\nAdapting with agile\nAdapting to agile doesn’t mean completely restructuring an organization in one day. If you want to accelerate processes and make teams more innovative, you can first test agile methods in suitable projects. This helps to reduce prejudices and to slowly anchor 'agile' in the minds of your employees. The most important thing to remember? Involving your own employees, equipping them with the necessary knowledge for modern work and sharpening their understanding of agile work processes and team leadership. If you know more and understand more, you will be able to tackle projects in new ways.\nThanks for reading!""]"	['<urn:uuid:2e33b78e-ef47-4763-a245-bff8d8f0b027>', '<urn:uuid:9473848a-68b7-430a-9b56-651cda852499>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T10:32:05.666290	8	74	2735
