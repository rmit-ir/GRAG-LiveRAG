qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	Which pioneering figures from the Indian community helped establish important institutions in Singapore's early development?	Several Indian pioneers made significant contributions to Singapore's early development. Mr G. Sarangapany founded the Tamil Murasu newspaper, while Mr Rajabali Jumabhoy, a Gujarati businessman, became the first president of the Singapore Indian Chamber of Commerce and Industry. Additionally, members of the Sikh community contributed significantly to the armed forces and judiciary.	"['Prime Minister Lee Hsien Loong lauded the contributions that Indian-origin people had made to Singapore, and said the Republic would support India\'s development where it could.\nRaising a toast to his visiting Indian counterpart at an official lunch at the Istana yesterday, PM Lee said Indian immigrants had played a large part in building modern Singapore and form an essential part of its multiracial and multi-religious mix.\nAside from Indian labour that helped build many landmarks - including the Istana - Indian businessmen, traders and civil servants were here from the start. No fewer than five foreign ministers of independent Singapore had been of Indian origin.\n""Singapore today has a thriving community of Indians who are well integrated in our society and form an essential part of our multiracial, multi-religious mix,"" he said. ""India has been very much a part of Singapore\'s history.""\nPioneers such as Mr G. Sarangapany founded the Tamil Murasu newspaper, while Mr Rajabali Jumabhoy, a Gujarati businessman, was the first president of the Singapore Indian Chamber of Commerce and Industry.\nPART OF SINGAPORE\'S HISTORY\nSingapore today has a thriving community of Indians who are well integrated in our society and form an essential part of our multiracial, multi-religious mix. India has been very much a part of Singapore\'s history.\nThere were also religious leaders like Maulana Abdul Aleem Siddique from Meerut who established social welfare organisation Jamiyah in 1932 and the Inter-Religious Organisation in 1949.\nSikhs also contributed in many fields, including the armed forces and judiciary.\nTracing the connection with India to the 13th century, Mr Lee noted that Singapore had been part of the Majapahit Empire, one of the Indianised kingdoms in South-east Asia.\n""Long before the term \'soft power\' was coined, Indian influences left their mark on our traditions and landscape,"" he said, noting that even the names Indonesia and Indo-China attest to India\'s far-reaching influence.\nBesides, Islam had come to the region through India.\nMr Modi\'s visit to Singapore, he said, came at a significant time when the two nations were celebrating the 50th anniversary of diplomatic relations.\n""We believe India is a great Asian country that has a lot to contribute to the region and we wish India well in its reforms and development,"" Mr Lee said.\n""We hope to develop our economic cooperation, air links and other fields of cooperation and we are happy to support India\'s development where we can, because this is a mutually beneficial relationship.""\nMr Lee said Singapore looked forward to India playing an important role in the region, promoting regional security and peace, integrating its economy with the rest of the region, and helping to shape an open and inclusive regional architecture.\nThe close ties started with Mr Lee Kuan Yew, who admired Indian leaders like Pandit Nehru for their peaceful struggle for freedom, commitment to secularism and leadership in the non- aligned movement.\nIndia, he noted, had declared a national day of mourning for Mr Lee when he passed away in March. Singapore was grateful to Mr Modi for this special gesture.']"	['<urn:uuid:33fdbc16-8c37-4d71-9c44-268ef813352c>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	15	52	508
2	facility manager seeking methods eliminate harmful microorganism growth cooling tower system	Cooling towers provide ideal conditions for biological growth due to their hot, humid environment and abundance of nutrients. Microorganisms like algae, bacteria, and fungi can enter through water supply or air, forming biofilms that reduce thermal efficiency. Control and prevention of biological accumulation can be achieved using FINEAMIN biocides applied as shock treatments, combined with biodispersants and non-ionic surfactant products that remove existing bio deposits.	['Cooling Water Systems Solutions\nCooling water imposes numerous challenges in the operation of industrial facilities and its quality is subject to always keeping track of and controlling of major issues like corrosion, biological contamination (microorganisms like bacteria, algae, fungus etc), fouling and scale.\nUnder these difficult conditions, the water quality in cooling towers determines the reliability of the facility and the lifetime of the equipment: towers, piping, heat exchangers.\nAn appropriate water treatment solution for cooling towers is needed in refineries, petrochemical facilities, paper and timber industry, pharmacy, food industry, air conditioning technology, construction and many others.\nIn most cases, film-forming amines based treatment is a sufficient and environmentally friendly alternative to conventional anti-corrosion water treatment methods like hydrazine. Its action is manifested through the creation of a microscopic, hydrophobic film on the metallic surface, by using polyamines, which are naturally connecting to loose nitrogen atoms present in the metal. This prevents oxygen to attack the metallic surfaces, while improving the heat transfer.\nRegarding the particular case of cooling systems and their harsh operational conditions, the polyamines represent the anticorrosion solution as one of the components of a more complex water treatment program. Used along with biodispersants, phosphonates and co-polymers, polyamines (FINEAMIN 06) are increasing their effect and protecting the whole system against corrosion and future deposits by forming a barrier between the metal and the water carrying all kinds of contaminants.\nHot, humid and overcrowded with nutrient substances, cooling towers are the ideal environment for unwanted biological growth. Microorganisms such as algae, bacteria, fungi that enter the system through the water supply or by the the air drawn through the tower, contribute to the formation of a biofilm on all surfaces reducing thermal efficiency.\nControlling and preventing such kind of biological accumulation can be done by using FINEAMIN biocides injected as shock treatments, along with biodispersants, surfactant, non-ionic products that eliminate the already existing bio deposits.\nBuild up of minerals in the cooling tower water and pipes clogging with debris and dirt are two of the main things that affect heat exchange surfaces and prevent the even distribution of water through out the cooling system increasing operational costs.\nCrystal formation and deposits can be inhibited by hardness stabilizers (FINEAMIN 91), mixtures of polymers and copolymers, phosphonates and copper inhibitors that modify crystal growth by disturbing the nucleation process. Deposits are blocked and the modified crystals prevented from sticking to each other and metal surfaces. These products are used together with biodispersants and alkalizing amines (FINEAMIN 06).\nCooling towers common problems\nFINEAMIN® Combined water treatment\nBefore FINEAMIN® biological control\nAfter FINEAMIN® biological control\nOur range of services and products for cooling towers:\n- Evaluation through sampling, water analysis and inspection of the plant.\n- Corrosion and scale inhibitors for cooling water systems, air scrubbers, air conditioners\n- Biocides and biodispersants against algae and bacteria growth\n- Efficent anti-legionella water treatment\nA custom-made water chemical treatment increases the efficiency and the economic viability. Any water management plan concerning the chemical regime of the cooling system will be established ONLY with our specialists and depending on the system parameters and contamination level.']	['<urn:uuid:87e844c7-d947-45a8-82eb-5bd529210343>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	11	65	520
3	what mechanisms does brain use restore function naturally post stroke healing	The brain uses three mechanisms to promote spontaneous recovery after stroke: reperfusion (restoration of blood flow to damaged areas), edema management (reducing swelling), and diaschisis reversal (reactivating depressed neural activity when blood flow improves and swelling resolves).	['Spontaneous recovery occurs after a stroke when the brain naturally repairs some of the damage it sustained. This recovery is usually seen within the first six months after a stroke and relies on several neural mechanisms.\nToday’s article will explain how spontaneous recovery after stroke works, plus some helpful ways to support your brain’s natural repair mechanism.\nWhat is Spontaneous Recovery?\nAfter a stroke, the initial damage due to lost blood flow results in two distinct areas of damage: the core and the penumbra.\nThe core refers to the area directly hit by the stroke. This area is considered dead and non-salvageable. However, the functions that were controlled by the core area can still return by activating neuroplasticity (more on this in a moment).\nOn the other hand, the penumbra, which is the area of the brain that surrounds the core, is damaged but not destroyed. Because it is damaged, it does not function correctly. But if this damage to the penumbra can be healed, then many functions will be restored. This is where spontaneous recovery comes in.\nSpontaneous recovery refers to the brain’s innate ability to repair and salvage the penumbra. When this occurs, some functions, such as speech, might naturally return without any intervention.\nThis happens because, when the penumbra was damaged, these functions were impaired, but they were never lost entirely. Once the damage resolves, then the brain can regain those skills.\nBut how does spontaneous recovery work? And are there any ways to promote it? We’ll answer those questions in the sections below.\nHow Spontaneous Recovery Works After Stroke\nThere are three mechanisms that the brain uses to promote spontaneous recovery after stroke. These three are reperfusion, edema management, and diaschisis reversal:\n- Reperfusion. This refers to the restoration of blood flow to the damaged areas. The brain can accomplish this in a number of ways, one of which is by elevating blood pressure. For this reason, researchers recommend caution when trying to lower a stroke victim’s blood pressure. While extremely high blood pressure can be dangerous, bringing it down too low might hamper spontaneous recovery.\n- Edema management. Decreased oxygen in the brain causes increased swelling (edema), which cuts off blood supply and causes further damage. Therefore, a major part of spontaneous recovery involves reducing edema. This can happen naturally but sometimes requires medication or oxygen therapy. Once the swelling decreases, blood flow can return, and function is usually restored.\n- Diaschisis reversal. Diaschisis, also known as neural shock, refers to depressed neural activity due to loss of input from damaged areas of the brain. When blood flow improves and swelling resolves, these areas become more active and functional again.\nDiaschisis reversal is the most important part of spontaneous recovery. It is the reason why some of your abilities might suddenly return after stroke.\nSpontaneous recovery only occurs during the acute stages of stroke rehabilitation, which usually lasts around six months. But during this time, you can also promote further recovery by taking advantage of your brain’s natural repair mechanisms.\nTaking Advantage of Spontaneous Recovery After Stroke\nWhile spontaneous recovery mostly occurs on its own, you can increase the number of abilities you recover by harnessing your brain’s neuroplasticity.\nNeuroplasticity refers to the brain’s ability to form new neural pathways. This allows the brain to transfer functions that were once held in damaged parts to new, healthy areas. You can activate neuroplasticity through consistent, therapeutic exercise.\nDuring the period when spontaneous recovery occurs, the brain also enters a heightened state of plasticity. This essentially means that neuroplasticity will operate on turbo-drive, and you can make incredible gains in a short amount of time.\nResearch shows that stroke patients who participate in rehab make fuller recoveries than those who do not, even though both might experience spontaneous recovery. Therefore, to enhance your recovery, do not just wait for your brain to heal on its own. Give it a boost by practicing stroke rehab exercises every day. You might just surprise yourself by how much progress you make.\nWhat Happens After Six Months?\nAbout six months after stroke, spontaneous recovery will cease, and plasticity will also decrease. This leads to a recovery plateau, where it might seem as though you have peaked in the progress you can make.\nIn fact, many doctors and therapists used to believe that once a patient reached this stage their recovery had ended. They would, therefore, discourage their patients from continuing with therapy.\nToday, however, research has shown that recovery can continue long after six months. That is because you can still activate neuroplasticity even during a plateau. It might take longer to see results than it used to, but recovery still continues. Even five and ten years after a stroke patients can keep regaining abilities.\nTherefore, if you have already passed the stage of spontaneous recovery, do not despair. If you can persevere with your therapy, you still have hope for improvement.\nUnderstanding the Brain’s Natural Healing Process\nSpontaneous recovery is part of the brain’s natural healing process after stroke. It helps patients regain many of the cognitive and physical abilities they have lost.\nHowever, spontaneous recovery is not the only way to regain function after stroke, nor is it the most reliable. Instead, activating neuroplasticity through consistent exercise is still the surest way.\nEven if your progress seems slow and tedious, persevering with therapy will help you recover more movement and cognitive functions. Good luck!\nFeatured Image: ©iStock/Zinkevych']	['<urn:uuid:55014b63-50d5-42b1-831c-83e9f0cf90e4>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	11	37	908
4	How do the Great Pyramids and Hanging Gardens differ in preservation?	The Great Pyramid of Khufu is the only ancient wonder that has survived to the present day, while the Hanging Gardens of Babylon have completely disappeared. In fact, some modern scholars believe the Hanging Gardens may have been fictional, as no firsthand accounts or Babylonian inscriptions mentioning them have been found. In contrast, the Great Pyramid has stood for over 4,000 years and remained the tallest building in the world until the 19th century.	"['History is very fickle in what it preserves, as a visit to a museum will\nreveal. Some museums do preserve wonderful statues and priceless jewelry\nbut, more often, the treasures are broken pieces of pottery, simple tools,\nweapons, objects of everyday life or, quite often, the items from a trash\nheap. That which is most treasured does not stand any better chance of\npreservation than the most common object.\nOne of my favorite poems is Ozymandias by Shelley. It tells of a pedestal\ninscribed with the words ""Look on my works, ye Mighty and despair!"" The\nirony is that all around is nothing but desert. The great Ozymandias built\nwonderful things for all to marvel at, but nothing remained but the mocking\nOf the seven wonders of the ancient worlds, only the Great Pyramid of\nKhufu remains intact. These were marvelous works in their time and probably\nthought to last forever. In view of the frailty of things, how is it possible\nfor genealogists to preserve their work? If the Hanging Gardens of Babylon\ndisappear, how can anyone insure that the history of the Barber family\nwill survive 50 years?\nPeople with all kinds of electronic gadgetry at their fingertips speak\nof how they can computerize, scan, digitize and otherwise preserve data\nelectronically. The truth is there is no sure way to store information\nto guarantee it will last for 100 years. Barring a direct hit, the Constitution\nof the United States in its sealed, guarded, bombproof case, will probably\nbe around, but most individuals do not have the resources to protect materials\nto that extent. So, what should you do with your research?\nKeep Everything in Perspective\nFirst of all, and this may sound cold-hearted, but keep everything in\nperspective. Most records are not as rare as they once were, so if something\ndoes happen to your research, chances are that much of the information\nwill be intact somewhere. Millions of public records, once subject to\nfloods, rats and fires, have now been microfilmed with several copies\nin different locations.\nThe original copy of my great grandparents\' marriage in 1846 is a case\nin point. It could easily have met with many mishaps over 150 years. Now,\nI have photocopied it, sent copies to my siblings, scanned it, and there\nis also an organization in their county of marriage collecting copies\nof all marriages. A copy of the record is in many places, so the record,\nif not the original, stands a much better chance of surviving.\nThat said, if you have spent countless hours finding records and connecting\nfamilies and want that research preserved, what is the best way to do\nPreserving Your Work\nTo preserve your work you need to use many techniques. Fortunately, there\nare many methods available for publishing, storing and archiving information.\nTry to use more than one to improve your odds. Here are the key points:\nInterest your children, grandchildren or a relative in genealogy\nso they will want to preserve the records.\nUse as many different methods as possible to store the material.\nKeep up with the technology and move your material when newer methods\nShare information, but use discretion. Share with people who will\nrespect it and not misuse it.\nHere are some options for preservation, as well as the pitfalls of each\none. Take a few moments to consider the possibilities. Are you doing what\nyou need to do to keep your research safe?\nUse Genealogy Programs for Organizing and Printing Data\nComputer databases are a wonderful way of organizing material but a poor\nway to preserve it for posterity. First of all, does anyone else in your\nfamily know how to run your program or is this hobby yours alone? If you\ncan pass your data on to someone else who knows how to operate the program,\nthen you have at least shifted the problem to other shoulders. However,\nif there is no one to pass it on to, it could go into limbo.\nHow many times in the last 5 years has your program had an update? If\nyou leave the information in a computer program, chances are very good\nthat by the time your grandchild or cousin takes an interest, the program\nwill be obsolete. One lady I know of left her data to a public library\nalong with a copy of the computer program. The problem was that the next\nperson who wanted to see what she had done had no idea how to operate\nAs I said before, computer databases are a wonderful way of organizing\ninformation, so use that strength to help preserve your data. If you use\na program, it should be able to print hard copy reports. Get your information\ninto a condition that lets you create a logical, consistent report summarizing\nyour research. All your research notes and documents are wonderful (to\nyou) but if your heirs are not into genealogy, most of your papers will\nprobably be tossed or stored in less-than-ideal circumstances. You need\nto have your information organized such that a non-genealogist can step\ninto it, see what is there, and take it home in a manageable form.\nBe Aware of the Same Pitfalls with Scanning Software\nBy the same token, many genealogists put a lot of faith into scanning\npictures. Pictures do fade, but even a faded picture can show us something\nand can often be restored. A CD with lots of perfect pictures is useless\nwithout the equipment to open the files. Technology is changing so rapidly\nthat the process you use today to digitize photographs may not be readily\navailable in ten years. If you are saving pictures electronically, you\nhave to be aware of changes and keep up with the latest advances. Usually\nprograms will be backward compatible for a few versions, but if you don\'t\nkeep moving the material to each new version, you will find that what\nyou end up with is not compatible with the available equipment.\nOne person I know said he intended to keep up with the latest methods\nand move his material. However, once he is gone, if his heirs are not\ninterested in computers and/or genealogy, it could soon become out of\ndate. I read recently (and I don\'t know if it is true) that the 1960 U.S.\ncensus is stored on tapes that can no longer be read because the machines\nno longer exist. If the U.S. Government, which has relied on census information\nsince 1790, cannot keep up with migrating information to useable technology,\nI don\'t know how one individual can cope. When was the last time you moved\nyour data to a new program that required editing or updating and how long\ndid it take you to complete the task?\nWhile paper is one of the most fragile things we use, it can be amazingly\nhardy, and the written word has the advantage of being understood by almost\neveryone. I have an announcement from a local newspaper of my great-great\ngrandparents\' marriage in 1800. Over the years, every family member that\ncame across this piece of newspaper could immediately recognize that it\nwas something unique and kept it. Even though it is 200 years old, I still\nhave it and anyone can read it.\nThe problem with paper is that many things were printed on poor-quality\npaper. For it to stand the best chance of survival, acid free paper is\na must. It also should be stored in a room where the temperature or humidity\ndoesn\'t vary too much. A library, either private or public, often has\nHave Multiple Copies in Multiple Places\nNo matter how hard we try to create the best environment, disaster can\nalways strike and paper can be destroyed. The best guardian against this\nis to create multiple copies which are stored in multiple places. Once\nyou have organized your material and printed it out, make sure that several\nfamily members receive a copy. Even non-genealogists usually keep genealogy\ninformation if it is in a form they can easily understand. Libraries are\nthe best keepers of books, so also donate copies to genealogical libraries.\nIn this way the information may also be microfilmed or otherwise electronically\nor digitally reproduced.\nStore Original Documents Separately\nSeparate the truly wonderful things from the mundane. Nowadays we can\nmake copies so easily and inexpensively that we tend to accumulate too\nmany of them. A long description of an area may be interesting, but is\nnot as important as original documents and pictures. Keep your very unique\nand irreplaceable pieces in a special place a carefully labeled\nbook or box that anyone could open and see that this is not to be thrown\nout. Photocopies of books and documents, even official certificates, are\nimportant but can be replaced if the source has been noted in your research.\nAcid free sheet protectors, papers and boxes along with old-fashioned\nfireproof and waterproof strong boxes might be your best answer for storing\nWhen it comes to electronic formats for storing your data, you should\nbe concerned not only with protecting the existence of your data, but\nalso the integrity of your data. There are a few things that you can do\nto safeguard it.\nA GEDCOM is a very easy way to exchange information, but also one that\nshould be used with caution. It is very useful if several people are combining\ndifferent branches of a family to combine into one database to be shared\nor from which to print a family history. It can also be handy if someone\nwants to compare information. However, it is too easy for the recipients\nto just incorporate into their own databases and use as their own without\nacknowledging or evaluating. You may put in links that you consider doubtful\nand you carefully document and otherwise indicate your unsureness. However,\nthe receiving program may not import these notes or surety indicators\nso things you don\'t want publicized as definite suddenly become written\nI put a lot of notes and documentation into my databases and I do not\nlike to send GEDCOM files to people. I will send them text files that\nthey can look over, read and evaluate and, if the material looks valid\nto them, they can enter into their own programs in their own way. I have\nworked on some lines for years and I am surprised when people ask me to\nsend a GEDCOM which represents 25 years of hard research so they can add\nto their research in 10 minutes.\nWorld Wide Web\nThe Web is another way to distribute material and store it outside your\nhome. It is very easy to create a Web page from many programs and you\ncan post your research for anyone to see. However, as with other storage\nmethods, you shouldn\'t rely on it alone, because there are downsides.\nIf you have a Web page, you will probably have it posted through an Internet\nprovider so it will not be on your computer. The Internet provider\'s computer\nis subject to crashes, hacking or the owner just shutting it down and\nAnother problem with the Web is that you not only have to worry about\nprotecting the data from loss, but also from corruption. When you post\nmaterial on the Web, you lose all control over it. A person can take whatever\nyou publish, copy it, mix it with other material and re-circulate it with\nlittle effort. It is more difficult to do this with printed material.\nMost researchers are willing to share information, but the biggest complaint\nis people who take information and publish it as their own without giving\nany credit to the person who did all the work. Many researchers have concluded\nthat you should not put everything on the Web. Give enough information\nso the reader will want to get in touch with you to find out more. In\nthat way you can also request information from them instead of always\ngiving it away.\nSimilarly, I have very mixed feelings about huge databases compiled from\nindividuals\' research. They are good because in many ways they can serve\nthe way printed queries used to. If you find someone researching the same\nfamily, you can get in touch with them and compare information. The downside\nis that if the compiler shuts down his Web site or goes out of business,\nyour information will no longer be available to new researchers. You also\nhave no control over the distribution and use of the databases. It is\ntoo easy for people with a casual interest to just download the information\nand dump it into their own programs.\nProcrastinating: One of the Biggest Dangers\nAs you can see, there are many ways to preserve your hard work, and that\nchoosing more than one method is important. However, one of the biggest\nstumbling blocks when it comes to safeguarding your research is inherent\nto genealogy itself: it is never finished. You think you will get it all\norganized once you have had a chance to study all those land records you\ncopied last summer. Or, you need to go through that box of letters and\ndocuments your cousin found in his attic. Or, you need to write up some\nof those wonderful stories your mother told you. Most genealogists prefer\nthe chase to organizing. If you will just take the time to (semi)-finalize\nat least some of your work now, not only will your heirs benefit, but\nyou too will find it much easier to evaluate new research when the older\nmaterial is so easy to use.', 'The amazing works of art and architecture known as the Seven Wonders of the Ancient World serve as a testament to the ingenuity, imagination and sheer hard work of which human beings are capable. They are also, however, reminders of the human capacity for disagreement, destruction and, possibly, embellishment. As soon as ancient writers compiled a list of “seven wonders,” it became fodder for debate over which achievements deserved inclusion. Ultimately, human hands joined with natural forces to destroy all but one of the wonders. Furthermore, it is possible that at least one of the wonders might not have existed at all. Still, all seven continue to inspire and be celebrated as the remarkable products of the creativity and skill of Earth’s early civilizations.\nGreat Pyramid of Giza, Egypt\nThe Great Pyramids, located at Giza on the west bank of the Nile River north of Cairo, are the only wonder of the ancient world that has survived to the present day. The three pyramids–Khufu (Cheops), Khafra (Chephren) and Menkaura (Mycerimus)–were built between 2700 B.C. and 2500 B.C. as royal tombs. The largest and most impressive is Khufu, which covers 13 acres and is believed to contain more than 2 million stone blocks that weigh from two to 30 tons each. For more than 4,000 years, Khufu reigned as the tallest building in the world. In fact, it took modern man until the 19th century to build a taller structure. Amazingly, the nearly symmetrical pyramids were built without the aid of modern tools or surveying equipment. Scientists believe that the Egyptians used log rollers and sledges to move the stones into place. The sloped walls, which were intended to mimic the rays of Ra, the sun god, were originally built as steps, and then filled in with limestone. The interior of the pyramids included narrow corridors and hidden chambers in an unsuccessful attempt to foil grave robbers. Although modern archeologists have found some great treasures among the ruins, they believe most of what the pyramids once contained was looted within 250 years of their completion.\nHanging Gardens of Babylon\nAccording to ancient Greek poets, the Hanging Gardens of Babylon were built near the Euphrates River in modern-day Iraq by the Babylonian king Nebuchadrezzar II around 600 B.C. The gardens were said to have been planted as high as 75 feet in the air on a huge square brick terrace that was laid out in steps like a theater. The king allegedly built the towering gardens to ease his lover Amytis’ homesickness for the natural beauty of her home in Media (the northwestern part of modern-day Iran). Later writers described how people could walk underneath the beautiful gardens, which rested on tall stone columns. Modern scientists have deduced that for the gardens to survive they would have had to be irrigated using a system consisting of a pump, waterwheel and cisterns to carry water from the Euphrates many feet into the air. Though there are multiple accounts of the gardens in both Greek and Roman literature, none of them are firsthand, and no mention of the gardens has been found in Babylonian cuneiform inscriptions. As a result, most modern scholars believe that the existence of the gardens was part of an inspired and widely believed but still fictional tale.\nStatue of Zeus at Olympia\nThe famed statue of Zeus was crafted by the Athenian sculptor Phidias and completed and placed in the temple of Zeus at Olympia, site of the ancient Olympics, around the mid-fifth century B.C. The statue depicted the god of thunder seated bare-chested at a wooden throne. Holding up the thrones’ armrests were two carved sphinxes, mythical creatures with the head and chest of a woman, the body of lion and the wings of a bird. The statue of Zeus was richly decorated with gold and ivory. At 40 feet, it was so tall that its head nearly touched the top of the temple. According to legend, the sculptor Phidias asked Zeus for a sign of his approval after finishing the statue; soon after, the temple was struck by lightning. The Zeus statue graced the temple at Olympia for more than eight centuries before Christian priests persuaded the Roman emperor to close the temple in the fourth century A.D. At that time, the statue was moved to a temple in Constantinople, where it is believed to have been destroyed in a fire in the year 462.\nTemple of Artemis at Ephesus\nThere was actually more than one Temple of Artemis: A series of several altars and temples was destroyed and then restored on the same site in Ephesus, a Greek port city on the west coast of modern-day Turkey. The most fabulous of these structures were two marble temples built around 550 B.C. and 350 B.C., respectively. The former was designed by the Cretan architect Chersiphron and his son Metagenes and decorated by some of the most celebrated artists of the ancient world. The building burned on July 21, 356 B.C., according to legend the same night that Alexander the Great was born. About six years later, the building of a new temple to replace it was begun. The new building was surrounded by marble steps that led to a more than 400-foot-long terrace. Inside stood 127 60-foot marble columns and a statue of Artemis. Archeologists disagree as to whether the building had an open-air ceiling or was topped with wood tiles. The temple was largely destroyed by Ostrogoths in A.D. 262, and it was not until the 1860s that archeologists dug up the first of the ruins of the temple’s columns at the bottom of the Cayster River.\nMausoleum at Halicarnassus\nLocated in what is now southeastern Turkey, the Mausoleum at Halicarnassus was a tomb built by Artemisia for her husband, Mausolus, the king of Carnia in Asia Minor, after his death in 353 B.C. Mausolus was also Artemisia’s brother, and, according to legend, she was so grief-stricken at his passing that she mixed his ashes with water and drank them in addition to ordering the mausoleum’s construction. The massive mausoleum was made entirely of white marble and is thought to have been about 135 feet high. The building’s complicated design, consisting of three rectangular layers, may have been an attempt to reconcile Lycian, Greek and Egyptian architectural styles. The first layer was a 60-foot base of steps, followed by a middle layer of 36 Ionic columns and a stepped, pyramid-shaped roof. At the very top of the roof lay the tomb, decorated by the work of four sculptors, and a 20-foot marble rendition of a four-horse chariot. The mausoleum was largely destroyed in an earthquake in the 13th century and its remains were later used in the fortification of a castle. In 1846, pieces of one of the mausoleum’s friezes were extracted from the castle and now reside, along with other relics from the Halicarnassus site, in London’s British Museum.\nColossus of Rhodes\nThe Colossus was an enormous bronze sculpture of the sun god Helios built by the Rhodians over 12 years in the third century B.C. The city was the target of a Macedonian siege early in the fourth century B.C. and, according to legend, the Rhodians sold the tools and equipment left behind by the Macedonians to pay for the Colossus. Designed by the sculptor Chares, the statue was, at 100 feet, the tallest of the ancient world. It was completed around 280 B.C. and stood for sixty years until it was toppled in an earthquake. It was never rebuilt. Hundreds of years later, Arabs invaded Rhodes and sold the remains of the statue as scrap metal. Because of this, archeologists do not know much about the exact location of the statue or what it looked like. Most believe that it depicted the sun god standing naked while he lifted a torch with one hand and held a spear in the other. It was once believed that the statue stood with one leg on each side of a harbor, but most scholars now agree that the statue’s legs were most likely built close together to support its immense weight.\nLighthouse of Alexandria\nThe Lighthouse of Alexandria was located on a small island called Pharos near the city of Alexandria. Designed by the Greek architect Sostratos and completed around 270 B.C. during the reign of Ptolemy II, the lighthouse helped to guide Nile River ships in and out of the city’s busy harbor. Archeologists have found ancient coins on which the lighthouse was depicted, and from them deduced that the structure had three tiers: a square level at the bottom, an octagonal level in the middle and a cylindrical top. Above that stood a 16-foot statue, most likely of Ptolemy II or Alexander the Great, for whom the city was named. Although estimates of the lighthouse’s height have ranged from 200 to 600 feet, most modern scholars believe it was about 380 feet tall. The lighthouse was gradually destroyed during a series of earthquakes from 956 to 1323. Some of its remains have since been discovered at the bottom of the Nile.']"	['<urn:uuid:4cc90f58-f0f2-4677-8be2-2b44118275e5>', '<urn:uuid:59f54d0e-4f3b-4788-b9df-fbe2c99e0d4d>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	11	74	3779
5	want my garden attract birds what new zealand plants best and are there any harmful ones to avoid	For attracting birds, New Zealand natives like hebes, coprosma, and flaxes are excellent choices as they produce berries that birds love. The nikau palm is also popular and attracts birds. However, you should avoid invasive plants like Japanese honeysuckle - while birds do eat their berries, these are low in nutrients and the plants can strangle native saplings and overcome ground covers, damaging natural areas.	['Using native plants in our gardens not only ensures their survival, but is also aesthetically rewarding\nRemember the days when we were in love with English-style gardens? Then there was the Mediterranean phase. Now we seem to have come to our gardening senses and are using more and more native plants.\nUsing natives in our gardens makes sense in many ways. For a start, native plants survive better simply because they are bred for local conditions. These tough and hardy plants also use less water and require little or no insecticide sprays. Many of New Zealand’s native plants also have berries and flowers that attract bird life to the garden.\nNative plants are the perfect solution for homeowners wanting not only lower maintenance, but also more architectural and interesting plants for their gardens. Many of our native shrubs are also excellent hedging plants — perfect for apartment dwellers with courtyards and balconies that require privacy.\nNew Zealand has a rich and unique flora covering many plant species suited to a range of climates — from the subtropical north of the country and wind-swept coastlines to alpine regions and shady bush. The foliage of native plants is rich in colour, texture and shape, from those with strappy and large leaves to tortured branches, delicate-looking fern fronds, glossy and variegated. Sizes also cover the spectrum from the lowest delicate groundcover and grasses through to versatile shrubs, feature plants and the tallest trees.\nIn the north of New Zealand, native plants can have quite a big-leafed subtropical look, for example puka, griselinia and the climber tecomanthe. However in the south, natives need to be much harder so tend to have smaller frost-hardy leaves.\nMany natives change character as they grow, looking quite different when young from their form when mature.\nAward-winning landscape architect Paddy Baxter of Queenstown-based Baxter Design Group worked on a recent project half way up the Remarkable Mountains, surrounded by tussock grass and cabbage trees. “It’s a pretty wild place so within that context, natives are appropriate.”\nPaddy likes the textural qualities of native plants and uses hebes, coprosma and mountain flaxes among others in his designs. He tends to use them as the framework of a garden and might introduce exotics closer to the house. “With most native plants, the flowers are a blip so that’s not what you use them for. Any flowers are seen as merely a bonus.”\nAlthough natives certainly haven’t been revered for their floral displays, some have magnificent flowers, particularly en masse, like kowhai trees, native clematis, kaka beak and, of course, the iconic pohutukawa tree. And with today’s increasing number of cultivars, there are many natives that shroud themselves in colour, for example, hebes.\nThe other big benefit of using native plants in our gardens is that it ensures their survival. Some species are becoming extinct in their natural environment, eaten by introduced pests like rabbits, goats and pigs.\nTerry Hatch of Joy Plants propagates and sells many such plants. There are too many to list but they include some of the smaller pittosporums, some coprosma and pseudopanax ferox.\nTerry says it is often easy to substitute a native plant to obtain the same look as an exotic, for example, using Melicytus instead of buxus. He also loves the look of less-formal hedges and using billowy hedges of mixed planting, where you might have a small-leafed corokia with a lush clematis growing through it, for example.\nSam Lockie of Landscape Transformations notes that nikau palms are popular because people know them well. “They carry a certain nostalgia value for many New Zealanders.”\nSam likes hebes, nikaus, and mini flaxes because they attract birds and cabbage trees — although he says there is a love/hate relationship with these because of their leaf drop.\nHe says corokias are becoming popular for hedges and can be clipped well. “They’ll survive a few bad haircuts,” says Sam.\nWith so many plant species to choose from, incorporating natives into your garden design is simple. And as long as you take some care to match the plant to the climate and situation, these tough native plants will reward you with good looks and low maintenance for years to come.', 'Native plants are doing battle with harmful invasives that are taking over natural areas. Some have even radically altered the ecosystems they have taken over. This month’s focus is on invasive shrubs and how to eliminate them.\nIn earlier Gardening Journeys, I’ve written about planting shrubs for wildlife, especially those that produce habitat and seeds for the birds. Shrubs with fruits that last throughout the winter are especially valuable. And, particularly valuable are high-fat, calorie-rich seeds like those on our native spicebush. I’ve also promoted both native and plants that have been introduced from areas with similar climates to ours, like Japan and southeast Asia.\nMost of these plants that have been introduced by intention or accident have been beneficial to us and are ecologically benign. But, a small percentage have run rampant. They have moved into natural areas where they’ve not only driven out indigenous species but, in the worst cases, radically altered the ecosystems they’ve invaded. Sadly, the misguided promotion – sometimes by the Department of Agriculture – of several exotic species for erosion and livestock control has been a very important contributor to the problem.\nOne list includes over 300 plants that are invading wild lands in the continental United States; about half of these were brought here to beautify our gardens. Some authorities claim that invasive plants are a major cause of the extinction of native shrubs. Not often discussed is a much tinier fraction of native plants that have shown signs of invasiveness. Examples of such introductions include: Honeysuckle (Lonicera japonica); Barberry (Berberis thunbergii); Tree-of-Heaven (Ailanthus altissima); Multiflora rose (Rosa multiflora); Autumn olive (Elaeagnus umbellata); and, south of us but on its way here, Kudzu (Pueraria lobata).\nHow These Plants Get Around\nThe distances seeds travel can be astonishing. Seeds move by wind or water, on fur or feathers, or in scat. They also arrive in sacks of grains or in mud stuck onto wheels. These invasive plants reproduce rapidly and form stands that exclude nearly all other plants. In the process, they damage natural areas, alter ecosystem processes, displace native species and, as it has become evident, can even hybridize with natives and change their genetic makeup.\nShrubs that become invasive usually produce many small seeds beginning early in their growth. The seeds typically have no special germination requirements such as exposure to cold. The plants are more generally self-fertile. They usually have long flowering and fruiting periods. But, there are also examples where many decades have passed between the first introduction and eventual rapid troublesome spread.\nInvasive plants also have the ability to adapt and can grow in many different conditions.\nInvasive Shrubs in Lancaster County\nAutumn olive (Elaeagnus umbellata)\nA major thug, this wide-spreading, multi-stemmed thorny shrub is tolerant of wind, heat, air pollution and drought. It will grow anywhere except full shade. It will thrive even in poor soil since it has the ability of fixing nitrogen from the atmosphere. It is typically found colonizing the edge of woodland. All parts of the shrub are scaly and covered with spines.\nThe flowers are inconspicuous apart from their delicious scent. A single autumn olive plant can produce an abundant annual crop of edible fruits (to 80 lbs.) containing up to 200,000 seeds. The berries cling on the plants through most of the winter. Birds – especially cedar waxwings, robins, mockingbirds, catbirds, thrushes, bluebirds and grosbeaks – consume the fruit and help to distribute the seed.\nAutumn olive has been found to easily out-compete, suppress and displace native plants in the landscape through rapid growth, root suckering and self-seeding, resulting in the creation of dense shady areas that prevent continued growth of nearby plants that require sunny conditions to survive.\nIn the 1950s, autumn olive’s vigorous growing habits prompted the U.S. Soil Conservation Service to recommend this shrub for rural windbreaks, erosion control and for wildlife food. However, the aggressiveness and tendency to form dense impenetrable stands was soon recognized. It is now illegal to sell it in many states. Elaeagnus is easily identified by the thorny, scaly twigs and leathery gray-green leaves (2-3 inches) that have wavy edges and distinctive silver scales on the undersides.\nJapanese honeysuckle (Lonicera japonica and maackii)\nThese shade-tolerant thugs grow as shrubs or twining vines. Honeysuckle, which was introduced in the mid-19th century as an ornamental fragrant shrub, quickly escaped gardens. Tolerant of many soil types and moisture levels, it invades roadside ditches and the edges of forests, overcoming native ground covers and strangling saplings. There are dozens of species of honeysuckles; some that are supposedly docile continue to be sold by nurseries.\nThe red fleshy berries contain six or seven seeds. A well-established shrub can produce up to 1.2 million seeds. These fruits are low in nutrients and are not even the first choice of birds, which is why they remain so long on the shrubs. They are harvested once other berries are gone. Of course, deer do not eat this shrub.\nIn addition to birds distributing them, the seeds can build up in the soil beneath the shrubs – up to 1,100 seeds per square yard of soil has been recorded! However, they do not germinate until we disturb the soil when we pull up the shrubs. Wicked!\nAnother reason honeysuckles are so successful is that they are among the first woody plants to develop leaves in the spring and among the last to shed them in fall. This gives them a photosynthetic window to produce and store the carbohydrates they need to grow and produce fruit in the low-light habitats of the understory.\nMultiflora rose (Rosa multiflora)\nThe rose is considered by some a magnificent garden plant and remains an essential and sometimes exclusive element in many home gardens. I do not grow roses because to keep them flourishing, a spray, fertilizer and pruning regimen is essential. But, the monstrous invasive multiflora rose shrub needs no such coddling and appears without invitation. I have battled it for decades in my rural garden.\nMultiflora rose is multi-stemmed with 4-10 foot-long stems that curve to the ground where the tip of the cane forms a root system and generates a new plant. Mature shrubs can be 12 feet wide. It can climb 15 feet up a tree. This rose engulfs entire pastures and edges of woodlands. The green stems are covered with prickles (sorry, botanically they are not considered thorns). By any name they are nasty because they are recurved so as to rip your clothes more easily. The abundant clusters of single white flowers appear in June and are followed by copious hips that persist into the winter to be dispersed by birds and other animals.\nAnd mull over this: the bright-red fruit contains seeds that can remain viable in the soil for 10-20 years.\nMultiflora rose thrives on idle land, fence rows and low-maintained hilly pastures. Everywhere it sprouts it creates an impenetrable tangle of brush. It was originally introduced from Asia and promoted as a “living fence” to control erosion and provide food and cover for wildlife.\nIt is very difficult to control. According to research, successful control requires mowing three to six times per season for more than one year. Repeated mowing defoliates the plant, depleting its root food reserves and eventually killing it. Unfortunately, repeated mowing can become quite costly, time consuming and laborious. Oh, and mowing disturbs the soil, which unearths those dormant seeds to germinate.\nDeer will not touch this shrub, but goats and sheep can help control it. Goats are superior because they are not deterred by the prickles and can browse higher up the plants since they can stand on their hind feet. A landowner will need eight to 10 mature goats per acre for two seasons to control infested pastures.\nHow to Deal with Invasive Shrubs\nNon-chemical methods of removal include hand pulling, weeding tools, girdling, mulching, heat (hot water and burning) and tilling.\nThe use of chemicals can be a difficult solution, as one must deliver an adequate amount of the appropriate chemical only at the target plants and at the correct time in their growth cycle. And, consider the potential risk to the worker and the environment.\nIf you take that route, apply the least amount of the safest chemical to specific species of plants in a specific area at the appropriate time to obtain desired control. Read the instructions before buying. Mix according to directions, and make sure it is suitable for your weed. Dress appropriately with protective clothing. Apply only during effective season. Be prepared to repeat treatments.\nLimit the use of fertilizers, as native plants are adapted to naturally low nutrient levels. High nitrogen levels often give an advantage to weedy species that are better adapted to using plentiful nutrients for explosive growth.\nScout your property annually for invasives, and remove them before they become a problem.\nProtect healthy native plant communities by limiting landscape disturbances. The often-promoted idea of thinning out native forests periodically is now being questioned. Opening up the canopy in a forest provides light, and the invasives will quickly reoccupy the available growing space. It is important to plant trees and shrubs in wooded areas where trees have been thinned or invasives have been removed.']	['<urn:uuid:e9047f63-3120-4065-9f87-9250bc657e2c>', '<urn:uuid:f77303dd-3305-490c-ae2e-1f0dcb782b08>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T06:15:13.904987	18	65	2238
6	What are the unique roles of domes and skylights in Orthodox and modern churches?	In Orthodox churches, domes serve spiritual purposes - they either gather goodness from Heaven or show the outpouring of the Holy Spirit upon the Church. Modern churches, as shown in one example, use skylights strategically placed above areas where sacraments like Baptism and Eucharist take place to symbolize these as privileged points of connection with God.	['Our Design Principles\nThree overarching principles have shaped the construction of our Church.\nBaptism and Eucharist define our community\nWe have skylights over the places where these two sacraments take place because we want to clearly say that these are the basis of our communion and are privileged places in which we connect with our God. We enter the Church in Baptism and we welcome Christ into our midst around the altar.\nThe present Church contains as many sacred objects from our earlier Church buildings\nThe stained glass in our Reconciliation rooms, conference room, and family room are from the former Church. The altar, ambo, and statues of Mary and Joseph are from the Church built-in 1980. The Stations of the Cross are from the original Church.\nThis a building for the liturgy\nAlthough it has been important for our parish that the building makes a good impression on Kingsley Avenue, the most important use of this building is liturgical. We want it to sound well, provide the space and elements necessary for good liturgy, and remind all visitors of the importance of the gathering of the Church.\nTour of Our Church\nThe Entrance: Hospitality is one of our core values and we want hospitality to be a mark of the Church from the moment one approaches. The entrance area is the shape of a courtyard to make gathering easy. Handicapped access is through the west columns and through the electric door at the right-hand entrance. A commissioned statue of St. Catherine, who welcomes all people to experience God’s mercy, stands in the center of the courtyard.\nThe Narthex (Gathering Area): The window dominating the gathering area is based on two themes – the mission of the Holy Trinity and the work of the Church. The Trinity is represented by the symbols of the Father, the Son, and the Holy Spirit. The shell of Baptism brings people of every race and nation into the Body of Christ working for justice. Six model Christians selected by the commissions of our parish flank the doorways. We want to show how the mission of the Trinity is completed in the lives of holy people. We want all members of our parish to be saints like the following:\n- Dorothy Day – a 20th-century woman who worked for justice\n- Saint Martin de Porres, OP – a brother who cared for the poor in Lima, Peru\n- Blessed Teresa of Calcutta – a contemporary prophet and guide\n- Saint Pope John XXIII – a reformer of the Church with a heart\n- Archbishop Oscar Romero – a 20th-century martyr from San Salvador\n- Saint Kateri Tekakwitha – an American Indian virgin who was baptized in honor of St. Catherine of Siena\nThe Baptismal Font: This font is placed near the entrance of the Church because we want everyone who enters for worship to remember his or her baptism. The oils of the Church are stored near the font. The font allows for the full immersion of those entering the Church in order to more clearly signify that baptism requires dying and rising with Christ.\nThe Nave: The nave is the central body of the Church where the community gathers. It comes from the same Latin root for navy or naval; therefore, if you look up, you will see the framework of the hull of a ship. There is seating for 1,200 people in the main body of the Church, including the choir area.\nThe Family Room: The family room on the east side of the Church is intended to be used by those with young children who are unable to sit still or remain quiet throughout the liturgy.\nThe Reconciliation Rooms: Both Reconciliation rooms are set apart for their special use with stained glass from the old chapel.\nThe Stations of the Cross: The Stations are the stations from our original Church.\nThe Chapel: The Pieta (the image of Mary accepting the body of her son from the cross) dominates this chapel in which Perpetual Adoration occurs. The Chapel has its own Stations of the Cross that are replicas of the Stations found in the Marian Garden. The stained glass window facing Kingsley Avenue is custom for our Church. The first window shows two important moments from the Virgin Mary’s life: the Annunciation and the Assumption. The second window shows St. Monica and St. Augustine, patron saints of the Diocese of St. Augustine.\nThe Chapel Entrance: There is a special garden outside the chapel for meditation and reflection when the doors of the chapel are locked. One can see the tabernacle through the glass doors. This area is meant to be a place of hospitality and welcome.\nThe Altar: The altar is the same one that was in our Church that was built in 1980.\nThe Crucifix: The large crucifix over the altar with stained glass window that surrounds it is a visible statement of our faith. We believe that Jesus of Nazareth, Son of God and a man like us, was obedient to death and has been raised to glory by the power of God the Father and the Holy Spirit. The angels sing His praises in heaven and prepare a place for us. The 24 angels that surround the cross are described in the Book of Revelation (4:4).', 'The Church building is divided into three main parts: the narthex (vestibule), the nave (the temple proper) and the sanctuary (also called the altar or holy place). A major difference of traditional Orthodox churches from Western churches is the absence of any pews in the nave.\nWhat do Orthodox wear to church?\nIn recent years, Greek Orthodox Churches have gotten a little more casual than they used to be. The general rule is to wear clothes that are classy and not too provocative. Business casual or a suit and tie for the men are both acceptable. For women, wearing a dress or skirt that falls around the knee is preferred.\nWhat are some important features of the Orthodox Church?\nEssentially the Orthodox Church shares much with the other Christian Churches in the belief that God revealed himself in Jesus Christ, and a belief in the incarnation of Christ, his crucifixion and resurrection. The Orthodox Church differs substantially in the way of life and worship.\nWhy are Orthodox churches so colorful?\nThe color of church cupolas can be also interpreted according to the church symbolism. The golden color is the symbol of celestial glory, that is why golden domes crowned main cathedrals consecrated to Jesus Christ and Twelve Great Feasts. Blue domes with golden stars are characteristic of Mother of God churches.\nHow is the Orthodox Church different?\nThe Catholic Church believes the pope to be infallible in matters of doctrine. Orthodox believers reject the infallibility of the pope and consider their own patriarchs, too, as human and thus subject to error. … Most Orthodox Churches have both ordained married priests and celibate monastics, so celibacy is an option.\nWhy are you not supposed to cross your legs in church?\nLeg crossing has long been linked to morality and etiquette. In some countries and cultures leg crossing is looked on as casual, disrespectful, and altogether lower class. For the same reasons, many orthodox religions frown upon leg crossing in church. And what’s good for the soul should be good for the feet as well.\nWhy is it called Greek orthodox?\nThe Greek word “orthodox” simply means “correct belief” and at the same time, “correct worship.” It became the name applied to the Christian Church that grew and flourished in the eastern, predominantly Greek speaking regions of the late Roman Empire.\nAre there pews in an Orthodox church?\nThe nave is the main body of the church where the people stand during the services. In most traditional Eastern Orthodox churches there are no seats or pews as in the West, but rather stacidia (A high-armed chair with arm rests high enough to be used for support while standing); these are usually found along the walls.\nWhich country has the most orthodox?\nOverview. Eastern Orthodoxy is the predominant religion in the world’s largest country, Russia (77%), where roughly half the world’s Eastern Orthodox Christians live.\nWhat kind of religion is Orthodox?\nOrthodox means adhering to accepted norms and creeds – especially in religion. In Christianity, the term means “conforming to the Christian faith as represented in the creeds of the early Church.” The Orthodox Church is one of the three main Christian groups – the others are the Roman Catholic and Protestant Churches.\nWhy do Greek Orthodox churches have domes?\nOne says that these domes are meant to gather or collect the goodness from Heaven, while another explanation is that they are meant to show the outpouring of the Holy Spirit upon the Church. The domes also come in different forms.\nWhat does a Russian Orthodox church look like?\nThe church has two or more domes shaped like tapering towers. An archaizing four-piered church on an elevated base with five outsize onion domes is surrounded by an enclosed gallery leading to several chapels, porches, and belfries of various shapes and sizes. Tiny single-dome gabled churches lacking in ornamentation.\nWhy do Russian churches have onion domes?\nOthers argued that onion domes first appeared in wooden architecture of the Russian North, above tent-like churches. According to this theory, onion domes were strictly utilitarian, as they prevented snow from piling on the roof. This theory became firmly entrenched in Soviet architectural theory.\nDo Orthodox priests marry?\nThe vast majority of Orthodox parish clergy are married men, which is one of the major differences between the Orthodox and Catholic Churches; however, they must marry before being ordained.\nDoes the Orthodox church worship Mary?\nIn the Orthodox view, devotion to Mary is considered an important element of Christian spirituality, and indifference to her by other Christian denominations is troubling to the Orthodox. Orthodox theologian Sergei Bulgakov called denominations that do not venerate the Virgin Mary “another type of Christianity”.\nHow do Greek Orthodox cross themselves?\nHome > Greek Orthodox Reli… > … For Greek Orthodox Christians, the sign of the cross is the ultimate expression of their faith. While making the sign of the cross, Orthodox Christians do so by touching their thumb, index, and middle fingers together, which represents the Holy Trinity.']	['<urn:uuid:cbc2cfae-734c-4407-ae5c-992f3cfaa9d4>', '<urn:uuid:f38e2b31-034f-4f53-bb19-c973fccf0a0d>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T06:15:13.904987	14	56	1721
7	differences between mechanical waves electromagnetic waves wave power generation characteristics limitations	Mechanical waves like ocean waves involve matter and energy, requiring a medium to propagate, while electromagnetic waves involve only energy. In wave power generation, this distinction is crucial as mechanical waves in oceans are harnessed through various devices like barrages and point absorbers. While mechanical waves offer high energy density and consistent availability for power generation, they face several limitations. These include susceptibility to weather hazards, potential damage from seawater corrosion, and technical constraints in current wave energy conversion systems. Additionally, mechanical wave power plants can impact marine ecosystems and navigation, while electromagnetic waves don't pose such environmental challenges.	"['Wave Motion - How it works\nR ELATED F ORMS OF M OTION\nIn wave motion, energy—the ability to perform work, or to exert force over distance—is transmitted from one place to another without actually moving any matter along the wave. In some types of waves, such as those on the ocean, it might seem as though matter itself has been displaced; that is, it appears that the water has actually moved from its original position. In fact, this is not the case: molecules of water in an ocean wave move up and down, but they do not actually travel with the wave itself. Only the energy is moved.\nA wave is an example of a larger class of regular, repeated, and/or back-and-forth types of motion. As with wave motion, these varieties of movement may or may not involve matter, but, in any case, the key component is not matter, but energy. Broadest among these is periodic motion, or motion that is repeated at regular intervals called periods. A period might be the amount of time that it takes an object orbiting another (as, for instance, a satellite going around Earth) to complete one cycle of orbit. With wave motion, a period is the amount of time required to complete one full cycle of the wave, from trough to crest and back to trough.\nHarmonic motion is the repeated movement of a particle about a position of equilibrium, or balance. In harmonic motion—or, more specifically, simple harmonic motion—the object moves back and forth under the influence of a force directed toward the position of equilibrium, or the place where the object stops if it ceases to be in motion. A familiar example of harmonic motion, to anyone who has seen an old movie with a clichéd depiction of a hypnotist, is the back-and-forth movement of the hypnotist\'s watch, as he tries to control the mind of his patient.\nOne variety of harmonic motion is vibration, which wave motion resembles in some respects. Both wave motion and vibration are periodic, involving the regular repetition of a certain form of movement. In both, there is a continual conversion and reconversion between potential energy (the energy of an object due to its position, as for instance with a sled at the top of a hill) and kinetic energy (the energy of an object due to its motion, as with the sled when sliding down the hill.) The principal difference between vibration and wave motion is that, in the first instance, the energy remains in place, whereas waves actually transport energy from one place to another.\nOscillation is a type of harmonic motion, typically periodic, in one or more dimensions. Suppose a spring is fixed in\nOnce it falls, the spring will again go lower than the position of equilibrium, but not as low as before—and so on. This is an example of oscillation. Now, imagine what happens if another spring is placed beside the first one, and they are connected by a rubber band. If just the first spring is disturbed, as before, the second spring will still move, because the energy created by the movement of the first spring will be transmitted to the second one via the rubber band. The same will happen if a row of springs, all side-by-side, are attached by multiple rubber bands, and the first spring is once again disturbed: the energy will pass through the rubber bands, from spring to spring, causing the entire row to oscillate. This is similar to what happens in the motion of a wave.\nT YPES AND P ROPERTIES OF W AVES\nThere are some types of waves that do not follow regular, repeated patterns; these are discussed below, in the illustration concerning a string, in which a pulse is created and reflected. Of principal concern here, however, is the periodic wave, a series of wave motions, following one after the other in regular succession. Examples of periodic waves include waves on the ocean, sound waves, and electromagnetic waves. The last of these include visible light and radio, among others.\nElectromagnetic waves involve only energy; on the other hand, a mechanical wave involves matter as well. Ocean waves are mechanical waves; so, too, are sound waves, as well as the waves produced by pulling a string. It is important to note, again, that the matter itself is not moved from place to place, though it may move in place without leaving its position. For example, water molecules in the crest of an ocean wave rotate in the same direction as the wave, while those in the trough of the wave rotate in a direction opposite to that of the wave, yet there is no net motion of the water: only energy is transmitted along the wave.\nFIVE PROPERTIES OF WAVES.\nThere are three notable interrelated characteristics of periodic waves. One of these is wave speed, symbolized by v and typically calculated in meters per second. Another is wavelength, represented as λ (the Greek letter lambda), which is the distance between a crest and the adjacent crest, or a trough and the adjacent trough. The third is frequency, abbreviated as f , which is the number of waves passing through a given point during the interval of 1 second.\nFrequency is measured in terms of cycles per second, or Hertz (Hz), named in honor of nineteenth-century German physicist Heinrich Rudolf Hertz (1857-1894). If a wave has a frequency of 100 Hz, this means that 100 waves are passing through a given point during the interval of 1 second. Higher frequencies are expressed in terms of kilohertz (kHz; 10 3 or 1,000 cycles per\nFrequency is clearly related to wave speed, and there is also a relationship—though it is not so immediately grasped—between wavelength and speed. Over the interval of 1 second, a given number of waves pass a certain point (frequency), and each wave occupies a certain distance (wavelength). Multiplied by one another, these two properties equal the speed of the wave. This can be stated as a formula: v = f λ.\nEarlier, the term ""period"" was defined in terms of wave motion as the amount of time required to complete one full cycle of the wave. Period, symbolized by T, can be expressed in terms of frequency, and, thus, can also be related to the other two properties identified above. It is the inverse of frequency, meaning that T = 1/ f. Furthermore, period is equal to the ratio of wavelength to wave speed; in other words, T = λ/ v.\nA fifth property of waves—one not mathematically related to wavelength, wave speed, frequency, or period, is amplitude. Amplitude can be defined as the maximum displacement of oscillating particles from their normal position. For an ocean wave, amplitude is the distance from either the crest or the trough to the level that the ocean would maintain if it were perfectly still.\nWhen most people think of waves, naturally, one of the first images that comes to mind is that of waves on the ocean. These are an example of a transverse wave, or one in which the vibration or motion is perpendicular to the direction the wave is moving. (Actually, ocean waves are simply perceived as transverse waves; in fact, as discussed below, their behavior is rather more complicated.) In a longitudinal wave, on the other hand, the movement of vibration is in the same direction as the wave itself.\nTransverse waves are easier to visualize, particularly with regard to the aspects of wave motion—for example, frequency and amplitude—discussed above. Yet, longitudinal waves can be understood in terms of a common example. Sound waves, for instance, are longitudinal: thus, when a stereo is turned up to a high volume, the speakers vibrate in the same direction as the sound itself.\nA longitudinal wave may be understood as a series of fluctuations in density. If one were to take a coiled spring (such as the toy known as the ""Slinky"") and release one end while holding the other, the motion of the springs would produce longitudinal waves. As these waves pass through the spring, they cause some portions of it to be compressed and others extended. The distance between each point of compression is the wavelength.\nNow, to return to the qualified statement made above: that ocean waves are an example of transverse waves. We perceive them as transverse waves, but, in fact, they are also longitudinal. In fact, all types of waves on the surface of a liquid are a combination of longitudinal and transverse, and are known as surface waves. Thus, if one drops a stone into a body of still water, waves radiate outward (longitudinal), but these waves also have a component that is perpendicular to the surface of the water, meaning that they are also transverse.', '17 Wave Power Advantages and Disadvantages Explained\nWave power is a renewable form of power that is generated from wave energy, using mechanical and electromagnetic technologies. Wave power advantages and disadvantages are; reliability, renewable operation, sustainability, fossil fuel independence, high potential, versatility, flexibility, consistency, minimal maintenance (advantages); cost, usability, weather susceptibility, technical limitations, transmission challenges, navigation problems, geographic constraints (disadvantages).\nThe characteristics of wave power and wave energy can be analyzed simultaneously, since wave power is a product of the conversion of wave energy.\nThis article discusses the advantages and disadvantages of wave power, as outlined below;\n-Advantages of Wave Power (and Wave Energy)\n-Disadvantages of Wave Power (and Wave Energy)\nAdvantages of Wave Power (and Wave Energy)\n1). Wave Power is Reliable\nWhen compared to other types of renewable energy, wave energy is significantly reliable.\nThis is because, unlike solar and wind intensity which change continuously with geographic conditions, wave motion occurs almost on a consistent basis.\nAnother way to describe the reliability of wave power is to state that, although the amount of wave energy (which is the source of wave power) may vary at different times, it is hardly ever equal to zero. There is an average amount of wave energy that is always available.\nWave power is reliable mainly because it does not depend on one variable. Unlike fossil fuels, solar and wind, wave energy originates from different sources, including solar heat, wind current, and Earth rotation.\nThe role of gravity in producing wave energy is yet another reason why it is reliable. Gravitational forces cause waves to occur in the ocean, as they try to balance the distribution of momentum across the water body, and to restore equilibrium after the disrupting effects of wind and solar radiation.\nThis settling effect and gravitational pull constantly works against the disrupting forces, leading to wave energy production. As a result, wave power can always be generated, although it may vary in magnitude at different times.\n2). Wave Power is Renewable\nWave power is renewable because wave energy is produced from solar, wind gravity and water, which are all renewable resources.\nWhile other types of power and energy are affected by changes like fuel availability and global warming, the oceans will always exist, as well as wind, gravity and solar radiation.\nWhat this implies is that electricity generation from wave energy will always be possible.\n3). Fossil Fuel Independence\nBecause wave power is renewable and reliable, it can serve as a substitute to fossil fuels, in its own capacity.\nIn spite of the advantages of wave power, it is an underdeveloped renewable resource . One of the reasons for this is the fact that adequate effort is yet to be made toward the development of wave energy technology.\nFossil fuels, on the other hand, constitute the most developed energy sector. Over-dependence on fossil fuels has economic and environmental disadvantages, which can be reduced by increasing the prominence and effectiveness of wave power.\nThe word ‘sustainable’ resonates with some important concepts, such as sustainability and sustainable development.\nThere are different reasons why wave power is sustainable. These reasons span across the three aspects of sustainability, which are environmental, social and economic.\nWave power is considered by many environment-friendly. The conversion of wave energy does not typically have any negative effects on water, soil or air quality.\nWave energy technologies and mechanisms are relatively simple, and do not emit any hazardous byproducts like greenhouse gases, toxins, and wastes .\nThe aesthetic impact and spatial footprint of wave power plants are also small, because these plants do not occupy large portions of the ocean, are often partially submerged.\nWhat this means is that wave power generation is a conservative practice. Estimates suggest that a 30MW wave power plant will occupy less than half square mile in the ocean.\nBecause it is mostly done offshore, wave power generation does not cause any notable damage to the land. This is unlike fossil fuels, which are known to cause significant soil and water pollution , or wind turbines and bioenergy plants that may cause aesthetic or air pollution.\nWhat all of these implies is that wave power is not a potential cause of environmental degradation.\nOther advantages such as reliability and renewable production, suggest that wave power is also sustainable and beneficial from social and economic perspectives.\n5). High Potential\nWave power has a relatively high potential because wave energy can be produced in large quantities .\nA vast amount of wave kinetic energy is generated in the oceans on a consistent basis. A reason for this is the vast scale of the oceans, and the magnitude of wind, solar and gravitational forces that act upon the ocean water.\nThe result is a very high energy density and energy efficiency of waves.\nAt the same time, the vastness of the oceans implies that wave energy can be accessed in all parts of the Earth. With availability, reliability, efficiency and high density, it can be said that the potential of wave power is significant.\nThis potential can be put to use simply by developing efficient technologies to harness wave energy and generate electricity.\n6). Versatility and Flexibility of Wave Power Generation and Consumption\nWave power generation is a versatile and flexible practice for different reasons.\nThe flexibility of wave power arises from the fact that it can be generated under a variety of conditions.\nThere are different devices, mechanisms and technologies for converting wave energy to wave power. These include barrages, longitudinal buoys, point absorbers, hydraulic piston systems, among others.\nVarious locations on the ocean are suitable for installing wave power plants, because wave energy is widely distributed across the water body.\nThis reduces the environmental impact of the wave power generation process, as plants can be installed in locations where there us minimal environmental effect, such as offshore areas.\nThe scale of wave power generation is flexible as well. It can be scaled up or down based on the size and design of the wave energy converter or power plant.\nIn terms of versatility, there is a variety of possible uses, of wave power. These include desalination, seawater pumping, and electricity generation.\nCompared to other power and energy sources, wave power is fairly consistent.\nThis is because wave energy is reliable, and its magnitude of output can be estimated with some accuracy.\nIt can therefore be said that wave power is ‘predictable’, because of the existence of a consistent, average amount of wave energy at all times.\n8). Wave Power Requires Minimal Maintenance\nBecause wave power plants are relatively simple in their design, the amount of maintenance required by these systems is much less than that which is required by other power plants.\nWhile some wave power plants are expensive to design, construct and install, there are various avenues where cost is minimized during the operational life of the plant.\nThese include hazard control, fuel usage and vandalism, among others.\nDisadvantages of Wave Power (and Wave Energy)\n1). Capital Cost of Wave Power Plants\nThe capital cost of generating wave power is relatively high, mainly because it is an underdeveloped sector.\nSince there have not been many wave power plant projects so far, the cost of acquiring equipment and carrying out the design and construction of these plants, is yet to be optimized.\nAlso, most wave power plants at this time, are built as a result of intensive research and development projects that are directed toward improving the efficiency and performance of wave power.\nSuch research and development projects add significant expense to the wave power generation effort.\nAlthough the power plants are relatively simple and require minimal maintenance, they are susceptible to corrosion as a result of prolonged exposure to seawater .\nThis means that corrosion-resistant materials must be used for building wave power plants, and these exposed metal parts must be assessed routinely to ensure that they are not damaged.\n2). Scale of Usability\nAlthough the amount of wave power produced by a plant depends directly on the size of the plant, there has not been much progress so far in terms of developing and implementing a practical wave power generation plant.\nExisting wave power plants are generally not yet capable of delivering utility-scale power for domestic use. This is a major setback, and limits the actual utilization of wave power.\n3). Wave Power Plants are Susceptible to Weather Hazards\nWave energy capture devices and other technologies used in wave power generation, are constantly exposed to the elements.\nThis exposure means that the equipment can get damaged or impaired due to harsh circumstances of operation.\nAsides the risk of damage, exposure of wave power equipment to harsh weather elements may reduce the performance of these equipment . The efficiency and scale of electricity generation are therefore inhibited at such times.\n4). Impact on the Marine Ecosystem\nIn order to capture wave energy and generate wave power, facilities must be installed on the ocean.\nHowever, this can have some negative effects on the biotic and abiotic components of the marine ecosystem .\nIn addition to occupying the natural habitat of marine species, wave power plants have moving parts that can be dangerous to marine life. Since water is a conductive material, there is also the risk of electrocution by transmission systems that are linked to wave power plants.\nChemicals used to operate and maintain the wave power equipment, such as lubricants, can also act as toxins, and may negatively affect water quality in the marine ecosystem.\n5). Technological Limitations of Wave Power Technology\nWave power also has some technological limitations.\nThese limitations arise solely from the fact that wave power technology is still a developing field and has not seen sufficient inputs and modifications to optimize its potential.\nAlso, because there are relatively few wave power systems that have been developed so far, there is not much practical information available to serve as a reference point for improvement.\nThe result of these conditions is a low technical-efficiency form of wave power, which does not meet the existing energy needs.\n6). Wave Power Transmission Challenges\nThere are two main reasons why wave power is often challenging to transmit.\nOne of these is the fact that wave power technology has some practical limitations. As a result, the transmission mechanism for electricity produced by wave power systems, has not been well developed.\nAnother reason is the obvious fact that wave power is produced offshore. Aside applications like water pumping where wave power is used offshore, it must be transmitted over long distances to the shore where it is needed.\nSuch long-distance transport of electricity across the ocean to the land is a notable challenge that affects the use of wave power.\n7). Sea Navigation may be Affected\nWave power facilities that have been installed offshore can affect navigation on the sea.\nThis is because such facilities may become obstacles within the travel route of marine vessels of various types.\nThe effect of such circumstances can be bad for the economy, as they could affect the marine economy by reducing the speed and sustainability of export and import, sea travel, tourism, fishing, and other marine-based activities.\n8). Noise Pollution May Occur\nThe equipment needed to convert wave energy to power, usually consist of moveable parts and generators.\nThese components can be loud when operating, and can cause significant, continuous noise pollution, that may affect inhabitants of the surrounding environs.\n9). Geographical Constraints of Wave Power Generation\nThere are geographical constraints involved in the generation and consumption of wave power.\nTo begin with, wave power generation varies with location. While wave energy is always being produced, not every marine region may produce enough energy to be harnessed in a profitable manner.\nThere may also be restrictions to the installation of wave power plants in some areas. This is especially the case in places where the marine sector is a notable aspect of the economy.\nLastly, wave power can only be beneficial where there are usable and reliable transmission schemes to send the electricity to where it will be used. In many coastal regions, such schemes and facilities are not available.\nWave power advantages and disadvantages are as follows;\nAdvantages of wave power are;\n- Wave Power is Reliable\n- Wave Power is Renewable\n- Fossil Fuel Independence\n- High Potential\n- Versatility and Flexibility of Wave Power Generation and Consumption\n- Wave Power Requires Minimal Maintenance\nDisadvantages of Wave Power are;\n- Capital Cost of Wave Power Plants\n- Scale of Usability\n- Wave Power Plants are Susceptible to Weather Hazards\n- Impact on the Marine Ecosystem\n- Technological Limitations of Wave Power Technology\n- Wave Power Transmission Challenges\n- Sea Navigation may be Affected\n- Noise Pollution May Occur\n- Geographical Constraints of Wave Power Generation\n1). Ankar, S.; Akdoğan, D. A. (2016). “Environmental and Economic Impacts of Wave Energy.” Handbook of Research on Green Economic Development Initiatives and Strategies. Available at: https://doi.org/10.4018/978-1-5225-0440-5.ch013. (Accessed 3 May 2022).\n2). Enferad, E.; Nazarpour, D. (2013). “Ocean’s Renewable Power and Review of Technologies: Case Study Waves.” In H. Arman, & I. Yuksel (Eds.), New Developments in Renewable Energy. IntechOpen. Available at: https://doi.org/10.5772/53806. (Accessed 3 May 2022)..\n3). Hammons, T. J. (2011). “Tidal Power in the UK and Worldwide to Reduce Greenhouse Gas Emissions.” International Journal of Engineering Business Management 3(2). Available at: https://doi.org/10.5772/50933. (Accessed 3 May 2022).\n4). Mørk, G.; Barstow, S.; Kabuth, A. K.; Pontes, M. T. (2010). “Assessing the Global Wave Energy Potential.” 29th International Conference on Ocean, Offshore Mechanics and Arctic EngineeringAt: Shanghai, ChinaVolume: Proceedings of OMAE2010. Available at: https://doi.org/10.1115/OMAE2010-20473. (Accessed 3 May 2022).\n5). Musabikha, S.; Utama, K. A. P.; Mukhtasor, M. (2016). “Corrosion in the Marine Renewable Energy: A Review.” Conference: 3rd International Conference of Ocean, Mechanical and Aerospace – Scientists and Engineers – (OMAse), Malaysia, Vol.3 & Sec.2. Available at: http://isomase.org/POMAse%206-2-0.php. (Accessed 3 May 2022).\n6). Neary, V. S.; Ahn, S.; Seng, B. E.; Allahdadi, N.; Wang, T.; Yang, Z.; He, R. (2020). “Characterization of Extreme Wave Conditions for Wave Energy Converter Design and Project Risk Assessment.” Journal of Marine Science and Engineering 8(4):289. Available at: https://doi.org/10.3390/jmse8040289. (Accessed 3 May 2022).\n7). Pichtel, J. (2016). “Oil and Gas Production Wastewater: Soil Contamination and Pollution Prevention”, Applied and Environmental Soil Science, vol. 2016, Article ID 2707989, 24 pages,2016. Available at: https://doi.org/10.1155/2016/2707989. (Accessed 3 May 2022).']"	['<urn:uuid:475b3b09-6c92-434a-bb0b-401586b2149e>', '<urn:uuid:6bdc7ecd-f9f4-4151-88c2-9a4a48229588>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:15:13.904987	11	99	3858
8	schindler sugihara motivation helping jews during holocaust religious beliefs impact	Chiune Sugihara explicitly stated his religious motivation, saying 'I may have to disobey my government, but if I don't I would be disobeying God' as an Orthodox Christian. For Oskar Schindler, while his early motivations were unclear and likely financial, his later efforts to protect his workers from concentration camps indicated his concern went beyond business interests.	"['Source: Orthodox Church in America Homepage\nOskar Schindler and Raoul Wallenberg are among the most commonly known people who have been recognized as taking extra-ordinary personal risks to help Jews and others targeted for extermination by Adolf Hitler and his National Socialist Worker’s Party in Germany , 1933-1945.\nPerhaps the least well known is a Japanese diplomat, Chiune Sugihara (d. 1986), who only later in life admitted to his own heroic actions, and was recognized in 1985 by the State of Israel with its highest honor. Even less well known is that Sugihara was a convert to Orthodoxy.\nBorn on January 1, 1900 , Sugihara enrolled in Tokyo ’s Waseda University , which to this day is considered to be one of Japan ’s top private institutions with a flair for international affairs. He studied English, and then was received into the Foreign Ministry. An accomplished linguist, he was sent in the 1920’s to the Japanese language institute in Harbin , the capital of Manchuria , China . There he learned Russian and converted to Orthodoxy. Such were his skills that he participated in negotiations with Russia for the sale of the Manchurian Railway to Japan .\nHe rose through the diplomatic ranks with the linguistic and social abilities commensurate with his positions. Multi-lingual, he was sent to Finland in that late 1930’s, and with war pending was entrusted to be the one-man consulate for Japan to Lithuania in March, 1939. Six months later, Hitler invaded Poland , and refugees poured into Lithuania headed east. Then in June 1940, the Soviet Union invaded Lithuania as part of its spoils from its non-aggression pact with Germany , signed just before the invasion of Poland .\nOne month later, July 1940, the Soviet government informed all foreign consulates in Lithuania to leave. Instead of leaving, Sugihara requested and received a 20-day extension, leaving only himself and his Dutch counterpart as the only two consuls in Lithuania .\nThese two consuls, along with the Soviet attaché, soon found themselves inundated with requests from refugees, mostly Polish Jews, who could emigrate to the Dutch Caribbean. To get there, they had to pass through the Soviet Union and Japan . The Soviet Government insisted that they have a valid transit visa from Japan in order to exit from the Soviet Union .\nSugihara’s request to issue these visas was denied by the Japanese Government three times. He decided to disobey his superiors, and began on July 29 issuing visas to the crowds outside his consulate. Night and day he worked, and in the end when he had to leave on September 1, 1940 , he threw his visa stamp from his train compartment to the desperate crowd. These “Sugihara Survivors” were upon arrival in Japan interred at Kobe , and then scattered, with many Jews staying under the protection of the Japanese government in Shanghai , China for much of the war.\nSugihara stayed in the Foreign Ministry until 1945, and was then dismissed. Some reports indicate that this was done unceremoniously, other reports claim that he did receive a pension for his services. He then worked for an export company near Tokyo for much of his remaining life, dying in 1986.\nThese are the common facts of his life available in English. What remains hidden to this writer to date, and what may be of singular interest to the readers of “Jacob’s Well”, is why he converted to Orthodoxy, and what impact his faith may have played in his unique role during World War II.\nAt a minimum, it is important to not discount his decision to disobey his superiors. While this may seem not especially noteworthy in a time of great confusion such as war, such an action for most Japanese would be fraught with tremendous trepidation. Japan ’s historical, literary or cultural tradition contains very little, if any, sense of the “rugged individualism” found in America . While we are familiar with stories embracing the hero who defies all things in order to save the day, Japanese in contrast are more familiar with hearing of sacrifices nobly made for the greater good. Add to these the normal expectations of obedience required within any government ministry, and the magnitude of Sugihara’s defiance cannot be underestimated.\nAnswers may lie in a formal study of Sugihara’s life, especially his formative 20s while stationed in Manchuria and its capital, Harbin . We can only imagine the lost world of Harbin in the 1920s, which has been devastated over the years by Japanese occupation in the ‘30s, World War II and the Chinese Civil War in the ‘40’s, and the rampant destruction of Mao and his followers especially during the Cultural Revolution of the ‘60s. This writer recalls Harbin of the 1980’s as horribly poor, with architecture and street design a unique mix of Chinese and Russian.\nWhat can be surmised of the milieu Sugihara encountered in Harbin ? What there might have lead him to Orthodoxy? We know that Harbin in the 1920’s was filled with White Russian refugees, and became an intellectual and cultural center of the White Russian diaspora. We know too that the Japanese Orthodox Church was at its peak in Japan , before it was decimated by the purges of the government in the 1930s. How these might have influenced Sugihara is unknown, and any insight readers of Jacob’s Well may have are invited to send their thoughts to the editor for further investigation.\nWhile the complete story of Sughihara’s involvement in the Orthodox Church is not clear, it had, and in fact, continues to encourage a response in others. In her autobiography, Visas for Life, Sugihara’s wife Yukiko acknowledges that given he had been baptized as an Orthodox Christian she also agreed to be baptized, taking the Christian name Maria, and they were married in February, 1935 in Tokyo.\nAn article in the Los Angeles Times ( September 21, 2002 ) entitled, “Greek Orthodox Cathedral Is Reaching Beyond Ethnic Roots,” tells the story of the growing interaction between St. Sophia’s Church and the Latino neighborhood where it is located. The pastor, Fr. John Bakas, affirmed that part of the inspiration for him came in 1995 through an invitation from the mayor of Los Angeles to attend a ceremony honoring Sugihara. Learning for the first time about his efforts which had saved the lives of thousands of Jews, Fr. Bakas also heard directly from his family that Sugihara’s actions were “propelled by his faith” as a member of the Orthodox Church. ""’Here\'s a man who did not take the comfortable road, who reached out beyond himself and did something sacrificial in providing service to others at the expense of himself,’ Fr. John said, tearing up even today as he recounted the story. ‘Sugihara had a tremendous impact on how I perceive my ministry.’"" <![endif]>\nSuffice it to say that, in his quiet, modest way, Sugihara very much embodied the noble concept of Tolstoy’s prince. He sought neither fame nor fortune, merely saying ""I may have to disobey my government, but if I don\'t I would be disobeying God.""\nLevine, Hillel. In Search of Sugihara: The Elusive Japanese Diplomat Who Risked His Life to Rescue 10,000 Jews from the Holocaust . ( New York : The Free Press, 1996).\nMochizuki, Ken. Passage to Freedom: The Sugihara Story. ( New York : Lee & Low Books, Inc., 1997). A book for children.\nSugihara, Yukiko. Visas for Life. ( San Francisco : Edu-Comm., 1995).\nThere is also considerable information on the Internet, especially on the website of the Holocaust Museum, Washington, D.C. http://www.ushmm.org/ - go to “Site Search”\n[Special thanks to Jurretta Heckscher for help in researching this article.]', 'Oskar Schindler, (born April 28, 1908, Svitavy [Zwittau], Moravia, Austria-Hungary [now in the Czech Republic]—died October 9, 1974, Hildesheim, West Germany), German industrialist who, aided by his wife and staff, sheltered approximately 1,100 Jews from the Nazis by employing them in his factories, which supplied the German army during World War II.\nSchindler was the eldest of two children born to a farm machinery manufacturer and his wife. Svitavy, where the family lived, was located in the Sudetenland, and, though the region passed from the Austrian Empire to Czechoslovakia in 1918, the Schindlers were ethnically German. After leaving school in 1924, Schindler sold farm equipment for his father, during which time he met his future wife, Emilie, whom he married in 1928. He took a variety of odd jobs, including running a driving school, before enlisting for a stint in the Czechoslovak army. Schindler then briefly lived in Berlin before returning to Czechoslovakia to start a poultry farm, which he soon abandoned. A self-professed sybarite, he spent much of his time drinking and philandering.\nIn 1935 Schindler joined the pro-Nazi Sudeten German Party (Sudetendeutsche Partei; SdP) and the next year began collecting counterintelligence for the Abwehr, the German military intelligence agency. In 1938 he was arrested by Czechoslovak authorities on charges of espionage and sentenced to death. After the annexation of the Sudetenland by Germany late that year as part of the Munich Agreement, Schindler was pardoned by the Reich and rose through the ranks of the Abwehr. His application for membership in the Nazi Party—thought to have been submitted out of pragmatism rather than ideological affinity—was accepted in 1939. That year, on the heels of the German invasion and occupation of Poland, Schindler journeyed to Kraków, where he became active in the emerging black market. Thanks to the network of German contacts he had arranged through liberal bribes, he secured the lease of a formerly Jewish-owned enamelware factory. He renamed the facility Deutsche Emaillewaren-Fabrik Oskar Schindler (known as Emalia) and commenced production with a small staff. Three months later he had several hundred employees, seven of whom were Jewish. By 1942 nearly half of the workers at the expanded plant were Jewish. (Ostensibly “cheap labour,” Schindler paid their salaries to the SS.)\nIn the fall of that year the Płaszów work camp opened nearby, and by February 1943 it was under the command of the notoriously sadistic SS officer Amon Göth, who would be executed after the war. Capitalizing on the officer’s appetite for drink and other luxury items available mainly on the black market, Schindler cultivated his friendship by ensuring a constant stream of them to the villa from which he oversaw the camp. Schindler thus managed to prevail upon Göth to create a separate camp for his Jewish workers, where they were free of the abuses suffered at Płaszów. Though Schindler’s motivations prior to this point are unclear, many scholars interpret his efforts to extricate his workers from Płaszów as indication that his concern for them was not purely financial.\nWhen in August 1944 his factory was decommissioned, Schindler successfully petitioned to have it moved to Brnĕnec (Brünnlitz) in the Sudetenland, close to his hometown. Schindler and his associates composed a list of Jewish workers that he deemed essential for the new factory and submitted it for approval to the Jewish labour office. (With several versions of the list known, it is difficult to determine how many people were ultimately selected.) Though those chosen were diverted for a time to other concentration camps, Schindler intervened, ensuring that 700 men and 300 women eventually arrived at Brnĕnec. They were later joined by 100 more Jews who had been transported from another concentration camp by the Nazis and abandoned in train cars in Brnĕnec. Those who reached the camp spent the remaining months of the war manufacturing munitions that were rigged to fail. A final head count compiled at this time listed 1,098 Jews at the camp.\nOn May 8, 1945, the war in Europe ended, and the next day Schindler and his wife fled the country with the help of several of the Schindlerjuden, as the Jews he saved came to be known. Schindler was wanted for war crimes in Czechoslovakia due to his earlier espionage activities. In 1949 they settled in Argentina with several of the Jewish families they had saved. Having spent the bulk of his profiteering fortune on bribes, Schindler unsuccessfully attempted to farm. He went bankrupt in 1957 and the next year traveled alone to West Germany, where he made an abortive entry into the cement business. Schindler spent the rest of his life supported by donations from the Schindlerjuden. He was named a Righteous Gentile by Yad Vashem in 1962 and was interred in the Catholic cemetery on Mount Zion in Jerusalem.\nSchindler’s story remained largely the province of Holocaust scholars until the publication in 1982 of Schindler’s Ark, a Booker Prize-winning novelization by Thomas Keneally. The novel, which became a canonical text of Holocaust literature, was later used as the basis for Steven Spielberg’s film Schindler’s List (1993), which starred Liam Neeson as Schindler and Ralph Fiennes as Göth.\nLearn More in these related Britannica articles:\nHolocaust: The extermination campsThe most famous was Oskar Schindler, a Nazi businessman, who had set up operations using involuntary labour in German-occupied Poland in order to profit from the war. Eventually, he moved to protect his Jewish workers from deportation to extermination camps. In all occupied countries, there were individuals who came…\nAmon Göth…as the principal adversary of Oskar Schindler, the industrialist who shielded a group of Jews during the Holocaust.…\nThomas Keneally…tells the true story of Oskar Schindler, a German industrialist who saved more than 1,300 Jews from the Nazis. Like many of Keneally’s protagonists, Schindler is a rather ordinary man who acts in accord with his conscience despite the evil around him. Controversy surrounded the book’s receipt of the Booker…\nPlaszow…alive, though the German entrepreneur Oskar Schindler had saved another 1,100 inmates by transferring them to a safer camp in 1944. The commandant of Plaszow, Amon Göth, an SS (Nazi paramilitary corps) officer, was tried and executed in 1946.…\nNazi Party, political party of the mass movement known as National Socialism. Under the leadership of Adolf Hitler, the party came to power in Germany in 1933 and governed by totalitarian methods until 1945.…']"	['<urn:uuid:488dc7fd-b2c0-40cd-90c4-017e75310332>', '<urn:uuid:d4d95ac3-5bfc-47b7-91db-7c58c7b98d01>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	10	57	2343
9	modern education digital assessment methods challenges	Today's education requires assessing more than just reading, writing and math, including skills like communication, critical thinking, and collaboration in a globalized world. Technology-based assessment tools need to advance significantly to evaluate these 21st century skills, requiring the ability to interpret cultural cues, languages, and nuanced scenarios in real-time. Virtual reality and immersive learning environments offer solutions by providing programmable, flexible environments that can be tailored to individual needs while maintaining high levels of engagement and interaction.	"[""How Ed Tech is Assessing New Skills, Changing the Assessment Landscape\n- By Joanna Gorin, Andreas Oranje\nToday's world is drastically different from what it was only a few decades ago, driven primarily by societal and cultural shifts and advancements in technology. As members of a globalized workforce, people are expected to integrate many, diverse modes of technology into their everyday lives, transforming the way we learn, interact and work together. The complexities that humans come face to face with on a daily basis are ever increasing. Because of this, the core skills that are demanded of students are shifting rapidly as we are trying to adapt to an ever-changing environment.\nThese skills and aptitudes are often referred to as 21st century skills, though they have existed for centuries. However, more so than ever before, they are necessary for people to be successful working and adapting in our multifaceted society. Some of the skills needed for today's workforce include:\n- Communication: Technological literacy has become a central part of everyday communication, including the ability to assess the credibility of digital information.\n- Critical thinking: In a world that's turning to automation and artificial intelligence for as many things as possible, critical thinking to obtain effectiveness, efficiency and better outcomes is imperative.\n- Social and emotional learning: Being self- and socially aware to drive one's decision-making is increasingly important.\n- Problem solving: Being able to solve complex issues and problems across varying contexts to prepare for the ever-changing future.\n- Collaborating: Working with others on a global scale, considering and adapting to cultural differences, language barriers, time zones and other challenges.\nThe need to acquire 21st century skills directly affects the education sector. We need to be teaching our children more than just reading, writing and math, such as how to communicate appropriately and work with others in our globalized world. As educators increasingly adapt to fulfill that need, we need to change the way we assess students. What we are assessing and how we are collecting this assessment data will need to transform to better serve teachers and students.\nThis is where ed tech will play a critical role. While technology has been part of education since before the turn of the century, our shifting needs are driving the push for the development and use of education technology that is designed to help students discover, practice, learn and acquire 21st century skills. Though, as we focus on developing the right technologies moving forward, we should also prepare for 95 percent of our current education technologies to become obsolete and prove unsuccessful. We can expect that our world, and therefore the future of ed tech, will look very different, and we'll have to simultaneously adapt.\nCurrently, ed tech is being used in classrooms in multiple ways — including interactive presentations, learning games, automated quizzes and a range of learning management tools. Ed tech is mostly used to try to engage students on the one hand and automate standard processes on the other. Only in some cases does it provide trustworthy data to inform further learning. Moving forward, we need ed tech to evolve past lower level interaction and basic functionality to advanced ways of letting students practice and develop skills. These are also the skills that are difficult to measure.\nThink about smart devices — artificial intelligence is just on the precipice of the kind of ed tech that is necessary to be most useful in today's environment. To truly assess 21st century skills, ed tech assessment tools must decipher and interpret cues from different cultures, languages, scenarios, nuanced inflections and more — and it needs to be able to continually adjust in real-time, as a conversation would progress, and constantly provide cues back to the student. In short, technology-based learning and assessment tools need to significantly advance to keep pace with the complexities that students are currently facing in the real world.\nThat said, developing and adapting technology-based assessments to adequately assess 21st century skills is a radical transformation, with ed tech at the heart of this monumental, yet necessary shift. This shift in assessments is necessary to ensure that students are presented opportunities to thrive and be successful for the future. The industry needs to be prepared to make these changes and serve students purposefully.\nTo do this, we need to understand that just because a technology is available, that doesn't mean that it is automatically effective or useful in every context. Technology needs to be properly utilized and understood in order to drive results. For example, ed tech developers need to consider all ramifications of using a piece of technology — could it desensitize a student's use of a particular skill, or create difficulties for students with disabilities? If so, that technology is not appropriately assessing students on their full potential and is therefore providing inaccurate data. Continuously collecting data will help us to better understand how to measure these new competencies and increase the usefulness of these assessments.\nTo continue on this critical trajectory, there needs to be an increased investment to fully understand which and how technologies are able to help different individuals in ways least restrictive to them. Only once we comprehend how to design technologies that can meet people's needs can we determine how to adapt and optimize to better help students master critical skills and fulfill their potential. It will allow us to meet individual students where they are in terms of ability, background and other key factors.\nAs our society, economy and workforce continue to change, powered by the rise of technology and globalization, ed tech is in a position to advance the field of education and empower students for future success. However, as we explore new ways of teaching and assessing students, we must remain critical and inquisitive of how we develop and use innovative technologies to ensure that we are leading our future generations on the best path forward."", 'TECHNICAL AND PRACTICAL TRAINING FOR EMPLOYEES PRESENTS SEVERAL DIFFICULTIES, FROM THE COSTS INVOLVED IN TRAINING WITH REAL MACHINERY TO THE HEALTH AND SAFETY RISKS THAT SUCH TRAINING CAN POSE.\nImmersive learning technology provides a valuable opportunity to engage in practical training in an easier, safer and more flexible way and, unlike other alternative methods, maintains a high level of interaction with the learning content.\nA research study conducted by the Virtual Human Interaction Lab at Stanford University, “The Effects of Fully Immersive Virtual Reality on the Learning of Physical Tasks”, highlights the positive impacts that virtual reality can have on technical training.(1) Its findings show that virtual reality technology obtained better results than traditional video methods in the learning of practical tasks and participants in the study also reported feeling a higher social presence within the virtual environment.\nResearch has shown that repeating practical tasks reinforces learning. Immersive learning environments allow students to repeat tasks as many times as they like at their own pace and, importantly, away from the social pressures of a real classroom. For training in high-risk situations, virtual reality provides a much safer environment in which to make initial mistakes. Like face-to-face training, virtual training also allows students to interact directly with their trainer and environment. Detailed 3D imagery provides life-like representations of objects and their surroundings, giving students a very ‘real’ training experience.\nTechnical training in an immersive learning environment\n“Learning is the development of experience into experience” (James, 1892).\nThe learning and understanding of practical tasks relies on experience and virtual reality training is highly experiential and immersive. Students can interact with objects and machinery and view them up-close, as well as experiencing how to operate them. Effective training is also reliant on students’ interest and motivation (2); immersive learning technology can create stimulating and engaging learning environments for students, increasing their motivation to learn. Such environments are flexible and programmable, meaning that they can be tailored to meet individual needs.\nAccording to Fabrizia Mantovani (3), students learn more effectively when they engage directly with learning content and build their own understanding of it. Immersion in and interaction with learning content encourages the active engagement and motivation of students.\nExperience, repetition and interaction are important processes in practical training. Virtual reality training provides this in a controlled and programmable environment. The application of virtual reality with a live trainer yields particularly high motivation from students and allows for technical tasks to be carried out under guidance in a safe and engaging environment.\n(1) Patel, K., Bailenson, J.N., Hack-Jung, S., Diankov , R., & Bajcsy , R. (2006). The effects of fully immersive virtual reality on the learning of physical tasks. Proceedings of PRESENCE 2006: The 9th Annual International Workshop on Presence. August 24 – 26, Cleveland, Ohio, USA.\n(2) Bricken, M. (1991). Virtual reality learning environments: potentials and challenges. Human Interface Technology Lab (HITL) Washington Technology Center, University of Washington\n(3) Mantovani, F. Virtual Reality Learning: Potential and Challenges for the Use of 3D Environments in Education and Training in Towards Cyberpsychology: Mind, Cognition and Society in the Internet Age, Riva, G. and Galimberti, C., 2001']"	['<urn:uuid:330f14e8-3e19-470d-9cc4-83c6aa38e25e>', '<urn:uuid:fad1c0af-b656-4505-8236-99aff755e94c>']	factoid	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-13T06:15:13.904987	6	77	1511
10	How are researchers using optical coherence tomography to advance the treatment of both heart disease and Parkinson's disease?	Researchers are using optical coherence tomography (OCT) to identify vulnerable coronary plaques associated with heart disease and to accurately guide placement of treatment probes in deep-brain structures for the treatment of Parkinson's disease.	['Optical Imaging is an emerging technology with great potential for improving disease prevention, diagnosis, and treatment in the medical office, at the bedside, or in the operating room. Optical imaging offers a number of important advantages over existing radiological imaging techniques. First, optical imaging uses non-ionizing radiation, which significantly reduces patient radiation exposure and allows for repeated studies over time. Second, optical imaging offers the potential to differentiate among soft tissues, and between native soft tissues and tissue labeled with either endogenous or exogenous contrast media, using their different photon absorption or scattering profiles at different wavelengths. Such a rich environment of photon absorption and scattering differences offers a great potential for providing specific tissue contrasts, and offers potential capabilities for studying functional and molecular level activities that are the basis for health and disease. Third, optical imaging is very amenable to multimodal imaging. It extends over a wide range on the imaging resolution scale and is often complimentary to and easily combinable with other imaging techniques.\n- During the last 15 years, rapid advances and developments in biophotonics (the science and technology of the interaction of photons within and on biological systems) have resulted in promising new imaging techniques with broad applications in high-resolution imaging.\n- Advances in lasers that produce very short (durations measured in fractions of a trillionth of a second), very intense light pulses and other light technology advances make possible the development of instruments at moderate costs.\n- Advances in genetics and genomics spurred applications to image cellular activity, such as visualization of gene expression in real-time, as well as detection of protein synthesis during biologic processes. The ability to probe physiology and molecular function using optical imaging enhances diagnostic accuracy and plays a vital role in therapeutic strategy and monitoring.\n- Recently, techniques transferred from the laboratory to the clinic resulted in the development of a broad variety of diagnostic technologies and applications, in particular imaging of the breast and the adult and infant brain. For example:\n- To improve the accuracy of surgical biopsies, NIH-supported researchers used tiny-fiber optic probes to detect malignant tissues. Women confronting breast cancer may soon have a more accurate test without undergoing painful surgical biopsies. Moreover, the number of cancers that go undetected could be reduced.\n- NIH-funded researchers are using optical coherence tomography (OCT) to develop methods to identify vulnerable coronary plaques associated with heart disease and to accurately guide placement of treatment probes in deep-brain structures for the treatment of Parkinson’s disease.\n- Researchers are developing techniques to study the margins of skin cancers during the surgery to microscopically determine when all the abnormal cells have been removed.\n- Modalities such as multiphoton microscopy are being used to study living cells and tissues without inflicting damage. This technique is used to study amyloid plaques associated with Alzheimer’s disease and fibrous collagen deposits associated with many liver diseases.\n- The ability to image, analyze, and manipulate living tissue at the cellular and molecular levels will enhance the practice of medicine, making it more predictive, personalized, and preemptive. Imaging techniques will be integrated with therapy allowing first the identification of diseased tissue followed by the optically-mediated releases of therapy directly at the target site. Further transfer of these new techniques into clinical tools remains a demanding problem and requires close collaboration between imagers, engineers, clinicians, mathematicians, and basic scientists. To advance this important field, the NIH remains committed to funding in vivo optical imaging translational research.\nFor additional information contact: Yantian Zhang, Ph.D.; firstname.lastname@example.org; 301-451-4780.\nNational Institute of Biomedical Imaging and Bioengineering']	['<urn:uuid:2fe0a0d7-a99e-46cd-a4a4-0c3933a75d64>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	18	33	594
11	fair algorithms accuracy trade off solutions	There is typically a tension between fairness and accuracy in algorithms. When trying to make algorithms procedurally fair, such as by equalizing false positive rates between racial groups, overall accuracy tends to decrease. FairVis approaches this by helping identify where bias exists in datasets and allowing comparison between groups. An alternative solution proposed is to focus on substantive fairness instead of procedural fairness - this means optimizing algorithms for maximum accuracy while implementing external measures to offset any unfair effects, similar to how healthcare systems handle resource allocation separately from medical treatment decisions.	['FairVis is Helping Data Scientists Discover Societal Biases in their Machine Learning Models\nResearchers at Georgia Tech, Carnegie Mellon University, and University of Washington have developed a data visualization system that can help data scientists discover bias in machine learning algorithms.\nFairVis, presented at IEEE Vis 2019 in Vancouver, is the first system to integrate a novel technique that allows users to audit the fairness of machine learning models by identifying and comparing different populations in their data sets.\nAccording to School of Computational Science and Engineering (CSE) Professor and co-investigator Polo Chau, this feat has never been accomplished by any platform before, and is a major contribution of FairVis to the data science and machine learning communities.\n“Computers are never going to be perfect. So, the question is how to help people prioritize where to look in their data, and then, in a scalable way, enable them to compare these areas to other similar or dissimilar groups in the data. By enabling comparison of groups in a data set, FairVis allows data to become very scannable,” he said.\nIn order to do accomplish this, FairVis uses two novel techniques to find subgroups that are statistically similar.\nThe first technique groups similar items together in the training data set, calculates various performance metrics like accuracy, and then shows users which groups of people the algorithm may be biased against. The second technique uses statistical divergence to measure the distance between subgroups to allow users to compare similar groups and find larger patterns of bias.\nThese outputs are then viewed and analyzed through FairVis’ visual analytics system, which is designed to specifically discover and show intersectional bias.\nIntersectional bias, or bias that is found when looking at populations defined by multiple features, is a mounting challenge for scientists to tackle in an increasingly diverse world.\n“While a machine learning algorithm may work very well in general, there may be certain groups for which it fails. For example, various face detection algorithms were found to be 30 percent less accurate for darker skinned women than for lighter skinned men. When you look at more specific groups of sex, race, nationality, and more, there can be hundreds or thousands of groups to audit,” said Carnegie Mellon University Ph.D. student Alex Cabrera.\nCabrera is the primary investigator of FairVis and has been pursuing this problem since he was an undergraduate student at Georgia Tech.\n“During the summer of my junior year I had been researching various topics in machine learning, and discovered some recent work showing how machine learning models can encode and worsen societal biases. I quickly realized that not only was this a significant issue, with examples of biased algorithms in everything from hiring systems to self-driving cars, but that my own work during my internship had the possibility to be biased against lower socioeconomic groups.”\nThis is when Cabrera reached out to Chau who then recruited the help of CSE alumni Minsuk Kahng, CSE Ph.D. Fred Hohman, College of Computing undergraduate student Will Epperson, and University of Washington Assistant Professor Jamie Morgenstern.\nMorgenstern is the lead researcher for a number of projects related to fairness in machine learning, including the study Cabrera mentioned about self-driving cars. This particular study shows the potentially fatal consequences of algorithmic bias which highlights the severity of software created without fairness embedded into its core.\nFairVis is one of the first systems that helps us achieve a dramatic step towards understanding and addressing the problem of fairness in machine learning, and prevents similar headlines from making their way to reality in the future.\nHowever, Cabrera stressed that the solution does not simply end with better data practices.\n“Fairness is an extremely difficult problem, a so-called ‘wicked problem’, that will not be solved by technology alone,” he said.\n“Social scientists, policy makers, and engineers need to work together to make inroads and ensure that our algorithms are equitable for all people. We hope FairVis is a step in this direction and helps people start the conversation about how to tackle and address these issues.”\nAs we step into 2024 and reflect on the previous year, 2023 was a huge year for news stories here at @GTcomputing . Dive into the 184 published news stories of 2023 and see if theres anything you missed! https://t.co/zUHBPiiEwp\n— Georgia Tech Computing (@gtcomputing) January 11, 2024\nThe College of Computing is proud to celebrate Black History Month this February and honor those who pave the way for equality within our community. pic.twitter.com/Rn5BRskogI\n— Georgia Tech Computing (@gtcomputing) February 1, 2024', 'Our lives are increasingly affected by algorithms. People may be denied loans, jobs, insurance policies, or even parole on the basis of risk scores that they produce.\nYet algorithms are notoriously prone to biases. For example, algorithms used to assess the risk of criminal recidivism often have higher error rates in minority ethic groups. As ProPublica found, the COMPAS algorithm – widely used to predict re-offending in the US criminal justice system – had a higher false positive rate in black than in white people; black people were more likely to be wrongly predicted to re-offend.\nFindings such as these have led some to claim algorithms are unfair or discriminatory. In response, AI researchers have sought to produce algorithms that avoid, or at least minimize, unfairness, for example, by equalizing false positive rates across racial groups. Recently, an MIT group reported they had developed a new technique for taking bias out of algorithms without compromising accuracy. But is fixing algorithms the best way to combat unfairness?\nIt depends on what kind of fairness we’re after. Moral and political philosophers often contrast two types of fairness: procedural and substantive. A policy, procedure, or course of action, is procedurally fair when it’s fair independently of the outcomes it causes.\nA football referee’s decision may be fair, regardless of how it affects the game’s outcome, simply because the decision was made on the basis of an impartial application of the rules. Or a parent’s treatment of their two children may be fair because it manifests no partiality or favoritism, even if it has the result that one child’s life goes much better than the other’s.\nBy contrast, something that is substantively fair produces fair outcomes. Suppose a football referee awards a soft penalty to a team that is 1-0 down because she thinks the other team’s lead was the result of pure luck.\nAs a result, the game finishes in a 1-1 draw. This decision seems procedurally unfair – the referee applies the rules less stringently to one team than the other. But if a draw reflects the relative performance of the two teams, it may be substantively fair.\nAlternatively, imagine that a mother and father favor different children. Each parent treats the disfavored child unfairly, in a procedural sense. But if the end result is that the two children receive equal love, then their actions may be substantively fair.\nAI researchers concerned about fairness have, for the most part, been focused on developing algorithms that are procedurally fair – fair by virtue of the features of the algorithms themselves, not the effects of their deployment. But what if it’s substantive fairness that really matters?\nThere is usually a tension between procedural fairness and accuracy – attempts to achieve the most commonly advocated forms of procedural fairness increase the algorithm’s overall error rate. Take the COMPAS algorithm for example. If we equalized the false positive rates between black and white people by ignoring the predictors of recidivism that tended to be disproportionately possessed by black people, the likely result would be a loss in overall accuracy, with more people wrongly predicted to re-offend, or not re-offend.\nWe could avoid these difficulties if we focused on substantive rather than procedural fairness and simply designed algorithms to maximize accuracy, while simultaneously blocking or compensating for any substantively unfair effects that these algorithms might have.\nFor example, instead of trying to ensure that crime prediction errors affect different racial groups equally – a goal that may in any case be unattainable – we could instead ensure these algorithms are not used in ways that disadvantage those at high risk. We could offer people deemed “high risk” rehabilitative treatments rather than, say, subjecting them to further incarceration.\nAlternatively, we could take steps to offset an algorithm’s tendency to assign higher risk to some groups than others – offering risk-lowering rehabilitation programs preferentially to black people, for instance.\nAiming for substantive fairness outside of the algorithm’s design would leave algorithm designers free to focus on maximizing accuracy, with fairness left to state regulators, with expert and democratic input. This approach has been successful in other areas. In medicine, for instance, doctors focus on promoting the well-being of their patients while health funders and policymakers promote the fair allocation of healthcare resources across patients.\nIn substance or procedure\nOf course, most of us would be reluctant to give up on procedural fairness entirely. If a referee penalizes every minor infringement by one team, while letting another get away with major fouls, we’d think something had gone wrong – even if the right team wins.\nIf a judge ignores everything a defendant says and listens attentively to the plaintiff, we’d think this was unfair, even if the defendant is a jet-setting billionaire who would, even if found guilty, be far better off than a more deserving plaintiff.\nWe do care about procedural fairness. Yet substantive fairness often matters more – at least, many of us have intuitions that seem to be consistent with this.\nSome of us think that presidents and monarchs should have the discretion to offer pardons to convicted offenders, even though this applies legal rules inconsistently – letting some, but not others, off the hook. Why think this is justified? Perhaps because pardons help to ensure substantive fairness where procedurally fair processes result in unfairly harsh consequences.\nMany of us also think that affirmative action is justified, even when it looks, on the face of it, to be procedurally unfair, since it gives some groups greater consideration than others. Perhaps we tolerate this unfairness because, through mitigating the effects of past oppression, affirmative action tends to promote substantive fairness.\nIf substantive fairness generally matters more than procedural fairness, countering biased algorithms through changes to algorithmic design may not be the best path to fairness after all.']	['<urn:uuid:7235cc21-e39c-4714-a108-2403427c4e58>', '<urn:uuid:98551582-9d74-4f97-962a-55a2099f4428>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-13T06:15:13.904987	6	93	1728
12	How do heat pumps achieve efficiency in heating, and what maintenance requirements do they have?	Heat pumps are highly efficient, generating about 4kW of thermal energy for each kW of electricity consumed, corresponding to 300% efficiency compared to 90-96% for conventional boilers. They achieve this by transforming energy from air, ground, or water via the refrigerant cycle. Regarding maintenance, heat pump systems have very low routine maintenance requirements. They can last over 20 years - longer than combustion boilers - and only need an annual maintenance agreement with the supplier. Ground source systems in particular are automated and often called 'invisible heating systems' due to their low maintenance needs, low noise, and being out of sight.	"['Renewable Heating and Cooling Sources are Biomass, Geothermal, and Solar Thermal, Air, and Water. These sources are often used with heat pumps to provide renewable heating and cooling to homes, businesses and industry.\nBiomass is the first energy source that humankind started to control.\nToday, in Europe, biomass is mainly used to heat households and, additionally, to provide hot water.\nA stove burns logs or pellets to heat a single room. A boiler can provide heating and hot water. A boiler burns logs, pellets or chips, and is connected to a central heating and hot water system.\nBiomass heating systems can often integrate into existing heating systems, or replace them, and fulfill all of the heating needs of a home.\nBenefits and considerations\nBiomass has the advantage of being controllable and available when needed, similar to fossil fuel heating. The disadvantage of biomass for facility heating is that the fuel needs to be purchased and stored.\nThe Advantages of Using Biomass for Heating:\n- A low-carbon option through substituting fossil fuels: The use of biomass in heating can replace oil, gas or coal. Biomass is considered to have a zero balance concerning carbon dioxide emission, since when wood is burned the amount of carbon dioxide emitted corresponds to the same amount that was absorbed over the period that the plant was growing and will be absorbed again when the plant starts growing again. The carbon emissions caused by the cultivation, manufacture and transportation of the fuel, especially when the fuel is sourced, are much lower than the emissions from fossil fuels. Also, the process is sustainable as long as the biomass used for fuel is regenerated.\n- Operational fuel cost savings: The costs of biomass fuels can be much lower than the fossil fuel being displaced and biomass heating systems can therefore provide attractive operational cost savings both for households and industries.\n- Reduced fuel price volatility: Security of energy supply is a recurrent concern for fossil fuels due to geopolitical instabilities which can lead to unexpected price changes. While biomass fuels are subject to changes in price over time, these are less extreme and independent from geopolitical circumstances as the biomass is sourced locally or from stable regions.\n- A source of economic development and job creation: Biomass used for heating, can stimulate local economy and create jobs by driving fuel supply in agriculture, forestry, logistics, installation and maintenance activities and give value to local resources that were not mobilized and used before.\nGeothermal energy is the heat from below the earth, which is extracted through boreholes. Geothermal heating & cooling can supply energy at different temperatures (up to 250°C, usually for industry), at different loads, and for different demands.\nCurrently, geothermal energy sources provide more than the equivalent of 4 million tonnes oil (Mtoe) per year for heating and cooling in the European Union, equivalent to more than 15 GWth installed capacity. Following current trends, in the European Union (EU-28), the contribution in 2020 will amount to around 40 GWth installed, corresponding to about 10 Mtoe.\nShallow systems typically use heat at depths of up to 400m coupled heat pumps to provide heating and cooling. Thermal energy can also be stored at these depths. Shallow geothermal energy is the most commonly used in Europe, with installations almost everywhere.\nTwo techniques exist for the use of shallow geothermal energy.\n- Open loop systems extract groundwater, reinjecting it after the thermal energy has been used.\n- Closed loop systems use a closed circuit underground. Closed loop systems can either be Horizontal closed loop, buried at 1-10m (more information at thermomap-project.eu), or Vertical loop, also known as a borehole heat exchanger- these can reach depths of hundreds of metres.\nStorage systems are known as Aquifer Thermal Energy Storage (ATES) and Borehole thermal energy storage system (BTES system)\nMore information about shallow geothermal energy can be found at regeocites.eu.\nThere are about 250 Geothermal District Heating systems in Europe, providing thermal energy to all sectors of the economy. This sector is set to grow, with a further 210 systems under development or evaluation in Europe.The short video from the GeoDH project (www.geodh.eu) gives a brief overview of how Geothermal District heating works.\nThere are also a number of direct uses for Geothermal energy. It is used for desalination, in agriculture, for snow melting and de-icing, and in balneology. More information can be found here (pdf).\nThe basic principle common to all solar thermal systems is simple: heat from solar radiation is conveyed to a transfer medium – usually a fluid but also air in the case of air collectors. The heated medium is used either directly or indirectly, by means of a heat exchanger which transfers the heat to its final destination. Solar thermal can be used in a wide variety of applications, including domestic water heating, space heating, district heating, process heat generation for industry, etc.\nSolar Domestic Hot Water (SDHW)\n- Thermosiphon (or natural flow) systems: Thermosiphon systems use gravity to circulate the heat transfer medium (usually water) between collector and tank. The medium is heated in the collector, rises to the top of the tank and cools down, then flows back to the bottom of the collector. Domestic hot water is taken either directly from the tank, or indirectly through a heat exchanger in the tank. The main benefit of a thermosiphon system is that it works without a pump and controller. This makes the systems simple, robust and very cost effective. In most thermosiphon systems, the tank is attached to the collector and both are situated on the roof. This solar thermal system is most common in the frost-free climates of Southern Europe.\n- Forced circulation systems: These are most common in Central and Northern Europe and the tank can be installed anywhere as the heat transfer fluid is circulated by a pump. Therefore, integration with other heating systems is easier. The aesthetic benefit of these systems is that the tank does not have to be located on the roof. A forced circulation system will need sensors, a controller and a pump.\n- Collective SDHW for larger buildings: Central water (and space) heating is common in larger buildings. Increasingly, collective solar domestic hot water systems are being installed into multi-family houses, hotels, office buildings etc. These collective systems have a collector surface ranging from ten to several hundred square metres.\nCombined DHW and space heating & Cooling (Combisystems)\nIn Central and Northern Europe, solar thermal systems that provide heat both for domestic hot water and for space heating are commonly installed. These combisystems are often more complex than solar systems supplying DHW only and, as a result, system design must be adapted to the specific requirements of the building. Different practices are used in different countries. In Southern Europe, combisystems are still rarely used, but there is a huge potential for these systems to generate space heating in winter and air-conditioning in summer, as well as DHW throughout the year.\nSolar district heating (SDH) plants are a very large scale application of the solar thermal technology. These plants are integrated into local district heating networks for both residential and industrial use. During warmer periods they can totally replace other sources, usually fossil fuels, used for heat supply. Thanks to technological developments, it is now also possible to store heat in summer for winter use. Currently there are many plants in operation in Sweden, Denmark, Germany and Austria.\nProcess heat for industry\nRecently, there have been developments in collector technology that allow solar thermal systems to be used in low temperature industrial processes ranging from drying to pasteurisation or sterilisation. There is great potential for this application in the food, beverage and transport equipment sectors.\nA heat pump is a device that can provide heating, cooling and sanitary hot water for residential, commercial and industrial applications. It transforms the energy from the air (aerothermal), ground (geothermal) and water (hydrothermal) to useful heat. This transformation is done via the refrigerant cycle. Today, more than 7 million heat pumps are running in Europe. Their prevalence is spread unequally across Europe. In Sweden, about 40% of the households are heated by heat pumps. In Germany and the UK, on the other hand, there is still huge untapped potential: Less than 2% of households are equipped with a heat pump.\n- Heat pumps are ENERGY EFFICIENT:\nFor each kW of electricity consumed by a heat pump, about 4kW of thermal energy is generated. This corresponds to a 300% efficiency (a state-of-the-art condensing gas/oil boiler has an efficiency of 90-96%).\n- Heat pumps are SUSTAINABLE:\nAbout 75% of the energy that is used is renewable, whereas 25% of the energy is generated by other sources (in 99% this is electricity). If the electricity is generated by renewables (PV, wind, hydro), then the heat pump is 100% renewable and CO2-neutral.The European stock of heat pumps in 2013 saved 20.04 million tonnes of CO2 emissions. This is almost the amount of CO2 emitted by Croatia in 2010 (20.9 Mt; source: World Bank).\nAccording to IEA, heat pumps could save 50% of the building sector’s CO2 emissions, and 5% of the industrial sector’s. This means that 1.8 billion tonnes of CO2 per year could be saved by heat pumps.\n- Heat pumps are EUROPEAN and they foster EMPLOYMENT:\nThe vast majority of the heat pumps installed in Europe are also manufactured in Europe. In fact, the EU heat pump companies play a leadership role in the development of this technology. (EHPA)More than 40 000 Europeans work full-time in the heat pump sector. This is a very moderate estimation based on the sales data in Europe, on which we applied a certain factor: man-hours needed to install the different types of heat pumps. (EHPA)\n- Heat pumps provide ENERGY SECURITY\nThe EU imports energy worth over 400 billion euros annually. Heat pumps reduce the use of primary and final energy. With greater uses of heat pumps we would need less energy and by consequence less would need to be imported. This saves costs and secures the supply of energy at the same time: we become more energy independent.', 'Ground Source Heat Pump - Domestic\nHow efficient is a ground source heat pump system?\nModern systems can be very efficient. For each kilowatt of electricity used to run the heat pump, three to four kilowatts of heat can be delivered to the building. The efficiency of a GSHP installation is very dependent on the quality of the design and installation. The efficiency of the installation is improved by utilising solar recharge of the ground.\nAre ground source heat pumps new?\nGSHP systems are common, particularly in the USA, Canada, Sweden, Switzerland and Germany. The principles of ground source heat pumps were first described by Lord Kelvin in the 1850s and continuous development since they were first used commercially more than 50 years ago has greatly improved their efficiency and reliability. They now provide a proven, cost-effective, safe and environmentally friendly alternative to fossil fuels.\nHow large are ground source heat pumps?\nA heat pump for a house is about the size of a large fridge. More powerful heat pumps for commercial buildings do not increase in size or price as much as they do in power output.\nCan a GSHP supply hot water?\nYes. Some domestic systems are able to heat domestic hot water via a modern high efficiency indirect water cylinder.\nCan GSHPs provide cooling?\nYes. Reverse-cycle heat pumps can deliver both heating and cooling very effectively. Cooling provided by heat exchange with cold ground is inherently more efficient than air conditioning which typically exchanges heat with hot air.\nCan a standard domestic electricity supply be used?\nYes. Heat pumps are designed to run on a standard UK single phase supply. However, a three-phase supply is a preferable option, and will be essential for larger installations.\nWould a ground source heat pump system be suitable for a well-insulated house?\nYes. Almost all new houses in the UK are designed to meet Building Regulations and should be able to benefit from a ground source heat pump. Building Regulations have been designed to conserve fuel, reduce heat losses and ensure greater energy efficiency, in order to ensure that all modern properties need less heating. For a well-insulated building the size of heat pump will be smaller, will need smaller ground loops and will therefore be less expensive. You should take advice from an installer with experience.\nMy architect suggests I install underfloor heating. Is this a good idea?\nYes. Ground source heat pump systems are ideally matched to modern warm temperature underfloor heating because a heat pump transfers heat at a higher coefficient of performance if it delivers to a large warm water circuit (like underfloor heating) rather than a small high temperature circuit (like wall mounted radiators).\nI have an older property. Can a ground source heat pump be installed?\nYes, but the cost of a system is directly related to the heat losses, which will generally be higher in older buildings. Money spent on upgrading insulation levels can save a considerable amount on the capital cost of a ground source heat pump system.\nCan radiators be used instead of underfloor heating?\nYes, but you will need larger radiators, sized for the typical 45°C to 50°C water temperatures obtained from efficient GSHP systems. If your house is well insulated they may be suitable. Your installer will need to check how big they would have to be and the space they would take up. Modern die cast aluminium radiators are very efficient and smaller than conventional radiators. Upstairs is usually less of a problem as bedrooms are normally kept at lower temperatures.\nCan the heat pump be installed outside or in a car-port, garage or basement?\nYes. There normally means the pump will be nearer to the pipe connections to your ground loops, which often makes the whole system easier to connect.\nWill long trenches have to be dug on my land for the ground loops?\nMy land is too rocky to dig trenches – can I still fit a heat pump system?\nI don\'t have enough land to dig long trenches – what do I do?\nVertical boreholes can be used for heat exchange with the ground instead of trenches.\nHow big are the trenches?\nA typical heating-only installation for a medium sized, new build detached house would need two narrow trenches, each 300 mm wide and 40 to 50 metres long and 2 metres deep. The trenches can be straight or curved and laid in any direction to suit your site, providing they are five metres apart. A standard excavator can dig the trenches and backfill them after the ground loops have been installed. Your installer would be able to do this work and plan it to ensure the minimum of disruption to your site. Once the ground loops are installed, pressure tested and buried, your renewable energy collection system should need no further attention. However, its location needs to be recorded to avoid it being accidentally disturbed!\nCan trenches be installed on a downward sloping site?\nYes, provided the trenches can be dug, a moderate downward slope is not a problem. Consideration needs to be given to purging air from a system with ground loops higher than the heat pump.\nI have a large plot of land but the ground is quite hard. Can a shallower trench be dug?\nYes, the ground loop coils can be laid so that the coils lie horizontally in the trench rather than vertically. This would need a wider but shallower trench depth to at least 1.2 metres. However, this is not as satisfactory as a deep trench in which the coils are vertical and you will need a special slinky configuration and probably longer trenches. Your installer will be able to advise you accordingly.\nI have some very wet land. Can I use this?\nYes, wet land is better at conducting heat so, as long as a trench can be dug, it is ideal.\nI have a large pond. I have a stream. Could this be used?\nYes, it is possible to use very large ponds and fast flowing streams as a heat energy source using an open loop system.\nAre GSHP systems environmentally friendly?\nYes. In the UK, there is now a strong move towards alternative technologies that are sustainable and environmentally much more acceptable. 40% of CO2 emissions are derived from the heating of buildings. By using renewable sources of energy to heat your property you can help to reduce these carbon emissions, particularly when compared to burning fossil fuels such as oil. Most electricity suppliers are now offering \'clean green\' electricity from a renewable energy source and, if you use this to power your heat pump, your property will be totally heated from renewable energy with zero carbon emissions.\nAre Ground Source Heat Pumps dangerous?\nThere are no hazardous gas emissions, no flammable oil, LPG or gas pipes, no flue or chimney and no unsightly fuel tanks. GSHP systems have absolutely NO site emissions. There is, therefore, no need for safety checks.\nAre Ground Source Heat Pumps noisy?\nNo, a ground source heat pump makes less noise than a gas boiler, and very much less than an air source heat pump which drives a fan to extract heat from ambient air.\nWhat about servicing and maintenance?\nAs with any valuable plant a ground source system should be covered by an annual maintenance agreement with the supplier. However, routine maintenace requirements are very low. A ground source heat pump can be expected to last over 20 years – longer than a combustion boiler – and the ground heat exchanger should have a life of over 100 years.\nGround source systems are automated. Because they come with low maintenance, low running costs, low noise and are out of sight, they are often referred to as ""invisible heating systems"".\nHow do running costs compare with conventional alternatives?\nIn a modern, well insulated house, a ground source heat pump can offer very high efficiency and low running costs. An oil-fired boilers cost considerably more to run, and electric heating is at least three times as expensive. Modern condensing gas boilers are more expensive to run at current gas prices, and gas prices are set to rise. Also, all fossil fuel boilers need regular servicing to maintain efficiency and check safety.\nWhat are the costs?\nThe initial purchase costs of a ground source heat pump system are more than a conventional oil or gas fired boiler. The initial capital expense is offset by lower running costs, lower maintenance and low servicing requirement. There is also the security of knowledge that the majority of your heating and cooling energy comes out of your ground, is under your control and will not increase in price.\nBe careful to avoid judging an installation on price alone. It is more important to ensure you have a well designed system if your aim is to save money over the life of the installation.\nIs Planning Permission required to install a GSHP?\nNo, a ground source heat pump installation is invisible, makes less noise than a gas boiler and issues no gases of any kind on-site: there is nothing for anyone to object to.\nAre grants available to reduce costs further?\nYes, the government introduced Renewable Heat Incentive for ground source heat pumps installed in domestic buildings at 18.8 p/kWhr on 9 April 2014. Heat pumps installed before April 2014 also qualify for RHI (if commissioned after 15 July 2009).']"	['<urn:uuid:3ef18f67-4fcd-463d-b493-5a9ff1a6da7d>', '<urn:uuid:ab813ab4-d1a1-4625-9cf9-ebf2cff7699c>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T06:15:13.904987	15	101	3261
13	crisis intervention teams service scope comparison	The crisis intervention teams in Washington County and the Investigation Crisis Management team have distinct service scopes. Washington County's team provides emergency mental health services, suicide risk assessment, grief counseling, and parent/child conflict resolution, with emphasis on immediate safety concerns and individual/family needs. The Investigation Crisis Management team, on the other hand, focuses on business-related crises, handling corporate investigations, regulatory compliance, misconduct allegations, and providing strategic advice for companies facing legal issues. Both teams offer 24/7 services and emphasize immediate response to crisis situations, though in very different contexts.	"['Investigation and Crisis Management\nOur Investigation and Crisis Management team provides comprehensive assistance to companies facing crisis situations and investigations typically this might be the discovery of, or allegations of, employee or director wrongdoing, the threat of, or actual, regulatory investigation. For example allegations of collusion, market manipulation, bribery, sanctions breaches, employee theft or misconduct, including harassment allegations., We can conduct an independent privileged fact-finding exercise, provide strategic advice and handle regulatory authorities across multiple jurisdictions.\nNon-compliance with laws and regulations can expose businesses to criminal and civil liability, and have huge detrimental reputational consequences. We assist private and public entities in preventing and detecting corruption and other economic crime or misconduct in Norway, the UK and internationally.\nIn a crisis scenario, we provide prompt and efficient assistance, assessing risk exposure and the need for preventive measures or damage control, advising on strategic and operational crisis management, and liaising with relevant regulators / public authorities. Our experience spans different types of investigations, including acting as counsel to companies under investigation, investigators in fact-finding exercises and external independent investigators. Our team is also experienced in white collar crime defence if this is called for.\nOur crisis management team operates from our headquarters in Oslo throughout Norway, and has extensive experience acting on global investigations. We also have a team of experienced crisis management lawyers in our other offices including London, Singapore and Shanghai.\n- Handling cross-border crisis management cases and investigations\n- Liaising with authorities across multiple jurisdictions\n- Investigating potential misconduct\n- Strategic and operational crisis management\n- Follow-up and investigation of whistleblowing reports\n- Establish a project organization to handle crises\n- Coordination of resources across disciplines\n- Project management of crisis teams involving external and internal resources\n- Assessing and advising on corporate criminal liability exposure\n- Acting as external independent investigators\n- Acting as counsel to companies under investigation\n- Assessing risk exposure and need for immediate preventive measures\n- Acting in civil or criminal court proceedings\n- Post-crisis clean-up\nSee our other Compliance areas\nRead our articles on Investigation and Crisis Management\nA High Court decision limiting privilege in relation to advice prepared for a company regarding claims of fraud and corruption has recently been overturned by the Court of Appeal in London. The decision widens the scope of protection for disclosing documents created by internal investigations and should give comfort to organisations facing such allegations.\nAn effective compliance program is essential for any business and it needs to be strong, both on paper and in practice. It addition to providing measures to assist companies to prevent, detect and respond to violations of laws and regulations, such programs have an important role to play as part of any defence against corporate criminal investigations and prosecutions.\nPrivilege against self-incrimination is a fundamental principle that protects witnesses from revealing information which might expose them to an accusation or criminal charge. Many legal systems recognise this privilege, and in Norway the right to protect oneself against self-incrimination is a rule of law. The privilege derives both from the right to a fair hearing as defined in article 95 of Constitution and article 6(1) of the European Convention on Human Rights in addition to article 14(3)g of the United Nations Convention on Civil and Political Rights. In the US the privilege is enshrined in the Fifth Amendment to the Constitution.', 'ACUTE CARE SERVICES - CRISIS INTERVENTION TEAM\nACUTE CARE SERVICES -\nCRISIS INTERVENTION TEAM\nA unit of the Behavioral Health Services division, Acute Care Services is a State-certified 24/7/365 Emergency Mental Health Services Program that partners with the community to empower individuals and families during a crisis. Primary focus on client-centered, least-restrictive crisis intervention and treatment. Services provided without regard for ability to pay.\nFor more information email : ContactACS@co.washington.wi.us\nCRISIS LINE (262) 365-6565\nTOLL FREE 1(866)906-6565\nAcute Care ""Crisis""\nServes as an INITIAL POINT of contact for emergency mental health and substance abuse needs, crisis intervention, suicide risk assessment and safety planning, parent/child conflicts and other behavioral health emergencies.\nStrives to address the IMMEDIATE SAFETY CONCERNS facing those struggling with mental health or substance abuse issues.\nWill assess INDIVIDUAL AND FAMILY NEEDS including counseling, parenting, mental health, alcohol/drugs, grief issues, inpatient or detoxification from alcohol, parent/child conflicts and family issues.\nHelp individuals and families DEVELOP PLANS OF ACTION that promote problem-solving and positive choices, including assessment, safety planning, short-term case management, treatment referrals and follow-up services.\nPROVIDE COMMUNITY EDUCATION AND OUTREACH to promote an increased understanding of the Washington County Human Services Department.\n- Telephone crisis counseling, intervention and stabilization\n- Mobile face-to-face intervention whenever possible\n- Treatment Information, Referral and Facilitation\n- Mental Health assessment\n- Suicide Risk Assessment and Safety Planning\n- Crisis Prevention Planning\n- Grief Counseling\n- Parent/child Issues\n- Walk-in services available\n- Follow-up services\n- After-hours Adult Protective Services assistance\n- Least restrictive treatment without regard for ability to pay\nCRISIS LINE (262) 365-6565\nTOLL FREE 1(866) 906-6565\nAdditional services available to the community include:\nIF YOU OR SOMEONE YOU KNOW IS EXPERIENCING A MENTAL HEALTH CRISIS PLEASE CALL\nIF YOU OR SOMEONE YOU KNOW IS EXPERIENCING A LIFE THREATENING EMERGENCY PLEASE CALL\n9 - 1 - 1\nTo learn more about suicide prevention in Wisconsin go to\nW I S C O N S I N\nP A R T N E R I N G T O S A V E L I V E S I N O U R S T A T E\nFor 24/7 crisis help you may also call the\nNATIONAL SUICIDE PREVENTION LIFELINE\nPlease do not use use CRISIS CHAT during an emergency.\nMore comfortable ""chatting"" about your feelings? Find it easier to ""chat"" than talk on the phone? Use crisis ""chat"" to discuss your feelings, get suggestions, or share what has been helpful with a trained crisis counselor. Click the link above to ""chat"" with a crisis counselor in real time. Crisis ""chat"" should be used for non-life threatening situations.\nIF CHAT IS OFFLINE - PLEASE CALL (262)365-6565\nIf you are experiencing a suicidal or life-threatening crisis, PLEASE CALL (262) 365-6565 OR 9 - 1 - 1 rather than using the ""chat"" feature.\nIf you are in need of housing or shelter resources, contact the\nWASHINGTON COUNTY HOUSING HOTLINE at (262) 334-4912\nHomeless Partnership Includes:\nFamily Promise of Washington County\nFriends of Abused Families\n(262) 334-7298 - 24 Hr Crisis Line\n(262) 673-7298 - Hartford Crisis Line\n(262) 255-9488 - Germantown Crisis Line\nThe Youth & Family Project\nWashington County Human Services\nSt. Vincent DePaul\n(262) 334-7291 - 420 N River Rd West Bend\n(262) 673-6980 - 52 S Main St Hartford\n(262) 644-4949 - 445 E Washington St Slinger\nSurvivors of Suicide Loss Support Group\n4th Tuesday of every month\n6:30 p.m. - 8:00 p.m.\nThe Public Agency Center (PAC)\nCall (262) 365-6565 for more information\nNEW SERVICE AVAILABLE IN WISCONSIN!!\nFeeling stressed - Anxious - Struggling - Need encouragement or Support?\nHaving a Bad Day, Feel better texting rather than talking?\nText ""Hopeline"" to 839863\nConfidential Help and Support are just a Button Away!\nService provided by The Center for Suicide Awareness']"	['<urn:uuid:06f8ccd9-e605-46f7-8253-06605eeb0ff4>', '<urn:uuid:935cc0e9-0bea-404c-b757-c63b3799e044>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	6	89	1190
14	Do Pioneer Square and Vancouver's West End share similar origins?	No, they have different origins. Pioneer Square was saved from demolition through preservation efforts in the 1960s-70s, while the West End began in 1862 as a planned development by the Three Greenhorns, initially intended for brickworks before becoming Vancouver's first upscale neighborhood.	"[""|Events||Get Involved||News||Resources||Customer Service Bureau|\nPioneer Square Historical District\nPioneer Square Preservation District FAQ\n1. What does being in the Pioneer Square Preservation District mean to business owners like me?\nIn contrast to the high-rise buildings and streets in other downtown districts, Pioneer Square is characterized by lower brick and stone buildings, tree-lined streets and avenues, cobblestone parks, and diverse street-level retail establishments and restaurants, all contributing to the neighborhood's warm and intimate feel. Pioneer Square exists as we find it today through the efforts of visionary architects, community leaders, and City officials who, in the late 1960s and 1970s fought to save the historic buildings from eminent demolition in the name of urban renewal. During that period, several beloved buildings were destroyed, prompting the grassroots movement that created Pioneer Square as Seattle's first preservation district. Today, property and business owners benefit from the tourists and shoppers attracted to Pioneer Square by the neighborhood's historic and architectural character. This unique character is protected through the Pioneer Square Preservation Board's implementation of use and design guidelines that were established to preserve the district's special appeal.\n2. What kinds of changes require approval?\nAny new business or service must be reviewed and approved by the Pioneer Square Preservation Board and Department of Neighborhoods Director before any other permits will be issued by the City. Any of the following changes in the appearance of a building also must be approved: any change to exterior of any structure, a new sign or a change to an existing sign, new construction, demolition of any structure, and any change in the public rights-of-way including public spaces such as parks and sidewalks. See the Rules for the Pioneer Square Preservation District for detailed design guidance.\n3. I want to put up a new sign. What requirements must it meet?\nThe Pioneer Square Preservation Board has enacted guidelines to ensure that the architectural character of the district is not loss through undue proliferation of signs, that signs do not impede visibility into and out of street-level businesses, and that signs are installed in a manner that does not damage historic building fabric. The Rules for the Pioneer Square Preservation District outline requirements for all signage visible from building exteriors. If you plan to add or alter a sign in Pioneer Square, please contact the Department of Neighborhood's Historic Preservation Program as early as possible so we can explain the review and approval process and recommend next steps. You can reach us at (206) 684-0227.\n4. How do I get approval to make a change in the appearance of the exterior of my building?\nBefore making make any change to the exterior of a structure in the district, the Department of Neighborhood's Historic Preservation Program as early as possible so we can explain the review and approval process and recommend next steps. To get your project reviewed and approved by the Pioneer Square Preservation Board, you need to submit an application for Certificate of Approval. (See Instructions for Applying for a Certificate of Approval in the Pioneer Square Preservation District.) If your proposal includes design changes, you will be asked to present your application to the Architectural Review Committee before presenting your plans to the full Pioneer Square Preservation Board. The Architectural Review Committee will help you sort out any unresolved issues regarding your proposal and will make a recommendation to the full Board. Environmental review is generally required for large projects and usually consists of review of an environmental checklist. Upon approval of your proposal, the Board recommends that a Certificate of Approval be authorized by the Director of the Department of Neighborhoods.\n5. What is the Pioneer Square Preservation Board and how does it make decisions?\nThe Board reviews applications for Certificates of Approval for changes of use and exterior architectural alterations in the district and recommends approval, approval with conditions, or denial to the Director of the Department of Neighborhoods, who makes final decisions concerning applications. The Board may also make recommendations to the Mayor, the City Council, and any public or private agencies concerning land use and social issues in the District.\nThe Board bases its decisions on the standards established in the District Ordinance (SMC 23.66), Rules for the Pioneer Square Preservation District and the Secretary of the Interior's Standards for Rehabilitation. Contact the Department of Neigborhood's Historic Preservation Program at (206) 684-0227 for a paper copy of these documents.\n6. Who is on the Pioneer Square Preservation Board?\nThe Board consists of nine members appointed by the Mayor and confirmed by Seattle City Council. Each member fills a specific position on the Board, representing property owners, retail business owners, human services providers, architects, historians, attorneys, and one at-large representative. At least one Board member must be a resident of Pioneer Square.\n7. When is the next District Board meeting?\nThe Board meets on the first and third Wednesday of each month starting at 9:00 a.m.. Architectural Review Committee meetings are held on an as-needed basis, usually one week prior to the full Board meeting, also starting at 9:00 a.m. Once an applicant has submitted a complete application, the Pioneer Square Preservation Board Coordinator schedules specific times for applicants to present their proposals to the full Board and Architectural Review Committee as necessary. All meetings are open to the public."", 'If a city is judged by its diversity and culture, we’re proud to say that Vancouver’s neighbourhoods take top honours. No less than a dozen different enclaves invite visitors and locals to stroll colourful sidewalks, savour spectacular settings and soak in a wide range of urban vibes with roots in many different cultures and ways of life. And the bonus for visitors is that many of the neighbourhoods in Vancouver are located close to each other, making it possible to see many sides of the city during your visit.\nIt would take weeks, if not months, to see all there is to see in each neighbourhood, so you can focus on just a few in great detail or hit the highlights of them all. To help you narrow down the choices, we’ve provided this guide to Vancouver neighborhoods.\nVancouver’s oldest neighbourhood, Gastown grew from a single tavern founded by John ‘Gassy Jack’ Deighton back in 1867. Today, the district retains its historic charm and independent spirit. Victorian architecture houses a thriving fashion scene, impeccably curated décor boutiques, one-of-a-kind galleries and some of the best culinary fare in Vancouver. It’s a gathering place for stylish locals and an ideal neighbourhood to explore on foot. Gastown lies along the north of the downtown Vancouver peninsula, from Richards east to Main Street, and south to Hastings Street.\nIn the early 1900s, Granville Island was home to factories, plants and sawmills. Things are a little different today—Granville Island is both a locals’ favorite and a huge draw for visitors. Technically a sandspit and not an island, the neighbourhood sits just south of the downtown peninsula, right under the Granville Bridge. The Granville Island Public Market acts as a hub of activity, but it’s also one of the city’s most important cultural districts with theatres, artisan workshops and craft studios.\nLocated in the heart of Vancouver, Granville Street is where you’ll find Vancouver’s main downtown entertainment district. Countless restaurants, bars and nightclubs make this a popular late-night hangout, particularly on the weekends. Stretching from the start of Granville Street by Waterfront Station, south to the Granville Street Bridge, the downtown portion this street is fairly sleepy by day, but buzzing at night. Many of the city’s bus lines transit through Granville, making it a main thoroughfare for the city.\nTucked up tightly against the city’s downtown core, the West End is one of the easiest neighbourhoods for visitors to explore, getting a real taste of how real Vancouverites live. A gateway to Stanley Park, the neighbourhood is bordered by the park on one side, as well as English Bay, Granville and Robson Streets. In 1862, the area was heavily forested when three gentlemen, known as the Three Greenhorns, each purchased 73 hectares (180 acres) of land with the purpose of establishing a brickworks. That plan soon changed, and the area instead became the city’s first upscale neighbourhood. These days, it’s an incredibly diverse area, home to the city’s gay community (centred around Davie Village), heritage homes, and around 40,000 people living in high-rise apartments. The parks and beaches are the main attractions, but the people-watching and dining come in close behind.']"	['<urn:uuid:6a5d2daf-2e09-4b20-90f6-5409f2171a3b>', '<urn:uuid:fe6ceaf8-896d-43f2-829a-cdec0e080f2b>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	10	42	1411
15	manx shearwater arctic tern flight distance compare	The Manx Shearwater travels 20,000km each year in migrations between the UK and South America, while the Arctic tern travels much further - 25,000 miles (approximately 40,000km) annually between Antarctica and the Arctic Refuge, making it the longest-distance flyer of any bird on the planet. Arctic terns can fly over 750,000 miles in their lifetime.	['Behaviour of seabirds during migration revealed\n1 May 2013\nThe behaviour of seabirds during migration – including patterns of foraging, rest and flight – has been revealed in new detail using novel computational analyses and tracking technologies.\nUsing a new method called ‘ethoinformatics’, described as the application of computational methods in the investigation of animal behaviour, scientists have been able to analyse three years of migration data gathered from miniature tracking devices attached to the small seabird the Manx Shearwater (Puffinus puffinus).\nThe Manx Shearwater is currently on the ‘amber’ list of UK Birds of Conservation Concern. Up to 80% of the world population breeds in the UK, travelling 20,000km each year in their migrations to South America and back.\nIn a continuing long-term collaboration, researchers at UCL and the University of Oxford collected data over three consecutive years. In this study, published in the Royal Society journal Interface, they show that the migration of the Manx Shearwater contains a complex pattern of three behavioural states; rest, flight and foraging.\nindicate that in winter, birds spend much less time foraging and in flight than\nin breeding season. Also, a much larger proportion of birds’ time in the\nsouthern hemisphere was spent at rest - probably a reflection of their release\nfrom the demands of reproduction and also the increased costs of flight during\nUnderstanding the behaviour of these birds during migration is crucial for identifying important at-sea locations and for furthering conservation efforts. By tracking the movements, foraging behaviour and environmental drivers of such species, and developing new techniques to do so is critical as they continue to be subject to environmental and anthropogenic pressure.\nDr Robin Freeman\nDr Robin Freeman, from the UCL COMPLEX (Centre for Mathematics and Physics in the Life Sciences and Experimental Biology), and first author of the study, said: “Understanding the behaviour of these birds during migration is crucial for identifying important at-sea locations and for furthering conservation efforts. By tracking the movements, foraging behaviour and environmental drivers of such species, and developing new techniques to do so is critical as they continue to be subject to environmental and anthropogenic pressure.”\nHe added: “Methods to understand animal behaviour from complex data series – what we’re calling ‘ethoinformatics’ - are increasingly important as we continue to gather large amounts of data about animals in the wild.”\nProfessor Tim Guilford, who leads the team at the University of Oxford, said: “At the Oxford navigation group, we have been able to gather an unprecedented amount of information about these elusive ocean wanderers. We trying to understand the processes that govern the behaviour seabirds at sea, and the decisions they must make during migration and foraging.”\nDuring the study, birds were fitted with miniature geolocators and lightweight GPS loggers. The geolocation devices have been developed by the British Antarctic Survey and record salt-water immersion and light levels. Using behaviours identified from GPS tracking during the breeding season, the team demonstrated that these behaviours could be predicted solely from data collected by the much smaller immersion-loggers.\nUnlike other devices that limit broad use because of their mass, cost and longevity (life span), these devices can record continuously for many years and weigh less than two grams.\nDuring the birds’ migratory journey the team identified areas of high foraging behaviour, with concentrations off south-eastern Brazil during the southbound journey and in the Western Atlantic during the return. Rest also occurs throughout migration, with greater concentration towards the very end of the route in both directions. This could reflect distinct stopover types, like foraging stopovers to take advantage of the high prey availability or rest stopover to recover from long flight periods.\nThe researchers also discovered that the birds’ behaviour responded to different environmental conditions. There was a significant relationship between behaviour and environmental variables such as net primary production (the rate at which all the plants in an ecosystem produce net useful chemical energy), chlorophyll and sea surface temperature. During migration, resting behaviour was found to occur in much more productive waters than other behaviours.\nImage: Manx Shearwater 18 February 2012 by Eric Bowles by Georgia eBirder on flickr', '(This piece was written and submitted by Susan Sorg, and originally appeared on OneWildLife.) Evolution has linked migratory birds with the perfect habitat, so much so that the benefits of the Arctic tundra and its abundant insects, invertebrates, and vegetation outweighs the risks of migrating thousands of miles.\nAbove: An Arctic loon stretches its wings. (Richard Spener Photography)\nBy: Susan Sorg\nA higher consciousness of interconnection will shift the global trajectory of habitat loss and reawaken us to the roots of human evolution and web of life. As advocates for wilderness, we validate our higher consciousness and what it means to protect a place we may never actually see, as the Arctic Refuge. Humans have never been separate from the web of life, and our own survival is inherently linked to acting on this understanding.\nA male and female king eider (Florian Schulz / www.florianschulz.org)\nEvolution has linked migratory birds with the perfect habitat, so much so that the benefits of the Arctic tundra and its abundant insects, invertebrates and vegetation outweigh the risks of migrating thousands of miles. Biologist and author Bernd Heinrich explains in Winter World: “Each bird species, like every organism on Earth, feels most at home in the specific environment to which it has been tailored by natural selection and instinctively seeks that environment and avoids others.”\nThe Arctic Refuge coastal plain is the tailored habitat for nearly two hundred species of birds that converge there every spring. An intricate evolutionary story of adaptation, evolution and biological forces has performed on this stage for tens of thousands of years. Birds from six continents transform the tundra into one of the planet’s most biodiverse nesting grounds with an explosion of new life.\nYet the unthinkable happened in December making the future of the Arctic Refuge precarious, when the GOP tax plan was signed into law. Pro-oil GOP politicians manipulated the legislative process and found a loophole to include drilling the Arctic Refuge in their tax bill.\nArctic tern (Tapio Kaisla Flickr CC)\nThe Arctic tern travels 12,500 miles one-way from its overwintering grounds in Antarctica to nest on the Arctic Refuge each spring and is the longest-distance flyer of any bird on the planet. This four-ounce bird with long, narrow wings is well adapted for slow, low-altitude flight and spends the majority of its life ‘en route’ to its next destination or a migratory stopover in-between. It migrates 25,000 miles every year for the Arctic’s abundant food and habitat. Terns can live over thirty years and may fly over 750,000 miles in their lifetime.\n“Virtually every species of bird that occurs in the Arctic National Wildlife Refuge is migratory,” explains David Sibley in Seasons of Life and Land, Subhankar Banarjee’s two-year photographic journey of the Arctic National Wildlife Refuge.\nIf drilling occurs on the coastal plain, terns would arrive to discover the nesting grounds they’ve evolved with and migrated to for possibly millions of years defiled by industrial trucks, drills, roads and noise.\nThe tern population on the Atlantic coast is declining because parent terns cannot find herring to feed their young, and chicks cannot digest any alternate fish. Thousands of tern chicks have been found washed up on the shore. Herring is disappearing from ocean warming caused by climate change and from over-fishing.\nSemipalmated plovers on an Arctic shore (Richard Spener / www.richardspenerphotography.com)\nTundra swans (Paul Konrad)\nHabitat loss, climate change, pesticides, herbicides and trophy hunting are taking a deadly toll on birds and wildlife. The Nature Needs Half movement launched by biologist and author E. O. Wilson gives hope that with cooperative efforts to preserve the intact wilderness that still exists, there may be time to save species.\nSince Subhankar Banarjee began his two-year photographic journey of the Arctic National Wildlife Refuge in 2001, climate change has dramatically accelerated. We wonder now if the mother polar bears are able to feed their cubs or even find enough snow for a den. The muskox baby he observed risked becoming a predator’s meal, but now the Arctic’s warmer, rainy winters induced by climate change are proving deadly to muskox herds as they fall through ice, are trapped and drown.\nPectoral sandpiper (Richard Spener / www.richardspenerphotography.com)\nClimate change, created by industry and agriculture, has become the planet’s predator. Changes in choices and behavior are the only way to slow its wrath.\nOpening the Arctic Refuge to oil drilling will only exacerbate the effects of climate change in Alaska, effects which are no longer theory but reality as villages lose coastlines and winter ice that their economies depend upon. We do not need the oil that certain politicians propose we do—we have a surplus. What we do need is for progressive representatives to help us achieve true national security through global leadership in sustainable energy. Increasing oil and gas drilling in the Arctic is taking a huge step backwards that will worsen climate change and destroy one of the Earth’s greatest remaining intact ecosystems.\nBiologists worried by migratory birds’ starvation, seen as tied to climate change, Darryl Fears. June 19, 2013. The Washington Post.\nBirds connect Arctic Refuge with the world, U.S. Fish & Wildlife Service. (Last updated: December 30, 2013.)\nHalf-Earth, Our Planet’s Fight for Life, Edward O. Wilson. 2016. Liveright Publishing Corporation.\nSeasons of Life and Land, A Photographic Journey, Subhankar Banerjee. 2003. The Mountaineers Books.\nWinter World – ‘The Kinglet’s Winter Fuel,’ Bernd Heinrich. 2003. HarperCollins.']	['<urn:uuid:b7de7b42-ea54-4eff-9867-b00793afbf0c>', '<urn:uuid:b302dafd-70d3-4a83-b522-5362e54a65ab>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	7	55	1588
16	windows kernel mode versus user mode access hardware memory what differences	Kernel mode has direct access to hardware and all computer memory, including memory spaces of user-mode processes. It cannot be paged out to disk and has higher priority than user mode processes. In contrast, user mode has fewer privileges, no direct access to hardware or memory, is limited to an assigned memory space, and can be paged out to secondary memory on hard disk.	['Microsoft Windows Operating System family has a\nprogramming interface which is known as Application Program Interface (API)\nwhich provides services like Graphical User Interface (GUI), system resource\naccess, audio, video etc. this API has thousands of subroutines such as\nCreateProcess and CreateFile. Some major categories of API provide the following\n1. Component Services\n2. Base Services\n6. Web Services\nWhen these services are being observed from\nprogramming point of view, it could be referred to as a callable job in an\noperating system. However, the services are considered to as processes (from a\nuser’s perspective) that are brought in user mode by the operating system,\nwhich are not dependent on logged-in user.\nConsidering the following figure:\nBoth user and kernel modes are separated by a line in the\nabove figure. It is important to remember that user mode has fewer privileges\nthan kernel mode. User mode uses applications that are well defined. The user\nmode process has no direct access to hardware or memory. It is also limited to\nan assigned memory space. User mode processes can be paged out of secondary memory\ninto RAM on a hard disk.\nAt the top right of the figure indicates the environment\nsubsystems which offer visible recognized interface between applications and\nnative APIs. The environment subsystem is used to decodes environment explicit commands\nfrom the application into the directives that the Executive Services (which is\nthe first layer of the kernel) can transmit out.\nMoving to the kernel mode which has direct admittance to\nthe memory and hardware, which also includes the memory spaces of all the\nuser-mode processes. The kernel mode components include the followings:\n1. It has direct access to the hardware.\n2. Kernel can access all the memory of the computer.\n3. It does not move to main memory page file on hard disk.\n4. Kernel mode processes have got higher priority than the\nuser mode processes.\nMemory in Windows:\nMemory management is the process of directing and managing\nthe memory in computer, allocating portion also known as block to different\nprogram which are in execution, to improve the overall system performance. It exists\nin Operating System hardware, and in programs and applications.\nA deeper understanding about memory leads on a particular\nsystem has turn into an important matter for system designer. Unlike other\noperating system, windows perform so many refined operations to be done with\nits job. Memory involves process, child process then threads and other\ndifferent aspects to complete a command from users.\nIt is important to know that memory is a physical\nlocation which is used to store some instruction or sequence of instructions.\nNow, this memory can be of two types i.e. the first one is primary memory and\nthe second one is secondary memory. The primary memory is used in physical\nsystems for the information which works at high rate (i.e. RAM). Unlike\nsecondary memory which is used for physical devices for program and storage of\ndate which are slow to access but provide high memory capacity.\nPrimary memory is frequently related with addressable\nsemiconductor memory. There are further two types of semiconductor memory, i.e.\nfirst one is the volatile and the other one is non-volatile (i.e. flash\nmemory). The hardware memory management components of today’s CPUs offer a way\nfor the OS to plot virtual address on the physical address. The windows memory\nmanager requires a demanded paged virtual memory subsystem.\nWhenever any process requests for a page, it brings the very\nfirst i.e. the oldest page first and these pages are not even over-writable\nbecause they only signify a copy of data that was used by this process once. So\nWindows preserve those on some paging list. Windows memory manager keeps track\nof this is in such a way that it keeps track of this unassigned memory in one\nof the four paging lists.\nFollowing are the types of paging list:\nIt is of definite location with limited lists. It is a must\nfor Windows memory manager to choose this location (or list) while executing a page\nThe zero-page list has a mutual feature, i.e. it has to\nbe of definite sized. When it gets that size, a zero-page thread (also known as\nkernel thread) is aroused and that also runs in priority 0. Further, Windows\nmay also need zeroed pages. The work that zero-page thread performs is that, it\nhas to zero out all those pages.\n& Standby page list:\nMemory manager pulls-out a page from a working set of a\nprocess. But that page may still have needed by that process and it is possible\nthat it may have to be reprocessed by the process if it requests for that page\nagain. Therefore, it may signify a code and be reused by another process as\nStorage and Management:\nThere are two tasks that need to be accomplished, while\nmanaging computer’s memory:\n1. Each process must have sufficient memory to be executed,\nand it can neither run into the memory space of another process nor be run into\nby another process.\n2. The system must use different types of memory properly so\nthat each process can execute most effectively.\nThe first task requires the operating system to set up\nmemory boundaries for types of software and for separable applications.\nLet us consider the above figure as an imaginary small\nsystem with 1MB of RAM. During the boot process, the above computer’s OS is planned\nto go to the topmost of existing memory and then “back up” far sufficient\nto meet up the requirements of the operating system.\nSuppose that the operating system necessities 300KB to\nrun. Now, the operating system goes to the end of the RAM pool and then it starts\nbuilding up with the several driver software required to control the computer hardware\nsubsystems. In our supposed computer, the drivers take 200KB. There are still\n500KB available for the application processes after the OS has been completely\nloaded. The application process will be loaded in the defined blocks with their\nrespective starting and ending on boundaries. These blocks and boundaries help\nto ensure that applications won’t be loaded on top of one another’s space by a\npoorly calculated bit or two. These blocks and the defined boundaries will\nensure that none of the application would be loaded on another’s another just\nby a bad calculated bit. The problem arises when there is no space for the\nupcoming application process when the space is occupied.\nIn Windows, it is\npossible to add more memory beyond the memory size. So for that, in comparison\nwith RAM, disk space is quite cheap, then also moving instructions in RAM to\nhard disk can significantly increase RAM space at no expense. This method is\ncalled virtual memory management.\nDisk storage is one of the memory types. It is the only\none that must be controlled by the operating system itself. Followings are the\ntypes of memory in a computer system in the increasing order with respect to\nIt is the fast, comparatively small amounts of memory that are available to the\nCPU. Cache controllers predict which data will be next fetch from the main\nMain Memory –\nThis is the RAM that you measured in MBs when you purchase a computer.\nSecondary memory –\nIt is mostly of a magnetic tape that preserves applications and data available\nso that it can be used.\nThe operating system must stabilize the desires of the several\nprocesses with the accessibility of the different types of memory, transferring\nthe data in block (which is called a page) between existing memory as process']	['<urn:uuid:db9e3c13-5c74-4407-a046-420d1bd6d7de>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	11	64	1263
17	Which requires more equipment: fault locating or TDR measurement?	Fault locating typically requires more equipment than basic TDR measurement. While TDR measurement needs only three major components (a conductor for testing, an electrical source for signal pulses, and an oscilloscope), fault locators employ multiple detection techniques and often include additional features like GIS integration, high voltage testing capabilities, and advanced data analysis tools.	['What is a Fault Locator?\nA fault locator is a specialized equipment or device used to identify and locate faults or breaks in underground cables or utility lines. These faults can occur due to various factors, including damage to the cable insulation, cable degradation, ground movement, or accidental excavation.\nKey Points About Fault Locators:\n- Purpose of Fault Locators: The primary purpose of fault locators is to accurately pinpoint the location of faults or breaks in underground cables or utility lines. By identifying the exact location of the fault, repair or maintenance crews can efficiently target their efforts, minimize excavation, and reduce downtime.\n- Types of Fault Locators: There are different types of fault locators available, each utilizing specific techniques or technologies to detect and locate faults. Some common types include:\n- Time Domain Reflectometry (TDR): TDR fault locators send electrical pulses into the cable and measure the reflected signals. By analyzing the time delay and magnitude of the reflected signals, the location of the fault can be determined.\n- Arc Reflection Method: This method involves generating an arc at the fault location and detecting the reflected arc energy to pinpoint the fault.\n- Impulse Current Method: Impulse current fault locators inject a high-energy pulse into the cable and analyze the resulting voltage response to identify the fault location.\n- Acoustic or Acoustic-Optical Method: These fault locators use sensors to detect acoustic signals or vibrations generated by the fault. By analyzing the signal characteristics, the fault location can be determined.\n- High Voltage Pulse Method: This method applies a high voltage pulse to the cable and measures the resulting voltage drop or current response to locate the fault.\n- Detection Techniques: Fault locators employ various detection techniques, such as time-domain reflectometry, impedance measurement, resistance measurement, or acoustic sensing. These techniques analyze the electrical or physical properties of the cable to identify deviations caused by faults or breaks.\n- Portable and Handheld Devices: Fault locators are often designed to be portable and handheld for ease of use in the field. These devices are lightweight, compact, and equipped with user-friendly interfaces and display screens, allowing technicians to quickly and accurately locate faults on-site.\n- Fault Location Accuracy: Fault locators strive to provide precise fault location information. The accuracy of fault location depends on factors such as the type of fault locator, the quality of the cable or line, the fault type (e.g., open circuit, short circuit), and environmental conditions. Some advanced fault locators can provide high-resolution fault location information, enabling precise excavation and repair.\n- Integration with GIS and Mapping Systems: Fault locators can be integrated with Geographic Information Systems (GIS) and mapping systems to enhance fault location and management capabilities. Integration allows for the visualization of fault locations on maps, record keeping, and better coordination with maintenance and repair activities.\n- Training and Expertise: Proper training and expertise are essential for operating and interpreting the results of fault locators accurately. Technicians or operators need to understand the equipment’s capabilities, follow standard procedures, and interpret the obtained data correctly to ensure efficient fault identification and repair.\nFault locators play a crucial role in minimizing downtime and disruption caused by faults in underground cables or utility lines. By accurately identifying fault locations, repair crews can quickly excavate and repair the affected area, restoring the functionality of the underground system.\nMore Points About Fault Locators:\n- Advanced Features: Modern fault locators often come equipped with advanced features to enhance fault detection and location capabilities. These features may include automatic fault identification, distance-to-fault measurement, waveform analysis, graphical displays, and data storage for record-keeping and analysis.\n- Multiple Fault Detection: Some fault locators are capable of detecting multiple faults along a cable or utility line. This capability is useful in situations where multiple faults or breaks may be present, allowing technicians to address all issues during a single repair operation.\n- High Voltage Testing: Fault locators may incorporate high voltage testing capabilities to help identify faults that are difficult to detect using low voltage methods. By applying a higher voltage to the cable or line, the fault locator can overcome certain types of insulation or resistance faults and accurately locate them.\n- Fault Distance Estimation: In addition to pinpointing the exact location of the fault, some fault locators can estimate the distance to the fault. This estimation is based on the measurement of signal propagation time or other characteristics of the cable, providing valuable information to technicians for planning repairs and estimating project timelines.\n- Data Analysis and Reporting: Fault locators often have built-in data analysis capabilities that allow technicians to interpret the obtained fault information. These devices may display waveform graphs, impedance curves, or other diagnostic data to aid in fault analysis and decision-making. Some advanced fault locators can generate comprehensive reports summarizing the fault findings and provide data for further analysis or documentation.\n- Integration with Testing Equipment: Fault locators can be used in conjunction with other testing equipment, such as insulation testers or cable fault location systems, to provide a comprehensive assessment of underground cables or utility lines. Integrated systems allow for efficient fault detection, accurate fault location, and comprehensive cable or line testing.\n- Maintenance and Calibration: Regular maintenance and calibration of fault locators are essential to ensure accurate and reliable fault detection and location. This includes periodic verification of measurement accuracy, calibration of internal components, and routine inspection and servicing of the device. Adhering to manufacturer’s recommendations and industry standards helps maintain the optimal performance of the fault locator.\n- Safety Considerations: When using fault locators, it is important to follow safety protocols to protect personnel and equipment. Fault locators may involve working with live electrical systems, so proper safety precautions must be taken, including the use of personal protective equipment (PPE) and adherence to electrical safety guidelines.\nFault locators are valuable tools for underground cable and utility line maintenance and repair. They enable efficient fault identification, accurate fault location, and streamlined repair processes, ultimately reducing downtime and minimizing service disruptions. Ongoing advancements in fault locator technology continue to enhance their capabilities, making them more reliable, accurate, and user-friendly for technicians in the field.', '13 Feb What is time domain reflectometry?\nThe ability for handheld electronics diagnostics tools like the T3 Innovation Cable Prowler to identify the state of a wire simply by plugging into it is an immensely powerful technological advancement that has sped up the growth in size and complexity of networks. Without these useful devices, finding a shorted wire in a maze of cables connecting thousands of computers would seem nearly impossible. They do this through the use of Time Domain Reflectometry (TDR), or the measurement of electrical “reflections” along the wire.\nWhen Does a Conductor Reflect a Signal?\nIf a conductor is connected to a circuit and has a continuous level of resistance throughout its length, then any electrical signal sent down it should dissipate at the end without generating any feedback. When there is a problem with the wire, such as an open or a short, the variation in current flow causes a reactionary waveform to be reflected back along its length. Noise in the wire can interfere with the reflected signal depending on the voltages being used.\nHow Does a Time Domain Reflectometer Work?\nThere are three major components necessary for measuring TDR. The first and foremost is a conductor that will be tested, whether it’s a coaxial cable or a Cat5e. The second item is an electrical source that can generate signal pulses for observation. Lastly, an oscilloscope needs to be placed down the line from the voltage source so it can capture both the primary and reflected waveforms. The exact operations of a diagnostic device’s TDR measurement function may vary, but the same underlying principle of having a conductor, a signal, and an observation method hold true.\nHow Do We Use the Properties to Our Advantage?\nBy sending just a small pulse (instead of a steady supply) of current down the conductor and then monitoring the electrical activity of the conductor through an oscilloscope, we can see the shape of both the primary signal and any reflections. By comparing the two, we can make assumptions about the state of the conductor. For example, a reflected waveform that has a lower amplitude and reversed sign of the original signal indicates a short circuit, whereas an amplitude reduction on its own would point towards an open circuit.\nThe minor variations in the signal tell the operator (or the program analysing the waveform) what sort of condition the wire is in, and the overall length of the function can be used to determine its length. One of the more interesting waveforms is the minor reflection caused by a wiretap on a phone line, but that’s one you shouldn’t come across too often. More commonly, TDR measurement is used for buried and inaccessible wires or extremely complicated and physically inconvenient systems like aeronautics equipment.\nAn Essential Part of the Technician’s Toolkit\nAlthough TDR measurement is now one of the more mundane features on advanced tools such as the Cable Prowler, it is no less vital to their operational goal of quickly and easily ascertaining the state of the network, both physically and digitally. And now you know the basic method of creating a Time Domain Reflectometer if you are locked away in the basement of a mad scientist’s castle and need to find and replace a shorted wire to escape! You will probably just have a similarly amusing daydream while using the convenient function on your Cable Prowler, though.']	['<urn:uuid:852396ac-0cb7-4fd8-a72c-e5704dfb6826>', '<urn:uuid:2b57ef10-1896-43db-bf92-6e82ae23d10a>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T06:15:13.904987	9	54	1592
18	I've been experimenting with radio building and noticed some older radios had really low power. How well did the Hull Hartley radio work in terms of power output?	With loose-coupling and 400 VDC plate voltage, the transmitter produced 5W of output power with 32mA plate current, achieving 39% plate efficiency. When close-coupled, it could reach 8W output with 47mA plate current and 43% plate efficiency. The transmitter produced a clean and crisp CW note that would have been considered 'XTAL' quality by 1929 standards.	"['BUILDING A CLASSIC - THE \'HULL HARTLEY\' TRANSMITTER\nRoss Hull was the Associate Editor of QST magazine from 1931 until 1938. By all standards, Hull was a technical genius and prolific author. Originally from Australia, Hull moved to Connecticut where he soon developed a reputation as a tireless builder and experimenter. Although Hull\'s main interest leaned towards the development of radio-controlled aircraft, his well-documented investigations into the field of VHF tropospheric propagation led to several major discoveries of what we now take for granted. Always pushing the boundaries of new technology, Hull\'s life was cut short at age 36 when he was accidently electrocuted while demonstrating his recently built television receiver to a friend. One can only imagine the many additional contributions Hull might have made to the technical world had his life not ended so soon. More on the life of Ross Hull can be found here..\nIn 1928, Hull penned a QST article which soon became a classic - ""Overhauling the Transmitter for 1929"". The article discussed the need for amateurs to clean-up their signals for the upcoming new regulations governing frequency control and stability. Hull demonstrated that even a simple breadboard transmitter could meet the challenge by simply paying more attention to operating parameters, proper tuning and good solid construction techniques. Hull\'s \'Hartley Oscillator\', referenced in the article, is the basis for my construction project described below. To read Hull\'s 1928 article, visit the page of W1UJR.\nWhen I first read Hull\'s article several years ago, I was struck by the unique appearance of his transmitter\'s design. With many of the components strapped tightly to the back of the plate capacitor and the tube socket mounted on the top, it struck me as the perfect example of \'form following function\'. Hull\'s genius of component placement makes it appear as if the National DX capacitors were made exclusively for this circuit! The symmetrical capacitor placements, along with their respective National Velvet Vernier dials, resulted in a radio whose eye-appeal has stood the test of time. Looking at the transmitter today, nothing screams ""1929 radio"" more than this one, yet at the time it represented basic state-of-the art for amateur radio!\nAlthough Hull\'s circuit could be duplicated using a variety of available capacitor styles, it would be almost impossible to reproduce the exact appearance without utilizing the uniquely constructed National DX variables. Once again, Lou (VE3AWA) offered his enthusiastic support for my project and sent me two of these old gems. A year earlier, Johnny (KE7V), knowing of my interest in vintage radio construction, had sent me a pair of National Velvet Verniers for use in a \'future project\'. Without the generosity of these two amateurs, this project would still be on the drawing board.\nConstruction is centered around the hallmark feature of Hull\'s layout - the high-mounted tube socket, firmly strapped to the top of the plate tuning capacitor. Placing the tube socket well out in the open not only allowed for short connections between the tube and vital components but also allowed the tube to quickly radiate any developed heat.This photo also illustrates the unique construction of the National variables, with separate stator and rotor connections available on either side as well as on the top and bottom, unlike most variables that bring these connectors out at the sides.\nHull was able to take advantage of the DX cap\'s unique construction by fabricating short low inductance straps to support the plate blocking capacitor and the grid capacitor, both Sangamo micas, as shown above. He maintained the symmetrical layout by mounting the filament bypass capacitors, also Sangamos, directly beneath the top pair.\nThe pictures from the originlal article are not clear enough to determine what Hull used for the center-tapped filament resistor. It seems as though it was placed under the plate capacitor and out of sight. I manufactured a pair of old-looking filament resistors by sanding the color bands from a pair of 47 ohm resistors and attaching a computer printed label. Once glued in place, the label was stained slightly so that it didn\'t look too new. I don\'t believe that Lavite ever produced resistors that looked like this but since they were not going to be visible beneath the capacitor, I was content with the final outcome.\nThe RF choke, hand-wound on a 3/4"" turned Cherry form, consisted of 160 turns of #31 wire resulting in a measured inductance of 166uH. Not much compared to the typical RF chokes used today but apparently more than adequate for the task. This photo also illustrates the method of fastening the 1/4"" copper plate coil to the capacitor. Wing nuts were used, as in the original, to facilitate easy coil changes. This connection must be solid as a large amount of RF current flows in the tank circuit.\nThe bottom part of the wire-wound grid leak can be seen below. As per original plan, it has been mounted on a 1/4"" dowel close to the tube socket and grid connection.\nHull\'s original design called for two glass towel rods to be used as coil supports for the plate coil and the antenna coil. Unlike the original, I substituted readily available plexiglass rods, supported on two Cherry end blocks.\nSCHEMATIC & PARTS LIST\nThe 1/4"" copper tank coils are identical to the ones used on the \'29 TNT transmitter. All of the coil-winding information can be found on the 1929 TNT Transmitter page.\nWith the antenna tank coil loosely-coupled (approximately 1.5"" space bewteen coils), a plate voltage of 400 VDC produces 32mA of plate current. The measured output power into a 50 ohm load (as well as a properly matched 50 ohm antenna) is 5W, indicating a plate efficiency of 39%...typical of most power oscillators. With close-coupling, the plate current rises to 47mA, with a measured output of 8W and a plate efficiency of 43%. Interestingly, with the loose-coupled antenna tank coil, an additional 330pF of capacitance (in parallel with the 500pf variable) was required to resonate the tank when running into an antenna. With tight-coupling, the tank resonated with the single 500pf variable alone.\nThe original article makes no mention of the method used to key Hull\'s transmitter. Since only four Fahnestock clips are shown near the tube, it is probable that Hull keyed the B- line, a scary but not unusual practice at the time. I elected to key the center-tapped filament circuit which required an additional two clips, creating another (yet far safer) departure from the original circuit. The transmitter\'s CW note is clean and crisp, and by 1929 standards, would be easily described as \'XTAL\' quality - more than surpassing Hull\'s objectives.\nIn preparation for upcoming 160m activity, newly allowed in the yearly Bruce Kelley \'29 QSO Party, I wound a set of coils for the Hull Hartley. Both coils were wound with information provided in George Grammer\'s QST article, ""A Low-Power 1715 Kc CW Transmitter"" (March 1932), a Hartley oscillator using a 45 triode. Both this design and the Hull Hartley utilize a 500pF plate tuning capacitor and I found the values provided to be spot-on. Unlike Grammer\'s design, using bare enamelled #12 and a co-wound string for spacing, I used jacketed #14 (Home Depot) and let the PVC jacket take care of the spacing. The plate tank coil form was a 3 1/2"" piece of 2"" ABS tubing (2 3/8"" OD) and was wound with 18T and tapped at 8T from one end; the 2 3/4"" long antenna coil was similarly wound with 12T in total. When finished, both coils were given a coating of hi-gloss sealant.\nThe keying on 160 sounds very good and could readily pass for a more modern rig. You can click here to listen to the on-air signal.\nCoil winding data for 160m is also available in the \'Files\' section of the AWAGroup website in the \'160 Meter Coil Winding\' file. You can find several different styles for both plate and grid ( TNT or TPTG).\nMy one and only CQ on 160m was answered by Paul, N1BUG (2500 miles to the east in Maine), who describes our contact as well as posting a few recordings of my signal. It would seem that \'29-style 160m transcontinental QSO\'s should be possible, given good conditions and good 160m antennas at both stations. Hopefully others will be encouraged to wind coil sets for their \'29 transmitters and get their feet wet on topband during the next Bruce Kelley \'29 QSO Party.']"	['<urn:uuid:fc70a2e2-fdc2-46fa-bd68-531d697cdb69>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	28	56	1410
19	I'm interested in major battles of the Vietnam War and have heard about the Tet Offensive. Could you tell me when exactly this important attack started and if it began at the same time everywhere?	The Tet Offensive began on January 31, 1968, across most of South Vietnam. However, in the central coastal area, the fighting started one day earlier, on January 30, because enemy commanders in that region used a different lunar calendar.	"['The Pentagon\'s Commemorating the 50th Anniversary of the Vietnam War. Just How Accurate Is It?Historians/History\ntags: Vietnam War\nThere has been controversy recently over a program to commemorate the 50th anniversary of the Vietnam War, run by the Department of Defense. An article in the New York Times October 10, “Pentagon’s Web Timeline Brings Back Vietnam, and Protesters,” by Sheryl Stolberg, mentioned that I said I had found minor inaccuracies in an Interactive Timeline, that the Department of Defense has placed online as part of this project. I have been asked to cite examples.\nI have looked at the timeline for the two years in which I am particularly interested, 1964 and 1968.\nFor 1964, I noticed the entries:\nMAY 22, 1964: AIR AMERICA FLIES FIRST SEARCH AND RESCUE MISSION IN LAOS AND NORTH VIETNAM\nAir America, formerly the Civil Air Transport, begins search and rescue missions in Laos and North Vietnam. Its first rescue of a downed flyer is 6 June 1964.\nComment: The first rescue of a downed flyer in Laos was on June 8, 1964. There had been an attempted rescue of a previous downed flyer on June 6, but it had failed. I do not believe any such search and rescue missions were flown in North Vietnam at any time in 1964.\nAUGUST 2, 1964: GULF OF TONKIN INCIDENT (¼)\n. . . On August 2, 1964, the destroyer USS Maddox is stationed in the Gulf of Tonkin in support of South Vietnamese espionage operations off the coast of North Vietnam. . . .\nComment: The operations off the coast of North Vietnam were not espionage operations, they were armed attacks against coastal facilities. And the Maddox was not really stationed in support of those operations; the Maddox was on an intelligence-gathering mission.\nWhen Nick Turse first asked me to look at the timeline early this year, the entries that caught my eye included:\nJULY 11,1964: JOINT CHIEFS OF STAFF (JCS) UNVEILED ""94 TARGET LIST""\nAUGUST 4, 1964: TWO U.S. AIRCRAFT DOWNED\nI told Turse that the Joint Chiefs had not even come close to unveiling that famous list of 94 potential bombing targets in North Vietnam during July 1964, and that no American aircraft had been downed on August 4, 1964. These entries have since then disappeared from the timeline, probably as a result of prodding by Turse.\nFor 1968, I noticed the entries:\nJANUARY 30, 1968: VIET CONG LAUNCH TET OFFENSIVE\nFrom January 30 to February 28, 1968, an estimated 84,000 communist forces carry out a countrywide offensive that starts during the Vietnamese Tet holidays. Viet Cong and North Vietnamese troops strike 36 provincial capitals and capture most of Hue, the old imperial capital of Vietnam. . . . In the central coastal area, the fighting starts on January 29 because enemy commanders there use a different lunar calendar.\nComment: This entry treats the Tet Offensive as having begun on January 30 for South Vietnam as a whole, but on January 29 in the central coastal areas. Each of these dates is one day off. It began on January 31 in South Vietnam as a whole, but on January 30 in the central coastal area.\nJANUARY 31, 1968: BATTLE OF HUE BEGINS\n. . . The defeat of the Communist forces at Hue possibly prevents them from taking the two northern provinces of South Vietnam.\nComment: The suggestion that if the Communist forces had managed to hold onto Hue, this might have enabled them to take the whole of the two northern provinces of South Vietnam, is an absurd exaggeration of the importance of Hue and of the forces fighting there.\nFEBRUARY 24, 1968: NAVY ACTIVATES TASK FORCE CLEARWATER TO KEEP SHIPMENTS TO KHE SANH\nConcerned that the water access to Dong Ha, the main transshipment point to Khe Sanh, would become cut off, General Creighton Abrams, Deputy Commander of MACV, requests that the Navy stand up a patrol force for the Cua Viet River. . . .\nComment: Dong Ha was not, during that period, a significant transshipment point, much less the main one, supporting Khe Sanh. If Khe Sanh had been getting supplies by road, Dong Ha would have been a vital transshipment point, but Khe Sanh was supplied entirely by air.\ncomments powered by Disqus\n- U.S. Planned for Military Occupation of Cuba\n- New picture emerges of Mata Hari, who faced firing squad 100 years ago\n- Massive section of Western Wall and Roman theater uncovered after 1,700 years\n- Fight over national monuments intensifies\n- Martin Luther: Reluctant reformer who rocked Christianity 500 years ago\n- Historian Keri Leigh Merritt defends activist scholars\n- Historian digs into the hidden world of Mormon finances\n- A historian who became a business professor?\n- Allan Lichtman\'s response to critics of his book that makes the case for Trump’s impeachment\n- ""Do We Have To Fight Nazis Again?” asks historian Paul Ortiz']"	['<urn:uuid:974432f2-1d25-4b3c-bbff-bfbd6e6d0546>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	35	39	820
20	sockeye salmon migration pattern vs bluefin tuna spawning locations where do they go	Sockeye salmon are born in lakes and rivers, then migrate to the ocean for 1-3 years before returning to their natal freshwater habitat to spawn. Bluefin tuna, on the other hand, gather in large numbers at specific spawning grounds in various locations including the Atlantic Ocean near Newfoundland and Iceland, the Gulf of Mexico, the Mediterranean, and the Pacific Ocean.	['For kids who have pet fish or enjoy catching or learning about all sorts of fish animal biology.\nFinding Nemo in Your Home\nTurning a child’s impulsive enthusiasm into a lasting hobby requires a realistic understanding of what keeping a clownfish in a marine aquarium setup really involves.\nMapping Fish Habitats at Home\nDo you have an aquarium? Here’s how to track your fish in a way scientists track fish in the wild. Why is it important to track fish?\nBiology of Fish\nFish Biology Just for Kids\nCan you say “Ichthyology?” That’s the study of fish. Learn about the difference between true fish and sharks. From the Florida Museum of Natural Biology.\nMust be a hundred questions here about fish and crustaceans answered here from the Northeast Fisheries Science Center.\nLife Of The Sockeye Salmon\nBorn in lakes and rivers, sockeye salmon migrate to the ocean after 1-3 years. At maturity, they return to their natal freshwater habitat to spawn.\nStrange Creatures of the Deep\nXiphactinus, with its enormous jaws swallowed its prey headfirst & whole! Leedsichthys, so huge nobody could imagine it alive!\nVocabulary building of Fishy Terms\n“A find kettle of fish.” “Like a fish out of water.””Other fish to fry.” “Fish or cut bait.” If you really love fish you may be an ichthyomaniac!\nCourses about Fish\nThe Diversity of Fishes\nTeens and older. From the American Museum of Natural History. Fishes were the first creatures with brains and bony jaws. But their incredible diversity—fishes make up half of all vertebrate species alive today, and live everywhere from mountain streams to ocean depths—makes it easier to determine what’s not a fish. This seminar explores this paradox by demonstrating how ichthyologists pursue the evidence through the study of evolution, diversity ecosystems, and biogeography.\nFish to Eat\nCalifornia’s Golden Seas\nAn activity book in Adobe Acrobat format, for students ages 9-11. Focus is primarily on fish for consumption. From the California Seafood council.\nRecipes for Kids from Gorton’s\nDo you like fish for dinner? Here’s some kid-pleasing recipes you’ll love: Clamaroni Casserole, Clam Cakes, Codfish Cakes, Crab Meat-Stuffed Baked Shells, Fish And Chips, Mixed Seafood Kebabs, and Parmesan Fish Stick Dippers.\nBest Pets: Top 7 Fish For Children and Kids\nKids love pet fish and they make for great exploration and learning. Fish of all size, colors, and shaped can really spark the imagination of everyone. It may sound a bit strange, but I like being in a room with a nice fish tank as it soothes the senses. The trickling water of the water pump is also quite soothing too!\nHow to Set up a Fish Tank\nAttention needs to be given to the location of the aquarium. This should be done before setting it up as moving it once operational can be a real hassle. When choosing a prime spot, keep the following points in mind…\nLarge capacily stackable media trays allow for multi-stage filtration\nQuick & easy push-button primer and two independent 360 degree rotating taps\nFlow rate shut-off valves adjust water flow and provide for easy maintainance.\nYou will need the sponge and floss pads sold along with the filter. Buy the correct size for your tank.\nPlaces for kids to fish for free\nAdd your state name or county to find nearby places where you can take you child to fish for free – no license required, just the pole and bait. Double check that the fish in that area haven’t unsafe levels of toxins in their bodies, or you’ll have to catch and release. (Which some kids prefer anyway.)\nFly Fishing for Beginners\nThis website is designed for the beginner (or even those who may need a refresher) wanting to learn the basics of fly fishing.\nTips for helping kids catch fish. Submit your fishing tales. If you live near Wichita, join in on the real fun, too.\nA Teenage Fisher Tells All\nHow many things in life improve over time? I can tell you that fishing, year after year, just gets better and better. From that first summer on, I was excited to go fishing. I was hooked! By Sarah Denise Williams.', 'Bluefin Tuna is one of the most expensive fish in the world and is largely sought after for its exceptional taste and tenderness. Bluefin Tuna is hunted by commercial fisheries all over the world, but they are on the verge of extinction.\n- Status: Critically Endangered, depends on species.\n- Species: Albacore Tuna, Atlantic Bluefin Tuna, Bigeye Tuna, Pacific Bluefin Tuna, Southern Bluefin Tuna.\n- Estimated numbers left in the wild: Unknown but likely ranging in the hundreds of thousands.\nThe tuna’s population has declined by 90% since the 1950s due to overfishing, which is why it’s endangered.\nIt was first caught commercially in 1908 and now there are only a few thousand left worldwide. Bluefin tuna can grow up to 10 feet long and weigh more than 1,500 pounds.\nThe Bluefin population has been severely depleted because of high demand from sushi restaurants around the world who prize this species for its light red meat to consume that remains moist when cooked or frozen.\nBluefin is one of the most expensive fish in the world, with Bluefin Tuna Sashimi selling for up to $24 per piece in Tokyo.\nThe Bluefin is very popular because it’s extremely fatty and tender, which makes it a favorite among sushi lovers. Avoiding Bluefin tuna products would be an easy way to save this fish from extinction.\nBluefin tunas are all hunters. They prey mostly on smaller fish that form schools or small squid that exhibit schooling or swarming behavior.\nHowever, they will also eat pelagic red crabs, krill, and certain sessile animals, such as sea anemones. Tuna are active predators who can put on spurts of up to 80 kilometers per hour to catch other fish.\nTo achieve this performance, they are warm-blooded. The bluefin species have the most control over their internal temperature. Very high blood hemoglobin levels oxygenate the tunas’ muscles for strength and power.\nSee Related: Fun And Interesting Devilfish Facts\nAnatomy and Appearance\nBluefin tuna are large predatory fish, stocky and sharp-finned, whose size can range up to 4.5 meters and whose weight can be as high as 450 kilograms. Their coloration tends to be striking, with metallic blue upper surfaces and, short pectoral fins, metallic white underparts.\nThis color gives these five species of tuna their general name of “bluefins,” and it serves a vital function in the species’ lives. The blue upper surfaces provide camouflage from above, while the silvery underparts help conceal the prowling tuna from below, allowing them to close more effectively with their prey.\nThe various tuna species are found in the Atlantic Ocean near Newfoundland and Iceland; the Gulf of Mexico; the Mediterranean; other areas of the Atlantic; and the Pacific Ocean. Tunas are deepwater fish that need a thriving biome to survive since they are predatory and feed on schools of smaller fish.\nSee Related: Environmental Organization in Europe\nBluefin Tuna Habitat\nIt is found in both the Atlantic, Pacific, and mediterranean sea. They inhabit a wide range of habitats, including open seas, coastal waters, bays, and estuaries. Bluefin Tuna are migratory fish and can travel long distances in search of food.\nThey are most commonly found in deep water, but they can also be found in shallow waters near the coast.\nBluefin Tuna Diet and Nutrition\nThese species are apex predators and have a very diverse diet. They mostly eat small fish, but they will also eat squid, crustaceans, and other types of prey.They are a very important part of the marine food chain and help to keep the populations of other fish in check.\nBluefin Tuna Mating Habits\nBluefin Tuna is a large fish that can grow to a weight of 1000 pounds and lengths over 10 feet. It takes from 7-10 years to reach full maturity, but they only reproduce once at this age. They are highly migratory, which makes them difficult to track by scientists.\nTuna are a migratory species and gather together in large numbers at chosen spawning grounds to produce their eggs to seek food.\nA single female tuna can produce anywhere from 5 million to 30 million eggs, though, of course, only a tiny fraction of the fry will ever grow to adulthood. If not caught by fishermen, a bluefin tuna can potentially live for anywhere from fifteen to thirty years.\nSee Related: Is a Fish an Animal? Here’s What You Need to Know\nAtlantic Bluefin Tuna vs Pacific Bluefin Tuna\nAtlantic Bluefin Tuna and Pacific Bluefin Tuna are both Bluefin Tuna, but they are different species.\nAtlantic tunas are the most common tuna and are found in the Atlantic Ocean. Pacific Bluefin is a rarer species and is found in the Pacific Ocean. Atlantic tuna is larger than Pacific tuna. Pacific tuna is more torpedo-shaped, while Atlantic tuna is more rounded.\nBluefin Tuna and Human Relationship\nThe Bluefin has been hunted by commercial fisheries for years all over the world for its exceptional taste and tenderness. Humans are among the leading cause of why such wonderful species is facing threats of endangerment.\nTuna is a prized catch, and because of this, its population has declined by 90% since the 1950s. The fish is a vital part of the marine ecosystem, and if it becomes extinct, it will have a profound impact on the marine food chain.\nThe main reason Bluefin tuna is endangered is overfishing and illegal fishing.\nTuna are being caught faster than they can reproduce, and if this trend continues, the species will become extinct. Bluefin is also hunted for their meat, and because the demand for this fish is so high, the prices for Bluefin have skyrocketed.\nThe species are a vital part of the marine ecosystem, and if it becomes extinct, it will have a profound impact on the marine food chain.\nThe main reason tuna is endangered is overfishing. They are being caught faster than they can reproduce, and if this trend continues, the Bluefin tuna will become extinct.\nBluefin Tuna Facts\nHere are the interesting facts about Bluefin Tuna that you need to know.\n- Tuna is the fish that is served at Nobu\n- Bluefin can swim up to 40 miles per hour\n- They are vulnerable because they are slow-growing and have a high demand\n- The aquatic predator is a large, predatory fish that is found in the Atlantic, Pacific, and western Mediterranean oceans.\n- Tuna can weigh up to 1,500 pounds and reach up to 10 feet in length.\n- Bluefin are commercially valuable fish and are hunted by fisheries all over the world.\n- Unfortunately, Bluefin is on the verge of extinction due to overfishing.\n- They have declined by 90% since the 1950s due to excessive hunting.\nAtlantic Bluefin tuna is a large, predatory fish that is found in the Atlantic and Pacific oceans. Bluefin is one of the most expensive fish in the world and is largely sought after for their exceptional taste and tenderness.\nBluefin is hunted by commercial fisheries all over the world, but they are on the verge of extinction. Tuna has declined by 90% since the 1950s due to overfishing, which is why it’s listed as an endangered species.\nOverfishing is the chief threat to all varieties of bluefin tuna. They have a migratory behavior of these large fish make it difficult for conservation efforts by any one government to make a significant difference in their overall fate, increasing the difficulty of tuna conservation.\nSushi and sashimi are among the most common uses of this fish. Demand and prices are very high in Japan, providing an ongoing market for fish caught despite various agreements, with 80% of the world catch going to Japan.\nSome secondary threats could also exist if the acidification of the oceans, caused by global warming, starts to affect the bluefin tunas’ food supplies.\nSee Related: Endangered Species in California\nSeveral good faith conventions have been made to limit fishing of these important members of the pelagic ecosystem, but how well they are observed depends largely on local decisions by fishers and governments.\nBoth Australia and Japan are attempting farming as an alternative to catching wild tuna, but the fish are difficult to raise due to the length of time it takes them to mature. Thus, there’s a need to conserve these giant beauties. The tuna were also under teh care of the national marine fisheries service.\nSee Related: Fascinating Facts About Conservation\nMonterey Bay Aquarium\nMonterey Bay Aquarium is a public aquarium located in Monterey, California. It is considered the sister institution to the Scripps Institution of Oceanography. The aquarium is home to thousands of marine animals and contains over 50 large tanks of water that are 10 feet high and 100 feet long, which house 3,500 different aquatic species.\nMonterey Bay Aquarium conducts research projects and tagging to study migration, habitats, diets, populations and protect different marine species like the Bluefin tuna.\nOceana is an international organization working to protect the world’s oceans. Oceana works with governments around the world in order to create policies that will help preserve our marine ecosystems so we don’t lose any more Bluefin Tunas or other marine life because of human actions such as fishing or pollution\nBluefin Tuna is one of the most expensive fish in the world and they are largely sought after for their exceptional taste and tenderness.\nBluefin tuna have declined by 90% since the 1950s due to overfishing, which is why it’s endangered.\nAccording to Monterey Bay Aquarium, tuna can swim up to 40 miles per hour and reach up to 10 feet in length. Bluefin tunas’ migratory habits make them difficult to track by scientists and if not caught by fishermen a bluefin could live anywhere from 15-30 years.\nSome secondary threats exist with Bluefin because they are slow-growing species that depend on food supplies from healthy oceans as well as acidification from global warming affecting their prey source.\nBluefin tuna is a commercially valuable fish and is hunted by fisheries all over the world. Bluefin is listed as an endangered species due to excessive hunting.\nBluefin tuna can swim up to 40 miles per hour, reach up to 10 feet in length, their migratory habits make them difficult for conservation efforts by any one government, 80% of the world catch go to Japan, and it takes about 15-30 years to mature.\nOceana is the largest international organization focused only on ocean conservation, protecting marine ecosystems and endangered species like the Bluefin Tuna. The Monterey Bay Aquarium conducts research projects and tagging Bluefin tuna in order to study migration, habitats, diets, Bluefin populations and more.\nBluefin have declined by 90% since the 1950s due to overfishing, which is why it’s endangered.\nWhat are Bluefin Tuna?\nBluefin Tuna are deep-sea fish that have a dark blue upper body and silvery-white underbelly.\nThey can grow up to 10 feet long and weigh more than 1,500 pounds. Bluefin is one of the most expensive fish in the world and is largely sought after for its exceptional taste and tenderness.\nBluefin tuna is hunted by commercial fisheries all over the world, but they are on the verge of extinction.\nWhy Bluefin tuna is endangered?\nBluefin Tuna is endangered due to overfishing by commercial fisheries.\nThink about it this way: there used to be a lot of these fish in the ocean, but now they’re almost all gone. That sounds kind of like the whole world ending if we don’t start protecting them soon.\nWhat can I do to help Bluefin tuna?\nYou can help Bluefin tuna by avoiding Bluefin products.\nIf there is no demand for Bluefin, then the fisheries will stop hunting it. You can also contact your local congressman to ask them to create laws that would protect Bluefin tuna.\nWhat will happen if Bluefin tuna becomes extinct?\nIf Bluefin tuna becomes extinct, the marine ecosystem will be disrupted. This could lead to the extinction of other species and a decline in the overall health of the ocean.\nThe loss of Bluefin tuna would also be a major economic blow to the Bluefin tuna industry. Bluefin tuna has declined by over 90% since 1950s, which is why it’s endangered.\nOther Species Profiles']	['<urn:uuid:6780099f-dbb9-4b7c-a394-acd0e9dfd645>', '<urn:uuid:7911b493-3211-49cc-a8b2-34f5d228cd7e>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-13T06:15:13.904987	13	60	2721
21	plasma heating tokamak safety features performance	The Tokamak fusion reactor employs sophisticated safety and heating mechanisms to control its plasma. The plasma, heated to 150,000,000°C, is confined using massive magnetic coils that keep it from touching the vessel walls, providing crucial safety. Three external heating systems work together: neutral beam injection fires high-speed particles into the plasma, while ion cyclotron and electron cyclotron systems use electromagnetic waves at specific frequencies to heat ions and electrons respectively. Recent achievements demonstrate the progress in plasma control - China's fusion project maintained plasma for 17 minutes, while the JET facility broke records with a 59-megajoule sustained fusion reaction. These results, though promising, have not yet achieved the goal of producing more energy than required for plasma heating and containment.	"['The Sun is a colossal source of energy. In fact, it outputs an estimated 384.6 yottawatts of energy in the form of electromagnetic waves. This power is produced by the process of fusion within the Sun’s core; therein can be found isotopes of hydrogen. Within these isotopes, the nuclei fuse to produce energy as well as other particles.\nIf we were to recreate and control this simple yet powerful set of reactions, we would have in our hands a way to generate incredible amounts of sustainable and carbon-free energy.\nWhat is fusion and how does it occur?\nWithin a star’s core, there is an abundance of hydrogen isotopes which go through a set of reactions to release the energy. The net reaction takes four protons from the hydrogen nuclei and fuses them into a helium-4 nucleus while also producing two positrons, two neutrinos, two gamma photons as well as a net energy output of 25 Mega electron Volts (MeV).\n4 11H ➔ 42He + 2 01e + 2 γ\nThis makes the Sun a massive source of neutrinos and gamma rays which are part of the energy output; the positrons instantly react with electrons to produce more gamma rays.\nThe reaction is very simple, however, some difficult obstacles must be overcome before we can start to simulate the Sun’s energy production here on Earth.\nReactions such as fusing two deuterons (hydrogen-2 nuclei) require energy inputs of 3.6 MeV per reaction. This is to reach temperatures of up to 10 000 000 K which is needed to give the nuclei enough energy to react. The need for this intense supply of kinetic energy is to overcome the electrostatic repulsion between two nuclei. Each nucleus has a net positive charge due to its protons, and these like charges repel. However, this can be overcome if the nuclei get close enough for the strong nuclear force to come into play. This force will overcome the repulsion and bind the two nuclei together into the newly formed helium-4 nucleus, releasing energy as it does so.\nFigure 1: Cross-section of the sun\nThe environment at the core of the Sun provides these huge energy requirements due to its high pressures. The gravitational force of the Sun draws all its mass down towards the centre, crushing the core to a relatively small volume. The core is put under a very large amount of pressure and this allows the hydrogen nuclei to obtain enough kinetic energy to move sufficiently close to each other to give rise to the strong nuclear force. After this takes effect, the nuclei speed towards each other to fuse into a helium nucleus and release energy.\nNow, the challenge we face is to build a reactor that can give hydrogen nuclei the same amount of energy as the pressure in the core of the Sun. This seems like a mountainous task, but many scientists are working together to overcome it and welcome a new era of sustainable energy production.\nThe ITER Agreement\nNearly 30 years ago, a group of powerful nations decided to collaborate on a project to develop a system that was capable of simulating the fusion that occurs in the Sun’s core to produce potentially limitless and sustainable energy. The idea of the development of fusion energy for peaceful purposes was first proposed in 1985 at the Geneva Superpower Summit by General Secretary Gorbachev of the former Soviet Union to the US President Reagan.\nAfter a year, the European Union, the Soviet Union, Japan, and the USA agreed to work collaboratively on the project. The development started in 1988 with initial designs. This led to a chapter of more focused engineering design work being conducted until the final design was agreed upon in 2001.\nThe People’s Republic of China and the Republic of Korea joined the journey in 2003 followed by India in 2005. That was also the same year in which the fusion reactor location was decided: Aix-en-Provence, France.\nThe very next year, the ITER Agreement was signed in Paris by the representatives of the seven member-countries. The construction of the experiment facility began in the year of 2010. Today, 2000 people are working in conjunction to develop the world’s most advanced magnetic confinement system that can be used for nuclear fusion, and they call it a Tokamak.\nWhat is a Tokamak?\nFigure 2: Generator diagram\nInside a power plant, the motion of a spinning turbine is converted into electrical energy using a generator. The turbine is attached to a large magnet which spins as the turbine spins. The magnet is inside a cylindrical space bounded by many coils of conducting wire. The spinning of the magnet causes a constantly changing magnetic field which induces current – the flow of electrons – in the coil of wire.\nFigure 3: Tokamak design\nA Tokamak is an experimental machine that works in a similar way to a nuclear reactor. It is designed with the function of harnessing the energy produced by fusion to heat a volume of water into steam to turn a turbine.\nHow does a Tokamak work?\nFigure 4: Tokamak with plasma\nThe main component of a tokamak is its doughnut-shaped vacuum. Under the effect of intensely high temperature and pressure, gaseous hydrogen fuels are converted into plasma. Plasma is what provides the environment for light elements – such as hydrogen – to fuse in the core of a star and this is what a tokamak is designed to copy.\nThe shape and movement of the dangerously hot plasma can be controlled by the massive magnetic coils placed around the vessel. This control means that the plasma can be kept away from the vessel adding a layer of safety to the reactor.\nThe first step is to remove any impurities from the vacuum as you only want to fuse the hydrogen fuel. The magnet system is then powered up. The hydrogen fuel is then released into the chamber and a powerful electrical current is run through.\nThis causes the gas to become ionised as all the nuclei are stripped of their electrons, this mixture then forms a plasma which is confined in a doughnut shape by the magnet system.\nThe plasma particles gradually increase in temperature as the particles in the chaotic mixture collide with each other and gain more kinetic energy.\nThe plasma is then heated to a point where the nuclei are at a sufficiently high energy level to overcome their electrostatic repulsion and fuse.\nHow is the Plasma Heated?\nThe environment inside the Tokamak chamber must reach a temperature of 150 000 000 °C. To put this into perspective, this is ten times the temperature at the centre of the blazing Sun. The plasma must be constantly kept at this temperature to provide the activation energy for fusion.\nThree external heating systems that will operate in conjunction to heat the chamber environment to the required level.\nResearchers hope to produce a ‘burning plasma’; they aim to produce a plasma in which the helium nuclei – produced by fusion – can provide enough energy to maintain the temperature of the chamber. If this were to happen, then the heating systems could be significantly reduced or possibly switched off entirely which would save incredible amounts of power and money.\nNeutral Beam Injection\nFigure 5: Neutral beam injection system\nOne of the external heating systems is a neutral beam injection system. This machine was engineered to accelerate a neutral particle to an incredibly high speed to be fired into the plasma and pass on its kinetic energy.\nPhysicists know that a charged particle experiences a force (and is therefore accelerated) when in a magnetic field. This is one way of accelerating the neutral particle but it requires the particle to have a non-neutral charge. This is why the particle starts off being ionised (by having one of its electrons removed) into a positive ion. The ion will then be passed through a magnetic field which will accelerate it to a high speed. It will then regain its electron by speeding through a cloud of gas before entering the reactor chamber so that it becomes neutral again. The particle must be neutral, otherwise, it will be deflected by the strong magnetic system around the chamber (controlling the plasma) because of it interacts with the magnetic field (similar to how it was accelerated).\nHowever, there is a problem with this method. The particle must be travelling at incredibly high speeds – three or four times that of previous systems – to penetrate deep enough into the plasma for the initialisation of the fusion reactions; positive ions become very difficult to neutralise (before entry into the chamber) at these high speeds.\nTherefore, for the very first time, ITER will be using a negatively charged ion to reach high speeds. This ion has its extra electron attached very loosely to the nucleus, so it can be removed with little difficulty. However, this poses a second problem as negative ions are much more challenging to produce and control.\nIon Cyclotron Heating\nFigure 6: Ion cyclotron resonance heater\nThe second method of heating is to simply use electromagnetic waves at very high frequencies to transfer heat to the plasma-like how a microwave oven transfers heat to the food inside.\nEnergy is transferred to the ions in the chamber by a high-intensity beam of electromagnetic waves at frequencies of 40 – 55 MHz.\nElectron Cyclotron Heating\nThis works very similarly to the ion cyclotron heating system. This uses electromagnetic waves to transfer energy specifically to the electrons in the plasma. The electrons then transfer this newly gained energy to the ions via electron-ion collisions.\nFigure 7: Magnet system used in the Tokamak\nThe magnet system being used in the ITER fusion reactor will be the most enormous and integrated superconducting magnet system to be ever built.\nThe whole system will contain 10 000 tonnes of magnets; the total stored magnetic energy will be approximately 51 GJ. They will be constructed from alloys of niobium-tin or niobium-titanium. The magnetic system becomes superconducting (allowing electrons to flow with absolutely no resistance) and functional when it is cooled with supercritical helium at a temperature of 4 Kelvin (-296 °C).\nThe advantage of using superconducting magnets is that they can carry high current and produce tremendously powerful magnetic systems while consuming less power and have a lower cost. This made them an obvious choice for the ITER fusion reactor which already has a very high energy consumption and financial investment.\nITER will be using high performance, internally-cooled superconductors. These are grouped strands of superconducting materials mixed with copper which are cabled together and fitted into a structural steel jacket.\nFigure 8: Superconducting cable\nFigure 9: Other designs\nThere are many other designs for fusion reactors currently being tested. An example is the laser design in which a small pellet of fuel is confined and heated with a laser until it reaches a hot and dense enough state to kickstart fusion.\nThe hope is that – with many innovative designs in development – our problem-solving skills and ingenuity as engineers and scientists can help us find the best solution to this newly found problem.\nWhy isn’t this a reality?\nThere are still many challenges that must be overcome in terms of the capabilities of today’s machines. Current experiments with fusion reactors require much more energy to start the reactions than the energy produced by them. This is giving scientists data to analyse and improve their designs, but it is also a huge loss in terms of the money required to power the reactors themselves.\nThere is also the issue of money. Countries are investing heavily in this 10 billion dollar project when they could be placing much safer investments into already proven clean energy solutions such as hydroelectric power, wind, and solar. Are they willing to bet their time, money and resources on a solution that could end up failing?\nHowever, the possibility of potentially generating virtually limitless supplies of energy for everyone has kept the leading nations focused on building this mini Sun on Earth.\nWhy do we need fusion?\nAs the human population is predicted to reach 10 billion by 2050, there will be a significant increase in the energy demanded (two or three times the current energy demands).\nThese demands will be difficult to reach with current energy production methods. This is especially true with fossil fuels which are becoming increasingly dangerous to use, yet they still make up the largest sector of energy production.\nThe development of fusion energy would lead to a potentially unlimited supply of energy; the deuterium required for fusion is abundant in seawater and tritium can be made from fusing lithium. Fusion energy doesn’t produce greenhouse gases or long-term radioactive waste which makes it a clear winner in strategies to meet energy demands.\nIn times where we are using more and more energy at the cost of the environment, it is clear that fusion energy is the way forward.\n- UTIA. The Sun’s Energy. UTIA Institute of Agriculture. https://ag.tennessee.edu/solar/Pages/What%20Is%20Solar%20Energy/Sun’s%20Energy.aspx (accessed 03 Sep. 2018)\n- CCFE. Introduction to fusion. http://www.ccfe.ac.uk/introduction.aspx (accessed 03 Sep. 2018)\n- Mike O’Neill. OCR A level Physics A 2. (London, Pearson Education Limited, 2nd edition), pages 223-224\n- ITER. THE ITER STORY. https://www.iter.org/proj/iterhistory (accessed 04 Sep. 2018)\n- ITER. WHAT IS A TOKAMAK?. https://www.iter.org/mach/Tokamak (accessed 04 Sep. 2018)\n- ITER. WHAT IS A TOKAMAK?\n- Kurzgesagt – In a Nutshell. Fusion Powers Explained – Future or Failure. (YouTube, 2016).\n- (accessed 04 Sep. 2018)\n- ITER. EXTERNAL HEATING SYSTEMS. https://www.iter.org/mach/Heating (accessed 04 Sep. 2018)\n- ITER. EXTERNAL HEATING SYSTEMS\n- ITER. EXTERNAL HEATING SYSTEMS\n- ITER. MAGNETS. https://www.iter.org/mach/magnets (accessed 04 Sep. 2018)\n- Kurzgesagt – In a Nutshell. Fusion Powers Explained – Future or Failure.\n- CCFE. WHY FUSION IS NEEDED. http://www.ccfe.ac.uk/Why_fusion.aspx (accessed 03 Mar. 2019)\nTitle image of a fusion reactor. Source URL: https://icdn2.digitaltrends.com/image/iter-fusion-reactor.jpg [Accessed 03 Sep. 2018].\nA cross-section of the Sun. Source URL: https://i.ytimg.com/vi/W1ZQ4JBv3-Y/maxresdefault.jpg [Accessed 04 Sep. 2018].\nGenerator diagram. Source URL: http://4.bp.blogspot.com/-bp7cWiFL5R0/TvQPCuYuGkI/AAAAAAAAAV4/ag28eDL8ifA/s1600/plant_generator.gif [Accessed 04 Sep. 2018].\nTokamak design. Source URL: https://www.iter.org/mach/Tokamak [Accessed 04 Sep. 2018].\nTokamak with plasma. Source URL: https://www.iter.org/mach/Tokamak [Accessed 04 Sep. 2018].\nNeutral beam injection system. Source URL: https://www.iter.org/mach/Heating [Accessed 04 Sep. 2018].\nIon cyclotron resonance heater. Source URL: https://www.iter.org/mach/Heating [Accessed 04 Sep. 2018].\nMagnet system used in the Tokamak. Source URL: https://www.iter.org/mach/magnets [Accessed 04 Sep. 2018].\nSuperconducting cable. Source URL: https://www.iter.org/mach/magnets [Accessed 04 Sep. 2018].\nOther designs. Source URL: http://i.gzn.jp/img/2016/11/11/fusion-energy-explained/cap00050.png [Accessed 04 Sep. 2018].\nAbout the author\nTawsif is an A-Level student pursuing a career in aerospace engineering. He has a keen fascination with physics and how it can be employed to engineer machines that solve real-world problems. He has previously conducted projects with Airbus and BRE to gain industry experience; he continues to research into physics and engineering to advance his learning and feed his curiosity.', 'There\'s no denying it.\nNuclear fusion has the potential to completely transform the U.S. energy industry, and become a primary source of zero-carbon energy.\nThis is why, in a bid to ""win the 21st-century economy,"" the United States is heavily investing in nuclear fusion technology, according to a March 17 White House summit reported on by Scientific American.\nWhile scientific consensus is firm that we\'re still not ready to roll out fusion technology for commercial use, the early stages of renewable energy sources like solar, wind, and small modular reactors demand that we double down on reliable backups.\nAnd, in harnessing the same power used by the Sun, nuclear fusion could transform our energy infrastructure, for keeps.\nNuclear fusion can help us achieve net-zero emissions by 2050\nFusion is a process in physics whereby two atoms are pushed into one another until they merge, and create a heavier atom. This releases a tremendous amount of energy — it\'s the same process happening at the core of every star — and it generates comparably low levels of radiation.\nThat makes it an appealing alternative not only to fossil fuel or coal mining, but potentially every other kind of sustainable energy technology — even solar and wind. But critics among scientific consensus have maintained for decades that, while empirically plausible, the practical realities of implementing fusion technology on a commercial basis are and will remain beyond our reach for decades more.\nGet more updates on this story and more with The Blueprint, our daily newsletter: Sign up here for free.\nBut now, the White House has joined with a group of investors who don\'t shy away from risk to put forward fusion as a crucial means to building a new economy for the U.S., one that can offer net-zero greenhouse gas emissions by 2050, according to a fact sheet from the White House.\n""We can lead the world with new energies and innovation and that is exactly what we are doing and why we are gathered here today,"" said Gina McCarthy, the White House\'s climate advisor, during the summit. ""We have to act on climate change so our country can win the 21st-century economy, and that\'s what fusion helps to present us with — tremendous opportunities as well as challenges we know.""\nNuclear fusion deployment by end of the 2020s ""possible""\nIncredibly, $45 million of the $1.5 trillion appropriations bill from Congress is committed to a new fusion program that will see private firms join forces with the Department of Energy (DOE) — united in a $700-million pursuit of novel fusion energy devices with the DOE\'s Fusion Energy Sciences program.\nThis multi-pronged effort to coordinate fusion energy research could see ""possible"" deployment by the end of the 2020s, said Jennifer Granholm, the energy secretary, during the summit.\nBut while it\'s tempting to rush to utopic depictions of a new fusion-powered future, we should definitely keep from expecting too much, according to Granholm. ""We\'ve got to manage expectations,"" he said. ""There\'s a reason why fusion is hard. So it\'s going to take time: Even as we are making amazing progress, we have to be careful about overpromising, and we have to be realistic.""\n🥳Record-breaking 59 megajoules of sustained fusion energy at world-leading UKAEA’s Joint European Torus (JET) facility. Video shows the record pulse in action. Full story https://t.co/iShCGwlV9Y #FusionIsComing #FusionEnergy #STEM #fusion @FusionInCloseUp @iterorg @beisgovuk pic.twitter.com/ancKMaY1V2— UK Atomic Energy Authority (@UKAEAofficial) February 9, 2022\nExtraordinary gains are in store for nuclear fusion this decade\nAlas, considering the scientific obstacles to realizing viable fusion power, she\'s not wrong. But this isn\'t to say fusion development is stuck, not by a long shot: A nuclear fusion project in China successfully sustained fusion reactions for 17 minutes at 126 million degrees Fahrenheit — five times the temperature of the Sun.\nIn February, scientists at the Joint European Torus (JET) facility in the United Kingdom\'s Oxford broke their own 24-year-old record by creating a 59-megajoule sustained fusion reaction. They even released a video of the breakthrough on Twitter.\nSmall moves, big gains - Ultimately, the skeptical attitude is apt when it comes to how soon nuclear fusion power that\'s viable on commercial scales will come. But with benchmarks and milestones beginning to follow one another like clockwork — from generating more power than is put into a fusion reaction to sustaining that output to more practical durations — we would be foolish to pretend that the 2020s won\'t be a time of extraordinary growth for nuclear fusion.']"	['<urn:uuid:3412c47b-5ec7-4473-943f-e66d6ccb18f8>', '<urn:uuid:e896b738-2e68-443f-aa84-a96082114d32>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	6	120	3224
22	fixed dose combination drugs fdc development single disease multiple conditions nda approval requirements manufacturing process	Fixed-dose combination (FDC) drugs were initially developed for single diseases but evolved to target multiple related conditions to justify manufacturing and distribution. For FDA approval through a New Drug Application (NDA), FDCs must provide comprehensive information including patent and manufacturing details, safety and effectiveness data, clinical trial reports, abuse potential, and proposed labeling. The application must demonstrate substantial evidence of effectiveness through controlled clinical trials and show that benefits outweigh risks.	"['There is not a Watson b53 but there is a Watson 853. If that is what it is it is Acetaminophen/Hydrocodone, an analgesic combination used to treat pain. Hydrocodone is a narcotic. The brand name is Vicodin.\nA combination drug most commonly refers to a fixed-dose combination (FDC), which is a formulation including two or more active pharmaceutical ingredients (APIs) combined in a single dosage form, which is manufactured and distributed in certain respective fixed doses. Terms like ""combination drug"" or ""combination drug product"" can be common shorthand for a FDC product (since most combination drug products are currently FDCs), although the latter is more precise if in fact referring to a mass-produced product having a predetermined combination of drugs and respective dosages (as opposed to customized polypharmacy via compounding). And it should also be distinguished from the term ""combination product"" in medical contexts, which without further specification can refer to products that combine different types of medical products - such as device/drug combinations as opposed to drug/drug combinations. Note that when a combination drug product (whether fixed-dose or not) is a ""pill"" (i.e., a tablet or capsule), then it is also a kind of ""polypill"" or combopill.\nInitially, fixed-dose combination drug products were developed to target a single disease (such as with antiretroviral FDCs used against AIDS). However, FDCs may also target multiple diseases/conditions, such as Caduet (atorvastatin/amlodipine) or Exforge (amlodipine/valsartan). In cases of FDCs targeting multiple conditions, such conditions might often be related — in order to increase the number of prospective patients who might be likely to utilize a given FDC product. This is because each FDC product is mass-produced, and thus typically requires having a critical mass of potentially applicable patients in order to justify its manufacture, distribution, stocking, etc.\nOrganic chemistry is a chemistry subdiscipline involving the scientific study of the structure, properties, and reactions of organic compounds and organic materials, i.e., matter in its various forms that contain carbon atoms. Study of structure includes using spectroscopy and other physical and chemical methods to determine the chemical composition and constitution of organic compounds and materials. Study of properties includes both physical properties and chemical properties, and uses similar methods as well as methods to evaluate chemical reactivity, with the aim to understand the behavior of the organic matter in its pure form (when possible), but also in solutions, mixtures, and fabricated forms. The study of organic reactions includes both their preparation—by synthesis or by other means—as well as their subsequent reactivities, both in the laboratory and via theoretical (in silico) study.\nThe range of chemicals studied in organic chemistry include hydrocarbons, compounds containing only carbon and hydrogen, as well as compositions based on carbon but containing other elements. Organic chemistry overlaps with many areas including medicinal chemistry, biochemistry, organometallic chemistry, and polymer chemistry, as well as many aspects of materials science.\nA social issue (also called a social problem or a social situation) is an issue that relates to society\'s perception of a person\'s personal lives. Different cultures have different perceptions and what may be ""normal"" behavior in one society may be a significant social issue in another society. Social issues are distinguished from economic issues. Some issues have both social and economic aspects, such as immigration. There are also issues that don\'t fall into either category, such as wars.\nThomas Paine, in Rights of Man and Common Sense, addresses man\'s duty to ""allow the same rights to others as we allow ourselves"". The failure to do so causes the birth of a social issue.\nHealth Medical Pharma\nIn journalism, a human interest story is a feature story that discusses a person or people in an emotional way. It presents people and their problems, concerns, or achievements in a way that brings about interest, sympathy or motivation in the reader or viewer.\nHuman interest stories may be ""the story behind the story"" about an event, organization, or otherwise faceless historical happening, such as about the life of an individual soldier during wartime, an interview with a survivor of a natural disaster, a random act of kindness or profile of someone known for a career achievement.\nHealth Medical Pharma\nScience of drugs including their origin, composition, pharmacokinetics,\npharmacodynamics, therapeutic use, and toxicology.\nPharmacology (from Greek φάρμακον, pharmakon, ""poison"" in classic Greek; ""drug"" in modern Greek; and -λογία, -logia ""study of"", ""knowledge of"") is the branch of medicine and biology concerned with the study of drug action, where a drug can be broadly defined as any man-made, natural, or endogenous (within the body) molecule which exerts a biochemical and/or physiological effect on the cell, tissue, organ, or organism. More specifically, it is the study of the interactions that occur between a living organism and chemicals that affect normal or abnormal biochemical function. If substances have medicinal properties, they are considered pharmaceuticals.', 'New Drug Application\nThe Food and Drug Administration\'s New Drug Application (NDA) is the vehicle in the United States through which drug sponsors formally propose that the FDA approve a new pharmaceutical for sale and marketing. Some 30% or less of initial drug candidates proceed through the entire multi-year process of drug development, concluding with an approved NDA, if successful.\nThe goals of the NDA are to provide enough information to permit FDA reviewers to establish the complete history of the candidate drug. Among facts needed for the application are:\n- Patent and manufacturing information\n- Drug safety and specific effectiveness for its proposed use(s) when used as directed\n- Reports on the design, compliance, and conclusions of completed clinical trials by the Institutional Review Board\n- Drug susceptibility to abuse\n- Proposed labeling (package insert) and directions for use\nTo legally test the drug on human subjects in the U.S., the maker must first obtain an Investigational New Drug (IND) designation from FDA. This application is based on nonclinical data, typically from a combination of in vivo and in vitro laboratory safety studies, that shows the drug is safe enough to test in humans. Often the ""new"" drugs that are submitted for approval include new molecular entities or old medications that have been chemically modified to elicit differential pharmacological effects or reduced side effects.\nThe legal requirement for approval is ""substantial"" evidence of effectiveness demonstrated through controlled clinical trials. This standard lies at the heart of the regulatory program for drugs. Data for the submission must come from rigorous clinical trials.\nThe trials are typically conducted in three phases:\n- Phase 1: The drug is tested in 20 to 100 healthy volunteers to determine its safety at low doses. About 70% of candidate drugs advance to Phase 2.\n- Phase 2: The drug is tested for both efficacy and safety in up to several hundred people with the targeted disease. Some two-thirds of candidate drugs fail in Phase 2 clinical trials due to the drug not being as effective as anticipated.\n- Phase 3: The drug is typically tested in several hundred to several thousand people with the targeted disease in double-blind, placebo controlled trials to demonstrate its specific efficacy. Under 30% of drug candidates succeed through Phase 3.\n- Phase 4: These are postmarketing surveillance trials in several thousand people taking the drug for its intended purpose to monitor efficacy and safety of the approved marketed drug.\nThe legal requirements for safety and effectiveness have been interpreted as requiring scientific evidence that the benefits of a drug outweigh the risks and that adequate instructions exist for use, since many drugs have adverse side effects.\nMany approved medications for serious illnesses (e.g., cancer) have severe and even life-threatening side effects. Even relatively safe and well understood OTC drugs such as aspirin can be dangerous if used incorrectly.\nThe actual applicationEdit\nThe results of the testing program are codified in an FDA-approved public document that is called the product label, package insert or Full Prescribing Information. The prescribing information is widely available on the web, from the FDA, drug manufacturers, and frequently inserted into drug packages. The main purpose of a drug label is to provide healthcare providers with adequate information and directions for the safe use of the drug.\nThe documentation required in an NDA is supposed to tell the drug’s whole story, including what happened during clinical tests, what the ingredients of the drug formulation are, results of animal studies, how the drug behaves in the body, and how the company manufactures, processes and packages it. Currently, the FDA decision process lacks transparency, however, efforts are underway to standardize the benefit-risk assessment of new medicines. Once approval of an NDA is obtained, the new drug can be legally marketed starting that day in the U.S.\nOnce the application is submitted, the FDA has 60 days to conduct a preliminary review, which assesses whether the NDA is ""sufficiently complete to permit a substantive review."" If the FDA finds the NDA insufficiently complete (reasons can vary from a simple administrative mistake in the application to a requirement to re-conduct testing), then the FDA rejects the application by sending the applicant a Refuse to File letter, which explains where the application failed to meet requirements.\nAssuming the FDA finds everything acceptable, they decide if the NDA needs a standard or accelerated review, and communicates acceptance of the application and their review choice in another communication, known as the 74-day letter. A Standard Review implies an FDA decision within about 10 months while a Priority Review should complete within 6 months. The decision comes in a Complete Response Letter.\nRequirements for similar productsEdit\nBiologics, such as vaccines and many recombinant proteins used in medical treatments are generally approved by FDA via a Biologic License Application (BLA), rather than an NDA. The manufacture of biologics is considered to differ fundamentally from that of less complex chemicals, requiring a somewhat different approval process.\nGeneric drugs that have already been approved via an NDA submitted by another maker are approved via an Abbreviated New Drug Application (ANDA), which does not require all of the clinical trials normally required for a new drug in an NDA. Most biological drugs, including a majority of recombinant proteins are considered ineligible for an ANDA under current US law. However, a handful of biologic medicines, including biosynthetic insulin, growth hormone, glucagon, calcitonin, and hyaluronidase are grandfathered under governance of the Federal Food Drug and Cosmetics Act, because these products were already approved when legislation to regulate biotechnology medicines later passed as part of the Public Health Services Act.\nMedications intended for use in animals are submitted to a different center within FDA, the Center for Veterinary Medicine (CVM) in a New Animal Drug Application (NADA). These are also specifically evaluated for their use in food animals and their possible effect on the food from animals treated with the drug.\n- ""The Drug Development Process"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- ""The Drug Development Process. Step 4: FDA Drug Review"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- Commissioner, Office of the. ""Public Health Focus - FDA and Marijuana"". www.fda.gov. Archived from the original on April 28, 2018. Retrieved April 30, 2018.\n- ""The Drug Development Process. Step 3: Clinical Research"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- ""The Drug Development Process. Step 1: Discovery and Development"". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- Food, Drug, and Cosmetic Act, Section 505; 21 USC 355]\n- 21 CFR 201.5: Labeling Requirements for Prescription Drugs and/or Insulin\n- ""Daily Med:Current Medication Information"". Archived from the original on November 12, 2008. Retrieved October 10, 2007.\n- Liberti L, McAuslane JN, Walker S (2011). ""Standardizing the Benefit-Risk Assessment of New Medicines: Practical Applications of Frameworks for the Pharmaceutical Healthcare Professional"". Pharm Med. 25 (3): 139–46. doi:10.1007/BF03256855. Archived from the original on February 6, 2012.\n- Kathie Clark (December 15, 2009). ""Updates from the Regulators:FDA"". The eCTD summit. Archived from the original on July 16, 2011.\n- ""Merck KGaA Receives Refuse To File Letter From FDA On Cladribine Tablets New Drug Application"". medicalnewstoday.com. Archived from the original on March 5, 2010. Retrieved April 30, 2018.\n- ""Archived copy"". Archived from the original on March 8, 2010. Retrieved February 23, 2010.CS1 maint: Archived copy as title (link)\n- ""Cadence Pharmaceuticals Announces Priority Review and Acceptance of NDA Submission for Acetavance for Treatment of Acute Pain and Fever"". drugs.com. Archived from the original on July 11, 2017. Retrieved April 30, 2018.\n- ""FDA, CDER Office of Generic Drugs"". fda.gov. Archived from the original on May 28, 2009. Retrieved April 30, 2018.\n- ""C&EN: COVER STORY - BEYOND HATCH-WAXMAN"". pubs.acs.org. Retrieved April 30, 2018.\n- Henninger, Daniel (2002). ""Drug Lag"". In David R. Henderson (ed.) (ed.). Concise Encyclopedia of Economics (1st ed.). Library of Economics and Liberty.CS1 maint: Extra text: editors list (link) OCLC 317650570, 50016270, 163149563\n- Chapter 11: Prescription Drug Product Submissions in: Fundamentals of US Regulatory Affairs, Eighth Edition 2013']"	['<urn:uuid:a9af258c-85ee-41ed-ba85-436e654473da>', '<urn:uuid:533abac9-692c-499a-87d7-9a087ebf7ea3>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:15:13.904987	15	71	2162
23	I'm interested in stories about peace during wars, and I've also been reading about coral reefs lately. What factors affect coral reef survival, and are there any examples of unexpected truces during wartime?	Regarding coral reef survival, several key factors affect their health. They are very vulnerable to warm sea temperatures, which can cause coral bleaching - a breakdown of the vital coral-algal relationship. Global warming has already impacted reefs significantly, with mass bleaching events becoming more frequent, especially during El Niño events. Even the modest 0.92°C global warming to date has proven detrimental. Additionally, ocean acidification accelerates reef degradation through increased bioerosion by organisms like the Cliona orientalis sponge. As for unexpected wartime truces, two remarkable examples stand out. During WWI in 1914, British and German soldiers spontaneously declared a Christmas truce, emerging from trenches to exchange food, sing carols, and even play soccer. Later, during WWII's Battle of the Bulge, another extraordinary truce occurred when Elisabeth Vincken welcomed both American and German soldiers into her cabin on Christmas Eve, insisting 'It is the Holy Night and there will be no shooting here.' She made them leave their weapons outside and shared a meal of roast chicken and potatoes, creating a brief moment of peace amidst the war.	"['It was Christmas Eve 1914. The Tommies of Britain’s Queen’s Westminster Regiment had returned to the frigid trenches the previous day, relieving regular troops after four days of rest.\nSuddenly, in the stillness and cold, the voice of a young farmer’s son, Edgar Aplin, rose up from the frozen earth with “Tommy, Lad!,” a popular song written in 1907 by American lyricist Edward Teschemacher and composer E.J. Margetson:\nTommy, lad! Tommy, lad!\nThough you’re scarce a wee year old;\nYet you’re long and you’re strong,\nAnd your head’s a mass of gold;\nAnd you’ve got a mighty will of your own,\nYou’ve got a kind of way,\nThat will carry you along, I know;\nWhen you face the world one day,\nA few hundred metres away, from the opposing trenches where the 107th Saxon Regiment was dug in, voices called out: “Sing it again, Englander. Sing ‘Tommy, Lad!’ again.”\nAnd so he did. And as the London Telegraph described it, thus began “one of the most remarkable episodes ever to take place in the history of armed conflict.”In a letter home, unearthed a century after it was written, Alpin described the chain of events that led to the now legendary Christmas Truce of 1914. Back in July, many had predicted the war would be over by then.\n“On Christmas Eve, the usual war methods went on all day, sniping, etc., until the evening, when we started a few carols and the old home songs,” he wrote.\n“Immediately, our pals over the way began to cheer, and eventually we got shouting across to the Germans. Those opposite our front can mostly speak English.\n“Soon after dark, we suggested that if they would send one man halfway between the trenches (300 yards), we would do the same–and both agreed not to fire.\n“So, advancing towards each other, each carrying a torch, when they met, they exchanged cigarettes and ‘lit up.’ The cheering on both sides was tremendous, and I shall never forget it. After a little while, several others went out, and a pal of mine met an officer who said that if we did not shoot for 48 hours, they wouldn’t. And they were as good as their word, too. On Christmas Day, we were nearly all out of the trenches. It was almost impossible to describe the day as it appeared to us here and I can tell you, we all enjoyed the peaceful time.”\nIt wasn’t the only unofficial truce in 1914, the only year of the Great War that there would be any Christmas truce at all.\nTroops, including Canadians, in scattered locations all along the front spontaneously, if not cautiously, declared their own truces, emerging from their trenches to exchange food and souvenirs, hold joint burial ceremonies and prisoner swaps, and sing carols. Men also played soccer, creating one of the most enduring images of the short-lived peace.\nAlpin would eventually be wounded in the legs, but he survived the war. After it was over, he started a milk round, pushing a cart around the streets of Tonbridge, England. Later, he set up a small chain of tea rooms, called Alpin’s Tudor Cafés.\nHis 89-year-old son Ian said his father had spoken often of the Christmas truce, which has so far proven unique in the annals of war.\nThree decades later, however, in a cabin in a forest just across the Belgian border into Germany, a much smaller but just as improbable truce took place, the details of which read like something out of the Brothers Grimm.\nIt was the Battle of the Bulge, one of the last German offensives of the Second World War. The paratroopers of the U.S. 101st Airborne were hungry and low on ammunition. Weather was preventing resupply drops.\nNevertheless, Brigadier-General Anthony McAuliffe, acting commander of the 101st at Bastogne, had two days earlier famously rejected his German counterpart’s surrender terms.\n“There is only one possibility to save the encircled U.S.A. troops from total annihilation: that is the honorable surrender of the encircled town,” said a typed note, delivered by four German soldiers and signed “The German Commander.”\nHe gave McAuliffe two hours to decide.\n“If this proposal should be rejected one German Artillery Corps and six heavy A. A. Battalions are ready to annihilate the U.S.A. troops in and near Bastogne.”\nMcAuliffe’s reply, typed and centered on a full sheet, was simple and direct:\nDecember 22, 1944\nTo the German Commander,\nN U T S!\nThe American CommanderMeanwhile, deep in the snow-covered Ardennes, three American soldiers—Jim, Ralph and Herby, who was wounded—had lost their way. They walked for three days, searching for the U.S. lines while the sounds of battle reverberated in the hills and valleys around them.\nOn Christmas Eve, the day the weather broke and resupply commenced, they stumbled on a small cabin in the woods. Inside, Elisabeth Vincken and her 12-year-old son Fritz had been hoping her husband Hubert would arrive for Christmas. The family home in Aachen, Germany, had been bombed and Hubert, a Wehrmacht baker, had moved them to the relatively secure hunting cabin in the Hurtgen Forest near the Belgian border.Hubert had been visiting whenever he could, but it had become evident at this late hour that there would be no family Christmas for the Vinckens this year.\nSuddenly, there was a knock on the door. Elisabeth opened it and there, standing before her, were two enemy soldiers. A third was prostrate in the snow with a gunshot wound to the leg.They seemed like boys, despite their ragged state. The fact they had knocked and remained at a respectful distance when they could simply have burst in betrayed an innocence that gave her reason to trust.\nElisabeth invited them inside and they laid the wounded man on a bed. She and one of the soldiers were able to communicate in French. Eventually, Elisabeth sent Fritz to fetch six potatoes and Hermann the rooster, named for Nazi leader Hermann Göring, for whom she had little time or affection.\nHermann was roasting in the oven when there came another knock on the door. Fritz opened it and was confronted by four German soldiers.\n“I was almost paralyzed with fear,” Fritz would later recall, “for though I was a child, I knew that harsh law of war: Anyone giving aid and comfort to the enemy would be shot.”\nA terrified Elisabeth pushed past her son, stepped outside and closed the door behind her. The German corporal and three young soldiers were lost and hungry. They politely wished her Fröhliche Weihnachten (Merry Christmas).\nElisabeth told them they were welcome to come into the warmth and eat until the food was gone, but she cautioned that there were others inside whom they would not consider friends.\nThe corporal asked sharply if her guests were Americans. She related the Americans’ story, so similar to their own, then answered the corporal’s hard stare with the words: “Es ist Heiligabend und hier wird nicht geschossen. (It is the Holy Night and there will be no shooting here).”She insisted they leave their weapons in the shed outside. The dazed Germans slowly complied. Elisabeth demanded the same of the Americans. They agreed and she stacked their weapons next to those of the Germans.\n“There was a lot of fear and tension in the cabin as the Germans and Americans eyed each other warily,” wrote historical blogger David Hunt, “but the warmth and smell of roast Hermann and potatoes began to take the edge off.”\nThe Germans produced a bottle of wine and a loaf of bread. While Elisabeth tended to the cooking, one of them, an ex-medical student, examined the wounded American. He explained in English that the cold had prevented infection but he had lost a lot of blood. He needed food and rest.\nBy the time the meal was ready, Hunt reported, the atmosphere was more relaxed.\nTwo of the Germans—Heinz and Willi—were just 16; the corporal was 23.\n“Then Mother said grace,” Fritz would later recall. “I noticed that there were tears in her eyes as she said the old, familiar words, ‘Komm, Herr Jesus. Be our guest.’\n“And as I looked around the table, I saw tears, too, in the eyes of the battle-weary soldiers, boys again, some from America, some from Germany, all far from home.”\nLater, Elisabeth read from the Bible and, said Fritz, she “declared that there would be at least one night of peace in this war.”The truce did, indeed, last through the night and into the morning. Referencing the Americans’ map, the corporal told them the best way to get back to their lines and gave Ralph a compass. When asked if they should instead go to nearby Monschau, the corporal shook his head and said it was now held by the Germans.\nThe enemies then shook hands, took up their weapons, and left in opposite directions, back to their respective sides and the battle.\nFritz and his parents survived the war. Elisabeth and Hubert Vincken died in the early-1960s. Fritz had married in 1958 and was living in Hawaii, where he operated Fritz’s European Bakery in a Honolulu neighbourhood.\nFor years, he tried to track down the German and American soldiers without luck.\nU.S. President Ronald Reagan referenced the improbable story in a May 6, 1985, speech after an emotional visit to the war cemetery at Bitburg, Germany. It was 40 years since the war had ended and six years before the fall of the Berlin Wall.\n“That night, as the storm of war tossed the world, they had their own private armistice,” said Reagan. “Those boys reconciled briefly in the midst of war. Surely, we allies in peacetime should honor the reconciliation of the last 40 years.\n“The one lesson of World War II, the one lesson of Nazism, is that freedom must always be stronger than totalitarianism, and that good must always be stronger than evil. The moral measure of our two nations will be found in the resolve we show to preserve liberty, to protect life and to honor and cherish all God’s children.”In 1995, the television program “Unsolved Mysteries” broadcast the story and it emerged that a man living in a Frederick, Maryland, nursing home had been telling the same yarn for years.\nFritz flew to Frederick in January 1996 and met with Ralph Blank, one of the U.S. soldiers who had been in the Vincken cottage that night so long before. He still had his map and the compass the German corporal had given him.\nBlank told Fritz: “Your mother saved my life.” Fritz said the reunion was the high point of his life.\nFritz Vincken later found another of the Americans, but none of the Germans. He died on Dec. 8, 2002, forever grateful that his mother had been given the recognition she so richly deserved.“Many years have gone since that bloodiest of all wars, but the memories of that night in the Ardennes never left me,” he said in a February 1997 interview with a local high school senior. “The inner strength of a single woman who, by her wits and intuition, prevented potential bloodshed, taught me the practical meaning of the words ‘good will toward mankind.’\n“Now and then, on a clear tropical winter night, I look at the skies for bright Sirius and we always seem to greet each other like old friends. Then, unfailingly, I remember mother and those seven young soldiers, who met as enemies and parted as friends, right in the middle of the Battle of the Bulge.”', ""Concept: Coral reef\nTropical corals live close to their upper thermal limit making them vulnerable to unusually warm summer sea temperatures. The resulting thermal stress can lead to breakdown of the coral-algal symbiosis, essential for the functioning of reefs, and cause coral bleaching. Mass coral bleaching is a modern phenomenon associated with increases in reef temperatures due to recent global warming. Widespread bleaching has typically occurred during El Niño events. We examine the historical level of stress for 100 coral reef locations with robust bleaching histories. The level of thermal stress (based on a degree heating month index, DHMI) at these locations during the 2015-2016 El Niño was unprecedented over the period 1871-2017 and exceeded that of the strong 1997-1998 El Niño. The DHMI was also 5 times the level of thermal stress associated with the ‘pre-industrial’, 1877-1878, El Niño. Coral reefs have, therefore, already shown their vulnerability to the modest (~0.92 °C) global warming that has occurred to date. Estimates of future levels of thermal stress suggest that even the optimistic 1.5 °C Paris Agreement target is insufficient to prevent more frequent mass bleaching events for the world’s reefs. Effectively, reefs of the future will not be the same as those of the past.\nGlobal warming is rapidly emerging as a universal threat to ecological integrity and function, highlighting the urgent need for a better understanding of the impact of heat exposure on the resilience of ecosystems and the people who depend on them 1 . Here we show that in the aftermath of the record-breaking marine heatwave on the Great Barrier Reef in 2016 2 , corals began to die immediately on reefs where the accumulated heat exposure exceeded a critical threshold of degree heating weeks, which was 3-4 °C-weeks. After eight months, an exposure of 6 °C-weeks or more drove an unprecedented, regional-scale shift in the composition of coral assemblages, reflecting markedly divergent responses to heat stress by different taxa. Fast-growing staghorn and tabular corals suffered a catastrophic die-off, transforming the three-dimensionality and ecological functioning of 29% of the 3,863 reefs comprising the world’s largest coral reef system. Our study bridges the gap between the theory and practice of assessing the risk of ecosystem collapse, under the emerging framework for the International Union for Conservation of Nature (IUCN) Red List of Ecosystems 3 , by rigorously defining both the initial and collapsed states, identifying the major driver of change, and establishing quantitative collapse thresholds. The increasing prevalence of post-bleaching mass mortality of corals represents a radical shift in the disturbance regimes of tropical reefs, both adding to and far exceeding the influence of recurrent cyclones and other local pulse events, presenting a fundamental challenge to the long-term future of these iconic ecosystems.\nAcute catastrophic events can cause significant damage to marine environments in a short time period and may have devastating long-term impacts. In April 2010 the BP-operated Deepwater Horizon (DWH) offshore oil rig exploded, releasing an estimated 760 million liters of crude oil into the Gulf of Mexico. This study examines the potential effects of oil spill exposure on coral larvae of the Florida Keys. Larvae of the brooding coral, Porites astreoides, and the broadcast spawning coral, Montastraea faveolata, were exposed to multiple concentrations of BP Horizon source oil (crude, weathered and WAF), oil in combination with the dispersant Corexit® 9500 (CEWAF), and dispersant alone, and analyzed for behavior, settlement, and survival. Settlement and survival of P. astreoides and M. faveolata larvae decreased with increasing concentrations of WAF, CEWAF and Corexit® 9500, however the degree of the response varied by species and solution. P. astreoides larvae experienced decreased settlement and survival following exposure to 0.62 ppm source oil, while M. faveolata larvae were negatively impacted by 0.65, 1.34 and 1.5 ppm, suggesting that P. astreoides larvae may be more tolerant to WAF exposure than M. faveolata larvae. Exposure to medium and high concentrations of CEWAF (4.28/18.56 and 30.99/35.76 ppm) and dispersant Corexit® 9500 (50 and 100 ppm), significantly decreased larval settlement and survival for both species. Furthermore, exposure to Corexit® 9500 resulted in settlement failure and complete larval mortality after exposure to 50 and 100 ppm for M. faveolata and 100 ppm for P. astreoides. These results indicate that exposure of coral larvae to oil spill related contaminants, particularly the dispersant Corexit® 9500, has the potential to negatively impact coral settlement and survival, thereby affecting the resilience and recovery of coral reefs following exposure to oil and dispersants.\nHerbivory is widely accepted as a vital function on coral reefs. To date, the majority of studies examining herbivory in coral reef environments have focused on the roles of fishes and/or urchins, with relatively few studies considering the potential role of macroherbivores in reef processes. Here, we introduce evidence that highlights the potential role of marine turtles as herbivores on coral reefs. While conducting experimental habitat manipulations to assess the roles of herbivorous reef fishes we observed green turtles (Chelonia mydas) and hawksbill turtles (Eretmochelys imbricata) showing responses that were remarkably similar to those of herbivorous fishes. Reducing the sediment load of the epilithic algal matrix on a coral reef resulted in a forty-fold increase in grazing by green turtles. Hawksbill turtles were also observed to browse transplanted thalli of the macroalga Sargassum swartzii in a coral reef environment. These responses not only show strong parallels to herbivorous reef fishes, but also highlight that marine turtles actively, and intentionally, remove algae from coral reefs. When considering the size and potential historical abundance of marine turtles we suggest that these potentially valuable herbivores may have been lost from many coral reefs before their true importance was understood.\nCalcium carbonate skeletons of scleractinian corals amplify light availability to their algal symbionts by diffuse scattering, optimizing photosynthetic energy acquisition. However, the mechanism of scattering and its role in coral evolution and dissolution of algal symbioses during “bleaching” events are largely unknown. Here we show that differences in skeletal fractal architecture at nano/micro-lengthscales within 96 coral taxa result in an 8-fold variation in light-scattering and considerably alter the algal light environment. We identified a continuum of properties that fall between two extremes: (1) corals with low skeletal fractality that are efficient at transporting and redistributing light throughout the colony with low scatter but are at higher risk of bleaching and (2) corals with high skeletal fractality that are inefficient at transporting and redistributing light with high scatter and are at lower risk of bleaching. While levels of excess light derived from the coral skeleton is similar in both groups, the low-scatter corals have a higher rate of light-amplification increase when symbiont concentration is reduced during bleaching, thus creating a positive feedback-loop between symbiont concentration and light-amplification that exposes the remaining symbionts to increasingly higher light intensities. By placing our findings in an evolutionary framework, in conjunction with a novel empirical index of coral bleaching susceptibility, we find significant correlations between bleaching susceptibility and light-scattering despite rich homoplasy in both characters; suggesting that the cost of enhancing light-amplification to the algae is revealed in decreased resilience of the partnership to stress.\nIncreasingly frequent severe coral bleaching is among the greatest threats to coral reefs posed by climate change. Global climate models (GCMs) project great spatial variation in the timing of annual severe bleaching (ASB) conditions; a point at which reefs are certain to change and recovery will be limited. However, previous model-resolution projections (~1 × 1°) are too coarse to inform conservation planning. To meet the need for higher-resolution projections, we generated statistically downscaled projections (4-km resolution) for all coral reefs; these projections reveal high local-scale variation in ASB. Timing of ASB varies >10 years in 71 of the 87 countries and territories with >500 km(2) of reef area. Emissions scenario RCP4.5 represents lower emissions mid-century than will eventuate if pledges made following the 2015 Paris Climate Change Conference (COP21) become reality. These pledges do little to provide reefs with more time to adapt and acclimate prior to severe bleaching conditions occurring annually. RCP4.5 adds 11 years to the global average ASB timing when compared to RCP8.5; however, >75% of reefs still experience ASB before 2070 under RCP4.5. Coral reef futures clearly vary greatly among and within countries, indicating the projections warrant consideration in most reef areas during conservation and management planning.\nIn the recent discussion how biotic systems may react to ocean acidification caused by the rapid rise in carbon dioxide partial pressure (pCO(2)) in the marine realm, substantial research is devoted to calcifiers such as stony corals. The antagonistic process - biologically induced carbonate dissolution via bioerosion - has largely been neglected. Unlike skeletal growth, we expect bioerosion by chemical means to be facilitated in a high-CO(2) world. This study focuses on one of the most detrimental bioeroders, the sponge Cliona orientalis, which attacks and kills live corals on Australia’s Great Barrier Reef. Experimental exposure to lowered and elevated levels of pCO(2) confirms a significant enforcement of the sponges' bioerosion capacity with increasing pCO(2) under more acidic conditions. Considering the substantial contribution of sponges to carbonate bioerosion, this finding implies that tropical reef ecosystems are facing the combined effects of weakened coral calcification and accelerated bioerosion, resulting in critical pressure on the dynamic balance between biogenic carbonate build-up and degradation.\nBACKGROUND: Knowledge of genetic markers that are correlated to stress tolerance may improve spatial mapping of reef vulnerability and can inform restoration efforts, including the choice of genotypes for breeding and reseeding. In this manuscript we present two methods for screening transcriptome data for candidate genetic markers in two reef building corals, Acropora millepora and Pocillopora damicornis (types alpha and beta). In A. millepora, Single Nucleotide Polymorphisms (SNPs) were pre-selected by targeting genes believed to be involved in the coral thermal stress responses. In P. damicornis (type alpha and beta), SNPs showing varying allele frequencies between two populations from distinct environments were pre-selected. Allele frequencies at nine, five and eight of the pre-selected SNP loci were correlated against gradients of water clarity and temperature in a large number of populations along the Great Barrier Reef. RESULTS: A significant correlation between environmental category and SNP allele frequency was detected in up to 55% of the tested loci, which is an exceptional success rate for these types of tests. In P. damicornis, SNP allele frequencies of beta-hexosaminidase and Elongation factor 1-alpha were significantly correlated to temperature in type alpha and to temperature and/or water clarity respectively in type beta. Type alpha also showed a correlation between water clarity and SNP allele frequency in a gene of unknown function. In A. millepora, allele frequencies at five (beta-gamma crystallin, Galaxin, Ubiquitin, Ligand of Numb X2 and Thioredoxin) SNP loci showed significant correlations. CONCLUSIONS: After validation of these candidate loci through laboratory or field assessment of relative stress tolerance of colonies harbouring different alleles, it is anticipated that a proportion of these markers may represent the first coral candidate Quantitative Trait Loci for environmental stress tolerance and provide an important genetic tool that can be incorporated into spatial management decisions and restoration efforts of coral reefs. One pertinent example would be to combine spatial data of tolerant populations with genetic connectivity and thus identify high priority conservation reefs and implement targeted coral husbandry and active restoration efforts that use locally- and stress-adapted genotypes.\nEyespots on the body of many animals have long been assumed to confer protection against predators, but empirical evidence has recently demonstrated that this may not always be the case and suggested that such markings may also serve other purposes. Clearly, this raises the unresolved question of what functions do these markings have and do they contribute to an individual’s evolutionary fitness in the wild. Here, we examined the occurrence of eyespots on the dorsal fin of a coral reef damselfish (Pomacentrus amboinensis), where these markings are typical of the juvenile stage and fade away as the fish approaches sexual maturation to then disappear completely in the vast majority of, but not all, adult individuals. By exploring differences in body shape among age and gender groups, we found that individuals retaining the eyespot into adulthood are all sexually mature males, suggesting that these eyespots may be an adult deceptive signal. Interestingly, the body shape of these individuals resembled more closely that of immature females than mature dominant males. These results suggest that eyespots have multiple roles and their functional significance changes within the lifetime of an animal from being a juvenile advertisement to a deceptive adult signal. Male removal experiments or colour manipulations may be necessary to establish specific functions.\nCurrent understanding of the factors influencing hawksbill sea turtle (Eretmochelys imbricata) hatch success is disparate and based on relatively short-term studies or limited sample sizes. Because global populations of hawksbills are heavily depleted, evaluating the parameters that impact hatch success is important to their conservation and recovery. Here, we use data collected by the Jumby Bay Hawksbill Project (JBHP) to investigate hatch success. The JBHP implements saturation tagging protocols to study a hawksbill rookery in Antigua, West Indies. Habitat data, which reflect the varied nesting beaches, are collected at egg deposition, and nest contents are exhumed and categorized post-emergence. We analyzed hatch success using mixed-model analyses with explanatory and predictive datasets. We incorporated a random effect for turtle identity and evaluated environmental, temporal and individual-based reproductive variables. Hatch success averaged 78.6% (SD: 21.2%) during the study period. Highly supported models included multiple covariates, including distance to vegetation, deposition date, individual intra-seasonal nest number, clutch size, organic content, and sand grain size. Nests located in open sand were predicted to produce 10.4 more viable hatchlings per clutch than nests located >1.5 m into vegetation. For an individual first nesting in early July, the fourth nest of the season yielded 13.2 more viable hatchlings than the initial clutch. Generalized beach section and inter-annual variation were also supported in our explanatory dataset, suggesting that gaps remain in our understanding of hatch success. Our findings illustrate that evaluating hatch success is a complex process, involving multiple environmental and individual variables. Although distance to vegetation and hatch success were inversely related, vegetation is an important component of hawksbill nesting habitat, and a more complete assessment of the impacts of specific vegetation types on hatch success and hatchling sex ratios is needed. Future research should explore the roles of sand structure, nest moisture, and local weather conditions.""]"	['<urn:uuid:60dc995a-a73a-4532-be53-61c6eab06d8a>', '<urn:uuid:0acd5f8b-2afc-48be-b794-257bf0e8c4b1>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:15:13.904987	33	176	4312
24	Which is safer: cosmetic fillers or Botox injections for facial rejuvenation?	Botox is considered very safe with only mild side effects like discomfort, minor bruising, swelling and headaches. In contrast, cosmetic fillers can have serious complications including tissue death and permanent disfigurement if accidentally injected into bloodstream, affecting up to 800 US patients with serious complications in 2012.	"['It’s rare, but sometimes things go wrong. In a matter of minutes, patients’ skin can turn red or blotchy white and the injected area becomes painful. Vital blood supply to the face is restricted and if untreated, parts of the tissue will die. That scenario is irreversible and can leave deep scars.\nSiavash Yousefi, U of Washington\nThis image shows a mouse ear after a successful cosmetic filler injection. The filler, in green, rests in the tissue without blocking the blood vessels and veins.\nPhysicians haven’t been able to pinpoint why this happens because until now it was difficult to see how the injected fluid, or filler, behaves in facial tissue.\nNew imaging technology from University of Washington engineers allows scientists to analyze what happens within the smallest blood vessels during an injection. This finding could be used to prevent accidents during procedures and help clinicians reverse the ill effects if an injection doesn’t go as planned.\n“Filler-induced tissue death can be a really devastating complication for the patient and provider,” said Shu-Hong (Holly) Chang, a UW assistant professor of ophthalmology specializing in plastic and reconstructive surgery. “This noninvasive imaging technique provides far better detail than I’ve ever seen before and helped us figure out why this is happening.”\nUsing this technology, Chang and her team saw that complications arose when filler was inadvertently injected into the bloodstream rather than in the intended soft tissues of the face. The gel builds up in a vessel, blocking blood movement and oxygen exchange. The team tested this in the ears of mice, which offer a model of what can happen in the blood vessels of a human face, Chang said.\nShe presented the results in November at the annual meeting of the American Society of Ophthalmic Plastic and Reconstructive Surgery showing that filler injections into blood vessels are most likely the cause of tissue death and other complications associated with the cosmetic procedure.\nRuikang Wang, a UW professor of bioengineering, and his lab pioneered this fine-resolution imaging, called optical microangiography. It can turn out 3-D images of the body’s vascular network by shining a light onto the tissue without touching it or adding any fluorescent dyes.\n“We can visualize how blood responds to the cosmetic filler gel, even looking at the responses of each individual vessel. No other technique can provide this level of scrutiny,” Wang said.\nThe optical imaging technique operates on the same concept as ultrasound, which leverages changes in sound to detect structures. This technique instead uses light to repeatedly scan tissue cross-sections, delineating unmoving pieces (surrounding tissues) from moving segments (blood cells in vessels). Researchers compare image frames and piece together the complex visual web.\nThis technology can see blood vessels as small as 5 microns in diameter. Capillaries, the smallest vessels in our bodies, are about 7 microns in diameter and a red blood cell is usually 3 to 5 microns wide.\n“Our niche is imaging the microvascular system,” said team member Siavash Yousefi, a UW graduate student in bioengineering. Other applications of the technology include analyzing how wounds heal, tracking what happens during strokes and traumatic brain injuries, and imaging human eyes to study diseases such as glaucoma and macular degeneration.\nCosmetic filler procedures have surged worldwide in recent years, particularly in Europe and Asia. In 2012 in the U.S. about 2 million procedures were performed. Up to 800 patients reportedly suffered serious complications, including potentially permanent disfigurement.\nDuring the procedure, a practitioner injects the gel-like solution, often a natural substance called hyaluronic acid, multiple times into a person’s face. Restriction of blood to the tissues, called ischemia, often doesn’t show up until later, when the patient develops pain and sees changes on the surface of the skin, meaning the tissue is dying.\nSome practitioners suggest using massage and warm compresses to treat the area, while others tell patients to take aspirin, but the field doesn’t have a standard course of action for treating these complications, Chang said. She has been called in for several emergencies to treat other practitioners’ patients who show signs of a failed procedure. This can lead to tissue death and even blindness if the affected area is near the eyes.\nWith this new understanding, practitioners can try to reverse the effect of vascular blockage by injecting an enzyme that dissolves hyaluronic acid fillers. The research team is now testing all types of available cosmetic fillers to see if their results hold on each brand and evaluating new treatments for reversing procedure complications.\n“Our lab is trying to develop novel and clinically useful biomedical imaging techniques for early diagnosis, treatment and management of human diseases. Using this technology to better understand facelift complications is a perfect example that fulfills this mission,” Wang said.\nThe research was funded in part by the organization Research to Prevent Blindness and a Latham Vision Research Innovation Award.\nFor more information, contact Yousefi at firstname.lastname@example.org or 541-602-9592; Chang at email@example.com or 206-897-4611; and Wang at firstname.lastname@example.org or 206-616-5025.\nMichelle Ma | EurekAlert!\nResearchers image atomic structure of important immune regulator\n11.12.2018 | Brigham and Women\'s Hospital\nPotential seen for tailoring treatment for acute myeloid leukemia\n10.12.2018 | University of Washington Health Sciences/UW Medicine\nOver the last decade, there has been much excitement about the discovery, recognised by the Nobel Prize in Physics only two years ago, that there are two types...\nWhat if a sensor sensing a thing could be part of the thing itself? Rice University engineers believe they have a two-dimensional solution to do just that.\nRice engineers led by materials scientists Pulickel Ajayan and Jun Lou have developed a method to make atom-flat sensors that seamlessly integrate with devices...\nScientists at the University of Stuttgart and the Karlsruhe Institute of Technology (KIT) succeed in important further development on the way to quantum Computers.\nQuantum computers one day should be able to solve certain computing problems much faster than a classical computer. One of the most promising approaches is...\nNew Project SNAPSTER: Novel luminescent materials by encapsulating phosphorescent metal clusters with organic liquid crystals\nNowadays energy conversion in lighting and optoelectronic devices requires the use of rare earth oxides.\nScientists have discovered the first synthetic material that becomes thicker - at the molecular level - as it is stretched.\nResearchers led by Dr Devesh Mistry from the University of Leeds discovered a new non-porous material that has unique and inherent ""auxetic"" stretching...\n10.12.2018 | Event News\n06.12.2018 | Event News\n03.12.2018 | Event News\n11.12.2018 | Physics and Astronomy\n11.12.2018 | Materials Sciences\n11.12.2018 | Information Technology', 'Botox Injections Procedures, Risks and Alternatives\nBotox is becoming an increasingly popular way for women (and men) to rejuvenate their skin and look younger. But what exactly is botox, and how does it work? Moreover, are there any risks involved with the procedure? This article answers all of these questions and more.\nWhat is Botox?\nThe full, scientific name for botox is Botulinum Toxin. It is produced by a bacterium and was discovered over one hundred years ago; however, it was only in the last few decades that botox was seen to have a practical use in treating muscle spasms.\nNowadays, botox is used to treat a range of ailments, from lazy eye and uncontrolled blinking to back and neck pain. But botox is most widely used by the cosmetic industry as a non-surgical means of smoothing out lines and facial creases. By blocking certain nerve impulses, botox stops the muscle contractions that lead to wrinkles.\nWhat is the procedure?\nThe botox application is relatively simple, and does not require an anesthetic. A fine needle injects the botox into the skin in the desired area, and the full procedure takes only a few minutes. In order to minimize the risk of bruising, you are advised to avoid alcohol and anti-inflammatory medication for two weeks prior to the procedure. The beneficial effects of botox usually last between three to four months.\nWhat are the risks and side effects?\nMedical authorities consider botox to be a very safe procedure. Most people only experience mild discomfort when botox is applied, but some patients experience minor bruising, swelling, and headaches.\nIf too much botox is applied, it can cause a “frozen look” whereby too many of the face’s muscles have been immobilized. It is important patients seek a qualified plastic surgeon or medical professional and communicate the look they are trying to achieve to make sure the Botox is applied in a way to achieve this.\nHow much does it cost?\nBotox is usually priced at around $10 to $18 “per unit” – The cost varies with whether a board certified plastic surgeon or dermatologist is applying it, or a nurse or other medical technician in a spa. Beware of deals that are too cheap since they may not be using Botox from an FDA approved supplier. Different areas require different amounts to treat. For example for the horizontal lines of the forehead – this requires anywhere from 12-20 units to treat. The “11” is anywhere from 10-14 units. The amount varies with each person’s anatomy.\nAre there any natural alternative treatments?\nBecause of the well-publicized success of botox in making people appear younger, a number of natural treatments have come onto the market. These claim to achieve the same result, but without the need for needles, and at a reduced cost.\nAmong the non-injectable products available are Frownies, serums, washes and moisturizers and anti-oxidants. Nothing is as effective as Botox which acts at the neuromuscular junction to prevent muscle contractions that create wrinkles but a good skincare regimen of cleansers and moisturizers is important for overall skin maintenance.']"	['<urn:uuid:b3bf293b-c6af-475f-87b2-37785129c0b0>', '<urn:uuid:7ba22dc9-c4cb-4907-80fc-c886eb0b6e79>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	11	47	1607
25	How does the rapid growth of cities affect food safety and the environment, particularly when it comes to food processing and markets?	The rapid expansion of cities can lead to several negative effects. These include contamination of soil and water which creates food safety problems. It also causes deterioration of the areas around processing facilities and market places. Additionally, it leads to degradation of living conditions through noise and air pollution. Because of this, any efforts to improve food supply efficiency must be made together with plans for waste management.	['Seminar participants acknowledged that rapid urban growth in most Asian cities is being accompanied by an increase in both the number and the proportion of poor households living close to or below the poverty line. Participants recognised that food insecurity is increasingly becoming an urban problem and that specific policies and programmes should be undertaken to improve the food security and nutritional well-being of urban populations, and particularly the poor. The establishment of an efficient food supply and distribution system (FSDS) that provides an adequate supply of good quality and safe foods, is affordable and accessible to all urban consumers and reduces or eliminates food-related health and environmental problems was considered key to improving food security in Asian cities. An adequate FSDS requires sound policies and strategies and development programmes spanning urban, periurban and rural sectors. These policies and programmes should be formulated in close collaboration with all concerned public and private stakeholders.\nAn adequate supply of good quality, safe foods must be made available and affordable to all urban consumers, including the poor and disadvantaged. The food supply to urban populations must be adequate in quantity, quality and variety to meet nutritional needs.\nWork toward building awareness of the importance of FSD issues was considered essential among all levels of government and stakeholders. Information, sensitization and training play a significant role in furthering the ability to feed Asian cities.\nSpecific, relevant information on all aspects of providing an adequate food supply to urban populations is seriously lacking. Such information is fundamental for sound policy and planning development and for effective programme design and implementation. In addition, there is insufficient dissemination and exchange of existing information and knowledge among key players in the food supply system and the broad public. Wide access to complete and consistent information will greatly enhance and facilitate efforts to improve urban food security.\nInformation is particularly lacking in the following areas:\nEffective coordination for timely action and effective planning is significantly hampered by the involvement of many departments and various levels of government in many issues related to production, transport, processing, storage and distribution.\nEnvironment and food safety issues throughout the food production to consumption chain require adequate attention so as to ensure an appropriate level of consumer health and environmental protection. Emphasis should be placed on preventative food safety management and the provision of training on issues relevant to food safety. Appropriate food quality and safety standards, codes of practice and other guidelines should be established to assist industry to achieve food safety goals. The required technical and administrative food control infrastructure should be developed to enable authorities to ensure compliance with regulations by industry.\nWaste management and related environmental impacts are key issues for food production, processing and distribution. Negative effects of the rapid expansion of cities can include contamination of soil and water leading to food safety problems. It can also lead to deterioration of the surroundings of processing facilities and market places as well as degradation of living circumstances from noise and air pollution. Any attempt to improve the efficiency of the food supply should be made simultaneously with plans for waste management.\nImprovement of infrastructure is needed for transportation, water supply, storage, processing and marketing facilities to enhance the efficiency of FSD activities and to address ongoing environmental degradation.\n1. Stakeholder participation and coordination\n2. Information and awareness\nGiven the lack of sufficient information on all aspects of ensuring a safe and adequate food supply to urban populations, it was recommended that:\n3. Food security and nutritional well-being among the urban poor\nGiven that the urban poor are particularly vulnerable to food insecurity and malnutrition, it was recommended that:\n4. Information, sensitization, training and capacity building\nThe seminar recognised the needs of senior policy makers for information and sensitization and the needs of technical and managerial staff of city and local authorities (CLAs) for training and capacity building, particularly in the following areas:\nInformation and sensitization:\nTraining and capacity building:\nCLAs and central government authorities should collaborate in ensuring capacity building in the following areas:\n5. Physical infrastructure\nRoad improvement should be conducted for enhancing production, processing and distribution of food commodities;\n6. Waste management and environmental protection\nCLAs are usually responsible for the proper management of liquid and solid waste from food market and slaughterhouses. Such waste can be used for agricultural production and livestock feeding provided adequate care is taken in avoiding health and environmental negative implications. Central governments are responsible for ensuring that appropriate regulations are adopted and adequate capacity developed to ensure their enforcement. Awareness of the issue is necessary for all the sectors.\n7. Legal issues\nCLAs and central governments should legally recognise the informal sector as a first step to addressing existing problems and opportunities presented by the sector;\n8. Enhancing private investment\n9. North-South and South-South partnerships\nCLAs in different countries have expertise and experience in many aspects of FSDS that can prove valuable to other cities. These should be shared through effective partnerships based on a proper understanding of local conditions, attitudes and requirements in a spirit of exchange rather than dependence. Various programmes by international and regional organizations such as CityNet, FAO and the European Commission exist to promote technical assistance partnerships and projects.']	['<urn:uuid:db792308-b106-4a0c-80b5-4c81d43ad13f>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	22	68	873
26	How does Centre Pompidou compare to Atelier des Lumières in size?	The Centre Pompidou has multiple large spaces totaling over 4,000 square meters, including a 2,000 square meter Forum, the Georges restaurant with 600 square meters plus terrace, and other various spaces. In comparison, Atelier des Lumières in Paris has a 3,300 square meter exhibition space which is used for immersive digital art projections using 140 laser video projectors.	"['Digital art and curated art that is presented in digital media are nothing new. However, in recent times, both have been gaining more and more attention both within the art world and among the general public. A big driver behind this has been the socially distanced world that became such an abrupt reality early in 2020. However, blockbuster immersive digital exhibitions were already making headlines well before the world knew about Covid-19.\nSo, what is going on? Has the pandemic simply sped up the uptake of digital media to exhibit art or was the world already heading inexorably in this direction already? Naturally, different people will take differing views of this depending on their standpoint. Consequently, it will be worth looking back at some of the digital exhibitions which have been put on in recent years to see whether there are some discernible trends. Digital are shows have been around for some time but their immersive nature is what seems to be most striking of all given where the world is today as it attempts to vaccinate itself back to normality.\nThe current trend in digital art exhibitions is for more and more immersion into the art. Digital technologies can certainly offer this more experiential form of art consumption. Is the public no longer simply happy to view images or art? Have tastes changed to such a degree that art exhibitions of the future will need to put on more of a show? Of course, no one can say for sure what the post-pandemic art world will look like. That said, there are some tantalising insights that can be gleaned from the immersive digital art boom we have witnessed in the last year or so. Before looking a little closer at some of those, however, it will be worth turning the clock back a little further.\nCurated for an initial show at the Barbican Arts Centre in the City of London in 2014, Digital Revolution offered a glimpse of what immersive digital art might look like. Before moving on to Sweden, Greece, Turkey, China and Germany, this exhibition put digital technology at its core.\nBringing together digital art from as far back as the 1970s with architectural design, film making, video game graphics, digitised music and even robotic dance, the show was seen as a landmark when it first launched. Exploring the then relatively nascent world of augmented reality along with other interactive elements, such as an immersive laser show, the exhibition was unequivocal about its subject matter – art in a digital world. Although some aspects of the show have necessarily aged as digital technology has progressed, it was the immersive nature of parts of the exhibition that set the standard for what was to follow even though many of the current crop of immersive digital exhibitions have a more backwards-looking curation style.\nAtelier des Lumières\nA prime example of the sort of blockbuster show that is showing just how popular digital exhibitions can be is Atelier des Lumières. Launched in 2018, and currently closed due to social distancing rules in the French capital, this exhibition is located close to the Bastille in a former Parisian foundry. Operated by Culturespaces, the initial digital show featured the artwork of Egon Schiele and Friedrich Stowasser, better known as Hundertwasser, as well as numerous artists of the Viennese school, notably Gustav Klimt.\nMaking use of state-of-the-art visual projections as well as audio, the various artists’ works were truly transformed into digital representations. Not only were the paintings reproduced in digital forms but they were projected at a much larger scale. Even walls that were ten metres high or more got the cinematic treatment by the use of no fewer than 140 laser video projector. Overall, the exhibition space comprised an enormous 3,300 square metre surface area in which the humans viewing the spectacle were inevitably dwarfed by the images. What’s more, the immersive digital show could be changed relatively easily. As well as devoting some of the smaller gallery spaces to emerging artists, Atelier des Lumières was able to show other famous images without needing to borrow them from their owners. For example, Van Gogh’s Starry Night was introduced into the exhibition at one point. It garnered attention well outside of the art world when it featured in a Netflix TV series called Emily in Paris.\nPerhaps because of the notoriety that Atelier des Lumières gained across the world for its creation of a truly digital space in which it was possible to roam freely, the Indianapolis Museum of Art – now known as the Newfields Museum – took the decision to radically alter an entire floor of the gallery.\nThe art museum’s director, Jonathan Berger, explained that over 3,000 square metres of the institution on its fourth floor would be devoted to a virtual space where immersive digital art shows could be installed. “The public is introduced to and engages with art differently now,” he said. Berger went on to add that the, as yet, unopened gallery space would be the first of its kind in the US. He also said that, as a cultural institution, the Newfields Museum has to reflect the changing nature of the way people interact with art these days. “Even though society has changed, [many]… museums have not,” he said.\nBassins de Lumières\nCulturespaces work in Paris is not a one off. As the developments in the US show, digital exhibitions are likely to be popular when attendance of galleries becomes possible once more.\nThat said, online versions of these shows can still be enjoyed despite the lockdown rules. This is the case with Bassins de Lumières which replicates much of the work Culturespaces did at Atelier des Lumières. This time, however, the exhibition space is a former submarine base and the artistic projections are even more monumental among the brutal architecture. Culturespaces’ president, Bruno Monnier, said that replicating and adjusting digital exhibitions for different spaces took skill but since they worked with only a digital projection, not the real art itself, there were virtual no transportation, security or insurance costs to speak of.\nImmersive Van Gogh\nThere have been a number of digitised shows featuring the work of one of the world’s most famous artists. The Van Gogh Museum, in Amsterdam, launched an an official Meet Vincent Van Gogh Experience back in 2016 when it started to use large-scale digital projections of the Dutchman’s art in a pop-up exhibition in Beijing as far back as 2016. This ‘experience’ as it was billed went on to be shown all over Asia, North America and Europe. Put together by Event Communications, a London-based museum design consultancy, the show would go on to win a Themed Entertainment Award in 2017 for the best touring immersive museum exhibit. When the show came to London, it was marketed as the Van Gogh Immersive Experience, a telling sign that the immersive nature of the project had become one of its signature selling points. Making use of 360-degree projections and elements of virtual reality, the show featured projections and sound to augment the immersive nature of the experience.\nAlthough it was panned in some areas of the British press when it first took up its residency on the South Bank of the Thames, close to the Tate Modern gallery, the exhibition was popular with members of the public until lockdown rules prevented it from reopening. Of course, it may well be that the public’s appetite for Van Gogh and large digital projections of his art is unabated and the show will continue to attract large numbers of visitors when it reopens later in the year. Certainly, this is the hope in San Francisco because a pop-up version of the show has been admitting visitors there since mid-March.\nFinally, although digital immersive art shows may simply be a recent fad that has become trendier simply because of the pandemic, it should be noted that art collectives like teamLab have been serving up this sort of thing for two decades. Referring to themselves as ultra-technologists, the artists in the collective have been curating and displaying their digital works in Tokyo since 2001. The collective launched a second digital museum in Shanghai a few years ago and has staged temporary exhibitions in Finland before.\nIn 2018, when teamLab first opened its doors to the public in its own, eponymously titled exhibition space, it succeeded in attracting 2.3 million visitors in the first year alone. That astonishing figure may belie the fact that the appetite for immersive digital experiences in art is far from a passing trend. After all, it is the largest attendance statistic for a single-artist institution anywhere in the world. In 2021, the collective announced it had teamed up with the Chinese social media platform, TikTok, to produce a new virtual immersive art experience. As such, few people would bet against it coming up with another hit digital show, even if it happens to be an entirely virtual one.\nAbout the author – Manuel Charr\nManuel Charr is a journalist working in the arts and cultural sectors. With a background in marketing, Manuel is drawn to arts organizations which are prepared to try inventive ways to reach new audiences.', ""Private events and guided tours\nWith a building that is considered an emblem of 20th-century architecture, an incomparable collection of more than 100,000 artworks and a hugely varied programme of events, the Centre Pompidou is one of the two major museums of modern and contemporary art in the world.\nIts strong brand image, large capacity and diversity of spaces in a range of sizes makes it the venue for all kinds of event: product launches, fashion shows, drinks receptions, awards ceremonies, ceremonial addresses, conferences, dinners, seminars, press conferences, marketing presentations and more.\nThe upper piazza (or “Triangle”, as it’s known) offers a street-level public space that is unique in central Paris, while the upper floors of the building offer magnificent and unforgettable views over the city. With a range of spaces of different kinds, moods and sizes, the Centre Pompidou offers a variety of space-hire solutions for every kind of event. Space hire can be combined with access to the museum or exhibitions, with or without guided tour.\nGeorges: 600 m2 + 270 m² terrace\nEspace U: 580 m2 + 250 m² terrace\nAccommodates up to 1,500 people\nDesigned by the well-known Jakob + MacFarlane agency, the Georges restaurant, with its panoramic terrace, offers a spectacular view of Paris.\nThe Georges is ideal for drinks receptions, dinners and dances. Espace U, which is adjacent to the restaurant, overlooks Galeries 1 and 2, where the Centre Pompidou's largest exhibitions are staged. Level 6 opens out on to the structure of the Centre Pompidou and beyond, to the city, and can accommodate up to 1,500 people for a cocktail-style reception, plus a private tour.\n80 m² + 580 m² terrace\nincluding 250 m² of pool area\nAccommodates up to 100 people\nA true haven in the heart of one of the largest collections of modern art in the world, the Salon du Musée is available to hire for meetings and receptions, followed by private tours.\nThis space, which runs to approx. 80 m², can accommodate around 50 people for a sit-down meal and up to 100 people for a drinks reception. The contemporary lines of the space open out on to a vast terrace, which, with its reflective pool, overlooks the Parisian skyline.\n2 000 m2\nAccommodates up to 2,000 people\nThe 2,000 m² Forum is the Centre Pompidou’s entrance hall and, with its large windows, is considered the true heart of the building.\nA symbol of the Centre Pompidou since its inauguration in 1977, it embodies its identity and modernity.\nIts height and scale mean it is totally flexible and suited to all sorts of events. Available to hire on Tuesdays.\nCentral: 280 m2\nMezzanine: 500 m²\nAccommodates up to 250 people or 500, with the mezzanine on\nDesigned as an “oasis” overlooking the entrance hall of the Centre Pompidou, Café Le Central is the work of the Spanish artist-designer Jaime Hayon and officially opened in 2019. It is based around geometrically-shaped pavilions made from transparent, multicoloured glass, which allow for both privacy and conviviality.\nIt is available to hire every day from 5.00 pm and all day on Tuesdays.\nOrganise memorable events in the Foyer, level −1, 800 m2 of space comprising the Galerie de Photographies, Cinema 2, the Petite Salle and the Grande Salle.\nOur performance venues and adjoining screening rooms are perfect for hosting your welcoming coffee or drinks receptions, conferences and business meetings.\n250 m2 of available space\n+ 130 m terrace\nAccommodates up to 100 people\nThe timeless setting of Brancusi’s Studio offers a private space filled with light streaming in from the roof windows and its own peaceful garden.\nSculptor Constantin Brancusi’s studio was reconstructed by the architect Renzo Piano and is located on the corner of the Piazza, in the heart of the Marais.\nPerfect for breakfast meetings and drinks receptions for around a hundred people, or sit-down meals in an elegant atmosphere, Brancusi’s Studio is available to hire on weekdays and all day on Tuesdays.\nStreet level, outside\nAdjoining the Rue Saint-Merri, one of the busiest streets in Paris, which links the Marais with the Forum des Halles, the “Triangle” is the 330 square metre area in front of the Centre that stands above the south end of the sloping Piazza.\nLevel with the street and entirely open to the public, it is the perfect spot for product launches, promotional campaigns or direct marketing.\nOn Saint-Merri Street: 250 m2\nOn Saint-Merri Church: 316 m2\nPlace Stravinsky is located close to Ircam and the Centre Pompidou. The Fountain – the result of a collaboration between Jean Tinguely and Niki de Saint Phalle – is 17 metres wide by 33 metres long. Within the 580 m2 basin are 16 sculptures inspired by the works of the composer Igor Stravinsky.\nThe spaces on both sides of the fountain can be privately hired, so they are ideal for public events or film shoots.\nIn 2009, the Centre Pompidou unveiled a new Atelier des Enfants aimed at a young audience and families.\nIn a completely white space, conceived by the mind of designer Mathieu Lehanneur, the three studios offer the flexibility for organising all types of private event: product launches, drinks receptions, birthday parties, ceremonial addresses, fashion shows, private parties, etc.\nIn 2010, the Centre Pompidou became the first major museum in the world to create a space especially for adolescents. The unique Studio 13/16 is a generous-sized space of 250 m2 under a double-height, 4.8 m ceiling. Its location on Forum -1 means it is closely associated with the live-show and cinema programmes. In the capable hands of designer Mathieu Lehanneur it has been arranged in a flexible way, in the form of workshops.\nStudio 13/16 is available to hire for private parties or business events and offers a unique and compelling atmosphere, located as it is at the heart of the Centre Pompidou, within a unique design space.""]"	['<urn:uuid:bf555c86-9eb5-4581-a0e1-f66bfd251722>', '<urn:uuid:4e192f36-9349-4627-8256-042c24e7b329>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	11	58	2518
27	I've inherited some old bottles of wine and want to open them properly - what specialized equipment do experts recommend for opening aged wine?	For wines older than 10 years, experts recommend using an Ah-So style cork puller, which has two prongs designed to remove delicate older corks without destroying them. While basic versions are available for $10, the gold standard is the Durand model which retails for around $145. You'll also need a special decanter for aged wines to carefully separate the wine from any sediment that has formed.	"['Professional Tips to Consider Before You Buy Aged Wine or Wine for Cellaring\nIn Part III of this guide:\nOther parts of this guide:\nIf you’re like most Americans, you probably don’t have a proper wine cellar. And unless you’re lucky enough to live in a climate with low summer temperatures and consistent underground temperatures, you’ll need this kind of storage to properly age wine.\nAlternatively, “wine fridges” are expensive, noisy, and bulky. A lot of folks, particularly who live in cities where real estate is at a premium and residences are small, don’t have a wine refrigerator either. If you don’t have a cellar or a good-quality wine refrigerator, you probably should not be aging your own wine.\nAdditionally, how many of us have the patience to age our own wine? My husband does, but I’d bet left to my own devices I’d drink much of our wine before it reaches peak maturity, simply because it’s too tempting not to.\nIf you lack a cellar and patience, I have great news: you can explore aged wine without a wine cellar or waiting years to try it!\nThe trick is to know where to buy wine that has already been aged. This article, Part IV of our Aged Wine Guide, will walk you through what you need to know before you buy aged wine and where to find perfectly-cellared age-worthy bottles.\nProvenance is the history of who has owned a bottle, where it was cellared, and under what conditions. Like fine art, the provenance of an aged fine wine is a must-know before you buy.\nFor this reason, I recommend only buying aged wine that has always been in the possession and control of the winery who produced it, specialty retailers with specialized transportation and storage equipment, and auction houses who will certify the wine they’re selling has been properly stored (sometimes because they’ve had the wine in their own storage facility since it left the winery).\nThe last part of the provenance is how the wine will be transported to you after you purchase it. Depending on its age and the distance it must travel, this can be a critical component of your enjoyment of the wine. Very old wine is delicate and must be handled carefully, ideally without much vibration or any exposure to light.\nYou may be wondering, why is aged wine so expensive? There are a few very good reasons why aged wine will set you back significantly more dough than buying a new release from a winery or wine shop: it started out expensive, cellaring costs money, scarcity pricing, provenance, and investment-grade wine profit.\nReason #1: Most wine worth aging started out expensive. Think classified Bordeaux and Burgundy, Napa Cabernet Sauvignon, and Sauternes (dessert wine from Bordeaux).\nReason #2: Wine that’s sitting around developing in a cellar has a carrying cost. The electricity to maintain the correct temperature and humidity in a cellar isn’t free. Neither is the real estate that wine occupies inside the cellar. Aged wine costs typically reflect these expenses.\nReason #3: Scarcity pricing. You often see the phrase “fine and rare wine” in the wine world. While these two things go hand in hand, rare wine starts to get pricey merely because there’s very little of it to be had. Michael Peltier, the Senior Fine Wine Specialist at Millesima, offered this perspective. Rareness is sometimes about production. A great example is Salon Champagne that only bottles wine in special vintages. The 2008 vintage was only bottled in magnum and only 2000 bottles were produced. Rareness can also be about general scarcity (particularly with aged wine) like a 1945 Mouton Rothschild.\nReason #4: Provenance. If you are looking at an aged rare wine to purchase, a wine with a documented provenance among professional sellers is worth much more than the same wine that your Dad’s friend had sitting in his basement.\nReason #5: Collectors. Some wines are never drunk, they’re just traded back and forth among collectors who treat them as investments.\nWhen it comes to aged wine, the standard equipment just won’t do. There are specialized cork pullers for delicate older corks and specialized decanters for delicate older wines. Here are the best ones to get according to most industry professionals. For more details about this process, read Vinous Media’s article on how to open and decant aged wine.\nSpecialized Wine Opener for Aged WineBuy it for $145.00\nThere is a style of cork puller (corkscrew) called an Ah-So. This is a two-pronged corkscrew with a wide handle designed specifically to remove a cork from an older bottle of wine without destroying the cork. A general recommendation is that any wine older than 10 years should be opened with this style of cork remover because corks degrade as the wine seeps through them during cellaring.\nWine Access has a video of how to use an Ah-So here and Jordan Winery also has a nice video of how and why to use it.\nLike most wine accoutrement, you typically get what you pay for. You can find an Ah-So style cork puller for as little as $10 on Amazon, you can go for the gold standard option made by Durand (retail is $150 at Durand.com or $145 on Amazon), or you can pick your favorite Ah-So at any price.\nWhen a wine is younger, we decant it to open it up and help it breathe. What does this mean exactly? Well, wine is a delicate balance of chemistry, and it is enhanced by the process of aeration. Aeration softens tannins and releases certain gases that develop in the bottle while introducing oxygen.\nThere are plenty of great resources online for the rules of when and when not to decant, but in general it’s a good idea to decant an aged wine right before service. With an aged wine, you don’t really want to introduce oxygen, or even jostle it around, because it is even more delicate. You still need to decant it though, because over time sediment forms in the bottle and sediment is unpleasant in your mouth. This helps you keep the sediment in the bottle and not in your wine glass.\nI think Jancis Robinson describes it best in her video about decanting wine. You can see the video and buy her Mature Wine Decanter at Wine.com. Here are two more affordable options for your consideration, as well.\nMessage: Undefined offset: 1287\nLine Number: 145"" class=""button green listicon cart is-size-5"" target=""_blank"" rel=""nofollow noopener"">Buy it for $\nElegant Aged Wine DecanterBuy it for $59.95\nFine Wine Decanter for Aged WineBuy it for $199.97']"	['<urn:uuid:afe96141-e00a-464c-ac28-4ecf4ea6a3ae>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	24	66	1102
28	professional cymbal maker differences stagg dream paiste manufacturing techniques quality price range	Different cymbal makers have distinct manufacturing approaches and price points. Stagg, operating since 2001, offers B20 bronze hand-hammered cymbals at surprisingly low prices, though with slightly shorter sustain. Dream produces professional-quality cymbals in China's Wuhan region, using B23 bronze for a mellower, spicier sound. Paiste manufactures their PST8s using their proprietary CuSn8 bronze, incorporating features from their higher-spec ranges. Each manufacturer's techniques and materials result in unique sound characteristics at different price points.	"['From pre-configured hi-hat/crash/ride cymbal packs designed to help beginner drummers get on the ladder, to hand-crafted, complex-sounding individual metals using the finest alloys, there has never been a better choice of cymbal companies, cymbals types and sounds available.\nAs always, the more cash you spend, the more you will be able to unlock quality craftsmanship, premium materials, and more pleasing cymbal sounds. However, a small budget doesn’t need to mean terrible sounding cymbals and cymbal companies are getting much better at extracting decent tones out of ‘inferior’ alloys.\nEach cymbal will have a slightly different tonal character to the next one, so we would always recommend trying before you buy. And if you’re replacing individual cymbals within your set-up, you should always aim to complement your overall sound with any new additions, rather than choosing a model that sticks out unnaturally.\nWe’ve tested a huge range of cymbals to suit all musical situations so, whether you’re looking for the best cymbals for rock, funk, metal, jazz, or any other style, we have you covered. In fact, your next cymbal is almost definitely on this list.\nNewcomers to drumming can be caught unawares when they budget for their first kit, not realising that drum manufacturers don\'t make cymbals and that the cheap brass cymbals thrown in with their starter kit are hideous duffers. Cheap cymbals just sound nasty. The SBR range covers the popular sizes a beginner is most likely to need. The medium weight and classic profile, along with the Sabian logo, give the cymbals authenticity. And they look the part with their deep, large-peen hammering and pinpoint lathing.\nThis is the same hand-guided, high-pressure hammering and hand lathing that Sabian lavishes on its expensive bronze cymbals. Beneath their initial thin attack, there is some real meat and tone. Brass cymbals will never sound as complex as proper bronze cymbals, but these are well worth upgrading to from the cymbals thrown in with your starter kit.\nStagg cymbals from China have been with us since 2001, offering B20 bronze hand-hammered cymbals at shockingly attractive prices. The Single Hammered SH line-up is the cheapest in the range. Cymbals have uniform shallow hand-lathing top and bottom and extremely light hammering, almost unnoticeable. What the lack of extensive hammering does mean is they are a little short on projection when compared with more heavily-worked cymbals, but overall they have universally clean tones.\nThe only thing stopping them from being top class cymbals is a slight foreshortening of sustain and shallowness of timbre which the extra working of expensive cymbals bestows. It\'s most noticeable by the time you get to the larger 16"" and 18"" crashes, but at these prices, it\'s unfair to be too sniffy. The SH may be Stagg\'s cheapest series, but for such budget cymbals, they are unusually refined in tone. B20 bronze, hand-hammered cymbals at knock-down prices.\nDream produces professional-quality cymbals in the ancient Chinese cymbal-making region around Wuhan city. Dream\'s Contact series lies somewhere between the slightly darker, trashier Bliss series and the rockier Energy series. The Contact series is, unusually, made from B23: 23 percent tin content and 77 percent copper. With such a high tin content the result is a lovely silvery-gold colouring and a slightly brighter stick response. The cymbals have a satin ﬁnish with Chinese-style graphics, and surfaces are lightly dimpled with barely discernible hammering, close-lathed top, and bottom.\nIf you\'re into heavy metal, you should maybe look away now as these cymbals won\'t deliver a face-melting. The vibe is softer and mellower: a hint of China trashiness, a spicy tang and bell-like clarity. They need coaxing, not slaying. But while they\'re not clangers, they can certainly be played hard. Each cymbal will sound slightly different, so you need to choose with care.\nFeaturing splashes, crashes, chinas, hi-hats, and rides, PST8s contain a fair few attributes from higher-spec Paiste ranges - chief among these is the CuSn8 bronze that the cymbals are made from. CuSn8 is Paiste\'s own blend of B8 and is also known as 2002 bronze after the legendary Paiste range that it was formulated for. Most of the diameters of cymbal are available in two weights - Medium and Rock. As well as being heavier, the Rock models also have larger bells, a slightly flatter profile and have been on the end of a few more hammer blows. Crashes open with a clean, silky blast of B8 that slides across the mix.\nThree pairs of 14"" hi-hats are available - Medium, Rock and Sound Edge, and ride cymbals come in 20"" (Medium & Rock) and 22"" (Rock only) sizes. The 20"" Medium gives a bright stick sound that generates a sympathetic accompaniment of wash. In comparison, the heavier 20"" Rock has a toppier, more lively feel, while the 22"" Rock ride has a deeper voice and bigger presence.\nThe S Family is extensive, with 38 cymbals available individually or in two boxed sets - a Performer and a Rock Cymbal set. Medium spaced rippled lathing covers the top and bottom surfaces, and there is intensive, graded machine hammering everywhere except on the bells. The collection encompasses four weights: Thin, Medium-Thin, Medium and Rock.\nA highlight of the range, stick definition on the 20"" Medium ride is exemplary, with the clarity of a top pro cymbal, quite woody and focused. The 18"" hole-strewn Trash crash has a funky thin and trashy element, like a super-thin fast crash. It would make great effect cymbals, regardless of budget. Improving on Zildjian’s old ZHT series, the B12 bronze alloy S Family is a good-looking intermediate level series with a range of weights offering a compelling range of sounds.']"	['<urn:uuid:1f7d32ce-9f4e-427b-84f7-34722a8e70cf>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	12	73	942
29	bicycle parking innovation convenience research	Modern bicycle parking facilities are becoming increasingly sophisticated and user-friendly. Utrecht's central station features the world's largest bicycle parking facility, accommodating 12,500 bikes with indoor ramps for quick access and a digital system guiding users to empty spaces. In Delft, a university campus parking garage combines 2,100 bike spaces with amenities like a coffee bar and repair workshop. Copenhagen's busiest transport station includes sunken areas for 2,500 bikes designed for clear sight lines. At the same time, these facilities contribute to transport efficiency - while cars can move just 4,400 people per hour on a given road, bicycles can move 14,400 people per hour while allowing door-to-door journeys, making them a spatially efficient and convenient transport solution in urban areas.	"['The Transport Committee of the United Kingdom parliament is holding an Urban Congestion Inquiry. This article forms the basis of Push Bikes\' submission to that inquiry.\nBuild for cars, and you will get more cars using the roads, which means more cars queuing on those roads¹. Meanwhile more and more of the finite space in cities is given over to roads, at great expense both in terms of construction and maintenance. Our roads are a potholed mess, because we cannot afford to maintain them, yet we go on building new roads in order to ""ease congestion"". Britain is like the gambler that believes the jackpot is just around the corner, if only he keeps gambling.\nBy way of example, the photo depicts the recently built A38 relief road for Selly Oak and Bournbrook high streets. It is already congested, whilst the original route (which was kept open) remains heavily trafficked. As can be seen, most of that traffic consists of private cars. The cars on the left are queuing to get into a residential road, one that the Birmingham City Council has made into a rat-run so that motorists can bypass the congestion on the main road. The residents are fed up with the heavy traffic and the speeding. Each of those cars is most likely to contain just one person. Leaving aside the bus, this sea of cars represents approximately just thirty people trying to get across the junction. So thirty people are delaying a bus which can carry around ninety people. And therein lies the clue; private cars are an incredibly inefficient means of transporting people, because a car is fundamentally much larger than the occupant, and in addition it requires a large amount of space around it when moving. It makes no difference whether the car is powered by petrol, diesel, or electricity. High-tech approaches such as driverless technology (which are not even mainstream yet) will make little difference to the equation, because they do not change the size of the vehicle, its speed whilst moving, or its braking traction. The automotive city concept (trying to move everyone by car) doesn\'t work, as is demonstrated by congestion.\nWhilst cars can move just 4,400 people per hour on a given road, on the same road trams will move 66,000 people per hour, and buses 52,800. The problem is they do not represent door-to-door transport, so unless your journey lies on the route, and the route is direct, they do not represent an attractive solution for many journeys. Walking will move 17,600 people per hour, but it is too slow for most people. In the sweet-spot of speed vs spatial efficiency vs convenience is the humble bicycle.\nCycles will move 14,400 people per hour at a speed high enough that most city journeys can be completed in a reasonable time², and they allow door-to-door journeys. They do all this without creating any additional pollution. Many urban areas in mainland Europe now have extensive infrastructure that enables cycling that feels and is safe. Every urban area that has built such infrastructure has seen a massive modal shift towards cycling. The better the infrastructure, and the more extensive it is, the bigger the modal shift. Britain is way behind, but Waltham Forrest Council has shown that the Dutch approach to creating infrastructure for cycling works here too.\nSo we need a plan for catching up. Fortunately, 15-25% cycling modal share (which will have a considerable benefit given the threefold improvement in spatial efficiency) is easy to achieve. Push Bikes, the Birmingham Cycling Campaign, suggests the following approach. In any urban area:\n- Use an OS map to see where are the A and B roads. These will naturally form a mesh. Use bollards, planters, and one way systems to ensure you cannot drive directly across the holes in the mesh; you have to drive out to the nearest A or B road, and go around the outside. Cyclists and pedestrians can use any public road in any direction, and where safe, cyclists can also use paths. This is called filtered permeability.\n- Build connectors for cyclists and pedestrians between the holes in the mesh. Very often there will be obvious points of connection, such as an existing crossing (controlled or zebra), or a crossroads. Improve junctions and build segregated cycle infrastructure at these points first. Signpost the quickest routes through the mesh.\n- Build segregated infrastructure along main roads to link up the initial sections as required (this is needed for people travelling longer distances). Steps (1) and (2) unravel transport modes, and may result in a link section in Step (3) being relatively unnecessary.\n- Polish. Make roads within the mesh holes look less like roads. Add more segregated sections.\n- Extend filtered permeability out into the countryside.\n- Every time something is going to be rebuilt, the design should include proper consideration for cyclists, even if right now it connects with nothing.\n- Remodel junctions that cyclists will be using to tighten up the geometry (so motorists are forced to slow down).\n- Implement chicanes to slow motor traffic, not pinch points and speed cushions (which cause all sorts of problems for everyone, and have no real effect). Just alternating the parking between sides along a road will create chicanes.\n- We need two additional junction designs, one of which may be currently legal, one of which isn\'t. If legal, start implementing, if not it needs to be legalised. The first is the Dutch-style roundabout (which is single lane, has tight geometry, an adverse camber, and cycle crossings that allow the cyclist to cross one lane at a time). The second is the simultaneous green junction (which allows cyclists to travel in all directions across a junction whilst motor traffic is held by red lights). These junctions are described on the Push Bikes website³.\n- The toucan crossing is a very poor compromise that is hopelessly slow and inefficient for all transport users. We need to copy the Dutch approach, where pedestrians and cyclists are provided with controlled crossings that reflect their very different speeds, controlled crossings that change as soon as the button is pressed⁴.\n- Highways engineers need professional training, and there need to be national design standards. The simplest, fastest, most reliable method of gaining national design standards is to copy the Dutch national design standards, which are tried and trusted, and available in English⁵.\n- There needs to be adequate funding. The Dutch spend around €25 per head per year, but we need to spend more to catch up. This would come from the existing highways budget, of which it would be a small percentage.\nBy these means one reduces private car use and creates space for the segregated cycling infrastructure. Reduced car usage means less congestion. Note that Germany and the Netherlands, for example, are both already at Step 5, which gives you an idea how far behind is the UK. The Netherlands represents global best practice, and they have achieved up to 60% cycling modal share. Denmark and Sweden are other good examples.\n- ¹ Cairns et al (1998), Duranton & Turner (2011), SACTRA (1994).\n- ² Birmingham Mobility Action Plan: https://www.birmingham.gov.uk/bmap.\n- ³ Building Blocks: http://pushbikes.org.uk/infrastructure/guiding-principles/building-blocks\n- ⁴ Dutch Crossings Change Quickly: https://vimeo.com/117470593\n- ⁵ Design manual for bicycle traffic: https://www.crow.nl/publicaties/design-manual-for-bicycle-traffic', 'While some cities still struggle to build safe bike lanes, others are creating infrastructure like a multi-level bike parking garage and an aerial path that connects to transit stations. The Bicycle Architecture Biennale, opening in Amsterdam on Monday, highlights 15 of the most interesting projects. “By showcasing the most dynamic, visual solutions, the BAB inspires a new way of thinking about what cities of the future should look like,” says Adam Stones, strategy director of Bycs, an organization aiming to move 50% of urban trips to bikes by 2030. “And by showing what is possible, it will lead to many more creative solutions being implemented.”\nFour of the projects are from the bike-obsessed Netherlands. But Next Architects, a Dutch architecture firm that helped curate the show, points out that the country was dominated by cars in the 20th century, and had to make conscious choices to transform–meaning that other countries can do the same thing. “It is thanks to decades of campaigning against car architecture, against plans without bicycles, that the space for slow traffic and residence has been reclaimed in the city,” says Bart Reuser, founding partner at Next Architects.\nBiggest bicycle parking in the world\nEctor Hoogstad Architecten (Utrecht, the Netherlands)\nIn Utrecht, where around 43% of trips take place on bikes, a massive bike parking garage at the central train station now fits 12,500 bikes. The space is so large that it includes indoor bike ramps so that riders can quickly ride to park; a digital system is designed to guide people to empty spaces. (Along with other bike parking near the central station, there will be more than 33,000 spaces for bikes in the area before the end of 2020.)\nCycling and pedestrian connection\nBatlle i Roig (Barcelona, Spain)\nFor decades, it’s been difficult for people walking or biking to cross an intersection in Barcelona where a ring road meets an expressway. A new path is more direct, or about 1,640 feet shorter than the old route, and designed to feel more like riding in the countryside than the middle of the city.\nCoffee & Bikes\nBureauVanEig/Biq architecten (Delft, the Netherlands)\nOver the last couple of decades, the main road at the university campus of TU Delft has transformed from a street filled with cars to a green zone with walking and biking paths. In the center, a huge new bike parking garage has space for 2,100 bikes combined with a light-filled coffee bar. It also has a bike repair workshop.\nXiamen Bicycle Skyway\nDissing+Weitling (Xiamen, China)\nIn Xiamen, an aerial bike path travels for nearly five miles—the longest elevated bike path in the world—with 11 exits to public transit hubs and bike rental for people who are rushing to a nearby bus or subway station. At rush hour, more than 2,000 bikes an hour can fit on the path.\nCycling through water\nVisit Limburg, Lens°Ass Architecten (Limburg, Belgium)\nIn the middle of a Belgian nature preserve, a sunken bike trail cuts directly through a pond, so cyclists can ride at eye level with the water and glide by swans. The path, called Cycling through Water, helps inspire people to ride bikes; an average of 800 visitors a day now come through the area.\nCycling through the trees\nBuroLandschap (Limburg, Belgium)\nNearby, another new Belgian bike path rises as high as 32 feet into the canopy of a forest. The circular path was designed to have as little impact on the forest as possible during construction, while giving riders a new experience in nature. Like Cycling through Water, it’s intended as a draw to convince more people to ride bikes.\nCurtin Bike Hub\nConiglio Ainsworth Architects (Perth, Australia)\nAt a university in Perth, a bike hub has 200 parking spaces along with showers and locker rooms to make it easier for students and faculty to commute by bike.\nNelson St Cycleway\nMonk Mackenzie, LandLAB, GHD (Auckland, New Zealand)\nA former highway off-ramp in Auckland has been transformed into a bright pink bike path that connects to a cycle loop in the city center, providing a new way to cross a busy street. The walls of the path hold an interactive light sculpture.\nSchneider+Schumacher (Raunheim, Germany)\nA sleek, spiraling white bike and pedestrian path crosses a river past an oil terminal, closing a key gap in a longer path between the cities of Frankfurt and Mainz.\nUpside Down Bridge\nNooyoon (New York, USA)\nThis proposal for an abandoned rail line in the New York City borough of Queens suggests building a “upside down bridge” with a community center at the base, a “floating forest” at each end of the top, space for urban farming, and bike paths along the side.\nCOBE and Gottlieb Paludan Architects, Sweco (Copenhagen, Denmark)\nA huge plaza next to Denmark’s busiest transport station used to be chaotic. The space was redesigned to be easier to navigate, with slightly sunken areas for bike parking that give a clear line of sight across them. The site has space for 2,500 bikes.\nSPADE (Cologne, Germany)\nA proposed bridge in Cologne would link to two other major bridges, creating a new connection between the city’s halves without a new span across the Rhein River. The curving design will make it easier to walk or bike from the city center to other neighborhoods.\nPaper planes e.V. (Berlin, Germany)\nUnder an elevated metro line in Berlin, a proposed project would transform underused space into a new protected bike path that shelters riders from rain and snow. The project includes plans for beer gardens, food truck stops, and charging stations for electric bikes along the way.\nBike Parking Canopy\nNL Architects (The Hague, The Netherlands)\nAt a new hall at The Hague’s central station, an elevated tram track will pass through the hall. In this proposed project, bike parking will also be elevated inside the hall, making it visible and easy to access.\nNEXT Architects (Purmerend, The Netherlands)\nThis unique bridge separates bikes and pedestrians, with a high arch that offers views (and exercise, with steep stairs) for people on foot, and a low path for people on bikes and in wheelchairs. The bridge connects the city center with another district across the river.']"	['<urn:uuid:c0bbc9cf-adf6-4746-a2d8-ece7c7d1e836>', '<urn:uuid:bec8a2fa-9a17-4475-9caa-ccc678933ad4>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	5	120	2253
30	What's more effective for preventing water system failures: calcium deposits or pressure reducing valves?	Pressure reducing valves (PRVs) are more effective at preventing water system failures. While calcium deposits can actually be beneficial in some cases - as shown by Roman lead pipes where calcium created protective plaque preventing lead poisoning - PRVs serve a crucial protective function by regulating water pressure below 80 psi. PRVs protect the entire plumbing system from excessive wear and tear by suppressing high incoming pressure, using an adjustable spring loaded diaphragm and disc to maintain preset pressure levels. Without a PRV, high pressure can cause rapid fixture deterioration, leaks, pipe erosion and system damage, especially in older homes. A properly functioning PRV typically lasts 7-12 years and helps maintain the designed lifespan of plumbing components.	"['Master plumbers on construction jobs may be involved with developing blueprints that show the placement of all the pipes and fixtures. Their input helps ensure that a structure’s plumbing meets building codes, stays within budget, and works well with the location of other features, such as electric wires. Many diagrams are now created digitally with the use of Building Information Modeling (BIM), which allows a building’s physical systems to be planned and coordinated across occupations.\n“Don’t assume that every Mr. Fix-it advertising his services in the local Pennysaver or on Craigslist is a licensed plumber,” says one New York plumber. There is no national standard for issuing licenses. Some plumbers are licensed by the state, others by the counties they work in. Check with your local city hall or chamber of commerce. They should be able to direct you to the appropriate source for a list of licensed plumbers in your area.\nDespite the Romans\' common use of lead pipes, their aqueducts rarely poisoned people. Unlike other parts of the world where lead pipes cause poisoning, the Roman water had so much calcium in it that a layer of plaque prevented the water contacting the lead itself. What often causes confusion is the large amount of evidence of widespread lead poisoning, particularly amongst those who would have had easy access to piped water. This was an unfortunate result of lead being used in cookware and as an additive to processed food and drink, for example as a preservative in wine. Roman lead pipe inscriptions provided information on the owner to prevent water theft.\nIn addition to Trenchless Technology, Our licensed technicians and OSHA certified excavation crews specialize in traditional tunneling and sub-slab plumbing repair / replacement. Detailed analysis and fixed cost proposals for repair are provided to customer upon completion of a hydrostatic pressure test and diagnostic slab leak testing, using a combination of cutting-edge pipelining techniques with traditional excavation to fix foundation leaks while minimizing digging and added expense. Traditional tunnels or trenches for water line repair or sewer repair are hand dug by crews with confined space training.\nIf left unfixed, a running toilet or a leaky faucet can end up costing you hundreds of dollars. Check out your water bill and how your water is running. If you find your kitchen dripping non-stop and your water bill going higher, you may have a leak! Let Parker & Sons put a stop to those leaks. Our team of expert plumbers will make sure to get the work done faster, get it right the first time, and do it efficiently.\nHello, long story. Toilet line stopped up two years ago. A company used high pressure water to clean line. Didnt work. Came back a second time with a plummer. Spouse was told that the line had collapsed, filled with rocks or tiles. Plummer then disconnected that line put in a second line, punched a hole in my tank, fed the new line in that hole. Now two years later, each heavy rain brings a strong sewer smell into the house. Found out later that there were no rocks or tiles in original line, only a large calcium build up over 25 years that had clogged the original line. I do not know if when the new line was put in, the old line was closed properly or if either line was or should be vented? I would like the original line reconnected and the newer line just removed. I plan then to repair the hole in septic tank where the new line entered with tar or concete. I need someone to fix my smelly home.\nAs with other construction workers, employment of plumbers, pipefitters, and steamfitters is sensitive to fluctuations in the economy. On the one hand, workers may experience periods of unemployment when the overall level of construction falls. On the other hand, shortages of workers may occur in some areas during peak periods of building activity.\nPipe is available in rigid ""joints"", which come in various lengths depending on the material. Tubing, in particular copper, comes in rigid hard tempered ""joints"" or soft tempered (annealed) rolls. PeX and CPVC tubing also comes in rigid ""joints"" or flexible rolls. The temper of the copper, that is whether it is a rigid ""joint"" or flexible roll, does not affect the sizing.\nWater heaters are tasked with heating the water that passes through the pipes to every shower, tub, and sink. These heaters are also supposed to heat water for the washing machine and dishwasher. These units can hold up for a long time, but may begin to cause problems over time. When the heater does not heat water properly all the time, leaks, or makes odd noises, homeowners will know that there is a problem. A water heater that begins to leak could start flooding the house at any moment. A water heater that makes odd sounds could be under pressure and about to burst. In each of these cases, it could be dangerous for the homeowner, and only a licensed Manhattan plumber can fix the problem or replace the water heater altogether.\nMost typical single family home systems won\'t require supply piping larger than 3⁄4 inch (19 mm) due to expense as well as steel piping\'s tendency to become obstructed from internal rusting and mineral deposits forming on the inside of the pipe over time once the internal galvanizing zinc coating has degraded. In potable water distribution service, galvanized steel pipe has a service life of about 30 to 50 years, although it is not uncommon for it to be less in geographic areas with corrosive water contaminants.\nBacteria have been shown to live in ""premises plumbing systems"". The latter refers to the ""pipes and fixtures within a building that transport water to taps after it is delivered by the utility"". Community water systems have been known for centuries to spread waterborne diseases like typhoid and cholera, however ""opportunistic premises plumbing pathogens"" have been recognized only more recently; Legionella pneumophila discovered in 1976, Mycobacterium avium, and Pseudomonas aeruginosa are the most commonly tracked bacteria, which people with depressed immunity can inhale or ingest and may become infected with. These opportunistic pathogens can grow for example in faucets, shower heads, water heaters and along pipe walls. Reasons that favor their growth are ""high surface-to-volume ratio, intermittent stagnation, low disinfectant residual, and warming cycles"". A high surface-to-volume ratio, i.e. a relatively large surface area allows the bacteria to form a biofilm, which protects them from disinfection.\nPlumbing reached its early apex in ancient Rome, which saw the introduction of expansive systems of aqueducts, tile wastewater removal, and widespread use of lead pipes. With the Fall of Rome both water supply and sanitation stagnated—or regressed—for well over 1,000 years. Improvement was very slow, with little effective progress made until the growth of modern densely populated cities in the 1800s. During this period, public health authorities began pressing for better waste disposal systems to be installed, to prevent or control epidemics of disease. Earlier, the waste disposal system had merely consisted of collecting waste and dumping it on the ground or into a river. Eventually the development of separate, underground water and sewage systems eliminated open sewage ditches and cesspools.', 'Everyone loves a high pressure shower, but did you know that high pressure can be costing you money? Has someone suggested you need to install a PRV and you have no idea what it is? This article will explain why you should have one, how to check if you need one, and explain what it does to keep your plumbing in working order. If we don’t answer all your questions we are always just a phone call away.\nCode requires that residential homes maintain water pressures below 80 pounds per square inch (psi). The average pressure from water companies in the Upstate vary from 80-130 psi. When water pressures are above 80 psi it may cause leaks and premature repairs. It shortens the lifespan of fixtures, appliances, and the piping of your plumbing system. Most newer homes have a pressure regulating device, sometimes called a pressure reducing valve, and commonly known as a PRV, to prevent damage due to high water pressure. Older homes were often built without a PRV when water pressures from water departments tended to be lower. If your home does not have a PRV or has one that has failed, your plumbing is being subjected to excess wear and tear.\nWHY IS HIGH WATER PRESSURE DANGEROUS?\nFlowing water carved the Grand Canyon, and it also carves away the walls of your pipes in the same manner. When a plumbing system is properly designed, water flows through the pipes in a controlled manner. When water pressure is high the water moves through the pipes at a higher velocity than necessary. This higher rate of speed causes the pipes to erode much faster than when water moves through the pipes at the proper velocity. You can’t see this process happening, but other effects of high water pressure are more visible.\nWhen you close a faucet or an automatic valve like those commonly found in ice makers, washing machines, and toilet fill valves shut off you are slamming a door on the water flowing through the pipes. Those doors are engineered for holding back water at pressures of 80 psi and under. When subjected to pressure in excess of 80 psi they get stressed out and fail. I liken normal water pressure to a 5 year old kid and high water pressure to an NFL defensive lineman. Most grown people can easily stop a charging child, but almost nobody can stop a 330 pound lineman, even if they were only standing in front of you and just decided to lean in your direction. Your fixtures can handle what they are designed for, which in this example would be a typical adult, but if you ask them to stop and hold back more, they will bend and eventually break. Items that may have a normal useful life of 7-10 years, may last 1-3 years at best under excessive pressure. If you have a newer faucet that is dripping, a toilet that runs non-stop or requires repairs every couple of years, or other leaky device you may have a water pressure issue.\nHigh water pressure can also cause banging pipes and whistling noises throughout the home. As that big lineman runs through the piping system and eventually runs into a brick wall in the form of a closed valve, all that momentum stops in its tracks. This can cause the entire length of pipe to slide if it isn’t well secured. It is most prone near fast closing automatic valves like the ones found in washing machines, ice makers, and toilets. If you hear banging noises, it is a very good indication that you have high water pressure.\nVERIFY IF YOU HAVE HIGH PRESSURE\nIf you think you have high water pressure, or just want to be certain that your pressure is normal, all you need is a simple pressure gauge with a garden hose connection on the end of it. You can find them at a quality hardware store, one of the big box centers. Prices range from about $10-$20 and they will tell you everything you need to know. Once you have a gauge simply connect it to your garden hose valve in the backyard, side of the home, or your water heater drain valve and turn it open. The hose bib on the front of the house is not recommended as a testing point because many PRVs are installed on the system just after the front hose bib. When this is the case you can get a false positive for high water pressure when the house may still be under protection. The needle on the gauge will move up and settle after a few moments telling you what the standing psi of your plumbing system is. It is important to let the pressure level off before you think you are in the clear. A failing PRV will often cause a gauge to start out under 80 psi, but the needle may begin to slowly climb. When a PRV is not present the pressure may fluctuate throughout the day as the water company experiences changes in demand. Gauges like the one show above have a second needle that will show you where the water pressure peaks while you leave it connected overnight. This type of gauge may also record another pressure issue known as thermal expansion which you can read more about in our next article.\nIF YOUR PRESSURE IS HIGH\nWhen the pressure entering your home is high, you can correct it by installing aPressure Reducing Valve commonly referred to as a PRV. A PRV takes the high pressure entering your home and suppresses it down under 80 psi. Most are adjustable and can be set anywhere from 40-80 psi. Water is constricted within the valve body and directed through an inner chamber where an adjustable spring loaded diaphragm and disc adjust continuously to maintain the preset pressure. Even if the supply water pressure fluctuates, the pressure reducing valve ensures a constant flow of water at a functional pressure. They can be installed just after the water meter, or just after entering the home. Anything beyond a PRV will be protected from high pressure as long as the valve remains functioning. A good quality PRV has a similar lifespan as a tank style water heater. You can expect 7-12 years as the typical age before failure. If you have a home built prior to 1996 or you know that your PRV is over 12 years of age, it is recommended that you verify its operation.\nA reputable plumber will have a pressure gauge and will always check your water pressure before attempting to diagnose any of your plumbing issues. If you are worried about lowering your water pressure, keep in mind that most people can’t actually tell the difference between 70 and 120 psi. If you have any questions give us a call or leave a comment below.']"	['<urn:uuid:1f0bd602-2b3d-4eb2-9fc8-91cc823642f2>', '<urn:uuid:d4eea254-c852-4a03-814d-b557b3a4a276>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T06:15:13.904987	14	117	2358
31	methods disposal medical waste yellow bags sharps chemical disinfection steam sterilization	Medical waste disposal involves multiple methods: hazardous and infectious waste must be discarded into yellow bags or containers, sharps require special biohazardous containers, chemical disinfection is used for waste carrying harmful microorganisms, and steam sterilization involves exposing waste to 250°F for up to 90 minutes in an autoclave.	"[""Email & Password\nNot a member? Register.\nAs part of Initial Medical’s ‘Follow the Colour Code’ blogs, here we look at yellow…\nOne of the three primary colours, yellow is associated with optimism, happiness, creativity, confidence and enlightenment; in Japan it signifies courage. The most luminous and psychologically the strongest colour in the spectrum, it is believed to lift spirits, stimulate mental processes and activate memory. Yellow is naturally found in various flowers, fruits and vegetables, and it is the colour of traditional post-it notes – which was apparently a complete accident and the result of simply there being some yellow scrap paper lying around!\nOn the other hand, yellow is thought to be the most irritating colour due to its excessive eye stimulation. It is also the colour used to signal caution as well – in car racing, football, school buses in the US – plus yellow is representative of ‘greed’ in Christianity and a sign of mourning in Egypt.\nFor UK providers of any cosmetic or aesthetic procedures, bright yellow is also the colour of clinical and highly infectious waste.\nAs part of the best practice colour coding protocols for safe waste segregation and disposal set out by the Department of Health Safe Management of Healthcare Waste Memorandum, all waste products that pose a high infection risk must be discarded into yellow bags or containers. This type of waste is almost always classed as hazardous and includes any waste contaminated with body fluids that has been produced from the treatment of infectious patients, or those suspected of having an infection.\nItems in this category and classed as ‘yellow’ waste include the following list when infection is present during their use.\nSoft waste includes:\nProduced during cosmetic treatments, particularly surgical procedures, yellow soft waste has a high potential of harbouring infection and disease. The high risk of infection could be identified following a local case of MRSA, an outbreak of C.difficile infection on the premises or even an infection the patient themselves carries and is aware of. All such situations will render any protective clothing, dressing materials or tubing that may have come into contact with the infection, as yellow, highly infectious clinical waste.\nSharps waste includes:\nAny sharps that may be produced, such as those used when providing dermal fillers, for example, must be disposed of into yellow lidded sharps containers.\nIn the UK, all movement of waste requires a paper trail. As such, it is important that the correct documentation is completed and handed to licenced contractors in order to fulfil legal obligations upon collection.\nHazardous waste, such as that coded yellow, requires a Hazardous Waste Consignment Note (HWCN). This will include information on the cosmetic clinic producing the waste and its premises code, as well as details of the waste itself including a description of the contents, the quantity of waste contained and information on the packaging. Codes must also be included here as required for the Department for Transport. The final documents are filled in by your service technician on the day of collection, with a signature from the relevant professional handing over the waste and again when the waste is consigned for final disposal.\nAt this point, ‘yellow’ clinical and highly infectious waste is sent for disposal by incineration. This is the most commonly used high temperature treatment for infectious waste but other technologies do exist such as pyrolysis and gasification. The steam generated from this heating process can sometimes be used for heating buildings near the incinerator itself.\nBefore you go.....\nLet's stay in touch, pop your details here and we'll send our editor's hand-picked updates on your fave subjects."", 'Before a medical facility can offer care, it has to have an effective method of medical waste disposal in place. But which method is the right one? There are so many guidelines and regulations at play, and the many methods of disposal come with varying price tags, often preventing a facility from aligning with their preference. Maintaining this proper disposal of medical waste is overwhelming. Between labeling, containment, and treatment, this issue has many healthcare facilities stumped as to their next steps in ensuring they are following federal guidelines. In this article we are going to cover:\nWhat wastes are considered to be medical waste?\nMost effective methods of disposal\nWhat Wastes are Considered to be Medical Waste?\nAccording to the World Health Organization, there are eight types of medical waste:\nInfectious waste: waste contaminated with blood and other bodily fluids (e.g. from discarded diagnostic samples), cultures and stocks of infectious agents from laboratory work (e.g. waste from autopsies and infected animals from laboratories), or waste from patients with infections (e.g. swabs, bandages and disposable medical devices);\nPathological waste: human tissues, organs or fluids, body parts and contaminated animal carcasses;\nSharps waste: syringes, needles, disposable scalpels and blades, etc.;\nChemical waste: for example solvents and reagents used for laboratory preparations, disinfectants, sterilants and heavy metals contained in medical devices (e.g. mercury in broken thermometers) and batteries;\nPharmaceutical waste: expired, unused and contaminated drugs and vaccines;\nCytotoxic waste: waste containing substances with genotoxic properties (i.e. highly hazardous substances that are, mutagenic, teratogenic or carcinogenic), such as cytotoxic drugs used in cancer treatment and their metabolites;\nRadioactive waste: such as products contaminated by radionuclides including radioactive diagnostic material or radiotherapeutic materials; and\nNon-hazardous or general waste: waste that does not pose any particular biological, chemical, radioactive or physical hazard.\nOut of all the waste created by healthcare facilities, only about 15% is actually considered hazardous. This means that a facility could potentially be overpaying for a medical waste disposal system, simply due to over-classification of its waste. Each state has clear definitions for what materials qualify as hazardous, so it’s vital to an organization to clearly understand the expectations put before them when it comes to properly disposing of medical waste. To view any state’s regulatory requirements for medical waste disposal, click HERE.\nMost Effective Methods of Disposal\nDisposed medical waste must be treated in a way that prevents the spread of possible pathogens. Obviously, it does not require sterilization, but it must be disposed of in a way that does not harm healthcare workers or the environment.\n1. Chemical Disinfection\nSome biohazardous wastes carry harmful and infectious microorganisms that should not be released into the environment once the waste has been discarded. For that reason, these wastes are chemically disinfected prior to leaving a healthcare facility and being deposited in a landfill. This method requires a facility to have a full area dedicated solely to the treatment and disinfection of medical waste, as well as packaging solutions for the transportation of that waste.\nThis method of medical waste disposal involves steam sterilization. According to the Center for Disease Control (CDC), exposure of the waste for up to 90 minutes at 250°F in a autoclave may be necessary to ensure an adequate decontamination cycle. After steam sterilization, the waste can be safely handled and discarded with all other nonhazardous solid waste. These wastes can then be incinerated, provided the incinerator is capable of burning these wastes completely and can stay within Environmental Protection Agency (EPA) emissions standards.\nCertain medical wastes such as sharps pose the greatest risk of injury to those involved with their disposal. Many sharps (needles, scalpels, syringes, lancets, auto-injectors, etc.) end up in the trash or flushed down the toilet, as users are often unaware of the dangers of accidental needlesticks. Due to this high level of risk involved with accidental needlesticks, the U.S. Food and Drug Administration (FDA) has developed regulations for proper sharps disposal. Disposal containers must display a biohazardous symbol indicating that the material inside is hazardous. When a container is about 3/4 full, it must be disposed of properly at sharps collection sites or through the use of a mail-back program.\nEncapsulation of medical wastes can prevent healthcare facilities from spending money on expensive incineration or disinfection tools. Once waste has been encapsulated, it is sent off to a separate facility for treatment and disposal, eliminating the need for those solutions to be accomplished in-house. Medical waste disposal doesn’t have to be an expensive endeavor for any facility that handles bio-hazardous material. Mail-back systems can significantly minimize the cost of medical waste disposal. To find solutions for mail-back waste disposal, click HERE.']"	['<urn:uuid:9ea9cb5a-c74a-490e-bfa8-16467c61229d>', '<urn:uuid:0031ee60-e4ae-4dd4-a80c-218877b27a40>']	factoid	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-13T06:15:13.904987	11	48	1384
32	immediate versus delayed treatment consequences costs	Delaying hearing loss treatment has both health and financial consequences. In terms of costs, people with untreated hearing loss have 46% higher healthcare costs over a decade, averaging $22,434 more per person. In terms of health impact, untreated hearing loss leads to faster cognitive decline, with adults developing cognitive impairment an average of 3.2 years earlier than their peers with normal hearing.	"['Living with untreated hearing loss is complicated and can lead to several communication difficulties. You might find it hard to talk on the phone or struggle to order a coffee at your favorite cafe. However, as we\'ll see later in this post, untreated hearing loss causes far more than these communication issues.\nMany hearing aid users wait years after their initial hearing test results before seeking treatment. What are the reasons why people put off seeking help for so long? There are two key reasons for this:\nHearing loss is a gradual process. The hairs or nerve cells in the cochlea transmit sound signals to the brain that can wear out with age and loud noise exposure. This process can last years or even decades. Because it is so slow, many people decide to neglect their hearing loss for long periods.\nHearing loss is commonly regarded as a sign of aging. Many people perceive hearing loss as a danger to their physical independence. They see it as one of the physical signals that they are aging. Those who take pride in their independence will find this line of thinking especially appealing.\nBut treatment needs to be done early for the best results. And the longer they wait, the more complex treatment becomes.\nThe potential consequences of untreated hearing loss\nHearing loss is often undiagnosed and untreated by American adults for the reasons mentioned above. Because of this, we often learn to change our behaviors to adapt. Although these temporary fixes can work in the short term, untreated hearing loss can have several adverse health and overall wellbeing implications. Here are some examples:\n**It is important to remember that while some of these studies show a correlation between these conditions and hearing loss, but we do not yet know if there is a causal relationship.\nAn increase in the number of injuries\nAccording to a study published by the Brigham and Women\'s Hospital in Boston, adults with hearing loss are more likely to be involved in incidents or injuries. The study used data from the National Health Interview Survey, a nationwide survey in which 232.2 million Americans answered questions about their health and wellbeing from 2007 to 2015. The survey contained questions about people\'s hearing health as well as reports on any recent injuries.\nAfter reviewing the results, researchers discovered that those who had ""a lot of trouble hearing"" were 50 percent more likely to be injured. 6.6 million people confirmed having been injured in an accident, with many of them rating their hearing as ""poor."" The research concluded that the worse the hearing loss, the more likely adults were involved in incidents.\nAn increase in hospital costs\nAccording to a report by the Johns Hopkins University Bloomberg School of Public Health, people with untreated hearing loss have a much higher total healthcare cost than those with good hearing. Over a decade, the study found that older adults with untreated hearing loss had 46 percent higher healthcare costs, averaging $22,434 per person. The impact was noticeable within two years.\nWhat explains this rise in healthcare costs? According to the report, over ten years, those with untreated hearing loss had a 50% higher risk of dementia, a 40% higher risk of depression, and a nearly 30% higher risk of falling than those without hearing loss.\nAn increase in the risk of dementia\nHearing loss has been linked to an increased risk of dementia, according to a 2011 report. Professor Frank Lin, Professor of Otolaryngology at the John Hopkins School of Medicine in Baltimore, led the research, which attracted a team of scientists from around the country.\nA total of 600 adults were included in the report. These individuals were followed until 2008 after completing a series of assessments to determine their cognitive and hearing abilities between 1990 and 1994. Scientists were also looking for Alzheimer\'s disease or dementia symptoms. A third of the participants had hearing loss, while the rest had no issues hearing. At the start of the research, none of the participants had dementia.\nThe researchers concluded that those who had hearing loss at the start of the study were much more likely to develop dementia, with the severity of the hearing loss suggesting a higher risk of dementia. Dr. Frank Lin of Baltimore determined that every 10 decibels of hearing loss increased the risk of dementia by 20%. Hearing loss was linked to 36% of the dementia risk in participants aged 60 and up.\nIf you have untreated hearing loss, the best time to seek hearing treatment was yesterday. The second best time is now. Contact us today to set up an appointment!', 'Hearing loss has a lot of different causes and manifestations. It can be sudden or gradual. It can occur in one ear or both ears. It can be temporary or permanent. It happens to people of all ages and is associated with the aging process. Before discussing causes and treatments for hearing loss, it is important to understand how hearing works.\nHow We Hear\nThere are three sections of the ear: the outer ear, middle ear and inner ear. Each section helps move sound through the process of hearing. When a sound occurs, the outer ear feeds it through the ear canal to the eardrum. The noise causes the eardrum to vibrate. This, in turn, causes three little bones inside the middle ear (malleus, incus, stapes) to move. That movement travels into the inner ear (cochlea), where it makes tiny little hairs move in a fluid. These hairs convert the movement to auditory signals, which are then transmitted to the brain to register the sound.\nTypes of Hearing Loss\nHearing loss is measured in four degrees: mild, moderate, severe or profound. The degree of hearing loss drives the selection of the best form of treatment on a case-by-case basis.\nSensorineural hearing loss (SNHL)\n• The most common type of permanent hearing loss.\n• Occurs when there is damage to the inner ear (cochlea), or to the nerve pathways between the inner ear and the brain.\n• Often, SNHL cannot be medically or surgically corrected, but can usually be rehabilitated through hearing aids.\n• Speech is usually distorted, quieter and incomplete.\nConductive hearing loss\n• Occurs when the outer or middle ear structures fail to properly conduct sound waves to the inner ear.\n• Conductive hearing loss is often medically or surgically treatable.\n• Speech is still clear, but is much quieter since sound is being blocked.\nMixed hearing loss:\nMixed hearing loss refers to people who have both conductive and sensorineural hearing loss. Most people experience more than one type of hearing loss.\nCentral hearing loss:\nThis occurs when the central nervous system fails to send a readable signal to the brain, which is called a central auditory processing disorder. People with central hearing loss generally can hear all sounds, but can’t separate or process them.\nCommon Causes of Hearing Loss\nPresbycusis – Age-Related\n• Result of the aging process or extended exposure to environmental noise factors throughout a lifetime.\n• Permanent change in inner ear.\nSociocusis – Noise-Induced\n• Damage to hair cells and cochlea\n• Occurs suddenly or gradually\n• Approximately 16 million Americans between the ages of 20 and 69 have high-frequency hearing loss due to exposure to noise at work or in leisure activities. 1\nCongenital – Genetic\n• Results from a family history of hearing loss or predisposition.\nOtotoxicity – Drug-Related\n• There are 200+ known ototoxic (toxic to the ears) prescription and over-the-counter medications on the market today.2 This list includes:\no Certain antibiotics\no Some cancer treatments\no Some anesthetics\nCommon Signs of Hearing Loss\n• Hear but not understand people\n• Have difficulty understanding radio, television or public address systems\n• Need to watch a person’s lips in order to understand\n• Find yourself asking people to repeat what they have said\n• Pretend to understand\n• May avoid people and isolate yourself\n• May find it emotionally and physically exhausting to communicate\n• Think that you are the only one who has these difficulties\nHow Much Hearing Loss Warrants a Consultation?\nAny noticeable change in hearing acuities:\n• Should result in a hearing consult\n• May represent a communication problem that interferes with one’s lifestyle\n• May respond well to hearing aids and should be counseled about the benefits of amplification\n• May be medically or surgically treatable\nHearing Loss Treatments:\nThe location, type and degree of hearing loss impact the choice of treatments for any hearing problem. The most common treatment options include:\n• Antibiotics, decongestants and pain medication to overcome ear infections.\n• Myringotomy, a piercing of the eardrum to allow for fluids to drain out of the outer ear.\n• Insertion of a tube into the Eustachian tube (part of the anatomy that connects the middle ear to the back of the throat) to keep it open and allow for normal fluid drainage. This technique may be recommended for people who get frequent ear infections.\n• Hearing aids.\n• Surgery to remove benign or malignant tumors or correct bone- or nerve-related problems.\nHearing Loss Risk Factors:\nCurrent smokers have a 70% higher risk, of having hearing loss than nonsmokers.3\nthe inner ear is extremely sensitive to blood flow. Studies have shown that a healthy cardiovascular system — a person’s heart, arteries and veins— has a positive effect on hearing. Conversely, inadequate blood flow and trauma to the blood vessels of the inner ear can contribute to hearing loss.4\nthere is a significant association between high blood pressure and hearing loss. Hypertension is an accelerating factor of degeneration in hearing ability due to aging. 5\nhearing loss is about twice as common in people with diabetes compared to those without the disease. 6.\nAdults with pre-diabetes, whose blood glucose is higher than normal but not at diabetes levels, have a 30 percent higher rate of hearing loss compared to those with normal blood sugar. 7\nEffects of Untreated Hearing Loss\nMany people put off getting help for their hearing loss because they think it’s insignificant – something they can deal with by simply turning the TV louder or asking friends to repeat themselves. But research has linked untreated hearing loss to significant issues such as:\n• Adults with untreated hearing loss are more likely to develop problems thinking and remembering than adults with normal hearing.8\n• Adults with hearing loss develop impairment in their cognitive abilities, earlier than their peers with normal hearing — an average of 3.2 years sooner.9\n• People* with a hearing loss experience a 30-40% faster decline in cognitive abilities compared to peers without hearing loss.10\n• Wearing hearing aids reduces the risk of cognitive decline associated with hearing loss.11\n• Seniors with hearing loss are significantly more likely to develop dementia over time than those who retain their hearing.\n• Adults with mild hearing loss are 2x more likely to develop dementia.\n• Adults with moderate hearing\nloss are 3x more likely to develop dementia.\n• Adults with severe hearing loss are 5x more likely to develop dementia. 12\n• Tinnitus is the #1 military-service disability 13\n• Sensorineural hearing loss is by far the most common cause of tinnitus.14\n• Tinnitus affects 45 million Americans.15\n• Hearing loss results in social isolation. Adults with untreated hearing loss tend to withdraw from engaging with family and friends.16\n• Adults with untreated hearing loss are more likely to report depression, anxiety, emotional instability and paranoia.17\n• The degree of depression and other emotional or mental health issues also increases with the severity of hearing loss.18\n• People with mild hearing loss (25 dB) are 3x more likely to have a history of falling. Every additional 10 decibels of hearing loss increases the chances of falling by 1.4 times the original risk.19\n• Hearing loss negatively impacts the average household income up to $12,000 per year, depending on the severity of the loss.\n• Hearing aids mitigate the impact of hearing loss on income by an average of 50 percent, which varies by severity of loss.20\n• Adults who treat their hearing loss have higher employment rates than peers who don’t.21\n• Though all human brains become smaller with age, shrinkage is accelerated in adults with hearing loss.\n• Those with impaired hearing lose more than an additional cubic centimeter of gray matter each year.22\n• Untreated hearing loss may result in serious long-term consequences to healthy brain functioning.23\n1 National Institute on Deafness and Other Communication Disorders. (2014, March). Noise-Induced Hearing Loss.\nRetrieved from: http://www.nidcd.nih.gov/health/hearing/pages/noise.aspx\n2 Cone, B., Dorn, P., Konrad-Martin, D., Lister, J., Ortiz, C., & Schairer, K. (n.d.). Ototoxic Medications (Medication Effects).\nRetrieved from: http://www.asha.org/public/hearing/Ototoxic-Medications/\n3 Cruickshanks, K., Klien, R., Wiley, T., Nondahl, D. M., & Tweed T. S. (1998). Cigarette smoking and hearing loss: the epidemiology of hearing loss study. Retrieved from: http://www.ncbi.nlm.nih.gov/pubmed/9624024\n4 Hull, R. H. & Kerschen, S. R. (2010). The Influence of Cardiovascular health on Peripheral and Central Auditory\nFunction in Adults: A Research Review. American Journal of Audiology, 19. 9-16. Doi:10.1044/1059-0889(2010/08-0040)\n5 Agarwal, S., Mishra, A., Jagade, M., Kasbekar, V. & Nagle, S. K. (2013). Effects of Hypertension on Hearing. Indian J Otolaryngol Head Neck Surg. 2013 December; 65(Suppl 3): 614–618. Published online 2013 February 17. Retrieved from: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3889339/\n6 Diabetes and Hearing Loss (2013). Retrieved from: http://www.diabetes.org/living-with-diabetes/treatment-and-care/seniors/diabetes-and-hearing-loss.html\n7 Hearing Loss Is Common in People with Diabetes (2008). Retrieved from: http://www.nih.gov/news/health/jun2008/niddk-16.htm\n8 Lin, F. R., Yaffe, K., Xia, J., Xue, Q. L., Harris, T. B., Purchase-Helzner, E., … Simonsick, E. M. (2013). Hearing Loss and Cognitive Decline in Older Adults. For the Health ABC Study Group. Hearing Loss and Cognitive Decline in Older Adults JAMA Intern Med. 2013;173(4):293-299. doi:10.1001/jamainternmed.2013.1868\n9 Lin, F. R., Yaffe, K., Xia, J., Xue, Q., Harris, T.B., Purchase-Helzner, E., … Simonsick, E.M. (2013). Hearing Loss and Cognitive Decline in Older Adults. JAMA Internal Medicine,173(4), 293-299. doi:10.1001/jamainternmed.2013.1868\n10 Lin, F. R., Yaffe, K., Xia, J., Xue, Q., Harris, T.B., Purchase-Helzner, E., … Simonsick, E.M. (2013). Hearing Loss and Cognitive Decline in Older Adults. JAMA Internal Medicine,173(4), 293-299. doi:10.1001/jamainternmed.2013.1868\n11 Amieva, H., Ouvrard, C., Giulioli, C., Meillon C., Rullier, L., & Dartigues, J. F. (2015). Self-reported hearing loss, hearing aids, and cognitive decline in elderly adults: A 25-year study. J Am Geriatr Soc. 2015 Oct;63 (10):2099-104. doi: 10.1111/ jgs.13649.\n12 Lin, F. R., Metter, E. J., O’Brien, R. J., Resnick, S. M., Zonderman, A. B., & Ferrucci, L. (2011). Hearing loss and incident dementia. JAMA Neurology, 68(2), 214-220. doi:10.1001/archneurol.2010.362.\n13 U.S. Department of Health & Human Services, National Institutes of Health, National Institute of Deafness and Other Communication Disorders (2015). Tinnitus. National Institute on Deafness and Other Communication Disorders. Retrieved from: http://www.nidcd.nih.gov/health/hearing/pages/tinnitus.aspx\n14 American Tinnitus Association. (n. d.). Understanding the Facts. Retrieved from: https://www.ata.org/understanding- facts/causes\n15 American Tinnitus Association. (n. d.). Understanding the Facts. Retrieved from: https://www.ata.org/understanding- facts\n16 National Center for Health Statistics, Inter-university Consortium for Political and Social Research (1989). National Health Interview Survey. United States Department of Health and Human Services. Doi:http://doi.org/10.3886/ ICPSR09583.v1\n17 Reinemer, M., & Hood, J. (1999). Untreated Hearing Loss Linked to Depression, Social Isolation in Seniors. Audiology Today, 11(4). Retrieved from: http://www.audiology.org/publications-resources/document-library/untreated-hearing- loss-linked-depression-social-isolation\n18 Reinemer, M. & Hood, J. (1999). Untreated Hearing Loss Linked to Depression, Social Isolation in Seniors. Audiology Today, 11(4). Retrieved from: http://www.audiology.org/publications-resources/document-library/untreated-hearing- loss-linked-depression-social-isolation\n19 Johns Hopkins Medicine. (2012). Hearing Loss Linked to Three-Fold Risk of Falling. Retrieved from: http://www. hopkinsmedicine.org/news/media/releases/hearing_loss_linked_to_three_fold_risk_of_falling\n20 Kochkin, S. (2007). The Impact of Untreated Hearing Loss on Household Income. Retrieved from: http://www. betterhearing.org/sites/default/files/hearingpedia-resources/The%20Impact%20of%20Untreated%20Hearing%20\n21 Kochkin, S. (2010). The efficacy of hearing aids in achieving compensation equity in the workplace. Retrieved from:http://old.betterhearing.org/pdfs/hearing_aids_and_employment.pdf\n22 Johns Hopkins Medicine. (2014). Hearing Loss Linked to Accelerated Brain Tissue Loss [News Release]. Retrieved from: http://www.hopkinsmedicine.org/news/media/releases/hearing_loss_linked_to_accelerated_brain_tissue_loss_\n23 Johns Hopkins Medicine (2013). Hearing Loss Accelerates Brain Function Decline in Older Adults [News Release]. Retrieved from: http://www.hopkinsmedicine.org/news/media/releases/hearing_loss_accelerates_brain_function_']"	['<urn:uuid:2c2a9669-0188-4a42-b418-5122f34eaa76>', '<urn:uuid:d3b2e902-f026-4992-ba81-5a55259d533f>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	6	62	2641
33	What is the deadline for filing Form D with the SEC?	In a Rule 506 offering, Form D must be filed with the SEC and most states where investors are resident within 15 days of first raising money.	['There’s nothing more relaxing than a long drive right? The stereo’s bumping, you’ve got your bouncing baby startup strapped tightly in the car seat in the back, and you’re looking for ice cream, because your startup is about to startup some major crying without ice cream. If you speed, you’ll get to it quickly. If you jump the curb and cut through someone’s lawn you might get to it even faster. But all of that is illegal and dangerous. Do it even once and there’s a chance you could suffer a calamitous outcome, and above all, you’d be putting your precious baby startup in jeopardy. The same applies to private company financings. You’ve GOT to know and adhere to the “rules of the road” or all of the work you’ve done up to this point could be for not.\nThe rules for most private company financings are found under the Securities Exchange Commission’s (SEC) “Rule 506,” which dictates how individuals and startups must conduct themselves when seeking investment funds by selling securities (a share of stock, a convertible note, etc.). Running afoul of these rules can not only prevent you from raising the funds that you need, it could subject your personal assets to exposure (no corporate liability shield will be there to protect you), and in the worst case scenario, subject you to civil and criminal penalties. So, these rules can’t be taken lightly.\nThe Rules of the Road for Private Company Financings under Rule 506\nBefore you start your securities offering, you have to identify an applicable securities law exemption from registration. What does this mean? Each time your company issues a security, the company must register the securities offering with the SEC and state securities divisions with jurisdiction unless there is an applicable exemption from registration under the securities laws. Meaning, you have to identify a specific provision under both state and federal law that says that either for the type of transaction you are involved with or the type of securities you are selling, you do not have go through a registration process with the SEC or with the particular state. Registration is a very expensive process. Thus, exemptions are key.\nIn general, for most private companies, the exemption most frequently relied upon in the fund raising context is Rule 506 under federal Regulation D. Under Rule 506, companies can raise an unlimited amount of money, as long as they abide by the following “rules of the road”:\n1) General Solicitation, Know What You Are Doing – Rule 506 used to flat out prohibit the offer or sale of securities by any form of general solicitation or general advertising. Now Rule 506 is split into two rules – Rule 506(b) (the “old rule 506”) and Rule 506(c). Under old Rule 506, now known as 506(b), you still can’t generally solicit. The rule states that “neither the issuer nor any person acting on its behalf shall offer or sell the securities by any form of general solicitation or general advertising.” Advertising seems like a great way to find prospective investors, but that’s a no-no under Rule 506(b). No matter how tempting it is to post to Twitter or LinkedIn that you are raising money, don’t do it.\nUnder new Rule 506(c), you can generally solicit, but if you do, you have to take additional steps to verify that all of your investors are in fact accredited investors. The rules say that this means you have to ask them for their Forms W-2, or their personal financial statements. Your prospective investors may get upset when you ask them for this information. Or they may back out of your deal. Thus, before you generally solicit, consider carefully what you are getting yourself into. Red this blog post: 10 Things To Consider Before Generally Soliciting.\n2) Accredited investors only – One of the keys in raising money in Rule 506 offerings is to accept funds only from accredited investors. If you accept funds from only accredited investors, there is no specific information that you need to provide (no audited financials, for example). Note: that doesn’t mean you don’t have to provide information; it just means that there is no specific format for the types of information you have to provide. You’re still subject to the anti-fraud requirements of federal and state law. Here’s another important note: The rule that says that in a Rule 506(b) offering you can raise money from up to 35 non-accredited investors —don’t be misled. Keep on reading the rule. If you do, you will see that, if you accept money from any non-accredited investors (even one), you will have to provide the potential investors a voluminous amount of information. The same as in a registered offering. Read—very, very expensive. You probably don’t want to do this. So, accept money from accredited investors only.\nAccredited investors generally are:\nHigh net worth individuals – The SEC defines this individual as “a natural person” (a human being as opposed to a “legal person” such as a corporation) who has individual net worth (meaning, net of debt), or joint net worth with the person’s spouse, in excess of $1 million at the time of the purchase, not including the value of their primary residence (vacation homes can count).\nHigh income person – This is a natural person with income exceeding $200,000 in each of the two most recent years or joint income with a spouse exceeding $300,000 for those years and a reasonable expectation of the same income level in the current year.\nEntities In Which All Owners Are Accredited – This can be a business in which all of the equity owners are accredited investors.\nDirector or Executive Officers of the Company – These individuals can be a director, executive officer, or general partner of the company selling the securities.\nCertain Other Entities – Entities with assets in excess of $5 million not specifically created in order to acquire the securities offered can qualify.\n3) File Forms D – In a Rule 506 offering, you are supposed to file a Form D with the SEC and with most states in which investors are resident within 15 days of first raising money.\n4) Be careful with finders – Be careful with folks who want to charge you a fee to help you raise money. First, any such fees will have to be disclosed to investors as part of the offering. Second, the SEC and state securities regulators take a dim view of finders, especially finders who are not registered as broker/dealers (for an example, see http://www.sec.gov/news/press/2013/2013-36.htm). The SEC will want you to disclose on your Form D who you used as a finder in your financing. And certain states give investors the right to rescind investments in which unregistered finders were involved (e.g., see California’s law on finders).\n5) Beware of “integration” with other offerings – In Rule 506 offerings, you have to be careful, because all sales that are part of the same offering must meet all of the terms and conditions of Regulation D. So, if you sold stock to friends and family who were not accredited three months ago—and now you want to sell stock to investors in an all accredited investor offering—your prior stock sales to your friends and family may be “integrated” into, and disrupt, your planned all accredited offering and not allow you to rely on Rule 506.\n6) No Bad Actors – If you are a regular reader of this blog, then you may have heard the term bad actors mentioned. For a full detailed description of bad actors see the article Willful Blindness: The Rule 506 Securities Law Exemption and the New “Bad Actor” Rules. The SEC has now adopted the final rules that prohibit companies from using Rule 506 if bad actors are involved with the company. The new rules are now in effect. You can read them here: http://www.sec.gov/rules/final/2013/33-9414.pdf\nIf you are raising money under Rule 506, you now have to investigate your directors, executive officers, 20% or greater shareholders, and other officers involved in your offering. If they are “bad actors,” you have to make disclosure of this to your investors, and or you may not be able to use Rule 506 at all. Always consult counsel on your offerings.\nSo let’s recap: don’t advertise for funds unless you have carefully thought it through, file your Forms D with the SEC and state securities regulators, choose only accredited investors, be wary of finders, look out for integration, and keep your eye out for bad actors. It seems like a lot, but nobody said that there weren’t going to be potholes in the road to your dreams. We just know that swerving around them is a heck of a lot less damaging than plowing into them. Now drive safely…somebody needs some ice cream.']	['<urn:uuid:2cd46347-a0d0-43f2-99c1-3c5ca70ce2d1>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	11	27	1474
34	As someone interested in family health history, I want to understand how our genes work - what's the difference between studying single genes versus all genes together, and why is this important for understanding diseases?	Studying single genes (genetics) focuses on conditions like Huntington's or Tay-Sachs disease where one gene causes the disorder. However, studying all genes together (genomics) is crucial because most traits and diseases involve multiple genes interacting with each other and the environment. This follows the genotype-phenotype distinction, where an organism's genetic makeup (genotype) influences but doesn't solely determine its observable characteristics (phenotype). This comprehensive understanding is particularly important because 9 of the 10 leading causes of death in the U.S. have a genetic component, yet these diseases also depend on environmental factors. Family history is valuable as it reflects both genetic and environmental factors, helping identify high-risk individuals.	"['Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nBiological: Behavioural genetics · Evolutionary psychology · Neuroanatomy · Neurochemistry · Neuroendocrinology · Neuroscience · Psychoneuroimmunology · Physiological Psychology · Psychopharmacology (Index, Outline)\nThe genotype–phenotype distinction is drawn in genetics. ""Genotype"" is an organism\'s full hereditary information, even if not expressed. ""Phenotype"" is an organism\'s actual observed properties, such as morphology, development, or behavior. This distinction is fundamental in the study of inheritance of traits and their evolution.\nThe genotype represents its exact genetic makeup — the particular set of genes it possesses. Two organisms whose genes differ at even one locus (position in their genome) are said to have different genotypes. The transmission of genes from parents to offspring is under the control of precise molecular mechanisms. The discovery of these mechanisms and their manifestations started with Mendel and comprises the field of genetics.\nIt is the organism\'s physical properties which directly determine its chances of survival and reproductive output, while the inheritance of physical properties occurs only as a secondary consequence of the inheritance of genes. Therefore, to properly understand the theory of evolution via natural selection, one must understand the genotype–phenotype distinction.\nThe mapping of a set of genotypes to a set of phenotypes is sometimes referred to as the genotype–phenotype map.\nAn organism\'s genotype is a major (the largest by far for morphology) influencing factor in the development of its phenotype, but it is not the only one. Even two organisms with identical genotypes normally differ in their phenotypes. One experiences this in everyday life with monozygous (i.e. identical) twins. Identical twins share the same genotype, since their genomes are identical; but they never have the same phenotype, although their phenotypes may be very similar. This is apparent in the fact that their mothers and close friends can always tell them apart, even though others might not be able to see the subtle differences. Further, identical twins can be distinguished by their fingerprints, which are never completely identical.\nThe concept of phenotypic plasticity describes the degree to which an organism\'s phenotype is determined by its genotype. A high level of plasticity means that environmental factors have a strong influence on the particular phenotype that develops. If there is little plasticity, the phenotype of an organism can be reliably predicted from knowledge of the genotype, regardless of environmental peculiarities during development. An example of high plasticity can be observed in larval newts1: when these larvae sense the presence of predators such as dragonflies, they develop larger heads and tails relative to their body size and display darker pigmentation. Larvae with these traits have a higher chance of survival when exposed to the predators, but grow more slowly than other phenotypes.\nIn contrast to phenotypic plasticity, the concept of genetic canalization addresses the extent to which an organism\'s phenotype allows conclusions about its genotype. A phenotype is said to be canalized if mutations (changes in the genome) do not noticeably affect the physical properties of the organism. This means that a canalized phenotype may form from a large variety of different genotypes, in which case it is not possible to exactly predict the genotype from knowledge of the phenotype (i.e. the genotype-phenotype map is not invertible). If canalization is not present, small changes in the genome have an immediate effect on the phenotype that develops.\nThe terms ""genotype"" and ""phenotype"" were created by Wilhelm Johannsen in 1911.\n- J. Van Buskirk and B. R. Schmidt, ""Predator-induced Phenotypic Plasticity in Larval Newts: Trade-offs, Selection, and Variation in Nature,"" Ecology 81 (2000): 3009-3028.\nThe development of phenotype\n|Key concepts: Genotype-phenotype distinction | Norms of reaction | Gene-environment interaction | Heritability | Quantitative genetics|\n|Genetic architecture: Dominance relationship | Epistasis | Polygenic inheritance | Pleiotropy | Plasticity | Canalisation | Fitness landscape|\n|Non-genetic influences: Epigenetic inheritance | Epigenetics | Maternal effect | dual inheritance theory|\n|Developmental architecture: Segmentation | Modularity|\n|Evolution of genetic systems: Evolvability | Mutational robustness | Evolution of sex|\n|Influential figures: C. H. Waddington | Richard Lewontin|\n|Debates: Nature versus nurture|\n|List of evolutionary biology topics|\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|', '***To schedule trainings for your staff, contact us at firstname.lastname@example.org***\nFrequently Asked Questions\nWhat is the difference between genetics and genomics?\nGenetics is the study of single genes and their effects. For example, Huntington\'s or Tay-Sachs disease would be considered ""genetics"" because a single gene causes these diseases, despite environmental interactions.\nGenomics is the study of all your genes including interactions of those genes with each other and with your environment. For example, heart disease, asthma, diabetes, and cancer would all be considered ""genomics"" because they are caused by genetic and environmental factors.\nThink of it like a math problem...\nGenomics = Genes+Environment+Genes\nHow does genomics apply to public health?\nGenomics can affect everyone in public health. In fact 9 of the 10 leading causes of death in the U.S. (2001) have a known genetic component.\n*Chronic lower respiratory diseases\n*Influenza and pneumonia\nRemember that genetics is just one piece of the puzzle! Diseases can be caused by genetics, environment, or an interaction between the two. Public health wouldn\'t dream about ignoring diet or exercise in interventions for heart disease or diabetes and the same thinking should apply to the genetic component of disease. Genomics holds promise to revolutionize public health by providing us with the knowledge to identify those at high risk and target interventions specifically to them that will prevent and treat diseases more effectively.\nHow does genomics affect health?\nWe\'ve all heard stories about people who exercised their whole lives, ate a healthy diet, got regular checkups and died of a heart attack at age 40. We\'ve also heard stories about people who smoked, never exercised, ate unhealthy foods and lived to be 100. What\'s the difference between these individuals? Perhaps genomics holds the key to understanding why some genetic variations, interacting with certain environmental factors, protect us from disease while others predispose us to disease.\nIs there a tool that I can use to apply genomics in my program right now?\nYes! Family history is a genomic tool available to you right now. Family history is a reflection of both genetic and environmental factors. It is a cost-effective and efficient tool that can identify high risk individuals and populations and can also help you target interventions to those at highest risk. Read more...\nWhat are gene-environment and gene-gene interactions?\nCommon chronic diseases are the result of gene-environment interactions. This included interactions between your genes and your environment, such as pollutants in the air or water, infections, diet, exercise, weight, smoking and alcohol use, sun exposure, and even stress.\nDisease may also be caused by gene-gene interactions. For example mutations in certain genes such as the BRCA 1 and 2 genes can increase one\'s risk of breast cancer. Remember though that the majority of genetic mutations have no harmful health effects. Some diseases may also be caused by multiple ""mutated"" genes interacting with each other and the environment to predispose individuals to disease. Heart disease is an example of a complex disease that can have many different genes interacting with each other and the environment to cause disease.\nWhat should I know about genomics?\nCheck out the CDC Genomics Competencies to see what specific skills you need to know in order to apply genomics to your job.\nWhere can I learn more?\nStill confused? Email your questions to us at email@example.com or check out our educational materials below.\nGenomics and You: what is it and why now? - PowerPoint presentation adapted from the CDC Genomics for Public Health Practioners presentation.\nGenomics and You newsletter pdf\nCurrent Projects - view presentations given to a variety of programs and groups.']"	['<urn:uuid:c4a96eec-d00d-44f0-8fb1-9cd52f262271>', '<urn:uuid:dc6067af-0bec-4a5c-b982-9647e174f4f6>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T06:15:13.904987	35	107	1299
35	programmed cell death plants benefits agriculture regulation differences intragenic transgenic organisms	Programmed cell death (PCD) in plants offers several agricultural benefits, including pest resistance and higher-quality, longer-lasting fruit. PCD helps plants defend against bacterial and fungal pathogens by initiating cell death near infection sites, and it also controls fruit ripening timing. Regarding regulation, while intragenics and transgenics are both genetically modified organisms, they differ in that transgenics contain foreign DNA fragments while intragenics do not. Some regulatory systems, like Australia's, explicitly exclude intragenics from GMO regulation, though this exemption is debated as intragenics may still pose significant environmental risks and should be subject to the same regulatory standards as other GMOs.	"['Programmed cell death in plants can help tackle increased food needs\nProgrammed cell death (PCD) plays a key role in plant health, with the resulting socioeconomic impact. PCD research is a relatively new field and will bring more pest-resistant plants and, in the long term, higher-quality, longer-lasting fruit.\n""We have to increase efforts in basic research to transform it into practical applications as soon as possible,” said Nuria Sánchez Coll and Marc Valls, researchers at the Center for Research in Agricultural Genomics (CRAG) and scientific leaders of the B·Debate scientific debates The death of plant cells: From proteases to field applications held in Barcelona from 2 to 4 October. More than 50 international experts participated in this event to debate the current state of research in this field and its future perspectives and impact on industry and agriculture.\nAccording to predictions from the United Nations, the human population will have increased 13% in the next 15 years and, as a result, so will our food needs. In order to provide for the world population, we must find new ways to protect the most important food species from disease and maximize their yield.\nCrop productivity can be drastically affected by pests. Having crops that not only grow better in healthy conditions, but are also able to fight fungal and bacterial pathogens has become a priority. PCD is key in this fight, as plants defend themselves against a bacterial or fungal attack by initiating a program of cell death in cells near the focus of the infection, thus eliminating the organisms and preventing them from spreading throughout the plant. Understanding these mechanisms on a molecular level is key to effectively fighting pests.\nWeeds can also decrease crop productivity in plants used for food. Some pharmaceutical companies are currently researching PCD to obtain specific herbicides. Their research consists in finding and studying the proteins involved in PCD in weeds in order to make herbicides that directly attack these molecules and thus eliminate, in a programmed manner, these plants. The pesticide-production industry is interested in basic research precisely to learn about these pathways.\nObtaining higher-quality, longer-lasting fruit\nWe know that the process through which fruit separates from the plant once ripe is controlled by PCD. As long as the fruit is attached to the plant, it receives nutrients from the plant that make it sweeter and of higher quality. According to Dr. Sánchez Coll, “knowing the mechanisms of PCD and being able to activate or inhibit them as we please will allow us to control fruit-ripening times.” More accurate knowledge of the process of programmed cell death can, thus, lead to higher-yield crops.\nNew technology to avoid transgenic crops\nIn order to achieve the aforementioned improvements, plants must be genetically modified. We must differentiate between genetically modified organisms (GMO) and transgenic ones. All transgenic organisms have been genetically modified but not all GMOs are transgenic. The difference lies in the fact that the latter include a foreign DNA fragment. According to Dr. Sánchez Coll “research into PCD was halted because transgenic plants are banned in Europe.” It is done through mutation of one of the plant’s genes, not by introducing foreign genetic material. This technique is similar in terms of invasiveness to traditional crossbreeding of agricultural varieties to obtain more resistant plants.\nFollow the debate on Twitter @BDebate with the hashtag #BDebate', ""David Bourget (Western Ontario)\nDavid Chalmers (ANU, NYU)\nRafael De Clercq\nJack Alan Reynolds\nLearn more about PhilPapers\nJournal of Agricultural and Environmental Ethics 21 (2):153-181 (2008)\nThis paper discusses the ethical and regulatory issues raised by “intragenics” – organisms that have been genetically modified using gene technologies, but that do not contain DNA from another species. Considering the rapid development of knowledge about gene regulation and genomics, we anticipate rapid advances in intragenic methods. Of regulatory systems developed to govern genetically modified organisms (GMOs) in North America, Europe, Australia, and New Zealand, the Australian system stands out in explicitly excluding intragenics from regulation. European systems are also under pressure to exclude intragenics from regulation. We evaluate recent arguments that intragenics are safer and more morally acceptable than transgenic organisms, and more acceptable to the public, which might be thought to justify a lower standard of regulation. We argue that the exemption of intragenics from regulation is not justified, and that there may be significant environmental risks associated with them. We conclude that intragenics should be subject to the same standard of regulation as other GMOs.\n|Keywords||consumers environment ethics genetically modified organisms intragenics nature regulation safety|\n|Categories||categorize this paper)|\nSetup an account with your affiliations in order to access resources via your University's proxy server\nConfigure custom proxy (use this if your affiliation does not provide a proxy)\n|Through your library|\nReferences found in this work BETA\nChristoph Baumgartner (2006). Exclusion by Inclusion? On Difficulties with Regard to an Effective Ethical Assessment of Patenting in the Field of Agricultural Bio-Technology. Journal of Agricultural and Environmental Ethics 19 (6):521-539.\nDonald Bruce (2002). Finding a Balance Over Precaution. Journal of Agricultural and Environmental Ethics 15 (1):7-16.\nLawrence Busch (2002). The Homiletics of Risk. Journal of Agricultural and Environmental Ethics 15 (1):17-29.\nRogeer Hoedemaekers (2001). Commercialization, Patents and Moral Assessment of Biotechnology Products. Journal of Medicine and Philosophy 26 (3):273 – 284.\nKathrine Hauge Madsen, Preben Bach Holm, Jesper Lassen & Peter Sandøe (2002). Ranking Genetically Modified Plants According to Familiarity. Journal of Agricultural and Environmental Ethics 15 (3):267-278.\nCitations of this work BETA\nHenrik Mielby, Peter Sandøe & Jesper Lassen (2013). Multiple Aspects of Unnaturalness: Are Cisgenic Crops Perceived as Being More Natural and More Acceptable Than Transgenic Crops? [REVIEW] Agriculture and Human Values 30 (3):471-480.\nSimilar books and articles\nSoemini Kasanmoentalib (1996). Science and Values in Risk Assessment: The Case of Deliberate Release of Genetically Engineered Organisms. [REVIEW] Journal of Agricultural and Environmental Ethics 9 (1):42-60.\nMargaret Ann Griesse (2007). Developing Social Responsibility: Biotechnology and the Case of DuPont in Brazil. [REVIEW] Journal of Business Ethics 73 (1):103 - 118.\nDiana M. Bowman & Graeme A. Hodge (2008). A Big Regulatory Tool-Box for a Small Technology. NanoEthics 2 (2):193-207.\nE. Ann Clark & Hugh Lehman (2001). Assessment of GM Crops in Commercial Agriculture. Journal of Agricultural and Environmental Ethics 14 (1):3-28.\nDane Scott (2005). The Magic Bullet Criticism of Agricultural Biotechnology. Journal of Agricultural and Environmental Ethics 18 (3):189-197.\nKristian Toft (2012). GMOs and Global Justice: Applying Global Justice Theory to the Case of Genetically Modified Crops and Food. [REVIEW] Journal of Agricultural and Environmental Ethics 25 (2):223-237.\nMikael Karlsson (2003). Ethics of Sustainable Development – a Study of Swedish Regulations for Genetically Modified Organisms. Journal of Agricultural and Environmental Ethics 16 (1):51-62.\nJ. Robert Loftis (2007). The Other Value in the Debate Over Genetically Modified Organisms. Journal of Philosophical Research 32 (Supplement):151-162.\nBjørn K. Myskja (2006). “The Moral Difference Between Intragenic and Transgenic Modification of Plants”. Journal of Agricultural and Environmental Ethics 19 (3):225-238.\nAdded to index2009-01-28\nTotal downloads25 ( #78,069 of 1,413,414 )\nRecent downloads (6 months)1 ( #154,636 of 1,413,414 )\nHow can I increase my downloads?""]"	['<urn:uuid:7cdf8866-b795-4c08-b369-f847aaeb6a4c>', '<urn:uuid:66ebfc71-5027-466f-9e22-f417dd5e4145>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:15:13.904987	11	100	1177
36	What was the location of General Curtis's headquarters during the Civil War?	Federal Gen. Samuel R. Curtis's headquarters was located at the Harris House Hotel, at Westport Road and Pennsylvania Avenue.	"[""A Civil War History Lesson\nCivil War re-enactments don't always happen in front of you. They are something you can imagine as you look back during a self-guided driving tour through modern-day Kansas City. There are plenty of historic sites and historic buildings in Kansas City to expose you to a mini Civil War history lesson.\nEstablished in the 1970s, this 32-mile tour leads visitors past several historic buildings and through the streets of Westport and the historic Country Club Plaza; to Jacob L. Loose Memorial Park, where much of the actual Battle of Westport took place; continuing to the Battle of Westport Visitors Center, in Swope Park, where you occasionally can witness real re-enactments.\nThe tour includes a section of the Marlborough Heights and Red Bridge areas, and concludes at 96th Terrace and Wornall Road, in south Kansas City.\nHistoric Sites from Mile One On Most visitors begin the route at Marker One, located at Westport Road and Pennsylvania Avenue, in the historic Westport District. The marker stands among plenty of historic buildings, at the the site of Federal Gen. Samuel R. Curtis’s headquarters in the Harris House Hotel, across the street from Kelly’s Westport Inn Irish Pub (the oldest structure still standing in Kansas City). Westport is a haven of restaurants, bars and nightspots.\nFrom Westport, the tour follows Broadway, past Mill Creek Park and the J.C. Nichols Fountain, into the world famous Country Club Plaza shopping, entertainment and dining district.As you envision Civil War re-enactments, the trail follows majestic Ward Parkway before ascending the rugged bluffs of Sunset Hill, which rises up from Brush Creek toward Loose Park. Originally the site of the Kansas City Country Club (from which the Country Club Plaza gets its name), the land was purchased by Loose; following his death in 1923, his wife, Ella, donated the 80-acre tract to the people of Kansas City.\nSites in Loose Park include markers depicting the respective positions of Union and Confederate troops during the battle, as well as a row of artillery at the south end of the park, along 55th Street.\nHead South to More Historic Sites and Visualize Civil War Reenactments\nTraveling south from Loose Park, more historic sites come into your view, including the John B. Wornall House, which was used as a field hospital and often struck by artillery shells. Wornall, who owned slaves but did his best to maintain neutrality, was the victim of numerous raids and robberies by both pro-Union and pro-secessionist forces prior to and during the war. At one point, bushwhackers attempted to hang Wornall from the balcony of his 1858 Greek Revivalist mansion.\nHistoric Sites in Brookside Are Next.From Wornall House, the tour heads east through the heart of Brookside, a suburban neighborhood surrounding a central shopping district built in the 1910s and 1920s, to Forest Hill Cemetery, site of Gen. Shelby's Last Stand, which today is marked by a monument. Many of Shelby’s troops are buried there. From Forest Hill, the tour comes to the Battle of Westport Visitor Center at the west entrance to Swope Park.\nMore Ways to Visualize Civil War Re-Enactments in MissouriFor information about events commemorating the Battle of Westport, contact the Westport Historical Society online; you may call 816-561-1821.\nThe Battle of Westport is just one of many Civil War tours in Missouri. See more historic sites and historic buildings from Missouri Civil War sites of note, including: Wilson’s Creek National Battlefield, in Republic, near Springfield, and the Battle of Lexington. A collection of Missouri Civil War-related tours and events is available online, at the Missouri Civil War 150 website.A map of this route is available on the Battle of Westport Visitors Center's website.""]"	['<urn:uuid:ae042533-cbc4-4038-a9a8-f7a5703a6d32>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	12	19	615
37	What happens to the picture quality when using very strong magnets in medical imaging machines, like those above 7 tesla?	Ultrahigh field MRI (7 T and higher) provides several advantages in image quality: a higher signal-to-noise ratio (SNR), better image resolution, and improved image contrast.	['Ultrahigh field (i.e., 7 T and higher) MRI could provide a higher signal-to-noise ratio (SNR), better image resolution, and improved image contrast (1-3). As the Larmor frequency increases with magnetic field strength, the resultant high frequency at ultrahigh magnetic fields makes the design of large-sized radio-frequency (RF) coils challenging. To address this problem, a variety of RF coil arrays have been proposed and developed, including the L/C loop arrays (4-6), microstrip transmission line arrays (7-12) and radiative arrays (13-18). Among them, radiative coil arrays, e.g., dipole or monopole arrays, demonstrate their unique performance in design simplicity and large imaging penetration and coverage, and are increasingly used for ultrahigh field MRI.\nMinimizing the electromagnetic (EM) coupling among coil elements is critical to RF coil array designs, given that better decoupling could lead to higher SNR and better parallel imaging performance. In telecommunication applications, a large amount of studies have been focused on reducing the coupling of monopole and dipole antennas. In references (19,20), different kinds of metamaterials were applied to decouple two nearby monopoles. Above from that, hybrid couplers using L/C network or microstrip line approach (21,22) have been used to achieve port decoupling for monopole or dipole antennas. All these methods could suppress the coupling effect, but suffer from complicated structures and might not be suitable for MRI applications.\nIn reference (23), a decoupling structure using a phase shifter and a bridge capacitor was successfully implemented to diminish the coupling between two printed monopoles. In MRI applications, tunable decoupling networks are preferred since different imaging samples or loads might cause the change of mutual coupling, consequently the fine adjustment of decoupling circuit is often needed. Since phase shifters are not easy to adjust in practice, this approach might not be suitable in MRI applications. In this study, we proposed a combined decoupling and matching network (DMN) for radiative coil arrays in ultrahigh field MRI, providing a convenient approach to feed and decouple radiative RF arrays.\nFigure 1 shows the circuit diagram of the proposed DMN for antenna arrays in MRI applications. The DMN was accomplished by an interconnecting reactive element (Xc), two parallel reactive elements (Xp), and two series reactive element (Xs). The reactive element is a capacitor or inductor in practice.\nSince Xc affects the odd but not the even mode, it can be used to make the odd mode impedance approach to the even mode impedance. In some special cases where the coupling is totally reactive, the even and odd mode impedance at the new ports can be equal to each other, which means that the new ports are decoupled. In general cases, a more complex network, e.g., T-shaped or π-shaped network, is needed for decoupling (24). In this study, Xp was partly used to form a π-shaped network with Xc for port decoupling, and partly used to form an L-shaped network with Xs for port matching.\nIn order to verify the proposed design, a 2-channel monopole array with DMN is numerically computed using ANSYS HFSS, as shown in Figure 2. The width and length of each monopole element are 1 cm and 25 cm, respectively. The distance of the two monopole elements is about 6 cm. A cylindrical water phantom with an outer diameter of 16 cm and a length of 30 cm is placed 2 cm below the monopole elements. The EM parameters of the water phantom are set as follows: conductivity σ =0.59 s/m; relative permittivity; εr =78.\nFor comparison, we also simulated a 2-channel monopole array without decoupling treatments and a single monopole. Values of all reactive components were obtained by RF circuit co-simulation method (25,26). The operate frequency is 297.2 MHz, which is the Larmor frequency of our unitized 7T MRI scanner.\nFigure 3 depicts the frequency response of the magnitudes of the S-parameters, with S11 and S21 representing reflection and mutual coupling, respectively. The values of reactive components used are as follows: Xc =6.6 pF, Xs =13.5 pF, Xp =71.9 nH. Figure 3A,C show the S-parameter of two coupled monopoles. As can be noticed, mutual coupling between the monopoles is nearly −5 dB at desired frequency when both monopoles are matched. With the proposed DMNs, ports 1 and 2 are well decoupled (−24.8 dB) and matched to 50 ohm (Ω) simultaneously, as shown in Figure 3B,D. It is worth noting that the S11 bandwidth of decoupled monopoles is rather small. This is partly due to the changing self-impedance, and partly due to the changing radiation pattern of the monopoles.\nWe also evaluated the normalized noise matrixes (Ψ) of the two arrays by calculating Eq.  and  (27), where Ekm is the local electric field of voxel k from channel m, σk is the local conductivity of voxel k, ∆x, ∆y and ∆z are the voxel size in x, y, and z directions. In this study, the voxel size for noise matrix calculation in x, y, z directions is 2, 2 and 5 mm, respectively. The noise correlation of the monopole arrays without decoupling treatments and with the proposed DMN are 0.41 and 0.15, respectively.\nFigure 4 shows the current distribution of two monopole elements when only port 1 was excited with 1W power. Figure 4A shows the current distribution of the two close-spaced monopoles without decoupling treatments. Arrows in red color indicate the current directions along the monopole elements. It is obvious that the current flows into port 2 and the right monopole, leading to strong EM coupling between two ports. When the decoupling network was added, however, the induced current becomes almost zero at the feed-point of the right monopole, as shown in Figure 4B. This has also been validated by S-parameter results as described above.\nFrom Figure 4B, it is also seen that not the monopole elements but the ports are decoupled. That means both monopole elements are excited when only one port is fed. The current distribution of the two-element array is characterized by “large” currents flowing in opposite directions. Similar results have also been observed in previous study (28).\nElectronic and magnetic field distribution\nFigure 5 shows the H field, E field and transmit field (B1+) on the water phantom in the transverse plane of single monopole antennas (A1-A6), two coupled monopole antennas (B1-B6) and two decoupled monopole antennas (C1-C6). In the simulation, one port was excited with 1W with the other port terminated with 50 Ω. B1+ field was extracted from simulation by Eq.  (29).\nAs expected, part of the power was transferred to the other monopole element when no decoupling treatments were used, as shown in Figure 5A,B. The field result is consistent with the S21 result as described above (about −5 dB). It is worth noting that the electromagnetic fields of decoupled monopole elements are still different from that of single monopole even though the S21 is as low as −24.8 dB, as shown in Figure 5A,C. This also indicates that the proposed method is a port decoupling method rather than an element decoupling method.\nDiscussion and conclusions\nIn summary, by applying the proposed L/C network, the port isolation between two monopoles can be improved from −5 dB to −25 dB meanwhile excellent matching performance can also be attained. In the conventional approaches, decoupling network and matching network are separated which increases the circuit complexity. In this study, we utilized a combined DMN and thus the number of required components can be minimized. Since less number of tuning components is required, this design is advantageous to MRI applications.\nIt is noted that the proposed method is a port decoupling approach rather than an element decoupling approach. Therefore, the electric and magnetic fields of the decoupled monopole are different from those of single monopoles. This feature can be seen from Figure 4 that the two monopole elements with decoupling network have a similar current distribution as the loop coil, with currents on two conductors flow in different directions. This makes the H field and B1 field strong at the peripheral area on the phantom, as shown in Figures 5C1-C6.\nAlthough this concept was exclusively presented for the particular case of a 2-channel monopole array, it can be generalized to monopole arrays with more channels, i.e., eight channels. Also, the idea can be extended to dipole arrays in MRI. In the case of the dipole antenna, the DMN might be different from the monopole antenna at certain level because of its enlarged self-impedance over that of the monopole antenna.\nFunding: This study was supported in part by the National Natural Science Foundation of China Grant (51228702) and National Institutes of Health (NIH) R01EB008699.\nConflicts of Interest: The authors have no conflicts of interest to declare.\n- Yacoub E, Shmuel A, Pfeuffer J, Van De Moortele PF, Adriany G, Andersen P, Vaughan JT, Merkle H, Ugurbil K, Hu X. Imaging brain function in humans at 7 Tesla. Magn Reson Med 2001;45:588-94. [PubMed]\n- Vaughan JT, Garwood M, Collins CM, Liu W, DelaBarre L, Adriany G, Andersen P, Merkle H, Goebel R, Smith MB, Ugurbil K. 7T vs. 4T: RF power, homogeneity, and signal-to-noise comparison in head images. Magn Reson Med 2001;46:24-30. [PubMed]\n- Collins CM, Smith MB. Signal-to-noise ratio and absorbed power as functions of main magnetic field strength, and definition of “90 degrees” RF pulse for the head in the birdcage coil. Magn Reson Med 2001;45:684-91. [PubMed]\n- Wiggins GC, Potthast A, Triantafyllou C, Wiggins CJ, Wald LL. Eight-channel phased array coil and detunable TEM volume coil for 7 T brain imaging. Magn Reson Med 2005;54:235-40. [PubMed]\n- Avdievich NI. Transceiver-Phased Arrays for Human Brain Studies at 7 T. Appl Magn Reson 2011;41:483-506. [PubMed]\n- Yan X, Zhang X, Feng B, Ma C, Wei L, Xue R. 7T transmit/receive arrays using ICE decoupling for human head MR imaging. IEEE Trans Med Imaging 2014;33:1781-7. [PubMed]\n- Zhang X, Ugurbil K, Chen W. Microstrip RF surface coil design for extremely high-field MRI and spectroscopy. Magn Reson Med 2001;46:443-50. [PubMed]\n- Zhang X, Ugurbil K, Sainati R, Chen W. An inverted-microstrip resonator for human head proton MR imaging at 7 tesla. IEEE Trans Biomed Eng 2005;52:495-504. [PubMed]\n- Adriany G, Van de Moortele PF, Wiesinger F, Moeller S, Strupp JP, Andersen P, Snyder C, Zhang X, Chen W, Pruessmann KP, Boesiger P, Vaughan T, Uğurbil K. Transmit and receive transmission line arrays for 7 Tesla parallel imaging. Magn Reson Med 2005;53:434-45. [PubMed]\n- Jasiński K, Młynarczyk A, Latta P, Volotovskyy V, Węglarz WP, Tomanek B. A volume microstrip RF coil for MRI microscopy. Magn Reson Imaging 2012;30:70-7. [PubMed]\n- Metzger GJ, Snyder C, Akgun C, Vaughan T, Ugurbil K, Van de Moortele PF. Local B1+ shimming for prostate imaging with transceiver arrays at 7T based on subject-dependent transmit phase measurements. Magn Reson Med 2008;59:396-409. [PubMed]\n- Yan X, Pedersen J, Wei L, Zhang X, Xue R. Multi-channel double-row transmission line array for human MR imaging at ultrahigh fields. IEEE Trans Biomed Eng 2015;62:1652-59. [PubMed]\n- Raaijmakers AJ, Ipek O, Klomp DW, Possanzini C, Harvey PR, Lagendijk JJ, van den Berg CA. Design of a radiative surface coil array element at 7 T: the single-side adapted dipole antenna. Magn Reson Med 2011;66:1488-97. [PubMed]\n- Hong SM, Park JH, Woo MK, Kim YB, Cho ZH. New design concept of monopole antenna array for UHF 7T MRI. Magn Reson Med 2014;71:1944-52. [PubMed]\n- Yan X, Xue R, Zhang X. A monopole/loop dual-tuned RF coil for ultrahigh field MRI. Quant Imaging Med Surg 2014;4:225-31. [PubMed]\n- Yan X, Zhang X, Wei L, Xue R. Magnetic wall decoupling method for monopole coil array in ultrahigh field MRI: a feasibility test. Quant Imaging Med Surg 2014;4:79-86. [PubMed]\n- Yan X, Wei L, Xue R, Zhang X. Hybrid monopole/loop coil array for human head MR imaging at 7T. Appl Magn Reson 2015;46:541-50. [PubMed]\n- Yan X, Zhang X, Wei L, Xue R. Design and Test of Magnetic Wall Decoupling for Dipole Transmit/Receive Array for MR Imaging at the Ultrahigh Field of 7T. Appl Magn Reson 2015;46:59-66.\n- Ferrer PJ, González-Arbesú JM, Romeu J. Decorrelation of two closely spaced antennas with a metamaterial AMC surface. Microw Opt Technol Lett 2008;50:1414-7.\n- Bait-Suwailam MM, Boybay MS, Ramahi OM. Electromagnetic Coupling Reduction in High-Profile Monopole Antennas Using Single-Negative Magnetic Metamaterials for MIMO Applications. IEEE Trans Antennas Propag 2010;58:2894-902.\n- Bhatti RA, Soongyu Y, Seong-Ook P. Compact Antenna Array With Port Decoupling for LTE-Standardized Mobile Phones. IEEE Antennas Wirel Propag Lett 2010;8:1430-3.\n- Yantao Y, Hon Tat H. Design of a Mutual Coupling Compensation Network for a Small Receiving Monopole Array. IEEE Trans Microw Theory Tech 2011;59:2241-5.\n- Shin-Chang C, Yu-Shin W, Shyh-Jong C. A Decoupling Technique for Increasing the Port Isolation Between Two Strongly Coupled Antennas. IEEE Trans Antennas Propag 2008;56:3650-8.\n- Cheng-Hsun W, Guan-Ting Z, Yi-Lung W, Tzyh-Ghuang M. Stub-Loaded Reactive Decoupling Network for Two-Element Array Using Even–Odd Analysis. IEEE Antennas Wirel Propag Lett 2013;12:452-5.\n- Kozlov M, Turner R. Fast MRI coil analysis based on 3-D electromagnetic and RF circuit co-simulation. J Magn Reson 2009;200:147-52. [PubMed]\n- Yan X, Ma C, Shi L, Zhuo Y, Zhou XJ, Wei L, Xue R. Optimization of an 8-Channel Loop-Array Coil for a 7 T MRI System with the Guidance of a Co-Simulation Approach. Appl Magn Reson 2014;45:437-49.\n- Cao Z, Park J, Cho ZH, Collins CM. Numerical evaluation of image homogeneity, signal-to-noise ratio, and specific absorption rate for human brain imaging at 1.5, 3, 7, 10.5, and 14T in an 8-channel transmit/receive array. J Magn Reson Imaging 2015;41:1432-9. [PubMed]\n- Chaloupka HJ, Esser D, Wang X. Port decoupling for antennas with narrow element spacing. 7th European Conference on Wireless Technology. Amsterdam, The Netherlands, 2004:221-4.\n- Hoult DI. The principle of reciprocity in signal strength calculations—A mathematical guide. Concepts in Magn Reson 2000;12:173-87.']	['<urn:uuid:2d40cc32-412a-483f-baef-cf564ea9f5a9>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	20	25	2280
38	I'm new to electronics - what simple parts do I need to check soil moisture?	You'll need: a micro:bit, a speaker with 3.5mm headphone jack, 5 crocodile clips, 3 female to male jumper wires, a soil moisture sensor, and either a USB power bank or micro:bit AAA battery pack. The sensor works by receiving 3V power from the micro:bit and measuring voltage conducted through the soil between two sensor tips, converting it to a value from 0 (no water) to 1023 (saturated).	"['Use micro:bit to water your plants!\nWhat will you need for this project?\n- Any computer with an Internet connection\n- A micro:bit\n- A speaker with 3.5mm headphone jack\n- 5 x Crocodile clips\n- 3 x Female to Male Jumper Wires (Du Pont connectors)\n- A Soil Moisture Sensor\n- A USB power bank or micro:bit AAA battery pack.\n- A plant\nAll of the code for this project as well as circuit diagrams can be found at\nYou can also try out the code in the editor.\nContinuing from our previous issue, we continue our look at the micro:bit. In this issue we shall create a plant protector. A device that will measure the moisture of our soil and alert us when it drops below a certain level. To do this we shall use a sensor to constantly monitor the conductivity of the soil. Wet soil is a good conductor of electricity and will mean that our plant is happy. Dry soil is a poor conductor and our plant protector invention will alert us to this.\nRemember that water and electricity don’t mix. NEVER submerge your micro:bit, sensor of battery pack in water as this will lead to damage. Also do not water the plant while the plant protector is in the soil.\nBy completing this project you will learn more about\n- To repeat portions of code\n- Measuring conductivity of soil based on its moisture.\n- Using the screen to visually indicate the state of the soil.\n- Using a “heartbeat” to alert the user that the plant requires water.\n- Learn how water affects the conductivity of soil.\nBuilding the Circuit\nOur Soil Moisture Sensor can be found on various online auction sites and via sellers such as Adafruit. The sensor receives 3V and GND from the micro:bit, effectively providing the sensor with power. This is then passed from one tip of the sensor, through the soil, to a receiving tip, which then sends the voltage as an output to our micro:bit. This is then read on Pin 1 and converted to a value of 0 to 1023, where 0 means that the plant has no water, and 1023 the plant is saturated with water. Please refer to the circuit diagram and the photo which shows how the wires are connected.\nOur headphones require two croc clips. Our black croc clip connects to the base of the headphone jack, and then it connects to GND on the micro:bit. Out white croc clip connects to the tip of the headphone jack and then to Pin0 of our micro:bit.\nGetting Started PXT\nTo code our project we shall open a web browser and visit https://pxt.microbit.org and we start with a blank project.\nThe pxt editor is block based, meaning that we click on the block menu, just left center of the screen. To open a menu, click on it and you will see the blocks that it contains. This will show an expanded window that will briefly lie over the coding area, to the right of the screen. Blocks can be moved from the menu to the coding area and connected in the same manner as with Scratch.\nOn the left of the screen we have an interactive micro:bit simulator that enables us to test our code before we deploy it to the real hardware. Very handy!\nThe Block Menu\nWe start the code for this project by first clearing the “on start” block from the coding area, drag the block to the menu and drop it to tidy it away. We shall write our code inside the “forever” loop and our first task is to create a Variable. So go to Variables and click on “Make a Variable”. Call the variable “soil” and then drag the block “set soil to 0” and place it inside the loop. If you can’t see this block, it might be called “set item to 0”, use that block and click on the dropdown arrow to change the variable to “soil”. Now we need to store some data inside the “soil” variable. In this case the variable is used to store the conductivity of our soil, this is a value from 0 to 1023, which represent the voltage conducted via the soil.\nOur sensor is connected to Pin 1 of the micro:bit, so to access the pin click on “Advanced” and then find the “Pins” menu. From there drag the “analog read pin P0” and place it over the zero of “set soil to 0”. Now change the pin to P1, this means that the value being read by the sensor will be stored in our variable, “soil”.\nIn order to check the value given by the sensor against what we consider to be “fairly dry” and “very dry” we need to use a conditional test call “if”. Go to the “Logic” menu and drag the “if true, then” block and place it inside the “Forever loop” and underneath our previous code. Now you can see a blue “cog” in the top left of this block. Click on it and you will see if, else if and else blocks. Assemble them so that we have from top to bottom, “if”, “else if” and “else”. To close the pop up click on the cog.\nWe now need to use an “ __ and __” block from “Logic”, drag and place this over the “true” of our conditional test. In the first blank space of “__ and __” we shall go back to “Logic” and drag a “0 < 0” block and place it in the blank. This block tests to see if the value in the first blank is less than the value in the second blank. Our first value will be the data stored in our “soil” variable, so go ahead and get that block and place it in the first blank. In the second blank we need to set a limit, in this case we typed the value “500” as this is roughly half the value of conductivity, meaning our soil may be a little dry. Now grab another “0 < 0” block and place that in the other blank space or our “and” block. Again the first value is our “soil” variable, but remember to change the < to a > symbol as we need to check that the soil moisture, and its conductivity is between 200 and 500 for this condition to be evaluated as True. In the last blank space type “200” to set the lower limit.\nIf the soil moisture reports to be between 200 and 500 then the “if” condition is “True” and the code inside the condition is activated. In this case we need to show the user that the plant is unhappy, we use the “show icon” block from “Basic” and set it to “sad” We also go to the “Music” menu and drag “play tone Middle C for 1 beat” so that it connects under the previous block. This will play a slow “heartbeat” indicating the plant requires water.\nFor the “else if” condition we shall once again use the “0 < 0” block from the “Logic” menu, but this time we only compare the value inside our “soil” variable with < 200, this means that the plant is in dire need of water. If this condition is True, then we use another “show icon” block from “Basic” and set it to “angry” and we also use a “play tone Middle C for 1 beat” block from “Music” but we change the tempo to “½ beat” to quicken the heartbeat.\nFor the final “else” condition we do not need to set any values to compare against. Else is used in case all of the previous conditions are False. If all are False, then Else MUST be True. In this case we use it to show that the plant is healthy and well watered. We use the “show icon” block from Basic, and give the plant a happy face. With this final block we now have all of the code for this project. So now click on Download and then connect your micro:bit to your computer. Copy the file from your Downloads folder to the micro:bit and this will cause it to reboot and then run the code.\nYou can now disconnect the micro:bit from your computer and power it using a USB power bank or the official micro:bit AAA battery pack. Place the sensor into the soil and listen to the health of your plant!\nCongratulations you have completed! During these projects we learnt that\n- Soil when wet is a good conductor of electricity.\n- We can connect sensors to the micro:bit and measure the environment around us.\n- Creating an invention does not require lots of code or expensive equipment.\n- Plants need lots of water to grow.\nHow could we take this further?\nfrom microbit import * import music while True: soil = pin1.read_analog() if soil < 500 and soil > 200: display.show(Image.SAD) music.play(""C4:1"") sleep(500) elif soil < 200: display.show(Image.ANGRY) music.play(""C4:4"") sleep(500) else: display.whos(Image.HAPPY)\nDare you take on the challenge that is Python? In the download for this project we have included the code to recreate the plant protector project using Micro Python. To use this code with your micro:bit you will need to go to http://python.microbit.org/ and write the code. Then plug in your micro:bit and click on Download to download the code, copy the file to your micro:bit and it will reboot and run the plant protector code. There is no need to change the wiring of the project.\nThis Project on Github\nMicro:bit Foundation Official Website\nAlso In The October 2017 Issue\nBuilding and creating your tools with the Minecraft toolbox helps you survive the game.\nSensors give robots the senses humans have.\n30+ ideas for all age holiday gifts, from books to apps to board games to VR and more.\nThere might be a reason that too-real robot and video game character creeps you out.\nYou computer mouse cursor finally gets a cat to chase it.\nCreate turtles with Python, the programming language.\nThis programming language uses colors instead of text and punctuation to add and perform other tasks.\nUse micro:bit to water your plants!\nKnowing how passwords are cracked can help you create better passwords.\nStudying satellite photographs shows a lot about what happens in the world.\nThere are a number of strategies teachers (plus parents and students) can take to learn programming.\nPigpen ciphers uses alien-like symbols to hide secret messages.\nThis project uses conductive thread to create a glove to activates your phone.\nThe repl.it website and React are one way to create mobile apps.\nSoftware programming does neat things with language, in this case, mixing capital letters.\nThis Scratch game has lots of ways you can customize the game play. No cats were harmed in the making of this article either.\nLinks from the bottom of all the October 2017 articles, collected in one place for you to print, share, or bookmark.\nInteresting stories about computer science, software programming, and technology for October 2017.']"	['<urn:uuid:7801a300-9c60-40c3-8e91-003a4753e716>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	15	67	1854
39	I'm doing research on industrial cleaning methods. What's the main way copper slag is used for cleaning things?	Copper slag is mainly used for surface blast-cleaning. In this process, a stream of abrasive grains called grit are propelled toward the workpiece to clean and shape the surface of metal, stone, concrete and other materials.	"[""The details of the copper-refining process depend on the type of minerals the copper is bound with , The matte layer, on the bottom, contains the copper, while the slag layer, on the top .\nCopper slag is an industrial by-product material produced during the process of manufacturing of copper About 22 tonnes of copper slag is generated, for every ton of copper production\ndemands further processing of the slag Processing of such, copper rich slag, is facilitated using flotation or electro-precipitation of remaining sulfides resulting with about 05% Cu in the remaining waste slag [2, 3] Those copper losses in the waste slag are strongly influencing the economy of the copper extraction process Also, it is a\nCopper slag is a by-product created during the copperÂ smeltingÂ and refining process As refineries draw metal out of copper ore, they produce a large volume of non-metallic dust, soot, and rock Collectively, these materials make up slag, which can be used for a surprising number of applications in the building and industrial fields\nRecovery of metal values from copper slag and reuse of residual secondary slag Article in Waste Management 70 October 2017 with 102 Reads DOI: 101016/jwasman201709024\nDescription: Copper slag is a by-product produced during smelting of copper, which is then processed into abrasive products Typically it is composed of Ferro silicate material and oxides formed when the molten slag is quenched in cold water\nJul 15, 2019· Copper slag is a by-product created during the copper smelting and refining process As refineries draw metal out of copper ore, they produce a large volume of non-metallic dust, soot, and rock Collectively, these materials make up slag, which can be used for a surprising number of applications in the building and industrial fields\nCopper slag is mainly used for surface blast-cleaning Abrasive blasting is used to clean and shape the surface of metal, stone, concrete and other materials In this process, a stream of abrasive grains called grit are propelled toward the workpiece Copper slag is just one of many different materials that may be used as abrasive grit\nCopper extractions from the copper smelter slag and the copper smelter slag flotation tailings as a function of time at different initial ferric iron concentrations By comparing Figure 2 and 4, it can be seen that both the copper extractions and\ncopper slag grinding by autogenous copper slag grinding by autogenous XSM excellent mining crushing machinery products or production line design, the company is committed to building the Chinese brand mine crushing and processing machinery, mainly crusher, mill, sand making our products (copper slag grinding by autogenous) in ,\ncan copper slag be processed in south africa XSD Sand Washer The efficient sand washing machine of XSD series is a kind of cleaning equipment of international advanced level for sand and slag pellets, developed on the basis of introducing foreign outstanding technology of the same kind of products\nCopper slag is a by-product created during the copper smelting and refining process Copper slag is a by-product obtained during the production of copper metal, which can be used as pozzolana in the production of cementing materials Recycling copper slag to recover metals has been focused on copper recovery\nAbrasives processed from slags have replaced silica sands in many steel fabrication shops because of their low free silica content Copper smelters and coal-fired electric generating plants are 2 major material sources for these slag products The chemical characteristics of the copper slags are dependent upon the\nThe process of extracting copper of copper carbonate The concentrated ore is heated strongly with silicon dioxide (silica) and air or oxygen in a furnace or series of furnac , Slag can also .\nApr 19, 2018· Copper slag is used in following Applications: * Sandblasting: Copper slag is mainly used for surface blast-cleaning Abrasive blasting is used to clean and shape the surface of metal, stone, concrete and other materials In this process, a stre.\ncopper slag use in china Crusher manufacturers quot can the copper slag be used instead of sand in concrete Copper Slag Suppliers Use Of DRI In EAF’s Slag ,\nAlthough this requires more ore to be extracted and processed, this process is less expensive, so oxides can still be mined at a profit On the other hand, while copper sulfide ores are less abundant, they contain higher amounts of copper Although the processing costs are higher, ultimately more copper can ,\nAlso, copper slag can be utilized to make the products like cement, fill, ballast, abrasive, aggregate, roofing granules, glass, tiles etc (Shen and Forsberg, 2003) Copper Slag Processing Recycling today’s copper slag is similarly possible Recovery of metals from\nCopper Slag Processing Unit What is copper slag, and can copper slag be processed? The copper slag is a by-product created during the copper smelting and refining process With the rapid growth of mining technology, many companies have developed their own copper slag processing unit\nJul 14, 2019· Copper slag is a by-product created during the extraction of copper from copper ore Sulfur is a common component of slag Metals start to oxidize as they are smelted and slag forms a top crust during this process that protects the liquid metal underneath\nCopper slag abrasive is dense enough to be reclaimed via a Cyclone separator in coarser sizes only It has a 115 to 120#'s/cubic foot THIS IS A HIGHER DENSITY THAN COAL SLAG AND WILL THUS BLAST FASTER THAN COAL SLAG, AND YOU CAN RECLAIM IT A COUPLE OF TIMES AS WELL\nCan copper slag be processed? The answer is y The copper slag represents a popular alternative to sand as a blasting medium in the industrial cleaning Get Information; What is a Copper Slag? (with pictures) Jun 16, 2018 Copper slag is a byproduct created during the copper smelting and refining process As refineries draw metal out of .\nCopper Slag Shape and Texture: Air-cooled copper slag aggregates are black in color, and typically have a glassy appearance Granulated copper slag aggregates are similar to air-cooled copper slag aggregates but more vesicular Gradation: Reverberatory copper slag can be processed into coarse or fine aggregate material for use in hot mix asphalt\nFeb 17, 2013· copper slag flotation plant in pakistan-Shanghai Zenith Company Altonorte – Xstrata Altonorte is a custom copper smelting operation located near the port of , acid plant, an additional anode furnace, a second casting wheel, and , »More detailed\ncan copper slag be processed meats , a review of innovative use of copper slag and effect of old spent hen meat replacement and tomato juice addition Chat With Sales » can copper slag be processed joomla twadsafewater Get Price; Study of the Properties of Concrete Containing Copper Slag .\ncan copper slag be processed kintera XSD Sand Washer The efficient sand washing machine of XSD series is a kind of cleaning equipment of international advanced level for sand and slag pellets, developed on the basis of introducing foreign outstanding technology of the same kind of products\nThe slag may be fed back into the flash furnace to act as a flux, and the sulfur dioxide is processed through the acid plant After the slag is removed, a final injection of oxygen removes all but a trace of sulfur , The recovery of sulfuric acid from the copper smelting process not only provides a profitable byproduct, but also .\nCopper sulfide and iron oxide can mix, but when sufficient silica is added, a separate slag layer is formed Adding silica also reduces the melting point (or, more properly, the liquidus temperature) of the slag, meaning that the smelting process can be operated at a lower temperature The slag forming reaction is: FeO + SiO 2 → FeOSiO 2\nCopper Slag Gradation: Reverberatory copper slag can be processed into coarse or fine aggregate material for use in embankment applications Copper slag can readily satisfy the gradation and physical requirements of AASHTO M145 (11) Unit Weight: Copper slag has a unit weight of 2800 to 3800 kg/m 3 (175 to 237 lb/ft 3)\nAs stated before, slag once was just dumped without any consideration to the effects to the environment Slag is not just the hazardous waste it once was thought of Slag contains reusable lime and metallic scrap that can be reused, in addition, being able to completely recycle it will save on cost of fuel, dumping, and transportation""]"	['<urn:uuid:9d773a36-d78b-47e3-aa21-366432087814>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	18	36	1400
40	coordinating air forces compared to teaching aerobatics which needs more communication	Coordinating air forces requires more intensive communication, as it is nearly impossible to effectively coordinate air attacks without voice communication. Text chat is insufficient for responding to the dynamic battlefield. In contrast, teaching aerobatics primarily involves demonstrating specific maneuvers like rolls, where the instructor can focus on observing student technique and providing feedback on specific control inputs.	['Now that the first version of the DCS: Combined Arms open beta is available, you have some learnin’ to do. This article will provide you with the elements to get you going. It is not intended as a Preview or a Review of DCS: Combined Arms. Those will come later. Think of this article as a training primer.\nBefore getting into the Ground Commander / JTAC functions, you need to learn how to use the DCS: Combined Arms interface and control your ground forces. Fortunately, we have some good reference videos already produced.\nCheck out Magnum’s “Basics” video:\nDCS: Combined Arms – Basics\nAnd Matt Wagner’s “Command Map” and “Tank Gunnery” videos:\nDCS: Combined Arms – Command Map Hands-On\nDCS: Combined Arms – NATO Tank Gunnery\nDCS: Combined Arms – IFV Tank Gunnery\nNow that you have the basics down, take a look at the battlefield (using the F10 map view) and figure out how the air element should factor in your battle plans. Do you want them taking out artillery behind the lines? Or weakening the enemy tanks on the front lines to support the advance of your ground troops? Remember that anti-air threats, like SAMs and AAA units, will threaten, harass, and otherwise render your air force less than 100% effective. Don’t forget to target them as well, with your air forces or your own ground forces (artillery, etc.).\nAlthough as Ground Commander, you have immediate access to the F10 map for situational awareness, your support pilots will have their hands too full with flying their aircraft and avoiding threats to be able to focus on interpreting the map. It’s up to you coordinate their attack and egress to maximize their effectiveness.\nA note here. it is about impossible to effectively coordinate air attacks without voice communication. Yes, it can be done via text chat, but I strongly recommend against it. You simply won’t be able to communicate effectively and efficiently or fast enough to respond on the dynamic battlefield. If you haven’t already, now is the time to invest in a decent headset and install Teamspeak 3. The SimHQ Teamspeak 3 Server is free to SimHQ registered users and you’ll always find plenty of folks willing to join in and help. Info on connecting is here(member log-in required).\nIt’s common practice for the Ground Commander / JTAC to establish one or more coordinates near the battlefield that attack aircraft will use as the beginning of their attack leg into enemy territory. This point is called the Initial Point (or Battle Position for rotorcraft). As you give targets to your aircraft, they will begin their run in from this point to the target, and then exit the area (egress) based on your recommendation (you know where the anti-air units are, right?… right?!). You can set up several IPs to conduct attacks from different directions, just be sure you communicate those locations to your pilots BEFORE you need them to get there.\nAn attack pilot needs more than just coordinates to execute a successful attack: what the target is, whether or not there are friendlies nearby, if the target will be marked by smoke, laser, or not at all, etc. So, in addition to the target’s location, you will need to provide this additional information, plus any amplifying information, like requesting a specific weapon, notifying the pilot of any nearby threats, etc.\nThe “9 Line”\nTake all that information, put it into an standardized convention, and you’ve got the JTAC CAS 9 line. Any good JTAC will know how to generate and deliver a 9 line. This standardized 9 lines of information is usually followed by remarks and additional amplifying information: read the Close Air Support Joint Publication document to learn more about this process.\nThe direct version:\n1. Initial Point (or Battle Point for helicopters) reference: “IP Charlie…”\n2. Heading from IP: “2-1-0 degrees…”\n3. Distance from IP: “1-1 miles…”\n4. Target elevation: “at 4-2-5 ft MSL…”\n5. Target description: “4 stationary artillery vehicles…”\n6. Target location (Lat long or MGRS): “north 4-3 5-4 point 4-1-6, east 0-4-2 4-2 point 7-0-0…”\n7. Type of mark: “marked by orange smoke…”\n8. Location of friendlies: “friendlies 4 miles west…”\n9. Egress: “egress west”\nJTAC calls the smoke\nEven if you don’t want to go through all the work to create a “by the book” 9 line, you will need to give your attack pilots this basic information:\nWhere the target is. Coordinates help, but any kind of information the pilot can use to pick out the target from the ground clutter is helpful. No fair using labels!\nWhat the target is. If there’s a tank and an anti air target next to each other and you want your pilot to hit the anti-air unit first, be sure to tell them that. They’re no better at reading your mind than you are at reading theirs!\nWhat sort of mark is on the target (if any). Include laser code, if lasing. If the target is offset from the mark, give reference in cardinal directions, e.g., “Target is 400 m to the north of the smoke”.\nIf you want your pilots to stay alive, you’ll also need to let them know where the threats are and give them helpful directions to avoid flying into those threats.\nGive coordinates in a consistent frame.\nLat Long decimal and MGRS grid are the two coordinate systems that the A-10C pilot can easily enter into their CDU. The Shark uses Lat Long decimal as well. Check the “Misc” tab under your Options menu to change the default coordinates listed in the F10 map and in your JTAC monocle. Remember that Su-25 and P-51D pilots don’t have any way to manually enter coordinates, so you’ll have to talk those pilots onto the target using visual cues.\nNote. You can get the coordinates for a point by hovering the mouse cursor over that point in the F10 map (upper left corner), clicking the enemy unit in the F10 map (bottom left corner), or by lasing a target through the JTAC monocle. Be aware that if the range to the target is greater than 10 km, the laser won’t reach and the coordinates listed at the bottom of the monocle will be YOUR OWN POSITION!!! (see step 4 below).\nA non-zero range at the right tells you the numbers at the bottom\nare the target’s coordinates, not your own.\nPick the right weapon for the job.\nIt might be fun to watch LGBs drop on your laser designation, but if the target is a tightly packed column of T-80 tanks, the CBU-97s hanging under the aircraft would do more damage in a single pass. Similarly, if the target is well defended by air defenses, a long-ranged attack with Mavericks would be a better choice than a more flashy gun run.\nMavericks can reach past many air defenses.\nWalk your pilots onto your targets.\nGuide pilots onto targets using big to small cues and be sure to give directions from their perspective. For example: “Turn to your right and fly down the big valley. Just past the river there is a large block of trees. The target is about ½ mile on the other side of the trees. Target is 200 ft to the left of the orange smoke.”\nThe world looks a lot different from 6,000 ft than from the F10 map.\nGet confirmation that the pilot has the right target.\nThe smoke option helps, but use whatever you can to determine that the pilot is looking at the same target you are: location, orientation, proximity to other objects, etc. A well-coordinated attack on the wrong target can be as big a failure as a missed attack. You especially want positive confirmation when friendly forces are in the area.\n“Uhhh… which tank at which orange smoke did you want me to strafe?”\nCoordinate the egress of the pilot.\nOnce the attack is over, be sure to direct the pilot to egress in a way that will allow them to exit safely and set them up for the next attack. It may be a simulation, but it’s really annoying to get shot down on every attack run!\nThe above steps can be challenging, especially in a dynamically changing battlefield, but add a few more planes to the mix and the workload just to coordinate planes in and out can be overwhelming… but oh so rewarding! See you out there!\nWe want your Feedback. Please let us know what you thought of this article here.\nThe SimHQ DCS: Combined Arms Forum is here.', 'MEMBER ALERT: AOPA will close at 2:30 p.m. Eastern time for a company-wide activity and will reopen July 23 at 8:30 a.m.We apologize for the inconvenience.\nJune 1, 2013\nBy Dave Hirschman\nA roll is the first aerobatic maneuver an aerobatic student learns, and it should be a confidence booster as well as a clean break from the past. It boosts confidence because rolling shows students they are capable of getting the airplane upside down and right side up again, and it’s not disorienting since the horizon is plainly in view throughout the maneuver.\nThe roll differs from normal maneuvers because it’s often the first time a student is encouraged to use full aileron deflection and hold it throughout the maneuver. Previously, students have been rewarded for subtle, smooth, and gentle control inputs. The roll—and aerobatic flight in general—is about maximum performance, so it’s imperative pilots use all available control authority.\nAerobatic trainers vary widely in performance from Citabrias or Cessna Aerobats with glacial roll rates to Pitts S–2Cs and Extra 300s with blindingly quick ailerons. But regardless of what airplane you’re flying, an introductory or “primary” roll begins the same way.\nStep one: At cruise speed, raise the nose of the aircraft well above the horizon. Airplanes with slower roll rates require higher pitch attitudes than faster ones, but 20 degrees is a good target. If your airplane doesn’t have an attitude indicator (and many aerobatic trainers don’t), simply pitch up to put your “heels on the horizon.”\nStep two: Unload the wings. Once your heels are on the horizon, stop pulling on the stick. You don’t want to pull and roll at the same time because doing so slows the roll rate and puts greater stress on the airframe. So stop the back pressure and draw a momentary straight line with the airplane in a 20-degree climb. If you have a G-meter, it should be reading no more than one G at this point, just as it does when the airplane is parked on the ramp.\nStep three: Roll! Use full aileron deflection—every bit of it—and hold it until the wings are right-side up and level again. Some tentative aerobatic students resist using full deflection. Others start out all right but lighten up, or even neutralize the ailerons completely, when confronted with the novel view of the inverted horizon. And just about all first-time rollers stop about 45 degrees before reaching the wings-level position. They instantly recognize this mistake and correct it, and then do the same thing on their very next roll, but they quickly get the hang of it.\nWhat about rudder? Use coordinated rudder to counter adverse yaw at the start the roll, but after that the pilot’s feet might as well be flat on the floor. The point of this exercise is using full aileron deflection to get the airplane upside down and then safely rightside up again. Rudder will play a central role in future slow rolls, point rolls, and snap rolls. But in this introductory maneuver, emphasize full aileron deflection to keep the roll as simple as possible.\nWhat about elevator?\nKeep the elevator neutral once you begin the roll. If you start the maneuver 20 degrees nose up, you may finish 20 degrees nose down, and that’s OK. Without timely rudder and elevator inputs the heavy nose of the airplane is going to fall. Accept that for the greater good of making the maneuver easy.\nOne useful trick involves elevator trim. Add more nose-down trim than you really need in cruise flight before the roll ever starts. That extra nose-down trim will help keep the nose up through the inverted portion of the roll, and the airplane will finish in a shallower descent than it would have otherwise.\nWhat about engine power?\nUse a high power setting throughout the roll, but be wary. The most dangerous mistake any aerobatic student can make while rolling is to become confused or disoriented, neutralize the ailerons while the airplane is inverted, then panic and pull on the stick. That reaction leads to a split-S (a half-loop started from the inverted position) in which the airplane can gain excessive speed, exceed G limits, and lose thousands of feet in altitude—all at the same time. Only by reducing engine power to idle (or near idle) during a split-S entered from a botched roll can an instructor prevent potentially catastrophic airframe and/or engine damage.\nMake sure aerobatic students thoroughly understand the perils of the split-S as well as the absolute necessity for raising the nose, unloading, and using full aileron deflection before performing their first rolls.\nAlso, make sure they practice rolling both left and right. (Most pilots have a built-in predisposition—like being right- or left-handed—and, if given the choice, will always roll in that direction.)\nOnce they start rolling, even formerly reluctant aerobatic students tend to become wildly enthusiastic—and the hardest part is getting them (eventually) to stop.\nHow would you fly this flight? This scenario is available on the AOPA Jay from Redbird.\nUnable to climb, and unable to lower the nose to accelerate without contacting the ground, he is in a spot.\nAirbus Group plans to certify production versions of its E-Fan electric twin and has a goal of late 2017 entry into service for the two-seater, the company said in a video from Farnborough International Airshow July 14.\nIs flight instruction safer than other sectors of general aviation?\nVOLUNTEER AT AN AOPA FLY-IN NEAR YOU!\nSHARE YOUR PASSION. VOLUNTEER AT AN AOPA FLY-IN. CLICK TO LEARN MORE >>>\nVOLUNTEER LOCALLY AT AOPA FLY-IN! CLICK TO LEARN MORE >>>\nBE A PART OF THE FLY-IN VOLUNTEER CREW! CLICK TO LEARN MORE >>>']	['<urn:uuid:feb179b9-2017-4d8f-8433-6ca07138b17c>', '<urn:uuid:48d0f8a3-e4ef-4f07-a8a0-63ee46d577ec>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	11	57	2379
41	What's the best way to keep employees from bringing contamination into cannabis growing areas?	Employees should go through a clean room process, change into scrubs or uniforms laundered after each shift, and wear sterilized footwear (Crocs). They must wash or sanitize hands before entering, use nitrile gloves when handling plants, and should not wear outside shoes in clean areas. Some facilities also use decontamination chambers and sanitizing foot baths at all doors.	['Broad mites, russet mites, spider mites, aphids, whiteflies, fungus gnats and powdery mildew are just a few of the threats for cannabis plants and cultivation facilities, be it indoor, outdoor or a greenhouse. As many cannabis pests as there are, there are about just as many potential infection/contamination sources.\nHere are some primary sources of contamination:\n- Home gardens. Some employees may have home grows (i.e., they grow cannabis and other crops in gardens at home). If said grow/garden is contaminated, it is possible to bring that contaminant to other locations, including that employee’s workplace. Some cannabis companies have put policies in place banning employees from participating in home cultivation. While this policy does indeed protect the business’s crop from home garden contamination, employers risk alienating talented, passionate prospective employees. Companies considering implementing these policies must weigh them carefully.\n- Unsterilized materials/supplies. Even if a product leaves the manufacturer’s facility in sterile condition, it can become compromised in shipping or by a distributor. For example, improperly storing soil or growing media in unsatisfactory conditions (e.g., a warm and wet/damp environment, near high traffic areas or near other plant matter), can lead to that medium getting infected. Always make sure you have a sterilized media/soil source by testing them in-house or with a trusted third-party lab.\n- Poor air intake filtration. For indoor and greenhouse facilities, there is a multitude of air filtration options. Greenhouses often utilize specific-sized bug screens on their air intake systems/vents, while most indoor facilities incorporate air filtration capabilities into their HVAC systems (including, but not limited to, UV light filters to kill powdery mildew spores). Failure to address air filtration in either facility is simply asking for problems. If a cultivation operation is not filtering incoming air, it’s not a question of “if” a facility will become contaminated, but “when” it will become contaminated.\n- Vegetation/crops surrounding a facility. Many indoor facilities and greenhouses containing cannabis or other crops have been infected by pests or diseases that were present on crops or vegetation that surrounded that operation. Cannabis cultivators should periodically inspect all surrounding trees, shrubs, and/or weeds, as well as any neighboring crops, for pests and disease.\n- Employee clothes/shoes. Insects and mold spores can hitch rides on your employees’ clothes, even in the time it takes for them to get from their parked car to the facility. Growers should set up decontamination protocols for all employees each time they enter the greenhouse or facility. Preferably all employees change clothes and shoes before entering a facility. Growers also can provide sanitizing foot bath floor mats at all doors, and/or disinfecting foam containing hydrogen peroxide at the entrance of a greenhouse. An employee decontamination chamber is also advisable for both indoor and greenhouse operations. To prevent the threat of infection by hand, all employees must wash or sanitize their hands prior to entering the facility, as well.\n- Clones/new genetics. In my experience, the No. 1 source of pest and disease in a cannabis cultivation facility is from new clones, especially those introduced during the pursuit of new genetics/cultivars. It is much less time-consuming to start a given cultivar via a clone of a preferred genotype that expresses preferred phenotypes than it is to start from seed. With seeds, growers must search for desirable traits and perform a “pheno-hunt,” a search for a phenotype from which to take clones to begin production. While easier to manage than seeds, sometimes those clones are indeed contaminated with a pest or disease (or multiples of both). All clones should be properly quarantined and guaranteed to be pest- and disease-free prior to introducing it to any production area. Preferably, new clones should be kept off-site (this discourages employees from checking in on new clones before moving to other production areas). At minimum, they should be held in a separate, contained space for at least one month. Assign dedicated employees who will inspect the new clones for signs of pests, disease or nutrient deficiencies, then have those employees sanitize themselves or change their clothing before moving on to production areas.\nRegarding neighboring farm crops, one must always be aware of how and when an infected neighbor/farmer uses preventative applications or treats their infected crops, as well, as those pesticides or harmful chemicals can affect your cannabis cultivation facility. Plan for the event and take necessary action such as closing ventilation openings and turning off any ventilation fans to prevent the intake of any potentially harmful chemicals. For instance, some Central Valley California greenhouses are in agriculture zones where airplanes crop dust large tracts of acreage. The potential for chemical drift is possible; therefore, it is best to know when a neighbor is planning any application of chemicals that could be harmful to cannabis plants or the humans consuming it.\nA Closer Look At Clones\nTypical past behavior has been to obtain a given desired cultivar in clone form and, after quarantine, to deem the clone a “mother,” meaning that the original clone is grown to maturity in order to produce more clones. Those clones could be grown to produce more clones (by turning them into mother plants), or simply utilized for production.\nThis has been the most common way cannabis growers have propagated crops for the past two decades (not counting autoflowering plants or feminized seeds). This system, unfortunately, led to what we now consider reckless behaviors and practices.\nAbout 20 or so years ago, I started noticing anomalies and unusual signs and symptoms on cannabis plants in multiple facilities in multiple states, starting in California. The symptoms were multiple: leaves curved sideways, with some showing discoloration as if it had been painted a faint yellow. Along with other strange deformities in new young growth, at its extreme, symptoms included diminished yields of up to 50% in combination with a major drop in both potency (THC content) and aromatic qualities (terpene content).\nAfter seeing these unexplained symptoms multiple times, I brought the observations up with one of my mentors, Robert C. Clarke, to see if he had heard of any disease or virus that would cause what I was observing. (Editor’s note: Clarke, co-founder of BioAgronomics Group, is a regular contributor to CBT and is featured in this issue.) He explained it could very well be a virus or another form of viroid contamination, but that there were no test kits available to positively identify the infection other than in a university setting, which was impossible at the time because the infected tissue was cannabis. But Clarke surmised the infection was either a virus or viroid and that possibly one could eliminate the infection via meristematic tissue culture—essentially taking a few cells from the newest plant material that had yet to be contaminated by the virus from which to propagate infection-free plants. Meristem tissue culture was a common practice in many other markets, including produce such as berries, but few if any cannabis growers used this process at the start of the millennium.\nHow a California Problem Became (Inter)National\nDuring the next 20 years, I continued to see the same symptoms in various gardens in multiple states but could never confirm what it was. I heard it hypostatized as hemp streak virus, then Sunn-hemp mosaic virus, but none of these were ever confirmed in any way. I’ve heard others describe their experiences dealing with similar symptoms as a broad mite infection to “genetic drift,” neither of which are correct.\nThen a few years ago I heard a term utilized to describe the symptoms I had been witnessing for years: “dudders disease” or “dudding disease.” While the terms did pick up in popularity, I’ve always had issues with them—I believed that whatever was causing these symptoms was not new, that it already had a name, and, as such, didn’t need a new one.\nWhile only a brave few would risk admitting it, in 2015, there were thousands of potentially virus- or viroid-infected clones being shipped to every emerging legal cannabis market in the U.S. from California—this is how a California-focused problem became a national one. If infected clones entered new legal markets, it could ultimately lead to the loss of many cultivars and create quality issues in those new markets. All because it was easier to have California clones sent eastbound than starting from seed.\nThe problem even crossed international borders.\nIn Canada, licensed production facilities had a given amount of time to obtain desired genetics, which they had to declare to the government. During the window of opportunity, some Canadian groups chose to obtain the most desirable genetics in the form of seeds and clones from the U.S. With those clones came the possibilities of viral infection. That may have been a blessing in disguise, however, as federal legalization in Canada opened the door to this infection being studied by government officials, university researchers, and private research labs (although little published research has been done on the viroid in cannabis).\nIn recent conversations with a friend who is part owner of a Canadian tissue culture company, I learned that some of the U.S.-born clones did indeed contain a viroid of some form or another, and that Canadian researchers were able to confirm the identity (although they were not the first) of the root cause of the symptoms I’ve been seeing for two decades: hop latent viroid, or HpLVd.\nHe also shared that, to prevent the possibilities of contamination caused by clones from the U.S., some Canadian cannabis producers chose to import seeds from America and other countries. I also learned that HpLVd is transmittable from parent plant to the seed it produces: if the mother is infected, so is the seed. (Canadian researchers traced HpLVd back to imported seeds.)\nThe most likely explanation for how the virus propagates is from the current cloning practices that have been employed for decades. When a clone is cut off the parent plant, the potential exists to contaminate the clone with a virus on the surface of the branch. When the cut was made, the virus was introduced to the cutting’s inner tissue.\nBecause the plant itself can become its own vector for disease, and because mother plants are especially at risk for viral contamination as the plant’s immune system gets weaker over time, infections generally can only be treated by destroying the crop, the mother plants and any clones and seed taken from those mother plants, and then starting from scratch.\nThat is unless growers can leverage meristematic tissue culture to save their genetics, something that Clarke had proposed years ago: meristematic tissue culture can rid the plant of certain viruses. (Not unrelatedly, tissue culture is also a great way to preserve a given cultivar for later use, or for storing useful male specimens for use in breeding projects. A cultivator could literally store 5,000 cultivar specimens in a relatively small area with a fairly low maintenance cost compared to upkeeping mother plants from which to clone, costs compounded by the potential risks associated with those clonal methods.)\nThe practice of cloning from a host mother is potentially problematic, and that tissue culture of cannabis for mass production is the only viable answer when one considers the risks of introducing a viral contamination by using clones and/or seed.\nAll of this to say: When considering disease vectors, cannabis cultivators also should look at their long-held belief and practices with an objective lens. Only unbiased evaluation should decide whether current practices put their crop, and their business, at risk.', 'Protecting the biosecurity of a facility involves a variety of best practices. Instead of explaining what we think is the best way to ensure healthy plants through biosecurity, we’d like to share what some of our clients and partners think.\n“We design facilities to eliminate as many biohazards from being introduced as possible. We design for exclusion. The best way of eliminating something is to not introduce it in the first place. That means keeping biohazards from coming in from the outside.\nAlways design facilities so that everyone coming in from the outside goes through a clean room process. This minimizes the number of outside contaminants that can be brought into a grow. All staff should wear scrubs or uniforms that are laundered after each shift. No outside shoes should be allowed inside the clean areas.\nAdditionally, access to the cultivation facility should be restricted. Minimize the number of people in the facility and only allow necessary personnel to have access to certain areas of the facility. This limits the number of people going through and reduces the likelihood of contaminants being introduced or spread throughout a facility.”\n-Jay Czarkowski, Founding Partner, Canna Advisors\n“Keep executive leadership and administrative staff OUT OF THE GARDEN. Isolate your garden, make it a fun safe place to work, and make it like Fort Knox for people to get in.\nRequired clothing changes, daily and weekly clean down procedures and processes. Cleaning the mechanical equipment thoroughly and regularly. Utilizing material when building your gardening rooms that doesn’t absorb moisture and can easily be cleaned to be sanitary.\nI call it a “Grow Flow” which is a garden based workflow designed to isolate your garden rooms in an indoor environment while allowing for the most productive workflow possible. It is an arduous concept to work on that requires diligence in both planning and execution. It is an ongoing challenge we face in this new industry to set the Grow Flow standards for future generations of indoor commercial cannabis producers. The more the world learns how we do it, the better the world will do it.”\n-Fitz Couhig, Pioneer Nuggets\n“Clean room techniques:\n- Filtered Supply air to pressurize the facility including Carbon, HEPA filtration and photo catalyst sterilization technology used within the facility.\n- Sterilized footwear (Crocs) for all persons on the facility floor and footbaths throughout.\n- Clean clothes for all employees and visitors and nitrile glove use mandatory for plant and product handling.\n- Thorough cleaning of grow rooms every cycle using checklists and atomizer equipment.\n- Monthly filter replacement in all areas.\n- Hydrogen peroxide, isopropyl alcohol, and both aqueous and gaseous ozone for various sanitation applications.”\n-Sean O’Leary, COO, Pioneer Nuggets\n“”Keeping a grow clean is a huge challenge since factors such as workflow, climate control, air sanitation, and employee compliance can impact the overall cleanliness of a facility. Facility hygiene should always start by focusing on controlling hazards, and preventing the introduction of contaminants rather than focusing on remediation when problems do arise. It’s important to frequently monitor a facility through environmental testing to ensure what you are doing is working. Through testing it’s easier to validate facility cleanliness as well as create accountability throughout the whole production process.”\n-Michael Caffrey, Biological Engineer, Surna']	['<urn:uuid:25420ef1-8938-4fe9-8b7b-d3f91442cb61>', '<urn:uuid:59df4b4c-0f36-4208-a3b1-3555ce996b83>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-13T06:15:13.904987	14	58	2458
42	Which religious groups helped study school worship reforms?	The commission was established by the Woold Institute, which studies relations between Christians, Muslims and Jews and includes Iqbal Sacranie, the former general secretary of the Muslim Council of Britain, and Lord Woolf, the former chief justice.	"['Mandatory Christian prayers in schools \'should be axed\'\nAn influential commission recommends radical teaching reforms to adapt to increasing diverse and secular UK schools\nSchools should no longer be forced by law to hold daily acts of Christian worship, as part of major reforms reportedly outlined by an influential commission.\nA report by the Commission on Religion and Belief in British Public Life, which includes the former Archbishop of Canterbury Rowan Williams, said the arguments for mandatory Christian prayers in secular schools was ""no longer convincing"".\nUnder existing laws, schools have a statutory duty to ensure pupils take part “each school day in an act of collective worship”. Parents in England and Wales have a right to withdraw their child from such services, but this is not the case in Scotland and Northern Ireland.\nWhile faith schools are permitted to hold acts of worship in accordance with their religious character, in secular schools the worship must be “wholly or mainly of a broadly Christian character”. This also applies to academies, free schools and foundations schools.\nDebating the issue on the BBC Nolan Show this morning, commentator Malachi O\'Doherty said: ""I do believe education should be secular; education should be separate from religious cultivation.\n""If people want their children raised within a religious tradition then that is their responsibility in the home.\n""I think that education and religion are in some measure incompatible, in that the point of education is to teach people to think and induct them into the sciences and the humanities and so on - but to think logically about things and there is no logical, firm argument that establishes the existence of God let alone the Christian interpretation of God and therefore if you\'re teaching this to children it has to be on that understanding that\'s its not a finished thing, it\'s not rounded off, we haven\'t come to any conclusions about it so it\'s for them to think about it themselves.""\nPeter Lynas, NI director of Evangelical Alliance, said: ""30% of the population are going once a week or more often to church. That\'s the single biggest grouping in society of any type.\n""Who else gets those kind of attendance numbers?\n""Let\'s remember that 200,000 people go to the Irish league football matches in the entire season; 400,000 people go to church every single weekend. That\'s an incredible number of people.""\nHe added: ""The legislation is there that we have to have a spiritual component [in schools]. We need to have a plurality of ideas in the public square... The idea that you can put nothing in its place is a false one because secularism is its own perspective.""\nAccording to the Observer, a draft of the Commission\'s report suggests replacing this with a more inclusive ""time for reflection"".\nThe Commission includes members from all the UK\'s major religions, and is being led by the former High Court judge Elizabeth Butler-Sloss.\nIt reportedly suggests new guidelines ""to build on current best practice for inclusive assemblies"" which would draw upon a range of sources and ""contribute to their spiritual, moral, social and cultural development"".\nAnd it warns against the greater segregation of communities as a result of the ""negative practical consequences of selection by religion"" in faith schools.\nThe commission was established by the Woold Institute, which studies relations between Christians, Muslims and Jews and includes, Iqbal Sacranie, the former general secretary of the Muslim Council of Britain, and Lord Woolf, the former chief justice.\nThe recommendations come after a report published by the Arts and Humanities Research Council (AHRC) last month, which urged for “fresh thinking” on school collective worship and more transparency in its implementation.\nIn that report, senior academics said the statutory duty to organise prayers in schools lacked ""a clear and accepted rationale"" and has been “controversial for decades”.\nThe National Secular Society (NSS) says the UK is ""the only Western democracy to legally impose worship in publicly funded schools"".\nThe NSS argues assemblies should “play an important role in fostering a sense of community in schools” and “with an ethical dimension”, they can contribute to pupils’ development.\nKeith Porteous Wood, executive director of the NSS, said: ""The right to withdraw from school worship is neither a practical nor an acceptable compromise. It inconveniences schools and leaves pupils ostracised, so parents wishing to withdraw their children are left in an impossible position.\n""Many schools flout the law because teachers know how inappropriate and unworkable it is, so in those cases abolition would simply formalise the arrangement that already exists, and it would give this common-sense approach a legal basis.”\nA Department for Education spokesperson said: “The current requirements for a daily act of collective worship won’t be changing – collective worship plays an important role in schools. It encourages children to reflect on belief, and helps shape fundamental British values of tolerance, respect and understanding for others.\n“It is for schools to tailor their provision to suit the needs of their pupils, and parents can choose to withdraw their children from all or any part of collective worship.”\nIndependent News Service']"	['<urn:uuid:e1df1975-8bce-45b3-96f8-1d5d45b3bd10>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	8	37	848
43	What percentage increase in email lists can companies expect to achieve through a well-planned and well-executed sweepstakes campaign, and in what timeframe?	A well-planned and well-executed sweepstakes can grow email lists by 20 percent or more in a matter of days.	['Email marketers that follow the best practices for acquisition, relevancy and deliverability still lose about one-third of their subscribers annually due to bounces, unsubscribes and spam complaints, according to the Email Experience Council. The attrition rates are even higher for less vigilant Web professionals.\nOne strategy that can quickly acquire the most qualified subscribers and keep them longer is an email acquisition sweepstakes, contest or other viral campaign. Email marketing firm Listrak recently published a white paper on the subject, warning that there are some risks to be aware of before successfully executing a sweepstakes acquisition strategy.\nEntitled Email Acquisitions via Sweepstakes: Attract the Right Subscribers Fast, the report highlights eight key tactics. The full white paper is freely available on the Listrak site, and a condensed version is provided below:\n1. Keep it simple\nLimited time frames give sweepstakes the appearance of exclusivity and are more likely to prompt responses than ones lasting for several weeks or longer. Maximizing acquisitions during a shorter time period, however, requires careful attention to automated emails, baner ads, social and mobile announcements, etc.\n2. Make it personal\nA cash prize may entice more entrants to your sweepstakes, but by awarding your own merchandise instead you will get a more qualified group that is clearly interested in your company’s brand and what it has to offer.\n3. Encourage social sharing\nYou will greatly expand the reach of your sweepstakes by offering an incentive for entrants to invite their friends from social networks. Consider offering a second entry or a free gift.\n4. Promote in the right channels\nDo not rely exclusively on email to promote the sweepstakes. While it will be your most effective channel for getting the word out, you should also utilize all social and mobile channels as well as including banner ads on your site and others.\n5. Send entry confirmations\nOne of the most important steps in the process is to send an email confirmation to all entrants. Consider adding a captcha on the registration form to prevent junk email addresses from infiltrating your sweepstakes.\n6. Welcome your new subscribers\nAn email welcome series is also especially important to conducting a successful acquisition sweepstakes campaign. Visit Listrak for more tips on crafting effective welcome messages.\n7. Monitor new subscribers\nEven if you follow the best practices for attracting the appropriate entrants for your brand, you’ll still get a number of invalid email addresses and people who are only interested in the prize. In order to protect your company’s reputation, closely monitor the results of your confirmation email and welcome series.\n8. Get the legalities right\nEvery promotion is subject to complex federal, state, and local laws, and you must be sure to have the correct procedures in place. Before launching a sweepstakes, it is imperative that you have attorneys review the concept and structure of the contest, set the official rules, perform the federal and state requirements for bonding and registration, complete the federal tax documents, create affidavits of eligibility and publicity/liability releases, set guidelines for winner selection and validation, validate content entry management and data collection, etc.\nListrak, which provides email marketing solutions and software to clients such as Waterford, Royal Doulton and Wolfgang Puck, says a well-planned and well-executed sweepstakes can grow email lists by 20 percent or more in a matter of days. January and February are the perfect times to launch a sweepstakes to engage new subscribers gained over the holiday season and start the year off right.']	['<urn:uuid:3fdb1a06-b0cf-41d9-895d-495e8904ee34>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	22	19	582
44	who built nine cottages at blaise hamlet in 1812	The architect John Nash built nine estate cottages at Blaise Hamlet in 1812. These cottages were commissioned by J S Harford to house former estate workers and employees. Each cottage was designed differently but shared common design features such as tall chimneys, dormer windows, and overhanging eaves.	"['BLAISE CASTLE AND HAMLET\nList Entry Summary\nThis garden or other land is registered under the Historic Buildings and Ancient Monuments Act 1953 within the Register of Historic Parks and Gardens by Historic England for its special historic interest.\nName: BLAISE CASTLE AND HAMLET\nList entry Number: 1001426\nThe garden or other land may lie within the boundary of more than one authority.\nDistrict: City of Bristol\nDistrict Type: Unitary Authority\nParish: Non Civil Parish\nNational Park: Not applicable to this List entry.\nDate first registered: 15-May-1987\nDate of most recent amendment: Not applicable to this List entry.\nLegacy System Information\nThe contents of this record have been generated from a legacy data system.\nLegacy System: Parks and Gardens\nThis list entry does not comprise part of an Asset Grouping. Asset Groupings are not part of the official record but are added later for information.\nList entry Description\nSummary of Garden\nLegacy Record - This information may be included in the List Entry Details.\nReasons for Designation\nLegacy Record - This information may be included in the List Entry Details.\nLegacy Record - This information may be included in the List Entry Details.\nMid C18 landscape garden overlaid by late C18 landscape park, laid out largely in accordance with Humphry Repton\'s suggestions, with further early C19 additions and structures by John Nash and G S Repton. Since 1949 used as a public park.\nDomesday refers to a wood one mile square in the manor of Henbury, held by the bishops of Worcester. There are traces of ancient woodland on the estate, with ridge and furrow and old field boundaries in the parkland south of Blaise Castle House and the remains of an Iron Age hillfort on Blaise Hill. After the Dissolution, and the seizure of the estate by the Crown, Henbury was granted to Sir Ralph Sadlier, of Standon in Hertfordshire. Henbury was sub-let by the Sadliers until they sold it in lots in 1675. A substantial part was purchased by Sir Samuel Astry, whose father-in-law, George Morse had built a house, known as Henbury Great House, on land in the village purchased from the Sadliers some ten years earlier. After Morse\'s death in 1688, Astry took over and enlarged the house and made formal gardens to the north, and planted a double avenue to a summerhouse on the top of Blaise Hill. The whole layout is depicted in an engraving by Kip published in 1712 (Atkyns 1712).\nAfter Astry\'s death, the manor passed via the Earl of Suffolk who had married one of Astry\'s daughters, to the Smyths of Ashton Court, Avon (qv). Although they held the estate until 1760, they sold the Great House in 1730 and it was finally demolished in the 1830s. In 1762, Thomas Farr, a wealthy Bristol sugar-merchant, bought 110 acres (c 45ha) of the estate, comprising the old Manor House, then a gabled farmhouse (depicted in one of Repton\'s Red Book paintings), Blaise Hill, the land between the Hill and the village of Henbury, and Hazel Brook in its spectacular gorge. Farr was responsible for commissioning the London architect Robert Mylne (1733-1811) in 1766 to design Blaise Castle, a triangular prospect tower, designed to command views of the shipping in the Severn and the Avon. Farr created an elaborate pleasure garden, inspired by that of his friend Valentine Morris at Piercefield near Chepstow, designed to capitalise on the sublime qualities of the topography, with bastions, wooden cannon, and look-outs on Blaise Hill overlooking the gorge. He made cascades and pools in the brook, with a steam-engine designed to raise water to supply water-features in the gorge in which he built a bath-house and a root-house. Farr went bankrupt in 1778 as a result of the blockade of shipping in the American War of Independence. The property was purchased by Dr Denham Skeet, who in turn sold it to the Bristol banker and Quaker, John Scandrett Harford, in 1789. Harford augmented the estate with further land purchases, including the east side of the gorge, and in 1795 commissioned a design for a new house, Blaise Castle House, from the Bristol architect, William Paty (1758-1800). At the same time, Harford invited Humphry Repton (1752-1818) to visit. Repton made two further visits in 1796 and a Red Book was completed the same year. After Paty\'s death in 1800, Repton\'s partner, the architect John Nash (1752-1835) became involved, designing an ornamental Dairy in c 1804, an Orangery in 1806, and picturesque cottages at Blaise Hamlet, completed in 1812. Repton\'s sons George Stanley Repton (1786-1858) and John Adey Repton (1775-1860) appear to have assisted Nash on these commissions. After Harford\'s death in 1812, the estate was inherited by his son, John Scandrett Harford Jnr. G S Repton may also have given advice on the park and gardens in the period 1812-20. Charles Robert Cockerell (1788-1863) was commissioned in 1832-3 to work on the House and immediate surroundings. Further ornamental planting continued during Harford Jnr\'s lifetime. J S Harford Jnr died in 1866, after which Blaise Castle ceased to be the family\'s principal seat. In 1926, the estate was purchased by the Bristol Corporation, excluding Blaise Hamlet which was bought by the National Trust in 1943. The Corporation opened the estate as a public park, and in 1949 the House as a museum. Under the Corporation\'s ownership, Blaise has served as a public park but with little alteration to its picturesque qualities. DESCRIPTION\nLOCATION, AREA, BOUNDARIES, LANDFORM, SETTING The Blaise Castle estate of c 100ha abuts the southern edge of the village of Henbury, now part of the north Bristol suburbs, c 5km north-west of the city centre. Its south-west boundary is formed by properties in the residential area known as Coombe Dingle; directly west it abuts the Kings Weston estate (qv) on Kings Weston Hill. The north-west boundary is formed by Kings Weston Road and properties in the village of Henbury. To the north-east the boundary runs behind C20 properties and a playing field on Henbury Hill but includes a Repton lodge towards the southern end of Henbury Hill, c 750m south-east of Blaise Castle House. To the south, the boundary runs along the north side of Coombe Hill including the higher, more wooded parts of a golf course, and around the rear of properties in the suburb of Coombe Dingle, abutting Kings Weston, Avon (qv) east of Henbury Lodge, c 1.7km south-west of the Blaise Castle House. Topographically, the site is dominated by Blaise Hill, at the eastern end of a ridge which rises some 2.2km to the south-west as Kings Weston Hill. Hazel Brook enters the estate at the north-east, 200m north-east of the House, and runs south-west through a gorge bounded on either side by high ridges with limestone outcrops. From Blaise Castle there are spectacular views south-west towards the Avon, while from the ridge of Kings Weston Hill there are northward views to the Severn and the Welsh hills.\nBlaise Hamlet (c 0.8ha) is now separated from the rest of the estate by suburban development but occupies a clearly defined enclave c 300m north-west of the House.\nENTRANCES AND APPROACHES A service drive c 50m long, now the main vehicular approach to Blaise Castle House, approaches the House from the north-west, past a stable courtyard designed by William Paty (1801, listed grade II), arriving at a forecourt on the north-west front. A long, winding drive enters the estate from Henbury Hill, 750m south-east of the House. The entrance is marked by a castellated lodge designed by H Repton (1798-9, listed grade II*) but built as a mirror image of the drawing in the Red Book. Some 380m west-north-west of the Gothic Lodge the approach passes Timber Lodge (early-mid C19, listed grade II*), a thatch-roofed cottage naturelle clad in bark and ivy thrums, and after a further 200m passes Woodman\'s Cottage (1798, listed grade II) which was depicted in the Red Book. Beyond this, the drive zig-zags down the steep east side of the gorge, crossing Hazel Brook by a stone bridge 350m south of the House (c 1798, listed grade II) before turning northwards along the west bank of the brook and through a cutting between lawns to the House. Part of this drive was diverted c 1820, or possibly by G S Repton when he was working at Blaise Hamlet in 1812, from H Repton\'s designed approach c 200m inside the entrance gate, running c 20m south of it before rejoining it north of the Woodman\'s Cottage. Repton describes the construction of his approach in the Red Book for Burley-on-the-Hill (1796):\n\'At Blaise Castle... I have lately finished one of the finest approaches in the kingdom; altho\' for many hundred yards the sides of the hills were so steep, that I had to be let down by ropes to mark the line of the road\'. (Humphry Repton 1796)\nThe line of this earlier approach is still traceable as low earthworks and is marked by plantings of beech and lime with northward views framed by Scots pine and yew trees. A second approach, now a footpath but formerly a carriage drive (c 1812), runs along Coombe Dingle, entering the estate outside the land here registered some 1.5km south of Blaise Castle House, and following Hazel Brook along the rear of early C20 properties in Pitchcombe Gardens for c 300m before entering the gorge.\nPRINCIPAL BUILDING Blaise Castle House (1795-9, listed grade II*) was built for John Scandrett Harford by the Bristol architect, William Paty. It is situated just a few metres north-east of the site of the old manor house occupied by Thomas Farr, in the north-east corner of the site. It is located on the south side of the village of Henbury and commands open views into the park to the south-west of Blaise Hill and its foreground, and to the south of the valley of the Hazel Brook. Former views to parkland south-east of the House are largely obscured by self-sown trees in the valley of Hazel Brook. H Repton advised on the siting of the new house, which is in accordance with the Red Book (1796). Paty\'s design for J S Harford was a two-storey box in a plain, Classical style with only a six-column Ionic porch on the north-east side as relief. Nash added a curved orangery (1806, listed grade II) to the east of the House. Work by Cockerell was largely to the House interior but included a link from the House to the orangery, as well as a terrace, balustrade, and Coade stone urns south and west of the House (1831-2, listed grade II).\nGARDENS AND PLEASURE GROUNDS Immediately east of Blaise Castle House, and overlooked by the orangery, stands a thatched dairy (Nash c 1804, listed grade II*) with a small pool in front, in a C19 garden enclosure. Some 40m north-west of the House is a stone seat (Cockerell 1832, listed grade II) constructed to look along a double elm avenue planted by Sir Samuel Astry (late C17), of which survivors were still standing as late as the 1970s. Part of this avenue has been replanted (late 1990s) with lime trees as part of a restoration programme. The pleasure grounds, as defined by the walk shown on Repton\'s plan in the Red Book, are laid out between the House and Blaise Hill. Some 200m south of the House the ground falls sharply to the Hazel Brook and footpaths follow the remains of a compact network of terraced walks and viewpoints, overlooking the brook. Approximately 120m due south of the House, is the levelled site of a root-house built for Thomas Farr. The prospect tower, Blaise Castle, 450m south-west of the House, is approached via two walks, one leading round the north side of the Hill, the other around the south side overlooking the gorge. The latter passes Robber\'s Cave (listed grade II) and a viewpoint, Lover\'s Leap, 500m south-west of the House, constructed after the demolition of Farr\'s bastions as part of Repton\'s improvements in the late 1790s. The former, known as the Vista Walk, runs c 400m along the north side of the Hill before turning and climbing south-east and east to approach the Castle (1766, listed grade II*) from the west, via another of Farr\'s bastions, and following the line of a substantial rampart of the Iron Age hillfort. On the summit of the Hill, in a lawn fringed with Scots pine, Corsican pine, and oak trees, stands the Castle which is now a shell but in 1919 was furnished on the first floor as a dining room for excursions from the House. The Castle was designed as an eyecatcher in views around the landscape and as a prospect tower from which Farr and his guests could watch the shipping in the Bristol Channel.\nThe pleasure grounds also include a walk south along Hazel Brook to a circular bath-house, c 600m south of the House, the foundations of which survive and are known locally as the Giant\'s Soap Dish. The brook contains remnants of Farr\'s ornamental stonework, including small cascades and footbridges. Some 30m north of the carriage bridge which carries the Repton approach over the brook, stands Stratford Mill (listed grade II), re-erected here from the Chew Valley in Somerset when the valley was flooded for a reservoir in the 1950s. Both sides of the gorge feature extensive hanging woods, predominantly of beech trees, and on the east side the high ground is characterised by limestone outcrops, the largest of which, Goram\'s Chair, is a feature of the sublime and picturesque views from Blaise Hill. Some 700m south-west of the House is Arbutus Walk, laid out in the mid C19, which runs north/south along the east slope of Kings Weston Hill. On the north-facing slope of a ridge to the east of Hazel Brook is Rhododendron Walk (c mid C19), c 450m south of the House.\nPARK Fairly level and open parkland lies north and west of Blaise Hill, south of Kings Weston Road, and to the south-east of the House, east of Hazel Brook. The latter is known as the Royals and was an important middle ground in views from Repton\'s approach; in it there are vestiges of early C19 planting. There is more open parkland c 600m south of the House, now partly used as a golf course, which climbs a hillside to the boundary of the site here registered at the summit of Coombe Hill. Footpaths follow former carriage drives to the south-west, connecting the Blaise and Kings Weston estates via Echo Gate, 700m south-west of the House. The park is dotted with occasional specimen trees, including a Wellingtonia, cedars of Lebanon, an Atlantic cedar, and a holm oak.\nKITCHEN GARDEN A brick-built kitchen garden (c 1800, listed grade II) stands c 50m north-east of the House. A large number of mid C20 glasshouses, built as part of Bristol City Council\'s nursery system, are mostly derelict although some are still in use by Cannington College.\nOTHER LAND Blaise Hamlet, which Pevsner (1979) described as \'the ne plus ultra of picturesque layout and design\' is located some 300m north-west of Blaise Castle House, and is now approached from Hallen Road. A footpath leads from the gate in the east boundary wall up six steps and through a shrubbery belt to the Hamlet\'s central green. To the south it is contained by a wall of 1812 (listed grade II). The Hamlet, which extends to approximately 0.8ha, was laid out and built in 1812 to designs by Nash in response to a commission from J S Harford to build nine estate cottages for former estate workers and employees. Each cottage (all listed grade I) is different, but with common design motifs, such as the tall chimneys, dormer windows, and overhanging eaves. A terraced walk runs past and links each cottage. The cottages are irregularly spaced around the undulating green which has a sundial and pump in the middle (1815, listed grade I). Nash\'s scheme included planting proposals to clothe the walls and porches of the cottages.\nR Atkyns, The Ancient and Present State of Gloucestershire (1712), pp 473-4 N Temple, John Nash & the Village Picturesque (1979) N Pevsner, The Buildings of England: North Somerset and Bristol (1958, reprinted 1979), p 468 G Carter et al, Humphry Repton (1982), p 152 N Kingsley, The Country Houses of Gloucestershire, Volume Two, 1660-1830 (1992), pp 79-81 Blaise Castle Estate: Historic Landscape Survey and Management Plan, (Nicholas Pearson Associates 1993) S Daniels, Humphry Repton (1999), pp 48-9, 230-5 T Mowl, Historic Gardens of Gloucestershire (2002), pp 109-13\nMaps Tithe map for Henbury parish, 1840 (Bristol Record Office)\nOS 6"" to 1 mile: 2nd edition published 1904 3rd edition published 1921 OS 25"" to 1 mile: 1st edition published 1881\nIllustrations J Kip, Henbury... The seat of Simon Harcourt Esq, 1712 (in Atkyns 1712) T Robins, View of Henbury, c 1755 (Bristol City Museum and Art Gallery) W Booth, View of Henbury, c 1780 (Blaise Castle House Museum) S H Grimm, topographical drawings, 1788 (Bristol City Museum and Art Gallery; British Library)\nArchival items H Repton, Red Book for Blaise Castle, 1796 (Bristol City Museum and Art Gallery) H Repton, Red Book for Burley-on-the-Hill, 1796 (private collection) Harford Papers (Bristol Record Office)\nDescription written: October 2002 Register Inspector: DAL Edited: September 2003\nNational Grid Reference: ST5575878470, ST5598678888\nThe above map is for quick reference purposes only and may not be to scale. For a copy of the full scale map, please see the attached PDF - 1001426 .pdf\nThe PDF will be generated from our live systems and may take a few minutes to download depending on how busy our servers are. We apologise for this delay.\nThis copy shows the entry on 21-Sep-2018 at 07:12:17.\nEnd of official listing']"	['<urn:uuid:b019a303-665f-43f2-a8aa-51de505e0ab8>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	9	47	2963
45	what temperature thermostat opens engine coolant flow	The engine thermostat is designed to open allowing antifreeze/coolant to flow to the radiator when the thermostat inside the engine reaches about 198F - 99C.	"['Temperature Warning Light On\nLight On for Engine Coolant Temperature Hot All warning lights are meant to alert you that a problem in one of the operation systems in your car has or is malfunctioning. The engine temperature warning light is conveniently placed in the instrument cluster display panel. Warning lights generally come in two colors, orange and red. Red is reserved for the more serious malfunctions that need attention immediately while orange is meant to be serviced at your earliest convenience. When a particular system fails a warning light device has been designed into most car systems such as brakes, engine, transmission, seat belts, air bag, vital fluid levels and more. All engine temperature warning lights are colored red because if this light illuminates the engine is overheating. If left unattended the engine will suffer irreversible damage. When the engine overheats the engine pistons expand causing the cylinder wall to become scored. When cylinder wall scoring occurs the piston rings cannot seal to the wall allowing combustion bypass when the engine cools. This is why after an engine has been overheated it can have low power and cause the engine to burn oil. This condition is called blow-by and cannot be repaired unless the engine is rebuilt or replaced. If you think this condition has occurred to a car an engine compression test is needed. If this test shows that the compression is below the normal range you can suspect engine overheating has occurred. In sever engine overheating conditions the internal engine parts can swell forcing the engine to stop turning or seize. If the engine seizes due to overheating the internal parts of the engine are damaged and replacement or disassemble for repair is needed. A common cause for engine overheating is lack of coolant in the engine cooling system. When lack of coolant is present the heat transferring properties is lost. This absence of heat transfer is what causes the engine overheating. A car radiator cannot work if there is no coolant inside the system. The coolant level should be the first check after an engine has overheated. Never check engine coolant until the engine has cooled completely, preferably after the engine has cooled over night. Another popular reason for engine overheating is the thermostat can stick closed not allowing the coolant to flow to the radiator. The engine thermostat is designed to open allowing antifreeze/coolant to flow to the radiator when the thermostat inside the engine reaches about 198F - 99C. Even if the coolant is at the proper level this condition can occur. Only check for this condition after the engine had cooled down a safe temperature. Then check the temperature of both the upper and lower hoses. If the top hose is warm and the bottom hose is cool you can suspect the thermostat has stuck and needs to be tested or replaced. If your car is equipped with radiator cooling fans it is vital these fans work properly. If the cooling fan fail to operate the heat from the radiator will not be removed from the engine coolant causing the engine to overheat. If the engine is overheating the cooling fans should be on in full force. If the cooling fans have not come on the electrical system that power the fans must be checked. If an engine overheating condition has occurred one of the most popular causes for this condition is cooling fan motor failure. These fan or fans use a high amperage draw and are prone to failure. Other popular reasons for failure is a fuse or relay operation failure. Use a test light or voltmeter to test these circuits and replace burnt or shorted components with new. In most cases engine overheating is obvious and can be detected by additional engine noise, poor running or low power even before the engine temperature warning light is illuminated. If you car is equipped with both a temperature warning light and a engine temperature gauge it is a good idea to periodically inspect the gauge while driving. When cars are designed with a temperature gauge along with a warning light it is a ""redundant"" system and is used as a failsafe. In this case the warning light works in correlation with the gauge and will only illuminate when the gauge is in the upper range of the gauge span. Never depend on a warning light to illuminate when the temperature gauge is in the ""hot"" range. If the temperature gauge is in the ""hot"" range shut the engine off in a safe place on the highway. Allow the engine to cool before inspection. Sometimes the temperature gauge can go to the hot range and then back down to the cool range if this condition happens check the electrical contact at the temperature gauge sender electrical connection. If this contact is loose it will cause intermittent high resistance allowing the temperature gauge circuit to connect and then go to high resistance when the engine it running due to engine vibration. When the connections goes to high resistance the temperature gauge will go to the ""hot"" position ever thought the engine is not hot. Always make sure the connection is tight and making good contact with no corrosion. Even if the connection is tight corrosion can cause high resistance when the engine is running. If the cooling system radiator is plugged it can cause the engine to overheat. A radiator can become plugged by corrosion or debris in the cooling system. If either of these conditions occur the radiator cannot pass coolant through the cooling tube which are connected to the cooling fin of the radiator. In turn the engine coolant will be hot as it re-enters the engine block and cylinder heads causing the engine to overheat. To check for this condition (after the engine has cooled overnight) remove the radiator cap or upper radiator hose. Then with a flashlight shine the beam into the radiator and down or over to the cooling tube openings. If the opening look clogged the radiator must be replaced or serviced (boiled out) by a radiator repair shop. Some of the overly un-popular causes for engine overheating are internal engine gasket failures such as a head or intake manifold gasket failure. This condition is accompanied by low coolant level in most cases. In extreme cases the engine block or cylinder head can develop a crack due to manufacturing flaws. This condition will present itself in much the same way as a failed internal engine gasket but the remedy is much most expensive. Usually when this condition occurs the engine block or cylinder head needs replacement. Anytime your car engine fails you must consider he engine mileage as it might be more beneficial to replace the engine as a unit opposed to repairing an high mileage engine. Also if the car has seen better days it might be time to purchase a different car. This decision can be difficult especially if the car is still in good condition, just old. Do not attempt to open the engine cooling system at anytime unless the engine has cooled overnight. The engine cooling system is contained under pressure, usually about 18 psi. This coolant is hot about 230F or 102C and it will burn you or anyone standing with range of coolant projecting from the engine compartment. Sometimes a car hood can be hot from a result of an overheating engine. Before opening the hood touch the hood quickly to measure the hood temperature before opening. Never open a overheated hood, allow a cooling period, preferably over night. Common causes for the engine temperature light to be illuminated\n- Low Coolant Level - When the temperature light comes on the first thing that should be checked is the coolant level. If this level becomes low due to a leak the engine can over heat\n- Failed Coolant Fan Operation - The cooling fan must be operational or the engine will overheat causing the engine temperature light to come on.\n- Plugged Radiator - The car radiator must be operational, free form corrosion with clean cooling fins\n- Failed or Stuck Thermostat - If the engine thermostat fails to open it will not allow the coolant/antifreeze to flow from the engine to the radiator']"	['<urn:uuid:6d6eccbe-630c-4594-8711-670b693abfb4>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	7	25	1381
46	quantum computer calculate large numbers advantage over classical computer speed difference example	According to a demonstration in 2019, Google ran a calculation on a quantum computer in just a few minutes that would take a classical computer 10,000 years to complete. Even more impressively, a team in China later claimed they performed a calculation in 200 seconds that would take a standard computer 2.5 billion years - making it 100 trillion times faster.	['Quantum computing is a space of registering zeroed in on creating PC innovation dependent on the standards of quantum hypothesis (which clarifies the conduct of energy and material on the nuclear and subatomic levels). PCs utilized today can just encode data in bits that take the worth of 1 or 0—confining their capacity.\nQuantum computing, then again, utilizes quantum bits or qubits. It tackles the special capacity of subatomic particles that permits them to exist in more than one state (i.e., a 1 and a 0 simultaneously).\nQuantum registering takes advantage of the perplexing conduct that researchers have been noticing for a really long time in nature’s littlest particles – think molecules, photons or electrons. At this scale, the old-style laws of material science stop applying, and on second thought we shift to quantum rules.\nWhile analysts don’t comprehend everything about the quantum world, what they can be sure of is that quantum particles hold huge potential, specifically to hold and handle a lot of data. Effectively managing those particles in a quantum PC could trigger a blast of figure power that would incredibly propel development in many fields that require complex computations, similar to sedate revelation, environment demonstrating, monetary enhancement, or coordination.\nQuantum PCs perform computations dependent on the likelihood of an article’s state before it is estimated – rather than only 1s or 0s – which implies they can possibly process dramatically more information contrasted with traditional PCs.\nTraditional PCs complete coherent tasks utilizing the clear situation of an actual state. These are typically double, which means their tasks depend on one of two positions. A solitary state -, for example, on or off, up or down, 1 or 0 – is known as a little.\nIn quantum computing, activities rather utilize the quantum condition of an item to deliver what’s known as a qubit. These states are the vague properties of an article before they’ve been identified, as the twist of an electron or the polarization of a photon.\nRather than having a reasonable position, unmeasured quantum states happen in a blended ‘superposition’, similar to a coin turning through the air before it lands in your grasp.\nThese superpositions can be caught with those of different articles, which means their ultimate results will be numerically related regardless of whether we know yet what they are.\nThe complicated arithmetic behind these agitated conditions of trapped ‘turning coins’ can be connected to uncommon calculations to make short work of issues that would take an old-style PC quite a while to work out… on the off chance that they might at any point compute them whatsoever.\nSuch calculations would be helpful in tackling complex numerical issues, delivering hard-to-break security codes, or anticipating numerous molecule cooperations in substance responses.\nHow Does a Quantum Computer Respond?\nUtilizing qubits, quantum PCs could buzz through estimations that would require some investment — on the off chance that they could even complete them.\nFor instance, the present PCs utilize eight pieces to address any number somewhere in the range of 0 and 255. Because of highlights like superposition, a quantum PC can utilize eight qubits to address each number somewhere in the range of 0 and 255, all the while.\nIt’s an element like parallelism in registering: All prospects are figured without a moment’s delay rather than successively, giving colossal speedups.\nIn this way, while an old-style PC ventures through long division computations each in turn to factor a humongous number, a quantum PC can find the solution in a solitary advance. Blast!\nThat implies quantum PCs could reshape entire fields, similar to cryptography, that depend on figuring what is today inconceivably enormous numbers.\nHow Does Quantum Computing Work?\nTry not to hope to construct your own quantum PC like a DIY PC with parts searched from rebate canisters at the neighborhood gadgets shop.\nThe modest bunch of frameworks working today regularly require refrigeration that establishes working conditions only north of outright zero. They need that registering cold to deal with the delicate quantum expresses that power these frameworks.\nIn an indication of how hard building a quantum PC can be, one model suspends a molecule between two lasers to make a qubit. Attempt that in your home studio!\nQuantum processing takes nano-Herculean muscles to make something many refer to as a trap. That is when at least two qubits exist in a solitary quantum express, a condition now and again estimated by electromagnetic waves simply a millimeter wide.\nWrench up that wave with a hair a lot of energy and you lose snare or superposition, or both. The outcome is a loud state called decoherence, the identical quantum registering of the blue screen of death.\nQuantum computing benefits\nScientists have dove profoundly into the quantum PC versus old-style PC examination and have since a long time ago anticipated that quantum PCs could handle particular kinds of issues — particularly those including an overwhelming number of factors and expected results, similar to recreations or streamlining questions — a lot quicker than any traditional PC.\nBe that as it may, presently we’re beginning to see traces of this expected becoming reality.\nIn 2019, Google said that it ran an estimation on a quantum PC in only a couple of moments that would take a traditional PC 10,000 years to finish. Barely a year after the fact, a group situated in China made this a stride further, asserting that it had played out estimation in 200 seconds that would take a standard PC 2.5B years — 100 trillion times quicker.\nGenuine Example of a Quantum Computer\nGoogle (GOOG) is burning through billions of dollars on its arrangement to assemble its quantum PC by 2029. The organization has opened aground in California, called Google AI, to assist it with meeting its objective. Google has been putting resources into this innovation for quite a long time. Also, so have different organizations, like Honeywell International (HON) and International Business Machine (IBM). IBM hopes to hit significant quantum registering achievements before long.\nWhile a few organizations have assembled individual (albeit costly) quantum PCs, there is as yet nothing accessible on the business side. What’s more, there’s an advantage in quantum registering and its innovation, with JPMorgan Chase and Visa investigating the innovation. Once created, Google could dispatch a quantum figuring administration by means of the cloud.\nOrganizations can likewise get to quantum innovation without building a quantum PC. IBM plans to have a 1,000-qubit quantum PC set up by 2023. For the time being, IBM permits admittance to its machines assuming they’re important for its Quantum Network. Those that are essential for the organization incorporate exploration associations, colleges, and laboratories.\nMicrosoft likewise offers organizations admittance to quantum innovation by means of the Azure Quantum stage. This is not normal for Google, which doesn’t offer admittance to its quantum computers.']	['<urn:uuid:8ae0a21b-b64e-4df9-91bc-58f487f919d2>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	12	61	1138
47	embedded systems expert difference between raspberry pi gpio connection cuda gpu parallel processing	Raspberry Pi GPIO connections and CUDA GPU processing represent different approaches to hardware interaction. The Raspberry Pi's GPIO enables direct serial communication with devices through physical pins (like TX/RX), making it suitable for tasks like firmware flashing where direct hardware access is needed. CUDA GPU processing, conversely, focuses on parallel computation through software abstraction - it allows programmers to write code that runs across thousands of GPU cores simultaneously, without requiring physical pin connections. While GPIO is about direct hardware interfacing through individual pins, CUDA enables massive parallel processing through high-level software interfaces.	['This tutorial will cover all of the steps necessary to flash an ESP device with ESPTool using a Raspberry Pi. The device I shall be flashing is a MagicHome RGB LED controller. It is a ubiquitous design that comes in several flavours, usually branded as Arilux. However this tutorial can easily be adapted to work with any ESP-based device.\nThe Raspberry Pi is an ingenious little device and has many uses. Every Tek enthusiast would benefit greatly from keeping a spare tucked away in a draw. In need of a new Pi? Click here to check the latest price of the newest and super-powerful Pi 4.\nIn this tutorial we will make use of the highly useful GPIO port on the Pi, as two of these pins share their function with the serial port. This means the Pi can communicate with devices that also have a serial port. Devices such as ESP devices have a serial bootloader that allows the new firmware to be sent over the serial connection.\nThe chip does in fact flash itself with the firmware we deliver to it over the serial port, using its own bootloader firmware. The tool we will use to transfer our firmware is ESPTool. In order to flash your device with your Pi, you will need to be able to access the circuit board inside and its likely you will need to solder some wires to the board.\nSome ESP devices can be flashed over the air or via USB. If you cannot open up your device to access the circuit inside or you want to try to avoid soldering, take a look at the tutorial on how to flash Tasmota over the air. If you are using a device like the Wemos D1 Mini that has a USB port, take a look at the tutorial for flashing over USB.\nIf you are new to Tasmota and you want to better understand some of its features and benefits, check out this article on the Tasmota and the cloud-free smart home.\n- Raspberry Pi with the Raspbian operating system\n- An ESP device to flash\n- An old RC servo lead or jumper wires\n- Soldering iron and some solder\n- A computer for SSH, or a screen, keyboard and mouse for your Pi\nSet up the Pi\nIf you are using your Pi as a Home Assistant Server, it is recommended that you install Raspbian to a separate USB card to use in the Pi whilst flashing your device. Once finished, you can switch back to your Hassio SD card. Check out this tutorial on how to install Raspbian to an SD card.\nIf you choose to connect a screen, keyboard and mouse to the Pi and use it as a computer, or if you are accessing the desktop through remote VNC, go ahead and open up the terminal.\nIf you would rather just control the Pi with another computer to save the hassle of digging out an old set of peripherals, details on how to set up SSH and remote desktop can be found in the latter part of this tutorial.\nYou should now be ready to start inputting commands in to the terminal at the Pi prompt. I will be using SSH running in terminal on my Mac.\nFirst of all we should make sure that the Pi is up to date, so go ahead and enter the following commands in to the Pi terminal.\nsudo apt-get update\nsudo apt-get upgrade\nIn order to flash the firmware to our device, we are going to use a program called ESPTool. It runs under either Python2.7 or Python 3.4+ and we will use pip to install it. Raspbian comes with Python pre-installed so we can go ahead and install ESPTool right away.\nThere is a script that makes it nice and easy to install. Go ahead and enter the following command to install the dependencies.\npip install esptool\nOnce the installer script has finished, we need to navigate to the folder where we would like to install the ESPTool script. You can choose any location you like but for the purpose of this tutorial, I will install it into the documents folder.\nEnter the following command to change to the folder where you wish to install the script.\nNow we simply need to clone the files from Git. Go ahead and enter the following command.\ngit clone https://github.com/espressif/esptool.git\nWait for the repository to be cloned and thats it! ESPTool is ready to use.\nNow that ESPTool is installed, we need to download the firmware. For the purpose of this tutorial we will use the recommended Tasmota binary, but you can use an alternative binary if you prefer. The binary is a pre-compiled version of the software and comes as a single .bin file ready to be loaded straight on to the ESP.\nTo make things easier I would recommend downloading the .bin file in to the same directory as ESPTool, but you can choose any location you like. Enter the following command to change directory to the location where you wish to download Tasmota. I will be downloading it in to the ~/Documents/esptool/ folder.\nTasmota binaries are available from http://thehackbox.org/tasmota/. The following command will download the recommended tasmota.bin file but you can change the link in the command to download a different binary of your choice.\ncurl http://thehackbox.org/tasmota/tasmota.bin --output tasmota.bin\nSet up the Pi\nThere are a few things that we need to do with the Pi configuration. By default the Pi is using the serial port in order to provide access to the terminal by serial connection. As we need the serial port for flashing, we must disable it.\nWe can disable this from the Pi config menu. Go ahead and enter the following command to bring up the menu.\nNext you will need to select interfacing options, followed by serial in the menus.\nYou will be presented with the option to disable the login shell being accessible over the serial port. Go ahead and select no.\nThe next screen will ask if you would like serial hardware to be enabled. As we wish to use the serial port for flashing, go ahead and select yes.\nThe final screen will the confirm the options that you just selected. The configuration should be as follows.\nYou will then be taken back to the main menu. Click finish and reboot your Pi. Now your Pi is configured for flashing and we can set up the hardware.\nWe need to connect our Pi to the device we wish to flash. The following diagram shows the pins that we are interested in on the GPIO header, TXD, RXD and Ground (GND). They are conveniently located next to each other, so we can recycle an old servo lead to make the connection to the Pi header.\nI would highly recommend that you use an external power source for your device, so as not to cause problems or damage the Pi. However if you are confident enough and know what you are doing, you can take power from the Pi too.\nDisclaimer: drawing too much current from your Pi power pins can cause problems and could even damage your Pi. You have been warned!\nOpening your device\nThe device I am flashing is a MagicHome LED controller. I really like these devices as they are very cheap, easy to open up and they have labelled test points for all of the connections on the bottom of the circuit, which we can use to easily connect the wiring.\nIf you are flashing one of these MagicHome devices, simply pry the case open where the RGB connector is located. There are little clips on the side of the case that will pop open. You can gently pry with a small flat-head screwdriver if needed. I have flashed a few of these devices and I didn’t break one of the clips yet! Once complete simply put the circuit back in the case and snap the lid back on.\nIt is also possible to flash these devices using their standard power supply adapter, we don’t need to use the power from the Pi.\nThis is device specific so you may need to search for the details of where to connect your device and how to power it. There will always be a TXD, RXD and GND somewhere on your ESP device, usually it will just be a case of locating the correct points to solder to.\nTip: I usually shop for devices that I know are easy to flash, rather than try to figure out if it is easy to flash something after I buy it. The Blakadder Templates Repository is usually a good place to start and everything here also has a corresponding Tasmota configuration template.\nIn my case the MagicHome device conveniently has the test points labelled. We need to connect TXD, RXD and GND to the Pi.\nWe also need to connect GPIO0 to GND in order to enabled flashing mode on the ESP chip. On these particular MagicHome devices this pin is labelled ‘IO0‘ and in this case it is partially covered by a black line, although we should still be able to solder to it. We could probably remove it with some rubbing alcohol if necessary.\nThe following block diagram should make it clear as to what needs to be connected where. I have omitted power as we will use the power adapter supplied with the device. Please note that it is important to still connect ground between the devices.\nThe easiest way to connect to the Pi header is to use an old RC servo lead, you can buy them or just cut one from an old RC servo if you have one lying around. Alternatively you could solder directly to the Pi header, but I prefer this more elegant solution.\nThe end of the cable with the plug will connect to the Pi header. Cut, strip and solder tin the ends of the wires at the other end of the lead ready to solder to the device.\nWe will also need another short piece of wire, stripped and solder tinned at both ends to connect IO0 to GND.\nNote that some devices have a button connected to GPIO0, you can use this instead if you like. You will need to press and hold it just before you run the ESPTool command.\nConveniently on these MagicHome devices there are two GND test points, therefore we can use one to connect to the Pi and one to connect IO0 to GND. If you do not have two GND pads available, just connect the Pi and IO0 to the same GND.\nSoldering the wires\nTo solder on the wires, first make sure the ends of the wires are solder tinned and trim them to around 2mm in length. Then apply a little blob of solder to the test points. Now it should be nice and easy to neatly solder on the wires.\nNote the little link lead connected between the extra GND pad and IO0, which puts the MagicHome device in to flashing mode. It is acceptable to solder this to the same GND as the Pi if you don’t have the extra GND point.\nYou MUST make sure you are absolutely certain that the wiring is correct, I always check THREE times. Remember, the first two pins on the header are +5V! Incorrect wiring could damage your device and/or your Pi, proceed at your own risk.\nNow that the servo wire is attached to the device, we can connect it to our Pi. I have specifically used black for GND, therefore the black wire should connect to pin 6 of the Pi header (see the Pi pin out diagram).\nCongratulations! We have our hardware ready and now we can start to flash our device.\nPower up your Pi and bring up the terminal. If you did not connect your device to its power supply yet, go ahead and connect it.\nNavigate to the folder where you installed ESPTool. Earlier in the tutorial I installed ESPTool to the ~/Documents folder but you should navigate to your chosen location from the earlier installation.\nErase the flash\nThe first thing we should do is erase the flash memory to prevent any problems. The following command is used to erase the memory.\nsudo ./esptool.py --port <your-device-port> erase_flash\nWe need to specify <your-device-port>, it is the port on the Pi that the ESP device is connected to.\nIf you are using a Pi 3 or 4 the port you need to use is /dev/ttyS0.\nIf you are using a Pi 2 then the port you need to use is /dev/ttyAMA0.\nThe reason for this is ttyAMA0 is the hardware serial port. On the Pi 2 this is connected directly to the GPIO pins. On the Pi 3 and 4 this is hard wired to the Bluetooth module and an additional serial port ttyS0 was added. This is the port that connects to the GPIO on the newer Pi 3 and Pi 4 boards.\nNow we can erase the flash memory. I would imagine that most folks are using the Pi 3 or 4 by now, therefore this would be the correct command.\nsudo ./esptool.py --port /dev/ttyS0 erase_flash\nI am using an old Pi 2 that I have lying around for flashing, therefore I will be entering the following command.\nsudo ./esptool.py --port /dev/ttyAMA0 erase_flash\nUpon successful completion you will receive confirmation from the script and your output should look something like the following.\nOnce complete you should disconnect your ESP device from the power supply and then reconnect it to reboot it.\nFlash the firmware\nNow it is finally time to flash the firmware! In order to do so we will use the following command.\nesptool.py --port <your-device-port> write_flash -fm dout -fs 0x0 <path-to-tasmota>.bin\nAs before you should use the correct port for your device. As I am using an old Pi 2, I shall be using port ttyAMA0. If you do not have your .bin file in the same directory as esptool.py, you should enter the full path to the .bin file. I will use the following command to flash my device with the tasmota.bin file in the same directory.\nsudo ./esptool.py --port /dev/ttyAMA0 write_flash -fm dout 0x0 tasmota.bin\nIf you are using a Pi 3 or Pi 4 then you will want to use /dev/ttyS0.\nsudo ./esptool.py --port /dev/ttyS0 write_flash -fm dout 0x0 tasmota.bin\nFlashing can take a minute or two so perhaps go get yourself a nice cold beverage. Once complete the script will confirm verification.\nThe last thing to do is disconnect your device from the Pi and the power supply and remove the servo lead/wiring that you added. Once you remove the link between IO0 and GND the device will boot as normal. If you are using Tasmota, you should now see it appearing as an access point.\nI hope this tutorial helped you flash your device successfully, please let me know what you thought in the comments below.', 'Numba is an open-source just-in-time (JIT) Python compiler that generates native machine code for X86 CPU and CUDA GPU from annotated Python Code. (Mark Harris introduced Numba in the post “NumbaPro: High-Performance Python with CUDA Acceleration”.) Numba specializes in Python code that makes heavy use of NumPy arrays and loops. In addition to JIT compiling NumPy array code for the CPU or GPU, Numba exposes “CUDA Python”: the CUDA programming model for NVIDIA GPUs in Python syntax.\nBy speeding up Python, we extend its ability from a glue language to a complete programming environment that can execute numeric code efficiently.\nFrom Prototype to Full Dataset with @cuda.jit\nWhen doing exploratory programming, the interactivity of IPython Notebook and a comprehensive collection of scientific libraries (e.g. SciPy, Scikit-Learn, Theano, etc.) allow data scientists to process and visualize their data quickly. There are times when a fast implementation of what you need isn’t in a library, and you have to implement something new. Numba helps by letting you write pure Python code and run it with speed comparable to a compiled language, like C++. Your development cycle shortens when your prototype Python code can scale to process the full dataset in a reasonable amount of time.\nWorking with Dr. Alex Dimakis and his team at UT Austin, we implemented their densest-k-subgraph (DkS) algorithm . Our goal was to extract the densest domain from the 2012 WebDataCommon pay-level-domain hyperlink graph using one NVIDIA Tesla K20 GPU accelerator. We developed the entire application using NumPy for array operations, Numba to JIT compile Python to CUDA, NumbaPro for GPU sorting and cuBLAS routines, and Bokeh for plotting the results. Continue reading →\nEvery year NVIDIA’s GPU Technology Conference (GTC) gets bigger and better. One of the aims of GTC is to give developers, scientists, and practitioners opportunities to learn with hands-on labs how to use accelerated computing in their work. This year we are nearly doubling the amount of hands-on training provided from last year, with almost 2,400 lab hours available to GTC attendees!\nWe have two types of training this year at GTC: instructor-led labs and self-paced labs. And to help you keep up with one of the hottest trends in computing, this year we’re featuring a Deep Learning training track. Keep reading for details. If you haven’t registered for GTC yet this year, keep reading for a discount code.\nDeep Learning Track\nThere is an explosion of Deep Learning topics at GTC, and it’s not limited to the keynotes, talks and tutorial sessions. We’ll feature at least six hands-on labs related to accelerating facets of Deep Learning on GPUs. From an introduction to Deep Learning on GPUs to cutting-edge techniques and tools, there will be something for everyone. Be sure to get to these labs early to get yourself a seat! Here are a few of the labs available in this track:\nIntroduction to Machine Learning with GPUs: Handwritten digit classification (S5674)\nSo far in the CUDA Python mini-series on CUDACasts, I introduced you to using the @vectorize decorator and CUDA libraries, two different methods for accelerating code using NVIDIA GPUs. In today’s CUDACast, I’ll be demonstrating how to use the NumbaPro compiler from Continuum Analytics to write CUDA Python code which runs on the GPU.\nIn CUDACast #12, we’ll continue using the Monte Carlo options pricing example, and I’ll show how to write the step function in CUDA Python rather than using the @vectorize decorator. In addition, by using the nvprof command-line profiler, we’ll be able to see the speed-up we’re able to achieve by writing the code explicitly in CUDA.\nIn the previous episode of CUDACasts I introduced you to NumbaPro, the high-performance Python compiler from Continuum Analytics, and demonstrated how to accelerate simple Python functions on the GPU. Continuing the Python theme, today’s CUDACast demonstrates NumbaPro’s support for CUDA libraries.\nThe optimized algorithms in GPU-accelerated libraries often provide the easiest way to accelerate applications. NumbaPro includes a Python API interface to the cuBLAS, cuFFT, and cuRAND libraries. In CUDACasts episode #11 I show you how to use cuRAND to accelerate random-number generation for a Python Monte Carlo options pricing example, achieving a 17x overall speed-up.\nCUDA 5 added a powerful new tool to the CUDA Toolkit: nvprof. nvprof is a command-line profiler available for Linux, Windows, and OS X. At first glance, nvprof seems to be just a GUI-less version of the graphical profiling features available in the NVIDIA Visual Profiler and NSight Eclipse edition. But nvprof is much more than that; to me, nvprof is the light-weight profiler that reaches where other tools can’t.\nUse nvprof for Quick Checks\nI often find myself wondering if my CUDA application is running as I expect it to. Sometimes this is just a sanity check: is the app running kernels on the GPU at all? Is it performing excessive memory copies? By running my application with nvprof ./myApp, I can quickly see a summary of all the kernels and memory copies that it used, as shown in the following sample output.\nIn its default summary mode, nvprof presents an overview of the GPU kernels and memory copies in your application. The summary groups all calls to the same kernel together, presenting the total time and percentage of the total application time for each kernel. In addition to summary mode, nvprof supports GPU-Trace and API-Trace modes that let you see a complete list of all kernel launches and memory copies, and in the case of API-Trace mode, all CUDA API calls. Continue reading →\nThis week’s CUDACast continues the Parallel Forall Python theme kicked off in last week’s post by Mark Harris, demonstrating exciting new support for CUDA acceleration in Python with NumbaPro. This video is the first in a 3-part series showing various ways to accelerate your Python code on NVIDIA GPUs.\nPython is a high-productivity dynamic programming language that is widely used in science, engineering, and data analytics applications. There are a number of factors influencing the popularity of python, including its clean and expressive syntax and standard data structures, comprehensive “batteries included” standard library, excellent documentation, broad ecosystem of libraries and tools, availability of professional support, and large and open community. Perhaps most important, though, is the high productivity enabled by a dynamically typed, interpreted language like Python. Python is nimble and flexible, making it a great language for quick prototyping, but also for building complete systems.\nBut Python’s greatest strength can also be its greatest weakness: its flexibility and typeless, high-level syntax can result in poor performance for data- and computation-intensive programs. For this reason, Python programmers concerned about efficiency often rewrite their innermost loops in C and call the compiled C functions from Python. There are a number of projects aimed at making this optimization easier, such as Cython, but they often require learning a new syntax. Ideally, Python programmers would like to make their existing Python code faster without using another programming language, and, naturally, many would like to use accelerators to get even higher performance from their code.\nNumbaPro: High Productivity for High-Performance Computing\nIn this post I’ll introduce you to NumbaPro, a Python compiler from Continuum Analytics that can compile Python code for execution on CUDA-capable GPUs or multicore CPUs. Since Python is not normally a compiled language, you might wonder why you would want a Python compiler. The answer is of course that running native, compiled code is many times faster than running dynamic, interpreted code. NumbaPro works by allowing you to specify type signatures for Python functions, which enables compilation at run time (this is “Just-in-Time”, or JIT compilation). NumbaPro’s ability to dynamically compile code means that you don’t give up the flexibility of Python. This is a huge step toward providing the ideal combination of high productivity programming and high-performance computing. Continue reading →\nProgramming environments like C and Fortran allow complete and unrestricted access to computing hardware, but often require programmers to understand the low-level details of the hardware they target. Although these efficiency-oriented systems are essential to every computing platform, many programmers prefer to use higher level programming environments like Python or Ruby, focused on productivity rather than absolute performance. Productivity-focused programmers solving large or intensive problems do need high performance, and many seek to exploit parallel computing, but without the costs of understanding low-level hardware details or programming directly to a particular machine.\nCopperhead is a project that aims to enable productivity-focused programmers to take advantage of parallel computing, without explicitly coding to any particular machine. Copperhead programs use familiar Python primitives such as map and reduce, and they execute in parallel on both CUDA-enabled GPUs as well as multicore\nParallel Hello World: axpy\nLet’s start with an example: below find Copperhead code for axpy, the “hello world” of parallel programs. (axpy is the type-generic form of saxpy. See Six Ways to SAXPY for more.)\nfrom copperhead import *\nimport numpy as np\ndef axpy(a, x, y):\nreturn [a * xi + yi for xi, yi in zip(x, y)]\nn = 1000000\na = 2.0\nx = np.random.rand(n)\ny = np.random.rand(n)\ngpu_result = axpy(a, x, y)\ncpu_result = axpy(a, x, y)\nThis post is a GPU program chrestomathy. What’s a Chrestomathy, you ask?\nIn computer programming, a program chrestomathy is a collection of similar programs written in various programming languages, for the purpose of demonstrating differences in syntax, semantics and idioms for each language. [Wikipedia]\nThere are several good examples of program chrestomathies on the web, including Rosetta Code andNBabel, which demonstrates gravitational N-body simulation in multiple programming languages. In this post I demonstrate six ways to implement a simple SAXPY computation on the CUDA platform. Why is this interesting? Because it demonstrates the breadth of options you have today for programming NVIDIA GPUs, and it covers the three main approaches to GPU computing: GPU-accelerated libraries, GPU compiler directives, and GPU programming languages.\nSAXPY stands for “Single-Precision A·X Plus Y”. It is a function in the standard Basic Linear Algebra Subroutines (BLAS)library. SAXPY is a combination of scalar multiplication and vector addition, and it’s very simple: it takes as input two vectors of 32-bit floats X and Y with N elements each, and a scalar value A. It multiplies each element X[i] by A and adds the result to Y[i]. A simple C implementation looks like this.\nvoid saxpy(int n, float a, float *x, float *y)\nfor (int i = 0; i < n; ++i)\ny[i] = a*x[i] + y[i];\n// Perform SAXPY on 1M elements\nsaxpy(1<<20, 2.0, x, y);']	['<urn:uuid:1190b08e-7045-4f81-a9bb-95f1ea5a38e1>', '<urn:uuid:bd398ab7-3a7f-4ea1-acad-7b931be226f7>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T06:15:13.904987	13	93	4264
48	how sustainable development water usage agriculture health climate change linked	Sustainable development, water usage in agriculture, health, and climate change are interconnected in several ways. Climate change impacts health by affecting water access and agriculture through changing weather patterns. Agricultural practices, particularly in conventional farming, consume about 70% of global freshwater and contribute to environmental degradation. Organic farming uses 30% less water than conventional methods while providing healthier produce and less groundwater pollution. These environmental impacts ultimately affect population health, which McMichael argues is the real 'bottom line' indicator of environmental conditions.	['The subject of the seventh and final taught week of the emerging trends in global health course focused on the concept of sustainable development and the environment and how these relate to health.\nA widely recognised definition of sustainable development is provided by the United Nations:\n“Sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs”\nUnited Nations 1987\nThere are three key pillars to sustainable development:\n- economic development,\n- social equity\n- and environmental protection.\nand health is involved in all three of these.\nClimate change and the negative impact of our activities on our environment continue to be controversial issues despite widespread recognition that the quality and functioning of our environment are key determinants of our health. McMichael (2009) argues that the health of the population is the real “bottom line” indicator of environmental and social conditions and he suggests that “environmental sustainability” must be ultimately about sustaining health.\nClimate change has the potential to negatively impact upon health and reduce living standards around the world, through changing weather patterns affecting access to water and agriculture and changes to regions affected by diseases. Many countries of the world lack the wealth, adequate technology and government institutions to deal with climate change, exacerbating inequities in health between poor and rich areas of the world. This reflects what is sometimes referred to as ‘environmental injustice’. Costello et al (2009)\nThe concept of ecosystem services was introduced which attempts to highlight how our natural environment functions to provide us with many of the basic necessities we need to live. For example clean water is originally provided by rainfall collected in a watershed and filtered through plants, soils and rocks. This concept aims to engage the business and government communities in appreciating these vital roles when taking development decisions.\nMeasuring sustainable development\nThe commonly used measures of development and progress focus solely on the economic pillar of sustainable development ignoring the other two pillars. So it is not surprising that the levels and changes in these economic measures do not correspond to levels or changes in population health.\n“Traditionally, it was thought that economic development (wealth, as measured by Gross Domestic Product) would automatically lead to health” however comparisons between countries such as the US and Costa Rica indicates that higher GDP does not on its own result in greater life expectancy. In this case US GDP is 4 times that of Costa Rica but average life expectancy is still one year less (Alsan et al., 2006).\nAn example of an attempt to provide a more holistic measure of development that also encompasses health and wellbeing is the Happy Planet Index. This uses three measures (life expectancy, experienced well-being and Ecological Footprint) to determine a score for each country that indicates “the extent to which countries deliver long, happy, sustainable lives for the people that live in them “. The map below shows the HPI resulting scores for the countries of the world where Green indicates a good score and red a poor score.\nThe food system\nObesity is now recognised as a global problem with enormous health implications. Current data suggests that in 19 out of the 34 OECD countries more than 50% of the population is over weight. While food selection and diet are considered an individual’s choice, the food systems that provide the options from which we can choose affect to a large degree the choices that individuals end up making. For instance subsidies that promote the growth of particular crops (e.g. corn) result in some ingredients being artificially low in cost (e.g. high fructose corn syrup). Inevitably these ingredients are then widely utilised in the production of the cheapest processed foods available. One course presenter, Prof. Schuter argued passionately that the world’s food systems need to be radically re-engineered and these subsidies be refocused to promote healthy food types over the unhealthy ones.\nThe secondary affects of the adoption of healthier diets also has implications for social equality (e.g. local food sourcing supporting small farmers) and the environment (e.g. reductions in livestock reducing green house gas emissions). The links between health related decisions and positive environmental implications go beyond the food system Kickbusch et al. (2010, p. 7) summarises this by arguing that\n“in many cases, the best choices for health are also the best choices for the planet; and the most ethical and environmental choices are also good for health”.\nPost 2015 Millenium Development Goals\nThe 8 Millenium Development Goals (MDGs) have been the focus of global efforts to achieve sustainable development since their agreement in 2000. For each goal a set of targets were defined to be achieved by 2015, but as you would anticipate despite much progress not all of these will not all be met. Haines et al. (2012) suggest this failure is due to a lack of integration between the efforts focused on separate social (including health), economic, and environmental priorities.\nWith 2 years remaining for the MDGs there is now a programme to determine a new set of Sustainable Development Goals (SDGs) to set the new development agenda. Contrary to the MDGs, which mainly focus on poor and emerging nations, it is anticipated that the SDGs will encompass both developed and developing countries. It is important that health issues are recognised in these new goals and the momentum achieved in the health related MDGs is not lost after 2015 (WHO task team 2013).\nThe United Nations has established top down and bottom up processes to gather opinions to influence the SDGs. The bottom up channel is an open consultation mechanism delivered by the My World and World We Want websites which can be used by anyone to add their perspective.\nImplications for Physical Therapy / Physiotherapy\nSustainable development is a worthy and important objective and we must consider the ability of future generations to meet their needs in the decisions we take daily. These decisions may be work related such as where to source our healthcare products, what form of transport we use to commute to work and personal such as what food we choose to eat each day.\nIt is apparent to us through our work with patients that wealth does not equate to health and many of the global health problems of the future will affect all nations. The identification in the Happy Planet Index of the importance of the “experience of well being” is very relevant to our profession as this one of our primary contributions to the health and quality of life of our patients.\nThe opportunity to participate in the definition of the Sustainable Development Goals (SDGs) must be grasped by our profession to ensure our perspective is heard. We can do this individually through the My World (a simple poll that takes 1-2 minutes) and the World We Want web sites and we can encourage our professional organisations to represent our perspectives. It will be another 15 years before this opportunity arises again!', '(June 2016) Not all meat and dairy products are the same; not in nutritional quality nor in how they affect our planet. Quality and environmental impact vary greatly from the variety of agricultural practices employed, and how food is processed. I have spoken often about the health and nutrition benefits of organic, grass-fed animal-based foods. This blog, however, is about sustainability and environmental stewardship.\nThe multitude of methods used for meat and dairy production include organic, free range, intensive livestock, and subsistence farming. Beyond organic are sustainable practices like no-till and biodynamic farming. Some organic operations are far superior to others, while some conventional farms are quite a bit cleaner than others, or on the path to becoming organic. So, some of these distinctions are not clear cut. Yet, it’s always good to know from where your food comes. That’s one of the clear advantages to buying local.\nWhen it comes to raising animals for food, there’s much to consider: Water consumption, methane gas emissions, the real cost of raising grain for feed, air and water quality, etc. Regardless, beef and milk production require tons of water, to the tune of 2000 gallons to produce just one pound of beef, or a gallon of milk. Much of that is used to grow grains for cow feed. That’s at least three times as much water as it takes to grow the same quantity of soy milk, and twice that needed for almond milk.\nIndeed, most of our water use goes to what we eat. Eighty percent of the water used in California is consumed by agriculture, according to the Pacific Institute. Irrigation projects in the former Soviet Union to grow cotton dramatically emptied the Aral Sea, destroyed fisheries, depopulated large areas and caused disease epidemics. Generally speaking, agriculture uses approximately 70% of the world’s freshwater supply. Looming climate change is likely to alter both water availability and agricultural water demands, so it behooves us to conserve as much as possible, and to improve on the goods and services it produces.\nThere is potentially an enormous difference between conventional and alternative farming practices in the amount of water and land consumption, fossil fuel use, and pollution. Organic farming uses 30% less energy and less water than conventional farming to produce the same corn and soybean yields, while yielding healthier, pesticide-free produce and far less groundwater pollution. Organic farming does not contaminate soil, rivers and drinking water like industrial farming does (Gonzalez, 2008). Conventional farming and livestock production are major stressors on the planet. They are major sources of greenhouse gases and loss of biodiversity, and major water polluters [Steinfeld et al., 2006].\nIndustrial agriculture relies on pesticides, herbicides, fertilizers and other toxic chemicals that increase the use of machinery, which contributes to a larger carbon footprint and global warming. Conventional systems emit nearly 40% more greenhouse gases per unit of crop than organic systems, while organic farming systems use 45% less energy (Middleton & Smith, 2011).\nOrganic farming may be especially competitive during droughts. Berkeley researchers concluded that organic farming creates biodiversity in the soil and environment not seen with synthetic chemicals. Organic plots can even outperform conventional ones, regarding yield, when the going gets tough. Organic plots are especially resilient to droughts, and extremes of frost and flooding, according to the Rodale Institute. Microbes and other organisms in organic soil create intricate relationships with plants and affect soil architecture. Organic soil absorbs and stores water better, and needs less water than conventional soil (Ferris, 2014). Organic farming is a more sustainable way of doing agriculture, hands down.\nThe Environmental Working Group has concluded that grass fed beef is better for the environment. Benefits from well managed grass fed operations are multifaceted, whereas large-scale confined feedlots have tremendous negative impacts on air and water quality. In “The Omnivore’s Dilemma” Michael Pollan points to the lighter carbon footprint in grass-fed operations, due to the fertilizer, pesticide and fossil fuel required for grain used in feed lots. More methane is produced by grass-fed cattle because it’s harder to digest grass than grain, but grasslands sequester enough carbon to more than make up the difference. A healthier soil and richer humus result from cows trampling their manure into the soil. The increased diversity maintains a healthy soil that keeps CO2 out of the atmosphere. Grazing cattle help stabilize soil, control weeds, and reduce the risk of wildfire (Profita, 2013).\nThe animals our ancestors ate roamed free and ate grass, whereas cattle and dairy cows today are force-fed grains. What we feed them has a major effect on the nutrient composition of the milk and beef. Numerous studies conclude that these nutrients vary significantly, depending on what the cows eat. For example, grass-fed, whole milk contains a third of the omega-3 fatty acids we need for brain, heart, skin and joint health, while conventional milk is virtually devoid of omega-3s. Grass-fed, full-fat dairy also contains 3-5 times as many antioxidants like beta-carotene, lutein and vitamin E (Chang, 2016). Grass-fed beef and milk also contains about twice as much conjugated linoleic acid (CLA) as grain-fed beef products. CLA is associated with reduced body fat and other benefits (Whigham et al., 2007). Plus, organic milk and meat do not contain the antibiotics, hormones and pesticides found in conventional animal-based foods. These drugs and chemicals contribute significantly to antibiotic resistance and the chronic diseases of modernity.\nSome folks opt to cut their environmental impact by not eating meat, or cutting back on their purchases. Yet meat and dairy provide nutrients that are hard to find in other foods. If done right, these foods are not only more nutritious, but also much less harmful to the earth. We should pay attention to how our ancestors thrived, and abide by nature’s wisdom.\nChang K. 2016. Organic Meat and Milk Higher in Healthful Fatty Acids http://well.blogs.nytimes.com/2016/02/15/more-omega-3-in-organic-meat-and-milk-review-of-studies-says/?_r=0\nDobson B. 2016. Ted Talk. https://www.youtube.com/watch?v=yp1i8_JFsao\nFerris R. 2014. Organic farming more drought resistant: Report. http://www.cnbc.com/2014/12/09/rought-resistant-report.html\nGeorge Mateljan Foundation. 2016. Cow’s milk, grass-fed. http://www.whfoods.com/genpage.php?tname=foodspice&dbid=130\nGonzalez N. 2008. Reports Show Less Water Used In Organic Farming. https://www.organicconsumers.org/news/reports-show-less-water-used-organic-farming\nGunners K. 2015. Grass-Fed vs Grain-Fed Beef – What’s The Difference? https://authoritynutrition.com/grass-fed-vs-grain-fed-beef/\nJerew B. 2014. Is Milk a Problem for the Environment? http://www.greenoptimistic.com/milk-problem-environment-20140908/#.V1mhZFfLnFI\nMiddleton S, Smith A. 2011. Organic farming blows conventional farming out of the water. http://anhinternational.org/2011/10/10/organic-farming-blows-conventional-farming-out-of-the-water/\nProfita C. 2013. Which Is Greener: Grass-Fed Or Grain-Fed Beef http://www.opb.org/news/blog/ecotrope/which-is-greener-grass-fed-or-grain-fed-beef/\nPacific Institute. 2016. Water, Food, and Agriculture. http://pacinst.org/issues/water-food-and-agriculture/\nScience Daily. 2016. Consumers have huge environmental impact. https://www.sciencedaily.com/releases/2016/02/160224132923.htm\nSteinfeld, Henning; Gerber, Pierre; Wassenaar, Tom; Castel, Vincent; Rosales, Mauricio; de Haan, Cees (2006), Livestock’s Long Shadow: Environmental Issues and Options (PDF), Rome: FAO\nWhigham LD, Watras AC, Schoeller DA. 2007. Efficacy of conjugated linoleic acid for reducing fat mass: a meta-analysis in humans. Am J Clin Nutr. 85:1203-11.\nWikipedia. Environmental impact of meat production. https://en.wikipedia.org/wiki/Environmental_impact_of_meat_production']	['<urn:uuid:a778721b-28bb-4b21-8141-a696a0895e60>', '<urn:uuid:34d0391d-d9b7-4702-baec-f50ddce4215d>']	factoid	direct	long-search-query	similar-to-document	three-doc	expert	2025-05-13T06:15:13.904987	10	82	2303
49	wood stove pipe maintenance tips ways prevent dangerous fires	Regular stovepipe maintenance is crucial for preventing fires. The pipe should be checked for creosote buildup every two weeks - more than 1/4 inch of creosote is hazardous and should be cleaned. To make cleaning easier and reduce buildup: perform morning 'burn outs' with dry 2-4 inch diameter wood and fully open vents, use chimney cleaning logs every couple months, and avoid burning trash or treated wood. Additionally, test pipe soundness by squeezing - walls should not be crushable.	"['Be sure to buy a wood stove that is the correct size for your heating needs.\nWood Stove Safety\nFrom the time you purchase your wood stove through the duration of its use, certain safety precautions should be taken. Here, you\'ll find a few general tips to carry you through the purchase, installation and maintenance of your wood stove as well as some guidelines to follow should your prevention measures fail and you\'re faced with a dangerous situation such as a chimney fire. Keep in mind these are just general guidelines. We recommend you have your wood stove installed by a certified installer, or, at the very least, contact your wood stove retailer, a certified installer, or your building code enforcement officer for more information. You can find certified installers through\nThe National Fireplace Institute.\nBecause safety is such a broad topic, only a few aspects of it are being covered here. For information on logging or wood sourcing safety, please see the links & resources to the left of this page.\nBurn safety & air quality concerns have been dealt with on separate pages within CCE\'s website.\nBuying Your Stove\n- Be sure to buy a wood stove that is the correct size for your heating needs. If the stove is too small, you will be tempted to run it hotter than it should be in order to attain the heat you need. This overfiring can overheat and damage the stove itself as well as stovepipe parts and adjacent structures in your house, which could cause a house fire. If the wood stove is too large and you reduce its output by restricting the air supply with the damper, then underfiring occurs. Underfiring reduces your stove\'s burn efficiency, creates a good deal more smoke and causes creosote to form more rapidly on the stovepipe. Creosote buildup is the main cause of chimney fires: the creosote is combustible, so once it has built up, one hot fire can cause all that creosote to ignite, dangerously raising the temperature in the stovepipe or chimney and potentially igniting adjacent structures as well (see below for more on chimney fires).\n- Check the wood stove carefully for deterioration or damage (cracks, warped components, etc.)! If you\'re buying a used stove, avoid buying anything made before 1990. Around 1990, EPA Phase II standards kicked in, requiring wood stoves to meet stricter emissions standards. Most older units are significantly more polluting than those made after 1990, affecting both indoor and outdoor air quality. They are also more inefficient, which will increase your fuel costs. So, make sure the wood stove bears the EPA Phase II-certified sticker. Newer stoves are a little easier to shop for. Underwriters Laboratories or another institution like UL should have given its approval to any new stove you\'re looking at. This certifies that the stove meets safety standards. Again, always look for the EPA Phase II-certified sticker. (The EPA is working on even stricter emissions standards, which may be in effect as early as 2012.)\n- Check your wood stove manual and always follow clearance guidelines specific for your wood stove. If your application requires tighter clearances, talk with your wood stove retailer about factory-made heat shields specific to your stove, or he or she may be able to manufacture an appropriate shield to attach to the adjacent wall(s).\n- Make sure the floor under where you intend to set the wood stove is strong enough to support its weight, and then build or buy a hearth to set it on. Non-combustible flooring must extend under the entire stove, continuing for 12"" past the sides and 18"" in front of any loading doors.\n- Technically speaking, stovepipe is single-walled, usually black metal pipe that can only be used to run from the wood stove to a couple of feet before a wall or ceiling. A Class ""A"" chimney--either double or triple-walled high-temperature stainless steel--must be run the rest of the way (through floors, walls, ceilings, and between floors). Run the stovepipe and chimney in the shortest route possible--this will improve your draft and reduce turbulence in the exhaust gases, making your installation safer. Also, as the length of the pipe increases, it becomes more likely that its joints will deteriorate over time. Limit the number of bends you use and make them curve as gradually as possible. If you\'re doing the installation yourself, check with your local building inspector and your local wood stove retailer to make sure you are complying with established safety standards.\n- If you have an existing chimney that was built for a gas, oil or coal furnace, chances are it will not be suitable for a wood stove, at least not without some modifications. First, most chimneys are over-sized for wood stoves, which will give you a poor draft and increase creosote buildup. Second, proper clearances to combustibles (wooden framing) are often not maintained. Third, if it\'s a masonry chimney, most are not lined with a proper flue liner. And fourth, many masonry chimneys develop cracks in the mortar over time, which will allow air leaks, reducing draft, allowing smoke to escape, and creating the potential for hot flue gases to contact combustibles. If you\'d like to use your existing chimney, have a certified installer take a look to see what you have. If it\'s metal, your options are somewhat limited. If it\'s masonry, you will probably have to reline the chimney with stainless steel pipe. Place an appropriately sized cap on top of the chimney. Poisonous gases or sparks may pass from one heating device to another, so don\'t connect more than one appliance to the same chimney flue.\n- When putting the stovepipe and chimney together, mount the female ends of the pipe sections up to prevent creosote from leaking out around the seams and then seal each junction to keep the smoke from escaping. Use black surface cement, which can be purchased from your wood stove retailer, for this purpose.\n- Building a ""crud trap"" into your system allows you easy access for cleaning out your stovepipe. Usually, this is built into the longest run of stovepipe. Also, if your stovepipe exits your stove out the back (rather than the top), instead of placing an elbow where the pipe leaves the stove, put a tee fitting there. If you put a removable cap on the lower end of the tee (be sure to fasten it with sheet metal screws) then cleaning the stovepipe is much simpler.\n- Keeping your stovepipe running through your house and out the roof is better than running it outside through a wall. Cool outdoor air causes outside chimneys to experience a cooling effect which makes the chimney about 20% less efficient & encourages rapid creosote formation.\n- Stovepipe thermometers are a cheap and easy way to monitor flue temperatures. They give you an easy way to make sure you are not firing your wood stove too hot (which may cause stove or stovepipe damage and could ignite adjacent structures) or too cold (increasing smoke levels and allowing creosote to build up). Check with the manufacturer or your wood stove dealer for best placement (most are screwed into the stovepipe or attach magnetically about two feet above the stove\'s top).\n- It\'s always a good idea to install a gate or small fence around your wood stove, especially if you have small children or you are going to have small children in your house.\nHave your chimney and wood stove inspected by a certified chimney specialist once a year, well before the heating season. If you would like, there are a few simple checks of your chimney and stovepipe you can do yourself (see our\nProper Maintenance pages for more on maintenance of your wood stove):\n- Stovepipe can deteriorate over time, so you should test the soundness of your pipe on a regular basis after the first year or two of use. You can do this by simply squeezing the pipe: the walls should not be crushable; if you can crush the walls then the creosote has eaten away too much metal for the installation to be safe.\n- To test the soundness of a chimney, plug the top of the flue then start a small smoky fire below. The smoke will puff out of any leaks that the chimney liner contains. This technique is known as puffing.\n- Before firing up the stove, check the amount of creosote accumulation. More than 1/4 inch of creosote is considered hazardous and should be cleaned out before your next burn. Perform the check for creosote every two weeks until you have become accustomed to the stove\'s behavior. You can do this by peering into the system or by tapping on pipe sections with a metal object. Once you\'re used to the ringing sound that a clean pipe makes, you should be able to distinguish the dull thud of a dirty one.\nChimney fires are caused by an accumulation of creosote on the stovepipe or chimney. Creosote is made up of condensed volatile gases created by incomplete combustion of the wood. As these gases rise in the chimney, they cool, mix with water vapor, and form a tar-like substance that clings to the chimney walls. When sufficiently heated, through normal wood stove operation, the creosote can ignite, causing a chimney fire.\nChimney fires can start quickly and be very powerful, shooting flames many feet above the chimney cap and producing a loud rumble like a freight train going by. They can also be less dramatic: slow-burning chimney fires don\'t get enough air to become ""ragers"". But they can still reach excessively high temperatures and can cause as much damage to the chimney structure and nearby combustible parts of the house as their more spectacular cousins. With proper installation and chimney care, chimney fires are entirely preventable, but if you are faced with one, here is some basic information about them.\n- Chimney fire signs\n- The chimney is emitting sparks\n- The stovepipe is exceedingly hot, maybe even glowing\n- Air is rushing through the draft in the stove\n- Loud cracking or popping noises can be heard or you hear a rumbling like that caused by a low-flying plane or a freight train\n- If you have a chimney fire\n- Call 911 and get yourself and your family out of the house.\n- If the fire has just kicked in and only if it is safe to do so, before leaving the house you should close up the stove to starve the fire of oxygen.\n- Never pour water into the stove! The rapid cooling of the stove\'s body can cause it to crack, which will only aid oxygen flow to the blaze.\n- If oxygen is leaking up to the fire through a crack or other imperfection in the stove then flashing can occur (brief, very hot ignitions caused by the influx of oxygen). Repeated flashing can tear apart masonry chimneys and stovepipes. If you suspect that flashing is occurring, the best line of defense is to leave the dampers open.\n- Retreat a safe distance from your home and wait for the fire department to arrive. Even if it looks like the fire has burned itself out, don\'t return until they have done a thorough check of your house. Spray your roof--not directly on the chimney--with a garden hose to help limit the possible spread of fire. Even if flames have stopped shooting out of the top of the chimney and the rumbling noise has stopped, there may still be enough heat on or near combustible parts of your house to ignite them.\n- After a chimney fire, have a certified installer or chimney sweep do an extensive cleaning and check of your system for soundness. The blaze likely transformed any creosote within your chimney into a thicker, crustier layer which will cause extremely rapid creosote accumulation during your next burn if left in the chimney. The stovepipe and chimney may also have become damaged by the fire.\nHeating with Wood resources developed by Guillermo Metz, Energy Team Leader at CCE-Tompkins\nLast updated June 14, 2018', 'Cleaning creosote from the pipes of your wood burning stove or your fireplace chimney could mean the difference between life and death. At the least, it could save you time and energy on cutting and chopping firewood.\nAt our house, this is a job for my own personal chimney sweep, my husband. He likes me to know what he’s doing and why, but I usually wind up in the way so I just pass him tools and empty buckets!\nWhen you maintain clean pipes on your wood stove, it will burn more efficiently and be safer in general. Did you know, according to the EPA, nearly 7% of all home fires in the U.S. can be contributed to the build up of creosote in stove pipes or chimneys?\nIf you live in a cold climate like we do, then you’re running your wood stove every day, probably all day. This can make us complacent to the reality of needing to maintain a cleaning schedule. We put a mark on the calendar and use our phones to set reminders. Even though you’re using your stove every day, creosote is building up a little at a time in there.\nUnderstanding Creosote Build Up\nJust how quickly creosote can build up in your pipes or chimney depends on a few factors.\n- The temperature outside\n- The quality and efficiency of the stove and pipes\n- The kind of wood you’re burning\nOf these factors, the type of wood you’re burning plays the major role in creosote build up. Choosing wood for clean burning is just as important as cleaning creosote from the pipes because one directly affects the difficulty of the other.\nChoosing The Best Wood To Burn\nObviously, some types of trees burn cleaner than others. They create less build up and make cleaning creosote out of the pipes easier. The area you live in determines your options for the kind of wood you use. It’s important to safety and efficiency to only use seasoned wood in your wood stove or fireplace.\nBesides the fact that green wood doesn’t put off as much heat, it also causes more creosote buildup quickly. In our area, Tamarack is considered top choice of wood because is has a long burn time and leaves little creosote in pipes. Because it burns so efficiently, it doesn’t leave much ash and the ash it does leave is fine and easy to clean out.\nDense hardwoods like maple and oak are higher in energy content so they provide more heat. They also burn longer than softer woods such as birch, pine, and spruce. These softer woods burn faster but produce less energy because they’re less dense.\nIn the deep south where I originate from, hardwoods such as oak are preferred. We would use softer woods in the spring and fall because it doesn’t get “cold” down there in those seasons which means we only need a little heat. Some wood stove installers have told me the combustion process in the newer stoves works so well you can use a wider variety of woods and receive good energy output.\nTips to Make Cleaning Creosote Easier\n- Dont’ burn a lot of “trash” in your wood stove. This is especially true of all that slick paper you get as junk mail and plastics. Not only do they release dangerous toxins, they can coat the stove pipe with chemicals.\n- Never burn wood that is painted or chemically treated. I’m sure this is obvious to you, but the fumes these release are deadly. At the least they can cause serious sinus problems.\n- Do a morning “burn out”. First thing in the morning, burn pieces of the driest wood you have that are 2″-4″ in diameter. Open the stove vents all the way. Doing this will not only warm the area quickly, but it warms the pipes loosening the creosote build up from the past 24 hours. We found doing this every morning significantly cut down creosote build up and helped the stove run more efficiently.\n- Use a chimney cleaning log. We’ve found burning a chimney cleaning log once every couple of months and especially a few days before cleaning creosote from the pipes, makes the job easier.\n- During the morning burn out or when you use a chimney cleaning log, use the opportunity to check the stove and pipes for any areas where smoke may be leaking inside the house and to see if creosote drips down the pipe inside the house. Identifying these trouble spots will help you know where to focus when you are cleaning creosote from the pipes.\nTools You’ll Need When Cleaning Creosote\nYou may not need all of these but I wanted to share a good list with you of the tools you could expect to need in order to do the job safely and right. Your setup and type of stove will determine which you’ll need.\n• A drop cloth or newspapers to protect your floor\n• A chimney sweep brush\n• Small hand brush\n• Ash shovel\n• Ash container to collect the ashes in for disposal later. We use a metal one.\n• Your choice of glass cleaner and old newspaper to clean the glass door if you have one\n• Screwdriver to disconnect the stove pipe where needed\n• Ladder to reach top of stove pipe\n*Remember: The stove and pipe should be cool before you start cleaning creosote from them.\nThe first step is to climb onto the roof and inspect the pipe, area around the pipe, and pipe cap for any signs of damage and make any repairs needed.\nNext, clean the pipe with your chimney sweep brush. Be sure to check the instructions which came with your brush and your stove to see if there’s anything special you need to do before, during or after you sweep the pipe.\nOf course, you clean the pipe from the top so gravity will carry the creosote down into the stove. When you’re done sweeping, remove the ash and creosote from the stove using your ash bucket.\nIf you have any trouble spots along the pipe, disassemble that section and check for buildup. It’s usually in elbows where any problems occur.\nBe sure to sweep out the ash pan compartment and add it to the ash bucket. It’s important to empty or set the ash bucket in a safe area outside in case of cinders. We use our wood ashes in the garden and compost. Your poultry will appreciate it if you add them to DE for their dust baths.\nI like a glass door on a wood stove. There’s just something entrancing about watching a fire burn. Being able to see the fire means keeping the glass door clean. Since the stove is cool and clean, now’s a good time to clean the glass.\nNow, you’re finished cleaning creosote from your stove pipes or chimney. Do you feel like a pro?\nShare your own special tips and experience on cleaning creosote with us in the comments below.\nSafe and Happy Journey,\nRhonda and The Pack\nA special thank you to my own personal chimney sweep for all his input!']"	['<urn:uuid:400d264b-3c64-4a78-b199-8bc9fd94e068>', '<urn:uuid:ac3bf27e-21b4-46f6-8679-cb7e3572b0ba>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	9	79	3230
50	What's worse between getting sick from milk allergy or milk intolerance?	A milk allergy is more severe as it is a life-threatening immune system response that causes immediate symptoms like breathing difficulty, throat swelling, skin rash, nausea, and chest pain. In contrast, milk intolerance is not life-threatening and causes milder, gradual symptoms like stomach cramps and irritability, typically only occurring when consuming a certain amount of dairy products.	"['Could Food Intolerance be the Cause of your Health Issues?\nFood intolerance is a medical condition that has come to the forefront in the last few years. Originally thought to be a ""phantom"" disorder, it is beginning to garner serious attention from health professionals and the food industry alike.\nDifferences between food allergy and food intolerance\nA food allergy is a systemic response to certain foods generated by the immune system that can be life threatening. Up to 4% of American adults suffer from food allergies with the most common allergens being peanuts, shellfish (like shrimp, lobster and crab), milk and eggs.\nSome symptoms of food allergy:\n- Itching or swelling of the mouth or throat\n- Skin rash\n- Breathing difficulty\n- Nausea and/or vomiting\n- Chest pain\nMillions more Americans are afflicted with food intolerance, which is a response to some foods and beverages NOT generated by the immune system and NOT life threatening. Symptoms can be triggered by a chemical reaction to preservatives or other compounds in foods, or they can be a result of low enzyme levels which inhibit digestion of certain foods most commonly wheat and dairy.\nSome symptoms of food intolerance:\n- Stomach cramps\n- Anxiety or irritability\nTypically, symptoms of food allergy are immediate, and they occur every time the food is eaten. Effects of food intolerance are more gradual and may only occur when a certain amount of the food is eaten. This amount (or threshold) varies from person to person.\nCauses for growing food intolerance\nGMO Foods: Experts believe that part of growing food intolerance in the United States can be attributed to food engineering or modification which has raised the amount of gluten in wheat from roughly 4% to about 14%.\nProcessed Foods: The typical American diet also contains a whopping amount of processed foods - packed with artificial preservatives and chemicals that can trigger reactions in certain people with repeated exposure.\nHere are some suggestions if food intolerance is a concern:\n- Keep a detailed food diary for two weeks that includes foods, portion sizes and any symptoms you may develop for evaluation by a registered dietician.\n- Make substitutions.\n- Gluten-free substitutes: Choose gluten-free breads, rolls and bagels there are a number on the market. Purchase quinoa in place of wheat-based pasta and corn tortillas instead of the flour variety. Gluten-free cereals can be harder to find, but Rice and Corn Chex are a good option. For snacking, try popcorn, rice cakes or rice crackers. Make sure products read ""gluten-free"" or check labels (some products contain modified food starch which contains gluten).\n- Non-dairy substitutes: Replace dairy milk with soy, almond, rice or coconut milk. Also there are a number of dairy-free cheeses and coffee creamers on the market and soy-based margarines.\n- Remove processed foods from your diet, and evaluate how you feel.\n- Try an elimination diet. Remove suspect foods from your diet for several weeks. If symptoms go away, then one of these foods is most likely the problem. One at a time, reintroduce each food back into the diet to determine if symptoms reoccur.\n- Give supplements a try. Probiotics can help restore balance of healthy bacteria in the gut, fish oil can lower inflammation associated with food intolerance, and enzymes can be helpful in the breakdown of foods for better digestion.\nWhile food intolerance is a growing problem, symptoms can also be attributed to other conditions. It is important to consult with a health care professional if there is a concern.', 'What’s an allergic reaction?\nAllergic reactions happen when your immune system overreacts to something that comes into contact with your body. The immune system, your body’s defense against infection, mistakenly treats substances called allergens as a threat and tries to get rid of them by producing antibodies to fight the allergen. Allergens can be breathed into the lungs, swallowed or even injected.\nIs there a cure for allergies?\nWhile there is no specific cure to allergies, you can prevent exposure to your allergens and promptly take allergy medication when needed. You can also talk to your doctor about immunotherapy such as allergy shots, tablets or drops. They might not cure your allergies completely, but they can significantly reduce your allergic reactions.\nWhat happens during an allergic reaction?\nYour body’s reaction to an allergen causes the release of histamines and other body chemicals, causing the symptoms of an allergic reaction. Then, your system raises its defenses every time it detects the allergen, resulting in an allergic reaction, such as runny nose, itchy and watery eyes, sneezing, and swelling. Once a person has had an exposure or an allergic reaction, even a limited exposure to a very small amount of allergen can trigger a reaction.\nWho can get allergies?\nReally anyone can get allergies. However, we have learned that genetics and the environment affect whether or not someone develops allergies. You can inherit the likelihood that you will develop allergies.\nFor example, people who are likely to develop allergies have an inherited condition known as atopy, the genetic tendency to develop allergic diseases. But it’s still unknown why people develop allergies to food, although they often have other allergic conditions, such as asthma, hay fever and eczema. Environmental factors can include things such as pollution, epidemic diseases and diet. Millions of Americans suffer from allergies. Allergies often begin in childhood, but can develop or re-emerge later in life.\nWhat are common allergens?\nMany things can trigger allergies. The most common are pollen, dust mites, mold, animal dander, insect stings, latex, and certain food and medications.\nAccording to WebMD, the following eight things cause about 90% of food allergy reactions:\n- Milk (mostly in children)\n- Tree nuts, like walnuts, almonds, pine nuts, Brazil nuts and pecans.\n- Wheat and other grains with gluten, including barley, rye and oats.\n- Fish (mostly in adults)\n- Shellfish (mostly in adults)\nWhat are symptoms of an allergic reaction?\nReactions to allergens range from runny nose, itchy eyes, allergic asthma and sneezing to hives and life-threatening anaphylaxis. If you think someone has the symptoms of anaphylaxis – such as sudden onset of breathing difficulties, lightheadedness, and feeling like they’re going to faint or lose consciousness – get emergency help right away. If the person has emergency allergy medicine on hand, help the person inject the medicine.\nWhat’s the difference between a food intolerance and a food allergy?\nPeople with food intolerance may have symptoms such as diarrhea, bloating and stomach cramps. This may be caused by difficulties digesting certain substances, such as lactose. However, this is not an allergic reaction.\nHere’s how to prevent allergic reactions:\n- Avoid triggers such as foods and medicines that have caused an allergic reaction in the past. Ask detailed questions about ingredients when you are eating away from home. Carefully check ingredient labels.\n- If you have a child who is allergic to certain foods, introduce 1 new food at a time in small amounts so you can recognize an allergic reaction.\n- People who have had serious allergic reactions should wear a medical ID tag and carry emergency medicines according to their health care provider’s instructions.\nBe well, stay well~\nAndy Stergachis, Ph.D., R.Ph. is the Director of the Global Medicines Program at the University of Washington in Seattle and a subject expert on public health and pharmacy-related topics.']"	['<urn:uuid:168ed136-7228-482b-8862-1a96acb34cf6>', '<urn:uuid:41ce349f-60a1-4629-aa3d-a241bdc73726>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	11	57	1228
51	compare epidermis dermis receptor types sensory function location melanocytes sweat glands	The epidermis and dermis contain different types of receptors and structures with distinct sensory functions. The epidermis contains melanocytes that produce melanin for UV protection and are found deep in this layer. The dermis houses several types of receptors including Meissner's corpuscles for light touch, Pacinian corpuscles for pressure, and also contains sweat glands that open to the surface through skin pores. The dermis has two distinct layers: the papillary layer with touch receptors that communicate with the central nervous system, and the reticular layer containing hair follicles, nerves, and glands.	"['BC: The End\nFC: Final Assessment Project\n1: The Integumentary System\n2: Epidermis -made up of epithelial tissue | JOB: The Epidermis is responsible for the protection of the inner layers.\n3: The Epidermis absorbs nutrients, and homeostasis takes place. There is also keratinocytes, which produce keratin. Keratin is a fibrous protein that aids in protection. The majority of the skin is kertinized, or waterproof.\n4: Most cells in the Epidermis are called squamous cells. The Epidermis also has melanocytes which give your skin color and produce melanin; they are found deep in the Epidermis. Melanin package melanosomes which form a pigment shield against UV radiation\n6: The Dermis\n7: The dermis has two layers: Papillary: make depressions and impressions of fingerprints. In this layer there are receptors which communicate with the central nervous system and respond to touch. Recticular: made of dense elastic fibers. It houses hair follicles, nerves, and glands.\n8: Hypodermis | Has addipose tissue and is used for fat storage. The Hypodermis\'s function is to insulate, store energy and aids in the anchoring of the skin.\n9: Receptors: -Meissner\'s corpuscles are common in the tips of the fingers and lips. They are sensitive to touch. -Pacinian corpuscles respond to pressure. Glands: -Sebaceous glands secrete an oily coating in the hair shaft. If plugged or infected, it becomes a skin blemish or pimple. -Sweat glands open up to the surface through skin pores. -Exocrine glands are a type of sweat gland and they occur all over the body. -Apocrine glands are also a type of sweat gland. They are larger than exocrine glands and occur in the armpits and groin areas. They produce a solution that bacteria act upon to produce ""body odor"". | Glands and Receptors\n10: Nervous System\n11: Neurons have three parts: 1. The cell body; contains nucleus. 2. Dendrites; hair like structures surrounding the cell body, which conduct incoming signals. 3. The Axon(nerve fiber) conduct outgoing signals emitted by the neuron. They are enclosed in a fat-like sheath, called myelin, which acts like an insulator and speeds impulse transmission.\n12: Typically a neuron is connected to many thousands of other neurons. The synapse is the point of contact between the two neurons. Messages are passed to and from the brain by electrical impulses called action potentials.\n13: At the synapse, the impulse causes a release of neurotransmitters and this drives the impulse to the next neuron. These impulses travel about 250mph.\n14: Skeletal System\n15: Long Bone | Short Bone | Flat Bone | Sesamoid Bone | Irregular Bone | The different bone types:\n16: Purposes: 1. Long bones- provides support and allow us to create movement. 2. Short bones- allow movement, provide elasticity and flexibility, and shock absorption. 3. Flat bones- protects and provides attachment sites for muscles. 4. Irregular bones- supports weight, protects spinal cord, contributes to movement and provides sites for muscle attachment. 5. Sesamoid bones- alters the angle of insertion of the muscle\n17: The bone consists of three layers: 1) The Periosteum, or the outside skin of bone. 2) The hard compact bone, supports weight of the body 3)Spongy bone, or bone marrow; occurs at ends of long bones.\n18: The spongy bone of the femur, humerus and sternum contains red blood cells, white blood cells, or platelets. Yellow marrow is used to store fat.\n19: The skin is smooth and flat allowing the under layers to take up less space. | The nerves are thin and slick allowing them to fit into small spaces in the body. | The structure of the skeletal system is straight and compact allowing it to hold up the weight of the body. | How the structure relates to its function', 'A major role of sensory receptors is to help us learn about the environment around us, or about the state of our internal environment. Different types of stimuli from varying sources are received and changed into the electrochemical signals of the nervous system. This process is called sensory transduction. This occurs when a stimulus is detected by a receptor which generates a graded potential in a sensory neuron. If strong enough, the graded potential causes the sensory neuron to produce an action potential that is relayed into the central nervous system (CNS), where it is integrated with other sensory information—and sometimes higher cognitive functions—to become a conscious perception of that stimulus. The central integration may then lead to a motor response.\nDescribing sensory function with the term sensation or perception is a deliberate distinction. Sensation is the activation of sensory receptors at the level of the stimulus. Perception is the central processing of sensory stimuli into a meaningful pattern involving awareness. Perception is dependent on sensation, but not all sensations are perceived. Receptors are the structures (and sometimes whole cells) that detect sensations. A receptor or receptor cell is changed directly by a stimulus. A transmembrane protein receptor is a protein in the cell membrane that mediates a physiological change in a neuron, most often through the opening of ion channels or changes in the cell signaling processes. Some transmembrane receptors are activated by chemicals called ligands. For example, a molecule in food can serve as a ligand for taste receptors. Other transmembrane proteins, which are not accurately called receptors, are sensitive to mechanical or thermal changes. Physical changes in these proteins increase ion flow across the membrane, and can generate a graded potential in the sensory neurons.\nStimuli in the environment activate specialized receptors or receptor cells in the peripheral nervous system. Different types of stimuli are sensed by different types of receptors. Receptor cells can be classified into types on the basis of three different criteria: cell type, position, and function. Receptors can be classified structurally on the basis of cell type and their position in relation to stimuli they sense. They can also be classified functionally on the basis of the transduction of stimuli, or how the mechanical stimulus, light, or chemical changed the cell membrane potential.\nStructural Receptor Types\nThe cells that interpret information about the environment can be either (1) a neuron that has a free nerve ending (dendrites) embedded in tissue that would receive a sensation; (2) a neuron that has an encapsulated ending in which the dendrites are encapsulated in connective tissue that enhances their sensitivity; or (3) a specialized receptor cell, which has distinct structural components that interpret a specific type of stimulus (Figure 13.1.1). The pain and temperature receptors in the dermis of the skin are examples of neurons that have free nerve endings. Also located in the dermis of the skin are lamellated and tactile corpuscles, neurons with encapsulated nerve endings that respond to pressure and touch. The cells in the retina that respond to light stimuli are an example of a specialized receptor cell, a photoreceptor.\nGraded potentials in free and encapsulated nerve endings are called generator potentials. When strong enough to reach threshold they can directly trigger an action potential along the axon of the sensory neuron. Action potentials triggered by receptor cells, however, are indirect. Graded potentials in receptor cells are called receptor potentials. These graded potentials cause neurotransmitter to be released onto a sensory neuron causing a graded post-synaptic potential. If this graded post-synaptic potential is strong enough to reach threshold it will trigger an action potential along the axon of the sensory neuron.\nAnother way that receptors can be classified is based on their location relative to the stimuli. An exteroceptor is a receptor that is located near a stimulus in the external environment, such as the somatosensory receptors that are located in the skin. An interoceptor is one that interprets stimuli from internal organs and tissues, such as the receptors that sense the increase in blood pressure in the aorta or carotid sinus. Finally, a proprioceptor is a receptor located near a moving part of the body, such as a muscle or joint capsule, that interprets the positions of the tissues as they move.\nFunctional Receptor Types\nA third classification of receptors is by how the receptor transduces stimuli into membrane potential changes. Stimuli are of three general types. Some stimuli are ions and macromolecules that affect transmembrane receptor proteins by binding or by directly diffusing across the cell membrane. Some stimuli are physical variations in the environment that affect receptor cell membrane potentials. Other stimuli include the electromagnetic radiation from visible light. For humans, the only electromagnetic energy that is perceived by our eyes is visible light. Some other organisms have receptors that humans lack, such as the heat sensors of snakes, the ultraviolet light sensors of bees, or magnetic receptors in migratory birds.\nReceptor cells can be further categorized on the basis of the type of stimuli they transduce. Chemical stimuli can be detected by a chemoreceptors that detect chemical stimuli, such as a chemicals that lead to the sense of smell. Osmoreceptors respond to solute concentrations of body fluids. Pain is primarily a chemical and sometimes mechanical sense that interprets the presence of chemicals from tissue damage, or intense mechanical stimuli, through a nociceptor. Physical stimuli, such as pressure and vibration, as well as the sensation of sound and body position (balance), are interpreted through a mechanoreceptor. Another physical stimulus that has its own type of receptor is temperature, which is sensed through a thermoreceptor that is either sensitive to temperatures above (heat) or below (cold) normal body temperature.\nAsk anyone what the senses are, and they are likely to list the five major senses—taste, smell, touch, hearing, and sight. However, these are not all of the senses. The most obvious omission from this list is balance. Also, what is referred to simply as touch can be further subdivided into pressure, vibration, stretch, and hair-follicle position, on the basis of the type of mechanoreceptors that perceive these touch sensations. Other overlooked senses include temperature perception by thermoreceptors and pain perception by nociceptors.\nWithin the realm of physiology, senses can be classified as either general or special. A general sense is one that is distributed throughout the body and has receptor cells within the structures of other organs. Mechanoreceptors in the skin, muscles, or the walls of blood vessels are examples of this type. General senses often contribute to the sense of touch, as described above, or to proprioception (body position) and kinesthesia (body movement), or to a visceral sense, which is most important to autonomic functions. A special sense (discussed in Chapter 15) is one that has a specific organ devoted to it, namely the eye, inner ear, tongue, or nose.\nEach of the senses is referred to as a sensory modality. Modality refers to the way that information is encoded into a perception. The main sensory modalities can be described on the basis of how each stimulus is transduced and perceived. The chemical senses include taste and smell. The general sense that is usually referred to as touch includes chemical sensation in the form of nociception, or pain. Pressure, vibration, muscle stretch, and the movement of hair by an external stimulus, are all sensed by mechanoreceptors and perceived as touch or proprioception. Hearing and balance are also sensed by mechanoreceptors. Finally, vision involves the activation of photoreceptors.\nListing all the different sensory modalities, which can number as many as 17, involves separating the five major senses into more specific categories, or submodalities, of the larger sense. An individual sensory modality represents the sensation of a specific type of stimulus. For example, the general sense of touch, which is known as somatosensation, can be separated into light pressure, deep pressure, vibration, itch, pain, temperature, or hair movement.\nIn this chapter we will discuss the general senses which include pain, temperature, touch, pressure, vibration and proprioception. We will discuss the special senses, which include smell, taste, vision, hearing and the vestibular system, in chapter 15.\nSomatosensation is considered a general sense, as opposed to the submodalities discussed in this section. Somatosensation is the group of sensory modalities that are associated with touch and limb position. These modalities include pressure, vibration, light touch, tickle, itch, temperature, pain, proprioception, and kinesthesia. This means that its receptors are not associated with a specialized organ, but are instead spread throughout the body in a variety of organs. Many of the somatosensory receptors are located in the skin, but receptors are also found in muscles, tendons, joint capsules and ligaments.\nTwo types of somatosensory signals that are transduced by free nerve endings are pain and temperature. These two modalities use thermoreceptors and nociceptors to transduce temperature and pain stimuli, respectively. Temperature receptors are stimulated when local temperatures differ from body temperature. Some thermoreceptors are sensitive to just cold and others to just heat. Nociception is the sensation of potentially damaging stimuli. Mechanical, chemical, or thermal stimuli beyond a set threshold will elicit painful sensations. Stressed or damaged tissues release chemicals that activate receptor proteins in the nociceptors. For example, the sensation of pain or heat associated with spicy foods involves capsaicin, the active molecule in hot peppers. Capsaicin molecules bind to a transmembrane ion channel in nociceptors that is sensitive to temperatures above 37°C. The dynamics of capsaicin binding with this transmembrane ion channel is unusual in that the molecule remains bound for a long time. Because of this, it will decrease the ability of other stimuli to elicit pain sensations through the activated nociceptor. For this reason, capsaicin can be used as a topical analgesic, such as in products like Icy Hot™.\nIf you drag your finger across a textured surface, the skin of your finger will vibrate. Such low frequency vibrations are sensed by mechanoreceptors called Merkel cells, also known as type I cutaneous mechanoreceptors. Merkel cells are located in the stratum basale of the epidermis. Deep pressure and vibration is transduced by lamellated (Pacinian) corpuscles, which are receptors with encapsulated endings found deep in the dermis, or subcutaneous tissue. Light touch is transduced by the encapsulated endings known as tactile (Meissner’s) corpuscles. Follicles are also wrapped in a plexus of nerve endings known as the hair follicle plexus. These nerve endings detect the movement of hair at the surface of the skin, such as when an insect may be walking along the skin. Stretching of the skin is transduced by stretch receptors known as bulbous corpuscles. Bulbous corpuscles are also known as Ruffini corpuscles, or type II cutaneous mechanoreceptors.\nOther somatosensory receptors are found in the joints and muscles. Stretch receptors monitor the stretching of tendons, muscles, and the components of joints. For example, have you ever stretched your muscles before or after exercise and noticed that you can only stretch so far before your muscles spasm back to a less stretched state? This spasm is a reflex that is initiated by stretch receptors to avoid muscle tearing. Such stretch receptors can also prevent over-contraction of a muscle. In skeletal muscle tissue, these stretch receptors are called muscle spindles. Golgi tendon organs similarly transduce the stretch levels of tendons. Bulbous corpuscles are also present in joint capsules, where they measure stretch in the components of the skeletal system within the joint. Additionally, lamellated corpuscles are found adjacent to joint capsules and detect vibrations associated with movement around joints. The types of nerve endings, their locations, and the stimuli they transduce are presented in the table below.\n|Mechanoreceptors of Somatosensation (Table 13.1)|\n|Name||Historical (eponymous) name||Location(s)||Stimuli|\n|Free nerve endings||*||Dermis, cornea, tongue, joint capsules||Pain, temperature, mechanical deformation|\n|Mechanoreceptors||Merkel’s discs||Epidermal–dermal junction, mucosal membranes||Low frequency vibration (5–15 Hz)|\n|Bulbous corpuscle||Ruffini’s corpuscle||Dermis, joint capsules||Stretch|\n|Tactile corpuscle||Meissner’s corpuscle||Papillary dermis, especially in the fingertips and lips||Light touch, vibrations below 50 Hz|\n|Lamellated corpuscle||Pacinian corpuscle||Deep dermis, subcutaneous tissue, joint capsules||Deep pressure, high-frequency vibration (around 250 Hz)|\n|Hair follicle plexus||*||Wrapped around hair follicles in the dermis||Movement of hair|\n|Muscle spindle||*||In line with skeletal muscle fibers||Muscle contraction and stretch|\n|Tendon stretch organ||Golgi tendon organ||In line with tendons||Stretch of tendons|']"	['<urn:uuid:49a9980e-4cfb-48b6-b067-794fa58685b2>', '<urn:uuid:1c3c98d2-cb37-4da4-9096-b8f6c54da74c>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T06:15:13.904987	11	91	2643
52	Do carrots or clover need to be planted deeper in the ground?	Carrots should be sown 1/4 inch deep in the soil, while clover needs to be no-tilled into the ground. No-tilling grass seeds like clover into existing sod is the best option as it maintains the original sod, conserves moisture, provides competition to weeds and protects against erosion better than conventional tillage.	"['Rebuilding a forage base\nLandowners experienced a terrible drought this past summer and many are wondering what to do now with the poor pasture that is left.\n""There are two responses to have toward a drought: short term and long term,"" said Jill Scheidt, agronomy specialist with the University of Missouri Extension in Barton County.\nA short-term response refers emergency crops to plant to address a drought now. This spring, short-term planting options as an emergency crop include spring oats, cereal rye or turnips.\nLong-term response options for drought are available in plantings like: over-seeding clover or lespedeza; thickening a forage stand in the spring; planting annual Sudan or millet in May; partially converting to a warm season grass pasture; controlled grazing; and meeting fertility needs.\n""To get back on track with a forage program a forage base needs to be established for the next season. If starting from a strong fescue base, 40 to 60 pounds per acre of nitrogen needs to be applied in the fall and clover no-tilled into the ground no later than September 1,"" said Scheidt.\nAnother option is frost-seeding clover or lespedeza from December to February.\n""If starting from a weak fescue base, frost seeding clover or lespedeza is still a good option,"" said Scheidt. ""Clover and legume stands suffer in a drought, so it is imperative to rebuild a lost stand.\nIf the drought severely injured a fescue field infested with toxic endophyte, then the drought gives producers a chance convert fields over to Novel endophyte fescues.\n""Converting fields to Novel endophyte fescue is the best option if a pasture is injured beyond repair,"" said Scheidt.\nLegumes are an excellent addition to a pasture. Not only do legumes fix nitrogen, but they also provide additional benefits like: increase animal gain by 100 pounds per acre; increase daily gains 0.1-1.0 pounds; increase cow conception rates by 15 to 25 percent; increase forage quality; lower endophyte toxicity; and even out feed supply.\nAnother way to get better usage of a pasture is adding warm season grasses to the mix. Converting 10 to 30 percent of the pasture to warm season grasses such as Bermuda, Caucasian bluestem, or native warm-season grasses provides pastures with lush, growing grass during months when fescue growth begins to decline.\nBest to no-till\nAccording to Scheidt, it is best to no-till grass seeds into an existing sod.\n""No-tilling is the best option because it maintains the original sod, conserves moisture, provides competition to weeds and protects against erosion better than conventional tillage. One downside to no-till is the risk of not controlling planting depth,"" said Scheidt.\nGrass seedlings need to be planted no more than 1/8 to 1/4 inch deep. If the drill does not provide good depth control, broadcasting is a more successful option.\nRemoval of competitive weeds is important when re-establishing a pasture. Just be sure to always read the herbicide label to determine when seedlings can safely be planted after an application.\nA bio essay is a great way to test whether herbicide residue is still in the soil. A bio essay is performed by taking five to eight samples of soil and planting seeds into those samples. If grasses emerge in one to two weeks, it is safe to plant; if not, wait a while then do another bio essay trial.\nPasture rationing is also essential in a drought. The more often livestock are rotated, the more recovery time grass has in between grazing periods.\n""Different types of rotational grazing are available. No one pasture is the same; trial and error is the best way to figure how often to move fences or switch paddocks for optimum grass usage in your pasture,"" said Scheidt.\nFor more information, contact any of these MU Extension agronomy specialists in southwest Missouri: Tim Schnakenberg in Stone County, 417-357-6812; Jill Scheidt in Barton County, 417-682-3579; John Hobbs in McDonald County, 417-223-4775 or Brie Menjoulet in Hickory County, 417-745-6767.', 'Have Seeds will Garden (Planting Guide for a wide variety of vegetables)\nSo you want to start a garden but you don\'t know exactly how deep to plant the seeds or how far apart. Check out this list for tips and guidance. ~Happy Gardening!\nAsparagus: Soak seed overnight. Sow ½ inch deep. 2 inch apart. 60 degree soil. Let grow 1 yr. then transplant to permanent area. Harvest begins 3rd year.\nBeans/Peas: Sow 1 inch deep. 24 inches apart. For peas and pole beans provide a trellis. Peas and pole beans can be planted a bit closer at 6-10 inches apart. Full sun/well drained soil. Keep pea pods picked early and often to keep plants producing, same for bush beans- keep bean pods picked often so that plants keep producing blossoms.\nCabbage: Sow 1/8th inch deep as soon as soil can be worked. Full sun. Thin to 24 inch apart. Average 100 days to mature.\nCarrot: Sow seed ¼ inch deep. Thin to 6 inches apart. Full sun. About 65 days to harvest.\nCucumber: Plant 4 seeds per mound. ½ inch deep. 1 foot apart. Provide trellis. About 60 days to harvest.\nEggplant: Start indoors. Transplant to garden when 6-8 inch tall. Full sun/well drained soil. About 80 days to mature.\nEndive: Sow seed directly in garden as soon as soil is workable. Thin to 12 inch apart. 85 days to mature.\nLettuces/ Chards/Greens: Sow directly in garden. No deeper than ¼ inch. Thin to 12 inch apart. Full sun. Average 40-55 days to mature.\nTomatoes: Sow seed 1/4 inch deep. Start indoors 6-8 weeks before last frost date. Transplant to garden when 6-8 inches tall. Space plants 2-3 ft. apart. Full sun. Provide trellising or tomato cage. Full sun. Matures at about 70-80 days.\nPeppers: Sow seed no deeper than ¼ inch. Peppers need soil temp. of 70 degrees to germinate well. You can provide a heat mat underneath your seed tray to assist in germination. Germination takes 7-24 days. Transfer plants to garden when 6-8 inches tall. Full sun. 70-80 days to mature.\nRadish: Sow direct in garden ¼ inch deep. Full sun. Thin to 2 inches apart. Matures in 28 days.\nOkra: Plant seed ½ inch deep directly in garden. Thin to 18 inch apart when 2 inches tall. Harvest pods when 3 inches long. Keep pods picked to ensure good production. About 55-70 days to mature.\nPumpkins: plant 2 seeds per hill 1 inch deep. 4-6 ft. apart. Full sun. Keep free of weeds. Rich fertile soil. Average 95-110 days to mature.\nMelons: Watermelon / plant 2 seeds per hill 3 ft. apart. 1 inch deep. After danger of last frost. Full sun. Keep weed free. 75-80 days to mature.\nCantaloupe/ plant 2 seeds per hill 1 inch deep. 4 ft. apart. Full sun. 10-20 days to germinate. 80-90 days to mature.\nSquash: Summer squashes/ Sow 2 seeds per hill. 1 inch deep. 3 ft. apart. Full sun. 60-65 days to mature.\nWinter squashes/ Sow 2 seeds per hill. 1 inch deep. 4-6 ft. apart. After danger of last frost. 80-90 days to mature. Full sun.\nTurnip: Sow seed directly in garden as soon as soil is workable. ¼ inch deep. Full sun. 50 days to mature.\nHerbs: Sow seed direct in garden or pot at a depth of 1/8th inch. Full sun.\nI\'m a photographer...and a country girl at heart.Born and raised in Florida, I have lived in many small/rural towns in Southern Florida. I love capturing the innocence and joy of childhood through photography.Along with photography I absolutely love gardening.\nContent /Writing copyright Tina Leavy/The Victory Gardener blog and The Urban Farmette blog and may not be reproduced without my written consent. Thank you for understanding. (images that have been used from Dk Images free clip art website have been duly noted..all other content/photos is/are copyright to the author of this blog)\nVisit our online Community Blog\n""Small Town Living""\nMy Victory Garden Etsy Shop\nI\'ve been stalked/My Victory Garden Shop Item featured here.']"	['<urn:uuid:3075b669-19a6-4015-9196-d0984ebf2faa>', '<urn:uuid:0f75a53a-d09b-47fa-a79b-2697792db029>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	12	51	1326
53	european governmental shortcomings immediately before first world war details	In the period before the 1914 mobilizations, Europe was characterized by government weakness, hesitations, indiscretions, and unavowed desires. The peaceful masses were left in stupefaction before the approaching cataclysm that would ultimately result in nine million deaths and ten million people crippled. This catastrophic situation demonstrated the significant failures of leadership during this critical period.	"[""Roger Martin du Gard's speech at the Nobel\nBanquet at the City Hall in Stockholm, December 10, 1937\nThe presence of so many illustrious persons\nassembled under the patronage of His Highness, the Crown Prince,\nheightens the emotions that I feel at finding myself here and\nhearing the words of praise that have just been addressed to me.\nI feel rather like an owl, suddenly roused from its nest and\nexposed to the daylight, whose eyes, used to the dark, are\nblinded by dazzling brightness.\nI am proud of the exceptional mark of esteem the Swedish Academy has bestowed on me, but I cannot conceal my surprise from you. Ever since I felt your favour lie upon and almost overwhelm me, I have asked myself how to interpret it.\nMy first thought was of my country. I am happy that in making a French author its choice for this year, the distinguished Swedish Academy has thought fit to glorify our French literature in particular. On the other hand, I know some great poets among my compatriots, noble and powerful minds, whom your votes might have chosen with much better reason. Why then am I today in this place of honour?\nThe demon of vanity, never completely silenced, at first whispered to me some flattering presumptions. I even went so far as to ask myself whether by granting this distinction to the «man without dogma,» that I profess to be, the Academy did not wish to emphasize that in this century, when everyone «believes» and «asserts», it is perhaps useful that there should be some who «hesitate», «put in doubt», and «question» - independent minds that escape the fascination of partisan ideologies and whose constant care is to develop their individual consciences in order to maintain a spirit of «inquiry» as objective, liberal, and fair-minded as is humanly possible.\nI should also like to think that this sudden honour acknowledges certain principles dear to me. «Principles» is a big word to be used by a man who says that he is always ready to revise his opinions. I must, however, admit that in the practice of my art I have imposed upon myself certain guidelines to which I have tried to be faithful.\nI was still very young when I encountered, in a novel by the English writer Thomas Hardy, this reflection on one of his characters: «The true value of life seemed to him to be not so much its beauty, as its tragic quality.» It spoke to an intuition deep within me, closely allied to my literary vocation. Ever since that time I have thought that the prime purpose of the novel is to give voice to the tragic element in life. Today I would add: the tragic element in the life of an individual, the tragedy of a «destiny in the course of being fulfilled».\nAt this point I cannot refrain from referring to the immortal example of Tolstoy, whose books have had a determining influence on my development. The born novelist recognizes himself by his passion to penetrate ever more deeply into the knowledge of man and to lay bare in each of his characters that individual element of his life which makes each being unique. It seems to me that any chance of survival which a novelist's work may have rests solely on the quantity and the quality of the individual lives that he has been able to create in his books. But that is not all. The novelist must also have a sense of life in general; his work must reveal a personal vision of the universe. Here again Tolstoy is the great master. Each of his creatures is more or less secretly haunted by a metaphysical obsession, and each of the human experiences that he has recorded implies, beyond an inquiry into man, an anxious question about the meaning of life. I admit that I take pleasure in the thought that, in crowning my work as a novelist, the members of the Swedish Academy wished to pay indirect homage to my devotion to that unapproachable model and to my efforts to profit from the instruction of his genius.\nI should like to conclude with a more sombre hypothesis, although I am embarrassed to disturb this festive mood by arousing those painful thoughts that haunt all of us. However, perhaps the Swedish Academy did not hesitate to express a special purpose by drawing the attention of the intellectual world to the author of L'Été 1914 [Summer 1914].\nThat is the title of my last book. It is not for me to judge its value. But at least I know what I set out to do: in the course of these three volumes I tried to revivify the anguished atmosphere of Europe on the eve of the mobilizations of 1914. I tried to show the weakness of the governments of that day, their hesitations, indiscretions, and unavowed desires; I tried above all to give an impression of the stupefaction of the peaceful masses before the approach of that cataclysm whose victims they were going to be, that cataclysm which was to leave nine million men dead and ten million men crippled.\nWhen I see that one of the highest literary juries in the world supports these books with the prestige of its incontestable authority, I ask myself whether the reason may not be that these books through their wide circulation have appeared to defend certain values that are again being threatened and to fight against the evil contagion of the forces of war.\nFor I am a son of the West, where the noise of arms does not let our minds rest. Since we have come together today on the tenth of December, the anniversary of the death of Alfred Nobel (that man of action, «no mere shadow», who in the last years of his life seems indeed to have put his supreme hope in the brotherhood of nations), permit me to confess how good it would be to think that my work - the work that has just been honoured in his name - might serve not only the cause of letters, but even the cause of peace. In these months of anxiety in which we are living, when blood is already being shed in two extreme parts of the globe, when practically everywhere in an atmosphere polluted by misery and fanaticism passions are seething around pointed guns, when too many signs are again heralding the return of that languid defeatism, that general consent which alone makes wars possible: at this exceptionally grave moment through which humanity is passing, I wish, without vanity, but with a gnawing disquietude in my heart, that my books about «Summer 1914» may be read and discussed, and that they may remind all - the old who have forgotten as well as the young who either do not know or do not care - of the sad lesson of the past.\nPrior to the speech, Professor A.E. Lindh of the University of Uppsala spoke: «It is with great pleasure and gratification that we find among our distinguished guests this evening Roger Martin du Gard, crowned today with the golden laurel of the Nobel Prize. We thank you most heartily for what you have given us through the medium of your literary work, and particularly for your great masterpiece, Les Thibault, which has come into being as a result of an intense study of reality, and of a profound knowledge of human dissimilitudes. In your psychological work survives that classical French realism which dauntlessly portrays life in all its naked truths, and which demands of its practitioners an incorruptible conscience and a great sense of justice. We admire the way in which you have permitted the family chronicles in Les Thibault to develop into a tragic and complete picture of Europe such as it appeared before those calamitous years of the World War. In acknowledging your powerful accomplishments we add our respect for the earnest pathos which runs through your literary works.»\nFrom Nobel Lectures, Literature 1901-1967, Editor Horst Frenz, Elsevier Publishing Company, Amsterdam, 1969\nCopyright © The Nobel Foundation 1937""]"	['<urn:uuid:dce55249-f560-45b3-b3a6-89841cdec4c2>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	9	55	1360
54	I'm researching WWI commemorations and would like to know about the central message that united all the peace vigils across New Zealand - what was this core message?	'Honour the war dead by ending war' was the central message that united the three themes of the vigils.	"['Nation-wide peace vigils to mark start of WW1\nNation-wide peace vigils to mark the 100th anniversary of the start of World War One “the war to end all wars”Peace Movement Aotearoa\n4 August 2014\nPeace vigils to mark the 100th anniversary of the start of World War One - ""the war to end all wars"" - will be held today in Hokianga, Waiwera, Auckland, Hamilton, Tauranga, Gisborne, Napier, Whanganui, Fielding, Otaki, Lower Hutt, Wellington, Blenheim, Nelson, Lower Moutere, Takaka, Dunedin, and Invercargill, updated details are at www.ww1.org.nzThe peace vigils on Monday, 4 August, are coordinated nation-wide by Peace Movement Aotearoa, in association with Quakers, and in conjunction with peace and faith groups in other parts of the world who are holding peace vigils to mark the anniversary and to highlight our mutual belief that conflict must be resolved peacefully rather than by war.\nThe peace vigils have three themes: remembrance of all of the casualties of war and of war resisters, ending war as a response to conflict, and promoting peace. There are four core messages that will be the same at all of the vigils, including the central message \'Honour the war dead by ending war\', which unites the three themes.\nDetails of the WW1 anniversary peace vigils around the country are available below and online at www.ww1.org.nz If you would like to mark the anniversary and cannot get along to a vigil or there isn\'t one in your area, please consider lighting a candle at 6pm on Monday, 4 August, in solidarity with all who resist war and promote peace.\nPhotos: please send any photos - whether of your solidarity candle lighting or from your peace or prayer vigil - to Peace Movement Aotearoa email firstname.lastname@example.org so they can be uploaded as a permanent record of our collective witness for peace on Monday, thank you.\nCandlelit peace vigils on Monday, 4 August:\n• Hokianga - Candlelit peace vigil at 6pm, at the Opononi boat ramp. Please bring your own candle, placards and banners will be available at the vigil. Organised by Hokianga Catholic Workers.\n• Waiwera - Candlelit peace vigil at 6pm, at the south end of Waiwera beach. Organised by local peace people,\n• Auckland - Candlelit peace vigil from 6pm to 6.30pm, at the Devonport War Memorial, corner Victoria Street and King Edward Parade, Devonport. Please bring your own candle, placards and banners will be available at the vigil. Organised by the Women\'s International League for Peace and Freedom and Pax Christi. The A4 poster is available at http://www.converge.org.nz/pma/vigil-akl-aug14.pdf and the Facebook event listing is at http://www.facebook.com/wilpfaotearoa.newzealand• Hamilton - Candlelit peace vigil, at 6pm in front of the Hamilton City Library in Garden Place. Organised by Hamilton Quakers, in conjunction with AVP Maori Focus.\n• Tauranga - Torchlight silent peace vigil, from 5pm to 9 pm at the Edgewater Fan, The Strand. Please bring a small torch and a sign with your message. Any sign must be free standing or held and not fixed to trees or structures. We welcome everyone who feels they wish to show their concern for the victims of the human tragedies due to the conflicts and wars which are every day news. Violence is not OK. Our combined hope is that peaceful solutions will be found to resolve grievances. Organised by Tauranga Quakers and the Tauranga Interfaith Council.\n• Gisborne - Lest we forget: candle light vigil for peace, gather at 5.30pm at the Wi Pere Memorial on Reads Quay (on the corner of Lowe Street), to walk as a group over Peel Street Bridge to the site of the Statue of the Soldier. Please bring a candle in a jar, an LED candle, a glowstick, a torch, any safe light to illuminate the dark. Organised by the Tairawhiti Interfaith Group. The A4 poster is available at http://www.converge.org.nz/pma/vigil-gis-aug14.pdf• Napier - Light a candle for peace / prayer vigil, from 12 noon to 12.30pm, St John\'s Cathedral, 28 Browning Street. You are invited to bring your own prayers / reflections to share with others, the emphasis is on remembering, on praying for an end to all war and for the promotion of the peaceful resolution of all conflict. People of all faiths and churches are invited to bring their own prayers or reflections to share with others. Organised by the Cathedral Environment, Justice and Peace Network.\n• Whanganui - Peace vigil from 12 noon to 12.30pm in Majestic Square. Organised by Whanganui Quakers and Operation Peace Through Unity.\n• Fielding - Details available on request from email@example.com\n• Otaki - Candlelit peace vigil from 6pm to 6.30pm at the Otaki War Memorial, 6 Main Street, next to the Otaki theatre. Organised by Kapiti Quakers and Otaki Catholic Workers.\n• Lower Hutt - World War One Memorial Vigil for Peace, from 6pm to 6.20pm in the carpark outside 153 Randwick Road, Moera. To remember all who have died or suffered in war - soldiers, civilians, war resisters - to end war as a response to conflict, and to promote peace in all that we do. Let us begin commemorations of World War One - the \'war to end all wars\' - by praying for an end to all war. Organised by the local intentional Christian community.\n• Wellington - Candlelit peace vigil from 6pm to 6.30pm at the Cenotaph, corner Bowen Street and Lambton Quay. Please bring your own candle, placards and banners will be available at the vigil. Organised by Peace Movement Aotearoa. The A4 poster is available at http://www.converge.org.nz/pma/vigil-wgtn-aug14.pdf and the Facebook event listing is at http://www.facebook.com/PeaceMovementAotearoa/events• Blenheim - Lest we forget: candlelit peace vigil at 6pm at the Seymour Square Memorial, help to find an alternative to war to solve our differences. Organised by Blenheim Quakers. The A4 poster is available at http://www.converge.org.nz/pma/vigil-blen-aug14.pdf• Nelson - Candlelit peace vigil from 5.30pm to 6.30pm at the Cathedral Steps, Trafalgar Street. Please being your candle to mark the anniversary and to highlight our collective belief that conflict must be resolved peacefully rather than by war. Organised by Nelson Quakers. The A4 poster is available at http://www.converge.org.nz/pma/vigil-nels-aug14.pdf - please note, Nelson Quakers are also holding vigils to mark Hiroshima and Nagasaki Days, details are on the poster.\n• Lower Moutere - \'Promoting peace\' vigil, at 6pm at the Riverside Centre, 289 Main Road, Lower Moutere (Moutere Highway). The Inakord choir will sing, followed by a reading and a minute’s silence; please bring your own candles. Organised by the Riverside Community, the Facebook event listing is at https://www.facebook.com/permalink.php?story_fbid=1444743095802048&id=1416689081940783• Takaka - Silent candlelit vigil at 6pm on Village Green, with an earlier gathering there for information sharing at 12.30pm. Organised by Golden Bay Quakers.\nDunedin - Candlelit procession at 5.45pm from First\nChurch, 415 Moray Place, to the Cenotaph, Queens Gardens -\nthe candlelit peace vigil will be at 6pm at the Cenotaph and\nwill include a time of silence and speakers. Organised by\nDunedin Quakers, in conjunction with the National Centre for\nPeace and Conflict Studies. The A4 poster is available at http://www.converge.org.nz/pma/vigil-dun-aug14.pdf• Invercargill -\nDetails will be available at www.ww1.org.nz when']"	['<urn:uuid:20bcd50b-2b1c-4b86-81b6-79192bb54062>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	28	19	1177
55	I'm researching urban food production solutions and wondering about the scalability of vertical farming - what evidence is there about its potential to address food security in major cities, considering factors like location flexibility and production capacity?	Vertical farming shows significant potential for urban food security. These facilities can be built anywhere within city limits, including in repurposed buildings like old factories, warehouses, and even nightclubs, eliminating transportation distances to urban consumers. Regarding production capacity, experts like MIT's Caleb Harper estimate that 30-40% of our diet could be produced in urban environments. Current facilities demonstrate this potential - Mirai produces 10,000 lettuces daily, while Green Sense Farms can distribute 3-4 million pounds of produce annually within 100 miles to serve 20 million people. This scalability is particularly important given projections that by 2050, 66% of 9.6 billion people will live in urban areas, creating unprecedented food demand in cities. The technology is already proving commercially viable, with companies like AeroFarms operating multiple facilities and achieving cash-flow positive operations while selling at prices comparable to organic produce.	['The interest in growing plants vertically was put in high gear in Milan, Italy in 2011 with a project that was conceived by architect Stefano Boer to build a vertical forest. He incorporated trees into the design of his high-rise residential towers. His buildings were completed in 2015, and launched an interest in vertical farming concepts like the one pictured above, also from Milan.\nFast forward several years and we have several communities around the world experimenting with vertical farming. The notion of renovating dilapidated, abandoned factories into urban farms is an exciting concept. This article describes one project in an abandoned steel mill in Newark, NJ that is setting a new standard to bring food to urban communities.\nThere is an informative video about vertical farming in general, and AeroFarms specifically.\nAeroFarms has put $30m into a green revolution that seeks to produce more crops in less space, but whether it’s economically viable is an open question\nAeroFarms grows its greens using a nutrient-rich mist on plants anchored in a reusable cloth made of recycled plastic bottles. Photograph: Malavika Vyawahare\nAn ambitious, almost fantastical, manifestation of agricultural technology is expected to come to fruition this fall. From the remains of an abandoned steel mill in Newark, New Jersey, the creators of AeroFarms are building what they say will be the largest vertical farm, producing two million pounds of leafy greens a year.\nWhether it even qualifies as a “farm” is a matter of taste. The greens will be manufactured using a technology called aeroponics, a technique in which crops are grown in vertical stacks of plant beds, without soil, sunlight or water.\n“I ate some of the arugula here,” said New Jersey governor Chris Christie after a recent visit to a smaller AeroFarms facility in the neighborhood. “It tastes fabulous. No dressing necessary.”\nThe farm, built in the economically depressed New Jersey city promises new jobs, millions of dollars in public-private investment, and an array of locally grown leafy greens for sale. The company has spent some $30m to bring to reality a new breed of “green agriculture” that seeks to produce more crops in less space while minimizing environmental damage, even if it means completely divorcing food production from the natural ecosystem.\nAeroFarms and other companies developing similar controlled growing climates claim to be transforming agriculture. Proponents of vertical farming call it the “third green revolution”, analogizing the developments to Apple and Tesla. They tout the potential of such technology to address food shortages as the world population continues to grow.\nAeroFarms touts their products as free of pesticides and fertilizer, an attribute that investors think will attract customers who buy organic produce. “We definitely see the need for healthy food in the local area and Newark in particular,” said Lata Reddy, vice-president for corporate social responsibility at Prudential Financial, one of the investors in the project.\nIs the arugula edible? Proponents say yes. Photograph: Malavika Vyawahare\nBut, food that is not grown in soil may not be palatable to many, even those who are opting for organic substitutes. “If you take the soil out of the system, is it a legitimate organic system?” questioned Carolyn Dimitri, director of the food studies program at New York University. The US Department of Agriculture does not consider the question of organic certification for growing methods that do not use soil, according to AeroFarms’ website.\n“Urban farming is trendy,” Dimitri said. It remains an open question, she said, whether it will be economically viable. Prudential Financial has invested “patient capital” in the venture, which is used to finance social impact projects that are unlikely to yield benefits right away. There are no aeroponics projects of this scale but AeroFarms has piloted the technology at Philip’s Academy charter school in Newark, where students are served greens grown at the school.\nSeventy times the yield of traditional farms\nMarc Oshima, the chief marketing officer at AeroFarms, yanked open a tiny grey door in a back alley in downtown Newark that leads into an old nightclub with vividly painted walls. In 2014, AeroFarms converted the space into a research and development facility. “Out there, in nature, we don’t have control over sunlight, rainfall,” Oshima said, “here, we are giving plants what they need to thrive.”\nThe moist sanitized air that envelops the R&D lab is missing one ingredient: the earthiness that permeates any agricultural operation.\nAt the repurposed sites, AeroFarms is pushing the limits of what David Rosenberg, the company’s CEO, calls “precision agriculture”. The scheme ditches the romanticized ideal of farming, acres and acres of open fields dotted with men and women toiling in the sun, getting their hands dirty, in favor of enclosed urban spaces where engineers, electricians and harvesters mill about, wearing protective clothing, masks, and gloves.\nWith its multicolored LED lights, computer screens lining the walls, and faithful preservation of club decor, AeroFarms’ research facility could easily pass off as a sci-fi themed club. It makes a befitting setting for a company that is promising to increase crop yields by as much as 70 times compared to traditional field farms, without using any pesticides or fertilizers.\nThe fine print is that the productivity is calculated using square footage occupied and not the vertical space utilized, making comparisons with ground floor-only traditional farms fraught. And critics point out that no traditional farm that size comes with a price tag of over $30m.\nThe leafy greens nurtured under multicolored LED lights. Photograph: Malavika Vyawahare\nMuch of the funding is coming from impact investing arms of big-ticket investors like Goldman Sachs and Prudential Financial. AeroFarms has leveraged its social impact goals to attract investments, promising to create jobs in a languishing economy and supplying fresh local produce to the community in Newark.\nFor New Jersey, where unemployment rates have been persistently above the national average, the promise of new jobs and fresh investment has ensured buy-in from the state. Christie, visiting the smaller aeroponics facility in March lavished praise on the “public-private” partnership.\nThe New Jersey Economic Development Authority provided nearly $9m in incentives, stretched over 10 years, which includes a $2.2m grant under the Economic Redevelopment and Growth program and $6.5m in tax credits.\nAeroFarms currently employs close to 100 people, and is promising more jobs in the months to come as the company grows. Like other companies in this space, it is relying on productivity gains to offset high cost of expensive technology and emerge as a successful business.\nBut even growing success isn’t a sure thing, let alone profit margins.\n‘More like a factory than a farm’\nA growing unit under construction in the Newark facility. Photograph: Malavika Vyawahar\nAeroFarms has grown over 250 types of leafy greens and sells more than 20 varieties of greens such as arugula, kale and spinach but hopes to expand their offering in the future. The scheme imposes height constraints; as of now, everything grown at vertical farms is a type of short-stemmed leafy green. And while controlled growing allows year-round production and protects these new-age farmers from the vagaries of nature, they still contend with the possibility of crops dying from human error or technological malfunction.\nRising from the middle of what used to be a dance floor is a gargantuan growing machine about 20-feet tall. The rectangular apparatus is a stack of growing beds, each about 20-feet long. It resembles a gigantic fridge missing its outer casing, but instead of being used to store greens, they are growing inside. Inhabiting patches on the seven-tier machine, are leafy greens of all ages: seedlings, shoots and fully grown plants. Freshly minted leaves fluttering gently in an artificially conditioned breeze.\nAbove each bed are columns of LED lights, bathing the plants in a sharp white glow. When plants photosynthesize, they convert light of certain wavelengths into chemical energy, and store it for future use. This light does not necessarily have to come from the sun, Oshima explained.\nUnder the bright lights the plants appear to be embedded in crumpled soggy blankets. The use of growing mediums other than soil is not unique to aeroponics; planting seeds in cotton has been a popular idea for many a school science project. In recent years, a related technology called hydroponics, that uses water as a medium to grow plants, has caught on. But Oshima is quick to distinguish aeroponics from hydroponics emphasizing that their technology is superior. And the key to the technology, is what happens under the microfleece membrane. If peeled it would reveal bare roots enveloped by nutrient-rich mist.\nFarming in artificially created conditions is itself not an entirely novel idea. Similar techniques are used in extreme environments where growing food the traditional way is not possible, including the United States South Pole Station, where researchers live in a isolated hostile conditions for months at a stretch, and the International Space Station has its own space garden deploying a growing system called Veggie.\nBreaking down the process. Photograph: PR\nThe rationale for using similar methods in places where land has for centuries been tilled to grow food emerged at the turn of the century in response to urbanization and population growth. The world’s population will bloat to 9.7 billion by 2050 and 70% of people will reside in urban areas, according to the World Health Organisation. Using large swathes of land for growing food will not be an option, supporters of vertical farming argue.\nDickson D Despommier, a microbiology professor and a top proponent of vertical farming, sees the agricultural technology not just as a response to food crisis but also as a means of returning land that was previously used for agriculture to its natural state.\n“We are just academics, we just sit here and watch these ideas grow,” Despommier said on a podcast he hosts on urban farming, marveling at the scale of the new operation.\nAeroFarms has built its sales pitch to investors around more pressing and concrete concerns like land and water shortages, meeting the demand for locally grown greens, and climate change. Growing and selling locally means emissions associated with transportation are reduced. What remains unclear is how the company accounts for emissions arising from the farm’s substantial energy needs.\nVertical farming cropping up around the world\nThe facility is still under construction on Rome Street in Newark. Photograph: Malavika Vyawahare\nIn the last decade, a few bold schemes have built on this seminal idea, with the first commercial vertical farm set up in Singapore in 2012. Japan boasts of its own semiconductor factory-turned-lettuce farm, an idea that gained some traction after the Fukushima reactor meltdown in 2011 exposed the susceptibility of arable land to long term contamination. In the UK Growing Underground has converted a second world war bomb shelter in London into a hydroponics farm.\nIn the US, at least five new commercial vertical farming operations have emerged over the past five years that use a range of controlled growing technologies to allow year-round harvests of crops that typically have a short growing season in Michigan, and more efficient water use in California. At Ouroboros Farm in California, for example, hundreds of fish are fed organic feed, the waste produced by them is used to nourish seedlings and plants floating on raft beds above the fish tanks.\nSome experts like Dimitri believe that such large urban farms are so far afield from traditional ones that “farm” may not be the word for them. “It is more like a factory than farm,” she said, “almost like broiler production, very controlled and regimented.”\n“People want to be hopeful, they want a solution that works,” Dimitri said. “Some people think it is the way of the future. I think it is just another production technology, I don’t think it is going to turn agriculture on its head.”\nNew agricultural technologies like aeroponics are unlikely to make a dent in the global food crisis, for now. Countries which face the highest food insecurity don’t dabble in expensive new technology, and even if they do, the produce may not reach underserved populations. “It is a technology whose time has come for the rich,” Despommier argued, noting that “it is already popular in Japan, and countries in the Middle East that want to reduce dependence on food imports have also shown interest.”\nReddy was also measured in her assessment: “What we see here will not disrupt the entire farming industry but a particular niche.”\nBut proponents like Despommier see enterprises such as AeroFarms as a way forward. “There is no limit to what you can do,” he said, while acknowledging that one of the biggest challenges going forward will be growing other crops like rice and wheat, crops that could feed the world.\nArticle written by Malavika Vyawahare\nThis story was reported with the support of the UC Berkeley-11th Hour Food and Farming Journalism Fellowship.\nOriginally published in The Guardian.com, August, 2016\nThis Farm of the Future Uses No Soil and 95% Less Water', 'In case you haven’t noticed, the world’s population is drastically moving. In two directions: upwards and inwards. By 2050 we will need to feed 9.6 billion people. A whopping 133% more than today’s 7.2 billion and 2200 times as many mouths than the population of New Zealand right now. It’s moving inwards too. Today, more than half of us live in densely populated urban environments. By 2050 66% (or 6.3 billion) of us will. This means one thing. An unprecedented demand for food – in urban areas.\nA scary thought, given food production and agriculture are already shockingly taxing on the natural environment – and in many cases – our health too. From water, energy and land use to the nasty issues surrounding fertilizers and pesticides, transportation and waste, conventional agriculture is far from earth friendly.\nSo it comes as no surprise that safeguarding global food security and sustainability for 9.6 billion people will mean doing more with less. And with the masses moving into our city centres, it will also mean doing things differently. Vastly differently. Especially when it comes to producing land and resource intensive (water and fertilizer) staples like horticultural land-based foods, including leafy greens, fruit and veggies. Because, while we can 3D print and bioengineer almost anything now (including the recently hyped Impossible Burger and other forms of juicy beef patties and protein alternatives made from real bovine or plant cells), one hard fact remains. We aren’t making any more land. And 3D printing can only go so far.\nMuch like the changing dynamics of our population itself, this will likely mean moving arable crop production inward (i.e. into urban centres) – and more importantly upward (i.e. into the ether). Think high-rise urban indoor farming – on a mass scale. A mixture of hydro, aqua and aeroponics – and at lofty heights.\nSome serious food for thought for New Zealand’s largely pasture and rural based agricultural and horticultural sectors valued at over 7.6 billion dollars – and nearly $4.3 of this export revenue (according to Industry publication Fresh Facts).\nData Science Meets Horticulture.\nIndoor (and vertical) farms are essentially highly controlled and automated multi-level growing environments that use a combination of software analytics, energy efficient LED lighting, sensor controls and closed loop moisture and nutrient recirculation systems to allow growers to monitor, nourish and grow plants like lettuces, greens and even fruit now using a fraction of – or no- resources such as water, soil and environmentally dubious fertilizers and pesticides. All year round. 24 hours a day. No matter what season and without a drop of real sunlight in sight. Yes, the veritable disruption of traditional field ag as we know it. How will our Kiwi field growers cope with such significant technological and social change?\nAnd before you naysayers out there think the world’s city dwellers will revolt at such a sterile, technologically complex concept such as industrial food production, think again. We’ve already seen how the “Impossible Burger” and Beyond Meat’s succulent meats range, including meatballs, patties and chicken strips are flying off the shelves in supermarkets throughout the States. Why would fruit and veg (which is actually the real thing) be any different? Something our traditional Kiwi horticultural farmers and the entire agricultural industry would be wise to take note of.\nThe arguments for high-rise farming are profound, to say the least. Indoor and vertical farming help to provide some very real and practical solutions to some of most pressing problems facing agriculture today, while being a highly profitable business model. And it’s happening already. In leaps and bounds.\nMirai, the world’s largest sensor-controlled hydroponic indoor vertical farm located in Japan, uses a closed nutrient recirculation system that requires 99% less water than traditional farming methods. Yes, 99% less. Spanning 25,000 square feet of vertical garden beds, its 17,500 energy efficient LED lights adapted with wavelengths to control the night-and-day cycle and accelerate growth mean highly productive and nutrient dense plants. 10,000 lettuces in a day productive – all bacteria and pesticide free.\nAeroFarms and Green Sense Farms, two (of many) US based mass scale vertical farming companies making huge inroads in this emerging market are also yielding similar environmental benefits. AeroFarm’s custom UV light spectrums, endless patented growth algorithms and 30,000 data points monitored by plant scientists mean it is able to produce leafy greens, herbs and lettuces en mass, 24/7, using almost no traditional field inputs in sight – including soil. “We use about 95 percent less water to grow the plants, zero soil, about 50 percent less fertilizer as nutrients and zero pesticides, herbicide, fungicides,” says David Rosenburg, AeroFarm CEO and co-founder. “Plants don’t need soil, they need nutrients. And they don’t need sun, they need spectrum of light”. According to Rosenburg, plants receive the perfect amount of moisture and nutrients misted directly onto their roots in a completely controlled environment. Sacré Bleu!\nAnd Green Sense Farm’s two industrial-sized, climate-controlled growing rooms outside of Chicago (each equipped with seven 12-meter tall grow towers) have over 14,000 LEDs that require less climate control, use less energy and absolutely no GMOs. Its cool burning green LED production modules mean lights can be placed closer to the plants, allowing for more levels to be stacked and more bang for your energy buck.\nCritics of vertical farming worry that the energy use involved in such growing techniques might increase the carbon footprint by an order of magnitude. However, with the energy efficiency of LED lighting increasing exponentially, and as solar powered renewable energy is rapidly reaching mainstream in major markets globally, this trepidation might become redundant before we know it.\nIn world with a constantly growing base of discerning consumers concerned about environmental and sustainability impacts of the food they are consuming, how will Kiwi field farmers compete with such staggeringly impressive resource efficiencies?\nThe Sky’s your Limit\nBut if environmental benefits aren’t enough to convince you of the real value of vertical farming then the numbers probably will. According to Caleb Harper, principal research scientist for the Open Agriculture (OpenAG) Initiative and CityFarm at MIT’s Media Lab, 30-40% of our diet (including greens, tomatoes and capsicums) could be produced in urban or peri-urban environments in the very near future – and it would be a lot better for us if it was.\nDubbed the “Vegetable Factory” of Japan, Mirai’s completely automated (including humidity, temperature, CO2, and irrigation) multi-level system spanning half the size of a football field is already 100 times more productive per square foot than traditional methods.\nRosenburg says AeroFarm’s indoor leafy crops grow twice as fast as regular field seeds inside its controlled environment and the company has specifically designed customisable stackable modules to get even more greens per square foot. At 12 levels high, this means crop yields that are 75 times greater per square foot than traditional field methods annually. What’s more, AeroFarm’s newest operation built in Newark, New Jersey, spans 70,000 square feet (6,503 square meters) of growing space 30 feet high – double that of its smaller (already productive) farms and leading Japanese competitors. “It has the capacity to grow just under 2 million pounds of baby greens annually” says Rosenburg.\nSignificant vertical economies also allow companies like AeroFarms to spread out operational costs (like HVAC, lighting and rent) amongst a much larger product base as well as reduce the cost of goods sold to the end consumer. “We sell at the same price that supermarkets buy from field farmers in the category of organics which is typically about a 20% premium” says Rosenburg. With layer after layer of thriving verdant greens sprouting day in – day out, the company says it is already cash-flow positive.\nAnd Green Sense Farms’ unique vertical stacking system in its 30,000-square foot (2,800 square metre) Chicago facility can distribute produce within 100 miles to over 20 million people. “At capacity, we’re producing about three to four million pounds of fresh produce a year” says CEO Robert Colangelo.\nFigures not to be sniffed at by even the most productive of Kiwi growers. Such unheard of levels of food productivity (volume) using a relatively miniscule footprint of land will be a godsend when considering the 9 billion people we’ll soon be having to cater for.\nWhat’s more, thanks to exponential improvements in robotics technology, Mirai and US vertical farms, including FarmedHere, along with AeroFarms, Green Sense Farms and Spread are also developing the technology to become robot-run farms devoid of human error. With Mirai already (human) hand harvesting over 10,000 heads of fresh lettuce, the mind boggles at just how many more greens these companies will be able to produce with robots in tow.\nTastebuds Gone Wild\nStill not sold? Controlled indoor farms (vertical included) – also mean crops are protected from insects, diseases and aren’t exposed to brutal weather conditions like drought and flooding – the bane of many farmers’ existence – making toxic chemical use and crop yield instability a thing of the past for growers. And for the consumer, this means healthy and nutritious food. Indoor farms like Aero, Mirai and Green Sense are entirely pesticide, herbicide, insecticide and fungicide free. Good news for health-conscious punters like me, since the last time I checked, no one wants to order a side of herbicide or chemicals with their salad.\nAeroFarms have taken this one step further by focusing as much on consumer nutrition as it does on low environmental impact. Greens are scrupulously macro & micro monitored for nutrient density, providing all the minerals and vitamins you could ask for. And the customers are lapping it up. Even top upmarket US restaurateurs, who would once have shunned the idea of buying indoor crops as part of their chic garden to table, local produce offerings, are sold on the stuff. “They taste the way greens are supposed to taste, the way I remember them tasting as a kid. There’s a flavour profile and if you put them up against greens from another big supplier, there’s no comparison in the freshness factor and in visual appeal” says Steven Yglesias, owner of a popular restaurant in Newark’s Ironbound neighbourhood.\nNo Nasty Food Miles\nBut perhaps one of the biggest advantages of vertical farming, at least when it comes to feeding the urban masses, is that you can build an indoor farm anywhere and to whatever size and height you want (within city limits). Without a ray of sun or speck of fertile NZ soil in sight. The sky’s your limit – quite literally. Mirai’s plant factory is located in an old semi conductor factory in eastern Japan’s Miyagi Prefecture – close to masses of urban mouths. Green Sense Farm’s green bounty is grown beneath 30 foot ceilings in a leased warehouse centrally located in an industrial park 40 miles outside of Chicago, a massive food distribution hub. And AeroFarm’s 9th domestic facility (yes 9th!) was built in a nondescript former paintball and laser tag facility in Newark, NY.\nIt also opens up the whole “buy local and fresh” model to the masses, traditionally a luxury only for the higher income earning echelons of society. Consumers everywhere will soon be able to eat nutrient dense fresh produce without the food miles. “When we build a farm, we become a part of the local community, bringing new life, to old buildings and reducing transportation miles” says Rosenburg.\nWhich begs the million-dollar question for our ag industry: Why buy clean green Kiwi produce when you can buy local and environmentally benign alternatives with no food miles attached? With indoor farms being built inside of, or in the fringes of massive cities, the need to transport (what once was) fresh and highly perishable farm produce (often) thousands of kilometres to urban consumers becomes close to ZERO. A huge plus for consumers and the environment, given the Tesla of the trucking world and mass scale electric transportation logistics have yet to reach mainstream.\nMoreover, what happens to our horticultural exports altogether (to the tune of $4.27 billion plus in 2015) as companies like Green Sense and AeroFarms rapidly gain stronger footholds in the lucrative markets we sell into? Green Sense has already signed a partnership to operate 20 mass scale vertical operations in China. The first farm is set to produce 750,000 to 1 million heads of lettuce and about 1.5 million leafy greens per year.\nOf course, not all of New Zealand’s horticultural exports are leafy greens suitable for inner city high rises. We export far more palatable plant based goodness than this. But it does beg the question as to how such technologies and bioscience could potentially disrupt these profitable hort subsectors too, and in a way that could possibly erode all our field pasture competitiveness we have steadily worked on over time. Our vital export revenue mainstay may be given a hideous shake up. Whether we like it or not.\nWhere to for Kiwi growers?\nWith stats like this, it is clear that vertical farming could play a key role in helping to avert a looming global food and environmental crisis – and one day become the future of agriculture. And with good reason.\nThe unheard of volume (productivity) of nutritious food that can be produced locally on the same footprint of land using a fraction of resources will be vital for feeding the 9 billion mouths we’ll soon have to cater for. And for areas where extreme weather conditions and resource scarcity routinely threaten their agricultural livelihood (think the sub-Saharan Africa, Middle East, United States and Southeast Asia) indoor farming techniques will very likely rise to vertical heights.\nThe question New Zealand now needs to ask itself is: As agrarian outdoor growers, what is our role in this rapidly changing new indoor, urban food producing dynamic? Yes, vertical farming may presently have found a niche in producing smaller lightweight crops that don’t require a huge amount of indoor real estate, all of which affords our fruit and larger crop growers some breathing space. For now. But we all know how quickly technologies can develop to shake up the status quo and erode industries overnight. Do we need to shift our focus away from bulky crops ill-suited to indoor warehouses for the future, or should we focus on plant varietals that labs and technology simply won’t be able to mimic in an indoor setting? Do we need to be riding on the coat tails of indoor farming’s technological success by investing in similar operations and technology? Or does New Zealand need to give up the ghost completely, let go of our entrenched desire to be leaders in agriculture and focus on new industry development altogether?\nThe answers remain to be seen. But such questions are highly worthy of asking – and planning for – right now.']	['<urn:uuid:c71999a1-8a2b-448c-84b4-b6565164eac3>', '<urn:uuid:f298393e-a09c-4ed1-9bf5-29662d9e86a3>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T06:15:13.904987	37	139	4621
56	What's the most affordable way to see a show at Vienna's opera?	The most affordable option is to get a standing ticket (Stehplatz) at the Staatsoper. With a federal theatre card, you can purchase a standing ticket for €4 or €5 the day before the performance. Alternatively, you can buy standing tickets on the day of the performance for €13-18.	['It was Molière who said, “Of all the noises known to man, opera is the most expensive”.\nTurns out our French playwright was wrong. At least in Vienna.\nIf you thought a ticket to The Magic Flute would cost more than an actual magic flute, I have some good news: opera in Vienna is priced for everyone.\n- Cheaper seats may obviously sell out fast\n- …but check for returns on the day\n- …and look for the remarkably inexpensive standing space tickets\n- See also:\nVisiting the opera is one of my recommended authentic Vienna experiences. You can, of course, spend a three-figure sum on a seat. But you don’t have to.\nAt the Staatsoper, for example, I’ve recently seen seats for the 2022/2023 season as cheap as €16 for some productions.\nSo what’s the catch (apart from the possibility of a restricted view)?\nNice as low prices are, the three main Vienna opera houses don’t have unlimited capacities. So you usually need to book early if you want to buy the inexpensive seats.\nThe Staatsoper (state opera house) is the most popular venue, and ticket sales and pre-bookings typically start as soon as the season previews are out.\nHowever, pre-bookings for later productions may be oversubscribed, and opera-hungry locals may also snap up the cheap tickets for redistribution to their private operatic circle of friends.\nEven if a Staatsoper performance is seemingly sold out when you search for tickets, my opera-going friend suggests you try the ticket offices on the day in case of returns.\nYou might be lucky, too. International travel is still relatively low in 2022, which may free up space for those activities (like a visit to the Staatsoper) that might normally be sold out.\nEven in “normal” times, I once got two tickets to Madame Butterfly just a month in advance for €30 total direct from the Staatsoper website (albeit with restricted views).\nImagine: two tickets to one of Puccini’s famous works for about the cost of a cappuccino, double espresso and four pieces of cake in a neighbouring coffee house.\nThe standing ticket solution\nAnother way to experience Viennese opera cheaply is through a standing-room ticket (a “Stehplatz” ticket).\nStaatsoper standing tickets\nEssentially, you have two options for getting hold of one of the 400+ standing tickets available for each performance at the Staatsoper:\n1. Register for a (free) federal theatre card\nAn Austrian Federal Theatre card allows you to purchase a standing ticket from a contingent online or at the box office from 10am the day before the performance for either €4 or €5, depending on where you stand. I used this system to see La Bohème and, most recently, Carmen (with Elīna Garanča!).\n2. Buy a standing ticket on the day\nRemaining standing tickets go on general sale on the day of the performance.\nHere’s how that works for the 2022/2023 season…\nYou go to the dedicated “Stehplätze” ticket office (“Stehplatz-Kasse”) on the Operngasse side of the state opera house for the evening performances. That’s the southwest corner of the building.\nThis ticket office opens 80-120 minutes before the start of a performance, when you can buy standing-only tickets for that day’s production. The tickets cost:\n- €13 for the Balkon area\n- €15 for the Galerie area\n- €18 for the wonderful Parterre area\nCheck the Staatsoper service pages for the latest situation on standing tickets.\nIf you can, get a Parterre ticket: these have some of the very best views in the house!\nIn previous seasons, you could only buy one ticket per person. So everyone who wants to attend should be present when you buy the tickets, just in case.\nWhatever method you use to buy your standing ticket, you get to see a performance in one of the world’s most prestigious opera houses for not much more than the cost of your average cinema ticket.\nNow for some info and insider tips for those standing at the Staatsoper:\n- If all you want is a taste of the Vienna state opera experience, this is a cheap way to do it. Another alternative, however, is to take a guided tour (which makes more financial sense now standing ticket prices have increased from the previous season)\n- If the idea of standing for four hours waiting for Isolde to finally keel over is not your idea of fun, fear not. The Staatsoper provides upholstered supports for you to lean on. Not only that, but small, unobtrusive monitors supply you with English subtitles to the libretto\n- I’ve been to the Staatsoper several times and picked up cheap standing-only tickets 30 minutes before the performance started. But…\n…it pays to get there early for more popular productions or those featuring the top stars. I tried twice to get standing tickets for Cavalleria Rusticana and failed both times because of the queues. That’s why I now have one of those Federal Theatre cards.\n- Standing positions in the opera house are numbered, so you don’t need to reserve your place with a scarf or similar\n(Previously, you could “reserve” standing room by hanging something across the relevant balustrade or upholstered support. Equally, you often found places already reserved in this way.)\n- A standing ticket entitles you to use the intermission bars just like any visitor. You can go to a bar before the performance and preorder drinks for the interval to speed up the process\nOther opera houses\nAt the Theater an der Wien, standing tickets are currently unavailable. The main house is undergoing renovations until autumn 2024, and performances have moved to other locations that don’t have that standing facility.\nAt the Volksoper, you order standing tickets (if available) just like any other ticket and they typically cost around €3 to €8.\nAll-in-all it seems opera need not be as expensive as you might think.\nSo take that, Molière.\nP.S. Here a few tips on what to wear.']	['<urn:uuid:a825d719-4970-4533-aaca-61e539807b6b>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	12	48	989
57	What connects artistic restraint in melody and room aesthetics?	In music composition, strategic avoidance of certain notes (like avoiding F# in G Major) can create interesting ambiguity and mood, reflecting Miles Davis' principle that 'it's not the notes you play, it's the notes you don't play.' This parallels Scandinavian interior design's philosophy of purposeful minimalism, where carefully selected elements and uncluttered spaces create sophisticated environments, demonstrating how restraint can enhance artistic impact in both domains.	['Ableton Liveschool trainer Michael Di Francesco has many many projects and equally many talents. In between albums and touring for Van She, one of his aliases, Touch Sensitive, managed an eight-week stint at number 1 on the Beatport indie chart in 2012 with his Anna Lunoe collab “Real Talk”.\nMichael recently dropped his latest Touch Sensitive track “Pizza Guy” and it’s already caught over 300,000 plays on soundcloud. “Pizza Guy” is in G Major, but has an intriguing mood, so we picked is brain on the music theory behind this.\n[soundcloud url=”http://api.soundcloud.com/tracks/87943423″ params=”” width=” 100%” height=”166″ iframe=”true” /]\nMichael: “The whole track is based on one simple idea, the ostinato melody that just repeats and repeats while the bass line moves around under it….the long release times of the synth playing the ostinato melodies notes gives the impression there are chords there when there aren’t really any”\nMichael’s ostinato melody is G-E-G-D-G-E-G-D repeated. The long release times on the synth makes these notes blend together and implies the presence of a chord. As the G is being played twice as often as the E or D in this melody, it’s reinforcing G as the home note (the root of G Major).\nMichael: “The overall effect of this melody combined with the bass notes gives it a particular mood”\nThe bass line in Touch Sensitive – ‘Pizza Guy’ is E-D-C, E-B-C. The emphasis here is on the C note as it’s being played twice as often as any other (and held longer also), this emphasis makes the C note feel like a second home. If we add the C note that the bass line revolves around to the ostinato melody, we’d have C, E, G and D – which together make a C Major add 9 chord. So before reading further, let’s re-iterate that the most played note in the ostinato melody is G, while the most played note in the bass line is the C.\nMichael: “I avoided using the F# note which really affects the mood”\nNow on to the really interesting part, Michael has avoided using the F#, let’s discuss that! Thinking about the circle of 5ths, we know that G Major is next to C Major, which means (amongst other things) that these two scales share all but one note. That one note of difference is the F# (if you’re in G Major) or the F (if you’re in C Major). So while Michael’s song is in G Major, by avoiding the F# note he’s also implying that he could be in the C Major key, which adds complexity and interest to the piece as it creates a feeling of playing in two keys at once. This technique of avoiding certain notes and as a result creating ambiguity as to which key your song is pretty simple, but can be really effective in creating a unique an interesting mood. The ambiguity between G Major and C Major has been further intensified in Pizza Guy by the how ostinato melody focuses around the G (implying the G as the root note), while the bass line focuses around the C (implying C as the root note). It really brings to mind Miles Davis’ classic quote “it’s not the notes you play, it’s the notes you don’t play”.\nWant to try it at home? Take a look at the circle of fifths and pick two adjacent keys, for example E Major & A Major. Work out which is the note of difference between them (in the case of E Major and A Major it’s the D / D#) and completely avoid using this note in your track. Then make some elements of the track focus around the E, while others focus around the A to further emphasize the ambiguity of which key your track is in. Good luck!\nFor another track breakdown check out Elizabeth Rose’s tutorial on the music theory considerations behind writing the hooks and melodies for her single “The Good Life”.\nSubscribe to our newsletter to keep up with our latest free tutorials, samples, video interviews and more.\nLearn more about Music Theory for electronic music.', 'Scandinavian interior design: Welcome to a world where simplicity meets sophistication, where functionality embraces beauty – the world of Scandinavian interior design. Hailing from the enchanting Nordic countries, this design philosophy has captured hearts worldwide with its understated yet impactful aesthetic. You might be thinking, “Isn’t Scandinavian interior design expensive?” Fear not, for this blog will unravel the secrets of infusing your home with Scandinavian charm without straining your wallet. Join us as we delve into the essence of Scandinavian interior design and unveil pocket-friendly ideas to revamp your living space.\nDecoding Scandinavian interior design\nScandinavian interior design, born in the mid-20th century across Denmark, Norway, Sweden, Finland, and Iceland, is a reaction to the fast-paced industrial world. It’s a harmonious blend of form, function, and nature’s touch. Here are the core principles that define Scandinavian interior design:\n- Less is More: Scandinavian interior design celebrates minimalism. It rejoices in clean, uncluttered spaces, creating an aura of tranquility and balance.\n- Embrace Nature: Natural materials like wood, wool, leather, and stone lie at the heart of this design ethos. They infuse a sense of warmth and authenticity, connecting your space to the great outdoors.\n- Light and Airy: With prolonged dark winters, Scandinavians have mastered the art of maximizing natural light. Light colors – especially whites and soft grays – dominate the palette, giving rooms an open and fresh ambiance.\n- Purposeful Furnishings: Functionality takes precedence in Scandinavian interior design. Furniture isn’t just art; it’s art with a purpose. Every piece is thoughtfully designed to be practical and beautiful.\nBudget-Friendly Scandinavian interior design Hacks\nScandinavian Interior design #1 – Thrifted Treasures\nDelve into the world of thrift stores, garage sales, or online marketplaces where hidden gems await your discovery. Search for a vintage wooden chair with sleek lines and a timeless silhouette. Even if the chair has lost some of its original luster, a fresh coat of paint can work wonders. Opt for a soft pastel shade or classic white that aligns with the Scandinavian palette. Before painting, lightly sand the surface to ensure a smooth finish. By giving new life to an old piece, you’re not only adding an affordable accent to your space but also contributing to the sustainability of your decor.\nScandinavian Interior design #2 – DIY Minimalism\nChannel your inner artisan with these DIY projects that capture the essence of Scandinavian minimalism. Transform wooden crates into a minimalist side table by stacking them to create tiers. Secure them in place, and you’ll have a functional and stylish addition to your living room or bedroom. To infuse your walls with natural charm, gather branches, twine, and dried leaves. Arrange them in a geometric pattern or let your creativity flow freely. Secure the elements with twine or simple frames for an art piece that pays homage to the raw beauty of nature.\nScandinavian Interior design #3 – Indoor Greenery\nEmulate the Nordic connection to nature by introducing houseplants that thrive indoors. Opt for ferns, succulents, or snake plants – they’re not only budget-friendly but also easy to maintain. Place them in pots made from affordable materials like terracotta or repurposed containers from thrift shops. These plants infuse your home with a refreshing vitality, and their presence aligns seamlessly with the Scandinavian love for the natural world.\nScandinavian Interior design #4 – String Lights Serenity\nString lights are a budget-friendly way to infuse your space with a serene and inviting atmosphere. These lights aren’t exclusive to the holiday season; they can be used year-round to create a calming ambience reminiscent of Scandinavian evenings. Drape them along a wall in a wave-like pattern, suspend them from the ceiling in a cascading arrangement, or intertwine them with potted plants to add a touch of enchantment to your surroundings.\nScandinavian Interior design #5 – Functional Wall Decor\nMake a stylish statement at your entryway with functional wall decor that captures the essence of Scandinavian interior design. Hang wooden pegs or hooks in a minimalist design and a light wood finish. These hooks serve as a coat rack that’s not only visually appealing but also practical. Say goodbye to cluttered entryways, and embrace the convenience of a designated space for coats, bags, and hats that echoes the Scandinavian emphasis on simplicity and functionality.\nScandinavian Interior design #6 – Textured Textiles\nElevate the comfort and aesthetics of your space with budget-friendly textured textiles that exude Scandinavian coziness. Seek out thrifted or affordable finds such as woven blankets, chunky knit pillows, or faux sheepskin throws. Drape a woven blanket over the back of your sofa to add a touch of texture and visual interest. Enhance your bedding ensemble with a collection of chunky knit pillows, and place a faux sheepskin throw on your favorite armchair for an inviting touch that’s both affordable and indulgent.\nScandinavian Interior design #7 – Multipurpose Furniture\nMaximize your budget by investing in furniture that serves multiple functions, embodying the very essence of Scandinavian practicality. Consider a wooden chest that can function as both a coffee table and discreet storage solution. This versatile piece not only complements the clean lines of Scandinavian interior design but also adds a touch of rustic charm. Its dual functionality helps you declutter while maintaining a sophisticated aesthetic.\nScandinavian Interior design #8 – Gallery of Memories\nCreate a personalized gallery wall that speaks to your memories and experiences while aligning with the Scandinavian interior design ethos. Opt for simple, minimal frames that let the content take center stage. Display a mix of your favorite photos, art prints, and pressed botanicals collected during your outdoor adventures. This gallery not only infuses your space with a touch of your personality but also celebrates the connection to nature that’s deeply rooted in Scandinavian living.\nScandinavian Interior design #9 – Repurpose with Purpose\nLet your creativity shine by repurposing wooden crates into stylish and functional shelving units that uphold the Scandinavian aesthetic. Stain or paint the crates in light hues that evoke the natural tones of Nordic landscapes. Arrange the crates in an asymmetrical configuration on your wall to showcase books, plants, and decor items. This DIY approach not only adds a unique focal point to your space but also reflects the simplicity and resourcefulness inherent in Scandinavian interior design.\nScandinavian Interior design #10 – Decluttering Zen\nEmbrace the heart of Scandinavian interior design by fostering a decluttered environment that allows your space to breathe. Keep only what’s necessary and meaningful to create a serene atmosphere. To maintain order while incorporating natural elements, use baskets made from materials like woven seagrass or rattan. These baskets serve as both practical storage solutions and decorative accents that align seamlessly with the Scandinavian ethos of simplicity and harmony.\nScandinavian interior design welcomes you to embrace the beauty of simplicity, functionality, and nature’s elegance. By integrating these budget-friendly ideas, you’re not just transforming your living space; you’re creating a sanctuary that encapsulates the essence of Nordic living. Each element, from the repurposed vintage chair to the DIY wall art, from the indoor greenery to the textured textiles, contributes to a harmonious narrative of tranquility and refinement. As you immerse yourself in these design hacks, your home evolves into an embodiment of Scandinavian grace – an environment that nurtures your well-being and fosters a deep connection with nature and purposeful design.']	['<urn:uuid:0af485ed-d9e2-4b86-8ba0-c72cca180494>', '<urn:uuid:4c686c3c-558b-40f4-92e2-f20ea8064e70>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	9	66	1905
58	What's the role of team leaders in project success, and how does poor communication affect security?	Project team leaders must provide a supportive management style suited to the project type and phase, ensuring team members work to capacity. Meanwhile, poor communication between development and security teams puts application security at risk, with 70% of security professionals reporting this divide threatens security and 77% of developers saying it affects meeting deadlines.	"['First Principles of Project Management\nBased on the criteria described in Part 1 of this paper, the following First Principles of Project Management have been articulated. These principles build extensively on the work of John Bing. All the principles presume certain assumptions about the cultural ambience in which the project takes place, which leads to our first principle.\n1. The Cultural Environment Principle\nManagement must provide an informed and supportive cultural environment that is suited to project-type work to ensure that the project delivery team are able to work to the limits of their capacity.\nThe ability of a project delivery team to produce results both effectively and efficiently is highly dependent upon the cultural environment. This cultural environment encompasses both internal and external project relations and values. Internally, the management style of the team leader must be suited to the type of project and its phase in the project life span. Externally, the management of the organization in which the project takes place must be supportive, the project sufficiently resourced, and the environment must be free of obstacles.\nThe resulting ambience is one that encourages and sustains teamwork and honesty and demonstrates that:\n- Everyone is clear on the project\'s ultimate goals and is working towards those same goals, whatever those might be.\n- Everyone is clear and agrees on who the customer is\n- Appropriate levels of skill or experience are available as needed, and\n- Everyone wants the project to be a success.\n2. The Project System Principle\nA well-managed project is a complex system in which the management process proceeds through an orderly timeframe that relies heavily on doing the right thing in the right way and at the right time.\nA well-managed project is one that is optimized for effectiveness in\nits planning phases but emphasizes efficiency in its implementation phases. Implementation\nincludes the transfer of the product to the care, custody and control of the customer.\nIn reality, the complex system referred to consists of an intricate collection\nof interacting balancing and non-balancing mental feedback processes, each with\ntheir own cause, effect and side effect patterns. This complicated, often random,\narrangement is enough to defeat many minds. In other words, ""an inability to see\nthe forest for the trees"" is a problem for many individuals. But for the project\nmanager, ""an ability to see the forest as well as the trees"" is an imperative\nfor running a successful project.\nThus, project management is dominated by high levels of decision-making activities that absorb a considerable amount of effort since decisions on one part of the system can have significant repercussions on other parts of the system. This is why establishing and maintaining a robust and up-to-date Business Case is an essential prerequisite for ""doing the right thing in the right way and at the right time"". We introduced the idea of a Business Case earlier in Part 1 of this paper under the heading Project Life Span.\n3. The Strategy Principle\nA strategy encompassing first planning then doing, in a focused set of sequential and progressive phases, must be in place.\nThe genesis of the project life span, in its most basic form, is to be found in the very term ""project management"" itself. A project has, by definition, a start and a finish with some activity in the middle. The essence of management is to ""plan before doing"". Hence the most fundamental project life span process consists of four sequential periods of Start, Plan, Do and Finish. Of course these four periods can be expanded into separate phases each with their own interim deliverables and Executive Control Points (or Gates) that can also be viewed as Emergency Exit Ramps. These can be designed to suit the control requirements of every type of project in every area of project management application and are particularly important from the perspective of project portfolio management. Indeed, this sequence is, in effect, equally applicable at every level and branch of the project structure as suggested by Figure 2 in Part 1 of this paper. It is also just as relevant where a fast-track strategy or an iterative approach is adopted.\nThe importance of this life span process and its influence on the management\nof the project cannot be over emphasized. This relatively short-term life-to-death\nenvironment, and the consequences that flow, is probably the only thing that uniquely\ndistinguishes projects from non-projects.\n4. The Success Principle\nThe measures of project success, in terms of both process and product, must be defined at the beginning of the project as a basis for project management decision-making and post-project evaluation.\nIt is axiomatic that the goal of project management is to deliver a successful product, otherwise the incurring of this management overhead is a valueless exercise. First and foremost, the project\'s proponents must define project success in terms of the acceptability of the project\'s deliverables, e.g. scope, quality, relevance to client needs, effectiveness, profitability, general benefits to the organization and so on.\nSecondly, success should be defined in terms of the project\'s internal processes, e.g. time, cost, execution efficiency, etc. The timing of the measurement of success itself may also need specifying. Moreover, the proponents must be in general agreement on the definition of these success criteria, for without agreement, it will not be possible to evaluate the success of the project overall.\nIt goes without saying that these measures of project success should be verified\nand reinforced throughout the project life span. As a corollary, if the success\nmeasures are no longer in alignment with the organization\'s business goals at\nany point, it should be perfectly acceptable to abort the project or at least\nhalt it pending re-evaluation. (See also Discussion\n- Success Principle, below.)\n5. The Commitment Principle\nAn equitable commitment between the provider of resources and the project delivery team must exist before a viable project exists.\nThe provider of resources (money, and/or goods and services, and general direction) is typically called the project\'s owner or sponsor. The project delivery team is responsible for developing appropriate tactics, plans and controls for applying the necessary skills and work to convert those resources into the required deliverables or product. An equitable commitment means that both parties are sufficiently knowledgeable of the undertaking, i.e. the overall objectives, the technology, the processes involved and their associated risks, and that both parties willingly undertake the challenge.\nThe attributes of both parties should encompass relevant skills, including those of the technology involved, experience, dedication, commitment, tenacity and authority to ensure the project\'s success. The owner of the project must understand that even with appropriate management controls in place, there must be a sharing of the risks involved. (See also Discussion - Commitment Principle below.)\n6. The Management Principle\nPolicies and procedures that are effective and efficient must be in place for the proper conduct and control of the project commitment.\nThis principle is an extension of the strategy principle. The Strategy Principle determines what is going to be done and when. The Management Principle establishes how it is going to be done and by whom. The attributes of this management control encompass the project\'s assumptions, its justification and a reference baseline in each of the core variables as a basis for progress measurement, comparison and course adjustment. The attributes of good policies and procedures encompass clear roles and responsibilities, delegation of authority, and processes for maintaining quality, time and cost, etc. as well as managing changes in the product scope and/or scope of work.\n7. The Single-Point Responsibility Principle\nA single channel of communication must exist between the project sponsor and the project manager (or otherwise the team leader) for all decisions affecting the product scope, quality, delivery date or total cost.\nThis principle is an extension of the management principle and is necessary for effective and efficient administration of the project commitment. For example, the owner of the eventual product, if represented by more than one person, must nevertheless speak with one voice through a primary representative with access to the sponsor\'s resources. Similarly, the project\'s delivery team must always have a primary representative.\nHowever, this only applies to the decisions affecting the product scope and quality and hence the project\'s overall cost and delivery. In all other respects, free and transparent communication is indispensable for the coordination of a complex set of project activities. Therefore, this principle must not in any way inhibit the proper exchange of information through the network of project communication channels that is required to integrate all aspects of the project.\n8. The Tetrad Trade-off Principle\nThe core variables of the project management process, namely: product scope, quality grade, time-to-produce and total cost-at-completion must all be mutually consistent and attainable.\nThis principle is an extension of both the Commitment Principle and the Success Principle. The core variables of product scope, quality grade, time-to-produce and total cost-at-completion collectively, often loosely referred to as scope, quality, time and cost, respectively, are measures of internal project management efficiency. If these variables prove not to be mutually consistent and attainable, the commitment is neither equitable nor are Key Success Indicators likely to be met. The interrelationships of these four separate variables are somewhat similar to a four-sided frame with flexible joints. One side can be secured and another moved, but not without affecting the remaining two.\n22. Bing, John, A. Principles of Project Management, PMNETwork, PMI, January 1994, p40\n23. For definitions of\nculture and environment in the project context, in the Wideman Comparative Glossary of Common Project Management Terms\n24. Contributed by Gerald Neal by Email dated 9/23/99.\n25. See the definition of Care, Custody and Control in the Wideman Comparative Glossary of Common Project Management Terms\n26. Section 60 Life Cycle Design and Management, CRMP Guide to the Project Management Body of Knowledge, Centre for Research in the Management of Projects, University of Manchester, 1999.', 'Address the Communication Gap Between Dev and Security Teams\nAs many organizations probably already know, development and security teams have a communication problem. Often they are communicating poorly, or not at all. This presents a big problem for any organization looking to succeed with DevSecOps and deliver secure applications.\nIf security and development teams are working independently of one another — or even worse, at odds with one another — that could lead to significant problems with regard to product security.\nA report by research firm Ponemon Institute in 2020 noted that organizations are at risk when application security and development don’t have a common vision for delivering the software capabilities the organization needs in a secure manner.\nThere must be a fundamental agreement that security is integrated throughout the application development process. As businesses push developers to build and deliver code on a continual basis and at a rapid rate, the perception of security as a hindrance emerges, the report noted.\nAs part of the research, Ponemon Institute surveyed 581 security practitioners who are involved in and knowledgeable about their organization’s application security activities, and 549 who are involved in and knowledgeable about their organization’s software application development process. Seventy-seven percent of developer respondents said the cultural divide affects their ability to meet deadlines, while 70% of the security respondents said the divide is putting the security of applications at risk.\nA large majority of the security respondents said that the state of security is undermined by developers, who don’t care about the need to secure applications early in the software development lifecycle.\nIt’s clear that the two factions do not always have the same goals for success. Developers are looking to create innovative software products quickly, leveraging automation to speed up processes as much as possible. The security of the finished products is not typically uppermost in their minds.\nThe security team, on the other hand, wants to ensure that code is secure and as devoid of vulnerabilities as possible. This can help ensure that the final software offering is safe to use, but this can also slow the pace of development.\nThese and other differences can create lots of friction, which in turn can lead to turf battles, lack of cohesiveness and even lower-quality products. Given this scenario, organizations need to make sure that the teams take steps to break down any barriers that exist and learn to understand each other better.\nFind Common Ground\nOne good practice is to find common ground between the two areas. Discovering and fixing vulnerabilities — or preventing them in the first place — should be the shared responsibility of both the security and development teams.\nAfter all, good quality software should arrive in production or on the market with as few vulnerabilities as possible. It’s in the best interest of both teams to see that it does. Once they fully realize this commonality, they need to collaborate to determine the best ways to address vulnerabilities.\nJust getting together to resolve security/development issues, in fact, can help bolster relationships. If members of the two teams meet on a regular basis, they might tend to develop greater empathy for each other and learn to be more flexible. They might come to realize that they’re working toward a common goal and seek ways to cooperate more.\nDeploying DevSecOps and leveraging DevSecOps automation can play a major role in fostering teamwork among developers and security professionals. The idea of bringing products to market not only quickly, but securely as well, should appeal to both groups.\nLeadership Must Step Up\nAnother key to success is having senior-level executive support for initiatives that bring security and development teams together. CISOs would be a natural choice to lead the efforts, given their overall responsibilities for ensuring all aspects of cybersecurity and their involvement in DevSecOps. But CIOs, COOs or other senior executives, could also lend support to such efforts.\nAs the Ponemon Institute noted, senior leadership must create an environment that encourages teamwork, collaboration and accountability. Most organizations are not actively taking steps to encourage security and development to work more effectively as a team, it said. Only 36% of security respondents and 45% of developer respondents think their organizations’ senior leadership is aware of this problem.\nThat has to change, and leaders need to grasp the importance of having security and development teams work as a cohesive, harmonious unit. With so many organizations advancing their digital transformation efforts and introducing new online services, it’s more important than ever that these two factions not only get along, but excel through effective collaboration.']"	['<urn:uuid:c53d4398-3984-40d7-b67f-e30de081759f>', '<urn:uuid:85a3b3c5-5597-49e0-9d98-7ea5fc20efac>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T06:15:13.904987	16	54	2408
59	How much did hospitals and education centers spend on building improvements and upgrades in 2018?	According to the U.S. Census Bureau, education construction exceeded $98.9 billion in 2018, while 12 hospitals and health systems announced or unveiled facility expansions and renovations with price tags of 1 billion dollars.	['Maintaining Safety and Security During Major Construction Projects\nConstruction projects in public spaces such as schools and hospitals are on the rise across the country. According to the U.S. Census Bureau, education construction is estimated to have exceeded $98.9 billion in 2018; and in that same year, 12 hospitals and health systems announced or unveiled facility expansions and renovations with price tags of 1 billion dollars.\nWhile this is great news for students, patients and others who occupy these types of public buildings in the long run, it can create difficulties in the short term for facility managers overseeing the projects, as well as those living through them. Facility managers are juggling a lot of priorities: keeping the public safe, while making sure work progresses on schedule and workers are qualified, certified and trained.\nAdding to this complexity, construction sites can be inherently complex environments, with heavy machinery and equipment, hundreds of workers on site at a time, and many activities occurring simultaneously. Given these challenges, it’s important to consider how well the contractor managing your building project can address them.\nBelow are five important questions to ask your contractor before undertaking a renovation or construction project:\nHow are you using technology on the jobsite? Technology advances can provide the visibility, transparency and oversight required to manage complex projects. From Internet of Things (IoT)-based systems, drones and advanced cameras, to productivity software, scheduling solutions and other apps, best-in-class contractors should be using advanced technology to enable a connected jobsite. While the construction industry was traditionally seen as a technology laggard compared to other industries, that perception is becoming a thing of the past and the most effective contractors are leveraging these technologies to bring added value to the jobsite.\nMany contractors are turning to technologies, such as Internet of Things (IoT) sensors and networks, to increase safety and security, reduce risk and operate more efficiently so projects can be wrapped up on a timely basis.\nAs another example, Building Information Modeling (BIM) technology, which creates three-dimensional digital models of buildings, is helping contractors design and engineer structures more accurately, while improving project efficiency.\nHow will you help me keep the public safe? Given the high level of security requirements – especially in K-12 schools, it’s important that facility managers know who is on-site at all times to ensure the safety and privacy of students and building occupants. It’s important to ask what steps contractors take to ensure this safety. They may conduct regular worker training to communicate the importance of maintaining building perimeters, or physically mark boundaries.\nIn addition, many are outfitting tradesmen and other workers with IoT-based wearable sensors, so managers can see, in real time, every worker who is present by trade (plumbing, electrical, etc.) and where they are located as they move around the jobsite. By integrating IoT, contractors can monitor worker locations, providing peace of mind in these sensitive and secure environments. Beacons placed at areas that are off limits to workers, such as student restrooms or classrooms, or patient rooms and surgical floors –can communicate with wearable IoT devices so site supervisors can see in real-time if a worker is near one of these places of interest and intercede.\nAre you in compliance with federal and local regulations? The focus on safety in construction is becoming of paramount concern as construction projects flourish across the country. For example, compliance to OSHA laws, means that contractors must ensure that workers have OSHA 30 training or face stiff fines. Facility managers should inquire about how well the contractor complies to mandated training, if they’ve ever received warnings or penalties for non-compliance and how they manage compliance. This could be through manual documentation carried on each worker, or through wearables that carry this certification information, which can be accessed as soon as workers enter the turnstile to immediately identify if a worker is supposed to be on site and has the necessary certification. Other technologies, such as advanced cameras can be used to augment worker identification.\nHow do you ensure the safety of your workers? In an occupied building, such as a school or hospital, it’s not only the public that needs to be kept safe, but workers as well. Contractors should be able to demonstrate their commitment to worker safety. They should be able to explain their Standard Operating Procedures (SOPs), or the amount of worker safety training they provide. They also can share OSHA 30 certifications. Many contractors conduct evacuation drills to ensure safe exit for workers in the event of an emergency, and others are using IoT sensors that automatically alert safety personnel to falls in real-time in the event of an incident, so they can get help to the scene faster, potentially reducing or limiting the severity of injuries. Workers also can simply push a button on these devices to send immediate notifications to supervisors about unsafe working conditions, enabling workers to be active participants in the site safety culture.\nIoT sensors also are often used on equipment, such as forklifts, that can communicate with workers’ devices to help ensure that only those who have the proper credentials are using them.\nHow quickly can you wrap up the project? Hearing that a major renovation project is nearing completion is music to a facility manager’s ears. The faster projects are wrapped up, the sooner they can get back to their day-to-day work. A contractor should be able to give him/her a realistic timeline for project completion and provide regular updates if delays occur.\nThis is also another area where technology can help contractors operate more productively to better meet project deadlines. For example, by putting sensors on equipment and tools, contractors can reduce the time wasted in tracking them down. Monitoring worker activities as well as equipment location, they also can see where projects may be progressing slower than they should because of inaccessibility of materials or time wasted tracking down equipment, for example, and make necessary adjustments. Gathering valuable data through advanced software also enables contractors to better analyze past projects to gain insights into time estimates for future projects.\nRenovations and other construction projects in schools and hospitals are almost always a boost to communities and the public they serve, yet they can be anything but peaceful for facility managers who must ensure that day-to-day operations continue with as little disruption as possible. Gaining greater peace of mind is possible when you ask your contractor the right questions to keep projects on time and on budget, and most importantly as safe as possible for the public and workers alike.']	['<urn:uuid:d16e901b-0789-4b49-883f-abd131bb8791>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	15	33	1096
60	studying beta glucan structure function relationship what are key molecular characteristics and effects on human health immunity	Beta-glucan's key molecular characteristics include β-1,4 and β-1,3 glycosidic linkages in its main chain structure, with glucose making up 99% of its monosaccharides. These structural features are crucial for its biological functions. In terms of health effects, β-glucans demonstrate significant immune-modulating properties, activating macrophages and white blood cells to enhance anti-infection activities. Their effectiveness is influenced by molecular characteristics like particle size and specific binding sites for immune receptors. Beta-glucans have been shown to have antimicrobial and antioxidant capabilities that are more potent than echinacea, with studies indicating they can help prevent infections and potentially enhance the effectiveness of antibiotics. The compound also plays a role in wound healing and overall immune system enhancement.	['Prices exclude VAT\nAvailable for shipping\n|Stability:||> 10 years under recommended storage conditions|\n|Monosaccharides (%):||Glucose = 99|\n|Main Chain Glycosidic Linkage:||β-1,4 and β-1,3|\n|Substrate For (Enzyme):||β-Glucanase/Lichenase|\nHigh purity β-Glucan (Oat; Medium Viscosity) for use in research, biochemical enzyme assays and in vitro diagnostic analysis.\nMedium viscosity β-glucan from oat flour. For the assay of β-glucanase and cellulase.\n33-β-D-Glucosyl-cellotriose P-CMC4M - Carboxymethyl Cellulose 4M P-GLCML - Glucomannan (Konjac; Low Viscosity) P-GLCMH - Glucomannan (Konjac; High Viscosity) P-XYGLN - Xyloglucan (Tamarind) P-BGLU12 - 1,2-β-Glucan\n(Bacillus subtilis) E-LICACT - Non-specific endo-1,3(4)-β-Glucanase\n(Clostridium thermocellum) E-CELAN - Cellulase (endo-1,4-β-D-glucanase)\n(Aspergillus niger) E-CELBA - Cellulase (endo-1,4-β-D-glucanase)\n(Bacillus amyloliquefaciens) E-CELTE - Cellulase (endo-1,4-β-D-glucanase)\n(Talaromyces emersonii) E-CELTH - Cellulase (endo-1,4-β-D-glucanase)\n(Thermobifida halotolerans) E-CELTR - Cellulase (endo-1,4-β-D-glucanase)\n(Trichoderma longibrachiatum) E-CELTM - Cellulase (endo-1,4-β-D-glucanase)\nGrundy, M. M., McClements, D. J., Ballance, S. & Wilde, P. J. (2018). Food Hydrocolloids, 83, 253-264.\nDepletion flocculation is a well-known instability mechanism that can occur in oil-in-water emulsions when the concentration of non-adsorbed polysaccharide exceeds a certain level. This critical flocculation concentration depends on the molecular characteristics of the polysaccharide molecules, such as their molecular weight and hydrodynamic radius. In this study, a range of analytical methods (dynamic shear rheology, optical microscopy, and static light-scattering) were used to investigate the interaction between lipid droplets and polysaccharides (guar gum and β-glucans) of varying weight-average molecular weight and hydrodynamic radius, and concentration. The aim of this work was to see if the health benefits of soluble fibers like β-glucans could be explained by their influence on the structure and digestibility of lipid emulsions. The apparent viscosity of the emulsions increased with increasing polysaccharide concentration, molecular weight, and hydrodynamic radius. Droplet flocculation was observed in the emulsions only at certain polysaccharide concentrations, which was attributed to a depletion effect. In addition, the water-soluble components in oat flakes, flour, and bran were extracted using aqueous solutions, to examine their impact on emulsion stability and properties. Then, the rate and extent of lipolysis of a sunflower oil-in-water emulsion in the presence of these oat extracts were monitored using the pH-stat method. However, the inhibition of lipolysis was not linearly related to the viscosity of the oat solutions. The water-soluble extracts of β-glucan collected from oat flakes had a significant inhibitory effect on lipolysis. The results of this study increase our understanding of the possible mechanisms influencing the impact of oat constituents on lipid digestion. This work also highlights the importance of considering the molecular properties of polysaccharides, and not just their impact on solution viscosity.Hide Abstract\nSahasrabudhe, N. M., Tian, L., van den Berg, M., Bruggeman, G., Bruininx, E., Schols, H. A., Faas, M. M. & de Vos, P. (2016). Journal of Functional Foods, 21, 104-112.\nOat β-Glucans were studied for their immunological impact before and after enzymatic digestion in order to enhance the efficacy of oat β-Glucans for application in functional foods. Oat β-Glucan is reported to have minimal impact compared to its fungal counterpart in vitro. Digestion with endo-glucanase enhanced its efficacy towards stimulating MCP-1, RANTES, IL-8, and IL-4 production in human dendritic cells as compared to the nondigested β-Glucan. This effect resulted from an enhanced activation of the Dectin-1 receptor. Our data suggest that the immune-stimulation was dependent on the β-(1-3) linkages and the reduced particle size of digested β-Glucans. Thus, we show that enzymatic pre-digestion of dietary fibres such as oat β-Glucan enhances its impact on specific immune receptors. We also demonstrate that particle size and/or molecular weight of oat β-Glucans and exposure of specific binding sites for the receptors might be important tools for designing efficacious functional feed and food additives.Hide Abstract\nLondono, D. M., Gilissen, L. J. W. J., Visser, R. G.F., Smulders, M. J. M. & Hamer, R. J. (2015). Journal of Cereal Science, 62, 1-7.\nΒ-glucan is one of the components that differentiate oats from other cereals and that contribute to the health-related value of oats. However, so far oats cannot easily be applied in bread-like products without loss of product quality. Here we have studied how the content and viscosity of oat β-glucan affect the technological properties of oat dough in both a gluten-free and a gluten-containing system. In both systems, increasing the β-glucan concentration resulted in an increase of dough stiffness and in a reduction of dough extensibility. β-glucan negatively impacted the elastic properties that additional wheat gluten conferred to oat dough. This effect was smaller for medium-viscosity β-glucan than for high-viscosity β-glucan. Interestingly, dough made from low β-glucan flour (<2%) had increased gas retention capacity. Overall, the impact of β-glucan on the properties of oat dough systems was governed by concentration and viscosity, with or without additional wheat gluten. Our findings indicate that β-glucan is a key component that determines the rheology of oat-based dough systems and, with that, the technological functionality of oat in dough systems.Hide Abstract\nVolman, J. J., Mensink, R. P., Buurman, W. A. & Plat, J. (2011). Scandinavian Journal of Gastroenterology, 46(5), 603-610.\nObjective. Antimicrobial peptides and tight junction proteins are crucial to maintain mucosal immunity. It is known that oat β-glucan may affect intestinal immunity. Therefore, the aim of the present study was to evaluate the effect of oat β-glucan on the presence of antimicrobial peptides and tight junction protein. Material and methods. We analyzed antimicrobial peptide levels in fecal water prepared from 24 h ileostomic bag contents obtained from ileostomic patients consuming oat β-glucan enriched or control diets in a cross-over design. In addition, intestinal sections of mice, which received oat β-glucan via oral gavages for 3.5 days, were analyzed for lysozyme and zonula occludens-1 expression. Results. We observed a trend toward lower lysozyme (−23%; p = 0.076) and bactericidal/permeability-increasing protein (−17%; p = 0.098) levels in oat β-glucan enriched fecal water as compared with placebo. Additionally, mice receiving oat β-glucan showed a lower lysozyme expression in stained distal small intestinal sections (p = 0.011). Staining of zonula occludens-1 was decreased in β-glucan treated mice indicating disruption of the tight junction integrity. Conclusions. In conclusion, the consumption of oat β-glucan seems to decrease the levels of antimicrobial peptides in fecal water from human ileostomy patients and its expression in distal small intestine sections in mice. The decreased intestinal integrity in mice could be explained by the drop in antimicrobial peptides.Hide Abstract\nLin, B., Gong, J., Wang, Q., Cui, S., Yu, H. & Huang, B. (2011). Food Hydrocolloids, 25(2), 180-188.\nDietary fibers (oat β-glucan, flaxseed gum, and fenugreek gum) can be fermented to short chain fatty acids (SCFAs) in batch cultures by pig intestinal digesta. Lactate in the β-glucan-grown cultures had the highest level among SCFAs produced, whereas flaxseed or fenugreek gum-containing cultures generated a significant amount of acetate, propionate and butyrate over three subcultures tested (p< 0.05). Denaturing gradient gel electrophoresis (DGGE) analysis of 16S rRNA genes in the initial and subcultures revealed that types of fibers impacted pronouncedly on the bacterial community structure. Identified DGGE DNA bands were mainly affiliated with Bacteroidetes, and Firmicutes, in which numbers of butyrate-producing species were widely distributed. The fermentability of dietary fibers to SCFAs and their selection towards the bacterial communities suggest their potential application in promoting animal/human health.Hide Abstract', 'Your ability to interact with the world around you and remain healthy is dependent to a large extent on the healthy functioning of your immune system. Your immune system is responsible for fighting foreign invaders to your body, like pathogenic bacteria and viruses, and also for destroying cells within your body when they become cancerous. Poor nutrition has been shown to result in increased infections, to slow healing from injury and infections, and to increase susceptibility to symptoms and complications from immune system dysfunction. Science has shown that immune function often decreases as we age, and recent research suggests this decrease is also related to nutrition and may be slowed or even stopped by maintaining healthy nutrition.\nOne of the most important steps you can take to maintain good health is to eat a healthy diet loaded with foods that boost immune system.\nFoods That Boost Immune System\n1. Coconuts and Coconut Oil\nBesides being excellent for your thyroid and your metabolism, coconut oil is rich in lauric acid, which converts in your body to monolaurin. Monolaurin is the actual compound found in breast milk that strengthens a baby’s immunity.\nA great deal of research has been done establishing the ability of lauric acid to enhance immunity. This medium-chain fatty acid (MCFA) actually disrupts the lipid membranes of offending organisms\nWhen selecting coconuts and coconut oil, make sure you choose organic ones that are unrefined, unbleached, made without heat processing or chemicals, and are non-GMO.\nProbiotics, or the “live active cultures” found in yogurt, are healthy bacteria that keep the gut and intestinal tract free of disease-causing germs. Although they’re available in supplement form, a study from the University of Vienna in Austria found that a daily 7-ounce dose of yogurt was just as effective in boosting immunity as popping pills. In an 80-day Swedish study of 181 factory employees, those who drank a daily supplement of Lactobacillus reuteri—a specific probiotic that appears to stimulate white blood cells—took 33 percent fewer sick days than those given a placebo.\nMushrooms strengthen your immune system because they are rich in protein, fiber, vitamin C, B vitamins, calcium, and other minerals.\nMushrooms make up a large part of the fabric of the earth. According to Paul Stamets, one of the world’s leading authorities on the health benefits of mushrooms, one cubic inch of soil can contain eight miles of mycelium.\nMushrooms also contain powerful compounds called beta-glucans, which have been long known for their immune enhancing properties. The beta-glucans in medicinal mushrooms (especially Reishi, Shiitake, and Maitake) are notable for their ability to activate/modulate your immune system.\nBeta-glucan enhances immunity through a variety of mechanisms, many of which are similar to those of echinacea or astragalus root. For example, it binds to macrophages and other scavenger white blood cells, activating their anti-infection activities. Numerous studies support this.\n4. Oats and Barley\nThese grains contain beta-glucan, a type of fiber with antimicrobial and antioxidant capabilities more potent than echinacea, reports a Norwegian study. When animals eat this compound, they’re less likely to contract influenza, herpes, even anthrax; in humans, it boosts immunity, speeds wound healing, and may help antibiotics work better.\nThis potent onion relative is one of the foods that boost immune system, and contains the active ingredient allicin, which fights infection and bacteria. British researchers gave 146 people either a placebo or a garlic extract for 12 weeks; the garlic takers were two-thirds less likely to catch a cold. Other studies suggest that garlic lovers who chow more than six cloves a week have a 30 percent lower rate of colorectal cancer and a 50 percent lower rate of stomach cancer.\n6. Sweet Potatoes\nYou may not think of skin as part of your immune system. But this crucial organ, covering an impressive 16 square feet, serves as a first-line fortress against bacteria, viruses, and other undesirables. To stay strong and healthy, your skin needs vitamin A. “Vitamin A plays a major role in the production of connective tissue, a key component of skin,” explains Prevention advisor David Katz, MD, director of the Yale-Griffin Prevention Research Center in Derby, CT. One of the best ways to get vitamin A into your diet is from foods containing beta-carotene (like sweet potatoes), which your body turns into vitamin A.\nIt’s not clear how the root of the ginseng plant works, but claims on behalf of Asian ginseng are many, including its ability to stimulate immune function. Despite the claims of a number of mainly small studies, the National Center for Complementary and Alternative Medicine (NCCAM) considers there have been insufficient large studies of a high enough quality to support the claims. NCCAM is currently supporting research to understand Asian ginseng more fully.\nBroccoli is super-charged with an arsenal of vitamins and minerals ready to do battle with any germ or infection. Packed with vitamins A, C, and E, as well as numerous antioxidants, broccoli is one of the healthiest vegetables you can put on your table. The key to keeping its power intact is to cook it as little as possible, or better yet, not at all.\nZinc deficiency is one of the most common nutritional shortfalls among American adults, especially for vegetarians and those who’ve cut back on beef, a prime source of this immunity-bolstering mineral. And that’s unfortunate, because even mild zinc deficiency can increase your risk of infection. Zinc in your diet is very important for the development of white blood cells, the intrepid immune system cells that recognize and destroy invading bacteria, viruses, and assorted other bad guys, says William Boisvert, PhD, an expert in nutrition and immunity at The Scripps Research Institute in La Jolla, CA.\nMost people turn to vitamin C after they’ve caught a cold. But did you know it tops the charts of foods that keep your immune system running at 100%? Vitamin C helps increase the production of white blood cells, key to fighting infections. Because your body doesn’t produce or store it, daily intake of vitamin C is essential for continued health. Almost all citrus fruits are high in vitamin C and with such a variety to choose from it’s easy to add a squeeze of C to any meal.\n11. Chicken Soup\nIt turns out there is something to chicken soup after all. In one study, hot chicken soup was more effective than hot or cold water at making noses run—a good thing since nasal secretions help rid the body of pathogenic viruses and bacteria. Like any hot liquid, soup also helps you to stay hydrated and raises the temperature of the airways, both of which are important for loosening secretions. Adding a few hot chiles to this Chicken Noodle Soup with Dill recipe might help loosen things up even more.\n12. Licorice Root\nLicorice root is used in Chinese medicine to treat a variety of illnesses. Most studies of licorice root have been done in combination with other herbs, so it’s not possible to verify whether any effects were attributable to licorice root per se. Because of the potential side effects of taking licorice and how little is known about its benefits — if any — for stimulating immune function, this is an herb to avoid.\n13. Green Tea\nPolyphenols, potent plant antioxidants, are what’s believed to give green tea its immune-boosting effects. One laboratory study suggested that a particular type of polyphenols called catechins may kill influenza viruses. To maximize benefits and minimize bitterness, use just-below-boiling water and steep green tea no more than a minute or two. A little lemon and honey can also help blunt the bitterness. But don’t add milk, because the proteins will bind to the polyphenols, making them ineffective.\n14. Red Bell Peppers\nVitamin C not only boosts immune systems but it helps maintain healthy skin, which is the frontline in our war on colds. Think citrus fruits have the most vitamin C of any fruit or vegetable? Think again. Ounce for ounce, red bell peppers have twice as much flu-fighting vitamin C, as well as being a rich source of beta carotenes.\nA key ingredient in many curries, this bright-yellow, bitter spice has been used for years as an anti-inflammatory in treating both osteoarthritis and rheumatoid arthritis. High concentrations of curcumin, which gives turmeric its distinct color, have been shown to contain strong flu and cold fighting properties.\nCarrots are an excellent source of beta-carotene, which gives them their bright color. One of beta-carotene’s jobs is to support the body’s mucus membrane, which lines the respiratory and intestinal tracts, making it harder for bacteria to enter the bloodstream and cause trouble. For a healthy diet, enjoy raw carrots along with cauliflower, broccoli, and other veggies as an appetizer. You can also pickle them or steam and puree them with some broth for a rich soup — a soothing meal when you’re sick.\nGinger is another ingredient many turn to after they’ve caught a cold. But like vitamin C, ginger can also help you prevent that cold from taking hold in the first place. While it’s used in many sweet desserts, ginger packs some heat in the form of gingerol, a relative of capsaicin, which gives chili peppers their distinctive heat. Ginger may help decrease chronic pain and may possess cholesterol-lowering properties.\nCinnamon is an antiviral, antifungal, and antibacterial machine, so it does more than just boost the immune system — it actually fights the pathogens that cause illness. Cinnamon is extremely versatile, making it easy to add to your healthy diet. You can mix it into your coffee grinds for an immunity-fighting brew, add it to tea, sprinkle it on oatmeal, stir it into hot chocolate, or dust it onto fresh fruit.\nWhen it comes to preventing and fighting off colds, vitamin E tends to take a backseat to the more commonly mentioned vitamin C, but vitamin E is also key to a healthy immune system. Vitamin E is a fat-soluble vitamin, meaning it requires the presence of fat to be absorbed properly. Almonds are packed with vitamin E and a half cup serving provides nearly 100% of the daily recommended amount.\nGreens such as kale, spinach, and Swiss chard are immune-boosting foods that contain high levels of vitamin C, which not only packs a powerful antioxidant punch, it helps fight off infection and regenerate other antioxidants in the body, including vitamin E. They also contain folate, another immune booster. Sautée kale, spinach, or Swiss chard with garlic and olive oil, or use fresh spinach to make an nutrient-rich salad — top it with fresh mushrooms to pack an extra immune-boosting punch.\nWatermelon is one of the foods that boost immune system. Once considered only a summertime treat, this immunity boosting food may also help ward off winter’s ills. Like other fruits, watermelon is packed with vitamins, minerals, and other nutrients, but what makes it a super food is its large concentration of lycopene. Lycopene gives watermelon flesh its red color and offers an array of health-building benefits, including reducing respiratory inflammation, which helps prevent infection. If possible, make watermelon part of your healthy diet year round.\nSpinach made our list not just because it’s rich in vitamin C but because it’s packed with numerous antioxidants and beta-carotene, which have been proven to increase the infection-fighting cells of our immune systems. Similar to broccoli, it is one of the foods that boost immune system and best cooked as little as possible so that its nutrients are retained.\n23. Wheat Germ\nWheat germ is a key source of fiber — a dietary essential that keeps your digestive system on track. It’s also packed with protein and vitamins, including vitamin E, and is a great source of choline and zinc, which also help boost immunity. Add wheat germ to your bread, muffin, and cookie recipes, dust it on fish instead of bread crumbs, or sprinkle it over yogurt and fresh fruit or your breakfast cereal.']	['<urn:uuid:4756154d-f923-4989-97cd-ec0312d00ba6>', '<urn:uuid:62d8223c-8571-4aa6-b062-7706376140a0>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:15:13.904987	17	114	3173
61	How long did Louis Vuitton reduce handbag production time to?	Louis Vuitton reduced the production time of a handbag from eight days to one day by clustering craftspeople in cells instead of assembly lines.	['Reset Fashion: from Dead Stock to Mass Customisation\nThe Stock Inefficiency\nThe cost of inventory that cannot be sold is no longer the largest inefficiency of the fashion industry alone. It is a phenomenon that contributes to social and environmental crises, with immense pressure for low cost production and inappropriate disposal of unsold items.\nIn an era where consumers are constantly being exposed to and influenced by new trends, not just by their peers, but by the entire world through a single click, brands are expected to surprise every other week. This new reality does not resonate with the traditional structure of the apparel industry, which focuses on a “push”-based supply chain: producing inventory in accordance with historical consumption patterns, pushing products on the customer through marketing, and offering discounts if necessary.\nBy working on a collection for months in advance, sometimes even for longer than one year, fashion brands aim for high volume with low unit costs. Some retailers knowingly overproduce to achieve the lowest cost-per-piece, hoping they will be able to sell it down the road. However, this framework can result in a vicious cycle of end-of-season markdowns, driving many companies into debt or even into closing operations.\nThe high-volume low-cost production most retailers work for may just be mirage. At the beginning of the sales cycle, they may be enthusiastic about the 60 to 70 percent margins. However, after all the markdowns and push activities, eroded profit margins end up at 1 to 2 percent. In the US alone, dead inventory costs the retail industry as much as $50 billion a year, without even taking into consideration the costs associated with holding inventory in the first place.\nOn this subject, the fashion industry seems to be falling behind, maintaining the same operational principles for decades. For years, the inventory efficiency of Zara has been the odd one out, responding to customer demand signals as close to real-time as possible, delivering new products every two weeks. The call for more agile supply chains is evident, the hard part is knowing where to start.\nCreating Valuable Supply Chains\nThe first step to moving away from pushing product into the market, hoping it will sell, into having the customer pulling the product from the brand, is to gather information from the market. More than ever, technology now allows us to have close and frequent contact with potential customers, gathering insights on what their preferences are. Here is where the powerful insights lay, letting brands know exactly what will sell, instead of guessing. With the use of augmented reality, customers can even try on virtual garments even before a sample is created. Just recently, Zalando has acquired a software company whose body-scanning app includes a virtual dressing room to help buyers easily judge whether a garment will be the right size for them. These solutions can drastically decrease product development time and speed up time-to-market, contributing to a more accurate bet on the products that the client will buy. Moreover, customers are willing to accept the wait times that come with on-demand production, so we find ourselves in a win-win scenario.\nWith the right information in mind, and with the confidence of having already tested the market, design and development teams should organise themselves in design cells. This work methodology creates multidisciplinary teams that work together in short design-test sprints, delivering finished products in record times. This is one of the secrets for the 15-day Lead Time of Inditex’s development teams. Reducing development time by more than 90% will give brands a significant competitive advantage in the market.\nHaving improved customer interactions and sped up the development process, it is now time to focus on production and delivery. The traditional strategy to search for low cost production facilities can take brands into distant geographies, resulting in increased transportation costs, and reduced flexibility when it comes to batch sizing and short-notice productions. With that said, when moving towards a pull system, sourcing localism takes on an important role. Not only will retailers save in shipping and duty fees, but they will also reduce turnaround time.\nIt is important to notice that suppliers need to become partners, as balancing risk and rewards is key to unlock the full potential of the supply chain. Developing suppliers so that they can deliver a better service will be a winning deal for both parties. Just-in-time production will be possible if manufacturers are able to create flow in their operations, allowing for capacity and flexibility of skills. Following this trend, Louis Vuitton was able to shrink the production time of a handbag from eight days to one, by clustering craftspeople in cells instead of assembly lines. This allows for faster response to customer demand and replenishment based on consumption.\nAnother important aspect is the timing for product differentiation. Postponing this differentiation can help speed up turnaround time without the risk of finished product stocking. By purchasing raw fabric and only differentiating it when production is scheduled, Inditex saves weeks in development and reduces trapped capital.\nSuppliers should become long term partners, growing together with their clients. As a response to the recent crisis, some companies managed to reduce stock-in-trade by almost 20 percent at the end of July 2020, by taking advantage of flexible purchasing agreements that allow the company to rapidly adapt to changes in demand. This is an example of how these partnerships can help in managing uncertainty.\nIt is clear that punctual and silo improvements are not going to transform the fashion industry. There needs to be a change movement in the total length of the supply chain that allows for the creation of an efficient ecosystem that delivers more value to all stakeholders, from suppliers, to retailers and customers.\nThe supply chain transformation requires moving the focus away from the cost-per-unit out of the factory, and into a collaborative structure that considers every step of the process, taking the most out of each transaction. Breaking down all the components of the finished product and its operations will be key to finding the most flexible and valuable solution. This is not only a process transformation journey, but also a cultural one, which is only possible with clear goals, strong leadership, engagement, and alignment of all stakeholders.\nBy witnessing and driving success stories with a 40% decrease in total inventory coverage, 60% decrease in turnaround time and 3 percentage points increase in market share, we are led to believe in a made-to-order business model, focussing on the elimination of waste and on sustainability, guaranteeing the success of the fashion ecosystem.\nIn an uncertain economy, with trends and technology changing at lightning speed, those who are able to adapt quickly will certainly be the ones to succeed. Agility and flexibility has never been more urgent for fashion industry players. The need is clear, the next step is to start the change engine.\nGet all the latest news about Kaizen Institute. Subscribe now.\n* required fields']	['<urn:uuid:972282f8-ffd3-4fef-98cb-ddba76024474>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	10	24	1159
62	biological mechanisms comparison male female vision differences chromosomes	Males, having one X-chromosome, will develop retinal conditions if they have a mutated gene on that chromosome. In contrast, females have two X-chromosomes, which creates two different phenomena: they can either become tetrachromats with enhanced color vision seeing up to 100 times more colors than normal, or in cases of X-linked retinal diseases, they may experience varying degrees of symptoms depending on how their cells express the two X-chromosomes.	"['Tetrachromats see the world with more colors, sort of like an impressionist painting. (Photo: AdrianHillman/iStock)\nHuman color vision is based on the structure of our eyes. We have three cones, one each for red, green, and blue — which is why human beings are designated as trichromats. We see more colors than most other mammals (which only have two color receptors), but birds see better than we do. ""Pigeons have five color receptors (and five types of cell receptors) and can process visual information up to 10 times faster than human beings,"" according to Softpedia. And certain types of spiders can see more than we do too, with four receptors. They\'re called tetrachromats. Some people might be tetrachromats, too. That is, some women.\n""In an odd twist of fate, the same genetic glitch that creates color-blind males may create females with better-than-usual color vision,"" writes Cynthia Wood in A Life More Colorful.\nWhat does it mean to have this extra ability to see color? Well, most normal-seeing humans see around 1 million different colors. Tetrachromats might be able to discern 100 million. Translated into a real-world situation, that means that while a non-tetrachromat can see seven colors in a rainbow, someone with extra ability can see hundreds.\nBeginning in the 1990s, researchers have been looking into this phenomenon (the first tetrachromat woman was documented at Oxford University in 1993). Studies since then have found that as few as 1 percent and as great as 50 percent of women might be tetrachromats. The greater number takes into account women who have at least more ability to see colors than is typical. That\'s because there can be some variation in how the fourth cone works within the eye\'s physical structure (which varies from person to person). The tests for determining which people might be tetrachromats are imperfect and the body of knowledge is evolving, but it\'s an idea that\'s no longer disputed.\nAccording to a definitive 2001 paper by a group of three female doctors then at UC San Diego, ""Women with four-photopigment genotypes are found to perceive significantly more chromatic appearances in comparison with either male or female trichromat controls."" The doctors Jameson, Highnote and Wasserman also note that ""...gender-based differences in color perception might be a greatly understudied aspect of color and cognition research.""\nIn other words, this phenomenon may have only recently been uncovered because previously, male subjects have been considered the basis for most science about human beings (and in fact this is an ongoing problem in health care research).\nBut back to the fascinating world of tetrachromats. What is it like to be one?\nConcetta Antico, who is — no surprise — a talented impressionist painter, told Popular Science that when she looks at a leaf, “Around the edge I’ll see orange or red or purple in the shadow; you might see dark green but I’ll see violet, turquoise, blue. It’s like a mosaic of color.”\nAntico, who calls herself ""the world\'s first tetrachromat painter,"" has been attuned to colors since she was a young girl. (The tiger painting at right is one of her works.) She first started painting at age 7. So her particular ability is tied to both her biology, and her neurology. Because she has been ""fascinated"" with color since she was a kid, Antico has developed strong neural pathways to see those extra colors, a great example of how neuroplasticity works. Other women, who may have the physical component of tetrachromacy, but have never recognized it, may not be as able to see as many colors, since they have not ""practiced"" seeing them. It\'s like when you happen to have been born with any other genetic advantage, but never utilize it. Being tall, for example, doesn\'t mean you are good at basketball. It just means you might be if you practiced it.\nResearcher Jameson, along with colleagues at the Institute for Mathematical Behavioral Sciences at the University of California in Irvine, has been working with Antico. And she\'s looking for more tetrachromats to test. Jameson says, “If we understand genetic potential for tetrachromacy and how their perception differs, we can understand quite a lot about visual processing of color that we currently don’t understand.""\nYou can check out the Human Tetrachromacy Research Collaborative to learn more about the genetic details, impacts on vision and myths of tetrachomancy.\nRelated on MNN:\n- Graphene contact lenses could give you predator vision\n- Watch these once-blind sisters see each other for the first time\n- Vision evolved 700 million years ago, researchers find\nInset photo of what water may look like to a tetrachromat: Alexander/flickr', 'Understanding female IRD carriers\nResearch into X-linked inherited retinal diseases often focuses on males, but a study at CERA is aiming to make sure everyone benefits from newly developed treatments.\nInherited retinal diseases (IRD), a broad group of genetic eye conditions caused by changes in a person’s genetic code, do not always affect men and women in the same way.\n“A majority of female carriers of X-linked IRDs have a near-normal retina, while at the other end of the spectrum is quite severe disease, similar to what males with these conditions experience,” says Sena Gocuk, optometrist, CERA researcher and PhD candidate at the University of Melbourne.\nThe condition of many female carriers can often be so mild that it is only identified after a male relative is diagnosed with an inherited disorder.\nThis lesser severity has led past research to focus only on males with these diseases, but a new study at CERA is seeking help change that.\nChromosomes, the thread-like structures located inside the nucleus of living cells, contain the instructions for how those cells should operate.\nThey also determine a person’s biological sex; females have two X-chromosomes and males have one X-chromosome and one Y-chromosome.\nFor IRDs caused by changes in the DNA code of eye genes found on the X-chromosome, the sexes can be affected in very different ways.\n“Conditions like choroideremia, and some forms of retinitis pigmentosa, are X-linked conditions, meaning they are caused by faults in the X-chromosome,” says Gocuk.\n“For males, who only have one X-chromosome, a single copy of the mutated gene means that they will develop the retinal condition.”\nFor females, who have two X-chromosomes, an X-linked retinal condition may present quite differently – with some women having no symptoms, and others noting changes in their vision.\n“If a female has one mutated and one normal gene, individual cells decide which of the two X-chromosomes are expressed,” says Gocuk.\n“Because of that, you get a wide spectrum of disease severities.”\nSurvey and study\nThe study involves clinical visits, where Gocuk will learn more about the eye health of female carriers of these X-linked retinal diseases.\nParticipants will receive genetic testing and counselling, undergo a comprehensive eye examination to assess their retinae, and also have the opportunity to continue in the study to monitor how their vision changes over time.\nIn another part of her study, Gocuk is asking mothers, sisters and daughters of males with X-linked IRDs to complete an online survey, in order to develop a better understanding of how the condition affects women, and also their opinion of whether future gene therapies might potentially benefit them.\n“Questionnaires that have looked at female carriers of other genetic conditions have found a lot of women experience shock, guilt and anxiety when their male relatives are diagnosed,” says Gocuk.\n“One of my participants became quite teary when speaking with me about her experiences, because she has a number of male relatives with retinitis.\n“She knows that this research might not benefit her or her children, but she wants to contribute.\n“It’s been quite rewarding to hear these stories.”\nGocuk is a recipient of the University of Melbourne’s Harold Mitchell Postgraduate Travelling Fellowship, and will use the funds to travel to the University of Oxford to examine 10-year data from a UK longitudinal study as part of the project.\nHer work is also running alongside another ongoing study at CERA, surveying people who are living with an IRD.\nGocuk says she hopes her research might help lay the groundwork to see whether more people can benefit from emerging genetic treatments.\n“I would be pleased if we were able to determine if some of these women would be eligible for emerging treatments, like gene therapy.\n“It would be quite rewarding, and potentially life changing for this group of people.”\nIf you think you’re a female carrier of an X-linked inherited retinal disease or would like further information, please email Sena Gocuk: email@example.com.']"	['<urn:uuid:c236c421-54ab-4af8-a10e-ff5e7abea8b9>', '<urn:uuid:10ff4be0-4275-4b2e-832c-f7836c13208b>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	8	69	1427
63	How is chocolate production linked to Amazon deforestation?	Chocolate production is linked to Amazon deforestation through the dairy supply chain. Companies like Cadbury use milk from farms that feed their cattle with soya from Cargill, a company linked to deforestation in Brazil. The chocolate manufacturing process requires milk powder or milk crumb to produce milk chocolate, and this dairy production contributes to Amazon deforestation, where areas are being cleared at the rate of one football-field every minute.	['Lesson—How Chocolate is Made\nA short overview of how chocolate is made from bean to bar. Each step in the process is crucial to entice the best flavor from the bean.\nChocolate processing practices haven’t changed much from the time of the Maya; it is just that the equipment and processes have been refined.\nThese processes can be divided into two phases:\nIn the Cacao Field\nHarvest and Fermentation\nCacao trees produce buds on a continuous basis—this can be year-round in subtropical areas, such as Central America or it can be tied to the rain cycle, as it is in Africa. Fluctuations in growth cycle and harvest can occur because of changes in climate conditions.\nHarvesting the cacao pods is still done by hand, as the mature pods need to be identified and cut from the tree, without damaging flower buds, immature pods or the stem area from which the bud grows.\nThe pods are carefully broken open to release the cacao beans, which are embedded in a moist, fibrous, white pulp. The beans and pulp are scooped out quickly and either heaped in a pile on mats or banana leaves and covered, or put into a bin or box with a lid.\nFermentation occurs when the pulp surrounding the cacao bean is converted into alcohol by the yeasts present in the air and the heat generated by the pile or box. The beans are mixed gently during this process to introduce oxygen into the pile or box, which turns the alcohol into lactic and acetic acid. Slits or holes in the box allow the resulting liquid with its alcohol content to slowly leak out of the pile of beans during the fermentation process, leaving just the beans.\nGermination in the cacao bean is killed by the high temperatures produced during the fermentation process. The beans gather moisture from the environment and plump. Their flavor begins to change from mainly bitter to the beginnings of the complex flavor called chocolate.\nThe fermentation process can take up to eight days, depending on the species of cacao bean and the climate temperature of the growing region. Better fermentation results in better flavor and requires less roasting time to bring out that flavor.\nDrying and Storage\nThe cocoa beans, as they are now called after fermentation, come out of this process with a high moisture content. In order to be shipped or stored, they must be dried.\nThe drying process differs, depending on the climate or size of the plantation. Cocoa beans can be dried out in the sun on trays or mats where the climate permits. Sun-drying usually happens in smaller plantations in drier environments. In tropical areas, where daily rainfall is the norm, the beans can be dried in sheds, as long as there is enough air circulating around the flats of beans. The use of wood fire to speed the drying process is disliked by bulk chocolate manufacturers and chocolatiers, as the process leaves the beans with a smoky taste.\nOnce the moisture percentage in the cocoa beans has reached 6 to 7 percent, they are sorted and bagged. The sorting process is very important because the cocoa beans are classified and sold in the industry by their size and quality. The bagged cocoa beans are then loaded on ships to be delivered to chocolate manufacturers.\nIn the Manufacturing Plant\nEach chocolate manufacturer has a closely guarded “secret recipe” for each chocolate product that it produces. This secret begins with the type and quality of the cocoa beans used. I will use the symbol: § to indicate points in the process where the manufacturer may use their own secret formula.\nTesting, Cleaning, and Roasting\nWhen the selected § cocoa beans arrive at the manufacturing plant they go through a very extensive sampling and testing procedure. Sample cocoa beans are tested for size and defects, such as insects or mold, and then ground into unsweetened chocolate, which is evaluated for flavor and aroma by company tasters. Once the testing is complete and the shipment is accepted by the manufacturer, the beans are thoroughly cleaned to remove any foreign matter. The cocoa beans then go into the roaster for anywhere from § 10 to 35 minutes.\nCracking (or Fanning) and Grinding\nWhile roasting, the shell of the cocoa bean separates from the bean kernel and is removed in the first step of the cracking or fanning process. The beans are cracked (not crushed) by being passed through serrated cones. The cracked beans are now called cocoa nibs.\nAs the shell is dry and lightweight, it can be winnowed from the cocoa nib. Winnowing is done by exposure to a current of air, so that the shells are blown free of the heavier nibs. The shells can be recycled for mulch and other non-edible products. The nibs contain approximately 50 percent cocoa butter, depending on the cacao species.\nGrinding or Refining\nThe first grind of the beans is usually done in a milling or grinding machine such as a melangeur. The nibs are ground or crushed to liquefy the cocoa butter and produce what is now called chocolate liquor or chocolate liquid.\nFor the second refining process, most chocolate manufacturers use a roll refiner or ball mill, which has two functions: to further reduce the particle size of the cocoa mass (and any other ingredients, such as sugar or milk powder) and to distribute the cocoa butter evenly throughout the mass, coating all the particles.\nThe rolling process itself creates heat that melts and distributes the cocoa butter. As well as the flavor of the chocolate, manufacturers must decide on the § particle size for each of their chocolates. This is the first step to developing chocolate’s smooth and creamy mouth-feel.\nDifferent percentages of § cocoa butter are removed or added to the chocolate liquor. Cocoa butter carries the flavor of the chocolate and produces a cooling effect on your tongue that you might notice when eating dark chocolate. Also, depending on the chocolate flavor desired, some or all of the following § ingredients are added: sugar, lecithin, milk or cream powder or milk crumb (used to produce a caramel-like taste in milk chocolate), and spices such as vanilla. The formula the chocolate manufacturer develops for combining specific ingredients with the chocolate liquor is what gives the chocolate its unique taste.\nThis process develops the flavor of the chocolate, releasing some of the inherent bitterness and gives the resulting chocolate its smooth, melt-in-your-mouth quality. The conch machine has rollers or paddles that continuously knead the chocolate over a § period of hours or days depending on the flavor and texture desired by the manufacturer.\nTempering and Forming Chocolate\nFor the last two steps in the chocolate process, the conched chocolate mass is tempered and molded into bulk bars (or wafers) or it may go into another production cycle to produce specialized retail products, such as coated-candy centers and molded items.\nIf you enjoyed this lesson, you might also like:↑\nGet reminders about upcoming classes! To receive program updates and news, click the link below.', 'Animal agriculture ignored at COP26\nVegans say they are disappointed at the lack of discussion on animal agriculture at COP26. While there was an agreement to cut methane emissions by 30% by 2030, the damage caused by the animal agriculture sector has largely been ignored, with discussions on the impact of the food system representing only 0.1% of all discussions at the summit. Read our full article on the outcome of COP26 here.\nEU passes resolution to end all animal experiments\nIn September the EU Parliament voted in favour of phasing out the use of animals in research, testing and education. The vote was nearly unanimous, with 667 in favour of the resolution and only 4 against.\n“The resolution we voted on today aims to accelerate the shift in mentalities, regulation and funding,” said MEP Tilly Metz, referring to a move towards alternative testing methods including computer modeling, in vitro cell models, bioprinted tissues, etc. In some cases, these models are more accurate than animal tests, as 90% of drugs tested on animals fail in human trials.\n87% of farming subsidies harmful to nature and health, says UN\nA report released in September by UNEP in collaboration with the UN Food and Agriculture Organisation has said that almost 90% of the $540bn in global subsidies given to farmers every year are “harmful”.\nThe report, entitled ‘A Multi-Billion Dollar Opportunity: Repurposing agricultural support to transform food systems’, says that agriculture susbsidies are damaging human health, fuelling the climate crisis, destroying nature and excluding small farmers.\nBeef and milk receive the largest subsidies and are often produced by large corporations. But beef and milk are also two of the biggest sources of greenhouse gas emissions.\nThe report says that support for the meat and dairy industry in rich countries must be reduced, and that subsidies should be used to support healthy diets, vegetable and fruit production, small farmers and the environment. This repurposing of subsidies could be a “game changer”, the report says, helping to reduce global warming, restore nature, improve nutrition and end poverty and hunger.\nEU Citizens Initiative ‘Save Bees and Farmers’ succeeds\nAn EU Citizens’ Initiative calling for an end to the use of pesticides has collected 1.2 million signatures. The Save Bees and Farmers initiative is asking for a reduction of 80% in the use of synthetic pesticides by 2030, and 100% by 2035, as well as measures to restore biodiversity on agricultural land, and massive supports to farmers to transition towards agroecology.\nThe collection of signatures has ended and the organisers are confident that they have enough to meet the EU criteria. They have handed over the signatures to the national authorities to be validated – a process that takes several months. Once completed, the EU Commission will have to answer to the petition’s demands. There will be a hearing and the proposal will be on the agenda in the European Parliament.\nSurvey indicates significant shift towards plant-based diets in Europe\nA pan-European survey, by ProVeg International in partnership with Innova Market Insights, the University of Copenhagen, and Ghent University as part of the Smart Protein project, has been published. The results indicate that a significant shift towards plant-based eating is taking place across the continent. A report was also created providing country-specific findings.\nJasmijn de Boo, Vice President at ProVeg International said: “The survey suggests tremendous potential for plant-based foods in Europe and gives a green light to all relevant players in the field to develop more and better products. Consumer demand for alternative proteins is growing at a remarkable rate, with no end in sight”.\nTesco cuts price of vegan products\nUK supermarket Tesco has announced it will lower the price of many of its vegan products in the Plant Chef range. A campaign to advertise the price reduction was timed to coincide with COP26 in an effort to encourage the public to eat more sustainably.\nStudies have shown that price is one the barriers to eating plant-based foods, although it is clear that more and more people are cutting their meat consumption.\nTesco is the UK’s biggest supermarket and was named recently as the number one supermarket for vegan food\nBanks must stop financing factory farming in order to combat climate crisis\nNearly 100 environmentalists, including conservationist Jane Goodall, are calling on banks and the International Monetary Fund to stop investing in factory farming to cut the risk of future pandemics.\nIn a letter to 22 leading financial institutions – currently channeling billions into multinational meat corporations – the environmentalists warn that industrial livestock production increases the potential for further disease outbreaks and also contributes to global warming, antibiotic resistance, environmental degradation and health problems as well as undermining small farmers in the developing world.\nBritish chocolate and cheese linked to massive deforestation in Brazil\nAn investigation by Greenpeace, ITV News and the Bureau of Investigative Journalism has linked deforestation in the Amazon to well known UK products including chocolate, cheese and butter. UK farms which supply milk and dairy products to Cathedral City cheddar, Anchor butter and Cadbury chocolate are feeding their cattle soya from Cargill – a company linked to widespread deforestation in Brazil.\n90% of the soya grown worldwide is fed to cattle, and vast areas of the Amazon forest are being razed at the rate of one football-field every minute in order to clear pasture for cattle grazing.\nBritish MPs and campaigners say the investigation’s findings reinforce the need for tough new laws to force firms to tackle deforestation in their supply chains.\nRestoring whales to pre-hunted numbers could sequester 1.7 billion tons of CO2 per year\nResearch carried out at the Greater Farallones Marine Sanctuary off San Francisco shows that the carcasses of dead whales could be responsible for around 60% of the carbon sequestration taking place in the ocean.\nWhales store large amounts of carbon in their bodies, and when they die they take it to the bottom of the ocean floor, so it is sequestered, rather than released to the atmosphere.\nWhales are also partially responsible for the growth of phytoplankton. Their iron-rich faeces stimulates the growth of the plankton which sequester 40% of the world’s CO2. It is estimated that whales could capture 1.7 billion tons of CO2 annually if allowed to return to their pre-whaling numbers.']	['<urn:uuid:ca462940-436b-4783-80d6-8275b19582e7>', '<urn:uuid:40290e17-a624-4dfc-917e-ac81405ab129>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:15:13.904987	8	69	2232
64	interested gardening prevent soil compaction planting techniques recommendations required	To prevent soil compaction, which is crucial for plant survival, you should create a hole with un-compacted soil. Use a trowel or garden spade to gently push into the sides of the hole and pry outward, creating small clumps and natural cracks. These cracks facilitate air and water movement, and root growth. Do not press down the soil with your foot after planting, as this will undo this work by compacting the soil.	['Protecting your investment in your plants starts by digging a hole the right size, with the right conditions. If you dig the hole incorrectly, essentially all you do is create a bigger pot for your plant.\nPlanting your transplants in the right type of hole can make all the difference in their success. Look at the difference in the same variety of tomato planted in holes prepared in different ways. Which tomato – or shrub or tree – would you prefer to have in your yard?\nThere are a few things to keep in mind:\n- Hole size – depends upon the height of the plant and the size of the pot. As the pots and plants get larger, so does the hole. Smaller plants – 3 inches or less – need a hole 6 to 12 inches deep. Larger garden plants – pots greater than 3 inches, I dig a hole at about twice the diameter of the existing pot and 1.5 to 2 times as deep as I want to plant. You want to make sure you have plenty room for loose soil in the bottom of your hole, so roots thrive.\n- Soil conditions – read the label for your new plant. Do the soil conditions required by the plant match your planting location? Perhaps your soil is too sandy. Or, it may have too much clay, and not allow for proper drainage. You’ll want to check that.\n- Other plant factors – such as light requirements, space requirements, and even wind, should be considered.\n- Loose, un-compacted soil – Perhaps the most important concept for the long term survival of your plant is create a hole in and with un-compacted soil. Plants need water and air to survive. Compacted soils – from heavy equipment, too much foot traffic or many other sources – don’t have large enough soil pores in them. And roots have a very hard time pushing through compacted soils. We can fix this a bit with a good hole, but if the overall area is too compacted, this may stunt growth, and you won’t get the robust plant you hoped for. So, working with a hole of the proper size (check back to step one!), take a trowel or garden spade and starting at the bottom of the hole, push gently into the sides of the hole and pry outward. The soil will break off in small clumps of various sizes. If you look closely at the sides of the hole now, you can see some natural structure and cracks along the sides of the hole. These cracks help air and water movement, and root growth. Do not follow conventional “wisdom” and use your foot to press down the soil after you plant. If you do, you compact the soil, undoing this good work you’ve done. Taking this step to make sure your soil isn’t compacted on the sides of you hole will pay off in future dividends.\n- Place your plant in the hole. I use some compost under the plant, to bring the crown just below the surface of the soil. I put the compost into my hole with about half the loose soil, then mix it together. Compost improves soil structure that improves air and water movement, and often increases water availability to plants.\nFor more information on digging a proper hole, ASA and SSSA have developed two videos. One is shorter, and can be found at https://youtu.be/EHq2CP6Gso4. It quickly describes steps 1 and 4. Another video goes into more depth about many variations regarding the soil that will help your plant investments best succeed. It can be found at http://soils.peachnewmedia.com/store/seminar/seminar.php?seminar=60123. I highly recommend that all serious gardeners and landscapers watch these videos, and help our soil work in the best way possible to support plant growth – and protect those garden center investments!\nAnswered by Clay Robinson, University of Illinois Urbana-Champaign\nTo receive notices about future blogs, be sure to subscribe to Soils Matter by clicking on the Follow button on the upper right! Explore more on our webpage About Soils. There you will find more information about Soil Basics, Community Gardens, Green Infrastructure, Green Roofs, Soil Contaminants, materials for Teachers and more.']	['<urn:uuid:3e05c275-17f4-4bfe-bb6f-b7baa8f91a6f>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	9	73	704
65	what were main historical events destruction carthage ancient tunisia	Carthage faced two major destructions in its history. The first occurred in 146 BC during the Third Punic War when the Romans destroyed the city. The second destruction happened in 698 during the Muslim conquest. Despite its first destruction, Carthage was later refounded by Rome and became one of the three most important cities of the Empire, maintaining this position until the Muslim conquest.	"['Site of Carthage\nThe Site of Carthage comprises the remains of a capital city that has been influential in the Mediterranean region, especially as representative of the Phoenico-Punic civilization.\nCarthage is an ancient city in Tunisia, originally founded by Phoenician colonists. Under the leadership of Elissa, Queen Dido, Carthage became a large and rich city and thus a major power of the Mediterranean until her destruction in the Third Punic War in 146 BC.\nAlthough the center of the Punic culture was destroyed, it continued in Roman times. Rome also refounded Carthage, becoming one of the three most important cities of the Empire, a position that would last until the Muslim conquest when it was destroyed a second time in 698.\nMap of Site of CarthageLoad map\nDr. Owen Mifsud\nUnfortunately not much of what was Carthage remains today. The museum however, explains really well its tragic past and former glory. Beautiful statues, figures and parts of buildings abound in the museum. The museum is right next to a Christian Cathedral that is worth visiting. We also got to see the Roman Amphiteathre and the Antonine Baths - the remains of the Roman Empire that conquered the Carthaginians. Carthage abounds with many historical places and we had planned only a day tour of the area and thus managed just a few sites. I would suggest spending a longer period to visit Carthage and the nearby towns and Capital city.\nI agree with the other posters that the present site doesn´t offer as much to see as one might expect, which is only understandable if you know what the Romans did to Carthage (completely razed the city, killed or enslaved its people and poured salt on the ground so that nothing would ever grow there again). However, a Roman city did eventually form there, and its ruins are still impressive, especially the baths. You mustn´t take pictures of the Tunisian President´s palace nearby, so beware! The guards may take your camera away! Anyway, besides the very few genuinely Carthaginian remnants and the great view of the sea, the most important thing is just to go there and to know that you´re standing on the site of one of the world´s great civilizations - one that brought mighty Rome almost to its feet.\nThere is not much left to see of Carthage which really is a testament to the Romans who totally obliterated their biggest rival of the period. It is pretty important therefore to get some background knowledge on the place before you go because it really is quite impressive.\nThe main site on Bysra Hill has some very impressive views across Tunis and to the Cape Bon Peninsula. If you go for a bit of a walk you will see that there are a few more intact bits below the forum. Then if you head down to the sea at the Punic Ports this is perhaps the best area as the lay-out of the ports that once held the mightiest navy in the world are still there, and next to them is the Tophet where it is reported that Child sacrificing went on, and the stalea that litter the site can be seen as an eerie testament to that. Further along to coast are the Roman Baths which are huge and probably the most impressive site to look at.\nCarthage is very easy to get to from central Tunis there is a TGM train that runs there from the end of Ave. Boiurigibba (the main avenue) it takes about 25 minutes and there are four stations scattered around the sites to help you get around. The locals are ridiculously friendly and helpful\nCarthage is a bit disappointing because not much is left but the trip alone is worth it. Take an electric trolley from Tunis and you can walk to the main part of the site from the Carthage Hannibal trolley stop. The museum is quite good but watch out for ""freelancing"" guides. The Roman mosaics in the courtyard of the museum are breathtaking and are just casually laying everywhere you look. Also, the view of the sea is fantastic from the hilltop. We tried to find the stadium but never could. Watch your belongings on the trolley!\n- Carmen Maria David Haining Guan WalGra MohamedOuali :\n- Christoph Tarquinio_Superbo Drazsika Klaus Freisinger M. Huineman de la Cuadra :\n- Roman Raab Christravelblog SaoDies Mikko Nmocosta Stanislaw Warwas :\n- Solivagant Alias65 Juha Sjoeblom Cobaltrage Riomussafer :\n- Peter Lööv Ian Cade MoPython Philipp Peterer :\n- Argo Stanimir Merveil :\n- Walter :\nThe site has 13 locations\nThe site has 26 connections\nScience and Technology\nWHS on Other Lists\n207 Community Members have visited.']"	['<urn:uuid:bdd4fe56-cff9-49b1-ae74-8f98264655a8>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	9	64	786
66	compare early steam tractor weight traction problems soft ground solutions modern cultivator soil preparation methods differences	Early steam tractors faced significant challenges with weight and traction on soft ground, often becoming stuck due to their heavy construction. The solution attempted was the implementation of endless tracks, as shown in various patents from 1859 onwards. In contrast, modern cultivators address soil preparation through various specialized methods, including using rotary motion of disks or teeth, and different types of shanks that pierce the soil. Modern cultivators are designed to disturb soil in careful patterns to protect crops while killing weeds, and can be equipped with hydraulic wings and various adjustable components for precise soil engagement. They also come in different sizes and can be either self-propelled or drawn as attachments, offering much more versatility than their early steam-powered predecessors.	"[""I’ve heard that story ever since I got into the hobby and, at the risk of upsetting Oliver and Hart-Parr enthusiasts, I’m here to tell you that ain’t necessarily so. Oh, I believe that Mr. Williams had probably never heard the word before and truly was convinced that he’d invented it, but no less an authority than the U.S. Patent Office attests to the fact that the word “tractor” was not at all new in 1906.\nAs soon as self-propelled vehicles powered by steam became common, men tried to adapt them to agricultural use. The biggest hurdle to steam traction devices in farm use was their weight and poor traction, with the heavy monsters often becoming hopelessly mired in soft fields. Many a brow was furrowed and more than one head was scratched over the problem and many fanciful ideas were the result.\nOne of the most promising methods of obtaining both traction and flotation on soft ground was to use endless tracks and one of the first patents along those lines was awarded in 1859 to Charles F. Mann, Troy, N.Y., for his “Traction Locomotive Carrying Its Own Railway.” Other patents for endless tracked “traction engines,” “road locomotives,” and the like followed through the next four decades, most with little or no success.\nA man named George H. Edwards from Lanark, a small town in northwestern Illinois, received a patent in 1872 for an “improvement in braced chains,” in which he described a new way of connecting the chain links of an endless track.\nGeorge Edwards next turns up in 1880, now living in Chicago, with a patent for an improved “traction-truck,” with an endless crawler-type track. In this patent, Edwards writes: “The object of my invention is to provide a tractor (author’s italics), to be propelled by a steam-engine or other suitable mechanical motor over firm or yielding surfaces, for pulling, pushing, and carrying purposes, as may be required for plowing and other uses.”\nEdwards improved that traction-truck in 1888 with a further patent in which he doesn’t mention the word “tractor,” but refers to “...a truck or wagon adapted to carry heavy burdens, and be propelled by a steam-engine or other suitable motor over cultivated fields, swamp lands, or the common highways, for plowing, hauling, and various kinds of work usually performed by animal power.”\nA U.S. Patent office drawing of George Edward’s tractor, patented in 1890.\nEdward’s final (as far as I can find) patent was issued in April 1890 and is for, of all things, a “tractor,” a word that was supposedly coined 16 years later by W.H. Williams.\nGeorge Edward’s “tractor” was long and awkward looking, although it had a couple of neat features. The entire drive mechanism, consisting of two side-by-side tracks, each on two large rollers, as well as the steam engine, were mounted on a sub-frame that was pivoted to the rear of the main frame in order to allow it to rock and to maintain contact with uneven ground. The water tank for the boiler was mounted between the tracks and below the engine to keep the center of gravity low for stability on hillsides. The firebox and boiler were mounted on the main frame in front of the tracks and provisions were made for flexible water and steam connections between the two.\nAlthough it doesn’t specify in the patent, the boiler was apparently oil-fired as what looks like a fuel tank is mounted above the rear of the machine with pipes to connect it to the boiler. The driver’s compartment is just ahead of the boiler (warm in winter, but not so great on a hot summer day). The front steering wheel or wheels appear to be operated by a power steering mechanism, driven by a universal-jointed shaft from the engine and controlled by a lever in the driver's compartment.\nI can find nothing at all about George Edward’s life or why he was so interested in tracked vehicles. It would be really interesting to know how he came up with the word “tractor,” and whether the word was used anywhere else during those years. No further trace of George or his “tractor” can be found, so he probably died a disappointed man. He should have forgotten about his mechanical inventions and just patented the word “tractor,” and he'd probably be fondly remembered today."", 'This article needs additional citations for verification. (July 2011) (Learn how and when to remove this template message)\nA cultivator is any of several types of farm implement used for secondary tillage. One sense of the name refers to frames with the teeth (also called shanks) that pierce the soil as they are dragged through it linearly. Another sense refers to machines that use rotary motion of disks or teeth to accomplish a similar result. The rotary tiller is a principal example.\nCultivators stir and pulverize the soil, either before planting (to aerate the soil and prepare a smooth, loose seedbed) or after the crop has begun growing (to kill weeds—controlled disturbance of the topsoil close to the crop plants kills the surrounding weeds by uprooting them, burying their leaves to disrupt their photosynthesis, or a combination of both). Unlike a harrow, which disturbs the entire surface of the soil, cultivators are designed to disturb the soil in careful patterns, sparing the crop plants but disrupting the weeds.\nCultivators of the toothed type are often similar in form to chisel plows, but their goals are different. Cultivator teeth work near the surface, usually for weed control, whereas chisel plow shanks work deep beneath the surface, breaking up hardpan. Consequently, cultivating also takes much less power per shank than does chisel plowing.\nSmall toothed cultivators pushed or pulled by a single person are used as garden tools for small-scale gardening, such as for the household\'s own use or for small market gardens. Similarly sized rotary tillers combine the functions of harrow and cultivator into one multipurpose machine.\nCultivators are usually either self-propelled or drawn as an attachment behind either a two-wheel tractor or four-wheel tractor. For two-wheel tractors they are usually rigidly fixed and powered via couplings to the tractors\' transmission. For four-wheel tractors they are usually attached by means of a three-point hitch and driven by a power take-off (PTO). Drawbar hookup is also still commonly used worldwide. Draft-animal power is sometimes still used today, being somewhat common in developing nations although rare in more industrialized economies.\nThe basic idea of soil scratching for weed control is ancient and was done with hoes or mattocks for millennia before cultivators were developed. Cultivators were originally drawn by draft animals (such as horses, mules, or oxen) or were pushed or drawn by people. In modern commercial agriculture, the amount of cultivating done for weed control has been greatly reduced via use of herbicides instead. However, herbicides are not always desirable—for example, in organic farming.\nThe powered rotary hoe was invented by Arthur Clifford Howard who, in 1912, began experimenting with rotary tillage on his father\'s farm at Gilgandra, New South Wales, Australia. Initially using his father\'s steam tractor engine as a power source, he found that ground could be mechanically tilled without soil-packing occurring, as was the case with normal ploughing. His earliest designs threw the tilled soil sideways, until he improved his invention by designing an L-shaped blade mounted on widely spaced flanges fixed to a small-diameter rotor. With fellow apprentice Everard McCleary, he established a company to make his machine, but plans were interrupted by World War I. In 1919 Howard returned to Australia and resumed his design work, patenting a design with 5 rotary hoe cultivator blades and an internal combustion engine in 1920.\nMeanwhile, in North America during the 1910s, tractors were evolving away from traction engine-sized monsters toward smaller, lighter, more affordable machines. The Fordson tractor especially had made tractors affordable and practical for small and medium family farms for the first time in history. Cultivating was somewhat of an afterthought in the Fordson\'s design, which reflected the fact that even just bringing practical motorized tractive power alone to this market segment was in itself a milestone. This left an opportunity for others to pursue better motorized cultivating. Between 1915 and 1920, various inventors and farm implement companies experimented with a class of machines referred to as motor cultivators, which were simply modified horse-drawn shank-type cultivators with motors added for self-propulsion. This class of machines found limited market success. But by 1921 International Harvester had combined motorized cultivating with the other tasks of tractors (tractive power and belt work) to create the Farmall, the general-purpose tractor tailored to cultivating that basically invented the category of row-crop tractors.\nIn Australia, by the 1930s, Howard was finding it increasingly difficult to meet a growing worldwide demand for exports of his machines. He travelled to the United Kingdom, founding the company Rotary Hoes Ltd in East Horndon, Essex, in July 1938. Branches of this new company subsequently opened in the United States of America, South Africa, Germany, France, Italy, Spain, Brazil, Malaysia, Australia and New Zealand. It later became the holding company for Howard Rotavator Co. Ltd. The Howard Group of companies was acquired by the Danish Thrige Agro Group in 1985, and in December 2000 the Howard Group became a member of Kongskilde Industries of Soroe, Denmark.\nWhen herbicidal weed control was first widely commercialized in the 1950s and 1960s, it played into that era\'s optimistic worldview in which sciences such as chemistry would usher in a new age of modernity that would leave old-fashioned practices (such as weed control via cultivators) in the dustbin of history. Thus herbicidal weed control was adopted very widely, and in some cases too heavily and hastily. In subsequent decades, people overcame this initial imbalance and came to realize that herbicidal weed control has limitations and externalities, and it must be managed intelligently. It is still widely used, and probably will continue to be indispensable to affordable food production worldwide for the foreseeable future; but its wise management includes seeking alternate methods, such as the traditional standby of mechanical cultivation, where practical.\nTo the extent that cultivating is done commercially today (such as in truck farming), it is usually powered by tractors, especially row-crop tractors. Industrial cultivators can vary greatly in size and shape, from 10 feet (3 m) to 80 feet (24 m) wide. Many are equipped with hydraulic wings that fold up to make road travel easier and safer. Different types are used for preparation of fields before planting, and for the control of weeds between row crops. The cultivator may be an implement trailed after the tractor via a drawbar; mounted on the three-point hitch; or mounted on a frame beneath the tractor. Active cultivator implements are driven by a power take-off shaft. While most cultivator are considered a secondary tillage implement, active cultivators are commonly used for primary tillage in lighter soils instead of plowing. The largest versions available are about 6 m (20 ft) wide, and require a tractor with an excess of 150 horsepower (110 kW) (PTO) to drive them.\nField cultivators are used to complete tillage operations in many types of arable crop fields. The main function of the field cultivator is to prepare a proper seedbed for the crop to be planted into, to bury crop residue in the soil (helping to warm the soil before planting), to control weeds, and to mix and incorporate the soil to ensure the growing crop has enough water and nutrients to grow well during the growing season. The implement has many shanks mounted on the underside of a metal frame, and small narrow rods at the rear of the machine that smooth out the soil surface for easier travel later when planting. In most field cultivators, one-to-many hydraulic cylinders raise and lower the implement and control its depth.\nRow crop cultivators\nThe main function of the row crop cultivator is weed control between the rows of an established crop. Row crop cultivators are usually raised and lowered by a three-point hitch and the depth is controlled by gauge wheels.\nSometimes referred to as sweep cultivators, these commonly have two center blades that cut weeds from the roots near the base of the crop and turn over soil, while two rear sweeps further outward than the center blades deal with the center of the row, and can be anywhere from 1 to 36 rows wide.\nSmall tilling equipment, used in small gardens such as household gardens and small commercial gardens, can provide both primary and secondary tillage. For example, a rotary tiller does both the ""plowing"" and the ""harrowing"", preparing a smooth, loose seedbed. It does not provide the row-wise weed control that cultivator teeth would. For that task, there are single-person-pushable toothed cultivators.\nVariants and trademarks\nRotary tillers are a type of cultivators. Rotary tillers are popular with home gardeners who want large vegetable gardens. The garden may be tilled a few times before planting each crop. Rotary tillers may be rented from tool rental centers for single-use applications, such as when planting grass.\nA small rotary hoe for domestic gardens was known by the trademark Rototiller and another, made by the Howard Group, who produced a range of rotary tillers, was known as the Rotavator.\n- The small rototiller is typically propelled forward via a (1–5 horsepower or 0.8–3.5 kilowatts) petrol engine rotating the tines, and do not have powered wheels, though they may have small transport/level control wheel(s). To keep the machine from moving forward too fast, an adjustable tine is usually fixed just behind the blades so that through friction with deeper un-tilled soil, it acts as a brake, slowing the machine and allowing it to pulverize the soils. The slower a rototiller moves forward, the more soil tilth can be obtained. The operator can control the amount of friction/braking action by raising and lowering the handlebars of the tiller. Rototillers often do not have a reverse as such backwards movement towards the operator could cause serious injury. While operating, the rototiller can be pulled backwards to go over areas that were not pulverized enough, but care must be taken to ensure that the operator does not stumble and pull the rototiller on top of himself. Rototilling is much faster than manual tilling, but notoriously difficult to handle and exhausting work, especially in the heavier and higher horsepower models. If the rototiller\'s blades catch on unseen subsurface objects, such as tree roots and buried garbage, it can cause the rototiller to abruptly and violently move in any direction.\n- Unlike the Rototiller, the self-propelled Howard Rotavator is equipped with a gearbox and driven forward, or held back, by its wheels. The gearbox enables the forward speed to be adjusted while the rotational speed of the tines remains constant which enables the operator to easily regulate the extent to which soil is engaged. For a two-wheel tractor rotavator this greatly reduces the workload of the operator as compared to a rototiller. These rotavators are generally more heavy duty, come in higher power (4–18 horsepower or 3–13 kilowatts) with either petrol or diesel engines and can cover much more area per hour. The trademarked word ""Rotavator"" is one of the longest single-word palindromes in the English language.\n- Mini tiller\n- Mini tillers are a new type of small agricultural tillers or cultivators used by farmers or homeowners. These are also known as power tillers or garden tillers. Compact, powerful and, most importantly, inexpensive, these agricultural rotary tillers are providing alternatives to four-wheel tractors and in the small farmers\' fields in developing countries are more economical than four-wheel tractors.\n- Two-wheel tractor\n- The higher power ""riding"" rotavators cross out of the home garden category into farming category, especially in Asia, Africa and South America, capable of preparing 1 hectare of land in 8–10 hours. These are also known as power tillers or walking tractors. Years ago they were considered only useful for rice growing areas, where they were fitted with steel cage-wheels for traction, but now the same are being used in both wetland and dryland farming all over the world. They have multiple functions with related tools for dryland or paddys, pumping, transportation, threshing, ditching, spraying pesticide. They can be used on hills, mountains, in greenhouses and orchards. Diesel designs are more popular in developing countries than gasoline.\n- ""Howard, Arthur Clifford (1893–1971)"". Australian Dictionary of Biography – Online Edition. Australian National University. 1983. Retrieved 2007-07-12.\n- Langmore, Diane (1983). ""Howard, Arthur Clifford (1893–1971)"". Australian Dictionary of Biography – Online Edition. Australian National University. Retrieved 2007-07-12.\n- ""The Howard Rotavator"". 2007-02-03. Retrieved 2007-07-12.\n- ""Machinery Manufacturers - Secondary Tillage"". Worldwide Agricultural Machinery and Equipment Directory. 2006-11-14. Retrieved 2007-07-12.\n|Wikimedia Commons has media related to Rotary tillers.|\n|Wikimedia Commons has media related to Cultivators.|']"	['<urn:uuid:506af9de-c402-4f02-b21c-05abf549970d>', '<urn:uuid:b0a89502-b32b-49ed-a70e-5953e3ec841d>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	16	121	2803
67	which sop template types most common	There are 4 popular standard operating procedure (SOP) template types. The Checklist format uses a to-do list approach ideal for onboarding new employees. The Step-by-Step List provides detailed explanations for each step and works well for both new and experienced employees. The Flowchart template illustrates decisions as an interrelated chain of events, which is especially helpful for visual learners. Finally, the Hierarchical Path organizes major milestones with subtasks beneath them, allowing different levels of detail for varying employee experience levels.	['Visionary entrepreneurs are accustomed to tackling big challenges and generating substantial results. True, not every idea yields gold. But the alchemical process of testing your ideas and solving problems is a legitimate rush.\nOn the flipside, there will always be less glamorous aspects of starting and running a business. The details that are tedious and drab to manage. But just as nuts and bolts hold together the most sophisticated and gorgeous sports cars, these basic components of your business will give it structure and enhance its performance.\nPerhaps there’s nothing more representative of this “nuts and bolts” principle than standard operating procedures (SOP). Are there any words in our language more boring than “standard,” “operating,” and “procedures?” Yet they combine to create the outlines that consistently get your business from “Point A” to “Point B.”\nStandard Operating Procedures (SOP) Meaning\nLet’s start with an SOP definition. Standard operating procedures are documents that outline the steps required to complete a process. Rather than a synopsis, you get helpful directions from an SOP, meaning a new employee could study the document and then carry out that specific activity for your business.\nReturning to the sports car example referenced above, if you owned a luxury car dealership you might need standard operating procedures for moving the Lamborghini Veneno from one side of the showroom to the other. After all, when an employee is operating a multi-million-dollar machine, you never want to simply say, “Go grab the keys and drive that thing over there in the corner.”\nYour standard operating procedure document might be a checklist. In other cases, it could just be a numbered series of steps. The SOP template you use isn’t nearly as important as the clarity you use. Choose the format that makes the process so clear that it’s impossible for it to be misunderstood.\nBenefits of Standard Operating Procedures\nThe fact that you are seeking out resources such as this guide proves that you’re an entrepreneur who cares about quality and improvement. Your thirst for knowledge has propelled you this far in life, and will continue to help you refine your business and deliver even better for your customers.\nStandard operating procedures will always take on a higher level of significance for entrepreneurs such as yourself. When quality matters, adherence to process matters.\nOf course, you won’t be carving your standard operating procedures into stone. There’s always room for new ideas and better efficiency. You should encourage this type of innovation among your team. But any of those enhancements must be promptly recorded within the SOP document, so that everyone’s on the same page at any given time.\nHere are some key benefits that often come with the usage of standard operating procedures:\n- Consistent quality\n- Employee efficiency\n- Increased productivity\n- Employee safety\n- Faster onboarding of new employees\n- More accountability\n- Fewer disagreements among employees\n- Faster resolutions to problems\n- Better customer experience\n- Retaining organizational knowledge\nRaise your hand if you’d like to bring more of those benefits into your business. Good, then let’s look at how to create these crucial documents.\nChoosing Your Standard Operating Procedure Template\nCan you imagine how hypocritical it would be for your standard operating procedures documents to go around preaching consistency to your team if they themselves were structured in a bunch of different formats? This is why you need to identify an SOP template that works for your business and then stick to it.\nWhen documentation is handled in a consistent way, your team will benefit from the equity. For example, they’ll know exactly which sections of the document contain specific information and will be able to quickly locate answers if they have questions.\nThe goal isn’t to choose the same standard operating procedure style that Apple or BMW or Coca-Cola use, but to find an SOP template that aligns with your business so that it feels intuitive. The style that works for you will be as unique as your strategic vision, so it’s important to consider a wide range of options in order to find your winner.\nHere are 4 of the most popular standard operating procedure template possibilities to consider:\n1. The Checklist: This is one of the clearest ways to break down the elements of a process. And if your document utilizes a to-do list approach that employees check off as they go, it’s easier to confirm completion before progressing to the next stage. For this reason, checklist-style SOP templates are ideal for onboarding new employees and simplifying processes for your existing team.\n2. The Step-by-Step List: Another simple way to outline your processes is with a step-by-step format. This standard operating procedure template allows the various steps to provide more robust details to ensure a streamlined and accurate process. Given these explanations, the step-by-step format can work for anyone from new employees to tenured leaders.\n3. The Flowchart: The success of your business is often based upon decisions, and this SOP template style illustrates those decisions as an interrelated chain of events. Visual learners in particular will be better able to grasp the prescribed path to the desired outcome. Because of the increased sophistication of this format, it takes more effort to compile a flowchart POS document than would be needed for the checklist or step-by-step formats.\n4. The Hierarchical Path: Many business processes involve key steps that each contain a series of smaller actions. Rather than create a mile-long checklist where each task is equally weighted, you can make buckets of subtasks under the major milestones. This hierarchical style is ideal for teams where members have varying degrees of familiarity with the process at hand, as those who know the drill can quickly navigate the main points and new employees can expand the subtasks to learn more.\nThere are strengths and weaknesses to each SOP template, so your objective is to find the option that maximizes your unique strengths and doesn’t exacerbate your weaknesses.\nCompiling Your Standard Operating Procedures Document\nAfter you’ve identified your preferred SOP template, it’s time to craft a document that is anything but standard. Your goal should be brevity, while also delivering information that’s so logical no reader could ever misunderstand. And if you don’t already know, you’ll learn that writing less content is more difficult than writing more.\nThe French philosopher Blaise Pascal perfectly captured the paradoxical nature of fewer words requiring more effort when he penned: “I would have written a shorter letter, but I did not have the time.”\nBe sure to carve out ample time when putting together your standard operating procedures. This will allow you the opportunity to review each task and ensure the most streamlined and accurate descriptions.\nSeek Insights from Others\nYou might know more about a certain business process than anyone on earth. After all, you probably created it in the first place. But that familiarity doesn’t mean you alone should be the architect of the standard operating procedures.\nTake a moment to think about the last time you explained something that you knew backward and forward to someone who had little experience with it. Your wealth of knowledge in these situations can actually be a hindrance, as you won’t be able to put yourself in the shoes of someone approaching the topic with a blank slate. You simply won’t be able to anticipate all the elements that will trip up the listener.\nFor this reason, it’s essential that you talk to your team members who are already carrying out the process. They’ll be able to provide insights into what steps are most important and even alert you of recent developments you might not have been aware of.\nYou’ll also want to seek the opinions of those who will be carrying out the process in the future. Find out which steps of the standard operating procedure make the most sense and then use those as your best practices for improving the less accessible steps.\nOne final benefit of seeking the input of others is that it will demonstrate how much you care about your team and respect their contribution. The end result is your people buying into the process and feeling more accountability over how it’s carried out.\nMapping the Journey\nYou’ve put your initial thoughts on paper. You’ve consulted with your team. Now it’s time to distill this information into a simple journey from “Point A” to “Point B.” To do this you’ll need to identify both the starting point and the expected outcome.\nNext, seek out the potential hurdles. Where has the process gone awry in the past? What are the current obstacles that need to be addressed, and ultimately, remedied? You’ll give your SOP document a much longer shelf life if you ensure it provides solutions in addition to facts.\nJust as any journey can get waylaid by distractions, your SOP document can suffer the same fate if you don’t keep it dialed in. There will always be more details and interesting anecdotes that could be included in the standard operating procedures than should ever live within it.\nYou can prevent this issue by drawing lines in the sand before you begin. What will be included and what is superfluous? Once the scope has been agreed upon, which is a discussion for all the stakeholders you involved in the earlier stages, you can rest easy that it won’t spiral out of control.\nPutting It All to the Test\nThink you have your standard operating procedure ready to roll? There’s only one way to find out. The best way to test the efficiency and efficacy of your SOP template is to release it into the wild of your business.\nWe recommend a two-pronged approach, as this is an experiment, after all. Have some team members continue using the legacy methods, while another group implements the new standard operating procedure.\nIf the test reveals problems with the new process, consider yourself lucky. Rather than doing a complete rollout that forces your team to toil under a flawed system, you’ve been alerted that there’s still work to be done. You have the option of either reverting to the old way until the new process is fine tuned, or moving forward with the new standard operating process with the knowledge that it needs improvement.\nDefining the Destination\nIt’s one thing to know where you want to go, but that knowledge is diluted if you don’t know how to confirm you’ve effectively reached it. Your standard operating procedures should be paired with metrics that will allow you to track and grade the performance.\nFor example, if you’re putting together an SOP document for your manufacturing process, you’d want to consider factors such as:\n- What is the average production time?\n- What is the desired manufacturing quota?\n- Have the preexisting obstacles been solved?\nViewing your processes through this lens makes it possible to identify issues that have still avoided correction, as well as new problems that might have unexpectedly popped up. It also provides the insights you need in order to set meaningful goals.\nYour teams handling the processes will appreciate these insights, as they’ve already weighed in on the SOP document and feel ownership. You can commend them for their successes whenever relevant, but also have a common language for tougher conversations when weaknesses are exposed. The good news is that individuals don’t feel personally attacked when you’re working off a collaborative document.\nIf you’re interested in more business-boosting insights that can improve your processes, check out our library of free training courses. From starting your business to perfecting your sales strategies, these courses are tailored for entrepreneurs who are on a mission to build something meaningful.']	['<urn:uuid:e439c421-90ef-452a-8174-b68af1014d92>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	6	80	1948
68	traditional bath modern system differences	Traditional Japanese onsen baths, like those at Dogo Onsen mentioned in Tales of Genji, were simple hot spring facilities. Modern systems now feature various water management approaches - some maintain traditional continuous flow from the source (Gensen kakenagashi) without adding water, while others use sophisticated circulation systems with heating, filtering, and sterilization. Modern onsens have also evolved into entertainment complexes, featuring additional attractions like Beppu's geothermal features tour and amusement parks, though bathing remains the primary focus.	['After traveling around Japan for all these years, I have come to really enjoy and appreciate the Japanese style baths. It is truly relaxing, and part of the Japanese experience that shouldn’t be missed. Bathing is an important part of Japanese life and culture. And one of the best places to experience bathing in Japan are at onsens. However, over the years I have become aware there is a basic misunderstanding of what an onsen is.\nAn onsen, by Japanese definition, is a hot springs resort. Many people have the mistaken idea that the baths themselves are the onsen. While there are many names the baths can be called, the most common name is “大浴場” (daiyokujou) which literally translates to “large bath place”. There are literally thousands of onsen all over Japan, and they can range in size from a single building to huge complexes with numerous accomodations available for guests. They can range in character from old, traditional-style buildings to areas that easily compare with Vegas for its gaudiness. It is generally thought that Dogo Onsen in Matsuyama is one of the oldest and original onsens. There are writings from the 6th century which mention Dogo Onsen, and it is even briefly mentioned in the Tales of Genji.\nThe primary reason the Japanese will go to an onsen are the baths. The secondary reason the Japanese go to an onsen are the baths. The third reason is…well, you get the idea. Actually, the food is very important as well. If you go to a travel agency in Japan and look at the travel beochures, you will find that the most common pictures shown are pictures of the baths and the meals. The rooms are usually an afterthought. The Japanese will sometimes fly long distances to visit their favorite onsen. Or, many times, people from a large city may visit a nearby onsen, such as people from from Osaka visting Arima and Shirahama Onsens.\nThe baths themselves can, and will usually be different from onsen to onsen, or even within the same onsen. Some onsen have numerous outdoor riverside or seaside baths. While others may have only indoor baths. Wakayama Prefecture’s Kawayu Onsen is famous for being able to dig a hole in the sand of the riverbed, and then waiting for it to fill up with hot water for your bath. There is a ryokan at Noboribetsu Onsen that has 17 seperate baths, each with its own temperature and type of water. (It’s kind of like an onsen Disneyland.)\nYou will usually find other things to do at onsen as well. For example, at the huge Beppu Onsen on the island of Kyushu, you will find the “Beppu Hells”. These are a grouping of geothermal features that can be seen as a collective tour. It is said the only other place on earth that produces more geothermally heated water than Beppu Onsen is Yellowstone. When you go to the Beppu Hells, don’t expect anything on the order of Yellowstone however. There will usually be amusement parks at the larger onsen for the family, and you can always find the ever-present pachinko and slot parlors. It is my observation and thought that all of these are nothing more than something to occupy the time between baths. But seriously, onsen are a part of Japanese life, and will be for years to come. I would highly suggest you visit one, if nothing else for the experience.', 'Have you ever tried the real onsen?\n1. Continuous flow from source (Gensen kakenagashi) Whilst heat might be added, there is no water added to this type of bath. With this kind of system, the high temperatures at the source are made more moderate and suitable for bathing by the use of heat-exchange devices either at the point where the water enters the bath, or in the bath itself.The hot spring of a ‘Gensen kakenageshi’ bath is always released without being used in a circulation system.\n2. Continuous flow (Kakenagashi) In order to maintain the moderate temperature of this type of bath, both water and heat are added to the water supply. Like the ‘Gensen’ varient of ‘Kakenagashi’ – once the water enters the bath, it is released and does not go through a circulation system.\n3. Direct flow from source – heated and circulation bath system the hot spring supply is untouched as it enters the bath from the entry point, but in order to keep the water at a suitable temperature, a circulation system is used.\n4. Direct flow from source – heated, filtered, sterilised, and circulation system.\nThe hot spring supply is untreated as it enters the bath from the entry point, but in order to keep the water at a suitable temperature, a circulation system is used to heat the water, whilst also purifying and sterilizing it.\n5. Hot spring supply is heated, filtered and sterilised before entering the bath.\nAs soon as the hot spring is supplied to the bath, in order to keep the water at a consistent volume and temperature, the water is first circulated, during which time it is also purified and sterilised before once more being released into the bath to be used.\nEven if the hot spring has a temperature of 25 degrees, the overall mineral composition within 1kg water adds up to less than 1000mg. The level of stimulation is low, but there will potentially be a wide range of effects when used.\nCarbon Dioxide Onsen\nCarbon Dioxide Onsen\nIt is where in 1kg of hot spring water, there is 100mg or more of released carbonic acid. This causes carbon dioxide to dissolve, creating a fine froth on the surface, and it is often referred to as a ‘bubbly spa’ or ‘soda spa’. The onsen has the effect of expanding blood vessels, helping to lower blood pressure.\nSodium Bicarbonate Saline Onsen\nThis type of onsen is alkaline, and there are two different types: Sodium Bicarbonate and Bicarbonate Earth Onsen. It is a clear colour and has the ability to make skin feel smooth, and in particular sodium bicarbonate makes skin soft and gentle, while removing dirt, often being called ‘the water of beautiful skin’.\nContaining salt similar to the kinds found in sea water, the salt has the effect of controlling perspiration after bathing, also means that the body’s ability to retain heat is also excellent. This type of onsen is known as ‘Fever water’.\nThe spring water, with a bitter taste when drunk, has been known as ‘Water for cuts and bruises’. Sulphate onsen are divided into three categories: Calcium sulphate, Sodium sulphate, containing traces of natrium, and a regular bitter spring which contains traces of magnesium.\nFerruginous Onsen (Iron)\nThere are two types of Ferruginous Onsen: Carbonated Iron Onsen, and Melanterite Onsen. The water is a colourless, transparent as it gushes up, but as it comes into contact with the atmosphere, it develops into a dark reddish-brown colour and weakens the waters effect. This type of spring helps the body’s ability to retain heat, and when drunk replenishes the body’s iron levels.\nColourless and transparent, or a yellowish-brown colour with a bitter taste, these acidic springs have strong sterilising properties.\nCupriferous/Ferruginous Onsen (Copper and Iron)\nSimilar to the Ferruginous spring, the water oxides with the atmosphere once exposed, becoming an opaque yellow colour, and leaving a red-brown sediment. Because there are traces of iron in the spring, the temperature can be very hot, but this kind of onsen is not very effective for cleaning the body.\nWith the distinctive and unique smell of bad eggs, this kind of spring has the ability to expand the blood vessels in the body. Because the hydrogen sulphide in the spring is toxic, proper ventilation is essential. The pale-yellow sediment are known as ‘Hot water flowers’. The sulphur can soften the hardened keratin of the skin, and is also good for acne and spots.\nThis type of spring has powerful sterilising properties, and a strong stimulation that permeates the skin. Acidic Onsen are not recommended for people with sensitive skin or weak health.\nOnly tiny amounts of radioactive minerals are found in this type of hot spring, and it can also be referred to as a ‘Radium Onsen’ or ‘Radon Onsen’. Strong evidence has been recorded that shows the miniscule amounts of radioactivity in the water have a beneficial effect on the body.']	['<urn:uuid:c9bd1148-31d6-4644-8de1-b02acd14d33c>', '<urn:uuid:aa1cc79c-4372-4478-aa40-6e0e7c886c94>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T06:15:13.904987	5	77	1397
69	calculate sleep debt per year lost hours	If you need 8 hours of sleep each night but only get 7 hours on average, you accumulate a sleep debt of 7 hours each week. Over the course of a year, this would result in 364 total hours of lost sleep, which is equivalent to nearly 7 weeks of sleep.	['“Sleep debt” is a term used to describe cumulative sleep loss resulting from ongoing sleep restriction. If you regularly get less sleep than you need, your accumulated sleep debt will grow larger over time. Some say this looming sleep debt makes you tired, grumpy, and prone to accidents and illness.\nIs Sleep Debt Dangerous?\nExperts disagree on the nature of a sleep debt. Some experts, most notably Jim Horne of Loughborough University, say that sleep needs naturally fluctuate with seasonal and social changes. For example, those living in Northern climates without much winter daylight may sleep for many hours during darker months, and naturally shift to sleeping less in the summer, when daylight peaks. And some studies show that social factors like boredom or isolation can increase the drive for sleep. So simply feeling tired or sleeping more doesn’t necessarily indicate that you have a sleep debt, he argues.\nHorne says these naturally occurring shifts in sleep cycles don’t indicate that people are incurring or repaying accumulated lost sleep, and may suggest that our bodies’ needs for sleep are more flexible than some assume.\nHowever, many other experts believe that chronic, long-term sleep deprivation is a public health problem with dire economic and social consequences. Research shows that accumulated sleep loss negatively impacts mood and cognitive performance.\nIn addition to these deficits in mental health and performance, ongoing sleep deprivation is linked to an increase risk for cardiovascular and metabolic illness, driving accidents, and fatigue. Sleep debt has been shown to cause death in lab studies on animals, and rarely, people die from lack of sleep.\nCalculating Your Sleep Debt\nWhat’s your personal sleep debt? Start with the amount of sleep you need to feel rested and alert. If you need 8 hours of sleep each night but only get 7 hours on average, you accumulate a sleep debt of 7 hours each week. Over the course of a year, you’ve lost 364 total hours of sleep, equivalent to nearly 7 weeks of sleep.\nCan you Repay Your Sleep Debt?\nWith the increased awareness of the dangers of sleep deprivation, many night owls and chronic sleep-skippers wonder if they can repay their looming “sleep debt.” The answer is no, for the most part. Once sleep is missed, it’s gone for good.\nSleeping more for a night or two can help restore some of the cognitive decline caused by a large sleep debt. But clearly, it’s not possible to “repay” a sleep debt totaling hundreds of hours.\nHowever, when a sleep-deprived individual slumbers, the body works to “make up” some lost sleep by spending more time in the deeper, more restorative stages of sleep, and less time in lighter-stage sleep. Scientists observe changes to the sleep stages in people who don’t sleep enough, which generally resolve if the person starts sleeping more. But because each sleep stage is vital to our mental and physical wellbeing, it’s vital to allow enough time for the body to move through all stages of sleep, and not chronically deprive yourself of the rest you need.\nErasing your Sleep Debt\nThe best way to stay out of sleep debt? First, stop adding to your sleep debt by making a real effort to get adequate sleep each night. Determine the amount of sleep you need to feel well-rested, and create a schedule that allows enough time for sleep. A sleep journal can help track your sleep patterns to help you see how much you’re sleeping overall.\nOnce you decide how much sleep you need each night, solid sleep hygiene habits will help keep your sleep routine on track:\nEstablish and stick to a consistent sleep time and wake up time\nAim to get in bed early enough to allow time for reading or intimacy without cutting into sleep\nSleep when you’re tired, and avoid using caffeine or other stimulants to “push through” fatigue and sleepiness\nKeep electronics and social media out of your sleeping space\nMaintain a sleeping space that’s dark, quiet, and cool\nUsing Naps to Ease your Sleep Debt\nA short daytime nap can help restore your alertness until you can get more rest at night. The best way to accomplish this is with a brief 15-20 minute nap between 11 a.m. and 3 p.m. Why is a midday nap important? If a nap occurs too early in the day, the body won’t be tired enough to fall asleep, while naps that occur after mid-afternoon can make it harder to fall asleep at night. If you can’t get enough coffee, you’re in luck: A “caffeine nap” could be your ideal midday snooze.']	['<urn:uuid:4abd01c3-f6ae-43fc-92eb-f8432f667a47>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	7	51	767
70	As someone studying early Celtic institutions, I'm intrigued by how both secular and religious power was maintained - what role did poets play in legitimizing rulers, and how were important manuscripts preserved through dedicated keepers?	Poets played a central role in validating Gaelic rulers from early times through the early modern era, particularly during inauguration ceremonies where they would enumerate the leader's ancestry and invoke blessings. As for manuscript preservation, the Book of Armagh provides an example of how important texts were maintained - it had a hereditary keeper called a Maor whose family name became Moyre/Moyer, and who received eight townlands as payment. Similar to this secular role, religious institutions by the 8th century had established hereditary keepers to safeguard holy relics in Gaelic regions.	"['The Book of Armagh\nFREE Catholic Classes\nTechnically known as LIBER AR(D)MACHANUS.\nA celebrated Irish-Latin manuscript preserved in the Library of Trinity College, Dublin. It is a vellum, in small quarto, and in a fine state of preservation, with the exception of the commencement, where a few pages are missing. In its present condition it consists of 221 leaves (442 pages) with the writing in double or, less often, in triple columns. The Irish band is used throughout, but some of the initial letters are in Greek character, and some of the letters are lightly coloured black, red, green, and yellow. The penmanship is, on the whole, very beautiful, distinct, and uniform. The only drawings in the manuscripts are four, representing the symbols of the Evangelists. Because of the value that the Irish placed on the Book of Armagh, it was often richly bound, and encased in shrines of artistic workmanship. The Book of Armagh was also known as the ""Canon of Patrick"", and it was once thought that it was the Patron\'s own book and in part, the work of Patrick himself. It was left for Bishop Charles Graves, however, to discover from the erasures in the manuscript itself, and from references in the Annals to names which he had pieced together from the Book of Armagh, that the name of the scribe of, perhaps, the entire work was Ferdomnach of Armagh, who died in 845 or 846, and that he wrote the first part of the Book in the year 807 or 808.\nThe Book of Armagh is, in the main, a transcript of documents of a much older period than the Book which has preserved them, and these documents are of inestimable value for the early history and civilization of Ireland. Above all, this collection is valuable because it contains the earliest writings that have come down to us relating to St. Patrick. The author of one of the Lives of Patrick, which the Book of Armagh contains, was one Muirchu Maccu Machteni, who wrote at the request of Aed, Bishop of Sletty. The author of the other Life was Tirechan, who wrote, we are told, for Bishop Ultan of Ardbraccan . Both these authors wrote at about the middle of the seventh century, and had as their authorities even older memoirs. The Book contains other miscellaneous documents relating to St. Patrick, and gives considerable information on the rights and prerogatives of the See of Armagh. Among the miscellaneous contents may be mentioned the ""Liber Angueli"" (so spelled in the Irish fashion to show that the g was not palatalized), ""the Book of the Angel "", wherein an angel is represented as entrusting to St. Patrick the primatial rights of Armagh; the Eusebian Canons, St. Jerome\'s letter to Damasus, Epistles of St. Paul, with prefaces, chiefly by Pelagius, Epistles of James, Peter, John, and Jude; the Apocalypse, the Gospels according to Matthew, Mark, John, and Luke, and the ""Life of St. Martin of Tours "", by Sulpicius Severus. At bottom of folio 16 verso , there is an entry which the scribe says was made ""in conspectu Briani imperatoris Scotorum"", that is, in the presence of Brian Borumha, probably in the year 1002.\nSt. Bernard, writing in the twelfth century, in his ""Life of Malachi "", speaks of a certain book which, he says, was one of the marks of the primatial rights of the See of Armagh. This was probably the ""Liber Ardmachanus"". In such high estimation was this Book held that a custodian was appointed for it and in virtue of his office he had, as his remuneration no less than eight townlands. It was probably one of his functions to carry the Book on occasions of state and ceremony. The name of the keeper (in Irish, Maor , ""steward"") became in the course of time the family name of the keeper, since the office was hereditary, and they became known is mac (pl. meic ) maor , or, anglicized, Moyre or Moyer. The precious Book thus changed hands frequently, and there is mention in the records that it was once pawned as security for a claim of five pounds. In the latter part of the seventeenth century it passed from the hands of the MacMoyres into the possession of the Brownlow family of Lurgan, with whom it remained until 1853, when it was purchased for three hundred pounds by the Irish antiquarian, Dr. Reeves, and by him transferred, on the same terms, to the Anglican primate Beresford, who presented it to the Library of Trinity College. There is evidence to show that the Book was often used when giving testimony, and that oaths were sworn, and covenants ratified on it. This may account for some of the pages having the appearance of having been rubbed or touched frequently. The Irish of the Book of Armagh is of the greatest importance for the history of the Irish language. It is not only one of the very oldest monuments of the Old-lrish, since it is antedated only by the fragmentary glosses in the Irish manuscripts preserved on the Continent, but it is the earliest extant specimen of a continuous narrative in Irish prose. It represents the language of the end of the seventh, or of the beginning of the eighth, century. The phonetic peculiarities of the Irish of that period, as evidenced in the Book of Armagh, are described briefly by Whitley Stokes and John Strachan in the preface to the second volume of their ""Thesaurus Paleohibernicus"", XIII, sqq. This same volume contains all the Irish found in the Book of Armagh.\nFREE Catholic Classes Pick a class, you can learn anything\nCopyright 2020 Catholic Online. All materials contained on this site, whether written, audible or visual are the exclusive property of Catholic Online and are protected under U.S. and International copyright laws, © Copyright 2020 Catholic Online. Any unauthorized use, without prior written consent of Catholic Online is strictly forbidden and prohibited.\nCatholic Online is a Project of Your Catholic Voice Foundation, a Not-for-Profit Corporation. Your Catholic Voice Foundation has been granted a recognition of tax exemption under Section 501(c)(3) of the Internal Revenue Code. Federal Tax Identification Number: 81-0596847. Your gift is tax-deductible as allowed by law.', 'Kingship and Authority\nThe rites of inauguration used to invest power in a new king are symbolic statements of cultural values and ideals at the highest level of society. Textual descriptions of inauguration and secular power are only preserved in Gaelic sources from about the 7th century onwards in Ireland and Gaelic Scotland (the illustration below depicts the inauguration of Uí Néill in the late 16th century). There is little left to inform us about the inaugural rites of the early Brittonic or Pictish kingdoms.\nInauguration is referred to as rígad in Old Gaelic, although in some accounts the term banais-ríge (literally “king-marriage”) drew explicit attention to inauguration as a form of wedding between the leader and the sovereignty goddess of the territory. Although there was no single ritual of inauguration in Gaelic society, there are a number of common\nelements which recur in accounts of these ceremonies in both Scotland and Ireland, from kings to clan chiefs:\n- A poet or cleric conducts the ceremony, enumerating the leader’s ancestry and invoking a blessing on him and his reign;\n- It is on or near a burial mound containing a ceremonial stone, and under or near a sacred tree;\n- It includes the leader’s horse or chariot (in reality or symbolically);\n- The leader’s garment or mantle is given to the master of the ceremony;\n- There is a single footprint, shoe or sandal into which the leader steps;\n- The leader is anointed;\n- The leader drinks alcohol from a ceremonial vessel;\n- The leader is presented with the rod of sovereignty (slat tighearnais), sometimes a wand cut from the sacred tree.\nBy the late 8th century, the church had inserted itself into the ordination of the kings of Ireland, as recorded in 793 at the inauguration of Artrí mac Cathail in Munster. Regardless, poets continued to play a central role in the validation of Gaelic rulers in both Scotland and Ireland up to the early modern era.\nWhat aspects of the inauguration ritual, and symbolism of kingship, are referred to by the poem “Aed oll fri andud nane”? How is the relationship between ruler and poet represented, especially in terms of what each one does for, or offers to, the other?\nWhereas in the previous time period (400-600), we saw the development of the office of the king and the ideology of kingship in the Insular Celtic context, during this time period we see that the institutions of the kingdom were being consolidated and extending their reach. This is visible amongst all of the ethnic groups (Celts and Anglo-Saxons) and had as much to do with secular ambition as the extension of the authority of religious institutions and the ideals of Christian kingship.\nAs the king’s power grew, so did the size of his territory and retinue. It would have exhausted the estate of the king himself to feed and house his military retinue (aka comitatus), so the king travelled with them around the clients on his territories who took turns billeting the royal company. Such obligations of lodging and feeding were an integral part of the client-lord relationship and allowed clients the chance to curry favour with their leader.\nAlthough originally, and in theory, each tuath in Ireland was a sovereign state impenetrable by overlords (or “overkings”), we can already see in the 8th century that some kings are being demoted in status to the lesser rank of “lord” (the title used in Latin sources is dux) due to the steady pressure of overkings. This title was used in Ireland of the ruler of the Delbna Ethra in 756, and the rulers of Luigne and Ciarraige in 771 and 796. The extension of Pictish overlord-ship over smaller kingdoms, and the consolidation of kingdoms in Wales, has already been discussed. The kingdom of Northumbria exemplified similar processes in the Anglo-Saxon(ized) sphere.\nThe instability of food resources – bad seasons, unpredictable harvests, etc – made conspicuous consumption at feasts all the more significant displays of status. High social status was closely related to good nutrition and health, as is apparent in many contemporary texts, though often expressed in formulaic language.\nThe Cult of Saints\nVirtually all religions recognize certain places which are sacred, charged with special significance and power, often because of their association with particular events and people revered in the narratives of the religion. Initially Christianity had disavowed the cult of holy places, but by the end of the fourth century it had been reconceptualized and reintegrated into the religion. Empress Helena, mother of Roman Emperor Constantine, claimed to have discovered the Holy Rood in the mid-4th century, sparking a fashion for other “discoveries” of fragments of the cross on which Christ had died and pilgrimages of Christians to see these relics.\nPopes of the late 8th and early 9th century sanctioned the practice of pilgrimage to the cemeteries of Rome, which encouraged other churches to create spaces where the remains of their own local dignitaries could be deposited and visited.\nThe cult of saints gave believers a tangible and local connection to the sacred. Rome was far away, and an almighty god in the heavens could seem equally distant: the cult of saints provided people with human intermediaries (saints with local connections) and physical objects (relics) which allowed them to participate in religious experiences rooted in their own localities.\nThe cult of saints born in Celtic regions, embracing the relics of those saints and the sites of their activities, was active in Scotland by the seventh century. Although Adomnán’s biography of Saint Columba (written by 704) may have further promoted the veneration of the saint, it is clear that Colum Cille enjoyed cult status by the time it was written. By the 8th century, the Virgin Mary also enjoyed special reverence by the Columban monastery on Iona. Adomnán’s book De Locis Sanctis “Of Holy Places” (about the holy places of Palestine) may have aided in spreading the practice of endowing hereditary keepers to safeguard holy relics in Gaeldom.\nThe cult of saints seems to have been a much more local affair in Celtic regions than elsewhere: many saints are known at only one site. It was taboo in Cornwall and Wales to relocate the remains of a saint, so the focus of the cult tended to be the site of the grave. In Ireland, by contrast, a saint’s remains could be broken into parts (finger joints, arm-bones, ribs, etc), making it easier to proliferate his/her cult.\nChurches were closely connected to secular power: they were run by people who typically came from powerful dynasties and were financed by the patronage of local leaders. It was to the advantage of such rulers to promote the popularity of the saint(s) whose cult was centred at their church; this could bring them further status, prestige, and authority to justify their secular power; it could also bring in the wealth of other patrons and pilgrims.\nSecular Legal Traditions\nLaw is one of the key elements in ethnic identity: the legal framework and traditions in Celtic communities continued not only to provide common standards of conduct and points of reference to those who shared it, but also provided a contrast with the alien legal systems of outsiders. It is thus no surprise that English kings made great efforts to impose English law on the Celtic peoples in the process of conquest and subjugation.\nThere is at least some small common core of customary law which had been practiced in early Celtic civilizations and survived in some Celtic communities into the medieval period, as evidenced by legal terminology inherited by the different Celtic languages. The Gaelic term for surety (a means of guaranteeing that all parties of a contract will fulfill their obligations) is macc, clearly related to Welsh and Breton mach.\nOne general pattern is that Celtic law placed more importance on restitution than on punishment: in other words, the victim of a crime (which extended to his kin-group) was to be compensated by the criminal (which again could implicate his kin-group). Punishment was reserved for special cases, as it could not usually heal the consequences of the crime or prevent another. However, the status of the individuals involved were also major factors in judgment, as these were societies based on rank, not on social equality.\nThe concept of honour-price [Common Gaelic lóg n-enech “face value”] is fundamental to Celtic law (as in many other law systems); this is the fine to be paid to a victim (or his kin) if any offense was done to him, whether to his honour (e.g., slander) or to his person (injury, murder, etc). Honour-price was generally directly related to social rank, with the king being highest. A person’s honour-price could be diminished if he behaved below his rank, or augmented if he upgraded his skill set or added to his wealth. A dependent (e.g., child, wife, disabled person) did not have an independent honour- price; it was, rather, a fraction of the legally-recognized person to whom they were attached, depending on age, gender, and nature of relationship.\nAnother common feature of Irish and Welsh law systems was the support for divorce initiated by the wife, a right which met with the disapproval of the church. Irish law allowed for polygamy and secular lawyers defended this pre-Christian practice by pointing out its use amongst the Jews in the Old Testament.\nUnlike many legal systems in continental Europe, Irish law was not a legacy of the Roman empire and could not rely upon the help of scholars familiar with the Roman legal tradition; after all, Irish law was written in Gaelic, despite the existence of many contemporary Latin scholars, and draws upon native terminology and concepts (even if it also borrows from Christian texts). One of the truly remarkable aspects of early Ireland, at least from our surviving evidence, is that despite the proliferation of dozens of independently operating kingdoms and a lack of a central authority, the country was surprisingly uniform in both language and culture. Whether law (and associated cultural practices) was the result of this or the cause of this unity, early Irish law is a wonder of early European civilization.\nBy the 7th century, the Irish clergy were involved in the recording and refashioning of customary Irish law, which had been transmitted and practiced orally, into a sophisticated written textual tradition. This allowed for the development of Gaelic law in new directions, but it also provided the ability of literate churchmen to edit and interfere in the legal system. Irish law is often called “Brehon law” in English after the Gaelic word for judge (in plural form) breitheamhan.\nThere are a number of clues about the origins of Irish law which, if they are not actually rooted in the pre-Christian past, at least provide the veneer of antiquity and indigeneity. Many of the principles and precedents of Irish law are attributed to pagan characters, many of which were clearly gods and some of whom represent professions: Bretha Déin Chécht “the judgments of Dian Cécht” related to legal aspects of medicine, Bretha Goibnenn “the judgments of Goibniu” related legal aspects of blacksmiths, and so on. None of the fundamental principles or judgments are named after mere mortals, although Saint Patrick was said in the prologue to one law tract to help transform pagan Irish law to a written tradition. After this act of transcription, the secular poets (filid) were said to have been taken out of the legal profession (no doubt oral law relied upon mnemonic devices such as ornament poetry). Although we cannot read account this as literal historical evidence, it is an important symbolic endorsement of the results of the “harmonization” of Irish law with the Christian order and literacy.\nThere was a complex and sometimes fraught relationship between secular law traditions and the church, who were seeking to modify secular society to meet Christian standards. Some Irish lawmen tried to defend the older laws against the new law (sometime referred cryptically to as the “written law” of the “men of books”). By the 8th century, clerics were trying to impose capital punishment for extreme crimes, whereas the traditional Celtic law system had allowed for the payment of compensation, even for crimes like homicide (éraic).\nEarly Irish law abounds in detail and precision: there is a large number of tracts on very specific issues, although many of these tracts only survive in fragments. Over the generations, the bare statements of laws were elaborated and explained in longer texts which “gloss” the original texts, resulting in strands of interweaving texts (such as that shown in the Book of Aicill above). Our understanding of the laws is complicated by the fact that it evolved independently in different regions under the guidance of different schools of law; thus, variations in usage and terminology appear over the centuries in the surviving records.\nThe same dynasties that wielded secular power were also those that became the leaders of religious institutions, with succession of office determined in the same manner. The honour-price of members of religious orders was calculated by mirroring their position in secular society. Certain legal concepts and aspects of social organization were adopted in the church’s own structure and operation: free/unfree status and clientship, usually applied to people, were extended to churches and relationships between monasteries and churches.19 As it was involved in the creation and operation of law, the church ensured its own rights to wealth (it took a third of the fine paid by a client who neglected the obligations to his lord) and tried to inherit for itself land owned by a landowner who died without leaving male heirs.\nThe legal system included mechanisms for adding or changing laws. A specifically enacted regulation (as opposed to customary law) was originally denoted as cáin (pl. cána), a term which by the late 8th referred to the laws of the church (by the late medieval period it denoted taxes). By the late 8th century, all of the major monasteries were proclaiming their own cána and collecting fines from those who broke these laws.\nLawyers attempted to modify legal procedures in order to minimize conflict and disruption society. The practice of distraint (the seizure of property to force a person to comply with a legal ruling or obligations) was modified from the initial policy of an immediate seizure of goods without notice, to informing the offender that the goods were taken, to giving an advanced warning of the intention to seize the goods, to finally threatening that goods would be impounded and rewarded to the claimant if no resolution was made within a set period of time.\nWhat does Críth Gablach “Branched Purchase” § 61-2 say about the relationship between church and secular society?\nHow does the Prologue to the Senchus Már attempt to resolve pre-Christian Irish law with the new Christian order?\nThe evidence of the Welsh law system, especially from what survives in the later Laws of Hywel Dda (codified in the mid-10th century), depicts Welsh society as\na stable, sedentary arable-based sector in which the extended family or clan-group lived in robust farm-steads. In terms of landholding, the general trend was for partible inheritance to nibble away at large estates (gwelyau) up to the mid-eleventh century. Apart from the slave, who possessed no rights, there were three important categories: the king (brenin), the noble (breyr) and the villein (bilain), who was tied to both lord and land. Within these groups, claims to status depended on descent, kinship, and wealth. (Jenkins, A Concise History)\nLike early Irish law, some texts in early Welsh law also appears to use the kind of linguistic features that would make it suitable for oral preservation and transmission, such as the use of triads. If this, and the evidence of shared terminology derived from a common Celtic source, were not enough to demonstrate that it has a pre-history which is now difficult to detect, there is the fact that some aspects of Brittonic law survived in the kingdom of Alt Clut to be later absorbed into Scottish law (discussed in a later unit).\nRead the Surexit Memorandum extract from the “Lichfield Gospel notes,” the earliest surviving evidence about Welsh law. What legal issues does this deal with? Why might this particular fragment have been recorded in a manuscript and preserved?\nCubitt, “Pastoral care.”\nHall, “Of holy men.”\nKruta, The Celts.\nMeek, The Quest.\nÓ Cróinín, Early Medieval Ireland.']"	['<urn:uuid:e7a40394-c2c8-47c1-91d0-02a3f61533c0>', '<urn:uuid:d6960229-ecf5-497d-9a98-f7b53a85ab11>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T06:15:13.904987	35	91	3784
71	time learn second language english academic achievement link impact	Studies show it takes 1-6.5 years for children to become proficient in English as a second language. During this period, students who are not yet proficient in English are likely to face academic difficulties compared to their peers, as they struggle with learning academic content while simultaneously learning the language. Better English proficiency at school start correlates with higher academic outcomes. This challenge is particularly significant because during second language acquisition, learners may experience interlanguage fossilization - a phenomenon where their linguistic development stalls at an intermediate stage, retaining certain errors despite continued instruction and practice. This can affect their overall academic performance and language mastery.	['The Surprising Link Between English Proficiency And Academic Performance: What This Could Mean For Thai Children In English-Based Schools\nSome bilingual children learn both of their languages from birth, whereas other children learn a second language well after their first. In Thailand, local families speak Thai at home, so children are usually first exposed to English once they attend an English-based school. Children who learn English during their school years face the challenge of learning a new language while trying to keep up with academic content. Some content is likely to be missed during this process . It is commonly assumed that after some time and English exposure, these children just “catch-up”, however, in Thailand where the local language is Thai, this is not necessarily the case.\nThere are several studies which have looked at how learning English at school can impact a child’s performance at school. The findings will surprise you...\nHow long does it take a child to become proficient in English?\nStudies have shown that it can take a child anywhere between 1- 6.5 years of exposure to English to become proficient in English . This is also influenced by other factors such as family, education, social and individual factors [3.]\nHow does English proficiency impact a child’s academic performance?\nAn Australian study published last year found that school-aged children who are not yet proficient in English are likely to face academic difficulties compared to their peers . This study also found that children who had better English proficiency when starting school achieved higher academic outcomes overall towards the completion of primary school .\nWhat do these findings mean?\nThese findings suggest that children who first learn English through exposure at an English-based school, could struggle with learning academic content at school for up to 6.5. years. These findings were based on populations living in countries such as Australia and the US, where the main language is English to begin with. However, in a country like Thailand, where the local language spoken is Thai, this could take even longer.\nAs a result, it is likely that local Thai children might be over-identified as requiring Speech Therapy during their school years. I often find myself teaching children concepts in English which I then realize during the session that they already have sound knowledge of in Thai (e.g. ‘past’ and ‘future’ tense). In these cases, the child’s underlying difficulty is not language, but specifically English. Thus, an ESL program is often more suitable for these children.\nHow can we use this evidence practically?\nChildren who are learning English at school would benefit from the use of strategies such as modeling, repetition and emphasis in the classroom. These strategies will help support their understanding of instructions and material presented in class. Once their understanding is supported, their performance when completing tasks will likely improve.\nSeveral children who learn English at school require individualized programs to be tailored to support their learning and performance at school. Before a program can be recommended, there needs to be a strong understanding of the child’s Thai language skills. If the child has adequate Thai language skills, then they can be enrolled in an ESL program. However, if a child is having difficulty with both Thai and English, Speech Language Therapy is recommended, as there is likely to be underlying language impairment.\nThere is a greater need for spreading information about the benefits of being bilingual as well as the difficulties a child is likely to face if they are expected to learn English while at school. This might encourage local parents to expose their children to both Thai and English from a young age if they intend to send them to an English-based school in the future.\nAttending day-care or pre-nursery is a fantastic way for children to become exposed to English from a young age. Attending playgroups or arranging play-dates with other English-speaking children will also help.\nLocal Thai children who first learn English at school are likely to face difficulties keeping up with academic content. Increasing exposure to English from a young age is likely to help with this. Certain classroom modifications and programs such as ESL can be provided for these children during their school years.\n1. Macswan, J., & Pray, L. (2005). Learning English bilingually: Age of onset of exposure and rate of acquisition among English language learners in bilingual education program. Bilingual Research Journal, 29, 653-678.\n2. Brinkman, S. et al. (2009). Population monitoring of language and cognitive development in Australia: The Australian early development index. International Journal of Speech Language Pathology, 11, 419-430.\n3. Halle, T. et al. (2012). Predictors and outcomes of early versus later English language proficiency among English language learners. Early Childhood Research Quarterly, 27, 1-20.\n4. Dennaoui, K., Nicholls, R. J., O’Connor, M., et al. (2016). The English proficiency and academic language skills of Australian bilingual children during the primary school years. International Journal of Speech-Language Pathology, 18: 157-165.\nWelcome to my blog!', 'Interlanguage fossilization is a phenomenon of second language acquisition in which second language learners develop and retain a linguistic system, or interlanguage, that is self-contained and different from both the learner’s first language and the target language. This linguistic system has been variously called interlanguage, approximative system, idiosyncratic dialects, or transitional dialects.\nAccording to Corder  this temporary and changing grammatical system, interlanguage, which is constructed by the learner, approximates the grammatical system of the target language. In the process of second language acquisition, interlanguage continually evolves into an ever-closer approximation of the target language, and ideally should advance gradually until it becomes equivalent, or nearly equivalent, to the target language. However, during the second language learning process, an interlanguage may reach one or more temporary restricting phases when its development appears to be detained;. A permanent cessation of progress toward the target language has been referred to as fossilization. This linguistic phenomenon, interlanguage fossilization, can occur despite all reasonable attempts at learning. Fossilization includes those items, rules, and sub-systems that second language learners tend to retain in their interlanguage, that is, all those aspects of interlanguage that become entrenched and permanent, and that the majority of second language learners can only eliminate with considerable effort. Moreover, it has also been noticed that this occurs particularly in adult second language learners’ interlanguage systems.\nSelinker suggests that the most important distinguishing factor related to second language acquisition is the phenomenon of fossilization. However, both his explanation that “fossilizable linguistic phenomena are linguistic items, rules, and subsystems which speakers of a particular native language will tend to keep in their interlanguage relative to a particular target language, no matter what the age of the learner or amount of explanation or instruction he receives in the target language” and his hypotheses on interlanguage fossilization are fascinating in that they contradict our basic understanding of the human capacity to learn. How is it that some learners can overcome interlanguage fossilization, even if they only constitute, according to Selinker, “a mere 5%”, while the majority of second language learners cannot, ‘no matter what the age or amount of explanation or instruction’? Or is it perhaps not that they cannot overcome fossilization, but that they will not? Does complacency set in after second language learners begin to communicate, as far as they are concerned, effectively enough, in the target language, and as a result does motivation to achieve native-like competence diminish?\nThe concept of fossilization in SLA research is so intrinsically related to interlanguage that  considers it to be a fundamental phenomenon of all SLA and not just to adult learners. Fossilization has received such wide recognition that it has been entered in the Random House Dictionary of the English Language (1987). Selinker’s concept of fossilization is similar to that of  and  all of whom attempted to explore the causes of fossilization in second language learners’ interlanguage.\nFossilization has attracted considerable interest among researchers and has engendered significant differences of opinion. The term, borrowed from the field of paleontology, conjures up an image of dinosaurs being enclosed in residue and becoming a set of hardened remains encased in sediment. The metaphor, as used in SLA literature, is appropriate because it refers to earlier language forms that become encased in a learner’s interlanguage and that, theoretically, cannot be changed by special attention or practice of the target language. Despite debate over the degree of permanence, fossilization is generally accepted as a fact of life in the process of SLA.\nMany researchers have attempted to explain this. Workers have attempted to discover: 1) why fossilization occurs  2) the precipitating conditions  3) what kind of linguistic material is likely to be fossilized  and 4) what type of learners are more prone to fossilize. However, there has been almost no investigation by SLA theorists on the possibilities of preventing or overcoming fossilization, and little explanation related to those adult second language learners who overcome one or more ‘areas of stability’ in interlanguage—those learners whose interlanguage does not fossilize, and who reach a high level of proficiency in the second language ).\nOne factor of obvious relevance is motivation, and studies have been conducted regarding motivation to learning second language, and the relationship of fossilization to the learner’s communicative needs. Arguments have emerged regarding adult learners’ general lack of empathy with target language native speakers and culture. According to, adults do not have the motivation to change their accent and to acquire native-like pronunciation. Unlike children, who are generally more open to target language culture, adults have more rigid language ego boundaries. Thus, adults may be inclined to establishing their pre-existing cultural and ethnic identity, and this they do by maintaining their stereotypical accent. Notwithstanding this, there is a lack of needed research, particularly regarding achievement motivation, especially considering that fossilization can be considered the most distinctive characteristic of adult SLA.\nThe text of this article is taken with permission from The Role of Achievement Motivation on the Interlanguage Fossilization of Middle-Aged English-as-a-Second-Language Learners by Dr. Zoran Vujisić (2007).\n- Nemser, 1971\n- Corder, 1971\n- Schumann, 1975\n- Omaggio, 2001\n- Selinker & Lamendella, 1980\n- Tarone (1976)\n- Sridhar (1980)\n- Adjemian, 1976\n- Corder, 1978\n- De Prada Creo, 1990\n- Nakuma, 1998\n- Schumann, 1976\n- Schumann, 1978a\n- Schumann, 1978b\n- Schumann, 1990\n- Naiman, et al., 1996\n- Seliger, 1978\n- Stern, 1975\n- Virgil & Oller, 1976)\n- Selinker & Lakshamanan 1992\n- Todeva, 1992\n- Scovel, 1969\n- Scovel, 1978\n- Scovel, 1988\n- Scovel, 2000\n- Selinker, Swain & Dumas, 1975\n- Virgil & Oller\n- Acton, 1984\n- Birdsong, 1992\n- Bongaerts, et al., 1997\n- Ioup, Boustagui, El Tigi, & Mosell, 1994\n- Gardner, 1988\n- Gardner & Smythe, 1975\n- Nickel, 1998\n- Ushioda, 1993\n- Guiora et al. (1972)\n- Guiora et al., 1972\n- Acton, W. (1984). Changing fossilized pronunciation. TESOL Quarterly, 18, (1), 71-85.\n- Adjemian, C. (1976). On the nature of interlanguage systems. Language Learning, 26,(2), 297-320.\n- Birdsong, D. (1992). Ultimate attainment in second language acquisition. Aneuaee, 68, (4), 706-755.\n- Bongaerts, T. (1999). Ultimate attainment in L2 pronunciation: The case of very advanced late L2 learners. In David Birdsong (Ed.), Second language acquisition and the critical period hypothesis. Mawah, NJ: Lawrence Erlbaum Associates.\n- Corder, S. P. (1971). Idiosyncratic dialects and error analysis. IRAL, 9, (2), 147-160.\n- Corder, S. P. (1978). Language-learner language. In J. C. Richards (Ed), Understanding second and foreign language learning (pp. 71–92). Rowley, MA: Newbury House.\n- Corder, S. P. (1981). Error analysis and interlanguage. Oxford: Oxford University Press.\n- De Prada Creo, E. (1990). The Process of fossilization in interlanguage. (Paper presented at the 9th annual meeting of the World Congress of Applied Linguistics, sponsored by the International Association for Applied Linguistics, Thessaloniki, Greece, April 15?25, 1990).(ERIC Document Reproduction Service No. ED 362 012).\n- Gardner, R. C. (1988). Attitudes and motivation. Annual Review of Applied Linguistics, 9, 135-148.\n- Gardner, R. C., & Smythe, P. C. (1975). Motivation and second language acquisition. Canadian Modern Language Review, 31, (3), 218-230.\n- Guiora, A., Beit-Hallahmi, B., & Brannon, R. (1972). The effects of experimentally induced changes in ego status on pronunciation ability in a second language: An exploratory study. Comprehensive Psychiatry 13, 421-428.\n- Han, Z. (2004). Fossilization in Adult Second Language Acquisition. Clevedon: Multilingual Matters.\n- Ioup, G., Boustagui, E., El Tigi, M., & Mosell, M. (1994). Reexamining the critical period hypothesis. Studies in Second Language Acquisition, 16, 73 – 98.\n- Naiman, N., Frohlich, M., Stem. H. H., & Todesco, A. (1996). The good language learner. Clevedon, Avon. England: Multilingual Matters.\n- Nakuma, C. (1998). A new theoretical account of “fossilization”: Implications for L2 attrition research. IRAL, 36, (3), 247-257.\n- Nemser, W. (1971). Approximative systems of foreign language learners. IRAL, 9, (2), 115-124.\n- Nickel, G. (1998). The role of interlanguage in foreign language teaching. IRAL, 35, (1), 1-10.\n- Omaggio, A. (2001). Teaching language in context. Proficiency oriented instruction. (3rd ed.). Boston, MA: Heinle & Hainle Publishers.\n- Schumann, J. H. (1975). Affective factors and the problem of age in second language acquisition. Language Learning 25, 205-235.\n- Schumann, J. H. (1976a). Second language acquisition research: Getting a more global look at the learner. In Brown, H. (Ed.), Papers in second language acquisition, language learning. Special Issue 4. Ann Arbor, MI: Michigan State University.\n- Schumann, J. H. (1976b). Second language acquisition: the pidginization hypothesis. Language Learning, 26, (2), 391-408.\n- Schumann, J. H. (1978a). Social and psychological factors in second language acquisition. In J. C. Richards (Ed.), Understanding second & foreign language learning (pp. 163–178). Rowley, MA: Newbury House Publishers.\n- Schumann, J. H. (1978b). The pidginization process: A model for second language acquisition. Rowley, MA: Newbury House.\n- Schumann, J. H. (1990). Extending the scope of the acculturation/pidginization model to include cognition. TESOL Quarterly, 24, 667-684.\n- Scovel, T. (1969). Foreign accents, language acquisition, and cerebral dominance. Language Learning, 19, (3 & 4), 245-253.\n- Scovel, T. (1978). The effect of affect on foreign language learning: A review of the anxiety research. Language Learning, 28, (1), 129-142.\n- Scovel, T. (1982), Questions concerning the application of neurolinguistic research to second language learning/teaching. TESOL Quarterly 16, 323-331.\n- Scovel, T. (1988). A time to speak: A psycholinguistic inquiry into the critical period for human speech. New York, NY: Newbury House/ Harper & Row.\n- Scovel, T. (2000). A critical review of the critical period research. Annual Review of Applied Linguistics, 20, 213-223.\n- Selinker, L. (1972). Interlanguage. IRAL, 10, (3), 209-231.\n- Selinker, L., & Lakshamanan, U. (1992). Language transfer and fossilization: The “Multiple Effects Principle”. In S. M. Gass, & L. Selinker (Eds.), Language transfer in language learning (pp. 197–216). Amsterdam: John Benjamins.\n- Selinker, L., & Lamendella, J. T. (1980). Fossilization in interlanguage learning. In K. Croft (Ed.), Reading on English as a second language (pp. 132–143). Boston. MA: Little, Brown and Company.\n- Selinker, L., Swain, M., & Dumas, G. (1975). The interlanguage hypothesis extended to children. Language Learning, 25, (1), 139-152.\n- Sridhar, S. N. (1980). Contrastive analysis, error analysis, and interlangauge. In Croft, K. (Ed.), Readings on English as a second language. Cambridge, Mass: Winthrop.\n- Stern, H. H. (1975). What can we learn from the good language learner? The Canadian Modern Language Review, 31, (4), 304-318.\n- Tarone, E. (1976). The phonology of interlanguage. In J.C. Richards (Ed.),Understanding second and foreign language learning: Issues and approaches. Rowley, MA: Newbury House Publishers.\n- Todeva, E. (1992). On fossilization in SLA theory. PALM (Papers in Applied Linguistics –Michigan), 7, 216-254.\n- Ushioda, E. (1993). Acculturation theory and linguistic fossilization: A comparative case study. CLCS Occasional Paper No. 31. Dublin, Ireland: Centre for Language and Communication Studies; 56pp. (ERIC Document Reproduction Services No. ED 368 172).\n- Vigil, N. A., & Oller, J. W. (1976). Rule fossilization: a tentative model. Language Learning, 26, (2), 281-295.']	['<urn:uuid:efc51235-e3e6-4e42-9cca-e2bb4fcf0429>', '<urn:uuid:2610bb28-e0ac-40ac-996d-c7da87f7c39c>']	open-ended	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-13T06:15:13.904987	9	106	2653
72	ways to stay cool inside homemade cardboard costume temperature control	To manage temperature inside a cardboard costume, you can use chemical ice packs which provide instant cooling without condensation issues. Since cardboard is an excellent insulator, the cool temperatures are maintained well. Additionally, you can install two personal battery-operated fans fastened with velcro on the inside of the costume. The fans can recirculate the cold air from the ice packs throughout the costume. This cooling system can be set up with a $4 ice pack and two $12 fans from Amazon.	"[""First off, I need to thank my wife for being patient with me as I transformed our apartment into an utter mess of cardboard pieces, stinking it up with burning glue and paint and long hours of me talking and obsessing over the smallest details of this costume. She is truly a champion and is worthy of a whole new level of praise for putting up with this. I’d also like to thank dannyeurena\n. Because he posted his method of building his costume, I was able to save so much time and expand on the ideas he presented. Thanks!\nThis year I wanted to making a really sick costume. I'm an artist and I wanted to push myself. I also didn't want to spend a lot of money on it either. After days of pondering what I wanted to be, I decided on Soundwave—not the latest crumpled soda can Soundwave from the Bay movies, the original toy/cartoon one. Hands down he was my favorite toy. He had a rocket launcher and came with his own army of minions. This robot was upper management. After some research of various depictions, and fabrications, I began building.\nStep 1: Materials\nThe following list are materials that I used:\n- Cardboard (thick) Make sure it is rigid\n- Cardboard (thin)\n- 2” Packaging tube\n- Paper towel tube\n- (1) Metal ruler\n- (1) Triangle ruler\n- (1) Large matte knife with 6 blades\n- (1) Small x-acto knife with about 25 blades\n- (1) Pair of scissors\n- (1) Medium sized cutting board\n- (1) small tape measure\n- (1) Glue gun with (24) glue sticks (This is a MUST!)\n- Bandaids when you burn yourself with the glue gun\n- (6) Rolls of packaging tape\n- (1) Roll of blue masking tape\n- (1) Construction helmet\n- (1) Can of spray primer\n- (1) Can of house paint (white)\n- (1) Can of spray glue\n- (2) Cans of blue spray paint\n- (2) Cans of silver spray paint\n- Various color acrylic paints (cheaper the better; blue, yellow, red, white, black, silver, gold)\n- Package of various sized brushes\n- Masking paper\n- (1) Cheap pair of shoes (or use your own they won’t be damaged)\n- (1) Backpack (won’t be damaged)\n- (1) Gumball container\n- (1) Reading light\n- (2) Glow sticks\n- (1) Yard of black stretchy fabric\n- (1) 1” strip of galvanized steel\n- (1) roll of adhesive cork\n- (1) Translucent yellow folder\n- Velcro (about a yard of it)\n- (3) yards of 1” nylon straps\n- Elastic waistband\n- (1) pair of hockey gloves\n- (1) small sheet of framing plexi (used to replace glass)\n- Access to print services that can print on 11x17 paper\n- Pop rivet gun with 1/8 rivets\n- (1) sheet of 1” vinyl letters\n- (7) plastic easter eggs\nStep 2: Legs\nFor this part, I made a dummy leg so that I could adjust it and modify until I got it the way I wanted them. I walked around, I sat and made sure I could spend some time in them comfortably. Then I took that mold, cut it apart and used it as a die to make the legs I was going to use for the costume. This is where I used the various rulers to take measurements and to make sure all my right angles were spot on. To make corner bends in the cardboard, first cut the top layer with an x-acto knife, then use a pen and push the tip into the cut to roll the edges, fold and done.\nNow that I have the legs built, I started to add in the little details such as the paper towel roll, peeling the first layer of cardboard to make vents, and using some of the plexi to make lights that will be lit by glowsticks. See Step 3\nOnce all the details are painted, I added the vinyl letters.\nStep 3: Leg Lights\nI wanted to make lights on this costume because I’m stupid for details. For the REC/BATT light, I took a small gumball container and a reading light I had. Combined with the red top of a seasoning container, I made a red battery powered fake LED light. For the light in the side-skirts of the legs, I made a removable tray that I can load with a glowstick. It’s not the greatest lighting effect, but it gets the job done and it’s cheap.\nStep 4: Feet\nThese were pretty easy. Looked at the shape and cut them out. Used the cardboard tubes on the sides to keep the legs from dragging on the ground. Also they make a perfect pivot point for the ankles, and since it’s not closed on the other end, I can tilt my ankles with no problems. To reinforce them, the outside of the foot has the hole cut into it. There is another piece of cardboard on the inside that keeps it from moving further in and reinforces the foot. With reinforcement from the glue gun, they are not going anywhere. I also took foam and closed off the spare space around a shoe that I can easily slip on and off with little difficulty.\nStep 5: Arms & Hands\nNot everything goes to plan. The first version I built was more for movement at the expense looks. It was a detached shoulder that after testing out, would look horrible. So with the help of dannyeurena and his Optimus Prime costume, attaching the shoulders to the body was the way to go. I rebuilt the arms with a new hinge that is less visible—one made of 1 inch galvanized steel that is pop riveted together. The sheet metal forms a “Z” so that it locks into the arm and forearm and is pop riveted into the cardboard with a washer. Painting was the same. For the extenders that pop the hands out on the outside of the forearm, I used more of the peeled off cardboard technique. For the hands, I bought a pair of used hockey gloves off Ebay.\nStep 6: Missile Launcher\nI cut out six inch circles and wrapped them in chipboard. You can get chipboard at any print shop. (it’s the same stuff on the back of notepads) I glued seven plastic easter eggs tops on the front and painted to cover up the seams. Because my shoulders are not nearly as wide as the toy, I had to angle out the arm that connects the missile launcher to the body and make a support block that rests over the right shoulder.\nStep 7: Body (Lower)\nI first sketched out the design of the body. I needed it to telescope so that I could have more maneuverability. Once I flushed out all my ideas, I built the lower torso and designed the upper around it. For the lower torso, I started to add in the details such as the buttons and the hips. I also fashioned it with suspender style straps and can move up and down freely from the body. The backside of it also slides up and down independently so that I can sit down comfortably and stand without my backside exposed.\nStep 8: Upper Legs\nUpper legs are pretty simple. I cut them to shape, and on the inside, sewed stretchy fabric so that they can adjust to my legs, whether bent or straight. To get the fabric attached to the cardboard, I sewed it to the front then to a strip of cardboard that was glue gunned to the inside of the back of the leg. To wear them, I slide my leg in and they have an adjustable velcro strap that attaches to a belt to stay in place.\nStep 9: Head\nI took a construction helmet and started adding on details. This is where I use the thinner cardboard. The first attempt I didn’t like how high I angled the crown. I was too focused on keeping it small and tight to my face, that I overlooked the crown. I later went back and adjusted it to my liking. Additionally, I added in more details like the circles on the crown (chipboard) and the vents. Once primed and painted, I cut a translucent yellow folder for my visor and some painted details for the finishing touches.\nStep 10: Body (Upper) & Shoulders\nI had a difficult time getting into the body, period. I know that the shoulders attached would make it even more problematic. I designed the shoulders to slide into place through a channel to make getting into the body easier. Additionally, I put a channel in the chest piece so that I could slide a plexiglass sheet with the picture of a tape on it, much like a picture frame. For me, it didn’t look the way I wanted it to. So I cut the chest piece back about an inch and a half, and decided to cut the whole body into two pieces making it easier to get on and off. I also incorporated a backpack to help keep the body from moving around on me since wearing it, it would constantly slide back To keep the two halves together, the channel made by the shoulder pieces served to keep the costume closed. I also made four velcro tabs on the inside so that it could sandwich together. Additionally, the missile launcher also helps to keep the two halves together. The costume has a storage compartment in the back for the backpack to hide in, since I didn’t want to destroy it. Once painted, I printed out the tape graphic and spray glued it to the chest. The logo was spray glued to the inside of the plexi.\nStep 11: Painting methods\nI tried all sorts of methods to cover the seams of the cardboard. I tried spray glue and wrapping paper to cover seams, sanding the packaging tape seams, but none of these methods worked right. What I found to work and be reasonably fast was to first cover all the edges in packaging tape. Then use the spray primer on the tape. Once this is dry, use house paint to help conceal the edges of the tape. The paint gives it a texture that helps to conceal it. I used a standard paint roller to give it an even texture. I gave it two coats. Once all of these are dry, I applied a base coat of blue, and let that dry. Then I applied the silver. Afterwards, I used the masking tape and acrylic paints to add in all the decal effects. To save money, I cut the blue masking tape into little strips with my x-acto and ruler, since for stenciling, you don’t need that much. the less the better. It’s less likely to pull up and of your other painted details.\nStep 12: Cooling\nOf all the cardboard costumes I've seen, very few people address just how hot it can get wearing one of these. Cardboard is an excellent insulator. This can work in your favor or against you. To make it work for you, finding a way to keep the temperature inside the costume cool is crucial, and not as hard as it might seem. A little goes a long way. To keep the temperature inside manageable, I used chemical ice packs. They not only get instantly cool, but you can avoid the condensation mess, and because you are in a insulated box, the cooler temperatures go a long way. In addition to the chemical ice packs, I have two personal battery fans that are fastened with velcro on the inside of the costume. The cold air from the ice pack would get recirculated into the costume through the fans. The ice pack cost $4, and I got the fans from Amazon.com for $12 a piece.""]"	['<urn:uuid:c3ba0a65-8e0a-460c-b70c-843fe2702ebe>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	10	81	1992
73	acoustic signal processing marine environment applications	In marine environments, MIMO SONAR systems process acoustic signals using multiple sending and receiving hydrophones, implementing beamforming and real-time processing techniques to detect and track targets. Meanwhile, the natural acoustic environment contains vital biological signals used by marine life for survival and reproduction, but these communications are increasingly masked by anthropogenic noise from vessels and industrial activities, which has become so pervasive it is now recognized as a significant pollutant requiring monitoring under both European and US environmental regulations.	['Cognitive systems have drawn the attention of researchers since several years. Different approaches have been proposed for RADAR as well as communication applications and recently also for SONAR systems. In contrast to a classical SONAR system the operator does not need to adjust the system parameters to account for changes in the environment or consider the potential type of the target (e.g. mammal, diver, submarine, etc.) to achieve an increased performance of the system regarding detection and tracking. Cognitive systems are capable to achieve such a performance gain without the assistance of a human being but essentially due to a feedback loop of the receive signal processing to the transmit signal processing. The cognition of the system is achieved by a control unit which extracts the relevant information and adapts the parameters of the system according to a certain pattern.\nThis promising approach is extended by the idea of MIMO signal processing. The usage of this, in the field of SONAR systems novel technique, reveals a vast number of possible improvements which are subject to research at the chair of digital signal processing and system theory. As the chair has dedicated himself to real-time processing the above mentioned theoretical approaches are finally fit into our own real-time capable framework KiRAT.\nIn this project, a MIMO system, consisting of a multitude of sending and receiving hydrophones as well as a PC-based signal processing hardware, is realized. In a first stage, the received hydrophone signals are processed by a multitude of beamformers. Each beamformer output signal is correlated with the send-signal sequence without modifications as well as with several temporally compressed and stretched versions. By analyzing the resulting three-dimensional correlation matrix using a CFAR detection algorithm, the angle(s) of arrival, the radial distance(s), and the radial target speed(s) can be estimated. In the picture on the top you see the graphical user interface that was implemented to visualize the detection results.\nIn order to improve the detection rate, a basic tracking algorithm based on Kalman filtering and multi-hypothesis tracking, is implemented. Tracked targets are distinguished between “interesting” and “non-interesting” (e.g. stationary, known) targets by a dedicated decision unit. In order to improve the detection of weak ”interesting” targets masked by strong ”non-interesting” targets, a sending beamformer is used to block the emission of sound into the direction of the ”non-interesting” targets. Due to the drawback of becoming blind towards some directions, blocking and omni-directional send modes are used alternatingly. In order to achieve a sufficient real-time performance, extensive frequency-domain processing and processor-specific code optimization are utilized.\nBeside that multi-channel system described above we also investigate single-channel systems that are used for either classifying received sounds stemming from marine mamals or for speech communcation between e.g. divers.\nB. Kaulen, A. Namenas, G. Schmidt: Entwurf und Implementierung einer kognitiven Steuereinheit für ein Echtzeit-MIMO-SONAR-System, Proc. DAGA, Germany, 2019\nA. Namenas, T. Kaak, T. O. Wisch and G. Schmidt: Zieldetekion und Tracking mit einem Echtzeit-MIMO-SONAR-System, Proc. DAGA, Germany, 2018\nT. Kaak, and G. Schmidt: An Introduction to Real-time Cognitive SONAR Systems Utilizing Novel MIMO Approaches, Proc. DAGA, Kiel, Germany, open access, 2017\nT. Claussen, V. D. Nguyen, U. Heute, G. Schmidt: A Real-time Cognitive-Sonar System for Diver Detection, MTS/IEEE Oceans 2015, Oktober 19-22, 2015, Washington D.C., USA\nT. Claussen, V. D. Nguyen: Real-time Cognitive Sonar System with Target-optimized Adaptive Signal Processing through Multi-layer Data Fusion, IEEE 2015 International Conference on Multisensor Fusion and Integration for Intelligent Systems, September 14-16, 2015, San Diego, USA\nT. Claussen, V. D. Nguyen, U. Heute, G. Schmidt: A Real-time MIMO System for Sonar Applications, Proc. DAGA 2015, March 16-19, 2015, Nuremberg, Germany', 'Aquatic habitats are no “silent world”; they are filled by several, different sound sources. Waves and water turbulences are just some example of physical sources of sounds, but physical sources are not the only ones. An immense variety of aquatic animals depends on sounds for activities that are crucial for their survival, and their vocalisations are called by scientists “biophony” (the symphony of life). Marine mammals are the ocean’s most famous singers, but fish vocalise too. More than 800 species of fish depend on sounds to survive and reproduce. Fish sound production is especially conspicuous during the reproductive season and is typically related to agonistic interactions and mating activities.\nIn the last fifty years, human activities have radically changed aquatic environments by adding incredibly numerous sources of noise. Small boats, commercial boats, seismic exploration, military activities, windfarms, pile driving…anything but a “silent world”, we are actually creating an ocean of noise. The most common man-made source of noise is shipping. The rumble of engines, propellers, and other commercial shipping noise can be heard in virtually every corner of the ocean. In a world in which 80% of global trading takes place at the sea, the number of merchant ships has tripled in the last 75 years and the number of recreational boats keeps increasing as well. Vessels noise in coastal area has become a form of chronic, constant pollution.\nAnthropogenic noise is now recognised as a significant pollutant in the marine environment, and the potential consequences for animal survival are of international concern. Some species may acclimatise to noise after chronic exposure, others may struggle to develop and survive. Considering the global extent and the wide range of effects of noise pollution on aquatic life, man-made noise has been identified as a target for the monitoring of a good quality coastal environment in both Europe and USA (e.g. inclusion in the US National Environment Policy Act and in the European Commission Marine Strategy Framework Directive).\nBut which type of effects can noise have on aquatic life? If they can, animals try to escape from noise, possibly giving up to other important activities, such as feeding and reproduction. For example, avoidance reactions to vessel noise have been noticed in several fish species such as herring, cod, rudd and roach. And if they can’t escape, physiological effects can be expected. Exposure to noise causes stress to several fish species: their heart bits quicker, they move more and their stress hormones increases. Anthropogenic noise can also have other physiological effects. The brown shrimp, for example, grows less, reproduce less and its mortality rate increases; other fish species grow less and produce less viable eggs.\nWhen the situation gets louder, an immediate physiological response has been noticed in several species: a temporary threshold shift. Is basically what happens to us after spending a night out in a really loud place, like a nightclub. For a short period of time, we do not hear very well: this happens because our hearing threshold has shifted a little bit up, meaning that the exposure to noise has temporarily compromised our hearing capabilities. Of course, if the noise is too loud or the time of exposure is prolonged, aquatic animals suffer of a permanent hearing damage: in other words, their hearing ability is permanently impaired. Sometimes, with very loud sounds like the one used in military activities and in seismic surveys, this damage can be so severe to make animals literally deaf or, worst, death. The continuous and increasing level of anthropogenic noise can also make impossible for animals to hear important signals, like predator noises or mate calls. As it happens to us when we try to speak in a noisy place, animals in a noisy environment simply do not hear each other and the communication is difficult, if not impossible. And if the communication was oriented to finding a mate…this could not lead to an unhappy ending…\nRaising our awareness to the problem of underwater noise pollution can help us lower man-made volume, in the oceans and in freshwaters. Aquatic animals would surely thank us for it!\nWant to learn more?\nCheck the video we produced, give a look to the reference lists below and…help us spreading the voice!\nProceedings of Meetings on Acoustics (4th International Conference on the Effect of Noise on Aquatic Life, 2016. All manuscripts are open-access): http://asa.scitation.org/toc/pma/27/1?expanded=27\nBuscaino, G., Filiciotto, F., Buffa, G., Bellante, A., Di Stefano, V., Assenza, A., Fazio, F., Caola, G. & S. Mazzola. 2010. Impact of an acoustic stimulus on the motility and blood parameters of European sea bass (Dicentrarchus labrax L.) and gilthead sea bream (Sparus aurata L.). Marine Environmental Research 69: 136–142.\nCodarin, A., Wysocki, L.E., Ladich, F. & M. Picciulin. 2009. Effects of ambient and boat noise on hearing and communication in three fish species in a marine protected area (Miramare, Italy). Marine Pollution Bulletin, doi:10.1016/j.marpolbul.2009.07.011.\nJasny, M., Reynolds, J., Horowitz, C. & A. Wetzler. 2005. Sounding the depths II: the rising tools of sonar, shipping and industrial ocean noise on marine life. Natural Resources Defence Council Report 2005, New York, USA. 84 pp.\nMcCauley, R.D., Fewtrell, J. & A.N. Popper. 2003. High-intensity anthropogenic sound damages fish ears. Journal of Acoustic Society of America 113:638–642.\nSlabbekoorn, H., Bouton, N., van Opzeeland, I., Coers, A., ten Cate, C., & A.N. Popper. 2010. A noisy spring: the impact of globally rising underwater sound levels on fish. Trends in Ecology & Evolution 25(7): 419-427.\nTasker, M.L., Amundin, M., Andre, M., Hawkins, A., Lang, W., Merck, T., Scholik- Schlomer,A.,Teilmann, J., Thomsen, F., Werner, S. & M. Zakharia. 2010. Marine strategy framework directive. Task Group 11 Report. Underwater noise and other forms of Energy. JRC Scientific and Technical report. http://www.ices.dk/projects/MSFD/TG11final.pdf\nVasconcelos, R.O., Amorim, M.C.P. & F. Ladich. 2007. Effects of ship noise on the\ndetectability of communication signals in the Lusitanian toadfish. Journal of Experimental Biology, 210: 2104–2112.\nWysocki, L.E., Dittami, J.P. & F. Ladich. 2006. Ship noise and cortisol secretion in\nEuropean freshwater fishes. Biological Conservation 128: 501–508.']	['<urn:uuid:3cefd579-913a-4716-a69f-50abb1ef0105>', '<urn:uuid:05f576a0-1920-4230-b88e-7ead0f460c40>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	6	79	1600
74	limonene versus isopropyl thermal properties boiling melting comparison	D-limonene and isopropyl alcohol have different thermal properties. While specific melting and boiling points for d-limonene are not provided in the documents, isopropyl alcohol has well-defined thermal properties, with a melting point of -89°C and a boiling point of 82.6°C. Both compounds are thermally sensitive - molecular distillation of d-limonene requires very low temperatures (around 22-30°C) to prevent thermal damage, while isopropyl alcohol requires careful handling due to its flammable nature with a flash point of 11.7°C (open cup) and autoignition temperature of 399°C.	"['versión impresa ISSN 0327-0793\nLat. Am. appl. res. vol.41 no.1 Bahía Blanca ene. 2011\nD-limonene and geranial fractionation from lemon essential oil by molecular distillation\nP. C. Rossi, A. A. Willnecker, J. Berti, A. V. Borgarello, G. N. Mezza and M. C. Pramparo\nFac. de Ing. Univ. Nac. de Río Cuarto, Ruta 36 Km 601, 5800 Río Cuarto, Córdoba. Argentina.\nAbstract - D-limonene and geranial are, respectively, the most abundant terpenic and oxygenated compounds found in lemon essential oil. The main objective of this research work is to study the technical feasibility of molecular distillation, in order to separate and concentrate those thermal labile compounds of lemon essential oil and to determine the best evaporation temperature and feed flow rate values which will lead to high separation efficiency. The highest temperature analyzed allowed to obtain a residue poor in d-limonene and enriched in geranial, with low geranial yield (between 35-50%). Regarding d-limonene, the highest temperature (30 °C) used, led to higher yields of d-limonene. Lower feed flow rate (0.6 ml min-1) led to low concentrations of d-limonene (320 g kg-1) and geranial (70 g kg-1) in the residue, with low yield for geranial (23.5%).\nA high yield of geranial in the residue (76.4%) can be obtained by using a feed flow rate of 1.3 ml min-1, which leads to the highest geranial concentration (113 g kg-1 ).\nKeywords - Essential Oils; Lemon; Molecular Distillation; D-limonene; Geranial.\nIn Argentina, there is a significantly important ""citrus"" fruits production, from which different products are obtained - mainly juices. Essential oils are the most important by-products of juice production, and they are extracted from the ""citrus"" peelings. Nowadays, the production of natural essential oils is being encouraged; therefore there seems to be a brilliant future concerning this economical activity (Bruzone, 2003; Lawrence and Reynolds, 1999).\nLemon essential oil is a product of particular interest for Argentina, since it is placed in a relevant spot in the international markets. Besides that, it constitutes a versatile product since it is used as a flavored and scented agent in cosmetic, pharmaceutical and foodstuff industries.\nOils obtained from different ""citrus"" fruits have in common a high amount of terpenes, which are volatile-low molecular weight compounds. The most frequent terpene found in lemon oil is d-limonene, which constitutes around 700 g kg-1 of the oil weight. The remaining part of it consists of low to medium molecular weight aldehydes, unsaturated aldehydes, ketones, esters and alcohols. Given that, d-limonene can be considered as a primary solvent for scented compounds present in lemon essential oil (Buccellato, 2000).\nAmong the chemical substances found in the essential oil, some of them have nutraceutical properties (Miyake et al., 1998). Some isoprenoids such as d-limonene and others such as farnesol, tocotrienol and geranial have been evaluated on their chemo-protective activity. When administered to rats, dogs and humans at levels between 0.1 and 5% tumor genesis was suppressed by a direct action over carcinogens and pre-carcinogens that require activation. D-limonene was found to be the most effective chemo-protector (Wildman, 2001).\nGiven these important properties of the compounds found in lemon essential oil, it is necessary to study beneficial techniques of separation and concentration that allow obtaining more valuable products. Several methods exist for processing valuable compounds present in essential oils. Concentration can be carried out by vacuum fractional distillation, extraction using oxygenated solvents with diluted alcohol or other solvents, or dragging steam distillation. Despite the fact that acceptable separations are obtained using these methods, they present some disadvantages such as the formation of degradation products due to the high operation temperatures, or the presence of solvent traces in the final product (Sinclair, 1984; Haypek et al., 2000; Pino and Sanchez, 2000; Stuart et al., 2001).\nIn order to overcome those difficulties in the separation process, alternative separation techniques have been searched for. One of them is the molecular distillation -also called short path distillation. This operation is expected to provide satisfactory results, since it requires a short residence time and a very low operation temperature, due to the high vacuum levels in which the separation takes place. These characteristics prevent the compounds of interest from suffering thermal damage, and lead molecular distillation to become a very useful technique in the purification of thermal labile substances (Pramparo et al., 2004; Pramparo et al., 2006; Zeboudi et al., 2005).\nMolecular distillation is based on the partial vaporization of the compounds of a mix, which usually moves as a falling film in contact with a heated surface, and the subsequent condensation that takes place in a very close and cold surface. The main feature of this operation is its very low operation pressure, around 10-4-10-6 atmospheres. In these conditions, relative volatility of the compounds increases, and the operation temperature needed in order to obtain a certain separation degree decreases considerably. As a result of this, it is possible to separate thermal labile compounds.\nSince the molecules travel a short distance before being condensed, molecular collisions are insignificant. This also leads to a high evaporation velocity, and a low residence time of the molecules in the equipment. Under these conditions, the separation occurs at a technologically acceptable velocity (Weissberger, 1951; Perry et al., 1984).\nThe main objective of this work is to study the technical feasibility of molecular distillation, in order to separate and concentrate terpenic and oxygenated compounds of lemon essential oil, and to determine the most adequate operation conditions which lead to an efficient separation.\nThe molecular distillation equipment used for these experiments was a UIC KDL4 falling film evaporator, with 4 dm2 evaporation surface and 2 dm2 condensation surface as well (see Fig. 1). The device has a spinning roller, whose velocity could be manipul ated. The vacuum system is conformed by a diffuser pump and a mechanical pump, able to reach a vacuum of up to 10-6 atmosphere, with a maximum feed flow rate of 0.5kgh-1. The equipment also counts with a steam trap, a cooling bath and a feed flow rate controller. The operational variables analyzed were evaporation temperature and feed flow rate, as they have been proved to constitute the greatest influence in the separation efficiency.\nFig. 1: Schematic diagram of wiped-film molecular still:(a) motor drive; (b) feed flask; (c) wiper; (d) residue receiver; (e) distillate receiver; (f) one-stage condenser inlet; (g) one-stage condenser outlet; (h) cold trap; (i) rotary vane vacuum pump.\nFirstly, a characterization of three different samples of lemon essential oil -which were supplied by national industries-, took place. The analytical determinations were carried out under the recommended Standards by the American Oil Chemist\'s Society. The quantification of the different compounds of the essential oil was made by gas chromatography, using a Hewlett Packard HP 5890 gas chromatograph, with a HP INNOWAX column, equipped with a flame ionization detector. There are many different chromatography operation conditions found in bibliography about temperature conditions (injector temperature, detector temperature, and programs for relating temperature/time in the oven) (Pino and Sanchez, 2000; Stuart et al., 2001; Atti dos Santos and Atti-Serafino, 2000; Vekateshwarlu and Selvaraj, 2000). The chromatography operation conditions used in this work were: initial temperature 60ºC, ramp rate 2ºC/min, final temperature 250ºC, held for 60 minutes. The split relation was 1:40. The injection volume was 1 µl. Carrier gas flow rate was 45 ml/min. Injector temperature was 250ºC. FID temperature was 350ºC.\nThree representative samples of lemon essential oil were distillated, with the objective of finding, on one hand, concentrated d-limonene, and on the other hand free-terpene essential oil rich in oxygenated compounds, with the best possible performance (high yields and high purity of the valuable compounds). Because of the three different raw materials have similar composition, the results shown in this paper are the average values of these samples.\nTwo fractions were obtained in each of the experiments: the distillate (monoterpenic hydrocarbons, mainly d-limonene) and the residue (terpene-free oil, rich in oxygenated compounds). The temperature of the condenser and the spinning velocity of the roller were set in the values usually used for this kind of applications. The operation conditions used in all the experiments are displayed in Table 1.\nTable 1. Experimental operation conditions\nTwo groups of experiments were carried out with the three samples of essential oil, and the main objective was to find the adequate conditions in the separation. In the first group of experiments, the variation of the parameters of interest was observed by the modification of the most important variables in this use of molecular distillation. Those variables are: evaporation temperature and feed flow rate. According to the results of the first group of experiments, the second group took place. Then the temperature was constant, and the feed flow rate was modified in a certain range, in order to analyze its influence on the separation degree obtained.\nB. Description of the first group of experiments\nThis stage of the experiment consisted of four tests: two different evaporation temperatures (22 ºC and 30 ºC) were fixed for two different feed flow rates (1.3 and 2.1 ml min-1). These temperatures were adopted because if the temperature is lower than the lowest of the range, there is no driving force in the condenser and therefore there is no separation; and if the temperature is higher than the highest of the range, a thermal degradation takes place in the sample and destroys it.\nC. Description of the second group of experiments\nTemperature was set at one determined value, obtained from the first group of experiments (22 ºC); and different experiments took place using different feed flow rates in a determined range (0.5 to 4 ml min-1).\nIn both cases, the process needed to be run using moderate temperatures for the essential oil, to prevent the formation of thermal reaction products with undesirable odors.\nD. Results and discussion\nFigure 2 shows the geranial and d-limonene location in the gas chromatogram of the raw material, and the different compositions obtained from it are presented in Table 2. D-limonene and geranial are, respectively, the most abundant terpenic and oxygenated compounds found in this sample. Because of this, the specific results of these two key compounds were analyzed. Their concentrations were determined in the feed flow rate, as well as in the distillation products.\nFig. 2: D-limonene and geranial location in the gas chromatogram of the raw material.\nTable 2. Key components present in the feeding mixture\nIn addition to this, the yield of d-limonene in the distillate and the yield of geranial in the residue were calculated. The equations used are the following:\nwhere: D = distillate mass, W = residue mass, i = d-limonene, j = geranial, xD = distillate composition, xW = residue composition\nAnother important evaluation parameter in order to analyze the molecular distillation process is the separation ratio, which refers to the amount of distillate divided by the amount of residue, D/W.\nThe results of the experiments at different feed flow rates are displayed in Fig. 3 to 5.\nFig. 3: Separation ratio D/W and W/F vs. feed flow rate.\nFig 4: Influence of the feed flow rate variations over d-limonene yield and d-limonene concentration in the distillate.\nFig 5: Influence of the feed flow rate variations over geranial yield and geranial concentration in the residue.\nTable 3 shows the results obtained from analyzing evaporation temperature and feed flow rate. In conclusion:\n- Variations of evaporation temperature do not imply great variations in the amount of d-limonene found in the distillate for both flow rates.\n- Modifications of feed flow rates lead to more significant changes on the concentration of the compounds of interest.\n- The highest temperature analyzed allowed to obtain a residue poor in d-limonene and enriched in geranial, with low geranial yield (between 35-50%).\n- Regarding d-limonene, the highest temperature (30 ºC) used, led to higher yields of d-limonene.\n- The concentration of d-limonene obtained in the distillate under these conditions is very similar to the concentration found by using a lower temperature and the same feed flow.\n- High yields of geranial in the residue can be obtained by using the lowest temperature.\nTable 3. Results of the experiments\nBased on these conclusions, the second group of experiments was designed at the lowest temperature analyzed while the feed flow rate was modified.\nAs the feed flow rate increases (F), the separation ratio decreases remarkably (Fig. 3). When a big amount of sample is fed, a larger volume is occupied in the evaporator, and the contact surface decreases. Therefore, heat transfer efficiency between fluid and the walls of the evaporator is reduced and the amount of distillate obtained is smaller.\nPercent yield of d-limonene present in the distillate tends to decrease in the case of higher feed flow rates (Fig. 4); meanwhile, higher yields of geranial present in the residue are obtained when the feed flow rate increases (Fig. 5).\nAlthough the d-limonene concentration in the distillate does not considerably change with the feed flow rate (Fig. 4), the geranial concentration in the residue presents a maximum of 113 g kg-1 at 1.3 ml min-1 (Fig. 5). Under these conditions, a high yield of geranial in the residue (76.4%) can be obtained.\nLower evaporation temperatures can be used to get a residue highly enriched in geranial and with a small amount of d-limonene. This allows the residue to become a valuable product, because of its scent properties.\nSince the yield obtained for geranial in the residue was better at the lowest temperatures, a second group of experiments was carried out at this temperature, in order to analyze the impact of feed flow rate variation more deeply.\nIn conclusion, the variation of the feed flow rate in molecular distillation allows obtaining a wide range of products which have different properties and characteristics as well. According to this, the most adequate operational conditions depend on the objective of the operation, that is to say, it is important to analyze whether it is more relevant to reduce the loss of a valuable component (by maximizing its yield) or to obtain a product enriched in one of the components of the mix.\n1. Atti dos Santos, A. and L. Atti-Serafinio, ""Supercritical carbon dioxide extraction of mandarin from South Brazil,"" Perfumer & Flavorist, 25, 28-36 (2000). [ Links ]\n2. Bruzone, A., ""Aceite esencial de limón,"" Cadenas Alimentarias, 38-41 (2003). [ Links ]\n3. Buccellato, F., ""Citrus oils in perfumery and cosmetic products,"" Perfumer & Flavorist, 25, 59-63 (2000). [ Links ]\n4. Haypek, E., L.H. Silva, E. Batista, D.S. Marques, M.A. Meirelles and A.J. Meirelles, ""Recovery of aroma compounds from orange essential oil,"" Brazilian Journal of Chemical Engineering, 17, 4-7 (2000). [ Links ]\n5. Lawrence, B. and R. Reynolds, ""Progress in essential oils,"" Perfumer & Flavorist, 24, 45-56 (1999). [ Links ]\n6. Miyake, Y., K. Yamamoto, N. Tsujihara and T. Osawa. ""Protective effects of lemon Flavomids on oxidative stress in diabetic rats,"" Lipids, 33, 689-695 (1998). [ Links ]\n7. Perry, R., D. Green and J. Maloney, Perry´s Chemical Engineers´ Handbook (Sixth Edition), McGraw Hill, NY (1984). [ Links ]\n8. Pino, J.A. and M. Sánchez, ""Chemical composition of grapefruit oil concentrates,"" Journal of Essential Oil Research, 12, 167-169 (2000). [ Links ]\n9. Pramparo, M., F. Molina and M. Martinello, ""Estudio de la obtención de monoglicéridos de alta pureza"", In Proccedings of Congreso Internacional de Ciencia y Tecnología de los Alimentos, Córdoba, Argentina (2004). [ Links ]\n10. Pramparo, M., S. Putruele and M. Martinello, ""Influencia del flujo de alimentación en la desterpenación de aceite esencial de limón por destilación molecular"", In Proccedings of Congreso Internacional de Ciencia y Tecnología de Alimentos, Córdoba, Argentina (2006). [ Links ]\n11. Sinclair, W.B, The biochemistry and physiology of the lemon, and other citrus fruit, Division of Agriculture and Natural Resources, University of California (1984). [ Links ]\n12. Stuart, G., D. Lopes and V. Oliveira, ""Deterpenation of brazilian orange peel oil by vacuum distillation,"" Journal of the American Oils Chemist Society, 78, 10, 1041- 1044 (2001). [ Links ]\n13. Venkateshwarlu, G. and Y. Selvaraj, ""Changes in the peel oil composition of Kagzi lime during ripening,"" J. Essent. Oil Res, 12, 50-52 (2000). [ Links ]\n14. Weissberger, A., Distillation under high vacuum in technique of organic chemistry, Interscience Publishers, 4, 495-602 (1951). [ Links ]\n15. Wildman, R., Handbook of nutraceuticals and functional foods, CRC Press, USA (2001). [ Links ]\n16. Zeboudj, S., N. Belhaneche-Bensemra and R. Belabbes, ""Use of surface response methodology for the optimization of the concentration of the sweet orange essential oil of Algeria by wiped film evaporator,"" Journal of Food Engineering, 67, 507-512 (2005). [ Links ]\nReceived: December 29, 2009.\nAccepted: April 21, 2010.\nRecommended by Subject Editor Ricardo Gómez.', 'Isopropyl alcohol (IUPAC name propan-2-ol; commonly called isopropanol or 2-propanol) is a colorless, flammable chemical compound (chemical formula CH3CHOHCH3) with a strong odor. As an isopropyl group linked to a hydroxyl group, it is the simplest example of a secondary alcohol, where the alcohol carbon atom is attached to two other carbon atoms. It is a structural isomer of 1-propanol and ethyl methyl ether.\n|Preferred IUPAC name|\n3D model (JSmol)\nCompTox Dashboard (EPA)\n|Molar mass||60.096 g·mol−1|\n|Density||0.786 g/cm3 (20 °C)|\n|Melting point||−89 °C (−128 °F; 184 K)|\n|Boiling point||82.6 °C (180.7 °F; 355.8 K)|\n|Miscible with water|\n|Solubility||Miscible with benzene, chloroform, ethanol, ether, glycerin; soluble in acetone|\nRefractive index (nD)\n|Viscosity||2.86 cP at 15 °C|\n1.96 cP at 25 °C\n1.77 cP at 30 °C\n|1.66 D (gas)|\n|Safety data sheet||See: data page|\n|GHS Signal word||Danger|\n|H225, H319, H336|\n|P210, P261, P305+351+338|\n|NFPA 704 (fire diamond)|\n|Flash point||Open cup: 11.7 °C (53.1 °F; 284.8 K) |\nClosed cup: 13 °C (55 °F)\n|399 °C (750 °F; 672 K)|\nThreshold limit value (TLV)\n|980 mg/m3 (TWA), 1225 mg/m3 (STEL)|\n|Lethal dose or concentration (LD, LC):|\nLD50 (median dose)\nLC50 (median concentration)\nLCLo (lowest published)\n|NIOSH (US health exposure limits):|\n|TWA 400 ppm (980 mg/m3)|\n|TWA 400 ppm (980 mg/m3), ST 500 ppm (1225 mg/m3)|\nIDLH (Immediate danger)\n|1-Propanol, ethanol, 2-butanol|\n|Supplementary data page|\n|Refractive index (n),|\nDielectric constant (εr), etc.\n|UV, IR, NMR, MS|\nExcept where otherwise noted, data are given for materials in their standard state (at 25 °C [77 °F], 100 kPa).\nIt is used in the manufacture of a wide variety of industrial and household chemicals, and is a common ingredient in chemicals such as antiseptics, disinfectants, and detergents.\nIsopropyl alcohol is also known as 2-propanol, sec-propyl alcohol, IPA, or isopropanol. IUPAC considers isopropanol an incorrect name as the hydrocarbon isopropane does not exist.\nIsopropyl alcohol is miscible in water, ethanol, ether, and chloroform. It dissolves ethyl cellulose, polyvinyl butyral, many oils, alkaloids, gums and natural resins. Unlike ethanol or methanol, isopropyl alcohol is not miscible with salt solutions and can be separated from aqueous solutions by adding a salt such as sodium chloride. The process is colloquially called salting out, and causes concentrated isopropyl alcohol to separate into a distinct layer.\nIsopropyl alcohol forms an azeotrope with water, which gives a boiling point of 80.37 °C (176.67 °F) and a composition of 87.7 wt% (91 vol%) isopropyl alcohol. Water-isopropyl alcohol mixtures have depressed melting points. It has a slightly bitter taste, and is not safe to drink.\nIsopropyl alcohol becomes increasingly viscous with decreasing temperature and freezes at −89 °C (−128 °F).\nIsopropyl alcohol can be oxidized to acetone, which is the corresponding ketone. This can be achieved using oxidizing agents such as chromic acid, or by dehydrogenation of isopropyl alcohol over a heated copper catalyst:\n- (CH3)2CHOH → (CH3)2CO + H2\nIsopropyl alcohol is often used as both solvent and hydride source in the Meerwein-Ponndorf-Verley reduction and other transfer hydrogenation reactions. Isopropyl alcohol may be converted to 2-bromopropane using phosphorus tribromide, or dehydrated to propene by heating with sulfuric acid.\nLike most alcohols, isopropyl alcohol reacts with active metals such as potassium to form alkoxides that can be called isopropoxides. The reaction with aluminium (initiated by a trace of mercury) is used to prepare the catalyst aluminium isopropoxide.\nIn 1920, Standard Oil first produced isopropyl alcohol by hydrating propene. Its major use at the time was not rubbing alcohol but for oxidation to acetone, whose first major use was in World War I for the preparation of cordite, a smokeless, low explosive propellant.\nIn 1994, 1.5 million tonnes of isopropyl alcohol were produced in the United States, Europe, and Japan. It is primarily produced by combining water and propene in a hydration reaction or by hydrogenating acetone. There are two routes for the hydration process and both processes require that the isopropyl alcohol be separated from water and other by-products by distillation. Isopropyl alcohol and water form an azeotrope and simple distillation gives a material that is 87.9% by weight isopropyl alcohol and 12.1% by weight water. Pure (anhydrous) isopropyl alcohol is made by azeotropic distillation of the wet isopropyl alcohol using either diisopropyl ether or cyclohexane as azeotroping agents.\nIndirect hydration reacts propene with sulfuric acid to form a mixture of sulfate esters. This process can use low-quality propene, and is predominant in the USA. These processes give primarily isopropyl alcohol rather than 1-propanol, because adding water or sulfuric acid to propene follows Markovnikov\'s rule. Subsequent hydrolysis of these esters by steam produces isopropyl alcohol, by distillation. Diisopropyl ether is a significant by-product of this process; it is recycled back to the process and hydrolyzed to give the desired product.\nDirect hydration reacts propene and water, either in gas phase or in liquid phase, at high pressures in the presence of solid or supported acidic catalysts. This type of process usually requires higher-purity propylene (> 90%). Direct hydration is more commonly used in Europe.\nIn 1990, 45,000 metric tonnes of isopropyl alcohol were used in the United States, mostly as a solvent for coatings or for industrial processes. In that year, 5400 metric tons were used for household purposes and in personal care products. Isopropyl alcohol is popular in particular for pharmaceutical applications, due to its low toxicity. Some isopropyl alcohol is used as a chemical intermediate. Isopropyl alcohol may be converted to acetone, but the cumene process is more significant. It is also used as a gasoline additive.\nIsopropyl alcohol dissolves a wide range of non-polar compounds. It also evaporates quickly, leaves nearly zero oil traces, compared to ethanol, and is relatively non-toxic, compared to alternative solvents. Thus, it is used widely as a solvent and as a cleaning fluid, especially for dissolving oils. Together with ethanol, n-butanol, and methanol, it belongs to the group of alcohol solvents, about 6.4 million tonnes of which were used worldwide in 2011.\nIsopropyl alcohol is commonly used for cleaning eyeglasses, electrical contacts, audio or video tape heads, DVD and other optical disc lenses, removing thermal paste from heatsinks on CPUs and other IC packages, etc.\nIsopropyl alcohol is esterified to give isopropyl acetate, another solvent. It reacts with carbon disulfide and sodium hydroxide to give sodium isopropylxanthate, a herbicide and an ore flotation reagent. Isopropyl alcohol reacts with titanium tetrachloride and aluminium metal to give titanium and aluminium isopropoxides, respectively, the former a catalyst, and the latter a chemical reagent. This compound may serve as a chemical reagent in itself, by acting as a dihydrogen donor in transfer hydrogenation.\nRubbing alcohol, hand sanitizer, and disinfecting pads typically contain a 60–70% solution of isopropyl alcohol or ethanol in water. Water is required to open up membrane pores of bacteria, which acts as a gateway for isopropyl alcohol. A 75% v/v solution in water may be used as a hand sanitizer. Isopropyl alcohol is used as a water-drying aid for the prevention of otitis externa, better known as swimmer\'s ear.\nEarly uses as an anesthetic\nAlthough isopropyl alcohol can be used for anesthesia, its many negative attributes or drawbacks prohibit this use. Isopropyl alcohol can also be used similarly to ether as a solvent or as an anesthetic by inhaling the fumes or orally. Early uses included using the solvent as general anesthetic for small mammals and rodents by scientists and some veterinarians. However, it was soon discontinued, as many complications arose, including respiratory irritation, internal bleeding, and visual and hearing problems. In rare cases, respiratory failure leading to death in animals was observed.\nIsopropyl alcohol is a major ingredient in ""gas dryer"" fuel additives. In significant quantities water is a problem in fuel tanks, as it separates from gasoline and can freeze in the supply lines at low temperatures. Alcohol does not remove water from gasoline—but the alcohol solubilizes water in gasoline. Once soluble, water does not pose the same risk as insoluble water, as it no longer accumulates in the supply lines and freezes, but is consumed with the fuel itself. Isopropyl alcohol is often sold in aerosol cans as a windshield or door lock deicer. Isopropyl alcohol is also used to remove brake fluid traces from hydraulic braking systems, so that the brake fluid (usually DOT 3, DOT 4, or mineral oil) does not contaminate the brake pads and cause poor braking. Mixtures of isopropyl alcohol and water are also commonly used in homemade windshield washer fluid.\nAs a biological specimen preservative, isopropyl alcohol provides a comparatively non-toxic alternative to formaldehyde and other synthetic preservatives. Isopropyl alcohol solutions of 70–99% are used to preserve specimens.\nIsopropyl alcohol is often used in DNA extraction. A lab worker adds it to a DNA solution to precipitate the DNA, which then forms a pellet after centrifugation. This is possible because DNA is insoluble in isopropyl alcohol.\nIsopropyl alcohol vapor is denser than air and is flammable, with a flammability range of between 2 and 12.7% in air. It should be kept away from heat and open flame. Distillation of isopropyl alcohol over magnesium has been reported to form peroxides, which may explode upon concentration. Isopropyl alcohol is a skin irritant. Wearing protective gloves is recommended.\nIsopropyl alcohol and its metabolite, acetone, act as central nervous system (CNS) depressants. Poisoning can occur from ingestion, inhalation, or skin absorption. Symptoms of isopropyl alcohol poisoning include flushing, headache, dizziness, CNS depression, nausea, vomiting, anesthesia, hypothermia, low blood pressure, shock, respiratory depression, and coma. Overdoses may cause a fruity odor on the breath as a result of its metabolism to acetone. Isopropyl alcohol does not cause an anion gap acidosis but it produces an osmolal gap between the calculated and measured osmolalities of serum, as do the other alcohols.\nIsopropyl alcohol is oxidized to form acetone by alcohol dehydrogenase in the liver, and has a biological half-life in humans between 2.5 and 8.0 hours. Unlike methanol or ethylene glycol poisoning, the metabolites of isopropyl alcohol are considerably less toxic, and treatment is largely supportive. Furthermore, there is no indication for the use of fomepizole, an alcohol dehydrogenase inhibitor, unless co-ingestion with methanol or ethylene glycol is suspected.\n- Nomenclature of Organic Chemistry : IUPAC Recommendations and Preferred Names 2013 (Blue Book). Cambridge: The Royal Society of Chemistry. 2014. p. 631. doi:10.1039/9781849733069. ISBN 978-0-85404-182-4.\n- ""Alcohols Rule C-201.1"". Nomenclature of Organic Chemistry (The IUPAC \'Blue Book\'), Sections A, B, C, D, E, F, and H. Oxford: Pergamon Press. 1979.\nDesignations such as isopropanol, sec-butanol, and tert-butanol are incorrect because there are no hydrocarbons isopropane, sec-butane, and tert-butane to which the suffix ""-ol"" can be added; such names should be abandoned. Isopropyl alcohol, sec-butyl alcohol, and tert-butyl alcohol are, however, permissible (see Rule C-201.3) because the radicals isopropyl, sec-butyl, and tert-butyl do exist.\n- ""Isopropanol_msds"". chemsrc.com.\n- Reeve, W.; Erikson, C.M.; Aluotto, P.F. (1979). ""A new method for the determination of the relative acidities of alcohols in alcoholic solutions. The nucleophilicities and competitive reactivities of alkoxides and phenoxides"". Can. J. Chem. 57 (20): 2747–2754. doi:10.1139/v79-444.\n- Yaws, C.L. (1999). Chemical Properties Handbook. McGraw-Hill. ISBN 978-0-07-073401-2.\n- ""Isopropyl alcohol"". Immediately Dangerous to Life and Health Concentrations (IDLH). National Institute for Occupational Safety and Health (NIOSH).\n- NIOSH Pocket Guide to Chemical Hazards. ""#0359"". National Institute for Occupational Safety and Health (NIOSH).\n- ""PubChem - Isopropanol"". Retrieved February 10, 2019.\n- Doolittle, Arthur K. (1954). The Technology of Solvents and Plasticizers. New York: John Wiley & Sons, Inc. p. 628.\n- The Merck Index (10th ed.). Rahway, NJ: Merck & Co. 1983. p. 749.\n- Logsden, John E.; Loke, Richard A (1999). ""Propyl Alcohols"". In Jacqueline I., Kroschwitz (ed.). Kirk-Othmer Concise Encylclopedia of Chemical Technology (4th ed.). New York: John Wiley & Sons, Inc. pp. 1654–1656. ISBN 978-0471419617.\n- ""Isopropyl Alcohol, UltimAR, Suitable for Liquid Chromatography, Extract/Conc, UV-Spectrophotometry"". VWR International. Retrieved 25 August 2014.\n- ""UV Cutoff"" (PDF). University of Toronto. Retrieved 25 August 2014.\n- Young, W.; Hartung, W.; Crossley, F. (1936). ""Reduction of Aldehydes with Aluminum Isopropoxide"". J. Am. Chem. Soc. 58: 100–2. doi:10.1021/ja01292a033.\n- Wittcoff, M.M. Green ; H.A. (2003). Organic chemistry principles and industrial practice (1. ed., 1. reprint. ed.). Weinheim: Wiley-VCH. p. 4. ISBN 978-3-527-30289-5.\n- Papa, A. J. ""Propanols"". Ullmann\'s Encyclopedia of Industrial Chemistry. Weinheim: Wiley-VCH. doi:10.1002/14356007.a22_173.\n- Logsdon, John E.; Loke, Richard A. (December 4, 2000). ""Isopropyl Alcohol"". Kirk-Othmer Encyclopedia of Chemical Technology. Kirk‑Othmer Encyclopedia of Chemical Technology. John Wiley & Sons, Inc. doi:10.1002/0471238961.0919151612150719.a01. ISBN 978-0471238966.\n- CRC Handbook of Chemistry and Physics, 44th ed. pp 2143–2184\n- Market Study Soviet. Ceresana. April 2012\n- ""Sodium Isopropyl Xanthate, SIPX, Xanthate"". 3DChem.com. Retrieved 2012-06-17.\n- ""Guide to Local Production: WHO-recommended Handrub Formulations"" (PDF). World Health Organization. August 2009.\n- Otitis Externa (Swimmers Ear). Medical College of Wisconsin\n- Burlage, Henry M.; Welch, H; Price, CW (2006). ""Pharmaceutical applications of isopropyl alcohol II. Solubilities of local anesthetics"". Journal of the American Pharmaceutical Association. 36 (1): 17–9. doi:10.1002/jps.3030360105. PMID 20285822.\n- Society for Experimental Biology and Medicine (1922). Proceedings of the Society for Experimental Biology and Medicine, Volume 19. p. 85.\n- ""Isopropanol"". Sigma-Aldrich. 19 January 2012. Retrieved 6 July 2012.\n- Mirafzal, Gholam A; Baumgarten, Henry E (1988). ""Control of peroxidizable compounds: An addendum"". Journal of Chemical Education. 65 (9): A226. Bibcode:1988JChEd..65A.226M. doi:10.1021/ed065pA226.\n- ""Chemical safety: peroxide formation in 2-propanol"". Chemical & Engineering News. 94 (31): 2. August 1, 2016.\n- García-Gavín, Juan; Lissens, Ruth; Timmermans, Ann; Goossens, An (2011-06-17). ""Allergic contact dermatitis caused by isopropyl alcohol: a missed allergen?"". Contact Dermatitis. 65 (2): 101–106. doi:10.1111/j.1600-0536.2011.01936.x. ISSN 0105-1873. PMID 21679194.\n- McInnes, A (1973-02-10). ""Skin reaction to isopropyl alcohol"". British Medical Journal. 1 (5849): 357. doi:10.1136/bmj.1.5849.357-c. ISSN 0007-1447. PMC 1588210. PMID 4265463.\n- Slaughter RJ, Mason RW, Beasley DM, Vale JA, Schep LJ (2014). ""Isopropanol poisoning"". Clinical Toxicology. 52 (5): 470–8. doi:10.3109/15563650.2014.914527. PMID 24815348.\n- Kalapos, MP (2003). ""On the mammalian acetone metabolism: from chemistry to clinical implications"". Biochimica et Biophysica Acta. 1621 (2): 122–39. doi:10.1016/S0304-4165(03)00051-5. PMID 12726989.\n- ""Isopropyl alcohol poisoning"". www.uptodate.com. Retrieved 2017-10-10.']"	['<urn:uuid:8ba9938a-83e5-4819-8359-15a3e3af5f3d>', '<urn:uuid:838386cf-d5e7-42a0-8e15-735cdf48bccc>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-13T06:15:13.904987	8	84	5100
75	I'm designing complex structures with multiple materials and I'd like to know - how can optical fiber sensors measure strain in different locations, and what makes them suitable for modern manufacturing?	Optical fiber sensors can measure strain at multiple locations by using variable spacing Bragg reflection gratings written into the fiber core at different positions. Each grating is formed by exposing the core to ultraviolet light at specific angles, creating periodic perturbations that reflect different wavelengths. When strain occurs, it causes shifts in these reflected wavelengths that can be measured. This technology is particularly suitable for modern manufacturing because it can handle today's complex structures that combine different materials like metals, composites, elastomers and foams. The sensors are material-independent and measure the integrated system response, allowing designers to understand the true capabilities and potential weaknesses of their designs before final structural testing. This is crucial since modern components often use various materials bonded, welded or riveted together in complex configurations.	"[""Method for impressing gratings within fiber optics\nA method of establishing a dielectric periodic index of refraction phase grating upon the core of an optical waveguide by intense angled application of several transverse beams of ultraviolet light, enabling the establishment of a distributed, spatially resolving optical fiber strain gauge.\nLatest United Technologies Corporation Patents:\nFIG. 1 is a schematic drawing of the spatially resolving optical fiber strain gauge according to the invention addressed herein;\nFIGS. 2A through 2C are partial schematics of selected sections of the optical waveguide including its cores, indicating grating patterns of varying spacing corresponding to selected regions A, B and C in a mechanical structure being monitored for strain;\nFIG. 3 is a graph of the intensity spectrum of the reflected light produced by injecting broadband light into the core of the waveguide with shifts in the spectral lines indicating strain at specific stations; and\nFIG. 4 shows a schematic illustration of a technique for establishing a grating pattern of variable spacing at selected positions along the length of the optical waveguide.BEST MODE FOR CARRYING OUT THE INVENTION\nFIG. 1 shows a schematic diagram of the spatially resolving optical fiber strain gauge 13. The gauge 13 includes an optical waveguide 15 or fiber operative to transmit a single or lowest order mode of injected light.\nThe core 19 of waveguide 15 is preferably a Germanium-doped silica or glass filament. The core 15 contains a series of variable spacing Bragg reflection gratings 16 written, impressed or otherwise applied by application of a variable two-beam ultraviolet (less than 300 nanometer) interference pattern. These periodic gratings 16 or refractive index perturbations are permanently induced by exposure to intense radiation.\nFIGS. 2A through 2C shows the establishment of different wavelength gratings 16 corresponding to respective locations on core 19.\nEach of selected gratings 16 is formed by transverse irradiation with a particular wavelength of light in the ultraviolet absorption band of the core material associated with a position in a structural component 22. This procedure establishes a first order absorption process by which gratings 16 each characterized by a specific spacing and wavelength can be formed by illuminating core 19 from the side with two coplanar, coherent beams incident at selected and complementary angles thereto with respect to the axis of core 19. The grating period is selected by varying the selected angles of incidence. Thus, a permanent change in the refractive index is induced in a predetermined region of core 19, in effect creating a phase grating effective for affecting light in core 19 at selected wavelengths.\nAs indicated in FIG. 1 the optical waveguide 15 and core 19 are attached or embedded in a section of structural component 22, in particular a plate for example. Core 19 contains characteristic periodic refractive index perturbations or gratings 16 in regions A, B and C thereof. A broadband light source 33 or tunable laser is focused through lens 33' onto the exposed end of core 19. A beam splitter 34 serves to direct the return beam from core 19 toward a suitable readout or spectrometer 37 for analysis. Alternatively, a transmitted beam passing out of the end 19' of core 19 could be analyzed.\nThe spectrum of the reflected light intensities from strain gauge 13 is shown in FIG. 3. A complementary tranmitted spectrum is also established passing out of the end 19' of core 19. The spectrum contains three narrowband output lines centered at respective wavelengths: lambda.sub.A, lambda.sub.B and lambda.sub.C. These output signals arise by Bragg reflection or diffraction from the phase gratings 16 at respective regions A, B and C. In this example, regions A and C of structural component 22 have been strained by deformation, causing a compression and/or dilation of the periodic perturbations in the fiber core.\nAs a result, the corresponding spectral lines are shifted as shown in FIG. 3 to the dotted lines indicated. The respective wavelength differences delta lambda.sub.A and delta lambda.sub.C are proportional to strain in respective regions A and C.\nFIG. 4 illustrates the formation of periodic perturbations or gratings 16 in a region of fiber core 19 in response to exposure of core 19 to intense transverse ultraviolet radiation. Grating spacings.DELTA. a and .DELTA.c are controlled by the incidence angle of incident interfering beams 99 and beam 101. As can be seen, the angles of incidence of beams 99 are complements (i.e. their sum equals 180 degrees) to each other with respect to the axis of core 19. The incident pair of beams 99 can be derived from a single incident beam 101 passing in part through a beam splitter 103 and reflecting from spaced parallel reflectors 105. By increasing the separation between reflectors 105 and correspondingly varying the angles of incidence of beam 101, the angles of incidence of beams 99 upon core 19 can be controlled. Accordingly, the fringe spacing in grating 16 is varied as desired along the length of core 19, to permit a determination of strain or temperature corresponding to location along gauge 13.\nSeveral spacings can be superimposed or colocated by this technique for the response set forth below.\nSensitivity to external perturbations upon structural component 22 and thus also upon core 19 depends upon the Bragg condition for reflected wavelength. In particular, the fractional change in wavelength due to mechanical strain or temperature change is: ##EQU1## q is the thermooptic coefficient, which is wavelength dependent; .alpha. is the expansion coefficient;\n.epsilon. is the axial or longitudinal strain;\nlambda.sub.i is the wavelength reflected by the grating at location i along the core 19;\nn is the refractive index of the optical waveguide; and\n.DELTA.T is the change in temperature.\nThis relationship suggests a way to compensate for temperature changes along the length of the fiber sensor. In particular, if superimposed gratings of different spacings are provided, each of the two gratings will be subject to the same level of strain, but the fractional change in wavelength of each grating will be different because q is wavelength dependent.\nAccordingly, each pair of superimposed gratings will display a corresponding pair of peaks of reflected or transmitted intensity. Accordingly, the shifts of these peaks due to a combination of temperature and strain can be subtracted. The shifts in these peaks due to strain will be the same in magnitude. Accordingly, any remaining shift after subtraction is temperature related. Thus, when it is desired to know the strain difference as between several locations possibly subject to a temperature difference, the temperature factor can be compensated.\nThe relationship therefore permits compensation for temperature variation during measurement, since the photoelastic and thermoptic effects are wavelength dependent. In other words, by superimposing two or more gratings at each location of interest, two or more spectral lines are established at each point of measurement. Strain will affect both lines equally; temperature will not. Thus, sufficient information is available to permit determination of the magnitude of strain and the temperature difference.\nThe information above is likely to cause others skilled in the art to conceive of other variations in carrying out the invention addressed herein, which nonetheless are within the scope of the invention. Accordingly, reference to the claims which follow is urged, as those specify with particularly the metes and bounds of the invention.\n1. A method of impressing at least one grating into at least one predetermined region of a core of an optical fiber which further includes a cladding surrounding the core and is centered on a longitudinal axis, comprising the steps of\n- making the core of a material of a material whose index of refraction can be permanently changed by exposing such material to light of a predetermined intensity at a wavelength in a predetermined range of the spectrum; and\n- simultaneously exposing the core through the cladding to two light beams propagating at respective different angles of incidence relative to the longitudinal axis of the optical fiber toward and through the cladding into the predetermined region of the core to form an interference pattern of longitudinally varying intensity propagating transversely through the core and forming permanent periodic refractive index perturbations in the material of the predetermined region of the core in dependence on the intensity of the interference pattern.\n2. The method as defined in claim 1, wherein said exposing step includes using light beams of a wavelength in the ultraviolet range of the spectrum.\n3. The method as defined in claim 2, wherein the wavelength of said light beams is less than 300 nanometers.\n4. The method as defined in claim 1, wherein said exposing step includes so selecting the angles of incidence of the light beams that the periodicity of the refractive index perturbations corresponds to one-half of a selected wavelength of light propagating through the optical fiber during the use thereof.\n5. The method as defined in claim 4, and further comprising the step of additionally performing said exposing step through the cladding on at least one additional predetermined region of the core at respective additional angles of incidence that differ from one another and from those used during said exposing step to form a different interference pattern of longitudinally varying intensity propagating transversely through the core and forming permanent periodic refractive index perturbations in the material of the additional predetermined region of the core in dependence on the intensity of the interference pattern, the additional angles of incidence of the light beams being such that the periodicity of the refractive index perturbations corresponds to one-half of another selected wavelength of light propagating through the optical fiber during the use thereof.\n|4068190||January 10, 1978||Ferguson|\n|4093339||June 6, 1978||Cross|\n|4286838||September 1, 1981||Huignard et al.|\n|4295738||October 20, 1981||Meltz et al.|\n|4400056||August 23, 1983||Cielo|\n|4653906||March 31, 1987||Dunphy et al.|\n- Tomlinson et al., Applied Physics Letters vol. 26, No. 6, Mar. 1975, pp. 303-306. Photosensitivity in Optical Fiber Waveguides: Application to Reflection Filter Fabrication K. O. Hill et al, Appl. Phys. Letter 32(10), May 15, 1978. Photosensitive Phenomena in Optical Fibers, J. LaPierre et al. Narrow-Band Bragg Reflectors in Optical Fibers, B. S. Kawasaki et al, Optics Letters, vol. 3, No. 2, Aug. 1978. S. K. Yao et al, Applied Optics, vol. 21, pp. 3059-3060, (1982).\nFiled: Oct 27, 1986\nDate of Patent: Feb 16, 1988\nAssignee: United Technologies Corporation (Hartford, CT)\nInventors: William H. Glenn (Vernon, CT), Gerald Meltz (Avon, CT), Elias Snitzer (Wellesley, MA)\nPrimary Examiner: Bruce Y. Arnold\nAssistant Examiner: Martin Lerner\nAttorneys: Peter R. Ruzek, Eric W. Petraske, Robert P. Sabath\nApplication Number: 6/925,512\nInternational Classification: G02B 612;"", 'Variation and complexity of materials, designs, and structures is accelerating at an incredible rate, beyond what individual sensors can grasp. Full-field optical strain provides a true understanding of the complete structural response of these complex designs. This full-field data brings a more holistic reality to computer models, allowing designers to precisely advance their designs. This capability is crucial for the development of our industries.\nTwenty years ago, advanced designs used a few standard materials and traditional configurations that had worked for generations. Today, every component can be of a different material, metals to composites, each well-suited for their applications, then bonded, welded, riveted, or screwed together, with sound dampening materials, elastomers, foams, and thermoplastics, creating increasingly complex structures. Current computer models use many assumptions when these assembly complexities arise, leading to weaknesses in the final design.\nOptical strain is material independent and measures the response of the integrated system, so designers can get a measure of the true response and strength of their designs. They can truly understand the capabilities of their designs and also understand unanticipated design weaknesses before manufacturing and final structural testing.\nBoeing uses optical strain to fully understand their complete range of material properties that their manufacturing specifications allow. When they allow +/- 3 degrees of ply orientation, they can model the true material strength variation in their designs based on precision optical strain testing of each of those parameters. Better material properties, allows for better designs. NASA Glenn reported that they get more consistent material properties with optical strain over any other method.\nSubassembly designs are made up of real parts with real manufacturing variations. As parts are assembled, they are made to fit to the final design, with inherent stresses built into these components, which should be compensated for in the design, if even known. Optical strain allows for more complete understanding of the assembly process.\nStructural testing is the ultimate test of vehicle performance, with holistic measurements critical for the full understanding of the true structural response of your vehicle. The method directly aligns to CAD, so all data is precisely in vehicle coordinates. Results show the complete strain and 3D displacement data.\nOptical strain is highly fast and efficient compared to traditional methods, allowing one person to do the work of teams in a fraction of the time. Boeing engineering estimates that it is 50x less labor than traditional gauges, and 10x less expensive. It also allows you to see the strain vectors so that you can fully understand the load vectors effecting your structures.\nOptical Strain Method\nNIAR calls optical strain the FEA of testing because it provides the continuous 3D deformation and strain field across the complete structure. It actually measures the 3D coordinates of each point across the surface of structure, which is the 3D shape, and the changes are the 3D displacement of the structure, with local displacements being strain. In 3D, these local displacements become the full strain tensor of material response, as if you had 10,000 rosette strain gauges in each field of view; each field of view is worth $1M of strain gauges.\nThe 3D shape is measured dynamically in 6-DOF (Degrees of Freedom), so complete 6-DOF deformations and the total strain tensor is known in 3D space and time. Optical strain measures a surface and the true strain tensor across the material surface with the desired accuracy from far finer than, or equivalent to, a traditional gauges. In many cases, you really do not know what you are measuring with a strain gauge. The method sees the development of strains going by strain gauges, and local material responses, to crystalline responses in metals, that are just seen as variance or errors in single point sensors.\nOptical strain measures 3D coordinates in time, so it is used for any combination of complex displacements, 6-DOF tracking, strain, vibration, modal analysis, etc., with a non-contact, simple to use method, providing immensely more data than costly individual mechanical sensors. You measure across the structure the entire strain response, as well as the displacements that are causing the strains, with the data aligned to directly to the CAD model, so every point of the measurement is precisely in your CAD coordinates.\nValidation and Certification\nOptical strain was introduced more than 30 years ago and has developed into a precision technology governed by iDICs (International Digital Image Correlation Society) directed by NIST and Sandia National Labs. In the U.S., NASA was an early adopter of the technology and was the core technology used for the model validation measurements for the Return to Flight of the Space Shuttle in 2004, with validation paper, with a broad application review. [1,2]\nA strong validation was performed by Lawrence Livermore National lab.  The USAF adopted it initially for thermal strain studies on the B-2 Stealth Aircraft,  and then for broad applications. Boeing was an early adopter for aerospace manufacturing, starting 2006, and finally presenting at 2012 ASNT and their ARAMIS Optical Strain certification at 2016 ASTM. \nThe NIAR was the first user of ARAMIS Optical Strain in the U.S., and lead to the FAA Technical Center Fuselage Test Facility implementing the technology in 2004. [ 6, 7]\nIt is also covered in the ASNT Handbook.  Northrop-Grumman also performed an independent precision verification of ARAMIS Optical Strain for the structural testing of the James Webb Space Telescope,  for which they saved NASA $2M, and the telescope is working well now.\nValidation of the optical strain system and the measurement is covered by VDI 2634-1 & VDI 2626, which are used to validate every ARAMIS Optical Strain system in manufacturing,  and is available to be used to truly validate every measurement FOV, using the same method.\nFor strain gauge measurements, only the amplifier is actually certified, not the installed strain gauge, so it really is not a validated measurement; you must rely on the precision of the installers.\nStandards that follow the basis of VDI standards include ASTM E-83, ISO 10360-1, and ISO 9513. Optical strain is a form of optical photogrammetry for measuring 3D coordinates, which is the basis of VDI 2634-1 that validates optical measurements in 3D coordinates, positioning a certified standard in multiple positions within the measurement volume. ARAMIS Optical Strain can track these positions in 6-DOF in real-time (live).\nOptical strain uses Digital Image Correlation (3D-DIC) for measuring strain on the surface of a material or structure, which is the basis of VDI 2626, which validates that strains throughout the volume are precisely measurable, moving a DIC rigid pattern in the volume and maintaining zero strain. The error during these tests validates the accuracy of the optical strain measurement. These accuracy/sensitivity measurements can be documented within an ARAMIS optical strain measurement, for validated measurements, with certified calibration and certified equipment.\nOptical Strain Implementation\nOptical strain is an accepted and used technology by thousands of companies and universities around the world and is critical for the understanding of modern materials, structures, and designs. Its implementation is far easier than the deep experience and knowledge required for the implementation of traditional gauges, and is actually fully certifiable. Training and certification is available from iDICs governed by NIST and Sandia National Labs.\nOptical strain systems, certified in manufacturing, set-up following basic rules, calibrated with a certified calibration standard, make precision full-field measurements very straight forward. The measurement itself can even be certified during the measurement, all of which is documented in the data set.\nCertified setup and calibration is at least 50-times faster than with traditional sensors, and the Boeing report included that optical strain produces 100-times more data. This holistic data is critical for modern materials and structures in aerospace, automotive, microelectronics, biomechanics, and civil.\n- Schmidt, T., Tyson, J., Revilock, D.M., Lyle, K., “Performance Verification of 3D Image Correlation using Digital High-Speed Cameras”, Proceedings of 2005 SEM Conference, Portland, OR, 2005.\n- Tyson, J., Schmidt, T., Coe, D., Galanulis, K., “3D Image Correlation for Dynamic and Extreme Environment Materials Measurements Holistic Structure Measurements from the Laboratory to the Field”, Proceedings of 2005 SEM Conference, Portland, OR, June 7-9, 2005.\n- LeBlanc, M.M., Florando, J.N., Lassila, D.H., Schmidt, T., Tyson, J., “Image Correlation Applied to Single Crystal Plasticity Experiments and Comparison to Strain Gage Data”, 2006.\n- Bailey, J.T., Coate, J.E., Tashiro, R., “Evaluating the B-2 Aft Deck Measured Response to the External Environment”, Proceedings of 2008 USAF Aircraft Structural Integrity Conference, 2008.\n- Grossnickle, J., Gordon, T., McCrary, K., Wanthal, S., “3-Dimensional Non-Contact Optical Strain Measurements for Structures Evaluation and Optimization”, ASNT Aero, St. Louis, MO, 2012.\n- Tomblin, J., Seneviratne, W., Pillai, G.R., “Effects of Disbonds, Lightning Strikes, and Low-Velocity Impact Damage on Adhesively Bonded Composite Joints”, DOT/FAA/AR-09/4, 2009.\n- Avitable, P., Niezrecki, C., Helfrick, M., Warren, C., Pingle, P., “Noncontact Measurement Techniques for Model Correlation”, Sound & Vibration, January 2010.\n- Tyson, J., Schwartz, E.I., “Optical Measurement of Strain and Displacement”, ASNT Handbook, Vol. 10, p.507-513.\n- Pokk, A., Gurden, C., “JWST Structural Test Monitoring, Instrumentation and Data Acquisition”, 30th Aerospace Testing Seminar, El Segundo, CA, 2017.\n- Witzel, O., “Acceptance and Verification of DIC Systems with Reference to VDI/ VDE Guidelines”, Intl. Digital Image Correlation Society (iDICs), Portland, OR, 2019.\nThis article was written by John Tyson II, PE, President, Joesph Horn, Engineering Manager, and Steve Openshaw, Senior Applications Engineer, all at Trilion Quality Systems. For more information, visit here .']"	['<urn:uuid:4e94f200-db1f-468f-b799-02679c13443c>', '<urn:uuid:ae52ed9a-510f-47a3-9285-a16c3bf173d5>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:15:13.904987	31	129	3314
76	compare river mouth estuary pacific ocean	Estuaries form where rivers meet large bodies of water like the Pacific Ocean. At these points, fresh river water mixes with salt water creating unique ecosystems with diverse organisms. A notable example is San Francisco Bay, which forms a massive estuary system with strong tidal currents exceeding 2.5 m/s where the Sacramento and San Joaquin Rivers meet the Pacific Ocean.	"[""Do you see the Sacramento and San Joaquin Rivers?\nThe farmland in the Central Valley of California is among the most productive in the world. Besides good soil and a mild climate, the region has a lot of water. Streams drain off of the Sierra Nevada mountains to the east and join the mighty Sacramento and San Joaquin Rivers in the Central Valley. How many of the features that are discussed below can you find in this image?\nStreams are bodies of water that have a current; they are in constant motion. Geologists recognize many categories of streams depending on their size, depth, speed, and location. Creeks, brooks, tributaries, bayous, and rivers are all streams. In streams, water always flows downhill, but the form that downhill movement takes varies with rock type, topography, and many other factors. Stream erosion and deposition are extremely important creators and destroyers of landforms.\nRivers are the largest streams. People have used rivers since the beginning of civilization as a source of water, food, transportation, defense, power, recreation, and waste disposal.\nWith its high mountains, valleys and Pacific coastline, the western United States exhibits nearly all of the features common to rivers and streams. The photos below are from the western states of Montana, California and Colorado.\nParts of a Stream\nA stream originates at its source. A source is likely to be in the high mountains where snows collect in winter and melt in summer, or a source might be a spring. A stream may have more than one source.\nTwo streams come together at a confluence. The smaller of the two streams is a tributary of the larger stream (Figure below).\nThe confluence between the Yellowstone River and one of its tributaries, the Gardiner River, in Montana.\nThe point at which a stream comes into a large body of water, like an ocean or a lake, is called the mouth. Where the stream meets the ocean or lake is an estuary (Figure below).\nThe mouth of the Klamath River creates an estuary where it flows into the Pacific Ocean in California.\nThe mix of fresh and salt water where a river runs into the ocean creates a diversity of environments where many different types of organisms create unique ecosystems.\nStages of Streams\nAs a stream flows from higher elevations, like in the mountains, towards lower elevations, like the ocean, the work of the stream changes. At a stream’s headwaters, often high in the mountains, gradients are steep (Figure below). The stream moves fast and does lots of work eroding the stream bed.\nHeadwaters of the Roaring Fork River in Colorado.\nAs a stream moves into lower areas, the gradient is not as steep. Now the stream does more work eroding the edges of its banks. Many streams develop curves in their channels called meanders (Figure below).\nThe East River meanders through Crested Butte, Colorado.\nAs the river moves onto flatter ground, the stream erodes the outer edges of its banks to carve a floodplain, which is a flat, level area surrounding the stream channel (Figure below).\nA green floodplain surrounds the Red Rock River as it flows through Montana.\nBase level is where a stream meets a large body of standing water, usually the ocean, but sometimes a lake or pond. Streams work to down cut in their stream beds until they reach base level. The higher the elevation, the farther the stream is from where it will reach base level and the more cutting it has to do. The ultimate base level is sea level.\nA divide is a topographically high area that separates a landscape into different water basins (Figure below). Rain that falls on the north side of a ridge flows into the northern drainage basin and rain that falls on the south side flows into the southern drainage basin. On a much grander scale, entire continents have divides, known as continental divides.\n(a) The divides of North America. In the Rocky Mountains in Colorado, where does a raindrop falling on the western slope end up? How about on the eastern slope? (b) At Triple Divide Peak in Montana water may flow to the Pacific, the Atlantic, or Hudson Bay depending on where it falls. Can you locate where in the map of North America this peak sits?\n- A moving body of water of any size is a stream.\n- A tributary begins at its headwaters on one side of a divide, comes together with another tributary at a confluence, and empties out at an estuary.\n- Base level is where a large body of water is located; sea level is the ultimate base level.\n- Very little land is below sea level and all of it does not drain to the sea. Why not?\n- What happens to two drops of water that fall on opposite sides of a divide?\n- What happens to a river's floodplain if the river is dammed?\nUse the resources below to answer the questions that follow.\n- Does the geology shape the river's path or does the river's path shape the geology? Explain.\n- Where is water speed and weight the greatest? What happens there?\n- Where is the water speed the slowest? What happens there?\n- What shape is created by this fast moving water?\n- What has destabilized the Minnesota River area? What was the result of that?\n- Why are there waterfalls in some places and ravines in others?\n- Where does most of the sediment end up?\n- Why are some sources of sediment considered to be augmented by human activity?"", 'This polygon shapefile depicts geological features within the offshore area of San Francisco, California. The map area includes the Golden Gate inlet which connects the Pacific Ocean and San Francisco Bay. San Francisco Bay, the largest estuary on the U.S. west coast, is located at the mouth of the Sacramento and San Joaquin rivers and drains over 40 percent of the state of California. The large surface area of the bay and diurnal tidal range of 1.78 m creates an enormous tidal prism (about 2 billion cu m) and strong tidal currents, commonly exceeding 2.5 m/s (Barnard and others, 2006a, 2006b, 2007). Acceleration of these currents through the constricted inlet has led to scouring of a bedrock channel that has a maximum depth of 113 m. Large fields of sand waves (Barnard and others, 2007) (unit Qmsw) have formed both west and east of this channel as flow expands and tidal currents decelerate. Active tidally influenced map units inside San Francisco Bay also include sand-dominated deposits (unit Qbs) and more coarse-grained sand, gravel, and pebble deposits (unit Qbsc). Sand wave fields resulting from tidal flow are also present in the nearshore along the Pacific Coast, both north and south of the Golden Gate inlet. The sand wave fields appear to be variably mobilized by both ebb and flood tides, but the presence of a large (~150 sq km) ebb-tidal delta at the mouth of the bay west of the inlet indicates net sediment transport has been to the west. The ebb-tidal delta west of the Golden Gate inlet is mapped as two units. The inner part of the delta (unit Qmst) comprises a semi-circular, inward-sloping (i.e., toward the Golden Gate inlet), sandy seafloor at water depths of about 12 to 24 m. This inner delta has a notably smooth surface, indicating sediment transport and deposition under different flow regimes (defined by tidal current strength and depth) than those in which the sand waves formed and are maintained. Further deceleration of tidal currents beyond the inner delta has led to development of a large, shoaling (about 8 to 12 m water depth), horse-shoe shaped, delta-mouth bar (unit Qmsb). This feature (the ""San Francisco Bar"") surrounds the inner delta, and its central crest is cut by a dredged shipping channel that separates the nothern and southern parts of the bar, the ""North Bar"" and ""South Bar,"" respectively. The San Francisco Bar is shaped by both tidal currents and waves, which regularly exceed 6 m in height on the continental shelf during major winter storms (Barnard and others, 2007). This mix of tidal and wave influence results in a variably hummocky, mottled, and rilled seafloor, and this surface texture is used as a primary criteria for mapping the unit and defining its boundaries. Outside the San Francisco Bar to the limits of the map area, the notably flat shelf (less than 0.2 degrees) and the nearshore are wave-dominated and characterized by sandy marine sediment (unit Qms). Local zones of wave-winnowed (?) coarser sediment (unit Qmsc) indicated by high backscatter occur along the coast offshore Ocean Beach. Unit Qmsc is also mapped inside and at the mouth of the Golden Gate inlet where it presumably results from winnowing by strong tidal currents. Coarser sediment also occurs as winnowed lags in rippled scour depressions (unit Qmss), recognized on the basis of high-resolution bathymetry and backscatter. These depressions are typically a few tens of centimeters deep and are bounded by mobile sand sheets (for example, Cacchione and others, 1984). This unit occurs primarily in the nearshore south of the Golden Gate inlet offshore of Ocean Beach (water depth less than 13 m) and north of the inlet offshore Muir Beach (water depth less than 17 m). Artificial seafloor (unit af) has several distinct map occurrences: (1) sites of active sand mining inside San Francisco Bay; (2) the dredged shipping channel at the central crest of the San Francisco Bar; (3) the sewage outfall pipe, associated rip rap, and surrounding scour channel offshore Ocean Beach; and (4) the location of a former waste disposal site about 2.5 km offshore Point Lobos. Although the map shows the areas in which several active sedimentary units (Qmsw, Qmst, Qmsb, Qms, Qmsc, Qmss, Qbsm, Qbsc) presently occur, it is important to note that map units and contacts are dynamic and ephemeral, likely to change during large storms, and on seasonal to decadal scales based on changing external forces such as weather, climate, sea level, and sediment supply. Dallas and Barnard (2011) have noted, for example, that the ebb-tidal delta has dramatically shrunk since 1873 when the first bathymetric survey of the area was undertaken. They document an approximate 1 km landward migration of the crest of the San Francisco Bar, which they attribute to a reduction in the tidal prism of San Francisco Bay and a decrease in coastal sediment. Map unit polygons were digitized over underlying 2-meter base layers developed from multibeam bathymetry and backscatter data. The bathymetry and backscatter data were collected between 2006 and 2010. A map that shows these data is published in Open-File Report 2015-1068, ""California State Waters Map Series--Offshore of San Francisco, California."" This layer is part of USGS Data Series 781.In 2007, the California Ocean Protection Council initiated the California Seafloor Mapping Program (CSMP) to create a comprehensive seafloor map of high-resolution bathymetry, marine benthic habitats and geology within the 3-nautical-mile limit of California\'s State Waters. CSMP has divided coastal California into 110 map blocks, each to be published individually as United States Geological Survey Open-File Reports (OFRs) or Scientific Investigations Maps (SIMs) at a scale of 1:24,000. Maps display seafloor morphology and character, identify potential marine benthic habitats and illustrate both the seafloor geology and shallow (to about 100 m) subsurface geology. Data layers for bathymetry, bathymetric contours, acoustic backscatter, seafloor character, potential benthic habitat and offshore geology were created for each map block, as well as regional-scale data layers for sediment thickness, depth to transition, transgressive contours, isopachs, predicted distributions of benthic macro-invertebrates and visual observations of benthic habitat from video cruises over the entire state. This coverage can be used to to aid in assessments and mitigation of geologic hazards in the coastal region and to provide sufficient geologic information for land-use and land-management decisions both onshore and offshore. These data are intended for science researchers, students, policy makers, and the general public. This information is not intended for navigational purposes.The data can be used with geographic information systems (GIS) software to display geologic and oceanographic information.']"	['<urn:uuid:b6419768-1a03-4324-a19e-16ee481e5c58>', '<urn:uuid:5b540eaa-3962-4d86-8ec5-ee327eee7d28>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T06:15:13.904987	6	60	2019
77	plant disease resistance codes versus natural immunity types human body defense classification systems	Plants and humans have different classification systems for disease resistance. Plants use specific letter codes to designate disease resistance (like V for verticillium wilt, F for fusarium wilt, N for nematodes), while humans have three main types of immunity: natural (innate) immunity, adaptive immunity that develops throughout life, and passive immunity that can be borrowed from others like maternal antibodies.	['Be sure to sanitize tools regularly to halt the spread of disease from one plant to another\nBy Kate Russell\nPlants may not have an active immune system, but that doesn’t mean they just roll over and take whatever hits them. When pathogens strike, plants can respond in two different ways: they use structures and chemicals already in place, and they can make changes when a pathogen is sensed.\nJust as our skin blocks many pathogens from entry, a plant’s skin, or epidermis, does the same thing. That’s why insect feeding and mechanical damage can increase the chance of a disease taking hold — the plant’s first line of defense has been breached.\nPlant cell walls also block viruses, bacteria, and fungi. Unlike our human immune system, which uses white blood cells to actively hunt down and destroy invaders, plant cells have antimicrobial defenses built right in.\nThese defenses take the form of saponins, glucosides, and antimicrobial proteins. Enzyme inhibitors can also stop some pathogens from feeding on the plant. Plants also have chemicals that can neutralize toxins created by a pathogen. Finally, receptors can recognize a pathogen and alert the plant to take further action.\nOnce a pathogen is recognized, cell walls are reinforced and defensive chemicals, such as hydrogen peroxide, as well as those enzymes and proteins, are manufactured.\nIn some cases, plants have evolved a behavior called a ‘hypersensitive response’ in which the tissue surrounding an infection is killed off, to block further infection to neighboring cells.\nRather than treating a disease after it occurs, it is far easier to grow plants that have built-in defenses. This reduces the need for pesticides and fungicides.\nWhen shopping for plants, look at the plant label to see if that particular variety is resistant to diseases that tend to appear in your garden. Plant labels use the following codes to designate specific disease resistances:\nA – Alternaria stem canker\nF – Fusarium wilt\nN – Nematodes\nT – Tobacco mosaic virus (TMV)\nS – Stemphylium gray leaf spot\nV – Verticillium wilt\nSo, if you see a plant label with the letters, V, F, and N on it, that particular plant is resistant against verticillium wilt, fusarium wilt, and nematodes. This does not mean the plant is guaranteed to not get these diseases, but it does mean the chances are significantly lower.\nYou can help your plants fight disease by selecting resistant varieties, spacing your plants properly, employing crop rotation, and providing an adequate amount of water and nutrients.\nAlso, be sure to sanitize your tools regularly, to halt the spread of disease from one plant to another.\nKate Russell is a UCCE Master Gardener in Santa Clara County. For more information, visit mgsantaclara.ucanr.edu or call (408) 282-3105 between 9:30 a.m. and 12:30 p.m. Monday through Friday.\nLatest posts by Kate Russell (see all)\n- Your Garden . . . with Kate Russell: Trap cropping can be effective way to lure pests from MH gardens - August 23, 2019\n- Your Garden . . . with Kate Russell: Plants use structures and chemicals already in place to fight disease - August 9, 2019\n- Your Garden . . . with Kate Russell: Morgan Hill is a great place to start a garden - July 26, 2019', 'The immune system defends against infectious organisms and other invaders. Through a process called the immune response, the immune system attacks substances that invade body systems and cause disease\nTypes of immunity\nEveryone is born with some natural immunity that serves as general protection. For example, many of the viruses that cause disease in other species don’t affect us at all. And animals are born with natural immunity that protects them from susceptibility to human diseases. Innate immunity also includes the body’s external lines of defense, like the skin and mucous membranes that prevent disease from entering the body.\nAdaptive immunity develops throughout our lives and is the sum of our exposures to disease as well as our vaccinations against diseases.\nPassive immunity is a short-term or borrowed immunity from a person, typically a child’s mother. For example, maternal antibodies are transferred to a baby through the placenta. It can also be induced artificially through injections that contain targeted antibodies.\nThe primary components of the immune system:\n- The tonsils and the thymus\n- Bone marrow\n- White blood cells\n- Lymphatic system\nNatural approaches to support immune system function\n- Exercise and the immune system (Engage in moderate exercise )\n- Sleep (Get enough sleep )\n- vitamins and trace minerals (Vitamin A, Vitamin C, Vitamin D, Vitamin E, Zinc, Selenium, iron)\n- Avoiding the use of tobacco and chronic consumption of alcohol\n- Eat more whole plant foods (Whole plant foods like fruits, vegetables, nuts, seeds, and legumes are rich in nutrients and antioxidants)\n- Limit added sugars\n- Stay hydrated\n- Manage your stress levels (Relieving stress and anxiety is key to immune health )\nA healthy Ramadan\n– drink plenty of fluids, choose fluid-rich foods to make sure you are well hydrated for the day ahead, and go for starchy foods for energy, choosing high fiber or wholegrain varieties where possible as these can help keep you feeling fuller and can aid digestion, helping to prevent constipation. Below are some examples:\n- Oats – these are whole grains and you could choose porridge, which will also provide fluids as it’s made with milk or yogurt or overnight oats\n- High fiber breakfast cereals – these provide plenty of fiber and are often fortified with vitamins and minerals, providing extra nutrients. Because they are consumed with milk, you also get fluid and nutrients like calcium, iodine, and b vitamins from the milk.\n- Starchy foods like rice, or couscous – you could try rice pudding with fruit or experiment with couscous or other grains with dairy or fruit. If you go for savory dishes at suhoor then it’s a good idea make sure these are not too salty or they may make you very thirsty during the fast.\n- Yogurt – this can be a good food to include at suhoor as it provides nutrients like protein, calcium, iodine and b vitamins and also contains fluid. You could combine it with cereal and fruit\n- Bread – go for wholegrain options as these provide more fiber, for example, wholemeal toast or chapattis. Avoid combining bread with salty foods like hard cheese, or preserved meats. You could try nut butter (without added salt), soft cheese, or banana. As bread is fairly dry, make sure you drink plenty of water or other fluids alongside or you could have fluid-rich foods such as a lentil soup, which is a traditional food at suhoor in some countries.\n- Drinks – water, milk, fruit juices or smoothies – water provides hydration without any extra calories or added sugars. Drinks based on milk and fruit provide some natural sugars and nutrients – these are also good to break the fast but avoid drinking a lot of drinks with added sugars after breaking the fast as these can provide too much sugars and calories.\n- Dates – traditionally eaten to break the fast since the time of the Prophet Muhammad, dates are a great way to break the fast as they provide natural sugars for energy, provide minerals like potassium, copper and manganese and are a source of fiber. You could also try other dried fruits such as apricots, figs, raisins or prunes, which also provide fiber and nutrients.\n- Fruit and vegetables it provides natural sugars for energy, fluid and some vitamins and minerals.\n- Soup – Traditional soups are based on a meat broth and often contain pulses, like lentils and beans, and starchy foods like pasta or grains\n- Replace fried foods and sugary desserts with healthy alternatives\n- Use minimal salt for cooking and instead add herbs, spices, lemon, garlic and other seasonings\n8.Red bell peppers']	['<urn:uuid:9b939cb4-2553-4355-aa8b-abac6b1f0c0b>', '<urn:uuid:2110eb29-0330-401e-9652-111cfa6a69b1>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	13	60	1322
78	when did ebitda become popular measure leveraged buyout debt	EBITDA became popular in the 1980s as a measure for evaluating a company's ability to repay its leveraged buyout debt. During this time, leveraged buyout investors started using EBITDA to examine distressed companies and their repayment capacity.	['The Definition of EBITDA\nThe acronym EBITDA stands for earnings before interest and taxes. This measure is more appropriate in many industries than others. However, it is misleading when used in industries such as oil and gas production or mining. Companies in those industries will likely perform better based on EBIT. In those cases, the investor should consider using the other metrics to gauge company performance. For more information, see the definition of EBITDA below. And keep in mind that EBITDA does not include depreciation or losses from asset value.\nAs an accounting metric, EBITDA does not include working capital, interest rates, taxes, or capital expenditures. As such, its value can fluctuate in conjunction with those items. In addition, an EBITDA value of less than zero may be a sign of trouble. Conversely, a positive EBITDA value does not mean that a company is healthy. Because this metric does not account for taxes and interest, it is difficult to convert earnings to cash.\nEBITDA is an important financial metric that enables investors to understand the financial health of a company before making strategic decisions or making financing arrangements. This measure is also useful in determining the company’s ability to repay debt, liabilities, and obligations. While this metric is a useful tool for investors, it has its limitations. Specifically, it is only useful for certain companies. Therefore, it is crucial to understand its definition and how it is used in financial transactions.\nEBITDA Formula and Calculation\nThe EBITDA, or earnings before interest and taxes, is a measure of the profitability of an operating business. EBITDA is a great way to compare businesses, and determining the amount you need to make a profit is vital. If you’re looking to improve your business, you should calculate EBI TDA regularly to see where you stand. This article will help you understand how this metric is calculate. Let’s get started!\nUnderstanding EBITDA Before Buying a Company\nIf you’re thinking about buying a company, understanding EBITDA is important. However, before you can decide whether EBITDA is worth your time, you need to know what it actually means. While this figure is not regulated by the GAAP, it is still useful to investors, lenders, and potential acquirers. Understanding EBITDA is important for a variety of reasons. It can help you understand how profitable a company is and how to make adjustments to its numbers in future years.\nEBITDA and Leveraged Buyouts\nIn the 1980s, EBITDA became popular as a measure for the ability of a company to repay its leveraged buyout debt. As a result, leveraged buyout investors began to use EBITDA to examine distressed companies and their repayment capacity. Since then, leveraged buyouts have been a popular method of investing. But how can you use EBITDA to assess a company?\nThe Drawbacks of EBITDA\nIn today’s market, many investors use EBITDA as a measure to compare companies. Often, this allows investors to see which company is growing faster in terms of revenue and product sales. For example, a company with similar revenue and product sales may have different capital structures and tax jurisdictions, so an analyst may compare them to determine which is the better value. However, there are some drawbacks to EBITDA.\nWhy EBITDA Ignores Costs of Assets\nA company’s EBI TDA is often a misleading measure of liquidity, because it ignores costs of assets that are on the balance sheet and on property, plant, and equipment. Depreciation and amortization are not reflective of management’s decisions in the current period, but they allow for easier comparisons. By including these costs, companies can show a clearer picture of their performance.\nEBITDA Ignores Working Capital\nWhile it may seem like the EBI TDA metric ignores working capital, it’s essential to remember that it’s still crucial. After all, working capital is a business’s cash required for daily operations. But what happens when the cash runs out? The answer lies in reinvesting it into profitable investments as quickly as possible. Fortunately, EBITDA can help you avoid this pitfall.\nThe Limitations of EBITDA\nA company’s cash flow is typically measured in terms of EBI TDA. This metric is very popular and provides a quick understanding of a company’s worth. Depending on the company’s industry, it can range anywhere from zero to many times its EBITDA. In addition, it is a very good indicator of financial condition in a sector that you’re familiar with. But there are several limitations to this measure.\nHow EBITDA Obscures Company Valuation\nUsing an EBI TDA multiple to value a company can be a helpful strategy. But not all EBI TDA multiples are created equal. In fact, they can vary greatly across industries. In this article, we’ll cover how to value a company using EBI TDA. Using a multiple to evaluate the value of a company is crucial, but only if you know how to interpret it.\nEBITDA Vs EBT and EBIT\nThere is a difference between free cash flow and operating cash flow. The former includes changes in working capital, which both use and provide cash. Using these figures is important to determine cash flow, as failing to account for these changes can mask signs of trouble ahead. As a result, EBI TDA is better used alongside other performance metrics. EBIT is a much better measure of a company’s performance than free cash flow.\nDifferences Between EBITDA and Operating Cash Flow\nOperating cash flow and EBI TDA are two different measurements of a company’s earnings. Both measures are important to understand, but each can have different implications. It’s important to know the differences between these two measures before you make a final decision. Listed below are the differences between EBI TDA and operating cash flow. If you’re looking for an investor, be sure to learn about EBI TDA and how to calculate it for your business.\nExamples of EBITDA\nIf you are looking for some examples of EBI TDA, you have come to the right place. Here you will find the basic definition of EBI TDA, as well as examples of how to calculate it. EBI TDA stands for “Earnings before Interest and Taxes.” This figure is generated after depreciation, amortization, and selling, general, and administrative expenses are deducted. Regardless of your business type, you will have to pay some form of business tax.\nWhat Is a Good EBITDA?\nMany business owners and investors alike are interested in the question of “What Is a Good EBI TDA?” Regardless of your goals, an understanding of this measure is critical. This ratio is crucial in determining the profitability of a business and can help you form an exit strategy. For example, EBI TDA can help convince a prospective buyer that a business is a good investment and set an appropriate asking price.\nWhat Is Amortization in EBITDA?\nWhat is amortization in EBITDA? EBI TDA stands for earnings before interest and taxes and is the measure of profitability in operating a business. Generally speaking, the higher the EBI TDA, the better. This measure also considers the impact of amortization. It’s important to understand the underlying principles of EBI TDA, which are to maximize profitability and minimize risk.']	['<urn:uuid:e80beef1-c77e-46df-b78c-5b839e6d20cb>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	9	37	1189
79	how quickly can thiamine deficiency develop	Because thiamine is water soluble, a person could become deficient in as little as two weeks of deficit or increased demand.	['For those who enjoy the occasional adult beverage, the end stage plight of the falling down drunk seems afar off. For those of us who see alcoholics in the ER setting, the long term negative impact of the habit on cognitive function is as clear as day…or night, as is more commonly the case. The most severe version of alcohol induced dementia is known as Wernicke-Korsakoff syndrome, and impaired memory and confusion are leading symptoms. The main cause of this dementia is thiamine (Vitamin B-1) deficiency. If identified early enough, brain injury in these individuals can be resuscitated with thiamine supplementation.\nIt turns out that you don’t need to be an alcoholic to have thiamine deficiency induced cognitive dysfunction. Studies from back as early as the 90’s have shown that thiamine deficiency can be associated with and even predictive of milder forms of cognitive impairment. In one study over 50% of patients with cognitive decline had low plasma thiamine levels. 1\nUnfortunately, all of us will experience some degree of cognitive decline as we pass through the midlife years. Those who use alcohol socially, even in modest amounts can accumulate thiamine deficiency over time. Using diuretics, acid blockers or having had gastric bypass surgery can accentuate this potential deficiency. Because thiamine is water soluble, one could become deficient in as little as two weeks of deficit or increased demand.\nA 12-week trial of thiamine support at a very reasonable dosage of 100 mg/day has shown benefit in early dementia patients. 2 If you are a social drinker, and feel that you are having minimal but persisting problems with word recall, remembering names, short term memory-such as details of recent events, multitasking, or learning new tasks…when not drinking, of course ; – )… you may want to consider reducing your alcohol intake, and trying a 12-week thiamine repletion trial at 100 mg/day. I have worked with a number of patients over the years who have noticed a meaningful improvement of mental acuity with these two measures. As an aside, a form of thiamine called benfotiamine is used at times to give an enhanced benefit for peripheral neuropathy or liver repair, and may be superior to thiamine in this regard as a form of Vitamin B-1. However, it may not enhance central nervous system levels as well, and for the purposes of this discussion, thiamine, not benfotiamine, would be the better choice.\nKeep in mind that other reasons for ‘senior moments’ for those in the mid 40’s to 60’s can also include:\n–sleep deprivation, or loss of deep restorative sleep. This can be aggravated by hormonal imbalance (in both men and women), sleep apnea and urinary or prostate problems, among others.\n–vascular problems, including long term hypertension, plaque in arteries or hyperviscosity of blood. This can be accentuated by diabetes or smoking.\n–depression and anxiety disorders\nIf the factors we’ve discussed above have been addressed or controlled and you have persisting symptoms of cognitive loss, consider asking your primary physician about neuropsychiatric testing. This is a series of tests administered by a psychologist. It can help to define early problems with brain aging and suggest additional therapeutic measures. While age will take some edge off even the sharpest knife in the box, attending to the little things that can promote cognitive decline will help keep your wits close at hand as the years go by.\n1 Kanofsky, et.al. J Am College Nutrition 1996:15, 197-198\n2 Mimori, Y, et.al. Metabolic Brain Disease 1996:11. 89-94']	['<urn:uuid:4876dfb6-4964-4c67-b035-675c1cf0f11d>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	6	21	581
80	How do spring and summer droughts compare in intensity?	Research has identified a prominent spring drying trend in the U.S. Southwest, with seasonal migration of wet and dry patterns. During summer, conditions can rapidly intensify into severe flash droughts, as seen in 2017 when record evaporative demand combined with precipitation shortages. The spring drying is driven by changes in large-scale circulation and increased atmospheric moisture, while summer droughts are exacerbated by longer growing seasons and earlier soil water loss through evapotranspiration.	"['A large portion of the west U.S. was under the grip of an Drought that is unrelentingSince early 2020. Record-breaking rainfall has coincided with the dryness. WildfiresLong-lasting, intense and lasting Heat waves, Low stream flowsAnd Reservoir water supplies are decliningThe region has millions of people who rely on it.\nThe summer is upon us. OutlookIt is quite grim. The National Weather Service’s The latest seasonal outlookMay 19, 2022: A statement that drought is still a problem in most of the West and parts the Great Plains described it.\nOne of the drivers of the Western drought is persistent La Niña conditionsSince the beginning of the Summer 2020. During La Niña, cooler tropical Pacific waters help nudge the jet stream northward. This tends to bring about a change in the jet stream. There are fewer storms in the southern tier of U.S.Southwest droughts can be severe.\nThe OtherPerhaps the most important part of the story is the A rapidly warming climate makes it hotter and more thirsty..\nAs a climate scientist, I’ve watched how climate change is making drought conditions increasingly worse – particularly in the western and central U.S. The temperatures have been more than 2 degrees Fahrenheit (1.1 Celsius), higher than average in these regions over the past two years. Large swathes of the Southwest have seen temperatures exceeding 3 F (1.7 C) in recent years. Studies suggest the Southwest’s ongoing 20-year droughtBased on how dry the soils were, this is the worst drought in at least 1,200 Years.\nA hotter atmosphere absorbs more moisture from soil\nA thristier atmosphere tends extract more water from the soil. It causes it to worsen evaporative stressespecially if the region is experiencing lower-than-normal precipitation. High evaporative stress can quickly deplete soil moisture, leading to higher temperatures. Evaporative cooling effectThis causes a decrease in growth. This hydroclimatic stress can cause plants to wither, dry out, and even die.\nThe U.S. Southwest has seen an 8 percent increase in evaporative demand as a result of a warmer climate since the 1980s. This trend is generally understood. This is happening in other parts the country.\nThe This is a more relaxed atmosphereIt is transforming what might otherwise be dry or near-normal conditions into severe or extreme droughts. As the climate heats up, the increasing atmospheric thirst will only increase drought stress. This will have severe consequences for water availability and long-lasting, intense heat stress.\nClimate models predict bleak prospects for the future Arider climates and severe droughts are more commonIn the Southwest and the southern Great Plains in coming decades.\nThese regions will see an increase in droughts and heat. There are fewer stormsAnd More days without precipitation. Climate models consistently project an a Poleward shift in midlatitude storm tracksAs the planet heats up, this century will see fewer storms in southern tiers of the country.\nEven in the wettest areas, flash droughts are possible\nThe changing nature of droughts is a concern Even in areas of the U.S. where there is expected to be a net rise,In the 21st Century, there has been a significant increase in annual precipitation. A hotter future could lead to significant droughts due to the high evaporative demands on the land. These areas can experience prolonged periods of below-normal precipitation for many weeks to months, even though the overall trend is for more precipitation.\nFor example, large parts of the northern Plains have experienced precipitation. Increase of 10% or moreThe last three decades. The region is susceptible to severe drought in hotter climates.\nThe tail end of the what was the wettest decadeThe northern Plains were the only region to record this. A severe flash drought in the summer 2017.This resulted in agricultural losses of more than $2.6 billion. WildfiresSurrounding millions of acres Record evaporative demandThe flash drought was exacerbated by a severe short-term shortage of precipitation. A flash drought can be defined as a drought that lasts for more than one day. In a matter of weeks, the intensity increases rapidlyIt often surprises forecasters. The possibility of flash droughts that can can have severe consequences for agriculture and ecosystems, and can promote large wildfiresExpected to IncreasingWith a warmer and thirstier environment\nIn the Northeast, flash droughts are becoming a growing concern. New England was home to an estimated 12% of the 2020 drought. Extreme hydrologic droughtThe drought was severe, with low stream flows, groundwater levels and widespread crop loss between May and September. The drought developed quickly from the above-normally wet conditions due to very warm and dry atmosphere conditions.\nAs humanity enters a hotter future it will be more concerning to experience prolonged periods of below-normal precipitation for weeks or months.\nHeading into unfamiliar territory\nOther types of droughts are also on the horizon.\nAtmospheric heat is a problem Snow droughtsSnow melts earlier if there is more rain than snow. Warmer temperatures result in shorter snow seasons, and longer growing seasons. Modifying the timing and frequency of ecological responses.\nLand is Greening up earlierThis can lead to an earlier loss of water on the land surface. evapotranspiration – the loss of water from plants and soil. This could lead to soils that are drier in the second half of the growing seasons. This could lead to dry soils in the western and central U.S. Both future greening and drying will be more frequentThey are seasonally distinct across the growing season.\nWe are entering an era of rapid change in the climate. unfamiliar territory. New ways will be needed in the world To better anticipate future droughtsThis could lead to a transformation NaturalAnd Human systems.', 'Atmospheric Sciences & Global Change\nWater Woes Projected in U.S. Southwest\nRegional water availability reductions ahead according to research that reveals spring drying trends\nA dry irrigation canal illustrates one challenge presented by large shifts in spring wet and dry periods across the U.S. Southwest—reductions in water availability. Authors of this study found that the intensity of the water cycle, due to more atmospheric moisture held captive in a warming climate, leads to changes in water availability with regional impacts and implications for society’s resilience to climate change. Enlarge Image.\nResults: New research from scientists at Pacific Northwest National Laboratory identified a trajectory of spring drying that will alter water availability across the U.S. Southwest. To describe the net change in water, they calculated the water gained through precipitation and subtracted the water lost through evaporation and transpiration in an ensemble of regional and global simulations. Their findings indicate future challenges for regional water resource managers and agricultural production.\n""Our research identified a prominent spring drying trend over the U.S. Southwest and a seasonal migration of the wet and dry patterns,"" said Dr. Yang Gao, post-doctoral researcher and atmospheric scientist at PNNL. ""These signals are strong and consistent across regional and global climate projections.""\nWhy It Matters: Water in the soil, streams, and groundwater is critical for Earth ecosystems and food and energy production, so understanding and projecting how water availability is going to change in the future is a major science challenge with important societal implications. While global precipitation is projected to increase in a warmer climate, water availability is a measure determined by regional precipitation and the amount of moisture that either evaporates or is taken up by plants, called transpiration. Together, precipitation and evapotranspiration vary greatly over a region, and their changes are due to complex alterations in atmospheric circulation, soil moisture, and many other processes. The new research underscores the importance of evaluating changes in regional North American water availability and propels efforts to evaluate their implications for climate impacts and adaptation.\n""Increasing greenhouse gases cause the atmosphere to hold more moisture in a warmer climate, intensifying Earth\'s water cycle,"" said Dr. Ruby Leung, PNNL Laboratory Fellow, atmospheric scientist and corresponding author on the study. ""Changes in the water cycle have significant implications for water availability that challenge regional resilience to climate change.""\nMethods: The PNNL research team calculated changes in water availability from a multi-model ensemble of climate simulations for the present (1975-2004) and the future (2070-2099) produced by a single regional climate model and 20 global climate models used in the Intergovernmental Panel on Climate Change (IPCC) Fifth Assessment Report (AR5). Through both model agreement and statistical significance they identified robust changes across the models. They investigated the relationships between seasonal large-scale circulation and water availability changes using regression analysis to evaluate the contributions of dynamical changes, in addition to increased moisture availability, to changes in precipitation minus evapotranspiration (P-E). Lastly, they depicted seasonal P-E changes by the seasonal timing of the dominant wet/dry trends derived from the regional and global model projections to highlight their robust features over North America.\nTheir research shows that seasonal changes in the large-scale circulation and increased atmospheric moisture due to warmer temperatures lead to wet/dry trends that are dominated by two distinct and opposite north-south and east-west patterns.\nWhat\'s Next? The multi-model ensemble of regional and global climate simulations will be further analyzed to evaluate changes in extreme events such as heat waves and cold air outbreaks, heavy precipitation, and other climatic features important for understanding societal vulnerability to climate change in the United States.\nSponsors: The U.S. Department of Energy Biological and Environmental Research supported this research as part of the Regional and Global Climate Modeling Program and Integrated Assessment Research Program. The regional climate simulations used in the analysis received support from the PNNL Platform for Regional Integrated Modeling and Analysis (PRIMA) initiative, a PNNL Laboratory Directed Research and Development program.\nResearch Area: Climate and Earth Systems Science\nReference: Gao Y, LR Leung, J Lu, Y Liu, M Huang, and Y Qian. 2014. ""Robust Spring Drying in the Southwestern U.S. and Seasonal Migration of Wet/Dry Patterns in a Warmer Climate."" Geophysical Research Letters 41(5):1745-1751. DOI:10.1002/2014GL059562']"	['<urn:uuid:dac81062-03ad-42f8-8195-a9c819c1270a>', '<urn:uuid:dbd48dc3-5734-44f5-8c42-ba90935c9193>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	9	72	1627
81	recent range changes adaptations fox coyote relative human presence	Coyotes have shown remarkable adaptability to human presence, being able to exploit human-modified environments including urban areas. They are opportunistic and generalist predators that can consume various food items including human-related resources like dog food. Their populations are increasing as humans modify landscapes. Red foxes have also established new populations in various regions, though through different mechanisms. In the western U.S., some populations originated from natural range expansions while others, particularly in western Washington and southern California, came from human-mediated translocations. However, unlike coyotes, the documents don't explicitly discuss red foxes' adaptability to human presence.	"[""|Scientific Name:||Canis latrans|\n|Species Authority:||Say, 1823|\n|Red List Category & Criteria:||Least Concern ver 3.1|\n|Assessor/s:||Gese, E.M., Bekoff, M., Andelt,W., Carbyn, L. & Knowlton, F.|\n|Reviewer/s:||Sillero-Zubiri, C. & Hoffmann, M. (Canid Red List Authority)|\nThe Coyote has a wide distribution throughout North America, Mexico and into Central America. They are abundant throughout their range and are increasing in distribution as humans continue to modify the landscape. The species is very versatile, especially in their ability to exploit human modified environments.\n|Range Description:||Coyotes were believed to have been restricted to the south-west and plains regions of the U.S. and Canada, and northern and central Mexico, prior to European settlement (Moore and Parker 1992). During the 19th century, coyotes are thought to have expanded north and west. With land conversion and removal of wolves after 1900, coyotes expanded into all of the U.S. and Mexico, southward into Central America, and northward into most of Canada and Alaska (Moore and Parker 1992).\nCoyotes continue to expand their distribution and occupy most areas between 8°N (Panama) and 70°N (northern Alaska). They are found throughout the continental United States and Alaska, almost all of Canada (except the far north-eastern regions), south through Mexico and into Central America (Bekoff 1982; Reid 1997; Bekoff and Gese 2003).\nNative:Belize; Canada; Costa Rica; El Salvador; Guatemala; Honduras; Mexico; Nicaragua; Panama; United States\n|Range Map:||Click here to open the map viewer and explore range.|\nCoyotes are abundant throughout their range and are increasing in distribution as humans continue to modify the landscape. Elimination of wolves may also have assisted Coyote expansion. Coyote density varies geographically with food and climate, and seasonally due to mortality and changes in pack structure and food abundance. Local control temporarily reduces numbers on a short-term basis, but Coyote populations generally are stable in most areas.\nCoyote densities in different geographic areas and seasons vary from 0.01–0.09 coyotes/km² in the winter in the Yukon (O'Donoghue et al. 1997) to 0.9/km² in the fall and 2.3/km² during the summer (post-whelping) in Texas (Knowlton 1972, Andelt 1985). Density in different geographic areas and seasons are listed in Sillero-Zubiri (2004).\n|Habitat and Ecology:||\nCoyotes utilize almost all available habitats including prairie, forest, desert, mountain and tropical ecosystems. The ability of coyotes to exploit human resources allows them to occupy urban areas. Water availability may limit Coyote distribution in some desert environments.\nCoyotes are opportunistic, generalist predators that eat a variety of food items, typically consuming items in relation to changes in availability. Coyotes eat foods ranging from fruit and insects to large ungulates and livestock. Livestock and wild ungulates may often be represented in coyote stomachs and scats as carrion, but predation on large ungulates (native and domestic) does occur (Andelt 1987). Predation by Coyotes on neonates of native ungulates can be high during fawning (Andelt 1987). Coyotes in suburban areas are adept at exploiting human-made food resources and will readily consume dog food or other human-related items.\nThere are no current threats to Coyote populations throughout their range. Local reductions are temporary and their range has been expanding. Conservation measures have not been needed to maintain viable populations. Coyotes adapt to human environs and occupy most habitats, including urban areas. Hybridization with dogs may be a threat near urban areas. Genetic contamination between dogs, Coyotes, and Grey Wolves may be occurring in north-eastern U.S. Hybridization between Coyotes and Red Wolves is problematic for Red Wolf recovery in south-eastern U.S.\nCoyote fur is still sought by trappers throughout its range, with harvest levels depending upon fur prices, local and state regulations, and traditional uses and practices. Many states and provinces consider Coyotes a furbearing species with varying regulations on method of take, bag limit, and seasons.\nThe species is not included on the CITES Appendices, and there is no legal protection of the species. Restrictions on harvest and method of harvest depend upon state or provincial regulations.\nThe Coyote occurs in almost all protected areas across its range.\nOccurrence in captivity\nOver 2,000 Coyotes occur in captivity in zoos, wildlife centres, etc., throughout their range. They readily reproduce in captivity and survival is high.\nGaps in knowledge\nSeveral gaps in knowledge still remain: coyote reproductive physiology and possible modes of fertility control; selective management of problem animals; effects of control; genetic differentiation from other canids (particularly the red wolf); development of non-lethal depredation techniques; interactions of coyotes and other predators; coyote-prey interactions; human-coyote interactions and conflicts at the urban interface; factors influencing prey selection; communication; adaptations in urban and rural environments; and interactions with threatened and endangered species.\n|Citation:||Gese, E.M., Bekoff, M., Andelt,W., Carbyn, L. & Knowlton, F. 2008. Canis latrans. In: IUCN 2013. IUCN Red List of Threatened Species. Version 2013.2. <www.iucnredlist.org>. Downloaded on 08 March 2014.|\n|Feedback:||If you see any errors or have any questions or suggestions on what is shown on this page, please fill in the feedback form so that we can correct or extend the information provided|"", 'Title: The origin of recently established red fox populations in the United States: translocations or natural range expansions?\nAuthor: Statham, Mark J.; Sacks, Benjamin N.; Aubry, Keith B.; Perrine, John D.; Wisely, Samantha M.;\nSource: Journal of Mammalogy. 93(1): 52-65\nPublication Series: Scientific Journal (JRNL)\nDescription: Red foxes (Vulpes vulpes) are native to boreal and western montane portions of North America but their origins are unknown in many lowland areas of the United States. Red foxes were historically absent from much of the East Coast at the time of European settlement and did not become common until the mid-1800s. Some early naturalists described an apparent southward expansion of native foxes that coincided with anthropogenic habitat changes in the region. Alternatively, red foxes introduced from Europe during Colonial times may have become established in the east and subsequently expanded their range westward. The red fox also was absent historically from most lowland areas of the western United States. Extant populations of red foxes in those areas are considered to have arisen from intentional introductions from the east (and by extension are putatively European), escapes or releases from fur farms, or range expansions by native populations. To test these hypotheses we compared mitochondrial DNA sequences (cytochrome b and D-loop) from 110 individuals from 6 recently established populations to 327 native (primarily historical) individuals from Eurasia, Alaska, Canada, the northeastern United States, and montane areas in the western contiguous United States, and to 38 individuals from fur farms. We found no Eurasian haplotypes in North America, but found native haplotypes in recently established populations in the southeastern United States and in parts of the western United States. Red foxes from the southeastern United States were closely related to native populations in eastern Canada and the northeastern United States, suggesting that they originated from natural range expansions, not from translocation of European lineages, as was widely believed prior to this study. Similarly, recently established populations in the Great Basin and in western Oregon originated primarily from native populations in western montane regions, but also contained a few nonnative North American haplotypes. In contrast, populations in western Washington and southern California contained nonnative, highly admixed stock that clearly resulted from intracontinental translocations. Several common haplotypes in these populations originated in regions where fur-farm stocks originated. Although European red foxes translocated to the eastern United States during Colonial times may have contributed genetically to extant populations in that region, our findings suggest that most of the matrilineal ancestry of eastern red foxes originated in North America.\nKeywords: fur farm, mitochondrial DNA, museum samples, phylogeography, red fox, translocation, Vulpes vulpes\n- We recommend that you also print this page and attach it to the printout of the article, to retain the full citation information.\n- This article was written and prepared by U.S. Government employees on official time, and is therefore in the public domain.\nXML: View XML\nStatham, Mark J.; Sacks, Benjamin N.; Aubry, Keith B.; Perrine, John D.; Wisely, Samantha M. 2012. The origin of recently established red fox populations in the United States: translocations or natural range expansions? Journal of Mammalogy. 93(1): 52-65.\nGet the latest version of the Adobe Acrobat reader or Acrobat Reader for Windows with Search and Accessibility']"	['<urn:uuid:71fe3a7a-9cda-4f1f-a57a-f4987a0c4a1d>', '<urn:uuid:744672fd-9ede-458d-ab95-a78211696af4>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-13T06:15:13.904987	9	95	1359
82	As someone who studies funeral customs worldwide, I'm curious about how Japanese funeral traditions blend with modern trends - what's the traditional dress code at Japanese funerals, and how is this evolving in contemporary services?	Traditionally in Japanese funerals, guests wore white, but today it is most common to wear formal black attire. In modern society, while dark colored clothing is still preferred, there's a shift toward less rigidly defined funeral wear, with some circles allowing for greater individual expression, including ostentatious ladies' outfits with large hats and showy jewelry, and more casual men's attire instead of the traditional dark suits and black ties.	['Japan is a country steeped in tradition. Many Japanese-Americans still follow these traditions especially those revolving around the major stages of life: birth, marriage, and death.\nIn Japan, the most common funeral services follow the Buddhist faith. The Buddhist ceremony consists of four parts: a wake, cremation, burial, and memorial services. Japanese-American funeral services follow the same elements although in varying degrees. To understand Japanese-American funeral services, you must first understand the funeral customs and traditions of Japan.\nAt the wake, the body is placed in a casket with the head facing north. Traditionally guests of Japanese funerals wore white, but today it is most common to wear formal black attire. At the wake a Buddhist priest will read a sutra. The priest will be carrying a prayer bead called a juzu. Guests may also be given a juzu upon arrival. Family members will then offer incense before the deceased. Traditionally, condolence money will be offered to the family to help with funeral expenses. People paying their respect will give the family an envelope called a koden which will contain anywhere from fifty to three hundred dollars. At the end of the wake all guests will be given an appreciation gift. Following the wake, close relatives will stay overnight with the deceased.\nThe Cremation Service\nThe day after the wake is the day of cremation. The traditional Japanese cremation service differs from traditional cremation in America. In Japan, the casket is taken to the crematorium, and the family watches as the casket is placed inside. The family members then leave and wait for the completion of the cremation. After the cremation is competed the family returns to transfer the bones of the deceased into an urn. This is traditionally done using chopsticks. Family members will pass the bones from chopsticks to chopsticks and into the urn. The order in which the bones are moved is important. The bones must be picked up starting with the feet to the head in order to ensure the deceased is not placed upside down.\nJapanese Burial Options\nAfter the cremation service, family and friends may travel directly to the family grave – haka in Japanese – or keep the urn at the family home for several days. Traditionally, the entire family will eventually be buried in a chamber beneath the family grave. Today, it is becoming common to keep the urn or to scatter the ashes in a symbolic location. If the deceased is buried in a family grave, family and guests may leave flowers, water, and incense in front of the site.\nJapanese Memorial Customs\nJapanese memorial customs depend strongly on family traditions. Generally, the first forty-nine days are very important for the soul of the deceased, since it is believed that the soul does not enter heaven until the forty-ninth day. A common custom is to hold a memorial service every seventh day until the forty-ninth day. At the memorial service, a Buddhist monk will chant a sutra, either at the family’s home or at the local temple. The family and friends will then place fresh flowers and burn incense at the family grave. Everyone will join together for a meal.\nFinally, it is very important to understand the religious beliefs of the person who has passed. Japanese Americans vary in traditions and beliefs. The first step in planning a funeral ceremony for a Japanese-American is to consider the beliefs of the deceased. For example, a Christian may want a funeral more similar to the traditional American style. Also, if family still lives in Japan, they should be contacted. They could help shed light on family customs. Lastly, it would be most appropriate to contact a Buddhist temple if attempting to conduct a Buddhist ceremony. They may aid the funeral home in properly performing the Buddhist funeral ceremonies.\nBuddhist Offering photograph from acdme on Flickr.', 'Subscribe to our free newsletter to receive all the latest offers...\nFuneral Etiquette: Trends for Modern Mourning\nIf you’ve never been to a funeral before and are concerned about what exactly funeral etiquette involves, don’t worry. The following guidelines will help you navigate the (often unspoken) ‘rules’, enabling you to rise to the occasion with confidence.\nUniversal Funeral Etiquette\nFuneral traditions, rituals and conventions vary considerably in different countries and cultures, worldwide.\nHowever, appropriate etiquette, wherever you may be in the world, is based essentially on showing profound respect for the deceased and conducting oneself in a dignified manner, according to the prevailing funeral conventions.\nDress Code Trends for Modern Mourning\nEven in modern society, the overriding aim should always be to dress respectfully and appropriately.\nAs a general guideline, dark coloured (not necessarily black), conventional clothing is still the preferred form of attire for attending funerals. Men, traditionally, wear black ties and dark suits; women, generally, choose subdued coloured outfits such as smart trouser suits, below-the-knee skirts, modest dresses and jackets, etc.\nConversely, wearing unconventional or bright-coloured clothing may be equally appropriate, particularly where the deceased has previously instructed family and friends not to wear black at the funeral.\nChildren & Funerals\nWhether children should be allowed to attend funerals depends largely on the age and maturity of the individual child involved, as well as the nature of child’s relationship with the deceased.\nA strong argument exists for children who are old enough to understand what is happening to attend the funeral, not least as a means of facilitating the grieving process and gaining closure.\nEqually, very young children, infants and babies who would inevitably be oblivious to events, are probably better off not attending the funeral. Also, on a practical note, the presence of a crying baby at a funeral can prove highly stressful, particularly for the adult in charge.\nIf in doubt, consult your funeral director about whether the presence of children is deemed appropriate; Funeral Directors.\nWhen attending a funeral, it is common practice for the mourners (apart from immediate family) to arrive early and take up position, at least 5-10 minutes before the funeral officiant commences proceedings.\nThe seating at the front of the funeral venue is reserved for immediate family, with other mourners occupying the seats to the rear.\nAfter the ceremony, the mourners stand while close family members leave the building, first. The immediate family then usually pauses to acknowledge those who have attended the funeral and maybe exchange a few brief words.\nTrends for Modern Mourning\nTrends in contemporary funeral etiquette include:\n● a shift away from religious observance and ritual towards personalising funeral ceremonies in a non-religious context;\n● less formal and rigorously defined funeral-wear, in certain circles, in favour of greater individual expression, sometimes even with a hint of drama, e.g. ostentatious ladies’ outfits complete with large hats, showy jewellery, dark glasses, etc., and more casual men’s attire instead of dark suits and black ties;\n● increased use of social networking websites to pay tribute and mourn the deceased, online.\n© Funeral Services Guide']	['<urn:uuid:d24bacb7-3e7a-404b-9499-a6e8bb5fd2c9>', '<urn:uuid:9c9e9205-62cb-4502-b475-2494f9482fa5>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	35	69	1157
83	What are the traditional business practices of merchandising companies, and how are some modern fashion retailers changing these practices to become more sustainable?	Traditional merchandising companies focus on selling tangible goods through promotional strategies that stimulate purchasing behavior, following retail cycles tied to seasons and holidays to maximize sales. They aim to increase total sales turnover through various promotional tools and strategic product placement. However, some modern fashion retailers are transforming these traditional practices with sustainable approaches. Companies like H&M are incorporating repair services in their stores, allowing customers to fix and upcycle clothing. Major brands like Nike and Adidas are adopting sustainable practices - Nike incorporates recycled polyester in 75% of their products and recycles manufacturing waste, while Adidas partners with organizations to recover ocean plastics for their products. Some companies like People Tree and Organic by John Patrick are focusing on eco-friendly materials and Fair Trade partnerships, demonstrating how merchandising can evolve beyond traditional sales-focused approaches to include environmental responsibility.	['What is Merchandising?\nMerchandising is the practice of promoting products that are available for retail. It entails selecting promotional tools available to both the manufacturer and the retailer – dubbed as a promotional mix.\nMerchandising strategies include personal selling, sale promotion, marketing strategies, creating coupons, and discounts. More broadly, merchandising may refer to in-store or on-store promotion other than personal selling meant to promote purchasing behavior.\nCycles of merchandising depend on culture and season and may accommodate seasonal holidays, school schedules, and weather patterns. With the advent of self-service, merchandising is now considered a scientific art, given that universal rules govern its activities, and customer behavior may be modified in predictable ways.\n- Merchandising is the process of promoting sales of goods and services to sustain and amplify customer activity within a retail environment.\n- The fundamental concept of merchandising is to stimulate customers’ purchase behavior to reduce retail stores’ off-the-shelve products.\n- Merchandising companies are different from service companies, in that the former sells tangible goods to generate income, unlike the latter, which provides services.\nHow Merchandising Works\nThe Concept of Merchandising\nThe precise definition of merchandising depends on the specific context, but in reality, it involves stimulating interest and enticing customers to make purchases.\nFor example, at the marketing level, merchandising refers to the management of a product’s life cycles and ensuring the right resources – such as space, brand, and display – to sell another product.\nBoth retailers and manufacturers are interested in motivating purchase behavior. However, retailers are often more interested in increasing total sales turnover, unlike manufacturers whose intention is to depress competitors’ sales and stimulate their own brands.\nThe two conflicting aims are reflected in sales. For example, Brand A is a low-profit line for retailers who do not devote much shelf space to it. On the other hand, the manufacturer is likely to emphasize on shelf space.\nIn that regard, some manufacturers draw from the benefits of good retail presentation to employ merchandising representatives to help move the products off the shelves.\nMerchandising and Sale Performance\nFinding the gross value of all sales can show the company’s performance since retailers may or may not be producers of products they sell. This notion is particularly true where a retailer acts as the third party by linking sellers and buyers without taking part, as in the customer-to-customer market.\nAnother sector that merchandising is commonly applicable is the consignment sector. Retailers in this industry never purchase their inventory officially. The retail business serves as the legal re-seller of another entity’s merchandise even though the items are stored in its location.\nIn reality, retail companies are never legitimate producers, as the real owner who placed an item on consignment may claim it at will. The total value of merchandise sold via a customer-to-customer exchange site over a period is called the gross merchandise value. It measures business’ growth.\nRetail Cycles in the U.S.\nRetail sales in the U.S. have a clear annual cycle that starts at the onset of January. The significant surge of retail activities is due to merchandising activities associated with Valentine’s Day and St. Patrick’s Day products. Special discounts and sales during Presidents’ Day usually follow shortly after January.\nAnother major holiday that depletes inventories in the U.S. is Easter. Higher sales are promoted by the holiday, springtime, as well as the associated warmer weather.\nThe lines of products that realize most sales traffic at such time are clothing and tools appropriate for outdoor activities and the warmer weather. The products are in plenty and readily available during the mid-winter with high store traffic and total sales turnover.\nThe high sales are thanks to strategic marketing and promotion to make room for the next batch of products. The cyclical behavior continues throughout the year, accounting for other special days such as Thanksgiving, Memorial Day, Father’s Day, Labor Day, and Christmas Day.\nBenefits of Merchandising\nMerchandising is one of the most inexpensive promotional mix tools, especially if a company is experienced and skilled in its use. It may be valuable to companies with limited promotional funds since techniques such as product facing control, and better shelf-positioning may incur little to no additional expenditure. Also, sales representatives whose salaries are already being paid can perform these tasks.\nInfluence at the point of sale\nMerchandising influences customers at that final stage of a buying decision. As a result, marketers strive to establish preference awareness of their brands before the customer enters the store. This allows the target customer to specify brands at the point of sale.\nThe preference is achieved through a consumer franchise. It is a public relation and an advertisement component of the promotional mix used to achieve brand preference.\nEnlightened manufacturers who are not ready to lose the sale at the store supplement their expensive products with inexpensive merchandising, especially when the competition is stiff.\nMerchandising Companies vs. Service Companies\nService companies provide services to customers who are attracted by their expertise and innovation to generate income. Such companies may include insurance providers and accounting firms.\nConversely, merchandising companies engage in the sale of tangible goods. They may incur additional expenditure on materials and labor to present products that stimulate consumers.\nCFI is the official provider of the global Commercial Banking & Credit Analyst (CBCA)™ certification program, designed to help anyone become a world-class financial analyst. To keep advancing your career, the additional CFI resources below will be useful:', 'It’s payday. You get off work, head over to your favorite store at the mall or your favorite online retailer, and use your latest check to overindulge in some retail therapy. You spend hours poring over shelves and racks (or web pages), considering styles and looking for the perfect bargain in your size. It’s extremely easy to get caught up in the excitement of the hunt for that perfect outfit. When you find it, the rush you get (especially when it’s a financial steal) is part of the appeal. You’re thinking about when and where you’re going to wear it, rather than the environmental implications of its production, purchase, and disposal.\nIt’s easy to see that the how of clothing production is less well understood and publicized than the marketing of the clothes themselves. Fashion retailers want you to find that consumer high and spend your money, not think about how the clothes are made or what happens to them when they go out of style.\nSome people would classify that as a blissful ignorance, but I think it has more to do with a general lack of mainstream media promotion of sustainable fashion as a real, attainable thing. How are people supposed to know about something they’ve never heard of? On the surface, sustainability and fashion are two completely unrelated things, so it’s not surprising that people don’t see how production and consumption influence one another. But the relationship between these two are very much intertwined. It’s important for the public to start taking note.\nPhoto Courtesy of GreenEarth Cleaning\nThe issue with the fashion industry as a whole, specifically regarding environmental sustainability, is its determination to keep up with quickly changing trends. The average shopper buys much more clothing than did a few decades ago, but keep items half as long. They want trendy, stylish clothing in an accessible location at a reasonable price and they want it NOW. Fast fashion refers to the production of clothing at high speed and low cost to meet increasingly impatient consumer demand. Fast fashion companies are most concerned with lowering costs and getting the product from design to shop floor as quickly as possible. While fast fashion is economically attractive to retailers, the fast-paced production expectations have an enormous environmental impact.\nAccording to the United Nations Economic Commission of Europe, the fashion industry is responsible for producing 20% of global wastewater and 10% of global carbon emissions. That’s more than the emissions of international flights and maritime shipping combined. Creating vibrant and colorful textiles is a critical part of the industry, yet the toxic chemicals associated with textile dyeing are the second largest polluter of clean water globally. Fast fashion has also led to oceans getting bombarded with over a half million tons of plastic microfibers per year due to the washing of plastic-based textiles (polyester, nylon, or acrylic). Textile waste may be an unintended consequence of fast fashion, but that doesn’t mean the industry shouldn’t be held accountable. The problem has now reached a global level. Wardrobes in developed nations are oversaturated. Retailers want to sell as many products as possible and to do that they must stay ahead of the curve, convincing shoppers that their brand is a “must-have” and the items already in their closet are no longer fashionable.\nSustainability and Fashion\nWhile eco-friendly fashion is a trend on the rise, no environmental organization had decided on a concrete definition for what sustainable fashion truly means and stands for. The Ellen Macarthur Foundation recently published a report titled, A New Textiles Economy: Redesigning Fashion’s Future, which establishes sustainability goals for the fashion industry. This report includes four recommendations:\nPhase out substances of concern and microfibre release\nA. Align industry efforts and coordinate innovation to create safe material cycles.\nB. Drastically reduce plastic microfibre release\nTransform the way clothes are designed, sold, and used to break free from their increasingly disposable nature\nA. Scale-up short-term clothing rental\nB. Make durability more attractive\nC. Increase clothing utilization further through brand commitments and policy\nRadically improve recycling by transforming clothing design, collection, and reprocessing\nA. Align clothing design and recycling processes\nB. Pursue technological innovation to improve the economics and quality of recycling.\nC. Stimulate demand for recycled materials.\nD. Implement clothing collection at scale.\nMake effective use of resources and move to renewable inputs\nThe report was written in consultation with many major fashion design companies. Its underlying message is that fashion that should be produced and sold in the most eco-friendly way possible during all stages of production, including design, raw material production, manufacturing, transport, storage, marketing, and final sale. Sustainable fashion can be achieved by ensuring efficient and careful use of natural resources, selecting renewable energy sources (wind, solar, etc.) at every stage, and maximizing repair, remake reuse, and recycling of the product and its components. It even uses sustainability storytelling by highlighting how environmentally conscious fashion can significantly lower costs to businesses and create positive long-term customer relationships.\nIdeally, the rules of A New Textiles Economy would be incorporated into every fashion and lifestyle brand’s business model and production processes. Some companies have done so, but there is more work to be done. These standards help raise awareness of sustainability practices, as well as show manufacturers that considering the environment doesn’t have to be expensive or overly complicated.\nPhoto Courtesy of Fashionista\nSo with A New Textile Economy’s standards in mind, here are some examples of companies that are incorporating sustainability into their brands.\n- Organic by John Patrick created the world’s first organic oxford cotton shirt and uses other eco-friendly materials such as vegetable-tanned leather, organic wool, and recycled cotton.\n- People Tree, partners with Fair Trade artisans and farmers to produce a wide collection of ethical and eco-wear for over 20 years.\n- Patagonia’s mission statement incorporates sustainability by making quality products that can be repaired, supporting grassroots activists by paying an Earth Tax, and supporting regenerative practices in ranching and agriculture.\n- Adidas partners with Parley Plastics to incorporate plastics recovered from the ocean in their shoes, clothing, and plastic hangers. On some of their product lines, they’ve also debuted ZeroDye, which features undyed polyester. Finally, they’re partnering with Fashion for Good to green the apparel supply chain. Read more about Adidas’ sustainability initiatives here.\n- Nike incorporates recycled polyester into 75% of their shoes and apparel. The dye process used for soles in the Nike Air line allows 99% of recoverable dye water to be recycled. All Air sole innovations designed since 2008 are composed of at least 50% recycled manufacturing waste. Nike also transforms used product into Nike Grind, which is created from recycled athletic footwear and surplus manufacturing scraps to make performance products, ranging from new footwear and apparel to sports surfaces. Read more about Nike’s sustainability efforts here.\n- H&M’s new Paris flagship store includes an entire floor dedicated to clothing repair. It includes a fleet of sewing machines, bottles of natural detergents, and eco-friendly stain removing sprays, along with bins of patches and embroideries. The idea is to invite customers to bring in clothing, both H&M and other brands, to be repaired and made new again. Customers can restyle their old clothing to update and upcycle their looks. The floor also includes a recycling station where customers can recycled their old clothes (not just H&M) in exchange for a store discount.\nPhoto Courtesy of Patagonia\nFor more info on sustainable brands visit http://www.thegoodtrade.com/features/fair-trade-clothing and https://www.marieclaire.com/fashion/a22237305/sustainable-closet-guide/.\nThe Bottom Line\nTurning the fashion industry into something noticeably more sustainable won’t happen overnight. Some brands will continue to pursue short-term economic gains over longer-term environmental costs. That’s why it’s so important for consumers to understand the impact of their choices on the industry. Fast fashion is preventable. If we take a second to slow down and pay attention to how much our desire for the latest styles at the lowest prices encourages companies to cut environmental corners, real progress can happen. The more we become informed about these issues and use that information to change our daily routines, the more the fashion industry will be pressured to think more sustainably. Next time it’s payday and you go on your shopping binge, shop with a conscience. Or don’t shop at all.']	['<urn:uuid:bd6dfeea-8bea-4458-b8a9-82b75ff3b26e>', '<urn:uuid:6f693a61-fd49-4e0c-bab1-a38fc9e1be10>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:15:13.904987	23	139	2283
84	What kind of materials and features did they use to make this Portuguese vacation home blend with the natural surroundings?	The house uses simple materials like smooth concrete flooring inside and a concrete water tank outside. The water tank serves as a quiet reflective pool and connects to an existing granite wall and a rural path. The design includes strategic openings in the plaster walls that frame views of nature and the valley, while the building is embedded into the site across two levels.	"[""Porto-based studio Pablo Pita designs a modest retreat near the Douro River\nPorto-based architecture studio Pablo Pita has designed a modest vacation dwelling for a family of four in a village near the Douro River in Portugal. The simple house is a product of the landscape and the orientation of the valley view, as well as its low cost budget.\nWhile the brief was admirably straightforward – the family required three bedrooms and an informal common space for slow holiday living – it was the site and the landscape that prescribed the design of the house.\n‘The project required a fairly long process because of the nature protection regulations of the specific site,’ says Pablo Rebelo. ‘The site had these slopes facing an incredible view, that unfortunately faced north west. We decided to create a narrow design with openings to both sides, so that we could keep the views but still have a good lighting orientation.’\nRebelo and fellow founding architect Pedro Pita, who Wallpaper* featured in the Architects Directory 2018, drew large, yet strategic openings into the plaster walls, that frame patches of pure nature or views into the valley and horizon: ‘As you enter the house you have this wide window facing the valley completely framed by the canopies of the trees,’ says Rebelo.\nThe slope allowed the house to embed into the site across two levels. The living, kitchen and dining space is open to the outside, designed around a central hard maple panelled box at its core – the lower level with the bedrooms has a ‘more cavernous and protecting feel’.\nFrom the smooth concrete flooring in the house, to the concrete water tank outside, while materials are simple, each layer of the design is thoughtful.\nModest design becomes an asset, and a careful continuation of the peaceful nature of the landscape. The flat rectangle of water in the tank becomes a quiet reflective pool, and a foundation of the social outdoor space that connects to the existing granite wall of the site and a rural path for animals that meanders discreetly by.\nForja House finds clarity and simplicity in the sloping neighbourhood, peppered with schist-built houses that each follow their own set of characteristics and ideas. The architects aimed to design a house that was a contrast, and a measured step away from a jumbled, accumulating vernacular style. ‘Most of them fail to copy old typologies, and we wanted to distance ourselves from this, assuming this abstract volume embedded in nature,’ says Rebelo.\nFor more information, visit the Pablo Pita website\nHarriet Thorpe is a writer, journalist and editor covering architecture, design and culture, with particular interest in sustainability, 20th-century architecture and community. After studying History of Art at the School of Oriental and African Studies (SOAS) and Journalism at City University in London, she developed her interest in architecture working at Wallpaper* magazine and today contributes to Wallpaper*, The World of Interiors and Icon magazine, amongst other titles. She is author of The Sustainable City (2022, Hoxton Mini Press), a book about sustainable architecture in London, and the Modern Cambridge Map (2023, Blue Crow Media), a map of 20th-century architecture in Cambridge, the city where she grew up.\nLast chance to see: Sharjah Biennial 15, ‘Thinking Historically in the Present’\nBuilt on the vision of late curator Okwui Enwezor, the Sharjah Biennial 15: ‘Thinking Historically in the Present’ offers a critical reframing of postcolonial narratives through major new commissions\nBy Amah-Rose Abrams • Published\nFor London Gallery Weekend 2023, the mood is hardcore\nWith London Gallery Weekend 2023 almost upon us (2 – 4 June), here’s our list of must-see art exhibitions\nBy Harriet Lloyd-Smith • Published\nBirkenstock celebrates its most memorable styles with colourful capsule (and matching socks)\nBirkenstock marks the 40th, 50th and 60th anniversaries of the Gizeh, Arizona and Madrid sandals, respectively, with limited-edition versions\nBy Jack Moss • Published\nSeosaeng House is a holiday home designed to capture the rising sun\nSeosaeng House, Studio Weave’s first project in South Korea, is a clifftop holiday home perfectly designed to capture a new day dawning over the East Sea\nBy SuhYoung Yun • Published\nThe finest brutalist architecture in London and beyond\nFor some of the world's finest brutalist architecture in London and beyond, scroll below. Can’t get enough of brutalism? Neither can we.\nBy Jonathan Bell • Published\nModernist architecture: inspiration from across the globe\nModernist architecture has had a tremendous influence on today’s built environment, making these midcentury marvels some of the most closely studied 20th-century buildings; check back soon for new additions to our list\nBy Ellie Stathaki • Published\nMJE House is a rural Brazilian retreat defined by its striking cantilever\nMJE House by Jacobsen Arquitetura is an idyllic, rural Brazilian retreat defined by its contemporary forms and bold cantilever overlooking the countryside in Upstate São Paulo\nBy Ellie Stathaki • Published\nVillaggio Eni: a modernist gem in the Italian Dolomites\nDesigned by Eduardo Gellner in the 1950s, the Villaggio Eni holiday resort, in the Italian Dolomites, is being repurposed as a modernist refuge for artists and creatives\nBy Emma O'Kelly • Published\nMembers’ club 1 Warwick is a new home away from home in Soho, London\nMembers’ club 1 Warwick by Fettle Design opens its doors in central London, sharing round-the-clock offerings\nBy Ellie Stathaki • Published\nAzabudai Hills to bring a slice of wildness to Tokyo’s megacity\nHeatherwick Studio’s design for Azabudai Hills aims to bring some soulfulness and a slice of wildness to the megacity\nBy Danielle Demetriou • Published\nHotel Marcel: the sustainable rebirth of a Marcel Breuer original\nHotel Marcel, a Marcel Breuer-designed, modernist architecture original, launches as sustainable hospitality in New Haven, Connecticut\nBy Daniel Scheffler • Published""]"	['<urn:uuid:b5d943bf-f8dc-4f2a-85c7-adfc0375a7bb>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	20	64	956
85	As a database architect, I'm concerned about storage efficiency when denormalizing data - what's different about how QlikView handles duplicate data compared to traditional relational databases?	QlikView handles duplicate data differently from traditional relational databases through its columnar database structure. In a traditional relational database, denormalization would result in storing duplicate values multiple times (like storing 'Bings' repeatedly in a product table). However, in QlikView, even when you denormalize by moving a value like CategoryName into the product table, the columnar database still only stores that value once. This means you can get the benefits of denormalization without the traditional storage overhead.	"['If you are unfamiliar with Dimensional Modelling, it has been around for a very long time - as long as QlikView! Ralph Kimball, one of the early data warehouse pioneers, has literally written the book on the subject and has many great articles on his website:\nIf you are not familiar with relational database theory, the idea of normalization is to remove redundancy. For example, if I have a simple product table like this:\nThe category name of ""Bings"" is repeated several times. In a relational database this value is actually stored several times. From a storage point of view, it is more efficient to split the product table into a product and a category table like this:\nIn this case, the numeric ID ""1"" is stored several times but that takes up a lot less space than the word ""Bings"" which is now only stored once. It also means that if I need to change the name of that category, I only need to change it in one location, thus improving my data consistency.\nThis system has been implemented widely. Some architectures that you might come across, such as SAP or Oracle Financials, have thousands of tables, all perfectly crafted to store the data most efficiently in the relational database. Normalization works great for transactional databases like this.\nFor designers, the problem occurred when people started reading data out of the transactional system, especially for reporting. Because the queries now had to hop across several joins, the performance suffers immensely. So, they started either undoing the normalisation process (denormalising) or, worse, keeping the normalised tables but duplicating the data into other tables so that queries ran quicker - a nightmare for data consistency. Both approaches lead to an increase in the data storage requirements meaning that databases bloat more an more - but we can just throw hardware at that issue, right?\nSo, what is the best thing to do in QlikView? A colleague of mine who was recently trained asked me about this because of the exercise that was to move the category table\'s data into the product table, hence denormalizing. ""Why?"", he asked. ""Doesn\'t QlikView work just as well either way?"".\nThere are a couple of reasons as to why you might do this. And a couple of reasons why you might not.\nIn one of his articles, Kimball suggests that such ""snow-flaking"" (leaving the category out of the product table) ""compromises cross-attribute browsing performance"". Not only that, it ""may interfere with the legibility of the database"".\n- Improved schema readability\nAnd I will add a 3rd:\n- Data size.\nIn the QlikView underlying data cloud, there will be an association between the product fields and the CategoryID field. There will also be an association between the CategoryID field and the CategoryName field. If I move the CategoryName into the product table (either map or join), I can then remove the CategoryID field and hence remove a redundant set of associations. The CategoryName is not duplicated in QlikView because the columnar database still only stores it once.\n- Improved schema readability\n- Data size\nI\'m sure that you will be thinking that there is some sort of error here. How can I have the same set of reasons ""against"" as I did ""for""?\nIt depends! It depends really on the number of records in each table and the number of fields. For a relationship like product and category, there is probably only a few fields in the category table and it can really make sense to denormalise (it almost always make sense to do this for a table with only 2 fields - using ApplyMap). However, with a couple of tables like, say, Order and Calendar, where Order may have millions of records and Calendar could have ten or more fields, it makes sense to keep them apart and just have the key association on the DateID field. If you did join them in, it would mean that you could drop the DateID field but you would then have associations maintained between each field in the Order table and Year, and Month, and Day, and Week, etc., etc. Many more associations being maintained than before and increasing your data size and possibly reducing your performance. It may also become difficult to see the date fields in the schema whereas they are plain to see in their own Calendar table.\nStephen Redmond is CTO of CapricornVentis a QlikView Elite Partner']"	['<urn:uuid:87d23357-2be4-4cc1-8a48-c85041e3b945>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:15:13.904987	26	76	742
86	what is reaction time in movement onset	Reaction time is the interval from the start signal to movement onset, indicating the temporal efficiency for generation or planning of motor action	['Ching-Yi Wu, Tien-Ni Wang, Yu-Ting Chen, Keh-Chung Lin, Yi-An Chen, Hsiang-Ting Li, Pei-Luen Tsai; Effects of Constraint-Induced Therapy Combined With Eye Patching on Functional Outcomes and Movement Kinematics in Poststroke Neglect. Am J Occup Ther 2013;67(2):236-245. doi: 10.5014/ajot.2013.006486.\nDownload citation file:\n© 2019 American Occupational Therapy Association\nOBJECTIVE. We investigated the effect of constraint-induced therapy (CIT) plus eye patching (EP), CIT alone, and conventional treatment on functional performance, eye movement, and trunk–arm kinematics in stroke patients with neglect syndrome.\nMETHOD. Twenty-four participants were recruited and randomly allocated to three intervention groups. All participants received intervention 2 hr/day, 5 days/wk, for 3 wk. Outcome measures included the Catherine Bergego Scale, eye movement, and trunk–arm kinematic analysis.\nRESULTS. The CIT + EP and CIT groups demonstrated larger improvements in functional performance than the control group. The CIT group showed better performance with left fixation points than the CIT+EP group and shorter reaction time than the control group. The CIT + EP group improved more in preplanned control and leftward trunk shift than the other two groups.\nCONCLUSION. CIT + EP and CIT were more effective interventions than conventional treatment of patients with neglect syndrome in daily functional performance.\nA right-side cerebral stroke\nNeglect syndrome as shown by results on two or more of four tests (double simultaneous stimulation test [Stone, Halligan, Wilson, Greenwood, & Marshall, 1991 ], line bisection test [Schenkenberg, Bradford, & Ajax, 1980 ], random shape cancellation test [Weintraub & Mesulam, 1988 ], and the random Chinese word cancellation test [Chen Sea, Henderson, & Cermak, 1993 ])\nAbility to reach Brunnstrom’s Stage II or higher for the proximal and distal upper extremity (UE; Brunnstrom, 1970)\nConsiderable nonuse of the more affected UE (MAL amount of use score <2.5; Taub et al., 1993)\nNo excessive spasticity in the affected arm, including shoulder, elbow, wrist, and fingers (Modified Ashworth Scale score ≤2 in any joint; Bohannon & Smith, 1987)\nNo severe cognitive impairment by showing awareness and ability to respond to oral instructions (Folstein, Folstein, & McHugh, 1975)\nNo severe impairment of visual acuity after rectification\nNo participation in any experimental rehabilitation or drug studies during the study period.\nReaction time, which indicates the temporal efficiency for generation or planning of the motor action, is the interval from the start signal to movement onset (Brooks & Watts, 1988).\nMovement time, representing temporal efficiency, refers to the total duration of the reaching movement (Wu, Lin, Chen, Chen, & Hong, 2007).\nTotal distance, representing spatial efficiency, refers to the path of the hand in three-dimensional space (Wu et al., 2007). Because of the varied task distance across participants, movement time and total distance were normalized by the direct distance between the hand at the start position and the target for each participant. The smaller normalized total distance shows the more direct movement path.\nThe PPV, reflecting the percentage of movement time used for the acceleration phase, characterizes the control strategy of reaching. A higher PPV value indicates less online error correction and more planned control of the reaching movement (Georgopoulos, 1986).\nTrunk lateral shift shows the change in trunk movement in the horizontal plane between the start and the end position. A larger value represents more trunk left shift.\nIf the treatment goal is to improve functional performance in patients with neglect, either CIT + EP or CIT is appropriate.\nCIT + EP may target enhancing preplanned motor control ability and induce more leftward trunk shifting than CIT.\nCIT might be better than CIT + EP if therapists are attempting to improve leftward eye fixation and speed up movement initiation.\nCBS might be a sensitive tool to evaluate treatment effects on functional performance among patients with neglect.\nThis PDF is available to Subscribers Only\nFor full access to this pdf, sign in to an existing account, or purchase an annual subscription.']	['<urn:uuid:69335f51-cc85-4baf-8eeb-c27070022e0f>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	7	23	637
87	Hi! I'm planning my garden and want to know the main differences between preparing soil for a lawn versus preparing it for growing vegetables. Can you help me understand?	Both lawn and vegetable garden soil preparation require thorough clearing and digging to at least 15cm deep, but they have key differences. For lawn preparation, you need to focus on achieving a crumble-like texture, adding pre-turfing fertilizer, and ensuring the surface is perfectly level without humps and hollows to prevent scalping during mowing. For vegetable gardens, the soil needs to be rich in organic matter and the preparation varies based on soil type. For clay soil, you need to add gypsum for calcium and breaking up clay, then incorporate compost for nutrients, aeration, and moisture retention. Vegetable gardens also require specific pH levels (6-7 range for most vegetables) which should be tested and adjusted with lime/dolomite for acidic soil or sulphur for alkaline soil.	"[""This weblog will define how one can put together your soil for turfing. The key to any mission is sweet preparation. Whether or not you might be hanging wallpaper, icing a cake or writing a dissertation, poor preparation is mirrored within the completed article and may be unattainable to appropriate.\nSo, whenever you’re creating a brand new garden, it’s higher to permit loads of time to get the soil prepared to your turf (or seed – the soil prep is identical)\nYou're reading: how to prepare your garden for turf\n- Clear away any crops, weeds and particles\n- Dig or rotovate soil to at the very least 15cm deep\n- Enhance soil texture or high quality if mandatory\n- Stage the world\n- Agency the soil\n- Add pre-turfing fertiliser\n- Rake and stage once more\n- Lay your turf\nClear the world\nEach final scrap of plant materials must be taken off the garden space. Grass, crops, leaves, roots, twigs – the whole lot. You are able to do this with a backyard spade or, if in case you have a big space, rent a mechanical turf cutter to avoid wasting your self time.\nThe outdated garden may be composted down or taken to your native inexperienced waste centre.\nIf you happen to want to make use of a systemic weedkiller (eg Roundup), go away the garden for at the very least 3 weeks after you’ve utilized it. It should take some time to kill the leaves and the roots.\nUtilizing a backyard fork or a mechanical rotovator, dig the soil over and take away any bricks, stones, builders’ rubble or different particles. As a tough information, if a stone is greater than a matchbox, it’s too massive to be below your garden.\nYou could dig at the very least 15cm (6 inches) down – extra in case you presumably can.\nIf there isn’t sufficient topsoil to get 15cm of unfastened soil – usher in some good high quality soil from a good provider.\nDoes your soil want bettering?\nAt this stage, you may add soil improvers in case you really feel you should. In case your backyard soil is of course sandy and doesn’t retain water for lengthy, dig in some natural matter akin to compost or very well-rotted manure.\nYou probably have clay soil which is heavy, sticky and will get boggy in winter, incorporate some gravel to assist with drainage and a few natural matter to encourage worms to come back alongside and aerate it for you.\nNow it’s time to rake the soil to interrupt down any massive lumps and to get a pleasant even floor. You’re aiming to get a texture that appears a bit just like the topping on an apple crumble.\nIt’s vital to do away with humps and hollows since you’ll discover that they catch on the mower blades and also you’ll be scalping the excessive areas of the garden each time you narrow it.\nThere’s a humorous little dance referred to as the gardeners shuffle – that’s what you should do subsequent. It could possibly get a bit tedious by yourself – so invite family and friends to hitch you on this one – belief me, it’s way more enjoyable in case you do.\nRead more: how to preserve tomatoes from the garden\nThis backyard has been ready for turfing. Observe the feel of the soil, the dearth of humps and hollows and the extent. The soil is approx 20mm beneath the extent of the paving – that approach the completed garden would be the similar top because the patio.\nCriss-cross your soil, taking tiny steps so that you simply press each single little bit of soil down. Begin off by going a method, then, whenever you’re completed, do it once more the opposite approach – simply to ensure.\nAdd pre-turfing feed\nYour garden space would possibly look barely lumpy proper now, however don’t fear, we’ll put that proper in a minute. First you should apply pre-turfing feed to the soil – comply with the directions on the pack. This may assist de-stress the grass crops after their journey from the turf discipline to your backyard. With the correct vitamins they’ll root in shortly they usually’ll keep sturdy and wholesome.\nRake it once more\nOne remaining rake to include the pre-turf fertiliser and to stage the floor and bingo – you’re able to measure up and order your new turf.\nOrder Turfing Topsoil On-line\nDo you want topsoil to put turf?\nWhat does turfing topsoil value?"", 'Preparing a spring/summer or autumn/winter vegie garden\n- preparation and care (soil preparation, watering, and site selection for individual vegetables)\n- spring/summer garden\n- autumn/winter garden\nPart 1 – Preparation and care\nThere are three keys to establishing a flourishing edible garden whether spring/summer or autumn/winter: soil preparation, watering and site selection.\n1. Soil preparation\nSoil preparation is everything. Soil needs to be rich in organic matter, free draining, friable and able to hold moisture well. The main types of soil are: loamy, sandy and clay. As Banyule is built on clay, we’ll concentrate on clay soil here. The best way to lighten and add nutrients to clay soil is first to add gypsum (available at your local nursery) which breaks up the clay and provides calcium. Then dig in as much compost as you can. This will add nutrients, aerate the soil, improve drainage and help retain moisture. You may even want to order some bulk Vegie Mix, (a combination of loam, cow manure and mushroom compost), to add to your soil. Vegie Mix is also useful if you are building raised beds or wicking beds. Clay soils in Banyule lack boron and molybdenum so add these trace elements as well.\nTest your soil with a pH testing kit. These cost about $25 and last forever. You might even want to share one with friends. The pH scale is from 1 – 14 with 7 being neutral. Below 7 is acidic, above 7 is alkaline. Most vegies grow in the 6 – 7 range but some will grow with alkalinity up to 8 and some with acidity as low as 5.5. Generally there is a chart in the kit showing the pH range appropriate to different vegetables. If your soil is too acidic, add lime or dolomite and if too alkaline add sulphur. Both dig and water these in. It is quicker to turn acidic soil alkaline than vice versa. Mushroom compost is alkaline so this can work as an additive but too much will turn your soil too alkaline. Follow the instructions on the additive packets carefully.\nView our video on testing soil pH.\nI cannot stress how much lack of water impacts development. Less than 10 mm of rain any day does not count at all. All seedlings need to be well watered at least every second day until well established and then watered deeply twice a week throughout autumn – winter – spring – summer. Water on to the soil not the leaves. This may mean placing the hose underneath cabbages or cauliflower, for example. These will not benefit from rain or overhead watering due to how dense the bed will become with leaves. Plants primarily take water up through their roots so that is where you need to get the water. Watering the leaves should be avoided.\n3. Site selection\nSome vegies need 6 – 8 hours of sun a day. This is what is meant by ‘full sun’. Some need only 3 – 6 hours per day. If they need afternoon sun this is referred to as ‘partial sun’, if they need morning sun this is called ‘partial shade’. The darker the leaf the less sun is needed so if you need to plant in shady areas (i.e. partial shade), plant spinach, silverbeet, rainbow chard, cress, and other dark leaved plants there. Coriander and chervil which bolt quickly in the sun would also be good choices. For more information, read the article ‘How much sun do my vegies need?‘\nPart 2 – Spring/summer garden\nTomatoes (Deadly Nightshade family)\nPlant September – December. Choose a site with full sun.\nDig over the soil with a fork, loosening it and breaking up clods. For a 2 x 1 metre bed dig in a barrow load of compost, a bag of cow manure and 2 handfuls of blood and bone. 4 – 5 handfuls of gypsum will also add calcium and break down clay. Also, sprinkle some potash around each plant after planting and again at the time of flowering. Choose the third hottest part of the garden for tomatoes, reserving the hottest for eggplant and capsicum.\nSow seeds in punnets and plant out as seedlings. Sow in June and plant out in September. Protect seedlings if frost is expected. To get a head start, plant seedlings into large pots with compost in them, and plant out well-established plants when you are certain frosts have passed. When planting, bury seedlings up to, even including, the lowest leaves. This anchors them well and produces a strong root system which will deliver more water to the plant, and result in more tomatoes. Plant seedlings 1m apart in rows 1m apart. Stake seedlings and, as they grow, add ties higher up the stake. As the plant grows, progressively remove leaves until the lowest 30cm of the stem is clear.\nWater directly, softly, onto the soil. By doing this, and through the removal of lower leaves, you will prevent septoria spores in the soil jumping onto the lower leaves, and making their way up the plant. Always keep tomato roots moist. If they dry out the plant will die. Water deeply several times a week.\nCapsicums, chillies and eggplants (Deadly Nightshade family)\nPlant October – December. Choose a site with full sun.\nPrepare the bed as for tomatoes (see above). These vegetables need long, hot summers. If you are germinating seeds at home without a heat mat, germination will be late and the plants will not have time to fully mature so I recommend buying seedlings from a nursery. Seeds germinate at 28C.\nPlant in the hottest spot in your garden. They need full sun and more heat than tomatoes.\nWater deeply several times a week. Never let the roots dry out. As the fruit gets close to maturing, reduce the water to ensure better quality fruit. Stake the plants and fertilise with chicken pellets every 6 weeks.\nCucumber, zucchini and pumpkins (Cucurbit family)\nPlant September – December. Choose a site with full sun.\nPlant these as seed or seedlings on mounds of compost. Avoid nitrogen based fertilisers. Until the plants are well established, cover the mound with wire netting to prevent birds scratching them out or small rodents eating the tender shoots. Snail bait is useful to protect seedlings from snails. Plant 3 lots of 2 seeds per mound and thin out weaker seedlings later if necessary (or transplant them). Mounds for cucumber and zucchini should be 40cm apart and for pumpkins 80cm apart. Rows for both should be 1m apart. Climbing cucumbers will need a frame. When cucumbers have 6 leaves pinch out the growing tips so that they develop lateral branches and more fruit. Add potash at flowering time for all three cucurbits.\nWater deeply several times a week.\nPlant September/October – December. Choose a site with full sun.\nDig over the soil with a fork, loosening it and breaking up clods. Add compost and well rotted manure. Sow seeds directly or transplant seedlings after frosts have passed. Plant in a grid pattern as corn is wind pollinated. Plant 45cm apart in rows 50cm apart. Plant a minimum of 5 x 5 rows. Corn requires full sun, protection from strong winds, and good drainage. It grows poorly on unimproved clay soils. For the best crop, hill up around the roots with compost.\nCorn needs to be well watered but since a grid layout makes this difficult a dripline is recommended. Harvest close to full ripeness and before the rats are attracted to their sweetness. Rats and possums can strip a plant overnight!\nPlant year round. Choose a site with partial shade.\nSalad greens are tastiest and softest if grown quickly. Grow in any good soil. Keep beds and their surrounds free of weeds to prevent transmission of viruses from the weeds. Salad greens are shallow rooted and require plenty of water.\nWater regularly every second day in summer. Spray with Maxicrop or Seasol every 2 weeks. Salad greens include all varieties of lettuce, corn salad, green and purple mizuna, rocket and arugula, and other exotic leafy greens.\nPlant August – December. Choose a site with partial sun.\nRoot vegetables need a fine tilth i.e. fine, powdery soil with all clumps broken up, and roots, sticks and stones removed so that nothing interferes with root development. Soil should be light, sandy and well drained and should contain well decomposed compost but no manure. Soil needs to be dug to a depth that is deeper than the mature vegetable, eg. an 18cm carrot needs a depth of at least 24cm of fine tilth. (A spade’s depth is about 30cm so preparing soil to a spade’s depth is ideal).\nSow seeds directly into prepared beds at a depth of 1-2cm and water well. Place a fence paling over the row to protect the seed from being scratched out by birds but do check regularly for emerging shoots and remove the paling when that occurs. Water again once the seedlings appear. Check seed packets for correct depth of sowing.\nAll seedlings will require thinning. Thin to the width of the fully grown vegetable plus a bit more. Thinnings, if small enough (2cm), can be transplanted. In general, however, root vegetables do not transplant well and punnets should be avoided. Carrots will not germinate if the temperature is over 25C so plant these early in the season. Do not add manure as this will cause the root vegetables to split and fork.\nInadequate watering in summer will also cause splitting and forking. (Root vegetables are ideally grown in wicking beds in summer as the consistent moisture produces excellent vegetables). The main summer root vegetables are carrots, radishes, daikon and beetroot but kohlrabi and turnips grow well too.\nPlant August – September. Choose a site with full sun.\nBeans need well dug, well-drained soil, prepared to a depth of 30cm. They do not require fertilising. Beans come in bush and climbing varieties, and in green, yellow and purple colours. They need to be planted after frost as they burn easily. While they need full sun early in the season to develop, they do badly in full sun in midsummer and will need shade protection to continue flourishing. Climbing beans need frames for support or a teepee (eg. made of bamboo sticks). If using a teepee, plant 3 seeds at the base of each pole 5cm deep. For bush beans broadcast the seeds and cover with 5cm of soil. Planting at this depth prevents rodents eating the seeds. Pick beans often as this will increase the crop and young beans are more tender.\nWater at the time of planting and then every day during flowering and pod development. Beans need 2.5cm water per week, which is a lot.\nPlant perennials year round and annuals in spring (or autumn, see below). Choose a site with full sun except for coriander and chervil. Annual herbs such as coriander and chervil should be planted in the shadiest part of the garden and well watered as they bolt easily.\nHerbs grow well in poor soil and require very little water. This makes their essential oils stronger.\nPart 3 – Autumn/ winter garden\nPlant March – April. Choose a site with partial sun.\nRoot vegetables need a fine tilth i.e. a fine, powdery soil with all clumps broken up, and roots, sticks and stones removed so that nothing interferes with root development. The soil should be dug to a depth that is deeper than the vegetable eg an 18cm carrot needs a depth of at least 24cm of fine tilth. Preferably, prepare to a depth of 30cm (a spade’s depth). Do not add manure as this will cause the root vegetables to split and fork.\nSow seeds directly into prepared beds at a depth of 1-2cm and water well. Place a fence paling over the row to protect the seeds from being scratched out by birds but do check regularly for emerging shoots and remove the paling when that occurs. Water again once the seedlings appear. Check seed packets for correct depth of sowing.\nWater every second day while seedlings establish and then deeply twice a week until the weather cools. Note that March nowadays, is usually a very hot month (due to climate change, +0.25C since 1970!), and watering regularly is important for root development. Through the winter, water deeply once a week unless it is unseasonally hot.\nAutumn root vegetables include carrot, parsnip, beetroot, radish, daikon, turnip and swede.\nPlant April – May and keep well watered. Choose a position of partial shade.\nLike root vegetables, leafy greens also need a fine tilth and plenty of compost but they do well also with well rotted manure which gives them plenty of nitrogen which causes leaves to grow quickly and therefore to taste better.\nWater deeply every second day till seedlings are established and then once a week.\nAutumn/winter leafy greens include lettuce, rocket, mizuna, kale, mustard greens, spinach, chard and silverbeet, and Asian greens.\nPlant March – May and keep well watered. Choose a site with full sun.\nThe onion family also need a fine tilth, lots of compost and no manure. Weed control is important so weed well before planting, keep well weeded and mulch with sugar cane mulch. Dig furrows 6cm deep, lay the seedlings in them, back fill with soil, and water. They will stand upright within 24 hours.\nWater consistently as roots are shallow. Onion family includes onions, spring onions, shallots, leeks and garlic. Garlic needs a false winter. Place whole heads in the fridge in paper bags for 40 days before planting in late May.\nPlant April – May and keep well watered. Choose a site with full sun.\nDig over the soil roughly with a fork, breaking up clods and aerating it. Avoid high nitrogen fertilisers such as fresh manure. Add compost and/or some very well rotted manure. Soak peas and broad beans in a diluted seaweed solution overnight before planting. Plant at a depth of 5cm to prevent rodents eating the seed. Peas need climbing frames and broad beans need support to prevent stem breakage in the wind. Provide a form of wind break or a lattice made of strings between stakes.\nPeas need the soil to be moist at all times. Water deeply once a week. During flowering and pod development water almost every day. Broad beans should be watered when planted and not again until 2 leaves have emerged. Overwatering causes germination to fail. Broad beans may be watered deeply once a week if necessary and also if they wilt, but the soil needs only to be moist.\nLegumes include broad beans, green peas, sugar snap peas and snow peas. Broad beans are frost tender and not suitable for autumn/winter planting.\nBrassicas (cabbage family)\nPlant April – May and keep well watered. Choose a site with partial shade.\nDig soil over roughly with a fork to open it up and aerate it. Add compost and well rotted manure. Avoid fresh manure which is too high in nitrogen and will cause leaves to develop at the expense of heads. pH should be 6.5 – 7.5 so test with a pH kit. If too acid add lime or dolomite and, a month later, add some well rotted manure. This means starting bed preparation about 6 weeks before planting.\nPlant seedlings deeply i.e.bury them up to, even including, their lower leaves. This results in good root systems. Wobbly brassicas will not produce good heads. (It is a common mistake not to plant them deeply enough).\nBrassicas require a lot of water. Small heads result from inadequate watering as do heads that are not firm. Rain is rarely adequate. Water deeply twice weekly throughout the winter. Place the hose beneath the leaves and water directly onto the soil so that the roots receive sufficient water. Do not water the leaves.\nTo prevent yellowing of cauliflower, fold a leaf over the cauliflower head. Don’t worry if the leaf snaps.\nBrassicas include broccoli, cabbage, and cauliflower, all of which have early, mid and late varieties; also included are kohlrabi, Brussel sprouts, kale, turnip and swede (but treat turnip and swede as root vegetables).\nSome annual herbs bolt in summer so any time in autumn or spring (see above) is an ideal time to plant them. This includes coriander and chervil. All perennial herbs such as the thymes, oreganos and marjorams, savouries, sage and rosemary can be planted in autumn or in any other season.\nWritten by Robin Gale-Baker.']"	['<urn:uuid:a73ebd85-12ec-4914-822c-1772a01016ef>', '<urn:uuid:0d223dd0-de98-4dfa-8a4f-3ad2d7e76eeb>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	29	124	3510
88	I work in international shipping and I'm curious - how much financial damage did the Ever Given container ship cause when it got stuck in the Suez Canal back in 2021?	The Ever Given cargo ship blocked Egypt's Suez Canal for almost 7 days and disrupted global trade, causing losses of almost $10 billion per day.	['Learn about supply chain disruption with examples, its causes and effects, and how to proactively respond\nUpdated 13 Mar 2023, Published 21 Jan 2022\nSupply chain disruption is an interruption in the flow of process that involves any of the entities associated with the production, sales, and distribution of specific goods or services. A systematized supply chain is crucial in maintaining the quality of products from start to end, and ensure that all resources utilized are of the best quality.\nSupply Chain Disruption\nBottlenecks or any congestion in the production system often result in delays and higher production costs. Supply chain disruption is also one of the many challenges brought about by the Covid-19 pandemic.\nChina’s strict “zero-Covid policy,” introduced early in the pandemic and aims to control the spread of the virus by strict travel restrictions and lockdowns, impacted international supply chains but was a win for China’s economy as the country was eventually able to ease local restrictions and became the only major economy to grow in 2020.\nThe new and highly-transmissible Omicron variant might change that though.\nAccording to this recently published article from CNBC, economists say that the policy could pose another threat to the global supply chains and may further weigh on consumer consumption and economic growth because, whereas other countries are learning to adapt and make their supply chains more resilient to disruptions, China is still counting on the ‘zero-tolerance’ policy.\nThis in turn can impact the supplies around the world as a great number of manufacturers are situated in China.\nSample of Supply Chain Disruptions by Sector in 2021\nJust like any other factors in the socio-economic industry, supply chain disruption has various causes and effects that may impact the economy and the process of turning raw materials to final goods.\nThe origin of supply chain disruption in different industries can be caused by internal or external factors. Below are the usual aspects that might cause these disruptions and that companies and organizations should be aware of:\nGlobal cost of disruptions such as those brought about by the pandemic have been enormous. These disruptions greatly impact the whole chain and cause several repercussions that will most likely materialize eventually. Below are the known effects of supply chain disruptions:\nThe impact of disruptions in various industries substantially affects the global supply chain and the global economy. A survey of 200 supply chain executives done by Ernest & Young LLP in the United States, shows that 72 percent of the companies reported that these disruptions adversely affected their operations and that only 2 percent responded that they were prepared for the challenges.\nSupply chain disruptions can come in many forms but can mostly be categorized into two factors; probability of occurrence—which refers to the likelihood of it happening; and severity—which can be classified in three levels, they are as follows:\nThe unprecedented Covid-19 pandemic affected innumerable industries and disrupted major supply chains globally. Known for its significant economic effects, different industry disruptions were experienced worldwide.\nPeople started working from home and this created a higher demand for electronic gadgets, therefore shifting the demand for semiconductors to consumer electronics which heavily affected the Automotive Industry. This shortage also recognized the opportunity for expanding chip supply chains, and production need not only heavily rely on a few manufacturing sites that produce the majority of the world’s semiconductors.\nIn relation to the above mentioned industry, the Electronics Industry is also experiencing a supply chain disruption for various reasons1) there was an increase in demand for semiconductors for the automotive industry; 2) covid-related manufacturing disruptions; 3) logistics/transportation concerns for the supply; and 4) increased demand for electronic gadgets itself.\nThe infamous 2021 Suez Canal obstruction by Ever Given cargo ship is an example of a possible human error in cargo consolidation and disruption. The cargo ship blocked Egypt’s canal for almost 7 days and disrupted global trade, which cost almost $10 billion in losses per day. In response to this, an expansion project for the canal is expected to be completed by July of 2023, according to the Suez Canal Authority.\nThe Retail Industry was also considerably affected by the pandemic. Just in 2020, a record number of 12,000 stores closed in the United States as consumers turned to e-commerce and avoided brick-and-mortar stores.\nOne other example of a supply chain disruption cause is Brexit. This geopolitical move created implications on how manufacturers source, transport, assemble, price their products, and make them consumer-available.\nThe effects brought about by the pandemic in the economy, various industries, government bodies, and societies are continually materializing — so it won’t be far-fetched to say that they are here to stay.\nIn order to keep up with evolving situations, companies need to make their supply chains more resilient and strategic. Listed below are some aspects enterprises could consider in addressing disruption:\nCreate Your Own Supplier Audit Checklist Eliminate manual tasks and streamline your operations.Get started for free\nEliminate manual tasks and streamline your operations.\nSafetyCulture (formerly iAuditor) is a powerful inspection platform used by industry leaders to monitor the status of every part of the supply chain process. Use SafetyCulture checklists starting from suppliers/vendors, producers, warehouses, distributors, to retailers in order to avoid disruptions in production and to guarantee the quality of products and services.\nTry SafetyCulture for free!\nA supplier audit checklist used to examine your supplier’s facility, inspect if they meet the criteria, and determine if they’re suitable to be your supplier.\nUse this to evaluate if the Context of the organization, Planning, Operation, Quality control, and Product improvement complies with the ISO standard.\nThis general warehouse safety checklist aims to evaluate potential risks observed during warehouse operations. Start by providing general information about the business then proceed with an evaluation of warehouse location, dock areas (exterior and interior), equipment, and sanitation.\nThis heavy vehicle inspection checklist can be used by a certified technician to identify problems and mechanical issues of heavy vehicles and to prepare for the DOT annual vehicle inspection.\nThis retail store operations checklist can be used to evaluate the following: employees’ product knowledge, facility cleanliness, product and window display’s appeal to customers, and safety of item storage.\nJaydee Reyes is a content writer and researcher for SafetyCulture. Her six-year experience in the field of data research and media monitoring adds expertise and quality to her work. She is also a champion of leveraging technology to promote a culture of safety in workplaces around the world. As a content specialist, she aims to help companies adapt to digital changes through interesting and informational articles.\nSupply chain risk management aims to reduce costly delays, improve operational efficiency, drive ...\nEmployee engagement may be influenced by various factors such as workplace culture, leadership, and ...\nIn this guide, you will learn what journey management is, why it’s important, and the most ...']	['<urn:uuid:c7696e39-425e-43df-ad42-4bdcc7c371cf>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	31	25	1141
89	Which artist focused more on food in their paintings?	Wayne Thiebaud focused more on food, particularly painting sweets and desserts with paint so thick it looked like real frosting. While Georgia O'Keeffe mainly painted subjects from nature like flowers, mountains, seashells and animal bones.	"['The 5th grade students learned about the life and artwork of Claude Monet through a power point. Students learned that Impressionist painters were more focused on how the light affected their piece of art. Monet would paint the same subject matter over and over but at different times during the day and different times of the year. Students were able to view Monet’s Haystack series and discuss what season or time of day they thought the painting was done. Students were able to create lays of paint using different techniques and Analogous colors to create an Impressionist piece of art like Claude Monet.\nThursday, June 6, 2013\nThe 5th Grade students studied the Pop Art Movement through a PowerPoint. We focused on the artist Wayne Thiebaud and learned about him through a video. Wayne Theibaud was an American Pop Artist who loved to paint sweets and deserts. Students were able to see Thiebaud apply his paint so thick that it even looked like real frosting! Theibaud was turned down by several art galleries before he ended up getting his first art show, which was an instant success. The 5th grade students were able to design their own cake keeping in mind how many layers and flavors they each would like for their cake. Students reviewed VALUE and learned how to create tints and shades using chalk pastels. Students also learned that colors that are next to each other on the color wheel are the best colors to blend together. Students were able to add a variety of details to their cake. Finally students designed a background trying to use colors that were not in their cake to create contrast to make their cake stand out.\nThe 4th Grade students learned about the life and artwork of Georgia O’Keeffe. Georgia O’Keeffe grew up on a farm in Wisconsin and has become one of today’s most famous painters. Her mother thought art was very important, so growing up O’Keeffe had many art lessons and was encouraged to go to Art College. O’Keeffe loved nature, so most of her paintings were of flowers, mountains, seashells and even animal bones. O’Keeffe often simplified her paintings, creating more emphasis on the beauty of nature. Eventually O’Keeffe moved to New York to start showing her artwork. After being in New York for a while, O’Keeffe took a teaching job in Texas and loved the clear skies and the bright sun there. After teaching for a few years Alfred Stieglitz, a famous photographer, offered Georgia to come back to New York and he would pay for her expenses and all she had to do was paint. It was hard for O’Keeffe to leave the beauty of Texas, but she could not pass up the offer and this was the beginning of O’Keeffe’s fame!\nThe 4th Grades students learned about sculpture through a power point. A sculpture is a 3-Dimensional object that can be made of various materials and can be any size. 3-D means you can see ALL three sides of the object; it pops out and is not flat. We looked at the difference between shape and form. Students were able to create an armature (the base for their sculpture) from a balloon and poster board. Students then used ANALOUGOUS colors to paint their fins. Finally they used glitter gloss and sequins to finish their fish sculpture.\nThe 3rd Grade students reviewed the Warm and Cool Colors. Students also reviewed what TEXTURE is and used texture plates to create texture in the background using only the cool colors. Students then learned what the word organic means and used only curvy/organic lines overlapping their texture. To create CONSTRAST students used yellow and orange watercolor to create their background. Next students added a tree and branches and added shadows to make it look more realistic. Finally students were about to draw a bird native to Wisconsin sitting in their tree. Students colored their birds using tints and shades. We talked about which direction the light is coming from and what part of the bird the light will hit first. This would be the tint and the farthest part of the bird from the light would be were the shading would be.\nThe 3rd Grade students learned about Hot Air Balloons through PowerPoint. They learned that hot air balloons fly because hot air weighs less than cool air, so by heating the air inside the balloons allows it to float. Students were able to look at and see various examples of hot air balloons and their bright, bold patterns. We reviewed horizontal, vertical and diagonal lines. Students were able to use different lines and different shapes to create a unique design for their hot air balloon. Students had their photo taken and added themselves into their hot air balloon. Then they used tissue paper and water to create a textured sky background. Finally students had to write at least three sentences starting with, “If I could fly I would . . . .”\nThe 2nd Grade students learned about The Mona Lisa through a short video done by a little girl named Sophia. Students loved this video! http://www.youtube.com/watch?v=Ww2yJ2grPBk&safety_mode=true&persist_safety_mode=1\nStudents brainstormed the sentence, ""If I could take Mona Lisa anywhere I would take her to . . . .""\nWe wrote various examples on the board and students had to choose a place and create a background. I then used the ipad app. Masterpiece and too the students photo, which puts their face into the Mona Lisa. I printed them off and students added them on top of their background.\nThe 2nd Grade students learned about the life and artwork of Henri Matisse. Students learned that Matisse loved to use his scissors to make various shapes overlapping to create a collage. He rarely threw any scraps away. Students learned the difference between positive and negative space. Students also learned the difference between geometric and organic shapes. The students created a collage with many overlapping geometric and organic shapes using the positive and negative images just like Henri Matisse did!\nROMEARE BEARDEN MUSIC COLLAGE\nThe 2nd grade students learned about the life and artwork of Romeare Bearden through a PowerPoint. They learned that he was very interested in jazz music and this influenced his art. We discussed the many different instruments that there are in a Jazz Band. The students also reviewed what a collage is because Romeare Bearden overlapped fabric, photos and paint to create beautiful collages inspired by music. Students were able to create a music collage using the piano keys pattern for their boarder and designing their own guitar. Then they overlapped musical notes and sheet music. Finally we reviewed the various types of lines and students had to fill the rest of their negative space repeating different colors of lines.', 'Sweet Art: Wayne Thiebaud\n[Sorry, the video for this story has expired, but you can still read the transcript below. ]\nELIZABETH FARNSWORTH: Above all else, the retrospective of Wayne Thiebaud’s paintings currently at San Francisco’s Palace of the Legion of Honor offers a feast for the eyes: A cake with luscious, thick frosting that looks good enough to eat, bowls of soup painted with surprising shadows and swirls of color. The show’s curator says installing the works reminded him of Thiebaud’s virtuosity as a painter.\nSTEVEN NASH, Chief Curator, Fine Arts Museums of San Francisco: This is a painting, for instance, which you cannot possibly understand in an illustration. There’s no way to get the depth of the paint handling, the way the surface is treated, the incredible richness of paint manipulation in it or the quality of light in it.\nELIZABETH FARNSWORTH: The exhibit features works like these pinball machines from the 1960s, and a room full of portraits, including this one of his wife, Betty Jean, who has posed for him repeatedly over the years. In the 1990s, Thiebaud began painting a series of vibrant landscapes of the Sacramento River delta just east of San Francisco. Farmlands are tipped up, sometimes seen from above, and are almost abstract in design and effect, as are his cityscapes, paintings of the streets and hills of San Francisco. It is both a real and imaginary place for him.\nWAYNE THIEBAUD: Originally, I painted right on the streets, trying to get some of the kind of drama I felt about the city and its vertiginous character, but that didn’t seem to work and… the reality was one thing but the fantasy or the exploration of it was another.\nELIZABETH FARNSWORTH: I love them because they’re a combination of both; they really are about San Francisco. I mean it’s the real city, it’s the real street, but you make up addresses even. I mean, 24th and Mariposa doesn’t exist– it can’t– they run parallel.\nWAYNE THIEBAUD: But Mariposa is such a beautiful word to put on a sign.\nELIZABETH FARNSWORTH: Thiebaud has a home near Mariposa Street on one of San Francisco’s hills, but because he has taught since 1960 at the University of California at Davis, he lives most of the time nearby in Sacramento. He was a commercial artist before he was a painter, working as a layout designer and cartoonist for companies like Rexall Drugs. During World War II he designed posters for the army air corps. At age 79, Thiebaud still competes in tennis tournaments and paints every day.\nWAYNE THIEBAUD: I love sweets anywhere.\nELIZABETH FARNSWORTH: He still loves the window displays that inspired his first big artistic success in the early 1960s.\nELIZABETH FARNSWORTH: So what do you see when you look at this?\nWAYNE THIEBAUD: Beauty.\nELIZABETH FARNSWORTH: He’d been working on more traditional subjects, he said, but decided to try something different.\nWAYNE THIEBAUD: I’d worked in food preparation. So I’d always seen these lined… The way they line up food, sort of ritualistically and I thought, “oh, I’ll try that”… So I started painting these ovals for the plate and then put a triangle on it. And I mixed up a pumpkin color, maybe, I’d put it on and it was so far away from pumpkin color that I thought, “oh, I’ve got to put other colors in there.” So I added blues and other colors to see if it could enliven it, but then I realized I’d painted this row of pies and started laughing and said, “well, that’s the end of me as a serious artist. Nobody’s going to take this seriously.”\nELIZABETH FARNSWORTH: But people did take the paintings seriously enough to spend big money for them when they were first shown at the Alan Stone Gallery in New York in 1962, and their value kept rising. This work sold in 1991 to the National Gallery of Art in Washington, D.C., for $1 million. Some critics have considered Thiebaud part of the pop art movement, but his interest in painting ordinary objects pre- dated the emergence of pop art, and Thiebaud’s work has never been as ironic or critical of mass culture as much pop art is. I spoke to Wayne Thiebaud in his studio in Sacramento.\nELIZABETH FARNSWORTH: Why was it risky for you to start painting pies and cakes?\nWAYNE THIEBAUD: Well, it’s silly, you know. It’s fun and humorous and that’s dangerous in the art world, I think. It’s a world that takes itself very seriously, and of course, it is a serious enterprise, but I think also there’s room for wit and humor because humor gives us, I think, a sense of perspective. And I think, like W.C. Fields said, “if we haven’t been able to see ourselves as a cartoon character, we’ve not seen ourselves clearly.” At least at some time, because it’s… That’s part of the human enterprise, isn’t it?\nELIZABETH FARNSWORTH: Some people see sadness in the toys and in the pies and even in the cityscapes, a kind of longing. Do you feel that?\nWAYNE THIEBAUD: Well, I think… Yes, I think, I think there’s something about what someone calls “bright pathos,” like circuses and clowns, and toys. I think toys… Someone said that toys represent something special. How does it go? A child’s toys are the grandfather’s dreams. And the sort of elegance of that in terms of our history and the way we see our evolutionary procedures.\nELIZABETH FARNSWORTH: What about the gumball machines? We shot a series of them and you’ve been painting them for a very long time– the penny machines. What do you like about those? How do you see them?\nWAYNE THIEBAUD: I just see them as sort of magical objects and interesting and… very, very interesting objects to work on because of the color. I mean, a big round globe is so beautiful and it’s really a kind of orchestration of circles of all kinds. But it’s also very sensuous, I think, and it offers wonderful opportunities for painting something like, almost like a bouquet of flowers.\nELIZABETH FARNSWORTH: You’ve said before that you don’t consider yourself an “artist,” what do you mean?\nWAYNE THIEBAUD: Well, isn’t it something for other people to make a decision about? I think it’s just like, as I say, it’s like a priest referring to himself as a saint. Maybe it’s a little too early or he’s not the one to decide that. It’s decided apart from you and that’s the way it should be. It’s… Being an artist I think is a very rare thing. There aren’t very many people who achieve that and I think we ought to keep it as a golden special word so that it… It doesn’t get all gummed up or dirty or too usual. It has to be special.\nELIZABETH FARNSWORTH: So what do you say when people say what do you do? You say “I’m a painter?”\nWAYNE THIEBAUD: A painter and then sometimes they ask me to paint, paint their house.\nELIZABETH FARNSWORTH: You say that you steal from everybody you can. What do you mean?\nWAYNE THIEBAUD: Well, I’m a visual bandit. Just… That’s just the way it is. It’s like anything. You learn by the help of other people, what they’ve done, and how to go about it. There are many people who I’m indebted to, people like Richard Diebenkorn who meant a lot to me in terms of this area.\nELIZABETH FARNSWORTH: Tell us about the five seated figures. What were you trying to do and why are they all sitting and looking away from each other?\nWAYNE THIEBAUD: People always think I have some message and one woman said that she knew it exactly what was happening to those five figures– they’re all mad at each other.\nELIZABETH FARNSWORTH: But that’s not what was happening?\nWAYNE THIEBAUD: I said, “no, that’s not true.” So then she chided me. She said, “why won’t you tell me what’s going on with those figures?” I said, “I don’t really… I don’t know, I don’t know. She says, “Come on, you can tell me. I’m a psychiatrist.” So I just don’t know and don’t really want to know– it’s that kind of a probe.\nELIZABETH FARNSWORTH: And the cityscapes, you’ve been painting them for a long time. Do you see them changing? And if so, how?\nWAYNE THIEBAUD: I think just setting different problems because you don’t want to repeat yourself. You try if you’re working on too many parallel streets and you try to take that away and maybe open up the thing to a greater distance, try to deal with a different kind of space. Sometimes it’s sort of telescopic space. Sometimes you try to expand the space so that you have a kind of infinity. So mostly it’s just really a series of problems– sometimes in color; sometimes you decide that you want to not use bright colors but take the register way down to grays or maybe to a very dark palette. So it’s like music where you transpose something into a lower key or a different key.\nELIZABETH FARNSWORTH: The delta paintings are tremendously colorful, the most colorful I think of, at least everything I’ve seen that you’ve done. Is there any explanation for that? Is it just the way the delta is? What you want to do right now with color?\nWAYNE THIEBAUD: That happens because of the phenomenon of the time span of the delta where you see it in various seasonal times, in the winter very dark, very gray. So the color aspect of those overall, are in some ways trying to encapsulate or anthologize those various seasonal changes.\nELIZABETH FARNSWORTH: Tell us about “Green River Lands,” what you wanted to do, what problems you were solving, what you were seeing.\nWAYNE THIEBAUD: I think in some ways it’s the most extreme one. It’s the most curious one in terms of the variety of points- of-view. A lot of them are read quite quickly as aerial views, which they really are not. They’re a combination of sort of ground level, and high middle, and very high. But it has a lot to do with, I think, Chinese painting or oriental painting where you really… it’s almost scroll- like in terms of its verticality. It may be a big failure. But it was a wonderful thing to try out these various kinds of things.\nAnd I like… I like the idea of extremes in some way. I think that’s part of how we get to something like art where you try– are willing to push the extremes — not so much me, but with someone like Rembrandt, where he’ll make the picture all go to almost black and he’ll leave just the forehead, nose, and little finger down here almost, you have to build the rest — or Cézanne with these little touches, little pieces of glass that you could almost shake his paintings and they would fall, you know. So those extremes, I think, are really wonderful to pursue.\nELIZABETH FARNSWORTH: Do you think– as you know, some writers and painters and musicians get better the older they get and some don’t– do you think you’ve gotten better?\nWAYNE THIEBAUD: You hope so, but you never know. I see paintings I think are better than I’m doing at some cases now. It’s odd. It’s something which I think is not so much to think about as to think about the… the wonderful thing that you can still keep going. I heard Robert Frost once say, if I can– he was like in his eighties I think– and he said, “if I can get up and have the presence of mind to make my bed, the rest of it’s all gravy.” I can go to work and make poems and it’s sort of that way with me at least.\nELIZABETH FARNSWORTH: Well, Wayne Thiebaud, thanks for being with us.\nWAYNE THIEBAUD: Thank you so much.']"	['<urn:uuid:09dc8c49-3d6f-4e99-98b3-039dd35617dc>', '<urn:uuid:7d4b66cd-3089-4ed0-9c23-ed041ed1e67e>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	9	35	3145
90	I'm planning some outdoor projects - which requires a deeper hole to start with: installing a retaining wall or making a brick patio from scratch?	A brick patio requires a deeper initial hole than a retaining wall. For a brick patio, you need to dig 9 to 10 inches deep, while for a retaining wall, you only need to dig a six-inch trench.	"['Looking for something to do for Father\'s Day? How about building a short retaining wall out in the garden? Sounds like a lot of work? Yep, it is. But it is worth all of the blood, sweat, and tears. Let\'s just hope we can keep the blood and tears to a minimum.\nThere are lots of different ways a retaining wall can solve problems. If your yard has a slope that is under three feet, a wall can help break things up. Measure the space you want to fill. If you aren\'t sure how the wall will look, lay a garden hose on the grass in its place so you can get a feel for how you want the wall to go. A can of spray paint or a bag of flour also can be used to make a line in the grass to keep you on track. Stakes and a long string create a straight line for a simple wall.\nYou can make the wall as long as you want, but the key is not to go any higher than three feet. If you want a taller one, check with a professional to make sure your wall is engineered to handle water drainage, heavy winter snow, and shifting soil.\nYou can make a retaining wall out of lots of materials, including wooden landscape ties, stone, concrete blocks, and bricks. No matter what type of material you use, begin with a plan on paper.\nDon\'t build a wall around the base of a tree or in an area with poor drainage. You may end up with more problems in the future.\nConcrete pavers are a quick way to build a short wall or a raised area around the yard. They are made to create gentle curves or straight lines and have indentations and bumps so they will fit together like a puzzle. But you have to work ""with"" the material, not against it. I\'ve walked through some gardens where the do-it-yourselfer installed the bricks backwards. Instead of gently sloping back into the bed, the stones pushed forward. That kind of mistake can result in a failing wall and even injuries if the pavers fall over.\nMeasure the length of the wall you are going to build. Take those measurements to your favorite home and garden center and they will help you get enough material to build the wall. Get a few extra pieces just in case some are broken or don\'t fit properly.\nBefore you stick your shovel in the ground, call your local utility company to mark the underground lines. You wouldn\'t want to dig up the phone line, would you? (Not that I have done that before!)\nGet the shovel out and dig a six-inch trench a little wider than the stone. Try to keep the bottom of it as level as you can. Add a two to three-inch layer of sand or crusher dust from a home and garden center and tamp it down so it is firm. Now you are ready to start building.\nA level base is key, so have a rubber mallet or hammer handy to keep the bottom layer as level as possible. Spend extra time making sure things are perfect at this stage; it will make your job a whole lot easier as the wall gets taller.\nStart laying the first row of pavers into the trench. When you get to the end, you might have to cut a paver in half with a chisel and hammer. You want the next row to stagger on top of the row below it, so you will have to add half a paver at the bottom. Save the leftover piece because you will use it in another row.\nOnce you get a couple rows going, back-fill the area with some new soil and tamp it in place. This helps keep the foundation solid. Then put on another row, being careful not to go over three feet high.\nWhen you reach the top, you are ready to put rich top soil in the bed. If you want to keep the weeds out, put the landscape cloth down first before you start planting. It\'s a perfect place for dad\'s new hammock!', 'Preparing the Patio Base\nDesign the layout of the patio. Allow adequate room for your family’s activities. Consider what patio furniture you plan to place on the new patio as well as where you intend to set the barbecue grill. If you want to use the house as shade, work that into your design.\nMark the boundary of the brick patio with stakes and string. Allow 6 to 8 inches more than you included in your design. This gives you some room to work with the bricks as you set them. As you set the string on the stakes at the level you intend to lay the bricks, allow for a slope of 1/8- to 1/4-inch per foot away from the house to allow for drainage.\nDig out the patio area with a shovel. If you have a large area, rent a skid steer loader. A depth of 9 to 10 inches provides a solid base and puts the bricks at or slightly above grade level.\nCompact the subsoil with a plate compactor. Cover the soil with 2 to 3 inches of angular gravel. Rake the stones until they create a level surface. Dampen the gravel with a garden hose. Compact it in place with the compactor. Add another 2- to 3-inch layer of gravel, rake it level and compact it. Continue adding gravel until the gravel surface gets to within 3 inches of the string guide you set up in Step 2.\nSet up parallel lines of 1-inch conduit pipes to use as screed guides on the gravel base. Screeding is the process of leveling the base by filling low spots and lowering high spots by rubbing a long, straight board over two parallel pipes. Space these pipes no more than 7 feet, 6 inches apart. Try to arrange them so that you can do the screed work from the edge of the patio. Once you have a pattern established, press the pipes into the gravel base about 3/4 inch. If you set a paver on top of the screed pipes, the paver’s top should extend 1/8 to 1/4 inch above the string line. If it doesn’t, raise or lower the pipes accordingly.\nPlace several shovels of gravel between two screed pipes closest to the house. Set a straight 8-foot length of 2-by-4 on the pipes. Work the board back and forth as you pull it toward you to level the gravel. Keep adding gravel until you complete that section. Add gravel and screed it in the remaining sections of the brick patio.\nRun the plate compactor over each section of the patio. Do not disturb the screed pipes. At this point your base should be solid enough to not leave a hand print.\nPull the screed pipes. Try not to disturb the gravel as you do so. Lay additional gravel in the trenches left by the pipes. Compact it into place.\nLaying the Brick Patio\nSnap a straight line parallel with the wall of the house, using a chalk line. Snap a second line perpendicular to the first. This gives you a right angle to lay all your brick from.\nSet the bricks, starting at the right angle. After you set the first brick, bring the edge of the next brick up to the first and drop it into place as close as possible to the first brick. Avoid scuffing the gravel underneath as you lay bricks in the pattern you selected for the patio. Extend bricks beyond the boundary where the curved edges are to be.\nLay 1/2-inch conduit pipe on top of the set bricks in the shape of the curved edge. Use a sharp metal object, such as a chisel, to score along the line marked by the pipe. Remove the pipe.\nCut along the scored line with a cutoff saw. In most cases you don’t need to remove the brick from the base to make the cut.\nLay a soldier course of bricks along the entire perimeter of the patio. Place the short edge of the bricks firmly against the last row of bricks you laid. If you prefer, lay a sailor course, by placing bricks end to end along the perimeter.\nSet the flanged edge of the paver edge restraint firmly against the soldier course. Drive the pins that come with the edging into the gravel base, using a hammer. Cover the remaining opening between the brick patio and the lawn with topsoil.\nSprinkle sand over the entire patio. Use a push broom to work it into the cracks between bricks. Set a protective pad underneath the plate compactor and go over the entire patio with the compactor to settle the sand. Add more sand and compact the patio again.\nThings You Will Need\n- Skid steer loader\n- Plate compactor\n- Garden hose\n- 1-inch conduit pipe\n- 2-by-4, 8 feet long\n- Chalk line\n- 1/2-inch PVC pipe\n- Cutoff saw\n- Paver edge restraint\n- Push broom\n- Contact the local utility companies before you dig. They can mark the location of any buried lines on your property so you don’t accidentally hit them.']"	['<urn:uuid:a71d3c40-0934-4939-9ec6-2bee31163522>', '<urn:uuid:7a1e7755-96d4-4203-bd1d-6b16e3a3cf8a>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T06:15:13.904987	25	38	1558
91	workplace injury prevention methods company financial advantages	Workplace injury prevention includes implementing a hierarchy of safety controls and providing comprehensive training, which offers significant financial advantages. Companies must use work area designs that segregate workers from moving equipment, install proper guarding on machinery, and use PPE as a last resort. From a financial perspective, this prevents costly accidents and lawsuits while increasing productivity and profitability. Training evaluation shows specific ROI measurements of the financial impact, where proper safety education leads to more efficient employees, better regulatory compliance, and improved worker retention, all of which positively affect the company's bottom line.	['How to Make Safety Training a Top Priority\nTraining is a major responsibility. Whether you’re a training professional, a veteran of company training, or have just been asked to take over or start up a training program, you have a lot riding on your shoulders. Just think about all the different kinds of training employees need throughout their careers:\n- New employee orientation\n- Sexual harassment\n- Confined spaces\n- Emergency procedures\n- Software upgrades\n- Refresher training\n- Hazard communication\nYou can probably add many more topics to this list. There’s no doubt that employee training is an ongoing need in every company. Unfortunately, however, it’s not always a top priority. As the person now responsible for training in your organization, you can bring employee learning to the top of the priority list—and become a hero in the process.\nHow can you accomplish such a feat? By turning yourself into a skilled trainer who inspires lifelong learning in company employees at every level and who keeps up-to-date on the ever-changing and always innovative training industry.\nBuilding the Best Training Program will show you how to create—and run—an effective learning program that motivates and trains your workforce to be the best in your industry. This guide is a one-stop resource for all your training needs. Use it to help you become a trainer who makes learning an engaging and enjoyable experience for your trainees.\nEmployees will look forward to learning\nnew skills and reinforcing old ones, and managers will look forward to the increased profits that your\nwell-trained workforce will produce.\nOne of the first challenges trainers face is a resistance to training from both employees and\nmanagement. Employees are frustrated at taking time away from their jobs, and managers aren’t\nconvinced that time spent training is time well-spent.\nYou can convince both groups that training is crucial by proving to them that training doesn’t\ntake employees away from their work; training is, in fact, a crucial part of their work. In order to\nremain competitive in today’s global marketplace, it is vital that your company’s workforce maintains\nthe best skills and know-how to produce the best products and services. That means initial\ntraining to get up to speed and continual training in all new technologies, systems, or methods that\nbring more efficiency, more features, and better services.\nNO TIME TO PREPARE OR DELIVER TRAINING?\nWith TrainingToday, your employees can start taking essential training\ncourses the same day you sign up! Start your free preview now!\nConversely, a poorly trained workforce is more likely to turn out poor-quality products. Even if\nemployees receive top-notch initial training, your company can fall behind in the competitive\nmarketplace if employees don’t continue to learn how to do their jobs better.\nTraining affects the bottom line in every department. Here are just a few examples:\n- Human Resources\n- Legal compliance avoids fines for the company.\n- Career training aids employee retention.\n- Discrimination training avoids costly lawsuits.\n- Harassment training avoids lawsuits.\n- More efficient employees save costs.\n- Training improves productivity—which increases profits.\n- Up-to-date training gives you an edge over competitors.\n- Regulation compliance avoids costly fines.\n- Proper procedures avoid accidents, lost workdays, and workers’ compensation costs.\n- Emergency preparation aids quick evacuation.\n- First Aid training saves lives.\nIn short, an effective overall learning program for your company is a good return on investment\n(ROI). In fact, ROI analysis is an integral part of effective training evaluation because it gives specific\nmeasurements to the financial impact of training on the company’s bottom line. ROI answers the\nquestion “For every dollar the company invests in training, how many dollars does the company get\nMore Reasons to Train\nTo justify the need for training in your company, consider that in order to thrive in today’s business\nworld, your company needs to depend on employee education to promote six critical interests:\n- Effective use of new technology. As technology continues to revolutionize the workplace, employees at all levels and with all degrees of experience rely on training to keep up with changes in their work processes. Due to its importance, this training will require a comprehensive and continuing effort.\n- Competitive edge in your market. U.S. companies now receive fierce competition from overseas operations. In many cases foreign companies are beating out U.S. firms in quality, cost, and service. In order to remain competitive in the current marketplace, employees need to know how to make better products and services for your market.\n- Safety and health of employees. In order to have a productive, creative, and committed workforce, employers need to make sure that employees are protected from workplace hazards and given the knowledge and skills they need to work safely. Safety training is a key component of any organization’s productivity and prosperity.\n- Retention of skilled workers. Skilled and creative employees seek opportunities for career development and personal growth in their jobs. They want the chance to do challenging work and be well-compensated; they also want to be with a company where they can continue to learn and enhance their skills. If they don’t receive adequate training opportunities in their organization, they will find someplace where they can.\n- Compliance with laws and regulations. Training is frequently required under a variety of government regulations. The Occupational Safety and Health Administration (OSHA), for example, requires employers to conduct annual employee training in a\nnumber of safety procedures. In other cases, although laws may not require it, training (at least of key employees) is highly advisable to avoid problems (for example, sexual harassment, discrimination, violence prevention, diversity). The cost of not adequately training employees in all of these areas can translate into large fines (for failure to meet the requirements of regulations) or expensive lawsuits (for failing to uphold the rights of protected employees). [See Appendix A: Master Training Guide for 29CFR and Appendix B: Human Resources Training.]\n- Productivity and profitability. Finally, training makes workers more skilled and knowledgeable, which makes them more productive, better able to meet quality standards, and more able to provide excellent service to customers. Training, therefore, makes organizations more competitive, more profitable, and more successful.\nThe State of the Training Industry\nThe American Society for Training & Development (ASTD) conducts an annual “State of the Industry” survey, which reveals what companies are doing for training, how much they’re spending, and what they’re reaping in return. Spending on training may fluctuate from year to year, but the overall trend is companies that are more committed to keeping their workforces well-trained and up-to-date see the best returns on their investments.\nASTD’s “State of the Industry” identifies the following training trends:\n- Company profitability is positively correlated with training expenditures.\n- Classroom training is steadily decreasing.\n- Training with learning technologies is steadily increasing.\n- The outsourcing of training is remaining steady.\nThe survey also reveals that most companies use all three methods of delivering training in a blended learning approach.\nConstruction is one of the nation’s largest industries. There are more than 702,000 construction companies in the United States. Employees in construction are more prone to injuries than other types of workers.\nOSHA believes that computer-based training programs can be used as part of an effective safety and health training program to satisfy OSHA training requirements, provided that the program is supplemented by the opportunity for trainees to ask questions of a qualified trainer, and provides trainee\nSafety meetings and safety talks are usually thought of as departmental or crew meetings. These frequent meetings, ranging from weekly to monthly depending upon the situation, allow the supervisor or trainer to review key safety topics and review any accidents incurred by members of the group.\nTesting your employee’s understanding of safety benefits everybody. Tests help you assess the effectiveness of your employee safety training. They help you measure employee understanding and retention of safe practices, gauge the competence of each trainee and document compliance with regulations.', 'No matter the industry, there are workplace hazards that can kill or seriously injure employees. As a Safety and/or Training Manager, you do your best to train employees on hazard mitigation. You do what you can to make the work environment safe.\nWork areas are designed with safe workflows to segregate workers on foot from moving plant and equipment. There are posters and signs everywhere to remind workers about workplace hazards that can result in fatal injuries. The safety hierarchy of controls have you working from the top to only rely on employees wearing PPE as a last resort.\nBut when all is said and done, there are still workplace hazards that are missed and injuries and fatalities occur.\nFor this article, we discuss the eight most common workplace hazards that can kill or have the potential to result in a serious injury. We look at complacency and how to reinvigorate employee focus on workplace hazards. We also provide critical control measures for each of the eight hazards to guide safe work practice.\nComplacency Has Set In\nYou’ve trained all new employees at your safety induction. It was likely to be comprehensive and many hours long. All the critical hazards have been covered, and you have emphasised their danger. You’ve provided face-to-face training to ensure practical competence. You’ve gone down the path of e-learning to remind and educate employees on how to work safely. But you’re still having incidents and employees are still getting injured. Or you’ve heard about or witnessed too many near-miss events. You’re concerned that the next near-miss will result in a workplace hazard that can kill an employee. What do you do?\nComplacency has started to set in where workplace hazards have become wallpaper. You need to reinvigorate and wake up employees to focus on workplace hazards that can kill or seriously injure them. There is now a need for dedicated hazard perception training. This training should be underpinned by the safety hierarchy of controls. The training should focus on critical risks and control measures to protect employees. Hazard perception training needs to be completed on a recurring basis to regularly refresh and remind.\nSee our course on understanding and applying the Hierarchy of Controls.\nThe Importance of Hazard Perception Training\nEngaging training, influences work health and safety behaviour and improves hazard perception skills. The Tap into Safety Platform offers interactive and engaging hazard perception training that is delivered via smart devices and online. We focus on critical risk and the common workplace hazards that can kill within industry-specific scenarios. The Platform has a substantial library of pre-built training modules across a range of industry settings. If we don’t have what you need, we also build custom training content.\nAs a business, we are embracing the enhancements in technology. Our training isn’t numerous PowerPoint slides shown to a group of employees, followed by a paper questionnaire at the end. We use real workplace photographic, 360-degree panoramic examples that workers relate to because it shows their work sites. We are using animation, gaming technology and microlearning to enhance training to engage employees and deliver training in 15 minutes or less.\nThe platform is far more than just a video library. We have a complete training platform that includes your safety induction, hundreds of courses that we’re adding to every week, customisation of the content, in-built assessments, certificates and in-depth GAP analysis reporting that provides a comprehensive audit trail and shows you where to focus your future training efforts.\nWith our flexible per-use ‘credits’ model pricing, you simply pay for what you use. You can purchase any number of credits at any time, that you can use within 12 months before they expire. There are no subscription fees or lock-in contracts. And we also cater for larger users with our unlimited annual use package.\nWhat Are the Common Critical Workplace Hazards That Can Kill You?\nThere are 8 common workplace hazards that sit on most company’s critical risk registers.\n1. Working at Heights\nFalls from height are workplace hazards that can kill or result in serious injuries. Working at height equipment should be regularly maintained and inspected before use. These include harnesses, lanyards and anchor points. If a person was to fall, anchor points must be capable of withstanding the force applied. Work platforms and scaffolds must have complete floors, handrails, edge protection, barricades and toe-boards. These prevent people, tools and materials falling to the levels below. Work platforms must have a safe means of access and egress and edge protection.\nSee our article on preventing fall from height risks.\n2. Suspended Loads\nWorking around cranes and lifting equipment can expose workers to suspended load hazards and fatal injuries. People are never to work, walk or reach under a suspended load. All lifting equipment must have clearly visible the safe working load that can be safely carried. For all lifting activities, a lift plan should be in place. The lift plan should include information about the total weight and height of the item to be lifted. Information on the lifting equipment to be used (crane and rigging), and the proximity to hazards, e.g. energised power lines. Barricades or an exclusion zone needs to established around the crane to cover the entire working area. The entry of unauthorised personnel into the drop zone must be prevented\nSee our course on Mobile Crane hazards.\nComing into contact with electricity is more than likely to result in a fatality. Portable electrical equipment should be inspected before use to identify any defects. If it is found to be faulty, it should be isolated, tagged and placed out of use. A current test tag should be on the equipment. A licensed electrician should be the only person to carry out electrical work. When working in high-voltage areas, ARC rated electrical PPE should be worn that shows the voltage exposure rating.\nSee our course, Manual Handling and Overhead Electrical Work.\nUnexpected movement of equipment and the release of stored energy hazards can lead to serious and fatal injuries. Personal isolation locks and tags should be applied to prevent unexpected movement of plant, equipment or releases of energy. Only the worker who has applied isolation tags or locks is permitted to remove them. It is also important that the tags and locks are removed immediately after the work has been completed.\n5. Hazardous Materials\nExposure to hazardous materials such as chemicals and asbestos can lead to serious and long-term injury. All containers or plant being used for the storage and handling of hazardous materials must be suitable, and safe, for use with those materials. Any spills, leaks and any inadvertent release of material, need to be confined within the area where they occur. Immediate action must be taken to assess and control any risks arising from them. All work areas should be provided with adequate fire control measures. All hazardous waste materials, including empty hazardous materials containers, should be disposed of according to the SDS.\n6. Physical Separation and Barricading\nSeparating people from moving machinery is one of the most common hazards for many workplaces. All moving machinery parts, where there is a risk of projectiles or entrapment, should have guarding installed. If guarding is to be temporarily removed or deactivated on plant, equipment or machinery, isolation procedures must be used. Plant and equipment should have emergency stops installed that are visible and located within easy reach. Signs and barricades should be in place to alert of the dangers and restrict interaction with moving plant and machinery.\nSee our course on Access Egress Barrier Selection.\n7. Fire and Emergencies\nFires and emergencies can cause multiple fatalities. An Emergency Plan should be prepared, that outlines what workers and others at the workplace should do in an emergency. An Emergency Plan should include emergency and evacuation procedures. How to notify emergency services. Instructions on medical treatment and assistance. Communication protocols between Safety Wardens and other personnel. Everyone should be trained on the types of emergencies and how to implement emergency procedures.\nSee our course on General Safety that covers many of the high-risk tasks we’ve discussed.\n8. Confined Spaces\nConfines spaces have hidden workplace hazards that can kill, where there can be low oxygen levels or gas present. Entry to a confined space should only be allowed after an authorised Confined Space Entry Certificate has been issued. Confined spaces should be clearly signed. When entering a confined space, you must have sign-in and sign-out procedures. Air and gas monitoring equipment must be in place. Choose an appropriate breathing apparatus. Provide adequate communication equipment. Develop a rescue plan with appropriate equipment. Always assign a sentry. The Sentry must be continuously positioned outside the confined space entry point while personnel are within the space. The Sentry should not be assigned any other duties.\nSee our article on recognising, recalling and reporting workplace hazards.\nThere are 8 common workplace hazards that can kill or have the potential to result in a serious injury. They include working at heights, suspended loads, electricity, isolating equipment, hazardous materials, physical separation and barricading, fire and emergencies, and confined spaces. Addressing these eight workplace hazards should be the staple diet of your safe work practices. To address complacency, and to keep them front of mind, you should invest in methods to regularly refresh the critical controls. This is where regular safety training that focuses on hazard perception skills plays a very important role.']	['<urn:uuid:630224ae-c369-4c12-9b40-9b1cecde4971>', '<urn:uuid:482029f0-03f0-4d74-a681-4d93fff26cdf>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T06:15:13.904987	7	93	2893
92	how much funding grants artists get starts residencies program support expenses	The STARTS Residencies program provides grants of up to €30,000 to artists as a contribution towards their participation and expenses. The program has allocated a total budget of €900,000 to support 45 residencies between 2017 and 2020. Additionally, producers can provide extra support to the residencies.	"[""Print your city. Photo and project by The New Raw\nSTARTS (Science, Technology, and the ARTS) is an initiative funded by the European Union's Horizon 2020 program, and coordinated by IRCAM. STARTS encourages collaborations between science, tech companies, the creative sectors and artists as a means to foster the development of human oriented technologies. The project uses four activities in realizing the goals of the project: an annual prize, residencies, Lighthouse Pilot projects and academies.\nThe STARTS Residencies project promotes and supports innovation processes involving artists who make original contributions to technology-based projects. A grant (of up to €30,000) is awarded to the artist as a contribution towards her/his participation in and expenses arising from the program. A producer can also provide additional support to the residency. The production of the artwork is also part of a co-creation process with the tech team. A total budget of €900k has been allocated for the artistic grants, supporting the implementation of 45 residencies between 2017 and 2020 on an as yet unprecedented scale.\nThe program’s promotion and selection have been organized into 3 calls which took place between 2017 and 2019, each one divided into a call for projects willing to host artists, a call for producers ready to support them, and then a call for artists who were proposed to select one tech project among those available and to draft a residency proposal. Artists of any nationality and artistic field were welcome. The artistic applications were first reviewed by their tech projects and then finally selected by a high-level international jury.\nRead more about the STARTS project here.\nIn this article we would like to present three artists who are participating in different STARTS residencies and are also members of the ULYSSES platform. The texts are based on interviews with Reiko Yamada and Aaron Einbond and a project description by Rafał Zapała.\nThyroid. Drawing by Reiko Yamada.\n1. Shortly, what is the aim of the project?\nExplore and stimulate the communication between thyroid patients and their physicians using the LUCA device.\n2. What are your own goals / challenges as an artist in this particular project?\nExplore and stimulate the communication between thyroid patients and their physicians using the LUCA device!\n3. Do you consider yourself and your art as ”a driver of technological innovation” in the project?\nTechnological innovation is not a linear process. My role is to push it sideways, in directions it may not have taken it otherwise, and most importantly bring a self-reflective element that is often lacking in the process of technological innovation.\n4. What have been the main challenges, if there are any?\nCommunicating across disciplines is extraordinarily difficult--both the scientist and myself have spent decades training ourselves in our respective fields, so it is hard to convey the nuances of what we do to each other. Beyond the specific methodologies on either side of that divide, differences in the culture that’s specific to each discipline as well as the institutional context in which the interactions take place, they often create unexpected obstacles. We overcome them, but that’s a lot of work.\n5. Do you think that the collaboration between music artists and technology companies or projects is or will be important in the future for the development of music as an art form and the work music artists do?\nMy work is fed by constantly discovering--and being inspired by--new worlds, and the research of the scientific team I’m collaborating with on this project is exactly that, a new world to me. The fact that their work has practical applications (improving the diagnosis of thyroid cancer), however, gives this particular project a unique importance. Art can easily live and thrive without collaboration with scientists, but that kind of collaboration does have the potential to feed the work of artists.\nRead more about Reiko Yamada and her project here.\nSee the STARTS pages of the project here.\nCosmologies is a project to delve into the inner life of the concert grand piano and situate the listener in a virtual space flickering with its real and imaginary sounds. Artificial intelligence research is ubiquitous, yet often ignores the spatial presence of the live instrument and performer. In this project, I propose to use machine learning to decipher the intricate interdependencies of timbre and space that bring an instrument to life. With OM7/om-spat the project will interrogate the musical microcosm of the piano interior as it is exploded out to the space of the concert hall surrounding the listener.\nWhile I have worked on projects with music technology throughout my artistic career, this project will make more complex demands than before on computer programs, algorithms, acoustical science and technical thinking. This requires collaboration to produce a project that is broader than I could conceive as a composer alone. Working with Jean Bresson on the technical project OM7/om-spat, as well as collaborators Diemo Schwarz and Thibaut Carpentier, I hope will provide this opportunity to conceive a sonic performance situation beyond what I could imagine alone.\nThis project, as with all my compositional work, could be viewed as a ”driver of technological innovation”, but it could equally be seen the other way around: in both cases, novel research (both scientific and artistic) drives me to collaborate with top researchers in their fields to produce collective projects that challenge all our creative assumptions. This could include interactions with computer music developers like my collaborators on this project Jean Bresson, Diemo Schwarz, and Thibaut Carpentier. It could also describe exchanges with instrumentalists whose experimentation with extended playing techniques helps to push their instruments’ capabilities as well as my sonic imagination. In each case, a technology (computer software, an acoustic instrument) is both a driver and is driven toward innovation.\nMusic and technological collaboration is and will continue to be important for artistic work. However, this does not implicate only private-sector technology companies: there is also an essential role for basic research in the arts and technology funded publicly through universities and research institutions like Ircam. Publicly funded arts institutions like music festivals and cultural centers are also invaluable supporters of artistic exploration. Although contributions from commercial technology to artistic work are welcome, profitability should never become a measure of artistic value, and basic research must continue to rely on public funding to support risky ideas that are not immediately profitable, nor may ever be.\nRead more about Aaron Einbond here.\nSee the STARTS pages of the project here and here.\nCosmologies of the Concert Grand Piano, Artist Aaron Einbond\nMy ambition is to go beyond the solely educational or artistic areas; it is based on integrating social and artistic ideas, that I have been working on for several years. The concept grew in shared conversations with PSNC researchers, since we both reside in Poznań, Poland, and the PSNC laboratories are located in the vicinity of The Ignacy Jan Paderewski Academy of Music in Poznań, where I work as associate professor.\nNew technological reality and new habits, (domination of visual culture, multi-tasking, mixing of “real” and “virtual” reality, etc.) cause “civilization disease” of problems with attention and focusing for the generation which grew in those new conditions. We observe these problems in our educational work; they also come under scrutiny at FutureLab PSNC. The goal is to design a new tool to help young people control their concentration and realize the continuity between the mind and emotions.\n“Sensorium” is an intimate, “anti-mass”, one-person experience. It will essentially consist transposition of dynamics of visitor’s emotional reactions (by means of biofeedback technology in the tradition of Alvin Lucier’s artworks) upon musical structures in a direct “live” experience. The goal is to create an “instrument“ (Concert Machine) which enables hearing one’s psychophysical reactions, experience (through organised sound) their variability, as well as comprehend the possibilities of exertion to take control over them. “Sensorium” is not an artwork to be exhibited as a concept; it is rather a next-generation space, encompassing new reality. “Sensorium” is intended to operate as a new kind of musical experience available in urban space. It is meant to operate permanently, with changeable artistic programme, though it has a potential to be used in various scopes of music compositions.\nSee the STARTS pages of the project here.\nSensorium Audio Theatre""]"	['<urn:uuid:32355eb0-80c3-4a54-b301-06dedff71656>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	11	46	1376
93	protect wood slice table maintenance	Regular maintenance of wooden cutting boards requires cleaning after each use and applying mineral oil periodically. Mineral oil is ideal because it's odourless, colourless, and makes the wood water-resistant. The oil should be applied using a clean, dry rag in slow and steady movements, coating the surface until visible, then left overnight to dry completely.	['A cutting board is the most used product in our kitchen, and with a quality wooden board in use, they can be even more valuable. Nowadays, people like to not only prepare their meals on these elegant boards, but also use them as serving plates at meal time.\nWhile a wooden cutting board is far more durable and safe to our knives and benchtops, they do require a little extra love and attention. Regular cleaning of your board is essential as well as applying an oil for cutting boards to keep it looking good and working at its best.\nAcacia wood cutting boards are one of the most popular types of material used for superior cutting boards, and this unique wood has a range of benefits to the owner. Just like any quality wood, though, you will need to care for it correctly with a mineral oil cutting board product and regular cleaning.\nAcacia is a species of hardwood, known for its gorgeous colours and durability. This unique wood is the ideal material for a cutting board as it’s strong enough to withstand years of use, but is also very resistant to scratches and dents.\nAnother reason for acacia’s popularity, particularly with cutting boards, is its affordability. Of all the quality hardwood options, it comes in at just a fraction of the price of the others. Just because it’s more cost-effective does not mean the wood is inferior, as it has all of the same benefits of the other hardwoods.\nThe surface of an acacia wood cutting board is soft to the touch, meaning your knives and benchtop will remain unharmed as you cook. However, this gentle surface covers a strong and durable wood that will last your kitchen for many years.\nWoods need to be cared for, whether they’re in our furniture, homes, or kitchen accessories. Maintaining and caring for an acacia wood chopping board can be done with regular cleaning and a cutting board oil applied every so often.\nThere are many advantages to oiling your cutting board regularly, and it’s such a simple job with dramatic effects that you may find yourself wanting to do it all the time. Here are just a few of the benefits from applying chopping board oil to your acacia:\nWhen selecting an oil for wood cutting board, there are a few important things to consider. Always choose an oil free from fats, as these can enter the cracks of the wood and turn rancid over time, leaving a very unpleasant smell.\nWhile there are many options available for specially marketed wood oils, the best oil for cutting boards is a mineral based food grade one. This cheap and effective product has all of the best qualities to help prolong the life of your acacia wood board.\nMineral oil is odourless and colourless, so it is ideal for wooden cutting boards. The natural properties of this oil make the wood water-resistant so it’s ideal for keeping moisture out which can damage the boards.\nNow that you have the best oil for the job, you’ll need to follow a few simple steps to finish the process. Allow for one to two days for the entire job, as there is some drying time in between steps.\nBesides the regular cleaning you should be doing after each use, it is helpful to give it a thorough clean before you decide to oil the board. Using white vinegar, scrub this into the board and let the pH levels of this product remove any excess germs or food particles left behind.\nAfter a thorough clean, leave your board out in the open air to dry. It’s best to do this overnight so you can ensure that no moisture is trapped deep inside the wood grains.\nTake your mineral oil for cutting board use and soak a clean, dry rag with it. In slow and steady movements, coat the surface of the board with enough oil so that you can see it.\nAfter applying the oil, you’ll need to leave the board again to dry completely. Try to oil it at night before retiring to bed and let it sit overnight for the best results.\nCheck the surface of your board the following day to see if you need to remove any excess oil. If the surface is sticky or oily feeling, get a new, dry cloth and make small circular motions over the top to remove any remaining oil.\nThe first time you oil your board, you can be a little more generous with the oil as this will allow it to reach deep into the surface. Once the board has been broken in, only apply enough oil to cover the surface without applying too much. With these few simple steps, you’ll be adding years to the life of your favourite kitchen accessory.']	['<urn:uuid:5c8851de-ead3-4b0d-a75d-e9238c572652>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T06:15:13.904987	5	55	806
94	How is benzoic acid used in skin care and cosmetics?	Benzoic acid is used to treat fungal skin diseases such as athlete's foot, tinea, and ringworm. It's also an important ingredient in cosmetic products like toothpastes, deodorants, and mouthwashes, where it acts as an anti-microbial agent.	['Global Benzoic Acid Market: Brief Account\nThe global benzoic acid market will witness a strong demand during the forecast period of 2018 – 2026 due to it wide variety of applications in food and beverage industry. Benzoic acid is a colorless and crystalline aromatic carboxylic acid. Benzoic was initially extracted from resin of tree that belonged to the Styrax genus. Benzoic acid is extensively used for preserving food products in food and beverage industry. It helps in curbing the growth of fungi and bacteria. Benzoic acid is also used as a chemical reagent. Surge in demand for packaged food and beverages due to rise in working class people and rapid urbanization have fueled the demand for benzoic acid. Increase in working hours and need for nutrient rich food have made consumers to opt for packaged food instead of homemade meals.\nGlobal Benzoic Acid Market: Trends and Opportunities\nThe global benzoic acid market is riding on the high demand for packaged food and beverages throughout the world. Benzoic acid is used to preserve these food products thus increasing the shelf-life of these products. Benzoic acid is also used for preserving acidified food products like pickles, sparkling drinks, and fruit juice, as the ability to produce desired results depends on the pH of food. Rising demand to substitute phthalate plasticizers for different applications is expected to boosts the global benzoic acid market growth.\nApart from food and beverage industry, benzoic acid also finds its application in plastics, pharmaceutical, chemical, and other industries. Benzoic acid can also finds its application for animal feed, dye intermediates, and medical purposes. Benzoic acid is used for treating fungal skin diseases such as athlete’s foot, tinea, ringworm, etc. All these factors are anticipated to boosts the global benzoic acid market growth. Furthermore, benzoic acid plays an integral role in manufacturing cosmetics products. Benzoic acid is an import ingredient in the toothpastes, deodorants, mouthwashes, etc. In these cosmetic products, benzoic acid acts as an anti-microbial agent. Apart from these, the cheap price and easy availability of benzoic acid is likely to favor the global benzoic acid market growth.\nGlobal Benzoic Acid Market: Regional Outlook\nThe global benzoic acid market is segregated into Europe, Latin America, Middle East and Africa, North America, and Asia Pacific. Of these, Asia Pacific has seen a gradual rise in consumption of benzoic acid over past few years, owing to rise in per-capita disposable income along with gradual shift towards packaged foods. The major contributor in this region are China and India. North America is anticipated to expand at a robust CAGR during the assessed period.\nGlobal Benzoic Acid Market: Companies Mentioned\nSome of the key players in the global benzoic acid market are Chemcrux Enterprises ltd, Emerald Kalama Chemical, Shri Hari Chemicals, Huangashi Taihua Industry, and Novaphene.\nThe report offers a comprehensive evaluation of the market. It does so via in-depth qualitative insights, historical data, and verifiable projections about market size. The projections featured in the report have been derived using proven research methodologies and assumptions. By doing so, the research report serves as a repository of analysis and information for every facet of the market, including but not limited to: Regional markets, technology, types, and applications.\nThe study is a source of reliable data on:\n- Market segments and sub-segments\n- Market trends and dynamics\n- Supply and demand\n- Market size\n- Current trends/opportunities/challenges\n- Competitive landscape\n- Technological breakthroughs\n- Value chain and stakeholder analysis\nThe regional analysis covers:\n- North America (U.S. and Canada)\n- Latin America (Mexico, Brazil, Peru, Chile, and others)\n- Western Europe (Germany, U.K., France, Spain, Italy, Nordic countries, Belgium, Netherlands, and Luxembourg)\n- Eastern Europe (Poland and Russia)\n- Asia Pacific (China, India, Japan, ASEAN, Australia, and New Zealand)\n- Middle East and Africa (GCC, Southern Africa, and North Africa)\nThe report has been compiled through extensive primary research (through interviews, surveys, and observations of seasoned analysts) and secondary research (which entails reputable paid sources, trade journals, and industry body databases). The report also features a complete qualitative and quantitative assessment by analyzing data gathered from industry analysts and market participants across key points in the industry’s value chain.\nA separate analysis of prevailing trends in the parent market, macro- and micro-economic indicators, and regulations and mandates is included under the purview of the study. By doing so, the report projects the attractiveness of each major segment over the forecast period.\nHighlights of the report:\n- A complete backdrop analysis, which includes an assessment of the parent market\n- Important changes in market dynamics\n- Market segmentation up to the second or third level\n- Historical, current, and projected size of the market from the standpoint of both value and volume\n- Reporting and evaluation of recent industry developments\n- Market shares and strategies of key players\n- Emerging niche segments and regional markets\n- An objective assessment of the trajectory of the market\n- Recommendations to companies for strengthening their foothold in the market\nNote: Although care has been taken to maintain the highest levels of accuracy in TMR’s reports, recent market/vendor-specific changes may take time to reflect in the analysis.']	['<urn:uuid:be3b346f-b274-49f4-93a2-a8898a63ca24>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:15:13.904987	10	36	858
95	ocean floor snow like substance composition nutrients falling from surface explained	Marine snow is a mixture of decaying bodies and feces that falls from the productive shallow oceans to the deep seafloor. This material is degraded by bacteria during its descent. Only about 2-5% of the total carbon from the ocean's surface reaches the deep seafloor - equivalent to roughly 2-3 tablespoons from a 5-pound bag of sugar. This marine snow provides essential nutrients in the deep ocean where photosynthesis cannot occur.	['Oh the dark deep sea is frightful,\nBut the food not so delightful,\nBut since we’ve got no place to go,\nLet It Marine Snow! Let It Marine Snow! Let It Marine Snow!\nIn the late 1960’s, two marine biologists, Howard Sanders and Robert Hessler, made a shocking find–the biodiversity of the deep-sea floor is astoundingly high. In an area the size of a coffee table over 300 species can coexist, a number that rivals tropical rainforests and coral reefs. Yet these findings also raised a paradox. High diversity is typically associated with physically complex habitats, like forests and reefs, plentiful with food that allow for a variety of niches. In the food poor, homogenous mud flats of the deep sea, how can so many species coexist? The answer is snow.\nThe lack of light in the deep oceans precludes photosynthesis. Thus, primary production of carbon, the base of a food web, is virtually absent. Deep-sea organisms are reliant upon a trickle of falling material from the productive shallow oceans overhead. This material is largely a low quality and low quantity mixture of decaying bodies and feces degraded further by bacteria on its decent into the deep. Roughly 2-5% of the total carbon on the ocean’s surface falls to the deep seafloor, the equivalent of roughly 2-3 tablespoons from a 5-pound bag of sugar. This sinking material, marine snow, falls as a dusting on the ocean bottom. But like a light snow in your yard does not form an even layer and Buffalo receives more snow than Miami, marine snow too is denser in some spots whether an area the size of coffee table or an entire ocean.\nIn this marine snow medley lays the answer for our deep-sea paradox.\nIn the 1970’s, Howard Sanders, Fred Grassle, and Paul Snelgrove proposed instead of the deep-sea floor being a homogenous wasteland, it was comprised of a variety of patches each with a unique set of organisms, i.e. the patch-mosaic hypothesis. The deep-sea floor is essentially a patchwork quilt of different small habitats. I began this year by publishing a study addressing how heterogeneity in marine snow of distances of just a few yards can lead to completely different communities of organisms. At the end of this year, just today in fact, I with coauthors show this same pattern over several thousands of kilometers.\nIn 2006, Jim Barry and I during my tenure at the Monterey Bay Aquarium Research Institute sampled a 3203-meter deep site off the Monterey Bay. We collected with the robotic arm of a remotely operated vehicle 44 sediment cores over approximately 400 yards. Each core we sieved and removed the small invertebrates living in the sediment, from worms to crustaceans to molluscs plus much more. Equally important, we measured the carbon content, of the sediment as an indicator of marine snowfall. Largely, we found that invertebrate communities in cores taken adjacent to each other were just as likely to be similar as dissimilar to one another. Indeed, cores adjacent to one another were just 3% more likely to share common species than cores taken 350 meters apart! Why would communities right next to one another be so different? Differences in marine snow accumulation. Invertebrate communities receiving comparable marine snowfall were more similar.\nToday in the Proceedings of the Royal Society with collaborators Allen Hurlbert and James Stegen from the University of North Carolina, I unravel the paradox of the deep a little further. Given the difficulty of conducting deep-sea work, patterns of diversity of entire oceans are rare. In 2008, John Allen, working previously with Howard Sanders, published an amazing dataset of deep-sea bivalves taken form 270 sites across the Atlantic Ocean. We combined this dataset with data on bivalve sizes and genetic relatedness with multiple datasets on the environment, including annual marine snow accumulation. We found that the availability of both chemical, i.e. marine snow, and thermal, i.e. temperature, energy explained differences in compositions of bivalves communities across the Atlantic Ocean. Interestingly, and in contrast to current thinking (including my own!) that invertebrates with planktotrophic larvae should be able to disperse everywhere, we also detected the importance of dispersal ability in explaining community differences. In other words, some of what determines where a bivalve is located in the Atlantic is determined by its dispersal ability and the amount of energy it requires.\nBut we went one step further and developed a simulation. We constructed virtual bivalves allowing them to evolve traits, fill environmental niches, and disperse across a virtual Atlantic Ocean. This is a computationally complex and demanding operation and required a cluster of computers at UNC to run. In each simulation, we could control the dispersal ability and food requirements for the bivalves. For each simulation, we would then compare the patterns that emerged with those in our real Atlantic bivalves communities. This would allow us to determine the exact level of dispersal ability and food requirements of bivalves to produce changes in community compositions across the Atlantic. From our simulations, we found that 95% of bivalves could disperse 749 km from their natal site. We also found that 5% of bivalve juveniles would not be able to persist in habitats that deviated from their optimum habitat more than 2.1 grams of carbon per meter squared per year. That translates to about 1 teaspoon over a dining room table over the course of an entire year! Bivalves are extremely sensitive to the amount food available.\nOverall, these studies illustrate that the deep-sea floor is like your Grandma’s quilt presenting a variety of patches of material. These patches, driven by differences in marine snow, whether occurring over inches or miles, provide unique habitats that allow a variety of different animals to coexist. And much like humans prefer different amounts of snow (give me warm weather or give me death!), deep-sea species are uniquely adapted to differences in marine snow.\nCraig R. McClain, James C. Stegen, and Allen H. Hurlbert Dispersal, environmental niches and oceanic-scale turnover in deep-sea bivalves Proceedings of the Royal Society B: Biological Sciences published online before print December 21, 2011, doi:10.1098/rspb.2011.2166\nAlso take a look at the great write ups by Wired and IO9. Love the titles! The Bounty of Species in a Single Scoop of Seafloor Mud and The Ocean Floor is Like a Rainforest Where Feces and Dead Animals Rain From the Sky\nUPDATE 2: I was asked for a legend to the animals above. Hopefully this helps. As for the species, I could give you the actual species names but perhaps that would not be helpful to readers. Instead I will give you the general groupings that may be more informative.\nSo in this orientation above from left to right\nRow 1 bivalve, polychaete, ophiuroid, polychaete, bivalve, cumacean, amphipod\nRow 2 cumacean, anemone, aplacophoran, bivalve, cumacean, bivalve, aplacophoran\nRow 3 polychaete, bivalve, cumacean, bivalve, big polychaete, amphipod, 2 oligochaetes\nRow 4 scaphopod, bivalve, aplacophoran, long polychaete, cumacean, amphipod, bivalve\nRow 5 bivalve, polychaete, aplacophoran, amphipod, bivalve, amphipod, polychaete\nRow 6 ostracod, polychaete with tube, bivalve, anemone, polychaete, amphipod, polychaete, bivalve\nRow 7 polychaete, gastropod, amphipod, caprellid shrimp, scaphopod, bivalve, polychaete, cumacean']	['<urn:uuid:1a86be05-82e9-4c23-b56c-e8067e06ff76>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	11	71	1190
96	What special wildlife can be found in Assam's ecosystems?	Assam is home to the one-horned Indian rhinoceros, the Royal Bengal Tiger, and numerous species of birds. It also provides one of the last wild habitats for the Asian elephant. These animals can be found in its rich biodiversity zones which include tropical rain forests, grasslands and wetlands ecosystems, now protected as Kaziranga and Manas national parks.	['Seven sisters of India: The Seven Sisters of India are relatively untouched, unexplored and isolated states of north east India. North East states are heavily forested and land of tribes peoples. These states are full of natural beauty,exotic places and wide variety of beautiful wild animals like rhinoceros, elephants and numerous species of birds. North east region consist of seven contiguous states Assam, Arunachal Pradesh, Manipur, Meghalaya, Mizoram, Nagaland and Tripura. Northeast India is ethnically distinct from the other states of India. Assam is the gateway through which the seven sister states are connected to the mainland. Sikkim, the only brother and some parts of North Bengal is also come under the north east India. India’s Northeast is one of South Asia’s hottest trouble spots.These seven sister states has major population of tribes peoples in India.\nAssam: The gateway of seven sisters is surrounded by other six states. Assam is one of the richest biodiversity zones in the world and consists of tropical rain forests, grasslands and numerous wetlands ecosystems now protected as Kaziranga and manas national parks. It is also know for its beautiful dark green tea gardens, Deep forest of Sal trees. Assam is the home for one-horned Indian rhinoceros, along with The Royal Bengal Tiger and numerous species of birds, it also provides one of the last wild habitats for the Asian elephant. Assam and other states offers so many big festivals like Bohag Bihu,Cheiraoba and Losoong.\nArunachal Pradesh: Land of the rising sun has many undiscovered places to attract tourist form many parts of the world. Tourist attractions include Buddhist monastery in tawang, Sela lake near Bomdila with its bamboo bridges overhanging the river and undiscovered scenic beauty of tuting. Rafting and trekking are also main attractable sports. Namdapha National Park located in Arunachal Pradesh, is one of the unexplored national park of India, It is also home for some of the giant animals, the great Hornbills and Red Giant Flying Squirrel.Manipur: “The originator of Polo game” is lies in an oval-shaped valley surrounded by blue mountains. There are four major river basins in Manipur and all the rivers in the valley area are in the mature stage. Manipur may be characterized in two distinct regions – Outer area of rugged hills and narrow valleys and the inner area of flat plain. The climate of Manipur is largely influenced by the topography of this hilly region. The valley region has a number of hills and lakes, The Loktak lake is an important feature of vally.Meghalaya: Meghalaya means “The Abode of Clouds”. Meghalaya lies in strip of hills in the eastern part, one third area of the states is forested. Meghalaya forest is also know for its biodiversity of mammals, birds, and plants. There are several falls around Shillong. The Shillong peak is highest in the state and is good for trekking. Meghalaya is the wettest place on earth, annual rainfall as high as 1200 cm in some areas. Cherrapunji holds the world record for most rain in a calendar month, while the village of Mawsynram, holds the distinction for heaviest yearly rains. The Langshiang Falls at Meghalaya is one of the largest waterfalls in India.Mizoram: Also called “land of the Mizo people” Mizo means “highlander”. Mizoram is a land of rolling hills, valleys, rivers and lakes,it has the most varicolored hilly terrain. Blue Mountain is the highest peak in Mizoram. These hills are separated by rivers, these rivers created deep gorges between the hill ranges. Hilly regions gives it a mild climate all year. Mizoram is rich in flora and fauna and many kinds of tropical trees and plants thrive in the area. The Palak lake is major attraction and it is the biggest lake in Mizoram.Nagaland: Nagaland is the one of the smallest as well as largely mountainous state of India. Mount saramati is the highest peak of Naga hills in state. Nagaland is rich among all other states in flora and fauna and in evergreen forests. The Great Indian Horn-bill is one of the most famous birds found in the state. Naga tribes also found in most of the sister states, There are total 15 recognized tribes in the present state of Nagaland.Tripura: Tripura is a landlocked hilly state with many rivers including the Manu River which originates here. Tripura has a rich cultural heritage of music, fine arts, handicrafts and dance. Music is an integral part of the tribal people of Tripura. Tripura is rich in flora and fauna and many species of trees and plants. Dance is an important to the tribal in all states of north east.']	['<urn:uuid:60e13cb8-0a5b-4008-a8bb-745e3259de14>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:15:13.904987	9	57	766
97	Having studied both Chinese and European porcelain traditions, I'd like to understand how the decorative styles and motifs differed between the original Chinese Kraak ware and early Meissen productions of the 1720s?	Chinese Kraak ware and early Meissen productions showed distinct decorative approaches. Kraak ware featured traditional motifs like stylized flowers and Buddhist emblems, with its most characteristic feature being the division into radial panels. In contrast, when Meissen began production in the 1720s, they developed their own extensive range of enamel colors and excelled in chinoiserie scenes - fanciful and imagined depictions of life in an exotic Far East. This difference reflected deeper artistic philosophies: Chinese decorations emphasized traditional symbolism, while European artists had been focused on creating illusion of depth since the Middle Ages. Chinese potters sometimes had to adapt to European tastes, even depicting European figures (often with Oriental features) and attempting Latin inscriptions they didn't fully understand.	"['Carola Hesse, Vienna\nAt the continents emerging from the North Sea, the coastline is nothing constant. The sea and its tide have a major impact on its appearance and expanse. As a child I’ve spent a lot of time at the shores of the North Sea and sometimes the Baltic Sea as well. There was always something new to discover, something new to learn.\nWe have a saying over there: the sea gives, the sea takes.\nMy first real interest in porcelain awoke just there, during one special summer holiday at the Baltic Sea. There I found at least fifty small pottery sherds with all different types of patterns and colours that had been washed ashore. Some where thicker, some thin and delicate. At that tender age I hardly knew anything about pottery really, but it seemed obvious that some sherds were fairly old, and others younger. And also, they were made of different clay materials. Unfortunately, those pieces got lost over time. But here and there I still find single sherds.\nPainted pottery sherds\nThe history of pottery — earthenware in particular — dates back to the Neolithic period. Perhaps even earlier. Porcelain however, was made of a material that was unknown in Europe until the 14th Century. It was the Chinese who invented it.\nThe first ancient ceramic kiln sites in China were located in the Zhejiang Province during the Shang Dynasty (1766-1027 BCE). It became a concentrated area for the production of celadon — an earthenware material with an opague blue-ish, blue-green or jade green plant-ash glazing. From the 9th to the 16th century Yaozhou ware (Northern Celadon) and Longquan celadon ware (Southern Celadon) were supplied to the imperial court, to domestic folk life, and also became important products for export.\nYaozhou ware, with carved and engraved decoration, 10th century.\nWhen it comes to Chinese porcelain, there is one particular town that, by the 14th century, had become the largest centre of porcelain production in the world, and remains so until today. This town is called Jingdezhen.\nOriginally, the ceramic production in Jingdezhen started in the Tang Dynasty (618-907 AD). Later, in the Song Dynasty (960-1279 AD) it witnessed a rapid increase in production. It was Emperor Zhenzong who decreed in 1004 AD, that Jingdezhen should be the production site for all imperial ceramics in the country, and also renamed the town according to his era name (Jingde).\nThe area was particulary rich in natural resources, but the clay soon became depleted through the excessive production during this period. Luckily, around the 10th century (Yung Dynasty), a new type of clay was discovered. Following the same steps in the line of production, but this time with a different locally mined rock (kaolinised granite), it became possible to produce a much purer whiteness. The essential ingredient that made the white material possible was kaolin. It can be found all over the world, but around Jingdezhen, the kaolin variety is particulary fine. And it was because of this high density, that a much harder porcelain could be produced, while at the same time being shiny and translucent. By then, Chinese trade had spread as far as Persia where cobalt was obtained. The use of cobalt blue underglaze eventually led to the recognizable Chinese blue and white style.\nLater on, it was Marco Polo who first brought porcelain from China to Europe. Although, there must have certainly been other merchants who traded with China. And yet, the stories of Marco Polo is the only known historical account. A porcelain object, the so called Marco Polo Jar, a Qingbai Porcelain Jar, might be the only artefact that is actually linked to him. It is suggested that it was brought to Venice in 1295 AD and is housed in the Treasury of San Marco in Venice today.\nQingbai porcelain jar\n(‘Marco Polo jar’)\n(LIN MEICUN, RESEARCHGATE)\nNot long after that, the Portuguese established trade routes to the Far East and began commercial trade with Asia. Porcelain however, was only a small part of the trade. The cargos were full of goods such as tea, spices, silks, or ivory.\nIn 1603 the Dutch East India Company seized a Portuguese merchant ship called Santa Catarina. The Dutch referred to the large amount of stolen blue and white porcelain as Kraakporcelain which was due to the type of the ship — a carrack. Subsequently, it is estimated that more than three Million porcelain pieces were shipped to Europe in little over fifty years — mainly by the Dutch. This Chinese export ware is reffered to as Kraak ware. It was often painted with variations of traditional motifs, such as stylized flowers and Buddhist emblems. The most characteristical decoration was the division into radial panels.\nKraak ware, Jingdezhen\nWhen Europeans first layed eyes on Chinese porcelain, they thought it to be much finer and superior to anything they could fashion — which was at that time stonewares and earthenwares, and those were rather rough in comparison. So the idea that utilitarian objects could also be pieces of art was revolutionary, and this would have a profound influence on our aesthetics. To many however, this was just an opportunity for conspicuous consumption. The English writer Daniel Dafoe even wrote, that chinamania would destroy whole families. Interestingly, there are paralles in this regard to 17th century tulipmania in the Netherlands.\nOver time, the demand for familiar, utilitarian forms from Europe increased. Meanwhile in Beijings Forbidden City, during the reign of Emperor Kangxi (1654-1722 AD), a ware so fine and so exquisit was potted that could make the most export wares look lumpen. The court even had their own colour palette. The imperial yellow glaze for instance was used as early as the Xuande period (1426-1435 AD).\nCup with imperial yellow glazing,\nThere where implications through the trade for both sides — China and Europe. Many of the forms that the Europeans ordered were unknown to the Chinese, candlesticks for instance. Also, the Chinese potters had to decorate the porcelain with images they didn’t understand. Most of them had never even seen Europeans but all of a sudden had to depict their likenesses. This led to Europeans showing Oriental features or wrongly spelled Latin inscriptions on the porcelain. Another aspect was that European artists had tried to create an illusion of depths in their paintings since the Middle Ages whereas traditional symbolism was more important to the Chinese in their decorations.\nIn the 16th century the Europeans still hadn’t discovered the secret of Chinese porcelain, but the race was on. In 1575 a Medici factory had been set up in order to replicate it, but couldn’t reach the Chinese porcelain qualities in their replicas. Then, in 1708, it was Augustus II, Elector of Saxony and King of Poland — or rather his obsession and money — who finally brought the white and translucent porcelain production to Europe. In 1710, he created the Royal Saxon Porcelain Manufactory in Meissen. There, only three people knew the secret formula called arcanum. The secret was so precious, that other Northern European rulers tried everything from bribary to kidnapping to get a hold on it, and eventually succeeded.\nDelft ware, lidded vase, around 1800\n(not officially dated)\nChinese porcelain, decorated only with blue pigments under the glaze, dominated the export trade until the very end of the seventeenth century. Today, Jingdezhen’s traditional technique of producing handmade porcelain has become a part of China’s national intangible cultural heritage.\nNational Commission of the People’s Republic of China for UNESCO:\nAncient Porcelain Kiln Site in China (2013) https://whc.unesco.org/en/tentativelists/5806/ (07.07.2021).\nMeicun, L., and Zhang, R.: A Chinese Porcelain Jar Associated with Marco Polo. A Discussion from an Archaeological Perspective. European Journal of Archaeology, 21(1), 39-56 (2018).\nMunger, Jeffrey, and Alice Cooney Frelinghuysen: East and West: Chinese Export Porcelain. In: Heilbrunn Timeline of Art History. New York: The Metropolitan Museum of Art, 2000. https://www.metmuseum.org/toah/hd/ewpor/hd_ewpor.htm (06.07.2021).\nPeter Lang: The Intriguing History of Porcelain. https://doyle.com/specialists/peter-lang/stories/intriguing-history-porcelain (08.07.2021).\nShanghai Daily: Jingdezhen, the home of porcelain (2017) https://archive.shine.cn/sunday../now-and-then/Jingdezhen-the-home-of-porcelain/shdaily.shtml (06.07.2021).\nTharp, Lars: Treasures of Chinese Porcelain. BBC Four Documentary (2011).\nVainker, Shelag: Chinese Pottery and Porcelain: From Prehistory to the Present. London: British Museum Press (1991).\nWikipedia: Santa Catarina. https://en.wikipedia.org/wiki/Santa_Catarina_(ship) (08.07.2021).\nWikipedia: Kraak ware. https://en.wikipedia.org/wiki/Kraak_ware (08.07.2021).\nWood, Nigel: Chinese Glazes: Their Origins, Chemistry, and Recreation. Philadelphia: University of Pennsylvania Press (1999).', 'Chinese and Japanese porcelains were highly esteemed in seventeenth-century Europe, and although they were imported in ever-increasing quantities throughout the century, Europeans did not know the ingredients necessary for the production of true porcelain, commonly known as hard-paste porcelain. However, in 1709 an alchemist named Johann Friedrich Böttger discovered the materials required to produce a white, translucent, high-fired porcelain body, and this discovery was to have profound consequences for the entire European ceramic industry.\nBöttger\'s experiments with the formula for porcelain included the development of a high-fired red stoneware, which led to several technological advances ultimately resulting in porcelain. This so-called Böttger stoneware was used both for wares and for figures, including one depicting Augustus the Strong, elector of Saxony (1982.60.318). Augustus had provided the impetus for Böttger\'s experiments and, based on the latter\'s success, he established a factory at Meissen, just outside Dresden, in 1710. The factory was soon producing a creamy white porcelain now known as Böttger porcelain (42.205.26), becoming the first European manufactory of hard-paste porcelain. In the 1720s, the Meissen factory developed a new and extensive range of enamel colors, and the factory\'s painters excelled in chinoiserie scenes. Fanciful depictions of life in an imagined and exotic Far East, chinoiseries were the most popular type of decoration during this decade (1974.356.488). One of the great achievements of the Meissen factory was the production of large-scale vases, which were difficult to fire successfully because of their size. Many of these vases were decorated with ground colors in imitation of Chinese porcelains, and chinoiserie scenes continued to be in vogue through the 1730s (1974.356.363). Frequently these vases are marked on the underside with the initials ARfor Augustus Rexand it is likely that these objects were intended for Augustus\' use or to be given as royal gifts.\nThe most ambitious of all of the projects undertaken at Meissen for Augustus the Strong was the production of large-scale animals for the Japanese Palace, the building intended to house all of his porcelain collections. The size of these animals presented enormous technical difficulties, and even though the factory used a more resilient hard-paste porcelain body for the animals, many of them display fire cracks and other evidence of problems encountered during firing. Nevertheless, they represent a remarkable achievement and remain among the most significant of all porcelain sculpture (1988.294.1).\nThe second factory in Europe to produce hard-paste porcelain was that founded in Vienna by Claudius du Paquier (died 1751). Du Paquier was assisted by the former kiln master from Meissen, and the new enterprise was able to make hard paste by about 1719. The du Paquier porcelain body was quite similar to that produced at Meissen, but the forms and styles of decoration employed at the Viennese factory were entirely original. The decoration on one du Paquier flower vase seems to depict du Paquier himself, seated near a group of porcelain objects, and the painted inscription calls attention to Vienna\'s accomplishment in producing porcelain (54.147.94). The concept of a dinner service with matching components was still novel in the 1730s, and the first dinner service to be made in porcelain was ordered from Meissen in late 1731. Du Paquier produced a partial dinner service, possibly composed exclusively of tureens, as early as the mid-1720s, and in the years 173640 made another servicecomposed primarily of tureens and wine coolersthat Emperor Charles VI gave to the Russian empress Anna Ioannovna (1982.60.330).\nThe success of the Meissen and du Paquier firms led to the establishment of other porcelain factories in Germany in the 1740s and 50s, and it was often workers coming from either Meissen or Vienna who provided the necessary technical expertise to the new operations. The factory established at Höchst in the mid-1740s began production on a viable scale only after the arrival of J. J. Ringler from the du Paquier factory. The Höchst factory employed a number of highly competent modelers, and the factory excelled in the production of figures. One of the most ambitious of these is The Audience of the Chinese Emperor (50.211.217), which was modeled by Johann Peter Melchior, director of the sculpture workshop beginning in 1767. This group probably decorated the table during the dessert course, accompanied by additional single figures also in a chinoiserie style.\nPerhaps the most popular subjects for figures and groups were characters from the Italian commedia dell\'arte. Porcelain figures depicting Harlequin, Columbine, Mezzetin, Isabella, and numerous others would have been instantly identifiable because of their costumes, for the traveling troupes of Italian commedia players had made these characters very familiar to eighteenth-century audiences. One of the greatest modelers of these figures was Franz Anton Bustelli, who worked at the Nymphenburg factory from 1754 until 1763. His figures (1974.356.524,.525) always display a pronounced elegance and a slight exaggeration of pose that simultaneously reflects some essential aspect of the character\'s personality.\nMunger, Jeffrey. ""German and Austrian Porcelain in the Eighteenth Century"". In Heilbrunn Timeline of Art History. New York: The Metropolitan Museum of Art, 2000–. http://www.metmuseum.org/toah/hd/porg/hd_porg.htm (October 2003)\nRelated exhibitions and online features\nThese related Museum Bulletin or Journal articles may or may not represent the most current scholarship.']"	['<urn:uuid:4c02d2a1-a71e-4d5d-8d36-9271d8f94027>', '<urn:uuid:3be421fb-4fd6-4e89-8e2e-c5109041ba1a>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	32	119	2241
98	water distribution body regulation hormones mechanisms	The body's water distribution and regulation involves several mechanisms. About 60% of an adult male's body weight is water, with two-thirds inside cells (intracellular fluid) and one-third outside cells (extracellular fluid). The regulation of water balance is primarily controlled by ADH (Antidiuretic Hormone), which is created in supraoptic and paraventricular nuclei and released from the posterior pituitary. ADH is released when plasma osmolality reaches 280-290, during volume changes, stress, nausea, and pregnancy. It acts on the collecting tubule to increase water reabsorption, thereby reducing urine output and increasing blood volume and pressure.	['Regulatory Functions of the Kidneys\nThe kidneys are involved in maintaining the fluid and electrolyte balance, and also the acid-base balance, of the blood. If the kidneys fail to carry out these vital functions, either hemodialysis or a kidney transplant is needed.\nFluid and Electrolyte Balance\nThe average adult male body is about 60% water by weight. The average adult female body is only about 50% water by weight because females generally have more subcutaneous adipose tissue, which contains less water. About two-thirds of this water is inside the cells (called intracellular fluid), and the rest is largely distributed in the plasma, tissue fluid, and lymph (called extracellular fluid). Water is also present in such fluids as cerebrospinal fluid and synovial fluid; in Figure 16.6, these fluids are referred to as “other” fluids. For body fluids to be normal, it is necessary for the body to be in fluid balance. The total water intake should equal the total water loss. Table 16.2 shows how water enters the body-namely, in liquids we drink, in foods we eat, and as a by-product of metabolism. We drink water when the osmolarity of the blood rises as determined by the hypothalamus. Table 16.2 also shows how water exits the body-namely, in urine, sweat, exhaled air, and feces. Similar to the gain and loss of water, the body also gains and loses electrolytes. Despite these changes, the kidneys keep the fluid and electrolyte balance of the blood within normal limits. In this way, they also maintain the blood volume and blood pressure.\nFigure 16.6 Location of fluids in the body. Most of the body’s water is inside cells (intracellular fluid), and only about one-third is located outside cells (extracellular fluid).\nReabsorption of Water\nBecause of the process of osmosis, the reabsorption of salt (NaCl) automatically leads to the reabsorption of water until the osmolarity is the same on both sides of a plasma membrane. Most of the salt, and therefore water, present in the filtrate are reabsorbed across the plasma membranes of the cells lining the proximal convoluted tubule. But the amount of salt and water reabsorbed is not sufficient to result in a hypertonic urine-one in which the osmolarity is higher than that of blood. How is it, then, that humans produce a hypertonic urine? We now know that the excretion of a hypertonic urine is dependent upon the reabsorption of water from the loop of the nephron and the collecting duct.\nLoop of the Nephron and Collecting Duct\nA loop of the nephron has a descending limb and an ascending limb. A long loop of the nephron penetrates deep into the renal medulla. In the ascending limb, salt (NaCl) passively diffuses out of the lower portion and is actively transported out of the upper portion into the tissue of the outer renal medulla (Fig. 16.7). Less and less salt is available for active transport as fluid moves up the thick portion of the ascending limb. Therefore, the concentration of salt is greater in the inner medulla than in the outer medulla. (It is important to realize that water cannot leave the ascending limb because the ascending limb is impermeable to water).\nThe large arrow to the side in Figure 16.7 indicates that the lowest portion of the inner medulla has the highest concentration of solutes. You can see that this is due not to the presence of salt, but to the presence of urea. Urea is believed to leak from the lower portion of the collecting duct, and it is this molecule that contributes to the high solute concentration of the lowest portion of the inner medulla. Because of the osmotic gradient within the renal medulla, water leaves the descending limb along its entire length. There is a higher concentration of water at the top of the descending limb, and so it takes a lesser amount of solute in the medulla to pull it out. The remaining fluid within the descending limb encounters an even greater osmotic concentration of solute as it moves along; therefore, water continues to leave the descending limb from the top to the bottom. Such a mechanism is called a countercurrent mechanism. At the top of the ascending limb, any remaining water enters the collecting duct. Surprisingly, the fluid inside the nephron is still not hypertonic-the net effect of reabsorption of salt and water so far is the production of a fluid that has the same tonicity as blood plasma. However, the collecting duct also encounters the same osmotic gradient as did the descending limb of the loop of the nephron (Fig. 16.7). Therefore, water diffuses out of the collecting duct into the renal medulla, and the urine within the collecting duct becomes hypertonic to blood plasma.\nAntidiuretic Hormone (ADH)\nADH released by the posterior lobe of the pituitary plays a role in water reabsorption at the collecting duct. In order to understand the action of this hormone, consider its name. Diuresis means flow of urine, and antidiuresis means against a flow of urine. When ADH is present, more water is reabsorbed (blood volume and pressure rise), and a decreased amount of urine results.\nFigure 16.7 Reabsorption of water at the loop of the nephron and the collecting duct. A hypertonic environment in the tissues of the medulla of a kidney draws water out of the descending limb and the collecting duct. This water is returned to the cardiovascular system. (The thick black line means the ascending limb is impermeable to water).\nIn practical terms, if an individual does not drink much water on a certain day, the posterior lobe of the pituitary releases ADH, causing more water to be reabsorbed and less urine to form. On the other hand, if an individual drinks a large amount of water and does not perspire much, ADH is not released. In that case, more water is excreted, and more urine forms.', 'Physiology of water balance • Almost all dysnatremias are a result of water balance • TBW = 60% total body weight • 2/3 ECF compartment with water that is freely diffusable b/w ICF and ECF to maintain identical osmolality • So, serum osmolality measures all compartments, i.e. -- when total body water is elevated, osmolality falls • ADH – governs the excretion of water by the kidney, which prevents dilution of urine when present\nOsmolality v. Tonicity • Osmolality: total number of particles in an aqueous solution (mosmol/kg H2O) • Normal Posm = 275-290 mosmol/kg • Effective osmolality (tonicity): particles that can exert osmotic force across membranes, via movement of water into or out of cells • Na+, glucose and BUN are major determinants of plasma osmolality • Posm = 2 x plasma [Na+] + [Glucose]/18 + [BUN]/2.8 • Effective osmoles (Na+ , glucose) exert water shifts unlike urea, ethanol\nWhat factors regulate the ECF? • There are multiple sensors (afferent limb that monitors EABV – amount of arterial blood that fills the arterial circulation) • Baroreceptors • Low pressure baroreceptors in venous beds that monitor for volume overload – atria, pulmonary vascular bed • High pressure barocreceptors (carotids) • JG apparatus (located at the thick ascending limb of the Loop of Henle to regulate afferent arteriole) – renin release • Efferent limb – primarily renal • Regulates rate of sodium excretion, which is most significant in the tubules • RAAS system – Ang II • ADH – released by posterior pituitary with 1% change in blood osm • Catecholamine release – directly stimulates sodium reabsorption at PT and LOH\nADH • ADH is created in supraoptic and paraventricular nuclei • Released as granules travels to posterior pituitary gland where it is released as osmolality increases • Site of action is collecting tubule • Regulated by plasma osmolality (released at 280-290), volume, stress, nausea, pregnancy\nApproach to hyponatremia • History and physical often gives clue to etiology of hyponatremia -- so take a good history 1. Check serum osm to make sure – hypotonic hyponatremia – total body water issue 2. Determine volume status • Hypovolemic • ↓ [Na+] = ↓↓TBNa/↓TBW • Euvolemic • ↓ [Na+] = ↔ TBNa/↑TBW • Hypervolemic • ↓ [Na+] = ↑TBNa/↑↑TBW\nApproach to hyponatremia 3. Is ADH response appropriate? Is H2O excretion normal or impaired • Uosm < 100 mosmol/kg indicates that ADH is appropriately suppressed • Primary polydipsia • Reset osmostat (when Posm is below normal) • Low solute intake • Uosm > 100 mosmol/kg occurs in majority of hyponatremic patients and indicates impaired H2O excretion\nHypervolemic Hyponatremia • Increased water much greater than sodium • Usually states of decreased effective circulating volume • CHF, nephrotic syndrome, cirrhosis, renal failure • What is urine sodium in these cases?\nHypovolemic Hyponatremia • Total body sodium loss > total body water loss • Determine if sodium loss is renal or extrarenal • U[Na] < 10 indicated extrarenal loss (Vomiting, diarrhea, third spacing) • U[Na] >20 renal losses (diuretics, mineralocorticoid deficiency, salt-losing nephritis, osmotic diuresis, bicarbonaturia, ketonuria\nClinical manifestations • Depends on rapidity of fall which does not allow for cerebral adaptation • Symptoms typically occurs below 125 mmol/L (if from previously normal levels) • Mostly neurologic symptoms • Nausea, headache, lethargy, ataxia, psychosis, seizures, coma\nSymptomaticHyponatermia • Key points: • 3% saline only if seizures or other neurologic manifestations (can consider faster infusion) • Concomitant lasix helps to eliminate free water • Frequent labs absolutely necessary • What is the risk of too rapid correction?\nCase examples • An 82 y/o woman is admitted from a nursing home with increasing lethargy and confusion. She has a baseline dementia, but is normally animated and interactive with family and staff. She has had a poor appetite over the past year with significant weight loss, and currently eats very little. Two weeks ago HCTZ was added to her medications. Over the past few days, the nurses note some n/v, no diarrhea, fever or other complaints. On exam, she has some dry oral mucosa but she is not orthostatic. There is no evidence of CHF, ascites or edema. She is awake, but lethargic. Neuro exam is nonfocal. Labs: Na 121 (last 130 4 weeks ago), normal renal/liver function. Serum osm 200, urine osm 220, urine Na 30.\nDifferential for hyponatremia • Volume depletion from n/v • Thiazide diuretic – how does this cause hyponatremia? • Tea and toast • SIADH\nHow would you treat this patient? • Stop diuretic +/- IVF (given ?SIADH) • If you gave patient 1L NS and she had SIADH, what would you expect to happen?']	['<urn:uuid:2e4c3c10-f7c4-436a-bc17-17164e1b0f52>', '<urn:uuid:0e08c056-d972-4f6f-ace5-94f43d8852b5>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:15:13.904987	6	92	1750
99	What are the key differences between Mini-ITX and Micro-ATX cases in terms of size and convenience, and how do their power supply options compare?	Mini-ITX cases are more compact and portable, with dimensions like the NZXT H1's 388mm height and 188mm width, making them easier to blend with room decor. However, they are more selective with power supplies, often requiring specialized SFX PSUs that are less common and more expensive than regular ATX power supplies. In contrast, Micro-ATX cases are larger but more accommodating, able to fit regular-sized PSUs and offering better accessibility to components like fan mounts, drive cages, and motherboard connectors.	['Miniature cases are extremely rare guests in the reviews of our portal. And they occupy a very narrow niche in the market. This is due to the complexity and high cost of selecting components for systems of this size. And with the fact that a small case limits the cooling, and hence the permissible power of the components, not to mention their dimensions. Although a lot of things have already been standardized in this area (not like eight years ago), in practice, building a small PC brings a lot of inconvenience. First, try to find the right motherboard. It will either be very expensive, or it will be cut down to the point of impossibility. Then pick up the body. The smaller it is, the more difficult it will be to choose everything else. For example cooler. Or power supply SFX or TFX format. And if there is also a limit on the size of the video card. And if the silence of the system is also required …\nWith its new H1 case, NZXT has decided to make life easier for fans of miniature and powerful PCs. The fact is that not only a decent power supply unit with a ten-year manufacturer’s warranty is immediately integrated into the chassis, but also a 140 mm LSS and an extension adapter for installing a video adapter separately from the motherboard. The result is a nice and compact, albeit expensive, harvester. I propose to consider all its features in more detail.\n|Type of shell||Mini-Tower|\n|Dimensions, mm||388 (H) x 188 (W) x 188 (D)|\n|Material||Steel, tempered glass|\n|Devices 3.5″ external||–|\n|Devices 3.5″ / 2.5″ internal||– / 2|\n|Supported number of expansion slots||2|\n|Interface connectors||1 x USB 3.1 Gen 1 Type-A, 1 x USB 3.1 Gen 2 Type-C, 1 x 3.5mm combo headset jack|\n|Other||Tempered glass front panel, two easy-to-remove side dust filters, built-in NZXT NP-S650M 80Plus Gold 650-watt SFX-L modular power supply, built-in AIO-FSS system with 140mm radiator and fan, built-in high-speed riser cable for PCI-e 16x Gen3 graphics cards, cable management mounts|\n|average cost||UAH 12,999|\nPacking and scope of delivery\nThe case comes in typical NZXT purple and white packaging. It has a small size and is characterized by an extreme degree of information content. Two massive polyurethane foam dampers form a thick inner box that reliably protects the chassis from damage during transit.\nThe parts kit is contained in a separate elongated brown box stowed in the graphics card compartment. There is:\n- instructions for assembling the system;\n- a dozen disposable plastic cable ties;\n- standard power cable for connecting the power supply to the outlet;\n- plastic reinforcing plate for mounting the LSS water block;\n- four screws for the motherboard;\n- latch bar with screw;\n- adapter for separate connection of front panel wires;\n- bracket for attaching the water block to AMD sockets;\n- two eyelets with screws for the AMD bracket;\n- four spring-loaded nuts;\n- four plastic racks;\n- five plastic retaining rings;\n- four small fixing screws;\n- four screw studs;\n- 3.5mm splitter from combo jack to two separate headphone and microphone inputs.\nLike many other cases from NZXT, the H1 chassis looks very concise. It has a perfectly rectangular shape of a slightly elongated parallelepiped. In the color scheme of the model, there are only two options — completely black and black with white, where only two side and one upper face will be white. The size of the base is 188×188 mm, and the height does not exceed 388 mm. It turns out a compact design with a volume slightly less than 14 liters. The front panel is made in its outer part of tempered and heavily tinted glass. You can’t see much through it, and the only noteworthy thing is the NZXT logo at the bottom. A more practical solution would be to put a grate here.\nThe top panel is metal and square. It is a monolithic part with a left and right panel. External ports and buttons are on the front right. There is a combined 3.5 mm headset jack, a USB 3.1 Type-C port, a regular purple USB 3.1 Type-A and a round system start button with a white power and drive activity indicator built around the perimeter.\nThe front panel is easily snapped off by pulling on the cutout from below, as it is held only by four ball-shaped fasteners and two pins at the top. A mounting frame is glued to its reverse side.\nThe left panel is metal and covered with abundant perforations with round air vents.\nThe right panel is similar to the left as two drops of water. There are plenty of ventilation holes here too.\nThe rear panel is also slightly different. It is almost entirely occupied by a ventilation grill, and below there is an arched opening for the wires to be brought out. At the bottom right there is an extension cord connector for connecting the power cable.\nThe rear panel can be removed just as easily as the front panel, since it has a similar mounting design.\nThe bottom panel conditionally consists of three parts. On the left side (in the photo — on top) there is an exit and a video card mount. In the center is the exit for the rear panel of the motherboard. On the right (in the photo — below), the fillet part of the radiator of the LSS system is visible through the hole. In its plane lies an extension cord from the power connector of the power supply.\nThere are four rubber feet on the corners. The margin of space to the motherboard mounting plane is 70 mm. This should be enough to connect the wires.\nTo remove the side panels, you must first detach the front and rear panels. And then pull the sidewalls and the top panel up the plastic guides.\nThis is what the case looks like without all the side plates.\nOn the reverse side of the side panels, dust filters are fixed on magnets.\nThe filters are made of a fine-mesh mesh glued onto a plastic frame.\nAnd now let’s see how the chassis is arranged inside.', 'Any avid gamer knows that fewer things evolve and develop faster than the average desktop personal computer. But amidst all this, one thing remains constant – the miniaturization of the core processing units. It was just a few decades ago when computers used to take up humongous spaces, at times even as big as an entire room. Fast forward, and currently, you can easily land a fully-fledged high-powered gaming PC as small as your briefcase. You might be wondering how such a small-sized PC can pack such mind-blowing graphics processing speed. The answer lies in the computer cases and motherboard which houses all the main components of a PC including the CPU, Memory, and GPU. These gaming PCs typically utilize smaller sized cases and motherboards that are below a certain size threshold, also known as small form-factor processing motherboards. Here, we will be comparing mini ITX vs micro ATX cases so as to fully understand their strengths and shortfalls.\nSo speaking of small computer cases, between the top two contenders, i.e., Micro-ATX vs Mini-ITX, which one trumps the other one as far as building a holistic gaming machine goes? Let’s see.\nMini ITX vs Micro ATX Compared\n1. The Costs of Mini ITX vs Micro ATX\nAll other factors held constant; a Mini-ITX system would theoretically cost less than a Micro-ATX system. And this is mainly because the mini-ITX employs a smaller case and motherboard in comparison to the average micro-ATX GPU processor. This will then translate to less raw materials, lower labor input costs, and less intensive manufacturing infrastructure. What’s more, mini-ITXs don’t use any exotic components in their assembly, so their manufacture is not in any way difficult. In other words, they ought to be cheaper than the ATX cases.\nRecommended: Best GTX 970 Gaming Graphic Cards Reviewed\nThe difference comes in after the introduction of other separate but related parts such as the motherboard’s power supply. As you may already know, Some of the best Mini-ITX cases such as this Thermaltake Core V1 Mini ITX cube computer case comes with systems utilizes a unique PSU processing form factor known as the SFX. And these PSUs are not as common as the regular ATX chip units. Consequently, the SFX PSU is pricier than the ATX’s power supply system for nearly the same build quality and output. Hence, it is very hard to bump on a Mini-ITX case for less than 50 US, especially if your target is extras such as ‘gold’ ratings or modular cables.\n2. The Performance of Mini ATX vs Micro ITX\nAside from cost, the performance is the next big thing that gaming machine builders consider. After all, no one wants to end up with a laggy machine that can barely handle the graphic intensive games that currently dominate our gaming stratosphere.\nBenchmark readings alone, Mini-ITX motherboards have the upper hand here. Here’s why.\nGround level Mini-ITX boards such as this GIGABYTE GA-Z270N-Gaming 5 LGA1151 Intel Z270 Mini-ITX DDR4 Motherboard can effortlessly overclock a full-sized ‘premium’ ATX motherboard. And this is thanks to the design that puts the extra voltage-regulation processing modules on a pre-mounted riser card. Therefore, the board’s form factor does not leave much room for the addition of any bonus power phases.\nThat being said, if you plan on running more than a single video card concurrently, then only a micro-ATX board is big enough to accommodate such. The larger case that the board boast of, also implies that there’s plenty of room to liquid-cool the system during high overclocks.\nOn the basis of performance alone, the Micro-ATX takes it hands down. However, factoring in sheer practicality when it comes down to handling massive graphics-intensive games, the Mini-ITX is the most suitable.\n3. Availability and Shopping Options\nStrictly genealogically speaking, Micro-ATX cases and boards have been around for a longer time than its newer and younger counterpart. This then implies that the engineering technology that it employs is by far more mature, seasoned, and developed. In addition to this, there’s also a broader range of more developed boards among the crop of Micro-ATX boards. As a consequence, you can fine-tune and adjust your operating budget in accordance with your individual preference and affordability. On the other hand, there’s no such flexibility when dealing with mini-ITX boards.\nStill on this, Micro-ATX boards suffer from a notable lack of variety, a majority if not all, are designed for the ATX hardware infrastructure. If you’re looking for something a little more compatible with different manufacturers, developers, and enterprising modders, you may want to go the mini-ITX way.\n4. Ergonomics of Micro-ATX vs Mini-ITX\nMini-ITX systems have the ill reputation of being more selective to power supply systems comfortably. Mostly, this is brought about by the less space clearance that these small boxes have. A typical example is the Cooler Master 130 Elite that is capable of accommodating a regular-sized average regular-sized PSU but not without the unit sticking out awkwardly by an inch or two.\nOn the side of the spectrum, a micro-ATX system such as the Corsair Obsidian Series 350D Performance Micro ATX can accommodate almost anything that is sensibly-sized. Don’t forget that you still have to factor in the accessibility of the system’s fan mounts, drive cages, and, most importantly, the motherboard connectors. This will help you avoid building a ‘shoebox’ tower where you have to re-assemble the unit each time you want to fiddle with just one or two internal components.\nAll said and done; the Mini-ATX system is undeniably less bulky, lighter, smaller, easier to carry around and more compact. Besides, it can easily blend in with an existing room decor or home theater unit.\n5. Ease of Upgrading a Mini ITX vs Micro ATX\nAny builder or serious gamer fancies expandability and ease of improvement, that’s why most choose to build/customize their own systems in the first place. All Mini-ITX boards have only one pair of RAM slots. This is a far cry compared to the four slots that the micro-ATX is known for, which translates to more options and ease of upgrading in the future. And it does end there. Mini-ITX towers also have just a single card slot that is reserved for the GPU. That implies that it is not possible to add peripherals such as sound cards, wireless routers, USB 3.0 ports, adapters, or that SATA Express that is on the lips of every gamer.\nThere’s no denying that these two are the cream of the best small standalone cases in the gaming realm. If you’re more interested in performance, the limited space of a mini-ITX tower will surely get in the way. But if you want an HTPC, then the mini-ITX is evidently the best pick. Nonetheless, the trophy of “Ease of Upgrading Winner” goes to the Micro-ATX which can accommodate more extras among the two top options. Choose wisely, won’t you?']	['<urn:uuid:1ab910f8-44bc-4a40-8929-86a7a4edc18c>', '<urn:uuid:f6a4c9f2-a117-435d-b64a-7bf6b2969eb5>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:15:13.904987	24	79	2177
100	What challenges do mining companies face with indigenous communities in Chile versus Yukon?	In Chile, Barrick Gold's Pascua Lama project failed despite extensive community meetings and investments because they didn't sufficiently engage disenfranchised indigenous communities, presumed government support equated to community support, and faced concerns about water contamination and cultural harm. In Yukon, the challenges are addressed through a more structured approach - there are established Modern treaties with most First Nations, formal consultation processes, and successful examples of cooperation agreements between mining companies and First Nations. Yukon's system recognizes First Nations as active partners in resource stewardship and economic development, with defined rights and responsibilities.	"[""In an effort to meet sustainability challenges companies are striving to live up to stakeholder expectations.\nIt is now widely accepted that the way in which organisations, particularly mining and industry businesses, identify and work with communities and interest groups can be the difference between success and failure. But, while good intentions and deep pockets can open doors, even well-thought out stakeholder strategies can go disastrously wrong.\nAs academics from different fields we recently looked, through our respective lenses, at a case study of Barrick Gold’s aborted Pascua Lama development, an US$8.5 billion mining project that was suspended after failing to gain a social license to operate.\nLaunched in 1983 as a socially-aware alternative to the apartheid-backed South Africa gold miners, Barrick Gold is recognised as a leading-edge practitioner when it comes to social responsibility having operated in some very difficult working environments. But the Pascua Lama project, straddling the Chile-Argentina border, presented unprecedented environmental and social challenges.\nBarrick Gold entered the project well aware of the need to create a map of stakeholders which would identify the various, often competing, interest groups and find ways to manage expectations. Indeed it held over 1,000 meetings with community leaders and over 100 open houses to respond to community concerns.\nGold mining by its very nature is a dirty and dangerous business and often takes place in fragile ecosystems, posing environmental and health and safety issues. In Pascua Lama there were specific issues related to the mines geography - 5000m above sea level - and the potential contamination of the water from surrounding glaciers which supplies communities across the valley.\nThere were concerns over the social impact it would have, the harm to the cultural traditions of the local indigenous communities and questions around who the mine would benefit and whether any of the revenue from the gold would find its way back to the local people.\nIn its effort to win community support, Barrick Gold spearheaded health and education initiatives, built houses for victims of Chile’s devastating 2010 earthquake, and formed a partnership with the Chilean government creating a fund to bring US$60 million in water improvements over the life of the mine.\nDespite this engagement, significant protests against the project escalated into legal action, resulting in the suspension of construction and operations. Today the mine is shuttered with ongoing care and maintenance costs of over US$100 million a year as legal wrangling continues. Barrick Gold founder Peter Munk stepped down as chairman in April 2014, leaving the board and the company he had created three decades earlier, while the stock’s price slipped from a high of $60 in 2011 to around $10 today.\nWhen social problems become business problems\nCraig Smith, INSEAD Professor of Ethics and Social responsibility, wrote the case study to highlight the way in which social and environmental problems are increasingly becoming business problems. If companies, particularly extraction businesses, expect to continue operating they need to address these complications and win community support. According to Craig, Barrick Gold knew the importance of getting this social licence to operate but made three principal mistakes.\n* First it thought in having the support of the government it had the support of the community – this was not the case.\n* Secondly, while it undertook relatively extensive stakeholder engagement, it didn’t sufficiently engage with certain key stakeholders – especially the disenfranchised members of the indigenous community, who were less likely to be beneficiaries of the mine’s development.\n* And thirdly, its engagement lacked authenticity. At no point did the company ever ask the people if they wanted the mine. It went into the community presuming the mine would be going ahead.\nAs well as failing to understand all key stakeholders, the gold miner failed to understand the value these stakeholders believed they would get from the project. The question of what value means for stakeholders is not necessarily something that is easily pinned down. It is often subjective and influenced by various biases of judgments and decision-making.\nAs a Professor of Negotiations at INSEAD, Horacio Falcao read the case with a very different perspective. The problems, according to Horacio, also stemmed from the way the engagement was addressed and the early unilateral concessions which were made without asking for commitments in return.\nOne of Barrick Gold’s mistakes was in approaching the stakeholders in individual blocks, as if each block were a self-contained set of interests that needed to be satisfied. When negotiating with multiple organisations and communities this is rarely the case.\nWhile the miner may have invested a lot of time trying to reach out (and probably thought that they were truly trying to understand stakeholder grievances) not enough effort was spent putting these concerns together or listening to thoughts which diverged from their own assumptions. In cases where stakeholders are connected in a single ecosystem, it is important to get all parties into one room and, as a community, try to understand each other to work towards a mutually beneficial scenario. By only negotiating one-on-one, as Barrick Gold did, the parties found themselves in competition, each seeking to get the better deal.\nWhile one-on-one negotiations help pave the way for multiparty discussions, they should not be the only way to go. Conversely, bringing different parties into a one room without knowing them individually can be quite dangerous.\nFollowing a process\nMulti-party negotiation for sustainability is about engaging and collaborating as a process, whereby parties work together to find an outcome that presents the greatest benefits and where stakeholders understand there is just so much money which can be put forward until the project stops being feasible.\nBut this type of collaborative negotiation only works when there is true listening, legitimate arguments and when people believe their concerns will be genuinely addressed. This requires trust. As well intentioned as Barrick Gold may have been, there was (as mentioned above) an authenticity problem. It did not really listen. The company took a position early on and entered into negotiations looking for ways to get social acceptance for that position, in many instances by offering early unilateral concessions. A better result could have been reached by creating engagement where everyone understood that only at the end of the process - after possibilities had been explored, obstacles understood and solutions indigenous to this particular community and environment, identified - would anything be given.\nMissing the bigger picture\nJean Francois Manzoni, INSEAD Professor of Management Practice, added three more angles to this analysis:\nFirst there was what he refers to as the “knowing-doing gap”. Barrick Gold was well aware of its corporate social responsibility, and the need to gain a social licence to operate. It had won awards for its corporate social responsibility in the past yet it failed in this case. Why this happened can be answered in part by the second issue, ethical blinders.\nRecent research shows people, particularly people who have a history of doing the right thing, don’t always notice when they are about to transgress their own ethical norms. In fact the more people believe (or are led to believe) that they are objective and ethically sound, the more they are likely to transgress. And when incentives are in play (and in the case of Pascua Lama with its estimated resource of 17 million ounces of gold and 635 million ounces of silver, there were very large incentives to see the project through) it can be very difficult for people involved to de-centre and see things from another point of view.\nThe third point is the “set up to fail syndrome” and negative labelling. Research shows that once we start labelling people negatively, these labels have a strong propensity to become self-reinforcing and self-fulfilling; parties tend to live down to expectations. In this case, Barrack Gold probably began to see the small, dissenting communities, not as poor, proud, indigenous people with legitimate concerns for their traditions and the environment, but as unreasonable leeches trying to extract extra money from the coffers. On the other side of the fence, it is likely the dissenting communities started labelling Barrick Gold as a profit maximiser uninterested in their present and future. Sadly, but predictably, both parties ended up living down to their adversary’s label and expectations.\nIn today’s world it is not enough to get regulatory approval and government support to ensure a project’s success. In fact recent research of 26 gold mines owned by different companies found that mines with better stakeholder engagement reported higher financial returns. And while there is no stakeholder management strategy that will sway every nay-sayer to favour a particular development, there are ways to map out and implement strategy to advance an organisation’s progress.\nAbove all the story of Barrick Gold’s Pascua Lama project shows the importance of effective, authentic stakeholder engagement.\nThis article is taken from a panel discussion at the 32nd INSEAD Sustainability Executive Roundtable, held in Fontainebleau in June.\nN.Craig Smith is the INSEAD Chair in Ethics and Social Responsibility, the Programme Director of INSEAD's Healthcare Compliance Implementation Leadership Programme and a specialist at the INSEAD Corporate Governance Initiative\nHoracio Falcão is a Senior Affiliate Professor of Decision Sciences at INSEAD.\nJean-François Manzoni is Professor of Management Practice at INSEAD, where he holds the Shell Chair in Human Resources and Organisational Development. He is the founder and director of the LEAP programme (for Leadership Excellence through Awareness and Practice), and co-directs INSEAD’s International Directors Programme."", 'Yukon First Nations Engagement and Consultation Guidebook is live Online\nThe purpose of the Yukon First Nations Engagement & Consultation Tool Website and Mobile App for the Mineral Exploration Industry (the Guide) is to provide relevant and timely information to mineral explorers and those who work with the industry on engagement and consultation requirements and expectations in Yukon. The Guide can be accessed through hard copy, and a web-based version, which is also available through an app. Although the Guide is primarily focused on information to mineral explorers, and will speak to that industry specifically, it will also be useful for those working with the industry such as investors, academic institutions and government organizations (public and First Nation).\nThe hard copy will give the reader best practices guidance and engagement and consultation information that is relatively constant. The web-based version can be regularly updated and provides the reader with additional temporal information such as the names of elected officials. The app is updated each time a user is connected to the Internet and the last update continues to be available when offline. The web version and app meet the challenges of both currency and accessibility.\nThis Guide, whether you are using the hard copy or the website, offers the following information.\nYUKON IN THE CANADIAN CONTEXT\nThere are unique aspects of engagement and consultation in Yukon that are important for the industry to know. Understanding the history of the regulatory environment in the territory and how public and First Nation governments play an active and shared role, will help to situate best practices that are identified later in the Guide.\nMINING IN YUKON\nYukon First Nations ancestors have lived in the territory since time immemorial. In the early years, before the Yukon Gold Rush in 1898, the relationship between First Nations and European explorers, fur traders and a few settlers was, generally, respectful and mutually beneficial. The influx of 40,000 gold seekers to the Dawson City area marked a turning point, leading to Ta’an Chief Jim Boss sending the Government of Canada a letter in 1902 asking the federal government to protect Yukon Indians’ way of life.\nINDIGENOUS GROUPS IN YUKON\nTraditionally First Nations see themselves as the protectors and stewards of land and resources for current and future generations. Exploration and mining activities throughout Yukon during the 20th century affected the traditional way of life of Yukon First Nations. Although not the only reason, this contributed to First Nations’ negotiation of the Modern treaties beginning with the Inuvialuit in 1984 including their interests along the North Slope of Yukon and the Gwich’in Tribal Council with interests in north central Yukon in 1992. Yukon First Nations negotiated the Umbrella Final Agreement (UFA), which was signed in 1993. The UFA created a foundation for the successful negotiation of eleven First Nation Final Agreements (also referred to in the Guide as “Modern treaties”). Each Yukon First Nation Final Agreement is composed of the UFA with boxes that identify clauses that customize the Agreement to each First Nation.\nThe UFA and Final Agreements affirmed Settlement Land and created or affirmed assessment and regulatory bodies with First Nation, Federal and Yukon government representation that apply throughout Yukon. Three Yukon First Nations did not reach settlement agreements. The regulatory and assessment processes created by the UFA, however, apply to lands in the non-treaty traditional areas of those three First Nations.\nFirst Nations play an active part in land and natural resources stewardship, as well as economic development activities in Yukon. The eleven Modern treaty Yukon First Nations have traded off Aboriginal Title over larger areas of land in their Traditional Territories for the right to be partners in such matters as environmental assessment (UFA Chapter 12), water management (UFA Chapter 14), Heritage (UFA Chapter 13) and Land Use Planning (UFA Chapter 11), to name a few. The bodies created or affirmed through these chapters of the Modern treaties apply to the Settlement Lands owned by Yukon First Nations, as well as all other Yukon lands and resources, including federal, Yukon, and municipal lands.\nOn their own lands, and in some cases over their people wherever they are in Yukon, the eleven First Nations with Modern treaties also have law making powers, which are set out in Self-government Agreements. On Settlement Land some public and First Nations laws may apply to exploration. Seeking legal advice in this regard is recommended.\nFirst Nations who opted out of the UFA-based treaty negotiations, have decided to pursue other avenues to reach reconciliation with the Government of Canada. They include White River First Nation beside the Yukon/Alaska border and the two Kaska First Nations in southeast Yukon: Liard First Nation and Ross River Dena Council. These three First Nations with non-treaty traditional areas have unique rights and interests, described in this Guide, that need to be recognized to enable the advancement of exploration projects.\nThe Taku River Tlingit First Nation has an overlap interest in southeastern Yukon that is also a non-treaty traditional area. This northern British Columbia First Nation has rights and interests that extend into the Yukon but which have yet to be reconciled. They have cultural, trading, and family ties to the citizens of the Teslin Tlingit Council and the Carcross/Tagish First Nation with their Tlingit ancestry, and have specific traditional interests in those Yukon Traditional Territories.\nLand management in non-treaty traditional areas is evolving. The website and app will be particularly valuable for up to date information for these First Nations.\nIn addition to the TRTFN, other Indigenous organizations have significant land interests in Yukon. In 1984, the Inuvialuit signed their Modern treaty. It includes an environmental assessment process that applies to the North Slope of Yukon. In 1992, the Gwich’in Tribal Council (GTC) signed its Modern treaty which includes 1554 kms2 of fee simple lands in the Peel region of Yukon as well as rights to hunting.\nThis Guide provides information on all fourteen Yukon First Nations as well as three indigenous groups centered outside Yukon with interests in Yukon. Whether working in Traditional Territories of Modern treaty First Nations, or non-treaty traditional areas, relationships are critical to the advancement of exploration projects and this Guide is your resource to navigate the path towards engagement and consultation.\nTHE YUKON ENVIRONMENT\nWhere a mineral exploration company takes necessary steps to understand the Yukon environment and works with the key Yukon partners, the Yukon and First Nations governments, mutually-beneficial outcomes can be achieved. Indeed, it can very well complement the increasing alignment of thinking among First Nations, industry and society. This alignment suggests that, if done respectfully and inclusively, positive relationships can develop between the exploration industry and First Nations, relationships that can provide direct benefits to the long-term economic development of Yukon. There are several examples of where the mineral exploration industry and First Nations have reached respectful and beneficial relationships. Recent examples include: the Prophecy Platinum Corp and Kluane First Nation cooperation and benefits agreement; Victoria Gold Corp and the First Nation of Na-Cho Nyak Dun comprehensive cooperation and benefits agreement; Strategic Metals and the Champagne and Aishihik First Nations mineral exploration agreement.\nThese success stories are further backstopped by what is happening in communities throughout Yukon. First Nations citizens are increasingly educated and technically trained, and the economic development corporations owned by their governments are fostering First Nation businesses and entrepreneurs. This means First Nations are developing as service providers, contractors, suppliers, managers, and professional staff.\nThe Truth and Reconciliation Commission Report released in 2015 says in “Calls to Action”, #92 the corporate sector is asked to “commit to meaningful consultation [and] building respectful relationships… ensure that Aboriginal peoples have equitable access to jobs, training, and education opportunities in the corporate sector, and that Aboriginal communities gain long-term sustainable benefits from economic development projects.”\nThis Guide is designed to further healthy relationships between the exploration industry and Yukon First Nations.\nThe Guide provides a comprehensive collection and integration of information that otherwise is difficult to find in one place. The Guide provides a comprehensive information base across all Traditional Territories in Yukon with a summary of Engagement and Consultation Best Practices that are designed to encourage mutually beneficial partnerships.\nIt identifies the indigenous groups with an interest in land that the exploration industry wishes to work in. In some cases there may be two, or more, Traditional Territories or non-treaty traditional areas affected by an exploration program. It is important to review all of the Modern treaties associated with those Traditional Territories. If the activity is planned in a traditional area of White River First Nation or one of the two Kaska areas in Southeast Yukon (Liard First Nation and Ross River Dena Council) there are no Modern treaties in effect; this is discussed in section 3.4 of the printed Guide.\nThe Guide also offers direct links to First Nation(s) engagement and consultation materials. This information is intended to provide a road map to support industry and indigenous groups in arriving at a positive working relationship in Yukon. This road map is supported by a number of Best Practices provided here. These Best Practices are based on a series of foundational themes.\nThe context for engagement and consultation has evolved. It is now in the best interests of industry, First Nations and governments to reach lasting relations based on active and sustained engagement.\nMisinformation and missing information sometimes underlies tensions between First Nations and the exploration industry. The Guide aims to improve awareness and understanding of the different priorities and perspectives of First Nations.\nIt should not be assumed that all First Nations have similar interests or priorities; sustained communications builds an understanding of unique perspectives. Understanding the unique political, social, economic and environmental context of each First Nation can assist in building the partnerships among industry and First Nations.\nEach of the parties involved in or affected by mineral exploration has rights. Modern treaties define the rights of Settled First Nations, the laws of general application relating to aboriginal law define non-treaty traditional area rights, and Yukon and federal laws provide rights to exploration companies. Navigating these areas is greatly supported by strong and transparent relationships.\nSpecifically for the exploration industry, it is recognized that mining rights in Yukon are well developed, recognized and protected. While the Yukon and federal governments exercise wide-ranging authority to approve and regulate mining activities, they do so in accordance with an established body of administrative law that imposes duties of fairness and natural justice.\nMineral tenure in Yukon is granted under the Quartz Mining Act (QMA) and the Placer Mining Act (PMA). This legislation provides the exclusive right to publicly-owned mineral resources through the establishment of claims and/or leases. All public lands are open for staking and mineral exploration unless they are expressly excluded or withdrawn by order-in-council (e.g. First Nation settlement lands, parks, interim protected lands, buildings, dwelling houses, cemeteries, agricultural lands). As the primary legislation governing hard rock mining activities on lands in Yukon, the QMA provides for prospecting, exploration, staking and development of mineral resources by providing an orderly system of allocation of exclusive rights to minerals (specific permission must be obtained where the surface is occupied by others).\nSome definitions are useful to set the context for the Guide.\nConcerns are often expressed about the different meanings of “Consultation”, “consultation”, and “engagement”. When Consultation is capitalized it refers to the federal and territorial “Crown duty to Consult” which is being defined by the Courts. Although the federal and territorial governments (those who have the responsibility to uphold the Crown Consultation requirements) can delegate some procedural aspects of that duty, they are ultimately responsible for meeting that obligation to safeguard Indigenous rights. As noted earlier, it is advisable to seek legal advice in this area.\nThe Guide will also address “engagement” and “consultation”.\n“Engagement” is about the establishment of relationships where the parties build understanding of the other’s interests. Engagement continues before and after the signing of any agreements. It is the ongoing commitment and dedication to a conversation among the principals or their delegates to ensure understandings are being fulfilled and people are staying connected. By engaging on an ongoing basis, the parties can adapt to changes that inevitably occur in the environment or the exploration program.\nIndustry “consultation” is more about developing products such as a Memorandum of Understanding, a Letter of Agreement, or an Exploration Agreement (there are various titles for these). Industry consultation produces agreements that ultimately lay the groundwork for the ongoing relationship.\n“Early engagement” is a term found in this Guide. As you will see from the Best Practices, early is better. Early and open engagement supports First Nations’ capacity to engage fully in any stage of exploration. It is better to open the dialogue to ensure “no surprises” at a later point.']"	['<urn:uuid:f77035ea-3b10-4715-9240-7a5b6ca15974>', '<urn:uuid:5dee4901-785d-433f-910b-513667c622ec>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T06:15:13.904987	13	93	3708
