qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	Does empathy and emotional pain affect the brain similarly?	Emotional pain and empathy involve different but interconnected neural processes. Emotional pain activates specific brain regions like the anterior cingulate cortex, which is also involved in physical pain processing. In contrast, empathy involves multiple elements including cognitive, emotional, and kinesthetic components that work together to help us understand and connect with others' experiences. While emotional pain serves as a discrepancy monitor signaling when something is wrong, empathy functions as a way to understand and hold space for others' experiences, including their pain. The key difference is that emotional pain is a direct neural response to personal distress, while empathy is a complex set of skills that allows us to understand and respond to others' experiences, including both their joy and suffering.	"['An Overview of Integrative Empathy\nEmpathy consists of at least five elements, with corresponding practices.\nPosted March 25, 2022 | Reviewed by Gary Drevitch\n- Empathy has many definitions and even within one field of study they’re far from consistent.\n- We consider empathy as holding space for others as they are.\n- We identify five distinct but interconnected elements of empathy: self-, kinesthetic, reflective, imaginative empathy, and empathic creativity.\nWhen we ask people what empathy is, a common response is “to put yourself into someone’s shoes."" We also hear that it involves knowing about the thoughts and feelings of others. However, in business, we detect an aversion to emotions. Understanding others: great; feeling with others: meh…\nIn short, empathy is holding space for another person to express themselves, as they are. It allows us to connect, feel, and understand the experiences of others. However, empathy has many definitions and even within one field of study they’re far from consistent. With increasing interest, empathy has become an umbrella term for many processes of shared experiences.\nScience now distinguishes three forms of empathy: cognitive empathy, emotional empathy, and empathic concern. Cognitive empathy is understanding another’s point of view. Emotional empathy is the ability to feel what someone else is feeling. Empathic concern is a response to the distress of another person which is consistent with the distress you perceive them to be in. These definitions, however, are influenced by current trends and limits in psychology and social neuroscience. Empathy is explored in other disciplines too.\nIn our practice, we combine knowledge from different disciplines into an integrated definition of empathy. Our claim that empathy is holding a space for others, as they are, distinguishes it from empathy as intervening with others, no matter how well intended. Holding space is not about interjecting, nor bringing good intentions. Rather, it brings a neutral quality to your relationships. Integrating research and practice on empathy in psychology, philosophy, the arts, primatology, evolutionary biology, medicine, neuroscience, the humanities, and anthropology, we identify five distinct, but interconnected elements of empathy:\n- Kinesthetic empathy\n- Reflective empathy\n- Imaginative empathy\n- Empathic Creativity\nBe present with self-empathy\nUnderstanding others requires that we first understand ourselves. In self-empathy we notice and recognise what is happening in us and work with it in order to be present for others. We’re not talking about feeling sorry for, or caring for, ourselves. Self-empathy starts with observing ourselves in an empathic manner, becoming aware of our conscious and hidden agendas.\nIt helps us to take control of how we are in a situation, taking a moment to sense and make sense of what we feel and think, with an attitude of openness and suspended judgment. If we don’t do this, we are likely to succumb to various pitfalls. We might confuse empathy with sympathy or emotional contagion or project ourselves on others. Biases and preconceptions cause us to confuse our own experience with that of others.\nSelf-empathy is the ability to bring awareness to your own state in the moment and thus to distinguish your own experience from the experience of others\nThrough self-empathy we develop a sense of agency, the awareness of ourselves as the initiator of actions, desires, thoughts and feelings. It enables us to differentiate our own experience from the experience of someone else and prepares us to empathize with them.\nWorking with the ‘unsaid’ with kinesthetic empathy\nKinesthesia is one of our subtle senses. We sense our body’s position and movement with proprioceptors in the muscles and joints. In kinesthetic empathy, we connect with others by coordinating and synchronizing with their bodily expressions. We intuit someone\'s’ inner experiences through their outer bodily expressions.\nKinesthetic empathy is the ability to sense into somebody’s movement, or their sensory experience of movement via direct motor representation.\nKinesthetic empathy has two main purposes: First, to become aware of and explore how we influence each other’s physical space. It establishes embodied connection and synchronization through experience rather than cognition. Second, we modify our own bodily expressions to hold space for another person. How do you express the space you hold for someone? How will they notice?\nPresence with reflective empathy\nHave you noticed, when listening to someone, how quickly you are inclined to share your own story or insights? Or have you felt yourself cut off by someone giving you unwanted advice? This happens because your listener listened to react. In empathic listening we don’t listen to react, but to reflect.\nIn reflective empathy we apply empathic listening skills to stay present and ensure we understand the person we listen to. Empathic listening is an iterative process of listening and reflecting back what we hear. The speaker hears an echo of his expressions, offered in a way that invites them to amend or reject what is reflected. If the reflection is a faithful representation of what the speaker had in mind, then mutual understanding is achieved.\nReflective empathy is the ability to actively listen and reflect back what you hear, including factual information and felt meaning.\nIn empathic listening, we listen to the content and the ‘felt meaning’ of a speaker. The content is expressed in words and the felt meaning, or subjective experience, is communicated through intonation, body language, and what is not being said.\nValuing perspectives with imaginative empathy\nImaginative empathy uses as-if acting and embodied imagination to gain insight into another’s experience. Using self-empathy, active inquiry, and wonder, we embody another person’s perspective in order to gain an experience of their mental life. This ‘as-if’ experience is not their experience, as we cannot inherently know the experience of others. Yet, the practice of imaginative empathy guides us to understand and value other perspectives.\nImaginative Empathy is the ability to gain insight into the experiences of others through embodied imagination, stepping in and out of perspectives.\nWhen we identify with what we believe, thinking we\'re right, we miss out on understanding other perspectives. Cultivating detached observation in the midst of our actions and interactions is vitally important. We have to drop the way we see things to really try to see it the way others do so that we open up to novel perspectives and experiences.\nFrom inspiration to action with empathic creativity\nPractising empathy sparks empathic creativity. It is not a practice of empathy itself but a creative consequence of empathy practices. Empathic creativity can be an insight, a discovery, or a creative outburst. It can show itself as inspiration to try new behaviours or to find solutions to problems. It can also be expressed in generosity: offering financial support, ideas or time; or as a generosity of heart: offering protection, emotional support and love to others.\nEmpathic creativity starts with an intention or plan to take action. The art of empathic creativity is to notice when creativity is sparked, take note of emerging intentions and translate them into actionable outcomes.\nEmpathic Creativity is the ability to notice significant change events and to convert them to actionable outcomes\nResearch shows that empathy training can be highly effective. Yet in order to master it, we need practices. Empathy is not learned through ‘understanding’ what it means but by practising specific skills, preferably in real life. And once you’ve acquired the skills, it’s up to you to maintain them. Empathy, after all, is not something that we are but something we do.\nHall, J. A., & Schwartz, R. (2019). Empathy present and future. Journal of Social Psychology, 159, 225-243.\nMoore, J. W. (2016). What is the sense of agency and why does it matter?. Frontiers in psychology, 7, 1272.\nTeding van Berkhout, E., & Malouff, J. M. (2016). The efficacy of empathy training: A meta-analysis of randomized controlled trials. Journal of counseling psychology, 63(1), 32.', 'What is Emotional Pain?\nI spent 7 years researching the communication of emotional pain from suicidal patients to mental health professionals. I became interested in emotional pain many years ago when analysing an incident of self-harm behaviour with a client. He described his feelings during an argument as ‘painful’, and at first I accepted this as a colloquial term, as in, ‘relationships can be painful’. But I was intrigued; This pain in my client’s body was a key factor in his urge to harm himself. So was he actually experiencing pain? Or was he speaking metaphorically? Was he describing a component of a bigger experience such as grief? Was he somehow scrunching his internal organs to produce actual tissue damage? What exactly was going on?\nI searched the literature and found a range of related phenomena but with slightly different names; social pain, psychological pain, emotional pain, mental pain, psyche-ache and psychic pain. Furthermore it seems that high levels of this experience are thought to play a role in completed suicide. Back in the 1970s eminent suicidologist Edwin Shneidman analysed suicide notes for recurring themes and identified that the most common one was a desire to escape from emotional pain.\nMore recently neuroscientist Naomi Eisenberger and her colleagues at UCLA studied the brain scans of subjects during a computer-game which was deliberately manipulated to have some players experience being left out by the others. The brain areas that became active in the rejected players were those also activated during physical pain. One such area is the anterior cingulate cortex. Decades ago brain surgeons did experimental surgery on this brain-site while attempting to treat intractable physical pain, because ablation of the anterior cingulate cortex removed the unpleasantness of the pain experience. Sadly these poor patients ended up with such horrendous physical injuries from NOT feeling pain (such as when touching a hot surface) that this surgery was discontinued.\nThis sharing of the same neural networks in physical and social pain has been called ‘Pain overlap theory’. So if the same neural architecture is involved in physical and emotional pain, how do they differ? Research suggest that physical pain has an additional component; information from the somatosensory cortices, i.e. a location marker from your skin or vital organs, pointing to a specific place in your body where injury or illness has occurred. But why would the same neural alarm sound in a social situation where no wounding to the body has been sustained? Eisenberger suggests that pain acts not so much as a damage indicator, but as a discrepancy monitor, signalling where something is deviant from its desired (or usual) state. Pain focusses your attention on what is wrong, enabling you to attempt to heal your body, repair your relationships or mitigate your losses.\nThis theory makes sense on so many levels. Firstly it accounts for the shared language across physical and emotional pain, for example we talk about bereavement or unrequited love as aching or agony. Secondly it provides a good rationale for self-harming behaviour. If you have intense but free-floating pain you are relieved once you can identify one area of the body for that pain, not only does it give ‘reality’ to the experience, but also allows you to predict its ending. As your flesh heals you might reasonably assume that so will this inner torture. It also explains how any emotion can hurt if it is discrepant from what is desired or anticipated – we can even experience painful joy if its intensity takes us by surprise.\nSome patients say it hurts when they self-harm, others that they don’t feel it. Could this depend on the difference in intensity between the physical injury and emotional pain in that moment? Like hearing a shout in a noisy factory – if the background noise is louder, it won’t be heard. If the emotional pain is more intense, pain from the injury site will go unnoticed. In these circumstances it is only seeing the injury that will have that reassuring ‘realising’ effect, and we know that some clients report it is the visual appearance of their wound that brings relief.\nBecause emotional and physical pain are so closely related analgesics work for both – alcohol, drugs and over the counter painkillers will numb emotional pain as they do physical pain. When people become suicidal they are often trying to destroy the seat of their pain where it resides – in the body. Intuitively we all know this already. The wonderful Paul Gilbert notes that when gangsters go to war they threaten to harm the children of their enemies rather than the enemies themselves. This is because the thought of emotional pain is the worst.\nHowever, culturally we do not yet treat emotional pain as equivalent to physical pain. A colleague who read my research remembered a relative who had killed himself without warning. “Although there were no suicide threats before his death, there was a lot of emotional pain communication”. Physical injury motivates us to move with urgency towards the injured party, to see what we can do to help. We In contrast subjects in my research reported that even being able to articulate their emotional pain somehow communicated the message, “if you can talk, it can’t be that bad.”\nI believe the current campaign for parity between mental health and physical health is an opportunity to call attention to these similarities. Let’s begin to ask our clients and patients routinely about their emotional pain. Just as when assessing physical damage a doctor usually asks, “How much pain is this causing you?” We can ask clients to monitor their emotional pain levels. We can assure them that this pain isn’t random, it’s not a sign of weakness, there is a proper physiological reason for it, and that we can help them problem-solve the issue that set it off. Emotional pain is the smoke to the fire of suicide and self-harm. We need to respond as if it is an actual fire, and as soon as we smell burning. Let’s not leave it until the flames have taken hold.']"	['<urn:uuid:8864dfae-95c9-4e8f-ab59-e9beb3588fd2>', '<urn:uuid:404e3052-f185-4e15-ae36-a36bed164d18>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:31:27.499155	9	121	2309
2	smart buildings efficiency security challenges	Smart buildings improve efficiency through automated systems for energy, water, and air conditioning, making inhabitants more productive through better lighting, thermal comfort, and air quality. However, they face security challenges as highlighted in SDN communications, where manual security configurations can lead to vulnerabilities and policy conflicts. SDN's centralized control offers improved security through holistic visibility and efficient policy enforcement, helping protect the sensitive data collected by the building's sensor networks.	"['“Smart buildings are the solution to check the energy consumed by buildings & the carbon emission caused by them. They are more than automatic lighting & CCTVs. These buildings are controlled by an intelligent network of sensors and actuators with computing & communication systems that greatly increases the efficiency of buildings while supporting the inhabitants in their day-to-day activities. Everything from energy consumption, water consumption to air conditioning, security & lighting are automated.”\n– Annrin K Thomas, Environmentalist & Civil Engineer at Advenser Engineering Services\nThe foundation of a smart building is made out of data. In order to leverage the most out of smart buildings everything starting from design, procurement, fabrication, assembly & implementation to commissioning, operation & maintenance has to be done through a fully integrated platform.\nWhere does BIM come in the case of smart buildings?\nUnlike constructing conventional buildings, constructing a smart building involves the integration of the core systems of a building such as lighting, electricity meters, water, Heating Ventilation & Air conditioning systems, firefighting system, water pumps with a network of sensors and an integrated control system.\nThis is where BIM comes into play!\nBuilding Information Modeling (BIM) is all about integrating the several aspects that go into construction and it is the pre requisite in constructing a smart building. Which makes BIM the right tool in the construction of smart buildings.\nHow does a Smart Build Environment (SBE) work?\nBefore we dwell deep into the benefits of incorporating the various BIM services in the construction of smart buildings, let’s have a quick look into how a smart build environment work.\nTraditional buildings have provided the inhabitant’s shelter, protection from extreme weather & safety at the same efficiency level for years. Unlike these buildings, smart buildings can be considered as living organisms connected to an intelligent network.\nSmart buildings are constantly evolving while significantly improving efficiency. There will be dozens of smart objects like sensors and actuators installed in the smart buildings. These smart objects will constantly monitor & interact with their immediate environment and will be in communication with each other either through Ethernet cables or over an internal wireless network.\nSmart buildings are known to make its inhabitants more productive with better lighting, thermal comfort, better air quality and better security at a lower cost and environmental than the traditional buildings.\nChallenges faced in a Smart Build Environment\nLike we discussed, dozens of smart objects will be ubiquitously installed in an SBE to perform sensing and control of the immediate environment. The three mail challenges faced in an SBE are\nChallenge 1: The physical location where smart objects such as sensors are embedded can greatly affect their performance and ability tocarry out certain tasks. For e.g. The location of light detecting sensor can greatly affect the ambient light or occupancy it senses.Apart from this, the wireless network of the Ethernet cable through which these smart objects communicate should be designed to enable perfect communication between them.\nChallenge 2: The way these smart objects embedded in SBEs interact with their environment is very crucial. These objects can’t really function solely on the data they collect from their immediate surrounding rather they need to be fed the space data such as floor plans of the buildings too. In the case of conducting a performance analysis or energy analysis of the SBE building, along with the information from the sensors & meters data on building architecture and geometry are also needed.\nChallenge 3: Maintaining the smart objects in the building and properly documenting the data from an SBE building is not a walk on the beach. In an SBE building, the data will be more complicated than the traditional structures\nBattling the challenges with BIM\nIn the pre-construction stages of a project, BIM can be extremely vital. The data rich graphical model can be utilized to identify any shortcomings before first breaking ground.\nThe primary advantage of designing an SBE building with BIM lies in the fact that a data rich BIM model can be used to plan the layout and placement of the sensors, actuators, tags and meters inside the SBE building. The performance of the SME building can be verified against the known parameters and the layout and location can be optimized for the best functional performance.\nIncorporating BIM in SBE buildings can be the answer to the challenge of asset tracking & maintenance during the post construction phases. A BIM model can store the physical information of smart objects & their installed location these data can be visualized in 3D.\nIt is time to embrace the fact that the buildings we work & live in shape us! A better building always influences how happy and productive the inhabitants are. Smart buildings are the future of the construction industry.\nThe journey to constructing these buildings start long before first breaking ground. BIM is the only futuristic technology that can drive that journey.', ""Secure Software-Defined Networking Communication Systems for Smart Cities: Current Status, Challenges, and Trends\nSmart city is a transformative and progressive vision that aims to revolutionize infrastructure systems and public services in an urban area with modern information technologies. Its ultimate goal is to greatly improve the livability Quality of Service (QoS) of its citizens and to optimize the utilization of its assets and natural resources sustainably. One of the key technical attributes in smart cities is to deploy a large number of sensors to collect data to enable real-time and intelligent\n... ecisions for various city functions and citizen needs. Many of the data have strict security requirements as they are either private to citizens or sensitive to critical infrastructures. As a result, how to securely and efficiently deliver and process the dramatically increasing volume of data becomes one of the grand challenges in materializing the smart city vision. In recent years, Software-Defined Networking (SDN) has emerged as a leading communication infrastructure candidate for smart cities. While many efforts have existed to research, prototype, and even deploy SDN on a small scale for some smart city applications, there is still a lack of cohesive understanding about SDN's impact on the secure communication need of smart cities. In this paper, we conduct a comprehensive survey of the core functionality of SDN from the perspective of secure communication infrastructure at different scales. A specific focus is put on the security threats and challenges in accordance with SDN plane-based architectures for various smart city-enabled applications. We further systematically categorize the state-of-art solutions and proposals to apply SDN to support typical smart city applications, such as transportation, health, and energy applications. Lastly, we cast a holistic view of future research trends. INDEX TERMS Communication system, OpenFlow, security, smart city, software defined networks. 12084 VOLUME 9, 2021 M. Rahouti et al.: Secure SDN Communication Systems for Smart Cities These manual security configurations (i.e., firewalls, IPSec, intrusion detection and prevention system (IDPS)) on a distributed set of network entities are vulnerable to inter-domain policy conflict and configuration and implementation errors, which may lead to earnest security ivulnerabilities and breaches  . Contrariwise, SDN improves security in a networkingenabled environment due to its centralized control of the network system and holistic visibility of the network behavior and run-time manipulation of inserting/pushing forwarding rules  . Therefore, the SDN non-distributed management of network allows for a more efficient enforcement of security policies and reduction of their conflicts. Additionally, security implementations such as security monitoring applications could efficiently inquire flow samples from data-paths via an SDN controller  . Once security analysis is finished, the monitoring application may guide the data path components to take action by either denying incoming traffic, redirecting the traffic to security-based middle boxes, or even restricting the traffic within a particular network authority. Moreover, SDN grants an efficient update of security applications and policy implementations. It allows for appending security modules at the controller platform instead of changing the hardware or even updating its firmware  . As the SDN controller detaches and centralizes the control plane of a network, it allows for the enforcement and automation of security policies due to the programmability features of the SDN controller. Therefore, SDN can deal with network threats and malicious traffic at runtime by leveraging applications of network security. To better represent an SDN architecture, Figure 2 depicts the main planes/layers of SDN and their functionalities. The three planes are shaped as follows: FIGURE 2. A high-level overview of SDN architecture layers. TABLE 1. A list of acronyms used in this article and corresponding definitions. KAIQI XIONG (Senior Member, IEEE) received the Ph.D. degree in computer science from North Carolina State University. Before returning to academia, he was with IT industry for several years.""]"	['<urn:uuid:2c8fbb53-810a-4c2d-a156-c9e9abd7e5dc>', '<urn:uuid:c0a9ac59-21ba-4c28-8cd9-721e827d86d6>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	5	70	1443
3	I'm a school counselor. When should I check if a student has suicidal thoughts?	It is recommended to conduct suicide risk assessments on all clients presenting for therapy. Specifically, clients showing depression or depressive symptoms, or those in crisis states should be questioned for suicidal ideation. Special attention should be given to questions related to suicidal thoughts when using depression inventories.	['Procedures for Assessment of Suicide Risk\n***Counselors-in-training are highly encouraged to seek consultation with supervisors or instructors if there is immediate concern regarding a client.\nAlthough there is much information to gather, there are no shortcuts to suicide assessment. Risk assessment requires directness, intentional questioning, and careful listening. The essential skills and conditions of counseling (empathy, reflections, restatements, attending, active listening, etc.) are important in suicide assessments and intervention.\nInformation that is gathered during assessment should be documented (See Ethical and Legal Aspects\nKnowing when a suicide assessment is necessary\nThere are recommendations that counselors conduct suicide risk assessments on all clients presenting for therapy (Laux, 2002). It is common practice that suicide ideation is assessed through intake forms and intake interviews\nSpecifically, clients presenting with depression or depressive symptoms\nor in states of crisis should be questioned for suicidal ideation. If using depression inventories\n, special attention should be given to questions related to suicidal thoughts (such as question 9 on the Beck Depression Inventory).\nAs the client tells his/her story, the counselor should be listening (and looking) for the presence of risk factors\nand protective factors\n. As the number of risk factors increases particularly in the absence of protective factors, suicide risk increases and should be questioned.\nAs a counselor attends to the client, language that reflects feelings of hopelessness and despair should be noticed and explored. For instance, it is paramount to ask for elaboration on statements such as “I can’t go on anymore.” “I want to end it all.” “I wish I were dead.” “This is hopeless, I don’t see any way out of this situation.”\nIn truth the first intervention for suicide is the assessment, in other words assessment begins the process of suicide intervention.\nThe point is to assess for risk AND leverage (information that can be used to intervene).\nQuestions to guide Suicide Assessments\nEither as part of an intake assessment, or based on information you have gathered indicating that a suicide assessment is in order, the starting point is:\n- Ask directly if the client has thoughts of suicide. “Have you thought of committing suicide?”\n- “Are you thinking of killing yourself?” In this case, subtlety is counterproductive.\nIf the answer is anything but a confident “No”, then assessment should proceed.\nEven in cases when a client answers by saying “No”, continued exploration and discussion of what the client has said or presented that may be related to suicidal ideation is warranted.\n- Have there been previous attempts? (When, surrounding circumstances, rescuer?)\nFor example: “When?” “How often?” “What happened?” “What was going on in your life at the time?” If attempts were made, then exploration of method and rescuer should be explored.\nIf the client indicates having thoughts or having made attempts in the past, even if there is no current ideation, past experiences should be thoroughly explored.\nIf the client does not answer questions about suicide, the answers are vague, or if the client conveys that he/she has entertained thoughts of suicide then…\n- Are the thoughts pervasive or intermittent? When was the last time the thought occurred to the client? Do these thoughts typically occur in times of crisis?\n- Is there a specific precipitating event?\nEven if answers to these questions continue to be vague or seem to be more intermittent, ideas of how the person might commit suicide need to be explored.\n- Is there a plan? What are the details of the plan? How extensive is the plan?\nExamples: “How have you thought of killing yourself?” “When would you carry out the plan?” “Do you have a date and time?” “Where would you be?” “Who would you want to find you?”\n- What is the lethality of the means/method?\n- Is there access to the identified means?\nExamples: “If you were to commit suicide, how would you do it?” “Do you have the pills?” “Where are they?” “What type of pills would you take?” “What type of gun?” “Where would you get the gun?” “Do you have bullets?” “Where is the gun? The bullets?” “Do you have a rope/cord?”\nThe previous questions have related specifically to suicide ideation. In addition, questions that assess for risk and protective factors are explored. All of this information aids in determining risk and subsequent interventions\n- Is the client using drugs or alcohol?\n- What are the client’s social supports?\n- Does the client have a religious or spiritual affiliation?\n- How is the client discussing suicide and potential aftermath? Do fantasies seem to be positive or painful?\n- Is the client able to see any alternatives to suicide?\n- How does the client respond to challenges to distorted thinking?\nThe Use of Assessment Instruments\nVarious instruments have also been used assessing for suicide risk. These include assessments such as the Hopelessness Scale (Beck, Weissman, Lester, & Trexler, 1974) and the Beck Depression Inventory (Beck & Steer, 1987) and the BDI-II (Beck, Steer, & Brown, 1996) that were not specifically designed to measure suicide ideation, but what is measured correlates with suicide ideation and can therefore be helpful.\nIn addition, there have been instruments developed specifically to assess for suicide ideation. These instruments include:\n- Beck Scale for Suicide Ideation (BSSI) (Beck, Kovacs, & Weissman, 1979)\n- Suicidal Ideation Scale (SIS) (Rudd, 1989)\n- Suicide Behaviors Questionnaire (SBQ) (Cole, 1988)\n- Reasons for Living Inventory (Linehan, Goodstein, Nielsen, & Chiles, 1983)\n- Suicidal Ideation Questionnaire (Reynolds, 1987)\nSome of the above instruments have also been validated for use with adolescent or college populations. In addition, there are instruments that have been specifically developed for these populations.\n- College Student Reason for Living Inventory (Westefeld, Cardin, & Deaton, 1992)\n- Suicidal Ideation Questionnaire – junior high version\n- Multiattitude Suicide Tendency Scale – for adolescents (Orbach, Milstein, Har-Even, Apter, Tiano, & Elizure, 1991)\n- Fairy Tales Test (Life and Death Attitude Scale for the Suicidal Tendencies Test (for children 10 and younger) (Orbach, Feshbach, Carlson, Glaubman, & Gross, 1983)\nThe use of suicide assessment instruments can be helpful, but should not replace the assessment interview. There are also times (due to the emotional and cognitive state of the client) when administration of a test would not be prudent.\n*For a discussion on suicide assessment instruments, see Brems, 2000 and Westefeld, Range, Rogers, Maples, Bromley, and Alcorn, 2000).']	['<urn:uuid:79e28425-8498-4dca-be37-c334972ce5f3>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	14	47	1056
4	fodder hydroponic benefits operation costs	Hydroponic fodder units offer multiple benefits while maintaining low operating costs. From a benefits perspective, they produce high-quality nutritional feed year-round, use minimal water (only 100 liters per month to feed 200-220 goats), generate minimal pollution, and save soil degradation. Regarding operations, modern hydroponic systems are relatively inexpensive to build and can be constructed with materials found at local hardware stores. The most cost-effective commercial method is the Nutrient Film Technique (NFT), which has minimal operating costs while delivering high levels of oxygen to plant roots.	['Follow CEPF Med:\nNew technology helps vulnerable villagers whilst protecting natural reserve\nSmall shrubs pepper an arid landscape of steep, sandy mountain slopes, where water is scarce and the sandy soil barren. On the edge of the Wadi Mujib Biosphere Reserve in Jordan, people live a traditional pastoral lifestyle below the poverty line. With livestock-keeping their main, or only, source of income, the conditions mean the 8,000 villagers of Faqou struggle to give their sheep the nutrition they need. Their care of their livestock leads to overgrazing in the Reserve, which supports a surprising variety of plant species including rare orchids, and several highly-adapted mammals including a threatened large wild mountain goat, the Nubian Ibex.\nImagine, then, a solution that allows people to grow cheap fresh green feed for their livestock in just seven days, all year round. Is there an innovative agricultural solution that takes little space, uses water efficiently, does not degrade the soil, uses no pesticides, improves food security, adjusts to climate change, improves people’s livelihoods and relieves pressure on nearby reserves so nature can flourish too? Yes, it is called a ‘green fodder unit’.\nWhere: Faqou village and Wadi Mujib Biosphere Reserve, Jordan\nKey species: Nubian Ibex Capra nubiana, 43 rare plant species\nProject partner: Sustainable Development of Agricultural Resources (SDAR)\nWhat is green fodder technology?\n› Fodder - Food given specifically to livestock, rather than foraging for themselves.\n› Green Fodder - Fresh green vegetation for livestock, rich in minerals and protein, as opposed to the expensive dry feed that herders would have to buy and import when they cannot produce fodder on their land.\n› Hydroponic Green Fodder unit - A method of growing green fodder in water without soil, using mineral nutrient solutions, and taking up little space as an indoor unit stacks green fodder horizontally. Electricity for lights is provided by solar panels on the roof of the unit.\nA pilot green fodder unit was installed by SDAR working with the sheep farmers of Faqou’s Agricultural Cooperative Association, and has proved very successful. The ownership of the unit was transferred to the association, and, despite difficult early stages where locals were hesitant to buy fodder produced by this new technology, sheep farmers continue to purchase the green fodder rather than grazing on the reserve.\nPilot green fodder unit produces 0.5 ton of green fodder per round, sufficient to feed 200-220 goats using only 100 litres of water per month (recycled for a period of one month).\nIt is estimated that this saves up to 10 hectares of grazing land on the reserve in the first year.\n› Hydroponic green fodder unit installed, which produces fodder reliably throughout the year with very low running costs.\n› The unit consumes minimal water, generates minimal pollution, and saves soil degradation.\n› Green fodder production saves vertical space.\n› Workshops were held to ensure local people fully understood the benefits and were on board with the project, so sheep farmers bought this fodder.\n› Ownership of the unit transferred to the local cooperative, so they sell green fodder at a price beneficial to the sheep farmers, and improving income and living conditions.\n› Fodder of high nutritional quality produced, allowing for increase numbers of livestock per family, so better production of meat, milk and other products.\n› Grazing pressure on the reserve relieved, and minimal impact on biodiversity, as opposed to traditional fodder production.\nAs well as promoting ‘community management’ of a new resource, the project has also raised villagers’ awareness of Wadi Mujib and its unique nature, and the impacts of the different choices they can make when feeding their livestock.\n“Now people realise the importance of the flora and fauna around them, and we see this technology expanding to other sensitive areas in the Middle East.”\nRami El-Akhras, SDAR\nRami El-Akhras | firstname.lastname@example.org | email@example.com\nSign up to the CEPF Med quarterly newsletter\nThe Critical Ecosystem Partnership Fund (CEPF) is a joint initiative of l’Agence Française de Développement (AFD), Conservation International (CI), the European Union, the Global Environment Facility (GEF), the Government of Japan, the John D. and Catherine T. MacArthur Foundation, and the World Bank. Additional support in the Mediterranean Basin is provided by the MAVA Foundation. More information on CEPF can be found at www.cepf.net\nA fundamental goal is to ensure civil society is engaged in biodiversity conservation.\nCEPF is more than just a funding provider\nA dedicated Regional Implementation Team (RIT) (expert officers on the ground) guides funding to the most important areas and to even the smallest of organisations, helps build civil society in the region, and shares learned lessons and best practices. In the Mediterranean Basin Biodiversity Hotspot, the RIT is entrusted to BirdLife International, including its Middle East office and the BirdLife Partners DOPPS/BirdLife Slovenia and LPO/BirdLife France.', 'Fresh water may soon become a costly commodity. The fundamental social problem of feeding society is growing larger due to a rising scarcity of water and an ongoing depletion of agricultural land. Population growth, climate changes, pollution, and agricultural water waste contribute to growing fresh water shortages around the world. Depletion of soil nutrients through poor farming techniques, floods, poor irrigation, and winds have seriously damaged agricultural land.\nApproximately 40% of the world’s agricultural ground is unsuitable for farming. The role of agricultural business desperately needs to align with the evolving ethos of a rapidly growing society. Can hydroponic farming provide a sustainable solution to environmental problems caused by traditional farming methods? What practical applications does hydroponics have in densely populated urban areas? Farmers describe soil degradation as thinning and unproductive land that leads to low yielding crops.\nLand degradation includes nutrient depletion, loss of biodiversity, climate change, erosion by water, erosion by wind, reduced vegetative cover, pollution, drought, compaction by animals or machinery, sedimentation, increased soil temperatures, reduced organic matter, and salinization (Stockings, 2000 pg 5). According to the United Nation’s food and agriculture program, 854 million people do not have sufficient food for an active and healthy life (Sample, 2007). The population has increased by nearly 2 billion in the last 20 years and food production increased by 50%.\nIt is estimated that by 2050 the population will reach 9 billion. (Sample, 2007). State and federal officials have drafted accords in attempts to rectify water shortages. UN officials have gathered to create and execute a plan of action to improve conservation of soil and restoration of degraded land, but such plans are merely band-aids on a broken limb. The ultimate form of soil conservation is eliminating the use of soil in agriculture completely. Hydroponic agriculture offers a permanent solution to this rapidly growing problem of water shortages, pollution, and land degradation.\nDevelopment of hydroponic systems took place from 1925 through 1935. Experimentation with soil-less nutrient solutions and advancement in agricultural plastic by Professor Emery Myers Emmert at the University of Kentucky sparked an interest in hydroponic food production. Efforts were primarily aimed at large scale commercial food growth, but Hydroponic food systems were eventually abandoned due to high construction and operation costs (University of Arizona). Modern hydroponic systems are relatively inexpensive to build and offer several effective methods of food production.\nThe most popular commercial agriculture hydroponic system is the Nutrient Film Technique. NFT consist of a plastic pipe or gutter, a water reservoir, and a water pump. A thin film of nutrient infused water constantly flows through the tubing. Plant roots are suspended over the piping with only the roots touching the stream of nutrient water. The pipe or gutter is slightly elevated at the far end to allow water to drain back into the water reservoir. The NFT system delivers high levels of oxygen to the roots promoting vigorous plant growth.\nNFT systems are ideal for leafy greens such as lettuce, cabbage and basil but are effective for a multitude of fruits, herbs, and vegetables. A minimal operating cost makes this system ideal for commercial applications. [pic] [pic] A popular system for larger plants is the ebb and flow or flood and drain system. This table system consists of a plastic tray, water pump, timer, reservoir, and tubing. The plants lay on the top table separate from the nutrient reservoir. The pump is programmed to turn on in 15 minute increments with variations specific to the type of plant and stage of development.\nWhen activated, the pump will fill the table at top with the nutrient infused water from the reservoir below. Once it is turned off the water will drain back into the reservoir below. [pic] One of the simplest hydroponic methods is the deep water culture system. Often used in commercial applications, this system can be built with materials found at a local hardware store or discount retail market. The deep water culture system can be built with a non-transparent storage tote, an aquarium air pump, and an air stone commonly used in fish tanks.\nPlant roots are permanently submerged in nutrient rich water. This system does not require a water pump or timer. Urbanization and technological developments extended the distance produce traveled to reach consumers. Heilbroner explains the changed by describing that “industrial technology has literally refashioned the human environment, bringing with it all the gains-and all the terrible problems-of city life on a mass scale” (Heilbroner, Milberg pg 83 ). Hydroponics is the tool to address the basic social problem of feeding society by making produce readily accessible and grown in urbanized areas.\nAlthough traditional agriculture has served its purpose for many years, the ethos of rapidly growing demands sustainable and healthy forms of food production. Hydroponic farming in conjunction with greenhouses can help solve land degradation and water pollution problems through out the world. Lufa Farms, a Canadian farm based in Monteral, engineered a series of greenhouses on top of a 31,000-square foot office building. Greenhouse growing allows the Canadian farm to supply fresh produce all year round, even in 45 degree temperatures.\nIt is estimated that Lufa Farms can deliver more than 1,000 baskets of produce per week, three times more than land-based competitors. ( Business and the Environment). Greenhouse hydroponic farming not only solves the problem of land degradation and a water shortage due to traditional farming practices but it also alleviates the pressure of fossil fuel consumption and vehicle pollution. Because sustainable farming systems can be established in city rooftops and barges, transporting fruits and vegetables from remote farms is no longer necessary.\nProduce grown in the city have shorter distances to travel reducing transportation expenses and fuel consumption. Consumers can purchase a fresher product at their local market. According to Columbia University professor of public health and microbiology Dickson D. Despommier, a 30-story, one square block farm could produce the same amount of produce as a 2,400 outdoor soil based farm ( Business and the Environment ). The Science Barge, a greenhouse and hydroponic system on top of a barge based in Yonkers, New York, is a fully functioning urban farm prototype for a fully sustainable food production sytem.\nThe Science Barge produces tomatoes, cucumbers, and lettuce without carbon emissions, chemical pesticides, or waste water runoff (New York Sun Works, 2010-2011). The barge uses solar and wind power to sustain greenhouses on top of the barge. Purified river water and rain water is used to irrigate the crops. Science Barge grown produce use seven times less land and four times less water than traditionally grown crops. (New York Sun Works, 2010-2011). The economic advantage of hydroponic farming can be identified through a cost-benefit analysis.\nConsidering that in 2000, 41% of all freshwater used in the United States was for Agricultural purposes (Agricultural Resources and Environmental Indicators, 2006 Edition); reduction in water cost is one of the main economic advantages of hydroponic farming. BrightFarms is a company that has taken this emerging technology and developed a business model of “Better food, better prices, better environment. ” BrightFarms designs, finances, builds, and operates hydroponic greenhouse farms on supermarket rooftops.\nBrightFarms pledge is “to deliver produce at equal or higher service levels than the retailer currently requires of its other suppliers. There is no cost to the retailer to build the BrightFarm, only an obligation to purchase the output. ” The company intends to increase profits by reducing shrink due to longer produce shelf life. They have calculated that this practice will produce higher gross margins for the retailer. The company implements a long-term price fix contract to protect the retailer from unstable prices, rising cost costs and inconstant supply. BrightFarms 2012). BrightFarms feels that “with the elimination of shipping, and the drastic reduction of fuel consumption, carbon emissions and water use, BrightFarms enables grocery retailers to change their produce supply chain in a way that improves the planet and their bottom line. ” (BrightFarms 2012). According to Brian H. Kurbjeweit, professor of Contemporary Business at the University of Redlands, economic progress is an interconnected system comprised of science, economics, law, and ethics all influenced by the ethos of a society.\nThe ethos of modern society has expressed an urgent need for sustainable business practices. Science has engineered the tools to effectively create a new form of responsible farming. Materials to build hydroponic systems are readily available at a relatively low cost. Through social media, instructional videos and written instructions to build and operate hydroponic systems are readily accessible world wide through urban farming forums free of cost on the Internet. There is a basic economic need to feed and sustain life.\nThe basic economic problem creates a market for those willing to invest in large-scale urban farming businesses. Responsible agriculture is an ethical guideline that all farmers in global and local economies should follow to ensure the basic economic problem is addressed. Continued land degradation, water pollution, and pesticide smothered produce combined with a rapidly growing world population will lead to further human suffering. It is an ethical duty of world leaders to implement sustainable forms of agriculture to nurture its citizens. Law, the missing factor is yet to be addressed.\nIt may be the ethos of future generations that persuades governments to execute laws to speed up the implementation of sustainable farming techniques or it may be severe ethical violations by non-complying agricultural corporations that spark the creation of laws to effectively protect citizens from illness caused by pesticides and water pollution. It may simply be an epidemic interest in well being or as author Malcom Gladwell describes, a tipping point may be reached where local co-ops solve the basic economic problem, one small community at a time, using sustainable forms of agriculture.\nStockings, M., Niamh, M. (2000). Land Degradation. Guidelines for Field Assesment, 5, 59-67.\nHeilbriner, R., & Milberg W. (2008) The Making of Economic Society (12th Ed.). Saddle River, NJ: Pearson Education, Inc.\nSample, I (2007). Global food crisis looms as climate change and population growth strip fertile Land. The Guardian.\nRetrieved from http://www.guardian.co.uk/environment/2007/aug/31/climatechange.food. Buying Local Takes on New Meaning. (2011). Business & the Environment, 22(9), 1-4. University of Arizona, Growing Tomatoes Hydroponically Retrieved from http://ag.arizona.edu/hydroponictomatoes/overview New York Sun Works, Center For Sustainable Engineering. (2011). Retrieved from http://nysunworks.org/thesciencebarge\nBrightFarms. (2012) Better Food, Fresher Food. Retrieved from http://brightfarms.com/about/']	['<urn:uuid:f9e0ed6e-0c7e-4207-9f3e-f4d7f4b4ec43>', '<urn:uuid:f48335fe-9324-48ac-9ffa-30fc843b9511>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:31:27.499155	5	86	2521
5	What kitchen hygiene practices and safety precautions matter most?	For kitchen hygiene, properly clean work surfaces, knives, and cutting boards to prevent cross-contamination. Always wash hands before and after food preparation, especially when handling raw meat. Regarding moldy food like fruit, never attempt to salvage partially spoiled items - discard them entirely to prevent illness. Check seasonings by cooking small portions first. For safety, avoid crowded cooking spaces, use protective gear like oven mitts for hot items, and keep sharp implements properly maintained. Handle all raw proteins carefully and don't leave perishable foods at room temperature for extended periods.	['Cooking is a necessary and valuable asset to possess. Cooking food at home tastes great and is way cheaper.\nThis will help to maximize the flavor of your food be more flavorful.\nGet a cookbook with easy recipes or a specific theme you find interesting to get you started. Try looking online or to your local library or bookstore. Try making three or four of the recipes that appeal to you the most, and give yourself plenty of time to perfect your newly acquired skill.\nThere are ways to fix your hard work and make it so you are able to use it instantly. Start with about two tablespoons and add more cornstarch until you reach the desired consistency. Add this mix to the sauce while it simmers in order to thicken it up. The mixture needs to be added slowly and constantly stirred so that it doesn’t get too thick.\nYou don’t ever want to try new ingredients or recipes you have not already tried when you are cooking for someone that you want to impress. This will ensure that your meal preparation.\nYour meat need to be sliced thinly across the grain when you are preparing stir-fry. This can be quite tricky and time-consuming. When the meat is firm, but prior to it becoming completely frozen, remove it from the freezer. Slice the meat across the grain at an angle of 45 degrees.\nHave you ever needed to throw away moldy fruit away? Do you wonder if it would be better to just cut the rotten part and save the rest. You should never eat or keep a piece of fruit that has begun to rot. Mold grows inward to places that you see and it can make you sick so throw moldy fruit in the garbage.\nEat a piece of meat when experimenting with seasoning before cooking all of it. Certain foods like hamburgers, meatloaf and meatballs have certain kinds of seasonings. Never prepare the full amount after its initial seasoning. You should cook a smaller patty first, so that you can make sure that the seasoning is delicious. Doing this allows you to test the seasonings and make appropriate adjustments.\nDo you enjoy preparing dishes that contain fresh basil in your dishes? Put a bunch of fresh basil in a cup. Fill the glass with water so that the stems are covered. Put it on top of your counter and it will be fresh for up to seven days! The basil will grow roots if you change out the water regularly. You should also trim the basil so that it keeps growing.\nLook at your recipes before starting and clue in on the steps can be done ahead of time without having any spoilage issues. You can simplify the day prior to actually cooking. This takes a lot of the stress out of even the most complicated recipes and fast.\nA sharp knife is imperative when preparing food. Not only can it be difficult to cut with dull knives, but it is dangerous to use them too. It is more likely that you will cut yourself while trying to get a dull knife to cut something than when you use a well-sharpened knife.\nThey behave like a sponge that soaks up all the water. Wipe off each mushroom with a cloth to clean them.\nGarlic is a great ingredient but it can leave a strong smell on your hands. Try rubbing your hands on stainless steel sink after you work with the garlic or other potent ingredients. This will clean your hands and prevent the smell that can get onto to the food you handle next.\nAdd cauliflower to your mashes potatoes to make this dish healthier and tastier. Cauliflower’s bland taste makes it easier for it to blend with the potatoes, so that you can’t taste it, and it picks up other ingredients. At the same time, cauliflowers mash into the same color and texture as potatoes, giving you a fail-proof way to add vegetables and subtract calories from a classic mashed potato dish.\nDo you want to take the guesswork out of deciding how long to leave your meat? Use a meat thermometer (a digital one is more accurate) so that you can ensure the inside is cooked correctly.\nPlan on preparing a big pot of stock so that the excess can freeze and store it. Good chicken stock that you make at home makes a great starter for soups, casseroles, casseroles and more.\nWhen you are using wooden skewers in cooking, soak them for at least half an hour before putting the veggies on them. This can cut down on the chances of the wood becoming burnt and singed during the cooking process. Also, use two skewers through your food to help secure it, so it doesn’t end up on the ground.\nThis will prevent them from burning up while you are cooking. Use two parallel skewers instead of one skewer to avoid food intact.\nThis will allow your food the maximum amount of flavor possible.\nYou can prepare some of the ingredients for a dish the night before so that you can avoid becoming overwhelmed when you have to cook your family dinner. Plan ahead by chopping veggies or readying a marinade the night before. This will result in less stress the next day, and you will be more than ready to begin the cooking process.\nAlways properly measure the amount of cooking oil! This will help you reduce how much fat is present in your foods while cooking. This will allow you to keep track of oil you use.\nAlways select fresh garlic available when you have a recipe that calls for your dishes. A good rule of thumb for garlic: Fresher means sweeter.Fresh garlic is not soft and firm to the touch.\nAny time you are faced with a large amount of leftover turkey, don’t be tempted to throw it in the garbage. Cut up the remaining meat, place it in Ziploc bags or Tupperware, and store it in your freezer. Storing turkey in this method helps it stay fresh for weeks and allows you to use it again in the future for sandwiches or salads.\nStoring these items in warm locations will lead them to lose their flavor.\nMake the most of any leftovers after your holiday feast by reusing the leftover turkey. Cut the leftover meat and put it away in airtight container. Turkey remains fresh up to three weeks and you can use it in sandwiches or a salad.\nYou must keep your cooking tools in order to keep your recipes on track. If you do not organize them, you will be scrambling around to find where you put everything. Keep things that are similar in the same place. For instance, parsley and basil are both spices and should be in the exact same cabinet.\nAlways clean your cooking with utensils that are spotlessly clean. Any leftover food post-washing can spoil the dish you plan on cooking next. This can also spread of bacteria.\nIf you’d like to boost the flavors in your pasta sauce, it’s important to save the water you use to cook the pasta itself. You will need to save roughly one quarter cup of water. When you are getting ready to mix the pasta and sauce together, pour in some of the water. The combination of water and the starch from the pasta will give your sauce body and creaminess.\nFollow these tips and you will quickly develop better cooking skills and an affinity for good home cooking. Once you improve your cooking skills, you’ll notice how delicious and cost-effective cooking from home can be. Your entire family will benefit from the focus on healthy-eating habits that homemade meals can provide. Think of these cooking tips the next time you are in the kitchen.', 'The holidays are perfect for getting together with loved ones and partying merrily, and you don’t want to spoil your fun with any mishaps. Here are 18 of the most common health hazards that occur over the holiday season, and how to avoid them.\n1. Burns and Scalds\nWith all of the cooking and baking that happens around this time, it’s more than likely that someone will end up burning themselves on a hot baking pan, or get spattered by some stray grease.\nMake sure that you have a supply of pot holders, oven mitts, and dishcloths handy, and touch handles very lightly before grabbing them. Use grease-spatter covers when frying things, and ensure that you keep the kitchen from getting too crowded: too many cooks = inevitable injuries. Don’t even think about deep-frying a turkey.\nAs with the example mentioned above, cooking and baking can often result in cuts and scrapes. Chopping onions with a sharp knife, grating lemon rind for cookies…all can result in pain and bleeding. That’s not much fun at all, especially since you can risk contaminating dishes with your own blood.\nBe present and aware as you work, as a lack of focus is a primary cause of kitchen injuries. Keep a first aid kit handy in case of any cuts, and if you do manage to slice yourself, wear latex gloves afterward to minimize infection and contamination.\n3. Food Poisoning\nIf you’re working with any kind of animal protein (like turkey or ham), or various condiments that have been in the fridge for a while, you run the risk of spreading salmonella or any number of harmful moldy bits.\nDouble-check every canned or pre-packaged item before you use it, and be sure to wash every knife, cutting board, and work surface regularly to reduce the risk of cross-contamination. Wash your hands before and after prepping anything, and before serving as well. Don’t leave any meat or dairy products on counters or tables for too long, and if your egg nog has been sitting untouched for an hour, you might wish to pour yourself a fresh glass.\n4. Ornament Dangers\nIf you have small children or pets, stay away from breakable glass ornaments, tinsel, and anything sharp. Little hands, muzzles, and beaks can get hurt very badly by the very things you’re using to beautify your home.\nHugging and kissing relatives, shaking hands while schmoozing at office parties, touching sticky door handles while holiday shopping…all of these are perfect opportunities for illnesses to find their way into your body.\nMake sure that you wash your hands often, keep a bottle of hand sanitizer in your pocket, and keep your fingers out of your mouth, eyes and nose. If one of your relatives is hacking and sneezing, keep distance from them and just bellow at them that you love them dearly, but are in the process of getting over something or another and can’t afford to be re-infected.\nSure, stringing a bunch of lights all over your house creates a beautiful effect, but you won’t be able to enjoy it if you’re stuck in the emergency room.\nBe sure to have someone spot you when you’re up on a ladder, and only climb up during daylight hours. Use a ladder indoors as well, as even the sturdiest chair can be more precarious than you think.\n7. Slipping on Ice\nIce is sneaky, and likes to hide on stairs, sidewalks, and driveways, just waiting for an unsuspecting person to step on it.\nBe sure to use a sand/salt mixture on any treacherous bits of pavement, and when you walk across this areas, shuffle your feet to slide them across while keeping your center of gravity low: you’ll have less chance of falling, and can regain your balance if you slip around a little. Wear flat boots or shoes with good treads, and for goodness’ sake, don’t be an idiot and prance around outside in stiletto heels: save those for indoor parties.\n8. Chills and Frostbite\nGoing tobogganing with your nieces and nephews sounds like a great idea, but there may be some chilly consequences. Although you might feel warm while you’re being active, you may end up a bit cooler than you’re trying to be. Sub-zero temperatures can wreak havoc on your skin, and fat-free extremities like toes and fingers are particularly susceptible to frostbite and cold damage.\nWear layers that you can add and subtract as needed, be sure to wear gloves and mittens, invest in a good pair of long johns, and make friends with woolen socks.\nIt sounds odd to think of sunburns occurring in wintertime, but they’re actually quite common. People who spend part of their holidays skiing, snowboarding, or just playing outdoors can end up with some pretty nasty pinking around the face and ears.\nGet yourself a good, high-quality sunscreen and apply it liberally before going outside. Actually, if you’d like to decrease sun damage to your skin in general, you might like to apply a thin layer of sunscreen any time you plan to be outside for a while.\n10. Car Accidents\nSlippery roads, poor visibility (yay snow!), and countless other hazards are common at this time of year. Please drive safely and responsibly, and never, ever drive after you’ve been drinking.\nKeep an emergency kit in your car, along with a bag of sand, some blankets, and snacks, and never assume that the drivers around you are as responsible as you are.\n11. Toy Hazards\nDo you have any idea how many people manage to slice themselves open on hard plastic toy or gadget covers every year? Those things can be treacherous, as can many other toys out there.\nBe aware of where your gifts originate from, as it’s best to avoid those made with toxic chemicals and tiddly bits that can choke your young relatives, and open packages with care.\nCooking a holiday dinner for a dozen people while trying to ensure that the house stays tidy, your outfit stays immaculate, and your dog doesn’t eat the decorations can be absolutely maddening. In fact, heart attacks increase by a full third when the holidays roll around, and the number of people who have strokes, panic attacks, and nervous breakdowns also increases exponentially.\nDelegate as much as you can to other people, and remember that the folks who’ll be celebrating at your place are friends and family members who love you: it’s absolutely okay if the cranberry sauce is a bit lumpy.\n13. Alcohol Poisoning\nIt’s fun to have a couple of drinks while you’re partying, but chugging a bottle of whiskey on your own isn’t terribly good for you.\nAlcohol poisoning is easily avoided by drinking responsibly, having a glass of water for every alcoholic beverage consumed, and snacking on oily/fatty foods like deviled eggs, olives, nuts, and cheese.\nThat Norwegian spruce might look lovely, but if you have any tree allergies, you might end up having an asthma attack or breaking into hives. People with pet allergies may find themselves having a sneeze-fest if they visit a home where cats reign, and food allergies may rear their heads after bites into unfamiliar pastries.\nBe sure to keep a supply of antihistamines in the house, and a tube of hydrocortisone cream at the ready in case of a rash or hive breakout. If you have an anaphylactic reaction to anything (peanuts, mushrooms, etc.), keep an EpiPen handy and let your hosts know about your allergy ahead of time.\n15. Loneliness and Depression\nMany people are alone at this time of the year, and Christmas is just behind Valentine’s Day when it comes to suicide rates.\nIf you’re alone over the holidays and feeling down, consider volunteering at a soup kitchen or visiting with elderly folks or those in hospitals. Accept invitations to meals and celebrations when asked by friends or co-workers, and if things get really bad, remember that there is always help available to you.\nChildren and pets are most at risk for electrocution, as sparkly holiday lights can look so tempting to gnaw upon, but you’d be surprised at how many adults get hurt this way as well.\nKeep outdoor lights unplugged until you’ve arranged them the way you like, and don’t let Grandpa use a knife to fish burnt crumpets out of the toaster.\nMany people light their fireplaces and wood stove around this time of year, and candles are common decorations in many households.\nKeep the doors of wood stoves closed while logs are burning, and make sure that kids aren’t tempted to throw miscellaneous bits into them just to see whether they’ll burn or not. Never leave a candle burning unattended, keep matches and lighters away from little hands (and pyromaniacs), and use a fireplace screen to keep sparks away from the carpet.\n18. Pet Poisoning\nThe last thing you want to do is accidentally poison your pet, so be sure to do your research on which plants and foods are toxic to your furred or feathered friend.\nKeep yew, lilies, and poinsettias out of the house if you have any pets, and don’t share your holiday dinner with your dog, cat, or bird until you’re certain that what you’re eating won’t have any adverse effects. Some tidbits that you may find delicious (like chocolate or avocado) can have really nasty effects on their health.']	['<urn:uuid:ab8ba8c2-8dc1-4a06-9764-f470ce1b28ce>', '<urn:uuid:689993cf-02f4-4e51-9af5-703da9ae4dbd>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T04:31:27.499155	9	90	2873
6	What storage conditions ensure microbial DNA stability in iSWAB-MB samples?	Microbial DNA collected in iSWAB-Microbiome devices remains stable for 40 days at 15-30° C. For longer storage periods, the samples can be stored at -20 to -80° C.	['Microbial content and diversity within collected biosamples can provide a wealth of clues about human and animal health. However, current microbiome collection methods subject the samples to harsh and stressful stabilization techniques such as freezing or harsh organic solvents. Both stabilization approaches have a high probability of altering the microbial representation in the sample, which could offer misleading results and hinder proper modeling or data analysis. Freezing the collected sample requires well-monitored cold chain storage and transport, increasing associated costs. Transport alone may alter the microbial representation from point of collection to processing. Thawing samples requires pre-processing which affects DNA quality and somewhat limits re-culturing of samples, therefore affecting proper modeling or data analysis. The use of toxic organic solvents requires strict regulations for transport and handling procedures, they also alter microbial representation from the point of collection to processing, limiting DNA and RNA analysis. The iSWAB-MB offers a non-toxic, non-organic solvent, free from hazardous fixatives and detergents, allowing the microbial representation to remain intact from the time of collection to several weeks at room temperature (real time testing ongoing).\n*The products shown contain the CE Mark and are approved for IVD use in countries recognizing this standard. In the USA, products are designated For Research Use Only and not for use in diagnostic procedures and will be labeled accordingly.\nDevices only for collecting microbial/viral DNA and RNA from oral, nasal, soil, skin, and fecal samples. Microbiome analysis (from Fecal, skin, vaginal, soil, etc.) qPCR, microarray, and NGS for microbiome research in health, wellness, and forensics.\nThe iSWAB™-Microbiome (iSWAB™-MB) collection device consists of a collection device that is pre-filled with 1.0 mL of the iSWAB™-MB non-toxic, stabilizing buffer and fitted with a proprietary insert. The insert is designed to optimize the release of specimens collected with swabs into the stabilizing buffer, creating a minimal footprint and allowing for swab-free transport of specimens. The iSWAB™-MB buffer is designed to maintain the status quo of samples at the time of collection, providing a representative snapshot of the microbial community that remains unchanged from the point of collection to processing.\niSWAB™-Microbiome collection device is a non-invasive, non-toxic sample collection technology intended for stable ambient transport and storage of microbial/viral DNA and RNA from various sample types including fecal, vaginal, skin, soil, nasal, oral, and saliva. Purified microbial/viral DNA or RNA isolated from collected samples are compatible with qPCR, microarray, and NGS for microbiome research in health, wellness, and forensics.\nThe iSWAB™-Microbiome sample collection technology is intended for specimen collection of biological samples. It is a non-invasive, non-toxic sample collection method and is intended to be used for long-term room temperature, stable transport, and storage of microbial/viral DNA and RNA from various sample types including fecal, vaginal, skin, soil, nasal, and oral. The iSWAB-Microbiome device is designed to recover and stabilize collected samples from a nylon flocked or molded swab head and then the swab is discarded at the point of collection. For saliva collection, only a spit funnel is needed to facilitate the collection of saliva into the iSWAB™-Microbiome device\nSample stability and storage, post-collection: 21 days for microbial RNA and 40 days for microbial DNA at 15-30° C; if longer storage time is required, collected samples can be stored at -20 to -80° C.\nFor external use and single use only. DO NOT drink, touch, or remove the buffer from the device. Do Not use if device is broken, leaking, or visibly damaged. These products should only be used in accordance with the instructions provided. Avoid eye contact with the liquid in the device. IF IN EYES: Rinse cautiously with water for several minutes. IF INGESTED: Call POISON CENTER. Rinse mouth. Not a Hazardous Combustion Product. Device cap can be a choking hazard. Do not ingest.']	['<urn:uuid:43f7e66c-dcf7-4ad5-9afb-298ec5231c1e>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:31:27.499155	10	28	624
7	I've got some wood ash from my fireplace and want to use it in my garden. What types of wood make the best ash for gardening, and what materials should I avoid burning if I want to use the ash as fertilizer?	Hardwoods like oak and beech make the best ash for gardening as they contain more nutrients. You should avoid using ash from coal, treated timber, or materials like paper plates and newspaper kindling. These non-organic materials can be harmful to your soil and plants as they may contain toxic additives. It's important to only use ash from dry, untreated wood without any additional foreign materials.	['If you have a fireplace or woodstove, use wood ashes to improve garden soil! Wood ash is full of nutrients that plants need, such as potassium and phosphorus, so it’s great for using on the vegetable garden. But it’s important to know where not to use it, too. In this short video, we show you when, where and how much wood ash to apply to keep your plants in tiptop condition.\nWood ash is particularly useful for fruiting plants.\nAsh from hardwoods like oak and beech are best as they contain more nutrients than ashes of softwoods like pine. Avoid using the ash from coal or treated timber, which could be harmful to your soil and plants.\nThe nutrients in wood ash are soluble. They must be kept out of the rain so they don’t wash out. Use a container with a close-fitting lid to keep your wood ash dry until you’re ready to use it.\nComposting Wood Ash\nWood ash is alkaline, so it can help to reduce the acidity of a compost heap. This creates better conditions for composting worms and results in compost that is perfect for mulching around vegetables.\nAdd thin layers of wood ash to your compost heap no more frequently than every six inches of material.\nUsing Wood Ash on Garden Soil\nMost vegetables need a pH of 6.5 to 7.0. If your soil’s below 6.5, fork or rake wood ash to help raise the pH. You can test your soil using an inexpensive test kit to find out its pH.\nWood ash is about half as effective as lime in neutralizing acid. As a general rule, scatter about two ounces of ash to every square yard. Do this on a still day in winter and wear gloves to protect your hands.\nBrassicas such as cabbage and Brussels sprouts are best grown in a more alkaline soil. Apply wood ash the winter before planting, or around actively growing plants.\nWood ash is high in potassium, which helps flowering and fruiting, so it’s ideal to use around most fruit bushes and around fruiting vegetables such as tomatoes.\nAvoid using wood ash around plants that require an acid soil such as blueberries. Don’t let it come into contact with seedlings or use on potato beds, as alkaline soil encourages potato scab.\nWood ash would need to be used in huge quantities to make your soil too alkaline for most other crops, but it would be worth re-testing your soil’s pH every two years to check it doesn’t go above 7.5.\nTaken from https://www.almanac.com/video/how-use-wood-ash-vegetable-garden?trk_msg=HF7ML22L6JN4VB30C98TJIPBDC&trk_contact=I366NNSLAJU6KS9H67PLAT219S&trk_sid=86D5A9EQV3MUET4I2BGNVKJLQK&utm_source=Listrak&utm_medium=Email&utm_term=How+to+Use+Wood+Ash+in+the+Vegetable+Garden+(title)&utm_campaign=Companion+Daily\n-Using Wood Ash in the Garden\nWe’ve accumulated a lot of wood ash over the winter. Can we add it to our garden soil or compost pile? wood ash in the garden\nWhether using wood ash in the garden is a good idea depends on your garden soil’s pH and fertility levels. If a soil test has shown your garden soil’s pH to be below 6.0 (meaning it’s moderately acidic), adding wood ash could be beneficial, says Garn Wallace, who holds a doctorate in biochemistry and is general manager of Wallace Laboratories in El Segundo, Calif.\nIn acidic soils, wood ash can increase soil fertility by increasing the availability of phosphorus and potassium as well as some micronutrients — although wood ashes won’t supply any nitrogen. Also rich in calcium, wood ashes are effective for raising soil pH — a potential benefit in places where pH is below the ideal level for most vegetables (6.0 to 7.0).\n“Moderation is the key,” Wallace says. “People tend to over-apply nutrients. And after you add something to the soil, you can’t take it away without replacing the soil. If you apply wood ashes without a soil test, it is possible to ruin soil in just one year.”\nIf, after testing your soil pH, you decide to add wood ash, start with a thin dusting across the soil surface, then work the ashes deeply into the topsoil, because most nutrients won’t move much in the soil. A Purdue Extension publication suggests that gardeners whose soils are below a pH of 6.5 can safely apply 20 pounds of wood ashes per 100 square feet if the ash is worked into the soil about 6 inches.\nThe following year, test the soil pH and nutrients again. If the pH is still low, work in another thin layer as you did the previous year. When your soil pH has reached 6.5 to 7.0, stop adding wood ash. If you add too much wood ash, you risk raising the pH over the neutral 7.0 to 7.2 range, which can tie up essential nutrients in the soil. Continue to test the soil every two to three years, and adjust soil amendments according to the test results.\nAs for composting the wood ash, it depends on what stage your compost pile has reached. Adding small amounts of ashes to a new compost pile is probably OK. If the compost is at or near maturity, however, adding wood ash would raise the pH and could increase the availability of heavy metals to harmful levels. “You want these minerals in minute amounts — too much of them is never a good thing,” Wallace says.\nOne last caution: Never use ashes from treated wood in your garden. Treated wood contains copper, arsenic, chromium and sometimes boron, and ashes that contain these heavy metals could harm soil, plants and animals.\n— Vicki Mattern, Contributing Editor\nTaken from https://www.motherearthnews.com/organic-gardening/using-wood-ash-in-the-garden-zb0z1303zsor\nTill next time this is Becky Litterer, Becky’s Greenhouse, Dougherty Iowa firstname.lastname@example.org 641-794-3337 cell 641-903-9365', 'Both urea fertilizer and wood ash are known to provide essential nutrients and growth benefits for gardens and lawns alike. When comparing the two, you’ll find some differences that create obvious advantages for each one, depending on the plant type.\nWood ash is an excellent fertilizer for alkalizing soil. It is an excellent source of potassium but can only be applied by sprinkling the ash on the soil. This organic fertilizer can work even when dry or with little moisture. In addition, you need significantly less ash than urea in your garden.\nIn contrast, urea fertilizer does a great job of increasing acidity. Urea fertilizer is a synthetic or organic material that offers a high source of nitrogen and can be applied in solid, liquid, or spray forms. However, the solid form requires adequate moisture to work.\nWhile both fertilizers are great resources for a healthy garden, using them may be detrimental to your plant’s growth if used on the wrong plant type. Read on for more differences between urea fertilizer and wood ash, so you can determine which fertilizer would benefit your garden.\n1. They Contain Different Primary Nutrients\nMost fertilizers are made up of three primary nutrients: nitrogen (N), phosphorus (P), and potassium (K). While many all-purpose fertilizers offer a healthy balance of all three nutrients, both urea fertilizers and wood ash provide a different approach—focusing on one key nutrient.\nUrea fertilizer has an extremely high nitrogen (N) component, which is great for green, leafy growth, cell regeneration, and creating protein and amino acids.\nWood ash, on the other hand, has a high potassium (K) content, which is essential for moving water and other nutrients throughout the plant system to provide balanced, healthy growth. Potassium is key to successfully moving through the process of photosynthesis and can cause stunted growth if deficient.\n2. They Have Opposite Effects on Soil pH\nDifferent types of plants have varying requirements for soil pH levels and nutrient content. Some require a high level of acidity, while others require soil that is more alkaline. Specific fertilizers like urea and wood ash are more appropriate for each of these needs.\nUrea fertilizer increases the acidity of the soil. Because of its high nitrogen content, it quickly supplies ammonia to the soil, causing an acidic reaction.\nSome examples of plants that often prefer an acidic environment are:\n- Holly plants\n- Magnolia trees\n- Sweet potatoes\nSome examples of plants that prefer an alkaline environment are:\n- Aloe vera\nDetermining your plant type and following up with a soil test will provide you with the context you need to know which kind of fertilizer and how much will work well in your garden.\n3. They’re Distributed Differently\nOne of the advantages of urea fertilizer is that it can be distributed in a variety of ways.\nUrea fertilizer can be applied in the following ways:\n- Solid: Sprinkled directly on or into the soil, the solid form of urea fertilizer mixes with the soil as it’s watered.\n- Liquid: By mixing the solid form with water, this solution can be applied by pouring directly onto the soil with a watering can.\n- Spray: Some plants do well with foliar spray. The plant’s foliage can be sprayed directly to soak up this nitrogen-rich nutrient by pouring the liquid solution into a spray bottle.\nIn comparison, wood ash is distributed by sprinkling the ash around the base of the plants and applying a thin layer over the top of the soil. No liquid or spray option is available for effective wood ash distribution, making urea fertilizer a bit more versatile in this regard.\n4. Wood Ash Is Organic, Urea Fertilizer Is Usually Synthetic\nIt’s no secret that fertilizing with organic options is an excellent tool for maintaining an environmentally-friendly garden. Still, not all organic options are as affordable or effective as synthetic fertilizers.\nFortunately, wood ash is a great, low-cost way to fertilize without harmful added chemicals. In fact, it’s likely you already have some wood ash that you can use at no additional cost. Next time you clean out your fireplace, instead of throwing the ash away, hang on to it for a cost-effective, beneficial garden fertilizer.\nThe type of wood you burn in your fireplace is also important, as different types of wood offer varying benefits as fertilizer. I’ll discuss this more later in the article.\nUrea fertilizer is most commonly a synthetic compound made up of inorganic materials and formulated into granules or prills. Less commonly, urea fertilizer can be made organically by using human or animal urine as fertilizer directly or combined with compost.\nWhen making organic urea fertilizer, it’s important to be sure that it doesn’t spill or leak into water systems, as it can be toxic and contaminating in some cases. The most effective and safe way to create organic urea fertilizer is to combine it with your compost heap and allow it to become part of the composting process, using the compost as one fertilizer when complete.\n5. They Need Different Moisture Levels to Work\nWhen wood ash is applied, it should be either completely dry or composted along with your compost heap. It immediately begins its work alkalizing and providing potassium to the soil with little moisture. Dry wood ash can also work to repel pests like slugs and snails.\nOnce it rains, the nutrients can be washed away, and new ash must be applied. Additionally, when wood ash gets wet, lye and salt are released, which can sometimes be harmful to plants. Because of this, it’s important to use the appropriate amount of wood ash so as not to create a toxic environment for your garden once it inevitably gets wet.\nIn contrast, urea fertilizer must be wet in order to fertilize the soil. Once urea fertilizer is dispersed, it begins to break down, and the ammonia will become unavailable to the soil. It’s essential to time urea fertilizer application with sufficient rainfall or be prepared to adequately water the garden yourself.\nThe combination of fertilizer and water allows an essential chemical reaction to occur, releasing the nitrogen into the soil.\n6. They Have Varying Concentrations of Lime\nWood ash naturally contains a healthy source of lime. Lime is usually made of ground limestone and can be found naturally occurring in wood, particularly oak. Lime is beneficial to soil due to its ability to improve water penetration, allowing the plants to more easily receive the nutrients needed to thrive.\nSome synthetic urea fertilizers can be purchased with lime additives, providing both nitrogen and lime benefits to the soil. Still, both synthetic and organic urea fertilizers on their own don’t typically contain beneficial levels of lime.\n7. You Need Less Ash Than Urea to Fertilize a Garden\nWhen it comes to applying fertilizer, wood ash and urea fertilizer require vastly different amounts to provide the essential nutrients they contain.\nThe National Gardening Association suggests 2 pounds (907 g) of nitrogen per 1,000 square feet (92.9 sqm), while potassium should be applied at one-tenth of that amount.\nBecause healthy soil requires so much less potassium than it does nitrogen, it’s necessary to apply more urea fertilizer (nitrogen-boosting) than wood ash (potassium-boosting).\nYou should also consider the amount of nitrogen in the particular form of urea fertilizer that you choose. While all are notably high in nitrogen, some are lower than others, and this should be taken into consideration when determining the amount of fertilizer to apply.\nBecause wood ash is organic, it’s impossible to know the exact content of potassium in the substance. However, you can make an educated guess based on the type of wood that was burned.\nFor example, wood ash that’s derived from oak usually has the highest potassium and lime content. The wood you burn should always be dry, untreated wood without any additional foreign materials.\nIn order to maintain the integrity of your wood ash, don’t allow any impure materials to be burned in when making wood ash for your garden, including:\n- Paper plates\n- Newspaper kindling\n- Other non-organic materials\nThese types of materials can interfere with the fertilizer and create more harm than good with potentially toxic additives.\nUrea fertilizer and wood ash are both excellent fertilizers for your garden, depending on the type of plants you’re growing.\nUrea fertilizer is great for increasing the acidity and lowering the pH of your soil, allowing acid-loving plants like strawberries, azaleas, and rhododendrons to thrive. Its high nitrogen content supports green, leafy growth- especially in the grass of your lawn.\nWood ash is terrific for providing a costless, organic fertilizer for your garden. Its high levels of potassium and lime increase the pH of the soil by alkalizing it and creating an ideal environment for garlic, spinach, lettuce, and more!']	['<urn:uuid:ec4014f5-3194-4935-bbcf-3ad1f5e23b57>', '<urn:uuid:d2b45ce9-f4a5-4d8f-82ed-78bd34a9fbf2>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	42	65	2391
8	safeguards protect antitrust whistleblower program abuse	Several safeguards exist to prevent abuse of antitrust whistleblower programs. The Antitrust Division has standards for opening grand jury investigations that help screen out fabricated claims. False accusations would fail to be corroborated by other evidence and could result in criminal charges for making false statements. Additionally, for culpable individuals, becoming a whistleblower requires first obtaining immunity or a non-prosecution agreement, which the Antitrust Division can deny to very culpable executives or negotiate terms that preclude them from seeking whistleblower rewards.	['Objections to an Antitrust Whistleblower Statute\nThe idea of an antitrust whistleblower is not new, but it has never gained much traction in the past. There have been significant objections, or at least disinterest—particularly from the Department of Justice. The mood seemed to be “Our cup runneth over with Amnesty applications so let’s not screw this thing up.” But, perhaps times have changed. Our analysis is that the objections to a whistleblower statute were either superficial, or when having merit, still not enough to outweigh the benefits of a whistleblower statute.\nBefore considering some of the possible downside to an antitrust whistleblower statute, a little explanation of what we have in mind may be helpful. We propose an SEC-style whistleblower statue where an informant can be awarded a level of compensation (bounty) when information of illegality leads to charges and recovery by the SEC. This is different than a False Claims Act qui tam case where a Relator brings a case in the name of the government alleging the government has been defrauded. In fact, an antitrust whistleblower statute is needed because a qui tam case is not generally available in price-fixing matters since it is the private sector, not the government that has been harmed.\nConcerns About an Antitrust Whistleblower statute\nIt’s worth noting that the Criminal Antitrust Anti-Retaliation Act has been passed twice unanimously by the Senate in the last two Congresses and is up for vote again on the Senate floor. It will no doubt pass—most likely again unanimously. There is agreement that a person who reports criminal antitrust activity should not face retaliation in the workplace. (Despite the consensus, the House has failed to take up this bill the last two times it has passed the Senate). There is controversy, however, about whether a whistleblower should be eligible for some type of bounty if the information leads to successful cartel prosecution and the imposition of fines.\nIn 2011, the General Accounting Office Published a report on Criminal Cartel Enforcement that reported stakeholders’ views on a possible antitrust whistleblower statute (here). This is a summary of the GAO findings:\nThere was no consensus among key stakeholders GAO interviewed–antitrust plaintiffs’ and defense attorneys, among others–regarding the addition of a whistleblower reward, but they widely supported adding antiretaliatory protection. Nine of 21 key stakeholders stated that adding a whistleblower reward in the form of a bounty could result in greater cartel detection and deterrence, but 11 of 21 noted that such rewards could hinder DOJ’s enforcement program. Currently, whistleblowers who report criminal antitrust violations lack a civil remedy if they experience retaliation, such as being fired, so they may be hesitant to report criminal wrongdoing, and past reported cases suggest retaliation occurs in this type of situation. All 16 key stakeholders who had a position on the issue generally supported the addition of a civil whistleblower protection though senior DOJ Antitrust Division officials stated that they neither support nor oppose the idea.\nThe GAO report is several years old and it may be that positions have been reevaluated. For example, I think the Antitrust Division today would support the anti-retaliation measures in whistleblower statute. But below is an analysis of some of the objections raised to making a bounty available to an antitrust whistleblower.\nThe Antitrust Division’s principal concern was that jurors may not believe a witness who stands to benefit financially from successful enforcement action against those he implicated. GAO Report p. 39. But, a whistleblower is highly unlikely to ever be a principle witness at a trial. An antitrust crime typically involves many culpable actors. A whistleblower would generally “get the ball rolling” and provide evidence that will turn other witnesses, and allow subpoenas and search warrants from target companies. Further, a single whistleblower who might receive a financial reward seems no less credible than witnesses from an amnesty company where everyone—including the highest-ranking culpable executives—will have escaped criminal prosecution. Also, criminal antitrust trials are relatively rare—almost all cases are resolved by pleas. Finally, it is not logical to worry about the credibility of a witness you would otherwise not even know about absent a whistleblower statute.\nA Whistleblower Reward Could Result in Claims That Do Not Lead to Criminal Prosecution:\nThere was some fear expressed in the GAO report that would-be whistleblowers would fabricate information in order to conjure up a cartel in the hopes of collecting a reward. GAO Report p. 40. Anything is possible, but the Antitrust Division folks are pretty savvy and have standards for opening grand jury investigations. Moreover, the possibility of fabricated charges exists today with a company applying for leniency in the hopes of knee-capping competitors who would have to deal with a criminal cartel investigation. The reality is a “false accusation” simply wouldn’t be corroborated by anyone else and could land the accuser in jail for making a false statement.\nIn a similar vane, concern was expressed that a whistleblower statute may result in a deluge of complaints to the Antitrust Division that would take additional resources to sift through. This seems like a good problem to have. When Ms. Justice and I were at the Division, we received a fair number of complaints that amounted to no more than oligopoly pricing. It did not take too much time to ask: “What else ya got?”\nUndermining Internal Compliance Programs:\nA concern that seems more legitimate is that a whistleblower reward could undermine companies’ internal compliance program. GAO report p. 42. An employee who learns about cartel behavior may decide there is more in it for her to go directly to the Antitrust Division than to report the information to company counsel. This seems like a plausible possibility, though it’s hard to envision how often it might happen. An employee may choose to go to the Antitrust Division because if the company is involved in a cartel, the employee may conclude the “compliance” program is not really working and it may be dangerous to report the information internally. But, even if the employee does go directly to the government, all is not lost in terms of a compliance effort. The Antitrust Division may decide to approach the company in question with the information and seek to flush out a leniency application—known as affirmative amnesty. Or, if the compliance program truly was robust, and the employee just chased the possible “whistleblower bounty” the Antitrust Division could still recognize the company’s compliance efforts in charging/sentencing decisions. But, in short, if the whistleblower statute is working, and a cartel is exposed by an employee who came to the government instead of reporting to company counsel, the balance of equities seems in favor of exposing and prosecuting the cartel—and dealing with the company’s compliance efforts in some other way.\nWrongdoers Should Not Be Rewarded\nThe GAO report did not address perhaps the strongest objection to an antitrust whistleblower statute, namely that wrongdoers should not be rewarded. Under the whistleblower legislation we propose, certain “members” of a cartel may be eligible for a whistleblower reward. A cartel almost always has many participants; both in terms of companies involved and number of people involved within a company. A typical cartel requires the participation, or at least acquiescence, of a company’s senior, highly-culpable executives. But the day-to-day execution of the cartel typically is left to subordinates who are directed to attend meetings, communicate with competitors and monitor the implementation of the scheme. Cartels often have their own nomenclature signifying culpability— “top level meetings,” “working group meetings,” and “Master and Sherpas,” to name a few. These lower level executives could make ideal whistleblowers. If one whistleblower gets immunity and a potential financial reward, there will still be many more culpable conspirators to prosecute.\nUnder conspiracy law, if an estimator, or a salesperson knows his company is involved in a price-fixing agreement, he is liable as a “coconspirator” if he takes a single act in furtherance of the conspiracy (prepares a bid; quotes a fixed price). Consequently, virtually 100% of witnesses who cooperate with the Antitrust Division demand immunity. But, these “culpable” individuals would make ideal whistleblowers. A financial reward to low-level cartel participants—given the considerable expenses such a witness will incur in providing cooperation seems, at least to us, a reasonable exchange.\nThere should be no concern that a senior member of a cartel, a “Master,” could obtain a whistleblower reward. The first step in becoming a whistleblower for one who has some criminal exposure is to obtain immunity or a non-prosecution/cooperation agreement. This “dance” with the Antitrust Division requires an experienced (and expensive) attorney [one of the reasons people do not come forward without some hope of financial reward]. The Antitrust Division would either simply not grant non-prosecution protection to such an individual, or it could negotiate an agreement that precludes a very culpable executive from seeking a whistleblower reward. Also, keep in mind that the whistleblower reward would be a “bounty” yet to be determined. It could be modest—not like a qui tam recovery where the Relator gets a percentage of the government’s recovery.\nThere are some legitimate concerns around having an antitrust whistleblower bounty for actionable information that leads to the discovery of and prosecution of a cartel. But, there are ways to ameliorate the legitimate concerns, and none of these concerns outweigh the benefit of adding another tool to the government’s ability to detect and prosecute cartels—and deter them from forming in the first place.\nThanks for reading.\nThis post originally appeared in the Cartel Capers blog.']	['<urn:uuid:d97f8557-4e7c-41c0-8fcc-cf41547776eb>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	6	81	1583
9	percentage species loss tropical forests complete disturbance across animal groups	If Earth's remaining tropical forests are completely disturbed, more than 18% of species will be lost in every studied group except large mammals and mosquitoes. Seven groups will lose over 28% of species. Specifically, trees could lose up to 30% of species, while ants could lose as much as 65% of species.	['May 8th, 2017\nBy: Mike Gaworecki\nA 2015 study found that humans activities are driving species loss at a rate 100 times faster than historical baseline levels — which the researchers behind the study characterized as a conservative estimate. This finding fueled speculation that we’re currently witnessing a sixth global mass extinction event.\nNew research published in the Proceedings of the National Academy of Sciences (PNAS) provides further evidence that, even if we haven’t already entered a sixth era of mass species loss on a global scale, it may yet be imminent.\nJohn Alroy, a professor of biological sciences at Australia’s Macquarie University, examined local-scale ecological data in order to forecast potential global extinction rates and found that hundreds of thousands of species are at risk if humans disturb all pristine forests remaining in the tropics. “Disturbance is no small matter, because roughly two-thirds to three-quarters of all the world’s species are found in tropical forests even though tropical forests only cover about 10 percent of the entire Earth’s continental area,” Alroy said in a statement.\nScientists have long believed that the rate at which we are destroying tropical forests, and the habitat those forests represent, could drive a global mass extinction event, but the extent of the potential losses has never been fully understood.\nMass extinction will occur primarily in tropical forests because Earth’s terrestrial biodiversity is so heavily concentrated in those ecosystems, Alroy notes in the study. In order to examine just how severe the impacts might be, he applied a highly accurate method of estimating species richness to data from 875 ecological samples of trees and 10 other groups of organisms “of keen ecological interest,” including bats, insects (ants, butterflies, mosquitoes, and scarabs), large and small mammals, and other vertebrates (birds, frogs, and lizards). The samples were collected in a variety of habitat types in tropical zones that were originally forested, from primary and fragmented forests to plantations and pasturelands.\n“About 41% of the tree and animal species in this dataset are absent from disturbed habitats, even though most samples do still represent forests of some kind,” Alroy writes in the study.\nAlroy projects that, if Earth’s remaining tropical forests are completely disturbed, more than 18 percent of species will be lost in every group studied except large mammals and mosquitoes. Seven of the groups he examined will lose greater than 28 percent of species. Trees, for instance, stand to lose as much as 30 percent of species, while ants could lose as much as 65 percent.\n“The overall implication of this research is that any substantial loss of primary forests will result in numerous extinctions across many groups,” Alroy said. He added that there is good reason to regard his estimates as conservative, as well, and that the full impacts of human activities on species survival could be more severe than he has predicted: “Even if we preserve forests of some kind in many places, unless we protect them from ever being logged, those forests may end up being empty.”\nRead More at MONGABAY.']	['<urn:uuid:f0ab7c10-028c-4964-be09-4710920c9cec>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	10	52	508
10	wooden patten manufacturing materials green alternatives	Historical pattens were manufactured using woods like alder, willow or poplar. Today, green nanotechnology offers alternatives to traditional manufacturing by using renewable materials and environmentally friendly processes like supercritical CO2, water, or ionic liquids instead of volatile organic solvents.	"['This text was copied from Wikipedia on 28 November 2015 at 3:24PM.\nPattens are protective overshoes that were worn in Europe from the Middle Ages until the early 20th century. Pattens were worn outdoors over a normal shoe, had a wooden or later wood and metal sole, and were held in place by leather or cloth bands. Pattens functioned to elevate the foot above the mud and dirt (including human effluent and animal dung) of the street, in a period when road and urban paving was minimal.\nThe word patten probably derives from the Old French pate meaning hoof or paw. Women continued to wear pattens in muddy conditions until the nineteenth or even early 20th century. In appearance, they may resemble contemporary clogs or sandals, but though historical usage was apparently not always consistent, the term now is used only to describe protective overshoes worn over another pair of shoes.\nPattens were worn during the Middle Ages outdoors and in public places over (outside) the thin soled shoes of that era. Pattens were worn by both men and women during the Middle Ages, and are especially seen in art from the 15th century: a time when poulaines, shoes with very long pointed toes, were particularly in fashion.\nMedieval pattens were known in English by the terms: \'patyns\', \'clogges\', and \'galoches\', but the original shades of meaning and distinction between these terms is now unclear. Medieval and Early Modern overshoes are now all usually referred to as \'pattens\' for convenience.\nThere were three main types of pattens: one with a wooden \'platform\' sole raised from the ground by either with wooden wedges or iron stands. The second variant had a flat wooden sole often hinged. The third type had a flat sole made from stacked layers of leather. Some later European varieties of these pattens had a laminated sole: light wooden inner sections with leather above and below. In earlier varieties of pattens, dating from the 12th century on, the stilt or wedge variety were more common. From the late 14th century, the flat variety became increasingly common. Leather pattens became fashionable in the 14th and 15th centuries, and in London appear to have begun to be worn as shoes over hose in the 15th century, spreading to a much wider section of the public. Most London patten soles were constructed of alder, willow or poplar woods.\nIn 1390, the Diocese of York forbade clergy from wearing pattens and clogs in both church and in processions, considering them to be indecorous: ""contra honestatem ecclesiae"". Conversely, the famous Spanish rabbi Solomon ben Abraham Ibn Adret, ""the Rashba"", (c. 1233 – c. 1310) was asked if it was permissible to wear ""patines"" on Shabbat, to which he replied that it was the custom of ""all the wise in the land"" to wear them, and certainly permitted.\nSince shoes of the period had thin soles, pattens were commonly used mainly because of unpaved roads and also that indoor stone floors were very cold in winter. Furthermore, refuse in cities – animal especially horse dung and human effluent (from chamber pots)– was usually thrown directly into the street (often with minimal advance warning). Making full foot contact with such an unpleasant surface was, understandably, highly undesirable. Thus, pattens tended to only make contact with the ground through two or three strips of wood and raised the wearer up considerably, sometimes by four inches (ten centimetres) or more in contrast to clogs which usually have a low, flat-bottomed sole integral to the shoe.\nEarly Modern period\nA later pattern of patten which seems to date from the 17th century, and then became the most common, had a flat metal ring which made contact with the ground, attached to a metal plate nailed into the wooden sole via connecting metal, often creating a platform of by several inches (more than 7 centimetres). By this time men\'s shoes had thicker soles and the wealthier males (the gentry or gentlemen) commonly wore high riding boots, thus pattens seem only to have been worn by women and working-class men in outdoor occupations. Since dress hems extended down to the feet for most of this period, it was necessary to raise the hem above the ground to keep the dress clean even in well-swept and paved streets. The motto of the London Worshipful Company of Pattenmakers, the former representative guild for this trade, was and remains: Recipiunt Fœminæ Sustentacula Nobis, Latin for Women Receive Support From Us. The 19th-century invention of cheap rubber galoshes gradually displaced the patten, as did the widespread use of urban paving, especially elevated, paved pathways only for pedestrians- the now ubiquitous pavements or hard road surfaces.\nEtiquette and Practicality\nWearing of pattens inside church was discouraged, if not outright forbidden: perhaps because of the noise they made, the oft-commented ""clink"" being the consensus term for the sound; Jane Austen wrote of the ""ceaseless clink of pattens"" referring to life in Bath. To talk excessively and too loudly was coined to be as if one: ""had your ""tongue run (or go) on pattens"", used by Shakespeare and others. In houses, pattens were taken off with hats (for men) and overcoats upon entering, not doing so being considered rude and inconsiderate by bringing dirt inside - literally a faux pas or wrong step. The aunt of the Brontë Sisters, Miss Branwell, seems to have been considered notably eccentric for wearing her pattens indoors:\n|“||...she disliked many of the customs of the place, and particularly dreaded the cold damp arising from the flag floors in the passages and parlours of Haworth Parsonage. The stairs, too, I believe, are made of stone; and no wonder, when stone quarries are near, and trees are far to seek. I have heard that Miss Branwell always went about the house in pattens, clicking up and down the stairs, from her dread of catching cold.||”|\nPattens were not always easy to walk in, and despite their practical intention, literary evidence suggests that they could appear, at least to males, as a further aspect of feminine frailty and dependency. Samuel Pepys recorded in his Diary for January 24, 1660:\n|“||Called on my wife and took her to Mrs Pierce\'s, she in the way being exceedingly troubled with a pair of new pattens, and I vexed to go so slow.||”|\nFrom the Middle Period Poems of John Clare (1820s):\n|“||She lost her pattens in the muck\n(""hitops"" are high boots)\n|“||...he saw before him the trim figure of a young woman in pattens, journeying with that steadfast concentration which means purpose and not pleasure. He was soon near enough to see that she was Marty South. Click, click, click went the pattens; and she did not turn her head.\nShe had, however, become aware before this that the driver of the approaching gig was Giles. She had shrunk from being overtaken by him thus; but as it was inevitable, she had braced herself up for his inspection by closing her lips so as to make her mouth quite unemotional, and by throwing an additional firmness into her tread.\n""Why do you wear pattens, Marty? The turnpike is clean enough, although the lanes are muddy.""\n""They save my boots.""\n""But twelve miles in pattens--\'twill twist your feet off. Come, get up and ride with me.""\nShe hesitated, removed her pattens, knocked the gravel out of them against the wheel, and mounted in front of the nodding specimen apple-tree.\nOther uses of the term\nThe word could also be used as a term for a wooden soled shoe, that is a chopine or clog, as opposed to an overshoe, until at least the nineteenth century. The word was also used for the traditional wooden outdoor shoes of Japan and other Asian countries. What are in effect snowshoes for mud, as used by wildfowlers, boatmen, and Coast Guards may also be called pattens, or ""mud-pattens"". These are shaped boards attached to the sole of a shoe, which extend sideways well beyond the shape of the foot, and therefore are a different sort of footwear from the patten discussed here. ""Horse-pattens"" were used on horses, especially for ploughing muddy fields. The word was also used for ice-skates, as it is in French (patiner, to skate).\nThe Worshipful Company of Pattenmakers\nIn London, the Worshipful Company of Pattenmakers remains the Livery Company, formerly guild of the Patten-makers, or Patteners, and their adopted church remains St Margaret Pattens. The first record of the guild dates to 1379, and there was still a pattenmaker listed in a London Trade Directory in the 1920s. A notice, probably 18th century, in the Guild Church still requests ladies to remove their pattens on entering; other English churches have similar signs, and in one case, a board with pegs for ladies to hang them on.\nLeft:German 15th-century wooden patten. Right:Dutch 18th-century patten of metal circle type (Deutsches Schuhmuseum Hauenstein)\n- Grew, F & De Neergaard, M: Shoes and Pattens\'2001.\n- OED despite quotation being in Latin: ""clogges et pattenes""\n- ""Medieval Jewish History: An Encyclopedia. Edited by Norman Roth, Routledge"". Myjewishlearning.com. Retrieved 2013-09-12.\n- ""Children\'s pattens made in Montgomery, 19th century"". Gathering the Jewels.\n- Persuasion, start of Chapter 14\n- Taming of the Shrew and OED\n- The Life of Charlotte Brontë, by Elizabeth Gaskell\n- ""Pair of Pattens"". Birmingham Museums & Art Gallery.\n- Arnold, Janet: Queen Elizabeth\'s Wardrobe Unlock\'d, W S Maney and Son Ltd, Leeds 1988. ISBN 0-901286-20-6\n- Ashelford, Jane. The Visual History of Costume: The Sixteenth Century. 1983 edition (ISBN 0-89676-076-6), 1994 reprint (ISBN 0-7134-6828-9).\n- Boucher, François: 20,000 Years of Fashion, Harry Abrams, 1966.\n- Kohler, Carl: A History of Costume, Dover Publications reprint, 1963, ISBN 0-486-21030-8\n- Laver, James: The Concise History of Costume and Fashion, Abrams, 1979\n- Payne, Blanche: History of Costume from the Ancient Egyptians to the Twentieth Century, Harper & Row, 1965. No ISBN for this edition; ASIN B0006BMNFS\n- Grew, F & De Neergaard, M: Shoes and Pattens\', Museum of London, The Boydell Press, Woodbridge 2001. ISBN 0-85115-838-2\n- Goubitz, O. et al. Stepping Through Time: Archaeological Footwear from Prehistoric Times Until 1800, Stichting Promotie Archeologie 2001. Reprinted 2007 in Paperback\n|Wikimedia Commons has media related to Pattens.|\n|Wikisource has the text of the 1911 Encyclopædia Britannica article Patten.|\n- Pattens and overshoes in 15th-century art\n- Excavated German patten\n- Scroll down to Finding a Patten of John Gough for good photos of a circle-type patten, and good text on Early Modern pattens\n- Several examples of 18th-century women\'s pattens\n- Pattens from Manchester Art Gallery\n- Website of The Worshipful Company of Pattenmakers, history page\n- Elaborate Victorian pattens\n- A Midwife going to a labour’, by Thomas Rowlandson, 1811', 'Nanotechnology has impacted all the major industrial sectors, including electronics, medicine, agriculture, chemicals, coatings, cosmetics and energy. By incorporating nanomaterials into their products, companies can make new technologies which are more sustainable. Not only does nanotechnology improve the performance and design of these products, there are also large revenues associated with nanotechnology enhanced product sales. A good example of this is Lux Research which has achieved over $1 trillion from the commercial sales of the nanotechnology-enabled products in 2015 in Europe and the USA.\nImage Credits | shutterstock.com/g/Kotkoa\nThe manufacturing process of nanomaterials can be compared to the chemical manufacturing process and is known to produce harmful pollutants as well as adverse environmental effects. Although these can be mitigated during the initial design stage, it has not been seen as a priority for current industries. However, as companies become more environmentally conscious, these harmful pollutants should be reduced, especially when manufacturing on a commercial scale. By using green technology, companies can minimize the environmental effects of the process.\nGreen technology works with nanotechnology to develop significant and practical protocols when manufacturing nanotechnology-enabled products. By modifying the production process, the risks that are associated with nano-products are minimized and the green products can be used for environmental applications. This can be through direct applications, such as nano-enabled sensors, wastewater and drinking water treatment and remediation of hazardous waste using nanomaterials. On the other hand, nanotechnology can also have indirect environmental applications such as saving energy during transport by using lighter nanomaterials or reducing the waste by designing products to be smaller.\nUsing green nanotechnology in the production process aims to make nanomaterials more environmentally friendly as well as using nanomaterials in order to make current processes involving chemicals less harmful. There are plenty of ways to do this such as using supercritical CO2, water, or ionic liquids to replace a volatile organic solvent. Another way to eradicate waste in the manufacturing process of nanomaterials is by implementing self-assembly or templating.\nOne of the simplest ways to create greener nanotechnology is by using renewable or non-toxic replacements of non-renewable starting materials. Furthermore, using techniques like microwaving, facile thermal and hydrothermal can aid the conservation of energy. Both catalytic and photocatalytic reactions are also able to increase the efficiency of the manufacturing process while also decreasing harmful byproducts. It should be noted that engineered nanomaterials are able to be used as catalysts in chemical processes. They can also be used as separation membranes.\nHowever, many researchers are investigating the toxicity of nanomaterials. Nanosilver and its effects on the ecosystem are of particular interest as nanosilver can be absorbed by plants if found in the soil. According the US Environmental Protection Agency, this is difficult because the existing tests “may not work to test the safety of nanomaterials.” In addition to this “they [nanomaterials] have unique chemical properties, high reactivity, and do not dissolve in liquid”\nNanotechnology and green technology can work together to create a more environmentally friendly process and products. As the world becomes more ecologically aware, the use of green technology in the planning and design stage will become more prevalent in the industry. However, in order to create truly green products, the entire lifecycle must be considered and how nanotechnology affects all of this is still not known.\nGrossman, E. (2016, January 14). Tiny materials, big questions: How green is nanotechnology? Retrieved from GreenBiz: https://www.greenbiz.com/article/tiny-materials-big-questions-how-green-nanotechnology\nKarn, S. W. (n.d.). Ensuring sustainability with green nanotechnology. Retrieved from IOPScience: https://doi.org/10.1088/0957-4484/23/29/290201\nUnited States Environmental Protection Agency. (n.d.). Research on Nanomaterials. Retrieved from EPA: https://www.epa.gov/chemical-research/research-nanomaterials']"	['<urn:uuid:d2c4edd1-40e4-4374-96d0-4d31d6fba967>', '<urn:uuid:f2581957-b432-4722-80ba-693f34ccd8f3>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	6	39	2380
11	grass seed rain planting when best time protect seeds from washing away	While it's better to plant grass seeds on a dry day, rain itself won't kill the seeds as they need moisture to germinate. However, heavy rains can cause problems by washing away seeds. To protect seeds, you can cover them with organic mulch, peat moss, or biodegradable burlap. Additionally, having proper soil drainage is crucial - if you find rain isn't draining away easily, your lawn may be suffering from compaction. You can improve this by aerating the soil and adding topsoil and sand, especially for heavy clay soils.	['It’s better to plant grass seeds on a dry day than when it’s raining, but that doesn’t mean you should avoid the rain when planting. Newly planted grass seeds require a lot of moisture to germinate see and start to grow, so rain won’t kill the seeds.\nHowever, heavy rains can erode the soil and wash away the planted seeds. You can keep your seeds in place by covering them with organic mulch, peat moss, or biodegradable burlap. This will cover the seeds and hid them from the birds and conserve the moisture the seeds need to germinate.\nWays to keep your grass seeds from washing away\nA newly seeded lawn requires watering at least twice daily and will require more water when it’s hot and dry. The goal is to ensure that an inch of the topsoil stays moist and rain can help soak your lawn so you don’t have to.\n- Lawn drainage\nProper drainage is the best defense against heavy rains. Pay more attention to the areas where water pools by adding more soil and grading away from your house, but if the spot is in a low area, create drainage to divert accumulated water to another area.\nDepending on the grade of your lawn, it can be a lot of work and the best option if you experience constant pooling.\n- Lawn aeration\nWith time your lawn soil gets compacted which makes it hard for the water to penetrate the soil which causes flooding or pooling. Creating small holes in the soil will not only hold the grass seeds but also improve the overall growth of your grass.\nThis process helps nutrients, air, and water to reach the roots of your plants for healthy growth. Lawns that experience heavy foot traffic need to be aerated at least every few years.\n- Change the soil on your lawn\nThe best soil to use on your lawn is loam soil because it can grain well in case of heavy rains, hold moisture, retain nutrients, and allow proper air circulation.\nLoam soil contains sandy soil that has the largest particles, very easy to cultivate, drains well, and warms up fast.\nSilt has medium-size particles that help to encourage air retention and water, while clay soil with the finest particles retains a lot of water, gets sticky when wet, and contains a lot of nutrients.\nHeavy clay-loam soil isn’t the best, especially during heavy rains because it tends to retain a lot of water, but adding sand or silt can improve the soil.\n- Clean the gutters\nFor your drainage system to properly function, it needs to be well maintained. Clogged gutters and drains near your lawn can cause water buildup on your lawn which can cause soil erosion and other problems in your home like flooded basements, wood rot, paint damage, and mold.\nFor the sake of your lawn and home, you should clear the dirt, leaves, and debris in your gutters and fix them if they’re aging, leaking, or damaged.\n- Wind protection\nHeavy rain is usually accompanied by heavy winds and building up the soil around your newly planted grass seeds will keep them from being washed away.\nIf you have a fence near your lawn, you can create a windbreaker by attaching a plastic sheet with a few cut holes so it doesn’t blow away.\n- Use protective covering\nStraw is a natural covering that acts as an erosion blanket to protect your plants from harsh weather. And like mulch, straw decomposes after a while and leaves all the nutrients in the soil.\nMulch is a great option for covering your grass seed and it’s more effective, cheap, and eco-friendly.\nA light mulch covering will help to protect your grass seeds from hungry birds, rain, and cold weather while still letting in air, sunlight, and moisture to reach the seeds.\nMulch will also provide nutrients to the soil once it decomposes improving the soil quality.\nHowever, mulch can contain weed seeds that can invade your lawn so ensure you use mulch that’s weed-free and only use enough to the extent you can slightly see the soil.\nDifferent kinds of mulch to use\nOat, wheat, or barley straw\nYellow grain straw that is readily available, cheap, relatively free of seeds, easy to cut with a mower\nAged pine straw\nThe needles might contain a chemical that suppresses plant growth, but the chemical evaporates when the needles fall, use well-aged brown needles with no aroma\nFinely screened mushroom or regular compost is best, breaks down to provide the soil with nutrients\nLoosened peat moss is a great mulch\nUse only ¼ layer\nYou can buy fabric rolls to cover your grass seeds then tack them at the corners using tent spikes.\nIf your lawn is sloppy, bury the ends of the fabric under a few inches of dirt at the top of the slope to prevent the water from running under.\n- Proper planting\nYou can limit the grass seeds that get washed away by the heavy rains or prevent erosion by ensuring you prepare and plant your lawn properly.\nBefore you sow your grass seeds, make sure you till and remove all the rocks, roots, leaves, and trash in the soil and level the area and break up the clods of dirt with a rake.\nWater the area a few times then rake and water it again before planting. Use a vertical path and a 90-degree angle to spread your grass seeds. Also, use a spreader to distribute the seeds evenly, then cover the seeds with the topsoil and rake gently.\nEven though grass seeds are resilient, if exposed to a lot of water for a long time, it will decrease their chances of germinating.\nThe above steps can help you protect your grass seeds during heavy rains which will give them a chance to germinate and grow.\nWill grass seed grow if not covered?\nYes, grass seed can grow if not covered, but it’s more beneficial if you add a layer of compost, mulch, or topsoil to keep the seeds moist and help them germinate.\nWhat if it rains after fertilizing my lawn?\nWater helps to activate the fertilizer so it gets deeper into the soil where it breaks down and is absorbed by the roots. So the rain is a good thing after fertilizing.', 'Whilst the British summer is busy buffering, in some parts of the UK we’ve been treated to 3 months of rain – so what does this mean for seed that you’ve recently sown?\nWell it really depends on how well your soil drains – in most cases, rain will drain away into your soil and do your watering for you and your seed will be largely unharmed. However, if you find your seed is sitting in a puddle – this could risk the rate at which your seed germinates.\nMost soils are free draining, if you find that rain doesn’t drain away easily your lawn may already be suffering from compaction or may not have an adequate draining system underneath.\nSo if you are finding that this is an issue, there are a few steps that you can take to help prevent it happening in future and a few you can undertake to help improve the situation, if the weather doesn’t!\nEnsure your lawn is well aerated\nAeration to most may seem like an out-there concept for the average home owner, but it does greatly help the dynamics and appearance of your lawn. Grass is a living thing, and soil is its greatest support system. If your soil is hard, dry and compacted it will struggle to drain. Aerate your lawn using an aerator machine, or if you have a small lawn use a pitchfork to spike the lawn and alleviate this issue.\nAdd some top soil and sand\nIf you have particularly problematic soils e.g. heavy clay, adding some top soil and sand will help to improve the top layer of soil for where you will be sowing your seed. We also have a great grass mix that contains grass seed that is deep rooting, this will help to improve the soil structure of your heavy clay soils and allow water to flow freely.\nMinimise the risk\nIf you have gutters or drains that are close or lead to your lawn, it is a good idea to clear these out to ensure there isn’t a build up of water that may then end up leaving your lawn sodden. Get rid of any leaves or debris – not the most pleasant job in the world, but one that will definitely help!\nIf rain puddles aren’t draining from your lawn you can manufacture some run off points, especially if your lawn does not have any slopes. Directing the water to more stable parts of your garden e.g. patio or concrete, will give your grass some breathing space.\nWhile slopes are useful in this situation, too many lumps and bumps in your lawn will cause water to collect in one area. By starting off your seeding with a level seed bed, created by raking with a spring tined rake, you can help to avoid this happening in the future.\nEven though grass seed is resilient stuff, if exposed to deep levels of water for an extended amount of time the successful outcome of its germination will decrease. By taking the above steps, you can help future proof your lawn and protect your grass seed if this wet weather continues!']	['<urn:uuid:c4c9d1ab-6bd5-4bc7-974b-979bd04ca784>', '<urn:uuid:8445ef98-c3c2-4177-bd4e-e83bd376b8c3>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	12	89	1586
12	unlock iphone use india compatibility	Unlocked iPhones can be used in India as they support Indian SIM cards and carriers. However, locked iPhones cannot be used in India as they do not support Indian SIM cards and carriers.	['- 1 Can I use AT&T in India?\n- 2 How much does AT&T charge for international roaming?\n- 3 Does AT&T charge for data roaming?\n- 4 How do I activate international roaming on AT&T?\n- 5 How do I avoid international roaming charges AT&T?\n- 6 Can I use unlocked iPhone in India?\n- 7 What is the best international plan for AT&T?\n- 8 How much do international phone calls cost?\n- 9 What happens when you turn on data roaming?\n- 10 Does data roaming cost extra?\n- 11 Do you need to turn on roaming for AT&T International Day Pass?\n- 12 How much does it cost for international roaming?\n- 13 Can I use AT&T data internationally?\n- 14 Does AT&T unlimited plan include International?\nCan I use AT&T in India?\nA quad-band unlocked GSM phone will be compatible with most GSM networks worldwide, including India. AT&T and T-Mobile will unlock phones. You can possibly jailbreak your phone to get it unlocked but this will void its warranty.\nHow much does AT&T charge for international roaming?\nYou’ll be charged $10 per device for each 24-hour period you use your device and $5 a day for additional lines on the same account used on the same day in any of the 200+ countries where International Day Pass is available.\nDoes AT&T charge for data roaming?\nWhat do you need to know about roaming with AT&T? With AT&T’s unlimited plans, domestic wireless roaming is free. If you are traveling international, you can pay additional fees on a monthly or daily basis. Or, you can spend on a per-minute basis.\nHow do I activate international roaming on AT&T?\nHow to Activate International Roaming on Android Phones\n- Tap “Settings.”\n- Click “Connections.”\n- Select “Mobile networks.”\n- Switch on “Data roaming.”\nHow do I avoid international roaming charges AT&T?\nFor Android devices, go to Settings, then Connections, and then Data Usage. Turn Data Saver to ON.\nCan I use unlocked iPhone in India?\nYou can use unlocked iPhones in India as they support the Indian SIM cards and carriers. But if the iPhone is locked then you cannot use those iPhones in India as they do not support Indian SIM cards and the Indian carriers. Make sure your iPhone is unlocked so that you can use it without any problem.\nWhat is the best international plan for AT&T?\nAT&T offers four international roaming plans, with the best value coming from the $10 per day International Day Pass. For the base North America plan, customers receive unlimited talk and text within North America, plus access to the same data plan you run at home with no roaming charges added.\nHow much do international phone calls cost?\nThe overall cost of international calling has become cheaper with the rise of internet-based, voice-over-internet-protocol (VoIP) services. Pricing for calls can range anywhere from just one cent per minute to upward of 10 cents per minute.\nWhat happens when you turn on data roaming?\nData roaming occurs whenever your phone disconnects from your carrier’s network and hops on another network. Roaming allows you to make calls, send texts, and use wireless data even when you‘re outside of your network’s boundaries. If you have the roaming feature turned on, all of this will happen automatically.\nDoes data roaming cost extra?\nRoaming is generally not included in your plan, and roaming rates may be higher. Most providers offer roaming packages or add-ons that you can buy before you go. Roaming charges apply to voice calls, SMS (text messages), MMS (picture messages), and data that you receive or send when you’re roaming.\nDo you need to turn on roaming for AT&T International Day Pass?\nA. If you don’t want to be charged another daily fee, stop using your device before your 24-hour International Day Pass expires. We recommend that you turn off Cellular Data Roaming in your device Settings or place your device in Airplane Mode to prevent it from accidentally triggering a daily fee.\nHow much does it cost for international roaming?\nWhile domestic wireless calls in an optimized pooled plan can be as little as 5 cents per minute and “unlimited” data plans provide data connectivity at a fixed cost of $40 to $50 per month, international roaming usage charges for U.S. customers are typically around $1.50 per minute, 50 cents per SMS, and $5 to $10 per\nCan I use AT&T data internationally?\nWith International Day Pass, you can use your Mobile Share or an AT&T unlimited plan while traveling abroad in more than 200 countries. You get unlimited talk within and between International Day Pass countries and back to the U.S., unlimited text, and use of the high-speed data plan that you use at home.\nDoes AT&T unlimited plan include International?\nAT&T Unlimited and MobileShare plan customers can talk, text, and use their plan data with an eligible device in Mexico or Canada without roaming charges. To call or send a text to another country, dial +, followed by the country code and local number.']	['<urn:uuid:a97b83ed-895c-4231-afc2-2871f798d043>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T04:31:27.499155	5	33	844
13	what is bidirectional accuracy machine tools	Bidirectional accuracy in machine tools takes into account both forward and reverse directions when measuring positioning accuracy. It considers how well the machine hits the target position when moving toward it (forward) and when moving away from it (reverse).	"['There are several positioning standards that provide methods by which an accuracy or repeatability specification can be determined. The same machine tool will give different accuracy and repeatability numbers depending on the standard used. This makes comparison shopping between various machine tools difficult.\nThis article compares six of the most common linear positioning standards used by the machine tool industry, in the United States, Europe and Asia to qualify machining centers. The standards that will be compared are:\nNMTBA (United States), ISO 230-2 (Europe), BSI BS 4656 Part 16 (British), VDI/DGQ 3441 (German), JIS B 6336-1986 (Japanese), and ASME B5.54-92 (USA).\nBut before getting into a comparison of these standards, it\'s necessary to define some critical terminology. Defining these terms permits discussion from a common frame of reference.\nWhat Is Accuracy?\nMost of us have, at one time or another, shot a gun or bow, or thrown a dart. In each case we were trying to hit a target, preferably the bull\'s-eye. If we fire six times at the target bull\'s-eye and hit it every time, we have very good aim—we are accurate. The bull\'s-eye would probably look like Figure 1.\nThis same analogy applies to machine tools. Both the machine builder and user want the machine slide to hit that target bull\'s-eye every time it\'s moved. But the machine sometimes has different ideas. An accuracy spec quantifies how well we hit the target. Each linear positioning standard defines a method for determining accuracy and other specifications on a consistent basis.\nMachine tool builders talk about three kinds of accuracy: unidirectional forward, unidirectional reverse and bidirectional. Let\'s say you were in a contest where you had to walk toward a target while shooting and then shoot at the same target while walking away from it. The accuracy while walking toward the target would be unidirectional forward accuracy and that walking away from the target is unidirectional reverse. Each of these are scored individually. You can also score the overall accuracy forward and reverse. This is called bidirectional accuracy because it takes both directions into account.\nMachine tool builders can describe accuracy with terms like error-band, accuracy, positional deviation, position uncertainty and mean-to-mean accuracy. Each of these terms is calculated differently.\nWhat Is Repeatability?\nIf we look again at Figure 1, the distance between each hit illustrates repeatability. We know they are all in the bull\'s-eye but chances are the person firing the gun wavered slightly or was just a fraction of an inch off when sighting, causing the bullets to hit not one on top of the other, which would be perfect repeatability, but rather in a pattern shown in Figure 1. This pattern is repeatability. Machine tool slides also vary about the target point. Here\'s another example of accuracy and repeatability.\nFigure 2 shows a very tight pattern. Therefore, the repeatability is very good. Even so, the person must have pulled left every time he/she fired, or maybe the rifle sight was off to the left slightly. The result is poor accuracy. Remember the bull\'s-eye is still the target.\nWe can enter the same contest of shooting at a target while walking toward it and away from it. Walking toward the target we get a pattern of holes perhaps like Figure 3. This represents forward repeatability.\nWalking away from the target we may get a pattern like Figure 4. This illustrates reverse repeatability.\nRepeatability is tested as the total range, so if the bottom bullet hole and the top bullet hole are 0.5 inch apart, then this figure represents the repeatability. There is also a bidirectional repeatability which takes into account both Figures 3 and 4, and as a value would be the distance from the bottom bullet hole on Figure 4 to the top hole on Figure 3.\nTransferring this comparison to a machine tool, we have six targets for each machine slide. We position to these targets from both directions seven times and from this information we derive repeatability.\nRepeatability may be described by terms such as forward repeatability, reverse repeatability, bidirectional repeatability, and positional scatter. Each of these terms is calculated differently.\nLost motion means motion is initiated but you don\'t see any results until all the lost motion is accounted for—like taking up slack on a block and tackle. If you look closely at Figures 3 and 4 you will notice our marksman has lost motion. In Figure 3, all the bullets are left of center, and in Figure 4 they are right of center. The difference between them is lost motion. Other lost motion terms are reversal error, and mean reversal error.\nNow assume we have six targets, and position to each target seven times. We position from an arbitrary starting point—let\'s call it zero. If we position to six targets, turn around and position to the same six targets on our way back to zero, save all the errors at each target point, then repeat this seven times, we have 84 pieces of data. Figure 5 will give you the general idea of the normal collection process for these data.\nFrom this data accuracy, repeatability, and lost motion specifications are calculated.\nBut before we calculate any of these values, we must talk about sigma. It\'s represented by the Greek letter s or S. Sigma is also known as Standard Deviation. Standard deviation is normally shown using a bell shaped curve as shown in Figure 6 (at right).\nOn each side of center (x) is 3s, which is three times one sigma.\nAs you can see, on the bell curve, ± 1 sigma covers about 68 percent of the area under the curve. ±2 sigma covers 95 percent and ±3 sigma covers 99.7 percent. If we position to our six targets seven times each, calculate the standard deviation, and multiply it by six, then 997 times out of 1,000, we would hit our six targets within that value.\nThe same raw data was used for all six comparisons. This data is in the inch system of measurement and all values are in microinches (millionths of an inch). So, for example a measurement of 982 as indicated on the chart would be 0.000982 or 982 millionths of an inch.\nWhat\'s It Mean?\nAll the standards except JIS output total bandwidth for the values, even though many builders advertise ± values regardless of which standard is used. This may render the appearance of a lower number.\nNMTBA is the only standard that statistically calculates using bidirectional data. The other statistical standards generate their bidirectional data from the individual forward and reverse information only. This is not necessarily better or worse than NMTBA, but it definitely does result in smaller figures, which can unfairly impact a prospective customer\'s spreadsheet. The fact is that NMTBA is the most stringent.\nASME B5.54-92 is really the ""new kid on the block,"" but has quickly become the new national standard of the United States, replacing the statistical NMTBA in many arenas. However, each standard has its place depending on your taste for either statistical data or actual raw data.\nIn summary, any of the standards described may be used to evaluate a machine tool\'s linear positioning. However, we must be aware that one standard can allow the machine to appear to be more accurate than it will be when evaluated using another standard.\nAbout the authors: Steven Klabunde is vice president of corporate technology and Ronald Schmidt is quality assurance test engineer for Giddings & Lewis, Fond du Lac, Wisconsin.blog comments powered by Disqus']"	['<urn:uuid:3e98c86a-dd19-41e6-b661-0964ac4ca6cc>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T04:31:27.499155	6	39	1253
14	How many positions did the final practice session leader improve from last year's pole position time at the Saudi Arabian Grand Prix?	The final practice session leader's time was almost three tenths slower than last year's pole time of 1:28.200, which was set by Perez.	['Max Verstappen topped the final practice session for the 2023 Saudi Arabian Grand Prix with Sergio Perez second, the pair on another planet.\nThe Red Bull pair were separated by six tenths, but were a whopping nine tenths away from their closest rivals one the chequered flag for FP3 was waved and the sights were set on qualifying next.\nFernando Alonso remained to closest challenger to the Bulls, finishing the final practice session third fastest but almost one second off the pace, nothing separating the Spaniard from teammate Lance Stroll in in fourth.\nClearly, reigning Formula 1 Champion, Verstappen left more time out on track, as his time was almost three tenths slower than last year’s pole time, a 1:28.200, set by Perez.\nBut for now the safest bet would be on a front row lockout by Red Bull, while Alonso doing his best to give them some trouble, but Ferrari, who clearly didn’t run their engines on full power in FP3, might edge closer, but pole migh be in doubt. But let’s see…\nIf Friday in Jeddah told us anything, it is that we just need to know by what margin Red Bull take pole today, unless Ferrari aren’t actually lost and were bluffing big time as Editor in Chief Paul Velasco wrote.\nFrom what we saw in the long runs on Friday, the gaps between the teams shrunk with the cars loaded with fuel which would make matters interesting on Sunday.\nWorth noting though, that despite still looking in decent shape, Fernando Alonso and Aston Martin seemed a bit lees impressive on the long runs, as their performance seemed to drop while Red Bull as reference kept delivering consistent lap times on the race simulations.\nNevertheless, their is nothing stopping the Aston Martin from delivering a birthday present to their Team Principal Mike Krack.\n— Aston Martin Aramco Cognizant F1 Team (@AstonMartinF1) March 18, 2023\nAlpine made progress well into the top ten after a shaky weekend in Bahrain, while Mercedes are so deep in the woods, it’s anyone’s guess when they can find their way again, that is if they do this season.\nOther than that, the revised Turn 22 has seen many drivers being caught out and running wide, so it would interesting to see how that works out in qualifying with everyone pushing hard, not to mention the infamous traffic around the Jeddah Corniche Circuit as well, many close moments registered over the course of Friday.\nJeddah FP3 session highlights\nTrouble hit AlphaTauri at the start of FP3, as the team announced there was an issue detected on Nyck de Vries’ AT04, which meant the power unit needed to be changed.\nNot the best way for the Dutchman to get ready for his second qualifying with the team.\nunfortunately, we’ve detected an issue on @nyckdevries’ car and we are now changing his PU.\nThe session started with Medium and Hard tyres in use in majority, while the teams started to switch more to the softs a bit later on. Worth noting is that the Soft tyre might end being a decent race tyre with many teams doing their long runs on Friday with it.\nMax Verstappen showed how much pace his RB19 has, shooting to the top of the timing screens almost three tenths faster than the closest rivals, and on Hard. That was 20 minutes into the session. Verstappen later improved on Hards as well reducing the gap to a tenth or so…\nSergio Perez soon took the top spot from his teammate but with a run on Soft tyres. He was over three tenths faster.\nAs we saw so many times on Friday in FP1 and FP2, Lewis Hamilton had a moment while on a timed lap with traffic ruining his lap.\nLewis Hamilton has his lap interrupted by multiple traffic moments out on track!\nMaking matter more interesting, FP3 was running in cloudy conditions, which meant cooler temperatures making conditions more similar to qualifying, meaning we can get a good indication from the final practice on how things might pan out in qualifying.\nAt the halftime mark of FP3, the Ferraris were still running under the radar, Charles Leclerc and Carlos Sainz ninth and tenth respectively, both their times set on Medium tyres.\nOn the other hand, both Alpine remain on strong form mingling with the others in the top ten, while Fernando Alonso was down in 19th after his first run on the Hards.\nAlex Albon, mixing it up in the top ten at the halfway mark, seems to be happy with his Williams, telling his team over the radio: “It’s the best the car has ever felt.”\nLando Norris had a dangerous moment with Max Verstappen towards the end of the session at Turn 8, the Briton unimpressed, saying over the radio: “It’s dangerous what these guys do – that could have been a massive crash.”\nVerstappen was quick to apologize to Norris, waving as he drove by, while complaining to his pit wall for not giving him the heads up.\nThe stewards noted the incident and looked into it but took no further action.\nLando Norris’ lap is impeded as he is accidentally blocked by Max Verstappen 😩']	['<urn:uuid:782a0549-62b2-4a03-83fc-29e2a2e88704>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	22	23	869
15	distinguishing characteristics comparing eidetic memory versus hyperthymesia abilities	Hyperthymesia is the ability to remember nearly every event of one's life in great detail, while eidetic memory is the ability to accurately recall an image after seeing it once for a short period. People with eidetic memory can hold detailed visualizations in their mind for up to several minutes, whereas hyperthymesia focuses on autobiographical experiences.	['Hyperthymesia is an ability that allows people to remember nearly every event of their life with great precision.\nHyperthymesia is rare, with research identifying only a\nIn this article, we define hyperthymesia and explain the causes, characteristics, and diagnosis.\nHyperthymesia is also known as highly superior autobiographical memory (HSAM). According to a\nThese details can include exact dates and intricate information about previous experiences. Current\nPeople can retain information as either short-term or long-term memories. People with HSAM likely process short-term memories in a similar way to most other people. However, a\nHSAM is different than having a good memory. For example, people with HSAM do not use mnemonic devices to remember long strings of information.\nPeople with hyperthymesia can recall details relating to past experiences with extreme accuracy.\nPeople with both conditions also tend to have certain structural differences in particular regions of the brain. However, despite these similarities, there is no definitive link between having HSAM and OCD.\nPeople with HSAM are unable to forget their life experiences. Further research is necessary to assess the long-term effects of this.\nAs there are relatively few people with hyperthymesia, there is a lack of research examining the causes. There are some theories as to what contributes to hyperthymesia, but the exact cause remains unknown.\nSome research suggests that the cause of hyperthymesia may be biological, genetic, or psychological. However, more studies are necessary to gain a better understanding of what causes hyperthymesia.\nThere is some evidence that hyperthymesia may have a biological cause.\nAnother theory is that hyperthymesia may be genetic. However, current research is ongoing as to whether the ability has a genetic basis.\nSome researchers believe that hyperthymesia may have psychological causes. This theory implies that people with HSAM\nAs hyperthymesia is a rare ability, there is currently no formal way of diagnosing it.\nSome research suggests that people with hyperthymesia have hyperactivity in certain parts of their brain. Doctors could potentially, therefore, assess whether a person has HSAM by taking an MRI scan while they undergo a memory test.\nComplex memory tests can also help confirm whether someone has hyperthymesia. These tests can include an\nA person with hyperthymesia can remember nearly every event of their life in a lot of detail. On the other hand, eidetic memory is the ability to accurately recall an image after only seeing it once for a short period.\nThose who have a superior eidetic memory can continue to visualize something they have recently seen with great precision. They may be able to hold the intact visualization in their mind for up to several minutes.\nFor most people, eidetic memories tend to fade after a few seconds. In some cases, the visualizations may change or become stored as a long-term memory.\nPeople often confuse eidetic memory with photographic memory. People who claim to have a photographic memory state that they can remember a visualization for a long time in the same detail as when they first saw it. However, scientific research debates the existence of photographic memory.\nSimilarly to people with hyperthymesia, people with a good eidetic memory do not rely on memory devices, such as mnemonics.\nThere is little research exploring hyperthymesia and eidetic memory. The reason for this may be that they are hard phenomena to test. However, it is likely that people with hyperthymesia do not have a superior eidetic memory.\nMore research is necessary into both eidetic memory and hyperthymesia to understand their similarities and differences.\nHyperthymesia is the rare ability to recall nearly all past experiences in great detail.\nThe causes of HSAM are currently unknown, but some theories suggest that it may have biological, genetic, or psychological origins.\nThere is currently no way to diagnose hyperthymesia formally. Possible ways to assess this may be through MRI scans and complex memory tests.\nHyperthymesia differs from eidetic memory in that it focuses on a person’s ability to recall their autobiographical experiences rather than to hold visualizations in their mind.\nFurther research is necessary to examine the differences and similarities between hyperthymesia and eidetic memory.']	['<urn:uuid:d9994a8b-403b-433d-938a-798ab556a26f>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	8	56	677
16	I'm thinking of getting a water borehole in my garden - what exactly happens during the drilling process to remove all the waste material that comes from drilling the hole?	During drilling, waste material like crushed rock is removed through 'flushing' techniques. There are two main methods: Air flush uses compressed air to operate a down-hole hammer that breaks up the rock and blows the crushed fragments out to the surface. Mud flush uses water and polymer to circulate through the drilling pipework, washing the drilling residue upwards and out of the hole while also stabilizing the formation and lubricating the drill bit.	['Independant water supplies, free of chemicals and additives\nUsing the ambient heat from the ground to heat your home, save money and reduce CO2\nDeep Bore Soakaways\nAn effective drainage solution when a standard soakaway is not appropriate\nWater wells and boreholes\nThe methodology for drilling water wells and boreholes is dependant on a number of factors including:\n•water table depth\nAll of these factors are taken into consideration when tailoring a system to meet your needs.\nEstablishing ground conditions\nThe first stage of the process is to assess the geology of the proposed site. On initial contact we would take a postcode/grid reference for your property and using our in house geological maps and knowledge ascertain the geological strata for the proposed site. We also look at what other water wells have been drilled in the area and at what depth water was found to give us some history/background to the areas hydrological characteristics.\nThe geological conditions of the site determine the depth and technique needed for drilling to obtain a water supply. You can find out more in the ‘what’s involved section’.\nIf there is any doubt over the ground conditions we would ask that you obtain a survey from the British Geological Survey who will send you a report of the area. However this is not necessary on all occasions.\nDrilling the borehole\nThe borehole is drilled using a large drill bit which cuts through the rock creating a hole held open by casing which is inserted as the borehole develops. In many cases where the rock is hard the casing will only be inserted for the first few metres and the hole will be naturally supported lower down. In other, softer, ground conditions casing will have to be inserted further to ensure the borehole does not collapse.\nThe drill continues to cut through the rock and the waste material from the drilling process e.g. crushed rock/stone needs to be removed from the borehole. The technique used for this process is called ‘flushing’ and is explained further in the next section.\nWater boreholes are generally drilled to a diameter of 200mm (8”) but this can vary depending on the amount of water required and the geology of the area. Our specialist equipment can enable drilling from diameters ranging from 4” to 15”.\nThe borehole will be drilled using either mud or air flush techniques. ‘Flush techniques’ relate to the process used to remove the waste material that has been cut away in the drilling process.\nDifferent flushing methods are used to remove the cuttings (waste) from the borehole depending on the geology of the site. Air flush technique uses compressed air to operate a down-hole air hammer on the end of the drill string that helps to break up the rock formation. The compressed air that is used to operate the down-hole air hammer also blows the crushed rock fragments out of the hole to the surface along with any water that flows into the hole during drilling.\nMud flush uses water and polymer to aid the drilling process and clear cuttings from the borehole as it is drilled. Mud flush technique uses a drill bit made from toughened materials such as tungsten to break through the substrata. Once the drill bit has broken through the substrata, the drill fluids are circulated through the drilling pipework into the borehole and back to the surface, at the same time washing the drilling residue or cuttings upwards and out of the hole. This fluid also serves as a formation stabiliser preventing possible cave-in of unstable sands or crumbly rock before the well casing or well screen is installed. In addition this fluid acts as a lubricant for the drill bit.\nDepending on the geology of the site where surface formations are unstable, such as sands, a temporary steel casing may have to be installed to stop the borehole collapsing until a solid, stable material is encountered.\nWell screen and casing installation\nOnce the borehole has been drilled to the required depth, the tools are removed and the well screen and casing can be installed. The well screen is installed in the lower section of the borehole. The well screen has a 5” diameter (in an 8” borehole) with 1mm precision cut holes to allow water to percolate into the borehole. Casing is then installed in the upper sections of the borehole. This is a solid pipe that prevents any surface water and contaminants entering the borehole.\nThe gap between the 5” casing and well screen and the 8” borehole wall is then back filled with washed pea shingle. The upper solid casing annulus is then grouted to ground level, preventing any surface contamination from entering the well.\nAll wells, once construction is complete, need to be test pumped in order for the pumping characteristics to be assessed. The standing water level, pumping water level and assessed sustainable yield all need to be known in order for a suitable pump to be selected for the desired application.\nOn a domestic well, 20m3 per day or less, a pumping test of 4 hours is usually adequate to record the pumping performance of the well. On larger abstractions where the Environment Agency require further information, pumping may be necessary for a longer period.\nThe borehole pump\nA borehole pump, put simply, is a submersible pump, of narrow enough diameter to be lowered inside the well liner. The pump is lowered into the well, below the water level and suspended a minimum of 5 metres from the bottom of the well. Attached to the pump is a pipe to bring water to the surface and a cable to supply power to the pump. The weight of this suspended system is taken by a wellhead set in a small concrete slab.\nThe water well ‘finish’\nThe finish of the well can be above or below ground, though we prefer to finish above ground on farms, in line with Environment Agency advice, minimising the risk of contamination to the well from run-off containing slurry, chemicals, fuel etc.\nThe borehole pump can be set up to deliver water to a tank or reservoir, controlled by a float switch, or to supply directly into your system, controlled on a pressure switch.\nWater for human consumption\nWhere water is required for the purpose of human consumption, it must be tested and any subsequent treatment applied.']	['<urn:uuid:772d7e9c-0de5-4621-863d-c90a618786f6>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	30	73	1067
17	As a brewery supplier, what varieties of hops grow in Palisade?	In Palisade, Hippie Chicks Hops grows Cascade and Chinook varieties of hops. These hops contain acids and oils that provide bitterness, flavor, and preservative qualities to beer.	['Julie Diers: She formed Hippie Chicks Hops into a ‘Cinderella Story’\nPortrait 2011 — Volume 3: Up-and-Comers\nThis is a story about a go-getter school marm who bought a little Palisade farm and started to grow beer — or at least one of the ingredients for beer: hops. She had lots of fun friends — and they liked to work on the farm, helping her plant and harvest her Cascade and Chinook hops. In gratitude, the school marm named her budding company after them — Hippie Chicks Hops — and if you think they lived happily ever after, you’re right.\n“It’s such a Cinderella story,” said Julie Diers, 47, who is a bubbly reading specialist at Clifton Elementary School.\nThe very first harvest, Hippie Chicks Hops secured a three-year contract with Odell Brewing Co. of Fort Collins, makers of handcrafted beers 90 Shilling Ale, Easy Street Wheat and Isolation Ale, among others.\nOdell brewers like to visit artisan farms, Diers explained, so when two Subarus full of Odell employees were on a Western Slope tour of hop yards last August, all the Hippie Chicks dressed up in sundresses and hosted a chicken and corn-on-the-cob barbecue, followed by a trip to the Palisade Brewing Co.\nBrewer Joe Mohrfeld of Odell blogged about the experience:\n“Lastly, we visited the new kid on the block … or rather a new school teacher on the block … Hippie Chicks Organic Hop Farm in Palisade. Julie, with the help of her volunteer Hippie Chicks, is in her first year and is already off to a beautiful start. We were so impressed with her operation that we decided to set up our sleeping bags right in her field and wake up amidst the hops to Palisade’s ‘Million Dollar Breeze.’ ”\nWhile it’s unusual that Diers had such success so soon, it does speak to both hops-farming as a growing industry in supplying Colorado’s 110 microbreweries, and to Diers’ resolve.\n“There’s certainly something to be said for determination,” she said.\nRon Godin, area agronomist for the Colorado State University Extension Service in the Delta office, said it was more than that: The school teacher did her homework.\n“The folks at Odell realized, when they walked into her hop yard, you can see the quality of the hops for the care she had given them,” Godin said.\nBack to where our heroine’s hops story begins.\nWhen Diers set her mind to owning a small farm for some additional income, she searched a year before she found just the right place, on G 4/10 Road. It had a carriage house, a garden spot, a big red barn and an alfalfa field.\nOh, and a house. She decided she wanted the place before she even turned a door knob on the 1909 home.\nThe whole tableau was so Norman Rockwell, Diers knew it was right for her.\n“It was just as precious as it could be,” Diers said.\nThe biology major/chemistry minor had grown grapes commercially before, but she wanted the potential of a better return, so two things happened that set her on the path to being a hops grower:\n1. She saw a hops vine growing on the patio of Kannah Creek Brewery and charged right up and asked the brewer if he’d buy hops from her if she grew them. He said yes.\n2. A neighbor advised her: “You know what you should do with this place? You oughta grow hops.”\nSo she read everything she could find about growing organic hops. She learned that hops, which are the flowers on the hops vines and look a little like pine cones, contain acids and oils that impart bitterness, flavor and preservative qualities to beer.\nIn the United States hops mainly are grown in the Pacific Northwest, but with the popularity of microbrews, the hop market no longer is limited to a handful of large, commercial breweries. Hops yards are springing up in the Midwest, in Utah, Colorado and elsewhere. These hops pioneers are learning which varieties grow best in their climates, and which pests and diseases might be a problem.\nDiers found a mentor in Glen Fuller of Rising Sun Farms in Paonia, the state’s first commercial organic hops grower, now in his fourth year of production. She volunteered on his farm and learned the ropes, literally — hops are trained on twine and grow on a trellis system some 18 feet high.\n“He’s so generous with the start-ups,” Diers said of Fuller.\nThen Godin, —“the nicest person on the face of the planet,” said Diers — suggested successful varieties and helped her lay out her own 1 1/2-acre hop yard. Godin some 10 years ago took an interest in hops and has become the unofficial epicenter of hops-growing in the state.\nIn fact, there is a Colorado Hops Growers Association forming. A first meet-and-greet meeting took place in January at Palisade Organic Hops Farm on G Road. The 3-acre farm is owned by David and Karen Pinnt. They’re in their second year of production.\nBy combining various growers’ efforts, they could save on supply shipping costs, seek research grants and process crops more efficiently, Karen Pinnt said.\nAnd also, “use each others’ knowledge and help each other out.”\nTo illustrate just how new hops growing is, Pinnt said that when they were standing up the poles for their hop yard, they were the subject of speculation.\n“We had so many people curious and we had them stopping by and ask us if we were building storage sheds, putting in grapes or if we were a training ground for linemen.”\nFor her hop yard, Diers had delivered 77, 44-long, beetle-kill pine poles. They had to be cut in half with a chain saw, then the bottom 4 1/2 feet stripped of bark and singed in order to delay rot once in the ground. There was augering, wire stringing and weeding — lots of weeding — to be done.\nHippie Chicks to the rescue.\nFellow teacher and longtime friend Nicolette Laurita is in the Hippie Chicks inner circle. She’s been there “from the beginning,” Diers said. Other Chicks include Hannah Odneal, Nancy Bartlett, Sophia Watchman, Barb Clark, Vivian Archuleta and daughter Abigail Diers.\nBoyfriend Russ Austin, sons Lucas and Patrick Diers, and intern Aaron Kinzig round out the Hippie Dudes, who provide brute strength and “mechanical skills,” Diers said. What does it take to get a dozen or so of your closest friends to give up evenings and weekends to help with your hop yard?\n“It’s totally the camaraderie,” Odneal said. “And the beer — the good beer.”\nLaurita pointed out Mount Garfield’s sculpted visage to the north, the blue shoulders of Grand Mesa to the east and the apricot and apple orchards on either side.\n“There could be a lot worse place to work,” she said.\nAnd the work is starting all over. Already they’ve spread 15 tons of composted chicken manure — “with a wheelbarrow and a shovel,” said Diers — removed old growth and recut furrows.\nSoon it will be time to cut back the first growth, restring twine, then train new bines.\nThere will be regular meetings of the Hippie Chicks, with agendas and pop quizzes. Diers is, after all a teacher.\nBesides, she wants people to know there’s some substance behind the spaghetti straps.\n“It’s just so fun to learn something new,” Diers said. “We actually have well-grown hops.”']	['<urn:uuid:87519bd9-fb45-442a-89fe-d9efc8738f47>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	11	27	1236
18	How did kayak construction evolve from traditional to fiberglass boats?	Early kayaks were made with animal skins over bone or wood frames, then canvas in the late 19th century. In the mid-1950s, fiberglass-epoxy composites revolutionized construction by eliminating the need for frames. Fiberglass itself is a glass fiber-reinforced plastic that is lightweight, durable, and easily molded into complex shapes.	"['January 9, 2018\nBeginnings of the Oregon Kayak and Canoe Club\nIn the Beginning\nToday’s kayaks are the result of a long evolutionary process that started with the kayaks used by the indigenous peoples of the Arctic regions. The early kayaks were used for hunting in the ocean and transporting goods needed for survival in the Arctic. In contrast, today\'s kayaks are for fun and sport.\nMost early kayaks had a single hatch for one person, the hunter, who was sealed with a skirt to keep out water. Some kayaks had two or more hatches. Kayaks for hauling goods were open on the top. Boats were constructed of ribs of graduated sizes made of bone, or wood from driftwood, spaced along the length of the boat. Longitudinal stringers were sewn to the ribs with animal sinews. The structure was covered with seal or sea lion skin. The boats were narrow and often 20 or more feet long.\nBy the late 19th century, canvas replaced the animal skins on both kayaks and birchbark canoes of North America. The canvas was stretched over the frame and waterproofed with pitch, or beeswax, or by rubbing clay into the fabric and painting it. Around the early 1900’s, the sport of kayaking first developed in southern Germany on rivers running from the Alps. The kayak was assembled on the bank and then inserted into its fabric skin. It was called das Faltboot, or foldboat. With the development of vinyl coated fabrics, boat skins were further improved, and both kayaks and canoes of this construction were popular into the 1950’s.\nPut-in, Wind River race, 1951. Few boats finished the race. Photo by Clyde Jones.\nThe Great Leap Forward\nIn the mid-50’s kayak construction changed drastically. Fiberglass-epoxy composites, developed in the 1940’s, allowed kayaks to be constructed without a frame. The hull and deck were each made from layers of fiberglass and epoxy. After curing, the hull and deck were united with a seam of fiberglass-epoxy completely around the kayak. In Europe fiberglass kayaks and decked canoes, which are very similar to kayaks, gave birth to the sport of whitewater boating. Slalom racing became very popular and the sport grew in Europe. Various kayak hull designs were developed but the hull length was standardized at 4 meters. That length was agreed upon to level the playing field for all slalom racers. These fast, sleek, maneuverable kayaks appeared in New England around 1960 and began to spread in the United States. From New England the fiberglass kayaks and the sport of whitewater kayaking spread quickly to the rivers of Appalachia, Minnesota-Wisconsin, The Ozarks, Colorado, California, and Washington.\nCanvas covered wood-frame tandem kayak-Foldboat. Willamette River, 1965\nMeanwhile, in Oregon…\nEven though Oregon had some of the finest whitewater rivers in the country, we were among the last places in the U. S. to see the sport of whitewater kayaking develop. When I came to the Willamette Valley in 1959 I saw no kayaks and only an occasional canoe.\nDuring the 1960\'s a few foldboat kayaks appeared. Some were the folding take-apart type, others were the fixed rigid-frame type. Being 16 to 18 feet in length, these kayaks were fast on flatwater, but in whitewater were difficult to maneuver through rapids while avoiding rocks or other obstacles. Tandem canoes were sometimes seen in Oregon. Most were made of aluminum and had a full-length keel. Canoers paddled mostly flat water and class 1 rivers.\nMcKenzie River drift boats, however, had been used in Oregon by fishers since the 1920\'s or earlier. Skilled rowers could run class 2 and 3 rapids. The boats were a smaller version of the New England dory used for fishing off the Atlantic Coast. Prince Helfrich of Eugene was a well-known drift boat guide who made first descents on several Oregon rivers, including the Owyhee.\nThus, the only boats in Oregon capable of running class 3 rapids were McKenzie River boats. But, things were about to change.\nThe Arrival of Scott and Margie Arighi\nScott Arighi, who had learned to kayak in Wisconsin, came to Oregon in 1964. He found it difficult to find other paddlers. In 1967 his friend Margie Smith came from Wisconsin as the co-leader of a University of Wisconsin Hoofers trip on the Lower Salmon River. They paddled an OC-2, tandem canoe, together. It went so well that Margie returned to Oregon a year later to marry Scott. However, Margie told me that “Soon after we were married, the canoeing harmony was over. We started paddling single kayaks and continued boating happily together ever since.”\nThe first kayak rolling classes in Oregon were taught by the Arighis, according to Scott “- - - in 1969 or 1970.” Scott was a chemistry professor at Reed College and got permission to use their pool. As the paddling group in the Portland area grew, things became more organized and they scheduled river trips. The date that the Arighis officially formed OKCC is not certain, but my old notes indicate, and the Arighis agree, it was probably 1969. Scott Arighi was the first president and Margie Arighi the first secretary. In 2015 at age 92, Bob Collmer told me that he was the first member to sign up.\nMargie and Scott Arighi, Founders of the OKCC in 1969\nThe earliest newsletter I have is a hand written single page post marked 27 April 1971. It is the Spring schedule from March 7 through May 31 with 19 trips scheduled. Most were on Saturdays and Sundays and include upper Wilson, upper Molalla, Nehalem exploratory, North Santiam, Washougal, Clackamas, Kalama, Sandy, upper McKenzie, and Toutle Gorge. The trip leaders included the Arighis, Clark Stanley, Bob Collmer, Lloyd Likens, Bev Karplus, and Jay and Jed Langley. The Arighis led nine trips!\nFiberglass Kayaks and Wetsuits. OKCC trip on the North Santiam River.\nAll boats were 4 meters (13 feet, 2 inches) long.\nNote that paddles were 90 degrees feather.\nThis small group of early OKCC kayakers was obviously very active. The OKCC grew rapidly as other members, including Rod Kiel, a kayaker from Germany, Spencer Beebe, and Hugh Dick joined the Club. The Arighis also were the first kayakers to explore many of the rivers in Oregon.\nBob Collmer, first member of the OKCC, 1969. Photo, John Day River, 1988.\nIn 1970 OKCC put on the first slalom kayak race in Oregon. It was on the Roaring River section of the Clackamas River. Some of the kayakers got knocked over in the race. Not all could roll back up.\nThe First Slalom Race in Oregon. Clackamas River, 1970\nI believe it was in 1971 that the Bob\'s Hole event was started on the Clackamas River. Bob\'s Hole was named for Bob Brietenstein because every time he kayaked the Clackamas River he tried to punch the hole, and every time it ate him. The event involved playing in Bob’s Hole. Judges gave points for the quality and finesse of each paddler. This became an annual event for many years.\nBob’s Hole Began circa 1971. Photo 1987\nIn the spring of 1970 the OKCC offered a beginning kayaking class organized by Scott and Margie Arighi who were also the principal instructors. Bob Collmer and Lloyd Likens helped with instruction and rescue. The first session was on the lower Sandy River where they taught basic strokes and eddy turns with a low brace. Eight of us beginners from Corvallis took the course and some of us continued boating regularly with OKCC. Later, in 1975, Chuck Leach and I, with others in the Corvallis region, formed our own club, the Willamette Kayak and Canoe Club. A few years later, a group of flat water canoers in Portland, who had been paddling together for several years, founded the Lower Columbia Canoe Club. The exact date is not known, but according to Dennis Deck, a canoer in that early group, it was probably 1979 or 1980.\nThe decked canoe made its appearance in Oregon in 1971 when Jay and Jed Langley joined the OKCC. The decked canoe is more like a kayak than a canoe. The cockpit, spray skirt, hull shape, and length are similar to a kayak and it behaves much like a kayak. Technically, it’s a canoe because the paddler kneels and uses a single blade paddle. Most of us had never seen a decked canoe before. About 1973, decked canoe paddlers Bruce and Genny Weber arrived in Oregon and also joined the OKCC. Bruce had raced in national competitions. The decked canoe never gained popularity in Oregon. The cramped kneeling position and a very small cockpit were quite uncomfortable, which resulted in its demise.\nFiberglass kayak punching Spencer’s Hole. North Santiam River. OKCC Trip, November, 1972.\nThe kayaker is the 16- year-old son of the author.\nFinding Boats and Equipment\nFinding kayaks for sale in the early years was very difficult because none of the manufacturers we know today existed. In Europe, however, kayaks were being manufactured. As boats made their way to the U. S. they could be replicated. In the late 1960’s a small fiberglass shop in Portland that built fiberglass water tanks began making kayaks from molds taken from two kayaks that OKCC members had purchased from Europe – a Prijon and a Mandesta, German and Italian slalom racing designs. The cost was $150. These were the first fiberglass kayaks made in Oregon. Quite a few were made in the early 1970’s, and they were popular among Oregon’s kayakers. All kayaks were still the same length, 4 meters or 13 ft 2 in. About this time the superior Mark IV kayak, from Germany, appeared in the eastern US and made its way to Oregon. Easy Rider in Washington State started making the German Augsburg kayak, a well-liked boat. The first roto-molded plastic boat, the Hollowform, appeared in the mid 1970\'s, but it\'s poor performance and susceptibility to hull cracking soon gave it a bad reputation. It was nicknamed “The Slug"".\nWe had to make our own spray skirts -- from coated nylon. Helmets were desired, but not available. Some boaters used bike helmets. I recall a three-day OKCC trip on the Deschutes River during which Rod Keil wore only a gentleman’s fedora, for sun protection. Flotation --- we hadn\'t heard of it, but when we did, we stuffed an old inner tube into the stern and a beach ball into the bow. Most paddles were made entirely of wood and were 86 to 92 inches (218 to 234 cm) long. Clark Stanley and Scott Arighi began making shorter paddles with larger, flat, fiberglass blades on aluminum shafts. All paddles had a 90-degree feather. If the blades were asymmetrical in shape, the paddle became either a right-hand or left-hand control.\nThe problem with fiberglass kayaks.\nThe standard was left-hand control, so most of us were left-hand control paddlers. By 1975 Bob Collmer of the OKCC began making very good paddles in his garage. The paddles had fiberglass blades on an aluminum shaft. I conducted bend and strength tests on his fiberglass–resin composites to help him optimize the types of epoxy and fiberglass lay-ups. Bob built many Collmer paddles until he sold his business to Hank Hays, an expert OC-1 paddler and an OKCC member. Hank continued the business as Lightning Paddles until selling the business in 2008 to a company in Germany.\nBy the beginning of the 1980s, fiberglass kayaks were essentially replaced by the new roto-molded plastic boats. These boats were much tougher and very resistant to hard knocks which would nearly destroy a fiberglass boat. It was about this time also, when the length of boats began shrinking. These shorter, tougher boats allowed boaters to make amazing moves with abandon and little concern over wrecking the boat.\nRiver Camping Trips\nThe Arighis loved extended river trips with camping along the river. Quite a few kayakers experienced their first overnight self-supported river trip by boating with the Arighis. During the 1960’s the Arighis were the first to lead kayakers down several sections of the John Day River. They also led overnight trips down the Grande Ronde River and the Deschutes River. These trips were always self-supported because there were no rafters to carry gear. Large rafts, outfitted with rowing frames, and rowed with the techniques used by driftboat oarsmen, had not yet appeared in this area. Such rafts existed, but they were owned and operated primarily by river guides on the Colorado Grand Canyon, the Rogue, the Salmon of Idaho, and a few other rivers. These rafts and those owned by individuals were war surplus life rafts from WW2. Rafts rapidly gained popularity because they were much more forgiving than wooden dories and driftboats.\nCampfires were normal. There was always plenty of firewood, but no chairs!\nThe Arighis developed packing systems that would allow them to easily put all the necessary gear into a kayak. Dry bags, as we know them today, had not yet been developed. But heavy-duty rubber bags were found in WW2 war-surplus stores, and they kept out water even better than the dry bags of today! Alternatively, we would double wrap items in plastic bags and stuff them into an abrasion resistant cloth bag, such as an Army canvas bivy sack. Margie developed minimalistic food packaging to an art. Food items were removed from their original packages and measured amounts of food and seasonings needed for a given one-pot meal were mixed and sealed in a small plastic bag. To prepare the meal, food was put into a pot of water and cooked. No fresh fruits or vegetables were allowed -- these had to be dehydrated to save space and weight. We took turns preparing the group dinner. This was efficient and provided a variety of excellent dinners. It also allowed everyone else some free time. To further save weight, neither a cook stove nor fuel was carried. Wood was plentiful at any campsite, so we cooked on wood fires. Cooking utensils and dishes were washed in the river. The toilet system was a trowel or the heel of a shoe for digging a hole. Not a problem, because no one would likely use that campsite again for several years and if so, it would probably be one of us.\nChuck Leach. River camp. Cooking a group dinner.\nWashing dishes in the river after dinner.\nThe Arighis were explorers. They led OKCC kayakers on many first descents, including the upper Molalla, upper McKenzie, Trask, Nehalem, and Little North Santiam Troll’s Teeth section. It is difficult for most kayakers today to realize how little information about the rivers in Oregon was available. Most of the put-ins and take-outs of today did not exist. The character of a river was often unknown – were there big pour overs, boulder gardens, log jams? We soon learned that asking the local folks about the details of a river was futile.\nWhile exploring rivers, Margie would take notes of rapids, river features, landmarks, camp sites and estimated distances. She wrote with a grease pencil on a sheet of plastic secured to the front deck of her kayak. The exploring and note taking by the Arighis culminated in their 1974 book, Wildwater Touring, published by MacMillan Co., which was among the very early whitewater kayaking books in the U.S. The book gave information and precautions on running rivers, packing for extended trips, maps showing major rapids, campgrounds, and mileage for several major rivers in Oregon and the Salmon River in Idaho.\nOne impetus for writing the book was the need for more private boaters on the rivers to fight for river protection and private boater rights. In the 70\'s and 80\'s Scott went to Salem several times to lobby against requiring boat licenses for kayaks and canoes. Margie published several articles in American Whitewater.\nI do not know much about OKCC after 1980 because I was involved with the Willamette Kayak and Canoe Club. The last time I boated with OKCC was in 1993 on an early trip down the Salmonberry River.\nThe author welcomes any additional pertinent information. Contact: email@example.com\nThank you Scott and Margie Arighi\nThank you for introducing whitewater kayaking to Oregon and founding the Oregon Kayak and Canoe Club. You instructed and encouraged many boaters along the way.\nI paddled many a mile with you, who were generous enough to invite me on many of your overnight river camping adventures. These include my first of many trips down the Owyhee from Three Forks to Rome, the Selway River, and sea kayaking among islands in British Columbia.\nNote: The Arighis are still living in the Portland area and Margie still kayaks the John Day River.\nAll photos are by the author, except the first one.', 'Fiberglass is currently the most common material used to craft small recreational boats and other watercraft.\nChances are, if you have purchased a recreational boat, it is made of fiberglass, also known as Glass Reinforced Plastic or GRP.\nIn this article, we will look at how long fiberglass boats last and the factors that impact fiberglass boats’ durability.\nHow Long Do Fiberglass Boats Typically last?\nFiberglass boats can be sound and seaworthy for up to fifty years or more. Fiberglass is very durable, and with proper maintenance and care, fiberglass boats can last for many decades.\nFiberglass itself will not break down but instead will break down due to outside factors.\nSome factors that will affect break down are:\n- Exposure to UV rays\n- Fatigue from movement\n- Water saturation\n- Salt from seawater.\nToo much UV rays can cause the fiberglass to become more brittle. Because most boating happens during sunny weather conditions, this is a highly probable issue.\nWater saturation can cause a breakdown between the fiberglass and the resin. This is most often caused by acid formation with the water and products hidden in the fiberglass.\nSalt from saltwater can move between the fiberglass and become deposited in the fiberglass’s larger porous areas. This causes the salt to add to the pressure on the fiberglass.\nOne final issue could be poor production at the beginning of the boat’s life. This is hard to combat, and you will want to ensure you purchase a high-quality boat.\nOne of the biggest issues you will encounter with your fiberglass boat is the other components. Your fiberglass is much more likely to last than your other structural, engine, and electrical components.\n3 Factors That Impact The Durability Of Fiberglass Boats\nSo, what if you already have a fiberglass boat?\nYou’re probably thinking: what should I look out for?\nThe main things you want to look for in your fiberglass are:\n- Small cracks\n- Water damage\n1) Small Cracks\nTo find aging damage, you will want to look for microscopic cracks.\nAt first, they might seem minor, almost hairline. These cracks should not be ignored as they can get bigger with time—the bigger the cracks, the more pressure is applied to the fiberglass’s structure and integrity.\nThis break in the integrity of the fiberglass can further compromise the structure of your vessel. For this reason, fiberglass boat owners need to monitor their boats for cracks constantly.\nWhen cracks are found, they need to be dealt with and filled in.\nFiberglass can also experience fatigue.\nVibration and impact on the fiberglass can cause stress and fatigue. Repetitive waves, engine vibrations, and other activities can cause strain on the fiberglass.\nAfter a certain amount of time, fiberglass can also get worn down. Without proper care, the fiberglass can become weak and brittle.\n3) Water Damage\nYou need to ensure your fiberglass is not experiencing water damage.\nWhile the resin itself is waterproof, if water gets in, it can damage the fiberglass. Eventually, after enough water is absorbed, the damage will apply more and more pressure, which can cause wear, blistering, and cracking.\nThe part of the boat that is most susceptible to this damage is the part of the hull that is below the waterline.\nThe last thing to look out for is heat and sun damage.\nWith enough heat and UV rays, the fiberglass in the hull can become rigid and brittle. This can even cause warping.\nOne major issue with a fiberglass hull is that warping or major damage to the fiberglass itself can be challenging and expensive to repair.\nAs previously expressed, if you own an older boat, your main issue is most likely not in the fiberglass. That doesn’t mean that you should not need maintenance and monitor your fiberglass. Vigilance, maintenance, and preventative care are the best way to maintain your fiberglass’s structural integrity.\nHow To Prolong The Life Of Fiberglass Boats\nSome people will try to tell you that your fiberglass “does not need maintenance.”\nThis is not true.\nCompared to other types of boats, especially wooden boats, fiberglass is much less maintenance. This does not mean that there is no maintenance involved, and you must keep up with it.\nMost importantly, when maintaining your boat, you need to protect the bottom constantly exposed to water.\nIf your boat is left in the water for even a few days, you will want to attempt to protect it from algae and other growth. To prevent this, there are certain protectants and bottom paints you can apply.\nYou will want to ensure whatever growth repellent you purchase works for the area and body of water you plan on having your boat in. Before application, you will want to make sure to properly sand or otherwise prepare your fiberglass.\nThe area between the railing and the waterline needs to have regular maintenance done because this part of your boat takes on most of the waves, spray, and sun. Without proper care, this area will fade, oxidize, and get hazy. To combat this wear, you need to make sure you keep up the wax on your boat.\nIf waxing becomes a hassle, you can get a buffer. This will enhance the process and make it easier to complete.\nThe deck of a boat also sees a lot of wear. This comes from sun, dirt, grime, spills, and other general wear and tear. The main way to combat this is regular washings and a good stiff brush.\nBe sure never to wash a “non-slip” area with wax. This is highly important on the deck where people walk. Safety should always be your priority.\nBesides this maintenance, you will want to maintain cleanliness on all other aspects of your boat. This will help to keep the condition pristine and avoid aging and wear.\nFinally, no matter what type of boat you own, you will want to maintain the engine properly. You want to make sure you maintain your boat engine constantly.\nThis is true if you use your fiberglass boat every day or only occasionally. If you only use your boat occasionally, you will still want to maintain the engine as if you use it regularly.\nMaintaining your boat engine is like maintaining your vehicle engine. You will want to make sure it is properly oiled fueled, and the boat battery is charged.\nLike a car, keeping a boat’s radio or lights on for a long time can drain a battery and even kill it.\nHow To Spot Aging Signs On Used Fiberglass Boats\nWhen buying an old fiberglass boat, especially a “fixer-upper,” there are some things to keep in mind.\nUnlike wooden trim items, fiberglass boat structural issues are more difficult to repair or restore.\nRestoring a fiberglass hull can be almost impossible, depending on the wear accrued. Because these hulls are made all at once, patching them is your only option if the problem is that simple.\nWooden boats are more maintenance and more difficult to maintain, but they do allow for the replacement of parts by competent and qualified artisans.\nWhen purchasing a used fiberglass boat, you will want to look out for the signs listed above. You will need to know if you are looking at any structural issues.\nMake sure to check for cracks, fading, wear, growth, or damage. Look at the high-stress areas for cracks, wear down, or other signs of damage.\nThe pressure created in the fiberglass can cause issues for you later.\nAs well as fiberglass wear, you will want to make sure that your hull is not chipped, blistered, or starting to flex excessively.\nStress on a fiberglass hull can cause flexing as degradation occurs. This can cause major cracks and severe hull issues.\nIssues found with a boat can be fixed. Based on the extremity of the issue and your experience, you should look for a professional opinion on maintenance and damage restoration. This is important before you buy. Taking on too many issues may quickly make your new boat purchase a headache you never wanted and were not prepared for.\nIf everything is intact with the boat, and you do not see any cracks, warping, mechanical failure, or other issues, it would be safe to purchase the used fiberglass boat.\nIf you keep up with the proper maintenance, a used older fiberglass boat could last you many years to come.\nWhy Do Fiberglass Last So Long?\nFiberglass is a form of plastic that is reinforced using glass fiber. The glass fiber is usually woven into a fabric. This makes the fiberglass superior when it comes to durability.\nFiberglass material is relatively lightweight, extremely durable, and less brittle than standard glass or plastic.\nFiberglass can be easily molded and is often used to make more complex shapes. Standard uses for fiberglass are boats, aircraft, bathtubs, etc.\nFor a boat to be considered a fiberglass boat, the hull, deck, liner, and even large parts of the console are made from fiberglass.\nFiberglass boat construction in a mold first uses a gel coat; fiberglass cloth layers are applied, with resin applied on each layer.\nBefore fiberglass, boats were constructed from bark, wood, animal skin, iron, or steel. Today, larger ships are still constructed from aluminum or steel, while smaller recreational boats are made from fiberglass or GRP.\nFiberglass boat construction was experimented with as early as the late 1930s and is the main manufacturing method.']"	['<urn:uuid:195e1277-df3f-469a-9310-f44624f7410f>', '<urn:uuid:e2437e29-1570-448f-98ca-a5b77dbb93e5>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	10	49	4357
19	mineral supplementation effects on zinc levels compare cattle feedyard versus horse amino acid chelates	In cattle feedyards, even with adequate ration levels, zinc serum levels don't return to normal until after 60 days in the feeding program. For horses, zinc supplementation is more effective when provided as amino acid chelates, as these are more bio-available and better absorbed through the intestinal lining compared to non-chelated forms that are mostly eliminated as waste.	"['With the fall run of calves knocking at the door, it pays to keep in mind management associated with a decreased incidence of mycoplasma (non-responsive pneumonia and arthritis). As researchers at Kansas State University (KSU) noted in the mycoplasma survey they conducted among stockers and backgrounders in 2001, ""As mycoplasma appears to be an opportunist occurring most frequently during times of stress or when a calf\'s immune system is weakened, management programs should focus on those procedures that can get calves started out in the right direction.""\nRecommendations by KSU researchers include:\n- Don\'t feed poor-quality hay, or hay in a form not easy for incoming cattle to eat. The relationship between poor nutrition and increased susceptibility to disease has long been recognized. Feed intake during the receiving period is typically low which potentiates the stress effects of shipment, processing and illness (Hutcheson and Cole, 1986).\nCalves need a high-quality, palatable diet on arrival. A high percentage of survey respondents were using native-grass hay in receiving diets. Not all native-grass hay is created equal. In Kansas, forage quality deteriorates monthly from peak protein values in May and June until September with crude protein values declining from a peak of around 9% to 4% (Baker and others, 1999).\nThe best way to know what you\'re feeding is to get your hay tested before the cattle start arriving. You can then build a receiving ration to match the needs of stressed cattle using a readily available hay commodity.\nProtein concentrations in the entire receiving diet should be in the 13.5-14% range. Limiting dietary protein can decrease immune function and increase susceptibility to respiratory pathogens. Calves already sick have decreased appetites and need additional protein in their diets to offset lowered intakes.\nIf using native hay in receiving diets, feed it in a form that minimizes the amount of time a calf has to work at eating. Unbroken, large, round bales require a lot more effort to eat and may limit the number of calves eating at one time. Breaking hay out into bunk-line feeders and top-dressing the protein and energy portion of the ration, or using a complete ration during the first two weeks, will increase consumption.\n- Provide a trace-mineral program that meets or exceeds recommended allowances for the weight of calf purchased. A nationwide sampling of zinc content in forage samples found only 2.5% to have adequate levels of >40 ppm (Corah and others, 1996). It appears most pasture-management programs require some form of mineral-supplementation program.\nSeveral trace minerals, including zinc, are critical for proper immune-system function. If the likelihood of receiving cattle from an area where forage zinc is low isn\'t risky enough, zinc serum levels will also decrease during transportation and stress. In a recent survey of feeder cattle by the authors, serum zinc levels on arrival were found to be deficient in 35% of incoming cattle sampled. In the same operation, 30% and 55% of cattle sampled at first treatment or at re-pull for treatment, respectively, were found deficient. Cattle didn\'t appear to have serum zinc levels return to normal until more than 60 days in the feeding program, even though ration levels were adequate.\nPasture mineral supplementation programs will carry over into the feedyard program (Greene and Chirase, 1998). In a Nebraska mineral-supplementation study, cattle receiving supplemental trace minerals (zinc, copper, manganese and cobalt) during the summer-grazing period had significantly fewer sick calves and fewer treatments per episode than unsupplemented cattle (Grotelueschen and others, 2001).\n- Get control of a respiratory disease early. Metaphylaxis is the group treatment of high-risk cattle with antibiotics before clinical signs of illness are present. The survey indicated a significant difference in the frequency of use on affected operations as compared to operations not receiving affected loads. That begs the questions: ""Did it cause the problem?"" or ""Did they use metaphylaxis in an effort to prevent affected loads because they\'d had affected loads before?""\nNeither question is answerable. In the final analysis, metaphylaxis didn\'t appear to play a significant role -- data suggests that within operations using metaphylaxis there didn\'t appear to be any relationship between affected loads and unaffected loads receiving the procedure.\nMetaphylaxis is a proven management practice to help reduce sickness, chronics and death loss rates in high-risk cattle. Its usefulness has been shown over many research trials and remains a practical management tool for targeted loads of cattle.\n- Minimize additional stresses at processing. If you can\'t buy steers and clean-headed cattle, delay those procedures for about 30 days post-arrival. Cramming them on top of everything else at arrival just adds to the stress load.', 'The topic of supplements has become quite complex. Knowing the basic nutritional elements your horse requires can improve his health and well-being.\nNutrition isn’t an exact science. When it comes to designing a nutritional program that includes the right supplements for optimum health and performance, there are many conflicting opinions. With the advent of processed and engineered foods, nutrition has become an even more complicated issue. In order to achieve perfect health, we’re left to research and study these complexities for ourselves and our equine companions.\nOne of the most critical mysteries lies in the area of supplements, such as vitamins and minerals. How much should you and your horse take? What does the Recommended Daily Allowance (RDA) really mean? Are you and your equine partner getting enough, or too much, of what your bodies need? Is the balance appropriate? How are you to know?\nAs we begin to explore basic nutritional requirements, let’s first discuss the importance of whether the nutrients provided in supplements are being used by our bodies or eliminated as waste products.\nHow we process nutrients\nMost of the nutrients in the foods we consume are absorbed in the gut. This applies to all animals – including humans and horses. The nutrients enter the system through the walls of the small intestine. So it’s critical that we accomplish the following in order to achieve good health and optimum performance:\n•Ensure that the gut is healthy and functioning properly.\n•Consume foods that contribute to optimal digestion and maintain proper gut function.\n•Ensure that any added supplements (vitamins and minerals) are bio-available.\n•Ensure that the intake of supplements is properly balanced.\nAn “essential mineral” is one that is required for survival, or at least for health. Essential minerals include calcium, chlorine, chromium, cobalt, copper, iodine, iron, magnesium, manganese, molybdenum, phosphorus, potassium, selenium, silicon, sodium, sulfur, and zinc.\nMinerals play fundamental roles in all living organisms. Many are active parts of enzymes that perform chemical conversions in our bodies. Others are structural components of certain tissues – such as silicon, which strengthens connective tissue, nails and hooves. Still others serve as regulators and signalers; potassium, for example, is involved in controlling the activity of nerve cells.\nWhat is bio-availability?\nIn order for minerals to be of maximum use, their chemical makeup must be usable by the body, so the body can absorb and utilize them to meet its needs. This is what is meant by bio-availability. “Minerals enter the body mostly as components of food,” writes Russell Mills in Mineral Transport. “To become bio-available, mineral atoms must be combined with other elements to form chemical compounds. For example, the body gets most of its manganese not from nodules of manganese metal, but from manganese-containing substances in food — that is, from plant and animal tissues in which the manganese is already incorporated into enzymes similar to those that our own bodies will make from it.\n“There is thus a great deal of mineral recycling going on in the biosphere: mineral compounds travel from one organism to another when the latter eats the former,” Mills continues. “But not all of the minerals found in biological organisms are the result of such recycling of mineral compounds. Some minerals enter the biological world when elemental minerals are converted to chemical compounds in the soil or elsewhere. As elemental minerals get converted to mineral compounds, these simple inorganic compounds become bio-available to plants. The plants then perform the conversion to more complex organo-mineral compounds which makes them bio-available to other organisms – to animals, for example.”\nWhen mineral compounds are consumed in food, the body must somehow absorb the minerals from the digestive tract and make them available to the tissues and cells where they’re needed. This process is not a simple one.\nSo how do these essential minerals become bio-available? By attaching an amino acid to the mineral component, forming an amino acid chelate. Amino acids act as chelators when they react with positively charged metal atoms (the minerals, in this case), forming a strong chemical bond.\nThe argument in favor of using amino acid-chelated minerals goes like this: the body is very efficient at absorbing amino acids. Dipeptides (two amino acids linked together via the amino group of one and the acid group of the other) are especially well absorbed thanks to a dedicated transport system found in cells of the intestinal wall. When mineral atoms are strongly bonded (i.e., chelated) to dipeptides, they get dragged by the dipeptides across the intestinal lining and into the body.\nMinerals that enter the body in an inorganic (non-chelated) form do not efficiently pass through the lining of the intestines, and we eliminate them as waste through our kidneys and digestive system.\nWhat to look for\nNow you know why chelated minerals are so important. They are bio-available to your body, and your horse’s, and can be utilized to improve health and performance.\n1. Always look for amino acid chelates on your nutritional labels.\n2. Avoid comparing the amounts of chelated and non-chelated minerals listed on ingredient labels. You won’t be comparing apples to apples. For instance, if you think that 500 mg of non-chelated calcium looks better and more powerful than 250 mg of chelated calcium, you will be mistaken.\n3. Search for products with a proper balance of chelated minerals. Minerals interact with one another in the body. Too much of one will lead to a deficiency of others that compete with it, creating an out-of-balance situation in the body. The balance of minerals is a complicated topic best left for another article; the important thing is that you look through the ingredients and make sure you find at least one chelated form of each mineral (or most of them).\nBe a smart consumer\nAs you can imagine, the chelation process makes the production of mineral supplements more costly. Non-chelated, non-bio-available mineral supplements may be less expensive – but they’re not useful to the body. In order to draw consumers, or offer a product that boosts only one or two minerals to achieve a specific result, manufacturers often use only a few chelated minerals. They then refer to their product as containing amino acid chelates. But since only a few of the minerals are bio-available and absorbed by the body, you are creating an imbalance by using these products. Imbalances result in mineral deficiencies, so this is something to be very cautious of when researching supplements for yourself and your horse.\nProducers of animal feeds are notorious for offering chelated forms of copper in their foods. This gives the animals a briefly healthy “bloom” in the coat color. Many old-timers in sale barns used a similar technique by putting pennies in the water buckets for a few weeks before a sale. This would give the horses a brighter coat color for a short time and make them more appealing to potential buyers. However, as the body continued to absorb copper without an appropriate matching amount of zinc, the “sister” mineral to copper, the new owners began to see signs of a zinc deficiency, such as poor hoof quality, decreased coat condition, and wood chewing behavior. Horses low in zinc also often lick or rub their teeth on galvanized steel gates as they seek more zinc.\nTo keep your horse and yourself healthy and happy, you need to fully understand what you’re putting into your bodies. Always read nutrition labels – what looks good on the outside may not be so great on the inside!']"	['<urn:uuid:166af373-337f-43e1-ad43-b4cda113945f>', '<urn:uuid:eac11fda-a499-4b39-b794-cf84a8807df3>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T04:31:27.499155	14	58	2020
20	what makes modern rnb music different from traditional songs original instruments used	Modern RnB has evolved to incorporate electronic sounds and production techniques similar to modern pop music, while still maintaining its jazz, blues and soul essence. Contemporary RnB commonly features electric or acoustic guitars, synths, keys, and modern drum beats with effects like reversed sounds. Sometimes horn sections from jazz and soul music are also included.	['The aim of our spotlights on different song genres is to make it easy for Music teachers to bring to life the different contemporary music styles. In addition, for English teachers to have the knowledge and a great instrumental track for pupils to write song lyrics in a style of music they choose.\nRnB are the initials for Rhythm and Blues, a genre of music that originated in African-American communities in the 1940s. That was the time where musicians transformed jazz based music by using new popular instrument like electric guitar and bass and more heavy consistent drum beats.\nSome artists who made this genre well known were Chuck Berry, Ray Charles, Little Richard. Nowadays, RNB music has evolved a lot, using electronic sounds and production technics like in modern pop music. But the essence is still the same as at its beginning, some Jazz, some Blues and a lot of Soul.\nWe’ve looked at some of the key features of instrumentation, rhythm & beat and chords & harmonies in RnB songs. Plus we’ve selected some RnB songs to have a listen to. Pupils can use our demonstration instrumental track to write their lyrics & melodies over.\nIn contemporary RNB, it is very often to hear electric or acoustic guitars. Synths and keys are used to add more colours and its even possible to hear sometimes horns sections which comes from jazz and soul music. And of course all kind of drum beats with modern effects, reversed sounds and noises that add character to a track.\nRhythm and Beat\nRNB evolved a lot through time and it is difficult to define one typical groove for this kind of music. The early “Rhythm and Blues” sounded very similar to Rock”n”roll. It had a fast tempo with the intention to make people dance. Nowadays RNB tends to be slower than typical dance Songs and they are sometimes even very slow. Most songs are in 4/4 which means that the rhythm goes 1, 2, 3, 4 but RNB is a genre that has a lot of songs using 6/8 rhythms like in the demo instrumental.\nChords and Harmonies\nPop and RnB music seems to be very similar nowadays because they are both very popular, very flexible, they use both acoustic and electronic sounds. But the roots of RNB are Jazz and Blues so you need to play jazzy chords to get the colours of a H.e.r or Jorja Smith´s song. By adding the 7th of each chords like Am7 instead of just Am, it will already get much closer to the authentic RNB sound.\nDemonstration instrumental track\nYou can use our demonstration instrumental track below to have a go at putting your lyrics and melodies over a RnB style instrumental. The Demo instrumental has a 6/8 feels which means that we count 1, 2, 3, 4, 5, 6. The song structure of this instrumental track is:\nIntro x4 (bars)\n2nd Verse x8\n2nd Prechorus x8\nFollowing songs are here to help you understand the groove of it, make you discover variations with different moods and also inspire you with melodic ideas.\nFALLIN – ALICIA KEYS\nOne of the most well known RnB songs of the 2000s. Alicia Keys loves to use the acoustic piano everywhere and a lot of backing vocals to add more dynamics while the electronic drum beat is almost just a loop.\nFORFEIT – KIANA LEDÉ\nTypical 6/8 guitar based RnB song with many movements and variations in the vocal melodies.\nCOMFORTABLE – H.E.R\nIntimate song with simple hooks with the choruses that leaves a lot of space for the music to vibe.\nIF YOU LET ME – SINEAD HARNETT\nThis doesn’t have a 6/8 feel but it is a great song with a lot of energy and passion although it is very slow and uses only electronic sounds.\nDANGEROUS WOMAN –\nThis would probably be described more as a Pop song rather than a RnB one but it is also in 6/8 and Ariana Grande is anyway known to have sung anything from EDM, Pop, HipHop, Jazz and of also RnB.']	['<urn:uuid:e13e954f-2766-4c5d-b37f-5cd25290a3fb>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	12	55	682
21	sensory memory types duration characteristics compared alzheimer disease memory symptoms progression	Sensory memory has two types: iconic (visual) memory lasting 0.3 seconds, and echoic (auditory) memory lasting 2-3 seconds. In contrast, Alzheimer's disease causes progressive memory deterioration where short-term memory deficits become increasingly severe, starting with mild memory loss and progressing to severe impairment requiring assistance with daily activities and essential functions.	['Memory Process by which information is: Acquired Encoding Stored in the brain Storage Later retrieved Retrieval Eventually (possibly) forgotten\nInformation-Processing Model of Memory Computer as a model for our memory Three types of memory Sensory memory Short-term memory (STM) Long-term memory (LTM) Can hold vast quantities of information for many years\nInformation-Processing Model of Memory Retrieval Attention Encoding Sensory Short-term Long-termStimulus memory memory memory Forgetting Forgetting Forgetting\nSensory Memory Stores all the stimuli that register on the senses Lasts up to three secondsSensory Two types Sensory Iconic memory Input Memory Visual Usually lasts about 0.3 seconds Sperling’s tests (1960s) Echoic memory (we’ll come back to this)\nSensory Memory Echoic memory Sensory memory for auditory input that lasts only 2 to 3 secondsWhy do we need sensory memory?\nShort-term Memory Limited capacity Can hold 7 ± 2 items for about 20 seconds Maintenance rehearsal The use of repetition to keep info in short-term memory CHUNK Meaningful unit of information Without rehearsal, we remember 4 ± 2 chunks With rehearsal, we remember 7 ± 2 chunks Ericsson & Chase (1982) 893194434925021578416685061209488885687727 31418610546297480129497496592280\nLong-term Memory Once information passes from sensory to short-term memory, it can be encoded into long-term memory Retrieval Attention EncodingSensory Sensory Working or Long-term Memory Short-term memory Input Memory\nLong-term memory - Encoding Elaborative rehearsal A technique for transferring information into long- term memory by thinking about it in a deeper way Levels of processing Semantic is more effective than visual or acoustic processing\nLong-term memory Procedural (Implicit) Memories of behaviors, skills, etc. Demonstrated through behavior Declarative (Explicit) Memories of facts Episodic – personal experiences tied to places & time Semantic – general knowledge\nRetrieval Retrieval Process that controls flow of information from long-term to working memory store Explicit memory The types of memory elicited through the conscious retrieval of recollections in response to direct questions Implicit memory A nonconscious recollection of a prior experience that is revealed indirectly, by its effects on performance\nRetrieval – Explicit Memory Retrieval failure Tip-of-the-tongue (Brown & McNeill) Retrieval failure is a common experience. Have you ever felt as thought a word or name you were trying to recall was just out of reach – on the tip of your tongue?\nRetrieval – Explicit Memory Context-Dependent Memory We are more successful at retrieving memories if we are in the same environment in which we stored them State-Dependent Memory We are more successful at retrieving memories if we are in the same mood as when we stored them\nRetrieval – Implicit Memory Showing knowledge of something without recognizing that we know it Research with amnesics Déjà vu The illusion that a new situation is familiar\nForgetting Lack of encoding Often, we don’t even encode the features necessary to ‘remember’ an object/event Decay Memory traces erode with the passage of time No longer a valid theory of forgetting Jenkins & Dallenbach (1924)\nForgetting Repression There are times when we are unable to remember painful past events While there is no laboratory evidence for this, case studies suggest that memories can be repressed for a number of years and recovered in therapy\nInterference theory Forgetting is a result of some memories interfering with others Proactive interference Old memories interfere with ability to remember new memories Retroactive interference New memories interfere with ability to remember old memories Interference is stronger when material is similar\nMemory Construction Schema theory Illusory memories People sometimes create memories that are completely false\nImproving Memory Practice time Distribute your studying over time Depth of processing Spend ‘quality’ time studying Verbal mnemonics Use rhyming to reduce the amount of info to be stored\nImproving Memory Interference Study right before sleeping & review all the material right before the exam Allocate an uninterrupted chunk of time to one course Context reinstatement Try to study in the same environment & mood in which you will be taking the exam\nPhotographic memoryThe problem essentially the confusion overthe term and many individuals believe thata photographic memory is supposed topresent a photographic image in their brainof the information.', 'Memory disorders are disorders of cognition, the ability to reason, remember, make decisions and communicate. Our team has in-depth experience treating a wide variety of memory disorders, including dementia, Alzheimer’s disease, mild cognitive impairment, vascular cognitive impairment and hydrocephalus.\nMemory disorders can be caused by one or more factors, including:\n- substance abuse\n- heredity (inheriting genes associated with Alzheimer’s or Huntington’s disease)\n- narrowing of the arteries that provide blood flow to the brain\n- cardiovascular disease\n- untreated infectious or metabolic disease\n- brain tumors\n- vitamin deficiencies.\nSome types of memory disorders can appear suddenly, while others may be present years before symptoms become apparent. A skilled neurologist can help determine whether the progression of a memory disorder can be slowed or even reversed entirely.\nTypes of Memory Disorders\nMore than 100 health conditions are associated with cognitive decline, the ability to reason, remember, make decisions and communicate.\nClick to expand a topic below and learn more about the different types of memory disorders:\nAlzheimer disease is a brain illness that can happen usually in older adults, but it can also happen as early as age 40. It is the most common cause of dementia. It is a progressive disease. This means it gets worse over time. It afflicts an estimated 5.3 million Americans, and it is the seventh leading cause of death.\nAlzheimer disease causes a series of changes to nerves of the brain. Some nerves form into clumps and tangles, and lose some of their connections to other nerves. Age is the most important risk factor for Alzheimer’s disease. Other risk factors include heredity, diabetes, hypertension, traumatic brain injury and poor nutrition.\nThe disease causes changes in behavior and thinking known as dementia. The symptoms include:\n- Memory loss\n- Personality and behavior changes\n- Problems with judgment\n- Problems communicating with others\n- Inability to follow directions\n- Lack of emotion\nAs Alzheimer’s disease progresses, short-term memory deficits become more severe and interfere more with the maintenance of daily activities. In moderate Alzheimer’s disease, individuals typically require more assistance with daily activities, and as the disease progresses may even require assistance with essential daily activities, such as preparing a meal or requiring reminders to attend to hygiene. n severe Alzheimer’s disease, individuals are almost entirely dependent on others to ensure adequate hygiene and proper nutrition, and they may require prompts or assistance for common functions such as using the toilet.\nLearn more about the treatment for Alzheimer’s Disease.\nEncephalitis is inflammation and swelling of the brain. This leads to changes in neurological function, resulting in mental confusion and seizures. Viruses are the leading cause of encephalitis. Vaccines for many viruses, including measles, mumps, rubella, and chickenpox have greatly lowered the rate of encephalitis from these diseases. But, other viruses can also cause encephalitis. These include herpes simplex virus and rabies. Encephalitis can also occur after an infection caused by disease-carrying agents including ticks (Lyme disease), mosquitoes (West Nile virus), and cats (toxoplasmosis). Encephalitis can also be caused by bacteria.\nIn addition to cognitive impairments, a wide variety of other neurologic manifestations may occur. These include:\n- Parkinsonism – slowness of movement, increased rigidity in the arms and/or legs, problems with walking\n- Weakness or sensory changes affecting one side of the body\n- Problems with speech, swallowing, double vision or other “focal” neurologic symptoms\n- Loss of ability to perform learned motor movements\n- Inattention to visual or sensory stimuli on one side (e.g., ignoring these things on the left side of the body)\nLearn more about the treatment for Autoimmune Encephalopathy.\nDementia is the name for a group of brain conditions that make it harder to remember, reason, and communicate. Dementia is a descriptive term rather than a diagnosis. The most common form of dementia is Alzheimer disease. Other types include vascular dementia, frontotemporal dementia, and Lewy body dementia. Years ago, dementia was often called “senility.” It was even thought to be a normal part of aging. We now know that it’s not normal. It’s caused by ongoing damage to cells in the brain.\nSymptoms differ depending on which parts of the brain are affected and the stage of the disease. The most common symptoms include:\n- Memory loss, including trouble with directions and familiar tasks\n- Language problems, such as trouble getting words out or understanding what is said\n- Difficulty with planning, organizing, concentration, and judgment. This includes people not being able to recognize their own symptoms.\n- Changes in behavior and personality\nDementia is a progressive disease. This means it gets worse over time. Symptoms differ for each person, but there are 3 basic stages. Each may last from months to years:\n- In the early stage, a person may seem forgetful, confused, or have changes in behavior. However, he or she may still be able to handle most tasks without help.\n- In the middle stage, more and more help is needed with daily tasks. A person may have trouble recognizing friends and family members, wander, or get lost in familiar places. He or she may also become restless or moody.\n- In the late stage, Alzheimer’s can cause severe problems with memory, judgment, and other skills. Help is needed with nearly every aspect of daily life.\nLearn more about the treatment of dementia.\nFrontotemporal dementia (FTD), a common cause of dementia, is a group of disorders that occur when nerve cells in the frontal and temporal lobes of the brain are lost. This causes the lobes to shrink. FTD can affect behavior, personality, language, and movement.\nThese disorders are among the most common dementias that strike at younger ages. Symptoms typically start between the ages of 40 and 65, but FTD can strike young adults and those who are older. FTD affects men and women equally.\nThe most common types of FTD are:\nFrontal variant. This form of FTD affects behavior and personality.\nPrimary progressive aphasia. Aphasia means difficulty communicating. This form has two subtypes: Progressive nonfluent aphasia, which affects the ability to speak. Semantic dementia, which affects the ability to use and understand language.\nA less common form of FTD affects movement, causing symptoms similar to Parkinson disease or amyotrophic lateral sclerosis (Lou Gehrig’s disease).\nThe cause of FTD is unknown. Researchers have linked certain subtypes of FTD to mutations on several genes. Some people with FTD have tiny structures, called Pick bodies, in their brain cells. Pick bodies contain an abnormal amount or type of protein.\nDamage to the frontal lobe of the brain may impact important functions. Common symptoms involve dramatic changes in behavior and personality. These may include:\n- An increased tendency to make socially inappropriate comments or actions\n- Decreased empathy, or new difficulties understanding how one’s actions may impact others\n- Difficulties with logical judgments or understanding the relationship between cause and effect\n- Changes in sexual behaviors\n- Aggressive behaviors or actions\n- Decline in personal hygiene, toileting habits, etc.\n- Severe mental rigidity\n- Language abnormalities, such as being unable to express language, find words or understand the meaning of words\n- Inattention, increased distractibility or a tendency to jump from one topic to another\n- Difficulty initiating or completing tasks\n- Significant changes in eating patterns\nIn addition to cognitive impairments, neurologic symptoms may occur including:\n- Parkinsonism: slowness of movement (bradykinesia), increased rigidity in the arms and/or legs, problems with walking (short stride length or a “shuffling” gait)\n- Muscle spasms and/or rippling of the muscles underneath the skin\nLearn more about the treatment for Frontotemporal Dementia.\nDementia with Lewy bodies (DLB) is a form of progressive dementia caused by degeneration of the tissues in the brain. DLB may be genetic, but it is not always clear why someone develops it.\nPeople with DLB have a buildup of abnormal protein particles in their brain tissue, called Lewy bodies. Lewy bodies are also found in the brain tissue of people with Parkinson disease (PD) and Alzheimer disease (AD). However, in these conditions, the Lewy bodies are generally found in different parts of the brain.\nThe presence of Lewy bodies in DLB, PD, and AD suggests a connection among these conditions. But scientists haven’t yet figured out what the connection is.\nDLB affects a person’s ability to think, reason, and process information. It can also affect movement, personality and memory. DLB becomes more prevalent with age. It often starts when a person is in his or her 60s and 70s. DLB is progressive, which means it continues to develop over time. There are several types of dementia with different causes.\nThe main sign of DLB is a progressive decline in things like memory, thinking, and problem solving. This decline is enough to affect the ability to work and do normal daily activities. Although memory may be affected, it isn’t usually as impaired as in someone with Alzheimer disease.\nDLB is generally diagnosed when at least 2 of the following features are also present with dementia:\n- Changes in attention and alertness. These changes may last for hours or days. Signs of these changes include staring into space, lethargy, drowsiness, and disorganized speech.\n- Visual hallucinations. These hallucinations recur and are very detailed. They generally don’t bother the person having them.\n- Movement symptoms consistent with Parkinson disease (PD), such as slow movement, shuffling gait, rigidity, and falls. The person may also have tremors, but not as pronounced as in a person with PD with dementia.\nOther signs and symptoms seen in DLB include:\n- Sleep disorder that affects REM sleep, causing vivid dreams with body movement\n- Dizziness, feeling lightheaded, fainting, or falling\n- Urinary incontinence\nIn DLB, memory problems often occur later in the disease. DLB can be confused with other forms of dementia, but it also has unique features, such as hallucinations and delirium.\nLearn more about the treatment for Dementia with Lewy Bodies.\nMild cognitive impairment (MCI) is an intermediate state between normal thinking and memory (cognition) and dementia. Patients with mild cognitive impairment can have difficulty with memory, language, thinking and judgment that are greater than would be expected for their age. People with MCI may be at an increased risk for developing Alzheimer’s Disease.\nPatients with a family history of Alzheimer’s and dementia are at greater risk for developing MCI. Other risk factors include age, high cholesterol, high blood pressure, diabetes and hypothyroidism.\nLearn more about the treatment for mild cognitive impairment.\nVascular cognitive impairment is the second most common form of dementia after Alzheimer disease. It’s caused when decreased blood flow damages brain tissue. Blood flow to brain tissue may be reduced by a partial blockage or completely blocked by a blood clot.\nSymptoms of vascular dementia may develop gradually, or may become apparent after a stroke or major surgery, such as heart bypass surgery or abdominal surgery.\nDementia and other related diseases and conditions are hard to tell apart because they share similar signs and symptoms. Although vascular dementia is caused by problems with blood flow to the brain, this blood flow problem can develop differently. Examples of vascular dementia include:\n- Mixed dementia. This type occurs when symptoms of both vascular dementia and Alzheimer’s exist.\n- Multi-infarct dementia. This occurs after repeated small, often “silent,” blockages affect blood flow to a certain part of the brain. The changes that occur after each blockage may not be apparent, but over time, the combined effect starts to cause symptoms of impairment. Multi-infarct dementia is also called vascular cognitive impairment.\nThe effect of decreased or no blood flow on the brain depends on the size and location of the area affected. If a very small area in a part of the brain that controls memory is affected, for example, you may be “forgetful” but it doesn’t necessarily change your ability to carry on normal activities. If a larger area is affected, you may have trouble thinking clearly or solving problems, or greater memory problems that do change your ability to function normally.\nVascular dementia is caused by a lack of blood flow to a part of the brain. Blood flow may be decreased or interrupted by:\n- Blood clots\n- Bleeding because of a ruptured blood vessel (such as from a stroke)\n- Damage to a blood vessel from atherosclerosis, infection, high blood pressure, or other causes, such as an autoimmune disorder\nSymptoms of vascular cognitive impairment (VCI) differ from the early symptoms of Alzheimer’s disease. Given the varied definitions of VCI, it is not surprising that clinical symptoms vary significantly in individual patients.\nThe symptoms of vascular dementia depend on the location and amount of brain tissue involved. Vascular dementia symptoms may appear suddenly after a stroke, or gradually over time. Symptoms may get worse after another stroke, a heart attack, or major surgery. These are signs and symptoms of vascular dementia\n- Increased trouble carrying out normal daily activities because of problems with concentration, communication, or inability to carry out instructions\n- Memory problems, although short-term memory may not be affected\n- Confusion, which may increase at night (known as “sundown syndrome”)\n- Stroke symptoms, such as sudden weakness and trouble with speech\n- Personality changes\n- Mood changes, such as depression or irritability\n- Stride changes when walking too fast, shuffling steps\n- Problems with movement and/or balance\n- Urinary problems, such as urgency or incontinence\nLearn more about the treatment for Vascular Cognitive Impairment.']	['<urn:uuid:b62cab71-7692-4973-b1ea-b55706d539d2>', '<urn:uuid:2b1797b3-9031-41b9-82e8-b70ad51c668d>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	11	51	2902
22	boise area ecoregions mining activities types	The Boise area is surrounded by diverse ecoregions where various mining activities take place. In the Northern Rocky Mountain ecoregion, mining is one of the important land uses alongside timber harvest and recreation. Mining methods vary depending on deposit location - some materials are found near the surface and can be mined cheaply, while others require deep underground tunneling. The mined materials include metals, coal, salt, diamonds, stone for building, phosphate for fertilizer, and gravel for highways.	"['The Boise Area from a Biologist’s Perspective\nOf the eight ecoregions encountered in Idaho, five are within a day’s drive of Boise. The city itself is situated in the Snake River Basin / High Desert region. This area is characterized by flat bottomland plains leading into gently rolling foothills (2500 to 5000 feet above sea level). Average rainfall in this area is between 8-15 inches per year. This domain is drained primarily by intermittent, ephemeral, and small perennial streams (at high elevations) coalescing into the Boise River (on the plain). Sagebrush steppe dominates the region, with cottonwood, alder, and willow lining the riparian areas along the Boise River. The river itself is impounded upstream of Boise, forming Lucky Peak, Arrowrock, and Anderson Ranch Reservoirs.\nIn a 2-3 hour drive to northeast, the Northern Rocky Mountain ecoregion begins. Rugged high mountains are the dominant feature. These mountains have sharply-crested ridges and steep slopes cut by steep-walled, narrow stream valleys. Elevation varies from 1300 to 8000 feet. Conifers include Douglas fir, subalpine fir, Englemann spruce, western hemlock, and western larch; Ponderosa pine is found in lower areas. Shrubs, forbs, and grasses represent forest understory vegetation. Perennial and ephemeral streams are usually high gradient and fast flowing. Stream density can range from less than one to as many as three miles of perennial stream flow per square mile. Lakes are common in higher glaciated areas. Both the Boise and Payette National Forests lie within part of this area. Timber harvest is the major economy, but wildlife habitat, recreation and mining are also important land uses. The multiple use of these forests provide diverse habitat types for scientific study.\nIn a 3-4 hour drive to the northwest, the Blue Mountains ecoregion begins. This region is comprised of several mountain ranges (Blue, Ochoco, Wallowa, Strawberry, Aldrich) separated by fault valleys and synclinal basins. Elevation varies from 2700 feet in low lying valleys to 7000-10,000 feet on mountain peaks. Rainfall averages 10 to 20 inches annually in lower valleys and basins, and greater than 40 inches in the mountains. Stream drainage density varies from one and a half to two miles of perennial streams per square mile in wetter areas, to no perennial stream flow in drier areas. Numerous springs are scattered throughout the region, and a few clusters of alpine glacial lakes occur in the higher mountains. The mountainous portions of the region support forests of grand fir, Douglas fir, ponderosa pine, and Englemann spruce. Stands of larch and lodgepole pine also occur. At lower elevation areas and foothills support large tracts of sagebrush/wheatgrass steppe and wheatgrass/bluegrass grasslands.\nTo the southwest of Boise lies the arid High Desert ecoregion. This area is characterized by smooth, flat topography occasionally dissected by lava and basalt plains and sand dunes giving way to the Owyhee Mountains. Large tracts of saltbush/greasewood, sagebrush, and wheatgrass vegetation occur throughout the region and along the streams due to their intermittent and ephemeral nature.\nTo the southeast of Boise begins the Northern Basin and Range ecoregion marking the start of the Great Basin Region. This region consists of numerous sub-basins and mountain ranges which present an extremely diverse physical setting. Surface water of the Basin include perennial, intermittent, and ephemeral streams; freshwater and saline lakes; playa lakes; freshwater and saline wetlands and thermal springs associated with faulting and volcanic activity.\nFinally, in a day’s drive due east, the middle Rockies ecoregion is encountered. Within this region Yellowstone and Grand Teton National Parks occur.', '- Easier - Mining is\nthe work or business of taking minerals from the\nearth. A hole or tunnel is dug in the ground to take\nout metals, coal, salt, or other minerals.\n- Harder - Most\nsubstances obtained from the earth are gotten by\nmining. Mining provides iron for steel making, salt\nfor food, coal for fuel, and gold, silver, and\ndiamonds for jewelry. Mined materials also include\nstone for building, phosphate for fertilizer, and\ngravel for highways.\n- There are many methods of mining, dependent on\nwhere and how a coal or mineral deposit is found. Some\nsubstances are mined relatively cheap because they can\nbe found at or near the earth\'s surface. Some minerals\nare found as a compact mass, while others are widely\nscattered. Other mined materials are found far beneath\nthe surface and removed by tunneling deep underground.\nSome mined substances are located beneath oceans,\nlakes, and rivers. Other minerals are concentrated in\nlarge bodies of water and are obtained by\nSafety and Health Administration\'s Kid\'s\n- Explore this site and learn more about\nand the Environment at About.com\n- This site connects you to government, industry and\nenvironment groups dealing with mining and environment\nissues in the United States and around the world.\n- Related Websites:\n- 2) Environmental Considerations of Active and\nAbandoned Mines, U.S. Geological Survey\nBulletin 2220 (Download pdf)\n- 3) How Coal Mining Produces Pollution in the Upper\nConemaugh River Basin\n- 4) Mining Waste at the National Mining\n- 5) Mining and Its Environmental Effects http://soilslab.cfr.washington.edu/esc110/2000Spring/projects/Group26/\n- 6) Tearing Down the Mountains from U.S. News\n- This website contains and links to information on\nmining history around the world.\n- Related Websites:\n- 2) Mining History Association http://www.lib.mtu.edu/mha/mha.htm\n- This comprehensive site focuses on the latest news\nrelated to mining and mining issues.\n- Other Comprehensive Mining Information\n- 2) InfoMine http://www.infomine.com/\n- 3) Mining Journal Limited http://www.mininginformation.com/\n- Related Links-Site:\n- 4) National Mining Association http://www.nma.org/\n- 5) Links and Connections from the Office of\nSurface Mining, Department of the Interior\n- Before you complete one or more of the\nfollowing activities, make sure that you\nexplore several of the mining\n- Create A Mining Map. Begin with\na map of your state, the United States,\nNorth America, or another continent. Then\nfind out what types of mining is being\ndone within the region. Identify the\ndifferent types of mining that are taking\nplace and find the production rates.\nDisplay your finished \'Mining Map.\'\n- Make A Mine Safety Poster.\nBefore starting to create your poster,\nvisit several of the sites for mine and\nmining safety - - Stay\nOut - Stay Alive: Mine Hazard Awareness\nCampaign and Safety\nand Health Information at MSHA.\n- Pick a Mineral. Select a\nmineral and learn about how it is mined.\nCompare the mining practices and processes\nof your mineral with other minerals.\nInvent a new ""high tech"" tool that could\nbe used in mining this mineral.\n- Rush for Riches. Explore the\nhistory of a mining community. What is the\nrelationship between the mining operation\nand the community? Create a visual map\nshowing the mine and its effect on the\nindividual and small town.\n- Get Organized. Mines were the\nlocations of some of the first organized\nlabor groups. Why did the miners feel it\nwas important to organize their concerns\nand interests? Trace the history of the\nlabor movement in the mining\n- Construct A Model of A Mine.\nVisit some of the mining websites to learn\nabout the different types of mines. Then,\nconstruct a model of a mine.\n- Mine Your Own Region. What type\nof mining or extraction takes place in\nyour region? Don\'t forget the mining of\nrock and aggregates such as limestone,\ngravel, rock quarries, etc. Are there\nabandoned mines? Find out about the\nhistory of mining in your area. Interview\nsomeone who has worked in the industry.\nGather photographs, maps, and information\nrelated to local mining. Put together your\nresearch findings into a presentation.\nPlace your completed project in the\nlibrary or online.\n- Complete A Mining WebQuest.\nFollow or adapt the procedures found at\nthe webQuest sites:\n- 1) Beverley Mine WebQuest\n- 2) Mining and the Community\nWebQuest by A. Barnett\n- 3) Mining and the\nEnvironment by A. Lee (Grades\n- Websites By Kids For Kids\nMining and Related Information for Golden,\nColorado by P. Murray and E. Wallin\n- This web site is a brief collection of information\navailable on coal mining activity in Golden,\nin Nevada County\n- Learn about types of mining and accompany a class\non a visit to a historic mine.\nFor Riches: a Portrait of Mining in America\n(2000 ThinkQuest Junior Project)\n- Take a cyber-journey into American mining and its\neffects on the individual, society, culture and\nof the Comstock (1999 ThinkQuest for\n- This site provides content, activities, and lesson\nplans related to the fields of study of science of\nPhysics, Chemistry, Earth Science, and Environmental\nScience applied to the historically and industrially\nimportant mining on the Comstock.\n- More Mining Websites\nDeportation of 1917\n- Learn about the Arizona event that influenced the\nlabor movement throughout the United States. What\nstarted as a labor dispute between copper mining\ncompanies and their workers turned into vigilante\naction against the allegedly nefarious activities of\nthe Industrial Workers of the World (I.W.W.).\n- This comprehensive site provides information and\nresources on coal and coal mining.\n- Other Coal Mining Sites:\n- 2) American Coal Foundation http://www.acf-coal.org/\n- 3) Black Diamond Net http://home.earthlink.net/~bela1/index.html\n- 4) Coalcracker http://www.tnonline.com/coalcracker/home.html\n- 5) Coal Facts (Illinois) http://www.commerce.state.il.us/resource_efficiency/Coal/facts.htm\n- 6) Coal Mine at Museum of Science and Industry,\n- 7) Coalming in Castlecomer (Ireland) http://www.sip.ie/sip019B/index1.htm\n- 8) Coal Mining, Mine Fires, & The Molly\n- 9) Coal Mining in the Gilded Age and Progressive\n- 10) Early Days of Coal Mining in Northern Illinois\nby R. Joyce http://www.kentlaw.edu/ilhs/earlydays.html\n- 11) Everything You Need to Know About Coal\n- 12) Facts About Coal from National Mining\n- 13) Haig Colliery Mining Museum (United Kingdom)\n- 14) History of Coal Mining in Nova Scotia\n- 15) In Search of King Coal at BHP http://www.BHP.com/default.asp?page=738\n- 16) Kentucky Coal Education History Page http://www.coaleducation.org/\n- 17) Office of Surface Mining (A Bureau of\nthe U.S. Department of the Interior)\n- 18) Strip Mining Coal http://geogweb.berkeley.edu/GeoImages/Starrs/DRAGLINE.html\n- 19) Unofficial History of Coal Mining in Illawarra\n(New South Wales, Australia)\n- 20) Virtual Museum of Coal Mining in Western\n- 21) West Virginia Coal Mining Facts http://www.state.wv.us/mhst/wvcoalfacts.htm\n- 22) What Coal Miners Do from United Mine\nWorkers of America http://www.umwa.org/mining/colminrs.shtml\n- 23) World Coal Institute http://www.wci-coal.com/pages/framemaster.htm\nDorado Gold Mine of Fairbanks, Alaska\n- Make an online visit to the El Dorado, an historic\nmine in the district where the rush started back in\n- Other Related Websites:\n- 2) California Gold Country http://www.malakoff.com/goldcountry/maintcgc.htm\n- 3) Gold Mining in North Carolina http://members.home.net/teylu/jenkins/gold_nc.html\n- 4) Lebanon Silver Mine (Colorado) http://www.gtownloop.com/mine.html\n- 5) Old Hundred Gold Mine Tour (Colorado) http://www.minetour.com/\n- 6) Reed Gold Mine (North Carolina) http://www.ah.dcr.state.nc.us/sections/hs/reed/reed.htm\nSalt City Beneath Detroit by P. Zacharias,\nThe Detroit News\n- Learn about the gigantic salt mine, operated until\n1983, that lies 1,200 feet beneath the industrial\nheart of Detroit. The mine, formerly operated by the\nInternational Salt Mine Company, spreads out over more\nthan 1,400 acres and has 50 miles of roads.\n- Another Salt Mine Site:\n- 2) Hallstatt Salt Mine http://www.salzbergwerke.com/traces.htm\nMine in Montana is a Lode of Controversy from\nthe Philadelphia Enquirer\n- Read about a modern mining controversy centered on\na community near Yellowstone National Park - - what\ncould become a 2000s version of the Gold Rush. Crown\nButte Mines, Inc. says that the old prospectors just\nmissed striking a lode that holds an estimated $600\nmillion in gold, silver and other metals.\nDreams and Silver Linings:\' A History of Mining in\nIdaho from the Idaho Mining\n- In the Fall of 1860, a party of ten prospectors\nled by Captain E.D. Pierce entered the Nez Perce\nReservation in search of mineral wealth.\n(Copper) from Asarco\n- This brief site provides information about\nopen-pit copper mining in Arizona.\nCollection Collection at University of\n- This collection of mining photographs depict\nvarious types of mining machinery and camps from the\nnineteenth century to modern times.\nIndustry in the Susquehanna River Basin\n- This site focuses on mining and its effects on the\n- Other Sites About Mining:\n- 2) Mining: Search, Exporation, and Development at\nSaskatchewan Interactive http://interactive.usask.ca/skinteractive/modules/mining/search/\n- 3) Mining Methods at Frontier Trails of the Old\n- This site for the mining industry connects to\ninformation and sources for equipment, products, and\nservices for mining.\n- WIM was founded in 1972 in Denver, Colorado, by\nseveral women whose intent was to facilitate education\nabout the mining industry.\nOut - Stay Alive: Mine Hazard Awareness\nCampaign (Mine Safety & Health\n- This is the site of a national public awareness\ncampaign aimed at warning children and adults about\nthe dangers of exploring and playing on active and\nabandoned mine sites.\n- Related Safety Websites:\n- 2) Health Hazard Information from MSHA\n- 3) Keepout: Old Mines Can Be Dangerous http://www.osmre.gov/keepout.htm\n- 4) Mining Accident and Injury Information at\n- 5) Mining Industry Accident, Injuries, Employment,\nand Production Statistics http://www.msha.gov/ACCINJ/accinj.htm\n- 6) Safety and Health Information at MSHA\n- Websites For Teachers\nActivities at Women in Mining\n- This site has a collection of activities designed\nto teach about mining and minerals.\n- Related Website:\n- 2) Games at Women in Mining http://www.womeninmining.org/games.htm\nand Coal Mining\n- In the book, Rebels in the Shadows, the Flannerys\nlike everyone around them, are bound intextricably to\nthe mines. Here is a plan to integrate this story into\na unit about mining.\n- Captain Catherine guides learners through the coal\nmining process. During your students adventure, they\nwill learn the different types of mining used in the\nUnited States and the factors that determine the type\nof mining used.\nProblems on Coal and Energy by J. Bryant\n(Grade 7-8) at Yale-New Haven Teachers\n- The purpose of this unit is to provide an\nhistorical aspect to the developments which lead to\nthe discovery of coal and coal mining through the\norigins and kinds of coal; types and methods of coal\nmining, hazards of mining, measures of safety and\nchemical byproducts of coal.\nPony in the Classroom\n- The lesson plans in this unit focus on two themes:\nthe human element and mapping. Each lesson can be used\nindependently or in combination with one or more of\nthe other lessons.\nUnit on Gold by R.R. Avila and L. Jones\n- This unit is organized to provide the student with\ninformation, history, hands-on experience and the\nprocess of mining and extracting minerals from mother\nroom & pillar mining\nopen pit mining\nhard rock mining\nadit or slope\n- Created by']"	['<urn:uuid:afe83c96-d716-411d-b369-1c00fae9aa4b>', '<urn:uuid:1d076bcd-befe-4425-90b6-da6e76f2a1cf>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	6	77	2399
23	skeletal rigging facial rigging which technique more complex process animation	Facial rigging is more complex than skeletal rigging. While skeletal rigging involves creating a basic digital skeleton with joints and bones attached to a character model, facial rigging requires multiple sophisticated components: creating a facial skeleton, setting up control systems, defining multiple blendshapes for different expressions, connecting controls to blendshapes, and adding secondary movements like blinking and subtle facial muscle contractions to enhance realism.	['Rigging and Skinning\nRigging and skinning for games are where you set up an object, normally a character of some kind, to animate. Here you can tell your\nRigging is all about creating a ‘skeleton’ that goes inside the character that defines exactly how the object should move and what restrictions it should have.\nSkinning is how you let your model know exactly which parts of it should move when the rig moves. Combined these two aspects allow the model to move and for the animators to do what they do best.\nHow To Rig An Object\nYou begin by creating a rig that is essentially the skeleton of your character, this is sometimes referred to as the skeleton. Here you will place bones which you will then later skin the model to. The level of detail in the rig can have a big impact on performance and so many older games or games that need less individual character detail such as troops in a strategy games or the crowds in racing games will cut corners and use as few a bones as possible. A common and often noticable one is when characters have ‘mitten’ hand where all of the fingers use a single bone and as such, they all move exactly the same. The level of detail you go in to will vary based on the task and the user’s skill level.\nRiggers will also set up ‘controllers’ which the animator is able to use and grab to move the model around. There are objects within the animation software that animators can use to manipulate the models in different ways. One of the most common ones you will see is usually placed at the ends of limbs but they can be used to control a variety of things from facial expressions to belly jiggle. These controllers will not be rendered in the final piece, whether it’s a video or rendered in the game, this means that can look like whatever the rigger likes and will help the animation best.\nRigs often have restrictions placed on them. A common one is set up so that bones can only rotate so far. For instance, an elbow joint only needs to be rotated just under 180° so the rigger can restrict it from bending in the opposite direction. This makes it easier for the animators to use and also less likely to break under stress. It also allows for automatic functions such as leg bends when lifting a foot without it doing something crazy.\nForward Kinematics vs Inverse Kinematics\nThere are two types of movement that can be set up for the rig to use, FK and IK. FK or Forward Kinematics is done by rotating the bones around their pivot points, which are found at the base of the bone, and positioning them one by one, starting at the top of the bone chain and moving down. This is useful for animating things like fingers as you can have a greater degree of control over them. A combination of both FK and IK\nIK or Inverse Kinematics is done by moving the end of a chain of bones into position and the parameters set in the rig will work out where the rest should be. This method also means you can change something further up the chain without affecting the endpoint. This is useful when the bones have limited movements and need to be rooted to a position in the world such as legs and arms when interacting with things.\nHow To Skin An Object\nAfter you have set up a skeleton you will need to skin the mesh to the skeleton. During this stage, you will paint ‘weights’ onto the vertices of the model which will determine which bones they should follow. These weights can be blended between multiple bones to create a smoother transition. This is how the program knows which parts of the model should follow the upper arm bone, which bits should follow the lower arm bone and which part of the body is the elbow, which will blend between the two bones to give it a nice bend.\nHowever, the body doesn’t move in a nice predictable way that can easily be replicated with simple movements. Areas such as the crease of the elbow are hard to get looking right as well as shoulders deforming in the right way. To fix these issues you need to do what is called ‘volume-preserving’ this will allow the model to know that it shouldn’t bunch up or cut off corners of the model, instead, it should try to preserve the volume that is there in a way that a physical body would.\nBlend Shapes For Faces\nFaces are handled differently to regular animation due to their complexity. They will often use blend shapes or similar tech to allow the animator to blend between different facial expressions based on predetermined settings. This can be done by taking the base head and manipulating the points to the expression you want and the program will know where to move them when that expression is selected. These expressions can then be blended together to create more natural looks. For example, a character could be both surprised and happy with the result of something, this can be achieved by turning the surprised expression up to 80% and then the happy expression to 50% as an example. This may not always work though and the best solution may be to create a unique expression, especially if it the main character in a story-driven.\nMotion Capture For Faces\nMore often these days, games will use motion-captured animation as apposed to hand-animated. For\nPrograms For Rigging And Skinning In Games\nAutodesk’s Maya is one of the most common programs for rigging. It’s capable of handling effective rigging for both games as well as movies and TV. It has a wide range of tools that are powerful enough to create any kind of rig you might need. Maya is traditionally the most popular of Autodesk’s pieces of 3D software for rigging.\nMaya is a very common piece of software within the games industry, possibly the most popular. If you are looking for an industry\n3ds Max is the sister program to Maya. It is also owned by Autodesk and both programs function and perform in very similar ways. Although Maya was traditionally the rigger’s favorite program the gap between them has lessened over time and either can be used easily enough.\n3ds Max is considered an industry-standard program and whether a company uses Maya or 3ds Max is usually up to the companies person choice. Maya is probably the better choice to learn, but either works well and the skills are fairly easily transferred from one program to the other.\nBlender is a free 3D program that has a wide array of functions that cover many areas. As the program is free it is a good place to see if you like the job and practice. Like 3ds Max and\nThe down side to Blender is it is not industry standard. While it can be a great place to start you will need to have experience with a more recognized program before being taken too seriously for a job position. This is why we recommend learning 3ds Max or Maya if you are serious about getting a job in industry.\nThis is part of our Making Games series. If you’d like to learn more about the other aspects of making games then check it out.', 'Hello there, dear readers! Have you ever watched an animated movie or played a video game and wondered how the characters move and come to life? This is where 3D rigging comes in. In this article, we will delve into the world of 3D rigging and its importance in animation and character design. Get ready to be amazed and learn something new!\nWhat is 3D Rigging?\n3D rigging is the process of constructing a digital framework for a 3D character or object, allowing it to be realistically and naturally animated. This involves strategically placing joints and controls on the model, which animators can manipulate to create fluid motion. Rigging is a crucial aspect of character design and animation, as it allows for realistic movements, facial expressions, and intricate interactions. Without proper rigging, characters would lack fluidity and appear rigid and lifeless. In essence, 3D rigging is the technique used to bring 3D models to life and give them movement in animation and character design.\nWhy is 3D Rigging Important in Animation and Character Design?\n3D rigging plays a vital role in animation and character design, providing essential flexibility and realism to the characters. It allows animators to have full control over the movement and expressions of the characters, ultimately enhancing their believability and emotional impact. Rigging enables intricate movements such as facial expressions, body deformations, and even complex interactions between characters and objects. Without rigging, animators would be required to manually manipulate every aspect of the character, which is not only time-consuming but also limits the creative possibilities. In summary, 3D rigging is crucial in animation and character design as it brings characters to life, making them dynamic and captivating.\nWhat Are the Benefits of 3D Rigging?\nThe advantages of implementing 3D rigging in animation and character design are numerous.\n- Enhanced movement: 3D rigging allows for realistic and fluid movements of characters, bringing them to life.\n- Time-saving: Once a character is rigged, animators can easily pose and animate them, saving time and effort.\n- Flexibility: Rigging provides the flexibility to modify and adjust the character’s movements and expressions as needed.\n- Consistency: Rigging ensures consistent proportions and movements throughout the animation.\n- Improved storytelling: Rigging helps convey emotions and expressions, enhancing the viewer’s connection to the character.\nThese benefits make 3D rigging an essential tool in creating compelling and engaging animations.\nIt’s like giving a skeleton to a virtual puppet, except this puppet can do more than just dance – it can bring characters to life with realistic movements and expressions.\nHow Does 3D Rigging Work?\n- Create a 3D model: Design and create a 3D character model using specialized software.\n- Joint placement: Determine the placement of joints on the model, which will allow for movement and articulation.\n- Skinning: Attach the model’s vertices to the corresponding joints, ensuring that the character moves realistically.\n- Control setup: Create a control rig that allows animators to manipulate the character’s movements.\n- Weight painting: Adjust the weights of the character’s vertices to ensure smooth movement and deformation.\n- Testing and refining: Test the rig by posing and animating the character, making adjustments as needed for better functionality.\nWhat Software is Used for 3D Rigging?\nThere are a variety of software programs available for 3D rigging in animation and character design. These programs offer the necessary tools and features to create rigging setups for 3D models. Some of the most popular options for 3D rigging include:\n- Autodesk Maya\n- 3ds Max\n- Cinema 4D\nEach software has its own unique set of features and capabilities, giving animators and riggers the ability to create intricate and realistic rigs for characters. These programs also provide a range of tools for creating joint systems, setting up controls, defining constraints, and managing deformations, resulting in smooth and lifelike movements for animated characters.\nWhat Are the Steps Involved in 3D Rigging?\nThe process of 3D rigging involves several steps to prepare a 3D model for animation. Here are the steps involved in 3D rigging:\n- Create a skeleton: Build a digital skeleton inside the 3D model, positioning joints and bones.\n- Bind the mesh: Attach the 3D model’s mesh to the skeleton, ensuring it moves with the bones.\n- Set up controls: Add control objects to manipulate the rig, providing animators with an intuitive interface.\n- Define weight painting: Assign weights to the mesh vertices, determining how much they are influenced by specific bones.\n- Create constraints: Apply constraints to restrict the movement of certain parts, such as IK (inverse kinematics) or FK (forward kinematics).\n- Add facial controls: Develop a facial rig using blend shapes or bone-based systems to animate facial expressions.\n- Test and refine: Continuously test the rig’s functionality, making adjustments and refining until it meets the desired requirements.\nBy following these steps, 3D rigging enables animators to bring characters to life with realistic movements and expressions.\nWhat Are the Different Types of 3D Rigging?\nIn the world of 3D animation and character design, rigging plays a crucial role in bringing digital models to life. But did you know that there are different types of rigging techniques that serve different purposes? In this section, we will take a closer look at the various types of 3D rigging, including skeletal rigging, facial rigging, and cloth rigging. Each type has its own unique features and uses, making them essential tools for creating dynamic and believable characters in the world of animation. So let’s dive in and explore the world of 3D rigging.\nSkeletal rigging is an essential aspect of 3D animation and character design. It involves creating a digital skeleton and attaching it to a character model, allowing for realistic movement and animation. By defining joints and establishing how they connect and move, skeletal rigging enables animators to easily manipulate characters. This type of rigging is commonly used for humanoid or animal characters. Skilled riggers utilize software such as Autodesk Maya or Blender to create a hierarchy of bones, assign weights to control the model’s deformation, and implement constraints to limit movement. Skeletal rigging plays a crucial role in bringing characters to life in animation and elevating the overall quality of the final product.\nFacial rigging is a crucial aspect of 3D animation and character design, as it brings life to facial expressions and movements. This process involves several steps:\n- Create a facial skeleton: Develop a digital framework that mimics the structure and movement of a human face.\n- Set up controls: Establish a system of controls, such as sliders or shape keys, to manipulate different areas of the face.\n- Define blendshapes: Sculpt a range of facial expressions by creating various blendshapes, which are different shapes of the face that can be morphed together.\n- Connect controls to blendshapes: Link the controls to the blendshapes, allowing animators to easily manipulate facial expressions.\n- Add secondary movements: Enhance realism by incorporating secondary movements like blinking, eye saccades, and subtle facial muscle contractions.\nFacial rigging is essential as it enables animators to effectively create characters that can emote and communicate, adding depth and believability to the animation.\nCloth rigging may sound like a fashion design technique, but it’s actually essential in giving animated characters those realistic flowing capes and billowing skirts.\nCloth rigging is a crucial aspect of 3D animation and character design that involves creating lifelike movement and behavior for cloth simulations. Here are the steps involved in cloth rigging:\n- Create a mesh: Begin by modeling the cloth object with the desired shape.\n- Add joints: Define the areas where the cloth will be attached to the character’s body using joint influences.\n- Set up constraints: Apply constraints to ensure the cloth interacts correctly with the character, such as collision and gravity.\n- Paint weights: Assign weight values to the cloth vertices to determine how much influence each joint has on the cloth movement.\n- Adjust parameters: Fine-tune the cloth simulation settings, including stiffness, stretchiness, and friction.\n- Test and refine: Run simulations to evaluate how the cloth behaves and make necessary adjustments to achieve the desired results.\nBy mastering cloth rigging techniques, animators can bring characters to life with realistic clothing and enhance the overall quality of the animation.\nWhat Skills are Required for 3D Rigging?\nIn the world of animation and character design, 3D rigging is a crucial aspect that brings life and movement to digital characters. But what does it take to become a skilled 3D rigger? In this section, we will discuss the essential skills and knowledge required for 3D rigging. From understanding anatomy and movement to mastering animation principles and 3D software, we’ll explore the key elements that make a successful 3D rigger. So, let’s dive into the world of 3D rigging and discover the necessary skills for this intricate art form.\nKnowledge of Anatomy and Movement\nHaving a strong understanding of anatomy and movement is crucial for 3D rigging in animation and character design. It allows riggers to comprehend the structure and mechanics of the human body, enabling them to create realistic movements for characters. Understanding how muscles, joints, and bones work together is key in achieving natural and lifelike animations. Additionally, being knowledgeable in movement principles such as weight, timing, and anticipation is essential for creating dynamic and believable character performances.\nRigging experts with a solid foundation in anatomy and movement can ensure that the characters they rig move in a realistic and expressive manner, elevating the overall quality of the animation.\nWithout understanding animation principles, your 3D rigging skills will just be a bunch of bones and wires.\nUnderstanding of Animation Principles\nHaving a thorough understanding of animation principles is vital for successful 3D rigging in character design. Animators must possess a strong grasp of concepts like timing, spacing, and weight in order to create movements that are believable and visually appealing. This knowledge enables them to ensure that the rig behaves realistically and adheres to the laws of physics.\nWith a solid understanding of animation principles, riggers can also anticipate how the character’s movements will impact other elements, such as facial expressions or cloth simulations. Moreover, this understanding allows riggers to collaborate more effectively with animators, resulting in a smoother workflow and higher-quality animations.\nExperience with 3D Software\nHaving experience with 3D software is an essential skill for 3D rigging. This requires proficiency in programs such as Maya, 3ds Max, or Blender. Rigging artists must possess a strong knowledge of the software’s rigging tools and features to construct intricate and lifelike character rigs. They should also be able to navigate the software interface, manipulate objects, create rigging controls, and establish constraints and deformations. Moreover, familiarity with 3D software allows riggers to troubleshoot and resolve any technical difficulties that may arise during the rigging process. Ultimately, a thorough understanding of 3D software empowers riggers to produce high-quality character animations.\nHow is 3D Rigging Used in Character Design?\nIn the world of character design and animation, 3D rigging plays a crucial role in bringing digital creations to life. This section will delve into the various ways in which 3D rigging is utilized in character design, from creating realistic movements and expressions to enhancing the character’s personality and improving the overall quality of the animation. By understanding the importance of 3D rigging in character design, we can gain a deeper appreciation for the intricate process behind creating dynamic and engaging animated characters.\nCreating Realistic Movements and Expressions\nCreating realistic movements and expressions in 3D rigging requires careful attention to detail and a thorough understanding of human anatomy and movement. Here are the steps involved in achieving this:\n- Study Reference Material: Analyze reference videos or images of real-life movements and expressions to capture the nuances.\n- Build a Skeleton: Create a digital skeleton for the character, ensuring proper joint placement and hierarchy.\n- Set Up Control Rig: Set up a control rig with controllers to manipulate the character’s joints and deformations.\n- Weight Painting: Assign weights to each joint to control the deformation of the character’s mesh.\n- Add Constraints: Apply constraints to limit and control the movement of certain body parts, such as IK (Inverse Kinematics) for limbs.\n- Create Facial Rig: Build a facial rig with controls for facial expressions, including blend shapes or bone-based systems.\n- Add Secondary Motion: Incorporate secondary motion to enhance realism, such as adding squash and stretch or cloth simulations.\n- Refine and Test: Continuously refine the rig and test it by animating the character to ensure realistic movements and expressions.\nEnhancing the Character’s Personality\nUsing 3D rigging to enhance a character’s personality is crucial in creating captivating animations. It adds depth and realism to their movements and expressions, making them come alive. Here are the necessary steps to achieve this:\n- Create a skeletal rig that accurately captures the character’s unique anatomy and movements.\n- Refine the rig to ensure smooth and natural animation, paying attention to joint placement and control.\n- Add facial rigging to enable expressive facial movements and emotions.\n- Incorporate cloth rigging to simulate realistic fabric movement and draping.\n- Implement secondary animation techniques, such as hair and prop rigging, to further enhance the character’s appeal.\nBy following these steps, 3D rigging can greatly enhance a character’s personality and bring them to life in the animation.\nWith 3D rigging, characters can go from stiff and lifeless to fluid and dynamic, making animation a whole lot more entertaining to watch.\nImproving the Overall Quality of the Animation\nEnhancing the overall quality of the animation is a crucial aspect of 3D rigging. By incorporating precise rigging techniques and paying attention to detail, animators can create more realistic and visually appealing animations. Here are some steps that can help improve the quality of the animation:\n- Utilizing proper skeletal rigging to ensure accurate and natural movement of the character.\n- Implementing facial rigging to create realistic facial expressions and emotions.\n- Utilizing cloth rigging to simulate realistic clothing movements.\n- Incorporating secondary motion to add depth and realism to the animation.\n- Utilizing proper inverse kinematics and control systems for smooth and controlled movements.\n- Adding appropriate weight and physics simulations to objects and characters for a more lifelike feel.\n- Applying efficient lighting and shading techniques to enhance the visual appeal of the animation.\n- Including post-production techniques like compositing and color grading to improve the overall look and feel.\nBy following these steps, animators can significantly improve the overall quality of their 3D animations and create captivating visuals for their audience.\nFrequently Asked Questions\nWhat is 3D Rigging For Animation & Character Design?\n3D rigging is the process of creating a digital skeleton or system of bones and controls for a character or object in a 3D animation. It is an essential step in the character design process, as it allows animators to manipulate and move the character in a realistic and fluid manner.\nWhy is 3D Rigging important in animation and character design?\n3D rigging is crucial in animation and character design because it gives the character movement and brings it to life. Without rigging, a character would not be able to move or emote realistically, making it difficult to tell a compelling story through animation.\nWhat skills are needed to become a 3D rigger?\nTo become a 3D rigger, one needs a strong understanding of 3D software such as Maya or Blender, as well as a knowledge of anatomy and movement. Knowledge of coding and scripting is also helpful in creating more advanced rigging setups.\nCan 3D rigging be used for more than just character animation?\nYes, 3D rigging can also be used in other forms of animation, such as rigging props, vehicles, or even environments. It can also be used in video games, simulations, and other forms of digital media.\nIs 3D rigging a time-consuming process?\nThe time it takes to rig a character or object varies depending on its complexity and the skill level of the rigger. It can take anywhere from a few hours to several days to create a complex rig, but the end result is worth the effort.\nWhat is the difference between 3D rigging and 3D animation?\n3D rigging is the process of creating a digital skeleton for a character or object, while 3D animation is the process of adding movement and bringing the character to life. Rigging is a crucial step in the animation process, but it is not the same as animation itself.']	['<urn:uuid:e6f8912e-3b7c-46af-bfc0-7291f8ba4a83>', '<urn:uuid:6b706758-88e0-4dff-8c4b-bad17bee2f99>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-13T04:31:27.499155	10	64	3996
24	infrastructure and resources needed digital preservation africa gpu performance challenges bandwidth power issues	For optimal digital preservation performance, infrastructure requirements include powerful GPU computing capabilities - as demonstrated by the 4-5.6x speedup achieved using multiple GPUs for 3D reconstruction. However, African institutions face severe infrastructure limitations: unreliable power supplies with frequent cuts that disrupt operations and can lead to data loss (affecting 50-70% of businesses), severely limited or expensive internet bandwidth that hinders collaborative preservation systems like LOCKSS, and lack of adequate ICT facilities even for basic digital operations. Additionally, there are financial challenges in maintaining international standards for digital formats, as institutions need to continuously update both hardware and software, while facing decreasing funding for preservation efforts.	"[""Optimization of a 3D Reconstruction Application\nInternet provides access to a vast, ever-growing collection of photographs of cities and landmarks from all over the world. A simple search for a city name in Flickr returns millions of photographs capturing different parts of the city from various viewpoints. Creating\naccurate 3D models of cities is a problem of great interest and with\nbroad applications. The 3D Scene Reconstruction application  that\nwe are using is a software aimed to reconstruct a 3D model from an\nunstructured collections of photographs. It is achieved by applying\nnew computer vision techniques.\nThis application has a pipeline as shown in Figure 1\narchitecture which consists of four parts: Sift-GPU , KeyMatchFull , Bundler  and Pmvs. Each of these parts represents\nan independent application that keeps the communication in\nthe pipeline through input and output files. The final output is a file\nin a ply format that can be visualized as a 3D model.\nThe work presented in this project is a solution for optimizing an\nexisting 3D Scene Reconstruction application . The application\nis aimed to match and reconstruct scenes from extremely large unstructured\ncollections of photographs. The reconstruction process,\nwhich performs complex algorithms from image matching to large\nscale optimization, can take days. In this paper we propose a way to\nspeed up the performance by porting the application on GPU computing.\nOur experimental results demonstrate that it is now possible\nto reconstruct 3D objects 4 - 5.6 times faster.\nThe solution that we propose in this project for optimizing the\n3D Scene Reconstruction application involves optimizing each of\nthe three main steps of the pipeline: SiftGPU, KeyMatchFull and\n- SiftGPU is the first step of the application pipeline. Sift,\nwhich stands for Scale Invariant Feature Transform, is a method\nfor extracting distinctive invariant features from images that can be\nused to perform reliable matching between different views of an\nobject or scene. SiftGPU is an implementation of Sift for GPU\n- KeyMatchFull is an intermediate step between the extraction of\nkeys in SiftGPU and the Bundler. It associates images with common\nfeatures. It compares two images by calculating the nearest\nneighbors for all the keys of the images and produces a list of\n- The Bundler application is an implementation of a Structurefrom-\nMotion (SfM) algorithm that takes the set of images, a list of\nimages features and a list of image matches as input, and produces a 3D reconstruction of camera and (sparse) scene reconstruction as\nThese three applications are independent and they communicate\nthrough input and output files. Therefore in each of them we\napply different approaches to reduce the execution time, which are\ndescribed in more details in section 3.\nFor measuring the performance of the application we used three\ndifferent data sets:\n- Xu31 Consists of 31 images of size 3456x5184 pixels.\n- Car31 Consists of 31 images of size 2048x1536 pixels.\n- Car61 Consists of 61 images of size 2048x1536 pixels.\nAnalysis and Results\nshows the improvement that was achieved over the total time of the\nSiftGPU processes pixels in parallel to build Gaussian pyramids\nand detects Difference of Gaussian key points. The result of the\nprocess contains the orientations and descriptors of the key points.\nThe original implementation of SiftGPU extracts key points for\nevery image on a single GPU. By using the maximum number (4)\nof GPUs on a node, we modified the SiftGPU implementation by\nprocessing four images in parallel. We used OpenMP for thread\nThe results obtained in Figure 3 show significant improvements\nbetween the two implementations. The performance of the key\npoint extraction process was speeded up to 4.53 - 5.1 times as\nshown in Figure 4. In addition, it proves the advantage of using\nOpenMP correctly for splitting the SiftGPU process into multiple\nprocesses. Moreover, Figure 3 also depicts the different execution\ntime of SiftGPU for the same number (31) of images for data sets\nXu31 and Car31 because the dimension and the key points of the\nimages used in those data sets are not the same. The achieved result was even more than expected (more than\nfour times) because the way of applying multiple processes reduces\nthe execution time for loading SiftGPU initialization and releasing\nThe initial implementation uses ANN library  to match the\nkey points between two images. Although ANN is highly optimized\nto minimize the number of calculations needed for finding the nearest\nneighbors, this task can be ported on GPU. We came across the\nFast K-Nearest Neighbor (KNN) Search integrated GPU Computing\n, a cuda library that performs the same task. Although the\nsearch algorithm is much simpler, resulting in much more calculations,\nwe can benefit a lot by the parallelization. KNN was very\npromising since it was documented that its performance can be 64\ntimes faster than ANN. We managed to replace the KNN library\nwith ANN library in the KeyMatchFull implementation. The KNN\nversion that we used is highly customized to meet our needs. The\noutput results we obtain by using KNN are slightly worse than the\ninitial ones but this doesn't affect significantly the whole process\nand is a fair trade-off for the speed up that we obtain.\nFigure 4 shows the timing results we obtained running the\nthree versions of KeyMatchFull (ANN-CPU, KNN-GPU, KNN-4\nGPU) with our testing data sets. The measurements do not include\nthe loading phase but contain the saving time. It is easily noticed\nthat the GPU version is about 2.5 times faster than the initial one.\nThe reason that there is a great differentiation between what KNN was promising and the final result is because the testing data that\nwere used in KNN's research paper  differ significantly from our\nFrom the profiler results, we noticed that the step that takes most\ntime during the execution in the CPU is the Levenberg-Marquardt\n(LM) algorithm. It is aimed to solve a linear system of equations for\nbundle adjustment created out of the parameters given as input for\nthe Structure-from-Motion (SfM) process. The function that solves\nthis equations system is in the library sba-1.5 , and it takes 22.1%\nof the execution time.\nBased on the described result we decided that the LM procedure\nshould be parallelized. In order to do it, an existing implementation\nof the Bundler that uses Multi-Core CPU and Single-GPU implementations\nof the SfM was used . That implementation is focused\non reducing the use of memory and time in the execution\nof the Hessian, Schur complement and Jacobian, which are usually\nused by LM to reduce the size of the linear system . The\ncode is contained in the library libpba, which was integrated in the\napplication and used instead of sba-1.5 .\nAfter the integration was performed, we ran experiments in\norder to compare the performance between the Single-CPU, Multi-\nCPU and Single-GPU approaches. The three data sets described\nin the introduction of this paper were used. In every experiment\ntwo measurements were taken, the time of execution of the SfM\nroutine (which was parallelized) and the total time of execution of\nthe Bundler. The results obtained are presented in Figure 5.\nAs it can be observed in the Figure 5, after the parallelization\nof the Structure-from-Motion the reduction in the execution time\nof this routine influenced the total execution time of the Bundler. The total time decreased according to the speedup factors of the\nStructure-from-Motion routine, which can be observed in Figure 6."", 'Digital preservation can be defined as the process and activities which stabilize and protect digital records and publications in forms which are retrievable, readable and usable over time. Digital preservation could also be defined as a set of processes and activities that ensure continued access to information and all kinds of records, scientific and cultural heritage existing in digital formats. This includes the preservation of materials resulting from digital reformatting but particularly information that is born-digital and has no analog counterpart. Digital preservation is an ongoing process of managing data for continued access and use.\nThe adoption of Information Communication Technologies (ICTs) has revolutionized the conduct of business and has greatly enhanced information accessibility. In particular, organizations are not only able to store large amounts of information but can also have quick access to it. This has improved service delivery and has ensured that policy makers react rapidly to social and economic developments. Further, the general public can also access information in remote areas. ICT has enabled archivists, records managers and librarians to carry out their mandate: that of information capture, preservation and dissemination. While use of ICT has occasioned these many benefits it has also brought challenges that have to be addressed. Principally, this new development has led to the generation of information in digital form which has to be managed. In spite of the benefits accruable, the technology has presented tremendous challenges which information professionals should be concerned with.\nThe purpose of preservation is to ensure protection of information of enduring value for access by present and future generations (Conway, 1990: 206). Libraries and archives have served as the central institutional focus for preservation, and both types of institutions include preservation as one of their core functions. In recent decades, many major libraries and archives have established formal preservation programs for traditional materials which include regular allocation of resources for preservation, preventive measures to arrest deterioration of materials, remedial measures to restore the usability of selected materials, and the incorporation of preservation needs and requirements into overall program planning.\nCHALLENGES OF DIGITAL PRESERVATION\nAn African perspective on preservation ought not to be different from other perspectives. However, digital preservation is often discussed in terms of technology, infrastructure and practices. Africa is largely composed of developing nations and thus has peculiar problems.\nIn African institutions these factors are attributed to:\nMost African countries have no policies on handling information be they in print; let alone in electronic format. In some African countries, years after independence they are still struggling with enacting a libraries act and as a result most institutions operate within a no policy framework. An enabling policy framework would allow institutions to implement various preservation strategies that are in line with their own parent institutions but operate within the overall country policy framework. These policy frameworks are essential especially if they can feed into broader continental policies such as the NEPAD initiative (The New Partnership for Africa’s Development which is a VISION and STRATEGIC Framework for Africa’s renewal). The NEPAD initiative itself is very silent on the preservation of Africa’s knowledge resources although it places prominence on the improvement of information and communication infrastructure (ICT). The improvement of ICT infrastructure will do well if there are policy frameworks at the country level that support the preservation and permanent storage of African knowledge resources wherever they might be found and in whatever format they might in.\nAfrica’s infrastructure is still lacking in handling large preservation of knowledge resources, especially resources that are in electronic form. Access to ICT facilities is a daily struggle for most institutions that are just barely managing to maintain access to print resources to be able to meet the daily requirement for academic learning in higher educational institutions.\nPreservation of knowledge resources is a continuous process not just a one off issue. To implement an effective and efficient preservation policy, there is need for commitment at both the institutional and national levels that preservation of the knowledge resources will be an incremental process that will be carried on from one generation to another. This effort entails that financial resources be committed to such a venture over long periods of time. This trend in funding has affected all areas of library operations including money that could be allocated for preservation of scholarly information materials. Financial commitments would also be needed to purchase and preserve the digital knowledge resources to permanently make them accessible to users, now and in the future.\nFinancial resources available for libraries and archives continue to decrease and will likely do so for the near future. The argument for preserving digital information has not effectively made it into public policy. There is little enthusiasm for spending resources on preservation at the best of times and without a concerted effort to bring the issues into the public eye, the preservation of digital information will remain a cloistered issue. The importance of libraries has been diminished in the popular press as the pressures from industry encourage consumers to see libraries as anachronistic while the Internet and electronic products such as Microsoft Encarta are promoted as inevitable replacements. Until this situation changes, libraries and archives will continue to be asked to do more with less both in terms of providing traditional library services, as well as new digital library services: preservation will have to encompass both kinds of collections.\nTechnical knowledge on the digital elements of electronic documents is largely lacking among staff that are in preservation departments. The presence of preservation departments in most of the libraries and information centers is really in name only as most of them concentrate on book and journal binding. This is coupled with the lack of preservation training. This lack of knowledge extends to deficient know-how on the equipment and software that is required for the preservation of digital information resources.\nDigital Technology Challenges\nDigital technology poses several challenges in the preservation of digital information resources. These are among others; technology comes in different formats, the cost of maintaining international standards of digital formats is expensive as it is often based on paying for upgrades to match the technology both the hardware and software. These come with subscriptions costs; so in essence a library/information center/archival center would have to subscribe to hardware; software and then to the electronic journal. This is unlike the paper format which has relatively changed very little since it was discovered as papyrus in Egypt 3000 BC. The electronic document is fairly new and has changed forms since then. If it is not the document changing from MS Word, PDF, html XML etc; it is the software requirement to be able to open and read the document. For example, if the document is in PDF you will need a PDF reader; JPEG would require a JPEG; just as a TIFF formatted document would require a Tiff reader. This means that institutions are always forced to change the facilities so they can meet various requirements such as software and hardware. Digital preservation presumes that there should be constant and continuous learning on the part of preservation staff both in software knowledge as well as hardware. This is because digital preservation methods are always changing depending on the nature of the hardware and software applied.\nDigitization of information requires obtaining copyright permission from various publishers to be able to duplicate anything in large quantities. However, most licensing agreements for journals or books produced by major publishers prohibit duplication of electronic documents or local storage of the document. What is allowed when one has a subscription is usually the online access to the particular journal for instance, without the subscribing institution having permanent access to content of the journal. Once subscription ends, access to the electronic content of journal is not possible. It is unlike in the print subscription model where once one has subscribed to the journal, the institution will have permanent access to the journal because the journal will be physically present the libraries own space.\nRecording media for digital materials are vulnerable to deterioration and catastrophic loss, and even under ideal conditions they are short lived relative to traditional format materials. Although librarians/archivists have been battling acid-based papers, thermo-fax, nitrate film, and other fragile media for decades, the threat posed by magnetic and optical media is qualitatively different. They are the first reusable media and they can deteriorate rapidly, making the time frame for decisions and actions to prevent loss is a matter of years, not decades. While acid paper is prone to deterioration, becoming brittle and yellowing with age, the deterioration may not become apparent for some decades and progresses slowly. It remains possible to retrieve information without loss once deterioration is noticed. Digital data recording media may deteriorate more rapidly and once the deterioration starts, in most cases there may already be data loss. This characteristic of digital forms leaves a very short time frame for preservation decisions and actions.\nMore insidious and challenging than media deterioration is the problem of obsolescence in retrieval and playback technologies. Information technologies are essentially obsolete every 18 months. Innovation in the computer hardware, storage, and software industries continues at a rapid pace, usually yielding greater storage and processing capacities at lower cost. Devices, processes, and software for recording and storing information are being replaced with new products and methods on a regular three- to five-year cycle, driven primarily by market forces. This dynamic creates an unstable and unpredictable environment for the continuance of hardware and software over a long period of time and represents a greater challenge than the deterioration of the physical medium. Many technologies and devices disappear as the companies that provide them move on to new product lines, often without backwards compatibility and ability to handle older technologies, or the companies themselves disappear. Records created in digital form in the first instance and those converted retrospectively from paper or microfilm to digital form is equally vulnerable to technological obsolescence.\nAnother challenge is the absence of established standards, protocols, and proven methods for preserving digital information. With few exceptions, digital library research has focused on architectures and systems for information organization and retrieval, presentation and visualization, and administration of intellectual property rights (Levy and Marshall). The critical role of digital libraries and archives in ensuring the future accessibility of information with enduring value has taken a back seat to enhancing access to current and actively used materials. As a consequence, digital preservation remains largely experimental and replete with the risks associated with untested methods; and digital preservation requirements have not been factored into the architecture, resource allocation, or planning for digital libraries.\nProliferation of document and media formats\nThere is a proliferation of document and media formats, each one potentially carrying their own hardware and software dependencies. Copying these formats from one storage device to another is simple. However, merely copying bits is not sufficient for preservation purposes: if the software for making sense of the bits (that is for retrieving, displaying, or printing) is not available, then the information will be, for all practical purposes, lost. Libraries will have to contend with this wide variety of digital formats. Many digital library collections will not have originated in digital form but come from materials that were digitized for particular purposes. Those digital resources which come to libraries from creators or other content providers will be wildly heterogeneous in their storage media, retrieval technologies and data formats. Libraries which seek out materials on the Internet will quickly discover the complexity of maintaining the integrity of links and dealing with dynamic documents that have multimedia contents, back-end script support, and embedded objects and programming.\nConcerns of authenticity and reliability\nThe authenticity and reliability of electronic records are often questioned because of possible changes to content or structure. Authenticity can be defined as the ability of the records to be reliable over time and act as evidence of organizational transactions. Reliability on the other hand, refers to a record’s authority and trustworthiness, and this is tied to the ability of a record to stand for a fact it is about. A number of authors among them, Hoffman and MacNeil, have argued that there are no guarantees of authenticity and reliability in the electronic environment, as records can be deleted or changed at any time. It is, therefore, important that electronic records are managed to ensure that they remain authentic and reliable as evidence. Perhaps in the paper environment, one can say that this is more straightforward, as records are physical objects, and this makes identification of their characteristics easier than it is in the virtual world. The records provide evidence of actions, but the computer systems may fail to capture the necessary information about the context of the creation and the use of records.\nAccess to electronic records and concerns of privacy\nThe use of computers has enabled organizations to create databases that now handle huge amounts of data on-line, which is made accessible anywhere and anytime. This has raised concerns that if the information is not properly managed, it may be made available too easily, resulting in lack of protection for the citizen’s individual rights. Further, the vast amount of information maintained about individuals by both government and private organizations threatens their privacy. Ojedokum has highlighted some of the privacy infringement as unauthorized acquisition of data, unauthorized penetration into computer networks. Computers allow fast and inexpensive communication of information and the collection and storage of large amounts of data. At the same time, these capabilities allow individuals and organizations to access information.\nPower cuts and backup strategies\nPower cuts and irregular electricity supplies are a major barrier. In most African countries there are limited power distribution networks which do not even reach rural areas where the majority of the population lives. African cities with higher population that have been experiencing power cuts include but are not limited to Accra, Dares Salaam, Lagos, Gaborone, Nairobi, Harare etc. These power cuts have disrupted business operations. Increased dependence on computers and their services for data processing also means increased reliance on the power supplies that keep the systems operating. Power failure means that organizations may lose valuable information and time. It is estimated that 50-70% of businesses that lose their data due to power cuts never recover it, and some go out of business. There is a need, therefore, for systems that will maintain quality power supply and protect electronic systems.\nInternet Bandwidth (Digital Divide)\nThe digital divide is still a major hindrance. In many parts of Africa there is little access to computers and the Internet. In those parts where there is Internet access, the resources, such as bandwidth, are severely limited or extremely expensive. Some digital preservation systems, such as LOCKSS, have questionable applicability. In the case of LOCKSS, a group of sites collaboratively maintain the integrity of collections. LOCKSS, however, does not cater for unstable and irregular bandwidth availability – its algorithms will not make the most efficient use of bandwidth and may exacerbate problems at sites with poor bandwidth. All online archives need to make use of bandwidth in a way that is both minimal and cognizant of the differences among sites.\nSkills and Education\nLibrarians, archivists and information professionals in African institutions are arguably not as technically skilled as their counterparts in other parts of the world. The availability of computer systems in some parts of the continent has the effect that curators of information do not receive sufficient training in electronic systems. Digital media is not the norm for many forms of communication and information storage. The level of education of the general population in many African countries also is a problem. The number of literate individuals, as well as the number of individuals with access to a computer and the Internet is lower than elsewhere in the world. This creates a challenge for digital preservation both in terms of collection building, especially for end-user submissions, and dissemination. Novel solutions are needed for both these problems to make digital archives effective.\nDigital collections facilitate access, but do not facilitate preservation. Being digital means being ephemeral. Digital places greater emphasis on the here-and-now rather than the long-term, just-in-time information rather than just-in-case. The research program for digital preservation has only recently been initiated to develop strategies, guidelines, and standards. The challenges to digital preservation are considerable and will require a concerted effort on the part of librarians and archivists to rise up to these challenges and assert in public forums the importance of protecting a fragile digital heritage.\n1. Douwe Drijfhout. 2006. Challenges in terms of Digital Preservation. LIASA Conference 2006.\nwww.nlsa.ac.za/...preservation.../Drijfhout.Challenges%20in%20terms %20of%20Digital%20P reservation.pdf\n2. Christine W. Kanyengo. 2006. Managing Digital Information Resources in Africa: Preserving the Integrity of Scholarship\n3. Hussein Suleman. An African Perspective on Digital Preservation\n4. Margret Hedstrom. Digital Preservation: A Time Bomb for Digital Libraries www.eric.ed.gov/ERICWebPortal/recordDetail?accno=EJ586788\n5. Margret Hedstrom. Digital Preservation: Problems and Prospects\n6. Terry Kuny. 1997. A Digital Dark Ages? Challenges in the Preservation of Electronic Information. 63rd IFLA Council and General Conference']"	['<urn:uuid:b62340be-9d36-4f18-89d8-3b1cac684550>', '<urn:uuid:ee408182-9bc7-4f44-994d-f50aa7dea141>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	13	105	4073
25	What are the steps to make salmon patties from canned salmon, and how many servings does the recipe make?	The recipe makes 6 servings. To make salmon patties, first drain canned salmon (14.75-ounce) in a strainer, remove large bones and skin, and break into chunks with a fork. In a large bowl, whisk an egg and mix it with the salmon, shredded whole wheat bread or crushed saltine crackers, chopped green onions or white onion, minced garlic or garlic powder, black pepper, and seasoning (paprika, chili powder, or dill weed). Form into 6 patties about 1/2 inch thick. Heat 2 teaspoons oil in a large skillet over medium heat, cook patties uncovered for 3 minutes on each side until they reach 145°F. Serve immediately.	['Serving Size: 1 patty | Serves: 6\n- 1 can (14.75-ounce) salmon, drained\n- 1 egg\n- 1 slice whole wheat bread, shredded, or 5 saltine crackers, crushed\n- 3 green onions (including the green stems) or 1/3 cup white onion (chopped fine) (about 1/3 medium onion)\n- 1 medium garlic clove, minced, or 1/8 teaspoon garlic powder\n- Dash ground black pepper\n- 1/2 teaspoon seasoning (paprika, chili powder, or dill weed)\n- 2 teaspoons oil\n- Open and drain can of salmon in strainer. Remove any large bones and skin from salmon. Break salmon into chunks with a fork.\n- Break egg into a large bowl. Whisk with fork. Add salmon, bread or crackers, onion, garlic, pepper, and additional seasoning. Mix gently.\n- Form into 6 patties about 1/2″ thick.\n- Heat oil in a large skillet over medium heat. Place patties in skillet. Leave skillet uncovered. Cook 3 minutes. Turn over patties with a spatula. Cook the other side 3-4 minutes to a temperature of 145°F. Serve immediately.\nNutrition information per serving: 110 calories, 5g total fat, 1g saturated fat, 0g trans fat, 75mg cholesterol, 230mg sodium, 3g carbohydrate, 1g fiber, 0g sugar, 14g protein\nThis recipe is courtesy of ISU Extension and Outreach’s Spend Smart. Eat Smart. website. For more recipes, information, and videos, visit www.extension.iastate.edu/foodsavings/.\nBacteria are everywhere, but a few types especially like to crash parties. Staphylococcus aureus, Clostridium perfringens, and Listeria monocrytogenes frequent people’s hands and kitchens. And unlike bacteria that cause food to spoil, these bacteria cannot be smelled or tasted. Safe food handling is necessary for prevention.\nStaphylococcus (“staph”) bacteria are found on our skin, in infected cuts and pimples, and in our noses and throats. They are spread by improper food handling. Prevention includes washing hands and utensils before preparing and handling foods and not letting prepared foods- particularly cooked and cured meats and cheeses ass well as meat salads- sit at room temperature more than two hours. Thorough cooking destroys “staph” bacteria, but the toxin it may produce is resistant to heat, refrigeration, and freezing and can make you sick.\n“Perfringens” is called the “cafeteria germ” because it may be found in foods served in quantity and left for long periods at room temperature. Prevent it by dividing large portions of cooked foods such as beef, turkey, gravy, dressing, stews, and casseroles into smaller portions for serving and cooling. Keep cooked foods hot or cold, not lukewarm.\nListeria bacteria multiply, although slowly, at refrigeration temperatures. Therefore, these bacteria can be found in cold foods typically served on buffets. To avoid serving foods containing Listeria, follow “keep refrigerated” label directions and carefully observe “sell by” and “use by” dates on processed products like deli meat. Thoroughly reheat frozen or refrigerated processed meat and poultry products before eating.\nIf illness does occur, contact a health professional and describe the symptoms.\nWhich is better at preventing a foodborne illness outbreak—a wooden or plastic cutting board? This is a long-standing food safety question. Some research suggests wood is a better option, because the pores in the wood can trap and immobilize bacteria, which then die. Other studies, however, suggest bacteria absorbed in wooden boards can in fact survive and could possibly multiply and recontaminate the surface in the future, making plastic seem superior.\nThe take-away message is that all cutting boards, plastic or wooden, can be sources of contamination. To help prevent contamination, your cutting board needs to be clean and in good condition.\n- After each use, scrub your cutting board in hot, soapy water, then rinse and allow to air dry.\n- Using the dishwasher to clean plastic and solid wooden boards is fine, but laminated boards can crack in the dishwasher.\n- Wooden and plastic cutting boards can be disinfected with a bleach solution (1 tablespoon traditional regular chlorine bleach [6% sodium hypochlorite] per gallon of water or 2 teaspoons concentrated bleach per gallon of water). Pour solution over the surface and let sit for at least one minute; then rinse well and air dry.\n- It is time to get a new cutting board if your board has cracks, crevices, chips, or grooves where bacteria can hide.\n- Designate one cutting board for raw meat, poultry, and seafood, and another for vegetables, fruits, breads, and other ready-to-eat foods to avoid cross-contamination.\nFor more information, visit the Iowa Food Safety website: http://www.extension.iastate.edu/foodsafety/\nSources: University of California, Berkley Wellness Letter (December 2014) Food Safety Tips for Food Event Volunteers SP 452: https://store.extension.iastate.edu/Product/Food-Safety-Tips-for-Food-Event-Volunteers\nCross-contamination is a potential risk when preparing a variety of foods. It happens when harmful bacteria from one food, especially raw meat, is transferred to another food. It usually happens when cutting boards, knives, counter tops, and your hands are not washed after handling each food item.\nSteps to prevent cross-contamination include:\n- When shopping, keep produce and meats separate by putting each in plastic bags. Make sure the grocery bagger keeps them separate as well.\n- Always use a clean cutting board. If possible use different boards for fresh produce and raw animal products. Also, do not use excessively worn cutting boards; replace them.\n- Wash dishes and counter tops with hot soapy water before and after preparing each food item. When refrigerating food, make sure juices from raw meat and other animal products do not drip onto other foods. Keep them in sealed plastic bags or containers and stored below ready-to-eat food items.\n- Wash your hands before and after handling food, allowing about 20 seconds while using soap and warm water.\nGet more information on how to avoid cross-contamination.\nCutting boards are one of the most common kitchen items that can cause cross contamination. A different cutting board should be used for raw meat, poultry, and seafood than is used for preparing ready-to-eat foods like salads and fruits.\nProduce may not be cooked before serving, so contaminants will not have a “kill step” prior to consumption. Consider purchasing different color cutting boards and designate each color for a particular food. For example, red for meat, yellow for poultry, white for grain products, green for vegetables.']	['<urn:uuid:d9fd56ca-fc8c-4e4d-8f05-8e91c60fa8ac>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	19	105	1019
26	sleep newborn baby hours needed vs rem vs non rem stages comparison	Newborns need 14-17 hours of sleep per day, including naps. During sleep, there are two main stages: REM (rapid eye movement) and non-REM sleep. REM sleep accounts for 20% of total sleep time and is characterized by high-frequency low-voltage brain activity and rapid eye movements. Non-REM sleep makes up the majority of sleep time and is divided into 4 stages, with Stage II typically accounting for more than half of one night's sleep.	['Promoting Healthy Sleeping Habits in Children and Adolescents\nWhat is sleep?\nSleep is a biological process that has two stages: rapid eye movement (REM) sleep and non-REM sleep. While we sleep at night, we go through REM sleep and non-REM sleep multiple times.\nWhy is sleep important for children and adolescents?\nGood sleeping habits are tied to a healthy immune system. Good sleep can clear one’s mind after a busy day. Sleeping well also can lead to good mental health overall.\nHow much sleep should we get?\nThe amount of sleep needed at night changes with age (see Table 1). As children get older, they need fewer hours of sleep. Newborns need around 14-17 hours of sleep. Preschoolers require 10 to 13 hours of sleep. Adolescents need fewer hours of sleep and have later bed and wake times.\nTable 1: Recommended hours of sleep by age\n|Newborn (0-3 months)*||14-17|\n|Infants (4-12 months)*||12-16|\n|Toddlers (1-2 years)*||11-14|\n|Preschoolers (3-5 years)*||10-13|\n|School Age (6-12 years)*||9-12|\n|Adolescence (13-18 years)*||8-10|\n|Young Adults (18-25)||7-9|\n|Older Adults (65 years and older)*||7-8|\nNote: *includes naps. Source: Hirshkowitz et al. (2015).\nHints for creating healthy sleeping habits\nCreating a bedtime ritual.\nParents should create a bedtime ritual for their children. This could include the child taking a bath and changing into pajamas. The parent also can read a short calming story or poem to the child. Going to bed around the same time each night is important at any age.\nRelax before bedtime.\nChildren have busy lives with school, sports, and home-work. These activities can increase stress and sleeping problems. Learning how to relax and unwind at night can help children fall and stay asleep. This can include teaching children not to focus on worries and “bad thoughts” just before bed.\nAvoid caffeine and large meals before bedtime.\nParents should teach their kids healthy drinking and eating habits before bed. Drinking caffeine and energy drinks and eating before bed can lead to sleeping problems. Also, drinking liquid (even water) before bed can disrupt sleep.\nCreating a healthy sleeping space.\nChildren can be taught how to create a healthy sleeping space. This includes having a regular space for relaxing and falling asleep. Having a door for the bedroom can block out noises and light. Keeping the room cool, dimly lit, and quiet can improve sleeping. There are also sleeping products, such as weighted blankets, that can help children fall and stay asleep.\nLimit screen time before bed.\nChildren should limit the use of cell phones, computers, and video games before bed. Children who use these devices a lot before bed sleep poorly. For instance, texting before bedtime can distract the natural process of winding down at night. The blue light created by the screens can lower melatonin levels. This can make it harder to fall asleep.\nParents are wonderful role models.\nChildren also learn good sleeping habits by observing their parents. Also, many of the suggestions mentioned above also work well with adults.\nYou CAN do it.\nTeaching children healthy sleeping habits can be hard for many parents. It can take\ntime to make these changes. Seeking the support of friends and family can help during\nthis process. Finally, it’s important to remember that you\ncan do this.', 'Quality sleep is important for many aspects of health, yet many of us struggle to get enough sleep every night. This post discusses the proven health benefits of sleep.\nWhat is Sleep?\nSleep is defined as a natural and reversible state of reduced responsiveness and relative inactivity, accompanied by a loss of consciousness .\nSleep occurs in regular intervals and humans have a consistent need for sleep, so a loss or delay in regular sleep results in subsequently prolonged sleep .\nThere are two types of sleep, i.e. non-rapid-eye-movement (NREM) sleep and rapid-eye-movement (REM) sleep.\nRapid Eye Movement (REM) Sleep:\nREM sleep is characterized by high-frequency low-voltage brain electrical activity (as measured by an electroencephalogram (EEG)) and bursts of rapid contractions of eye muscles, causing the eyes to move rapidly.\nA healthy young person’s normal night of sleep typically includes 4-5 distinct REM cycles. REM sleep accounts for 20% of the total time spent asleep .\nNon-Rapid Eye Movement (NREM) Sleep:\nMost of a night’s sleep is spent in NREM sleep, which is further divided into 4 sub-stages (stages I-IV).\nStage I (N1) is the lightest stage of sleep\nStage II (N2) sleep is defined by the emergence of specific types of peaks in the EEG spectrum known as K-complexes and sleep spindles. N2 sleep typically accounts for more than half of one night’s sleep.\nStage III and IV (N3 and N4) are the deepest stages of sleep and are characterized by slow brain waves called delta brain waves. Together stages III and IV are referred to as “Slow-wave sleep” (SWS).\nSlow wave sleep (SWS) is considered the most restorative stage of sleep where the greatest impact of immune regulation happens.\nThroughout one period of sleep, both REM and NREM sleep alternate cyclically. For a healthy young person, the first progression through the four non-REM sleep stages typically takes 70-100 minutes.\nBoth REM and NREM sleep are important. Irregular cycling and/or the absence of sleep stages are associated with sleep disorders .\nTypically people with inflammation tend to have trouble getting enough deep sleep (stages 3 and 4) because inflammation can result in HPA axis activation and CRH increase, which reduces deep sleep .\nSleep and Brain Health\n1) Improves Memory and Cognitive Functions\nSleep is essential for effective cognitive function and sleep improves both cognitive function and memory, while a lack of sleep is detrimental to cognitive function .\n- Sleep helps strengthen and stabilize new memories acquired before sleep [7, 8, 9].\n- Sleep promotes the reprocessing of fresh memories and their integration into the pre-existing network of long-term memories [10, 11, 12].\n- Sleep loss diminishes many cognitive abilities including attention, language, reasoning, decision-making, learning and memory [13, 14, 15].\n- In children, shortened sleep duration, especially before the age of 41 months, is associated with lower cognitive performance and developmental tests .\n- Humans completely deprived of sleep showed deficits in the ability to perform unconscious and repeated movement memory such as bicycle riding and shoelace tying as well as deficits in short-term memory .\n- A short period of sleep prior to learning can enhance the capacity to remember new information .\n- Naps can reduce sleepiness and improve cognitive performance .\n2) Helps Remove Waste from the Brain\nThe glymphatic system clears metabolic waste from the mammalian brain while the fluid surrounding the brain and spinal cord cells (cerebrospinal fluid) removes beta-amyloid metabolites from the brain [20, 21].\nSleep stimulates the glymphatic system and the cerebrospinal fluid to clear metabolic waste from the adult brain .\nWhen individuals sleep well, the glymphatic system can effectively remove cellular waste byproducts that have accumulated outside and inside of brain cells .\n3) Sleep as a Neuronal Detoxification process\nLevels of oxidized glutathione, a sleep-inducing substance, increase at the end of the day and during sleep. These high levels of oxidized glutathione may counteract glutamate toxicity and prevent neuronal cell death level .\n4) Sleep and Mental Illness\nMost patients with depressive disorders also have sleep disturbances. Difficulty in initiating or maintaining sleep or both have been reported in about three-quarters of all depressed patients .\nPatients with persistent insomnia have a 2 to 3.5-fold increased risks of developing depression than those without insomnia [26, 27]. 72.7% of youths with major depression also reported insomnia and hypersomnia .\nSleep disturbances are also commonly associated with other psychiatric illnesses, including \n- generalized anxiety disorder\n- panic disorder\n- posttraumatic stress disorder\nUnfortunately, most antidepressant medications suppress rapid eye movement (REM) sleep .\n32% of people with brain fog also have a diagnosed sleep disorder, most commonly insomnia, sleep apnea or restless leg syndrome .\nIn mental health practice settings, children and adolescents with ADHD frequently reported sleep problems, particularly difficulty in initiating and maintaining sleep .\nSleep is Important for Healthy Metabolism\n5) Reduces Appetite\nThe two hormones that control appetite regulation are leptin and ghrelin. Leptin decreases your appetite, while ghrelin increases it.\nIn sleep-deprived young men, leptin levels decreased by 18% and ghrelin levels increased by 28% causing a total increase in appetite of 23% when compared to the levels present after healthy sleep .\n6) May Help with Weight Loss\nUnhealthy sleep is associated with obesity and eating problems .\nAdditionally, a National Health and Nutrition Examination Survey analysis showed that adults who slept less than 7 hours per night were more likely to be obese .\nPeople who slept only 5.5 hours per night lost 55% less body fat and 60% more fat-free mass (e.g bones and muscles) compared to people who slept 8.5 hours per night .\n7) Unhealthy Sleep Lowers Insulin Sensitivity and Increases Diabetes Risk\nGlucose tolerance was significantly lower in sleep-deprived young men compared to those who slept well .\nSleep deprivation also results in reduced insulin sensitivity .\nIn a cohort study, men reporting a short sleep duration of less than or equal to 6 hours per night were twice as likely to develop diabetes as those men who reported 7 hours of sleep per night.\nIn the same study, men who reported a long sleep duration or greater than 8 hours of sleep per night) were three times as likely to develop diabetes than those who reported 7 hours of sleep per night .\nTherefore, both short and long sleep durations increase the risk of developing diabetes .\nMetabolic pathways linking Sleep disorders with the development of Type 2 diabetes\n8) Poor Sleep Increases the Risks of Nonalcoholic Fatty liver Disease (NAFLD)\nNonalcoholic fatty liver disease (NAFLD) is a disease characterized by too much fat found in the liver of people who drink little to no alcohol which can lead to cirrhosis of the liver.\nShort sleep duration and poor sleep quality were significantly associated with an increased risk of NAFLD .\nWhile the Dongfeng-Tongji study showed that long nighttime sleep duration was also associated with a modestly increased risk of NAFLD \nSleep and Cardiovascular Health\nSleep influences cardiovascular function in healthy people and in those with heart diseases . Getting quality sleep can reduce cardiovascular risks, while unhealthy sleep is associated with cardiovascular disorders .\n9) Sleep Loss Increases the Risks of High Blood Pressure\nA sleep duration of fewer than 5 hours per night was associated with a significantly increased risk of high blood pressure (hypertension) in subjects between the ages of 32-59 years old .\nThe increased risk for hypertension and increased sympathetic nervous system activity caused by lack of sleep may underlie the relationship between sleep deprivation and coronary heart disease .\n10) Poor Sleep Increases Risks of Coronary Heart Disease\nBoth abnormally short and long self-reported sleep duration are independently associated with a modestly increased risk of coronary heart diseases .\nMen who sleep 4 hours or less per night are more likely to die from coronary heart disease than those who sleep 7-7.9 hours per night .\nWomen reporting 9 or more hours of sleep per night had higher risks of developing coronary heart disease than those with 7 – 9 hours of sleep .\nSleep Balances the Immune System\nHealthy sleep supports a healthy immune response, while a lack of sleep can worsen inflammation and autoimmunity.\n11) Enhances Immune Responses\nSleep supports the Initiation of an adaptive Immune response\nGood sleep also enhances the immune system’s ability to remember a previous infection and mount a more effective response in the case of re-infection [54, 56]. This is the methodology behind vaccines, and thus quality sleep improves the response to vaccination.\nSleep deprivation in humans changes the level of circulating immune cells (T cells and NK cells) and inflammatory cytokine levels (IL-1, IL-6, TNF-ɑ, etc.) . Increased levels of inflammatory cytokines can contribute to inflammatory diseases.\n12) Sleep Loss May Trigger Autoimmune Disease\nSleep Deprivation Reduces Regulatory T Cell Activity\nT regulatory cells (Treg) suppress inappropriate immune responses and prevent our immune system from attacking our own cells (self-tolerance) . The breakdown of the self-tolerance process can cause autoimmune diseases .\nIn experimentally sleep-deprived healthy people, Treg activity is reduced, suggesting that sleep deprivation may contribute to the development of autoimmune diseases .\nSleep Deprivation Increases Inflammation and Th17 Immune Responses\nSleep deprivation increases proinflammatory cytokines like IL-1, IL-1β, IL-6, TNF-ɑ, and IL-17.\nDisordered sleep may induce systemic inflammation and activate Th17 cells, thereby leading to autoimmune diseases [60, 61, 62]. The Th17 cytokine IL-17 remained elevated following sleep deprivation, even after recovery for 7 days .\nRead this post to learn more about Th17 and autoimmunity, and how to fix Th17 dominance.\nTh17 activation has been associated with several autoimmune diseases including systemic lupus erythematosus (SLE), rheumatoid arthritis (RA), inflammatory bowel disease and multiple sclerosis .\nSleep is Important for Hormone Balance\n13) Sleep Loss Activates HPA Axis and Increases Cortisol\nThe Hypothalamic-Pituitary-Adrenal (HPA) axis is the stress response axis which is activated in response to physiological or emotional stress .\nSleep, particularly slow wave sleep, inhibits the HPA axis and cortisol secretion .\nSleep loss and sleep disruption activate the HPA stress response axis, thus increases Corticotropin-releasing hormone (CRH) and cortisol levels [68, 69]. Read this post to learn about the negative effects of increased CRH.\n14) Sleep and Thyroid Hormone Axis\nThe hypothalamus senses low circulating levels of thyroid hormone and responds by releasing thyrotropin-releasing hormone (TRH). TRH then stimulates the pituitary to produce thyroid-stimulating hormone (TSH). TSH further stimulates the thyroid gland to secrete the thyroid hormones T3 (triiodothyronine) and T4 (thyroxine).\nTSH levels increase with acute sleep deprivation but may decrease with chronic sleep deprivation . The increase in TSH suppresses reproductive functions.\nGnRH= Gonadotropin-releasing hormone, LH= Luteinizing hormone, FSH=Follicle stimulating hormone\nThe Hypothalamic-Pituitary-Gonadal (HPG) axis mainly controls development, reproduction, and aging.\n15) Sleep Loss Lowers Testosterone Levels\nIn young adult men, testosterone levels begin to rise upon falling asleep and reach peak levels at the time of first REM sleep. Once at its peak, testosterone levels remain at the same until awakening [77, 78]. The amount of increase in testosterone levels during sleep and decrease during time awake varied between individuals .\nSleep deprivation disrupts this natural testosterone level rise and fall. Total sleep deprivation reduces testosterone levels in men [80, 81]. This reduction may be dependent on age, with a greater reduction in older men [78, 82].\nIn a clinical cohort study, men with lower testosterone levels also had lower sleep efficiency .\n16) Sleep Is Important for Healthy Estrogen Levels\nEstradiol (a type of estrogen produced by the ovaries) is essential for the maintenance of healthy reproductive tissues in women.\nIn reproductive age women, partial sleep deprivation and a variable sleep schedule increased estradiol levels by approximately 60% compared to non-sleep deprived women .\nHowever, in late reproductive age (perimenopausal) women, poor sleep quality is associated with lower estradiol levels .\nIn addition, both pre- and post-menopausal women with sleep-disordered breathing have lower estradiol levels compared to age- and cycle-matched women without sleep-disordered breathing .\n17) Sleep Loss May Cause Infertility in Women\nSleep loss elevates several pituitary hormones, which interfere with normal reproductive functions in women.\nIn reproductive-aged women, an increase in TSH (as seen in hypothyroidism) due to sleep deprivation can cause several reproductive issues including problems with ovulation (anovulation), recurrent miscarriage, the absence of menstruation, and irregular menstrual cycles [87, 88].\nIn addition, high TSH levels can increase prolactin, which can also increase fertility. High prolactin is also associated with anovulation, polycystic ovary syndrome, and endometriosis .\nHigh luteinizing hormone (LH, typically elevated in polycystic ovarian syndrome) can cause infertility. Healthy sleep keeps LH within normal, although fluctuating, levels . Sleep deprivation significantly increases the fluctuation of LH, with overall higher than healthy levels .\n18) Sleep Increases Growth Hormone\nIn healthy young adults, medically induced slow-wave sleep resulted in an increase in growth hormone release .\nAverage growth hormone levels were higher during slow wave sleep compared to other sleep stages .\nGrowth hormone levels also increase during naps but increased more during afternoon naps than during morning naps .\nSleep Deprivation and Digestive Health\nSleep deprivation appears to worsen symptoms for all digestive disorders . During SWS smooth muscles in the colon contract less, so this phase of sleep is considered the “rest period” for the colon.\n19) Stomach Ulcers\nIn animals, partial sleep deprivation has been shown to compromise stomach lining integrity by :\n- increasing stomach acidity\n- increasing blood levels of gastrin (a hormone that stimulates stomach acid secretion)\n- increasing histamine\n- increasing norepinephrine\n- decreasing stomach mucosal blood flow\nSleep deprivation damages the stomach and could be one of the risk factors for ulcer formation.\nHealthy sleep may protect against ulcer formation, as women who sleep more are less likely to get peptic ulcer disease .\n20) Bowel Diseases\nIBS patients symptoms decreased when subjects slept better .\nIn Crohn’s disease, the increase in IL-6 levels caused by sleep deprivation can worsen Crohn’s disease symptoms .\nFurther, a cross-sectional study found a nearly 3-fold increased risk of bowel disorders in patients with insomnia .\nSleep and Cancer Risks\n21) Poor Sleep Increases Cancer Risks\nSleep problems such as difficulty falling asleep, problem maintaining sleep, poor sleep efficiency, early awakening, and excessive daytime sleepiness are prevalent in patients with cancer .\nMeta-analysis studies suggest a positive association between long sleep duration and colorectal cancer .\nBoth extreme short and long sleep duration moderately increased the risk of colorectal cancer in postmenopausal women .\nA cohort analysis in women suggests that longer sleep may be associated with increased risks of estrogen-mediated cancers .\nLess than/equal to 6 hr of sleep increases the risk of developing prostate cancer, and sleep greater than/equal to 9 hr has a lower risk .\nSleep duration may influence breast cancer risk possibly via its effects on melatonin levels .\nMelatonin prevents tumor initiation, promotion, and progression possibly by :\n- Inhibiting abnormal/cancerous cell growth (anti-proliferative effects) [113, 114].\n- Increasing p53, a tumor suppressor protein .\n- Inducing cell differentiation \n- Preventing cancer cells from metastasizing (migrating and invading other tissues in the body) \n- Preventing angiogenesis (stopping cancer cells from growing their own blood vessels, which will allow the tumors to enlarge) \n- Decreasing telomerase activity, thus causing cancer cells to age and preventing the cells from dividing indefinitely [119, 120]\n- Functioning as a free radical scavenger .\nMelatonin production is closely related to sleep duration. Night-shift work disrupts sleep pattern and thus decreases melatonin levels, this may explain why the night-shift workers have higher cancer risks [122, 123, 124].\nIn addition, sleep also helps prevent cancer by:\n- controlling inflammation [52, 53]\n- maintaining healthy immune function \n- maintaining healthy levels of estrogen production (not too high) \n- maintaining healthy levels of oxidative stress \nAlso, check out our new book Biohacking Insomnia ($).']	['<urn:uuid:6ced8d2a-6f1b-4e11-bb03-b9415a19346c>', '<urn:uuid:1d04660b-c358-41ac-a365-43e66f89225c>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	12	73	3173
27	piano preparation methods tsybuleva grosvenor comparison	The two pianists have different approaches to preparation. Anna Tsybuleva developed her unique performance style focused on creating intimacy with listeners, drawing from her early experiences in the small village of Nizhny Arkhyz. Benjamin Grosvenor maintains he hasn't changed his basic preparation approach over the years, stating he simply tries to be as well prepared as possible, though he now must coordinate more efficiently due to playing more concerts and repertoire. Both pianists demonstrate careful attention to their craft - Tsybuleva through her focus on intimate connection, and Grosvenor through his systematic approach to preparation.	['Described by Gramophone Magazine as embodying “superb pianism and intelligent musicianship”, Anna Tsybuleva shot into the international spotlight in 2015 when she was crowned First Prize Winner of the Leeds International Piano Competition. She received wide critical acclaim for her winning performance, and was described as “A pianist of rare gifts: not since Murray Perahia’s triumph in 1972 has Leeds had a winner of this musical poise and calibre” (International Piano Magazine).\nNow a regular performer in major cities worldwide, Tsybuleva’s early experiences were more modest. Born in 1990, she was raised in Nizhny Arkhyz – a small village of approximately 500 inhabitants – in the Karachay-Cherkess Republic of Russia, where nature and the beauty of her surroundings proved a constant source of inspiration. These beginnings have served to feed directly into the development of her unique performance style today, which is one of captivating intimacy: drawing the listener into a private sphere of music-making in even the largest of concert halls.\nHighlights of Tsybuleva’s 2019/2020 season included major international debuts in recital at Het Concertgebouw Amsterdam and Istanbul’s Cemal Reşit Rey Concert Hall. She signed an exclusive recording contract with Signum Classics, and made her debut concerto recording on the label (Brahms Piano Concerto No.2, with DSO Berlin/Ruth Reinhardt). Amongst those performances requiring postponement due to the 2020 COVID19 pandemic were Tsybuleva’s German debut as soloist with Sinfonieorchester Wuppertal/Julia Jones, and North American debut at the Brevard Music Festival, North Carolina.\nThe 2020/2021 season opens with a return to National Philharmonic Orchestra of Russia/Vladimir Spivakov, and as the season unfolds we see Tsybuleva performing on prestigious recital stages across Europe, including her French debut at the Salle Cortot, Paris. Tsybuleva also makes her Polish debut with the Silesian Philharmonic Orchesrtra/Sebastian Perłowski, and embarks upon a major Chinese recital tour culminating with a performance at the Shanghai Oriental Arts Center. In a major season highlight, Tsybuleva embarks on the debut recording of a newly commissioned piano concerto, at Abbey Road Studios with the London Symphony Orchestra/Marin Alsop.\nIn recital, Tsybuleva has appeared on some of the greatest international stages, including Palais des Beaux-Arts, Philharmonie Luxembourg, Tonhalle Zürich, and the Wigmore Hall. As concerto soloist, recent highlights have included performances with Basel Symphony, Mariinsky, Oxford Philharmonic, Royal Philharmonic, Royal Liverpool Philharmonic, Singapore Symphony, St. Petersburg Philharmonic, and Tokyo Philharmonic Orchestra. She enjoys working regularly with such esteemed conductors as Sir Mark Elder, Michał Nesterowicz, Vladimir Spivakov, Yuri Temirkanov, and Joshua Weilerstein, amongst others. Tsybuleva is in high demand in Asia, where she recently undertook an extensive 14-concert tour as soloist with the Asian Youth Orchestra, covering China, Hong Kong, the Republic of the Philippines, Taiwan, and Japan.\nTsybuleva took her first piano lessons with her mother at the age of 6, before attending the Shostakovich Music School in Volgodonsk aged 9. From age 13, she continued her studies at the Moscow Central Music School and the Moscow State Tchaikovsky Conservatoire, under internationally renowned pedagogue Professor Lyudmila Roschina. During this time, Tsybuleva garnered her first major competition wins – including the Grand Prix of the International Gilels Piano Competition (2013), and top prizes from the Hamamatsu International Piano Competition (2012) and Takamatsu International Piano Competition (2014).\nAfter graduating from the Moscow Conservatoire in 2014 with the coveted award for ‘Best Student’, Tsybuleva furthered her studies with Claudio Martínez-Mehner at the Hochschule für Musik Basel. During these two years, she developed her growing passion for Romantic repertoire of the German School, and won the Leeds International Piano Competition in 2015 with her captivating performance of Brahms Piano Concerto No.2, under the baton of Sir Mark Elder and the Hallé Orchestra. Tsybuleva has since combined her international performance career with a thirst for further knowledge, and has just completed post-graduate studies at the Moscow State Tchaikovsky Conservatoire.\nTsybuleva’s debut recital recording (Fantasien, released on Champs Hill, 2017) comprised piano fantasies by C.P.E. Bach, Beethoven, Schubert, and Brahms. It garnered universal praise in the media for its imaginative and carefully crafted programme, with reviews including: “The playing of this magnetic young Russian artist is thoughtful, elegant, and exciting… I have long admired Sviatoslav Richter’s take, but this new recording is even more satisfying for its broader approach” (Fanfare Magazine).\nWith her “energetic elan, bravura, and heart-on-sleeve communication” (International Piano Magazine), Anna Tsybuleva is fast emerging as one of the finest pianists of her generation, “destined to become a world piano star” (APE Musicale, Italy).', 'INTERVIEW: CATCHING UP WITH THE BRITISH PIANO SENSATION BENJAMIN GROSVENOR\nBritish pianist Benjamin Grosvenor has been in the spotlight for over half of his lifetime, having won the BBC Young Musician of the Year in 2004 at the age of 10. He was already demonstrating profound maturity in his interpretations and command of the piano at that young age, and Grosvenor has continued to develop over the years. The first British pianist in 40 years to be signed to the Decca label, Grosvenor has now released four albums and continues to tour worldwide with solo recitals, chamber music collaborations, and concerto appearances. Vancouver-based writer Mark Ainley of ‘The Piano Files’ has followed the pianist’s career with special interest, taking in both New York and Vancouver concerts in Grosvenor’s 2017 North American Tour. He has interviewed the pianist previously (interview), and was commissioned to write the liner notes for Grosvenor’s third Decca CD (‘Dances’). Ainley’s new interview attempts to bring us up to date on the artist’s current thoughts and preoccupations, and his recollection of his experiences growing up with so much acclaim. Benjamin Grosvenor has now made three Vancouver appearances with the Vancouver Recital Society; his debut concert was April 2013 (review).\n1. I HAD THE PLEASURE OF INTERVIEWING YOU IN 2011, WHEN YOU HAD JUST TURNED 19 AND WERE ABOUT TO GIVE YOUR MUCH-ANTICIPATED OPENING CONCERT AT THE PROMS. YOU MUST HAVE GONE THROUGH A LOT BOTH PERSONALLY AND PROFESSIONALLY IN THE LAST 6 YEARS.\nYes, quite a lot has happened in that time, from completing my studies at the Royal Academy of Music to taking on things like the vexing responsibilities of home ownership. Professionally, I have had so many rewarding musical experiences and partnerships and I have learned a great deal from each one. I have also undertaken a lot of new repertoire. I’m sure that certain aspects of my musicianship have developed as a consequence although it is difficult for me to tell you the specifics of this process.\n2. MUCH WAS MADE OF THAT PROMS DEBUT AND THE FACT THAT YOU WERE THE FIRST BRITISH PIANIST SIGNED TO DECCA FOR SEVERAL DECADES. HOW HAVE YOU ADAPTED TO BEING A HIGH-PROFILE ARTIST IN YOUR NATIVE COUNTRY? IS YOUR EXPERIENCE DIFFERENT AT HOME THAN ABROAD?\nIn some ways, the nature of the coverage I receive in the UK can be a little trickier to deal with: the fact that I’ve been receiving media attention since I was 11 can mean that local journalists and some in the music business approach me based upon things that I did many years ago. Of course, I’m fortunate to be receiving attention at all, yet I do think it is refreshing to perform in countries where there are no baked-in preconceptions about having been perceived as a prodigy playing a certain range of works. The two things I most appreciate about playing ‘at home’ are, first, not having to go through the stresses of airports and delayed flights to get to a venue and, second, the opportunity to play with orchestras and conductors with whom I’ve established a musical bond over time. For example, I enjoyed greatly a tour I made last autumn with the Hallé Orchestra and Sir Mark Elder playing the two Liszt Concerti. I learned a lot from that collaboration and Elder is an ideal, generous partner.\n3. YOU HAVE NOW GIVEN A GREAT MANY CONCERTS ALL OVER THE WORLD. DO YOU THINK YOUR WAY OF PREPARING FOR A PERFORMANCE HAS CHANGED?\nIn terms of musical approach, this will sound horribly boring, but I don’t plan the process of preparation differently now than I did ten years ago. I simply try to be as well prepared as I can. The major difference is that achieving this goal now takes considerably more coordination because I’m playing a lot more concerts and repertoire. Specifically, each season will usually involve 3-5 concerti plus one and a half recital programs plus chamber repertoire, as well as time for preparing new solo and concerted works. I’ve had to learn to try to use practice and preparation time as efficiently as possible.\n4. YOUR TRAVELS HAVE OBVIOUSLY BROUGHT YOU TO A LOT OF DIFFERENT PIANOS AND HALLS. WHAT DO YOU HAVE TO DO TO ADAPT TO EACH INSTRUMENT AND PERFORMANCE SETTING?\nI was always fussy about pianos and this has only grown with the years. I do have a particular sense of the texture and colour of sound that I’m seeking in each passage, as well as the overall dynamic range that I think is right for the acoustic. Of course, changes to tone can be improvised depending upon the character of a given instrument, and I might actually discover things through being forced to make adjustments on what seems to be, at first, an unfriendly instrument. But the real problem comes with those pianos that simply don’t have much capacity for colour or a grotty action. Experiences like this come with the trade, and I’ve had years of dealing with such situations. I’d like to think that I’m getting better at soldiering on through, but it doesn’t get any less frustrating.\n5. HOW DO YOU GO ABOUT SELECTING YOUR REPERTOIRE? ARE THERE SOME WORKS THAT YOU ARE EXPLORING NOW THAT YOU HAD WAITED ON EARLIER?\nI’ve always enjoyed eclectic programmes and, within that overall approach, try to set myself some challenges. This season, for example, I’m playing the Berg Sonata. I’d loved Berg’s music since playing some of the Seven Early Songs in a chamber concert when I was a student. I’ve read through his violin concerto a couple of times with a friend who was performing it but, apart from these two experiences, I haven’t played any music from the Second Viennese School. I had felt rather little affinity with Prokofiev until about two years ago, but now I’d like to play one of the concerti and one or more of the sonatas in a future season.\n6. EARLIER GENERATIONS OF ARTISTS PRIOR TO THE DEVELOPMENT OF RECORDING TECHNOLOGY LEARNED THE MUSIC FROM THE SCORE FIRST. HOW DO YOU APPROACH NEW WORKS THAT YOU WERE FIRST EXPOSED TO THROUGH RECORDINGS OR CONCERTS?\nIt can be awkward if one has a particular performance of a work already imprinted in the mind, but there is really no way to prepare a new piece other than via first principles, which means beginning with the score, and also comparing different editions and Urtexts. In fact, when something like this has happened with me, I have usually found that a detailed study of the score prompts possibilities different from those of, say, a ‘favourite’ recording. And that launches me on thinking about the piece in my own way.\n7. WE HAVE PREVIOUSLY DISCUSSED THE VALUE OF STUDYING HISTORICAL RECORDINGS BY THE GREAT PIANISTS OF THE PAST. DO YOU THINK PIANISTS TODAY STILL HAVE A LOT TO LEARN FROM THEM?\nI do feel that there is a great deal to learn from historical recordings. And pianists should listen not only to recordings of historical pianists, but to conductors, violinists, singers… There are a great many current artists who I admire too, but I wish I currently had more time for listening in general – hearing recordings and, particularly, attending concerts – but generally I’m so busy now that listening time is limited.\n8. YOU RECENTLY RELEASED YOUR FOURTH ALBUM ON DECCA - A SOLO DISC THEMED AROUND ‘HOMAGES’ - AND WE ARE ALL ANTICIPATING NEW RECORDINGS FROM YOU. CAN YOU GIVE US A HINT OF WHAT’S NEXT?\nI prefer not to make a recording until I’ve reached a robust level of confidence in the repertoire, which will nearly always mean plenty of concert outings first. I’m currently in discussions with Decca about the next disc. They’d like a concerto recording, but that does involve a lot of coordination with venue, orchestra, and conductor. Unfortunately, I can’t give any further details simply because there aren’t any at this point!\n9. IN CLOSING, WHAT WOULD YOU REGARD AS THE MOST UNIQUE (STRANGE, HUMOROUS, WONDERFUL) PERFORMING EXPERIENCE YOU HAVE HAD THUS FAR?\nI think that often the more entertaining, or at least unusual, stories are when things don’t quite go as smoothly as planned. I can think of a few such stories – from fainting double bassists to tactless Sicilian photographers – but there is not one that is quite golden enough to set down into words yet. I have to say that some of the most wonderful performing experiences have been at the BBC Proms. The atmosphere there is really something so unique. I’ve recently had fantastic experiences in parts of the world that I had not previously visited. I have fond memories of visiting Brazil when I was thirteen and I was able to undertake a longer tour of South America this year. I was struck by the warmth of the audiences and the charming people I met there.\n© Mark Ainley 2017\nPhoto Credits: Patrick Allen, Juan Diego Castillo, Sophie Wright']	['<urn:uuid:857d1fcb-8d8b-4a51-bcbc-ba517ac7b6ce>', '<urn:uuid:a2577e0e-924f-408f-b57d-0e35f2b7e931>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T04:31:27.499155	6	95	2242
28	natural landscape protection tourism development regulations environmental impact unesco sites	Canada's UNESCO World Heritage site Kluane National Park demonstrates successful protection of natural landscapes, including Canada's highest peak Mt. Logan and vast expanses of glaciers, while maintaining tourism access through activities like rafting and hiking. This exemplifies effective regulation of tourism in protected areas, contrasting with developing regions like Central Kalimantan where tourism development policies are still premature in balancing environmental protection with tourism growth. The analysis of tourism regulations shows that clear policy frameworks are essential for protecting natural landscapes while developing sustainable tourism.	['Canada’s largest island is a pristine wilderness of fjords, mountains, glaciers and tundra. Sledge across frozen Frobisher Bay, spot whales, seals and polar bear cubs from the floe edge, gawp at colossal icebergs, encounter caribou, musk-ox and migrating birds, kayak amid jaw-dropping scenery, and discover Inuit traditions.\nBanff National Park\nPicture Canada and you might imagine dramatic, jagged mountains criss-crossed by glacier-fed rivers and peppered with vivid, emerald lakes. Look no further than Banff National Park. Canada’s oldest national park sits at the heart of the Canadian Rockies; highlights include stunning cerulean Lake Louise and the striking peaks and shimmering glaciers of the Icefields Parkway.\nBay of Fundy\nThis 270km-long (170 miles) bay stretches between the Atlantic provinces of New Brunswick and Nova Scotia and possesses the world’s highest tidal range – sometimes 15m (49ft) or more. Witness the inter-tidal zone’s incredible changes, go whale watching, then take an exhilarating rafting trip on the Shubenacadie River tidal bore or a thrilling jetboat ride on Saint John’s Reversing Falls.\nOne of the world’s most scenic drives, the Cabot Trail loops 300km (186 miles) round Nova Scotia’s Cape Breton Island, hugging the coastline and traversing Cape Breton Highlands National Park. Take in pretty fishing villages and picturesque beaches, enjoy spectacular hiking and horse-riding trails, paddle hidden coves and revel in the region’s Celtic musical heritage.\nKluane National Park and Reserve\nA UNESCO World Heritage site since 1979, Kluane National Park in the Yukon is a vast expanse of vertiginous mountains, gigantic glaciers and extraordinary wildlife. At its heart is Canada’s highest peak, 5,959m (19,550ft) Mt Logan. Raft, hike or canoe through this exceptionally beautiful backcountry, home to eagles, grizzlies, wolves and moose.\nRomantic yet cutting edge, North American with a distinctly French flair, the nation’s second biggest city dazzles and enchants. Wander the elegant, cobbled streets of Old Montreal, potter among the quirky stores, funky cafés and colourful houses of bohemian neighbourhood Le Plateau, or soak up some serious tunes during the world’s largest jazz festival in June and July.\nThunderous, mighty and mesmerising: the spectacle of up to 170,000 cubic m (6 million cubic ft) of water roaring down the equivalent of a 13-storey building is truly awesome. Sail to the base of the Canadian Horseshoe Falls aboard the classic Maid of the Mist tour, soar overhead on a helicopter or crank up the adrenaline on a nail-biting jetboat trip through Niagara Gorge.\nThe ethereal lights of the Aurora Borealis are frequently visible in the Northwest Territories. To observe this magical, kaleidoscopic phenomenon, join a tour from Yellowknife and travel by dogsled across a frozen lake or splash out on a trip to a fly-in lodge where you could also try snowshoeing, ice fishing or snowmobiling.\nPrince Edward Island beaches\nPEI’s shoreline encompasses over 800km (500 miles) of gloriously sandy beaches with enormous mobile dunes, distinctive red sandstone cliffs and surprisingly warm water for its latitude. Listen out for the ‘singing sands’ at Basin Head, dig for clams at Teahill or stargaze at Cavendish in PEI National Park.\nUNESCO-listed Old Quebec is North America’s only remaining walled city, its charming 17th- and 18th-century houses a world away from the glitzy skyscrapers elsewhere. Gaze up at the impressive Château Frontenac, explore the star-shaped citadel, or track down local arts and crafts and pause at tantalising bistros in the Petit-Champlain district.\nThe Prairie provinces\nComprising Alberta, Saskatchewan and Manitoba, the Prairies’ expansive grasslands in the south give way to parkland and coniferous forests further north. Swim, sail or waterski in one of Saskatchewan’s 100,000 lakes, spy polar bears and beluga whales in Churchill, Manitoba or hunt for fossils in Alberta’s Dinosaur Provincial Park.\nCanada’s largest metropolis curls along the rim of Lake Ontario. Steel your nerves and gaze through glass floors from 342m (1,122ft) up on the CN Tower, explore the vibrant cafés, boutiques and galleries of the Distillery District’s beautifully restored Victorian industrial architecture, or grab an ice-cream and some rays on the beach.\nNewfoundland’s Twillingate Islands are considered the best place in the world to see icebergs. Hop aboard an ocean cruise between May and August to glimpse hunks of ice the size of a five-storey house as well as humpback whales, dolphins and an astonishing array of seabirds.\nNestled beneath the towering Coast Mountains on the shores of the Pacific Ocean, Canada’s gorgeous west coast city exudes an easygoing, cosmopolitan vibe. Ski or hike Grouse Mountain, cross the dizzying Capilano Suspension Bridge, bike or blade round the Stanley Park seawall, or hit Chinatown for some top-notch dim sum.\nTwo mountains, 3,307 hectares (8,171 acres) of terrain, 1,609m (5,280ft) of vertical and over 10m (33ft) of annual snowfall: Whistler beats its rivals hands down. This is the place to ski or snowboard in North America, thanks to an abundance of powder-filled bowls, magical gladed runs, gnarly chutes and cruisy slopes, alongside a lively pedestrianised village.', 'Main Article Content\nEcotourism globally has become an alternative to mass tourism to minimize environmental impacts, and at the same time, it is also useful to improve the living standards of local communities. The Central Kalimantan government also underpin ecotourism activities as a tourism focus and written it on its regional regulations. This study is a literature study to analyse RIPPARPROV (Rencana Induk Pengembangan Pariwisata Provinsi/Provincial Tourism Development Master Plan) of Central Kalimantan 2013-2028 with a conceptual approach from Hall and Jenkins by looking at how clearly the desired policy issues are set forth in regulations. The method of analysis uses qualitative-comparative methods, by exploring the main themes of the global concept of ecotourism (which are ecotourism definition, ecology, education, responsibility, awareness of conservation and economy of local communities) in the RIPPARPROV of Central Kalimantan 2013-2028. The results of the analysis show that the Central Kalimantan Provincial Government is still premature in an effort to develop ecotourism in its territory. This is indicated by the lack of explanation about ecotourism specifically the absence of a policy on tourist education, awareness and conservation participation by tourists. There is only a policy of improving the local economy which is very prominent in Central Kalimantan’s RIPPARPROV.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBoyd, S. W., & Butler, R. W. (1996). Managing ecotourism: An opportunity spectrum approach. Tourism Management, 17(8), 557-566. doi: 10.1016/s0261-5177(96)00076-3\nBPS. (2018). Kunjungan wisatawan mancanegara berdasarkan tahun. Jakarta: BPS.\nOladi, J., & Bozorgnia, D. (2010). Evaluating the ecotourism potentials of Naharkhoran area in Gorgan using remote sensing and geographic information system. Earth Resources and Environmental Remote Sensing/GIS Applications. doi:10.1117/12.860095\nCengiz, T., & Çaliskan, E. (2009). Ecological approach in sustainable tourism: Savsat district example. Sci. Res. Essay, 4(5), 509-520.\nCochrane, J. (2009). New directions in Indonesian ecotourism. Tourism in Southeast Asia: Challenges and New Directions, 254-269.\nCrawford, J., Mulvey, M, Quinn, S. (2012). Ecotourism handbook for Ireland. Dublin: Failte Ireland.\nCREST. (2010). Responsible Travel: Global Trends & Statistics. Stanford University & Washington, DC.\nDalem, A. A. G. R. (2002). Ecotourism in Indonesia: Linking Green Productivity to Ecotourism: Experiences in the Asia-Pacific Region. Tokyo: Asian Productivity Organization.\nDel Chiappa, G., & Lorenzo-Romero, C. (2014). Environmental issues to profile the consumers’ attitude: a latent segmentation approach. Environmental Engineering and Management Journal, 13(10), 2449-2457.\nDecree of the Minister of Tourism, Post and Telecommunications No. 5 of 1989 concerning Guidelines for Organizing Enchantment of Sapta Pesona. Jakarta: Departemen Pariwisata, Pos, dan Telekomunikasi.\nDror, Y. (2017). Public policy making reexamined. Routledge.\nDunn, W., N. (2015). Public policy analysis. Routledge.\nErwiantono, E., Susilo, H., Aditya, A., Saleha, Q., & Budiayu, A. (2017). Analisis kebijakan untuk pengembangan ekowisata berkelanjutan di kawasan perairan Labuan Cermin–Kabupaten Berau, Kalimantan Timur. Jurnal Kebijakan Sosial Ekonomi Kelautan dan Perikanan, 6(1), 47-63.\nFennell, D. A., & Weaver, D. B. (1997). Vacation farms and ecotourism in Saskatchewan, Canada. Journal of Rural Studies, 13(4), 467-475.\nHall, C. M., & Jenkins, J. M. (2003). Tourism and public policy. Cengage Learning EMEA.\nHarahap, S. A., & Baiquni, M. (2015). Kebijakan Pengelolaan Ekowisata Taman Nasional Terkait Otonomi Daerah Taman Nasional Komodo Di Kabupaten Manggarai Barat (unpublished dissertation). Yogyakarta: Universitas Gadjah Mada.\nHoag, H. (2007, June 23). Green to go. The Globe and Mail. Retrieved November 1, 2018, from http://www.theglobeandmail.com/life/green-to-go/article4095237/?page=all\nHwang, K., & Lee, J. (2018). Antecedents and consequences of ecotourism behavior: independent and interdependent self-construals, ecological belief, willingness to pay for ecotourism services and satisfaction with life. Sustainability, 10(3), 789.\nIanniello, J. (2013). Ecotourism Remains a Dream for Too Many. Roy Morgan Research.\nIslam, M. U. (2013). Prospective of ecotourism in Poonch District. International Journal of Physical and Social Sciences, 3(12), 541.\nJamal, T., Borges, M., & Stronza, A. (2006). The institutionalisation of ecotourism: Certification, cultural equity and praxis. Journal of Ecotourism, 5(3), 145-175.\nMinister of Home Affairs Regulation No. 33 of 2009 concerning Guidelines Development of Ecotourism in the Region. Jakarta: Minister of Home Affairs.\nMcLaughlin, J.M. (2011). Ecotourism Assessment: Applying the Principles of Ecotourism to Paddle-Based Recreation in St. Lawrence Islands National Park and Environs (unpublished thesis). Kingston: Queen’s University.\nNirwandar, S. (2015). Ecotourism in Indonesia. Jakarta: Ministry of Tourism and Creative Economy.\nObenaus, S. (2005). Ecotourism – sustainable tourism in national parks and protected areas: A case study of Banff National Park in Canada and National Park Gesäuse in Austria – a comparison (unpublished dissertation). Austria: University of Vienna.\nOgato, G. S., Abdise, F., Gammie, T., & Abebe, W. (2014). Promoting rural local development: The case of Wonchi ecotourism society, West Shoa Zone, Ethiopia. Prime Journal of Social Science, 3(4), 662-673.\nPaulus, A. (2009). Impacts of Ecotourism on the Behaviour of Sulawesi Crested Black Macaques (Macaca nigra) and Spectral Tarsiers (Tarsius spectrum) in the Tangkoko-Batuangus Nature Reserve, North Sulawesi, Indonesia (unpublished dissertation & thesis). Plymouth: The University of Plymouth.\nCentral Kalimantan Regional Regulation No. 2 of 2013 concerning RIPPARPROV Central Kalimantan 2013-2028. Palangka Raya: Pemerintah Daerah Kalimantan Tengah.\nGovernment Regulation No. 50 of 2011 concerning the Master Plan for National Tourism Development in 2010-2025. Jakarta: Pemerintah RI.\nPerera, P., & Vlosky, R. (2013). How Previous Visits Shape Trip Quality, Perceived Value, Satisfaction, and Future Behavioral Intentions: The Case of Forest-Based Ecotourism in Sri Lanka. International Journal of Sports Management, Recreation & Tourism, 11, 1-24. doi:10.5199/ijsmart-1791-874x-11a\nPratt, L., Rivera, L., Bien, A., & Bertrand, N. (2011). Tourism: Investing in energy and resource efficiency. Spain: UNWTO.\nRamchurjee, N, A., & Suresha, S. (2013). Ecotourism in Bagalkot District, Karnataka, India: An assessment of the inhabitants’ awareness level and attitudes. International Journal of Environmental Sciences, 3(6), 2278-2290.\nRhama, B. (2017). The implications of the values and behaviors of actors for ecotourism policy: A case study of Sebangau National Park, Central Kalimantan, Indonesia (unpublished dissertation). Lancashire: University of Central Lancashire.\nRoss, S., & Wall, G. (1999). Ecotourism: towards congruence between theory and practice. Tourism Management, 20, 123-132.\nRoss, S., & Wall, G. (1999a). Evaluating ecotourism: the case of North Sulawesi, Indonesia. Tourism Management, 20(6), 673-68.\nRoss, S., & Wall, G. (2001.). Ecotourism: A theoretical framework and an Indonesian application. Tourism,\nRecreation, and Sustainability: Linking Culture and the Environment, 271-288. doi:10.1079/9780851995052.0271\nScheyvens, R. (1999). Ecotourism and the empowerment of local communities. Tourism Management, 20(2), 245-249.\nSerra, G. (2007). Ecotourism in the Palmyra Desert, Syria: A Feasibility Study. BirdLife International.\nSharpley, R. (2006). Ecotourism: A consumption perspective. Journal of Ecotourism, 5(1-2), 7-22.\nSharpley, R. (2009). Tourism development and the environment: Beyond sustainability? London: Earthscan.\nShum, K. (2007). Trends in ecotourism. Retrieved November 1, 2018, from http://www.lohas.com/green-travel\nSTCRC. (2009). Wildlife tourism: Challenges, opportunities and managing the future. Australia: Sustainable Tourism Cooperative Research Centre.\nTheobald, W. F. (2012). Global tourism. Routledge.\nTIES. (2006). Global ecotourism fact sheet. Washington.\nTjiptoherijanto, P. (2012). Civil Service Reform in Indonesia: Culture and Institution Issues. Depok: Department of Economics, Faculty of Economics, University of Indonesia.\nVar, M., Yalcinap, E., Pulatkan, M. (2010). A potential offer-demand problem in ecotourism: Different perspectives from eco-tourists and indigenous people. Scientific Research and Essays 5(17), 2517-252.\nWall, G. (1997). Is ecotourism sustainable? Environmental Management, 21(4), 483-491.']	['<urn:uuid:1b37a08d-5524-43f0-9737-2cc9e92b2eb3>', '<urn:uuid:930149be-25a9-4f85-84a0-0ed55292f4a2>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:31:27.499155	10	85	1980
29	How does climate change affect water resources and farming?	Climate change causes less rainfall and serious droughts, with water sources like the Draa River drying up. This impacts farming by making it harder to grow crops and leading to decreased agricultural yields. When water is found, it's often contaminated and unsuitable for consumption. These changes also affect food security and make previously arable land barren.	['Terrain in Morocco\nMorocco covers an area of more than 446 square kilometres. It has coastlines along both the Mediterranean Sea and the Atlantic Ocean. The interior of the country is mainly mountainous, with the Atlas Mountains, split into three sections, and the Rif Mountains. Lands close to the Sahara Desert are hot and dry, and the large Moroccan Plateau is relatively flat. The plains are used for agriculture, while the drier areas are used to grow many palm trees.\nTraditional Ways of Life in Morocco\nTo a large extent, the two ethnic groups of Morocco—the Arabs and the Berbers—have led very different traditional lives. While the Arabs tended to settle in communities, causing cities and towns to grow and flourish, the Berbers were historically nomadic. If the flatlands of Morocco attracted the Arabs, the mountains were definitely the domain of the Berbers. It is the Arabs who have been responsible for much of Morocco’s striking architecture. That’s not to say, however, that the Berber groups haven’t made huge contributions to the country’s history and culture.\nAlthough many Berber groups now live in mountain villages, they still typically rely on farming to make a living and survive. Additionally, Morocco still has around 25,000 nomadic people, who move around and have no fixed home. For many years there were groups who were always on the move at the fringes of the Sahara Desert, seeking water and grazing land for their animals. The number of nomadic people is decreasing though, with climate change cited as one of the main reasons for people giving up the traditional nomadic way of life.\nHow the Climate Has Altered in Morocco\nGlobal warming has led to the temperatures in Morocco becoming even more extreme. The effects are especially noticed in areas around the desert. Existing high temperatures are becoming even higher. These higher temperatures are causing conditions to be even drier. There is less rainfall, leading to serious droughts. The outlook isn’t positive; experts predict that the temperatures in North Africa, along with the Middle East, will increase twice as quickly as the average around the world.\nImpact on Morocco’s Landscapes\nHot air becomes trapped around the Sahara Desert, which leads to the arid sandy expanses growing. Places that were once arable land are now covered in sand. Land that could previously be used for farming is now barren. Sand is found in areas that were once covered in date palms. There are also fewer places where nomadic herders can find suitable places for their animals. It is gloomily forecast that certain parts of the country will actually become uninhabitable.\nIt isn’t only the expanding desert that spells sorrow for certain parts of Morocco; the lack of rain and the drying up of existing water sources is also a huge effect of the rising temperatures. The Draa River, for example, is now dry for most of the year. (The construction of a dam played a key role, however, in the river ceasing to flow.) When water is found, contamination often means that it isn’t suitable for consumption.\nVillages, such as M’Hamid, have now sprung up in places where once-nomadic people have chosen to settle.\nImpact on Moroccan Life\nThe changing climate and conditions mean that people, nomadic or otherwise, cannot adequately take care of their animals. Goats, sheep, and camels cannot survive. This removes not only meat, milk, skins, and wool from families, some of which would be sold to make an income, but also an important means of transportation. Farmers struggle to grow crops. This has a knock-on effect, with less produce available for the wider community.\nThe change from nomadic life to village life can be a challenge for individuals. Instead of living by the seasons and having huge amounts of independence, people find themselves in unfamiliar situations with a whole host of social rules and expectations that they are just not accustomed to. The vast majority of nomadic people have little to no formal education, making it difficult to secure alternative employment.', 'The various effects of climate change on rural communities are expected to include: drought, depletion of water resources and biodiversity, soil erosion, decreased subsistence economies and cessation of cultural activities. South Africa contributes considerable CO2 emissions, being the 14th largest emitter of CO2.\nWhat are 5 effects of climate change?\nIncreased heat, drought and insect outbreaks, all linked to climate change, have increased wildfires. Declining water supplies, reduced agricultural yields, health impacts in cities due to heat, and flooding and erosion in coastal areas are additional concerns.\nWhat are the effects caused by climate change on the economy of South Africa?\nWhat are the predicted economic impacts of climate change in South Africa? … Tourism may be affected due to a loss of habitats and biodiversity, and due to changes in temperature, humidity and malaria risk, and represents the biggest potential economic loss since tourism contributes as much as 10% of GDP.\nWhat are the major effects of climate change?\nThe direct consequences of man-made climate change include:\n- rising maximum temperatures.\n- rising minimum temperatures.\n- rising sea levels.\n- higher ocean temperatures.\n- an increase in heavy precipitation (heavy rain and hail)\n- shrinking glaciers.\n- thawing permafrost.\nWhat is the environmental impact of climate change in South Africa?\nAnnual temperatures in South Africa have increased by at least 1.5 times, while globally with 0.65°C over the last five decades. This increase in average temperatures will greatly influence water resources, food security, health, infrastructure and the environment, which includes biodiversity and the ecosystems.\nIs climate change bad 2020?\nGlobal temperature rise\nThe planet was warmer by 1.2 degrees Celsius from January to October in 2020 than the pre-industrial average measured between 1850 and 1900, according to the Nature report. This is the second-warmest recorded when compared to similar periods in historical data.\nWhat can we do to stop climate change?\nDemand Climate Action\n- Speak up! …\n- Power your home with renewable energy. …\n- Weatherize, weatherize, weatherize. …\n- Invest in energy-efficient appliances. …\n- Reduce water waste. …\n- Actually eat the food you buy—and make less of it meat. …\n- Buy better bulbs. …\n- Pull the plug(s).\nWhat are the effects caused by climate change on the economy?\nWarmer temperatures, sea level rise and extreme weather will damage property and critical infrastructure, impact human health and productivity, and negatively affect sectors such as agriculture, forestry, fisheries and tourism.\nWhat are the main causes of climate change in South Africa?\nThe burning of fossil fuels (e.g. through driving cars and to produce electricity), the cutting down of rainforests, the destruction of native vegetation, unsustainable development, the increase of livestock farming, industrial processes and an increased amount of waste going to landfills all contribute to speeding up …\nWhat are the effect imposed caused by climate change?\nMore frequent and intense drought, storms, heat waves, rising sea levels, melting glaciers and warming oceans can directly harm animals, destroy the places they live, and wreak havoc on people’s livelihoods and communities. As climate change worsens, dangerous weather events are becoming more frequent or severe.\nWhat are the 6 major factors that affect climate?\nLOWER is an acronym for 6 factors that affect climate.\n- Latitude. It depends on how close or how far it is to the equator. …\n- Ocean currents. Certain ocean currents have different temperatures. …\n- Wind and air masses. Heated ground causes air to rise which results in lower air pressure. …\n- Elevation. …\nWho is most affected by climate change?\nThe Germanwatch institute presented the results of the Global Climate Risk Index 2020 during COP25 in Madrid. According to this analysis, based on the impacts of extreme weather events and the socio-economic losses they cause, Japan, the Philippines and Germany are the most affected places by climate change today.\nWhat are the 3 main impacts of climate change?\nThere are three major ways in which global warming will make changes to regional climate: melting or forming ice, changing the hydrological cycle (of evaporation and precipitation) and changing currents in the oceans and air flows in the atmosphere.\nWhat can we do to reduce climate change in South Africa?\nFive key steps are essential for achieving climate justice for Africa:\n- Phase out fossil fuel subsidies. Many rich countries say they want a climate deal. …\n- Clean up climate finance. …\n- Drive Africa’s low-carbon energy transition. …\n- Leave no-one behind. …\n- Adopt new models of planned urbanization. …\n- Have you read?\nWhat are the effects caused by climate change on humans?\nClimate change increases the risk of illness through increasing temperature, more frequent heavy rains and runoff, and the effects of storms. Health impacts may include gastrointestinal illness like diarrhea, effects on the body’s nervous and respiratory systems, or liver and kidney damage.\nWhat are the causes and effects of climate change?\nHumans are increasingly influencing the climate and the earth’s temperature by burning fossil fuels, cutting down forests and farming livestock. This adds enormous amounts of greenhouse gases to those naturally occurring in the atmosphere, increasing the greenhouse effect and global warming.']	['<urn:uuid:5559e013-0b8e-40b4-a4c5-29193a0904fc>', '<urn:uuid:283cd713-1eac-409b-bd38-a5811650cc5d>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	9	56	1528
30	gem lake camping spots elevation access	Gem Lake is located at 4922ft elevation and has several campsites, though the area shows signs of heavy use with multiple small trails. To reach it, follow trail #1012 from Snow Lake junction at 3.32 miles. The lake is small and compact, sparkling in the midday sun, surrounded by dramatic peaks reminiscent of the Alps. The camping area is accessible via well-worn paths that lead to views of surrounding peaks and valleys.	['Get full access to Outside Learn, our online education hub featuring in-depth fitness, nutrition, and adventure courses and more than 2,000 instructional videos when you sign up for Outside+ Sign up for Outside+ today.\nStart at the Alpental ski resort parking lot, and you get a sense of what’s in store for you, as the surrounding peaks here rocket skyward, giving one the sense that you’ve left the state of Washington for more distant, famous peaks, perhaps even the namesake of the local ski resort, the Alps. The mountains here are more vertical, but don’t let that fool you, for the elevation gain here is moderate. After filling out your backcountry permit at the trail head for Snow Lake #1013, start the gentle climb up to Snow Lake, along a shaded forest trail with occasional openings across large talus fields of granite. These openings give you wide open views of the surrounding peaks along the route. At 1.7 miles in, you reach your first trail junction, at the Source Lake trail. Stay right here, and begin your first uphill switchback to reach your first saddle before dropping down into Snow Lake. It’s appx. .6 miles up, and a little over 530ft of elevation gain, across steep rocky terrain, mostly in the open, so during hot summer days, you’ll want to tackle this in the morning. Once at the top, you get your first views of this side of the area, and Snow Lake below, is beautiful! Deep turquoise waters surrounded by granite cliffs and small meadows full of alpine firs. This is all visible, with distant views of other mountain tops. Drop down into the Snow Lake basin at 2.8 miles in, and pass the campsite trail which veers off to your left. If you desire, you can continue on the campsite trail as it will re-join the main trail on the other side, so you can see the different sites available. Just past the campsites, you’ll get an awesome opportunity to take a quick break, and view the lake from the best viewpoint, a rocky bluff overlooking all there is to see here, including hulking Chair Peak, that keeps a watchful eye on the lake below, also providing the necessary shade to keep the namesake snow fields from melting out. Stow your camera, and continue along lakeshore to your next junction, at 3.32 miles, and turn left and follow the signage for Gem Lake, trail #1012, your next lake destination. The trail continues upward from here, and you will remain in the open, following the stair-stepped little meadows up to Gem Lake. There is no shortage of talus fields on this hike, and you will cross many large ones on the way to the lake. Once at Gem lake, you’ve reached your highpoint, at 4922ft. There are several campsites here, and you can tell that this place is heavily used, little trails running everywhere, but it’s not surprise. The views here are spectacular, and the lake is a destination all to itself. Small and compact, it sparkles in the midday sun, vanquishing any doubts as to why it was so aptly named. The peaks here are dramatic, like spear points piercing the sky, ominous and inspiring all at the same time. They remind you of the famous pictures we’ve all seen of the Alps, the only thing they’re missing is the snow caps on top. You’ll want to take some extra time to follow some of the well worn paths here that lead you to big views of the surrounding peaks and valleys. I waited until my return trip to wander around the banks here, for I wanted to make sure that I would be able to secure a spot at the upper lakes. The trail rounds Gem Lake here, and then once on the other side of the lake, drops down across a steep mountain side. Again, it’s fairly open here, broken up by sparse old growth, and the valley and mountains here will cause you to pause, and marvel at the ruggedness and beauty that is spread out before you. From this vantage point at the beginning of the downhill, you can see distant Mt. Roosevelt, and know that at its base, is where your destination for the day is at. The hillside is steep, but the switchbacks do a good job of getting you down with minimal effort, it’s about 713ft loss in ¾ of a mile. At the bottom, you’ll cross a meadow, then a small stream at the beginning of another large talus field. After crossing the talus field, the trail skirts lower Wildcat Lake, and officially ends at the lake shore, where you’ll find one lone campground at 6.62 miles. At the last stream crossing, you can see where people have camped as well, where the ground is flat and cleared out. Here around the lake there is an abundance of Huckleberry brush, and tree cover. The lake here, after all you’ve seen so far, is a bit of a disappointment, but don’t despair! Just to the right of the campsite here, you can see a faint trail after crossing the outlet stream that runs uphill to Upper Wildcat Lake. It’s only about another 15 minutes on a faint path through overgrown Huckleberry brush to the lake, and it’s well worth the effort. This trail is not on any map, but can be fairly easily followed. Once you get to Upper Wildcat, there are at least 3 campsites that are good for 3 person tents. The first, and most obvious, is right at the water’s edge as you reach the lake, and is by far the best site, large enough for at least two tents. Continue to follow the faint trail on the right bank, and you’ll see a site about 50 feet up in the forest, nice and flat, and another below it, next to the water’s edge, that has a nice accessible sandy beach. The water is cold, but inviting, especially on a hot day. This too, is a beautiful lake, filled with trout, and on the opposite side of the lake is Mount Roosevelt, providing a dramatic backdrop to the lake. It also has a small tree covered island in the lake, and everywhere there is a foothold in the steep granite here, you will see small alpine trees eking out a living. Another benefit to the way this lake is set up, is the trees that line this eastern shore, seem to provide good shelter from the wind that blew the entire time I was here. You could hear it pushing it’s way through the upper ring of trees and see it ripple out in the middle of the lake, but never felt it’s effects here, only a slight breeze. Although not that many people come to Upper Wildcat to stay, you definitely want to get here early, so you can claim your spot. Snow Lake is a heavily used lake, mostly by day hikers, as it’s a beautiful destination close in. Once viewing it yourself, you will be able to see why so many make the trek here.\n- Distance: 23.0\nLocation: 47.444923, -121.423561\nPark here at large parking lot at end of road, to your left.\nSnow Lake trl#1013\nLocation: 47.445518, -121.423325\nStart of trail, here you will fill out a backcountry permit.\nSource Lake Overlook\nLocation: 47.458183, -121.447853\nTrail junction here. Stay Left, and follow the trail uphill.\nSnow Lake Saddle\nLocation: 47.460144, -121.446968\nTop of the saddle, where you drop down into Snow Lake Basin\nRocky point View\nLocation: 47.466076, -121.44854\nA great place to stop and take pictures of the lake. Best view spot.\nGem Lake Junction\nLocation: 47.468513, -121.448067\nStay left here, follow signage to Gem Lake, and trail #1012\nLocation: 47.475998, -121.464546\nLocation: 47.477345, -121.468735\nSwitchback begins here, descending to lower Wildcat lake.\nLocation: 47.479244, -121.473747\nSwitchback ends here in small meadow.\nLocation: 47.47821, -121.476112\nA stream crossing here with small waterfall, and begin crossing large talus field.\nLocation: 47.484825, -121.482109\nOnly one small campsite here, at end of trail. look for trail continuation to your left\nLocation: 47.486759, -121.487442\nLocation: 47.487476, -121.487709\nLocation: 47.474342, -121.465767\nA worthwhile side trip, to see the valley and mountains here. Also, can look down upon Snow Lake from here.\nLocation: 47.444575, -121.423516\nParking lot at Alpental Ski Area.\nLocation: 47.445562, -121.423087\nTrailhead sign for Snow Lake Trail #1013\nLocation: 47.452411, -121.434546\nLooking west towards Bryant Peak, and Chair Peak. Moon is still visible over the mountains.\nLocation: 47.460594, -121.446133\nAs seen from the saddle, before dropping into Snow Lake Basin.\nLocation: 47.465816, -121.447849\nFrom the view point, looking at NW end of lake. Can still see the snow fields that feed the lake, in mid September\nLocation: 47.467151, -121.447935\nMore views from the banks of Snow Lake\nLocation: 47.470908, -121.455274\nViews from the trail across Snow Lake to Kaleetan Peak.\nLocation: 47.474418, -121.465917\nThe shark fin is Kaleetan Peak, as seen from Gem Lake viewpoint\nSnow Lake birds eye\nLocation: 47.474331, -121.465487\nFrom the viewpoint at Gem Lake, looking down into Snow Lake\nLocation: 47.477841, -121.464629\nGem lake and the surrounding peaks.\nLocation: 47.479233, -121.470315\nJust some of the views available on the steep switchbacks coming out of the valley\nLocation: 47.479349, -121.47716\nJagged peaks that supply ample talus to the valley below.\nLocation: 47.480233, -121.47819\nJust a small pond along the way to Lower Wildcat, that acts as a mirror.\nLower Wildcat Lake\nLocation: 47.484743, -121.48201\nView of the lake, as seen from the single campsite at end of trail.\nUpper Wildcat Lake\nLocation: 47.487339, -121.487653\nMorning provides the best light for reflecting Mt. Roosevelt off the surface of the lake.']	['<urn:uuid:3d663dad-256f-4b88-b67e-120e0228bfd1>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	6	72	1617
31	What are the main differences in how carbon monoxide and radon gas enter and accumulate in residential buildings?	Carbon monoxide and radon enter homes through different sources. Carbon monoxide primarily comes from indoor appliances and equipment within the home and office, including cars and motor vehicles. In contrast, radon gas naturally seeps into homes and structures from the rock and soil beneath the foundation, affecting both new construction and older buildings. While carbon monoxide accumulation is typically linked to malfunctioning equipment or poor ventilation, radon's presence is determined by the natural composition of the earth's rock and soil in the area, and it exists in every home and structure, with approximately 1 in 3 homes having unsafe levels in certain regions.	['Carbon Monoxide Information:\nCarbon monoxide is a potentially deadly gas, which has earned the name of ‘the silent killer’. This gas is colourless, odourless and tasteless, and as such is extremely difficult to detect. However, there are many sources of carbon monoxide pollution, and everyday items within the home and office can pose a potential risk. Even your car or motor vehicle poses the risk of possible carbon monoxide poisoning.\nThe effects of carbon monoxide poisoning can be devastating, and this gas can and dies kills thousands of people each year. Some people don’t even realise that they have been poisoned, simply slipping away in to unconsciousness or a deep sleep from which they will never reawaken. For those that don’t die, carbon monoxide poisoning can still cause long term damage and permanent damage and disability.\nIn order to try and minimise the risks of carbon monoxide poisoning to you and your family, it is essential to know the causes, the treatments and how to recognise the symptoms of carbon monoxide poisoning. This site will enable you to find out more about this deadly gas, and how you can help to minimise the risks in your home, car and office.\nThe dangers of carbon monoxide provides information on the various dangers associated with this gas, from the difficulty in being able to detect its presence, to its ability to cause serious damage and death.\nWhat is carbon monoxide poisoning? goes into detail on what carbon monoxide is and how it can affect our health and out lives.\nSymptoms of carbon monoxide poisoning details the various symptoms that are associated with this type of poisoning, as well as highlighting their non-specific nature.\nSources of carbon monoxide poisoning goes in to the various appliances and sources that can pose a risk of CO poisoning, both in the home and at work.\nPrevention of carbon monoxide poisoning describes various steps and precautions that you can take in order to minimise on the risks of carbon monoxidepoisoning to yourself and your family.\nLong term effects of carbon monoxide poisoning lists the possible long term damage that can be caused by breathing in this deadly gas, and why these effects may come about.\nTreatments for carbon monoxide poisoning explains the various treatments that are available, both in terms of the poisoning of the blood as well as the related illnesses that stem from CO poisoning.\nThe chemistry and science behind carbon monoxide poisoning explains more about how and why breathing in carbon monoxide can affect us so badly, and what sort of symptoms can be expected with various levels of saturation.\nCarbon monoxide in your car details the history behind carbon monoxide emission from motor vehicles and highlights the importance of getting your vehicle checked. You can also find out more about how CO (carbon monoxide) fumes from your car can also affect your home.\nCarbon monoxide in your home provides details on the types of appliances within the home that may pose a CO risk, as well as highlighting ways to make your home as carbon monoxide-proof as possible.\nHow to detect a carbon monoxide leak explains the difficulties of detecting carbon monoxide, and how to look for clues. You can also find out more about CO detectors and how they can help.\nWhat to do if you suspect a carbon monoxide leak details the action you should take if you do suspect a leak, and the importance of acting quickly in order to minimise on CO poisoning.\nThe articles on this site should enable you to get a more detailed insight into what carbon monoxide is, what sort of dangers it poses and how we can all help to minimise on the risks that could cause serious damage or death.', 'Be Informed About Radon Facts.\nDo you know the facts about radon? Radon is a naturally-occurring gas that exists in every home and structure. Radon reaches unsafe levels in an average of 1 in 3 homes in the Kansas City region, across both Kansas and Missouri. The EPA deems levels at or above 4 pCi/L as a health risk. We believe in educating our customers and the community about radon facts so they can make informed decisions to protect their families.\nWhat is Radon?\nRadon is a radioactive gas created when uranium in the earth’s rock, soil or water decays. Indoors, it is not naturally occurring, but a result of the way our homes, schools, and workplaces are designed, constructed, and maintained. Radon is one of the most significant environmental health risks that exists today.\nRadon is cancer-causing radioactive gas that has no smell, is invisible to the naked eye and has no taste. It is considered one of the most life-threatening forms since it cannot be detected without proper testing. The Surgeon General and American Lung Association warn that radon gas has been proven the second-leading cause of lung cancer in the United States.\nIf you smoke and your home has high radon levels, you’re at higher risk for developing lung cancer. Some scientific studies of radon exposure indicate that children may be more sensitive to radon. This may be due to their higher respiration rate and their rapidly dividing cells, which may be more vulnerable to radiation damage.\nThis Radon map was created to assist National, State, and local organizations to better target internal resources as well as to implement building codes to address radon-resistance. This map is not intended to be used to determine if a home in a given zone should be tested for radon. Homes with elevated levels of radon have been found in all three zones. All homes should be tested regardless of geographic location.\nFacts: Where Does Radon Come From?\nRadon gas seeps into homes and structures from rock and soil beneath the foundation. It can be found in new construction and older homes and buildings. Certain areas of the region and country can produce higher levels based on the composition of the earth’s rock and soil in that area. You cannot detect radon with smell, taste, or sight, but it exists in EVERY home and structure. The only way to determine if radon levels are above the EPA’s Action Level of 4 pCi/L is to conduct a radon test.\nWhy Is It Dangerous?\nRadon is a radioactive gas and Class A carcinogen. When exposed to high levels of radon, individuals are at risk for lung cancer, especially those who smoke, have lung disease or are children. The American Lung Association and U.S. Surgeon General classify radon exposure as the second-leading cause of lung cancer in the United States.\nWhen you breathe in radon gas, particles settle in your lung tissue and begin to decay. As the radon particles decay, they release bursts of energy that damage the lung tissue cells. Over time, cell damage can lead to the development of lung cancer. Scientists now estimate that between 15,000 – 22,000 deaths caused by lung cancer each year are related to radon in the United States.\nHow Do You Fix It?\nThe most cost-effective and successful method of remediation – or radon removal – is the active depressurization system (ADS). The ADS system draws air from under the slab or from under a radon polyethylene barrier (if there is no slab) and exhausts it to EPA’s standards through PVC pipe and a specially designed in-line fan. The exhaust of these systems can be installed on an outside wall or internally and exhausted through the roof. These systems are virtually maintenance-free. A visual indicator is provided, allowing the homeowner to monitor performance.']	['<urn:uuid:cceb92e6-cf28-434e-be3f-137e7f8ebedf>', '<urn:uuid:56c7ce7f-6ae3-4045-a89f-8fc6b26f089a>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T04:31:27.499155	18	103	1265
32	sustainability assessment tools implementation barriers electric vehicle corporate acceptance	Sustainability assessment tools face implementation barriers including lack of common understanding of sustainability concepts within companies and conflicting corporate goals that may prevent selecting the most sustainable options. Similarly with electric vehicles, while there are initial signs of corporate acceptance with Amazon ordering 100,000 electric delivery trucks and UPS ordering 10,000 electric trucks, widespread adoption remains challenged by infrastructure limitations, though companies are increasingly motivated by factors like avoiding anti-idling fines and pursuing carbon neutrality goals.	['2.1.1. Existing Tools and Decision Support Methodologies\nThere are many decision support tools and assessment methods available, and many have previously been described [5\n]. Thuvander et al.\n] evaluate the functionality of several tools and methods, and show that few of the tools consider all aspects of sustainability or can be accommodated to local conditions. Most of the tools available today focus on single aspects of sustainability or do not have a balanced, integrated approach to the evaluation of sustainability [5\n]. Further, many of the established tools do not address the various complexities surrounding, and integration of, technical, environmental, economic, architectural, cultural, and social values [5\n]. There are, however, tools which cover several aspects, but these have often been described as being too comprehensive or aggregated, lacking in transparency, or providing insufficient consideration of all necessary aspects [5\n]. Singh et al.\n] argue that tools and rating systems which evaluate sustainability are also, to some extent, subjective, in spite of the purported objectivity of the evaluation of each aspect [9\n]. Thuvander et al.\n] identify the need for a simplified decision support framework that focuses on the early stages of renovation projects.\nMethods are available with which to evaluate sustainability in the early stages of a renovation project, although these generally take slightly different approaches to those used by the Renobuild project. For example, a similar method uses an existing evaluation tool—Miljöbyggnad—which is an established tool used by property owners and developers—to assess environmental aspects [12\n] but has a different focus and does not directly address CO2\nand primary energy in explicit terms for materials included in the renovation, nor does it accommodate a life-cycle perspective [13\n]. As regards comparing different renovation alternatives for the same building, this can be seen as a drawback.\nSimilarly, a tool has been developed in Denmark based on the idea that there is a lack of simple and holistic tools available to help stakeholders in early stage decision-making [14\n]—an impetus which is shared by Renobuild [7\n]. As compared to Renobuild, however, this tool is more simplistic and builds on multiple stakeholders’ subjective views, rather than calculations and evaluations. Thus, while it provides quick results, its major drawback is its being based on the subjective, and thus often differing, views of various stakeholders.\nThe ECBS Retrofit Advisor is another tool to help decision-makers in the process of selecting an appropriate level of building retrofit [15\n]; currently, however, it is restricted to predefined countries and fixed building types.\nThe REBO model is a conceptual framework which aims to include central and often overlooked qualities in Swedish housing built between 1941 and 1960 [16\n]. Social, cultural, and architectural values are evaluated qualitatively, alongside more easily measured values such as technical, environmental, and economic.\nThe multi-variant design and multiple criteria analysis tools have been developed by Kaklauskas et al.\n]. They aim to evaluate many aspects of renovation, such as economic, technical, architectural, aesthetic, and comfort. Based on the needs, weights, and data of buildings, the system can compare up to 100,000 options so as to automatically find the best.\nPoel et al.\n] have developed a tool which analyses the energy performance of buildings. It can provide building owners with recommendations for cost-effective measures which can improve energy performance. Calculations are made independent of local context, but the interface in the tool must be localised so as to accommodate differences in weather files, construction libraries, etc\nJuan et al.\n] have developed a hybrid decision support system for the sustainable renovation of office buildings. It is an integrated approach with which to assess the current condition of buildings, and to suggest sustainable renovation actions based on cost, quality, and environmental impact. The system can analyse trade-offs between preferred budget and expected improvement, and compare energy performance for different scenarios.\nTo summarise, many decision support systems and methods have been developed, and some of these efforts incorporate several aspects of sustainability in an automated process. Too much automation, however, may be a risk if the user is not privy to the logic behind how the tools prioritise and suggest solutions. Further, there is also a risk that these tools are not able to consider all of the specific conditions of a renovation project, which could lead to decisions being made based on incomplete information. These risks need to be considered and borne in mind by the users of decision support systems.\nMost tools focus on energy, cost, and the technical aspects of renovation; however, some also consider the social aspects. It is our understanding that social aspects are at present gaining more attention from the perspective of tenants and society, and thus must be an integral part of a decision support system. Based on the above, there is a need to develop easy-to-use, effective tools which can provide results relatively quickly based on actual data and take into account a life-cycle perspective. There is a need for tools which compare all aspects of sustainability equally, which has been a shortcoming in many of the tools that have been surveyed. This is the foundation on which the Renobuild methodology and tools have been developed. Since then, several tools, both competing and complementary, have been developed with a similar scope; none of them, however, have presented quantifiable indicators for environmental, economic and social aspects.\n2.1.2. The Decision Process for Swedish Property Owners\nThe decision process can vary between companies, over time, and even between property managers within the same company [6\n]. Häkkinen and Belloni [18\n] state that the implementation of sustainable building can be hindered by a lack of a common understanding of the concept of sustainability. The implementation of tools for assessing sustainability within companies could help to establish a common definition and process, as this process requires an explicit definition of what to evaluate.\nWithin companies, there can be entirely logical reasons behind a decision to not select the most sustainable renovation alternative proposed by the used methodology or tool, as a result of the fact that companies often have conflicting goals. Our understanding is that it is important for both users and those developing methodologies and tools to understand that there is some degree of subjectivity in the results, as argued by Singh et al.\n]. Consequently, and in spite of the input of decision support tools, conflicting opinions can often be legitimate. Therefore, it is particularly important to have a structured and transparent decision-making process, in which alternatives can be evaluated and compared based on several perspectives, including those which focus on economic, environmental, and social consequences, all of which are in line with company policy, while maintaining sufficient openness within the discussion as to allow for the potential challenging and even improvement on current policy.\nAn investigation of Swedish property owners showed several shortcomings regarding renovation processes [6\n]; for example, that LCC approaches are seldom used, and that there is a lack of guidelines for data input as relates to economic evaluation, modest sustainability targets, and limited routines for managing sustainability aspects in projects. The shortcomings identified further show the need for structured approaches which include a focus on sustainability aspects in renovation processes.\nThe need of building owners to quickly remedy technical problems within their buildings can lead to the selection of immediate solutions instead of long-term and sustainable ones, particularly if no rigid decision processes are in place and/or these decisions are made at the discretion of an individual project manager. Additionally, the fact that improving the energy efficiency of a building is primarily performed in conjunction with other renovation measures [19\n], further demonstrates the need for methodologies or tools for the evaluation of total sustainability so as to provide an overview of different alternatives, rather than just those which can remedy urgent problems.\nBased on the above reasoning, the introduction of a decision support tool for the evaluation of renovation alternatives could act as a mechanism for ensuring that the solutions chosen for renovation are sustainable in the long term.', 'Electric vehicles (EVs) hold a lot of promise for the private sector — especially as consumers, who are increasingly aware of the relationship between emissions and climate change, are starting to demand eco-friendly delivery options. EV adoption, however, has been slowed down by a few different challenges — the US’s poor EV charging infrastructure in particular.\nNow, however, we’re beginning to see signs that major businesses are willing to buy into EVs, despite potential road bumps.\nHere are the businesses that are leading the way when it comes to EV adoption.\nAmazon and UPS Lead Way on EV Adoption\nTwo delivery giants — Amazon and UPS — have begun to aggressively add EVs to their delivery fleets.\nEarlier this year in January, Amazon ordered 100,000 electric delivery trucks from EV manufacturer Rivian, as well as 10,000 electric delivery rickshaws for their operations in India. Then, around the end of the month, UPS announced that it had ordered 10,000 electric trucks from the UK-based manufacturer Arrival Ltd., and would soon be teaming up with self-driving car manufacturer Waymo for a pilot test of self-driving delivery vehicles.\nThe moves are part of broader pushes towards carbon neutrality and self-driving delivery by the two companies. Last year, Amazon announced the company’s plan to be 100 percent carbon-neutral by the year 2040. UPS already offers carbon-neutral and carbon-offset delivery options.\nThe moves also come as more cities around the U.S., including New York and Philadelphia., have begun to adopt anti-idling laws that allow the city to fine companies over idling delivery vehicles.\nSome cities have even developed apps that allow citizens to report idling vehicles based on that vehicle’s DOT number — making these policies even more costly for delivery companies. Because electric vehicles produce no emissions, they’re typically free from being fined — meaning savings for businesses that adopt EVs for city deliveries.\nThe announcements are both historic. While other companies have announced EV purchases — like Lyft, which plans to deploy 200 EVs in Denver as part of its rental vehicle program there — there’s been nothing near scale of these announced by Amazon and UPS.\nWhile neither UPS nor Amazon has plans to go fully electric any time soon, the purchases are a welcome sign for the EV industry. Coupled with similar positive signals from the individual consumer side of the industry, they likely demonstrate that despite early growing pains, EVs may be on track for widespread adoption in the near future.\nChallenges Facing Further EV Adoption\nHowever, there still remain significant barriers that may slow or prevent full EV adoption, primarily the weak EV charging infrastructure in the US and limited number of charging stations — although this, too, seems like it’s starting to change.\nChargePoint, in coalition with the National Association of Truck Stop Operators (NATSO) has formed the National Highway Charging Collaborative, which plans to install new charging stations at more than 4,000 highway-side locations in the U.S., in order to increase the availability of EV charging stations in rural areas.\nAt the same time, legislative support for stronger EV infrastructure is beginning to build. In February, Democratic lawmakers in the House of Representatives announced a new bill that would create a nationwide EV charging network within the next five years.\nUpgrades to existing infrastructure would likely encourage further adoption. They may also be especially beneficial for businesses like Amazon and UPS, as both companies regularly make deliveries to rural parts of the country — areas that don’t always have the charging infrastructure needed to support EVs.\nThe Future for EVs in Business\nEV adoption in the private sector, which has lagged in the past, seems to be accelerating. Two major delivery companies have now announced that they will be adding significant numbers of EVs to their delivery fleets, with more likely to come in the near future as both pursue low-carbon delivery options.\nWhile challenges remain that may slow down EV adoption — primarily the nation’s weak EV charging infrastructure — the purchases are likely a good sign for the industry and the future of EVs in the private sector.']	['<urn:uuid:eac82333-f4aa-46d0-83c6-78e7e69b5fb1>', '<urn:uuid:a3b2914f-bfcd-4107-a76d-db9ca5d880cd>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:31:27.499155	9	76	2026
33	earliest galaxies discovery gravitational lensing hubble webb telescope differences tools	Gravitational lensing helps discover early galaxies by bending light from distant objects around massive foreground galaxies. The Hubble Space Telescope uses this technique to study galaxies formed shortly after the Big Bang, while the upcoming James Webb Space Telescope will be able to look even further back in time to observe the very first stars and galaxies, collecting more light than Hubble.	"['Gravitational lensing is a phenomenon by which a body containing a large mass (stars, for example) bends the space around it, thereby bending the path of light as it travels.\nSome physical objects are simple, but have profound applications. Take lenses, for example. The glasses your grandma wears to recognize a stranger is a lens. The microscope used by science geeks like us is a lens. The magnifying glass you use to read tiny text is a lens.\nIf you casually observe a lens, you might say that a lens distorts the view of things behind it (sometimes for the better). However, there are more complex applications of a lens, as well as a profound cosmological significance. There is a type of lens that is helping us explore new galaxies and see what is otherwise unseeable. That lens is a gravitational lens, powered by gravity!\nWhat is gravitational lensing?\nAlthough our understanding of gravity is limited, we can certainly say that it’s a pretty amazing force. It is good not just for pulling the poor Coyote from Looney Tunes abruptly off a cliff, but also for keeping our own feet on the ground. Gravity is also the force that binds together the planets of our solar system. Interestingly, the incredible force of gravity also has an eccentric bag of tricks up in its sleeves, and one of those tricks is its ability to act as a lens.\nGravitational lensing is a phenomenon by which a body containing a large mass (such as stars) bends the space around it and thereby bends the path of light as it travels. It might sound like a surreal idea, but it has fascinated scholars for years, and is extremely helpful when we look out into the vast cosmos.\nGeneral relativity and gravitational lensing\nIn 1915, Einstein put forth his new theory of gravity called general relativity. Einstein suggested that matter distorts the fabric of space and time around it. That implies that gravity bends light waves, i.e., making their paths curve around massive objects like stars or black holes.\nWith this theory, Einstein gave us a whole new perspective of looking into space. General relativity treats space as a malleable entity, wherein the “shape of space” can itself be distorted. Going by this theory, celestial objects don’t really orbit around other celestial objects—because they would eventually merge as a result of attraction. Instead, these celestial entities travel in a straight path through curved space. What this implies is that the orbit of Earth around the Sun is a straight line, when observed from the Earth. This is a mind-bending phenomenon to imagine, but let’s try to picture it more clearly with an example.\nImagine someone walking in a straight line on the surface of the Earth. For simplicity’s sake, let’s assume he can walk over anything on the surface (including liquid, like water). In principle, if this person manages to walk long enough, he would eventually come back to the same point from where he started. By walking in a “straight line” on the surface of a sphere (Earth), a person is able to trace a CIRCULAR PATH!\nBasically, that’s the whole idea of relativity! Now, let’s come back to light and how gravity is able to bend it.\nBending of light and its aftermath\nLight normally travels in a straight line, but if you see light traveling in a curved path in outer space, using principles of general relativity, you are witnessing the bending of space and thereby light.\nTo grasp how gravity acts as a lens by bending the path of light, you need to understand a few things.\nThe sources of light in our universe are primarily stars. These stars emit light in all directions, but we can only see light as a point, because the light must enter your eyes through a narrow opening (which can be considered a ‘point’) for you to see it. Also, only a small amount of emitted light goes through the eyes. Thus, the light you SEE follows a very narrow path.\nAccording to Einstein’s theory of relativity, a massive body will distort the space around it, thereby bending the light that passes through this distorted space. Let’s try to understand how this works with an example. Imagine that you’re facing two stars that are in your line of sight, with one behind the other:\nThe corner star (the star farthest from you, shown in yellow in the image below) emits light that is much brighter than the light emitted by the middle star (shown in blue in the image below). This middle star would bend the light coming from the more distant star, such that you would get to see the distant star—although the view would be distorted—as something like seeing blurry two stars.\nSolar eclipse and the manifestation of gravitational lensing\nDuring the solar eclipse of 1919, physicists Arthur Eddington and Frank Watson Dyson confirmed to have observed gravitational lensing (and this was likely the first confirmation of gravitational lensing). Stars behind the Sun were observed to be shifted from their fixed positions. In reality, they weren’t really shifted, but the light coming from the stars was bent by the gravitational influence of the Sun!\nSince then, scientists have tried to leverage the power of gravitational lensing to peer deeper into the unexplored corners of our Universe. In fact, by harnessing the power of gravitational lensing, astronomers have even been able to ascertain the distant yet pristine galaxies formed just a few million years after the Big Bang. This method of discovering distant stars and galaxies was later termed gravitational microlensing. In gravitational microlensing, stars or even galaxies in the foreground act as a lens for stars or galaxies in the background. These foreground stars/galaxies will facilitate one’s view of background stars/galaxies, if they emit enough light. While their appearance and location would be distorted, the presence of a celestial entity behind a given star/galaxy can be confirmed.\nEinstein cross and Einstein rings\nIn 1985, using gravitational lensing, astronomers were able to see this highly intriguing effect for the first time, which was later called the Einstein cross. In an Einstein cross, four images of the same object (galaxy or galaxy cluster) are arranged perpendicular to each other when viewed from the lens. The lensing effect, in this case, was generated by the galaxy called ZW 2237+030.\nSimilarly, in 1988, scientists first observed another beautiful phenomenon called Einstein rings. This was the effect arising from gravitational lensing wherein a distant galaxy appeared to be warped by a much closer galaxy inside a circle like a halo.\nFinally, for those interested in studying the bizarre things of the Universe, gravitational lensing can be a powerful tool. Besides observing beautiful phenomena like an Einstein cross and Einstein rings, gravitational lensing can also be used to detect and study the mysterious material in the Universe called dark matter. It would take a complete article to explain what dark matter actually is, but to put it simply, it’s an invisible form of matter that has a mass. Unlike ordinary mass that we can see, dark matter doesn’t emit heat, light, or any other form of electromagnetic radiation. This quality is what makes it so stealthy. It was only when techniques like gravitational lensing were discovered that we have been able to learn more about the elusive and invisible presence.\nThe key thing about dark matter is that it has a mass, so this dark matter, like any other massive celestial body, bends light in its vicinity. This has helped astronomers in confirming the presence of dark matter. Interestingly, it is speculated that in total, dark matter is around five times more massive than all the ordinary matter of the universe put together! Studies like the Dark Energy Survey (DES) is ongoing and seeks to confirm these figures by studying dark matter using gravitational lensing. This study may be able to provide a precise map of matter in the universe, including the mysterious dark matter.\nIn closing, Einstein’s work on general relativity has paved the way for countless astronomers, allowing them to study the very structure of the Universe—both the visible and the invisible. Gravitational lensing, a simple but powerful tool, is helping us achieve that goal!', 'with guest Dr. Rogier Windhorst, Arizona State University\nThe first stars and galaxies began forming several hundred million years after the Big Bang. Astronomers are now able to probe back almost that far – to study the light from these distant objects. Among their major tools of exploration are the images and data in large surveys taken using the Hubble Space Telescope. The upcoming James Webb Space Telescope will be able to look back even farther in time to see the births of those first stars and the creation of the earliest galaxies. In this episode, we explore the galaxies in the early universe and find out how astronomers know what they know about these very distant objects.\nListen (mp3, 12.2 MB)\nDownload Transcript (pdf)\nProduced by Loch Ness Productions for the Astronomical Society of the Pacific\nWritten and narrated by Carolyn Collins Petersen\nSoundtrack and original music by Mark C. Petersen\nAdditional resource materials by Andrew Fraknoi\nSpecial thanks to Dr. Rogier Windhorst, Scott Smas, and Keith Jennings.\nFraknoi (Foothill College & ASP)\nThis episode of Astronomy Behind the Headlines is indeed ripped from recent headlines, when astronomers used the newly refurbished Hubble Space Telescope to see deeper into space (and thus farther back in time) than had ever been possible. What made an ever deeper look possible than even the Hubble telescope alone permits was that several galaxies in the foreground probably acted like a giant lens, amplifying the light of really distant objects and making outrageously faint galaxies discernible. When the next space telescope (the James Webb Space Telescope, which will collect far more light than the Hubble) goes into operation, we expect to be able to peer even deeper into the early moments of star and galaxy formation.\nIn this resource guide, you can learn more about the search for the first generation of galaxies (following the Big Bang and the so-called ""Dark Ages"" of the universe) and get introduced to how astronomers are already using gravitational lenses (first predicted by Einstein) and how they hope to use the Webb Telescope to use them to even greater effect.\nA. Introductory Resources on the First Galaxies, Gravitational Lenses, and the Next Big Space Telescope\nThe Hubble Space Telescope news releases that go with this episode are at:\nTalcott, R. ""Galaxies Near the Dawn of Time"" in Astronomy, May 2010, p. 56. Describes the work of two teams, including Rogier Windhorst’s, using the Hubble to search for very distant galaxies.\nVillard, R. ""In Search of the First Stars"" in Astronomy, June 2011, p. 26. Discussion of the how the stars in the earliest galaxies formed and how the JWST will help us learn more about them.\nGardner, J. ""Find the First Galaxies"" in Sky & Telescope, Jan. 2010, p. 24. On Hubble observations and what will be possible with JWST.\nLarson, R. & Bromm, V. ""The First Stars in the Universe"" in Scientific American, Dec. 2001, p. 64. On the ""dark ages"" after the big bang and before stars formed, and how these ages ended.\n[For the more advanced reader] Loeb, Abraham How Did the First Stars and Galaxies Form? 2010, Princeton University Press. Requires background in physics and math.\nDistant Galaxies Seen by the Hubble: There is a green circle around some of the really faint galaxies on this deep Hubble image that are close to some brighter galaxies in the foreground. Rogier Windhorst\'s team estimates that about 1/5 of these galaxies are probably brightened by the gravitational lensing effect. (NASA/ESA)\nThe First Galaxies web site (by some of the researchers involved):\nThe James Webb Space Telescope page on finding the first stars and galaxies:\nPetersen, C. ""The Universe through Gravity’s Lens"" in Sky & Telescope, Sept. 2001, p. 32. Good introduction with nice diagrams and photos.\nWambsganss, J. ""Gravity’s Kaleidoscope"" in Scientific American, Nov. 2001, p. 65. On gravitational lenses and micro-lensing.\nGates, Evalyn Einstein’s Telescope: The Hunt for Dark Matter and Dark Energy in the Universe. 2009, Norton. A book that focuses on some of the ways gravitational lensing is being used to explore the large-scale properties of the universe.\nOverbye, D. ""Lenses in the Sky"" in Discover Magazine, May 1984, p. 30. An early overview.\nHow Gravitational Lensing Works: Einstein\'s General Theory of Relativity predicts that, under the right circumstances, the gravity of a foreground galaxy can bend and focus light from a distant object whose light passes nearby. Thus the more distant galaxy can appear brighter than it otherwise would and become unexpectedly noticeable on a deep photograph taken with a large telescope. (NASA)\nA Brief History of Gravitational Lenses at Einstein On Line:\nUniversity of Tennessee Tutorial:\nHubble Space Telescope Archive of News Releases about Gravitational Lenses:\nESA Hubble Site Brief Introduction:\nReddy, F. ""The Next Great Space Telescope Takes Shape"" in Astronomy, Sep. 2010, p. 24. On the construction of JWST and on its capabilities.\nThe Webb Telescope Site at the Space Telescope Science Institute:\nNASA’s JSWT Pages:\nJWST Pages at the European Space Agency:\nSpace News story on delays and cost overruns in preparing JWST:\nTwo lensing demonstrations to help students understand gravitational lenses, from ""Einstein’s Universe"" are at:\nAnother version of the lensing demonstration can be seen at:\nA Tiny, Youthful Spiral Galaxy, ESO 418-008. (NASA, Rogier Windhorst (Arizona State University, Tempe, AZ), and the Hubble mid-UV team)\nTo learn more about galaxies in general, try the Galaxy Sorting activity at the Seeing in the Dark PBS special web site (scroll down to see the activities):\nFor more on telescopes in general, and the Hubble Space Telescope (the precursor to JWST) specifically, see the ""Telescopes from the Ground Up"" web exploration at:']"	['<urn:uuid:25eaa060-3236-44a2-8cac-2970a019b7d7>', '<urn:uuid:ae4f7e49-75e9-49c5-9118-ee011f143e04>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:31:27.499155	10	62	2322
34	house loan vs auto loan collateral	While both mortgage (house loan) and hypothecation (vehicle loan) allow the borrower to keep possession of the asset, they differ in the type of asset involved. Mortgages are typically used for non-movable assets like houses, while hypothecation is used for movable assets like vehicles.	['Hypothecation means offering an asset as collateral security to the lender. The ownership lies with a lender, and the borrower enjoys the possession. In the case of default by the borrower, the lender can exercise his ownership rights to seize the asset.\nIt is usually done in movable assets to create the charge against collateral for the loan given. Under hypothecation, the possession of the security remains with the borrower itself. Hence, if the borrower defaults on payments, the lender would have to first take possession of the security (asset under hypothecation). And then sell the asset to recover dues. Sometimes a bank or financial institution puts the already pledged asset as collateral for borrowing from another bank is called rehypothecation.\nExample of Hypothecation\nIn the case of vehicle loans, the vehicle remains with the borrower, but the same is hypothecated to the bank/ financer. If the borrower defaults, the bank takes possession of the vehicle after giving notice and then sells the same. The loan account is credited with the sales proceeds of the asset to recover the dues towards the principal amount and interest amount. Any balance left after that shall be given back to the borrower. Apart from vehicles, It can be done for stocks and bills receivables.\nThough pledge seems similar to hypothecation as both are types of charges created on movable assets. There lie some differences between pledge, hypothecation, and mortgage. Let us look at the differences to understand these terms better.\nPledge V/s Hypothecation\nThe possession of the asset remains with the lender in case of a pledge, while it remains with the borrower in case of hypothecation. Common examples include the gold loan in case of pledge and vehicle loan in case of hypothecation. Read a detailed article on Pledge vs Hypothecation vs Lien vs Mortgage vs Assignment.\nMortgage V/s Hypothecation\nThe possession remains with the borrower in both these cases; however, mortgages are usually for non-movable assets while hypothecation is for movable assets. Common examples include the home loan in case of mortgage and vehicle loan in case of hypothecation. For more about differences: Mortgage V/s Hypothecation.\nAlternate Asset for Hypothecation\nIt can also be done for investments/stocks. This is a common practice in stock trading, better known as margin lending. In such a case, the buyer, buying shares on margin, places his existing shares as collateral with the brokerage firm. The brokerage firm can sell these shares if the buyer faces a margin call. A margin call is received when the value of securities bought decreases more than a certain limit or the account value reduces beyond a certain limit.\nThis activity usually requires an agreement to be made and is known as the hypothecation deed.\nThe hypothecation deed is an agreement that contains standard features and rules; which usually cover the following points: Definitions, Insurance , Inspection rules , rights and remedies of each party , security details marked for hypothecation, sale realizations, insurance proceeds, the liability of each party, jurisdiction prevailing, marking of the assets, etc. This deed protects the rights of both parties to the contract.\nTo learn more, you can visit our Advantages & Disadvantages of Hypothecation.\nHypothecation is a way by which borrowers can raise funds by providing security (movable) as collateral. And still get to use it since the possession remains with the borrower. The bank/financer gives this source of a loan at a rate lower than the unsecured loan as it provides a sense of security to the lender.\nThe lender runs a risk as there may be instances where the borrower sells off the hypothecated asset without the lender’s knowledge. However, periodic checks and proper clauses in this deed can provide protection to a large extent to both the borrower and the lender.\nQuiz on Hypothecation']	['<urn:uuid:20af125b-8228-4167-8195-04c620225bdb>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	6	44	633
35	what special features isla salas gomez island easternmost polynesian triangle pacific	Isla Salas y Gómez consists of two rocks of different sizes connected by a narrow 30-meter wide isthmus, with a total area of just 0.15 square kilometers. The island has cliffs along the shoreline making landing dangerous, and while it lacks permanent freshwater sources, it has a transient rainwater pool in a rock depression that supports seabirds. The Chilean Navy has installed a tsunami warning system on this uninhabited island, which has been declared a nature sanctuary.	"[""Insular Chile refers to a dispersed group of oceanic islands in the Chilean Sea. Created by volcanic activity, these islands are located relatively far from mainland Chile. The four islands that make up Insular Chile are listed below.\nJuan Fernández Islands\nThe Juan Fernández Islands are located in the South Pacific Ocean approximately 670 km from the coast of mainland Chile. The three main volcanic islands of this group are Robinson Crusoe, Santa Clara, and Alejandro Selkirk. Robinson Crusoe Island is the only inhabited island. Fishing and tourism are the two most important economic activities in the archipelago. Robinson Crusoe Island has earned fame due to Alexander Selkirk, a marooned sailor, who spent four years on the island in isolation from the rest of the world. The group of islands has been named after explorer Juan Fernandez who discovered the islands in the 1570s.\nThe Desventuradas Islands\nThe Desventuradas Islands is a group of four small islands located in the Pacific Ocean about 850 km off the coast of mainland Chile. Due to the remote location, no permanent human settlements have been established on the islands. There is, however, a Chilean Navy detachment stationed on one of the islands. There is little animal life on the Desventuradas Islands, and the only vertebrates include 10 species of migratory birds and one resident bird species. These islands also lack a permanent source of fresh water. Vegetation cover on the islands includes different types of trees and shrubs.\nEaster Island is one of the world’s most remote inhabited locations and has been declared a UNESCO World Heritage Site. Located in the southeastern Pacific Ocean, the island sits in the south-easternmost point of the Polynesian Triangle. The island was possibly inhabited by the Polynesian people between 700 and 1100 CE. Easter Island’s most notable feature is the group of 887 extant massive statues created by the indigenous Rapa Nui people. The cultural heritage of the island is protected within the limits of Rapa Nui National Park.\nIsla Salas y Gómez\nIsla Salas y Gómez is a small uninhabited island that forms the easternmost point of the Polynesian Triangle. The island is located in the Pacific Ocean approximately 3,220 km west of mainland Chile. The island and its surrounding coastal waters are part of a Chilean Marine Protected Area. The closest landmass to the island is Easter Island. Isla Salas y Gómez actually consists of two rocks of different sizes that are connected by a narrow isthmus about 30 m wide. The total area of the island is only 0.15 square km. Landing on the island is dangerous at most times due to the presence of cliffs along the shoreline. There are no permanent freshwater sources on the island. However, a transient rainwater pool is present in a rock depression that supports the island's seabird population. The island has been declared a nature sanctuary. A tsunami warning system has been installed on the island by the Chilean Navy.\nYour MLA Citation\nYour APA Citation\nYour Chicago Citation\nYour Harvard CitationRemember to italicize the title of this article in your Harvard citation.""]"	['<urn:uuid:0acbc7b6-b73b-4799-b2ab-3f509432deae>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	11	77	516
36	check engine light on dashboard meaning and nutrition requirements check for plant based diet	A yellow/orange Check Engine light indicates potential sensor or emissions problems if the car is running normally, but requires immediate attention if the car is sluggish or other gauges are abnormal. For plant-based diets, key nutrients to monitor include Vitamin B12 (from fortified foods), Vitamin D, Calcium, Iron, and Zinc, which can be obtained from various plant sources like legumes, fortified beverages, whole grains, and leafy greens.	['FREQUENTLY ASKED QUESTIONS\nMy car won’t start and won’t crank over. What do I do?\nIf your car won’t crank over, check the battery voltage by turning on the headlights. If the lights come on bright, have someone watch the headlights while turning the ignition key to the cranking position. If the lights dim or go out, your battery is low or dead. Also check your car’s battery cables. They might be loose or corroded.\nIf the headlights stay bright, you probably have a bad starter or starter solenoid. If your car has an aftermarket alarm system, you might have an alarm system problem.\nRemember if you try to use Jumper cables, Red is the positive terminal and Black is the negative terminal (Ground). Hook up both the Red terminals, then the Ground terminals, then start the car you are using to jump the broken car. Rev up the engine on the running car, this will give you extra power to start the broken car and protect the electrical of this car. Then have someone start the broken car. If possible, remember to wear glasses when working around batteries.\nMy Check Engine light is on! What does it mean?\nWhen the yellow or orange Check Engine (or Service Engine Soon) light on, the car’s computer is monitoring the fuel injection, smog system and some ignition systems. If the car is running normally, don’t worry too much. Look at the other gauges and warning lights. If they are normal, you probably have a sensor or emissions problem. Bring it to your repair shop at your convenience. You are probably using extra gas, but you will not get stuck with your car.\nIf your car is sluggish or missing and the Check Engine light is on and the rest of the gauges look normal, you need to drive to a shop as soon as possible. Your car’s ignition system or injection system maybe malfunctioning and you could get stuck!\nIf your Check Engine light is on and the other gauges are not normal, TURN OFF YOUR CAR! Tow your car in to prevent further damage and to avoid getting stuck on the way to a shop!\nHow do I keep my car from breaking down?\nMost people driving today’s cars do not understand how to repair their own car. Regular, routine maintenance from a high-tech auto repair shop will help avoid a breakdown. During routine maintenance, the shop can catch problems early before they become major repairs. Routine maintenance will also keep your car in better shape for a longer period of time.\nWe would like to see our customers every 3,000 miles for a Lube, Oil & Filter. At that service interval, we can get a good overall look at you car. Depending on your driving habits, rotate your tire every second or third service.\nEvery 15,000 miles change your air filter. Every 30,000 miles service the automatic transmission and replace fuel filters. 60,000 miles is usually the biggest service interval. Tune ups, cooling system and timing belts are all done at 60,000 miles. This service is expensive, so remember to budget for it in advance.\nWe can always estimate the price to you in advance.', 'More people are choosing to eat a plant-based diet, whether it be a strictly vegetarian or vegan diet; a pescetarian diet (includes fish), or a more flexible diet that includes a mix of plant-based meals and those that include meat and poultry (the “flexitarian” diet, or other options like Mark Bittman’s “vegan before 6”). People are including more plant-based meals in their diet for many reasons including health, environmental, religious, personal, economic, and compassion for animals. The Academy of Nutrition and Dietetics has affirmed that a vegetarian diet can meet all known needs for nutrients. In fact, vegetarians generally have fewer occurrences of heart disease, high blood pressure, diabetes, obesity, and some types of cancer.\nA Healthful Plant-Based Diet\nSimply eating removing meat from your diet does not ensure that you are living a healthier lifestyle. The key to a healthy plant-based diet, as with any other diet, is to eat a wide variety of foods, including fruits, vegetables, leafy greens, whole grain products, nuts, seeds, and legumes (peas, lentils, and beans).\n- Move your dial: Whether you eat meat daily, are a strict vegetarian, or fall somewhere in between, most people can benefit from adding more plants to your diet. Aim to make one additional meal per week a plant-based one – both health and environmental benefits are seen with small changes.\n- Include a wide variety of whole grains, beans, vegetables, and fruits: Don’t get in a rut with the same foods day after day. This will not only lead to boredom but may cause you to miss some important vitamins and minerals.\n- Beware of higher calorie and lower nutrient vegetarian selections: Even vegetarian diets can be high calorie and low nutrient if it is filled with processed foods, sweets, and too many servings of high-fat dairy products. As with any diet, eating mostly whole foods from a variety of plant sources is the foundation of a healthy plan.\n- Be relaxed about protein: As long as calories are sufficient and the diet is varied including plant-based proteins such as beans, legumes, nuts, seeds, and soy products, most vegetarians easily meet protein needs. Don’t rely too heavily on cheeses as a primary protein source.\nNECESSARY NUTRIENTS IN A PLANT-BASED DIET\nWhether you occasionally include animal products in your diet or follow a strict vegan diet, planning meals is essential to ensure you are eating adequate amounts of the following nutrients:\n- Vitamin B12—sources include fortified soy beverages, cereals, and nutritional yeast. There are no plant sources of B12 unless fortified.\n- Vitamin D—sources include fortified soy, nut, or rice beverages, some margarine, and as always a bit of sunshine.\n- Calcium—sources include tofu processed with calcium, broccoli, seeds, nuts, kale, bok choy, legumes, greens, lime-processed tortillas, soy beverages, grain products, and beverages such as orange juice and nut milks enriched with calcium.\n- Iron—sources include legumes, tofu, green leafy vegetables, dried fruit, whole grains, and iron-fortified cereals and breads, especially whole-wheat. Absorption is improved by vitamin C, found in citrus fruits and juices, tomatoes, strawberries, broccoli, peppers, dark-green leafy vegetables, and potatoes with skins\n- Zinc—sources include whole grains (especially the germ and bran), whole-wheat bread, legumes, nuts, and tofu\nPlant-based diets can be simple and nutritious as long as you keep in mind the basics of balance and variety. Look for items in the café that are marked with the vegetarian or vegan icon and don’t be shy to ask questions about ingredients in specific dishes. Your Bon Appétit staff is there to help you to eat a healthy, balanced diet.\nThe Vegetarian Resource Group. www.vrg.org. Accessed July 2016.\nToday’s Dietitian Nutrients of Concern for those Following a Plant-Based Diet. www.todaysdietitian.com. Accessed April 2015.']	['<urn:uuid:d9cda14d-b532-449c-97f9-3905f2f24a15>', '<urn:uuid:e6a8e51c-a54c-4f0e-9c5a-d1549ce7a04e>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	14	67	1153
37	reasons colonial merchants agreed boycott british goods 1768	The Boston merchants agreed to boycott British goods in 1768 due to several economic hardships: the scarcity of money in the colonies, large sums being collected by customs officers for duties, heavy taxes to pay war debts, trade restrictions from recent Parliamentary acts, and declining success of cod fishing. These factors made it difficult for colonists to pay their debts to British merchants and continue importing goods. They agreed to stop importing goods from Great Britain from January 1769 to January 1770, with few exceptions like salt, coals, and fishing equipment. They also specifically banned the import of tea, glass, paper, and painters' colors until duties on these items were repealed.	"['""Nervous tension"" is the term that best describes the relationship between the American colonies and England in the aftermath of the Stamp Act repeal.\nSeveral issues remained unresolved. First, Parliament had absolutely no wish to send a message across the Atlantic that ultimate authority lay in the colonial legislatures. Immediately after repealing the Stamp Act, Parliament issued the Declaratory Act.\nThis act proclaimed Parliament\'s ability ""to bind the colonies in all cases whatsoever."" The message was clear: under no circumstances did Parliament abandon in principle its right to legislate for the 13 colonies.\nIn the Western Hemisphere, leaders were optimistic about the repeal of the Stamp Act but found the suggestions of the Declaratory Act threatening. Most American statesmen had drawn a clear line between legislation and taxation. In 1766, the notion of Parliamentary supremacy over the law was questioned only by a radical few, but the ability to tax without representation was another matter. The Declaratory Act made no such distinction. ""All cases whatsoever"" could surely mean the power to tax. Many assemblymen waited anxiously for the issue to resurface.\nFrom infancy I was taught to love humanity and liberty. Inquiry and experience have since confirmed my reverence for the lessons then given me by convincing me more fully of their truth and excellence. Benevolence toward mankind excites wishes for their welfare, and such wishes endear the means of fulfilling them. These can be found in liberty only, and therefore her sacred cause ought to be espoused by every man, on every occasion, to the utmost of his power. As a charitable but poor person does not withhold his mite because he cannot relieve all the distresses of the miserable, so should not any honest man suppress his sentiments concerning freedom, however small their influence is likely to be. Perhaps he may ""touch some wheel"" that will have an effect greater than he could reasonably expect...\n– John Dickinson, Letters from a Farmer in Pennsylvania to the Inhabitants of the British Colonies (1767)\nSure enough, the ""truce"" did not last long. Back in London, Charles Townshend persuaded the House of Commons to once again tax the Americans, this time through an import tax on such items as glass, paper, lead, and tea.\nTownshend had ulterior motives, however. The revenue from these duties would now be used to pay the salaries of colonial governors. This was not an insignificant change. Traditionally, the legislatures of the colonies held the authority to pay the governors. It was not uncommon for a governor\'s salary to be withheld if the legislature became dissatisfied with any particular decision. The legislature could, in effect, blackmail the governor into submission. Once this important leverage was removed, the governors could be freer to oppose the assemblies.\nTownshend went further by appointing an American Board of Customs Commissioners. This body would be stationed in the colonies to enforce compliance with tax policy. Customs officials received bonuses for every convicted smuggler, so there were obvious incentives to capture Americans. Given that violators were tried in juryless admiralty courts, there was a high chance of conviction.\nTownshend also pressed the Americans to the limit by suspending the New York legislature for failing to provide adequate supplies for the British troops stationed there. Another showdown appeared imminent.\nReactions in the colonies were similar to those during the Stamp Act Crisis. Once again nonimportation was implemented. Extralegal activities such as harassing tax collectors and merchants who violated the boycotts were common. The colonial assemblies sprung into action.\nAugust 1, 1768\nThe merchants and traders in the town of Boston having taken into consideration the deplorable situation of the trade, and the many difficulties it at present labours under on account of the scarcity of money, which is daily increasing for want of the other remittances to discharge our debts in Great Britain, and the large sums collected by the officers of the customs for duties on goods imported; the heavy taxes levied to discharge the debts contracted by the government in the late war; the embarrassments and restrictions laid on trade by several late acts of parliament; together with the bad success of our cod fishery, by which our principal sources of remittance are like to be greatly diminished, and we thereby rendered unable to pay the debts we owe the merchants in Great Britain, and to continue the importation of goods from thence;\nWe, the subscribers, in order to relieve the trade under those discouragements, to promote industry, frugality, and economy, and to discourage luxury, and every kind of extravagance, do promise and engage to and with each other as follows:\nFirst, That we will not send for or import from Great Britain, either upon our own account, or upon commission, thisfall, any other goods than what are already ordered for the fall supply.\nSecondly, That we will not send for or import any kind of goods or merchandize from Great Britain, either on our own account, or on commissions, or any otherwise, from the 1st of January 1769, to the 1st of January 1770, except salt, coals, fish hooks and lines, hemp, and duck bar lead and shot, woolcards and card wire.\nThirdly, That we will not purchase of any factor, or others, any kind of goods imported from Great Britain, from January 1769, to January 1770.\nFourthly, That we will not import, on our own account, or on commissions or purchase of any who shall import from any other colony in America, from January 1769, to January 17 70, any tea, glass, paper, or other goods commonly imported from Great Britain.\nFifthly, That we will not, from and after the 1st of January 1769, import into this province any tea, paper, glass, or painters colours, until the act imposing duties on those articles shall be repealed.\nIn witness whereof, we have hereunto set our hands, this first day of August, 1768.\n– Boston Non-Importation Agreement (August 1, 1768)\nIn a circular letter to the other colonies, the Massachusetts legislature recommended collective action against the British Parliament. Parliament, in turn, threatened to disband the body unless they repealed the letter. By a vote of 92 to 17, the Massachusetts lawmakers refused and were duly dissolved. Other colonial assemblies voiced support of Massachusetts by affirming the circular letter.\n|PERIOD||BRITISH PRIME MINISTER||EVENT|\n|1762-63||John Stuart, Earl of Brute||End of Seven Years War, Treaty of Paris|\n|1763-65||George Grenville||Issue Sugar Act, Stamp Act, and Currency Act|\n|1765-66||Charles-Watson Wentworth, Marquess of Rockingham||Repeal Stamp Act, Issue Declaratory Act|\n|1766-68||William Pitt the Elder, Earl of Chatham||Issue Townshend Acts|\n|1768-70||Augustus Fitzroy, Duke of Grafton||Unable to implement policy of conciliation towards colonies because of chaos in Parliament|\n|1770-82||Lord North||Boston Massacre, Repeal Townshend Duties, Issue Tea Act and Intolerable Acts, American Revolution begins with Battles of Lexington and Concord|\n|1782||Charles-Watson Wentworth, Marquess of Rockingham||Open peace negotiations with America|\n|1782-83||William Fitzmaurice, Earl of Shelburne||End of American Revolution, Treaty of Paris, 1783|\nThe tighter the British grip grew, the more widespread was the resistance. By 1769, British merchants began to feel the sting of nonimportation. In April 1770, news of a partial repeal — the tax on tea was maintained — reached America\'s shores.\nThe second compromise came at a high price. It was reached only after a military occupation of Boston and the ensuing Boston Massacre.']"	['<urn:uuid:abaef7a9-df98-48c0-a51f-6bd491459b3c>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	8	111	1213
38	What conditions must be met before declaring war?	Several key conditions must be met before declaring war according to Just War Theory. There must be a just cause, such as resisting an attack or defending another state. The intention must be right, without ulterior motives like acquiring land. War must be declared by legitimate authority (like a president) and must be formally declared - no surprise attacks allowed. There must be a reasonable chance of success to avoid wasting lives. It must be a last resort after other peaceful means have failed. Finally, the expected gains must be proportional to anticipated losses. These conditions aim to ensure wars are fought for morally justified ends while protecting fundamental human rights.	"['Just War Theory\nThis theory dates back to St Augustine who had to persuade the pacifist Christian tradition in Roman times that war can sometimes be necessary. St Aquinas and other theologians later developed this theory. There are three parts to this ethical theory JUS AD BELLUM –rules set out for going to war, JUS IN BELLO – rules set out for being in war and JUS POST BELLO – the nature of how a war is ended.\nSo what are the Jus Ad Bellum Conditions?\n- Just cause – in order for a war to be declared there must be a just cause. But who is to judge what is a just cause?\n- Right intention – It is linked with the part of Natural Law that states that intention is really important. Ulterior motives such as acquiring land or protecting oneself should not be intended. But how can you test/measure intention?\nProportionality – The damage caused by the war should not exceed the good expected to come out. How are you supposed to predict this?\n- Declaration by a legitimate authority – This principle is linked with this quote from the Bible “Obey the powers that be for they are ordained by God” Romans 13:1-2. Some like Wilcockson say that this principle has been overlooked so that terrorists now have rights of this nature.\n- Last resort – Every other must have been used before going to war e.g. peaceful negotiations. If an authority has already decided this is what they want to do is there any point in negotiations?\n- Formal declaration of war – Some countries do not recognise the UN rules for going to war and they may not recognise this principle. We have seen some of the most horrific example of when this rule has been violated e.g. Pearl Harbour\n- Reason chance of success – There should be a relative chance of success but if one knows they don’t have a chance of success they may feel ashamed in declaring it.\nThen what are the Jus In Bello conditions?\nThere are two conditions according to Just War Theory for how we should behave in war. Jus In Bello requires agents of war to be responsible for their actions.\n- Principle of discrimination – innocent people or ‘non- combatants’ should not be directly or directly attacked according to the Geneva Convention. Is this really possible?\n- Principle of proportionality – Minimum force should be used to achieve good and just like with proportionality before the damage caused should not outweigh the good. With nuclear weapons is this really possible?\nAnd what about Jus Post Bello then?\nThis is a fairy new addition to the Just War Theory. This concerns the aftermath of war and restoring peace and fairness.\nRestoring human rights – bringing back the equality and rights that everyone has.\n- Distinguishing between innocent civilians and those who should be punished\n- Bringing to trial war criminals and ensuring they receive justice\n- Giving the defeated country the opportunity to reform.', 'Why think about this?\nAfter I commanded an infantry company in the 82nd, the Army sent me to study philosophy, and I used the opportunity to seek an answer to the trooper\'s question. I wrote my thesis on the moral justification for killing in combat, and I have since taught ethics and just-war theory for four years at\nThe Army has no official just-war doctrine (unless you count FM 27-10), and nowhere does it address the morality of killing. Soldiers are expected to do their duty, which is to fight and kill without violating the laws of land warfare. However, the inattention to soldiers\' moral concerns has had consequences: more than 1,000 conscientious objectors in the 1991 Gulf War; soldiers and units that have refused to engage the enemy in combat; and soldiers who have felt needlessly guilty about their actions in combat.\nLeaders who train their soldiers how to kill, and who order them to kill, owe it to their soldiers to explain to them why it is morally permissible for them to kill. And soldiers who believe in what they\'re doing will fight more effectively.\nI am not currently assigned to a deployable unit, but if I were, this is what I\'d talk about with my soldiers. Of course, these are just my unofficial thoughts, but use them if you find them helpful.\nIs this a just war? Are we fighting it justly?\nThe morality of any war is judged at two levels: the morality of going to war (Is it a just war?); and the morality of how the war is being fought (Are we fighting it justly?). Political leaders bear responsibility for the first judgment; after all, they make the decision whether or not we go to war. Military leaders, however, must bear responsibility for the second judgment, because soldiers are the ones ""on the ground"" actually doing the damage.\nWhen is a war a ""just war""?\nThe traditional criteria used to judge (legally and morally) the political decision to go to war are:\nJust Cause. The state must be fighting for a morally justified end. What is a morally justified end? A state may fight to resist an attack (or to help another state resist an attack), to restore the rightful borders of a state that was attacked, and to do what is reasonable to prevent an aggressor state from attacking once again. A state may also launch a pre-emptive strike against a state that threatens it if the threat is imminent and if waiting will make it impossible to successfully defend against the imminent attack.\nRight intention. The state must not only have a just cause, but also it must actually be fighting for it. For example, the\nLegitimate authority. Only the leaders of a political community have the moral authority to commit its people to war. In our case, that\'s the president.\nFormal declaration. You can\'t sucker-punch another nation. See FM 27-10, para.20.\nChance of Success. Don\'t waste human lives in a hopeless cause. This is always a judgment call for the political leaders. In the history of war, some underdogs have won. Appeasement is a tough pill for a nation to swallow; it\'s the forfeiture of those people\'s fundamental human rights.\nLast resort. War should not be the first option for resolving disputes. If it\'s possible to accomplish an outcome without resort to war, then it\'s morally obligatory for political leaders to do so. ""Last resort,"" though, is a misleading term. Read literally, it would require appeasement. So, it should be read to mean, ""Non-violent means to resolve the conflict have been tried and failed.""\nProportionality. When political leaders commit their nation to war, what they expect to gain must be proportional to what they expect to lose. This ties in closely with criterion #5. It might, for example, be immoral to fight to defend\nThis laundry list of conditions has its limitations. It assumes that all wars are between states (inter-state wars), but the majority of recent wars have been and continue to be intra-state wars (Somalia, Bosnia, Kosovo, Chechnya) or non-state wars (the wars against narco-trafficking and terrorism). Perhaps we need to examine the moral principles that serve as the foundation for the traditional rules of war, and apply those principles to today\'s international situation.\nMorally, when is a war just? When it defends the innocent from those who threaten them. All human beings possess the rights to life and liberty, and they can best exercise those rights within ordered communities, so they establish governments and become recognized as countries. The primary purpose of any country (i.e., government, state) is to protect the rights of its people. That is why it is morally right to defend a state against an attacker-because you are protecting the rights of the innocent. And that is why it is presumptively wrong to attack another country-because its government is protecting the rights of its people. However, that is also why it is morally acceptable to intervene militarily in a country where the government is violating its own people\'s rights (\nThe profession of arms is a noble profession because its Soldiers, sailors, airmen, Marines, and Coast Guardsmen willingly risk their own lives and liberty to protect the lives and liberty of others.\nFight with Honor!\nThis brings us to the question of what actions are morally right within a war. Just because a war is just does not mean that any military action in that war is just. Soldiers still have to conduct themselves morally.\nFM 27-10 (Law of Land Warfare) is a great starting point for discussing moral conduct in war, but it does not address every situation that our soldiers face. It is largely unchanged from its initial, 1956 publication, so it addresses 20th Century warfare more adequately than 21st Century warfare. It is therefore very important that soldiers understand the moral principles that serve as the foundation and inspiration for the written laws of war and even for our rules of engagement. Morality is enduring and universal. Let\'s focus on assessing the morality of actions within war.\nHow do we fight morally? By killing only combatants. Who are combatants? Morally, combatants are those who-through some choice of their own-have forfeited their right not to be killed by choosing to engage in an activity that is threatening to others. As such, all soldiers are combatants, and any ""civilians"" who choose to threaten you are combatants.\nWhy is this so? All human beings have the rights to life and liberty (or, stated another way, the rights not to be killed or oppressed). But, they forfeit those rights if and when they threaten the rights of another. That is why it is ok to use lethal force against someone who attacks you on the street or in your home. The attacker has forfeited his rights by threatening yours. Likewise, that is why it is morally permissible for you to kill enemy combatants. By participating in a force that is trying to kill you and others, and that violates or threatens the rights of innocent people, enemy soldiers have forfeited their right not to be killed by you. Of course, this applies to all combatants, on both sides of the conflict. By being a soldier during time of war, you are a threat to the enemy, and they do nothing morally wrong when they try to kill you.\nIn general terms, when is it morally ok to kill someone? When that person threatens someone else, thus forfeiting his own right to not be killed. (See Ethics, Killing, and War by Richard Norman for a full discussion of these criteria.) Think of it in terms of killing in self defense in everyday life. Imagine someone attacking you as you walk down the street. When is it morally permissible to kill him in self-defense? When he:\n- -is responsible for his actions;\n- -threatens a value worth killing for (life or liberty);\n- -poses an imminent threat;\n- -and leaves you no other option to avoid the threat.\nEnemy soldiers are responsible for the threat that they pose. At some level, they chose to be soldiers, and they must know that they are at war. Fully informed volunteers, of course, are more responsible than poorly informed conscripts, yet the fact remains that even conscripts chose to become soldiers. They had other options, however unpleasant they may have been. Human beings, after all, are not responsible for circumstances beyond their control, such as whether their nation goes to war. They are, however, responsible for the choices they make within those circumstances. People who make the choice to be soldiers in war are morally responsible for the threat they pose to their enemy.\nSoldiers do fight to defend values that are worth killing and dying for, when the war is just.\nSoldiers also do face an imminent threat from enemy soldiers. All enemy soldiers are either direct threats or accomplices to direct threats. They all act for the same end-to deny the soldier and/or those he is defending their rights to life and liberty. Soldiers have no recourse to a ""higher authority"" to defend them. They must fight, or they and other innocent persons will lose their cherished rights.\nFinally, soldiers do not have a non-lethal option. If they flee before the enemy, the threat will follow them. Again, there is no ""higher authority"" to offer protection to soldiers and to those who depend on soldiers to defend their lives and freedom.\nTherefore, not only is it morally permissible for us to kill enemy soldiers in combat, but also it is morally obligatory for us to use the force necessary to defend the rights of those who depend on us. We soldiers are the last line of defense for the rights of life and liberty.\nHow should we treat civilians in a combat zone? We should assume that civilians are noncombatants, and thus they retain their right not to be killed. We should respect their rights. If, however, a civilian chooses to become a threat, then he or she forfeits her right not to be killed. Remember, though, to treat enemy civilians as noncombatants until they give you a reason to believe otherwise, whereas enemy soldiers should be treated as combatants unless they surrender or become incapacitated.\nHow should we treat surrendering or injured enemy soldiers? When an enemy soldier is no longer willing or able to be a threat, then he regains his right not to be killed. It is morally wrong to kill an EPW or incapacitated casualty.\nWhat should we do when our actions will likely cause collateral damage to noncombatants? We must never target noncombatants, and we must take actions to limit collateral damage that affects noncombatants.\nHowever, sometimes soldiers face tough decisions that involve attacks against legitimate military targets that will likely cause collateral damage. In such circumstances, we should refer to the framework of the so-called ""doctrine of double effect"" for guidance. This is laid out beautifully in the best texts on the topic of justice in war, Just and Unjust Wars by Michael Walzer [and in later book The Morality of War by Brian Orend].\nWhen a proposed military action has an intended good effect (usually, killing the enemy) and an unintended but likely bad effect (collateral damage), then it is morally permissible to take that action only if:\n- Your action itself is moral. A war crime (like executing a prisoner) can never be justified.\n- Your direct effect (the intended outcome) is moral. It has to directly impact combatants or other legitimate targets.\n- Your intent is good. You aim only at the good effect; the bad effect is not the means to the good effect. In other words, you can\'t do something bad, like kill noncombatants, to bring about a good result, like the surrender of enemy troops.\n- You must accept some risks to yourself and your own troops to minimize the risk of collateral damage. In other words, you shouldn\'t put all the risk on noncombatants just to limit your own risk. At the same time, you may put a level of risk on noncombatants that\'s necessary to accomplish your mission. It\'s a judgment call that leaders must make. This, I know, is hard for many of us to accept, because we love our soldiers and want to bring all of them home. Still, as soldiers we must remember our calling-to risk ourselves to protect the innocent.\n- The good effect has to be proportional to the bad. Accomplishing the mission has to be worth the collateral damage. Don\'t destroy a village to kill a sniper.']"	['<urn:uuid:f38d9bb0-f69b-4f9b-bb75-af94b0cb3488>', '<urn:uuid:08baef70-c2f3-48c8-a183-e1ceca30b731>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T04:31:27.499155	8	111	2616
39	north america primary processes studied bhm	Over North America, the Bayesian Hierarchical Model (BHM) primarily studied two processes: vertical land motion and hydrology, which were observed using GPS stations and GRACE gravimetry.	['With a full project team finally in place from late 2019, making progress on the key research aims and objectives was the main focus of our work during 2020 – something that was made somewhat more complicated by the coronavirus pandemic and subsequent need to work remotely from March 2020 onwards.\nDespite this unforeseen challenge, work began in earnest to develop our Bayesian Hierarchical Model (BHM) to investigate GIA and mass trends for North America. We chose North America as it represents a ‘data rich’ region, allowing us to develop and test the BHM over a discrete area for a smaller number of relatively well observed (and understood) processes.\n(1) GIA and mass trends for North America\nOver North America, we were essentially only concerned with two processes – vertical land motion and hydrology – as observed by GPS stations and GRACE gravimetry. However, and as we came to learn, behind that seemingly simple set-up were many hidden difficulties and pitfalls. So, after almost 12 months’ work building, testing and tweaking the BHM, we are now finally in a position where we are confident that the BHM is working correctly, allowing us to produce internally consistent and geophysical meaningful outputs for GIA and hydrology across North America. More details about the method and outputs from this work will start to emerge through 2021.\nTowards the end of 2020 our attention started to turn to extending the BHM to investigate the global sea level budget, the overall aim of the GlobalMass project. By design, our work on North America provides a crucial stepping-stone towards deployment of BHM on a global scale to solve for the multiple processes that dictate sea level. This is the work that will increasingly become the focus of our attention in early-2021.\n(2) Improving interpretation of hydrology trends from short time series\nAnother area of investigation has been the extent to which short-term changes in water storage (in this case using data from the Gravity Recovery And Climate Experiment (GRACE) satellite mission) can be used to identify emergent – potentially unprecedented – trends, given the wider spatial and temporal variability of the hydrological processes in question.\nFigure 1. An illustration of how short-term trends in terrestrial water storage (ΔTWS) for six major river catchments – as highlighted by the coloured bars and comparable in length to those obtained from GRACE – are dependent on the time-period selected and do not always provide a reliable indicator of longer-term change. [Source: Vishwakarma et al. (2020) DOI: 10.1088/1748-9326/abd4a9]\nTo improve this, we introduce a new metric (Trend to Variability ratio, TVR) for assessing the severity of GRACE trends with regards past variability and show that GRACE-derived water storage trends are better interpreted when set in the context of historical variability. Using GRACE data complemented by TVR, we find that several regions thought to be losing water at a moderate rate are more endangered when longer term natural variability is considered, and vice-versa. We also estimate that greater than 3.2 billion people are currently living in regions facing severe water storage depletion, and that over one-third of river catchments that lost water in the last decade have suffered unprecedented losses.\n(3) Attributing variability in coastal sea level short time series\nOur BHM framework aims to solve the sea level budget and will initially use the `golden period’ of overlapping Argo and GRACE missions (2005 to 2015), with 11 years of excellent quality and high spatial resolution data. Sea level changes on inter-annual to decadal time scales are driven in-part by internal variability, particularly from climatic atmosphere-ocean interactions. To better understand what this internal variability can look like, we have also been asking: how much of the decadal, coastal sea level variability can be described by climate modes?\nWe have determined spatial patterns of sea level variability from a high-resolution ocean-model and compared decadal time series against key climate indices that describe large-scale climate variability. In doing so, we obtain reconstructed time series of the decadal sea level variability with a known source (e.g. Figure 2). Reducing the observed sea level by these reconstructions can reduce the uncertainty in trend and acceleration estimates and clarifies the signal from other sources in the sea level budget. We will be reporting more results from this work later in 2021.\nFigure 2. The percentage of variance in sea level decadal time series that can be explained by a reconstructed time series using only climate indices (like the El Nino Southern Oscillation, or ENSO, index). Whilst the spatial picture is complex there are regions, including those that are observation-poor like the West African coast, where we can account for much sea level variability from simple indices. [Source: the authors]\nVishwakarma B.D., Bates, P., Sneeuw, N., Westaway, R.M. and Bamber, J. L. (2020). Re-assessing global water storage trends from GRACE time series. Environmental Research Letters (DOI:10.1088/1748-9326/abd4a9 ).\nVishwakarma, B. D., Royston, S., Riva, R. E. M., Westaway, R. M., & Bamber, J. L. (2020). Sea level budgets should account for ocean bottom deformation. Geophysical Research Letters 47, e2019GL086492 (DOI:10.1029/2019GL086492 ).\nRoyston, S., Vishwakarma, B. D., Westaway, R. M., Rougier, J., Sha, Z., and Bamber, J. L. (2020). Can we resolve the basin‐scale sea level trend budget from GRACE ocean mass? Journal of Geophysical Research: Oceans 125, e2019JC015535 (DOI:10.1029/2019JC015535).']	['<urn:uuid:c1199c6a-1f1e-4576-ad61-467f429204d9>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	6	26	881
40	software tools challenges unify product lifecycle management application lifecycle management explain main issues	According to research by Accenture, the main challenges in unifying ALM and PLM are integrating requirements and ensuring consistent change management. Currently, most companies use standalone solutions with manual integrations to ensure data consistency, but these manual processes can increase the risk of errors. A common platform with automatic integration is necessary to overcome these challenges.	"[""Today's complex products increasingly require the integration of hardware and software development processes, making ALM-PLM convergence one of the hot topics in the industry. Application and Product Lifecycle Management software platforms have been designed to support the development of software and hardware end products. However, they have evolved separately to suit the needs of these two disparate industries.\nWith the emergence of cheap sensors, memory modules, etc., embedded software, and lately, the Internet of Things, products increasingly contain software (and even service) components. Therefore, more and more products rely on software to replace or enhance the functionality of certain hardware parts. The integration of hardware and software development processes, both contributing to the same product, has become necessary. Coordinating the processes involved in product development increases efficiency and synergies.\nThe difference between PLM and ALM\nFirst, let's take a look at the basic differences between PLM and ALM, and why their integration is such a highly discussed topic today.\nTraditionally, Product Lifecycle Management (PLM) tools (information management systems) have been used by hardware engineering teams to support the physical design & manufacturing of products. PLM comes from the automotive industry, and covers the engineering, design, manufacturing, service and disposal processes of physical products. Such platforms help integrate data, processes and align teams throughout the lifecycle.\nApplication Lifecycle Management (ALM) can be thought of as PLM for software. Similarly to Product Lifecycle Management, ALM encompasses the entire lifecycle, from requirements management, through development, testing, maintenance, all the way to the release and maintenance of software products. Project management, integrated data management and collaboration are important parts of any ALM solution.\nWhile ALM and PLM are used for a generally similar purpose (product development), they have evolved in industry sectors or disciplines that used to be entirely separate and fundamentally different. However, information siloed in different data management systems can no longer adequately support the development of complex devices that combine software and hardware, as well as services enabled by network connectivity. Keeping these processes separate could lead to inconsistencies, insufficient quality control, and an increased risk level.\nWith the convergence of hardware and software development, the need for multidisciplinary lifecycle management emerged, and the integration of PLM and ALM became necessary.\nConnecting development lifecycles\nSince this integration of lifecycles is such a recent need, there's no single silver-bullet solution to it yet. Lifecycle management software solution vendors are just realizing the importance of this integration, and are working to fuse these previously isolated data management systems. Therefore, most development companies are taking a best of breed approach: using tried and tested standalone solutions, and (manually) creating integrations to ensure data consistency. However, manual processes can increase the chance of errors.\nAccording to Accenture research, integrating requirements and ensuring consistent change management are the greatest challenges in unifying ALM and PLM. To achieve these goals and to avoid manual errors, a common platform with automatic integration is necessary.\ncodeBeamer has been designed to manage complexity. It lets you handle large volumes of data from different sources, while ensuring complete change control and data consistency throughout the lifecycle. While codeBeamer is by nature an ALM solution, PLM-related data can be easily integrated to allow you to manage the entire process using one platform. Workflows can be interconnected and automated, allowing you to connect processes and automatically create work items to stay in control of complex lifecycle processes.""]"	['<urn:uuid:48753fa2-1e63-421e-86b3-6eec4fb56e5d>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	13	56	564
41	vietnamese cultural artifacts destroyed during ming invasion comparison modern preservation efforts	During the Ming invasion in the 15th century, Chinese forces systematically destroyed Vietnamese cultural landmarks, including towers, pagodas, statues, and books. Only five Amitabha statues from the Lý Dynasty survived this destruction. In contrast, modern preservation efforts have been more successful, as evidenced by the safeguarding of artifacts like the complete Amitabha statue at Ngô Xá Pagoda (now a National Treasure since 2013) and the preservation of historical documents like Đặng Thùy Trâm's wartime diaries, which were saved by Fred Whitehurst and eventually returned to Vietnam.	"['Đặng Thùy Trâm\nĐặng Thùy Trâm (born November 26, 1942, in Huế, Vietnam; died on June 22, 1970, in Đức Phổ, Quảng Ngãi Province, Vietnam) was a Vietnamese doctor. She worked as a battlefield surgeon for North Vietnam during the Vietnam War. At age 27, she was killed in disputed circumstances by US forces while traveling on a trail in the Ba Tơ jungle in the Quảng Ngãi Province of south-central Vietnam. Her wartime diaries, which chronicle the last two years of her life, attracted international attention following their publication in 2005.\nOne of Trâm\'s handwritten diaries was captured by US forces in December 1969. Following her death in a gun battle on June 22, 1970, a second diary was taken by Frederic (Fred) Whitehurst, a then 22-year-old military intelligence specialist. Whitehurst defied an order to burn the diaries, instead following the advice of a South Vietnamese translator not to destroy them. He kept them for 35 years, with the intention of eventually returning them to Trâm\'s family.\nAfter returning to the United States, Whitehurst\'s search for Trâm\'s family initially proved unsuccessful. After earning a Ph.D. in chemistry he joined the FBI, but was unable to reach anyone from the Vietnamese embassy. In March 2005, he and his brother Robert - another Vietnam veteran - brought the diaries to a conference at Texas Tech University. There, they met photographer Ted Engelmann (also a Vietnam veteran), who offered to look for the family during his trip to Vietnam. With the assistance of Do Xuan Anh, a staff member in the Hanoi Quaker office, Engelmann was able to locate Trâm\'s mother, Doan Ngoc Tram, and subsequently reached the rest of her family.\nIn July 2005, Trâm\'s diaries were published in Vietnam under the title Nhật ký Đặng Thùy Trâm (Đặng Thùy Trâm\'s Diary (Last Night I Dreamed Of Peace)), which quickly became a bestseller. In less than a year, the volume sold more than 300,000 copies and comparisons were drawn between Trâm\'s writings and that of Anne Frank.\nIn August 2005, Fred and Robert Whitehurst traveled to Hanoi, Vietnam, to meet Trâm\'s family. In October of that year, Trâm\'s family visited Lubbock, Texas to view the diaries archived at Texas Tech University Vietnam Archive, and then visited Fred Whitehurst and his family.\nThe diaries were translated into English and published in September 2007. They include family photographs and images of Trâm. Translations of the diaries have been published in at least sixteen different languages.\nIn 2009, a film about Tram by Vietnamese director Đặng Nhật Minh, entitled Đừng Đốt (Do Not Burn It), was released.\nThis article includes a list of references, but its sources remain unclear because it has insufficient inline citations. (June 2015) (Learn how and when to remove this template message)\nThis article\'s use of external links may not follow Wikipedia\'s policies or guidelines. (June 2015) (Learn how and when to remove this template message)\n- ""Trở lại một khát vọng hòa bình""(September 14, 2007)\n- ""Nhật ký Đặng Thùy Trâm có giá trị toàn cầu và vĩnh cửu"" (September 18, 2007)\n- ""Last night I dreamed of peace"", published worldwide by Random House, September 11, 2007.\n- Full text of The Diary of Dr. Dang Thuy Tram from The Vietnam Center site at Texas Tech University (scans of original Vietnamese text; English translation removed at request of family)\n- ""Tram Diaries: Soldiers Preserve Writings of Vietnam War""\n- ""War\'s cruel poetry moves search by 2 N.C. veterans"" Charlotte Observer (Charlotte, North Carolina), October 6, 2005\n- ""Vietcong Doctor\'s Diary of War, Sacrifice""\n- ""Mother Reads Daughter\'s Vietnam Diaries... 35 Years Later"" (October 6, 2005)\n- ""The real stuff: what a Vietnamese army doctor saw"" (September 22, 2005)\n- ""A daughter returns home — through her diaries"" (October 12, 2005)\n- ""Best-selling diary transformed into television show"" (August 15, 2005)\n- ""Copyright and the translation of The Diary of Dang Thuy Tram""\n- ""Diarist\'s mother visits US, holds daughter\'s manuscript"" (October 7, 2005)\n- ""The Diary of Dr Tram"" (February 13, 2006)\n- ""Day to Day Among the Viet Cong"" (August 4, 2006)\n- Dang Thuy Tram video from Texas Tech University\n- ""Archived copy"". Archived from the original on June 27, 2009. Retrieved December 26, 2010.\n- ""Vietcong Doctor\'s Diary of War, Sacrifice - OhmyNews International"". english.ohmynews.com. Retrieved 2016-05-19.\n- ""Archived copy"". Archived from the original on September 6, 2008. Retrieved December 26, 2010.\n- Vietnam Archive Archived September 30, 2008, at the Wayback Machine.', '|Piece of history: The original statue of the Việt people from 900 years ago. — VNS Photo Phạm Hoàng Văn|\nTo many first-time visitors, Hà Nội seems to be a young city not only because of so many young people out on its streets, but also because of new buildings and supermarkets, shops and restaurants, colleges and libraries. Thousands of new buildings have sprung up in the past 20 to 30 years.\nMany people, particularly expatriates, find it hard to look into the 1,000-year-old history to find traces of the many dynasties that had their capitals up and down the Red River.\nThe architectural landmarks of distant dynasties, such as the Lý and Trần, are few and far between. So when a stone statue of Amitabha, the future Buddha, was declared a National Treasure, people jumped at the chance to see it.\nThe statue, which is housed at Ngô Xá Pagoda in Nam Định Province, is “said to be the oldest remaining complete stone statue of the Việt people from the Lý Dynasty”, said history professor Trần Lâm Biền.\n“We found it in the 1960s when we went to examine a collection of Chương Sơn relics,” he said. “The statue was originally up in the mountains, but people had carried it down to preserve it at the Ngô Xá Pagoda at the foot of the mountain.”\nThe Lý Dynasty started with founder Lý Công Uẩn, who later became King Lý Thái Tổ, who moved the royal capital from mountain-trapped Hoa Lư, to the flatland surrounding Đại La Citadel, where the Red River meets a busy residential and trading centre.\nHe named the new citadel Thăng Long, the Ascending Dragon, and started a dynasty that lasted for more than 200 years, that was militarily powerful and economically successful. Religion and art also bloomed.\n|First phase: The original stone Amitabha Statue before it was gold-gilded. Photo courtesy of http://hoidonghuongnamdinhhcm.com|\nLý Công Uẩn grew up in a pagoda. During his life, Lý Thái Tổ, as he was known, was an observant Buddhist. Buddhism flourished under the dynasty.\nIt was written in the Đại Việt Sử Ký Toàn Thư, the Complete History of Great Việt, that the king ordered thousands of Buddhas be chiselled, thousands painted and tens of thousands of religious banners made.\nIt is written on a stone stele at Quỳnh Lâm Pagoda in Quảng Ninh Province, that a monk, Thượng Đức, carved a Buddhist statue 1m80 tall.\nAuthor Nguyễn Tiến Cảnh writes in The Arts of Lý Dynasty published in 1973, “Buddhist towers and pagodas played a vital role under the Lý Dynasty. Then, pagodas were built everywhere in the country. Where there was man, there was pagoda. At beautiful scenic mountains and landscapes, there would be a pagoda built.”\nOther Lý kings, Lý Thái Tông and Lý Nhân Tông, both took care of Buddhist and State affairs. Buddhism was actively practised, the teachings were wide-spread. Historian Lê Văn Hưu in the 13th century wrote that, “Over half of the population are monks and pagodas can be seen everywhere.”\n|Still standing: Old and rusty, the pagoda was built in the 17th century. — VNS Photo Mỹ Hà|\nUnder the Lý Dynasty, a great number of pagodas and towers were built. Regent Queen Ỷ Lan built a total of 100 pagodas in her lifetime.\nThe country’s most influential Buddhist landmarks were built in the period. They included One Pillar Pagoda, the Celestial Tower in Thăng Long (now Hà nội), Phật Tích Pagoda and tower in Bắc Ninh Province and Chương Sơn Tower in Hà Nam Province.\nThe glorious epoch of the Lý Dynasty was later followed by the illustrious Trần Dynasty, both pivotal to the Việt civilization.\nIn the 15th century, the Ming from China invaded Đại Việt, as then Việt Nam today was named, and placed it under Chinese rule for 20 years. During this time they destroyed all the important landmarks of the Việt people.\nThey literally pulled down the towers and pagodas, destroyed statues and pavilions. They also burnt all the books.\nOf all the magnificent pagodas and towers, palaces and residence, only five statues of Amitabha, the original treasures of the Lý Dynasty, remain for us to see today.\n|Ancient motif: The Lotus petal bears a pair of dragons typical of Lý Dynasty. — VNS Photo Mỹ Hà|\nAnd of all these five statues, the one at Ngô Xá Pagoda is the most complete. It was listed as a National Treasure in 2013.\n“This is the most important statue, the Amitabha,” says historian Trần Lâm Biền. “It is the oldest. It’s unique.”\nThis year marks the 900th anniversary of the completion of the Chương Sơn Tower and we went to observe the statue, now preserved in the Ngô Xá Pagoda at the foot of Chương Sơn Mountains.\nStarted in the early days of spring 1108, the tower took nine years to complete.\nKing Lý Nhân Tông visited the Chương Sơn mountains in 1107, 1114 and 1117 — and each time a dragon is said to have appeared. According to Đại Việt Sử Ký Tòan Thư, or the Complete History of Great Việt Complete, “King Lý Nhân Tông ventured to Chương Sơn Mountains to inaugurate the Sacred Tower Vạn Phong Thành Thiện.”\nDuring his reign, King Lý Nhân Tông warded off northern invaders and pacified southern neighbours. He reinforced the royal exams to choose talented scholars for royal postings. He banned the killing of buffaloes for meat to maintain farming capacity for rice cultivation. He built nine towers including twin towers with white tops in front of the Diên Hựu or One Pillar Pagoda in Hà Nội now.\nBut today, only the square stone foundation remains of the original Chương Sơn Pagoda. One side measures 19m long.\nThe Amitabha statue was originally placed in a 100-compartment pagoda standing by the tower. The statue was taken downhill from the mountains and now rests in Ngô Xá Pagoda, which was built in the 17th century.\nLocated at the foot of the mountains and further out of the town centre, Ngô Xá Pagoda is quiet and solemn. It stands in a large yard under age-old trees facing a rice field.\nThe sitting statue at Ngô Xá rests on a three-level stone pedestal. The lowest level shows water waves, dragons and flowers. The lower half of this level shows water wave after wave, above which lively dragons fly in between daisy flower strings. These motifs are quite popular and typical of the decoration in the Lý Dynasty.\nThe next level features a pair of lions playing with a jade ball on top of an inverted lotus. The lions sport little flowers on their bodies. The lotus petals are carved with chrysanthemum flowers.\nThe top level features a lotus bed with two layers of petals. Each petal bears a pair of dragons beautifully and lively carved.\nAll the ornate decorations lead to the most important statue on top. The whole statue and pedestal stand at 2m16.\nThe statue had a removable headpiece but it was stolen from the pagoda in 2002.\nA country-wide police search was announced and all border gates checked. Nearly a yearly later, the head was found wrapped in a red piece of cloth, left in the commune’s cemetery.\nNow if you visit the pagoda, you’ll see the statue clad in gold.\nNguyễn Văn Vinh, 80, a long time guard at the pagoda explained that the gilded coat was to hide the cut at the throat of the statue.\nIn a paper about Buddhism during the Lý Dynasty, historians Trần Lâm Biền and Chu Quang Trứ wrote that, “Only the statue at Ngô Xá Pagoda best represents the Buddhist human sculpture during the Lý Dynasty.”\nThe same paper also has it that, “The 100-compartment pagoda was beautifully carved\nwith wood, but it was all gone.”\nPointing to a broken piece with water wave motifs, Nguyễn Văn Vinh says, “This is what is left of the border of the western hall at the 100-compartment pagoda up in the mountain.”\nHe then sighs, “It was huge.” — VNS\nBy Nguyễn Mỹ Hà\n|Carved creatures: An image of lions playing with a jade ball, a popular image in the Lý Dynasty, not seen in later dynasties. — VNS Photo Phạm Hoàng Văn|\n|Historic artifact: The Stone pedestal, the most complete to date from the Lý Dynasty. — VNS Photo Phạm Hòang Văn|']"	['<urn:uuid:b91a74c1-a78f-42bd-8ce6-cf8e4e1eb4d5>', '<urn:uuid:0b5c7937-d5db-40c8-b803-4bd898888454>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	11	86	2145
42	What needs more urgent care: bloated belly in horses or pets?	Both conditions require immediate veterinary attention. The documents specifically mention that a bloated or hardened abdomen in pets requires immediate veterinary treatment, while in horses, colic (which can manifest as bloating) is described as an 'ever-present threat' that needs careful management.	"[""It's an unpleasant prospect for both horse and owner. The vet has visited and orders your horse into a stable while it overcomes injury or illness. It could be a two weeks, a month, even longer.\nIt is unfortunate that animals used to wide open spaces occasionally need confinement while problems are rectified. For the horse, it means a radical change to its daily routines, not only in terms of behaviour, but also diet.\nBy applying a few strategies, you can minimise stress on both horse and owner.\nProper feed is essential to your horse's wellbeing during confinement. Your horse's complaint may have specific feed requirements. Your vet should discuss these with you. If not, the aim is to give your horse the daily protein it needs, as well as the bulk needed in its diet so that he doesn't feel hungry.\nGreat care is needed in changing a horse's daily feed routine, with colic an ever-present threat. Where possible, try to change the feed regime gradually.\nGood quality meadow hay is a great option. It's unlikely to provide an excess of protein, and will give the horse the bulk it needs in its diet. If your horse has been in work and getting a lot of energy from grain, you will need to cut back the grain (and therefore protein) which you can safely replace with hay.\nLet your horse eat as much meadow hay as he wants. Aside from keeping him entertained, the bulk will aid digestion and, provided the quality is good, it is a great maintenance feed. If it is a growing horse, or a pregnant or lactating mare, its protein requirements are likely to be higher. This can be addressed with lucerne hay, or a grain-based commercial feed. But again, it remains critical that the diet doesn't become too rich in protein or colic and founder issues may arise.\nIf hay is unavailable, or the quality of available hay is poor, you may have little choice but to opt for more concentrated feeds. If this is the case, ensure you keep the fibre levels up in your horse's diet. Opt for either a low-protein, high fibre sweet feed or use dampened bran to replace a portion of your horse's regular grain - say, one third or thereabouts.\nProcessed sugar beet, lucerne hay, and even boiled barley are good nutritious feeds that don't pack quite the same punch as high-energy grains.\nMonitor your horse's droppings to ensure they're not too sloppy or too dry. Bran can be a laxative so if the dung is loose you may need to reduce the quantity. If the droppings appear dry, start adding a little vegetable oil to the diet.\nIf you suspect your horse is simply not drinking enough water, a little salt added to it may encourage him to drink more.\nHorses that take a dislike to any of your offerings may change their tune once you start adding a little molasses to the mix. It can also prove useful for masking the taste of any bitter-tasting medicine your vet may have prescribed.\nAs well as keeping your stabled horse clean, grooming is an excellent way for you to spend time together and help pass the time for a sick horse.\nYour vet should be able to provide good advice on a feeding regime, depending upon the condition of the animal and its needs during confinement.\nIt's important to establish a regular feeding routine. Hopefully, meadow hay will be available at all times. Other foods should be given as regularly as you reasonably can. Several smaller meals will be much better, and safer, than one big daily nosh-up.\nA sick horse will be much like a normal human patient. Any distractions that help pass the day will be great, but it needn't include anything too stressful like a badly behaved horse in an adjoining stall making a racket all day.\nGet a strategy in place for feeding any medications. If the medicine is taken by mouth, the method of administering you adopt may depend upon its palatability. It can easily be mixed in with their feed but you need to ensure the horse is eating its entire meal and not wasting it on the stable floor. If it proves so unpalatable that your horse will refuse his meal, you may have little choice but to use a syringe (minus the needle, of course) to squirt it in his mouth.\nEven this has its risks, with the horse possibly getting annoyed and the obvious risk of wastage.\nSome people report success in mixing the medication with peanut butter, which is likely to stick inside their mouth until they clean it up and swallow it.\nRepeated injections are never fun - for horse or owner. Your vet should show you how to inject safely and properly. Make sure they explain this thoroughly: some injections will only be effective if put directly into the muscle, while others are best directly under the skin. Get advice on several injection sites, as a horse will quickly get sick of you picking on one spot.\nA horse being confined for a long time deserves the best stall. The larger the better. Dust must be kept to a minimum and breathing urine fumes all days will do his health no good. It therefore needs to be well ventilated and all dung and urine dealt with regularly throughout the day to minimise the smell. When sorting the ventilation, ensure your sick horse doesn't have to stand in a cold draft all day.\nA regularly cleaned stall will reduce the fly problem and keep your horse in a more pleasing environment. Wounds and dressings are also likely to stay cleaner.\nIf the stable provides a good outlook, all the better. The most interesting vista you can give him is not spectacular views down the valley, but simply other horses. Even watching your day-to-day stable activities will keep him amused for at least part of the day.\nIf confined for a lengthy period of time, your horse may come to view the stable as a prison. It's your job to break up the monotony!\nA stable blanket may make your horse more comfortable, particularly during colder months.\nA sick horse may well spend more time lying down than normal. The bedding you choose is important. A layer of straw is ideal, but it must be thick enough to provide good cushioning underfoot. Change soiled bedding regularly.\nSome people have two stalls in operation for sick horses, moving the animal from one to the other to make clean-ups and maintenance easier.\nNo matter how thick the bedding, there is a real risk of a stabled horse developing capped hocks or other friction wounds. This is because a horse is likely to paw at its bedding, creating low spots. It doesn't appear to matter what base material is used. Sores are still likely to develop, and they are notoriously difficult to control.\nNeoprene hock boots are a good option, but you must install an absorbent layer beneath them otherwise the horse will sweat and irritation will result. They need to be removed very regularly for inspection and the inside layer should be changed.\nPay the horse plenty of attention. Keep him clean and well groomed. If possible, make this part of his daily routine to help him pass the hours. Pay careful attention to his hooves, which can begin to dry out and develop fungal problems.\nClean them out daily and apply a nourishing hoof oil to keep them in good condition.\nDressings will need to be changed regularly. Your vet should leave instructions on how often this should be done, and how to keep them in place.\nAnother threat is laminitis, either through too much load bearing on the horse's good legs or inactivity. A diet too rich in protein will increase the risk.\nPay careful attention to your horse's habits, appearance and behaviour. Has he grown grumpier and, if so, why? Is there new swelling in his legs or joints? Monitor overall condition. If your horse changes his behaviour, try to figure out why. Is he sore? Bored? It could be, of course, that he is feeling better.\nA record of your horse's resting pulse and respiration rate at the time of confinement will give you something to compare to if you suspect other health problems are developing.\nKeep a written record in the stables of medication, feed, and other essentials in case a returning vet asks for information.\nAs the horse improves, carefully supervised sessions outside the stall will be desirable. The vet should give you some guidance. It is essential the horse be kept under control at all times. The last thing you want is him galloping off to aggravate an injury you have been carefully tending for a month.\nIf your horse gets free, make no mistake: they'll be taking to their scrapers and possibly undo all your good work.\nYour horse may well come to consider his stall the equine equivalent of Colditz prison, especially when he starts to feel better. Be ever watchful of break-out attempts when you head into the stall for your daily tasks.\nIs there a risk of him jumping over the stall door? If so, provide additional security.\nLong periods of confinement will not be fun for owner or horse. There is no way around this.\nYou will have to forgive your horse its impatience, and have it yourself by the bucketload!"", 'Just like humans pets can have days where they feel a little lethargic and under the weather, but it is the natural instinct of an animal to try and disguise any signs of illness. They do this in the wild as showing weakness leaves them vulnerable to predators and open to attack. Unfortunately this can make it tricky to determine if your pet is feeling a little unwell or if they are suffering from a more serious illness.\nThere are a number of symptoms and changes in your pets’ appearance, behavior and physical condition that you can look out for. These include but are not limited to:\nAbnormal vocal noises\nBloating of the abdomen\nBlood in the stools or urine\nDecreased energy or activity levels\nDiarrhea and/or vomiting\nDischarge from the nose or eyes\nExcessive scratching or licking of the body\nFoul odor from ears, mouth or skin\nIncreased shedding or bald patches\nLumps or tumors\nReluctance to use stairs\nStraining or an inability to pass urine or stools\nAny of the above symptoms should be checked out by a veterinarian within 24/48 hours.\nSymptoms that require immediate veterinary treatment include:\nBloated or hardened abdomen\nExcessive vomiting or diarrhea\nInability to stand up or urinate\nWhilst a sick pet may require inpatient treatment in care in your veterinary surgery for days or even weeks, you will need to continue providing them with care and compassion to aid their recovery when they come home. This can include administering medication, supporting physical rehabilitation, emotional care, and fulfilling any special dietary requirements.\nMedicating your Pet\nMedicating your pet can be difficult if you are unfamiliar with the best ways in which to administer the drugs. Your veterinarian will explain about the dosages of any prescribed medication and will support you by demonstrating the easiest ways of administering them. Many medications can be incorporated into meal times making the process simpler and less stressful for your pet.\nEnsure that your pet finished the entire course of prescribed medications. Not doing so means that the virus or infection may not be fully eradicated and your pet could become unwell again. Even if your pet looks and acts as if they are at full health, still finish all prescribed medication.\nIf your veterinarian has prescribed special food then be sure to feed your pet separately from any other animals in the house. Adhere strictly to the instructions given as any deviation from the plan, no matter how small, could potentially be harmful to your pet.\nYour pet may need to be kept isolated from any other animals in the house. It will need a quiet environment with food and water nearby as they may be physically weak for some time. Ensure plenty of fresh water is always available.\nYou should also keep young children away from your recovering pet as they may not understand the space that it needs to rehabilitate fully.\nAny changes or worsening of symptoms must immediately be reported to your veterinarian. They could indicate that the medication your pet has been receiving needs urgent review, or they could indicate that your pets’ illness has become more serious. Do not delay in making an appointment and explain the situation fully to the receptionist on duty.']"	['<urn:uuid:d4cd6bb0-f470-41b1-8f77-2a8a718255f8>', '<urn:uuid:9c69f624-4064-4c6e-b18a-9e55be47e429>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:31:27.499155	11	41	2125
43	While researching bacterial growth patterns, I'd like to know how the oxygen requirements compare between the newly discovered plastic-eating bacteria and obligate anaerobes in terms of their relationship with oxygen?	The plastic-eating bacteria Ideonella sakaiensis appears to function in oxygenated environments since it was found in PET-contaminated samples from outdoor sites, while obligate anaerobes cannot tolerate oxygen at all since O2 is very toxic to them and causes formation of harmful compounds like superoxides and free radicals. Obligate anaerobes lack the enzymes needed to convert these toxic compounds to harmless ones and can only grow where oxygen is absent.	"['Newly discovered bacteria can eat plastic bottles\nA team of Japanese scientists has found a species of bacteria that eats the type of plastic found in most disposable water bottles.\nThe discovery, published Thursday in the journal Science, could lead to new methods to manage the more than 50 million tons of this particular type of plastic produced globally each year.\nThe plastic found in water bottles is known as polyethylene terephthalate, or PET. It is also found in polyester clothing, frozen-dinner trays and blister packaging.\n""If you walk down the aisle in Wal-Mart you\'re seeing a lot of PET,"" said Tracy Mincer, who studies plastics in the ocean at the Woods Hole Oceanographic Institution in Massachusetts.\nPart of the appeal of PET is that it is lightweight, colorless and strong. However, it has also been notoriously resistant to being broken down by microbes-what experts call ""biodegradation.""\nPrevious studies had found a few species of fungi can grow on PET, but until now, no one had found any microbes that can eat it.\nTo find the plastic-eating bacterium described in the study, the Japanese research team from Kyoto Institute of Technology and Keio University collected 250 PET-contaminated samples including sediment, soil and wastewater from a plastic bottle recycling site.\nNext they screened the microbes living on the samples to see whether any of them were eating the PET and using it to grow. They originally found a consortium of bugs that appeared to break down a PET film, but they eventually discovered that just one of bacteria species was responsible for the PET degradation. They named it Ideonella sakainesis.\nFurther tests in the lab revealed that it used two enzymes to break down the PET. After adhering to the PET surface, the bacteria secretes one enzyme onto the PET to generate an intermediate chemical. That chemical is then taken up by the cell, where another enzyme breaks it down even further, providing the bacteria with carbon and energy to grow.\nThe researchers report that a community of Ideonella sakaiensis working this way could break down a thin film of PET over the course of six weeks if the temperature were held at a steady 86 degrees Fahrenheit.\nMincer said the study was impressive and did a good job showing that these organisms were eating the plastic pretty well. However, he said it was not immediately clear whether or not it would help keep plastics out of the ocean, for example.\n""When I think it through, I don\'t really know where it gets us,"" he said. ""I don\'t see how microbes degrading plastics is any better than putting plastic bottles in a recycling bin so they can be melted down to make new ones.""\nHe added that the research could make it easier to identify other microbes that might have similar PET-degrading capabilities.\n""This process could be quite common,"" he said. ""Now that we know what we are looking for, we may see these microbes in many areas around the world.""\nMore information: U. T. Bornscheuer. Feeding on plastic, Science (2016). DOI: 10.1126/science.aaf2853\nS. Yoshida et al. A bacterium that degrades and assimilates poly(ethylene terephthalate), Science (2016). DOI: 10.1126/science.aad6359\nJournal information: Science\n©2016 Los Angeles Times\nDistributed by Tribune Content Agency, LLC.', ""Unformatted text preview: Chapter 6: Chapter 6: Microbial Growth Environmental Requirements Environmental Requirements Physical and Chemical Factors Temperature pH Osmotic pressure Oxygen availability Hydrostatic pressure Radiation Temperature Requirements Temperature Requirements\nType Psychrophiles Mesophiles Thermophiles Range 020°C Optimum 15°C 1545°C 2045°C 4570°C 60°C Hyperthermophiles 70120°C 90°C Temperature Requirements Temperature Requirements Temperature Requirements Temperature Requirements Psychrophiles: Cause spoilage of foods while refrigerated Include most human pathogens (body is 37°C) Found in compost heaps Hydrothermal vents, hot springs (above 104°C) Mesophiles: Thermophiles Hyperthermophiles: pH Requirements pH Requirements\nType Acidophiles Neutrophiles Alkalophiles Range pH 1.0 – 5.5 pH 5.5 – 8.0 pH 8.5 – 11.5 Acidotolerant and alkalotolerant microbes can persist for short periods under these conditions, but are unable to reproduce pH Values of Some Environments pH Values of Some Environments\nAcidic 1 2.5 3.54.5 6 8 9 10 Basic 11 ammonia gastric juices vinegar peaches, tomatoes peas, corn, shrimp seawater alkaline lakes/soils soap solutions household pH Requirements pH Requirements Large changes in [H+] can: Disrupt the cytoplasmic membrane Inhibit enzymes and transport proteins Most bacteria are neutrophiles Even in acido or alkalophiles, cytoplasmic pH remains neutral Keeping the Cytoplasmic pH Neutral Keeping the Cytoplasmic pH Neutral Antiport exchange of K+ for H+ in neutrophiles (Na+ for H+ in alkalophiles) Synthesize proteins under acidic conditions Acid shock proteins use ATP to actively transport H+ out of the cell Fermentation produces acids Putrefaction produces ammonias Export wastes to the environment Osmotic Pressure Osmotic Pressure Moderate halophile Require NaCl; found in marine habitats Require NaCl; found in hypersaline habitats like the Great Salt Lake Can grow in salty situations, but grow better without the NaCl; skin bacteria Requires sugars; yeasts & molds Extreme halophiles Osmotolerant Saccharophiles Osmotic Pressure Osmotic Pressure Effect of Osmotic Pressure Effect of Osmotic Pressure Osmotic pressure and water activity are inverse Solution has low water activity = high osmotic pressure Low water activity draws water out of cells via osmosis to dilute external [solute] Water is essential to macromolecule breakdown High water activity may lyse cells by drawing too much water into cell cytoplasm via osmosis Cell wall offers great protection from this Mechanosensitive channels in plasma membrane open Regulating Osmotic Pressure Regulating Osmotic Pressure To prevent growth, reduce water availability inside the cytoplasm Add solutes (sugars, salts) Lyophilization (freezedrying) Dessication (drying) Oxygen Availability Oxygen Availability Major groups based on O2 use and tolerance: Obligate aerobes Facultative anaerobes Obligate anaerobes Aerotolerant anaerobes Microaerophiles Obligate Aerobes Obligate Aerobes Undergo aerobic respiration: O2 required as a terminal electron acceptor No other respiratory paths available In a broth medium: Growth only near top of liquid Limited by the penetration of dissolved O2 from the atmosphere Facultative Anaerobes Facultative Anaerobes In the presence of O2: Undergo aerobic respiration using O2 as terminal electron acceptor Producesmore ATP = more growth capable Undergo anaerobic respiration or fermentation paths less ATP produced, and hence less growth than aerobic In the absence of O2: Facultative Anaerobes Facultative Anaerobes In a broth medium: Growth will be densest at top where O2 is available Growth will occur throughout the depth of the medium via anaerobic respiration or fermentation, or both Obligate Anaerobes Obligate Anaerobes Undergo anaerobic respiration: Ions such as NO3 are required as a terminal electron acceptor No other respiratory paths available In a broth medium: Limited by the penetration of dissolved O from the atmosphere very toxic Growth only near bottom of tube 2 Obligate Anaerobes Obligate Anaerobes O2 presence causes the formation of toxic compounds (superoxides, free radicals) Disrupt cytoplasmic membranes & other cell components Do not have enzymes capable of converting toxic compounds to harmless ones: Superoxide dismutase (SOD): converts superoxides to peroxides Catalase: breaks down hydrogen peroxide to H2O and O2 Peroxidase: converts peroxides to water and NAD+ Aerotolerant Anaerobes Aerotolerant Anaerobes Ignore O2 No toxic effects of O2 due to SOD presence Use fermentation or anaerobic respiration for ATP production In a broth medium: Grow throughout the depth of the medium no denser region at surface Microaerophiles Microaerophiles Too high of O2 content is damaging Low enzymes = inability to adequately prevent damage from toxic species Require 210% O2 concentration Normal atmosphere is 20% Microenvironments with aerobically respiring consortia reduce [O2] to tolerable range Many respiratory pathogens are microaerophiles Oxygen Use/Tolerance Groups Oxygen Use/Tolerance Groups Hydrostatic Pressure Hydrostatic Pressure Barophiles (piezophiles): optimal growth rate where pressure > atmospheric pressure Pressureadapted microbes growing at higher temperatures are mostly Archaea Pressureadapted microbes growing at moderate and cold temperatures are mostly Bacteria Adaptation to pressure is not too extreme Slight genomic differences between pressure adapted vs. normal atmosphere isolates Radiation Ultraviolet Rays Radiation Ultraviolet Rays Damages DNA base pair bindings to produce mutations like thymine dimers Mutations will result indirectly in cell death Inability to replicate chromosome Inability to correctly transcribe mRNA Radiation Ultraviolet Rays Radiation Ultraviolet Rays UV light DNA damage can be repaired: Photoreactivation Blue light energizes a specific enzyme which breaks the thymine dimers Allows normal crosshelix basepairing by hydrogen bonding Dark reactivation Thymine dimers are excised by endonucleases Missing bases in the DNA sequence are replaced by other endonucleases Ionizing Radiation Ionizing Radiation Cause atoms to lose electrons Include Xray and gamma radiation Low levels cause mutation and can indirectly cause cell death High level exposure causes direct cell death Breaks HH bonds, oxidizes double bonds, breaks ring structures, polymerize some molecules Often used as a sterilizing treatment What is Microbial Growth? What is Microbial Growth? Defined as an increase in number Achieved by: Budding Binary Fission Cell duplicates its components, then shares them between 2 daughter cells Daughter cells independent when septum forms betweencell ‘halves’ Bacterial Cellular Growth Cycle Bacterial Cellular Growth Cycle C phase = chromosome replication D phase = delay period Nucleoid partitioning Septation begins Cytokinesis = septation complete Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log vs. Arithmetic Scales Log vs. Arithmetic Scales Conversion to log scale compresses the distance between data points evenly. Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Culture Growth Phases: Lag Log (Exponential) Stationary Death (Decline) Easily measured during growth in liquid media by spectroscopy or densitometry Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Lag phase: Metabolically active but NO increase in number Adaptation: induce enzymes needed; synthesize new ribosomes, ATP, and cofactors; replicate chromosome Repair cellular components, increase in cell size Unbalanced growth rates of synthesis of cell components varies with one another Length of entire phase varies w/ species & environmental conditions Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log (Exponential) phase: Population doubles each generation Generation (doubling) time ranges from 7 min to 20 hr – average is 20 min Growth is asynchronous not all cells divide at exact same time Growth rates are saturable; limited to [celluar enzyme] Balanced growth all cellular constituents made at constant rates to one another Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log (Exponential) phase: Rapid expansion with 20min generation time: also called doubling time Population doubles in number every 20 minutes)\n0 m in 2 0 m in 4 0 m in 1 h r 2 h r 3 h r 4 h r 5 h r 6 h r 1 0 ce lls 2 0 ce lls 4 0 ce lls 8 0 ce lls 6 4 0 ce lls 5 , 1 2 0 ce lls 4 0 , 9 6 0 ce lls 3 2 7 , 6 8 0 ce lls 2 , 6 2 1 , 4 4 0 ce lls Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Stationary phase: Curve horizontal: population growth ceases New cells made at same rate as old cells die (growth rate = death rate) Reasons for stationary phase: Nutrient limitation or O2 limitation Accumulation of toxic wastes Cell density Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Stationary phase: Very common in nature (oligotrophic) Not simply a time when things run out and cell enters a stasis we see changes in: Gene expression: starvation proteins and other proteins, as well as antibiotics, are produced Peptidoglycan crosslinking Nuceloid condensation Endospores formed by certain species Changes make them more resistant to unfavorable conditions Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Death (Decline) phase: Number of viable cells decreases exponentially Constant number of cells die per hour Usually a logarithmic, but not always so clear Bacterial cell death is defined by the inability to grow (reproduce) Death (Decline) phase: Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle\nWhat causes cell death or loss of viability?\nBuild up wastes/toxins and poor environmental conditions for survival Survival of the fittest to reproduce Viable But Not Culturable (VNBC) Temporarily unable to grow under lab conditions but resuscitate upon entry into different environment Programmed Cell Death Certain % of cells that commit suicide to provide nutrients to survivors Bacterial Death/Lossof Viability Bacterial Death/Loss of Viability Continuous Culture Systems Continuous Culture Systems Continuous growth = a constant state of the growth curve; often exponential phase Achieved in a chemostat chamber: Fresh growth medium added at same rate as spent medium and cells are removed Removes pressure of limiting nutrient Also removes toxic waste buildups Growth rate is adjusted by exchange rate Model Chemostat Model Chemostat Population level and generation time are controlled by dilution rate Increase dilution = increase generation time because less of limiting nutrient is available; density kept low most energy used for maintenance, not reproduction Decrease dilution = decrease generation time because little limitation of nutrient; density also increase Measuring Microbial Growth Measuring Microbial Growth By cell number By cell mass Viable vs. Total Direct Microscopic Counts Coulter Cell Counters Viable Counts Calculations and conversions Total cell weight, or by individual chemical (carbon, protein, etc.) Turbidity Direct Microscopic Counts Direct Microscopic Counts PetroffHauser Counting Chamber Accepts fixed volume (0.1 ml) Count number of cells per volume Cannot distinguish live from dead cells Calculations done to determine original cells/ml Coulter Counter Coulter Counter Automated counting device Microbial suspension directed through a small hole the size of an individual cell Change in electrical resistance when cell passes through the hole = 1 cell counted Can’t distinguish live vs. dead cells or cells from small particles of debris Viable Counts Viable Counts Measures colonyforming units rather than cells (due to possible clumping) Seek statistically “countable” plate having 30 300 colonies Serial dilution, plate count, membrane filters Does NOT necessarily count all living bacteria present in the sample just those able to grow under certain conditions given Used with spread and pour plates, also membrane filtration Viable Counts Via Membrane Filtration Viable Counts Via Membrane Filtration Turbidity (Cell Mass) Turbidity (Cell Mass) Done by measuring the amount of light scattered by a cell More mass = more scatter (proportional) Uses a spectrophotometer to measure optical density (OD) of the cells Create standard curves to determine population density based on turbidity Turbidity (Cell Mass) Turbidity (Cell Mass) Low population of cells = low scatter = low OD High population of cells = high scatter = high OD Top scale = % transmittance Bottom scale = optical density Turbidity (Cell Mass) Turbidity (Cell Mass) Create standard curves to determine population density based on turbidity Done in conjunction with viable plate counts initially OD700nm is plotted against the number of viable cell counts taken at the same time points to produce the standard curve Future growth can be estimated from this established relationship Species and environmental conditions must be identical as when the original curve was produced Standard Curve Standard Curve ...\nView Full Document\n- Fall '09\n- Bacterial Culture Growth, Culture Growth Cycle""]"	['<urn:uuid:5f6a8693-a542-4c67-a3cc-30db2218dd2f>', '<urn:uuid:0ef80001-466f-4e36-95aa-42bee82f8185>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T04:31:27.499155	30	69	2466
44	how long do pump toilets last	Macerator toilets typically last for 10 years or more with proper installation and maintenance. Some manufacturers state their devices can last even longer when well installed and regularly cleaned, provided they don't become blocked with inappropriate waste products.	"[""Now this is a really great idea that has made it possible to have a toilet which can be fitted just about anywhere in the home, basement or small cupboard which you are designing to be a cloakroom area.\nThe principle does not have to rely on the traditional system of a large soil pipe which is designed to remove all the waste and paper from a WC, this method ensures that all the waste matter is macerated in a unit which is situated behind the WC or false wall into a size and consistency that will allow it to be pumped into the main sewerage system through a very small pipe. The Saniflo systems allow for ensuite facilities to be added to bedrooms, or a guest room, a shower and WC can be accommodated by the same unit. These are ideal for B&B operations as we all now prefer en-suite facilities. The Sanivite illustrated also allows for a kitchen or utility room to be added to a property which would not have been possible without a traditional waste system. This allows a connection to a sink, washing machine, glass washer, bath, shower, dishwasher or basin. Using 32 mm pipework, it pumps waste water away with a vertical pumping capacity of 5 metres and horizontal discharge of 50 metres. We will be happy to discuss the appropriate Saniflo unit which will be best suited for the room you may be considering.\nMacerator pump buying guide\nA macerator is a waste disposal unit and pump, and is commonly used to remove waste and paper waste when access to the main sewage network is impossible or would cost too much money. Mains toilet are gernerally located above the sewage pipe, as building regulations dictate that waste from the toilet must pass vertically straight to the sewage outlet. This isn’t usually a problem in existing bathrooms, and it may be possible to install a ground floor toilet in such a position where you can get the required access to the mains sewage. However, for en-suite bathrooms, basements, and where an additional cloakroom cannot be fed straight to the main sewer, an alternative method of waste disposal is required.\nA macerator pumps the waste through a smaller outlet pipe than the standard sewage pipe, but this means that it first needs to convert solid waste into slurry by cutting it up with a fine blade and combining it with water. Once the waste has been converted into a pumpable slurry, it is then pumped through the outlet pipes until it reaches the main sewage system. This means that it is possible to pass waste under rooms, or even horizontally, before it reaches the sewage.\nA macerated toilet can prove extremely beneficial in certain circumstances, but you do need to ensure that you have the room, that you buy the right type, and that it is installed and fitted properly. This way, your new toilet should not require any maintenance or servicing, beyond a typical toilet clean, and most models will last for ten years+ of trouble free use.\nTypes of macerator\nThere are, in fact, a number of different types of macerator pump available. Choose the right model according to the type of waste that will pass through it, whether the device needs to pump horizontally, and also whether you want to discretely secure the macerator away so that people cannot see it.\nSome macerators allow you to install your own or existing toilet as part of the installation process. This means that you can use an existing toilet, and convert it to a macerator washroom. Alternatively, you can buy macerator toilets that include the pump already built in. This helps alleviate the problem of having to find somewhere to hide the pump while making sure that you have all of the fixtures and the room that you need.\nIs a macerator suitable for your needs?\nA macerator may not be the best option for your needs. If you have simple and cost-effective access to the main sewage system, then you should consider installing a standard toilet. It could work out cheaper, and it means that you don’t have to worry about whether or not you have installed the new toilet properly.\nYou don’t need planning permission to install this kind of toilet, although if you are installing in a residential property, then you must already have a gravity-fed toilet available for your use as well.\nDifferent types and models of macerator have different features, while different models also come in different sizes and have different sizes of inlet and outlet pipe. Every model has an energy rating, which can determine how well a pump deals with solid waste but will also govern the noise level of the toilet once it is fitted.\nInlets and outlets\nInlet pipes are those that transport the waste from the toilet to the pump, and outlet pipes then take the slurry from the pump to the mains sewage. You need to make sure that the pump you choose has the appropriate number and size of inlet and outlet pipes or face disappointment. Every model of macerator is unique, so you should make sure you choose one appropriate to your circumstances.\nHorizontal and pump rating\nYou should have at least a basic idea of your washroom or WC layout, but try to leave some room for movement. Lower capacity pumps may only be able to transport waste over a short distance, and if you need to transport the waste over several metres and horizontally, then you need to be careful that you choose one that is capable of this. Check the flow rate and the horizontal pump distance, and if in any doubt then err on the side of caution when choosing a pump.\nYou will need some electric and plumbing knowledge if you intend to install your own macerator in your home. The electrical requirements of a macerator are basic, but if you aren’t comfortable with moderate plumbing then you should hire the services of a professional to install your new toilet. Errors in the installation process can greatly reduce the life of the device, and may cause problems once you start using it.\nCheck your waste\nDifferent models of macerator are designed for different purposes, and while a shower model can cope with soapy water, a standard or slimline model may not be able to. Furthermore, home macerators are not designed to cope with waste like sanitary products, while some may become blocked if too much toilet paper is used; especially if the paper is thick. Devices like the Sanibest can accept additional types of waste, but check the manufacturer's details first, otherwise you risk wrecking your toilet.\nWhen measuring for the installation of the pump and the toilet, don’t forget that you will need access to the device. Although manufacturers state that the devices do not require regular maintenance, they can become blocked and damage can occur that prevents them from working properly. If you haven’t allowed ample room to access the pump, then you won’t be able to make any repairs or conduct any maintenance without first removing the pump.\nCleaning a macerator\nThe manufacturers claim that, if this device is properly installed, it does not require any regular maintenance. However, it does require cleaning, just like any other type of toilet. In fact, the process is very much the same, as long as you don’t use disinfectants and bleaches that are too frothy. Also remember to turn the unit off, or remove the fuse, before you clean it out.\nLifetime and servicing\nDifferent companies make different claims regarding the lifespan of their toilets and pumps. Some of the best known manufacturers state that their devices will last for a period of up to 10 years, and it is possible that a well installed device will last even longer than this. Regular cleaning and ensuring that the toilet does not become blocked with sanitary products or other waste products that are difficult to break down will help.\nReduce noise through proper installation\nUse the noise reduction pads that came with the device, do not install it straight on to floorboards or laminate flooring, and if you hear loud and continuous noises from your toilet, then it is time to worth considering having it checked. Unevenly installed devices can also make a noise, while putting the device under too much strain could cause it to break down over time.\nMacerator pumps are extremely convenient, and are especially beneficial to those unable to install a standard, gravity-fed toilet. Whether you are converting a loft or basement, creating an en-suite or a cloakroom, you can use this type of device to avoid having to rip up floors and without having to compromise regarding the placement of the toilet.""]"	['<urn:uuid:401151b3-8733-450e-afce-c82e743c4a06>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	6	38	1470
45	shipping boats distance pollution reaches land	70% of all ship emissions are within 400km of land, affecting the health of communities in coastal and inland regions around the world.	"['Britain and other European governments have been accused of underestimating the health risks from shipping pollution following research which shows that one giant container ship can emit almost the same amount of cancer and asthma-causing chemicals as 50m cars.\nConfidential data from maritime industry insiders based on engine size and the quality of fuel typically used by ships and cars shows that just 15 of the world\'s biggest ships may now emit as much pollution as all the world\'s 760m cars. Low-grade ship bunker fuel (or fuel oil) has up to 2,000 times the sulphur content of diesel fuel used in US and European automobiles.\nPressure is mounting on the UN\'s International Maritime Organisation and the EU to tighten laws governing ship emissions following the decision by the US government last week to impose a strict 230-mile buffer zone along the entire US coast, a move that is expected to be followed by Canada.\nThe setting up of a low emission shipping zone follows US academic research which showed that pollution from the world\'s 90,000 cargo ships leads to 60,000 deaths a year and costs up to $330bn per year in health costs from lung and heart diseases. The US Environmental Protection Agency estimates the buffer zone, which could be in place by next year, will save more than 8,000 lives a year with new air quality standards cutting sulphur in fuel by 98%, particulate matter by 85% and nitrogen oxide emissions by 80%.\nThe new study by the Danish government\'s environmental agency adds to this picture. It suggests that shipping emissions cost the Danish health service almost £5bn a year, mainly treating cancers and heart problems. A previous study estimated that 1,000 Danish people die prematurely each year because of shipping pollution. No comprehensive research has been carried out on the effects on UK coastal communities, but the number of deaths is expected to be much higher.\nEurope, which has some of the busiest shipping lanes in the world, has dramatically cleaned up sulphur and nitrogen emissions from land-based transport in the past 20 years but has resisted imposing tight laws on the shipping industry, even though the technology exists to remove emissions. Cars driving 15,000km a year emit approximately 101 grammes of sulphur oxide gases (or SOx) in that time. The world\'s largest ships\' diesel engines which typically operate for about 280 days a year generate roughly 5,200 tonnes of SOx.\nThe EU plans only two low-emission marine zones which should come into force in the English channel and Baltic sea after 2015. However, both are less stringent than the proposed US zone, and neither seeks to limit deadly particulate emissions.\nShipping emissions have escalated in the past 15 years as China has emerged as the world\'s manufacturing capital. A new breed of intercontinental container ship has been developed which is extremely cost-efficient. However, it uses diesel engines as powerful as land-based power stations but with the lowest quality fuel.\n""Ship pollution affects the health of communities in coastal and inland regions around the world, yet pollution from ships remains one of the least regulated parts of our global transportation system,"" said James Corbett, professor of marine policy at the University of Delaware, one of the authors of the report which helped persuade the US government to act.\nToday a spokesman for the UK government\'s Maritime and Coastguard Agency accepted there were major gaps in the legislation. ""Issues of particulate matter remain a concern. They need to be addressed and we look forward to working with the international community,"" said environment policy director Jonathan Simpson.\n""Europe needs a low emission zone right around its coasts, similar to the US, if we are to meet health and environmental objectives,"" said Crister Agrena of the Air Pollution and Climate Secretariat in Gothenburg, one of Europe\'s leading air quality organisations.\n""It is unacceptable that shipping remains one of the most polluting industries in the world. The UK must take a lead in cleaning up emissions,"" said Simon Birkett, spokesman for the Campaign for Clean Air in London. ""Other countries are planning radical action to achieve massive health and other savings but the UK is strangely inactive.""\nThe calculations of ship and car pollution are based on the world\'s largest 85,790KW ships\' diesel engines which operate about 280 days a year generating roughly 5,200 tonnes of SOx a year, compared with diesel and petrol cars which drive 15,000km a year and emit approximately 101gm of SO2/SoX a year.\nShipping by numbers\nThe world\'s biggest container ships have 109,000 horsepower engines which weigh 2,300 tons.\nEach ship expects to operate 24hrs a day for about 280 days a year\nThere are 90,000 ocean-going cargo ships\nShipping is responsible for 18-30% of all the world\'s nitrogen oxide (NOx) pollution and 9% of the global sulphur oxide (SOx) pollution.\nOne large ship can generate about 5,000 tonnes of sulphur oxide (SOx) pollution in a year\n70% of all ship emissions are within 400km of land.\n85% of all ship pollution is in the northern hemisphere.\nShipping is responsible for 3.5% to 4% of all climate change emissions\n• This article was amended on 25 August 2015 to correct the number of deaths per year attributed to pollution from the world\'s 90,000 cargo ships.']"	['<urn:uuid:22ade1a9-e563-46ce-a863-e1fe00c1ec07>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	6	23	878
46	wrapping stuffed animals bottles techniques compared recycling materials alternatives reduce waste	For stuffed animals, wrap with tissue paper, tape ends, add cellophane, and tie with wire ribbon like a candy wrapper. For bottles, layer tissue paper, cellophane, and wrapping paper, then cinch at neck and add ribbon. However, to be more environmentally conscious, you can use alternative materials like newspapers, magazines, brown paper bags, or old maps. You can even repurpose plastic cups and bottles for wrapping. When buying new wrapping paper, choose recycled options. These eco-friendly alternatives help reduce the massive holiday waste production, as just reusing small amounts of ribbon could save thousands of miles of material.	"['It’s not just you or your lack of artistry – wrapping presents can be difficult. Tape is sticky. Scissors are unwieldy. Paper is fragile. On top of all that, some gifts don’t fit the box-shaped mold. Throw it in a bag and call it a day? Sure, you could, but the excitement and mystique is gone. Instead, take these instructions and make something beautiful. Or, at least try.\nFor stuffed animals:\nLay your fluffy fellow down on a sheet of tissue paper and tape where the ends meet. Put another sheet of tissue on top of that, taping at the opposite end. Cover with cellophane and tie wire ribbon around the ends, in the style of a giant piece of hard candy. Snip the ribbon for a polished finish.\nWrap tissue paper or cellophane around the bottle. Then, roll a layer of wrapping paper around that. Tape the ends together. Fold the paper over the bottom of the bottle and tape this as well, securing it in place. Cut the top layer of wrapping paper at the top so that it’s shorter than the tissue. Cinch the paper at the neck of the bottle and fluff it out with your hands. Use wire-trimmed ribbon to tie a double knot around the neck, and add a custom bow for flair!\nFor cylinder shapes:\nMaybe you have a round tin of cookies or a few jar candles to wrap. Cover the curved side of the object with paper until the ends meet. Tape it in place. On the bottom, take one of the points of the paper and tuck it against the bottom of the object. Start creating a pleat by folding various points of the paper until everything meets in the center. Repeat on the other end. Your finished product should look a bit like a pinwheel on each side.\nAt a loss for t-shirt boxes? Place a piece of cardboard underneath soft, bulky garments. Wrap from the base up to create crisp corners that won’t tear or wrinkle. Top with a decorative bow!\nFor small items:\nCreate an envelope-style puffy box to place your items inside, and fill with tissue paper as you see fit. This step-by-step video is a key example on how to get this effect:\nFor items with protruding shapes:\nKids’ toys can provide the biggest challenge with their plastic packaging and odd edges. Here’s a solution: Create a gift sack by cutting a large square of wrapping paper and placing the item in the center. Pull up opposite corners until all four edges meet at the center. Tuck in the sides, and cinch at the top, wrapping a ribbon around the excess paper.\nTake an old mason jar and tape an empty toilet paper roll to the bottom. Fill the rest of the jar with colorful candy. Fold your money inside the cardboard roll and replace the lid. Your loved ones will never know there’s a surprise gift inside until they open the jar for a sweet treat!\nDon’t forget the name tags! Grab a blank notecard, cut off two corners, and punch a hole in the side. Lace a ribbon through it and spell out the recipient’s name in pretty, bold cursive. Decorate to your heart’s content! Attach to your gifts and avoid any confusion come Christmas morning.', ""10 Eco-friendly Tips for the Holidays\nIt’s time for some eco-friendly tips for the upcoming holidays. Christmas is considered the most wasteful time of the year. Did you know that the U.S. alone produces an extra 25 million tons of garbage during the holidays from Thanksgiving to New Year?\nExtra waste includes the gift wrappers, ribbons, Christmas cards, decorations, and other stuff that people throw away after the holidays. If only people would learn to reuse or recycle these things, our planet will be much better off. For example, if families just reuse two feet of ribbon, think about the 38,000 miles of ribbon saved. If individuals re-use gift wraps, such as gift bags and wrapping paper, imagine how much paper will be saved.\nHave you ever thought about how you can personally help reduce Christmas' carbon footprint? By taking simple steps to make the holidays more eco-friendly, it doesn’t mean that Christmas will be less special or memorable. It would actually make a good impact on your pocket, the environment, and the people around you.\nIf you would like to take part in this earth-friendly movement, then you’ll want to take a look at these awesome tips you can adopt during the holidays. Check them out below:\nChristmas Card Recycling\nTip #1: Turn them into handmade Christmas cards\nDon’t throw away the Christmas cards you receive. Recycle them by reusing the decorative part of the cards. Just buy a pack of thin card and adhesive. Cut off the back part of the card and mount the graphic design on a blank card. Fold in half and write your Christmas greetings inside.\nMake the cards more personal and fun by adding some glitter or star decals. This is a great activity for children who enjoy doing arts and crafts. See an example on the right of a pretty handmade recycled Christmas card.\nTip #2: Turn them to Christmas decors\nThere are many recycled Christmas card crafts you can do that makes good Christmas decors. Martha Stewart has tutorials on making Christmas ornamental balls and holiday card monogram labels.\nYou can also simply make Christmas card garlands. Just cut the Christmas cards into triangular pieces, tape them to a string and hang them up.\nRecycled Christmas Card into Decors\nTip #3: Reuse envelopes\nFor every Christmas card, you get an envelope. You can also reuse these when you need to send your handmade Christmas cards. Just buy special “sticker labels” to cover up the old details.\nTip #4: Send digital Christmas cards or e-cards\nTo reduce the quanity of Christmas cards sent each year, you can opt to go digital. Design your own greeting cards on the computer and send it to people that you regularly communicate with via email. Limit the paper greeting cards you send to only your family and closest friends.\nRecycling Christmas Gift Wrapping Paper\nWrapping paper is often used once and thrown away. But if you’re really careful when opening gifts, you can reuse the wrapping paper again. Gently remove the adhesive tape and fold or roll the paper neatly before keeping it.\nHere are some ways to recycle gift wrappers:\nTip #5 Reuse them for wrapping gifts again\nYou can iron out the wrinkled part of wrapper before using. Or just choose portions of the wrapping paper that have no wrinkles. You can fold on the old folds to be able to use more of the paper.\nTip #6 Reuse them for your craft projects\nHere are some craft ideas you can use for old gift wrappers:\n- Making paper beads which you can make into a mosaic of any pattern you like or make them into necklace or bracelet.\n- Use them for embellishing greeting cards, posters and bookmarks.\n- Use them as matting for picture frames.\n- Cut them into squares or circles and use them as box labels.\n- They’re great for making origami.\nTip #7 Make them into handmade paper\nYou can pretty much use any paper for making handmade paper. Then you can use what you make as stationery or wrapping paper once again. The colorful design of the wrapping paper will add extra nice details to handmade paper.\nHow to Make Recycled Handmade Paper\nTip #8 Reuse them to make money or cash envelopes\nIf you like giving cash as gifts for the holidays, you can use old Christmas wrapping paper to make colorful little envelopes. You can also use the envelopes for your own personal use like when budgeting your finances.\nHow to Make Money Envelopes\nTip #9 Use other recyclable materials for wrapping your Christmas presents\nYou can use old newspaper, colorful magazines, brown paper bag, and old maps as gift wrapper. You can even use plastic cups and bottles to wrap gifts. If you’re buying new wrapping paper, look out for those made of recycled paper.\nTip #10 Reuse ribbons and bows\nWhen you receive gifts, be sure to save the ribbons and bows. You can use them again for embellishing presents or for your craft projects like bookmarks or scrapbooks.\nHow to make paper cups gift boxes\nHopefully, you can use some of these tips to make Christmas celebration this year more eco-friendly.""]"	['<urn:uuid:3b302c04-6cb6-4dee-bcda-4fd091264ad5>', '<urn:uuid:e7b4259b-12a9-45a0-b648-e19fc879d567>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	11	98	1421
47	curious about school shields significance what does the harp symbol mean in gregorys academy emblem	The harp in Gregory the Great Academy's crest represents multiple elements: St. Gregory the Great's patronage of music, the poetic education through ancient bards' epics, and the strong musical tradition and formation of the academy. It also refers to Thomas Moore's poem 'The Minstrel Boy,' which serves as the school song.	['The St. Gregory’s Academy Crest is an image well-known and well-beloved by alumni and friends of the academy. Since the school’s institution in 1993, the crest has served as St. Gregory’s defining symbol and has been worn over the hearts of the Highlanders with pride on and off the rugby pitch.\nNow, as the spirit and traditions of St. Gregory’s Academy advances to a new stage of existence, a new crest has been created to signify this development. The original crest was designed by Mr. Alan Hicks, the founding Headmaster of the academy. He conceived it as an emblem depicting the chief principles of education and the Catholic life. Now that St. Gregory’s Academy has had 19 good years, and is blessed to have the opportunity to evolve into a new, independent institution hearkening from a particular history and a particular character, so too arises an opportunity to represent that history and character in a new shield. The story of the Gregory the Great shield’s genesis is here presented.\nMr. Hicks primarily wanted St. Gregory’s Academy to have for its motto, Bonum, Verum, Pulchrum (the Good, the True, the Beautiful). This motto expresses that the ultimate end of education (and human life) is to love the good, to know the truth, and to have joy in the beautiful. Mr. Hicks intended that this motto accompany a shield depicting images of the Sacred Heart of Jesus and the Immaculate Heart of Mary, to indicate their love for us; and an image of the Holy Spirit, to indicate that ultimate wisdom cannot be gained by our efforts alone. Mr. Hicks approached a graphic artist with these concepts and commissioned some images. After a period of mixing and matching and revision, Mr. Hicks was satisfied and the St. Gregory’s Academy crest was born: a shield with a bend gules, having the crowned Hearts of Jesus and Mary in the upper field and the Holy Spirit in the form of a haloed dove in the shield sinister, and a motto-banner emblazoned with Bonum, Verum, Pulchrum.\nThe crest of the new school, Gregory the Great Academy, has been designed to incorporate the imagery and symbolism of the old crest, and add to them images and symbols that reflect the character that the school spirit has organically formed over the years. This new crest bears four fields divided per saltire (the St. Andrew’s Cross). This mode of division symbolizes the Scottish traditions of the school and the Highlanders. In chief are depicted the crowned Sacred Hearts of Jesus and Mary, and in base, the sword superimposed by an open book. In fess is charged the old image of the Holy Spirit and a new image of a harp on a field gules. The motto, set in a banner below, remains the same as in the old crest.\nThe Sacred Hearts and the Holy Spirit represent the same principles of Divine love and inspiration, as designed by Mr. Hicks. The sword and harp now act together as icons of Thomas Moore’s poem, “The Minstrel Boy,” which has served as the school song since the early years.\n“One sword at least thy rights shall guard,\nOne faithful harp shall praise thee!”\nThe Harp also invokes St. Gregory the Great’s patronage of music, the poetic education the school upholds through the epics of the ancient bards, and finally, the strong musical tradition and formation of the academy. The sword, turned to create a cross-hilt, represents the chivalric traditions of knighthood and manly courtesy, while being a weapon by which the knight upholds justice. In relation to the open book, it stands as well for the defense of the Faith. The open book, considered alone, is a symbol of both academia and the Divine Liturgy of the Church.']	['<urn:uuid:c46064a8-d470-45de-82ea-ec838bf57e79>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	15	51	628
48	How common are backhoe accidents with cables, and what safety measures exist to prevent them?	Backhoe accidents with telecom cables are extremely common - in Sweden alone, about 8,000 cables are cut annually, costing around $30 million in repairs. In the US, there were 64,000 incidents of damaged telecom cables in 2007, resulting in hundreds of millions in repair costs annually. To prevent these accidents, several safety measures exist, including the One Call project in the US where companies can check cable locations before digging. Additionally, Subsurface Utility Engineering (SUE) practices provide methods to locate and identify underground utilities through various quality levels, from records research to physical exposure via vacuum excavation. When properly implemented, these preventive measures can reduce accidents by up to 70%, as demonstrated in Denmark.	"['If you suddenly find yourself without an Internet connection, there’s a good chance that somewhere a team of construction workers just uttered a collective “uh-oh” because their backhoe dug up a telecom cable. Oops. It turns out that this problem is so common that it is costing millions upon millions of dollars in repairs every year. Backhoes, drilling and digging are serious cable killers. A SMALL EXAMPLE TO START: SWEDEN In Sweden, admittedly quite a small country, around 8,000 telecom cables are cut off by backhoes every year. In fact, a construction crew managed to do this very thing not far from the Pingdom office a few months ago. The repair costs alone for these accidents are roughly $30 or so million per year in Sweden (an estimate from a recent Computer Sweden article). If we have that many incidents in a small country like Sweden, how common are they in a big country like the United States? Let’s have a look. THE US BILLION-DOLLAR FAIL Finding recent exact data for the US proved to be tricky. A Wired article from 2006 gave us some numbers to start with. In 2004, there were 675,000 excavation incidents where cables and pipes of various kinds were damaged in the US (often referred to as “underground utility damages”). More than a quarter of those, 27.5%, were telecom-related. That would mean 185,625 cases where US telecom cables were damaged in 2004. If the cost of repairing a cable is similar in Sweden and the US, telecom cable repairs may have been around $700 million in 2004. However, there are reports that things have improved. In 2007 there were an estimated 256,000 underground utility damages in the US, a lot less than in 2004. If a quarter of those were telecom-related, that leaves us with around 64,000 incidents where underground telecom cables were damaged. Even after such a big improvement, the yearly repair costs still end up in the hundreds of millions of dollars. Over a longer period of time, say 5-10 years, the repair costs will count in the billions. AND WHAT ABOUT WORLDWIDE? We have just looked at Sweden and the US here. Now imagine the costs worldwide. Those will easily amount to several billions of dollars and hundreds of thousands of telecom cable breaks due to our careless digging. Every year. And a ton of downtime for various networks, we suspect. PREVENTIVE MEASURES In the US there is a project called One Call where both companies and individuals can call in to find out what cabling exists where they are planning to dig. An interesting option has been explored in Denmark, where a central, nationwide website provides information about the underground cabling that exists in various areas. This website can be consulted by people and companies preparing an excavation. This has cut (no pun intended) the number of accidents by 70% which is a huge improvement. Sweden has just launched a similar initiative, so we’ll see if it works as well here. Now we just hope that that the operator of that huge backhoe that recently drove by doesn’t get any funny ideas… this is article is from http://royal.pingdom.com/\nUtility Locating and Marking Best Practices\n2.14 Sub Surface Utility Engineering Best Practice Statement: When applied properly during the design phase, Subsurface Utility Engineering (SUE) provides significant cost and damage-avoidance benefits and the opportunity to correct inaccuracies in existing facility records.1 - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf Practice Description: In certain cases and environments, it may be difficult or impossible to determine the locations of all utilities and/or impediments with sufficient accuracy to avoid damage or delay during construction. In these cases, SUE is applied during the design phase to locate, identify, and characterize all existing utility infrastructure (and other relevant nonutility features) found within a given project/area. SUE is applied in a structured manner in accordance with practices and quality levels found in ASCE 38-02 “Standard Guideline for the Collection and Depiction of Existing Subsurface Utility Data.” The project owner dictates the required quality levels as well as the amount of effort expended by the SUE provider on each. Although the standard is more detailed and comprehensive, the following is a brief summary of the quality levels defined therein: QL-D involves utility records research and interviews with knowledgeable utility personnel. QL-C involves surface survey and identifying and recording aboveground features of subsurface utilities, such as manholes, valves, and hydrants. QL-B involves application of “surface geophysical methods,” such as EM-based locating instruments, GPR, radar tomography, metal detectors, and optical instruments, to gather and record approximate horizontal (and, in some cases, vertical) positional data. QL-A involves physical exposure via “SOFT-DIGGING” (VACUUM EXCAVATION or hand-digging) and provides precise horizontal and vertical positional data. SUE results are integrated into the design process, in which design engineers use the information to create construction plans that accommodate existing infrastructure, thereby reducing the overall risk of conflicts and/or damage.2 - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf References: U.S. Department of Transportation—FHWA (12/1999). Cost Savings on rojects Utilizing Subsurface Utility Engineering. Pub. No. FHWA-IF-00-014 U.S. Department of Transportation—FHWA (3/2001). Subsurface Utility Engineering: Enhancing Construction Activities. Pub. No. FHWA-IF-01-011 ASCE 38-02 Standard Guideline for the Collection and Depiction of Existing Subsurface Utility Data Pennsylvania state law - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf 1. TR-2007-02: Modification to statement approved by the CGA Board on August 24, 2007 2. TR-2004-03: Amendment approved by the CGA Board on March 4, 2005 - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf US Utility Potholing & Air Excavation uses provides quality level A and quality level B Sub Surface Utility Data.\nPotholing saves time and money.\nPortable vacuum excavators have been a regular sight on job sites for years. These unique units were originally used to clean septic tanks and car wash pits and to remove slurry from horizontal directional drilling projects. Now contractors are discovering that these machines have a wide range of uses on the job site, from potholing for utilities to cleaning valve boxes. By Greg Ehm Construction Equipment News Letter http://www.constructionequipment.com/potholing-utilities-saves-time-and-money July 28, 2008 Portable vacuum excavators have been a regular sight on job sites for years. These unique units were originally used to clean septic tanks and car wash pits and to remove slurry from horizontal directional drilling projects. Now contractors are discovering that these machines have a wide range of uses on the job site, from potholing for utilities to cleaning valve boxes. Vacuum excavators are self-contained units that use pressurized air or water to displace soil and create a dry or wet spoil. The displaced dry or wet spoil is removed from the area through a hose using high-velocity suction and stored in a holding tank on the vacuum. Vacuum excavators can be mounted to a trailer or the back of a truck and range in size from 100 to 1,200 gallons of capacity. Since vacuum excavators use low-pressure air or water to remove spoil, they are perfect for potholing or identifying existing utilities during underground construction projects. ""Damaging existing utilities can be costly in terms of project downtime and potential contractor fines,"" says Dave Gasmovic, president of McLaughlin Boring Systems in Greenville, S.C. ""The low-pressure water and air will not damage existing utilities like a backhoe, compact excavator or shovel. In fact, the air and water move around the existing utilities, giving the operator a clear view."" Operators can select the amount of air or water pressure appropriate for the utility. A lower pressure of 1,500 psi should be used for gas and fiber lines in order to not damage the line coating. A higher pressure can be used for water lines. Line Exposure While locators are becoming more accurate, it\'s still important to see exactly where the line or pipe is located. Contractors are not allowed to dig in the safe zone, which may be from 18 inches up to 3 feet from either side of the marked line. The required distance varies by state. Contractors are only allowed to dig by hand or use a non-destructive method like vacuum excavators in the safe zone. Using a vacuum excavator instead of a shovel has advantages. A shovel against a water pipe is non-destructive, but on a fiber optic line a shovel can be as destructive as a backhoe, especially in hard ground conditions. ""A lot of cable has been installed using horizontal directional drills (HDD) rather than trenchers, so you don\'t have the old-fashioned ditch line like in the past,"" says Gasmovic. When lines are installed using a trencher or backhoe, a lighter material like sand is placed around the line. As a contractor digs, the ground gets softer. This indicates the line is in close proximity. Lines installed using HDD don\'t disturb the ground or leave a ditch line, so the ground is the same hardness and it is difficult to know if you are getting close to the line or cable. Since the ground may be hard, you can easily cut a cable line with a shovel. Using a vacuum with air or water at a non-damaging pressure will safely expose the line. Selecting The Right Unit Vacuum excavators come in all sizes and options, so it\'s important to select a unit that will best fit your intended use. Water-based units typically dig faster through a wide variety of spoil types and reduce the volume of the excavated material. These units move more displaced wet spoil into a holding tank than an air system. However, the displaced spoil is wet and cannot be returned to the site immediately without drying. While spoil from air systems can be directly returned to the site, these systems do not cut as well in hard ground conditions, such as clay. ""I encourage contractors to look for a unit with a good-quality vacuum blower, the heart of the vacuum,"" say Gasmovic. ""They should also select a tank that has the capacity to hold a half-day\'s or day\'s worth of spoil. This will reduce the number of trips you need to make to dump the holding tank."" If you are working in areas with cobble rock, then a unit with a 4-inch hose and 1,025-cfm blower unit will be more productive. Cobble soils will require a larger blower to effectively remove the spoil. The larger diameter of 4-inch hose will help reduce the potential for clogging. In areas without rocks, a 575-cfm system and 3-inch hose will suffice. The blower size also affects the amount of engine power required — a larger blower will increase the cost of the unit. Gasmovic recommends that contractors pay special attention to the filtration system and select a system that will filter the spoil and avoid clogging. Finally, be sure to select a strong trailer frame that will support the weight of the unit and a full tank of spoil. There are a number of options available: controls that allow the contractor to reverse the flow of the vacuum to blow the spoil back into the hole; booms that support the weight of the hose, placing less effort on the operator; combo units that include a jetter to clean sewers and remove the resulting trash; and automatic tank clean-out systems and auxiliary hydraulic systems that allow the contractor to run a concrete saw or breaker off the unit. The Payoff Taking the extra steps to pothole may seem like an added expense or more time, but Gasmovic stresses that safety is important. ""Hitting a gas line with a backhoe, trencher or HDD could be catastrophic. A water line hit could put a hospital out of business,"" says Gasmovic. ""The cost of shutting down a project for a day is sure to exceed the cost for a $3,000 locator and a little extra time."" In this photo Chris is starting a Pothole to verify existing utilities with the use of air / vacuum excavation. This is an example of our employee Mario digging a utility test hole or utility pothole to collect Sub Surface Engineering Data using air vacuum excavation method. We collect SUE Data through these methods at US Utility Potholing & Air Excavation.', 'A branch of ENGINEERING PRACTICE that involves managing certain risks associated with: Utility Mapping at appropriate ASCE Quality Levels Utility Coordination Utility relocation and adjustment through conflict matrix resolution Utility relocation design and cost estimates Communication of utility data to concerned parties Implementation of Utility Accommodation Policies and utility design\nCI/ASCE Standard 38-02 The American Society of Civil Engineers (ASCE) has developed a National Consensus Standard, CI/ASCE 38-02, titled “Standard Guideline for the Collection and Depiction of Existing Subsurface Utility Data”. This National Consensus Standard (NCS) is used by courts and lawyers, along with contractual instruments, to assist in both defining a professional’s standard of care and level of responsibility.\nASCE Standard – Quality Levels QL-B QL-A QL-C QL-D\nASCE Quality Level D (QL-D) Existing Utility Records Involves the use of existing utility records, permits, plans to depict the ‘approximate’ horizontal position of underground utilities.\nASCE Quality Level C (QL-C) Survey of Visible Features Involves surveying visible above ground utility facilities to assist with determining ‘approximate’ horizontal position of underground utilities. Used with QL-D. Manholes Power poles Hydrants\nASCE Quality Level B (QL-B) Utility Designating Involves the use of geophysical prospecting equipment to determine the existence and horizontal position of underground utilities. Paint markings Flags\nDesignating Approach: QA/QC All data is checked and cross- checked for accuracy – Records research vs. designating file Field sketch Designating data point & numbering system Utility # of points Comments/ Notes/ Location W120East side of road W24West side of road W312NB lane\nASCE Quality Level A (QL-A) Utility Locating Involves the use of non-destructive digging equipment at critical points to determine the precise horizontal and vertical position of underground utilities, as well as the type, size, material, and other characteristics.\nLocating Approach Permitting Minimal disturbance Saw cut in pavement Air vacuum excavation Centerline of facility Permanent reference points Proper restoration Compaction in 6” lifts Perma-Patch\nLocating Approach: QA/QC Test Hole Data Collection – 2 photographs of every test hole for permanent record Final QA/QC of all Subsurface Utility Engineering work is done by registered Professional Engineer\nWhen to Use SUE? Type of Project Piping and Facility Design (storm, sanitary, water) Site and Plant Design Roadway / Bridge const. Widening / Intersection Improvements Signal Replacement Location of Project Urban / Suburban Rural\nWhen to Use SUE? Utilities involved Major or Minor Conflicts with utilities Compensable interest Limited Access R/W Accuracy required ASCE Quality Level? Level of Risk\nWhat are the Risks? Project delays Damage to utilities Safety of workers Safety of public Redesign costs Higher bids Change orders Extra work orders Construction claims Higher insurance / financing / construction costs Detours for travelling public Negative publicity\nHow Do We Use SUE? Commitment to avoid unnecessary utility relocations Communicate with Utilities early & often Records Research – QLD Utility Designating – QLB Topographic Survey – QLC Review of QLB, QLC & QLD Preliminary Design Utility Conflict Analysis Utility Locating – QLA Design Alternatives\nGeometric Alignment Change grade Shift alignment Widen on one side of the road Shift ramps or driveways\nDesign Alternatives Structure Design Alter footing / piling designs Provide alternative foundations Modify bridge layout Shift or modify retaining walls\nDesign Alternatives Drainage Design Dual trunk line in lieu of single trunk Shift ditches Shift structure locations (inlets, pipes, manholes) Open vs. closed system Modify side slopes or ditch slopes\nHow Do We Use SUE? Project Example Roadway Reconstruction Added Travel Lanes Drainage Improvements\nHow Do We Use SUE? CONFLICT 4 - G CONFLICT 3 - G CONFLICT 6 - G CONFLICT 5 - T CONFLICT 1 - G CONFLICT 2 - G\nHow Do We Use SUE? Utility Conflict Analysis / Matrix Identify every utility conflict with the proposed design Conflict Number Station and Offset (BL) Utility Identified Conflict Test Hole Needed Test Hole No. Utility Impact with Cost (""As- designed"") Recommended Resolution *Benefit of Resolution\nHow Do We Use SUE? Utility Conflict Analysis / Matrix Conflict Number Station and Offset (BL) Utility Identified Conflict Test Hole Needed Test Hole No. Utility Impact with Cost (""As- designed"") Recommended Resolution *Benefit of Resolution 1 43+78 25’ L 2” G Storm pipe Yes#107 Adjust gas main $5K Adjust drain pipes Avoid impact 2 43+75 27’ L 16” G Storm pipe Yes#108 Adjust gas main $20K Adjust drain pipes Avoid major impact 3 44+24 10’ L 16” G Storm pipe Yes#109 Adjust gas main $20K Adjust drain pipes Avoid major impact 4 44+25 8’ R 20” G Storm pipe Yes#142 Adjust gas main $20K Remove pipe Avoid major impact\nWhy Use SUE? Make Informed Design Decisions Designers require accurate utility information, including constructability of multi-phase projects. Avoid Using Unreliable Underground Utility Information Avoid uncertainty and second guessing where a utility may be located.\nWhy Use SUE? Avoid Costly Conflicts / Utility Relocations By knowing the exact horizontal and vertical locations of underground utilities, costly conflicts and utility relocations may be avoided, along with not having to depend upon the utilities to relocate before construction can occur. Savings and Safety Inaccurate information can result in costly conflicts, utility damage, construction delays, service disruptions, redesign, claims, and even injuries and loss of life.\nRelative Cost Savings & Benefits Purdue Study Commissioned by FHWA 71 projects studied in 4 states (VA, NC, OH, TX) Projects valued at >$1B SUE was < 0.5% of construction costs Both QLB & QLA performed\nRelative Cost Savings & Benefits Purdue Study Cost savings of $4.62 per $1.00 spent on Subsurface Utility Engineering Largest return on investment was $208/$1 1.9% savings on construction costs Quantitative costs only (Qualitative costs not included) SUE is a viable practice that reduces project costs related to the risks associated with underground utilities If used properly it could result in a minimum savings of $1 billion per year\nSUE Current Practice Owner and Engineer Responsibility In an increasing number of states, lawyers and courts are using guidelines, agency policies, and contractual instruments to define and hold professionals accountable for their standards of care. It is an Engineer’s responsibility, on behalf of an Owner, to utilize all available resources and methods to gather and provide the most accurate information possible.']"	['<urn:uuid:2c52c253-66da-4e18-a666-896baf004532>', '<urn:uuid:197e138b-45d2-4fdd-89a6-877706faeea1>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	15	114	3029
49	Who needs to be present at an IEP meeting?	The required attendees at an IEP meeting include: parents, general education teachers, special education teachers, someone who can interpret evaluation results, a school system representative, and when appropriate, the child themselves. The school may also invite experts with special knowledge about the child, and transition service representatives when needed.	['What is an IEP?\nAn IEP, short for Individualized Education Program, is a comprehensive and tailored document designed to support students with disabilities in their academic pursuits. This personalized roadmap is created through a collaborative effort involving teachers, parents or guardians, school administrators, and, when applicable, specialists such as therapists or counselors. The main purpose of an IEP is to ensure that students with disabilities receive appropriate and tailored educational services and support to help them succeed academically and make progress toward their educational goals. The IEP serves as a blueprint for your child’s educational journey.\nThe IEP is legally mandated under the Individuals with Disabilities Education Act (IDEA) in the United States. This law ensures that eligible students with disabilities have access to a free appropriate public education (FAPE) and that their educational needs are met through the development and implementation of an IEP. Other countries may have similar laws or educational support systems in place for students with disabilities.\nWhat does it mean when a student has an IEP?\nWhen a student has an IEP, it means they are eligible to receive special education. To receive Special Education, you must have an IEP. If you have an IEP, you are receiving Special Education.\nIEP refers to the actual document that details what type and frequency of Special Education Supports and Related Services they will receive.\nIEPs are defined by IDEA. Your local school district develops and implements an IEP. The IEP age range is 3-21. Prior to age 3, children get an IFSP.\nTo get an IEP, your school team of evaluators must have evaluated your child and found them to be eligible under one of the IDEA 14 Categories of Disability.\nWhether or not you refer to your child as ‘disabled’ is up to you and your child. I do not think ‘disabled’ is a derogatory term, nor does much of the disability community.\nThe IEP Process\nIt’s a common myth that IEPs give students an advantage over students without one. This is not true. It is to level the playing field and address any disabilities that are affecting your child’s ability to access and benefit from their education.\nIf your child has been evaluated and found eligible for an IEP, that means they have been identified as a child with a disability. And that disability is interfering with their education.\nFor an IEP, there is General Education and Special Education. With an IEP, you can receive Special Education in the General Education setting. Receiving Special Education does not mean you forfeit regular education.\nWho writes an IEP?\nIt is developed by an IEP team. Before an IEP can be written, your child must be eligible for special education. Per IDEA, a multidisciplinary team must determine that your child is a child with a disability and your child requires special education and IEP-related services to benefit from the general education program.\nIDEA defines who must attend an IEP meeting as:\n- Parents – As a parent, you have valuable information and insights about your child’s needs and strengths, as well as ideas to enhance his education.\n- General Education Teacher/s – They share information on your child’s performance versus the expectations in the classroom.\n- Special Education Teacher/s – The teacher has the experience and training in educating kids with disabilities. They also work with other teachers in planning accommodations.\n- Results Interpreter – The person who interprets your child’s evaluation results that can help in planning for the appropriate instructional program.\n- School System Representative/LEA – The school system representative knows special education services well and is authorized to commit resources.\n- Knowledgeable Experts – people with special expertise or knowledge about your kid invited by the school district or by you.\n- Transition Service Agency Representative – When related services are discussed, representatives from transition service agencies may be invited.\n- The Child – When discussing transition, and whenever appropriate, the child may also be invited. Yes, it is considered a ‘best practice’ to include them (depending on their age).\nWhat must be included in my child’s IEP?\nPresent Levels of Performance\nParents, teachers, and school staff tasked to evaluate the child present information on the child’s needs and strengths. It also includes comments on how your child is doing within the general education classroom, interventions, and any data that has been collected.\nSetting Goals and Objectives\nOnce the team has a clear understanding of your child’s strengths and needs, the next step is to set measurable, achievable, and yet ambitious goals. These goals encompass both academic and functional aspects, with the intention of fostering growth and progress. By setting realistic objectives, we empower our students to reach their full potential. The goals are based on the discussions, data-based, and documentation in the current educational performance levels. The goals are not meant to help the child achieve above grade level or to maintain skills.=\nSpecialized Support and Services\nPerhaps one of the most valuable aspects of an IEP is its provision of specialized services and support. These services can range from specialized instruction to various related services like speech therapy, occupational therapy, or counseling. Moreover, the IEP may include the use of assistive technology and accommodations tailored to the student’s unique needs. These adaptations ensure that students can access the curriculum on an equal footing with their non-disabled peers. In addition to the above, an IEP includes:\n- The limit of your child’s participation with kids without disability in regular school and class activities.\n- When will the services be given, where, how often, and for how long?\n- The necessary transition services (by age 14/16 or the initial IEP to take effect on the child’s 14/16th birthday).\n- Strategies and supports for behavioral management if the behavior affects the child’s or other children’s learning\n- Language requirements concerning the IEP in case the child has limited English proficiency or mastery\n- Communication needs\n- Assistive technology services or devices needed to receive FAPE\n- Needed classroom accommodations in general education\nInclusion and General Education\nAn essential consideration in the IEP process is determining the extent to which the student will participate in the general education setting. Inclusion is highly encouraged whenever feasible, as it not only fosters a sense of belonging but also allows for valuable social interactions and opportunities to learn from peers.\nUpon completion of the IEP, the team decides on the implementation. The school district must provide the FAPE (Free Appropriate Public Education) under an LRE (Least Restrictive Environment. The IEP team will consider the most appropriate for both in educating your kid together with children without disability.\nThe team identifies the services your child requires to reach the objectives and goals, as well as the delivery. General classroom education is preferable for most kids. However, there are various options available. These include special day classes.\nProgress Monitoring and Flexibility\nAn IEP is a living document, subject to regular reviews and updates. Educators continuously monitor your progress and make necessary adjustments to ensure the plan remains effective and relevant. Flexibility is key in responding to the evolving needs of your child and adapting strategies accordingly. This progress should be reported and shared timely but you can request this data at any point if you have questions about your child’s progress.\nTransitions and Beyond\nFor students approaching the threshold of adulthood, the IEP also includes a crucial transition plan. This plan outlines the steps for transitioning from school to post-secondary life, including further education, vocational training, employment opportunities, and essential life skills development.\nYour first IEP is written once your child had been found eligible. At that point, you will come back together and rewrite the IEP every year. The annual meeting is something you’ll hear parents talk about a lot if you are in this space. Per IDEA, a child’s IEP is reviewed and updated at least annually. As stated above, IDEA clearly defines who must attend an IEP meeting. Anyone on the team can request a meeting to review or make changes at any time.\nNo, you don’t have to think about or communicate with your teachers daily. But if you only think about or act on your child’s IEP once a year at renewal time, I can almost guarantee you that it will be a stressful experience.\nEngage and stay involved all year long.\nAlison Whiteley has been a special education teacher for over 15 years, spending most of her time working with elementary students and families. After graduating from the University of Colorado with a Bachelor of Arts in Special Education and Psychology, she continued her education with a Masters in Reading from Walden University. In addition, she has achieved endorsements supporting Early Childhood Special Education and Diverse Learners which she uses to help identify needs across all learners.\nMs. Whiteley is trained in Wilson Reading System and Yoshimoto Orton-Gillingham. She believes all students can learn to read and be successful. She has served as a Special Education Coach and Mentor to fellow specialists and teachers, facilitated the creation of her elementary school’s Response to Intervention/Multi-Tiered Systems of Support (RTI/MTSS) process and helped parents through the Special Education process as IEP Coach for parents. In 2013 she completed the National Boards of Professional Teaching in Exceptional Needs with recertification in 2022. Her areas of expertise involve working with students with learning disabilities, supporting stakeholders moving through the special education process, and helping parents and teachers understand what they can do to support struggling learners in the public school settings.\nShe is a founding member of the Colorado Reading League and a member of the International Dyslexia Association in Colorado. Alison has two greyhounds and two nephews who keep her busy outside of school. She is the CEO of Toad-ally Exceptional Learners at http://www.toad-allyexceptionallearners.com. Alison is a valuable source of information to support teachers and parents, although she is not a lawyer and does not give legal advice. Her services support families through the IEP process and how they can be an equal member of the team through positive interactions. She focuses on collaboration and using tools to take IEPs to the new level of helping students achieve.']	['<urn:uuid:d2f765e2-114c-4ec9-82cb-406692241b56>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	9	49	1699
50	what core values financial industry promotes	The financial industry promotes fundamental ethics principles based on the International Ethics Standards Board for Accountants Code of Ethics. These core values include integrity to be straightforward and honest in all professional relationships, objectivity to avoid bias and conflicts of interest, professional competence to maintain knowledge and provide competent services, confidentiality to respect private information, and professional behavior to comply with relevant laws and regulations.	['The importance of moral courage: more than just a principle?\nMoral courage is important when putting ethics into action but how should this be recognised in the ICAS Code of Ethics?\nIn November 2015, ICAS launched its business ethics initiative, The Power of One, calling on every CA to place ethical leadership at the heart of their professional responsibilities.\nICAS established the Ethics Board to add further prominence to the issue of business ethics and undertook to take a strong leadership role in the advancement and application of ethics.\nIn conjunction with the launch of The Power of One, ICAS reviewed the five fundamental ethics principles contained within the ICAS Code of Ethics (the Code). These principles are:\n- Integrity: to be straightforward and honest in all professional and business relationships.\n- Objectivity: to not allow bias, conflict of interest, or undue influence of others to override professional or business judgements.\n- Professional competence and due care: to maintain professional knowledge and skill at the level required to ensure that a client or employer receives competent, professional services based on current developments in practice, legislation and techniques, and act diligently and in accordance with applicable technical and professional standards.\n- Confidentiality: to respect the confidentiality of information acquired as a result of professional and business relationships and, therefore, not disclose any such information to third parties without proper and specific authority, unless there is a legal or professional right or duty to disclose, nor use the information for the personal advantage of the professional accountant or third parties.\n- Professional behaviour: to comply with relevant laws and regulations, and avoid any action that discredits the profession.\nThe Code is largely based on the International Ethics Standards Board for Accountants (IESBA) Code of Ethics, which provides the foundation for most professional accounting bodies worldwide.\nProposals for change\nThe conclusions of the ICAS review were set out in a discussion paper, The Five Fundamental Ethics Principles: Time for Evaluation? This discussion paper sets out some proposed amendments to the current principles, including:\n- the introduction of a new, separate principle of “moral courage”;\n- a greater focus on personal responsibility, ethical leadership and public interest responsibilities within the existing principles;\n- highlighting the need for “ethical judgements”; and\n- the adoption of a proposal by IESBA relating to the principle of professional behaviour.\nThe focus on moral courage was the most significant of these.\nThe courage to act morally\nIn recent years, unethical behaviour has often been at the root of scandals involving both individuals and corporations. Michael Woodford, former CEO at Olympus, hit the headlines when he demonstrated moral courage, blowing the whistle on unethical practices at the company.\nHowever, many CAs face pressures and difficult professional decisions on a daily basis. The consequences of an inappropriate decision are potentially ruinous, not only for the individual, but also for their organisation.\nIn many situations, doing the right thing may not be easy; CAs need courage to act morally.\nFor accountants, ethical or moral courage is required to counter the fears that they face, such as damage to their reputation and livelihood.\nThe additional principle of moral courage aims to recognise that it can be difficult to stand up against others and take an ethical stance, and it requires an inner resolve to do so.\nResponses and conclusion\nThe discussion paper prompted responses from several large UK accountancy firms, professional bodies, a regulator and individuals.\nThere was support for the concept of moral courage, although most respondents did not support its introduction as an additional fundamental ethics principle.\nOne professional body described moral courage as “an enabler”, and noted that the need for it applies across all existing principles. One accountancy firm who responded to the paper noted that moral courage was an integral part of the existing framework, and they would support it receiving more prominence.\nICAS wishes to thank all of the respondents to the discussion paper for their valuable input. On the basis of the feedback received, it is not intended to unilaterally add a new fundamental ethics principle of moral courage to the ICAS Code of Ethics.\nHowever, it is envisaged that the concept of moral courage will be introduced into the ICAS Code of Ethics in order to reflect the ICAS belief that moral courage is an underpinning qualitative characteristic required of a CA.\nConsideration is currently being given as to where this content should most appropriately be included.\nRead the full version of this article in the February 2017 edition of CA magazine.']	['<urn:uuid:fef8f0fe-21b1-4ff7-af34-36b25f89a56c>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	6	65	757
51	How does storing paintings differ from storing silver items?	Paintings and silver items require different storage approaches. Silver items should be stored in cool, dry areas to prevent tarnishing from oils and hydrogen sulphide compounds in the air. For paintings, large ones should be stored off the floor, preferably hanging up or standing upright in closets - never flat under beds as this stresses the stretchers and canvas. Smaller paintings can be stored upright back to back and face to face on separated shelves with acid-free foam core dividers to prevent scratching.	"['Across the country, many people who attend my antiques appraisal events are shocked to hear about some of the little-known methods used in major museums to preserve and protect precious art and antiques.\nWhile museums make a long-term commitment to preserving and protecting objects in their care to educate the public, most of us are equally committed to keeping our family heirlooms and keepsakes in good condition in order to retain their value.\nSome of the most common ways an object can be harmed include: pests and other insects, pollutants (dust, mold, etc.), temperature and humidity fluctuations, lights or sunshine, and oils from the human touch.\nFor instance, the oils on your hands and the hydrogen sulphide compounds in the air cause silver to tarnish and will leave a permanent mark on your valuable silver pieces.\nSigns that read ""Do not touch"" seem extreme but necessary when objects are on display in museums. When it comes to collectibles that we live with on a daily basis, it is a good idea to handle with care and handle only occasionally.\nSo, if you must handle an object, don\'t handle it too often. Remember, the oils and small dust particles on your hands can cause permanent damage to your heirlooms and aging treasures.\nIt is best to store your private collections in an area of your home where it is cool and dry.\nAttics (too hot with poor ventilation), basements (too damp), foyers (where the front door opens and closes often are bad because temperature changes are frequent), kitchens (too many cooking odors and too much heat), bathrooms and laundry rooms (too much moisture and possible mold) are not the best places for art or antiques.\nImproper climate conditions can stimulate mold growth and cause objects to mildew, dry out and crack. Never use harsh chemicals or abrasive pads to clean antique objects.\nHanging a framed print in a sunny window, storing objects in acidic cardboard boxes and over-cleaning your antiques can damage your pieces forever.\nSunlight is the first culprit that damages most works of art. Heat is a close second.\nPainted objects, prints and textiles should not be placed in sunny areas of your home as they are sensitive to light and will be damaged in a few short months.\nThere are few options to repair sun damage and fading once it happens. However, you can prevent heat from damaging your antiques. One of the hottest places where you display your collectibles is your china cabinet.\nThe glass doors act like a greenhouse and your objects are baking inside. Be sure to open those doors and let your objects get some good air flow every three months or so.\nSpray the rag, not the Renoir\nCleaning a framed work of art, such as a print, seems straightforward.\nHowever, there is a right way and a wrong way to clean it. Spray the rag first. Do not spray the cleaner directly onto the glass as the chemical could drip in between the glass and the work of art and damage it.\nBeware of bugs\nInsects are monsters, killers. They carry bacteria and they will eat and not stop eating until they have damaged your antique - particularly wooden ones - beyond recognition.\nYou may stop an infestation by wrapping a small wooden object in acid free tissue paper and placing the object in a freezer. The bugs will die off in the cold.\nAlso, bugs love dark spaces and close quarters.\nAn easy way to protect your antiques from insects is to clean around your objects regularly, don\'t eat food near your collectibles and use insect traps when necessary.\nCertain types of art and antiques need special types of care.\nBe diligent and handle your antiques carefully and you\'ll enjoy them for years to come.\nPh.D. antiques appraiser, author, and award-winning TV personality, Dr. Lori presents appraisal events to audiences worldwide.', 'Preservation tips for paintings and prints\nArt & Antiques by Dr. Lori\nPeople love their collections. No matter the type of object — cookie jars, military memorabilia, fine art posters — collectors want to add to an existing collection, display their assembled objects, and learn more about their cherished treasures. One of the most important and interesting aspects of collecting is preserving art, antiques, and collectibles for the long term. Many collections include family heirlooms or assembled collectibles that will be handed down to younger generations so preserving a collection is very important. Here are some key points about how to protect, preserve, and enjoy your collections.\nLight is the real problem when it comes to the preservation of paintings and works on paper. UV protection using UV-filtered or opaque materials helps prevent fading and light damage. One of the best ways to preserve fine art is investing in quality framing. For an oil on canvas painting, a frame will protect both the stretcher and the canvas as well as give a finished look to the painting once it is on the wall. Paintings exist best when kept out of direct sunlight and hung away from elements that may spark temperature and humidity changes like heaters, radiators, and air conditioners.\nPrints require a different type of protection when it comes to framing and display. Prints and other works on paper like antique maps, historic documents, and the like should be matted and framed under glass using materials that are free of acid. Acid free materials like mats and storage boxes should have a pH level of 7.0 or greater and the adhesives used in the framing of a fine art print should be pH neutral to protect fragile works on paper. Some acid free materials are made free of lignin, which can produce acid and darken paper, this process is known as acid burning or tanning. Avoid acid burning or tanning whenever possible.\nSome of the most critical damage that happens to art and antiques happens when objects are stored. Although it is little known, significant damage can occur during storage. When you first put an antique object away in storage, everything is fine but over time, changes in temperature and humidity can occur. When no one is looking, other affects may take place which will impact the condition and value of an antique or collection. It is important to store objects in archival boxes intended for a certain type and size of collectible. Physical support is necessary for fragile objects and storage containers like archival boxes need to be constructed to stand the test of time. What you put into a storage box like acid free tissue paper along with an antique is as important as the storage container. One size does not fit all when it comes to archival storage.\nLarge paintings should be stored off the floor, preferably hanging up even in storage locations. If there is no room for a hanging storage solution, then store large paintings standing upright in a closet or storage area. While it may seem like a convenient place to store paintings, never lay paintings flat, face up under a bed. This will put stress on the stretchers and the canvas itself. Smaller paintings may be stored upright back to back and face to face on separated shelves. Use acid free foam core dividers to prevent the wire from the back of one painting from scratching the frame or canvas on the front of another painting positioned next to it. There are specific techniques to protecting art, a good rule of thumb is to handle with care, display works of art away from direct sunlight and store works in areas where temperature and humidity fluctuations are minimal.\nDr. Lori Verderame is the award-winning Ph.D. antiques appraiser on History channel’s #1 hit show about the world’s oldest treasure hunt, The Curse of Oak Island. For more information, visit www.DrLoriV.com and www.YouTube.com/DrLoriV.']"	['<urn:uuid:3eeb8572-d410-42c9-afd1-c1b6a8e44a3f>', '<urn:uuid:1892276b-6ff4-4ac9-b00c-da15321a2cb7>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T04:31:27.499155	9	83	1308
52	breast milk storage transport hospital versus home temperature control methods differences	When transporting breast milk to the hospital, it must be kept in a cold environment using a cooler. For home storage, there are multiple options including refrigeration and freezing. The milk should be stored in clean, sealed containers and labeled with date and time. Fresh expressed milk that won't be used within 4-8 days should be frozen, and it's important not to mix milk expressed at different times in the same container. Both storage methods require proper temperature control, but hospital transport specifically requires the use of a cooling system.	['Meeting the nutritional requirements of preterm baby represents a continuing challenge facing doctors, nurses, as well as the baby’s family. Nutrition during early life is now recognized not only as a key determinant for immediate neonatal survival, growth and mental development during infancy, but also as a major conditioning factor for long-term health. Preterm babies very often need admission to neonatal intensive care unit after birth. This make breastfeeding a little more challenging. However, most mothers are able to express their breast milk to feed their little ones.\nExpressed breast milk (EBM)\nBreast milk is the feed of choice for preterm baby. Besides providing nutrients needed for growth and development, human milk also contains numerous immune-protective components that protect the premature babies from infection. However, most of the preterm babies are unable to feed directly from mother’s breast at the initial stage due to their sickness or immaturity of sucking-swallowing reflex. Therefore it is very important for a mother to express her breast milk so that her baby can still receive breast milk using alternative methods such as orogastric tube feeding, cup feeding etc.\nEBM expression and storage\nClean your hands before expressing breast milk. Express breast milk in a clean container. Write the date and time on the EBM container each time after expressing. Do not mix EBM at different time in a same container. You can consult local healthcare staff in wards/ clinics regarding technique of expressing, storing and transporting EBM.\nAfter expressing, EBM should be kept in the fridge/ freezer if not sending to hospital immediately. You can store your EBM in a number of ways as follow:\n|Freezer compartment inside refrigerator||\n|Freezer section of refrigerator with separate door||\nStoring EBM in the fridge\nHow to transport expressed breast milk to hospital\nWhen transporting EBM to hospital, keep the EBM in a cold environment such as by using a cooler.\nExample of a cooler\nHuman milk fortification (HMF)\nPreterm babies often need HMF to be added in EBM. The aim of fortification is to raise the concentrations of specific nutrients in relation to energy to such levels that nutrients needs are met whenever energy needs are met.\nTotal parenteral nutrition (TPN)\nPreterm babies are born with limited nutrient stores, and take time to establish enteral feeding. Most babies < 32 weeks or < 1500g will require TPN at the initial stage while establishing enteral feeding. TPN is a type of nutritional formula giving intravenously to the baby. It contains nutrients such as glucose, protein, fat, vitamin and dietary minerals. TPN is an important source of energy and nutrients when the baby is still not on full enteral feed.\nPreterm growth is usually monitored by checking the weight, head circumference and length regularly. The growth parameters will then be plotted on a growth chart to monitor the trend of the growth. Higher growth velocity on the neonatal intensive care unit that was similar to intrauterine growth rates predicts better neurological outcome.\nFeeding the preterm babies after discharge\nThe very preterm infant at the time of discharge represents a nutritional challenge to healthcare providers as well as infant’s family on the decision on what type of milk should be given after discharge and on the need to continue to supplement mother’s milk. The World Health Organization, American Pediatric Society, and European Society for Paediatric Gastroenterogy Hepatology and Nutrition (ESPGHAN) recommend mothers’ own milk for nutrition of infants for the first 6 months of life and beyond. Close monitoring of feeding and growth is recommended after hospital discharge.\n- B. Koletzko et al. Nutritional Care of Preterm Infants.2014;110.\n- C. Agostoni et al. Enteral Nutrient Supply for Preterm Infants: Commentary From the European Society for Paediatric Gastroenterology, Hepatology, and Nutrition Committee on Nutrition. JPGN. 2010;50(1).\n|Last Reviewed||:||28 Oktober 2015|\n|Writer||:||Dr. Leong jen Jen|\n|Accreditor||:||Dr. Neoh Siew Hong|', 'Breast Milk Storage\nA guideline on breast milk storage and how to prepare your stored breast milk.\nIf your babies are spending some time in the NICU or you’re having\nproblems with breastfeeding, expressing your breast milk is an option.\nMost hospitals will have a place for you to refrigerate or freeze your\nmilk until you need it while you are there. Talk this over with your\nhospital for their procedures.\nBreast Milk Storage Guidelines\nWash your hands before pumping and handling your breast milk.\n- Have your containers with seal-able tops ready, which have been sterilized. One option is to use\nspecial bags which made for storing breast milk.\n- You can freeze or\nrefrigerate your milk, you will need to label it with the date and time\nso you know which supply should be used first and which has expired.\nBreast Milk Storage Tip:\nTry not to waste your breast milk. In each bag or container store\nenough for one feed, have some smaller amounts ready as well in case\nyour baby starts feeding more, like they do during growth spurts.\nHow Long can you Store Breast Milk for?\nThis is a guide only as it really depends on each individual fridge\nand freezer and how well it works. A deep freezer for instances, would\nbe more effective as it’s opened less often.\n- In the Fridge: Keep the milk at the back of the fridge and most agree refrigerated breast milk can be stored for 4 – 8 days.\n- In the Freezer:\nOnly freeze milk you don’t plan on using in the next 4 – 8 days, also\nkeeping it at the back. Breast milk can be stored for up to 4 months in\nSomething to note, freezing breast milk destroys some of the antibodies but in comparison to formula it’s still much healthier.\n- Thawed Breast Milk:\nMilk which has been frozen then thawed in the fridge can be used within\n24 hours. If you have thawed using a warm water bath then this milk\nmust be used straight away or back to the fridge for a maximum of 4\n- Partially Fed Milk:\nDiscard milk that’s been partially eaten. If you have warmed a bottle\nup which has not been fed on at all, you can keep this in the fridge for\nup to 4 hours.\nBreast Milk Storage Tip: Milk which has been frozen then thawed out cannot be refrozen.\nHow to Heat up Stored Breast Milk\nYou can thaw it in the refrigerator overnight or by swirling it in a\nwarm water bath, be sure not to use hot water as it can destroy some of\nthe benefits that breast milk provides. Using a microwave is a No-No,\nthis can also be damaging to breast milk as well as the possibility of\nhot spots, which can scald a baby’s mouth.\nMore Info at Having Twins\nTwin Pregnancy Q & A\nCaring for Twins\nYou Are Here...\nBreastfeeding Premature Babies\nBreast Milk Storage\nLike This Page?']	['<urn:uuid:5b7e2868-7bb9-40ab-bae3-b9cc19be9b1e>', '<urn:uuid:1cd74a17-9555-4ca0-9353-5f96fe85fe88>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-13T04:31:27.499155	11	90	1140
53	difference tortious interference contract business relationship	The main difference is that tortious interference with contract requires an existing, valid contract between parties, while tortious interference with business relationship does not require a written or signed contract. The latter applies when someone interferes with a business relationship where parties are still negotiating or working out details, but the harm done is considered the same in both cases.	['The term “tortious interference” covers two kinds of economic injury for which one person can sue another for damages in a civil law case.\nThe first, tortious interference with contract, occurs when a person who is not a party to a contract says or does something to cause one of the parties to breach the contract. For example, suppose that A and B have a contract that states that A will sell B 1,500 widgets, and that B will pay A $1.00 per widget, or $1,500. Two days before the sale is completed, however, C tells A that B is cheating her by only paying $1.00 per widget when B knows they really sell for $10.00 or more. A breaches the contract with B by refusing to sell B the widgets unless B pays $10.00 per widget.\nIn this case, B may try to sue C for tortious interference with a contract, because C’s actions made A decide to breach the contract between A and B.\nIn most U.S. states, B (the plaintiff) can only hold C (the defendant) liable for tortious interference with a contract under the following circumstances:\n- a contract exists between the plaintiff and someone else (here, A).\n- the defendant knows that the contract exists.\n- the defendant does or purposely does not do something in order to get one of the parties to the contract (here, A) to breach the contract, or to disrupt A’s ability to hold up his end of the contract.\n- because of what the defendant did or didn’t do, A actually did breach the contract or was actually unable to hold up his end of the contract.\n- the plaintiff, B, suffered damages caused by A’s breach of, or inability to perform as promised in, the contract.\nThe second kind of economic injury is tortious interference with a business relationship. This tort is similar to tortious interference with a contract, except that an actual contract does not have to exist. In a case for tortious interference with a business relationship, it is enough that C interfered with the relationship between A and B. A and B do not have to have a written, signed contract between them in order for either one to hold C liable for any damages they suffered when C induced one of them to break off or change the business relationship.\nOften the difference between tortious interference with a business relationship and tortious interference with a contract depends on whether or not a contract actually exists. For instance, in the example above, suppose that C talked A out of the widget deal while A was negotiating with B, but before either one had written down or signed a contract. In this case, B most likely has a case for tortious interference with business relationships if it seems highly likely that A and B would have made a widget-sales contract, but for C’s meddling. “Tortious interference with a business relationship” extends the original tort of inducement to breach of contract by recognizing that the harm done is the same whether or not the parties have already signed a contract or are still working out the details.', 'Wrongful or Tortious Interference with Contracts\nLocate a Local Business Lawyer\nWhat Is Wrongful or Tortious Interference with Contracts?\nWrongful or tortious interference with contracts refers to a situation in which a third-party intentionally causes a contracting party to commit a breach of contract. This may be accomplished through inducement or by disrupting a party’s ability to perform their contractual obligations. The purpose of tortious interference laws is to allow parties the freedom to contract with one another and fulfill their contractual obligations without third-party meddling.\nThe third-party interferer, called the “tortfeasor,” is usually an individual that was not party to the contract and is interfering for his own financial gain. For this reason, the plaintiff’s remedy will be in tort law, rather than contract law. The plaintiff (the non-breaching party to the contract) will have to show that the tortfeasor acted intentionally, both with regards to his own actions and the resulting contractual breach (meaning he must have known about the contractual relationship and caused the breach anyway).\nThis cause of action goes by many other names, including:\n- Tortious interference with contractual rights\n- Intentional interference with contractual relations\n- Unlawful interference with contractual relations\n- Interference with a contractual relationship\n- Interference with a contract\n- Inducement of a breach of contract\n- Procurement of a breach of contract\nA related cause of action is “tortious interference with business relations,” which does not require a valid contract to be in existence at the time of the interference.\nWhat Are the Elements Required to Prove Wrongful Interference?\nThe specific elements for proving tortious interference may vary from jurisdiction to jurisdiction. Usually, a plaintiff will be required to show:\n- A valid contract or contractual relationship existed\n- The tortfeasor had knowledge of this contract or relationship\n- The tortfeasor intended to induce one of the contracting parties to commit breach\n- The tortfeasor was not otherwise privileged or authorized to induce breach\n- The contract was in fact breached\n- The plaintiff suffered specific economic damage as a result\nWhat Are the Remedies for Wrongful Interference?\nRemedies available to plaintiffs in a tortious interference case include both legal damages and equitable relief.\n- Legal damages can encompass economic losses, such as lost profits, as well as punitive damages, which are awarded to plaintiffs as a way of punishing malicious wrongdoers.\n- Equitable relief may consist of an injunctive order preventing the tortfeasor from benefitting from their interference.\nDo I Need a Lawyer for Tortious Interference with Contract?\nIf you have suffered losses due to an interference with your contract rights, you should contact a business lawyer immediately. Your attorney will be able to advise you on your rights to economic recovery or equitable relief. Failure to bring a claim in a timely manner can limit or even prevent your ability to recover your losses. Likewise, if you are being accused of tortious interference, a lawyer can help defend you if a lawsuit is filed in court.\nTort Law in other States:\nConsult a Lawyer - Present Your Case Now!\nLast Modified: 10-03-2014 09:18 AM PDT\nDid you find this article informative?\nLink to this page']	['<urn:uuid:ef5e41ca-f1c2-4266-bd9e-6fe11102e252>', '<urn:uuid:f2b67441-358a-4012-826a-0a4321756e70>']	factoid	direct	short-search-query	similar-to-document	three-doc	novice	2025-05-13T04:31:27.499155	6	60	1057
54	As an immigration lawyer, how does filing fake papers affect visa status?	If convicted of filing false documents with intent to defraud, it is considered a crime of moral turpitude which can lead to deportation if committed within 5 years of admission to the US. It also makes the person inadmissible, meaning they cannot re-enter the US, become citizens, or get green cards if they leave the country.	['From time to time, you may be required to file documents with a government office in California. The state expects the documents you file are truthful and not falsified or forged. If you are arrested for filing false documents with any state agency, you will face prosecution under California Penal Code 115 PC. Violating PC 115 is a felony crime and attracts severe consequences. Common cases under this statute include filing of forged or false title deeds of trusts or real estates.\nAny document filed with a government office must be truthful, failure to which you face criminal prosecution. When these allegations are leveled against you, the California Criminal Lawyer Group can help fight the allegations for the best outcome.\nDefining PC 115 According to the Law\nThe best way of defining the crime of filing forged or false documents, as documented under PC 115, is by understanding the various elements of the offense. The prosecutor is expected to prove these crime elements before the court can convict you. If there is a doubt in the prosecutor’s evidence, the charges against you can be dropped or dismissed. These elements are:\nYou presented a forged or false document in a government office for registration or recording, or you facilitated the filing of a forged or false document for the same purpose in a government office\nAs you were doing this, you were aware the papers were forged or false\nAnd if the documents were authentic, they may have been filed, registered or recorded in a public office\nTo fully understand these elements, you should know the meaning of various phrases as used in the statute.\nPresented or Facilitated for the Filing of the Document\nYou can face charges and a conviction for filing forged or false documents when you:\nPresent a document in a public office to be registered, recorded or filed\nCause or facilitate for the documents to be registered, filed or recorded\nOffering or Presenting a document for these purposes in a public office means you purposely brought the document to the offices. If the document is identified as a forgery and not filed, you are still guilty of the offense and could face charges. However, you can get charged with a lesser crime of trying to present a false or forged document.\nFor instance, you went to the office of your local county clerk to register a property deed. The document is notarized and signed, transferring property ownership from your father to yourself. Unfortunately, the clerk is your dad’s friend and knows that the two have had a strained relationship for years.\nBecause of these suspicions, the clerk engages the notary because their seal is used in the property deed. The notary clarifies that he never signed nor notarized the title deed and that the signature and seal are forged. After receiving this information, the clerk declines to register the deed but calls the police instead.\nIn such a case, you could face charges for violating Penal Code 115 and get convicted to jail, prison, or pay hefty fines. Similarly, when you cause forged or false documents to be registered, filed, or recorded, you can be charged with violating PC 115. This means you may have asked another person to file it on your behalf.\nFor instance, you run a fraud scheme where you target immigrants faced with a possible foreclosure of their homes. These immigrants are not native English speakers and have a challenge understanding or communicating in English. You take advantage of this and promise them that you can negotiate with the bank on their behalf to avoid the foreclosure. The immigrants trust you, and you present them with documents authorizing you to negotiate on their behalf. Unknown to the immigrants, the documents are authorizing the transfer of their home’s ownership to you.\nAs you carry out your scheme, you involve your secretary, who acts as a notary. You and your secretary go to the immigrant’s house to notarize the deed for the victim signs. Afterward, your secretary, together with the victim, goes to the government office to file the documents.\nAt the government office, it is discovered the transfer is fraudulent because the documents are false. Even if you never accompanied the victim and your secretary to the government office, you are guilty of violating PC 115. You will be convicted because you facilitated or caused the filing of the forged or false documents.\nForged or False Document\nGovernment offices deal with various documents. This means that any document you file in a government office must be genuine and authentic, or it will attract charges of violating PC 115. The law prohibits the filing of forged or false documents if one of the below is the case:\nThe documents’ details are such that public offices rely on them according to the law\nThe documents details affect the rights of third parties in a manner that is legally contemplated by providing the documents for filing.\nFor instance, you fight and break up with your girlfriend over infidelity allegations. You accuse each other of violence, and you ask the court to issue a restraining order against your ex-girlfriend. When you receive the order, you are told you will be responsible for taking it to the marshal that will serve it to her.\nBefore submitting the order, you alter it to read that your girlfriend’s new boyfriend should also keep away from you and your ex-girlfriend. The officer suspects the alterations you made, and instead, they alert the police. After your arrest, you could be charged with filing a false document.\nAnother incidence where you may find yourself charged with violating PC 115 is if you forge the hours of your probation. For instance, you had a prior conviction where you were sentenced to community service for several hours. You are referred to the community center and collude with one of the supervisors to fill that you have completed the hours when you have not. You then present the completed form to the probation officer who discovers your deception. This can make you face charges for filing a forged or false document to a government official or office.\nKnew the Documents Were Forged or False\nAs earlier discussed, a critical element of this offense is the knowledge that the documents were forged or false. If the prosecutor is unable to prove this element, it is unlikely you will be convicted. For instance, looking at the earlier example of foreclosure fraud, your secretary may not be very conversant with English. She believes the documents you send her to file are authentic.\nIf she is charged with violating PC 115, her lawyer can challenge the charges arguing she was not aware the documents were forged or falsified. On the other hand, you could face prosecution for these charges.\nPenalties for Filing Forged or False Documents\nIn California, when you file a forged or false document, the prosecution could charge you with a felony offense. If convicted, possible penalties include:\nFormal probation for one to five years\nState imprisonment for sixteen, twenty-four, or thirty-six months\nA fine not exceeding $10,000 instead of or in addition to the prison sentence\nNote that for each falsified or forged document you file, you could face a separate charge. This happens even in cases where the filed documents are related closely. Additionally, you will not qualify for probation as a form of punishment if:\nYou hold a previous record for PC 115 violation\nYou get sentenced for more multiple counts for PC 115 violation, and because of your actions, the victim loses $100,000 or more\nSentence Enhancement in Penal Code 115 Violations\nWhen convicted of this offense, a few aspects of the offense can enhance the penalties you receive. If the forged or false documents affect the title to a mortgage on one family dwelling with not more than four residential units, you will face additional fine not exceeding $75,000.\nIf, by your actions, a victim loses $65,000 or more and the prosecution proves you planned to cause the loss, you can face additional imprisonment of one to four years. The additional sentence is served consecutively with the original sentence.\nIf all the below are true, you will likely face the aggravated white-collar crime enhancement sentence in California. These are:\nDuring the same criminal trial, you get convicted of at least two felonies on embezzlement or fraud, with one offense being a violation of PC 115\nThe felonies you are charged with are part of consistent and related criminal activities\nYou committed the crimes against two different victims or one victim but two or more felonies on different occasions\nYour offense or actions made the victim lose over $100,000\nWhen your sentence is increased under the white-collar crime enhancement, you may face an additional state imprisonment sentence between one and five years. Similarly, you can be fined instead of getting additional imprisonment.\nThe fine amount can be as high as $500,000, and in some cases, you could get fined double the crime’s value. In most instances, the court will look at the greater of the two and sentence you to that.\nConsequences to your Immigration Status\nAny crime that involves moral turpitude has adverse consequences on your immigration status. If the prosecution proves your intention to defraud, your crime is said to entail moral turpitude. As a result, you may get deported, and more specifically:\nIf in five years after getting admitted into the country you violate PC 115 and get convicted, deportation as a consequence is highly likely\nIf you have a prior conviction in your record that involves moral turpitude and you are now convicted of violating PC 115, you may get deported\nIf you have been previously convicted of a crime involving moral turpitude irrespective of when it occurred, you become inadmissible. This means, once you leave the United States, you cannot get back in, qualify to become a citizen or be eligible for a green card\nAdditionally, a crime involving moral turpitude has severe repercussions on your career if you require a professional license to practice. For instance, if you practice as a doctor or a lawyer, the professional body issuing your practicing license can deny you a license, meaning the end of your career.\nLegal Defenses for Violating PC 115 in California\nWhen you are accused of knowingly filing forged or false documents with a government office, you face felony charges and severely punished. You want to fight the allegations against you for an acquittal or a more favorable outcome.\nYou want to engage an experienced criminal attorney who understands Californian criminal law and how to navigate the legal system in your favor. Your attorney will study your case and develop defense strategies to help you overcome the allegations. Possible legal defenses your lawyer could use include:\nYou Presented the Documents with Permission\nMany cases under PC 115 involve accusations of a defendant tricking a trusting homeowner into signing his or her property deed or forging their title deed’s signature. In many of these instances, the homeowner is usually disabled or elderly.\nEven though a situation is suspicious, it’s not automatic you committed an offense. The homeowner could have signed the deed to their property and permitted you to file or register the title deed. Unfortunately, the property owner forgets about giving consent or changes their mind later after discussing it with a relative or friend.\nWith the help of your lawyer, you can prove the owner gave consent but changed their mind later and never informed you. If there were a witness to the incident, he or she could testify on your behalf. Character witnesses could also testify to your character and say you would not defraud the alleged victim. Equally, your lawyer can call an expert who could prove the property owner’s signature on the deed is genuine.\nYou Never Knew the Documents were Forged or False\nOne of the crime’s critical elements is knowledge that the document you were filing was forged or false. This can be challenging for the prosecutor to determine. If there is doubt that you were aware that the document was forged, you could be acquitted.\nHowever, there are many times where a defendant is innocent and does not know the details of a document. For instance, if you worked as a personal assistant in a real estate company, your boss may be fraudulently acquiring property by getting property owners to sign the documents or forging their signatures. He or she sends you to file the documents to a government official, and you believe they are authentic.\nOn arriving at the government office, the officers are suspicious of your boss’s dealings, and they call the deed’s owner to confirm their consent. If the owner denies giving consent or signing the deed, you get arrested for filing forged or false documents. In your defense, you never knew that your boss gave you forged or false documents to file.\nYour lawyer can argue your case that you were not aware of your boss’s dealings, and as such, you are not guilty of the offense. If your argument convinces the court, the charges against you are dismissed, but your boss is prosecuted.\nThe Allegations Against You are False\nFalse accusations are common in criminal cases. You may be a victim of false allegations that require the aggressive defense to get an acquittal. False allegations are not uncommon where multiple parties owning one property or business have disputes. Family disagreements can also see one disgruntled member falsely accusing you of filing falsified documents.\nEven if you know the allegations against you are false, the court does not know that, and you may end up convicted wrongly of the crime. To avoid a wrongful conviction, engaging an experienced attorney to defend you against the allegations is vital. Your attorney knows the loopholes in false allegations and investigating tactics to unearth the truth. When the court is convinced that the allegations were false, the charges against you get dismissed or dropped.\nExpungement of Your Criminal Conviction\nIf you get convicted of this offense, the public can access your criminal record. A convicted felon faces many challenges in life, with one of them being a lack of trust. When everyone can see your past conviction, they will avoid engaging you even if you have excellent ideas.\nFortunately, the state of California allows defendants that have served their sentence to petition for the expungement of their records. When your record is erased, the public cannot access it, except law enforcement agencies. No one will victimize or discriminate against you because of your criminal past after your criminal record is expunged.\nYour attorney will petition for your record expungement once you complete your sentence to the satisfaction of the prosecution and court. The court will hold a hearing for this, and your lawyer should give a convincing argument. If the prosecutor is not opposed to your request, and the judge is convinced it is the right thing to do, then the expungement is granted.\nA related offense is that which can get charged together with or instead of PC 115, filing forged or false documents. Some common offenses related to this crime include:\nPC 368 – Senior Fraud\nIf the person you are accused of defrauding or attempting to defraud is 65 years or over, it is considered financial abuse against an elderly according to PC 368. In this case, you will face prosecution for filing forged or false documents and financial abuse charges against the elderly.\nIf the offense involved an amount or property value of over $950, the financial abuse offense is a wobbler, meaning you face either a misdemeanor or felony charge. If convicted of senior fraud as a felony, the penalty is two or three or four years of state imprisonment.\nPC 470 – Forgery\nYou are accused of violating PC 470 or California forgery laws if you do the below things:\nYou signed another person’s name\nYou faked another person’s handwriting or seal\nYou altered or falsified legal documents\nYou faked, altered, or presented a falsified financial statement as authentic\nIf you did any of the above actions on a document that requires recording and presented it to a public office for filing or recording, you would face two criminal charges. The prosecutor can charge you with violating PC 470 forgery and filing a forged or false document under PC 115.\nFind an Experienced Criminal Attorney Near Me\nAny criminal allegations are overwhelming and could result in severe punishment under the law, especially when not aggressively challenged. When charged with violating California PC 115, you need an experienced criminal defense lawyer and who can develop sufficient defense strategies. At San Diego Criminal Attorney Law Firm, we are passionate about defending our clients using the best strategies. Call us at 408-622-0204 to discuss your case further and prepare your defense.']	['<urn:uuid:bac60934-5ba5-43ad-ac3c-f4650801e373>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	12	56	2822
55	How does Einstein's theory challenge the idea of universal time, and what does this mean for the possibility of faster-than-light space travel?	Einstein's relativity theory showed that there is no 'universal time' that all clocks measure - everyone has their own personal time, and clocks will disagree if people are moving relative to each other. According to the theory, nothing with mass can travel faster than light speed - this is a fundamental limit of nature. Even a spaceship would need more energy than exists in the universe to reach light speed. Only massless particles like photons and neutrinos can travel at light speed. This contradicts common assumptions that advanced civilizations might overcome the light speed barrier, just as the sound barrier was broken.	"['Einstein\'s third 1905 paper was entitled ""On the Electrodynamics of Moving Bodies."" Although this paper challenged foundational notions about space and time, each of its parts was simply a response to an important problem facing the physics community of Einstein\'s time.\nOne of these three challenges Einstein addresses is the relationship between Maxwell\'s electromagnetic equations and the mechanical worldview. Scientists in Einstein\'s day searched for a unifying theory that would explain both electromagnetism and mechanics. Einstein was attracted to this problem because he was troubled by an electromagnetic principle that did not make sense according to the mechanical world view: Faraday\'s 1831 magnet-coil experiment. In this experiment, a magnet is moved near an electric circuit, and then the circuit is moved near the magnet. According to Faraday, an electric current should be formed whenever there is relative movement, regardless of whether the magnet or the circuit is moving. However, according to Maxwell\'s equations, an electric current is only induced when the circuit is at rest and the magnet moving. This asymmetrical explanation disturbed Einstein, who was committed to aesthetic principles in his science. In order to resolve this asymmetry, Einstein analyzed the arrangement of magnet and current in terms of relative movement. He proposed that the existence of an electric current depends on the relative velocity of the magnet and circuit with respect to one another. His relativity theory wa s thus the product of his aesthetic discomfort with an asymmetrical explanation.\nEinstein was not the first to formulate a relativity theory, however: Galileo had considered the concept in the early seventeenth century. According to Galilean relativity, the laws of mechanics are useless to an observer in a non-accelerating reference frame trying to determine whether he or she is moving with respect to another reference frame. When Newton revisited this problem fifty years later, he attempted to solve it by postulating an ""absolute space"" eternally at rest, relative to which any reference frame was either at rest or in motion. However, the fundamental pri nciple of relativity remained the same: the laws of mechanics are the same in all inertial (non-accelerating) reference frames, so it is impossible to determine whether an observer in one frame is moving or stationary with respect to another frame of refe rence.\nIn Einstein\'s day, physicists questioned whether the relativity principle could be applied to electrodynamic theory as well. Was it also true that the laws of electrodynamics were the same in all reference frames? Physicists were particularly interested in whether the earth\'s velocity could be detected with respect to the ether, a substance postulated by scientists as a medium through which light waves travel. In the 1880s, the American physicists Albert Michelson and Edward Morley constructed a dev ice called an interferometer to measure the earth\'s velocity with respect to the ether, but were unable to detect any movement. However, there is no evidence that Einstein was familiar with these results when he dismissed altogether the concept of the et her in his relativity paper. Einstein claimed that it is impossible to detect whether or not one is moving with respect to the ether, rendering meaningless the whole notion of an ether. His dismissal of the ether also meant that every concept involving space and time had to be considered in relative terms, a fundamental challenge to all of nineteenth-century science.\nEinstein\'s relativity theory was presented as a principled, rather than a constructive, theory. A principled theory is one that begins with principles and then uses these principles to explain the phenomena; a constructive theory starts with the observat ions and culminates in theories that explain and reconcile those observations. Einstein\'s principled account began with the postulate that the laws of science should appear the same to all freely moving observers. In particular, all observers should mea sure the speed of light as the same regardless of how fast they are moving. Thus, there is no ""universal time"" that all clocks measure; rather, everyone has his or her own personal time. If one person is moving with respect to another, their clocks will not agree. To an observer moving in one frame of reference with uniform velocity relative to a second frame of reference, the clock in the second frame will appear to move more slowly than his own clock. Moreover, since velocity is the measurement of d istance per unit of time, a measuring-stick in the second time frame would appear contracted to the observer in the reference frame. Of course, we do not observe these effects in everyday situations of movement; we do not see a ruler as contracted if we are moving by on a bus. Rather, these phenomena are noticeable only at speeds near the speed of light. Nonetheless, Einstein\'s relativity paper showed that time and space are not a priori categories of human understanding; rather, they are relative quan tities that are defined operationally.\nOne implication of relativity is the famous ""twin paradox,"" a hypothetical situation in which one twin embarks on a journey through space while the other twin stays on earth. When the first twin returns home after traveling at a velocity close to the spe ed of light, he finds that he has aged by merely a couple of years, while his brother on earth has been long since dead. This is because the twin on earth has been traveling through space at a constant time (as the earth orbits the sun), whereas the twin in the spaceship has had to decelerate and then accelerate in order to turn back home, so she has not remained in an inertial (non-accelerating) reference frame. This paradox runs counter to our commonsense view of time, but it is a natural consequence of relativity theory.', '13) What is Time Dilation? Time dilation is counter-intuitive and seems to be totally nonsensical when taken to its logical conclusion. Basically what Einstein discovered was that time is not a constant! It depends on how fast you are travelling. The faster you travel the less you age! The ‘Twin Paradox’ says that if one twin flew off in a hypothetical space ship at close to the speed of light, on a 10 year round-trip to another star system, his brother, who had stayed behind on earth, would have died of old age when he got back! This is not mumbo jumbo, but scientific fact! The Sat-Nav in your car has to take time dilation into effect or you would end up miles from your desired destination!\n13) Are we alone in the universe?\nThere is a very strong tendency among people to believe what they want to believe rather than what is demonstrably true. Many, myself included, would love to believe that flying saucers are real and that we are not alone in the universe, but wanting to believe can cloud your judgement. There has to be more concrete evidence than a few blurred photographs, some indentations in the ground, crop circles and stories of abduction before we can conclude that aliens really have visited us. You could ask yourself, ‘If a race of intelligent beings was advanced enough to build spacecraft, capable of transporting themselves trillions of miles across the galaxy and taking thousands of years to do so, would they really come all this way to our little planet just to make some fancy impressions in wheat fields? Does that strike you as being particularly intelligent? Scientists lean towards a premise known as ‘Occam’s Razor’ which roughly states: ‘The simplest explanation is most likely to be the correct one.’ As Richard Dawkins explains, ‘ It really comes down to parsimony, economy of explanation. It is possible that your car engine is powered by psychokinetic energy, but if it looks like a petrol engine, smells like a petrol engine and performs like a petrol engine, the sensible hypothesis is that it is a petrol engine!…If you hear the clip-clop of hooves in the street it could be a zebra or even a unicorn, but before you assume that it is anything other than a horse, you should demand a minimal standard of evidence! …By all means let us be open-minded, but not so open-minded that our brains drop out!’\nInvoking Occam’s Razor should lead us to the very strong likelihood that crop circles are man-made and not indications of visitations by extra terrestrials from some planet trillions of miles away!\nSome of these UFO enthusiasts even insult our intelligence (and demonstrate their own appalling ignorance) by claiming that the aliens have travelled from another galaxy! Our galaxy, the Milky Way, (comprising around 200,000 million stars), is part of a cluster of galaxies called the Local Group. This stretches the term ‘local’ to the limits because some of the others in this group are more than two million light years away from us. (One light year being the distance a beam of light, travelling at 186,000 miles per second, would cover in a year. About 6 million, million miles.) So even if they could travel at the speed of light (which they couldn’t) it would take them two million years to get here! And if they arrived at the opposite side of the Milky Way from us, it would still take them another thousand centuries just to cross our galaxy!\nSome people argue, ‘If these aliens are very advanced they might well be able to travel at (or above) the speed of light. The very idea that there are limits in nature is abhorrent to many. After all, they argue, the speed of sound was a barrier once, but these days most jet fighter aircraft can exceed it. However, I’m afraid there really are limits in nature. There is an absolute lowest temperature of minus 273 degrees centigrade and nothing can be colder. The speed of light is also finite and cannot be exceeded. Only particles with no mass, such as photons, neutrinos etc can travel at the speed of light. Anything with mass, i.e. a spaceship, would need more energy than there is in the universe to accelerate it to light speed!\nAlthough nothing can travel through space faster than the speed of light, space itself can expand faster. Currently the best theory for the origin of the universe postulates that there was an inflationary period of expansion of the universe, after the big bang, which was well above light speed. This concept is difficult to grasp, but Stephen Hawking’s book, ‘The Universe in a Nutshell’ explains it very well.\nIt is of course possible to travel faster than light relativistically. The American astronomer Edwin Hubble (after whom the space telescope is named) proved that the universe is expanding in all directions. By definition, the further away the other stars and galaxies become the faster they appear to be travelling, relative to us here on earth. Eventually their relative speed exceeds 186,000 miles per second and they disappear from our view. This means that there is an ‘edge’ to the universe beyond which we can never see! The Hubble telescope was pointed at a blank, dark area in space, where nothing was visible and the Ultra Deep Field viewer turned on. What it saw was incredible. There were millions of primitive galaxies looking as they did just a billion years after the big bang. When we look deep into space we see the stars not as they are now, but as they were when their light set off on its journey toward us, hundreds, thousands or millions of years ago! Hubble can see more than half way to the edge of the observable universe and most of the distance back to the beginning of time!\nWe live in exciting times in the exploration of space. In just the last few years we have found over 400 new planets orbiting stars other than our own sun! We always suspected that other star-systems would have attendant planets in orbit around them, but we couldn’t be sure until now. How these planets were found is a resounding triumph for science, because trying to see an ‘extra-sol’ planet, (a planet around another star) is like trying to see a firefly perched on the rim of a searchlight beamed straight at you!\nNo one has yet actually seen one of these planets, but we know they are there because of their dance with their parent star. Each causes the other to wobble as they circle each other and that wobble tells astronomers a great deal about the planet. They can deduce its size, mass, orbital radius and even the length of its year. Much of this information has been ratified by measuring the reduction in the amount of light radiating from the star as the planet crosses in front of it. In one instance they found not just one planet, but three orbiting the parent star! Although we have not yet seen such a planet, we are hopeful that we will soon be able to. Scientists are even now working to build a Hubble-style orbiting interferometer that will damp down the glare from these stars and actually allow us to see these new planets directly. Up to now the only planets we have identified have been Jupiter-sized giants in close orbits. These are highly unlikely places for life to exist, but when the interferometer is up and running, we hope to see earth-like planets that could harbour life, and possibly even intelligent life!\nIt would be egotistical in the extreme to suppose that amongst all those zillions of planets, around all those trillions of stars, in all those billions of galaxies ours is the only one with intelligent life.\nRecently, however, a disturbing theory has been put forward as to why we have still not made contact with ET, even after SETI (The Search for Extra-Terrestrial Intelligence) has spent years scanning the radio waves from space. The theory is that Hypernovas may be sterilising all planetary life in their parent galaxies! A hypernova is the biggest explosion since the universe was formed by the big bang. Previously the greatest explosions known to astronomers were the Supernovas. When a massive star has burned most of its hydrogen fuel the internal pressure can no longer withstand the star’s huge gravitational attraction and the monster implodes! This triggers an explosion of staggering intensity, momentarily brighter than an entire galaxy, hurling matter light-years out into space and seeding the universe with heavy metals, elements and the ingredients of life. The corpse of the dead star is a bizarre object indeed; a rotating neutron star called a pulsar. It is like a giant atomic nucleus. Its immense magnetic field focuses its radiation into twin beams, from the star’s poles, like a cosmic lighthouse sweeping through space.\nThe discovery of the, even bigger, Hypernovas is worth a mention. In the 60’s and 70’s U.S. satellites, checking for Russian A-bomb explosions in space, detected hundreds of gamma-ray bursts coming from very distant galaxies, billions of light-years away. The Compton Gamma Ray Observatory, in orbit around the earth, is detecting more every day. These explosions are so powerful that they briefly outshine every star and galaxy in the whole universe combined! These are the Hypernovas. If one went off in our own galaxy it would strip away the atmospheres of every planet, including the earth, and we would all be roasted by the intense radiation. Perhaps this is why we are listening to a silent cosmos!\nThe best guess as to what generates these incredible events is that a super-massive star collapses so catastrophically that it forms not a pulsar, but a huge black hole.']"	['<urn:uuid:eb8a14fe-9194-4d89-bb08-243c15766a93>', '<urn:uuid:52756b4a-8d66-44b0-9f3e-91aa6eb76807>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:31:27.499155	22	102	2599
56	artificial brain replicate human consciousness explain	According to the theory discussed, a machine that functions indistinguishably from humans would have experiences indistinguishable from humans, regardless of whether it's made of silicon chips, beer cans, or other materials. This is based on the idea that it's the way a system functions, not its physical embodiment, that determines what that system experiences. However, this view is controversial as it suggests that any system - even virtual minds or symbols manipulated by a population - could experience consciousness like humans do, provided they simulate human mind's operations accurately.	"['According to Chalmers, a theory of consciousness should, at the very least, ""give the conditions under which physical processes give rise to consciousness, it should specify just what sort of experience is associated. And we would like the theory to explain how it arises, so that the emergence of consciousness seems intelligible rather than magical. In the end, we would like the theory to enable us to see consciousness as an integral part of the natural world. Currently it may be hard to see what such a theory would be like, but without such a theory we could not be said to fully understand consciousness.""(p5) To what extent does Chalmers make progress, judged by his own criteria? He touches on a few of the scientific findings which might reveal physical processes which ""give rise to"" consciousness, but that is not what his book is about. His real concern is to suggest what an appropriate theory of consciousness would be like. This is a book by a philosopher addressed primarily to philosophers. Chalmers writes well and many of the chapters will be accessible to the general reader. However, non-philosophers will find the technical nature of some of the chapters heavy going (and might be better served by his more succinct presentation of his views in ""Facing up to the problem of consciousness"", in the Journal of Consciousness Studies, 2, 200-219, 1995). Given the fluid state of consciousness studies, progress towards the ""shape"" of an appropriate theory, fully argued, will nevertheless be of interest to many.\nIn the 20th Century, philosophy of mind has been predominantly reductionist, arguing that consciousness will eventually be shown to be nothing more than a physical state or function of the brain. By contrast, Chalmers joins a small, but growing band of theorists who maintain that consciousness can only be understood within a nonreductionist science of the mind. His central argument against reductionism hinges on the claim that consciousness is ""naturally supervenient"" but not ""logically supervenient"" on physical states. That is, it actually supervenes on brain activity, but it is conceivable that it might not. This, he argues, makes consciousness different from all other properties, including ""emergent"" biological properties such as ""life."" If we know all the microproperties of a physical system and their relationships, we necessarily know the macroproperties - for example, according to Chalmers, given that the physical facts are as they are, it is inconceivable that life could be other than it is. In this sense, a higher order property such as ""life"" is nothing more than its combined, physical constituents. By contrast, he can conceive of a zombie which is physically indistinguishable from himself, but which lacks consciousness. Consequently, consciousness cannot be a physical property. Nor can it be reduced to physical properties. There are, he claims, no other nonphysical properties in this sense.\nThis tension between what is ""natural"" (or actual) and what is ""logical"" (or logically possible) runs throughout the book, sometimes uncomfortably. Many scientists, for example, will be dubious about the actual knowledge of macroproperties that is provided by knowledge of microproperties. Morphogenesis (the creation of life forms) for example, does not seem to be very well accounted for by gene theory (at present), let alone quantum mechanical events. So even physical wholes might, in some sense, exhibit properties not reducible to the sum of the parts. Chalmers\' response to this would be that in such cases we have insufficient knowledge of the parts; if we had complete knowledge of genes, subatomic particles, and so on, it is logically inconceivable that macroproperties such as morphogenesis would not be understood. This insistence on complete knowledge provides Chalmers with an escape from any possible counterexample, as he can always claim these to be cases where our knowledge of microproperties is incomplete. However, in actuality, our ability to account for all complex forms and functions in terms of the activities of physical microproperties seems to be more a matter of faith, than logical necessity. Functions, for example, often need to be understood not just in terms of their constituent processes but also in terms of the environments which embed them. And the reducibility of psychological beliefs, desires and so on to physical microproperties seems counterintuitive; how, for example, could even a complete knowledge of quantum mechanical events provide an understanding of why one needs to catch a 94 bus?\nA simple, definitive argument against the reducibility of phenomenal properties to physical ones would nevertheless be very welcome, and this is Chalmers\' more central concern. Unfortunately, the force of Chalmers\' argument rests entirely on what he finds to be conceivable - and its consequent weakness, that different theorists might claim to be able to conceive of entirely different things. Zombies are not just functionally indistinguishable but also physically indistinguishable from humans. A reductionist might therefore claim that he cannot conceive of such a creature lacking consciousness. If consciousness just is a state of the brain, then it inconceivable that a creature with a brain in that state could lack it. Faced with this stand-off, the only recourse would seem to be to take a vote (about what is conceivable) - hardly a satisfactory way to resolve the issue. Chalmers nevertheless goes on to assemble a range of standard arguments against reductionism from the literature, elaborates these where necessary, and gives a critique of the counterarguments. His mastery of philosophical terrain is impressive, and taken together, his arguments pose a serious challenge for materialist theories of mind.\nBut what is his alternative, nonreductionist, theory? In Chalmers\' ontology, there are two kinds of property, physical properties and phenomenal ones, and there are two kinds of law, physical laws and psychophysical laws or ""bridging principles"" which govern the way phenomenal properties ""naturally supervene"" on physical ones. Nothing else, he claims, is required for a complete account of consciousness and mind. Although Chalmers presents this as a novel, nonreductionist position, this much is fairly standard. A nonreductionist might claim more, for example, the existence of mental laws. But it would be hard for a nonreductionist to claim less than the existence of physical and phenomenal properties along with some way of relating the two. Positing the existence of ""psychophysical laws"" might raise some eyebrows in philosophy, but the search for such laws is as old as experimental psychology (witness the ""Weber-Fechner law"" and ""Stevens\' power law"" which relate the way changes in physical stimuli are translated into perceived changes). Muller, in 1896, also gives an instructive formal description of psychophysical laws (which is rather lost in Chalmers\' presentation in note 12, on page 385).\nNot all of Chalmers\' ontology, however, is so straightforward. Sometimes for example Chalmers describes his position as a ""double-aspect"" theory - which poses the question ""double-aspects of what?"" According to Chalmers, phenomenal properties and their physical correlates in the brain will be structurally coherent, in the sense that they will encode the same information. On these grounds Chalmers justifiably describes his position as a ""double-aspect theory of information."" This much is plausible (I presented an identical ""dual-aspect theory of information"" in ""Is human information processing conscious?"" in The Behavioral and Brain Sciences, in 1991). However, at other times, to avoid positing some transcendental ground for physical and phenomenal properties Chalmers describes his position as ""naturalistic dualism"" - in which consciousness becomes ""basic"" in the same sense that energy is basic in physics. This raises the question, ""If phenomenal and physical properties are equally basic, distinct, and not grounded in something more fundamental, then what is it that relates them to each other so precisely?"" Alternatively, if phenomenal properties ""supervene"" on physical ones (as he argues throughout the book), then why regard the phenomenal properties as ""basic""? It is up to Chalmers to decide which position he wishes to defend (the rest us will have to wait and see).\nFurther problems arise from Chalmers\' psychofunctionalism - perhaps the most innovative (and controversial) aspect of his theory. Although Chalmers refers in much of his work to the need for ""psychophysical"" bridging laws, it eventually becomes clear that these are psychofunctional rather than psychophysical. That is, he believes there to be an invariant relationship between physical and phenomenal properties, but it is the way a system functions, and not its physical embodiment that determines what that system experiences. According to Chalmers a machine that functions in a way that is indistinguishable from humans has experiences that are indistinguishable from humans (a version of ""strong AI""). This would be true whether the system is made out of silicon chips, beer cans, or the population of China - provided only that in their detailed activity, these systems instantiate the same causal relationships, ie., function in the same way. Chalmers\' combination of a nonreductive phenomenology with standard functionalism is, to my knowledge, novel within philosophy of mind (standard functionalism claims that consciousness is nothing more than brain functioning) - and, given that the phenomenology of consciousness has proved to be the stumbling block of functionalism, it is not surprising that Chalmers\' position has attracted considerable interest.\nChalmers develops this position from two thought experiments, which he describes as ""fading qualia"" and ""dancing qualia."" In these he considers the familiar scenario in which the neurons of the brain are gradually replaced by silicon chips which exactly replace the functioning of the neurons they replace. As the replacements progress, do the qualia gradually fade? Or, if one were able to switch between one\'s normal brain and a replacement silicon brain (with exactly the same functions) would the qualia dance? According to Chalmers if one replaced the functions exactly one could not notice the difference either externally in terms of behaviour, or internally in terms of what one experiences. One would, after all, have to report the same things - otherwise the functioning of the silicon systems would not be the same as the neural systems they replace. Hence, functioning of certain sorts is necessarily accompanied by experiences of certain sorts (there is no way to distinguish any difference).\nThis argument was initially put in the special issue of the Journal of Consciousness Studies (mentioned above), and in my own commentary (in the same issue) I suggested that Chalmers had presented the options in the silicon replacement experiments in an unnecessarily restrictive way. In actual practice, one could envisage replacing the absent functioning of some damaged brain circuitry with silicon or other hardware in an otherwise normally functioning human being - and leave open the question of whether repairing the functioning (defined in information processing terms) would also replace any missing experience. For example, it might be possible to devise a cortical implant for blindsight. Patients with blindsight cannot see stimuli presented to their blind hemifield, but when forced to guess, they can identify them. A cortical implant which restored the ability of visual circuitry to communicate the results of pattern recognition to the systems used for reporting might restore the ability to identify stimuli with ease. But whether it restored the ability to have a visual experience of the stimuli remains an open question. If so, functioning can be dissociated from experience experimentally, at least in principle.\nChalmers\' response to this was that the two outcomes are not functionally equivalent. In one outcome subjects report that they can see the stimulus as well as identify it and in the other, that they can\'t see the stimulus. My response would be that what is at stake is what subjects can actually see, not what they can report. In the human brain, the visual system is dissociable, at least in part, from the systems serving language and speech (as blindsight itself demonstrates). The aim of the ""cortical implant for blindsight experiment"" is to determine whether the input-output functioning of the visual system (discrimination, pattern recognition, etc) can be restored without restoring the accompanying visual experience. Subjects\' discrimination ability can be assessed (by external observers) from their behaviour, but only the subjects themselves can tell us about what they experience. Leaving subjects free to tell us one way or the other is therefore central to the experiment. In Chalmers\' version of the thought experiment the visual system and the language/speech system are treated as non-dissociable (ie as one system). Consequently, ""functional equivalence"" requires identical verbal reports by definition. Set up this way, the claim that functioning cannnot be dissociated from experience becomes unfalsifiable - which, for science, removes the value of the experiment.\nWhatever one may think about the ""fading/dancing qualia"" arguments, the view that ""bodies don\'t matter"" for what we experience is highly counterintuitive. On Chalmers\' account, not just machines made of silicon chips might experience in the way that humans do, but so would virtual minds (instantiated in the symbol manipulations of programmes) and even systems consisting of symbols written on bits of paper by the population of China, provided only that the causal relationships governing the creation of those symbols, simulate those of the human mind\'s symbol manipulations accurately. Processes within the human brain normally thought of as unconscious would also have to be conscious in Chalmers\' system (by virtue of their functioning) - in which case the conscious/nonconscious distinction loses its meaning. The theoretical cost of this position to consciousness studies is considerable. If the conscious/nonconscious distinction cannot be made, how could one investigate the conditions for consciousness in the human brain - which rely on contrasts between neural conditions adequate or not adequate for conscious experience? How could one make sense of the extensive experimental literature on the differences between preconscious, conscious and unconscious processing? And what of psychodynamic theory - is all talk of a personal or transpersonal unconscious just muddled thinking?\nNote that Chalmers is forced into this uncompromising position by his fading/dancing qualia argument (whatever functions is conscious by virtue of its functioning). Given this, all brain functions must be conscious. Consequently, he maintains that those functions which do not seem to enter into our consciousness must be autonomously conscious (they are conscious to themselves). This leads to the extravagant claim that there are as many distinct consciousnesses cohabiting in the human brain as there are distinct functions.\nNor does Chalmers see any reason to draw the line at brains or systems which simulate the functioning of the brain. If consciousness of given sorts is invariably associated with functioning of given sorts then all forms of functioning are associated with experiences, irrespective of their embodiment. This ""panpsychofunctionalism"" (my term for this) is quite different from panpsychism (the view that all material forms are accompanied by forms of experience). If true, then not only do thermostats experience in ways that relate to their function (sensing hot and cold), but so does rain falling in that it functions to make the earth wet - and even rainbows experience something relating to their production of beautiful sensations in the human mind. The central difficulty for Chalmers is that functioning is observer-relative. Chalmers\' defence is that the structure of physical systems does, to some extent, constrain their potential functioning. But this really misses the point. Does the chinese statue on my desk really experience something different when it functions as an incense holder rather than as a decorative object? And if it does both simultaneously, does it really simultaneously have both experiences? As John Searle has commented in his recent New York Times review of Chalmers\' book, ""It is rather as if someone got the result 2+2=7 and said ""Well maybe 2 plus 2 does equal 7.""\nIn many ways this book provides an impressive display of expertise and argument and it has been well received by many (although not all) philosophers of mind. But it gives rather confused directions, in my view, to some future theory of consciousness. I find it hard to take issue with its antireductionist stance, or its view that consciousness is somehow ""basic"", or that ""information"" may have a central role to play in any theory of how phenomenology relates to the brain (I have argued for the same positions myself). But ""double-aspect theory"" isn\'t interchangeable with ""naturalistic dualism"", conscious processes do need to be distinguished from nonconscious ones in the brain, thought experiments with unfalsifiable conclusions need to be distinguished from actual experiments, and I very much doubt that the stiff whisky I have just swallowed is aware of its function in sustaining my ability to write this review. Philosophers, quite rightly, feel free to contemplate possible worlds as well as actual ones. But once one has explored where the logic of an argument leads, one still has to consider whether to temper logic with common-sense.']"	['<urn:uuid:25e3ca4e-24c0-4813-bf1d-b5d8359b01a4>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	6	89	2761
57	How do product backlogs differ between traditional agile and DataOps?	While traditional agile backlogs focus on user stories for product features, DataOps backlogs are broader - they include improvements to dashboards, actionable insights, data quality enhancements, automation tasks, and data governance deliverables. The analytics owner manages these requirements instead of a traditional product owner.	"[""Modern Project Management for Product Managers\nOne of the critical responsibilities of product managers is driving the overall execution of their product. Relentless execution will ultimately determine whether you'll be able to make your product vision a reality. Driving the execution of your product not only means doing whatever it takes to make your product win, but it also encompasses a set of core project management responsibilities. While many product managers are familiar with agile methodologies for managing a development team, I don't believe it provides a full view of how a product manager should be effectively managing their overall product process.\nToday I wanted to provide a complete picture of a modern project management process for product managers. This covers a set of planning and project management activities that product managers should drive annually, quarterly, bi-weekly, and daily to effectively manage a product development process. It's rooted in the agile movement, with a deep recognition that customer needs and product requirements are ever-evolving and agility is absolutely paramount to enable you to swiftly change plans as soon as it's appropriate. At the same time, it recognizes that planning is absolutely necessary for enabling blue-sky thinking, thoughtful trade-offs of priorities, driving team alignment, and ultimately for enabling you to realize your product's long-term vision.\nLet's walk through each planning activity, it's purpose, and benefits.\nThe annual planning process is an important opportunity to take a step-back from the day-to-day execution of your product. It often starts with a re-assessment of the market you play in, taking the time to look at the industry you operate in, think through emerging or waning technology trends, and taking an updated pulse on your standing in the market. It's a good time to reflect on how well you are serving the needs of your customers and their overall satisfaction with your product offering.\nIt's also a good time to ensure that the vision you've established for your product is still appropriate. While a vision should have long-term stability and should change rather infrequently, there do come occasions when it makes sense to revise it. Treat the annual planning process as a check-in to ensure your vision is still serving you well.\nWhile the output of an annual planning process can take multiple forms, I've found 5-year roadmap themes to be a great aid in facilitating the right level of discussion. I've typically seen this done as a single slide or single-page document. While the upcoming year will have a great degree of specificity, the outer years will likely only have a few sentences or bullets associated with their themes. This exercise facilitates long-term thinking into where you ultimately aspire to go, but also forces a very real trade-off discussion of whether the next year is going to be the year that you start to take on some of these longer-term objectives or not.\nFor example, we are currently experiencing an explosion in artificial intelligence and machine learning capabilities and for many consumer & B2B services, it's important to understand this technology trend. After careful consideration you might decide that your product is a perfect fit for it, the time is right, and you have the right team capabilities to put machine learning related themes into next year's roadmap. On the other hand you might decide that it's not well-suited for your product, you have far lower-hanging customer challenges that need to be addressed, and the next 2 years are not the time for you to pivot away from your existing roadmap. This is exactly the kind of discussion you want to facilitate with your team's stakeholders.\nWhen it comes to costing & estimation, I find that roadmap themes are too high-level to be able to get granular costing of effort required. And since the actual implementation of features can be so far out, the effort required for granular costing is rarely worth it. Instead I find leveraging experienced product & engineering leads with a specific stated team size can usually provide t-shirt sizing (S, M, L) that can be used to have a high-level trade-off discussion of what can realistically fit in this upcoming year vs. not.\nAs an example, Facebook released a 10 year roadmap which mimics the later years of the 5-year roadmap themes that I mentioned in terms of just high-level bullets, though you'll want to have more detail in the 1st and 2nd years than shown here:\nQuarterly OKRs provide an opportunity to concretely define the product outcomes you plan on achieving in the next three months. I like using OKRs, or objectives & key results, because of their specific focus on measurable outcomes. The objective defines what you want to accomplish, usually covering the product functionality or improvements you plan on shipping. And the key results defines the measurable product outcome you expect to achieve, including specific metric lifts you expect to see as a direct result of the product initiatives. These tend to be acquisition, engagement, or monetization metric lifts. But it could be anything worth measuring (increase in net promoter score, reduction in customer support tickets, increase in sales velocity, etc).\nTaking the time to appropriately cost and estimate both the effort it will take to execute the desired product initiatives as well as appropriately forecast the metric lifts is a very important part of quarterly OKR planning. Here you do need enough specificity in the product initiatives including high-level scope in order for engineering to appropriately cost what they will be able to accomplish with a given set of resources assigned to it in the quarter. Similarly, product should develop as accurate of a forecast on the expected metric lift as they can. This is undoubtably a tough exercise, but putting a number out there is important. You should leverage as much past data on how your various initiatives have moved metrics in the past to come up with appropriate proxies. Even though you're estimate will likely be off, having an estimate and seeing how you did against it at the end of the quarter is the only way to tune your product intuition on what kinds of initiatives are in-fact metric-moving initiatives. Without going through this exercise on a quarterly basis, you'll never build this muscle.\nEach quarter your product team should come up with 3-5 such OKRs and these should easily fit on a single slide or single-page document. Having any more than this is usually means a lack of focus for a product team. Of course this list will certainly not cover absolutely every product change you'll end up making in the quarter and that's fine, but this is an important prioritization exercise to rally the team behind the most important efforts as well as the priorities in terms of impact you're team is hoping to have on the product's success.\nAn equally important part of quarterly OKRs is the end-of-quarter post-mortem, when you go through the past quarters OKRs and score them each as either Green (met or exceeded the key results), Yellow (achieved ~70% of your stated key results), or Red (failed to meet the expected key results). As I mentioned, this is where the real learning happens to improve your team's ability to cost and forecast initiatives. Oftentimes you'll miss an initiative because it was delivered late. This serves as a great opportunity to discuss whether the issue was an inaccurate initial forecast or unforeseen challenges and what might you do in the future to improve your estimation. Similarly, you may fail to achieve the metric win you expected. And again, this is a great opportunity to discuss what could have been done differently for next time to try to find initiatives that will in-fact meet their metric expectations. The post-mortem is also a great opportunity to re-asses what you are hearing from customers and the market and ensure that your annual plan still makes sense in light of this. If not, you're encouraged to change your plan for the upcoming quarter to address the new market realities. There is nothing sacred about your annual plan, it was just a guidepost that you should feel free to move from as new market dynamics become apparent.\nAs an example, here is potential OKR for an online consumer website:\nObjective: Redesign the homepage to focus on driving social actions\n- Key Result: Increase social actions (likes, comments, shares) per session by 20%\n- Key Result: Increase homepage weekly active users by 15%\nBi-weekly sprints are the core mechanism by which engineering teams are run. The sprint planning meeting takes the form of your classic agile notion of picking off items from your overall product backlog and adding them to the upcoming sprint's backlog, with the engineer who is going to be responsible for the task directly costing the associated effort and committing to completing the tasks he takes on during the sprint planning process. This includes product features that are planned to be implemented, bugs that will be fixed, as well as any engineering tasks they are also taking on. It's helpful to include all R&D functions in this process, so design also commits to what design deliverables they are going to finish in the upcoming sprint, as well as product committing to various requirement and spec deliverables as well. The output of sprint planning is a single list of prioritized tasks for the upcoming sprint with associated costs & owners. The goal here is to not change the sprint tasks during the sprint in order to allow the team to focus on effectively completing those tasks and avoid randomization and instead only consider changes in the next sprint. The 2-week sprint timeframe is short enough that you can still be very agile but provide enough predictability and focus to engineering teams to make meaningful progress.\nAt the end of the sprint, the team holds a sprint review to show off the shippable product functionality completed in the sprint as well as a sprint retrospective to look back on how the team faired in sticking to the sprint timeline. A sprint schedule is a probability. And what you want to be doing is always improving your team's ability to accurately forecast to make the probability higher in the future.\nIt's helpful as the first thing in the morning each day to have a quick 15-minute standup with the full R&D team where each individual briefly discusses what they accomplished yesterday, what they are planning on accomplishing today, and if there is anything blocking them from meeting their sprint goals. This allow the team to assess how they are doing against their sprint goals, resolve any bottlenecks, and potentially move around team resources to address those bottlenecks if needed. The goal here is to avoid issues festering for days, resulting in the project getting delayed and instead have an opportunity to address it every 24 hours to keep the team running as efficiently as possible. This in-person live format works better than issues lingering in long unresolved email threads. This is typically run by a scrum master (usually an engineering lead or delegate on the team) who goes around to each team member and ensures each bottleneck is addressed either during or immediately after the standup.\nI've started to see some teams turn this into a Slack/HipChat channel discussion instead of an in-person session. While I like the daily ritual of getting the whole team together in-person, the chat version does work. It's important though to make it a live session so issues can still be immediately addressed.\nIn addition to a daily scrum, it's also helpful to have a daily bug triage process which reviews new bugs, appropriately sets the bug's priority, and assigns the bug to the appropriate engineer. I also encourage you to establish bug SLAs (service level agreements) that define when a bug is expected to be fixed based on it's priority. For example, P0 bugs that are incredibly severe are reason to stop everything else and get the bug fixed immediately. P1 could have an SLA of fixed in production within a week. P2 could be fixed in production within a month. It's best if you can assign someone to go through this task for new bugs on a daily basis in case there are any new P0 bugs that need immediate attention. I've typically seen this done by either an engineering lead, a test lead, or product manager.\nProject Management Tools\nThere are a variety of tools available which are typically used to streamline project management activities.\nI've typically seen annual planning and quarterly OKRs published in a team wiki (like Confluence), in shared documents (like Google Docs), or shared slides (like Google Slides). Since each planning document is never more than a page each, it's pretty easy to keep the full history of past and present versions altogether.\nSprints are most often managed in a full project management tool. Any of the popular project management tools work well, like Atlassian JIRA, Asana, Trello, Wrike, or Basecamp. Some specifically have support for sprint planning and daily standups, enabling you to run your daily standup more efficiently with a scrum board that everyone can see, including Atlassian JIRA and Trello. Pick a tool that your team is familiar with and comfortable using as they'll ideally be in there frequently updating the status of tickets.\nIt's important to remember that there is nothing sacred about the specific intervals references here (annually, quarterly, bi-weekly, daily). You can easily customize these to meet the cadence of your product development cycle and industry dynamics. These cadences though have worked well for me across a variety of desktop, internet, and mobile applications I've built throughout my career.\nI hope this provides an overview of a modern project management process for product managers. While it may look like a lot of project management overhead, the reality is that each of the planning tasks is fairly quick and the output of each planning activity is never more than a single page of roadmap themes, OKRs, prioritized tasks, etc. The overall planning process not only facilitates long-term thinking, course correction, and learning, but is absolutely essential for enabling your team to achieve it's ultimate product vision.\nEnjoyed this essay?\nGet my weekly essays on product management & entrepreneurship delivered to your inbox.\nDec 06, 2016"", 'How to apply agile to data science and DataOps\nAn agile approach to dashboards, machine learning models, cleansing data sources, and data governance\n19 June 2020 | 0\nJust about every organisation is trying to become more data-driven, hoping to leverage data visualisations, analytics, and machine learning for competitive advantages. Providing actionable insights through analytics requires a strong DataOps program for integrating data and a proactive data governance program to address data quality, privacy, policies, and security.\nDelivering DataOps, analytics, and governance is a significant scope that requires aligning stakeholders on priorities, implementing multiple technologies, and gathering people with diverse backgrounds and skills. Agile methodologies can form the working process to help multidisciplinary teams prioritise, plan, and successfully deliver incremental business value.\nAgile methodologies can also help data and analytics teams capture and process feedback from customers, stakeholders, and end-users. Feedback should drive data visualisation improvements, machine learning model recalibrations, data quality increases, and data governance compliance.\nDefining an agile process for data science and DataOps\nApplying agile methodologies to the analytics and machine learning lifecycle is a significant opportunity, but it requires redefining some terms and concepts. For example:\n- Instead of an agile product owner, an agile data science team may be led by an analytics owner who is responsible for driving business outcomes from the insights delivered.\n- Data science teams sometimes complete new user stories with improvements to dashboards and other tools, but more broadly, they deliver actionable insights, improved data quality, DataOps automation, enhanced data governance, and other deliverables. The analytics owner and team should capture the underlying requirements for all these deliverables in the backlog.\n- Agile data science teams should be multidisciplinary and may include DataOps engineers, data modelers, database developers, data governance specialists, data scientists, citizen data scientists, data stewards, statisticians, and machine learning experts. The team makeup depends on the scope of work and the complexity of data and analytics required.\nAn agile data science team is likely to have several types of work. Here are three primary ones that should fill backlogs and sprint commitments.\nDeveloping and upgrading analytics, dashboards, and data visualisations\nData science teams should conceive dashboards to help end-users answer questions. For example, a sales dashboard may answer the question, “What sales territories have seen the most sales activity by rep during the last 90 days?” A dashboard for agile software development teams may answer, “Over the last three releases, how productive has the team been delivering features, addressing technical debt, and resolving production defects?”\nAgile user stories should address three questions: Who are the end-users? What problem do they want addressed? Why is the problem important? Questions are the basis for writing agile user stories that deliver analytics, dashboards, or data visualisations. Questions address who intends to use the dashboard and what answers they need.\nIt then helps when stakeholders and end-users provide a hypothesis to an answer and how they intend to make the results actionable. How insights become actionable and their business impacts help answer the third question (why is the problem important) that agile user stories should address.\nThe first version of a Tableau or Power BI dashboard should be a “minimal viable dashboard” that is good enough to share with end-users to get feedback. Users should let the data science team know how well the dashboard addresses their questions and how to improve. The analytics product owner should put these enhancements on the backlog and consider prioritising them in future sprints.\nDeveloping and upgrading machine learning models\nThe process of developing analytical and machine learning models includes segmenting and tagging data, feature extraction, and running data sets through multiple algorithms and configurations. Agile data science teams might record agile user stories for prepping data for use in model development and then creating separate stories for each experiment. The transparency helps teams review the results from experiments, decide on the next priorities, and discuss whether approaches are converging on beneficial results.\nThere are likely separate user stories to move models from the lab into production environments. These stories are DevOps for data science and machine learning, and likely include scripting infrastructure, automating model deployments, and monitoring the production processes.\nOnce models are in production, the data science team has responsibilities to maintain them. As new data comes in, models may drift off target and require recalibration or re-engineering with updated data sets. Advanced machine learning teams from companies like Twitter and Facebook implement continuous training and recalibrate models with new training set data.\nDiscovering, integrating, and cleansing data sources\nAgile data science teams should always seek out new data sources to integrate and enrich their strategic data warehouses and data lakes. One important example is data siloed in SaaS tools used by marketing departments for reaching prospects or communicating with customers. Other data sources might provide additional perspectives around supply chains, customer demographics, or environmental contexts that impact purchasing decisions.\nAnalyst owners should fill agile backlogs with story cards to research new data sources, validate sample data sets, and integrate prioritised ones into the primary data repositories. When agile teams integrate new data sources, the teams should consider automating the data integration, implementing data validation and quality rules, and linking data with master data sources.\nJulien Sauvage, vice president of product marketing at Talend, proposes the following guidelines for building trust in data sources. “Today, companies need to gain more confidence in the data used in their reports and dashboards. It is achievable with a built-in trust score based on data quality, data popularity, compliance, and user-defined ratings. A trust score enables the data practitioner to see the effects of data cleaning tasks in real time, which enables fixing data quality issues iteratively.”\nThe data science team should also capture and prioritise data debt. Historically, data sources lacked owners, stewards, and data governance implementations. Without the proper controls, many data entry forms and tools did not have sufficient data validation, and integrated data sources did not have cleansing rules or exception handling. Many organisations have a mountain of dirty data sitting in data warehouses and lakes used in analytics and data visualisations.\nJust like there is not a quick fix to address technical debt, agile data science groups should prioritise and address data debt iteratively. As the analytics owner adds user stories for delivering analytics, the team should review and ask what underlying data debt must be itemised on the backlog and prioritised.\nImplementing data governance with agile methodologies\nThe above examples all help data science teams improve data quality and deliver tools for leveraging analytics in decision making, products, and services.\nIn a proactive data governance program, issues around data policy, privacy, and security get prioritised and addressed in parallel to the work to deliver and improve data visualisations, analytics, machine learning, and DataOps. Sometimes data governance work falls under the scope of data science teams, but more often, a separate group or function is responsible for data governance.\nOrganisations have growing competitive needs around analytics and data governance regulations, compliance, and evolving best practices. Applying agile methodologies provides organisations with a well-established structure, process, and tools to prioritise, plan, and deliver data-driven impacts.\nIDG News Service']"	['<urn:uuid:a5a9e809-4cbe-4b44-a967-51b2bdc971ef>', '<urn:uuid:36047be5-00b2-4f5f-a45e-a5abaf4059c9>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	10	44	3584
58	I'm building my house. What is thermal grout mainly used for?	Thermal grout is used to transmit heat from underground electrical cables into the surrounding soil or rock. It's a critical element in constructing segments of underground power transmission lines, where it surrounds the individual conduits holding each cable and fills the casing completely.	['Grouts have a multitude of uses for many types of construction.\nFor utility construction, grouts are most often thought of for sealing leaks and joints in water and sewer pipe infrastructure, manhole rehabilitation and filling annular space between pipes and surrounding structures.\nIn addition, thermal grout is an essential part of the process of placing high-voltage power transmission and distribution lines underground, a trend that is becoming more common (see the June issue of Underground Construction). The use of plastics for insulation, advances in cable design and manufacturing, and other factors have made underground placement of high-voltage transmission lines a viable or necessary option in many situations.\nThe right thermal grout mix correctly applied is a critical element in constructing segments of underground power transmission lines. Thermal grout is used to transmit heat from underground electrical cables into the surrounding soil or rock, said Guy Dickes, president, Constellation Group LLC, who over the past four years has been responsible for planning and executing grouting for four major underground power transmission projects from 13.6 kV to 230 kV with segments from 100 to 2,000 linear feet in length.\nHeat is the enemy of electric transmission, Dickes said. Heat increases wire resistance and shortens the lifespan of the cable insulation. Cross-country transmission lines suspended high in the air from steel towers are not insulated and heat is easily dissipated into the air. But for cable buried in a conduit under the ground, heat is confined.\n“Thermal grout surrounds the individual conduits holding each cable and fills the casing completely,” Dickes explained. “This grouting technology is rapidly advancing.”\nFor example, Dickes said horizontal directional drilling and thermal grouting is used in underground applications where open-cut-and cover construction cannot be utilized. Some examples where open cut construction cannot be used are under creeks and rivers, expensive golf courses, developed properties, urban areas and many other conditions.\nOf all the elements contributing to successful underground power transmission construction projects, grouting is arguably the least understood and has been considered by many contractors to be “difficult.”\n“Indeed, there are stories of project failures attributed to ‘bad’ grouting,” Dickes said. “In my experience, few contractors like to grout and unfortunately, grouting is left to the end of the job and given to the low man on the totem pole with little oversight. Untrained personnel pumping the wrong mix design into an expensive underground casing can be a very expensive mistake. A correct grouting operation starts at the planning stages of a project, should be fully developed prior to construction and properly executed by skilled personnel.\n“Thermal grouting should be treated like any other technical part of the project — properly designed from thermal performance and constructability standpoints. In all its forms, grouting is a sophisticated technology that requires a specialist to perform. It is an operation that needs to be properly planned and executed. The mix design needs to be correct for the application.”\nThere is no one thermal mix design. Each grouting project has its own unique characteristics and must be planned accordingly.\n“Thermal grouting is an engineered solution,” Dickes emphasizes. “There are no ‘cookie cutter’ specifications or mix designs. Often, we try to use locally available materials in order to minimize costs. Manufactured materials are available, suitable for long distance thermal grouting, but their thermal properties may not be as good as quarried aggregates and the cost is higher. Grout costs using these manufactured materials can run as high as $300 or more per cubic yard. The advantage of the manufactured materials is the ability to design grouts for pumping over greater distances.”\nDickes said his experience and role on thermal grouting projects is as a consultant and job-site manager. He considers Deepak Parmar, president of Geothermal Inc., the foremost scientist on thermal grouting.\n“Deepak has been designing thermal grouts for underground power transmission projects for more than 30 years,” said Dickes.\nParmar said no product should be marketed or promoted as ‘thermal grout’ unless its thermal characteristics are defined and acceptable for the intended use.\n“Each mix design of thermal grout is different and is based on several requirements: thermal conductivity, flow (time of efflux), strength, rate of hardening and heat of hydration,” said Parmar.\nTwo essential characteristics of thermal grout for underground power projects are flowability and pumpability, terms that are not interchangeable. A mix may “flow,” but not be easily pumped.\nFrom the installation viewpoint,” Parmar explained, “flow and pumpability are related and are the governing factors for limiting pumping pressure and rate of pumping. Various other factors such as the total distance to be pumped, size of the casing, number of ducts, spacers, change in elevation, etc., must be taken into account because they all contribute to the total resistance to pumping. From the grout installer’s view-point, flow and pumpability are of primary importance for any project.”\nFor most civil engineering applications only the flow and strength may be of interest, Parmar added.\n“In order to specify/spec a thermal grout the engineer must have a clear understanding of what is required (material), why is it required (performance) and how it should be installed (process). Design of the thermal grout must be conducted by ‘experienced’ person; taking into consideration the thermal, mechanical and project specific requirements. The material supplier and installer must be equally knowledgeable and experienced to understand the requirements and means of delivering them. A successful demonstration of the material and the process prior to the actual installation will gain the confidence of the owners and consulting engineers.”\nIndustry sources expect the number of projects including underground power transmission lines to steadily increase. Buried 230 kV lines are not uncommon in the United States, and several 345 kV have been made. In Asia and Europe, transmission networks are in operations that contain 400 kV and 500 kV underground segments.\n“One thing I can say for certain,” said Dickes, “the amount of knowledge developed in thermal grouting construction technology has grown by orders of magnitude over the last several years. Owners and engineers need to know that long distance thermal grouting of underground casings has been performed successfully in the United States.”\nDickes is participating in planning of a project now that will include 3,000 feet of 230 kV underground transmission lines in an uncased bore. Geotherm and Constellation Group have developed the specified thermal grout for this project during the summer of 2010.\nFOR MORE INFORMATION:\nConstellation Group LLC, (410) 484 0672; www.cgllc.us']	['<urn:uuid:517a4c59-a27e-4b05-97f4-1caab466e1a5>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	11	43	1073
59	drought resistant needle tree types for landscaping wet soil problems and winter injuries	For drought resistance, most pines and eastern larch are good choices. For wet soil conditions, water-tolerant species like eastern larch, red maple, and green ash are recommended. To prevent winter injuries, select appropriate planting sites, ensure sufficient root zone moisture before soil freezes, avoid late summer fertilization, and use mulch. Narrow-leaved evergreens like arborvitae, yew, juniper, pine, and hemlock are commonly affected by winter damage.	"['Like many backyard evergreens, these diseased spruce trees are dying from the bottom up.\nYou’re not alone if you’re seeing your needled evergreens go downhill. Gardeners throughout the Northeast and Midwest have been noticing branch diebacks and tree failures in their spruce, firs, pines, hemlocks, and other needled evergreens, especially in the past three years.\nRather than a single plague killing everything in its path, such as the emerald ash borer that’s devastating ash trees or the new blight disease that’s deadly to boxwoods, the needled-evergreen’s troubles trace to a variety of issues.\nSome of the issues are general, including hotter weather and soggy soil from excess rains. Others are species-specific, including needle cast diseases that attack Douglas firs and heat-loving spider mites which target the popular dwarf Alberta spruce.\nOne issue that has been grabbing attention recently is the demise of one of America’s favorite home-landscape trees, the Colorado blue spruce. This is the evergreen with the stiff, powdery blue needles and sleek, upright habit.\nSeveral fungi have stepped up their infection of blue spruces the past two years, especially one called Rhizosphaera needle cast that causes browning of needles from the bottom of the tree upward. After several years of spreading infection, trees often die.\nHere’s what a healthy Colorado blue spruce should look like.\nBehind the Woes\n- In the case of Colorado blue spruce, the changing climate has been a key factor. Blue spruces are native to the western U.S. mountains, where it’s cool and dry.\n- Higher temperatures are increasingly stressing cooler-preferring species, which includes Douglas fir, Fraser fir, larch, and concolor fir, as well as the blue spruce.\n- The extreme rains many areas have had in the past two years have led to rotting roots, as most needled evergreens don’t tolerate wet soil for long.\n- Sometimes rotting takes months to become apparent, which explains why trees seem to mysteriously die the spring or summer after a rainy year.\n- Rain and humidity are also ideal conditions for many disease-causing fungi. Healthy trees may head off attacks, but ones planted in compacted soil or in poor sites struggle.\n- Extreme and erratic shifts in temperature have been another recent stressor, especially when temperatures nosedive in fall before trees have had a chance to slide into winter dormancy.\n- Bugs are another major threat to evergreen, leading to a host of issues. Bagworms, spider mites, bark beetles, aphids, scale, sawflies, borers, and adelgids are among the insects that commonly target different needled evergreens.\n- As with disease, bugs tend to gravitate toward plants that have been stressed or compromised by other issues.\nNeedle cast disease is causing the needles of this Douglas fir to brown and drop.\nWhat to do?\nShort of regular fungicide and/or insecticide spraying, which is both difficult and expensive on mature trees, there’s not a lot that can be done to save diseased evergreens. The best course of action is to keep your trees as healthy as possible with good cultural practices. Some of these practices include:\n- Watering deeply once a week if a drought strikes.\n- Not packing excess mulch on the roots or trunk.\n- Testing the soil to provide adequate fertilizer and pH conditions.\n- Avoiding bark damage with weed-whackers and mowers.\nBetter yet is doing your homework at tree-buying time to pick the most trouble-free needled-evergreen choices and to get them in sites they prefer, which are generally full sun and loose, well-drained soil.', 'Solving Abiotic Tree Problems\nBy Dr. Sharon M. Douglas\nDepartment of Plant Pathology and Ecology\nThe Connecticut Agricultural Experiment Station\n123 Huntington Street\nP. O. Box 1106\nNew Haven, CT 06504-1106\nTelephone: (203) 974-8601 Fax: (203) 974-8502\nWhile tree problems attributed to cultural and environmental factors are common, they are becoming increasingly more common as populations increase and urbanization continues. In many cases there is little that can be done about these problems once they are observed so prevention is usually the best approach.\nI. Stress Promoting Elements:\nA. Acute- These are stresses that occur suddenly and cause damage soon after. Examples: improper pesticide sprays, frosts, freezes, injuries during transport.\nB. Chronic- These are stresses that occur gradually and appear as a general decline. Examples: nutritional imbalances, improper pH of the soil, too low light.\nII. Common Environmental Problems:\nA. Meteorological Effects:\na. Symptoms: Quite variable, depending upon time of damage; includes twig and branch dieback, bark cracking or splitting, leaf distortion; frozen tissues can turn blackish brown.\nb. Causal Factors: Damage can occur in late spring, early fall, and during dormancy; dormant frost damage can result in failure to leaf out.\na. Symptoms: Variable, including bark splitting and leaf scorch; new growth of ornamentals may be affected under extremely high temperatures; often occurs on the southwest side of the tree during winter or early spring (""southwest injury"").\nb. Causal Factors: Periods of extremely high temperature combined with windy conditions; often a combination of above freezing temperatures during the day and freezing temperatures at night.\na. Symptoms: Poor growth and vigor; undersized, off-colored leaves or needles; lower branch dieback is common.\nb. Causal Factors: Incorrect light level for tree species.\na. Symptoms: Trunk shattering, splitting of bark and canopy dieback; a long slash up the cambium spiraling down the tree where a 1-4 inch wide strip of bark has been ripped off; long splinters of wood at base of tree.\nb. Causal Factors: Lightning charge follows the most conductive path between top and roots, sometimes along the surface but often in outer sapwood.\n5. Winter Injury-\na. Symptoms: Dieback, foliar browning, sunscald, and bark splitting.\nb. Causal Factors: Late spring frosts (after growth has started), cool summer followed by a warm fall and drop in temperature, excessive or late season nitrogen fertilization, dry soil or root injury, frost cracking, excessive temperature fluctuations and drying winds, lack of snowcover.\nc. Commonly Affected Plants: Wide range of plants including broadleaved evergreens (rhododendron and mountain laurel), narrowleaved evergreens (arborvitae, yew, juniper, pine, and hemlock), deciduous trees and shrubs (weeping cherry, rose), and ground covers (pachysandra and ivy).\nd. Control Measures:\n1. select appropriate site for planting;\n2. have sufficient moisture in root zone before soil freezes;\n3. avoid late summer and early fall fertilization;\n4. mulch to increase moisture retention in winter;\n5. prune out dead branches or twigs in spring and fertilize to stimulate new growth;\n6. use of anti-transpirants or anti-desiccants.\nB. Air Pollution:\n1. Symptoms: Highly variable, depending upon type of pollutant and plant host; typically classified as acute or chronic; acute injury normally involves the death of cells and develops within a few hours or days following exposure to high levels of pollutants; symptoms include stippling or altered pigmentation, flecking, bleaching, chlorosis, interveinal and marginal necrosis, and tip necrosis; chronic injury typically develops more slowly, within days or weeks following exposure; this type of injury usually appears in response to long-term, low-concentration exposure; in some cases, visible symptoms are not present but exposure results in suppressed photosynthesis rates, stimulated respiration, and suppressed growth; symptoms are often subtle and easily confused with other problems such as normal senescence, nutritional disorders or other environmental stresses.\n2. Causal Factors: Major classes of phytotoxic air pollutants, in descending order of direct damage are: oxidants (ozone O3, PAN), sulfur dioxide (SO2), and fluorides (hydrogen fluorides HF).\n3. Commonly Affected Plants: Significant differences in sensitivity of plant species to specific pollutants occur; particularly sensitive tree species to specific pollutants are:O3--white ash, eastern white pine, black cherry, catalpa, honey locust; SO2--larch, birch, American elm, eastern white pine; HF--young, expanding needles of pines and spruces, paulownia, Douglas fir, serviceberry.\n4. Control Measures:\na. plant resistant or tolerant species where pollutants are known problems;\nb. maintain good plant vigor by proper cultural practices.\nC. Water Problems:\na. Symptoms: Loss of turgor in needles or leaves, drooping, wilting, yellowing, premature leaf or needle drop, dieback, poor growth, stunting, plant death; predisposes plant to secondary problems and cultural injuries; symptoms often not evident until the year after drought occurs.\nb. Causal Factors: Soil water becomes deficient and results in feeder root damage and death; plant unable to take up water.\nc. Commonly Affected Plants: Broad range of deciduous and evergreen trees and shrubs; effects are particularly severe on seedlings or recent transplants but established plants are also affected; especially affected this year were maple, ash, hemlock, juniper, dogwood, rhododendron.\nd. Control Measures:\n1. water in periods of low soil moisture;\n2. select appropriate site and use proper planting practices;\n3. select native plants adapted to local seasonal and annual variations in the water supply; drought sensitive (e.g., dogwood, many oaks, arborvitae, many Viburnum) vs. drought tolerant species (e.g., most pines, many Prunus, eastern larch, some junipers);\n4. prune out dead branches or twigs in spring.\n2. Excess Water-\na. Symptoms: Highly variable, including epinasty (downward bending of petioles), stem swelling, chlorosis, edema, reduced and stunted growth, twig dieback, wilting, leaf drop, root and plant death.\nb. Causal Factors: Root damage in flooded or waterlogged soils is associated with oxygen deficiency; damaged fibrous roots die, decay, and plants are unable to take up water; predisposed plants are subject to secondary invaders and opportunistic pests.\nc. Commonly Affected Plants: Seedlings and new transplants are more sensitive than established ones; dormant plants tolerate flooding longer than those in active growth; angiosperms are generally thought to be more tolerant than gymnosperms; particularly affected are yews, hemlocks, maples, rhododendrons.\nd. Control Measures:\n1. avoid plant stress by appropriate site selection and proper planting practices;\n2. maintain vigor by fertilization to stimulate good growth;\n3. select appropriate species for site and soil conditions, water-tolerant species (e.g., red maple, eastern larch, forsythia, green ash) vs water-intolerant species (e.g., gray and paper birch, crabapple, dogwood, eastern hemlock);\n4. prune dead or dying tissues to minimize problems from secondary invaders.\nD. Soil Modification: Trees are affected by many types of mechanical and chemical injuries and symptoms often do not show up until considerably after the damage has been done and often not until it is too late to save the tree; these injuries can result in significant damage the root system.\na. Construction Injuries: ""Bulldozer Blight"" often damaging the base of the trunk.\nb. Soil Compaction (Root Smothering): Roots are crushed by driving heavy construction equipment or trucks over roots.\nc. Root Cutting: Roots are cut when excavating for foundation walls, sidewalks, or streets.\na. Salt: Salt damage results from both direct sprays and from absorption through roots; one type of damage results from coastal flooding with salt water; a second type is associated with de-icing salts which cause damage to roots when they buildup and leach into soil and damage foliage and branches when salt-containing water form the ""spray zone"" comes in direct contact with plant tissues.\nb. Excess Fertilizer: Often results from over-application of lawn fertilizers and can cause excessive levels of soluble salts and subsequent root damage.\nc. Natural Gas: Gas leaking into the soil induces anaerobic conditions; microorganisms in the soil transform sulphates to hydrogen sulfide which inhibits respiration by the roots and nutrient uptake\nd. Herbicide: Careless or misapplied herbicides, most frequently associated with lawn applications of broadleaf weed killers such as 2,4-D or dicamba in root zones of woody ornamentals.\nE. Mechanical Injuries: These types of injuries result in direct physical damage to the tree and cause a variety of symptoms from canopy thinning to tree death.\n1. Lawnmower, String Trimmer\n2. Storm/Wind Damage\n3. Snow and Ice Damage\n4. ""Human"" Damage\nV. Nutritional Problems: Although considerable research has been conducted in order to understand nutrient imbalances associated with toxicities and deficiencies, the effects of either extreme are very difficult to diagnose. In many cases, soil and plant tissue analyses are necessary for accurate diagnosis. Symptoms of imbalance may appear on all or any parts of the tree but are most common on foliage. In some cases, nutrients may be present in the soil but are unavailable for uptake by the tree due to many factors including soil pH problems, competition with other ions, and root damage.\nVI. Animal Damage: Animals can cause significant damage to woody ornamentals in urban, suburban, and rural settings; damage results in a variety of symptoms from decline to sudden death.\nB. Voles/Meadow Mice\nVII. Other Problems: Trees are subject to so many other problems they are too numerous to mention. However, two common cultural problems are worth mentioning.\nA. Girdling Roots\nB. ""Flower Disease""\nTree problems associated with cultural and environmental factors are commonly found in the landscape. This discussion covers symptoms, causal factors, and ways to prevent these injuries from occurring. Among the topics covered are meteorological factors (e.g., frost, sunscald, lightning), air pollution, water problems, soil modification (e.g., compaction, herbicide misuse), mechanical injuries, and nutritional problems.']"	['<urn:uuid:2bc6c66a-cd68-4b27-ae27-d95724af80ee>', '<urn:uuid:600286cb-8f3c-4f3a-980e-c61206b969dc>']	factoid	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-13T04:31:27.499155	13	65	2130
60	telehealth rural challenges policy barriers	Telehealth implementation in rural areas faces dual challenges in infrastructure and policy implementation. The infrastructure challenge is significant - many rural and remote areas lack reliable, good quality internet access, which is essential for telehealth services. This requires coordinated lobbying efforts with other sectors that also need adequate internet services. Additionally, there are complex policy implementation challenges, including legal issues like licensure portability for healthcare providers, information disclosure requirements, and patient privacy concerns. The successful integration of telehealth requires addressing both the technical infrastructure needs and establishing clear policy frameworks that protect patient rights while enabling healthcare delivery.	['Jennifer Doggett reports:\nOne of the most important outcomes from the National Rural Health Conference in Darwin will be a set of recommendations for action to improve health services in rural and remote areas and to overcome some of the barriers to good health.\nThese recommendations will be developed via a collaborative and structured process open to all delegates. This process centres around the ‘Sharing Shed’, an online portal through which Conference delegates can propose recommendations to the full body of delegates, with every individual delegate being able to express a view on all ideas proposed by providing comments and by ranking or weighting separate recommendations.\nThe process will be moderated by a Conference Recommendations Group, whose members have already scrutinised the recommendations in the concurrent papers and are also reviewing those submitted during the Conference by delegates in attendance.\nThe Sharing Shed is accessible via laptops, tablets and smart phones and on-site computers are available for those without access to their own devices.\nAfter reviewing all the recommendations made, along with the votes and comments they attract, the Conference Recommendations Group will present a set of priority recommendations to a Conference plenary session tomorrow to enable delegates to indicate their broad support.\nNon-delegates are unable to vote but can view all the recommendations and comments on the conference website.\nRecommendations cover a broad range of issues, from public health, to clinical services, workforce issues and prevention. Infrastructure support for rural and remote health is a key theme and currently, the recommendation attracting the most votes is the following, which picks up on a recurring message arising from the plenary and concurrent sessions:\nDon’t assume rural and remote citizens have good quality, reliable internet. For Telehealth and other connecting technologies to increase access for rural and remote communities, we need to lobby for infrastructure that for some regions is just not there. The opportunity is to work with other sectors of the economy who also need adequate internet services to take advantage of the innovation online technologies can bring.\nThe next most popular recommendation comes from the Royal Australian College of General Practitioners and focuses on workforce reform:\nMore support for multidisciplinary team practice in addressing health disparities: a supportive policy focus is required which recognises that in overcoming significant disadvantage, including in rural areas, it is the capacity of the primary healthcare workforce that remains key to realising improved health outcomes in the community. Responsive policy needs to be broad and flexible enough to support community need, it must tackle the main drivers of poor health and ensure a fairer share relative to need if it is to provide for an enduring health benefit for all. Supporting multidisciplinary team practice through flexible and targeted funding will help to shift disparities but this requires sustained funding and commitment from Government.\nThe need to recognise the role of Indigenous people in the development of health programs and services is the subject of another recommendation which states that we should:\nEnsure that policies and program guidelines require local Aboriginal people to be involved in co-designing and delivering research and other projects. Ensure that programs recognise the investment in time and funds required for local Aboriginal people to co-design and participate in delivering projects.\nAnother recommendation focusses on the role of women in disaster response efforts:\nThe inclusion of women in local disaster preparation and response planning is essential. We need to identify and harmonise best gender and disaster practice across levels of government. This can include individual women, representatives of women’s services and or of women’s organisations, particularly women who have been trained in this area.\nReflecting the diversity of delegates at this Conference, the following two recommendations express conflicting views on non-medical prescribing:\nExpand Nurse Practitioner access to MBS Item numbers to reflect the work they do, particularly in rural and remote settings. Some NPs do the work of GPs where no GP will go and therefore should have similar Item numbers available. Also allow Nurse Practitioner access to Provider Numbers for facilities that are not 19(2) Exempt OR expand the exemption criteria to include facilities in towns with populations <50,000 people.\nRole and task substitution is not the answer to improving health outcomes or workforce shortages in rural communities. Patient safety and quality of care must underpin any development in delivering patient services. Non-medical prescribing in the case of pharmacists, in particular, is cause for concern: competency to prescribe and fragmentation of care being key issues. The known benefits of having a continuous and coordinated patient care framework cannot be ignored. The solution lies in more investment in GP-led primary care teams and team based care models in rural and remote areas to improve health outcomes.\nLess controversially, another delegate has focused on supporting access to allied health services in rural areas proposing that:\nAccess to Allied Health Services under Medicare should not be limited per calendar year to enable all people with Chronic Disease access to services to manage their chronic and complex care needs.\nPicking up on the comments this morning from Professor Alan Cass on the cost of food in remote communities, a number of recommendations address the need to improve the supply of fresh food in these areas. The following two recommendations propose some innovative solutions:\nGiven food security in many communities is a major problem, we need to look at sustainable funding models to address access, affordability and skills/knowledge. I propose an investigation of the feasibility and impact of taxing sugary drinks across Australia to fund programs which address these issues, with a particular emphasis upon targeting communities where healthy food is unaffordable for many families. Furthermore the money raised could be used to fund preventive nutrition programs which are currently funded in piece-meal ad hoc fashions. We saw this work for tobacco in Australia, we can apply the same principles for nutrition.\nFree fresh Australian food to Aboriginal remote communities. Many exporters throw away fresh healthy food as it is not the right shape or other obsessive standard for export quality. This food can be transported to Aboriginal remote communities at the government’s expense. Government interventions is needed because of market failure and a public health need to provide fresh healthy food to Aboriginal remote communities.\nVoting on the Sharing Shed recommendations closes tomorrow at 12.45pm and the input from delegates will be used to create a priority set of recommendations that will be taken to Government and other key influential bodies by the National Rural Health Alliance and its member bodies.\nThis will ensure that the ideas generated at the NRHC will continue to have an impact long after delegates have returned home.', 'The development of health policy can be challenging but actually implementing policies, once they become law can be even more challenging. This is due to the numerous legal, ethical, and financial management challenges associated with the implementation of various types of health policies. In addition to these challenges, conflicting ideas concerning what constitutes best management practices for successfully resolving those challenges can also complicate matters. Legal challenges include licensure portability, information disclosure, and mandated testing (Rutkow, Gable, & Links, 2011); ethical challenges include right of treatment, availability/accessibility of treatment, disclosure of knowledge concerning side effects, and ramifications of treatment (Mansour, 2011); and financial challenges include cost of treatment to patients and cost incurred by medical institutions (Richard, Witter, & Brouwere, 2010). This brief analysis will describe these eight legal, ethical, and financial challenges associated with the successful implementation of new policies regarding the delivery of healthcare as well as management practices that can help smooth the transition during the integration of the new policies into functioning practices.\nLegal challenges like licensure portability, information disclosure, and mandated testing (Rutkow, Gable, & Links, 2011) are all complex issues that arise through the instigation of new healthcare policies and procedures. Licensure portability involves the right of medical practitioners to practice in states that they are not licensed in, especially in the event of an emergency (Rutkow, Gable, & Links, 2011). Information disclosure pertains to the patient’s right to privacy in regards to the details of their health status, especially when regarding the particulars of their mental health (Rutkow, Gable, & Links, 2011). Mandated testing includes institutional regulations necessitating regular practices like mental health screening or mandatory vaccinations for first response workers or healthcare workers. Best management practices for the implementation of procedural regulations without the threat of obstructions due to legalities can be achieved through programs designed to afford medical practitioners the opportunity to participate in emergency licensure portability programs, ensuring patient’s have a right to privacy and that their information remains secure, and that mandated testing is not invasive and does not violate the worker’s rights or that their results are not disclosed.\nEthical challenges pertaining to the patient’s right of treatment, the availability and accessibility of treatment, disclosure of knowledge concerning side effects, and the ramifications of treatment (Mansour, 2011) are all details pertaining to the safety of the patient. The patient’s right to treatment pertains to the obligatory aspects of pharmaceutical production and whether companies have a moral duty to make vital medications available to all people. This ethical dilemma also directly correlates to the availability and accessibility of treatments to patients and whether medical practitioners have an obligation to make sure the benefits of life-saving treatments are available to all that need it. Informing patients about the true nature of any side effects or ramifications that may follow treatment is also an issue of grave ethical concern within the medical community due to the many complications that can arise from the adverse effects of numerous pharmaceutical products. Organizational issues can either enhance or jeopardize the safe administration of patient treatment, which is why it is important for administration to enforce stringent procedural practices that stipulate full patient disclosure regarding the benefits and detriments of all medications prescribed and also enforce practices that include programs that makes vital medications or treatments available to all that may need it.\nThe financial challenges associated with cost of treatment to patients and cost incurred by medical institutions (Richard, Witter, & Brouwere, 2010) is a huge challenge facing many institutions. The cost of treatment to patients includes the fees patients pay to insurance companies as well as fees required by the medical facility providing the treatments. The cost incurred by the medical institution relates to costs of treatment development or obtaining the treatment from the companies that have developed it, fees paid to medical practitioners, and general costs of operation. The significant costs associated with medicinal practice makes the establishment of regulations intended to make treatment available to all a difficult process since such policies could mean billions of dollars in lost revenues resulting from the research and development of medical treatments. This would impose a substantial stumbling block in the path of the future development of effective treatment.\nHowever, to augment the cost of life-saving medications and treatments to patients, many institutions have established policies that allow them to reduce the cost of treatment to patients that cannot afford it and medical practitioners have the option to practice in areas where the people do not have easy access to medical care. Fully resolving the legal, ethical, and financial challenges related to the implementation of policies and procedures in the healthcare industry is not something that can be achieved in a one solution fits all method and many institutions have to reevaluate methods and procedures to find strategies that work best with their employees and procedures.']	['<urn:uuid:5a5c6c17-2b5d-42b6-baa1-e3a0a8d7d2f3>', '<urn:uuid:74dbd240-df97-46fb-9da2-b91100cb95ae>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T04:31:27.499155	5	98	1922
61	zelenskyy baltic visit nato path details	During Zelenskyy's visit to the Baltic states starting in Vilnius, Lithuania announced a €200 million aid package and plans for joint defense production, particularly in anti-drone weapons. Simultaneously, European lawmakers, including representatives from Baltic states, are pushing for Ukraine to receive a clear pathway to NATO membership at the upcoming NATO summit, arguing this is essential to prevent further Russian aggression.	"['Zelenskyy\'s visit to Baltic States: Nausėda announces €200 million aid package for Ukraine\nOn Wednesday, January 10, President of Ukraine Volodymyr Zelenskyy arrived in the Lithuanian capital on a visit: the country\'s leader Gitanas Nausėda announced a €200 package of long-term assistance to Ukraine\nThe Ukrainian President announced the information on Telegram.\n""Lithuania, Estonia and Latvia are our reliable friends and principled partners. Later I will be in Tallinn and Riga, and today - Vilnius,"" he wrote.\nZelenskyy noted that he would hold talks with Lithuanian President Gitanas Nausėda, Prime Minister Ingrida Šimonyte, and Seimas Speaker Viktorija Čmilytė-Nielsen. Furthermore, scheduled meetings include dialogues with representatives from various political forces, media, and the Ukrainian community.\n""Security, integration into the EU and NATO, cooperation in the field of electronic warfare and drones, further coordination of European support. And of course, our great gratitude. For the uncompromising support of Ukraine throughout the 10 years of war, and especially now, after the start of the full-scale invasion,"" the President added.\nLithuanian President announces aid package for Ukraine\nDuring a joint press conference with Volodymyr Zelenskyy, Lithuanian President Gitanas Nausėda announced a €200 million long-term assistance package for Ukraine.\nHe highlighted that in January, the country plans to supply Ukraine with ammunition, generators, and detonation systems. Following this, in February, the provision will extend to M577 armored personnel carriers. In addition, the country will train Ukrainian soldiers and strengthen cooperation in the defense industry.\n""The agreements signed today between the Ukrainian and Lithuanian defense industries are another example of our close military cooperation. We must also ensure the continuity of long-term support from allies and partners. To help Ukraine, the European defense industry as a whole must step up its game, accelerate and respond adequately to the challenging security situation,"" Nausėda said.\nHe added that Lithuania is also forming a mine clearance coalition.\nAt the same time, the President of Ukraine emphasized that 2024 would be crucial not only for Ukraine but also for its partners.\n""In many ways, this year will be crucial for our countries: for Ukraine, for Lithuania, for all our partners. Together we must determine what decisions this year will bring us. Our strength must determine this, our unity must do it. These should be decisions in favor of our freedom, integrity, our states, cultures and a just peace for all of us,"" Zelenskyy said.\nHe emphasized that the weapons provided by Lithuania have been helping to defend Ukraine since the first days of the full-scale invasion.\n""All this gives us strength. I am grateful for the new defense support we have now agreed on. It is not just about assistance, but about joint production. In particular, anti-drone weapons, as well as other areas of defense production. The relevant documents have been signed today,"" the Ukrainian president summarized.\nZelenskyy holds talks with Nausėda\nThe President of Ukraine thanked his Lithuanian counterpart for the support provided to Ukraine.\n""We are immensely grateful for your personal support, the support of your team and the entire Lithuanian society, for the very cordial attitude towards Ukrainian people,"" Zelenskyy said.\nPhoto: Getty Images\nHe noted that Lithuania strongly supports Ukraine in the defense, humanitarian and political spheres.\n""Volodymyr Zelenskyy also emphasized the importance of the Republic of Lithuania\'s support for the historic decision to open negotiations on Ukraine\'s membership in the EU. The parties will discuss further steps towards Ukraine\'s European integration,"" the statement said.\nAt the same time, Nausėda said that during the talks, the priority will be given to issues that ""bring Ukraine closer to victory in the fight against Russian aggression."" In particular, the discussion will focus on the frontline situation, coordination of defense and financial support.\nPhoto: Getty Images\n""The parties will also coordinate positions in preparation for the NATO Summit to be held in Washington in the summer of 2024,"" the Presidential Administration added.\nEarlier, Lithuanian President Gitanas Nausėda said that if the West stops supporting Ukraine, the likelihood of Russian aggression against NATO member states will increase.', ""European lawmakers want Ukraine to have a pathway to NATO membership\nLEILA FADEL, HOST:\nThe Biden administration says that NATO members will be united in their support for Ukraine when the alliance holds a summit next month. But some members want the Western allies to do more than that and offer Ukraine a real pathway to join NATO. NPR's Michele Kelemen reports.\nMICHELE KELEMEN, BYLINE: Lithuanian lawmaker and former ambassador to the U.S. Zygimantas Pavilionis says the NATO summit in his country will be a big test. He was at the 2008 summit in Bucharest, when NATO agreed that both Ukraine and Georgia would become members. But the alliance refused to give either country a clear pathway to membership so as not to provoke Russia.\nZYGIMANTAS PAVILIONIS: Actually, non-enlargement of NATO provoked war in Georgia, in Ukraine, because we created gray zones. We signaled to Russia and Bucharest that we have no clue what to do in Ukraine and Georgia, though those nations are fighting and dying for our values; you know, those countries are up for grabs.\nKELEMEN: Pavilionis is the chairman of the Foreign Relations Committee in Lithuania's parliament. He came to Washington, along with his counterparts from Denmark, Poland and Estonia, saying the alliance needs to tell Russia's Vladimir Putin that there will be no more gray zones in Europe.\n(SOUNDBITE OF ARCHIVED RECORDING)\nPAVILIONIS: And the sooner we express it clearly in Vilnius, the sooner the Russians get this collective message of unity. This will be the end of war.\nKELEMEN: The Biden administration has just announced another $325 million in security assistance to Ukraine. NATO Secretary General Jens Stoltenberg told reporters in Washington that he believes the support is paying off on the battlefield as Ukraine carries out a counteroffensive against Russia's occupying forces.\n(SOUNDBITE OF ARCHIVED RECORDING)\nJENS STOLTENBERG: And Ukrainians are making progress, making advances. It's still early days. But what we do know is that the more land Ukrainians are able to liberate, the stronger hand they will have at the negotiating table and also the more likely it will be that President Putin, at some stage, will understand that he will never win this war.\nKELEMEN: In public, neither he nor President Biden addressed the issue of NATO membership for Ukraine, and the lawmakers who are visiting from Europe understand that this is a long-term proposition. But Estonia's Marko Mihkelson says he believes that giving Ukraine a clear pathway to join is the only way to restore stability in Europe.\nMARKO MIHKELSON: This is a biggest geopolitical battle in in the world since the end of World War II. And we have to understand that this is not only about the fate of Ukraine or Ukrainian nation, but also about our security architecture - transatlantic, Euro-Atlantic security architecture. Russia is trying to destroy NATO.\nKELEMEN: The chairman of the Foreign Affairs Committee in Denmark's parliament, Michael Aastrup Jensen, is hoping that Americans can rally behind this despite the divided politics in Washington.\nMICHAEL AASTRUP JENSEN: U.S. has been a world leader for so many years, and if we still want to uphold that world order, that is not China, it's not Russia that decides; but it's the free world, then we need leadership right now. And that's, I think, one of the messages that we try to convey to our American friends.\nKELEMEN: He says he was glad that the Biden administration recently agreed to allow countries like Denmark to provide Ukraine with F-16s. But he adds, it took too long, and it was too late to help with Ukraine's counteroffensive.\nMichele Kelemen, NPR News, the State Department. Transcript provided by NPR, Copyright NPR.\nNPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR’s programming is the audio record.""]"	['<urn:uuid:772bc7f9-143d-4239-9a0a-80a7f696365b>', '<urn:uuid:037cba2d-af77-4cd9-84ed-eb1116360288>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T04:31:27.499155	6	61	1325
62	how develop team unity sports training informal activities	Team unity can be developed both during training and through informal activities. During practice, strategies include rotating partners, creating small group activities, and establishing common team goals. Beyond training, it's beneficial to organize informal meetups where players can spend time together, share impressions, and strengthen their bonds in a relaxed environment. This combination of formal and informal interaction helps create stronger team cohesion and improves communication between team members.	"['Leader & Team Dynamics\nMaterials presented here are based in part on information in Wann, D. L. (1997). Sport psychology. Upper Saddle River, NJ: Prentice-Hall.\nThe purpose of this Page is to develop an understanding of the intricate relationship between leader style and behavior and sport team performance.\nIntroductionMost successful organizations or teams are managed by effective leaders. A leader may be the expert, the one who\'s in-charge, the person most respected by her/his followers, the man that controls aversive power or the individual that has the capacity to dispense rewards. In fact, a leader may possess any one, or any combination of the above described sources of leadership powers (Wann, 1997).\nEarly systematic studies on leadership focused on psychological traits that are common among proven successful leaders. By the end of the 1970s it became quite clear that leaders, such as, Joseph Stalin and Martin Luther King, were successful despite possessing very different traits, leadership styles and goals. Thus, the focus of inquiry has shifted from trait to other possible explanations of successful leadership.\nThe study of personal leader characteristics did not produce dependable predictions of successful leadership. An alternative explanation to successful leader performance is based on the study of leader behaviors. The premise of this approach is that leaders engage in specific behaviors that contribute to their success. Thus leaders are better defined by what they do as apposed to who they are.\nA directive or possessive style coach takes full charge of her/his team and its business, and closely monitors athlete behavior and performance.\nA permissive or ""laissez-faire"" style coach delegates responsibility to her/his athletes and thus has more time to personally handle issues that he/she deems most critical.\nAn autocratic or command style coach maintains single-handed control over decisions and action regarding team business. As pointed out by Wann (1997), a directive autocratic style is effective with young, unexperienced athletes who have a lot to learn and little to offer.\nA democratic or interactional style coach involves assistant coaches, team captains and other player representatives in team business related decisions. To maintain credibility, interactional coaches must lead by persuasion, i.e., explain their choice for action rather than force it on their assistants and athletes.\nTask- or Person-Oriented\nA task- or production- oriented coach, according to Wann (1997), are mostly interested in the task at hand. Precise descriptions of team members\' roles and responsibilities are of primary concern to the production-oriented coach. Assistant coaches and athletes alike are expected to be familiar with practice and match protocols.\nPerson-oriented coaches, on the other hand, emphasize interpersonal ties on the team. Teams headed by a player-coaches, as Wann (1997) suggested, and less competitive teams that display strong social relationships are most likely to have and benefit from this leadership style.\nTheories of Leadership\nFiedler\'s (1978) Contingency Theory\nUtilizing laboratory controlled experiments, Fiedler (1967), tested his contingency theory of leadership. His model includes three situational factors (group atmosphere [good or poor], task structure [high or low], and leader position power [strong or weak]). His analysis predicted group performance based on a leadership style that corrsponds to situational factors and their direction or value. A re-examination of Fiedler\'s original data during the early 1980s using a different statistical analysis procedure yielded insignificant and unconclusive results. Fiedler\'s model like Kohlberg\'s, and the McClelland-Atkinson models is still relevant, not because it provids an accurate theoretical framework, but because it sparked a debate that paved the way to new, more robust theories of leadership (moral development, and motivation).\nThe Normative Theory of Leadership\nUtilizing Vroom and Jago\'s (1988) normative theory of leadership, Chelladurai (1993) proposed a normative model of decision styles (autocratic, participative, and delegative) in coaching (Wann, 1997). A casual observer of the dynamics on a typical competitive sport team would conclude that coaches make all decisions alone and take the blame for failure. Athletes on the other hand, like to concentrate on their responsibilities as players and prefer not to get involved in coaching. The available research clearly suppports the above observation.\nThe Attributional Theory of Leadership\nThe Normative Theory of Leadership\nThe Multidimensional Theory of Leadership\nDream Teams: From Collections of Individuals\nto Effective Sport Teams\nAnyone traveling around the world and observing different people can readily notice that human beings spend a great amount of time doing things together in groups. German sociologist, Kurt Lewin coined the term ""group dynamics"" and created fertile grounds for new ideas such as ""group culture"" and ""group mind.""\nYet, do terms like ""group mind,"" or ""team spirit"" make any sense when examined through a Newtonian perspective of reality? Can the extent of the team\'s spirit be somehow measured? Is a team\'s performance a reflection of the added individual talents of its individual members or is a team\'s performance a reflection of a sum that is greater, or smaller than, the tally of each of the individual performances?\nSir Isaac Newton\'s motto was to make no hypotheses. Newton\'s criterion for the validity of any observation was an experimental verification of that observation. Newton would put an observation in writing if he could produce the same result over and over again, and others using the same procedures could also get the same results. Given the fact that one has all the necessary information, according to Newtonian physics, it is possible, in principle, to predict exactly how a given event is going to unfold.\nQuantum mechanics (the study of motion of quantities) or the new physics is not an alternative for the old physics but is an extension of it. When the atomic and sub atomic levels are studied, the available data can only predict the probability of a certain event as opposed to the precise event as is the case in Newtonian physics. Subatomic particles cannot be pictured as an object, rather they can be viewed as ""tendencies to exist"" or ""tendencies to happen."" Experiments in the subatomic realm demonstrate that there is no way to predict individual events at that level. Therefore, quantum mechanics concerns itself only with group behavior (example of billiard balls vs. subatomic particles).\nPsychologists William McDougal and Floyd Allport led two opposing views regarding the ""group mind"" controversy (Gergen, 1982). F. H. Allport was a harsh critic of the anthropomorphic conception of human groups. In his view only individuals were real and groups or institutions were ""sets of ideals, thoughts, and habits repeated in each individual mind and existing only in those minds (Allport, 1924)."" Allport\'s view of interactions between members of a group is analogous to the relationships among billboard balls. They move and hit each other and affect each other in precisely predictable ways, and stay intact throughout the whole process. McDougal, on the other hand, held the position that groups, institutions, and culture formed new realities and forces that could not be explained by strictly adding the particular individual group members\' talents and contributions. McDougal\'s view is analogous to the relationships between subatomic particles which mix and merge with the neighboring particles and create new relationships.\nFloyd Allport\'s individualistic orientation was the dominating view in academia until Mayo (1933) and his associates reported their extensive research at the Hawthorn plant of the Western Electric Company. What started as a project to investigate the relation between conditions of work and the incidence of fatigue among workers ended up changing radically and irrevocably the thinking about industrial worker dynamics. ""The role of the leader began to shift from one who directed work to one who enlisted cooperation. The incentive to work was no longer seen as simple and unitary but rather infinitely varied, complex, and changing (Haire, 1954).""\nSports, as most sport scientists would agree, is a microcosm of society--it mirrors the values, structure, and dynamics of the society in which it exists (Coakly, 1994). It is no wonder, therefore, that the concept ""group mind"" was eventually investigated in the realm of sports and sports teams. Different groups, as well as sport teams, display a great variety of properties such as size, duration, objectives, internal structures, norms and many other aspects. The large variety of properties displayed was the main reason for the difficulty in the formulation of an all encompassing definition of the term ""group."" Such a definition would have to provide a clear distinction between those social entities to be called ""group"" and those to be given some other name.\nAccording to Homans (1950) ""A group is defined by the interaction of its members."" Lewin\'s (1948) point of view was that a group is best defined as ""...a dynamic whole based on interdependence rather than on similarity,"" and Bass (1960) defined \'group\' as ""...a collection of individuals whose existence as a collection is rewarding to the individuals."" It does not take a sport sociologist to see how each of the above mentioned definitions of \'group\' describe some aspects of a sport team.\nThis presentation focuses on the term first introduced by Kurt Lewin in 1935 ""cohesiveness"" (Cartwright & Zander, 1968) and the term ""group dynamics."" The relationship between social cohesion, group dynamics and sport team participation and performance will be examined. Athletes coaches and researchers alike often assumed that when players on a team display unity and ""stick together,"" they will have a greater chance of team success. Although some evidence supports this view (Arnold & Straub, 1972); Ball & Carron, 1976; Carron & Chelladurai, 1981; Gosset & Widmeyer, 1981; Widmeyer & Martens, 1978), there is also research which fails to provide support (Melnick & Chembers, 1974; Ruder & Gill, 1981), or research that support the opposite view; that is--there is a negative relationship between team cohesion and performance (Landers & Lueschen, 1974; Martens & Peterson, 1971).\nDeveloping the Sport Team Concept\nTeam Roles: Formal vs. Informal Roles\nrole clarity, role acceptance, and role performance\nEstablishment of Group Norms\nNorm for Productivity\nStabilizing Group Structure\nZander (1982, cited in Carron, 1986, p. 82) provides the following suggestions for the stabilization of group structure:\nShow individual team members how the group\'s standards can contribute to the achievement of desirable qualities for the team, more effective team performance, and a greater sense of team unity.\nPoint out to all team members how their contribution toward developing and maintaining the standards can contribute to the team\'s success.\nDevelop a method of assessing whether there is adherence to the group\'s standards, and then reward those team members who do adhere and sanction those who do not.\nProperties Associated with Cohesiveness\nTeam Cohesiveness Defined\nFestinger, Schachter, and Back (1950): ""...the sum of the forces that cause members to remain a part of the team.""\nGross and Martin (1951): ""...the resistance of the group to disruptive forces"".\nCarron (1982, p. 124): ""...dynamic process which is reflected in the tendency for a group to stick together and remain united in the pursuit of its goals and objectives."" Models of Team Cohesiveness\nCohesiveness: A Positive or a Negative Force?\nThe Circular Nature of Team Cohesiveness\nCorrelates of Team Cohesiveness\nSummary and Discussion\nYukelson\'s (1984) nine effective ways to enhance coach-athlete communications and team harmony:\nExamples of specific strategies that may be utilized to facilitate team cohesiveness: (Adapted from Carron & Spink, 1991)\nExample of Intervention Strategies Used\nDistinctiveness: Solicit suggestions for team name and vote on the submitted titles. Similarly, choose a team logo and uniform. Create chants, slogans, and team routines.\nIndividual Positions: Assign personal lockers and personal equipment. Let team members pick their own spot in the grid or on the field during warm-up, and encourage them to remain in it throughout year.\nGroup norms: Have members introduce each other to increase social aspects. Encourage members to become fitness friends. Establish a goal to improve certain aspects of a skill together. Promote a smart work ethic as a group characteristic.\nIndividual Sacrifices: Use the help of the more skilled team members to improve the performance of the less skilled ones. Ask individual members for their goal for the day and try to accommodate them, even though it may not be the wish of the entire team.\nInteraction and Communication: Create activities that require a partner. Make sure that team members practice, at least for part of the time on any given day, with a different partner. Create activities that engage small groups and rotate group members among the mini teams.\nDefinitions: (Based on lecture notes, and unpublished manuscript by Dr. Merril Melnick, SUNY Brockport, New York, Spring 1982)\n1. Clique: ""A relatively small, informal, voluntary group (two persons or more), without a formal structure, based on mutual interests and usually friendship. The relationship among members of the clique are usually intimate and cooperative ... The members of a clique share certain common interests that may be at variance with the structure or goals of the larger organization. The structure of social relationships within the clique is not part of and may to some extent run counter to the formal social structure of the group"" (Theodorson & Theodorson, 1969).\n2. Conformity: ""Behavior that is in accord with the expectations of a social group . . . the endeavor is to maintain the group\'s standards"" (Theodorson & Theodorson, 1969).\n3. Culture: ""The common meanings, the definitions of a situation\nCohesiveness Defined and Conceptualized\n1. Cohesiveness as attraction-to-group (a-t-g).\n"" Cohesiveness of a group is... the resultant of all the forces acting on the members to remain in the group... in other words, cohesiveness is the attraction of membership in a group for its members (Festinger, Schacter, and Beck, 1950).\n2. Cohesiveness as interpersonal attraction (IA)\n""... regardless of the unique properties in terms of which groups may be described, a group is inescapably made up of individuals. It is suggested, therefore, that when we speak of a group as being attractive, we are referring actually to the attractiveness of the members of the group"" (Lott, 1961)\n""... that group property which is inferred from the number and strength of mutual positive attitudes among the members of a group"" (Lott and Lott, 1965)\n""When the group dynamicist speaks of the \'attraction of the group for the individual\' does he not mean just attraction of the individuals for one another? If individuals are all drawn toward one another, are they not ipso facto drawn to the group?"" (Allport, 1962)\n""Cohesiveness and attractiveness are two similar ways of describing the same thing. It seems obvious that if a collection is more attractive to each of its members, each of its members must be more attracted to each other"" (Bass, 1960).\n3. Cohesiveness as intrinsic attraction (a-t-g) and instrumental attraction (social satisfaction).\n""Group cohesion refers to the relative attraction, both intrinsic and instrumental, of a small group for its individual members... Cohesion should be reviewed as a bidimentional property of small groups considering of intrinsic attraction or \'sociometric cohesion\' (a-t-g) and, instrumental attraction or \'social satisfaction\' (IA) (Enoch and McLemore, 1965)\n4. Cohesiveness as the unification of the group field.\n""Cohesiveness appears to be a group concept but has, in reality, been dealt with on an individual level...Attraction-to-group is on a lower level of abstraction than cohesiveness... It might be conceivable to develop a concept of \'cohesiveness\' which... refers to the group as a whole and not only to the individuals composing it. Such a concept, however, should not include a-t-g. To prevent contamination with individual motivation, the tipological approach seems to be more fruitful. The essence of cohesiveness should not be the staying in or leaving of the group but should be related to the degree of unification of the group field"" (Bergen and Kolkebakker, 1959)\n""...the ability of individual members to work together... The coach often refers to this ability as teamwork, togetherness, or morals, while the researcher refers to it as group interaction or group cohesiveness... The ability of individuals to effectively interact with teammates to obtain a group-desired goal has been recognized as contributing to team effectiveness"" (Martens and Peterson, 1971)\n5. Cohesiveness as uniformity/conformity.\n""Although there may be numerous definitions of team cohesion, each of which would be correct, for our purpose, we define it as a group of individuals thinking, feeling, and acting as a single unit"" (Tutko and Richards, 1971).\nCal State LA\nCoaching Philosophy ||\nPlayer Development || Fundamentals || Setting Goals || Practice Ideass\nLeadup Games || Getting Ready || Endurance || Flexibility || Nutrition\nPositive Discipline || Safety Tips || Related Links\nLast Modified: February 19, 2011', 'How to Develop Social Intelligence Through Sports\nFor humans, as social animals, social intelligence is a fundamental aspect of relating to others. We need other people in order to live, grow, and satisfy our physical and emotional needs.\nIntelligence is a global concept that deals with all facets of the human mind, including social and emotional. Every day we’re learning more about how this concept isn’t just for doing mental calculations or solving problems.\nThe area of sports, especially team sports, can provide a great opportunity to develop this type of intelligence. Being part of a team, interacting with your teammates, and living significant experiences together helps us understand emotions and communicate with others.\nFor this reason, we’ll explain how to develop social intelligence within the sports world.\nCreate spaces of emotional expression\nSports can generate strong emotions in people, both positive and negative. For the majority of individuals, sports and teams are an essential part of their identity. Everything that happens within this context can be felt with great intensity.\nTrainers and coaches should aim to create spaces in which athletes can express how they feel. This also means being listened to and supported by their teammates. Expressing our feelings isn’t only therapeutic and liberating, but can also help people to better understand the emotions of other people and learn to regulate them.\nYou have to carefully choose these moments of expression. It’s not appropriate to do it, for example, right after a game when emotions can be really intense. It’s better to let a few days pass so that the players have time to reflect on what happened.\nSolve conflicts through dialogue and negotiation\nConflicts shouldn’t be seen as something negative or something that should be avoided. It’s normal that, after hours of training and competitions, there might surge strong emotions between two people.\nThe best strategy is to teach players to resolve this conflict through dialogue. Acting as if nothing happened isn’t a good option. There should be an opportunity for those involved to resolve the misunderstanding and arrive at an agreement.\nIn the case that the athletes can’t solve the conflict themselves, a coach could intervene as a mediator. The mediator has to listen to both sides and get both parties to arrive at an agreement that satisfies both.\nDevelop affectivity outside of training\nThe relationships between players go beyond the moments of training. The links between them can be so strong that they consider themselves a large family.\nTo develop social intelligence, it’s beneficial to take the affectivity of the players outside the world of training. This means searching for informal moments where players can meet up and share impressions, sensations, or just spend time together as a team.\nThese informal moments allow players to strengthen ties and create a relaxed environment that can be difficult to create during practice. The coaches and other team professionals should also participate in these types of meetups. This can help stimulate the feeling of belonging.\nPotential for cohesion within the team: social intelligence\nCohesion is the glue that keeps the team together. It’s an internal force that sustains the group and permits the interactions among the athletes.\nA team that’s cohesive is a team in which communication is good and fluid. In addition, since there’s a strong link between members, the importance of understanding and listening to each member is greater. In this way, athletes will feel that their teammates are essential for the team’s function.\nThere are many ways to increase the cohesion of a team. Establishing common goals and making sure that everyone is included in making big decisions for the team are ways to increase group identification.\nDeveloping social intelligence can improve performance\nIntelligence deals with basic cognitive abilities such as attention, memory, or the ability to multitask. However, this concept would be incomplete if we didn’t take into consideration other ways of relating to others and interpersonal intelligence.\nIncreasing social intelligence has a positive impact on sports performance. If you make an effort to understand and communicate with others, the group will work together in a more organized, cohesive way.\nFinally, according to a study published by the European Scientific Journal, cohesion is positively related to beliefs of efficacy and group performance. As we can see, emphasizing the social part of teamwork is a safe investment to ensure that it can work better.It might interest you...']"	['<urn:uuid:85dd213d-ffcc-480c-be7f-229f2c548938>', '<urn:uuid:b034e023-7ddf-4f0c-9a9d-e520518fdf4f>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	8	69	3460
63	engineer behind steam evaporation process	The multiple-effect evaporator was invented by Norbert Rillieux, an African-American engineer. He revolutionized sugar processing with this invention, which efficiently uses the heat from steam to evaporate water under vacuum conditions.	"[""advantages of multiple effect evaporator pdf file size\nMultiple Effect Evaporator | Evaporation | Steam A multiple-effect evaporator is an apparatus for efficiently using the heat from steam to evaporate water. In a multipleeffect evaporator, water is boiled in a sequence of vessels, each held at a lower pressure than the last. Multiple Effect Evaporator. The multiple-effect evaporator was invented by the African-American engineer Norbert. Rillieux.\nPrinciples and Pressure Drop Calculation in Multiple Effect ... In this article discussed about history of multiple effect evaporator, Rillieux's principles for multiple effect evaporator bodies and distribution of pressure drops across the multiple effect evaporators like Triple effect, Quadruple effect and Quintuple effect and also given online calculator for pressure drop calculation in multiple effect evaporator with values of latent heat and ...\nMultiple Effect Evaporators - Rosenblad Multiple-Effect Evaporators. Multiple-effect evaporators maximize steam economy as vapor generated from one effect is used to drive another. This type of multiple-effect system is perfect for continuous feed applications, as fluids are progressively reduced in each effect to final concentration discharge.\nMultiple Effect Evaporator - Swenson Technology The multiple-effect configuration combines two or more evaporator bodies to conserve steam, which is condensed in the first-effect heat exchanger only. Water evaporated in the first-effect vapor body is condensed in the second-effect heat exchanger, which provides energy for evaporation in the second-effect vapor body (and so on for additional ...\nMultiple Effect Evaporators - Advantages :: Shailvac :: Surat ... Multiple Effect Evaporators - Advantages Understanding the needs and concerns of our customers we have been taking complete turnkey projects. As a part of TURNKEY PROJECTS, we have been designing and developing AXIAL FLOW PUMPS in various MOCs which are regularly tested on our test benches and then dispatched to our customers.\nWiped Film Evaporator –WFE In essence, the Pfaudler Wiped Film Evaporator (WFE) is designed to continuously separate volatile compounds by introducing a mechani-cally agitated, thin film of feed material to a heated surface. The short residence time allows for efficient and reliable processing of a wide variety of high-boiling, heat-sensitive and/or viscous products.\nMultiple-effect evaporator - Wikipedia A multiple-effect evaporator, as defined in chemical engineering, is an apparatus for efficiently using the heat from steam to evaporate water. In a multiple-effect evaporator, water is boiled in a sequence of vessels, each held at a lower pressure than the last.\nMulti Effect Evaporators - slideshare.net Multi Effect Evaporators 1. Multiple Effect Evaporators By: Nishant Arora [email protected] 2. Definition Evaporation is the process used to concentrate a solution by removing the solvent (mainly water) in a purified form by the application of heat. Evaporation is the unit operation by which solvent is evaporated from the solution by boiling the liquid in suitable vessel and ...\nInvestigation of Multiple Effect Evaporator Design 3.1. Feeding of multiple effect evaporator Multiple effect evaporator divided in to four categories on the basis of feed direction as shown Figure 2-5. These are forward feed multi effect evaporator, backward feed multi effect evaporator, mixed feed multi effect evaporator and parallel feed multiple effect evaporator. In the\nUnit Operations in Food Processing - R. L. Earle 8.1. Single effect evaporator - steam usage and heat-transfer surface 8.2. Water required in a jet condenser for an evaporator 8.3. Heat exchange area for a surface condenser for an evaporator 8.4. Triple effect evaporator - steam usage and heat transfer surface 8.5. Duhring Plot for sodium chloride 8.6.\nDynamic Simulation of Multiple Effect Evaporators in Paper ... Dynamic Simulation of Multiple Effect Evaporators in Paper Industry Using MATLAB Deepak Kumar', Anjana Rani Gupta', Somesh Kumar' UDepartment of Mathematics, NIET Greater Noida, UP: India\nEvaporators - Visual Encyclopedia of Chemical Engineering The recycle rate within the evaporator is much greater than the feed rate. A downtake ensures that the liquid from the vapor head is recycled to the bottom of the evaporator in the minimum time. Usage Examples Short-tube vertical evaporators are used extensively in the evaporation of cane-sugar juice.\nNorbert Rillieux - American Chemical Society Norbert Rillieux (1806-1894), widely considered to be one of the earliest chemical engineers, revolutionized sugar processing with the invention of the multiple effect evaporator under vacuum. Rillieux's great scientific achievement was his recognition that at reduced pressure the repeated use of ...\nFundamentals of Multiple Effect Evaporation - Desalination In the evaporator, the seawater fed in is heated to its boiling temperature and is partly ... Vol. II - Fundamentals of Multiple Effect Evaporation - M.A. Darwish ...\nMathematical modeling of multiple-effect evaporators and ... In modeling of multiple-effect evaporators, a pressure and temperature value is set for each evaporator. The necessary enthalpies for these pressure values are found from thermodynamic tables and diagrams. The mass, component and energy balances are provided for each evaporator and also for the system.\nCOMPARISION OF SINGLE AND MULTIPLE EFFECT EVAPORATOR - Blogger The single effect evaporator uses rather more than 1 kg of steam to evaporate 1 kg of water. The latent heat of the vapor leaving in single effect evaporator is not used but is discarded. Much of this latent heat, however, can be recovered and reused by employing a multiple - effect evaporator, that is, vapor from one effect serves as heating ...\nThermal Analysis on Triple Effect Falling Film Evaporator pressure. Multiple effect evaporator having a multistage flash evaporator and adapted for making fresh water from feed water, and more particularly to a single casing multiple effect evaporator. Multiple effect evaporators generally have several evaporator effects which are arranged serially according to the operating temperature.\nCirculation evaporator - Wikipedia Multiple heating effects can be used to increase thermal efficiency. In this system design extracted vapor is used as a heating medium for the 2nd heating effect at a lower pressure than the first effect. This can be repeated for multiple effects. Natural Circulation evaporator characteristics\nmultiple effect evaporator - S.K.Associates Our Manufacturing team expert in stage wise evaporator manufacturing with strict in quality control and perform various test to complete multiple effect evaporator. This enables us on Timely delivery of equipments and site erecting work. Our Service Engineers give operation and maintenance training to Clients end.\nUS2769489A - Multiple effect evaporator - Google Patents The vapor from the vapor-liquid separator 33 leaves through an outlet 35 connecting with a source of vacuum so that the multiple effect evaporator operates under vacuum. The vertical vapor line or tube 16 has an upward extension 36 leading to the bottom of a separation chamber 37 having a large horizontal area.\nAnalysis of single-effect evaporator desalination systems ... or multiple effect evaporation (MEE). This problem is addressed to a large extent by the design and development of single-effect evaporator desalination processes. Such systems are planned with a strong emphasis on their production capacity and cost. A single-effect evaporator desalination unit must have sufficient\nEvaporators - SlideShare Multiple effect evaporator Advantages Suitable for large scale & for continuous operation. Highly economical when compared to single effect. Multiple effects, or stages, are now used to minimize the energy input required to evaporate or boil off undesirable water content. The total evaporation achieved in these systems is approximately the ...\n(PDF) A simplified and generalized method to size multiple ... A simplified and generalized method to size multiple-effect evaporator systems with any feed scheme Conference Paper (PDF Available) · August 2005 with 4,119 Reads\nModeling and Simulation of Multiple Effect Evaporator System The modified seven effect evaporator system, obtained using best model, requires four shell and tube heat exchangers and five pumps. This modification has total capital investment as Rs 29.3 lakh. However, saving in steam consumption is found as Rs 21.8 lakh/year thus, total payback period for the modified seven effect evaporator system is 1.3 ...\nOptimization of a multiple Effect Evaporator System called a single effect evaporator system and if more than one evaporator is used for the concentration of any solution, it is called a multiple effect evaporator system. In a multiple effect evaporator the vapour from one evaporator is fed into the steam chest of the other evaporator.\nSimulation of Quadruple-Effect Evaporator with Vapor Bleeding ... obvious that a realistic simulation of quadruple-effect evaporator must take into account its interaction with juice heater. Previous studies concerning multiple-effect evaporator have paid little attention to this interaction -. In this paper, a coupled model of the quadruple-effect evaporator and juice heater is presented. This model takes\nRMP Lecture Notes - Christian Brothers University Multiple Effect Evaporators. Evaporators are classified by the number of effects. In a single-effect evaporator, steam provides energy for vaporization and the vapor product is condensed and removed from the system. In a double-effect evaporator, the vapor product off the first effect is used to provide energy for a second vaporization unit.\nModeling and dynamic simulation of mixed feed multi-effect ... In the present study dynamic behavior of multi-effect evaporator system of a paper industry is obtained by disturbing the feed flow rate, feed concentration, live steam temperature and feed temperature. For this purpose an unsteady-state model for the multi-effect evaporator system is developed.\nEvaporator Handbook - APV Hemisan contrast, the falling film evaporator does not have a driving force limitation—permitting a greater number of evaporator effects to be used within the same overall operating limits. For example, if steam is available at 220°F (104°C), then the last effect boiling temperature is 120°F (49°C); the total available ΔT is equal to 100°F (55°C).\nMULTI-EFFECT DISTILLATION (MED) Figure 1. Schematic view of industrial Multi-Effect evaporation (McCabe et al., 2001). Figure 2. Schematic view of a horizontal tube Multi-Effect Distillation plant (IDE Design, Internet publication). Steam condensation inside horizontal tubes and seawater evaporation on the outer side is the heart of one of the most common MED processes.""]"	['<urn:uuid:77caefdd-1ae7-4d42-b7f4-1b70e121495b>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	5	31	1623
64	As someone interested in organic farming, I've always wondered - do organic farms actually put more organic matter into their soil compared to regular farms?	Not necessarily. Field management data shows that organic matter returns are not always larger in organic systems. Many non-organic farms regularly apply manure, and their higher-yielding crops may actually return larger crop residues to the soil. Conversely, many organic fields receive little or no manure, instead relying on fertility building ley phase for organic matter input. Management practices vary greatly within both organic and non-organic systems, and all these different practices have consequences for soil fertility.	['Shepherd, Dr M. A. (2002) Understanding soil fertility in organically farmed systems (OF0164). ADAS Consulting Ltd .\nThis is the final report of the Defra project OF0164.\nOrganic farming aims to create an economically and environmentally sustainable agriculture, with the emphasis placed on self-sustaining biological systems rather than external inputs. Building soil fertility is central to this ethos. ‘Soil fertility’ can be considered as a measure of the soil’s ability to sustain satisfactory crop growth, both in the short- and longer-term. It is determined by a set of interactions between the soil’s physical environment, chemical environment and biological activity. The aim of this project was, therefore, to provide a better scientific understanding of soil fertility under organic farming.\nThe approach was to undertake a comprehensive literature review at the start of the project to assess and synthesise available information. Studies were then designed to address specific questions identified from the literature review.\nThe literature review was written during the first year of the project. In addition to submitting written copies to DEFRA, the chapters were posted on a project website: www.adas.co.uk/soilfertility.\nThe Review was based around key questions:\n• What are the soil organic matter characteristics and the roles of different fractions of the soil organic matter?\n• Do organically managed soils have higher levels of organic matter (SOM), with a resultant improvement in soil properties?\n• Is the soil biology different in organically managed soils, in terms of size, biodiversity and activity?\n• Do organically managed soils have a greater inherent capacity to supply plant nutrients?\n• What are the nutrient pools and their sizes?\n• What are the processes and rates of nutrient transfer in relation to nutrient demand?\n• What are the environmental consequences of organic management?\nThe project also included a large amount of practical work. This necessarily covered a wide range of topics, which were examined in a series of separate studies:\n• Soil microbiology: a series of measurements focusing on two sites, undertaken by University of Wales Bangor (UWB)\n• Field campaigns in autumn 1999 and spring/summer 2000: separate field sampling campaigns focusing especially on nutrient pools, undertaken by HDRA, ADAS and IGER\n• Incubation studies: a series of three separate experiments to look in more detail at N dynamics, managed by ADAS, with support from IGER and HDRA\nFrom the literature review and the practical work, the following was concluded:\nOrganic matter is linked intrinsically to soil fertility, because it is important in maintaining good soil physical conditions (e.g. soil structure, aeration and water holding capacity), which contribute to soil fertility. Organic matter also contains most of the soil reserve of N and large proportions of other nutrients such as P and sulphur.\nField management data gathered from farmers showed, however, that organic matter returns are not necessarily larger in organic systems. Many non-organically farmed soils receive regular manure applications and the generally higher yielding crops on conventional farms may return larger crop residues. Conversely, many organic fields receive little or no manure, relying on the fertility building ley phase for organic matter input. This observation is important. Management practices within organic and non-organic systems are diverse, and all have consequences for soil fertility.\nThe Executive Summary at the start of the main attached report has additional sections on Soil Structure, Soil Biology, and Nutrient Cycling with some greater detail on comparisons of organic and conventional management and the consequences for soil fertility.\n|Keywords:||soil fertility, rotations, nutrient cycles, soil structure, soil biology, knowledge transfer, nitrogen, phosphorus, potassium, OF0164|\n|Subjects:|| Soil > Nutrient turnover|\nSoil > Soil quality > Soil biology\nKnowledge management > Education, extension and communication > Technology transfer\n|Research affiliation:|| UK > Garden Organic (HDRA)|\nUK > Institute of Grassland and Environmental Research (IGER)\nUK > Other organizations\nUK > ADAS\nUK > Department for Environment, Food and Rural Affairs (DEFRA)\n|Deposited By:||Defra, R&D Organic Programme|\n|Deposited On:||13 Apr 2006|\n|Last Modified:||12 Apr 2010 07:33|\nRepository Staff Only: item control page']	['<urn:uuid:164f39af-8a37-4864-abcd-2ab1c37f2099>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	25	76	660
65	What makes Zeplin stand out for app developers?	Zeplin is specifically designed for app design and offers notable features for development workflow. It includes easy style guides, Slack integration, and efficient development hand-off features. The tool provides web inspector and Sketch syncs, making the development stage smoother. It has a gentler learning curve compared to Adobe products and is increasingly expanding into web design.	['Whether you are a novice or an experienced designer, the vast expanse of designing tools can seem overwhelming and even more daunting than the design work itself. In this guide, some of the most popular and useful options are explained for you.\nFeatures: Mac design, Shared Styles (Reusable Elements), Layer Editing, Real-time Previews\nThis robust and versatile design tool is one of the most popular out there. The large community has opened doors for an open-source feel of app resources. The tool heavily focuses on UI design, which makes web, app, and product design more intuitive as you design.\nStyling text, colors, and other repeating elements are easily accessible and the use of artboards and layers allows for quick and precise work.\nAdobe has been the leader of design tools for years, and for a reason. Their design products come in the form of multiple options.\nAdobe Photoshop Features: Robust Image Editor, Suitable for Web Design, Easy Interface, Mac & PC\nPhotoshop was initially created for (and is amazing at) photo editing. The interface takes a bit to learn to use but with such high popularity, tutorials and classes are available at a huge quantity.\nEarly web developers used Photoshop’s design options to by-pass the photo editing and skip right to graphic design because it was one of the only options. The tools for this purpose are available, but limited compared to new products created for web design or graphic design. Things like repeating elements, UI design, and other graphic/web design elements can be lacking.\nAdobe Illustrator Features: Use of Vectors, Robust Illustrative Tools, Detail Oriented, Large Popularity and Support, Mac & PC\nJust as the name implies, Adobe’s Illustrator has a hardy list of tools for illustrations. Most logos, cartoons, and other graphics are made on a tool like (if not on) Illustrator. Illustrator is based on vectors, meaning they can scale to be infinitely large without any pixelation because it doesn’t use pixels.\nAlthough it has an infamously large learning curve, Illustrator is ridiculously powerful and has all the options you could dream of. Just like Photoshop, though, the design processes for web design are lacking. This tool is better used for logos, icons, or other vector graphics, not web pages. Many still use Illustrator for this purpose and is not to be mocked because of the power of Illustrator, but if UI design is important to you, other tools may be more fit for your work.\nFeatures: App Design, Easy Style Guides, Slack integration, Easy Development Hand-Off\nZeplin, a product designed specifically for app design, is quickly becoming a popular choice. The exporting features and collaboration options are often sought after to help workflow and Zeplin is dedicated to help streamlining it. As for when you pass it off to the development stage, the web inspector and Sketch syncs are very helpful.\nThis tool is starting to grow into the web design field as well. Zeplin has gained fans quickly as an alternative to traditional options. The interface has less of a learning curve than other large tools like the Adobe products as well.\nFeatures: Cross Platform Performance, Non-Destructive Effects, Good for Print Exporting, Non-Destructive, Intuitive Interface\nAffinity has been explained as a mixture of the highlights of both Photoshop and Illustrator and has been rumored to be the software that will rise to the top. The learning curve is considerably less intense and the results can be just as amazing.\nAs for web design, the software offers artboard behavior when you change the size. These kind of features are important when designing responsive websites. The lower prices are also appealing in contrast to steep Adobe prices. The Affinity Designer is more akin to Illustrator in that it is popularly used for graphic design, icon design and web design over photo editing.\nFeatures: Free, Vector Graphic Design, Mac & PC, Drawing Tools, Simple Interface\nThis free, open source product offers a relatively simple option that offers both graphic design options and photo editing. This product is a wonderful option for those needing simple graphics on a budget, who are beginner designers, and those who need a stepping stone to the more robust and complex design softwares.\nFeatures: Online, Free, Template Options, Screen Design, Branding, Vector Illustration\nThis online design tool is another great stepping stone to more heavyweight designing tools. You can easily import photos and icons that Gravit supplies, as well as your own. The easy interface is meant for beginners, but also those who need to design on the go. It’s offered on your tablet and phone, too!\nLet it be known that each design product has it’s pros and cons and have been designed to fit into different workflows. With each option comes stresses, reliefs, and a learning curve. Be prepared for these! But also be open to trying out new options that may work better for you.']	['<urn:uuid:c006d827-0efb-4e87-a9b1-41dc582a6ea2>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	8	56	814
66	Was capitalism always a global system like today?	No, capitalism was not always global. For 200 years, capitalism was closely tied to nation states, operating through national markets and territories. Neither British capitalism in the 19th century nor American capitalism after 1945 was truly universal. It is only in contemporary times that capitalism has spread to the remotest parts of the world and positioned itself as a truly global system.	['For 200 years capitalism was inextricably linked to the nation state. It emerged in the form of national markets, was based on national territories and relied on the state for support. Two nation states - Britain in the 19th century and the United States in the 20th - successively formed the hegemonic core of capitalism: each of them set the technological pace, set the rules of trade and production, and imposed the constraints of the world system. According to current wisdom, however, the bond between the nation state and capitalism is now coming to an end. Globalisation is said to be making the nation state obsolete, politics irrelevant and national sovereignty an empty shell.\nThis alleged demise of the nation state and national sovereignty is part and parcel of the universalist claims of contemporary capitalism. For the first time in history, capitalism has spread its reach to the remotest parts of the world and posits itself as a global system. Neither British capitalism in the 19th century nor even the American post-1945 version was truly universal. Today, capitalism is said to have finally broken away from its national moorings. It has become, as it were, extra-territorial, rootless, identity-less.\nHence the withering away of the nation state. Reduced to a managerial role in which it strives to cope with economic constraints that are beyond its control, it watches helplessly as the balance of forces swings towards the global markets. Within its historical borders it has ceased to be the locus of political action and identity, of social cohesion and the general interest. Beyond its frontiers it often retains only the formal attributes of sovereignty. In short, the state is supposed to have become, at best, just one among a number of otherwise private players in the international system. At worst, to have lost control altogether and to be no longer capable of influencing the course of events.\nThis view is particularly fashionable in Europe, where unification is proceeding by way of agreed transfers of sovereignty, but it does not stand up to an analysis of the origins of globalisation. It ignores the decisive role of the state in creating the global free market paradigm. It conceals the underlying aims of social policy. And it fails to appreciate the balance of power in the international system resulting from globalisation. Though in many parts of the world the state has indeed lost control, the fact remains that the American state has not withered away in the new free market utopia. On the contrary, US hegemony and sovereignty have been strengthened in spectacular fashion. In Europe, state power has been redeployed in accordance with the logic of globalisation to achieve economic unification. While the role of the state has been redefined (at the cost of growing social hardship), there has been no automatic weakening of state power.\nJust as the intervention of the British state was decisive in establishing a free labour market to promote the expansion of industrial capitalism in the 19th century (see ’Globalisation then and now’), so the necessary conditions for the emergence of a global free market at the end of the 20th century have had to be created. The capitalist world economy in the period following the second world war was by no means a “free market”. It was subject to a system of monetary regulation that ensured its stability and predictability. The state, as guarantor of social cohesion, coordinated economic, industrial and labour policy at a national level.\nGlobalisation is tearing apart this post-war social contract. The creation of a worldwide free market is rooted in a series of decisions taken by the US over the last 30 years which dismantled the post-war international monetary system, liberalised world markets and granted the financial sector an autonomy and power unparalleled since the golden age of British finance. The industrial capitalism of the “30 glorious years” after the second world war gave way to finance capitalism. And it is the financial sector - divorced from the economic foundations on which it rests - which now sets the pace, generates systemic constraints, and imposes normative behaviour.\nThe US began by abandoning the system of fixed exchange rates established by the Bretton Woods Agreements in 1944 (1) and introducing a system of generalised floating exchange rates. There was a strong economic motive for the decision, which the US authorities took unilaterally in 1973. They were seeking to compensate for declining competitiveness and a growing national debt by exporting the country’s macroeconomic imbalances. The floating exchange rate system provided a flexible and efficient monetary tool that enabled them to avoid the adjustments that would otherwise have been required by America’s new situation as a debtor. In a system of fixed exchange rates and gold convertibility, the US would have been obliged, like every third-world country today, to pay for its indebtedness with a relative loss of sovereignty and highly unpopular domestic austerity measures.\nThe new system also allowed the US to maintain a high standard of living at home by dipping into the planet’s savings. Thanks to its political power and to the dollar, which was the world’s only reserve currency, the US was able to keep its monetary sovereignty intact. Its allies could not question American policy without destabilising the institutional fabric and the cold-war security system from which they derived undoubted benefits. The burgeoning US deficit was funded for decades by Japan and Europe.\nA decisive step was taken in the 1980s with the deregulation of the US finance industry, which paved the way for its globalisation via the Wall Street banks, brokers, hedge funds (2) and pension funds that dominate the world’s financial flows. Worldwide liberalisation in the 1980s and 1990s gave the US finance industry access to the savings of the newly industrialised and emerging countries, where rates of return were very high. In short, the establishment of a global free capital market was essential for the economic and financial wellbeing of the world’s leading debtor (3).\nThis explains the continuity of US policy on financial liberalisation, the “Washington consensus”. In 1985 Ronald Reagan set out to knock down barriers to trade, foreign investment and the free movement of capital between industrialised countries, especially in Japan. His successor continued this effort though the Enterprise for the Americas Initiative, designed to support free markets and the free movement of capital in the western hemisphere. “Previous administrations had pushed for financial liberalisation principally in Japan, but under President Clinton it became a worldwide effort” directed in particular at the new area of wealth accumulation in East Asia, “seen as a potential gold mine for American banks and brokerages” (4).\nThe US secured the liberalisation of the Japanese financial system and the revaluation of the yen under the 1985 Plaza Accords through a mixture of coercion and cooperation typical of a hegemonic power. In so doing it inflated the bubble that eventually burst at the end of the decade. However, when it came to organising the forced march towards liberalisation of the newly industrialised countries, the government set itself on a war footing. The overall plan, coordinated by the US Department of Commerce, identified 10 rising economic powers from the Pacific to the Atlantic whose economies were to be opened up, and it called upon all government departments from the CIA to US ambassadors abroad (5).\nAs an emanation of the most powerful Western states that make up its membership, the International Monetary Fund legitimised this strategy. While some emerging countries and ruling castes have benefited from liberalisation, this does not alter the fact that it was imposed by coercion. As Robert Keohane and Helen Milner have pointed out: “During the 1980s intense political pressure was exerted by advanced industrialised countries on developing countries to open their economies ... the national economic regulations of developing countries were called into question” (6).\nHegemony has many faces. In the early 1990s Washington set itself three objectives: to maintain the global balance resulting from the end of the cold war, to ensure its technological lead and military supremacy, and to create an economic environment favourable to its own interests. For the most part, these objectives have been achieved. Admittedly, international balances are not static and hegemony does not mean absolute freedom of action. But no country or group of countries appears able to constitute a political counterweight to the US in the foreseeable future, let alone call into question its primacy in the hierarchy of nations. As political pundit Thomas Friedman puts it: “In the globalisation system, the United States is now the sole and dominant superpower and all other nations are subordinate to it to one degree or another” (7). In other words, they ought to accept America’s “benevolent global hegemony”.\nBenevolent or not, US hegemony is a fundamental reality that conditions the international political economy. The worldwide free market is strengthening the American model, which today relies on its strong comparative advantages in the post-industrial sectors of financial and cultural services, communications, leading-edge technologies and scientific-technical production. At the same time, a normative world culture is emerging in the realms of economic activity, social practice and private international law.\nAnd it is the US which is laying down the new groundrules, i.e. the dominant economic norms (profitability, shareholder value), the regulatory criteria (ratings of companies and states), and the legal rules (international commercial arbitration). For instance, the behaviour of the markets is shaped by the ratings awarded by two major US private rating agencies, Moody’s and Standard & Poor. Acting both as judge and party, they are imposing US normative criteria on the rest of the world (8).\nAmerican capital thus operates in a universe of rules which it is constantly redefining and which determine the constraints of the international system. The US itself is not subject to those constraints. Nor has the American state lost control of the markets: the Federal Reserve’s decisive action following the stock market crash in 1987 and the US Treasury’s intervention in 1994-95 after the collapse of the Mexican peso are obvious cases in point. The state also played a crucial though belated role in 1997-98 during the Asian crisis in preventing the collapse of international banking system and ensuring that liberalisation could continue.\nWithin this overall primary hegemony, the other Western powers participate to varying degrees in a broader pattern of western hegemony vis-à-vis the “third world”. Globalisation is institutionalising a new balance of power between states that hardens the sovereignty of some while reducing the autonomy of the others. The worldwide free market accentuates the disparity between the centres of capital and the peripheries. The players with knowledge and power lay down the rules; the others fall into line.\nTrapped in an international division of labour that forces them into often harmful specialisation, the most vulnerable third-world countries are losing the last remnants of their sovereignty, while the newly industrialised countries have become even more dependent over the last few years, as recent experience in East Asia proves (9). This is hardly surprising. “Emerging” countries have never had more than limited autonomy, and the formal sovereignty of those on the weakest fringes has always proved more theoretical than real.\nWhile the European Union is an active participant in the worldwide free-market utopia, at the same time it constitutes a potential counterweight. Since the early 1980s European unification has been directed towards the creation of an entity capable of competing with the US, rather than opposing it. By combining forces in a larger unit, the member states have been attempting to assert their sovereignty jointly in response to globalisation, since none of them is any longer able to do so individually.\nFrom 1981 to 1983 France still believed it could go it alone. In the end it was forced to abandon its policy of growth stimulation in favour of an antisocial austerity package chillingly described as competitive deflation. It might seem that the constraints of globalisation and the demands of economic unification were now absolute and left national governments no room for manoeuvre. A closer look at the redeployment of sovereignty and political power in Europe shows this conclusion to be false.\nTransfers of sovereignty to the EU - with regard to monetary matters or competition law - do not necessarily imply a reduction of national sovereignty. They are not a zero sum game. Under pressure in the new international political economy, the European nation states are pooling their sovereignty to resist submersion. In other words, they are attempting to recover the sovereignty under threat at national level by relying on the strength of a larger regional entity.\nThe EU has no central authority. Decision-making bodies vary from sector to sector. But on matters of strategic importance, the influence of the member states often remains decisive. The Council of Ministers (i.e. the national executives of the member states), and especially the ministers of finance and economic affairs, have a privileged position among the EU institutions, at the expense of the European parliament and the national parliaments.\nIf we consider sovereignty as relative autonomy within the inter-state system, there is little doubt that the national executives have been able to exercise it through the EU institutions, at least in key areas relating to the world economy. If there is one subject of European consensus, it is free competition, which has been raised to the status of an absolute good. There can be no doubt about the concordance of national and European policy on this matter, since many of the reforms introduced by member states at national level preceded the corresponding EU regulations and sometimes go much further than required by strict compliance with EU constraints. France’s deregulation of its financial markets in 1984, on Anglo-American rather than German lines, is a case in point (10).\nPopular sovereignty, on the other hand, is breached with increasing frequency by EU practices that prevent parliaments and, even more so, civil society from playing their proper role in areas of crucial concern. In the context of globalisation and European unification, we have a situation - often referred to as the “democratic deficit” - in which the redeployment of state sovereignty is being achieved at the cost of a considerable increase in the autonomy of the political authorities, barely concealed by a barrage of new regulations designed to attenuate the effects of social distress. And when it comes to social measures, the Commission is deliberately holding back on the grounds that the complexity of national systems of social protection, and the specific historical development in each member state, would make social harmonisation highly problematic if not impossible.\nNational governments, which are closer to their citizens, are supposed to be in a better position to defend their interests when it comes to respect for social traditions and national temperament. Nevertheless, all the national social reforms proposed or implemented are converging towards the same goal: the liberalisation of labour markets. Contrary to the new conventional wisdom, the fact that such reforms are entrusted to the member states, and are implemented incrementally, by no means indicates any resistance by national governments to the forces of globalisation.\nThe nation states are simply playing the role which Karl Polanyi identified in the context of the first “great transformation” (see ’Globalisation then and now’), that of “altering the rate of change, speeding it up or slowing it down as the case may be”. Governments are defusing resistance by reforming step by step. But as the combined effect of the measures comes to be felt, they too are experienced as faits accomplis.\nThere is much lamenting over the powerlessness of national governments. Yet these very governments are contributing fully to the elaboration and implementation of the new hegemonic political economy. They have chosen to participate actively, rather than simply adapt (11), and are acting simultaneously at national, regional, local and European levels to redefine the rules in line with current neoliberal dogma and practice. The role of EU institutions has been less to usurp national sovereignty than to enable the member states to pursue their national interests by other means.\nBecause of the way it was conceived from the outset, European unification is a finality without a goal, a forced and blind march forward towards a final objective that is always receding into the distance (12). Since there is no turning back, member states cannot go back on their word. They are trapped in the machinery. In defining general policy options, they bear responsibility for rules subsequently laid down by the Commission that are binding on all their citizens and take priority over national legislation.\nSo far, responsibility for the consequences of the policy choices of the nation states has been largely attributed to Europe, thus protecting them from blame. But through this blame avoidance strategy, states could well end up losing control of the process. If that happened, there could be no return to the status quo ante. Left to themselves, nation states would lose the room for manoeuvre they had regained by concerted action. The only solution would be to redefine the purpose of European unification.\nThe growth of inequalities not only raises ethical issues. In the end it always holds back economic development and undermines social cohesion. The transnational dynamics of the EU could provide an opportunity for upward social harmonisation in line with the most favourable rules and practices (on working conditions, wages, employment, social protection, etc.) That would require political determination that is currently lacking but, if it could be mustered, would set an excellent example. Failing that, the establishment of a European free-trade empire in the face of US hegemony may perhaps result in multipolarity but will certainly not lead to a fairer world.']	['<urn:uuid:a88458a4-7cbf-4801-ab70-f71e6d948b2a>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	8	62	2951
67	lifestyle changes treat fatty liver sleepiness	Both fatty liver disease and excessive daytime sleepiness can be improved through similar lifestyle modifications. For fatty liver, key changes include losing weight in a healthy way, limiting alcohol consumption, and making dietary changes to lower blood fat levels. For excessive daytime sleepiness, effective lifestyle changes include establishing regular sleep and wake times, getting regular exercise, reducing caffeine consumption, avoiding alcohol, and avoiding large meals close to bedtime. Additionally, creating a comfortable sleep environment by keeping the bedroom dark, cool, and quiet can help promote restful sleep. These modifications can help reverse fatty liver conditions and reduce daytime sleepiness symptoms, though it may take several weeks to see effects.	['Hepatic steatosis or fatty liver is the term used to describe a buildup of fat within the liver. It is normal to have a bit of fat in the liver, but if it is too much it can become a big health problem.\nThe liver is one of the most important organs in the body. It is the second-largest organ and its job is to process everything that we eat and drink and remove harmful substances from the blood. If there is too much fat in the liver this process can be interrupted. Fatty liver is described as when fat makes up more than five percent of the weight of the liver.\nFatty liver can be reversed with the proper lifestyle modifications. In some cases, a person with fatty liver will not have any symptoms. In addition, there is often no permanent damage caused by fatty liver unless it is allowed to progress.\nThe condition of fatty liver is actually quite common and affects anywhere from ten to twenty percent of Americans. Most cases occur in people between the ages of forty to sixty. If fatty liver is not\ntreated it can become quite harmful.\nHere are eight causes of fatty liver to consider.\nAlcohol consumption is one of the most common reasons that a person will develop fatty liver disease. Drinking too much alcohol is very hard on your liver and can cause all types of damage. For this reason, it is important to limit your alcohol consumption.\nIf you have some of the symptoms of fatty liver disease already, limiting the amount of alcohol that you drink is extremely important. In fact, it is a good idea to not drink any alcohol at all. This will give your liver more time to repair itself and some of the effects of the fatty liver could be reduced.\nThere are many health issues that arise from being overweight. From heart disease to strokes to breathing difficulties, being overweight is not good for a person. In addition, obesity can also cause the fatty liver to develop. As you gain more weight, the fatty deposits need a place to go.\nThese fatty deposits often gather around the liver. When this occurs it makes it more difficult for the liver to do its job, thus leading to many health complications. If you are currently overweight, losing weight in a healthy way can help alleviate many of the symptoms associated with fatty liver.\n3. Insulin Resistance\nWhen your cells do not take up sugar in response to insulin, there can be many issues that develop. Insulin resistance in the body can cause a lot of different health issues, including a fatty liver. A person who has developed insulin resistance should also make sure to have their liver checked.\nThis can help determine whether or not there are any issues with the liver. Often, making dietary changes can help alleviate these symptoms.\n4. High Blood Sugar\nHigh levels of sugar in the blood can be an indication of diabetes. This is known as hyperglycemia. A person who has prediabetes or types 2 diabetes is at more of a risk of developing fatty liver disease as well. Once again, a person who is diabetic is more likely to be overweight, which is one of the causes of fatty liver development.\nIf you have been diagnosed with prediabetes or type two diabetes, it is a good idea to have your liver checked as well. Listen to your doctor and make the lifestyle changes that are recommended to help alleviate the symptoms of these diseases.\n5. High Fat Levels in the Blood\nThe liver is responsible for filtering the blood. When there are high levels of fats in the blood, in particular, triglycerides, it can be more difficult for the liver to function and do its job correctly. When this happens, the liver may begin to store more fat around it, which causes the fatty liver disease to develop.\nChanging your dietary habits is recommended in order to help lower the amount of fat found in the blood. This will help alleviate the symptoms and issues caused by fatty liver.\n6. Hepatitis C\nHepatitis C is a common bloodborne infection in the United States. There are several factors that can increase a person’s risk of developing hepatitis C including hemodialysis, HIV infection, intravenous drug use, and being exposed to blood from a person who is infected.\nOne of the symptoms of hepatitis C is the inflammation of the liver. While there are many other things that can cause fatty liver disease, hepatitis C should be tested for if you are having symptoms of fatty liver.\n7. Sleep Apnea\nWhile it may seem weird, if you have been diagnosed with sleep apnea, you may be at a higher risk of developing fatty liver. Most people who have sleep apnea are often overweight. This pressure from the weight causes a person to stop breathing at times during the night.\nSleep apnea itself is something that needs to be treated by a medical professional. In addition, if you have sleep apnea, it is important to consider some of the symptoms of fatty liver to determine whether or not you have developed this issue as well.\n8. Thyroid Issues\nA person who has an overactive or underactive thyroid may develop many different health-related issues, including fatty liver. The thyroid is responsible for releasing hormones that help regulate many areas of the body.\nWhen it is not working properly, many different health issues may develop. A person who has hyperthyroidism or hypothyroidism, may gain or lose weight quickly. This development can lead to fat stores throughout the body, including in the liver.', 'What is excessive daytime sleepiness (EDS)?\nExcessive daytime sleepiness (EDS) is a common problem that affects both adults and children. It occurs when a person experiences an overwhelming urge to sleep during the day, despite having had a full night’s sleep. A variety of medical conditions can cause EDS, including sleep apnea, depression, narcolepsy, and sleep deprivation, as well as lifestyle choices such as alcohol or drug use. It can also be a side effect of certain medications.\nCauses of EDS\nDaytime sleepiness is a condition that can undermine a person’s quality of life. While occasional episodes of daytime sleepiness are normal, chronic EDS can have serious underlying causes.\nOne of the most common causes of excessive daytime sleepiness is not getting enough sleep at night. The average adult needs between 7-9 hours of sleep every night, but many people do not get enough. Sleep deprivation can cause a wide range of symptoms, including daytime sleepiness.\nAnother common cause of daytime sleepiness is sleep apnea. Sleep apnea is a condition in which a person’s breathing is interrupted while they are sleeping. This can lead to poor quality sleep and excessive daytime sleepiness.\nPoor sleep hygiene can also lead to excessive daytime sleepiness. Sleep hygiene refers to habits and activities that promote better sleep. Poor sleep hygiene can include things like drinking caffeine late in the day, using electronic devices in bed, and eating an enormous meal before bed.\nMedications can also cause excessive daytime sleepiness. Many medications, including sedatives and antihistamines, can cause drowsiness. It is important to talk to a doctor before taking any medication, as some medications can have serious side effects.\nNumerous medical conditions can cause excessive daytime sleepiness. Conditions such as narcolepsy, thyroid problems, and depression can all cause symptoms of daytime sleepiness.\nSymptoms of EDS\nThe most common symptom of Excessive Daytime Sleepiness is a powerful urge to sleep during the day, even when one has had enough sleep the night before. This can lead to difficulty in concentrating and an inability to focus on tasks.\nOther symptoms include:\n- difficulty in waking up in the morning,\n- feeling tired and sluggish throughout the day,\n- and difficulty in staying awake for extended periods of time.\nDaytime napping is another symptom of EDS, as well as having difficulty in waking up from naps. People with EDS may also experience feelings of irritability, depression, and anxiety because of their lack of sleep.\nLifestyle changes can be an efficient way to reduce excessive daytime sleepiness. These changes include getting regular exercise, reducing caffeine consumption, establishing regular sleep and wake times, avoiding alcohol and other drugs, and avoiding huge meals and arduous exercise close to bedtime. Creating a comfortable sleep environment, such as keeping the bedroom dark, cool and quiet, can help to promote a restful sleep. Medication can also treat EDS.\nStimulant medications, such as modafinil, are commonly prescribed to increase alertness and reduce fatigue. Other medications, such as sodium oxybate, are used to improve the quality of sleep. In addition, medications such as antidepressants and anti-anxiety medications can help to reduce sleep disruption and improve sleep quality.\nWhen treating excessive daytime sleepiness, it is important to talk to a healthcare provider to determine the best course of action. Depending on the severity of the condition, lifestyle changes and/or medication may be necessary. It is also important to note that lifestyle changes may take several weeks to take effect, while medications may take several days to become effective.\nWith the right combination of lifestyle changes and medication, we can manage excessive daytime sleepiness.\nExcessive daytime sleepiness can have serious implications for an individual’s physical, mental, and emotional well-being. It can lead to impaired concentration and memory, mood swings, and a decrease in productivity. It can also be a sign of an underlying sleep disorder or medical condition, so it’s important to address the issue and seek professional help as soon as possible. With proper diagnosis and treatment, EDSs can be managed and even eliminated.\n- NCBI – WWW Error Blocked Diagnostic. (n.d.). https://pubmed.ncbi.nlm.nih.gov/33840518/\n- Pacheco, D. (2022, June 10). Causes of Excessive Sleepiness. Sleep Foundation. https://www.sleepfoundation.org/excessive-sleepiness/causes\n- Roland, J. (2021, July 13). Why Do I Feel Excessively Sleepy? Healthline. https://www.healthline.com/health/excessive-sleepiness\n- Sleep Disorders and Hypersomnia Treatment. (2000, January 1). WebMD. https://www.webmd.com/sleep-disorders/hypersomnia-treatments']	['<urn:uuid:c0821e9f-bf59-43ce-b499-eb80910bf5fb>', '<urn:uuid:846a0e8f-f3dc-4836-b65e-b5cc2cfa6768>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-13T04:31:27.499155	6	109	1663
68	risk groups comparison mrsa cholera affected	MRSA and cholera affect different risk groups. For MRSA, those in locations with frequent skin-to-skin contact are at higher risk, such as people in schools, dormitories, military barracks, and day care facilities. For cholera, the main risk groups include children who play near sewers or contaminated water sources, careless healthcare workers who handle cholera patients without proper protection, and family members or friends caring for cholera victims. Cholera particularly affects people in heavily populated areas with poor sanitation, especially in South-East Asia, parts of Africa, Central America, and Central Mexico.	"['|Can I get MRSA from someone at work?\nMRSA is transmitted most frequently by direct skin-to-skin contact\nor contact with shared items or surfaces that have come into contact\nwith someone else\'s infection (e.g., towels, used bandages).\nMRSA skin infections can occur anywhere on the body. However,\nseveral factors make it easier for MRSA to be transmitted. These\nfactors, which NIOSH has referred to as the 5 C\'s, are as follows:\n- Frequent skin-to-skin Contact\n- Compromised skin (i.e., cuts or abrasions)\n- Contaminated items and surfaces\n- Lack of Cleanliness\nLocations where the 5 C\'s are common include schools, dormitories,\nmilitary barracks, households, correctional facilities, and day care\n|If I have MRSA, can I go to work?\nAccording to the Centers for Disease Control and Prevention (CDC),\nunless directed by a healthcare provider, employees with MRSA\ninfections should not be routinely excluded from going to work.\n- Exclusion from work should be reserved for those with wound drainage (""pus"") that cannot be covered and contained with a clean, dry bandage and for those who cannot maintain good hygiene practices.\n- Workers with active infections should be excluded from activities\nwhere skin-to-skin contact with the affected skin area is likely to\noccur until their infections are healed.\n|What should I do if I think I have a MRSA infection?\nSee your healthcare provider and follow their advice about returning to work.\n|If I have a MRSA skin infection, what should I do to prevent the\nYou can prevent spreading MRSA skin infections to others by following these steps:\nspread of MRSA at work and at home?\n- Cover your wound. Keep areas of the skin affected by MRSA covered.\nKeep wounds that are draining or have pus covered with clean, dry\nbandages. Follow your healthcare provider\'s instructions on proper\ncare of the wound. Pus from infected wounds can contain MRSA, so keeping the infection\ncovered will help prevent the spread to others. Bandages or tape can be discarded with the regular trash.\nClean your hands. You, your family, and others in close contact\nshould wash their hands frequently with soap and warm water or use\nan alcohol-based hand sanitizer, especially after changing the\nbandage or touching the infected wound.\n- Do not share personal items. Avoid sharing personal items such as\nuniforms, personal protective equipment, clothing, towels,\nwashcloths or razors that may have had contact with the infected\nwound or bandage.\nDo not touch other person’s cuts or bandages.\n- Talk to your doctor. Tell any healthcare providers who treat you\nthat you have or had a MRSA skin infection.\n|What should I do if I suspect that my uniform, clothing, personal protective equipment or workstation has become contaminated with MRSA?\nWash soiled uniforms, clothing, sheets and towels with water and\nlaundry detergent. This helps kill bacteria in clothing. Dry clothes\ncompletely in a hot dryer, rather than air-drying.\n- Cleaning contaminated equipment and surfaces with detergent-based\ncleaners or Environmental Protection Agency (EPA)-registered\ndisinfectants is effective at removing MRSA from the environment.\nBecause cleaners and disinfectants can be irritating and exposure\nhas been associated with health problems, such as asthma, it is\nimportant to read the instruction labels on all cleaners to make\nsure they are used safely and appropriately. Where disinfection is\nconcerned, more is not necessarily better.\nAdditional information on\nappropriate use of cleaners and disinfectants can be found in the\nHospitals for a Healthy Environment (H2E) ""10 Step Guide to Green\nCleaning Implementation"". Environmental cleaners and disinfectants should not be used to treat\ninfections. The EPA provides a list of selected EPA-registered disinfectants effective against MRSA.\n|What can my boss (employer) do to prevent the\nAs part of a comprehensive\nSafety and Health Management System, your employer can take steps to decrease or minimize the spread of MRSA at the workplace. Some steps are:\nspread of MRSA at the workplace?\nPlace importance on worker safety and health protection in the\n- Ensure the availability of adequate facilities and supplies that\nencourage workers to practice good hygiene.\n- Ensure that routine housekeeping in the workplace is followed.\n- Ensure that contaminated equipment and surfaces are cleaned with\ndetergent-based cleaners or Environmental Protection Agency\nView the complete CDC/NIOSH (National Institute for Occupational Safety\nand Health) Workplace Safety and Health Topic page: MRSA and the Workplace.', 'Cholera is an infection that is caused by a bacterium known as Vibrio cholerae, which attacks the small intestines. This disease is extremely contagious, especially in areas where there is poor sanitation or during disasters such as floods. It is capable of killing people who are infected within hours in severe cases if it is not treated promptly.\nAccording to research statistics, an approximate of a million to four million cases of cholera infections are reported yearly. The annual number of deaths due to cholera can range from twenty thousand to an eighth of a million.\nCholera affects adults and children, as well as men and women alike. Therefore, cholera is a disease that can affect anyone. However, there are people who are more prone to getting it than others. There are places where cholera is endemic. In other areas, it breaks out as an epidemic.\n""Endemic"" means that a particular disease is native to the place. There have been several cholera outbreaks in the past that have caused deaths in thousands or even in millions. Cholera epidemics usually arise in areas with poor sanitation and scarce resources like water and toilets.\nThe areas that have been affected by wars, natural disasters, poverty, or any other factors that force people to overcrowd are the most prone to massive outbreaks of cholera. The reason is that such places will most likely have poor sanitation and means of waste disposal more so feces. The bacteria that cause cholera will thrive under such conditions.\nCholera is mainly transmitted through contaminated food and water. Flies are the main carriers of the bacteria to the food. When they land on the food, they contaminate it, and whoever eats the food, also gets infected. Eating an undercooked food will also make people get infected if the bacteria were present in the meal.\nContaminated water also helps in spreading the disease. Basically, human feces are the carriers of Vibrio cholerae. When clean water gets into contact with the bacteria, it becomes contaminated. Anyone who drinks such water or uses it to cook food gets infected.\nWho is more prone to the infection?\nCholera is a highly communicable disease. Aside from living in areas where there are epidemics or factors that favor its spread, the following people are more likely to be infected:\nKids like playing a lot. They can carry out their games near sewers, pits, around latrines, and at times, as they play with water, their games can be held in ponds. All these are the places that are prone to have the bacteria that cause cholera.\nTherefore, children get exposed to factors that aid in the spread of the disease more than adults. When they come into contact with the contaminated water, the likelihood that they will get cholera is very high. The reason is that some will suck their fingers afterward or eat food without washing their hands. Through unhygienic practices, they will introduce the bacteria into their bodies.\n2. Careless Doctors and Nurses\nCareless doctors and nurses who handle cholera patients are also likely to get the disease. The reason is that some of the healthcare professionals do not observe maximum caution when dealing with their patients, e.g., not wearing gloves. Eating food without washing their hands after handling the patients also heightens their risk of getting infected.\n3. Family Members and Friends of Victims\nWhen people get sick, the closest around them are the ones who take care of them. Such people may include friends or family, which makes them prone to getting infected, too.\nParts of the World that Are Most Affected by Cholera\nThe parts of the world that are most affected by cholera are:\n- South-East Asia\n- Some parts of Africa\n- Central America\n- Central Mexico\nThese areas are heavily populated and the levels of sanitation are extremely low. These areas are able to provide a suitable place for the bacteria to exist and multiply.\nHow does cholera cause death?\nWhen the bacteria get inside your body, they cause diarrhea that can be very violent in some cases. Severe diarrhea results to dehydration of the body. In severe cases, the blood pressure drops and infected people go into a state of shock. If they do not receive medical attention quickly, death occurs within hours.\nWhat are the symptoms of cholera?\nPeople with cholera show symptoms differently. Some may experience symptoms within a few hours after infection, while in others, symptoms may begin to show after several days. The symptoms of cholera may include:\n- very watery diarrhea (main symptom)\n- muscle cramps can also occur at times\n- diarrhea that usually leads to an excessive loss of water from the body (dehydration)\nDehydration can cause the following conditions:\n- development of wrinkles on the hands and feet\n- the skin becoming less elastic\n- sunken eyes\nPeople who experience severe dehydration will require an immediate correction of the fluid imbalance in their body. Therefore, they have to be hydrated in order to prevent any further damage or complications.\nTips on How to Minimize the Risks of Getting Cholera\nOne of the ways used to control cholera is the use of a vaccine. However, vaccination is not a guarantee, as the vaccine does not usually last for long. That is why it becomes necessary to avoid getting infected with cholera. There are a number of things that you can do to lower your chances of being infected by the disease. Listed below are tips on how to prevent a cholera infection:\n- Parents should always be aware of their kids’ activities. They should keep track of where they spend their time and make sure that it is not in areas where they are likely to pick up the bacteria. The levels of hygiene should also be very high. They should make sure that the kids wash their hands before they eat and after they use the toilet.\n- Implementing high standards of sanitation and hygiene also make you less likely to get infected.\n- Doctors, nurses, or any other medical practitioners who handle patients with cholera should also exercise caution by using protective gears such as gloves. They should also observe high levels of sanitation.\n- Any friends or relatives around a person with cholera should also be careful in how they carry out themselves, especially on hygiene issues.\nThe Bottom Line\nCholera is a water-borne disease. Since this bacteria will more likely be transmitted through water, it is very important to be careful when using water in areas that are prone to the disease. Drinking water should be well-boiled and food should also be cooked properly. These precautions will help ensure that the bacteria are killed by the heat.\nThe main issue is for hygiene to be exercised at the highest of levels. If it is possible, avoid areas that prone to the disease. If you cannot avoid these places, then following the above-given tips should allow you a better chance at avoiding the disease.\nAnyone can get cholera and children are the ones that are most likely to get infected. Thus, keep an eye on your kids and should they show any symptoms, visit a medical facility as quickly as possible as they can get worse quickly and even die.\n- Who Gets Cholera?\n- Cholera Risk Factors\n- Prevention of Cholera']"	['<urn:uuid:ec29edf8-e6a6-4a0e-89ed-b3b0b737e924>', '<urn:uuid:46c4c8dd-72b2-464a-8d37-ff6bdee772f1>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T04:31:27.499155	6	90	1942
69	protect ears during loud music advice	To protect your ears during loud music exposure, take several key precautions: Use proper ear protection like earplugs, earmuffs, or custom-fitted in-ear monitors. Walk away from extremely loud sounds and increase distance from sound sources. Limit time spent in noisy environments and take regular breaks. Keep personal audio devices at low volumes and limit headphone use. Stand farther from speakers during concerts or practice. For optimal protection, get custom-fitted ear protection from a hearing healthcare professional and monitor your hearing health with regular tests. Warning signs of hearing damage include ear pain, ringing/buzzing in ears (tinnitus), and difficulty understanding speech after noise exposure.	['WASHINGTON, DC, JUNE 29, 2015—The Better Hearing Institute (BHI) is encouraging people of all ages to protect their hearing this summer so they can treasure the sounds of the season for a lifetime. Packing earplugs along with the sunscreen for summer outings is just one of six easy tips that BHI is offering.\nWhile many noisy recreational activities are part of summer fun, it’s extremely important to take precautions to ensure that these activities don’t harm our hearing.\nProlonged exposure to loud outdoor concerts, lawn mowers, power tools, motorized recreational vehicles, target shooting, sporting events and fireworks can potentially damage our ears. In fact, the single bang of a firecracker at close range can cause permanent hearing loss in an instant, making it forever more difficult to hear the quieter sounds of summer.\nAccording to the World Health Organization (WHO), 1.1 billion teenagers and young adults (12 to 35 year olds) are at risk of hearing loss due to exposure to damaging levels of sound at noisy entertainment venues and the unsafe use of personal audio devices.\n“Hearing is the sense that connects us to each other,” says William Hal Martin, Ph.D., Professor of Otolaryngology, National University of Singapore, Program Director MSc of Audiology, Center for Hearing, Speech & Balance, and Co-Director of Dangerous Decibels. “Exposure to high level sounds cannot only destroy our ability to hear, it can cause tinnitus—ringing in the ears.”\n“People of all ages are at risk of hearing loss from high level sounds, but it easily can be prevented by simple steps,” Martin continues. “It is important to recognize when your ears are in danger and to safeguard them so you can enjoy listening to friends, music, and sounds you love for the rest of your life.”\nHow Noise Affects Our Hearing\nWe hear sound when delicate hair cells in our inner ear vibrate, creating nerve signals that the brain understands as sound. But just as we can overload an electrical circuit, we also can overload these vibrating hair cells. Loud noise damages these delicate hair cells, resulting in sensorineural hearing loss and often tinnitus (ringing in the ears). The cells that are the first to be damaged or die are those that vibrate most quickly—those that allow us to hear higher-frequency sounds clearly, like the sounds of birds singing and children speaking.\nSound volume is measured in decibels, with the softest sound a normal hearing human can hear measuring at 0 dBA. Any sounds above 85 dBA for 8 or more hours are considered unsafe. Most firecrackers produce sounds starting at 125 dB peak SPL, presenting the risk of irreversible ear damage.\nRepeated exposure to loud noise, over an extended period of time, presents serious risks to hearing health as well. If you have to shout over the noise to be heard by someone within arm’s length, the noise is probably in the dangerous range. Here are the warning signs:\nYou have pain in your ears after leaving a noisy area.\nYou hear ringing or buzzing (tinnitus) in your ears immediately after exposure to noise.\nYou suddenly have difficulty understanding speech after exposure to noise; you can hear people talking but can’t understand them.\nFor more information on hearing loss and to take the free, confidential, online BHI Hearing Check, visit www.BetterHearing.org. Follow BHI on Twitter @better_hearing, and like us on Facebook at www.facebook.com/betterhearinginstitute.\n6 Easy Tips for Protecting Your Hearing This Summer\nWalk away and plug your ears. If a loud noise takes you by surprise, quickly plug your ears with your fingers and walk away. Increasing the distance between you and the source of the sound will help reduce the intensity (or decibels) at which the sound is reaching your ears.\nUse earplugs. When you know you’ll be around loud sounds, use earplugs. Disposable earplugs, made of foam or silicone, are often available at local pharmacies. They’re practical because you can still hear music and conversation when they’re in your ears. But when they fit snuggly, they’re effective in adequately blocking out dangerously loud sounds.\nLeave the fireworks to the professionals. Be smart when you celebrate 4th of July festivities. Leave the fireworks to the professionals. And when watching the show, stay a safe distance away—where you can enjoy the colors and lights but not expose yourself and your family to loud noises. To protect your hearing, make sure you’re wearing earplugs and that they’re securely in place before the show begins. Also be sure to keep them in for the entire show.\nLimit your time in noisy environments. Do all you can to limit the length of time you spend in a noisy environment. When you do participate in noisy activities, alternate them with periods of quiet. And remember to use ear protection.\nTurn it down. When listening to smartphones and other electronics, keep them at a low volume. Importantly, limit your use of headphones and ear buds. Remember, it’s not just the volume that matters. It’s also the duration of time spent listening.\nVisit your local hearing healthcare professional for custom-fitted ear protection and a hearing test. A hearing healthcare professional can provide a hearing test to determine your baseline hearing level and determine if you have any hearing loss that should be addressed. Hearing care professionals also can provide custom ear protection to ensure a proper fit.', 'As a member of a band, it’s essential to protect your ears during practice sessions. Prolonged exposure to loud sounds can lead to hearing damage, which can be irreversible. In this article, we will discuss the best ear protection for band practice to ensure that you can practice without harming your hearing.\nUnderstanding the Importance of Ear Protection\nAs a musician, protecting your ears from the damaging effects of loud music is essential. Exposure to loud noises, such as those produced by live concerts and band practices, can lead to permanent hearing damage and tinnitus. Tinnitus is a condition that causes a ringing, buzzing, or humming sound in the ears, and it can be extremely distressing. That’s why it’s crucial to invest in high-quality ear protection.\nTypes of Ear Protection\nThere are several types of ear protection available on the market, each with its own advantages and disadvantages. the most common types are:\nEarplugs: These are small, foam or silicone plugs that fit into the ear canal to reduce noise levels. Some earplugs are designed to block out all noise, while others are designed to reduce noise levels while still allowing you to hear music and conversation.\nEarmuffs: These are large, over-ear headphones that cover the entire ear to block out noise. They are a great option for musicians who want to protect their hearing while still being able to hear their music.\nIn-Ear Monitors: These are custom-fitted earphones that provide a high-quality listening experience while also protecting your ears from loud sounds.\nChoosing the Right Ear Protection\nWhen it comes to choosing the right ear protection for band practice, there are several factors to consider. The first is the level of noise reduction you require. If you’re playing in a loud band, you’ll need ear protection that can block out a significant amount of noise. On the other hand, if you’re playing in a quieter setting, you may be able to get away with earplugs that reduce noise levels without blocking out sound entirely.\nKey takeaway: Investing in high-quality ear protection is crucial for musicians to prevent permanent hearing damage and tinnitus caused by exposure to loud music during band practice or live concerts. Earplugs, earmuffs, and custom-fitted in-ear monitors are the most common types of ear protection available. When choosing the right ear protection, consider factors such as noise reduction, comfort and fit, and durability. Top ear protection brands for musicians include Etymotic Research, Westone, and ACS Custom. Using ear protection correctly, taking regular breaks during practice sessions, and avoiding exposure to other sources of loud noise are essential methods for protecting your ears.\nComfort and Fit\nAnother essential factor to consider is comfort and fit. Ear protection that doesn’t fit properly can be uncomfortable to wear and may not provide adequate protection. Custom-fitted earplugs or in-ear monitors are the best options for musicians who want maximum comfort and a secure fit.\nDurability is also an important consideration, especially if you play in a band regularly. Ear protection that is made from high-quality materials and is designed to withstand regular use is essential.\nTop Ear Protection Brands for Musicians\nThere are several top ear protection brands that are popular among musicians. Some of the most well-known brands include:\nEtymotic Research: This company specializes in custom-fitted earplugs and in-ear monitors that provide excellent noise reduction while still allowing you to hear your music.\nWestone: Westone is another popular brand that offers a range of custom-fitted earplugs and in-ear monitors.\nACS Custom: ACS Custom is known for its high-quality custom-fitted earplugs and in-ear monitors that are designed specifically for musicians.\nKey takeaway: As a musician, it is essential to invest in high-quality ear protection to prevent permanent hearing damage and conditions such as tinnitus caused by exposure to loud music during band practice and live concerts. There are several types of ear protection to choose from, including earplugs, earmuffs, and in-ear monitors, and factors to consider when selecting the right ear protection include noise reduction, comfort and fit, and durability. Musicians can also protect their ears by standing farther away from speakers, using sound-absorbing materials, and avoiding other sources of loud noise.\nTips for Using Ear Protection\nUsing ear protection is essential for protecting your hearing, but it’s also important to use it correctly. Here are some tips for using ear protection:\nMake sure your ear protection fits properly and is comfortable to wear.\nChoose ear protection that provides adequate noise reduction for your needs.\nFollow the manufacturer’s instructions for inserting and removing your ear protection.\nTake regular breaks during practice sessions to give your ears a rest.\nAvoid listening to music at high volumes on headphones or earbuds outside of band practice.\nGet regular hearing tests to monitor your hearing health.\nAs a musician, it is important to protect your ears from the damaging effects of loud music. Exposure to loud noises can cause permanent hearing damage and tinnitus, a condition that causes a ringing, buzzing, or humming sound in the ears. There are several types of ear protection available, including earplugs, earmuffs, and in-ear monitors, each with their own advantages and disadvantages. When choosing ear protection, factors such as noise reduction, comfort and fit, and durability should be considered. Some popular ear protection brands include Etymotic Research, Westone, and ACS Custom. It is also important to use ear protection correctly and take other methods to protect your ears, such as standing farther away from speakers and using sound-absorbing materials. Regular hearing tests are also recommended to monitor your hearing health.\nOther Methods for Protecting Your Ears\nIn addition to using ear protection, there are other ways to protect your ears from loud music. For example, you can:\nStand farther away from the speakers during band practice or concerts.\nUse sound-absorbing materials in your rehearsal space to reduce noise levels.\nUse a noise meter to monitor the sound levels in your rehearsal space.\nAvoid exposure to other sources of loud noise, such as power tools and loud machinery.\nFAQs: Best Ear Protection for Band Practice\nWhat are the benefits of using ear protection during band practice?\nUsing ear protection during band practice can protect your hearing from damage caused by loud music. Noise-induced hearing loss can occur when exposed to prolonged loud sounds, which can be irreversible. Therefore, wearing ear protection can prevent hearing loss and tinnitus, which is the perception of ringing or buzzing in your ears.\nWhat types of ear protection are recommended for band practice?\nThere are various types of ear protection that can be used during band practice, including earplugs, earmuffs, and in-ear monitors. Earplugs are the most common type of ear protection and are available in disposable and reusable forms. Earmuffs are another option, which are designed to fit over the ears and block out the sound. In-ear monitors are custom earpieces that fit into the ear canal and provide sound reduction through noise-cancellation technology.\nWhat level of noise reduction should I look for in ear protection?\nThe level of noise reduction you need depends on how loud your band practice is. A noise reduction rating (NRR) is assigned to ear protection, which indicates how much sound it can block out. A high NRR rating (above 25) is recommended for loud music environments. However, it’s important to note that even the best ear protection won’t block out all noise, so it’s important to monitor the volume of your band practice as well.\nAre there any other important factors to consider when choosing ear protection for band practice?\nComfort is an essential factor when choosing ear protection. You will likely be wearing ear protection for extended periods, so it’s important to find something that fits comfortably and doesn’t cause irritation or discomfort. Other factors to consider include the cost, durability, and ease of use. It may be worthwhile to invest in custom ear protection for optimal comfort and protection.\nHow often should I replace my ear protection?\nEar protection should be replaced regularly, especially if you use them frequently. Disposable earplugs should be discarded after each use, while reusable earplugs and earmuffs should be replaced periodically according to manufacturer’s recommendations. In-ear monitors should be cleaned regularly with a disinfectant and replaced as needed.']	['<urn:uuid:38a46781-e96f-4116-b1d0-4aad90c3d5bb>', '<urn:uuid:9aa01fa1-a408-4ed7-a599-05efe8f8cd78>']	open-ended	with-premise	short-search-query	distant-from-document	three-doc	novice	2025-05-13T04:31:27.499155	6	103	2256
70	Could you explain the historical incidents where US presidents have mishandled or lost access to nuclear launch codes?	There have been several incidents of presidents mishandling the nuclear codes. President Bill Clinton lost the nuclear football once and left it behind at a NATO meeting on another occasion. President Jimmy Carter lost the authentication codes card (known as 'the biscuit') when he sent his suit to the dry cleaners. During the assassination attempt on President Ronald Reagan in 1981, his authentication codes card was thrown away in a trash can at the George Washington University Hospital.	['The “nuclear football” is guarded by a senior military aide-de-camp and kept in close proximity to the US president whenever he is away from the White House. Following World War II, nuclear weapons were a new reality of the world’s superpowers, and when the US and Soviet Union squared off in the Cold War these superweapons were strategic methods for deterrence. After the Cuban Missile Crisis in 1962, President John F. Kennedy questioned whether there was a need for a doomsday weapon capability that could allow its operator to order a nuclear strike from anywhere in the world.\n“What would I say to the Joint War Room to launch an immediate nuclear strike?” he asked, according to declassified reports. “How would the person who received my instructions verify them?”\nThe solution was a 45-pound aluminum-framed black leather briefcase, officially called the Presidential Emergency Satchel. It became more commonly known as the nuclear football because the nuclear plan was code-named Operation Dropkick — it needed a “football” to complete the sequence. The most common misconception about the nuclear football is that the president flips a switch or hits a big red button and the world ends moments later. If that were the case, the world should be very concerned. Fortunately, it verifies the identity of the president and connects him to the Pentagon, which is responsible for carrying out the military strike.\nIn 1980, Bill Gulley, the former director of the White House Military Office, wrote a tell-all book, Breaking Cover, describing the shady money deals under four different administrations — those of Lyndon B. Johnson, Richard Nixon, Gerald Ford, and Jimmy Carter. The Washington Post gave Gulley, who even disclosed the different components of the nuclear football, the unflattering title of the “mercenary snitch.”\n“There are four things in the Football,” Gulley writes. “The Black Book containing the retaliatory options, a book listing classified site locations, a manila folder with eight or ten pages stapled together giving a description of procedures for the Emergency Broadcast System, and a three-by-five inch card with authentication codes [which the president usually carries separately from the football].”\nCarter later found these retaliatory options super complicated, so he started the process of simplifying the nuclear codes, or “the biscuit.” Air Force Col. Robert “Buzz” Patterson, a senior military aide-de-camp responsible for President Bill Clinton’s nuclear football, explained the refined codes were similar to a “Denny’s breakfast menu” because “it’s like picking one out of Column A and two out of Column B.” On the day when the Clinton and Monica Lewinsky scandal hit the national press, the president forgot where he had put the nuclear football.\n“I was floored — and so was the Pentagon,” Patterson recalled. “It had never happened before.”\nAlthough Clinton once lost the nuclear football and then left it behind at a NATO meeting on another occasion, he wasn’t the only president guilty of misplacing the highly sensitive and secret world-ending capability. Carter lost the biscuit when he left the card in his suit and it was sent to the dry cleaners. When President Ronald Reagan was shot in an assassination attempt in 1981, his biscuit was thrown away in a trash can in the George Washington University Hospital.\nThe most recent ordeal involving the nuclear football came in 2017 when President Donald Trump visited China. A scuffle between Chinese security officials and the US Secret Service ensued after the nuclear football wasn’t allowed inside Beijing’s Great Hall of the People.\n“Then there was a commotion,” Axios reported in 2018. “A Chinese security official grabbed [Chief of Staff John] Kelly, and Kelly shoved the man’s hand off of his body. Then a U.S. Secret Service agent grabbed the Chinese security official and tackled him to the ground.”\nSince the nuclear football was first photographed on May 10, 1963, it has become the focus of the media, a concern for foreign governments, and a token of strength and military might for the US government. It was even replicated by the Soviet Union, which created its own version called the Cheget.']	['<urn:uuid:db1d4e36-2131-45d0-93f2-6e5aba6e0916>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	18	78	678
71	disbelievers claim about life after death	The disbelievers claimed that there was only worldly life and no life hereafter. They believed that humans die just like a watch stops functioning, and that no soul survives the body that could be breathed back into it in the future.	"['It is derived from the sentence wa tartt kullu ummat- in jathiyat-un of verse 28, implying thereby that it is the Surah in which the word jathiyah has occurred.\nThe period of the revelation of this Surah also has not been mentioned in any authentic tradition, but its subject matter clearly shows that it was revealed consecutively after Surah Ad Dukhan. The close resemblance between the contents of the two Surahs makes them look like the twin Surahs.\nIt answers the doubts and objections of the disbelievers of Makkah about Tauhid and the Hereafter and warns them for their attitude that they had adopted against the message of the Qur\'an.\nThe discourse begins with the arguments for Tauhid. In this connection, reference has been made to the count- less Signs that are found in the world, from man\'s own body to the earth and heavens, and it is pointed out that everywhere around him man finds things which testify to Tauhid which he refuses to acknowledge. If man sees carefully the variety of animals, the day and night, the rainfall and the vegetation thereby, the winds and his own creation, and ponders over them intelligently, without prejudice, he will find these Signs sufficiently convincing of the truth that this universe is not Godless, nor under the control of many gods, but it has been created by One God, and He alone is its Controller and Ruler. However, the case of the person who is determined not to acknowledge and wants to remain involved in doubts and suspicions is different. He cannot be blessed with the faith and conviction from anywhere in the world.\nA little below, in the beginning of the second section, it has been reiterated that the things man is exploiting in the world, and the countless forces and agencies that are serving his interests in the universe, did not come into being just accidentally, nor have they been provided by the gods and goddesses, but it is One God alone, Who has supplied and subjected these to him from Himself. If only a person uses his mind properly and rightly, his own intellect will proclaim that God alone is man\'s real Benefactor and He alone deserves that man should pay obeisance to Him.\nAfter this, the disbelievers of Makkah have been taken to task and reproved for their stubbornness, arrogance, mockery and insistence on disbelief with which they were resisting the invitation of the Qur\'an they have been warned that this Qur\'an has brought the same blessing which had been granted to the children of Israel before, by virtue of which they became distinguished above all the people of the world. Then, when they failed to recognize the true worth of this blessing and disputed their religion and lost it, this blessing now has been sent to them. This is such a code of guidance which shows the clear highway of Religion to man. The people who would turn it down by their own folly, would only prepare for their own doom, and only such people would become worthy of God\'s succour and mercy who would adopt obedience to it and lead a life of piety and righteousness.\nIn this connection, the followers of the Holy Prophet have been instructed that they should forbear and pardon the absurd and foolish behavior towards them of the people fearless of God, for if they showed patience God Himself would deal with their opponents and would reward them for their fortitude.\nThen, there is a criticism of the erroneous ideas that the disbelievers hold about the Hereafter. They said that life was only this worldly life there was no life hereafter. Man dies in the course of time just as a watch stops functioning suddenly. The body is not survived by any soul, which might be seized and then breathed again into the human body some time in the future. In this regard, they challenged the Holy Prophet, saying: ""If you lay a claim to this, then raise our dead forefathers back to life."" In answer to this, Allah has given the following arguments:\nAfter giving these arguments Allah says most emphatically: ""Just as you did not become living of your own accord, but became living by Our power, so you do not die of your own accord, but die when We send death on you. And a time is certainly coming when you will all be gathered together. If you do not believe in this because of your ignorance and folly today, you may not; when the time arrives, you will see for yourself that you are present before your God and your whole book of conduct is ready accurately, which bears evidence against each of your misdeeds. Then you will come to know how dearly has your denial of the Hereafter and your mockery of it cost you.""\n[Next] [Top] [Previous]']"	['<urn:uuid:98b69f0a-5ffb-49a1-97b0-243cc5dea6dd>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T04:31:27.499155	6	41	813
72	car safety checks importance frequency	Regular vehicle safety checks are essential for preventing breakdowns and accidents. Weekly checks should include oil level verification, while other important inspections involve testing horn, lights, windscreen condition, brakes, seatbelts, and battery condition. Additionally, modern vehicles are equipped with safety systems like TPMS (Tire Pressure Monitoring System) that help monitor tire pressure, though these shouldn't replace regular manual inspections.	"[""We know about the frustrations of vehicles and break downs. We've been providing roadside assistance for over 20 years. We understand that a break down is not anyone’s fault. It just happens whether your car is big, small, old or new. At the beach, at home, in the outback or at work, you never know where or when your vehicle will break down, it just does. That’s why we have an extensive network of providers that ensure our customers are quickly attended to, even in the outback. But we also want to make sure you and your family stay safe on the road, so we've prepared some tips to help keep your vehicle maintained.\nLet’s be honest, we get caught up in life and forget to look after our vehicle. We just use it to get from A to B. But when was the last time you checked your oil? Tyre pressure? Or even booked it in for a service?\nDoing regular routine safety checks on your vehicle could save you from a blown out tyre or damage to your engine. All these things can mean a massive dent on your finances too. Here are some basic safety checks to help keep your vehicle safe.\nCheck your oil level on a weekly basis.\nIs your horn working? You might just need it. Check the operation of your horn, indicators, headlights, brake lights and tail lights. These could help you avoid an accident.\nCheck for any damage on your windscreen. This includes chips which can lead to nasty irreparable cracks and replacement costs.\nDon’t be caught with a dirty windscreen, it impairs your vision. Check the operation of your windscreen wipers and ensure the rubbers are not damaged and are in good working order. Also check the water level in your windscreen washer reservoir.\nCheck tyre pressures regularly and ensure they meet manufacturer’s specifications and legal limits.\nCheck the operation of your handbrake and braking system.\nSeatbelts are lifesavers. How’s yours? Check the condition of all seat belts, not just the driver’s belt. It is the responsibility of the driver to ensure the safety of their passengers.\nHave you checked the radiator coolant level lately? Most modern vehicles have a coolant reservoir which will show the minimum and maximum levels to refill.\nCheck the general condition of your battery and also the electrolyte level. Some batteries are maintenance free however ensuring the terminals are free from corrosion and are tightly fitted is paramount to keeping your battery lasting for a long time.\nThere’s a lot to prepare when going on a long road journey. So don’t forget to check one of the most important things—your vehicle.\nWe recommend that you have a thorough safety inspection by a qualified technician prior to any long distance travel. Having a technician bring to your attention any worn or unserviceable items may save you a lot of money down the track, and it just may save you breaking down while on your journey.\nLooking after your vehicle is like looking after your health. You need to constantly check it’s in good condition. So it is good policy to check your engine oil weekly, maybe when you refuel your vehicle. Make sure your engine is warm and the vehicle is located on a level surface. Remove the dipstick and check that the oil is located between the minimum and maximum levels located on the dipstick.\nDon’t forget the important items that make your vehicle move. Your tyres. Checking your car’s tyre pressure regularly not only ensures the safety of you and your passengers but it will also extend the life of the tyre. A good tip is to check your tyre pressure each time you refuel your car."", 'The Tire Pressure Monitoring System (TPMS) is an essential tool for drivers to let them know if their vehicle’s tire pressure is normal or if something may be amiss. The TPMS is part of your onboard diagnostic system that communicates to you through your dashboard. This system monitors the pressure in each of your tires to alert you if one or more of the tire’s air pressure has fallen below the ideal PSI (pounds per square inch) recommended for your vehicle.\nYou may recognize the TPMS symbol as an exclamation point inside of a horseshoe with ridges on the bottom similar to tire treads. This system is considered a safety feature and is now required to be equipped in all newer vehicles. Learn how TPMS sensors work and how they alert you to potential troubles related to your tires.\nWhy Do Cars Have TPMS?\nTPMS was first introduced in the early 1980s in mostly European luxury vehicles. In 1997 the first American car, the Chevrolet Corvette, introduced the monitoring system. TPMS was not required until about ten years later, however. In 2000, prompted by the attention from serious injuries and fatalities related to tire pressure incidents, the U.S. Government passed the TREAD Act that mandated several new safety standards as well as required that by 2007, automotive manufacturers equip all new vehicles with TPMS.\nDoes My Car Have TPMS?\nMost cars and light trucks produced in 2008 and later are equipped with some form of Tire Pressure Monitoring Systems. Refer to your owner’s manual for confirmation. Vehicles that do have TPMS will have a warning light displayed on the vehicle’s dashboard if one or more of the tires falls below the recommended pressure as set by the vehicle and tire manufacturer.\nHow Does TPMS Work?\nThere are two types of monitoring systems that are used for TPMS: Direct or Indirect TPMS.\nDirect TPMS utilizes a sensor that is affixed to the wheel to measure the air pressure in each tire. Once the air pressure drops past 25% of the recommended pressure, the sensor communicates the level to the car’s internal computer and causes the light to illuminate.\nIndirect TPMS is part of the car’s Antilock Brake System (ABS) wheel speed sensors. When tire pressure is low, the wheel rolls at a different speed compared to the other tires. The car’s internal computer recognizes the difference and illuminates the light on the dashboard.\nWhy TPMS Is Important\nCaring for your tires is vital to your safety on the road. Ensuring your tires are properly inflated is one of many ways to care for your tires. Tires that are over or underinflated cause premature wear that may require early replacement.\nOverinflated tires reduce traction, increase wear, and struggle to make contact with the road due to excess wear in the center tread. Underinflated tires are a risk to your safety on the road because driving on underinflated tires creates immense heat that results in tire blowouts that could cause severe accidents.\nProperly inflated tires on the other hand, are able to maintain better contact with the road, which increases your vehicle’s handling. Properly inflated tires also:\n- Extend the life of the tire\n- Direct water away from the tires when traveling over wet surfaces\n- Increase fuel efficiency by reducing rolling resistance\n- Provide a cushion for comfort while traveling over rough surfaces\n- Provide better support for the weight of the vehicle\n- Make controlling the vehicle during braking, acceleration, and cornering easier on the driver\n- Disperse heat created from movement\nDo I Still Need to Check My Tire Pressure with TPMS?\nAbsolutely. TPMS is a fantastic tool but it should not be used as a substitute in checking and maintaining your tire pressure. Relying solely on your TPMS system is a risk because the sensors could be malfunctioning and inaccurately transmitting data to your vehicle’s computer, the system may be set to alert you well below the necessary pressure needed to support the weight of the vehicle, and it may not be able to accurately determine if a tire is dangerously low or if other tires are also losing pressure. The sensor is programmed to turn on the warning light when the pressure is about 3lbs above or below the manufacturer’s recommended pressure.\nHow to check your tire’s air pressure:\n- Locate the required pressure level often located on the yellow sticker placed on the frame of the driver’s side door or the owner’s manual. Keep in mind, based on your vehicle type, your vehicle’s front tire pressure may vary from rear tires.\n- Remove the valve cap and press the tire gauge onto the valve stem. The gauge will instantly provide a number that translates to the PSI of the tire.\n- Based on the PSI reading, add or release air as necessary to achieve the ideal air pressure required for your tires.\nNote: be sure to check tire pressure when your tires are “cold”, meaning before you have driven the vehicle.\nCan TPMS Malfunction?\nJust like many components on your vehicle, over time the sensor can wear out. Sensors typically last between 5 and 10 years. TPMS malfunctions can occur due to worn sensor seals, damage to the sensor, dead sensor battery, or transmission to the on-board computer fails. When your Tire Pressure Monitoring System is in failure, the light on the dashboard may come on, though tire pressure is accurate, or the system’s light may flash on and off. In either case, it’s best to take your vehicle to an automotive professional to resolve the issue right away.']"	['<urn:uuid:c04cb370-0956-45bf-a171-1a2266827215>', '<urn:uuid:7535f53a-b111-4af6-b36c-cdb74db0c767>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	5	59	1555
73	For my core training, I'd like to incorporate weighted sit-ups - what's the safest way to perform dumbbell sit-ups to avoid neck and back strain?	To perform dumbbell sit-ups safely, start by lying flat on your back on an exercise mat with knees bent and feet positioned about hip-width apart. Hold two dumbbells against your chest, then brace your core and slowly lift your torso up until the dumbbells touch your knees. Lower yourself back to the ground with control and repeat. It's crucial to pay close attention to your form to avoid potential injury, as incorrect execution can cause back and neck strain. This exercise not only trains your abdominal muscles for a toned six pack but also allows you to work different muscles by adding the weight component.	['Because dumbbells are so versatile, you can complete a full body dumbbell workout in just 30 minutes. You can do these workouts anywhere, in a gym, at your home, or you can take your workout on the go.\nWhile many people think dumbbells are just for your arms, you can use them to work every muscle, including the deep stabilizer muscles. And because dumbbell sets are symmetrical weights, training with dumbbells allows you to get a perfectly even workout on both sides since. This means you won’t have to worry about cheating your weaker side.\nWalking Lunges With Dumbbells: 2 minutes, 2 sets\nWalking lunges are a great way to tone your hamstrings and glutes. This is a good free weight exercise on its own, but adding dumbbells to an already difficult workout will add killer resistance to this exercise.\n- Start by holding a dumbbell in each hand and standing with your feet shoulder-width apart.\n- Take a large step with your right foot and then lower yourself down until your right knee is at a 90-degree angle.\n- Then, stand back up and bring your left foot to meet your right foot.\n- Repeat the lunge on your left foot and continue moving forward.\n- As you lunge make sure your knee does not pass your toes and that your back knee doesn’t touch the ground.\nDumbbell Sit Up: 2 minutes, 3 sets\nDoing sit-ups helps to train your abdominal muscles for a sleek and toned six pack. Adding weight to your sit up not only makes them more difficult, it also makes it possible to work different muscles.\nIf done incorrectly sit-ups can cause back and neck strain. To avoid potential injury, make sure you are paying close attention to your form and doing sit-ups correctly.\n- Start by lying flat on your back on an exercise mat.\n- Bend your knees and set your feet about hip-width apart.\n- Grab two dumbbells and hold them against your chest.\n- Brace your core and then slowly lift your torso up until the dumbbells touch your knees.\n- Then lower yourself back to the ground with control and repeat.\nThrusters: 2 minutes, 2 sets\nThrusters are an extremely challenging full body movement that will work your legs, arms, and core and provide a cardiovascular workout. If you want to add a move to your routine that will really challenge you and get you sweating, then you should be looking at thrusters.\n- To do this move get two dumbbells and stand with your feet shoulder-width apart.\n- Rest the dumbbells on your shoulders in the rack position.\n- Start the movement by doing a squat, getting as low as you can, engaging your lower body.\n- Keep your torso lifted as you go into your squat. Once you have reached your lowest squat position, extend your legs explosively to return to standing.\nWhen you come up into the standing position, press the dumbbells straight above your head. Then lower the dumbbells back to your shoulders and go back into your squat.\nThis move should feel like one fluid motion instead of three separate moves. Try to keep the flow without pausing.\nWhen you first start doing thrusters you may not be able to go full force for two straight minutes. That’s okay, just take it slower if you need to and push yourself a little harder each time you try.\nHammer Curls: 2 minutes, 3 sets\nIncluding hammer curls into your 30-minute dumbbell routine will help you to tone your entire arm. The main effect is in your biceps, but your triceps and forearms will feel it as well.\nThis is the perfect move to add to your routine when you’re looking for anew way to burn out your arms.\n- To start, hold a dumbbell in each hand while taking an athletic stance.\n- Let the weights fall to your side and your palms facing each other. Keep a neutral grip on the dumbbells.\n- Next, bend at the elbow to raise the dumbbells to your shoulders, just like you would with a regular bicep curl.\n- When you have lifted the weights as much as you can, slowly lower them back down with control.\nThe biggest difference between the hammer curl and the bicep curl is that your palms remain facing each throughout the entire movement. This is a good beginner’s exercise for when you want to start your journey to fitness.\nYou should be holding the dumbbells with the same grip you have on a hammer for the whole move. To maintain proper form, keep your feet balanced and ensure that your upper arms are not moving at all when you are doing the curl, only the forearms should move.\nAdditionally, make sure you are always keeping control of the weights without letting momentum take over.\nSingle Arm Plank Rows: 2 minutes, 2 sets\nPlank rows are a good move to include in your 30-minute dumbbell workout because they work your back, chest, core, and arms.\n- You will need to start by getting into a high plank position with your hands resting on two dumbbells.\n- Try to engage your core and hold a high plank for the entire 2 minutes, but if you can’t you can do a modified plank as needed.\n- To start the movement, pull the weight in your right hand up to about your hip height to do the row.\n- Pause for a second when you reach the top and then slowly lower the weight back to the ground.\n- Then do a row with the weight under your left hand.\nIf you need an easier version, try doing this move by supporting yourself on a bench and holding the dumbbell in just one hand.\nIt is amazing what you can do for your body in just 30 minutes. The more you work the faster you are going to see the results.\nTry to do this 30-minute dumbbell workout 2-3 times a week, pushing yourself just a little bit harder every time. Before you know it, you will see your body changing for the better.\nIf you like this, we also have a comprehensive guide on a 30 minute kettlebell workout.']	['<urn:uuid:6b23e354-1a85-446e-881b-87dcd01837fd>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:31:27.499155	25	105	1043
74	first time companies house accounts filing deadline after registration	For first accounts filed with Companies House, the deadline is 21 months after the day of registration with Companies House.	['What is a year end for limited companies?\nBefore you delve into the year-end accounting checklist, it’s important to understand what the year end actually refers to.\nFor limited companies, the year end (which is also known as the ARD or accounting reference date) is the completion of an accounting period. It does not necessarily correlate with the calendar or fiscal year, but rather is usually set according to when the company was incorporated.\nFor example, if a company began on 26th October, their year-end date would be the 31st October and their financial year will run from 1st November to 31st October.\nYear-end accounting checklist\nPrepare your expenses\nThe first step in the accounting checklist is to get your expenses in order. If you make sure to claim everything that you can, you will reduce your overall company profit figure, meaning that you will pay less Corporation Tax.\nThe main rule when it comes to expenses is “wholly and exclusively”, meaning that an expense can only be claimed if it is solely for business use.\nChase up unpaid invoices\nUnpaid invoices can provide a skewed figure at your company year end. It’s important that all of your accounts are as accurate and up-to-date as possible, so the year end is a key time to round up your debts owed. Tracking invoices is much easier with dedicated accounting software.\nMake a note of important deadlines\nThe deadline for filing your Company Tax Return (CT600) with HMRC is typically 12 months after the end of the accounting period it covers. However, it’s best to submit it as early as possible as you’ll have to pay a penalty if you miss the deadline.\nAlthough the deadline for filing your tax return is 12 months after the year end, the deadline for paying your Corporation Tax (or telling HMRC that you don’t owe any) is 9 months and 1 day after your year end.\nThe deadline for Companies House is slightly different, and it’s important that you file your annual accounts before the deadline to avoid a fine. If it’s your first account filed with Companies House, this will be due 21 months after the day you register with them. For your regular annual accounts, these will be due 9 months after your company’s year end.\nFile the relevant documents with HMRC\nThe next step in your year-end accounting checklist is to actually file the following documents with HMRC:\nCompany Tax Return\nHMRC will issue a “notice to deliver a tax return” shortly after your year end. The Company Tax Return (CT600) is filed online with HMRC and contains details of your company’s income, minus allowable expenses and tax allowances. HMRC use your profit to calculate the Corporation Tax you must pay.\nStatutory accounts - also referred to as annual accounts - are prepared from your company’s financial records at the year end.\nThe accounts must include:\n- A profit and loss account.\n- A balance sheet - this shows everything the company owns, owes and is owed at the year end. It must have the name of the director printed on it and be signed by the director.\n- Footnotes on the accounts (information about the transactions between your company and its directors).\n- A director’s report.\nFile the relevant documents with Companies House\nYour statutory accounts must also be submitted to Companies House and most companies can submit these together when filing with HMRC. If you are a small company or micro-entity, you may be able to send ‘abridged’ accounts to Companies House. This will contain a simplified balance sheet with footnotes, and you can choose whether or not to send the director’s report and profit and loss account.\nWe know that company directors can feel extremely overwhelmed at the year end, especially when you’re trying to focus on running your business day-to-day. However, hopefully this year-end accounting checklist has provided you with the necessary information!']	['<urn:uuid:c8ec9042-8698-4daf-a576-cccf76fc63c5>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	9	20	655
75	What role do libraries play in protecting users' personal information while encouraging social learning, and how does this apply to children's use of educational technology?	Libraries follow core principles where users own their data and decide who can access information about their library interactions, while the library strictly protects these privacy choices. At the same time, libraries recognize the value of social learning and allow users to share their information if they choose. For children's educational technology, this translates into a framework where content, context, and the individual child are all considered. Parents and caregivers are encouraged to engage with children during screen time, making it an interactive social experience rather than passive consumption. Libraries provide digital resources like iPads with educational apps that families can use together, while maintaining privacy protections and letting parents control how their children's information is shared.	"['Rebooting Library Privacy in the Age of the Network\nMay 19, 2011\nWhy library privacy matters\nWithout library privacy, individuals might not engage in free and open inquiry for fear that their interactions with the library will be used against them.\nLibrary privacy thus establishes libraries as a sanctuary for thought, a safe place in which any idea can be explored.\nThis in turn establishes the institution that sponsors the library — the town, the school, the government — as a believer in the value of free inquiry.\nThis in turn establishes the notion of free, open, fearless inquiry as a social good deserving of support and protection.\nThus, the value of library privacy scales seamlessly from the individual to the culture.\nPrivacy among the virtues\nLibrary privacy therefore matters, but it has never been the only or even the highest value supported by libraries.\nThe privacy libraries have defended most strictly has been privacy from the government. Privacy from one’s neighbors has been protected rather loosely by norms, and by policies inhibiting the systematic gathering of data. For example, libraries do not give each user a private reading booth with a door and a lock; they thus tolerate less privacy than provided by a typical clothing store changing room or the library’s own restrooms. Likewise, few libraries enforce rules that require users to stand so far apart on check-out lines that they cannot see the books being carried by others. Further, few libraries cover all books with unlabeled gray buckram to keep them from being identifiable in the hands of users.\nPrivacy from neighbors has been less vigorously enforced than privacy from government agents because neighborly violations of privacy are perceived to be less consequential, and because there are positive values to having shared social spaces for reading.\nWhile privacy has been a very high value for libraries, it has never been an absolute value, and is shaded based on norms, convenience, and circumstance.\nPrivacy as a default\nSocial norms about privacy are obviously changing. No one knows yet where they will end up, but clearly we are undergoing a generational transformation.\nNorms are what holds if exceptional circumstances need to be cited to justify contrary actions. In a grocery, the norm is that once an item has been placed in a shopper’s cart, other shoppers are not free to take it for themselves; if you do wish to take an item from another shopper’s cart, you need to give a reason.\nIn software and social systems, norms are expressed as defaults: functionality and configurations that encourage certain uses and behaviors. Defaults and norms are fundamental to human society; without them, we would have to go back to first principles every time we entered a grocery, and would have to renegotiate fundamental rules of behavior every time we queued. They are the implicit that enables us to live together.\nPrivacy is a set of norms expressed by defaults. In a library, for example, the norm is that you can glance at what someone is reading, but if you stare over someone’s shoulder, it will eventually become rude, and after some more staring, a librarian will be called over.\nAs these norms go through a generational change, it is crucial that libraries get the defaults right — or at least righter. There is, however, no possibility of perfection: The privacy norms are changing, the norms are less homogeneous than ever before, and the changes to the defaults will themselves influence the norms.\nDespite this uncertainty, libraries need to do their best to re-balance the values and risks of publicness, in order to address the new norms, and new opportunities\nThe social opportunity\nLibraries have valued reading primarily a private activity, with exceptions when an author is invited to give a reading, or during read-aloud hour in the children’s room.\nThis is not surprising since the technology of reading has been primarily aimed at individual use. Now, however, our new reading technology requires a network, and thus occurs not within a quiet carrel but in a social sphere.\nThe social has been under-represented in traditional library policies that have thought about privacy in terms of a strong dichotomy between the public and the private.\nOf course the social sphere has always been with us, and libraries have enabled many social activities, from book clubs to public readings. But the Internet has made social groups — social networks — easier to form, more visible, and more persistent than ever.\nThe paradigmatic reading experience is no longer that of the solitary individual in front of a fire. The default is changing to that of a networked reader who is sharing what she learns, leaving traces of what she’s read, and contributing back to the network. The marks that people make, the notes they take, the order in which they read, and information about the pages on which they linger all can be used to enrich the network of readers.\nIn a traditional library, a reader’s checking out of a book deprives the library of some value that checking the book back in returns. It’s at best a zero-sum transaction. In a library that enables social reading, the library gets smarter with each user’s interactions with it:\nUsers’ annotations of all sorts are preserved for those users\nUsers can choose to make their annotations available to help guide other users.\nSocial interactions — questions and answers, comments, explications, reviews, etc. — are permanently available to help other users.\nUsers’ interactions help others make sense of the library — how books relate, which ones are worth consulting, how to contextualize a book so that it becomes more understandable and valuable.\nAnonymized metadata about patterns of behavior help the library understand its users’ needs better, and can help users discover other holdings that they would value.\nTogether, these networked social interactions continuously increase the value of the library. That opportunity must be placed on the scales when weighing the value of traditional library privacy policies, for such policies prevent the accrual of this new value.\nA new default\nIt is time to move away from the old default, which was based on non-social, solitary reading as the paradigm. The new networked environment is more complex, and requires a more explicitly nuanced default.\nTo rebalance the risks and opportunities, a framework for new defaults might be:\nIndividual library behaviors will continue to be strictly off-limits to governments, and to any other organization that the user has not explicitly authorized.\nIndividuals will have access to information about their own library behaviors, and libraries will preserve this information by default, unless the individual requests that it not be saved.\nThe library will permit a user’s social network to access that user’s information to the extent to which the individual authorizes it, while informing the user of the risks.\nAggregated information about library behaviors will be made public, taking care to anonymize the information and to protect against re-identification attempts É while recognizing that there always has been and always will be some risk of identification.\nThe Three Laws of Library Privacy*\n1. Users own their data.\nUsers decide who has access to the data about their own interactions with the library and what may be done with that data.\n2. The library fiercely protects the decisions made according to Principle #1.\nThe library enforces the user’s decisions about privacy, and enables public and social access in accord with the user’s decision.\n3. The library is transparent, except where it affects Principles #1 or #2.\nThe library is transparent about its principles, and about how it is handling users’ decisions about privacy, except when such transparency would betray information users have decided not to make public or social.\nCodicil: A new imperative\nLibraries will make use of all available and permitted data in order to help further the interests of its users. They will do this because it advances the values core to the mission of libraries, and thus advances the value of libraries.', 'Where There\'s An App, There\'s A Lap\n""There is no scientific or technological advance that is either good or bad in itself. It is only as we human beings give meaning to science or technology that they will have a positive or negative thrust."" -Fred Rogers\nWe are in a brave new digital world. How can we help support children to develop 21st century skills and a healthy digital diet? With more than a million apps and other new media available to parents, caregivers and educators the search for quality material can be overwhelming and confusing.\nWe have pulled together tips, articles and resources from the leading experts on how to evaluate new media and use new media with children to support their learning and development.\nChildren still learn best when engaging with the world around them and with others. Technology is merely one more tool to facilitate learning.\nThe Three C\'s: Content, Context, and Your Child\nLisa Guernsey, author of Tap, Click, Read: Growing Readers in a World of Screens has created an important and helpful framework for parents, caregivers and educators.\n- content of the media\n- context in which it is being used\n- the individual child who is engaging\nTake the quiz to see how you are using technology today with your children and see how you score.\nQuick Tips on Using Screen Media With Young Children\nSelect content that is designed for your child\'s age and is interactive. Children learn best from any experience when you engage together. Talk about what you are doing together and build on what you are leaning by relating it to the real world. For example, studies have shown that toddlers are 22 times more likely to learn something from a screen when an adult engages in the learning!\n- How you can make screen time more interactive?\n- How do your own screen habits impact children?\n- How can we move children from being consumers of media to creators of media?\n- Each child is unique. Just as you want to find books on topics that interest your child to make reading together more meaningful, consider you child\'s interests and development when selecting and engaging in media together.\n- Watch for children who become dependent on media. Children may be using it as a way to self-soothe or find media to be more predictable than other children and adults. Find media that is interactive and social for these children and set limits.\n- Expand what you are learning with a screen by following it with a real world activity. Are you playing an app with leaves and trees? Go outside and find some leaves together. Create an art project out of the leaves you find. Identify your leaves together.\n- When reading an ebook, read it first without the bells and whistles, this will improve story comprehension, conversation and vocabulary. Children thrive when we talk together and ask questions during reading.\n- The American Academy of Pediatrics (AAP) recommends stopping media use 1 hour before bedtime/naptime to not disrupt sleep patterns. Poor sleep habits impact: behavior, mood, learning and obesity. AAP has a new online tool to help families create a healthy media plan.\n- Engaging in new media is a 21st century skill. Explore apps, ebooks and other media that allow you and your child to also be creators! Take photos and narrate your own story, this reinforces vocabulary, story awareness and seqencing.\nYou know you child best! Find more easy, practical suggestions.\nSource: Screen Sense: Making Smart Decisions About Media Use for Young Children Young Children: March 2015. Lerner, Claire.\nTools to Evaluate Quality & Educational Apps\nMany apps and new media claim to be ""educational"". Are they apps that foster real learning?\nHere are some tools with questions to help guide you in identifying quality apps and media:\nReview Sources for Apps & Media\nHere are links to trusted review sources and curators of quality apps for children. Find the sources that work best for your needs as a parent, educator or caregiver.\nReview sources with some paid subscriptions:\nGeneral review sites:\nSources for educators:\nApp reviews especially for parents:\nPerfect for Circle Time & Stoytime:\nIn addition to the many resources above, Little e Lit has a list of book apps and best storytime apps field tested and loved by children\'s librarians and much more to explore!\nBeanstack, a Sno-Isle Libraries resource available to customers has a list of curated apps to browse. The app descriptions in Beanstack always include an idea for parents and educators on how to extend the learning of the app with children.\nSno-Isle Libraries Resources\nThe library has many new resources for you to use with children using your laptop, smartphone or tablet. You can watch a Mo Willems video on Hoopla. Read a kid\'s magazine together while waiting for an appointment. These are just a few of the many things you can download with your library card!\nRead more about digital tools for young children in our newsletter.\nEach of our 23 libraries also have iPads available for families to use together in the library. These are loaded with early learning apps for children and caregivers to explore and enjoy together. Contact any of our community libraries to learn more.\nSources & Further Reading\nALSC White Paper: Media Mentorship in Libraries Serving Youth. Cen Campbell et al. March 11, 2015.\nAmerican Academy of Pediatrics. Growing Up Digitial: Media Research Symposium. October 1, 2015.\nAsk I-LABS Outreach: What\'s the Big Deal About Screen Media? (posted 2/23/16)\nHendricks, Clara. Ten Ways to Help Parents Navigate Technology with Children. Children & Libraries. Summer 2015.\nJacobson, Linda. Welcome to Readialand: A Bold New Movement Embraces Tech\'s Potential to Support Early Literacy Across Income Levels. School Library Journal. July 2016.\nKabali, Hilda et al. Exposure and use of Mobile Media Devices by Young Children. Pediatrics Vol. 136, No. 6, December 15. Accessed June 17, 2016.\nRainvulle, Kristin Nicole and Bill Gordh. Toward a Narrative Classroom: Storytelling, Media and Literacy. Young Children September 2016.\nThe Joan Ganz Cooney Center. Always Connected: The New Digital Media Habits of Young Children.\nThe Joan Ganz Cooney Center. Family Time With Apps.\nThe Joan Ganz Cooney Center. Take a Giant Step: A Blueprint for Teaching Young Children in a Digital Age.\nNational Association for the Education of Young Children. Technology and Interactive Media as Tools in Early Childhood Programs Serving Children from Birth through Age 8.\nWhen It Comes to Kids, Is All Screen Time Equal? Part 4 of the TED Radio episode Screen Time - Part 1.']"	['<urn:uuid:78b6a2b5-f92f-419a-a8a2-6def64fa2c1a>', '<urn:uuid:4a01a843-07de-416f-889f-0b68c7689e1d>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:31:27.499155	25	117	2433
76	pregnant want good temperature sleep room	According to the National Sleep Foundation, the ideal bedroom temperature for sleep during pregnancy is between 60 and 67 degrees Fahrenheit (15.5 and 19.4 degrees Celsius).	['Sleep is essential for a healthy lifestyle, but even more so when you’re pregnant or postpartum! However, that doesn’t mean sleep is easy to come by. Most women experience sleep disturbances during pregnancy or postpartum so here are 10 tips to help you rest as much as you can! We want you to take care of yourself during this extra special time of your life so that you can enjoy it as much as possible!\nSleep requirements during pregnancy:\nIt can seem hard to get the required amount of sleep during the night with all the possible sleep disturbances women experience!\nAccording to this study, “Hormonal alterations during early pregnancy, enlargement of the fetus during late pregnancy, and a newborn with random sleep-wake patterns all contribute to disrupted sleep.”\nFun fact: progesterone secretions from the placenta (the hormone essential for maintenance of pregnancy) is known to cause fatigue and earlier sleep onset.\nSo how much sleep should you aim to get? Current evidence supports the general recommendation for obtaining 7 or more hours of sleep per night on a regular basis to promote optimal health. Health care providers should prescribe 8 hours of bed time during pregnancy to assure adequate sleep as researchers found “women who slept less than 6 hours at night had longer labors and were 4.5 times more likely to have cesarean deliveries. Women with severely disrupted sleep had longer labors and were 5.2 times more likely to have cesarean deliveries.”\n“Another study reports that the sleep you get in your first trimester can affect your health in the third trimester. Women who don’t get enough sleep (less than five hours per night) in the first trimester are nearly 10 times more likely to develop preeclampsia late in pregnancy.”\nSleep position for optimal sleep during pregnancy and postpartum:\nAccording to Expecting and Empowered: The best position for sleeping while pregnant is on your left side. You’ll want to keep your ribs tucked in, right over your pelvis. Use a pregnancy pillow (or just regular pillows) tucked in alog your back to keep you from rolling! Tuck a pillow between your legs, all the way up to your groin to keep your pelvis in good alignment.\nOther considerations for sleep positioning during pregnancy:\nScrunch a pillow under your head and neck\nKeep your chin tucked in\nTuck a wedge under your bump or roll a towel up and place it right above your hips\nKeep your knees together/at the same level\nThe optimal position for sleeping during postpartum is on your back, surprisingly. Place pillows under your legs to support your low back, one under your thighs and one under your lower leg/feet!\nGetting a massage or regular chiropractic adjustments:\nGetting to relax and/or making sure your body is aligned well can go a long way in keeping you comfortable, especially when you’re near the end of your pregnancy! Staying comfortable and pain free is one way to improve your sleep while pregnant as well as postpartum. Your body goes through a lot of changes and potentially trauma during labor/delivery.\nDecreasing your swelling while pregnant and postpartum can greatly increase how comfortable you feel while sleeping thus leading to better sleep! Some ways to manage any swelling includes:\n- Avoid standing for long periods\n- Watch your sodium intake\n- Drink plenty of fluids\n- Rest with your feet elevated\n- Minimize outdoor time if it’s really hot\nOptimal Sleep Environment:\nGetting good sleep could be as simple as creating an optimal sleep environment! An ideal sleep environment is cool (According to the National Sleep Foundation, the ideal bedroom temperature for sleep is between 60 and 67 degrees Fahrenheit (15.5 and 19.4 degrees Celsius), dark (use black out curtains), and with white noise.\nEstablish a bedtime routine:\nJust like children benefit from a bedtime routine, adults do as well! One of the most important things is going to bed at a regular time, ideally around 10:00pm or whatever will allow for 7-8 hours of sleep.\nSome key components for a bedtime routine include:\nSwitching off your electronics 30-60 minutes before bed\nSimple relaxation or meditation type activities\nRead (from an actual book or magazine)\nListen to music\nWrite down worries or tasks for the next day\nHygienic tasks like a shower and brushing your teeth\nAvoid certain foods and drinks:\nIf you’re having trouble sleeping, make sure to avoid foods that can cause heartburn for dinner (like spicy foods, red foods, etc) and avoid caffeine after 2:00pm as that can make it more difficult to fall asleep later!\nThis may not be possible if you work during the week, but getting extra rest during the day (and/or on the weekend) can be helpful if you’re experiencing sleep disturbances at night. Quick snoozes (little cat naps) are better than taking long naps as those can make you feel more tired!\nWhat to do if you experience insomnia:\nIt can take many people up to 30 minutes to fall asleep. So if you’re still awake after half an hour, it could be that you’re just not ready to sleep yet. Lying awake in your bed, trying to fall asleep can be counterintuitive and perpetuate insomnia.\nIt might help to get up, go into another room, have dim lighting only and repeat some of your routine. After a few minutes, go back to bed and try to fall asleep again.\nHowever, if you feel that you’re wide awake, it might be better to stay up longer until you notice yourself feeling sleepy.\nHumans are programmed to be outdoors while the sun is shining and home in bed at night. This is why melatonin is produced during the dark hours and stops upon exposure to daylight. When people are exposed to sunlight or very bright artificial light in the morning, their nocturnal melatonin production occurs sooner, and they enter into sleep more easily at night.\nThe melatonin precursor, serotonin, is also affected by exposure to daylight. Normally produced during the day, serotonin is only converted to melatonin in darkness. So the more sun you get, the more serotonin that is produced, and the more melatonin you have resulting in a better night’s sleep!\nWe hope you are able to stay as rested as possible during pregnancy with these tips!\nAUTHOR: Ashley Olson is a certified paediatric sleep consultant, owner of Heaven Sent Sleep, and passionate about helping new parents, experienced parents, desperate and sleep-deprived parents form healthy sleep habits for their children.\nShe has over 3 years of experience in working with families and has completed over 150 hours of coursework plus continuing education related to infant and toddler sleep. The focus of her work is on fostering a routine that grows your bond with your child while improving their sleep habits. She specializes in custom sleep plans and one on one support in changing sleep practices!']	['<urn:uuid:103b7b0b-75be-4cff-bd6e-098abc1ed214>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T04:31:27.499155	6	26	1145
77	What's different between underwater and portrait photo rights?	Underwater photography requires technical expertise in swimming and scuba diving to capture marine scenes, while portrait photography focuses on faces and expressions. For legal rights, portrait photos require model releases for commercial use from any identifiable person, while underwater photos generally don't need releases unless featuring recognizable people.	"['7. Nature Photography:\nNature photographers are very much sought-after by media houses. Works of these creative geniuses can be seen in magazines like ʻNational Geographicʼ. Nature photography encompasses various other types of photography, such as-\n8. Landscape Photography:\nThese photographs mainly highlight the natural beauty of any place. It includes areas of wilderness, deserts, mountains or even waterfalls.\n9. Underwater Photography: Underwater photography is an interesting art form that explores a whole new world beneath the deep blue seas. Itʼs an extremely challenging task because a photographer should be equipped with all the knowledge about swimming and scuba diving to capture the best images while swimming in the blue waters!\n10. Seascape Photography: Capturing nature in her many moods would be interesting for all lovers of the environment. A photographer who is dedicated completely to seascape photography should always lie in wait for the beauty of the sea to unfold. Here the concentration is mainly on capturing various moods of the ocean, at different times of the day or even seasons!\n11. Cloudscape Photography: This genre of photography is completely dedicated to capturing various images of the cloud formations. Black and white photography in particular can be creatively used for this branch of photography.\n12. Travel Photography: Travel photography involves capturing images of a particular landscape or even people from different countries. A photographer who is dedicated to this type of photography captures various traditions and customs of a place.\n13. Architectural Photography: Architectural photography, as the name suggests, is all about the capturing images of different styles of architecture in different lights. This type of photography would require the photographers to be an expert in the technical, as well as the visual department.\nRichard J. Levy\n14. Portrait Photography: Portrait photography is all about capturing the mood of a person with an emphasis on the face and expression of a person. This style of photography need not only be about professional models, it can encompass any kind of a person. Family portrait photographs are very much in demand but this does not mean itʼs all about a basic snapshot. There are many creative ways a photographer can use to make a portrait photograph look equally stunning!\n15. Photojournalism: Photojournalism is all about telling a story about a particular event or incident through a single photograph. This genre of photography is used mainly by publications to represent the latest news. In this case, the photographerʼs visuals and the writerʼs story should compliment each other. For example, if a writer is making a specific mention of a particular location in the story, the photographer can find innovative ways to capture the same on camera. Photojournalism can be further classified into –\nBrian L Frank\n16. Documentary photography: Here, the photographer has to present an image that best represents the incident being spoken of in true form. In this case, press photographers usually submit their images to the concerned publication but in recent times, press photographers from all over the world often display these at exhibitions as well.', ""Q1: What is Model Release?\nA1: Model Release is a legal binding document indicating that you have consent from the people (model) shot in your photo. They are aware of being photographed and grant you permission to use their portrait or likeness in a commercial context (e.g. advertisement).\nQ2: Why do we request Model Release?\nA2: Some jurisdictions provide legal protection against a person's image, likeness or property being used for commercial purposes when they have not provided permission. Therefore buyers, especially those representing business, are very cautious about the images with portrait they intend to use in commercials / advertisements. This results in an increased value of the images with proper release clearance.\nQ3: What if I don’t know the model(s) or if I am unable to get their consent?\nA3: Don’t panic. Your photo is still sellable in Fotor Market under the Fotor Editorial License. No model release is required for publication as news (“Editorial Use”) of a photo taken of an identifiable person when the person is in a public space. But since there are many constraints in usage of these photos, they have limited value for business buyers other than news agencies.\nQ4: Who will buy my photo?\nA4: As a comprehensive photo solution platform, Fotor has a large user base. Fotor users consist of both photographers and photo buyers. Independent designers, advertising companies, art institutions, small and middle sized enterprises, news agencies, online merchants, websites and mobile app developers, these are all potential buyers of your photo. Their use of your photo shall be in compliance with the License Agreement published on Fotor’s website. For avoidance of doubt, none of the Fotor License Agreements has exclusive terms, which means you still keep the ownership of the copyright and can sell your photo’s license to elsewhere, unless an additional exclusive agreement is reached between you and Fotor.\nQ5: What if the faces in my photo are not identifiable?\nA5: If the faces on your photo are not identifiable by the general public, Fotor does not require a model release. Keep in mind, some world-leading stock photo providers may apply more stringent standard on this issue. If you are willing to sell photos through their global networks via Fotor, you may be requested to provide a model release at a later time.\nQ6: Can I ask all the models to sign only one document if this is a collective photo?\nA6: Yes, you can. After signature, take another collective photo of them together with the signed model release.\nQ7: Do I need to sign another agreement to sell my photo under the Editorial License if I fail to get the model release?\nA7: No, you don’t have to if you already checked the option of granting Fotor the right to sell your photo when you uploaded it to Fotor. Please see Fotor’s Terms of Services for more information about the permission that you gave Fotor.\nQ8: Do I need to provide model release(s) if my work is street / travel photography?\nA8: In most countries, you can take photos of people in public space, in condition that the subject does not explicitly object being shot, that you do it with respect and that you do not violate any laws. Respect in this case means that a photo taken does not undermine dignity and / or private life of a person in question. The real issue to consider is whether the photo will be used for commercial purposes after it is taken.\nIf you wish to sell your photo under a commercial license, the model release is required from the persons whose faces are recognizable. Considering the difficulty in practice when photographers are travelling or doing street photography, alternative means can be applied, such as using a pocket release form or model release apps. It’s much easier to get a model release immediately before or after photographing a subject than it is to try to track down a stranger for a waiver after the fact.\nQ9: The child in the photo is myself of 30 years ago. Is this photo sellable? Do I need to provide a model release?\nA9: The old photo also has its commercial value but you must ensure that you are now the copyright owner of this photo or the legal heir of the rights. If you wish to sell the photo under a commercial license, the model release is required. Generally, a model release for a photo shooting a child shall be signed by the child’s legal guardian (usually parents) but not the child himself / herself. In your special case, you can use either the Minor’s Model Release template or a standard model release template, but sign it by yourself (adult now).\nQ10: People in the photo are not professional models but my friends. So I don’t think I need to provide a model release to Fotor?\nA10: Unfortunately, if you wish to sell your photo under a commercial license, you have to provide a model release for each of the recognizable persons on your photo, with no exception even if it’s yourself, your families, friends, or strangers in the street. It’s an extreme case but it occasionally happens: if the person you shot on your photo passed away, you shall get the release or consent from his / her family (legal heirs). If the subject on your photo is a child, a Minor’s Model Release is required and it shall be signed by the child’s legal guardian (usually parents) but not the child himself / herself.\nStill need help? Let us know !""]"	['<urn:uuid:e9a04ec6-49aa-4f1f-ac29-be5daf9d74c0>', '<urn:uuid:afa97275-7ecc-4175-aec5-fdc2fcc784c8>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T04:31:27.499155	8	48	1438
78	What causes Down syndrome in terms of genetics, and how does this genetic difference relate to the high occurrence of Alzheimer's disease in these individuals?	Down syndrome is caused by an error in cell division called nondisjunction, resulting in 47 chromosomes instead of 46. Specifically, in 95% of cases, there are three copies of chromosome 21 instead of two. This extra chromosome 21 is significant because it contains the Amyloid-beta Precursor Protein gene, leading to a 1.5-fold increase in Amyloid-beta protein. This increased protein production is directly linked to Alzheimer's disease development, though interestingly, individuals can have this pathology for over a decade before cognitive decline becomes apparent.	"['Home > Preview\nThe flashcards below were created by user\non FreezingBlue Flashcards.\nWhat is Down syndrome?\nA genitic condition that causes delays in physical and intellectual development\nincidence of down syndrome?\none in every 700 live births, most frequently occuring chromosomal disorder. NOT related to race, religion or socioeconomic status.\nCause of Down Syndrome?\nan error in cell division called nondisjunction. occurs at conception and is not related to anything the mother did during pregnancy.\nWhat increases the incidence of Down Syndrome?\nadvanced maternal age but 80% of children born to women under 35 yrs old.\nWhat is different about the chromosomes of individuals with Down Syndrome?\nThey have 47 chromosomes instead of 46.\n3 chromosomal patterns that result in Down Syndrome and which is the most common?\n- trisomy 21 (nondisjunction) - MOST common (95%)\n- translocation (3-4%)\n- mosaicism (1-2%)\nWhat is trisomy 21 caused by?\na faulty cell division that resluts in the baby having three #21 chromosomes instead of two. Prior or at conception, a pair of #21 chromosomes in either the egg or the sperm fails to separate properly. The extra chromosome is replicated in every cell of the body.\nWhat is translocation caused by?\npart of chromosome #21 breaks off during cell division and attaches to another choromosome. The presence of an exra piece of the 21st chromosome causes the characteristics of down syndrome. May indicate that one of the parents is carrying chromosomal material that is arranged in an unusual manner.\nWhat is mosaicism cause by?\nOccurs when nondisjunction of chromosome #21 takes place in one of the initial cell divisions after fertilization. when this happens, there is a mixture of two types of cells, some containing 46 chromosomes and some with 47. because of the mosaic pattern of the cells, the term mosaicism is used.\nWhen is Down Syndrome diagnosed?\n- before or at birth. Before birth, ultrasound 10-14 weeks gestation - thick neck (nuchal translucency)\n- initially the diagnosis is based on physical characteristics.\n- The diagnosis must be confirmed by a chromosome study (karyotype)\nCommon physical features of Down Syndrome?\n- Flattened nasal bridge\n- almond shaped eyes\n- flat occiput\n- short limbs, short broad hands and feet\n- high arched palate - protruding tongue\n- muscle hypotonia\n- jt hyperextensibility\n- simian line (transverse palmer crease)\nCommon medical manifestations of Down Syndrome?\n- hypothyroidism (8%) require medication\n- heart disease: (50%) defects may require medication or surgery\n- increased incidence of childhood leukemia (1%) and incidence of colds and infections.\nSecondary medical manifestations (after age 30-35) of Down Syndrome?\nobesity, diabetes, CVD, osteoarthritis, orstoporosis.\nby age 40 almost everyone with Down Syndrome will have symptoms of alzheimers.\nCommon orthopedic impairments of peopl with Down Syndrome?\nexcessive foot pronation, scoliosis, slipped capital femoral epiphyses (hip abduction with hypotonia), late hip dislocation (after age 2), patellofemoral disroders (patellar dislocation), atlantoaxial (C1-C2) instability.\nvision impairments of patients with Down Syndrome?\n35-60% have affected vision. Near or far sighted, esotropia (cross eyed) and/or cataracts. May require surgery or glasses.\nHearing impairments of patients with Down Syndrome?\n66-89% have hearing impairments. Ear (internal and external) deformities may lead to hearing loss. Regular hearing exams recommended.\nCognitive, communication, and learning impairments of people with Down Syndrome?\n""intellectually imapired"" can be mild to severe. Learning disabilities may be present requiring special education. Receptive and expressive language may be delayed requiring speech therapy, augmentative communication, and/or sign language.\nGross motor and mobility skills of people with Down Syndrome?\ndelayed due to low muscle tone, loose ligaments, and decrease strength. Walking usually occurs around age 2. Gross motor development continues throughout the lifespan, yet it remains delayed. Increased incidence of overweight with age may affect fitness level.\nGross motor clinical obsercations of patients with Down Syndrome?\nPoor upper extremity midline movements, slow reaction time, and slower postural reaction times.\nGait of people with Down Syndrome?\nShort steps, wide base of support, increased knee flexion in swing, increased knee flexion in swing and hyperextension in stance, Decreased single limb support.\nPrognosis and outcomes of patients with Down Syndrome?\nLife expectancy is approximately 55 years. Education and work training available, allowing great opportunity for productivity: many go on to college, have jobs, and live independently.\nWhat is atlantoaxial (C1-C2) instability?\n- Subluxation between C1 and C2. Most cases are asymptomatic. Cervical subluxation greater than 4.5 mm is an indicator for intervention (surgical).\n- Secondary to ligamentous laxity or poor development of the odontoid or abnormal syringomyelia.\nUpper cervical spine instability signs and symptoms?\nhyperreflexia, clonus, positive babinski\'s sign, loss of strength, changes in senstion, torticollis, loss of established bowel and bladder control, derease or loss of established motor skills.\nComplications of arthrodesis?\nbone graft reabsorption, wound dehiscence and infection, instability of adjacent motion segment, incomplete reduction of C1-C2, neurologic sequelae.\nWhat is the AA debate about?\nage for x-rays to identify AAI, reliability of x-ray results to identify AAI, surgical intervention.\nWhat do people need in order to participate in special olympics?\nThey need to be medically cleared for AAI. Need to communicate with local orthopedic surgeon.\nWhat activities should people with Down Syndrome avoid?\nDirect downward forces such as tumbling, diving, horse back riding, carnival rides, driving on poorly maintained road.\nWhat is PT treatment focused around?\nThere is no cure, treatment is focused on specific impairments. PT cannot change muscle tone.', ""Individuals with Down syndrome (DS) have been largely neglected in therapeutic and biomarker studies of Alzheimer's disease (AD). Adults with DS are uniformly affected by AD pathology by their 30's and have a 70-80% chance of clinical dementia by their 60's. In 95% of cases, DS is associated with three copies of chromosome 21, each containing of copy of the Amyloid-beta (A?) Precursor Protein gene (leading to a 1.5-fold increase in A? protein). Yet, nowhere is it clearer than in DS that A? deposition is not sufficiet to produce dementia, as individuals harbor this pathology for over a decade before cognitive decline is apparent. DS can be seen as a setting of amplified sensitivity to risk and protective factors that moderate the relationship between A?, neurodegeneration and clinical dementia. Understanding the factors that moderate this relationship in DS and biomarkers for those factors is critically important in the design of therapeutic trials for AD in DS and in general. Thus, this longitudinal study of Neurodegeneration in Aging DS (NiAD) and its relationship to cognition has the potential to: 1) identify critical factors that link A? deposition to neurodegeneration and, ultimately, dementia; 2) define biomarkers for these factors; and, most importantly, 3) set a foundation for an efficient transition from this biomarker study to a therapeutic trial to combat A in DS augmented by biomarker outcomes. For the past 5 years, the three independent research groups included in this application have been studying the course of A? deposition and other imaging biomarkers and their impact on cognitive/functional measures in adults with DS [(a) the combined Pittsburgh/Madison study; (b) the Banner Alzheimer's Institute study; and (c) the Cambridge study]. In their ongoing work, 140 adults with DS (including 23 with DS/AD-dementia) have undergone magnetic resonance imaging (MRI) and amyloid-positron emission tomography (PET) scans and neuropsychological/ functional assessments. These three research groups now propose to combine resources and harmonize all protocols in response to the request from NIA/NICHD to develop a large AD biomarker study in DS. This study will be further strengthened by aligning NiAD with the three largest ongoing longitudinal studies of AD biomarkers in the general population: the Alzheimer Disease Neuroimaging Initiative (ADNI), the Dominantly Inherited Alzheimer Network (DIAN) and the Alzheimer Prevention Initiative (API). All data will be made available in an open-access format using a model similar to ADNI. The established DS cohort is a significant advantage that will shorten the recruitment phase, maximize longitudinal data that can be acquired and allow for addition of new biomarkers to be compared to longitudinal clinical and imaging measures. The proposed 5-year longitudinal study will examine progression of AD related biomarkers (A?-, tau- and fluorodeoxyglucose-PET, structural and functional MRI, cerebrospinal fluid A? and tau, plasma A? and proteomics, genetics, neuropathology) and cognitive/functional measures in 180 adults with DS (>25 yrs. of age) and 40 biomarker-controls. Subjects will be re-evaluated every 15 months to assess changes in cognition/adaptive functioning and every 30 months to detect biomarker changes.\nAdults with Down syndrome (DS) are at an extremely high risk for developing Alzheimer's disease (AD), with most individuals over age 40 evidencing neurofibrillary tangles and neuritic plaques (which are thought to be associated with the eventual appearance of AD symptoms). The goal of the current application is to recruit and follow 180 adults with DS and 40 biomarker controls to enable the identification of the longitudinal progression of AD in adults with DS using clinical, cognitive, imaging and genetic and biochemical biomarkers. This data is not only necessary to deepen our understanding of the pathophysiology of AD in DS, but may also offer information that will prove useful in the design of treatment trials to slow or prevent AD in DS.\n|Hu, Ziheng; Wang, Lirong; Ma, Shifan et al. (2018) Synergism of antihypertensives and cholinesterase inhibitors in Alzheimer's disease. Alzheimers Dement (N Y) 4:542-555|\n|Handen, Benjamin L; Mazefsky, Carla A; Gabriels, Robin L et al. (2018) Risk Factors for Self-injurious Behavior in an Inpatient Psychiatric Sample of Children with Autism Spectrum Disorder: A Naturalistic Observation Study. J Autism Dev Disord 48:3678-3688|\n|Lao, Patrick J; Handen, Ben L; Betthauser, Tobey J et al. (2018) Alzheimer-Like Pattern of Hypometabolism Emerges with Elevated Amyloid-? Burden in Down Syndrome. J Alzheimers Dis 61:631-644|\n|Lao, Patrick J; Handen, Ben L; Betthauser, Tobey J et al. (2018) Imaging neurodegeneration in Down syndrome: brain templates for amyloid burden and tissue segmentation. Brain Imaging Behav :|\n|Cohen, Ann D; McDade, Eric; Christian, Brad et al. (2018) Early striatal amyloid deposition distinguishes Down syndrome and autosomal dominant Alzheimer's disease from late-onset amyloid deposition. Alzheimers Dement 14:743-750|\n|Hartley, Sigan L; Handen, Benjamin L; Devenny, Darlynne et al. (2017) Cognitive decline and brain amyloid-? accumulation across 3 years in adults with Down syndrome. Neurobiol Aging 58:68-76|\n|Mihaila, Iulia; Hartley, Sigan L; Handen, Benjamin L et al. (2017) Leisure Activity and Caregiver Involvement in Middle-Aged and Older Adults With Down Syndrome. Intellect Dev Disabil 55:97-109|\n|Lao, Patrick J; Handen, Ben L; Betthauser, Tobey J et al. (2017) Longitudinal changes in amyloid positron emission tomography and volumetric magnetic resonance imaging in the nondemented Down syndrome population. Alzheimers Dement (Amst) 9:1-9|\n|Lao, Patrick J; Betthauser, Tobey J; Hillmer, Ansel T et al. (2016) The effects of normal aging on amyloid-? deposition in nondemented adults with Down syndrome as imaged by carbon 11-labeled Pittsburgh compound B. Alzheimers Dement 12:380-90|\n|Hartley, Sigan L; Handen, Benjamin L; Devenny, Darlynne A et al. (2014) Cognitive functioning in relation to brain amyloid-? in healthy adults with Down syndrome. Brain 137:2556-63|""]"	['<urn:uuid:1b4601bd-27ac-4646-820a-4337667a5bdb>', '<urn:uuid:50a2013c-e96b-4b56-a8f9-3b95f71d35cd>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T04:31:27.499155	25	83	1812
79	How many lifeboat stations does RNLI operate in the UK and Ireland?	The RNLI operates 238 lifeboat stations in the UK and Ireland and more than 240 lifeguard units on beaches around the UK and Channel Islands.	['Multiple Saturday launches for Largs RNLI.\nThe volunteer crew at Largs RNLI were tasked twice by UK Coastguard on Saturday 11 June in response to a 35-foot yacht which had run aground.\nThe volunteer lifeboat crew quickly arrived on scene and ascertained all six people on board the stricken boat were safe.\nThe initial decision was taken to try to tow the vessel into deeper water, however this was unsuccessful due to the on shore wind and falling tide.\nInstead the boat was made safe, the lifeboat crew monitored as the crew from the casualty vessel used their tender to lay out their anchor, allowing them to wait for the rising tide. It was recommended by the lifeboat crew that non-essential personnel were taken off the stricken vessel, but this offer of assistance was declined.\nUK Coastguard stood the lifeboat down and it returned to the lifeboat station at 3:40pm, where it was cleaned, refuelled and made ready for service.\nAt 6pm on Saturday evening concern had been expressed about the casualty vessel’s ability to re-float unaided so Largs lifeboat launched for the second time.\nWith the onshore wind of around 24-30 knots and a flood tide the decision was made to assist the yacht as it was re-floated.\nA lifeboat crew member was put onboard the casualty vessel to connect the tow line. The lifeboat then pulled the vessel bow onto the weather and held it there until the tide had flooded enough for the casualty to be towed into deeper water.\nThe casualty vessel had to let go its anchor to allow the lifeboat to manoeuvre it. The lifeboat crew tried to recover the anchor but were unable to do so.\nThe lifeboat crew member remained on the casualty vessel until it was safely berthed in Largs Yacht Haven and met by Largs Coastguard Rescue Team.\nThe lifeboat was then stood down by UK Coastguard, when for the second time on Saturday the lifeboat and its volunteer crew returned to the lifeboat station, where the lifeboat was cleaned, refuelled and made ready for service.\nJohn Griffiths, Lifeboat Operations Manager at Largs RNLI, said, ‘We would remind anyone planning a trip to sea to always respect the water. It is important to check the local tide times and weather forecast before venturing out, always carry a means of communication and wear a suitable lifejacket for your activity.\n‘In a coastal emergency, or if you see someone in trouble at sea, you should dial 999 and ask for the Coastguard.’\nKey facts about the RNLI\nThe RNLI charity saves lives at sea. Its volunteers provide a 24-hour search and rescue service around the United Kingdom and Republic of Ireland coasts. The RNLI operates 238 lifeboat stations in the UK and Ireland and more than 240 lifeguard units on beaches around the UK and Channel Islands. The RNLI is independent of Coastguard and government and depends on voluntary donations and legacies to maintain its rescue service. Since the RNLI was founded in 1824, its lifeboat crews and lifeguards have saved over 142,700 lives.\nLearn more about the RNLI\nContacting the RNLI - public enquiries']	['<urn:uuid:0b02bf05-d348-4059-b7e9-5ca8a32bc2b1>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:31:27.499155	12	25	523
80	environmental engineer salary power plant health hazards occupational risks working conditions	Environmental engineers working in power plants earn a median salary of $88,860. They face various occupational health hazards including exposure to extreme heat stress, toxic chemicals, risk of fires and explosions, and potential hearing loss. They often work alongside other engineers in potentially hazardous infrastructure projects and must follow strict safety protocols.	"['If you are fascinated with nature and like applying math and science concepts and thinking analytically, then you may consider getting an online environmental engineering degree.\nA bachelors degree in environmental engineering can help qualify you for jobs in the field while opening doors to even more promising career options down the road.\nEditorial Listing ShortCode:\nAn environmental engineering program includes courses that require advanced math and technical knowledge, and a career in this field typically involves additional learning on the job.\nOnline Environmental Engineering Degrees\nIf you want to get your degree online, you should find many online environmental engineering degree programs at accredited schools that offer flexible and convenient schedules.\nThere’s growing research in the field of environmental science and the growing need to protect natural habitats and resources. Environmental engineers have vital roles to play among other engineering specialists for years to come.\nThe pressing need to find solutions to problems like air pollution and climate change may also drive job growth in the field. Many environmental engineers work alongside civil engineers or engineering project managers.\nEditorial Listing ShortCode:\nEnvironmental engineers can help design, operate, retrofit, or manage any number of environmentally sensitive infrastructure projects. These can include rural development projects, projects impacting major water ways, and projects for waste management facilities.\nA bachelors degree is required for many entry-level jobs in the environmental engineering field. So, earning an online environmental engineering bachelors degree from an accredited university can be a strategic first step if you want to pursue a career in this field.\nEnvironmental Engineering Careers & Salaries\nAccording to the Bureau of Labor Statistics, a bachelors in environmental engineering can open the doors to a wide field of challenging and rewarding career opportunities.\n|Careers||Annual Median Salaries|\n|Architectural and Engineering Managers||$144,830|\n|Natural Sciences Managers||$129,100|\n|Environmental Engineers, Federal Government||$105,410|\n|Environmental Engineers, Engineering Services||$89,050|\n|Environmental Engineers, All||$88,860|\n|Environmental Scientists and Specialists||$71,360|\nEngineering careers related to the environment and natural resources come in many forms. An online environmental engineering bachelors degree program can be instrumental in preparing you to forge an exciting and well-paid path.\nEnvironmental Engineering Bachelor’s Curriculum\nAn online environmental engineering program’s curriculum will typically expose you to a range of inter-connected engineering trends, perspectives, concepts, and methods. Curricula will vary by program, but some common courses include the following:\n- Fundamentals of Environmental Engineering: This course is an overview of methods and knowledge applied in the field, including a survey of the roles environmental engineers play in the larger field of civil engineering.\n- Engineering Physics and Design: You’ll learn critical physical and mechanical concepts and methods governing engineering design work.\n- Materials and Structures: All engineers need to study the fundamental properties and dynamics of diverse materials used across structural and mechanical applications.\n- Solid Waste Management and Waste Water Treatment: You’ll learn how environmental engineers design, operate, and manage systems for handling solid waste and waste water while protecting ecosystems and human health.\n- Environmental Law and Impact Analysis: This course provides insights into the critical ethical, legal, and policy frameworks that guide environmental engineering decision-making and introduces environmental impact analysis methods.\n- Principles of Surveying: This course covers the principles and methods used in land surveying, including mapping work, measuring slopes and grades, and methods for surveying diverse land formations.\n- Geo-Environmental Engineering: You’ll learn about rock and soil mechanics, principles of hydrology, and other aspects of environmental ecosystems for engineering design and decision-making challenges related to the natural environment.\n- Hydrology: You’ll study the properties and dynamics of the planet’s water systems, including the study of the water cycle, surface water dynamics, and methods and principles for measuring water impacts in diverse engineering scenarios.\n- Civil Construction Management: You’ll learn about the logistics of developing and managing large civil engineering projects with an emphasis on land use, environmental forces and materials, and environmental impacts and regulations.\n- Environmental Engineering Design: This course focuses on design methods and processes that reflect sound, safe, and ethical environmental engineering methods and introduces you to computer-assisted design tools and concepts.\nA bachelor’s degree program in environmental engineering can offer specialized and very practical knowledge to help open doors to meaningful career opportunities in this field.\nWhen pursuing an environmental engineering degree online, it is necessary to have already received your high school diploma or passed the GED examination. Prerequisite coursework in math and science are likely to be required for admission as well.\nAdmission requirements can vary by school, but they typically include:\n- A high school diploma or GED\n- A satisfactory GPA, usually 3.0 or better\n- Satisfactory SAT or ACT scores, if required\n- Completion of relevant high school courses, like physics and advanced math\nIf you skipped over some important prerequisite courses in high school, there are some online degree programs that are designed to allow concurrent enrollment in the courses you need to make up.\nGetting a college degree is a big investment and a serious commitment, so it’s typically best to stick with regionally accredited universities when you’re deciding what school to attend.\nWhen a school and program have regional accreditation, their curricula, instruction, and academic resources and support have been found to meet acceptable standards. Accreditation also helps ensure that your degree will be honored by other schools and by prospective employers. It can be required for certain types of financial aid as well.\nEditorial Listing ShortCode:\nThe Council for Higher Education Accreditation (CHEA) can help you learn more about accreditation and finding accredited schools.\nFinancial Aid and Scholarships\nWhen pursuing a bachelor of engineering online or on campus, the upfront costs for tuition or other expenses can sometimes pose a challenge.\nMany students overcome financial barriers to getting a degree by applying for a need-based financial aid package. A financial aid package may include educational grants, tuition discounts, scholarship money, and student loans.\nIt’s beneficial to look over any financial aid package carefully, but if you qualify, you may be able to start studying sooner rather than later. You can start by filling out your Free Application for Federal Student Aid (FAFSA).\nProfessional Environmental Organizations\nWhether you’re just starting your bachelor’s degree program in environmental engineering or still searching for the right school, it’s not too soon to join one of these professional organizations:\n- American Academy of Environmental Engineers and Scientists\n- American Society for Engineering Education\n- National Society of Professional Engineers\nA professional membership can help you stay on top of news in your field. It may also lead to some valuable opportunities to learn specialized knowledge in the field and build a professional network.\nWhat Can You Do with an Environmental Engineering Degree?\nThe expertise of environmental engineers is needed today for the development of new civil engineering projects and for upgrading or retrofitting existing infrastructures.\nEnvironmental engineers are also needed to build, restore, and manage the environmental impacts of human activity. This can include trying to control pollution, solve the climate crisis, or improve the management of waste-handling facilities.\nProfessionals in the field may work as environmental engineers, hydrologists, civil engineers, geoscientists, environmental scientists, and natural sciences managers.\nHow Long Does It Take to Get a Bachelors in Environmental Engineering Online?\nLike other bachelor degree programs, most accredited online environmental engineering degrees will typically take 4 years with full-time enrollment and a traditional, 16 week semester schedule.\nSome online programs, including accredited ones, are designed to help you finish sooner. If you follow an 8 week semester and stay continuously enrolled year-round, you might finish in less than 4 years.\nSome online programs offer more flexible study options. This can be an especially beneficial option if you plan to keep working or have other commitments to juggle. This flexibility might be what you need, but it may mean taking longer to complete your degree.\nIs Environmental Engineering a Good Degree?\nA bachelors degree program in environmental engineering deals with the natural environment, natural resources, environmental science and protection, and the study of civil engineering principles and methods. If these topics interest you, then this field may be a good fit for you.\nThe jobs you may qualify for can vary greatly depending on your degree concentration, where you want to work, your past experience, and your job preferences. Engineering jobs generally pay very well. The average salary for environmental engineers is $88,860, according to the Bureau of Labor Statistics.\nEditorial Listing ShortCode:\nAn aptitude for math and science can be very beneficial in this field. Aside from specialized knowledge, an environmental engineering degree can also teach you lots of practical knowledge and job skills.\nHow Much Does an Environmental Engineer Make?\nEnvironmental engineers make an average annual salary of $88,860 (Bureau of Labor Statistics). Environmental engineers’ salaries can vary based on their work history, area of concentration, employer, and geographic location.\nProfessionals in the field may also work as hydrologists or geoscientists. Hydrologists make an average salary of $81,270 while geoscientists earn average salaries of $92,040. A natural sciences manager is a common leadership position in this field. The average salary for this position is $129,100, according to the Bureau of Labor Statistics.\nA bachelor’s degree in environmental engineering or in a closely related field is often the academic requirement to qualify for job opportunities in this field.\nIs There a Demand for Environmental Engineers in the Future?\nThe field of environmental engineering is predicted to maintain steady job growth. The Bureau of Labor Statistics forecasts that jobs for environmental engineers will grow at a 3% rate through 2029, which translates into average job growth compared to other fields.\nJob growth in many closely related careers, such as hydrologist, geoscientist, and natural sciences manager, are forecast at 5%, which is above average job growth.\nIs Environmental Engineering Worth It?\nYes, a bachelors degree in environmental engineering is worth it for many students. A bachelors degree in the field can equip you to explore opportunities for a range of entry-level jobs that might open doors for new learning based on your engineering career interests and goals.\nThere is a need for infrastructure expansions and upgrades, and there are growing concerns about environmental impacts and evolving environmental laws. So, environmental engineering professionals are increasingly in-demand.\nIn addition to working on infrastructure projects, professionals in the field also work in resource management, waste management, geo-science research, and engineering design. These areas can fuel future job opportunities for environmental engineers and those working in related fields.\nThe Bureau of Labor Statistics predicts that jobs for environmental engineers will see continued average job growth over the next ten years. Many related occupations are also forecast for equal or greater job growth over this same period.\nUniversities Offering Online Bachelors in Environmental Engineering Degree Programs\nMethodology: The following school list is in alphabetical order. To be included, a college or university must be regionally accredited and offer degree programs online or in a hybrid format.\nAmerican Public University offers an online Bachelor of Science in Environmental Science program. This degree program focuses on the study of ecological disasters on public health and how to increase sustainability and responsible development.\nThe program requires 120 credit hours to graduate, and you may be able to transfer up to 90 credit hours.\nAPUS is accredited by the Higher Learning Commission.\nOakland University’s online Bachelor of Science in Environmental Health and Safety is designed to teach how to identify, evaluate, and eliminate environmental hazards.\nStudents have the option to obtain their degrees entirely online, entirely on-campus, or as a combination of both types of classes. The degree requires 125 credit hours of coursework.\nOakland University is accredited by the Higher Learning Commission.\nOregon State University offers a Bachelor of Science in Environmental Engineering program that is accredited by the Engineering Accreditation Commission of ABET.\nSubjects studied in this program include analysis and design of water and wastewater treatment systems, hazardous substance management, and air pollution control. Students are required to participate in a capstone-design experience.\nOregon State University is accredited by the Northwest Commission on Colleges and Universities.\nPortland State University offers a Bachelor’s in Environmental Engineering. Topics of study in this program include energy systems, water distribution systems, and the development of infrastructures that support sustainability.\nStudents in this program have access to internships in the Portland Area. Coursework can be completed online, on campus, or in a combination of both.\nPortland State University is accredited by the Northwest Commission on Colleges and Universities.\nSouthern New Hampshire University offers an Environmental Science degree that is entirely online. Custom lab kits are mailed to students, and labs are led by instructors online so you can still get hands-on experience. You may customize your degree with electives or add a natural resources concentration.\nSouthern New Hampshire University is accredited by the New England Commission of Higher Education.\nThe University of Arizona offers a Bachelor of Science in Environmental Science that is entirely online. The program offers two areas of emphasis to choose from: Soil, Air, and Water, or Leadership, Sustainability, and Communication.\nThis degree program is designed to be suitable for someone wanting to conduct scientific research or to work in advocacy and policy.\nThe University of Arizona is accredited by the Higher Learning Commission.\nThe University of Phoenix offers an online Bachelor of Science in Environmental Science. The degree program covers topics such as renewable resources, sustainable green living, and urban infrastructure. The program requires 120 credit hours, including 15 credit hours in a focus study of your choosing.\nThe University of Phoenix is accredited by the Higher Learning Commission.\nThe B.S. in Environmental Engineering program at the University of Toledo is designed to provide a well-rounded education. It includes courses in engineering, environmental and natural sciences, economics, and policy. This is an on-campus degree program.\nThe University of Toledo is a research university, and undergraduates often have opportunities to work on research projects.\nThe University of Toledo is accredited by the Higher Learning Commission.\nThe University of Washington added a Bachelor of Science in Environmental Engineering in 2017 to respond to the increasing demand for environmental engineers.\nThe degree program is ABET accredited and focuses on applying engineering principles to concerns such as air pollution control and environmental sustainability. This is an on-campus degree program.\nThe University of Washington is accredited by the Northwest Commission on Colleges and Universities.\nA Bachelor of Science in Environmental Science and Policy from Wilmington University can be earned entirely online. Students in this program receive kits so they can conduct labs at home.\nInternships are available through Wilmington’s partnerships to help students get real-world experience. This degree program also emphasizes strong writing and communication skills.\nWilmington University is accredited by the Middle States Commission on Higher Education.\nGetting Your Environmental Engineering Degree Online\nIf you have a general interest in the stewardship of ecosystems and natural resources, and if you are fascinated by the features, properties, and forces of the natural world, then a bachelor’s degree program in environmental engineering might be a good fit for you.\nYou can choose to pursue a graduate degree later on, such as an environmental engineering masters online or, if it fits in with your work schedule, through a traditional campus program. But a bachelor’s degree in environmental engineering is enough to help you take the first steps toward a rewarding career in this field.\nWith so many convenient and flexible online degree options being offered now, this could be a fine time to explore accredited online bachelors programs in environmental engineering.\nEditorial Listing ShortCode:', 'ppt on occupational health hazards in power plant\nSEYED ALI JOZI HEALTH, health-safety, biophysical and socioeconomic sections of the power plant, fac-tors influenced by the power plant activities like fire and explosion, hearing loss, quantity of groundwater, power generation are among the most important factors causing risk in the power plant The drop in ,Temporary Electrical Power, Keeping it Safe ,, Temporary Electrical Power, Keeping it Safe! , This article originally appeared in the September 2016 issue of Occupational Health & Safety Printable Format; , Steel Plant Becomes Kentucky\'s .WORLD BANK GROUP Environmental, Health, and Safety ,, Environmental, Health, and Safety Guidelines THERMAL POWER PLANTS DECEMBER 19, 2008 2 WORLD BANK GROUP the EHS Guidelines for Mining and the EHS Guidelines for Electric Power Transmission and Distribution Decisions to invest in this sector by one or more members of the World Bank Group are made within the context of the World Bankoccupational health hazard in stone crusher in ppt, occupational health hazard in stone crusher in ppt stone crushing machines safety and health power points pdf stone crushing plant presentation-[mining plant] Joyal-Stone Crusher,Stone Crusher Machine,Rock ppt file of stone crusher platnt safety inspection checklist for crushing plant Health and Safety in Stone Crushing Get A Free QuoteProcess Safety Management, Process Safety Management of Highly Hazardous & Explosive Chemicals , and a natural-gas-fired power plant construction site OSHA is particularly concerned about the recent number of serious incidents at refineries that have scalded, burned or struck down your fellow workers , CIH [email protected] Georgia Tech Occupational ..\nThe Health and Safety risks and regulatory strategy ,, THE HEALTH AND SAFETY RISKS AND REGULATORY STRATEGY RELATED TO ENERGY DEVELOPMENTS , Tide and wave power Health and safety risks Regulatory strategy Standards , and safety, the established technology areas of major hazard sites, and occupational hygienePowerPoint Presentation, Occupational Safety and Health Administration , Occupational Safety & Health Administration 200 Constitution Avenue, NW Washington, DC 20210 eTools Home : Evacuation Plans and Procedures , Procedures to be followed by employees who remain to operate critical plant ,Potential Occupational Exposures and Health Risks ,, Jul 22, 2015· Biomass is increasingly being used for power generation; however, assessment of potential occupational health and safety (OH&S) concerns related to usage of biomass fuels in combustion-based generation remains limitedScaffold Safety in Coal, Coal-fired power plants present several occupational hazards to workers during routine work activiti Occupational hazards include extreme heat stress, toxic chemical exposure, falls, and tight .Water treatment plant operator, Water treatment plant operator , using hand tools and power tools Tests water samples to determine acidity, color, and impurities, using colorimeter, turbidimeter, and conductivity meter , ILO Encyclopaedia of Occupational Health and Safety, 3rd Ed, Geneva, 1983 4 ILO Encyclopaedia of Occupational Health and Safety, 4th Ed, Ch 55 ..\nEnvironmental, Health & Safety Policy, Environmental, Health & Safety Policy Contact Power Generation Tenaska is committed to operate its business in a manner protective of human health, the environment, and property, while complying with all applicable laws and regulationsAn Investigation of Health and Safety Measures in a ,, An Investigation of Health and Safety Measures in a Hydroelectric Power Plant Author links open overlay panel Amevi Acakpovi 1 Lucky Dzamikumah 2 , occupational health and safety in power plants has seen a significant improvement due to increased oversight and government regulations in safety In South Africa, an occupational Health and .Coal Power Plant Explosion Kills 26 in India ,, The death toll from an explosion at a coal-fired power plant in northern India on Wednesday has risen to 26, the government said Thursday, as 10 more people died of their injuri , Section 44 of the Occupational Safety and Health Act, 2007 requires that before any person occupies or uses , Read more Hazards All Plumbers Need To KnowTHERMAL POWER PLANT SAFETY |authorSTREAM, thermal power plant safety - authorstream presentation relief and safety valves for thermal power plantsA HIRARC model for safety and risk evaluation at a ,, A HIRARC model for safety and risk evaluation at a hydroelectric power , There are many formal techniques for the systematic analysis of occupational safety and health in general, and risk analysis in particular, for power generation plants at hydroelectric power stations , health at a hydroelectric power generation plant at Cameron ..\nPpt Power, Boiler Safety - Occupational Safety and Health Administration PPT Presentation Summary : Boiler Safety Developed by Western , danger tag to the source of power to indicate that the equipment may not , claims and become a permanent part of plant .Power plant safety: a wise business move, Power plant safety: a wise business move Going to work in a controlled and safe environment is not an unreasonable expectation for any worker In many occupations, a level of danger is expected because it is inherent to the job, but this is only more reason for companies to take employee safety seriouslyThis work is licensed under a Creative Commons Attribution ,, This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike License Your use of this , polyurethane foam insulation manufacturing plant in a suburban community in the state , Occupational Safety and Health Act, 1970CHAPTER 8 OCCUPATIONAL HEALTH AND SAFETY 81 ,, The Tata Power Company Ltd EIA Report for 1000 MW Coal Based Thermal Power Plant at Naraj Marthapur, Cuttack, Orissa SGS India Private Limited 166 CHAPTER 8 OCCUPATIONAL HEALTH AND SAFETY 81 Health and Safety Aspects Plant operation will involve storage handling and use of fuels and several chemicalsPowerpoint Presentations Occupational Hazards ,, Aug 15, 2017· OCCUPATIONAL HEALTH HAZARDS ppt img source : slideplayer , namely Powerpoint Presentations Occupational Hazards Some people attempting to find specifics of and of course one of these is you, is not it? , Power point environmental and occupational health.\nA, Tree Care Safety & Health Trenching & Excavation Tuberculosis (TB) Tularemia Tunneling and Hyperbaric Safety Underground Utilities Utility Installation Valet and Shuttle Service - Hotel-Motel Industry Safety & Health Variances from Safety & Health Rules Vehicle Removal, Transporting and Salvage Ventilation Vessels, charter Vest, SafetyEnvironment, Health and Safety, These are a part of many senior managers\' and plant managers\' annual performance objectiv Yearly manager performance evaluations, including considerations related to compensation, take into account , and the Occupational Health and Safety Assessment Series (OHSAS) 18001 to properly manage hazards ,01, Project Health & Safety Plan, Contingency Plan Environmental Abatement and Demolition Work at Old Main Steam Plant, Incinerator Building, and Surrounding Property 01-034-12-1074 Old Main Steam Plant 1180 Main St SE Minneapolis, Minnesota 55455 University of Minnesota Health and Safety ,Elec Hazard Awareness Study Guide, 1 Ralph H Lee, ""Electrical Safety In Industrial Plants"" ©1971 , 4 Occupational Safety & Health Administration Standards 29 CFR 1910331 -1910335 5 , 104 Explain the characteristics and hazards associated with power arcs and precautions that shouldOccupational Health and Safety in a Power Plant: A study ,, Occupational Health and Safety issues now a day have become an important esteem in context of Industrial Production This study is designed to measure the workplace hazards at Kohinoor Energy Limited Mainly focused hazards were Heat Stress, Noise Level, Light Level and Ergonomics.\nPowerPoint Presentation, The Occupational Safety and Health Administration, or OSHA, developed the Hazard Communication Standard to help protect employees from the dangers of hazardous chemicals , Health hazards are chemicals that are harmful to your health and cancause: Short-term (acute) health problems , HazCom starts at the chemical manufacturing plant .Power plant safety, Among the most common hazards to power plant workers are electrical shocks and burns, boiler fires and explosions, and contact with hazardous chemicals , workers in the Occupational Safety and .PPT, The Occupational Health Management in Coal Chemical Plant in CSC Chuan-Fang Li , Shih-Ping Cheng, Pao-Yin Lu, Pao-Hong Tong November 22, 2010 Outline Introduction Operation and production processes in coal chemical plant Occupational health management Slideshow 5620113 by kimberlySafety, At Adani Power Limited (APL), Occupational Health and Safety is given prime importance All our Power Generation plants have been certified with OHSAS 18001:2007 Management Systems Safety is an integral to all Adani Power operations and every Power station prioritizes awareness-building, in built safety design and use of technology to enhance .Workplace Housekeeping, Storage of materials should meet all requirements specified in the fire codes and the regulations of environmental and occupational health and safety agencies in your jurisdiction CLOSE ALL Document last updated on June 4, 2018.\nLeave Your Needs\nDear friend, please fill in your message if you like to be contacted. Please note that you do not need to have a mail programme to use this function. ( The option to mark \' * \' is required )\nCopyright © 2021.SBM All rights reserved. - Sitemap']"	['<urn:uuid:86e1a632-7a9f-4d6d-ae65-34aef8276be9>', '<urn:uuid:d1b78576-02f3-4807-bcb3-c3d1613c379c>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	11	52	4038
81	I'm studying tumor immunology and I'm curious about how T regulatory cells contribute to poor cancer prognosis - what is known about their role in different types of cancer?	T regulatory cells (Tregs) have been found to infiltrate and develop in the tumor environment, particularly in ovarian, renal, and colon cancers. An increased number of these cells is associated with poor prognosis. This is because Tregs create a suppressive tumor environment that prevents lymphocytes from attacking the tumor cells. The induced Tregs (iTregs) that develop in the tumor environment are particularly problematic as they have been shown to have more suppressive capabilities than natural Tregs (nTregs), making them more robust regulators of lymphocyte function.	['- Natural human CD4+CD25+FOXP3+ T regulatory cells isolated from PBMCs\n- Confirmed suppressive capacity\n- Used for a wide variety of in-vitro applications, including co-culture suppression and cytotoxicity assays\n- Carefully cryopreserved to ensure high viability (> 90%) upon thawing\n- All orders come with an iQ Certificate of Analysis\n- Normally ships out same business day\nHuman CD4+CD25+FOXP3+ Natural T Regulatory Cells\nHuman CD4+CD25+FOXP3+ T Regulatory Cells\nHuman T regulatory cells (Treg cells) are characterized as CD4+ lymphocytes cells that can down-regulate the activity of T cells, B cells, NK cells, and macrophages. Their suppressive function in lymphoid and non-lymphoid tissues help to establish and maintain tolerance to prevent autoimmunity and hyper-activation in inflammatory responses. Numerous studies have demonstrated the role of Treg cells in the development of autoimmune diseases and efforts are underway to target them with therapeutics to treat these diseases.\nRecent studies have also shown that Tregs can infiltrate and develop in the tumor environment. Ovarian, renal, and colon cancers are among the human cancers that have been linked to increased Tregs. Not surprisingly, poor prognosis is associated with an increase in the number of these cells. With these emerging findings, researchers are developing therapeutics that specifically target Tregs to eliminate them from the tumor environment so that remaining lymphocytes can become activated and target tumor cells.\nTreg cells come in various subtypes and can develop in the thymus or periphery. Those that develop in the thymus are called Natural Tregs (nTregs), while those that develop in the periphery and tumor are termed Induced Tregs (iTregs). Both nTregs and iTregs express the surface proteins CD4 and CD25, as well as the transcription factor FOXP3. An additional criteria to identify both types of Tregs is the low expression of CD127, the IL-7 receptor.\nWhile both nTregs and iTregs are identified using similar markers, they are thought to have different roles in health and disease. The nTregs are believed to maintain peripheral tolerance to prevent autoimmunity and immune system hyper-responsiveness. In contrast, iTregs are thought to develop from conventional T cells in a suppressive tumor environment. In addition to an already suppressive cytokine environment, the presence of iTregs makes the tumor environment even more potent in preventing lymphocytes from attacking the tumor. More so, it has been shown that iTregs have more suppressive capabilities than nTregs, making them more robust regulators of lymphocyte function.\nTregs can suppress lymphocyte function through multiple mechanisms. They can secrete immuno-suppressive factors, such as IL-10 and TGF-b, to prevent the activation or down-regulate the activity of lymphocytes. They may also work by being an “IL-2 sink” in which the high expression of the IL-2 receptor on Tregs bind the local concentrated amount of IL-2 to prevent it from binding lymphocytes to drive an inflammatory response. In addition, Tregs may express checkpoint proteins, such as CTLA-4 and PD-L1, to act through cell-cell contact. Finally, some data suggest that iTregs may utilize multiple mechanisms to suppress lymphocyte function, which is consistent with the observation they are more potent than nTregs.\nApplication Summary for Natural T Regulatory (nTreg) Cells\nNatural Tregs are a good source of cells to study the function of Tregs in an in vitro setting, in particular co-culture suppression assays. Here, Tregs are co-cultured with lymphocytes and then assayed for lymphocyte proliferation and cytokine release. In the presence of Tregs, lymphocyte function should be down-regulated.\nWith the current trend towards development of molecules that modulate the immune system to treat cancer and autoimmune diseases, these co-culture assays are also commonly used to test whether a potential therapeutic can inhibit or promote Treg function. Further, because Tregs are difficult to isolate from tissue and a great number of them are required for co-culture assays, frozen nTregs are a valuable source to enable the execution of these studies.\nFor some antibody-based cancer therapeutics being developed, cancer-associated iTregs are being targeted for antibody-dependent cellular cytotoxicity or complement dependent cytotoxicity. However, targeting nTregs should not be avoided in order to maintain tolerance. Therefore, frozen nTregs are also a good source of target cells for these studies interrogating cell death or the avoidance thereof.\nMany questions about how nTregs develop and progress through developmental stages also remain unanswered, and frozen nTregs are a good source of cells to study this development through genetic and transcriptional profiling. DNA or RNA can easily be isolated from these cells and used for downstream applications and analysis.\nIsolation of nTregs\nnTregs are enriched from PBMCs of human donors using a negative selection kit and magnetic isolation. To ensure that nTregs were isolated, cells are analyzed for the expression of CD4, CD25, FOXP3, and CD127. Our frozen nTregs are a reliable and ideal source to obtain plentiful cells for your studies.\nCryopreservation and Storage\nOur nTregs were cryopreserved carefully using iQ Biosciences’ cryopreservation protocol that ensures high viability (> 90%) after thawing.\nCells should be stored at < -120°C once they are received, such as within a liquid nitrogen tank (vapor phase).\nQuality Control Process\nWe implement random sampling per lot to test for viability and cell counts to ensure they meet specifications, which are recorded on the Certificate of Analysis that is included with each shipment. Cell counts are obtained using a manual hemocytometer and then cross-referenced with an automated cell counter. Each lot is also characterized for unique cell populations by immunophenotyping, in which the results are also recorded on the Certificate of Analysis.\nWe are Committed to Ethical Practices\niQ Biosciences’ human primary cell products are lawfully obtained in accordance with Local, State, and Federal U.S. requirements, and the collection of cells complies with ethical requirements. Our cells are obtained from normal or disease patient volunteers participating in a donor program that is approved by an Institutional Review Board (IRB) or Human Subject Committee. A signed and witnessed consent form is obtained from donor volunteers prior to starting the collection protocol. Strict controls on personal identifiers of volunteers are in place in order to protect their privacy.\nFor US customers, we ship via FedEx Overnight Shipping. Shipping charges will vary per shipping address (based on ZIP code) and are estimated to be $110.\nFor international (non-US) customers, we work closely with you and our couriers to ensure all necessary documentation is in place for international shipments to significantly reduce the chance of delays at Customs. For the export of non-human primate samples, this includes preparing CITES permits, as well as any other documentation as required by country. Please submit an inquiry to firstname.lastname@example.org for your estimated time of delivery and shipping charges.\nFisher Scientific (United States)\nQuartzy (United States)\nVWR International (United States)']	['<urn:uuid:1e4dab31-4c71-40f9-9676-ecac99fc9086>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T04:31:27.499155	29	85	1103
82	How can pregnant women effectively manage common pregnancy-related eating difficulties while ensuring adequate nutrition for both mother and baby's development?	Pregnant women can manage eating difficulties while maintaining proper nutrition through several strategies. For nausea and vomiting, which are common in the first trimester, soft cooking methods without heavy seasonings are recommended - such as boiled, steamed foods and soups, while avoiding fried or grilled foods with intense flavors. Despite these challenges, it's important to note that even with strong nausea, providing nutrients to the baby is essential. Choosing nutrient-dense foods like cottage cheese and fruits over processed foods is recommended. It's a misconception that pregnant women need to 'eat for two' as the body's absorption increases during pregnancy. Instead, focus should be on nutritious foods. Regular weight monitoring and keeping a food diary can help track nutrition adequacy. If severe vomiting occurs (more than 4 times daily), medical attention is necessary as it can affect water-salt metabolism and nutrient delivery to the fetus.	"[""Is it Ok to Lose Weight in Pregnancy?\nIt is believed that the expectant mother should gain weight throughout the pregnancy. However, there is a normal amount of weight that the woman should gain at each term. In total, the woman should normally put on 10-15 kg, although this figure may vary. For example, the woman can lose some weight during the pregnancy.\nWhat is the reason for the weight loss during the pregnancy? What is normal and what is not? How it may affect the health of mother and baby? Let’s try to understand the causes and evaluate the results to decide whether it is necessary to raise alarm.\nThe First Half of Pregnancy\nIf the pregnancy is healthy, the woman comes to see the doctor once a month until the 28th week and twice a month after this term. The baby's health is worth suffering inconvenience and visiting the doctor regularly. In addition to the necessary tests and other studies, the doctor weighs the expectant mother, and recommends monitoring the weight and homing too.\nYou should weigh yourself in the morning, in the same clothes — to make it possible to compare the figures. The first 8-9 weeks of pregnancy, the woman does not put on weight, and it is easy to understand why. Toxemia, vomiting, sometimes diarrhea, loss of appetite, increased salivation are the usual symptoms of the first trimester of pregnancy. During the first three months, the woman can gain only one or two kilograms, or even lose weight.\nIn case of strong toxemia and vomiting more than 4 times a day, the water-salt metabolism is disturbed, which affects the health of both mother and baby, as the fetus can’t receive nutrients from the dehydrated mother’s body. In this case, it’s necessary to see the doctor or even to stay at hospital. In hospital, the woman will receive proper treatment.\nHowever, you shouldn’t worry: a small weigh loss during the pregnancy is admissible. The pregnant woman shouldn't be nervous, especially because her anxiety may provoke the weight loss. You need to weigh yourself regularly, at least once a week. Put the figures down in a diary. It is also recommended to track what you have eaten to analyze the results and draw conclusions. Even in case of strong nausea and appetite loss, it’s necessary to provide the baby with nutrients. You’d rather have some cottage cheese and fruit than a piece of pie, as well as a small slice of boiled meat is better than three sausages. The quality of the food is more important than its quantity for both mother and baby.\nThe Second Half of Pregnancy\nIn the second half of pregnancy, the woman begins to put on weight. It is important to gain weight gradually. If woman gains weight, it means that the baby is developing normally. However, the weight loss is not always alarming. The baby can develop normally at the expense of resources accumulated in the mother’s body.\nIf the weight loss is significant, you should inform your doctor. He will assess your state of health, track your day regimen and diet, and give necessary recommendations. Routine blood and urine tests, ultrasound, CTG, Doppler scan and other ordinary studies will show whether the child is developing normally and the placenta is functioning correctly. That is why the expectant mother should see her doctor regularly.\nThe rapid weight loss releases ketones (they increase acetone levels in the blood, which affects the baby’s brain development). Ordinary tests can show acetone levels. That is why it is so important to inform your doctor about your weight fluctuations. A disease or a stress can cause weight loss. In this case, the elimination of the cause will solve the problem. If the doctor says that the tests’ results are normal, it means that you have nothing to worry about.\nEven if the pregnant woman eats a lot, she can lose weight. Malnutrition during the pregnancy is one of the causes of the weight loss. The pregnant’s weight depends not only on how much the woman eats, but also on the quality of the food. The pregnant should eat enough food rich in vitamins and microelements.\nFirstly, a small weight loss should not worry the pregnant woman. You should to weigh yourself and keep a diary to track weight increase or reduction, and your diet. If a problem arises, these records will help you to determine its cause.\nSecondly, pay regular visits to the doctor and take tests. Don’t hesitate to ask the doctor questions. The task of the doctor is to do everything to preserve the health of both mother and baby. Don't be afraid of hospitalization: this unpleasant period will go away quickly, and your baby will be healthy.\nThirdly, take care of the diet: the food should be balanced and rich in vitamins and microelements. Food quality is more important than its quantity."", 'Safe and unsafe food for pregnancy\nSuitable diet for pregnancy\nDIET FOR PREGNANCY\nAdequate food for pregnancy\nHow to eat so that food can be healthy for both, you and your baby? In pregnancy, eating is an act of sharing in the broadest sense of the word.\nDiet during pregnancy should allow the good health of the mother and the growth and health of the baby.\nIt is also important to say that most teas and medicinal plants are contraindicated in pregnancy, so many home remedies are inadequate. In these cases it is always recommended to consult a doctor.\nTips for changes in pregnancy\nAmong the various changes that can occur during pregnancy, we have some disorders such as:\n- Nausea and vomiting: These may hinder proper nutrition. To avoid these problems soft cooking without too many seasonings are recommended: boiled, steamed, soups,... Avoid fried or grilled foods, and intense flavors.\n- Caries in pregnancy: Produced by hormonal changes or vomiting. Pregnant women should maintain strict dental hygiene to avoid these problems. A protective food: cheese helps prevent cavities.\n- Constipation is normal: the body decreases bowel movements for greater absorption of nutrients.\n- Eating for Two: Because absorption increases it is not necessary for the pregnant to ""eat for two"", but she needs to eat very nutritious foods.\n- Pregnancy cravings: It is a false belief that pregnant woman have a craving for what they need. They should not usually eat much processed trans fat or refined sugar products, which are nothing beneficial (pastries, cookies, snacks, fried industrial, etc.).\n- Diabetes in pregnancy: It will be important to control blood glucose and the elimination of refined industrial products rich in sugar or high-fat (jams, pastries, cookies, chocolate,...). Instead we recommend: avocado, low sugar fruits such as citrus, sandwiches, toasts, oats porridge,...\n* Natural cosmetics in pregnancy\nHow much weight should be gained during pregnancy?\nDuring pregnancy weight should be increased from 9 to 12kg. Normally this weight gain occurs as follows:\n- During 1st trimester 0.5 to 1 kg\n- During 2nd trimester from 3.5 to 4 kg\n- 3rd trimester 5 to 6 kg\n- Weight loss in pregnancy: Do not perform strict diets during pregnancy. a healthy diet that provides enough calories for mother and baby is recommended. (Consult a dietitian)\nRECOMMENDED AND NOT RECOMMENDED FOOD FOR PREGNANCY\nFoods rich in omega-3 during pregnancy\n- Omega 3: This is the most recommended type of fat because it improves cardiovascular health (blood volume and heart rate increases during pregnancy) and prevents postpartum depression (especially DHA fats).\n- Walnuts: They are the foods rich in omega 3 and omega 6. With 7 walnuts a day is enough to meet these needs.\n- Blue Fish: It\'s a food to be considered as it is the only one that provides lots of EPA and DHA fatty acids, important for the development of the fetal nervous system. A serving of fish provides more EPA and DHA than any nutritional supplement.\nThe small blue fish (sardines, mackerel,...) is recommended. Not to cook at too high temperatures, which would impair their omega 3. Vegetarian women can get omega 3 from walnuts and ground chia seeds (2 teaspoons daily).\n- Avocado: It provides a lot of healthy fats. It is also a very valuable food for pregnant women because of its enormous folic acid content. You can eat homemade guacamole sauce, add avocado to salads, sandwiches, etc.\nDiet rich in folic acid during pregnancy\n- Foods rich in folic acid: Folic acid is one of the most important vitamins for both the conception and development of the baby.\nInvolved in the multiplication of cells, it is essential for the correct course of pregnancy. Folic acid deficiency can cause spina bifida in the unborn baby.\nFolic acid needs are stablished 400 mcg. in a balanced diet, 600 mcg. during pregnancy and 500 mg. during breastfeeding.\nThis vitamin is also important when the child grows, between 3 and 12 years, and during youth, reason why it is recommended that children eat vegetables.\nFoods rich in folic acid are:\n* Related Information: Recipes rich in folic acid\nSheet summary of the recommended diet during pregnancy.\nProtein-rich foods during pregnancy\n- Foods rich in protein: Protein needs to be increased progressively during pregnancy. Approximately one should take between 0.8 to 1 g. of protein per kilo of weight. For example, a 70 Kg. Should take between 56g. and 70 grams of protein per day (70 Kg. x 0.8 g. / kg. = 56 g. of protein).\nIn the third trimester of pregnancy protein needs increase +20 g. of protein a day, which can be translated into an extra serving of meat, fish or 2 eggs. Similarly, throughout pregnancy, protein foods should be consumed, and spread during the day, in the following way:\n- 1 plate of vegetables (= 20 grams of protein)\n- 2 eggs (= 15 g protein.)\n- A handful of almonds, 12 units (= 5 g of protein.)\n- If you take milk:\n2 yoghurts (= 10g protein.): Choose low-fat if overweight.\nA glass of milk (= 7 g. Of protein)\n40 g. cured cheese (= 15 g. of protein)\n75 g. fresh cheese (= 10 g. protein)\n- In non-vegetarian diets:\n150 g. fish or white meat (= 20-25 g. protein)\n* Related Information: How to calculate protein intake\nOther recommended foods for pregnancy\nOther plant foods that provide beta-carotene, vitamin C and components that improve health are:\n- Carrots: They provide a lot of beta-carotene, which the body converts into vitamin A. This vitamin helps in the formation of the fetus and improves skin condition, prevents stretch marks and improves overall health of the body. It is very interesting contribution of vitamin A as beta carotene during pregnancy. (Eat at least 3 carrots daily)\nA preparation is very suitable grated carrot with squeezed lemon and virgin olive oil.\nBeta carotene resist cooking and are also found in the cream of carrot and pumpkin cream.\n- Leafy greens: For its high content in folic acid, they are essential: spinach, asparagus, artichokes, beets, chard, arugula, okra, chayote, Brussels sprouts, etc.\n- Citrus: They are rich in vitamin C, folic acid and calcium (oranges, tangerines, etc.).\n- Nuts: They are a source of minerals and calcium.\n- Ginger: Ginger Infusions are suitable to treat nausea and vomiting if there is no contraindication. (Consult your doctor)\nPregnant women can meet their nutritional requirements and what foods are more desirable referring to page:\n- Menu for pregnancy\n* Other items of interest:\nMore information on pregnancy care in the listing above.\nOther interesting articles\nThis material is for informational purposes only. In case of doubt, consult the doctor.']"	['<urn:uuid:281ce6dc-e154-46a7-bb90-5ca5e3d1243c>', '<urn:uuid:626e95b1-0e8c-4bde-b3ca-f4d0204a9651>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T04:31:27.499155	20	144	1938
83	What happens if detergent leaves spotty dishes?	If dishwasher detergent leaves a film, you can add a small amount of white vinegar to the mix to solve the problem. For homemade dishwasher detergent, the basic recipe combines equal parts borax, washing soda and citric acid, using 1 or 2 tablespoons per load. The citric acid component specifically helps prevent streaking and spotting in the dishwasher.	"[""Substitutes for High-Efficiency Detergents\nModern Energy Star appliances run 25 to 50 percent more efficiently than older models, according to the Natural Resources Defense Council. New washers and dishwashers use less water at lower temperatures, so they need a detergent that can clean effectively in cool water without producing too many suds. Suds can prevent efficient cleaning, and the lower volume of water can leave residue behind. That's why newer appliances call for high-efficiency detergent specifically formulated to meet these needs.\nThe usual high-efficiency detergent may contain phosphates --- a chemical that drains out of your washer or dishwasher after the rinse cycle and can make its way into groundwater and waterways. It fuels algae blooms that cut off the water's oxygen supply, killing fish. Eco-aware companies are beginning to market phosphate-free high-efficiency detergents, and some go so far as to use only plant-based ingredients. Check the cleaning supplies aisle at your local store, or buy a sustainable detergent on the Internet. Never substitute a traditional detergent, no matter how eco-friendly --- it may not clean properly, causing you to run the cycle twice and waste water and energy. It may even overflow your machine.\nBy using homemade detergents, you avoid the chemicals and packaging waste associated with commercial products. The most basic powdered laundry detergent contains equal parts borax, washing soda and grated bar soap, and you only need 1 to 3 tbsp. per load. Homemade dishwasher detergent mixes equal parts borax, washing soda and citric acid, and requires 1 or 2 tbsp. per load. The similarity in the recipes reflects the power of borax and washing soda as non-toxic cleaning agents. Grated soap adds extra cleaning power to laundry detergent, and citric acid prevents streaking and spotting in the dishwasher. Both low-suds formulas work effectively.\nThe powdered laundry detergent recipe can be used to make a liquid as well, except the proportions change to one part soap, and two parts borax and washing soda. For liquid detergents, melt the soap first, then mix it with the remaining ingredients, dilute it with water and allow it to sit overnight. This gives the ingredients time to interact and form a gel consistency. Some people use a tablespoon of liquid castile soap in place of the grated soap --- this can be applied to the dish detergent recipe as well, but be careful not to use too much. Castile soap can be very sudsy and must be diluted heavily before use in a high-efficiency machine.\nCustomizing Your Blend\nThe basic recipes work, but you can also customize the formulas to create your own blends. Use a scented castile soap in your laundry detergent, or add a few drops of essential oil for fragrance. Don't use fragrance in your dishwasher --- it may leave a nontoxic but foul-tasting residue. If the dish detergent leaves a film, add a small amount of white vinegar to the mix. If your laundry comes out dingy, add more borax. If clothes don't get clean the first time through, increase the amount of washing soda.\nAngela Brady has been writing since 1997. Currently transitioning to a research career in oncolytic virology, she has won awards for her work related to genomics, proteomics, and biotechnology. She is also an authority on sustainable design, having studied, practiced and written extensively on the subject.""]"	['<urn:uuid:5002654d-0082-4c2d-aa1d-5454d3df6276>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	7	58	554
84	How can you tell if violin strings need replacing?	Upper strings need replacing when there are signs of wear like stretched-out or missing winding, deformation of the winding on the open-wound D string, or when the tone sounds impure. For understrings (sympathetic strings), they should be replaced when there is evidence of rust or damage.	"['When and How Do I Change the Strings on My Hardingfele?\nby Loretta Kelley\nHere are some techniques and tips that I have collected over the years on when and how to change hardingfele strings, which I hope may be helpful to others.\nI change my upper strings when:\n1) Any sign of wear appears. This would include stretched-out winding or missing winding on the G, A (if metal) and E strings (and D string if close-wound). To check for missing winding on the portion of the string that is not touched by the bow, run your finger slowly up and down. Fingertips are sensitive enough to detect extremely small variations. To check for missing winding on the bowed portion of the string, first clean the string well with a Q-tip swabbed in rubbing alcohol (Warning: cover the top of the fiddle with plastic wrap to prevent the alcohol from accidentally dripping on the varnish), then view with a magnifying glass. Avoid touching this part of the string; skin oil prevents the bow from gripping properly.\n2) Any deformation of the winding on the open-wound D string is noticed, particularly over the bridge or the nut. The winding of an open-wound D string is particularly sensitive to getting caught by the bridge or nut as the string is tightened or loosened, causing the winding to bunch up. To prevent the winding from being caught, make sure that the groove where the D string passes over the bridge or nut is well-lubricated with pencil lead (graphite) and that there are no sharp edges particularly at the nut that might catch the winding.\n3) The tone on an upper string sounds impure or ""false"". This is much harder to determine on a hardingfele than on a violin, because the sound of one string is so sensitive to the tuning of all the other strings, much more than on a violin. To check for falseness, I recommend muting the understrings first by wrapping them with cloth, or if you are skillful you can hold your finger on them while bowing the upper strings. Tune the upper strings as exactly as you can using your ear, not an electronic tuning device. Then take a long firm but not heavy bowstroke on one string alone, lifting the bow at the end of the stroke to allow the string to ring. Listen to the ringing sound after the bow is removed, and check for any change in pitch. If the sound after the bow is removed starts off in tune but then changes pitch as it dies out, then this means that the string is no longer perfectly round, or has another physical defect too small to see. Note that the understrings can still affect the sound even when muted, so try this procedure several times using different techniques over a period of days to see whether you get consistent results.\nI change my understrings (sympathetic strings) when I see evidence of rust or damage.\nIf a fiddle is well cared for this should be very infrequently. Clean the understrings using an alcohol swab as described above, especially where the finger touches them to check the tuning. Then examine carefully for any black or brown spots, imperfections, kinks, etc.\nThis is how I change my understrings:\n1) Remove all of the understrings.\n2) Starting from the right to the left as you face the instrument (high to low), thread the top understring under the fingerboard and fasten it, and tune it up just enough to hold it in place.\n3) Thread the next highest understring through the fingerboard slot, making sure to keep the natural curve of the string facing away from the string that is already in place, and fasten it, and tune it up just enough to hold it in place.\n4) After attaching each understring check to make sure that it has not looped itself around the neighboring understring by holding the fiddle up to a light and sighting down the tunnel underneath the fingerboard.\n5) Proceed with the remaining strings. After all the understrings are attached then tighten them all up to pitch gradually.\nAbout winding the strings on the pegs: Insert the end of the string in the hole in the peg, then pull the end of the string out about 1/2 inch. Wind the string a turn or two in one direction, then wind it the other direction so that it overlaps the first windings for more security. Continue so that the windings lay tight and even, not bunched up. The windings should stay as close as possible to the center of the peg. If the windings get too close to the sides of the pegbox, they will interfere with the natural in and out of the peg as wood shrinks and expands with temperature and humidity.\nAbout checking the bridge after changing strings: Tuning up a new string, especially the D string, has a tendency to pull the bridge over towards the top of the fiddle (the scroll). Leaving the bridge in this condition can cause it to become permanently warped. To correct, check the bridge frequently as you tune up the new string. If it starts to pull over, place the fiddle securely in your lap and, bracing the bridge on both sides with thumb and forefinger, very slowly and gently pull it back into place, making sure to place equal pressure in the opposite direction so that it doesn\'t go over too far. Loosen the strings a little if it is too difficult to pull it back.\n1. See also David Golber’s article, ""Changing Strings"", Sound Post Vol. 29, no. 1 (to order back issues of our journal click on Merchandise Sales).\n2. If you want to be absolutely perfectly in tune when playing by yourself or only with other fiddles, then you cannot use an electronic tuning device to check the intonation of individual violin strings, because electronic tuners use notes in the equal temperament scale, which is by definition slightly out of tune. Bowed instruments, when not playing together with equal temperament instruments such as the piano or accordion, use the just intonation scale. You can use an electronic tuner to set the pitch of the A string, but then the other three strings and the understrings must be tuned by ear.']"	['<urn:uuid:0d1e2635-c620-46a6-af2f-4bb04200d5bc>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	9	46	1057
85	How do model kit manufacturers handle detail differences in parts?	Model kit manufacturers handle detail differences in various ways. For original parts that get modified or lose detail in reissues, some enthusiasts create and sell resin replacement parts to restore authenticity, as seen with the Robin model. Manufacturers also make intentional detail improvements - for example, Trumpeter's C-47A kit includes exquisite engine details with full plumbing, white metal landing gear options, and photo-etched parts, while maintaining elements like accurate rivet details. The level of detail can vary significantly between manufacturers, with some focusing on basic representations while others pursue maximum authenticity.	"['(left) reissue of Robin model by Revell, and (right) original 1966 Aurora issue.\nWhen Aurora folded for good, much of their tooling was sold to Monogram. A typical set of molds to create an injection styrene kit represents an investment of many thousands of dollars and a lot of time, so acquiring an existing set of molds can be a wise business decision. When Monogram received the lot, they sorted through and sold as scrap metal the ones that were worn out, damaged, or just plain didn\'t interest them to produce. There is a widely held belief that a train wreck while the molds were being transported destroyed a lot of these, and the wreck did occur, but most of the now missing molds were probably the result of the sorting process in the warehouse.\nAnother thing Monogram did was eventually retool some of the molds, for whatever reason. This means favorite old models like Superman, Batman, and Robin are not identical to the original. Since I happened to have an original Robin unbuilt in box, I decided to compare this one with the recent reissue by Revell. Below you see every part that is in any way different from the original. The ones on the left are the Revell, on the right is the Aurora.\nThe first thing I noticed was the original parts were molded in an off-white or ivory color, the reissue was bright white. This may be due to the age of the older plastic, though. Since my Aurora kit was sealed inside the box until I opened it (yes, I removed the cellophane, I was getting ready to build it, and was surprised by the announcement of the reissue), then it\'s likely all these kits are this color. That\'s one way of telling the old from new parts.\nHere\'s a complete rundown of the differences between the parts. Part of the retooling was a change in some part numbers, so I\'ve referenced the two.\nAurora# Revell# Name of part Difference\nhead parts face\nand hair redone (see pic below)\n4 4 front body half missing R on Revell part\n25 22 equipment panel missing ON/OFF and dial detail\n19 32 lab wall panel missing START/STOP (see pic below)\n24 25 chemical unit missing grill and dial detail (see pic below)\n20 31 railing slot missing on Revell\n18 33 base tabs added for opening to chemical testing unit\nA closer look at some examples:\nI actually like the new head (left) better, with the side part and redone face. All the heads for the reissues were redone, for some reason. The new Superman head is terrible, it\'s huge, way out of scale compared to the old one.\nAn obvious missing detail in the Revell lab wall on the left, I used a label maker to put in the lettering on my reissue.\nThe reissue just doesn\'t have a lot of the small detail, as in the chemical testing unit panel shown here. The Revell part on the left gives you smooth bumps where dials are supposed to go, and eliminated an inset screen.\nWhat to do about the missing detail and other changes? Well, if you ask around on the Net there are people who actually, believe it or not, care enough to cast replacement parts in resin and will sell you a set of the most obvious ones.\nBut to the casual observer, it\'s still the old Robin model we baby boomers built way back in the days of ""Same Bat time, same Bat channel."" Just don\'t look too close.\nClick HERE to return to the Robin model\nClick HERE to go back to model index.\nClick HERE to return to home page.', 'Trumpeter 1/48 C-47A Skytrain Kit First Look\n|Date of Review||July 2008||Manufacturer||Trumpeter|\n|Kit Number||2828||Primary Media||Styrene|\n|Pros||The ultimate C-47 kit||Cons|\n|Skill Level||Basic||MSRP (USD)||$149.95|\nThe Douglas C-47 Skytrain was a military adaptation of an available commercial airliner in production during the outbreak of World War II. This commercial aircraft, the DC-3, was first developed in the mid-1930s as a result of the success of their DC-2 airliner, but adding additional features sought by the airlines. The DC-3 was a tremendous success and transformed civil aviation in those days leading up to the war.\nThe DC-3 was transformed into the most important allied airlift asset in World War II by simply removing the commercial interior, fitting a reconfigurable cargo/passenger compartment, adding large cargo and paratroop doors to the port side, among other changes. The initial C-47 was the first adaption of the DC-3 with these changes and nearly 1000 were produced. The C-47A added a 24 volt electrical system and over 5200 of these versions were produced. The C-47B changed the engines to supercharged R-1830 engines and more fuel for flights over the China/Burma/India \'Hump\'. The Navy designated their C-47s as R4D. The Army Air Corps did not exclude the available DC-3s from military service, but these retained most of their passenger service fittings and were pressed into service as the C-48. In RAF service, the C-47 became the \'Dakota\'. In operational service however, the aircraft drew the nickname of the large seabirds of the Pacific - the Gooney Bird.\nTrumpeter has released their 1/48 C-47A kit! This subject came as a bit of a surprise when images of the test shots started appearing on the internet a number of months ago. Until this kit came along however, the best kit of the C-47 was the Monogram (now Revell) 1/48 scale kit, which has recently been re-released with new parts to render the AC-47 gunship. With a suggested retail price of $31.50 USD, the Revell kit is roughly 1/5 the price of this new Trumpeter kit. So of course that begs the question ""Is it worth the difference in price?"" Let\'s take a look:\nThe kit is molded in light gray styrene and presented on 10 parts trees, plus two trees of clear parts, one fret of photo-etched details, one set of rubber tires, and one set of white metal main landing gear struts. The detailing is all finely scribed including the now-standard Trumpeter recessed rivet details. Fortunately, the rivet detailing is not excessive and if there was ever an aircraft with rivet details, the \'Goon\' was it.\nThe detailing inside the airframe is really impressive, from a well-appointed flight deck, radio operator/navigator\'s compartment, and extending into the main troop/cargo compartment. The cockpit is the nicest I\'ve seen for this subject and Trumpeter has definitely captured the look and detail. How much of that detail you\'ll see through the windscreen is another matter.\nThe compartment between the cockpit and the troop compartment houses the radio racks, the navigator/radio operator, and galley. Trumpeter has really done this area up nicely, but how much of that beautiful detail will be visible after assembly is the question. If someone were to build this as a cut-away project, you\'d really have something to show off here.\nThe troop compartment has separate \'bucket seating\' from the cargo floor. While the instructions are set for a D-Day paratroop airlifter, you have the flexibility to render a cargo aircraft as well. This is the first C-47 model to render the overhead lighting in the cargo compartment, a detail that is easily seen after assembly.\nLike most Trumpeter kits, the engines are nicely detailed and will look great with or without the cowlings installed.\nThe wings are naturally a simple portion of the assembly process, but these are attached to the fuselage using dual main spars for strength and alignment of the wing/fuselage joint.\nThe flight control surfaces are all molded separately. The ailerons, elevators, rudder, and flaps are all separate and positionable. On an older tail-dragger, the dropped flaps and drooping elevators are visible details overlooked by modelers.\nOne interesting \'glitch\' in the kit are the ailerons. The kit provides two sets of styrene \'hinges\', one for a neutral ailerons, and one for dropped ailerons. In other words, Trumpeter seems to have extended the landing flaps out to the ailerons to create full-span flaps, and while this is an innovative feature, the concept of a STOL Goon didn\'t exist back then. Don\'t use the drooped hinges in this kit!\nAs I mentioned earlier, the engines are a major plus in this kit with all of the plumbing for the intake manifold, exhaust collector ring, and plumbing that even extends into the main wheel wells. If you pose your C-47 undergoing maintenance with the cowlings removed, you won\'t need any aftermarket stuff to make this work.\nThe main landing gear struts are provided as white metal or styrene, your choice. Either way, the main wheel hubs install on the rubber tires and an axle through the hubs provide the attachments to the main struts.\nMarkings are included for two examples:\n- C-47A, 42-100521, 92 TCS/439 TCG, June 1944\n- C-47A, 42-92189, 61 TCS, Operation Market Garden, September 1944\nOkay, we\'ve looked over the kit, so is it worth five times the Monogram/Revell kit\'s price? Let\'s contrast the kits:\n- Surface Detailing: Trumpeter - scribed; Monogram - raised\n- Interior: Trumpeter - exceptional; Monogram - less details, but still very usable\n- Windows: Trumpeter - individual; Monogram - slab strips that detract from the interior details\n- Engines: Trumpeter - exquisite; Monogram - minimalistic\n- Flight controls: Trumpeter - separate; Monogram - not\n- Photo-etch details: Trumpeter - yes; Monogram - no\n- Main wheels: Trumpeter - rubber; Monogram - styrene\n- Main gear struts: Trumpeter - metal or plastic; Monogram - plastic\n- Rear in-flight \'bathroom\' (honeybucket compartment): Trumpeter - no; Monogram - yes\n- Overall fit: Trumpeter - good; Monogram - not so good\nOn this last point, we\'ve noted that the fuselage halves in the Monogram kit have some fit issues and I don\'t know if this is a function of the age of the model or \'shrinkage\' of the styrene over time. With some work, these can be overcome, but at the expense of the raised surface detailing that will need to be repaired.\nSo if you\'re just wanting to build a quick (and large) C-47, perhaps the Monogram kit is right for you. But if you are wanting a detailed build featuring contemporary detailing and lots of options without hunting for aftermarket parts, the Trumpeter kit is clearly the winner.\nOnly you can decide which kit is a better choice for your project. Both have their pros and cons, but if you\'re a fan of this aircraft and want to have a stunning model when you\'re finished, the Trumpeter kit is your best choice to get you there with the least effort.\nMy sincere thanks to Stevens International for this review sample!']"	['<urn:uuid:68adbc1e-2c2d-4278-8df1-24bfd034b7af>', '<urn:uuid:663a570d-09ee-4162-9e26-d3b703716025>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	expert	2025-05-13T04:31:27.499155	10	91	1783
86	new antibody drug screening using ai machine learning vs traditional target screening which faster	AI/machine learning approaches are significantly faster than traditional antibody drug target screening. While traditional screening takes 10-20 years studying single genes and proteins one at a time, new AI technologies like LabGenius's Eva platform can rapidly analyze billions of antibody sequences simultaneously and make predictions with heightened accuracy. AI combines high-speed wet lab work with machine learning to improve molecules through experimental cycles, dramatically reducing the historical drug discovery timeline.	['What’s hot in synthetic biology drug discovery?\nFor years, synthetic biology aided our efforts to treat a myriad of aliments and illnesses. Now, with significant scientific advancements, the potential for new drug discovery has never looked more promising. When searching for the latest synthetic biology revelations, the number of available stories seem endless—in the best way possible. To help, we’ve rounded up a few of the many noteworthy topics shaking up synthetic biology drug discovery conversations here at IDT.\nArtificial intelligence and machine learning combine to accelerate antibody discovery\nArtificial intelligence (AI) technologies now provide an antibody design platform that combines a high-speed wet lab with machine learning (ML) to create improved molecules through learnings from each experimental cycle. New AI/ML models help determine how mutations affect an antibody’s molecular properties to better inform scientists on high-quality options that might have otherwise remained hidden. This new technology shows promise for significantly accelerating new antibody discovery. For more information, check out our antibody discovery tools.\nGeorge Church and Nabla Bio receive funding to expand AI-designed antibodies\nThe father of synthetic biology is at it again, working with his team at Nabla Bio to create new antibody drugs that have been specifically engineered to address challenging clinical targets and to become commercially viable. Their process combines AI technology and wet-lab experimentation capable of analyzing billions of antibody sequences to build a “biophysical fingerprint” for one million new antibodies at a time, rapidly predicting the most effective options. This new approach has the scientific community on the edge of their seats with hopes to ramp up and improve the quality of antibodies moving from lab to clinic.\nSARS-CoV-2 pandemic enables researchers to pinpoint effectiveness of semisynthetic naïve antibody libraries\nEarlier this year, Nature Communications published a study on the ways the SARS-CoV-2 pandemic created a surge in comparative research that highlighted the benefits of semisynthetic naïve antibody libraries. Historically, it was believed the most potent neutralizing antibodies had to be generated from convalescent patients and immunized animals. In this study, the team demonstrated how appropriately designed and constructed naïve antibody libraries can effectively provide therapeutic antibodies against viral pathogens, potentially saving critical time and resources during future viral outbreaks.\nLabGenius machine learning technology is revolutionizing drug discovery\nThe new platform technology, Eva™, from our friends over at LabGenius, rapidly pinpoints novel therapeutic antibodies for devastating illnesses like cancer and inflammatory diseases. This is all possible through machine learning models that can describe sequence-to-function relationships to accuracy evaluate protein fitness and improve multiple drug properties simultaneously. These ML approaches are trained to rapidly extrapolate ever-growing data libraries to make critical predications with heighted multi-parametric optimization and higher dimension perception that will dramatically reduce the historical drug discovery timeline and do so with greater accuracy.\nLow-cost Mycobacterium tuberculosis (Mtb) drugs derived from engineered E. coli\nHistorical screening for Mtb has been complicated by the pathogen’s slow growth period and biocontainment requirements. With the expansion of chemical genetics within synthetic biology, there is growing interest around new tools that will help to break through remaining challenges on the path toward new antimicrobial drug discovery.\nRecently, a study sought to apply a synthetic biology framework for assaying Mtb drug targets in engineered E. coli. To do this, the research team built Target Essential Surrogate E. coli (TESEC) strains within which an essential E. coli enzyme was deleted and replaced with an equivalent target enzyme. They then screened 1,280 approved drugs and found that benazepril was an effective, expression-dependent inhibitor. Benazepril is currently used to control hypertension, but the completion of this study serves as a new proof of concept for future research to look at the 100 known conditionally essential E. coli metabolic genes for additional antibiotic discovery efforts.', 'The Antibodies are secreted by plasma cells transformed from B cells, and each B cell line can produce only one specific antigenic determinant. The antibody, produced from a single cell line, is called the monoclonal antibody (McAb). The first generation of monoclonal antibodies came from hybridoma antibody technology developed by Koehler and Milstein in 1975. On the basis of cell fusion technology, mouse B cells capable of secreting specific antibodies and mouse myeloma cells with unlimited reproductive ability were fused into B cell hybridoma. A specific antibody against an antigenic epitope can be prepared by culturing a group of cells with a single hybridoma cell with this property, as shown in figure 1. However, the human immune system can recognize mouse monoclonal antibodies, which can cause human anti-mouse antibody (HAMA) response. This not only shortens the half-life and weakens the efficacy of therapeutic monoclonal antibodies, but also sometimes causes serious adverse reactions, so the clinical application of the first generation of monoclonal antibodies is greatly limited.\nFigure 1. Illustration showing the production route of hybridoma technology.\nSince the advent of the first mouse monoclonal antibody Muromonab OKT3 in the world in 1986, nearly 80 monoclonal antibodies have been on the market in the world. So far, the monoclonal antibody has developed to the fourth generation: the first generation is mouse monoclonal antibody (momab), the second generation is human-mouse chimeric monoclonal antibody (ximab), the third generation is humanized monoclonal antibody (zumab), the fourth generation is fully human monoclonal antibody (mumab). The advantage of humanized monoclonal antibody and human monoclonal antibody is that it can overcome the reaction of human anti-mouse antibody, prevent the monoclonal antibody molecule from being quickly eliminated by the immune system as heterogenous protein, and improve the biological activity of monoclonal antibody molecule. In particular, the variable and constant regions of human antibodies are human, which can further remove immunogenicity and side effects on the basis of humanized antibodies. Humanized antibodies and human antibodies have the characteristics of high affinity, high specificity and low toxicity and side effects, which greatly overcome the shortcomings of mouse antibodies and chimeric monoclonal antibodies. Therefore, it has become the inevitable trend of the development of therapeutic antibody drugs.\nMonoclonal antibodies usually target disease-related antigens or specific receptors on the cell surface, such as the PD-1 receptor on the surface of tumor cells and the PD-L1 ligand on the surface of T cells. PD-1/PD-L1 inhibitors, which are in the limelight, belong to monoclonal antibodies and are the focus of tumor immunotherapy in recent years. Nivolumab and pembrolizumab, which are on the market, are PD-1 inhibitors and are mainly used in the treatment of melanoma and non-small cell lung cancer. PD-L1 inhibitors atezolizumab (trade name Tecentriq), durvalumab (trade name Imfinzi) and avelumab (trade name Bavencio) have been approved for the treatment of urethral epithelial cancer. On September 28, 2018, FDA approved the listing of Libtayo (cemiplimab-rwlc) jointly developed by Sanofi (Sanofi) and Regenerative. It is used to treat metastatic skin squamous cell carcinoma (CSCC) or locally advanced CSCC patients who cannot receive healing surgery or radiotherapy. This is also the third anti-PD-1 antibody approved by FDA.\nFigure 2. Mechanism of PD-1 receptor and PD-L1/L2 inhibitors mediated cancer immunotherapy.\nUp to now, monoclonal antibody drugs have become an important part of biomedicine and have broad application prospects in medical treatment. It has been successfully used in the treatment of tumors, autoimmune diseases, infectious diseases, transplantation rejection and other diseases. However, there are many bottlenecks in the preparation of monoclonal antibodies. At present, the bottleneck of antibody drug research and development lies in the screening of target molecules, humanization of antibodies and preparation of human antibodies, high-throughput and large-scale screening of antibodies, prediction, modeling and analysis of antigenic epitopes, construction of three-dimensional configuration of antigen-antibody interaction and various techniques to increase the function of antibody effect.\n1.Target Screening of Antibody Drug\nTraditional antibody drugs are developed at the level of a single gene, a single protein, and a single antibody. First of all, it will take many years to study the function of the gene and its coding protein to confirm whether the gene and its encoded protein can be used as antibody drug targets to develop antibody drugs. The main drawback of this method is that the number of antibody drug targets obtained is extremely limited, and these targets were discovered more than a decade ago and it takes a long time, usually 10 to 20 years. With the continuous progress of genomics, transcriptome, proteomics and sequencing technology, more and more new genes and proteins have been found, which is expected to select suitable antibody drug targets.\nWhat are the criteria for screening antibody drug targets? In the case of antineoplastic drug targets, first of all, there should be differences in target expression, such as differences between normal and tumor tissues, or loss of expression in key host organs, or persistent expression in the progress of the disease. Second, the target should play a role, when the use of antibodies for treatment, the antigen cannot be easily degraded by enzymes. The production of high affinity antibodies from known therapeutic targets is not a major obstacle, but the main challenge is to screen target molecules.\nNow, scientists have used humanized antibody technology and fully human antibody technology, hoping to find some better antibody targets through the human immune system, so as to develop better antibody drugs. The fully human antibody technology is optimized by human-mouse hybridoma technology, human-human hybridoma technology, B cell immortalization and high-efficiency and high-throughput fully human antibody library technology. The selection, maturation and production of antibodies are all formed in the human body, so they are all human antibodies in the strict sense. Antibody targeting, antibody production and post-transcriptional modification are all completed by the human immune system after a series of screening. The antibodies produced by this technique have the best natural affinity and binding power, and act more effectively on the human body. High-efficiency and high-throughput fully human antibody library technology will be able to secrete antibodies of the target cell isolation, purification, enrichment and proliferation. The specificity of the antibody secreted by B cell subclone can be screened and identified by ELISPOT, ELISA or hemolytic plaque test. The gene sequence of the target antibody was obtained from the monoclonal cultured cell line, and the prokaryotic or eukaryotic expression vector was constructed and transferred into engineering bacteria or cells to reconstruct the activity of the antibody.\n2.Immunogenicity Analysis of Monoclonal Antibody Drugs\nAt present, the common adverse reactions of monoclonal antibodies are mainly caused by their immunogenicity. The anti-drug antibodies caused by immunogenicity have a great influence on the safety and efficacy of the drug. Immunogenicity is one of the decisive factors in the development of biotech drugs, so their immunogenicity should be taken into account when evaluating drug safety. To this end, scientists have taken measures to improve the immunogenicity of monoclonal antibody drugs, such as humanization of antibodies, improvement of solubility, protein modification (such as protein polyethylene glycol modification) and improvement of effector molecule function.\nFigure 3. Polyethylene glycol modification of antibodies.\nThe current methods for evaluating and analyzing the immunogenicity of monoclonal antibodies include enzyme-linked immunosorbent assay (Elisa), liquid chromatography–mass spectrometry (LC-MS), surface plasma resonance (SPR), electrochemiluminescence (ECL) and radioimmunoassay (RIA). However, these methods have not yet reached a unified conclusion on the critical value of immunogenicity, and the critical value of immunogenicity is different due to different distribution laws and calculation companies, which makes it impossible to unify the acceptance criteria among different drugs. However, with the continuous progress of molecular biology technology, the humanized components of monoclonal antibodies have been improved, and even the whole human antibodies have been reached. Improving the binding and effector molecular function of these antibodies, combined with protein modification, is expected to avoid the immunogenicity of monoclonal antibodies. At the same time, improving the immunogenicity detection method, unifying and standardizing it will make the clinical trial have clear guiding principles, and finally accelerate the clinical application of monoclonal antibody drugs.\n3.High-throughput Animal Cell Expression Technique\nIn terms of protein expression system, in recent years, scientists have developed and optimized the expression system of many antibody molecules, such as bacteria, yeast, insect cells, mammal cells, plant cell expression systems and in vitro expression systems. Among them, mammalian cell expression system has many important advantages, such as high activity and good stability, and has become the most important expression system in the manufacture of antibody biotechnology products.\nFrom the point of view of the scale, speed and function of antibody preparation, the development of high-throughput antibody preparation technology is very important. Large-scale and efficient culture of mammal cells is the main mode of production and key bottleneck technology of biomedical products. At present, there are flow culture technology and perfusion culture technology in the world.\n4.Construction & Optimization of Humanized and Fully Human Antibodies.\nWith the development of immunology and molecular biology, DNA recombination technology is more and more used in antibody construction and optimization. The techniques for the construction and optimization of humanized antibodies include resurfacing antibody and reshaped antibody. Resurfacing antibody refers to the humanization of amino acid residues on the surface of heterogenous antibodies. The principle of this method is to replace only the regions which are obviously different from the surface of human antibody, and to replace amino acids similar to the surface residues of human antibody on the basis of maintaining antibody activity and reducing heterogenicity. In addition, there should not be too many segments to be replaced, and the residues that could affect the size of the side chain, charge, hydrophobicity, or may form hydrogen bonds, which influence the conformation of the complementary determining region (CDR) of antibody, should not be replaced as far as possible. The reshaped antibody refers to the antibody constructed by the splicing of the antigen-binding residues of the heterogenous antibody with the human antibody, including complementary determining region transplantation, partial complement determining region transplantation and specific determining region transplantation.\nFigure 4. Chimeric antibodies and humanized antibodies.\nBoth light and heavy chains of humanized antibodies come from human beings and are the development region of therapeutic antibodies. At present, there are antibody library screening techniques for the construction and optimization of humanized antibodies, such as chain replacement and genetic engineering mice to prepare humanized antibodies. The more mature antibody library screening techniques include phage antibody library, synthetic antibody library and ribosome display technology.\nAntibodomics technology is based on genomics and proteomics, combined with hybridoma technology and genetic engineering antibody technology, after high-throughput screening of antibody targets, the establishment of large-scale antibody library, and finally applied. Compared with the traditional monoclonal antibody technology, the antibody library technology has the advantages of large library capacity, more species screening, easier to obtain highly active monoclonal antibodies against specific antigen epitopes and so on. At the same time, the antibody library technology is more timesaving, labor-saving, efficient and economical in the screening process.\nMice are still the easiest animal species for immunization and subsequent genetic engineering, but the mouse antibody V region gene is still obtained through the mouse antibody library. In order to make it safe for clinical use, follow-up humanized transformation must be carried out. The transgenic mouse technology of fully human antibody developed in the past two years enables us to prepare a human immune antibody library through transgenic mice with a complete set of human antibody genes, from which we can directly screen the V region gene of the fully human antibody with therapeutic value. There is no need for humanized transformation.\n5.Development of New Antibody Drugs\nTraditional antibody drugs inhibit tumor growth by blocking a single signal pathway, and the drug resistance of antibody drugs is easy to appear in clinic. Therefore, bispecific antibody (BsAb) came into being. By means of genetic engineering, two antibody fragments targeting different antigens are combined together, which has two antigen binding sites, which can play a synergistic role and improve the therapeutic effect. This structural design can effectively improve the pharmacokinetic process of antibody drugs in vivo and enhance the clinical therapeutic effect. However, the design of BsAb with good curative effect, high stability and conducive to production still needs to be further studied.\nThe antibody drugs on the market in recent years reflect the new trend of the development of the next generation of antibody drugs. The first direction is to make antibody drugs have a smaller molecular weight, so that they have better pharmacokinetic and pharmacodynamic parameters, and are easier to manufacture on a large scale, such as Fab fragment, Fab’ fragment, F(ab’)2 fragment, Single-chain variable fragment (scFv), single domain antibody (sdAb), diabody, triabody and minibody. The second direction is to connect at least two molecules with certain biological functions to form fusion proteins based on known drug molecules, such as bispecific antibody, trifunctional antibody, synthetic antibody (synbody), antibody-drug conjugate (ADC).\nAntibody-drug couplers are composed of monoclonal antibodies and small molecular drugs with therapeutic effect. These drugs realize the targeted delivery of chemical drugs to tumor tissue with the help of antibodies. The antibody-drug conjugate has high stability in the blood, and the drug molecules will not fall off, so the toxic and side effects are small, but the inhibitory effect on tumor cells is much higher than that of naked antibodies. This design strategy can not only improve the killing ability of antibody drugs, but also improve the treatment window of small molecular chemicals.\nFigure 5. Antibody-drug conjugate.\nMonoclonal antibody drugs provide a new way for the treatment of a variety of diseases. At present, monoclonal antibody drugs have been widely used in the clinical treatment of tumors and other diseases. From the perspective of anti-tumor monoclonal antibody drug development process, it is mainly divided into five parts: target discovery, target selection, antigen preparation, selection of monoclonal antibody preparation technology and antibody function identification. Through the technical characteristics of these links, we can find the risk factors that affect the R&D results, find the risk factors, and use the thinking and methods of risk management to analyze. Different technologies used in the preparation of monoclonal antibodies will encounter different challenges. For this reason, it is necessary for developers to carry out specific analysis of specific problems and constantly overcome these technical challenges in order to develop truly useful monoclonal antibodies and bring new life-saving drugs to patients.']	['<urn:uuid:637ac6ac-ba47-41c3-97ff-6a3ddfaa6fe5>', '<urn:uuid:a82fb98f-1152-40f8-b244-572b7d2a13c0>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T04:31:27.499155	14	70	3016
87	compare snack foods tax rules nutrition	In Manitoba's tax system, there's a complex division where some snack foods like Twinkies and Pop-Tarts are tax-exempt if sold in packages of six or more servings, while items like candy, chips, and marshmallows are taxable. From a nutritional perspective, experts recommend focusing on snacks that provide real nutritional benefits, including protein-rich foods like eggs and Greek yogurt, fiber-containing foods like whole grain crackers, and complex carbohydrates like fresh fruits. This creates a contrast between tax policy and nutritional recommendations, as many tax-exempt items don't align with healthy eating guidelines.	"['It turns out that snack foods like Twinkies and Pop-Tarts are tax-exempt in Manitoba, alongside grocery staples like vegetables and meat.\nScroll down and take the quiz at the bottom of this story. See if you can tell which foods are tax-exempt or not.\nThe list of basic grocery products that are tax-exempt across Canada, including in Manitoba, includes fruits, vegetables, meat, fish, poultry, breakfast cereals and some dairy products.\nBut a closer look at the basic groceries list reveals that Twinkies snack cakes and Pop Tarts toaster pastries are also not subject to the GST or PST.\nThat has Winnipeg dietitian Phyllis Reid-Jarvis wondering if governments should reassess what should be on the tax-free list.\nPop Tarts and Twinkies have no health benefits, Reid-Jarvis said, and making them tax-exempt does not promote healthy eating.\n""I don\'t know what the government\'s intention was, but I could discern … you\'re making it easier for Manitobans to make those poor eating choices,"" she said.\nWoman taxed for bag of potatoes\nCBC News examined the lists of tax-exempt grocery products after a Winnipeg woman said she was incorrectly charged both the GST and PST on a bag of potatoes at a local Wal-Mart last week.\n""I expected to pay $3.25, but it came to $3.50,"" said Pam Treller.\nTreller said the store manager told her the potatoes were not on the list of tax-exempt basic grocery items.\nWal-Mart admitted that it was an ""isolated incident"" that was corrected in 24 hours. As well, Treller was offered a refund, the company said.\n""We take this issue very seriously and wish to apologize to the customer for any inconvenience we have caused,"" a company spokesperson stated in an email.\nBut Treller said the provincial government should fine Wal-Mart for the mistake and look more closely at what taxes are being charged.\n""It makes you wonder what else they\'re charging on,"" she said.\nProvincial tax rules mirror federal rules\nIn an email to CBC News, a Manitoba government spokesperson said the retail sales tax ""parallels federal GST rules to simplify the tax application for both the retailer and consumer.""\nA Manitoba Finance bulletin on the Retail Sales Tax Act indicates that ""sweetened baked goods"" like cakes, pies and donuts are on the list of tax-exempt basic groceries, provided that the products sold exceed a single serving or contain six or more single servings.\nHowever, sweetened baked goods that are ""sold to consumers in quantities of less than six items, each of which is a single serving of less than 230 grams"" can be taxed, according to the bulletin.\nAmong the snack foods that CBC News found were tax-free were an eight-pack box of Pop Tarts, an eight-pack box of Twinkies, and a package of nine chocolate banana muffins.\nSnack foods that are taxable include candy, marshmallows, potato and corn chips, cheese puffs, salted nuts or seeds, fruit-based snack foods, single servings of chocolate milk — plain milk is exempt — and granola products that aren\'t sold as breakfast cereal.\nFederal health officials did not respond to a request for comment on Tuesday.\nOfficials with Food Matters Manitoba, a group dedicated to food security, say they\'re against raising taxes for junk food because it would create a burden for low-income families, which tend to buy junk food because it\'s generally cheaper than healthy food.', 'Snacks done right can fuel your child’s brain, body and athletic performance. Are you making the right choices for your young athlete?\nBy Jill Castle\nIn a world where food is plenty, and there are plenty of rules about food, the question of what’s the best snack is always top of mind for parents. As well it should be—getting snacks wrong can be a problem for health, growth and athletic performance.\nLook at the statistics—23 percent of calories eaten by U.S. kids come from snacks, and some of these are providing high amounts of sugar and saturated fat, according to the 2010 Dietary Guidelines for Americans. When it comes to the young athlete, snacking healthfully isn’t a given.\nOne study showed that although young athletes have a better intake of fruits, vegetables and whole grains, they also eat excessive amounts of junk food and sweets. In other words, they err on the side of too much snacking, and on the wrong foods.\nSnacking isn’t bad. In fact, snacks can be very useful for the young athlete if done right. Snacks help athletes get the variety of nutrients they need each day and help their brain and body be adequately fueled for training and competition. A dual benefit!\nSo, which snacks with benefits should athletes be eating?\nSnacks made with real food: Sticking to mostly real food (food you can identify, from the ground up) covers all the high points: nutrient-rich food, satisfaction after eating, stocked with ingredients that encourage health and growth, and missing the additions that can cause problems: too much sugar, unhealthy fat, and additives like food dyes and flavors.\nExamples: apples, nuts, edamame, carrots, oatmeal, milk\nSnacks containing protein: Protein is a key nutrient for the development, growth and repair of muscles. It also does a good job of keeping the tummy satisfied after eating. You don’t have to load up on protein—most kids eat enough in their regular diet without making extra additions. But, if you want to reap the benefits of protein timing is key. Athletes should try to eat protein with snacks, and especially after lengthy exercise (longer than an hour) to help repair muscle tissue.\nExamples: hard-boiled eggs, cheese stick or square, deli meat, beef or turkey jerky, Greek yogurt\nSnacks containing fiber: Fiber also has the benefit of contributing to the sensation of fullness after eating. Translated: this means you feel fuller longer and avoid the trappings of overeating. Fiber also helps promote regular bathroom visits. Make sure you’re drinking plenty of fluids.\nExamples: Triscuit crackers, a cup of black bean soup, baked sweet potato, Wheaties cereal, celery stick\nSnacks containing complex carbohydrate: All athletes need a steady supply of carbohydrate throughout the day – it is the energy source upon which muscles rely. But it’s the complex ones athletes should focus on, rather than the simple or refined versions like sugar, desserts, candy and other sweets. Complex sources of carbohydrate food sources should appear at most meals and snacks.\nExamples: cubed cantaloupe, sliced apple, pasta salad, whole grain bagel, pretzels, baked potato\nSnacks containing vitamins and minerals: Vitamins and minerals help the body process food, so it’s important to make sure food choices contain a variety of these. If you’re eating mostly real food, it won’t be a problem getting vitamins and minerals in your diet. But, if your snacking focuses on candy, cookies, chips or fried foods, you could be cutting yourself short on these important nutrients.\nExamples: vegetables, fruit, grains, dairy products, fats and lean meat\nSnacks containing healthy fats: Healthy fats have many different health functions, but for the young athlete they help the brain function well, which is important for school and on the court. However, the diet of kids and athletes tends to be too high in the wrong kinds of fats—saturated fats and trans fats. This is partly due to eating shelf-stable packaged foods, fried foods, whole dairy foods or high fat meats (ie, skin on chicken). Stick with healthy fats—mono-unsaturated and poly-unsaturated fats such as olive oil, nuts and avocado—to get the most benefits for your body and brain.\nIt’s hard to go wrong when you focus on the nutrients your child’s body needs. Combine any of these snack benefits to create a healthy, nutritious and delicious snack that will fuel your child’s brain, body and athletic performance.\nJill Castle, MS, RDN is a childhood nutrition expert and co-author of Fearless Feeding: How to Raise Healthy Eaters from High Chair to High School. She is the creator of Just The Right Byte, a childhood nutrition blog. She lives with her husband and four children in New Canaan, Conn. Contact her at Jill@JillCastle.com.\nWhen Missouri basketball coach Cuonzo Martin watches his kids participate in sports, he’s not critiquing the coach. Discover what he focuses on, and then follow his approach\nThe Winter Olympics going on in PyeongChang, South Korea, are a great opportunity for parents to engage in sports-related discussions with their young athlete. A leading expert shares how to make the most of your viewing time\nHealthy parental relationships are crucial for children to develop and thrive in all areas of their life\nParents know how special those moments of playing catch with a child are, and one of the many fascinating stories in My First Coach features the Harbaugh family’s memories of those backyard tosses']"	['<urn:uuid:e4abda9c-3aff-4fca-bd2c-5422a5df58aa>', '<urn:uuid:fbdc5643-e8e4-45da-9ecf-a8b3d6de7b30>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	6	90	1449
88	beginner need instructions protecting against sudden imbalance kayak tipping prevention methods	The basic low-brace is your primary defense against slight instabilities. Hold the paddle with the back face of your blade down, paddle shaft resting across the cockpit with your knuckles down and elbows up, like a push-up position. Keep the paddle close to your navel. When the boat tilts, stop the tilt by gently pushing the backside of your paddle blade flat against the water's surface - you should hear a gentle 'kersploosh'. This arrests your fall long enough to right the kayak by tilting your pelvis back to level. To remove the paddle, roll your hands upwards to slice the blade back out.	"['Bracing, it keeps your head in a gaseous oxygen environment. The low-brace is your primary defense against flushing those sinuses unexpectedly. It should be automatic, much like catching yourself with the flats of your hands from a slip on an icy sidewalk. Automatic, as in don\'t think about it, just do it. Its part of your basic skills toolbox along with your forward stroke and sweep stroke.\nHere\'s how its done:\nBasic low-brace: used for recovering from slight instabilities caused by boat wakes, waves and reaching for things you dropped.\nYou hold the paddle with the back face of your blade down, paddle shaft resting across the cockpit with your knuckles down and your elbows up...pretty much a push-up position. Your paddle will be tucked right in close to your navel to get this body position. Now that we have our paddles and torsos in place, let\'s get our tails into the action. Maintaining a vertical torso, gently tilt your pelvis to the left (this will tilt your boat) just enough to simulate a slight tip. As your boat tilts to the left, stop this tilt by gently pushing the backside of your paddle blade flat against the surface of the water. You should hear a gentle ""kersploosh"" when your blade hits the water. This action will arrest your fall long enough for you to right the kayak by tilting your pelvis back to a level position. Oops, hey, that pesky paddle may still be stuck just below the surface of the water now...not a good time to pull straight up on it. Try simply rolling your hands upwards to slice the blade back out. In review: push up position, boat tilts, arrest this tilt by kersploosh of the back of your paddle blade, right the boat with your backside, slice the paddle back out. Okay, now practice this on the other side. Not once in a while either, but every time you paddle, practice your low-braces slowly on each side, so that your body develops a memory of its own.\nSkulling low-brace: used for adding stability while tilting the boat to one side during a flurry of confused waves or inconsiderate jetski activity.\nFirst, tilt your boat slightly to one side, so you know which way it will go if it wants to capsize. Next, back to your basic low brace body position. Now, glide your paddle back and forth gently on the water\'s surface in about a 30-45 degree arc...just like frosting a cake. The key is to tilt the leading edge of the paddle blade up a few degrees so it glides across the surface rather than diving. Put more energy into sliding the paddle back and forth than into forcing an angle into the blade...the paddle will often readjust its own angle if you\'re gentle and let it do its job. This works well and, well, it looks pretty cool....so practice, practice, practice.\nLow brace return for sweep strokes: Refer to the last article in ""Guidelines"" for the sweep stroke and this little enhancement will open up a whole new world of snappy turns for you and your boat. Turning your kayak by doing a whole lot of sweep strokes in succession on one side takes a lot of effort. We\'ve learned that we can edge the kayak slightly to one side and it will turn away from our tilt. So, I am doing a forward sweep on the left and edging my boat to the left to carve a turn to the right. Okay, got it...although when my stroke exits the water to come forward for a second forward sweep, I feel a little unstable tilted like that. Enter the low-brace return....simply return that paddle blade back to the bow in the low brace position, rotating forward with the back face of your paddle blade just above the surface of the water. It\'ll be right where you need it to catch you if you start to fall. Remember to practice this on both sides, forward and backward.\nOne of the great things about kayaking is that on the very simple level you can just slip on a life jacket, gr…\nSo, what are our goals with paddling? First I want to thank everybody for all the fantastic feedback and al…\nOne of the great things about kayaking, is that almost anyone can do it. But it\'s still important to develop…']"	['<urn:uuid:1948755d-8608-4cf5-b9d9-0b627b12a089>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T04:31:27.499155	11	104	735
89	clinical trials outcome measures dementia interventions and strength model implementation strategies practical aspects	Clinical trials for dementia interventions use various outcome measures including quality of life scales and resource utilization tools, with QoL being recognized as particularly important alongside clinical efficacy measures. The implementation of strength-based models requires specific practical strategies including: conducting assessment through friendly conversations about abilities and interests, coordinating services to support achievement of personal goals, ensuring cultural sensitivity, and maintaining ongoing monitoring and review processes. The practical implementation emphasizes building hope through relationships, recognizing people as experts in their own lives, and focusing on remaining capacities rather than deficits.	"['Measurement tools of resource use and quality of life in clinical trials for dementia or cognitive impairment interventions: protocol for a scoping review\n© The Author(s). 2017\nReceived: 2 June 2016\nAccepted: 18 January 2017\nPublished: 26 January 2017\nDementia and cognitive impairment could severely impact patients’ life and bring heavy burden to patients, caregivers and societies. Some interventions are suggested for the older patients with these conditions to help them live well, but economic evaluation is needed to assess the cost-effectiveness of these interventions. Trial-based economic evaluation is an ideal method; however, little is known about the tools used to collect data of resource use and quality of life alongside the trials. Therefore, the aim of this review is to identify and describe the resource use and quality of life instruments in clinical trials of interventions for older patients with dementia or cognitive impairment.\nWe will perform a search in main electronic databases (Ovid MEDLINE, PsycINFO, EMBASE, CINAHL, Cochrane Databases of Systematic Reviews, Web of Science and Scopus) using the key terms or their synonyms: older, dementia, cognitive impairment, cost, quality of life, intervention and tools. After removing duplicates, two independent reviewers will screen each entry for eligibility, initially by title and abstract, then by full-text. A hand search of the references of included articles and general search, e.g. Google Scholar, will also be conducted to identify potential relevant studies. All disagreements will be resolved by discussion or consultation with a third reviewer if necessary. Data analysis will be completed and reported in a narrative review.\nThis review will identify the instruments used in clinical trials to collect resource use and quality of life data for dementia or cognitive impairment interventions. This will help to guide the study design of future trial-based economic evaluation of these interventions.\nSystematic review registration\nKeywordsResource use Quality of life Tool Clinical trial Dementia Cognitive impairment\nAs the population ages rapidly, the prevalence of dementia and cognitive impairment is a growing public health concern worldwide and it has been estimated to increase within the next 20 years . These two disorders could impact patients’ cognitive function, behaviour and activities of daily living, and have become one of the principal causes of disability and decreased quality of life (QoL) among older people . It is increasingly recognised that psychosocial interventions contribute to the care of people with dementia and their families in a wide range of domains . For example, the sensory rehabilitation has been shown to improve patients’ QoL and increase their social engagement, which could help them live well with dementing conditions .\nIn light of expanding health care costs and finite budget, cost-effectiveness analysis is essential for national health care decisions and resource allocation. The outcome of effectiveness used in such analysis is the quality-adjust life years (QALYs), which take both the quantity and quality of life into account. In dementia research, QoL has been recognised as an important measure as the clinical efficacy measure . Several instruments have specifically been developed to assess QoL in dementia [2, 6, 7]. According to the most recent systematic review , more than 10 QoL measures were identified and properties assessed, but this review was limited to disease-specific QoL measures only, and such measures may not be used directly to generate health utility scores for QALYs calculations in cost-effectiveness analysis.\nAmong the methods available to assess health care interventions, trial-based economic evaluations are considered as an ideal vehicle for data generation because of the availability of patient-level data and unbiased estimates of clinical outcomes . But more information is needed on the tools for data collection alongside the trials. Schölzel-Dorenbos et al.  performed a systematic review in 2006 on the use of QoL measures as an outcome in intervention trials in patients with mild cognitive impairment or dementia and found only three studies and two QoL scales. Following this review, a lot of new QoL instruments were developed and widely used, e.g., Dementia Quality of Life questionnaire (DEMQOL)  and dementia specific quality of life instrument (QUALIDEM) .\nResource use is also an essential component in the cost-effectiveness analysis. Instruments are recommended for cost data collection to improve the quality and uniformity of data generated from trials, suggested by the International Society for Pharmacoeconomics and Outcomes Research (ISPOR) . The Resource Utilisation in Dementia (RUD) instrument is a standardised tool and the most widely used instrument for resource use data collection in dementia . It has been used in several clinical drug trials for Alzheimer’s disease [13–15] and several observational studies [16–18]. But there is a lack of information about the use of RUD in clinical trials for dementia or cognitive impairment, especially for non-pharmacological interventions, and whether there are other instruments available to collect resource use data in such trials is yet unknown.\nTherefore, in this review, we aim to identify and describe the resource use and QoL instruments that have been used in clinical trials of dementia or cognitive impairment interventions.\nThis review will follow the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) Statement  and consists of acquiring, extracting and assessing the data. This protocol is in accordance with the PRISMA-Protocols (PRISMA-P) 2015 checklist  (Additional file 1: Table S1 for the PRISMA-P 2015 checklist)\nPopulation—older adults with dementia or cognitive impairment\nIntervention—all types of interventions, both drug and nondrug therapies\nComparator—no intervention or the usual care\nOutcomes—measurement and reporting of QoL, or resource use or both\nStudy type—randomised clinical trial, or feasibility study or pilot study\nNo language restrictions will be imposed during the literature search but the abstract should be available in English. There is no restriction on date of publication. All studies should be original research published in a peer-reviewed journal. For the definition of ‘older adults’ used in this review, we will accept any age cut-off if a study describes their population as being ‘older adults’. The definition of ‘patients with dementia or cognitive impairment’ will also be based on each individual study. The outcomes should be measured using standardised questionnaires or tools. Quality of life is an abstract and broad concept including physical function, perceptions of well-being, satisfaction, and sense of self-worth. Given the aim to guide cost-effectiveness analysis study design, quality of life, quality-adjusted life years, health utility and QALY will be used as the search terms.\nThe following major databases for the discipline of medicine and nursing will be searched: Ovid MEDLINE, PsycINFO, EMBASE, CINAHL, Cochrane Databases of Systematic Reviews, Web of Science and Scopus. The systematic search will be conducted in September 2016, and the searches will be re-run just before the final analyses to retrieve further studies for inclusion. A hand search of the references of included articles and general search, e.g. Google Scholar, will also be conducted to identify potential relevant studies.\nKey terms have been determined through discussion between two authors (FY and BG). The following terms or their synonyms will be used: older, dementia, cognitive impairment, cost, quality of life, intervention and tools. The search terms used in Ovid MEDLINE can be found in Additional file 2: Table S2. The search strategies will be created specifically for each database using relevant index and free text terms. The titles and abstracts of all identified studies potentially eligible for inclusion in the review will be screened. Full-text versions of the included articles will be obtained.\nAll results from database and hand searches will be exported into Endnote X7 software (Thomson Reuters, 2016). Duplicates will be removed using a standard function before each entry will be screened from eligibility. After dropping duplicates, all the titles and abstracts of the studies retrieved will be imported to an Excel spreadsheet (Microsoft Corporation, 2010) for screening.\nStudy selection will be undertaken in two stages: first, titles and abstracts will be screened against the inclusion criteria; second, the full-text for all eligible articles will be screened to confirm whether or not the study should be included in the final review. Two authors (FY and BG) will carry out the selection process. If there are discrepancies and the two investigators cannot reach a consensus, the disagreements will be resolved through discussion and consultation with a third reviewer (PD).\nPublication characteristics (title, year of publication, author, study objective, type of study);\nParticipant characteristics (country, inclusion criteria, exclusion criteria, age, sex, and disease, e.g. mild cognitive impairment, dementia, or both);\nIntervention characteristics (what intervention, type of intervention, duration of intervention and comparator);\nOutcome characteristics (whether a cost/QoL measure was used, what instrument or instruments used, time points at which the instrument was assessed, patient/proxy reported, and type of QoL measure).\nIf any of the previously described data is not clearly presented in the research article, we will contact the authors for clarification by sending emails.\nSince the aim of this review is to identify and describe measures of resource use and QoL in trials, without reporting quantitative evaluation of measurement properties or trial effect estimates, the quality of included studies will not be assessed.\nFirst, the characteristics of included studies will be tabulated based on the data extracted using Additional file 3: Table S3. Second, the frequency of each resource use or QoL instrument used in the trials will be reported. Third, the characteristic of each measurement instrument will be summarised using a table (Additional file 4: Table S4), which is based on the summary table used in one systematic review of dementia-specific QoL scales , and will include instrument name, conceptual basis, patient report (Yes/No), proxy report (Yes/No), patient population, subscales, items, response options and scoring. One author (FY) will summarise the results, and a second researcher (BG) will review and highlight any discrepancies. Disagreements between the two authors will be resolved by discussion, with involvement of a third review author (PD) where necessary.\nThe main aim of this review is to identify and describe the tools/instruments used in clinical trials to collect data of resource use and QoL for dementia or cognitive impairment interventions. The results of the review will provide information about potentially useful instruments in future similar trials and contribute to the study design of trial-based economic evaluation of dementia or cognitive impairment interventions. We anticipate that the review will be useful to a variety of stakeholders who have an interest in dementia and cognitive impairment care.\nThis review is part of the Work Package 4 of the SENSE-Cog project, which has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 668648. The funding body has no role in the study design, data collection, data analysis, data interpretation or manuscript writing.\nAvailability of data and materials\nData are available to all interested researchers upon request. Please contact the corresponding author.\nFY contributed to the study design, drafting the article and the final approval. BG contributed to the study conception and design, critical revision of the article draft and the final approval. All authors read and approved the final manuscript.\nThe authors declare that they have no competing interests.\nConsent of publication\nEthics approval and consent to participate\nOpen AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\n- Handels RL, Wolfs CA, Aalten P, Verhey FR, Severens JL. Determinants of care costs of patients with dementia or cognitive impairment. Alzheimer Dis Assoc Disord. 2013;27:30–6.View ArticlePubMedGoogle Scholar\n- Scholzel-Dorenbos CJ, van der Steen MJ, Engels LK, Olde Rikkert MG. Assessment of quality of life as outcome in dementia and MCI intervention trials: a systematic review. Alzheimer Dis Assoc Disord. 2007;21:172–8.View ArticlePubMedGoogle Scholar\n- Moniz-Cook E, Vernooij-Dassen M, Woods R, Verhey F, Chattat R, De Vugt M, Mountain G, O’Connell M, Harrison J, Vasse E, et al. A European consensus on outcome measures for psychosocial intervention research in dementia care. Aging Ment Health. 2008;12:14–29.View ArticlePubMedGoogle Scholar\n- Livingston G, Kelly L, Lewis-Holmes E, Baio G, Morris S, Patel N, Omar RZ, Katona C, Cooper C. Non-pharmacological interventions for agitation in dementia: systematic review of randomised controlled trials. British Journal of Psychiatry. 2014;205:436–42.View ArticlePubMedGoogle Scholar\n- Winblad B, Brodaty H, Gauthier S, Morris JC, Orgogozo JM, Rockwood K, Schneider L, Takeda M, Tariot P, Wilkinson D. Pharmacotherapy of Alzheimer’s disease: is there a need to redefine treatment success? Int J Geriatr Psychiatry. 2001;16:653–66.View ArticlePubMedGoogle Scholar\n- Bowling A, Rowe G, Adams S, Sands P, Samsi K, Crane M, Joly L, Manthorpe J. Quality of life in dementia: a systematically conducted narrative review of dementia-specific measurement scales. Aging Ment Health. 2015;19:13–31.View ArticlePubMedGoogle Scholar\n- Banerjee S, Samsi K, Petrie CD, Alvir J, Treglia M, Schwam EM, del Valle M. What do we know about quality of life in dementia? A review of the emerging evidence on the predictive and explanatory value of disease specific measures of health related quality of life in people with dementia. Int J Geriatr Psychiatry. 2009;24:15–24.View ArticlePubMedGoogle Scholar\n- Hughes D, Charles J, Dawoud D, Edwards RT, Holmes E, Jones C, Parham P, Plumpton C, Ridyard C, Lloyd-Williams H, et al. Conducting economic evaluations alongside randomised trials: current methodological issues and novel approaches. Pharmacoeconomics. 2016;34:447–61.View ArticlePubMedGoogle Scholar\n- Smith SC, Lamping DL, Banerjee S, Harwood R, Foley B, Smith P, Cook JC, Murray J, Prince M, Levin E, et al. Measurement of health-related quality of life for people with dementia: development of a new instrument (DEMQOL) and an evaluation of current methodology. Health Technol Assess. 2005;9:1. −+.Google Scholar\n- Ettema TP, Droes RM, de Lange J, Mellenbergh GJ, Ribbe MW. QUALIDEM: development and evaluation of a dementia specific quality of life instrument—validation. Int J Geriatr Psychiatry. 2007;22:424–30.View ArticlePubMedGoogle Scholar\n- Ramsey SD, Willke RJ, Glick H, Reed SD, Augustovski F, Jonsson B, Briggs A, Sullivan SD. Cost-effectiveness analysis alongside clinical trials II—an ISPOR Good Research Practices Task Force report. Value Health. 2015;18:161–72.View ArticlePubMedGoogle Scholar\n- Wimo A, Gustavsson A, Jonsson L, Winblad B, Hsu MA, Gannon B. Application of Resource Utilization in Dementia (RUD) instrument in a global setting. Alzheimers Dement. 2013;9:429–35. e417.View ArticlePubMedGoogle Scholar\n- Wimo A, Winblad B, Stoffler A, Wirth Y, Mobius HJ. Resource utilisation and cost analysis of memantine in patients with moderate to severe Alzheimer’s disease. Pharmacoeconomics. 2003;21:327–40.View ArticlePubMedGoogle Scholar\n- Wimo A, Winblad B, Shah SN, Chin W, Zhang R, McRae T. Impact of donepezil treatment for Alzheimer’s disease on caregiver time. Curr Med Res Opin. 2004;20:1221–5.View ArticlePubMedGoogle Scholar\n- Gustavsson A, Cattelin F, Jonsson L. Costs of care in a mild-to-moderate Alzheimer clinical trial sample: key resources and their determinants. Alzheimers Dement. 2011;7:466–73.View ArticlePubMedGoogle Scholar\n- Wimo A, Nordberg G, Jansson W, Grafstrom M. Assessment of informal services to demented people with the RUD instrument. Int J Geriatr Psychiatry. 2000;15:969–71.View ArticlePubMedGoogle Scholar\n- Wimo A, Winblad B. Societal burden and economics of vascular dementia: preliminary results from a Swedish-population-based study. Int Psychogeriatr. 2003;15 Suppl 1:251–6.View ArticlePubMedGoogle Scholar\n- Gustavsson A, Brinck P, Bergvall N, Kolasa K, Wimo A, Winblad B, Jonsson L. Predictors of costs of care in Alzheimer’s disease: a multinational sample of 1222 patients. Alzheimers Dement. 2011;7:318–27.View ArticlePubMedGoogle Scholar\n- Moher D, Liberati A, Tetzlaff J, Altman DG, Group P. Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement. PLoS Med. 2009;6:e1000097.View ArticlePubMedPubMed CentralGoogle Scholar\n- Moher D, Shamseer L, Clarke M, Ghersi D, Liberati A, Petticrew M, Shekelle P, Stewart LA, Group P-P. Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement. Syst Rev. 2015;4:1.View ArticlePubMedPubMed CentralGoogle Scholar', 'The strength based model represents a paradigm shift — a movement away from a deficit based model which is one that can lead to a long list of the things that are considered to be ‘wrong’ with people’s learning and development or that people cannot do, and insufficient information about strengths and strategies to support the people’s learning and development.\nIf you need assistance with writing your nursing essay, our professional nursing essay writing service is here to help!Find out more\nStrength based model is valuing everyone equally and focused on what the person can do rather than what the person can’t do and create hope by focusing on what has worked well for them in the past. It is describing learning and development respectfully and honestly. It is building on the person’s abilities and acknowledging that people meet difficulties that need awareness and provide so communication can be seen as resources not barriers.\n1. It builds self-esteem and sense of competence or accomplishment for patients with dementia and their families.\n2. For patients with dementia it focuses on health and well-being by embracing an asset-based approach to promote the positive.\n3. It values everyone equally including patients with dementia and focuses on what the person can do rather than what the person can not do and create hope by focusing on what has worked well for them in the past.\n4. It builds on the abilities for clients with dementia and acknowledges that they meet difficulties so they learn awareness so that can communicate with other without barriers.\n5. It attempts to identify ‘what works’ and ‘how it works’ for clients with dementia so that they can be continued and developed to match the client’s abilities.\n1. It may not work in people with dementia and families with safety and high risk factors such as abusing patients by carers or families or addictions like smoking and drinking.\n2. It may set people with dementia up for disappointments especially with unrealistic goals since they are not able to achieve the goals at all.\n3. It may be impediment to relevant information such as feeling that people with dementia have expected to frame the statement in positive terms and it may not allow them to get a complete picture of the theirs improving and development.\n4. People with dementia may not find out the services they can get voluntarily and may be seen as resistant or non-compliant.\n5. People with dementia who are suffering from financial issues or having social stigma may not be able to access the services.\nAssessment should begin with the person’s interests and attributes, rather than their deficits. Questions to obtain strengths, capacity and interests are a key part of assessment. The information should be elicited through a friendly conversation with the person about the abilities, interests, daily routines and desires such as\n- Tell me about yourself – and about you as a person.\n- What are your interests? What do you enjoy?\n- What do you like to do?\n- Who are the people that are especially important to you? Tell me about these relationships.\n- What community connections do you have – who is part of your community? Or What community activities are important to you?\n- What do you want to achieve? What is getting in the way of this happening? (Elicit why strengths are not used, for example religious or cultural belief or restriction.)\n- What ideas do you have to overcome these hurdles?\n- Tell me about your daily routine and what makes a good day for you.\n- What are the things you do, each day or each week, because you really want to – not because you have to?\n- Can you describe how you do specific tasks and their components (for example, can push a shopping trolley and select items from a shelf but cannot lift heavy bags; can push the vacuum but cannot bend down to plug it in; can shower but cannot step over the bath edge into the shower).\n- What kind of exercise do you get each day?\nPlanning is to maximise and improve the person’s independence and quality of life. It supports the person’s strengths and abilities, and lists strategies to respond to their needs. It occurs in discussion with the person, and with their carers, family, friends, guardian and other organisations where relevant. It also has to be the person centered and individual recovery plans may be used.\n- is an active process that includes interpreting assessment information, feedback, review, monitoring and exiting\n- involves balancing comparative and competing needs, and assisting the person make decisions proper to their needs, wishes, values and situations.\n- is reactive to the cultural needs of the person and maintains cultural sensitivity\n- may require entrance to counseling or information from a area of sources to expand a proper solution\n- takes into account the availability of services (within and beyond the organisation) and improve creative and flexible solutions to proactively\n- assist the person to obtain their goals\n- may be a staged process.\nPlans change as a person obtains their goals continually or their preferences or circumstances changes.\nCo-ordination of this models have to consistent with the Right arguments which clarifies that people with dementia have the right of access to all services, resources, options and choose to live actively and participate in the community. The services and support for people with dementia have to address present clinical problems but also include social, housing and spiritual needs. Coordination may include the following tasks :\n- states shared goals and outcomes\n- outline the duties and responsibilities of each service provider\n- coordinate service providing to support the person to achieve their goals\n- assist communication of agreed strategies and interventions, to ensure all service providers are well-informed and working for the same goals\n- identify the person responsible for care coordination, such as a key worker, care coordinator or case manager, as appropriate\n- monitor and review service provision and plan for discharge, transition or exit from the service.\n1. Build hope through strengthened relationships with people, community and culture.\n2. Strengthens the belief that people are experts in their own lives and the professional or carer’s role is to increase and explain choices and encourage people to make their own decisions and informed choices.\n3. Love the positive perspectives such as glass as half full rather than half unfilled,\n4. Experts take a quality based system with client’s remaining capacities, not the client’s handicaps.\n5. Provide more than possessing time, the point of quality based models is to give individuals with dementia with on-going and significant intercession all through the time of their sickness.\n6. Collaboration and reduce power among individual, family and staff based on the difference.\n- Summary of expected outcomes\n1. Person and individual centered support services because the model focused on client’s needs.\n2.. Have a strong sense of identity such as if there is the person with dementia who came from Korea, staying in residential facility in New Zealand and when careers encourage the person to use his/her own language which is one of the person’s strength and respect the culture, the person will get more strong self-esteem and confidence.\n3. Be more effective communicator through the strength based model. If the person with dementia is good at understanding non-verbal communications and when careers use this strength to try to communicate with the person, strength based model can make him/her to be good communicator to improve communication skills.\n4. Make them to be more independent from their families and careers through improving their strength and getting services which is focused on the strength.\nDepartment of education Auckland (Sep 2011) Strength-Based Approach Retrieved from http://www.eduweb.vic.gov.au/edulibrary/public/earlychildhood/learning/strength-workingpaper.pdf\nDepartment of Health information for a healthy New York Strength based care planing Retrieved from https://www.health.ny.gov/diseases/conditions/dementia/edge/strength/index.htm\nDepartment of education Auckland (Sep 2011) Strength-Based Approach Retrieved from https://www.eduweb.vic.gov.au/edulibrary/public/earlychildhood/learning/strength-workingpaper.pdf\nIRISS Strengths-based approaches for working with individuals Retrieved from http://www.iriss.org.uk/resources/strengths-based-approaches-working-individuals\nAdvance healthcare network Strength-Based intervention for adults with Alzheimer’s Retrieved from http://speech-language-pathology-audiology.advanceweb.com/Article/Strength-Based-Intervention-for-Adults-with-Alzheimers-4.aspx\nVictorian Government initiative Strengthening assessment and care planning Retrieved from http://www.health.vic.gov.au/hacc/downloads/pdf/assess_guide.pdf\nPMC Social role valorization in community mental health housing Retrieved from http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3644172/\nKendrick, Michael (1994) Some reasons why social role valorization is important Retrieved from http://www.cheshire.ie/docs/infobank/servicedesign/SomeReasonsWhy.pdf\nNDA Supporting people with autism spectrum disorder to obtain employment Retrieved from http://www.nda.ie/website/nda/cntmgmtnew.nsf/0/091BDD567113418180257B050032020C/$File/autism_paper.htm\nJoe Osburn (1998) Social role valorization Retrieved from http://www.socialrolevalorization.com/articles/overview-of-srv-theory.html\nCite This Work\nTo export a reference to this article please select a referencing stye below:\nRelated ServicesView all\nRelated ContentAll Tags\nContent relating to: ""alzheimers""\nAlzheimer’s disease (AD), is a degenerative disorder that leads to memory loss and bodily functions and is the most common form of dementia.\nDementia Of Alzheimers Type Health And Social Care Essay\nAgeing brings with it changes in number of domains, including attitudes, health, self-image, relationships, status, generational changes, sexual functioning and an awareness of time and mortality. The...\nPerson Centred Care for Dementia Patients\nIngrid Joy Moreno Castaneda Abstract This paper presents the different aspects of person-centred approach in the promotion of health to the elderly with dementia and other geriatric health condi...\nDMCA / Removal Request\nIf you are the original writer of this essay and no longer wish to have your work published on the NursingAnswers.net website then please:']"	['<urn:uuid:40665dac-dbc8-4d3a-8e74-75142ee585cf>', '<urn:uuid:adaf5dbd-2010-4501-b582-66b7c894db1f>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	13	90	4168
90	How do modern choreographers explore human relationships differently?	Different choreographers explore human relationships through distinct approaches. While Charles Weidman's works like Lynchtown examine dark aspects of human behavior and mob mentality through dramatic narrative movement, Pina Bausch's Bluebeard explores human relationships through intense ritualistic movement that exposes the violence and absurdity between men and women. Her characters are locked in compulsively repeated gestures that demonstrate themes of seduction and domination, creating a breathless world that examines power dynamics and desires.	"[""Home » Posts tagged 'Weidman'\nTag Archives: Weidman\nThis past spring, the Charles Weidman Dance Foundation had the pleasure of supplying video footage of Charles Weidman’s Lynchtown to the Centre National de Danse Contemporaine (CNDC) in Angers, France. The video footage was included in the Dance is a Weapon NDG 1932/1955 exhibit in the lobby of the Le Quai performance space from May 16 to June 17 and was free to the public. Julia Jurgilewicz, Charles Weidman Dance Foundation’s administrative assistant, and Weidman Dance alumna Claire Westby, happened to be performing at Le Quai on May 19th with Liz Gerring Dance Company and were able to stop by the exhibit. Julia recounts her tour experience, from exploring the Loire valley to taking Cunningham class at the CNDC with Robert Swinston, and visiting the Dance is a Weapon exhibit:\nGetting to Angers, France was an adventure in itself. I was able to dust off my French to change some of the dancers’ train tickets to stay in Paris for a few hours. We locked up our bags at the station, then walked along the Seine river, saw the Eiffel Tower, and got a delicious Parisian breakfast. The train to Angers yielded picturesque views of French countryside chock-full of roaming cows, ancient stone houses, and rolling hills.\nAngers itself is a quaint town complete with the glorious Cathédrale d’Angers, the sturdy 13th century Château Angers, and the steady Maine river which clips the town in two. We were fortunate enough to have the first few days off to explore the Loire valley. We took a long van ride through ancient villages to the town of Saumur where we saw the fantastical Château de Saumur and explored and ate lunch at the sprawling Château Villandry.\nSo far this trip sounds more like a fairy tale than the sometimes grueling, hard work of a dance company’s tour, but we were not without our sweaty rehearsals and long soaks in baths. Liz Gerring Dance Company is an extremely athletic company that allows its dancers to focus on strength and endurance. The company explores non-narrative movement derived from pedestrian gesture and athletic training. The hour-long work we were performing in Angers, Horizon, is a feat of of just that, or as Robert Johnson of New Jersey Arts put it, Horizon “is a dance for heroes.”\nBefore rehearsals, we had the pleasure of taking Cunningham class from Robert Swinston alongside his company’s dancers. Swinston was appointed the artistic director of the CNDC Angers in 2013, where he teaches Cunningham class, re-stages Cunningham’s dances, and creates his own works. Liz Gerring Dance Company and Swinston’s company were able to show-and-tell their dances in the studio and hang out after rehearsals to compare living and dancing in Paris and NYC. I had a great time practicing my French with these dancers and seeing the juxtaposition of Cunningham and Liz Gerring’s work.\nThe performance of Horizon took place at Le Quai, an amazing arts space along the Maine River. The center seeks to include dance, theater, opera, world music, and more. The facilities are a treat compared to the often cramped dressing rooms of NYC theaters and there is a great restaurant on the roof of the building. Le Quai is also a hip hang out space for the community; our first day there a skateboard and tattoo festival was going on out front.\nAfter the performance, I had some time to check out the Dance is a Weapon exhibit in the lobby of the building. It was a great exhibition with audio/video media, iconic photos, and colorful information banners. I was excited to see pictures of Charles Weidman and Martha Graham among other modern dance pioneers. Between the exhibit about early American modern dance in a home that features Cunningham’s legacy, and performing work by the next generation of contemporary choreographers, I had an array of dance influences melding to create an amazing experience.\nIn Liz Gerring’s Horizon, the dancers explore non-narrative, athletic movement to an original soundtrack by Michael Schumacher and set by Robert Wierzel. The collaborative nature of the work is reminiscent of Merce Cunningham’s creations that often incorporated multiple artists, from sound designers like John Cage to visual artists like Andy Warhol. The movement is both contemporary and inclusive of codified modern dance techniques. The NYTimes describes the work as “fluently combin[ing] modern technique with a postmodern and quasi-analytical scrutiny of pedestrians and athletes.” The dancers use similar theories from Weidman technique including fall and recovery, flattening and curving of the spine, and released and suspended movements. At the close of Horizon, I do a series of repeated falls across the stage reminiscent of the falls in Weidman’s Brahms Waltzes.\nDoris Humphrey and Charles Weidman were interested in how gravity and momentum affected movement, an idea that is explored in rehearsal for Liz Gerring Dance Company. All affectation is stripped away, and we are asked to fall, throw, lunge, run, and jump honestly. While ballet and modern techniques are inherent in our bodies, the movements are results of gravity working for or against us and how much momentum we are allotted. Sometimes we are asked to spring from one shape to the next without “winding up”, or conversely, we are asked to gather all of our energy and hurtle across the space. This cause and effect theory is intrinsic in Weidman’s “kinetic pantomime”, though he played with the order and explored reversing these properties under a narrative context.\nWhen I was studying for my Bachelors of Fine Arts degree at NYU, I had the pleasure of performing Weidman’s Easter Oratorio and Brahms Waltzes. A great lesson I took from learning these works was the importance of timing and duration of movement. A lot of attention was placed on how long a développé or suspension took or how still we were while holding a position. Liz Gerring’s work Horizon is centered around the duration of movement and the sustaining of shapes. Often times the music is adjusted live to our performance of the work as each movement and section can vary slightly in timing. While Weidman used these methods to convey an idea or feeling, Horizon uses timing and duration to give the audience an experience similar to a natural time lapse film- abstract, evolving, and surprising.\n* For more detailed information on the characteristics of Humphrey-Weidman technique, see “A Reaffirmation of the Humphrey-Weidman Quality” by Svea Becker and Joenine Roberts, 1983, Dance Notation Journal vol 1 no 1 (available on the internet at the Dance Notation Bureau Theory Bulletin Board).\nThe end of our tour included a train ride back to Paris where the company parted ways, some for the States and some for more Parisian nights. I was able to do some traveling through Paris, Barcelona, and Madrid. Now all back in NYC, the Liz Gerring Dance Company is now gearing up for the premier of their new work (T)here to (T)here at Baryshnikov Arts Center November 10-12th, 2016.\n“…in Lynchtown (from his Atavisms suite) grim horror was the keynote. In this work, the audience witnessed not only the injustice with which a minority group of our population has been treated but also the primitive blood lust, the sadism which supposedly civilized persons reveal when a scapegoat for their savagery is found. Lynch Town strikes home, it strikes the very being of the American, for the trembling evil of the lynchers themselves and the evil of the lookers-on who share vicariously in the horrible thrill seem to vibrate across the footlights and attack the complacency of those who sit in the safety of the theater. The dancers move with racing frenzy, halting to look at death with lust and, perhaps, with fear. A figure stretches forward to get a better view of murder, and horror stretches along the invisible waves of art communications to remind the beholder that the battle for ‘the land of the free and the home of the brave’ is not yet won.” -From Walter Terry, The Dance in America, Harper & Row, 1956\nOn May 7, in historic downtown Jersey City, Tachair Bookshoppe hosted a fascinating, multimedia lecture by Dr. Jeff Friedman. “Weidman’s Lynchtown: American Dialectics, Moral Questions and the Art of Persuasion” considered Weidman’s work from a dazzling array of perspectives including Laban Movement Analysis, Piaget’s ideas on the development of morality in children, and Cicero’s importance of gesture in Oratory.\nNimbus Dance Works dancer, Yuko Monden, demonstrated movement from Lynchtown as Dr. Friedman related them to Laban’s concepts of free flow, bound flow, weight, spoking and arcing. The entrance walk of the lynch mob is an example of “bound flow”, while Monden’s final exit as she leaps off the stage is “free flow.” Using archival footage, Friedman also showed how the lynch mob’s entrance creates a “wall of movement” that physically and emotionally separates the audience from the victim.\nDr. Friedman went on to discuss Weidman’s early interest in comedy and satire. Archival photos of The Happy Hypocrite (1931) and The School for Husbands (1933) (choreographed with Doris Humphrey) were used as examples. Dr. Friedman’s comments on the social significance of satire were especially interesting.\nThe lecture was followed by a lively discussion on a wide range of topics including the psychological challenges faced by performers in portraying such evil, the choice of the smallest dancer to portray “the Incitor” character of the mob, how the dances in Atavisms (Stock Exchange, Bargain Counter, and Lynchtown) relate to current events, mob behavior, and how best to teach about fascism.\nIf you missed Dr. Friedman’s lecture, you still have one more opportunity to attend on May 23 at 7pm at the Highland Park Public Library, 31 North Highland Park, Highland Park, NJ. For live performances of Lynchtown, don’t miss Nimbus Dance Works’ Jersey City spring season, “UNPLUGGED” May 30,31, and June 1 at the Barrow Mansion, 83 Wayne Street, Jersey City.\nWords by Nadira Hall\nOn May 17th, 2013, dance lovers and connoisseurs will have a rare opportunity to see the choreography of Pauline Koner. Preeminent Koner expert, Evelyn Shepard, has lovingly and painstakingly reconstructed three important Koner works that will be presented at the 92nd Street Y’s Fridays at Noon series.\nDancefusion Company will perform Concertino (1955). The dance takes place in the time of the Renaissance where “a lady and her ladies-in-waiting are first at court” where they present themselves as “elegant, formal, conversational.” Next a solo reveals “the woman behind the elegant façade” and is followed by a lively dance where “the wear and tear of court formality are forgotten.”\nRyoko Kudo and Pablo Francisco Ruvalcaba of the Jose Limon Dance Company will perform Poeme (1962), a ”tender yet provocative” love duet,”influenced by Chagall*, whose women, when transported emotionally, fly in the air or soar upside down.”\n360 Dance Company will perform The Shining Dark (1956), a trio inspired by the life of Helen Keller. In Pauline’s words: “ I had long been thinking about Helen Keller whose only medium of communication was movement—the manual alphabet…so I dug in and learned the manual alphabet”. The dance is comprised of four sections: “World of Nothingness,” “World of Awakening,” “Panic of Loss,” and “Remembered Image.”\nWhile dance maverick Pauline Koner is impossible to categorize, we consider her part of the Humphrey Weidman family. Pauline Koner’s initial dance training was with Michel Fokine. Early on she pursued her own solo career, while also performing extensively with Michio Ito and then Yeichi Nimura. In the mid 1940s, seeking guidance in the choreographic process, she began a long association with Doris Humphrey. Especially memorable for her role as Emilia in Limon’s Moor’s Pavane, Koner was also a guest artist with the Jose Limon Dance Company from 1946-1960.\nLess known is Koner’s association with Charles Weidman. Inspired by Abner Dean’s** drawings, Pauline became intrigued with creating a satire on “the insanities, complexities and hilarities of living.” As the characters “crystallized”, she naturally thought of Weidman. “I approached Charles with trepidation. After all he was a senior member in the hierarchy of modern dance. Charles accepted and I was thrilled.” Thus, Amorous Adventure was born. Pauline played “A Kind of Wife” and Charles ”A Sort of Husband”, while Lucas Hoving portrayed “Variations from the Norm.” After it’s premiere in 1951, Winthrop Palmer wrote: “Pauline Koner’s Amorous Adventure …was a delightful spoofing of comic eugenics and the battle of the sexes with never a moment of social significance, for which it deserves a gold medal…”\nAlso a noted teacher, Koner developed her famous course “Elements of Performing.” Her elegantly articulated concepts of breath, suspension, rebound, and weight could easily be part of a primer on Humphrey Weidman technique.\nDon’t miss this chance to see Pauline Koner’s artistic creations. Films will be shown in the lobby starting at 11:00 AM, followed by live performance and panel discussion at noon.\nFriday May 17, 2013\n92nd Street Y Harkness Dance Center\n1395 Lexington Avenue, NY, NY\nTo learn more about the event, visit the 92nd St Y website here.\nAll quotes from Solitary Song by Pauline Koner, Duke University Press, 1989\nAlso recommended: Elements of Performance by Pauline Koner, Harwood Academic Publishers, 1993\n*Marc Chagall (1887-1985) was a Russian born artist known for his use of many artistic mediums including painting, stage sets, book illustrations, and ceramics to name a few\n**Abner Dean (1910 – 1982) was an American cartoonist who often depicted extremes of human behavior\nToday is a very special day! It marks the 85th anniversary of the first presentation of Charles Weidman’s work!\nOn March 24, 1928, Charles Weidman and Doris Humphrey presented their first concert at the Brooklyn Little Theater (now called the Brooklyn Music School Playhouse). Weidman’s Submerged Cathedral (Cathedrale Engloutie) and Humphrey’s Color Harmony and Air for the G String were among the important works that premiered on the program.\nColor Harmony, considered to be America’s first abstract ballet, was based on the color theory of light. Groups of dancers represented as different primary colors interact and mingle around the stage. Quoted from Doris Humphrey’s notebook, she describes the flow of the dance poetically; “Through the wild colors shoots a silver arrow–it separates the couples–it draws them one by one into form—all the flaming colors are laid down in rhythmic patterns—in a pyramidal form—up high steps to a climax, where a silver streak molds itself into a stream of light that goes up into infinity.”1 Also innovative for its time, Clifford Vaughan composed the music for the work after Humphrey composed the movement.\nWeidman’s Submerged Cathedral is based on a Breton legend about a cathedral that periodically “rises out of the water. The ringing of the bells and the chanting of the monks are heard—silence when the cathedral sinks back into the sea.”2 In his performance, Weidman “indicated with a truly moving quality the surge of the sea depths, the rising and sinking of the submerged structure, and the tolling of the underwater bells.”3Opening and closing with swirling circular movements contrasted by sharp upward thrusting movements in the middle, the choreography foreshadows Humphrey’s 1931 Two Ecstatic Themes: Circular Descent and Pointed Ascent.\nWeidman continued to perform Submerged Cathedral until his death in 1975. In 1993-1994, Peter Hamilton recreated the choreography which has since been performed by Craig Gabrian (pictured above) at the Sylvia & Danny Kaye Playhouse, the Kennedy Center for Performing Arts and the Brooklyn Academy of Music. It has also returned to its first home, the Brooklyn Little Theater, where, in 1996, the Charles Weidman Dance Foundation presented the Brooklyn Music School with a plaque commemorating the first concert. Again in 2003, for the 75th anniversary, the program included Easter Oratorio, Fables for Our Time, Submerged Cathedral and Two Ecstatic Themes.\nThe CWDF was thrilled when Brooklyn Borough President Marty Markowitz proclaimed March 24th Humphrey Weidman Day. Celebrate Humphrey Weidman Day today and remember the great modern dance pioneers and all they established for the future modern dancers of the world. Thank you Charles and Doris!\n1from Doris Humphrey’s notebook, quoted in Days on Earth, the Dance of Doris Humphrey by Marcia B.Siegel\n2from Weidman’s program note, quoted in Reclaiming Charles Weidman by Jonette Lancos\n3Soaring by Jane Sherman\nPhotos at Little Theater by Larry Hall\nWords by Nadira Hall\nPost by Julia Jurgilewicz"", 'The Paris Opera Ballet 2023-2024 season at the Palais Garnier and Opéra Bastille theatres is sure to satisfy both lovers of classical ballet and those who wish to see innovative 20th and 21st century choreography.\nThe full-length ballets – The Nutcracker, La Fille Mal Gardée, Don Quixote, Giselle, and Swan Lake – are joined by programs dedicated to Jerome Robbins and Jiří Kylián, as well as one that features women choreographers; Crystal Pite’s The Seasons’ Canon shares the evening with world premieres by Marion Motin and Xie Xin.\nHighlighting contemporary dance for the Company next season is Ohad Naharin’s Sadeh21.\nParis Opera Ballet 2023-2024 Season Trailer\nParis Opera Ballet 2023-2024 Season Schedule\nOpening Gala | September 21, 2023\n- The Seasons’ Canon by Crystal Pite\n- The Last Call by Marion Motin\n- Horizon by Xie Xin\nOn the stage of the Palais Garnier, the magnificent Défilé du Ballet presents the entire Company. The School’s students join the Étoiles adorned for the occasion in sumptuous tiaras and tutus designed by Chanel.\nThree female choreographers are honored in this Gala with a decidedly contemporary approach. Created in 2016, Crystal Pite’s The Seasons’ Canon is already a classic in the Opera’s repertoire. Marion Motin’s The Last Call and Xie Xin’s Horizon are both first creations for the Company’s dancers.\nXie Xin / Marion Motin / Crystal Pite | September 23 – October 12, 2023\n- The Seasons’ Canon by Crystal Pite\n- The Last Call by Marion Motin\n- Horizon by Xie Xin\nCrystal Pite’s The Seasons’ Canon, which premiered successfully in 2016 at the Paris Opera, is bathed in stormy light. Her overwhelming choreography in “canons” unleashes chain reactions and mirrored movements. Organically swarming human bodies merge with Vivaldi’s string music enhanced by Max Richter’s electronics.\nIn The Last Call, her first creation for the Opera Ballet, Marion Motin tells the story of a phone call that upends a man’s life. Between distortion and vitality, the choreography plunges the audience into a supernatural dimension.\nLastly, Chinese artist Xie Xin signs her first creation for the dancers of the Paris Opera. Her piece Horizon plays on illusions and mirages between natural elements.\nJerome Robbins | October 24 – November 10, 2023\n- En Sol by Jerome Robbins\n- In the Night by Jerome Robbins\n- The Concert by Jerome Robbins\nA fellow traveller of George Balanchine and choreographer of West Side Story, Jerome Robbins is a major figure of American neo classicism. Three of his works in the Paris Opera Ballet repertoire highlight his inimitable blend of lightheartedness and wit.\nSet to the jazzy tones of Maurice Ravel, En Sol is a knowing nod to Broadway musicals in a sunny beach setting.\nIn the lyrical setting of Chopin’s Nocturnes, In the Night places classical technique at the service of a moving narrative. Three couples embody three moments of love: discovery, blossoming and turmoil, against the backdrop of a starry night.\nSubtitled ‘The Perils of Everybody’, The Concert features characters who appear to have escaped from a cartoon strip in a series of expressive and humorous sketches.\nJiří Kylián Evening | December 8-31, 2023\n- Stepping Stones by Jiří Kylián\n- Petite Mort by Jiří Kylián\n- Sechs Tänze BY Jiří Kylián\nFilled with eerie images on the border of reality and dream, Jiří Kylián’s work blends oneirism and wry humour.\nSet to music by John Cage and Anton Webern, Stepping Stones pays tribute to the memory and heritage of dance. On stage, Egyptian cats are the guardians of tradition.\nPetite Mort poetically explores the double theme of death: the lesser one symbolizing orgasm and the greater one that brings life to a close. Pas de deux and swordplay alternate against a backdrop of Mozart concertos.\nTwo new pieces by Kylián enter the Ballet’s repertoire this season: Sechs Tänze extends the Mozartian universe and caricatures it in a series of dances with a quirky sense of humor, while Gods and Dogs explores the borderline between normality and madness: when dogs become gods and vice versa. In a midnight blue light, the dancers perform in front of delicate strings that recall those of a Beethoven quartet.\nThe Nutcracker | December 8 – January 1, 2024\n- The Nutcracker by Rudolf Nureyev\nRudolf Nureyev restaged The Nutcracker at the Paris Opera with sets and costumes emphasising the tale’s uncanny nature. Snowflakes, flowers and enchanted landscapes form the backdrop to a dazzling choreography. Guided by the wooden puppet who has become Prince Charming, the young Clara confronts her desires and anxieties in an initiatory tale.\nSadeh21 | February 7 – March 2, 2024\n- Sadeh21 by Ohad Naharin\nCrossing the breadth of the stage one by one and then together, the dancers develop Ohad Naharin’s fascinating body language in a streamlined set. From an initial abstract grey to the evocation of a sandy beach, the monochrome decor changes color over the course of 21 studies in movement.\nIn Hebrew, “sadeh” means “field”, in the sense of field of study or field of action. Here and there, a narrative thread accompanies the audience through a labyrinth of virtuoso motions. Sadeh21‘s movements – elastic, swift and unpredictable – create powerful and moving images.\nIn this piece created in 2011 and now entering the Paris Opera Ballet’s repertoire, Ohad Naharin continues to explore his body language, Gaga, in magnetic and surprising episodes where bodies first collide before dancing together to soaring music.\nLa Fille Mal Gardée | March 15 – April 1, 2024\n- La Fille Mal Gardée by Frederick Ashton\nIn 1960, the English choreographer Frederick Ashton’s virtuoso and humoros version of La Fille Mal Gardée set roosters, old ladies and umbrellas dancing. A gallery of irresistible characters perform to the sound of popular songs and opera buffa arias.\nA fine example of the “ballet d’action” theorised in 1760 by Jean-Georges Noverre, a choreographic genre that emphasizes expressiveness, La Fille Mal Gardée dazzles and entertains thanks to its sheer freshness. Bet it in the farmyard or the cornfield, the hearts of Lise and Colas search for and eventually find each other. In the manner of a musical, the original script, reworked by Ashton, carries us away with its whimsy and smiles.\nDon Quixote | March 21 – April 24, 2024\n- Don Quixote by Rudolf Nureyev\nInspired by Marius Petipa’s choreography, Rudolf Nureyev’s Don Quixote is a true celebration of dance with a Spanish flavor. The soloists and the corps de ballet are carried away in ensembles and pas de deux to the strains of a spirited score.\nWritten in the 17th century, Cervantes’ novel recounts the adventures of Don Quixote, an idealist and bookworm who one day decides to ride across Spain with the naive Sancho Panza. In Nureyev’s ballet they meet Kitri and Basilio. The two lovers use every trick in the book – from a puppet performance to a fake suicide – to be reunited, despite Kitri’s father’s resistance.\nIn the end it is Don Quixote who delivers the happy ending after battling windmills and crossing paths with Cupid, Dulcinea and the Queen of the Dryads. The costumes and colorful sets sublimate a vivacious and entertaining work.\nGiselle | May 2 – June 1, 2024\n- Giselle by Jean Coralli and Jules Perrot\nDiaphanous tutus, pointe shoes, white gauze, tulle: Giselle marks the pinnacle of romanticism. In a bucolic landscape, a young girl dies of love and is transformed into a spirit that haunts the forest.\nTaken in by the Wilis, she enters an ethereal world where dance is the language of the soul. Her lover Albrecht, distraught, pursues this ghost at the risk of his life. The ballerinas, with their aerial presence, defy him just as they do gravity. The mist-shrouded set reveals spectral visions enhanced by Adolphe Adam’s bewitching score.\nSwan Lake | June 21 – July 14, 2024\n- Swan Lake by Rudolf Nureyev\nRudolf Nureyev brought his own interpretation of Swan Lake to the Paris Opera in 1984. The encounter between a dreamy prince and a bird-woman continues to fascinate with its elegant choreography and poetic story.\nRudolf Nureyev’s striking psychological insight magnifies the love story between Siegfried and Odette, the white swan bullied by the sorcerer Rothbart and Odile, the black swan. Ezio Frigerio designed the set as an enclosed space, a mental space in which the prince gives free rein to his fantasies.\nBluebeard | June 22 – July 14, 2024\n- Bluebeard by Pina Baush\nCreated in 1977, Pina Bausch’s Bluebeard, which enters the Company’s repertoire this season, transforms Bartók’s opera into a wild and intense ritual: that of a man confronting his thirst for power, his desires and his fantasies.\nPerrault’s original fairy tale is the inspiration for this major Tanztheater piece. Men and women dive headlong into a choreography that exposes the violence and absurdity of human relationships. The compulsive nature of desire becomes a principle of writing: locked in a series of gestures repeated to the point of exhaustion or explosion, Pina Bausch’s tragic characters draw us into a breathless world where seduction and domination converge.\nFeatured Photo for the Paris Opera Ballet 2023-2024 Season of Valentine Colasante and Paul Marque in Don Quixote from the company’s website. Photo by Julien Benhamou.\nLeave a Reply']"	['<urn:uuid:63c3257d-744f-4d74-8fb0-205b365fecbd>', '<urn:uuid:7cd5a0d4-26c2-4005-b658-4f43b3e14913>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T04:31:27.499155	8	72	4253
91	how much water athabasca river tar sands	Approximately one million cubic metres of water is diverted from the Athabasca River to tar sands operations each day. Only 8 percent of the water removed from the river is returned, while 92 percent ends up in the tailings ponds.	['Last week, the Boreal Songbird Initiative, Pembina Institute and the Natural Resources Defence Council released a report describing the predicted impact of the tar sands on bird populations. The report, Danger in the Nursery, used modelling based on best current knowledge of bird populations in northeastern Alberta, combined with documented and estimated impacts of different elements of tar sands development and expansion on bird populations.\nThe picture is grim for many reasons. Impacts include:\n- direct lost of habitat to strip mining\n- settling ponds threat to migrants\n- fragmentation and destruction of habitat from deep drilling installations with their road and pipeline networks\n- air pollution from the operations and the production and refining processes\n- water withdrawal, diversions and contamination\nHow do the tar sands impact habitat?\nOne of the most common ways to extract the bitumen, the oil saturated sand and soil particles, is by stripping the vegetation, top soil and sub soils, draining the watercourses, and then scooping it out with giant machinery. 3,000 square kilometres of boreal forest will be strip mined in the next 30 to 50 years, based on current predictions. Strip mining destroys everything in its path. All the life-giving processes are removed. Soils are “stock-piled” as they are in more familiar residential housing developments. However, once stripped and piled, the vitality of the soil is destroyed.\nEfforts to reclaim mined lands and restore boreal forest fail miserably. The complex relationships between soil organisms such as bacteria, fungus, plants, invertebrates and larger fauna (including birds that are the hallmark of the boreal forest) are thousands of years in the making, but take only a few moments for the giant machines to destroy. This is the fate of habitat for up to 3.6 million birds!\nThe tailings ponds are created to store and ‘cap’ the residual waste product, after most of the oil has been removed from the bitumen. The residual is a toxic sludge that is pumped into artificial lakes, some several kilometres across, and ‘capped’ with clean water. These lakes will eventually cover about 100 square kilometres of area. They are death traps to birds landing in them, as was documented when 500 ducks died after landing in a Syncrude tailings pond in the spring of 2008. Annual mortality from tar sands to bird populations could be as high as 100,000 individuals!\nDeep drilling used to extract deeper bitumen deposits, requires a huge infrastructure of road networks, rigs, and pipelines and a reactor to produce steam. These typically burn natural gas, but there is much talk about using nuclear energy to produce steam, as is done for electrical generation. These operations and its infrastructure will destroy 5,000 square kilometres of boreal forest and result in significant fragmentation of a much larger area. These remaining fragments imbedded in the network of roads, pipelines and drilling rigs will be subject to excessive noise, dust, and pollution. Up to 14.5 million birds could be lost due to these activities!\nThe tar sands are by far the fastest growing source of greenhouse gases in Canada, producing as much as three times the amount of greenhouse gases as conventional oil production. In addition, production and refining operations produce huge emissions of toxins, from nitrogen oxides that acidify hundreds of square kilometres, to cadmium and arsenic that cause cancer. Many of these chemicals bioaccumulate in the food web, concentrating in predators such as birds, and ultimately impacting their reproductive success. Climate change is happening at a rate faster than wildlife can adapt, particularly in the north. For example, insect hatches on which so many species of migrating songbirds depend can be out of synch with migration timing.\nWater diversion and contamination\nApproximately one million cubic metres of water is diverted from the Athabasca River to tar sands operations each day. This water is used both in the tailings ponds and in the process to remove the oil from the soil particles. This is done by using steam, requiring vast amounts of water. The process uses approximately three times the water for every unit of oil produced. For the deep in situ extraction process, steam is injected into the ground to heat up the bitumen so that it can be pumped out. Tailings ponds are constructed in close proximity to the river, raising the potential for contamination of one of the Canada’s largest watersheds. Cancer rates in First Nations communities downstream from the tar sands operations have sky rocketed. Only 8 percent of the water removed from the river is returned. Ninety two percent ends up in the tailings ponds. The Athabasca watershed downstream is threatened, as the River is already under increasing stress from dropping water levels as the glaciers that feed into the Rocky mountains gradually retreat and sources diminish.\nBirds most at risk\nOf the 22 to 170 million birds that breed in the area that is and could be impacted by the tar sands, a large number of species are in trouble. Here are two very different examples, one big and one small.\nThe only natural population of Whooping Crane, a critically endangered species currently numbering around 400, is in Wood Buffalo National Park, directly northwest of the tar sands. The strip mines, forest fragments, and most ominously the 50 to 100 square kilometres of toxic tailing lakes which appear particularly inviting from the air, lie directly on their migration route. What are the chances over the next fifty years that a group of migrating Whooping Cranes drops out of the sky to take refuge from a storm in the toxic death traps below?\nOlive-sided Flycatcher was added to the official list of Canadian Species at Risk in 2007. The population of this exclusively insect eating bird has declined almost 80 percent in the last 40 years in North America. Most of its world population occurs in the Canadian boreal forest. Like many other boreal dependent species, it is being assaulted on many fronts, both on its breeding grounds, non-breeding grounds in the Amazon basin and Andean slopes of South America and during its extremely long migration in between.\nThe Olive-sided Flycatcher lives exclusively off flying insects, catching them in flight. The boreal forest in north eastern Alberta is an important area for this species. Loss of thousands of square kilometres of habitat will remove a chunk of its population. Climate change adds an additional stress. Climate change, particularly global warming, alters hatching dates for the insects, putting this important food source out of synch with the timing of bird migration. Climate change also leads to desiccating droughts and contributes to subtle changes in habitat that have not-so-subtle impacts. The tar sands are the biggest single contributor by far to greenhouse gases in Canada.\nIt is time to put a moratorium on the tar sands. It is also time to ask the Federal Government of Canada why it is not using the Migratory Bird Convention Act as an instrument to better protect boreal birds. This will be discussed in my next blog.\n(Photos: Evening Grosbeak, Jeff Nadler; Whooping Cranes, USFWS; Olive-sided Flycatcher, Mark Peck)']	['<urn:uuid:bbc769df-d50a-410f-b7cf-d91527ce4cab>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	7	40	1180
92	When was ZOTAC's gaming brand launched?	ZOTAC launched its gaming-specific brand, ZOTAC GAMING, in 2017.	['If you’re gaming, most of the parts within the PC shall be operating full power. But if you’re simply re-encoding a video, it will be primarily the CPU doing the work and most different parts will be idle therfore using little energy, so the CPU would be the most power hungry. Various specialty tools, corresponding to Torx bits, antistatic luggage and gloves, and built-in circuit pullers, can be used to restore and maintain computer systems. Always avoid magnetized instruments, such as screwdrivers with magnetic heads, or instruments that use extension magnets to retrieve small steel objects that are out of reach.\nFull Record Of Laptop Components\nMotherboards come in several sizes, and with a wide variety of sockets. Choose one that’s suitable with the processor and case that you want. SSDs use flash memory, which stores knowledge on MOS reminiscence chips consisting of floating-gate MOSFET memory cells. Some techniques could use a disk array controller for higher efficiency or reliability. More powerful graphics cards are higher suited to deal with strenuous duties, corresponding to enjoying intensive video video games or operating pc graphics software program. A video card contains a graphics processing unit and video memory , both fabricated on MOS built-in circuit chips. Also it depends on what kind of task the PC is doing, when most parts are idle they use less energy.\nAsrock B450m Pro4 Matx Motherboard $Sixty Five\nData that is entered into the computer have to be saved whereas it awaits processing by the CPU. It should even have someplace to go after it has been processed. The various kinds of laptop reminiscence shall be discussed in Sections 1.5 and 1.6. All of the digital and mechanical components of a microcomputer system are collectively often known as the hardware. A motherboard makes attainable the electrical connections through the opposite parts of the system can communicate. In different words, it holds and allows communication amongst several key digital elements of a system, such as the central processing unit and reminiscence, and provides connectors for different peripherals. A laptop hardware maker, ZOTAC makes a speciality of producing gaming PCs, mini PCs, motherboards, video playing cards and different equipment.\nFounded in 2006, the company launched its new gaming-particular model, ZOTAC GAMING, in 2017. Corsair makes a wide range of peripherals and elements that gamers use to construct PCs. This California-primarily based company can also be identified for offering excessive-efficiency reminiscence. In 2019, Corsair acquired Origin PC, which focuses on making pre-built gaming computers.\nUsing magnetic tools could cause loss of data on hard drives and floppy disks. Magnetic tools can also induce present, which may harm internal pc components. With as much as 14 teraflops of compute performance, 32GB of memory, and 1TB/s of reminiscence bandwidth, the MPX Module with Radeon Pro Vega II is a powerhouse. For extra power, two Radeon Pro Vega II GPUs mix to create the Vega II Duo. With double the graphics efficiency, reminiscence, and memory bandwidth, itâ€™s the worldâ€™s strongest graphics card. The two GPUs are linked through the Infinity Fabric Link, which permits knowledge switch as much as 5x faster between the GPUs.']	['<urn:uuid:ad99c0c6-8255-4d8f-bf64-452c2d43f852>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	6	9	522
93	what are main reforms tiberius gracchus tried to do in ancient rome land ownership	Tiberius Gracchus proposed significant agrarian reform in ancient Rome. His main proposal was to limit citizens to possessing no more than 500 iugera of public land (ager publicus) that was acquired during wars. Any excess land would be taken by the state and redistributed to poor and homeless families in small plots of about 30 iugera per family. He also attempted to use the wealth inherited from King Attalus III of Pergamum to fund his agrarian law, which directly challenged the Senate's power.	['The Gracchi, by Jean-Baptiste Claude Eugène Guillaume / Wikimedia Commons\nA law gave the Senate the power to raise mobs and declare anyone an enemy of the state and execute him without trial by a jury.\nBy Steven Fife / 01.18.2012\nTiberius and Gaius Gracchus were a pair of tribunes of the plebs from the 2nd Century BCE, who sought to introduce land reform and other populist legislation in ancient Rome. They were both members of the Populares, a group of politicians who appealed to the average citizens and that opposed the conservative Optimates in the Roman Senate. They have been deemed the founding fathers of both socialism and populism.\nTiberius Gracchus, born in 168 BCE, was the older of the Gracchi brothers. He is best known for his attempts to legislate agrarian reform and for his untimely death at the hands of the Senators. Under Tiberius’ proposal, no one citizen would be able to possess more than 500 iugera of public land (ager publicus) that was acquired during wars. Any excess land would be confiscated to the state and redistributed to the poor and homeless in small plots of about 30 iugera per family.\nThe Senate was resistant to agrarian reform because its members owned most of the land and it was the basis of their wealth. Therefore, Tiberius was very unpopular with the Senatorial elite. His main opponent was Marcus Octavius, another tribune who vetoed Tiberius’ bills from entering the Assembly and whom Tiberius had previously gotten removed from office.\nThe Curia. The meeting house of the Senate of Rome. The present building was begun by Julius Caesar in 44 BCE and later completed and dedicated by Augustus Caesar around 29 BCE. The building was rebuilt around 238 CE by Diocletian. / Photo by Chris Ludwig\nWhen King Attalus III of Pergamum died, he left his entire fortune to the people of Rome. Pergamum was one of the richest cities in the ancient world, and Tiberius wanted to use the wealth from Pergamum to find his agrarian law. This was a direct attack on Senatorial power and the Senate’s opposition to Tiberius began to increase.\nWith his term coming to an end, Tiberius sought re-election as tribune for the following year. This was unprecedented and his opponents claimed that it was illegal and Tiberius was trying to become a tyrant. On election, violence broke out in the Senate between Tiberius’ followers and his opponents. Tiberius was beaten to death with wooden chairs and nearly 300 of his supporters suffered the same fate. These deaths marked a turning point in Roman history and a long-lasting association between violence and the office of the tribune.\nTiberius was succeeded by his younger brother, Gaius Gracchus, who was also a social reformer. He was quaestor in 126 BCE and tribune of the plebs in 123 BCE. He is generally considered to be a more complex and confrontational figure than Tiberius, and he had a much clearer legislative agenda that extended beyond simple agrarian reform. Some of his laws appear to have been directed toward the people responsible for his brother’s death.\nDetail of the mosaic floor of the Curia. Just inside the doors of the Curia in the Forum Romanum. / Photo by Chris Ludwig\nHe renewed Tiberius’ land law and founded new colonies in Italy and Carthage. He introduced a law that no conscription of Romans under age 17 would be allowed and that the state would pay for basic military equipment. Previously, the soldier had to pay for his own equipment, which was especially difficult for the lowest census class. Like his brother, he also funded state-subsidized grain. Another law passed by Gaius imposed the death penalty on any judge who accepted a bribe to convict another Roman guilty.\nGaius’ opponents tried to win away his support and he lost popular appeal by 121 BCE. After a riot broke out on the Capitoline Hill and one of Gaius’ opponents was killed, the ‘ultimate decree of the Senate’ (Senatus consultum ultimum) was passed for the first time. This law gave the Senate the power to declare anyone an enemy of the state and execute him without trial by a jury. A mob was then raised to assassinate Gaius. Knowing that his own death was imminent, Gaius committed suicide on the Aventine Hill in 121 BCE. All of his reforms were undermined except for his grain laws. Three thousand of his supporters were subsequently arrested and put to death in the proscriptions that followed.\nThe tribunates of Tiberius and Gaius Gracchus began a turbulent period in Rome’s domestic politics, and their careers and untimely deaths emphasize both the strengths and the weaknesses of the tribunate. In the following decades, the tendency toward violence became even more clear as numerous tribunes saw their time in office come to an end with their deaths.\nOriginally published by the Ancient History Encyclopedia under a Creative Commons: Attribution-NonCommercial-ShareAlike 3.0 Unported license.']	['<urn:uuid:3af9dbab-ad98-405c-9d80-4c5b8dd6ebe2>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	14	83	825
94	What actions must tractor operators perform while working?	Tractor operators must perform several tasks including steering of tractor, looking backward to observe and control the machine/implement, and operating clutch, brake, and hydraulic control lever.	['STUDY OF TRACTOR VIBRATION AND ERGONOMIC DESIGN OF TRACTOR SEAT FOR OPERATORS COMFORT\n|Sr. No. |Content |Page No. | |1 |Objectives |1 | |2 |Introduction |2-4 | |3 |Reviews |5-10 | |4 |Vibration Studies |11-29 | |5 |Ergonomic Design of seat |30-51 | |6 |Summary |52 |\n1. To study the tractor vibration in context of discomfort to the operator\n2. Ergonomic design of the tractor seat for operators comfort.\nIn the past, human and animals were the only power sources available for almost all the agricultural works. In the modern age of agriculture, human and animal powers are replaced by mechanical power. Tractors, power tillers, and irrigation pumps are leading machineries for agricultural mechanization in the world as well as in the South East Asian region. Agricultural mechanization has changed the characteristics of labor in agriculture, has also influenced the workload. With the timeliness of operation and increased capacity has resulted the need for higher speeds, bigger and heavier machines. During the operation of these machines by human beings, the load on the operator as well as occupational hazards and diseases are found to be increased, which lead to impair the performance of the operator. In farm works, the fatigue and discomfort to which human beings are subjected is due not only to physical labor, but to vibration and noise as well\nThe problem of the vibration transmitted through the seat of agricultural tractors to the driver has been widely discussed over the last 40 years (Matthews, 1964a and 1964b). The tractor operator usually is exposed to a comprehensive magnitude of noise and vibration levels. With the constant need to improve the operator comfort and safety, a progress has been made in subsiding noise and the emphasis is being diverted to reduce the ride vibration levels. A large number of options may be considered to improve the ride of an agricultural tractor viz., suitable tyres, primary suspension at front and rear axles, cab suspension, and seat suspension. Large diameter and soft tyres reduce the forces transmitted in the bounce and pitch modes. This option is infeasible due to size limitation on the tyres\nThe nature of tasks on a tractor necessitates a number of actions to be performed by the operator, which puts varying physiological demands on the body. Examples of these tasks are steering of tractor, looking backward to observe and control the machine/implement, and force required to operate clutch, brake, and hydraulic control lever. The task and workplace determine the postures and create a pattern of loading on the structures of the body of the individual. The seat is one component affecting these loads. Tractor seat design can be used as a means to modify loads on the body structures to reduce operator’s discomfort. The tractor seat system must withstand the rigorous forces during different field operations and still maintain their dimensional stability, firmness and essential characteristic that enable the seat to retain its contoured shape\nVibrations experienced by the driver at the tractor seat lie especially in this vulnerable...']	['<urn:uuid:9ceab60f-b31d-4e5f-a3b0-f52e54795b13>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	8	26	503
95	chicago school model urbanization planning differences	The Chicago School model, which suggested cities grow outward in concentric zones from the urban core, held true in the early 20th century but has become more complicated recently. While some cities show vigorous downtown growth as predicted by the New York model, they also display outlying areas developing without clear patterns as in the Los Angeles model. Modern urban planning has evolved to include planned approaches like New Urbanism and the Garden City Movement, which emphasize pedestrian-centric, mixed-use communities and carefully balanced areas of residences, industry, and agriculture.	"['In the last hundred years, one of the most enduring models of urban development has been the iconic ""concentric zones"" map. Outlined by Chicago School sociologist Ernest Burgess, it was initially published in the classic 1925 volume The City, by Burgess and his University of Chicago colleague Robert Park.\nThis Chicago School model suggests that cities grow steadily outward from the urban core or central business district. Surrounding this commercial core is a ""zone in transition,"" with factories and warehouses. Beyond this comes the tenements and apartments of the working class, next the middle-class neighborhoods of larger homes, and ultimately the affluent commuter zones.\nHow well has this model -- or, really, any model -- held up over the past century of urban growth?\nIt\'s a question that animated Andrew A. Beveridge, a professor of sociology at Queens College. Beveridge used detailed, census tract-level data on changing population density since 1910 to determine whether our patterns match up to existing theories. In addition to the Chicago School model, he considered two others:\n- The Los Angeles model, based on the theories of urbanists from UCLA and USC, which argues that growth does not follow an orderly concentric pattern but, based on the experience of post-war L.A., occurs in a sprawling fashion, as a multiplicity of commercial, industrial and residential areas spread outward without noticeable pattern.\n- The New York school, which Beveridge associates with Jane Jacobs and William Whyte, suggests that most economically productive districts and the most desirable residential areas are concentrated in and around the city’s dense center; growth in the periphery is less patterned.\nUltimately, Beveridge\'s interesting analysis found that the basic Chicago School pattern held for the early part of the 20th century and even into the heyday of American post-war suburbanization. But more recently, the process and pattern of urban development has diverged in ways that confound this classic model.\nThe maps (below) from his study below contrast changes of density of these major metros for the earliest decade available – in Chicago and New York from 1910 to 1920, and in Los Angeles from 1940 to 1950.\nThe pattern of urban growth and decline has become more complicated in the past couple of decades as urban centers, including Chicago, have come back. ""When one looks at the actual spatial patterning of growth,"" Beveridge notes, ""one can find evidence that supports exponents of the Chicago, Los Angeles and New York schools of urban studies in various ways."" Many cities have vigorously growing downtowns, as the New York model would suggest, but outlying areas that are developing without any obvious pattern, as in the Los Angeles model.\nThe second set of maps (below) get at this, comparing Chicago in the decades 1910-20 and 1990-2000. In the first part of the twentieth century, decline was correlated with decline in adjacent downtown areas, shown here in grey. Similarly, growth was correlated with growth in more outlying suburbs, shown here in black. In the earlier period growth radiated outwards -- a close approximate of the Chicago school concentric zone model. But in the more recent map, growth and decline followed less clear patterns. Some growth concentrated downtown, while other areas outside the city continued to boom, in ways predicted more accurately by the New York and Los Angeles models. The islands of grey and black--which indicate geographic correlations of decline and growth, respectively--are far less systematic. As Beveridge writes, the 1990-2000 map shows very little patterning. There were ""areas of clustered high growth (both within the city and in the suburbs), as well as decline near growth, growth near decline, and decline near decline.""\nOn the on hand, the ongoing ""back to the city"" is bringing middle class people back to the core, and shifting poverty to the the suburbs, a process Alan Ehrenhalt dubs ""the great inversion."" But we are also seeing increasingly divided cities, and inequality that has not existed before. This is something I have explored in my series of posts on class-divided cities. Important studies by Robert Sampson and Patrick Sharkey note that as inequality has grown, and once high-paying manufacturing jobs have disappeared, our economy and labor market has divided. Those with more high paying knowledge jobs have clustered in and around the core, and a much larger number of low-wage service workers have been pushed to the outskirts of both urban and suburban knowledge zones.\nThe post-industrial city and metropolis is evolving as a patchwork of concentrated and persistent disadvantage alongside concentrated and increasingly self perpetuating advantage.\nTop Image: Ernest Burgess\'s Concentric Zones, first published in The City in 1925.', 'Urbanization is the increase over time in the population of cities in relation to the region\'s rural population. It has been the trend of many countries since the Industrial Revolution and continuing through the twentieth century, a trend that has shown few signs of slowing down. Although initially regarded as an advance in the quality of human life, as advances in technology, diversity of people, and cultural opportunities were abundant, problems quickly emerged. Without clear attempts to adapt the city to the population increase, urbanization may prove detrimental to the city\'s survival. Traffic congestion, increased pollution, limited real estate, and decreasing resources are all possible side effects of urbanization. The realization of these dangers has led to city planning that de-emphasizes the automobile and encourages walking, car pooling, or public transportation to reduce pollution. Movements, such as the New Urbanism movement, have shown that city architecture and construction can be a display of art, not just functional buildings. With this rise in urban artistic expression comes a greater cultural pride for living in the city—it no longer looks overpopulated, crowded, and stifling, and so city life becomes more attractive.\nAt the same time, the rise of computer technology, and particularly the internet, has resulted in an opposite trend, that of telecommuting, or working from home. With advances in communications technology, many people are able to work in a location of their choosing, often a rural area, in constant and close contact with their colleagues all over the world. Such advances herald possibilities of developing living environments that cater to all needs and interests, while allowing people to pursue their educational and career goals without geographical constraints. In such a world, urbanization can reach an equilibrium, with those who prefer to live in cities doing so, and many others choosing alternative locations.\nUrbanization is the growing number of people in a society living in urban areas, or cities. Urbanization means increased spatial scale and density of settlement as well as business and other activities in the area. Urban areas tend to attract businesses because of their large and dense population. This in turn draws more people to the area, working in a kind of circular process.\nUrbanization could occur as a result of natural expansion of the existing population, however most commonly it results from a large influx of people from outside.\nThe most striking impact of urbanization is the rapid change in the prevailing character of local areas. As agriculture, more traditional local services, and small-scale industry give way to modern industry, the urban area draws on the resources of an ever-widening area both for its own sustenance and goods to be traded or processed.\nLarger cities provide more specialized goods and services to the local market and surrounding areas, function as a transportation and wholesale hub for smaller places, and accumulate more capital, financial service provision, and an educated labor force, often concentrating administrative functions for the area in which they lie.\nAs cities develop, there can be a dramatic increase in rents, often pricing the local working class out of the market, including such functionaries as employees of the local municipalities:\nUrban development in our period [1789–1848] was a gigantic process of class segregation, which pushed the new labouring poor into great morasses of misery outside the centres of government and business and the newly specialised residential areas of the bourgeoisie. The almost universal European division into a \'good\' west end and a \'poor\' east end of large cities developed in this period.\nThis separation of the quality of housing into east and west sides is likely due the prevailing southwest wind, which carried coal smoke and other airborne pollutants downwind, making the western sides of towns preferable to the eastern ones.\nAround two thousand years ago, the world had less than 250,000 people, and cities exceeding over twenty thousand citizens were rare. Cities ranged from two thousand to twenty thousand up until the sixteenth century, when cities with populations climbing to and exceeding one hundred thousand began to spring up. From 1800 to 2000, the population climbed to six times its size, greatly increasing the numbers of urban inhabitants. In 1900, only a handful of cities had populations over one million. At the beginning of the twenty-first century, roughly half of the world\'s population lived in urban areas, with the number of cities of over one million inhabitants increased many times over compared to 1900.\nImportant cities in ancient times, such as Rome, had very large populations and developed infrastructures to support their needs. Thus, the Roman Empire built aquaducts to bring drinking water to the inhabitants. After the Industrial Revolution, great advances in technology drew people to cities.\nCities emerged from villages due to improvement in the cultivation, transportation, and preservation of food and other resources. The rise of the city broke down a mechanical way of life and led to an organic society: Cities were not closed to outsiders, and often many different types of people with new ideologies would come to live together within the same city. Cities developed an organized social core, where the entire community centered itself; villages often lacked this cohesiveness.\nThese early towns and cities were often quite small but densely populated. Distances were small enough that people could walk everywhere; particularly to a source of water. To protect the inhabitants from attacks, cities were often walled, limiting their ability to expand spatially despite increases in population. The elite lived in the center, close to the important buildings—government, religious, and so forth—while the poor lived nearer the edge, sometimes even outside the walls.\nThe variety of people and activities found in the cities became attractions that drew more and more people. Samuel Johnson, well-known for his statement, ""When a man is tired of London, he is tired of life; for there is in London all that life can afford,"" suggested that indeed ""A great city is, to be sure, the school for studying life.""\nHowever, a city often breaks the bonds human beings have with nature—in a city, one is surrounded by man-made structures and technologies, and the former connection with nature as a provider is severed. These processes are detailed in different stages of urbanization.\nThe first stage of urbanization was dependent upon the amount and productivity of the available agricultural land. Population increases had to be limited—more people could mean fewer resources. The second stage of urbanization was the development of sea-river transports and the creation of roads. This built on the first stage, but because trade and industry were developed, populations were no longer limited in their growth. The third stage, which is still currently in progress, is the shift in the economy to technological advances and population growth. This stage is set for an indeterminate amount of time, and is proving to change the interaction between urban dwellers and cities.\nUrbanization rates vary across the globe. The United States and United Kingdom have a far higher urbanization level than China, India, Swaziland, or Nigeria. Their annual urbanization rates are far slower, however, since a much smaller proportion of the population is still living in a rural area and in the process of moving to cities. Areas that have been affected by urbanization in these countries in more recent times include:\nFew cities have seen such a rapid population growth as Seoul in South Korea. Starting at a population of 900,000 in 1945, the population rose to over ten million by 1990. This urbanization boom brought increased revenue and economic prosperity for the city, but it also created new kinds of problems. Incineration plants and garbage dumps were constructed without consulting local residents, leading to angry residents and their migration from the area. Transportation systems have not been easy to coordinate, as competing transit systems have different bus routes and time tables. Construction also has played a role, as physically expanding a city requires heavy construction, which creates traffic congestion. The government of Seoul has found it essential to work closely with local authorities and citizens to manage these issues.\nAt the turn of the nineteenth century, Africa south of the Sahara had a total urban population of less than five percent, most opting for more traditional agricultural jobs. By 2000, the number of urban inhabitants reached nearly 38 percent, with an expected jump to over 45 percent by 2015. The growth of urbanization in Africa is slow, but it is steady.\nPredictions regarding Africa\'s urbanization have been inaccurate, however, and this is partially due to the AIDS epidemic, unexpected government coups, and wars between nations. Times of war have seen a strong rural-urban population flux. Nevertheless, the Nigerian city of Lagos which, in 1963, had 665,000 residents, jumped to nearly nine million residents in 2000, and is expected to reach 16 million residents by 2015, making it the eleventh largest city in the world. Urbanization is happening in Africa, just slower than originally anticipated.\nUrbanization can be planned or organic. Unplanned (organic) cities are the oldest form of urbanization and examples can be seen in many ancient cities. With exploration, however, came the collision of nations, which meant that many invaded cites took on the desired planned characteristics of their occupiers. Many ancient organic cities experienced redevelopment for military and economic purposes—new roads were carved through the cities, and new parcels of land were cordoned off serving various planned purposes giving cities distinctive geometric designs.\nPlanned urbanization, such as New Urbanism and the Garden City Movement, is based on an advance plan, which can be prepared for military, aesthetic, economic or urban design reasons. Generally, it is preferable to install urban infrastructure before urbanization occurs. landscape planners are responsible for landscape infrastructure (such as public parks, sustainable urban drainage systems, greenways) which can be planned before urbanization takes place, or afterward to revitalize an area and create a more pleasant living environment within a region.\nThe Garden City Movement is an approach to urban planning that was initiated in 1898 by Ebenezer Howard. Garden cities were to be planned, self-contained communities surrounded by greenbelts, and containing carefully balanced areas of residences, industry, and agriculture.\nInspired by Edward Bellamy\'s utopian novel Looking Backward, Howard organized the Garden City Association and founded two cities in England: Letchworth Garden City in 1903 and Welwyn Garden City in 1920. Both designs are durable successes and healthy communities today, although not a complete realization of Howard\'s ideals.\nThe idea of the garden city was influential in the United States (in Pittsburgh\'s Chatham Village; Sunnyside, Queens, New York City; Radburn, New Jersey; Jackson Heights, Queens; the Woodbourne neighborhood of Boston; Garden City, New York; and Baldwin Hills Village in Los Angeles) and in Canada (Walkerville, Ontario). The first German garden city, Hellerau, a suburb of Dresden, was founded in 1909. The concept was drawn upon for German worker housing built during the Weimar years, and again in England after World War II when the New Towns Act triggered the development of many new communities based on Howard\'s egalitarian vision. The garden city movement also influenced the British urbanist Sir Patrick Geddes in the planning of Tel Aviv, Israel. Contemporary town planning charters like New Urbanism and Principles of Intelligent Urbanism find their origins in this movement.\nNew urbanism was a movement in urban design which started in the late 1980s in the United States. The idea is to shift design focus from the car-centric development of suburbia and the business park, to concentrated pedestrian and transit-centric, walkable, mixed-use communities. New urbanism is an amalgamation of old-world design patterns merged with present day demands. It is a backlash to the age of suburban sprawl, which splintered communities, and isolated people from each other, as well as had severe environmental impacts. Concepts for new urbanism include bringing people and destinations into dense, vibrant communities, and decreasing dependency on vehicular transportation as the primary mode of transit.\nThe European Urban Renaissance, a movement stemming from American new urbanism, was unveiled in 1996. Many of the criteria for urbanism in Europe included revitalizing the city garden, healing the city, founding new traditional cities, urbanizing the suburbs, and constructing new traditional public buildings. The success of urbanism projects in Europe has led to new projects throughout the continent, some of which include re-inventing major cities to the standards of new urbanism.\nThe 2005 Revision of the UN World Urbanization Prospects report described the twentieth century as witnessing ""the rapid urbanization of the world’s population,"" as the global proportion of urban population rose dramatically from 13 percent (220 million) in 1900, to 29 percent (732 million) in 1950, to 49 percent (3.2 billion) in 2005. The same report projected that the figure was likely to rise to 60 percent (4.9 billion) by 2030.\nThe 2009 Revision World Urbanization Prospects confirmed that the level of world urbanization crossed the 50 percent mark in 2009. Nonetheless, in both Africa and Asia 60 percent of the population continued to live in rural areas. Population growth is projected to involve increasing the urban population in developing nations.\nBetween 2009 and 2050, the world population is expected to increase by 2.3 billion, passing from 6.8 billion to 9.1 billion. At the same time, the population living in urban areas is projected to gain 2.9 billion, passing from 3.4 billion in 2009 to 6.3 billion 2050. Thus, the urban areas of the world are expected to absorb all the population growth expected over the next four decades while at the same time drawing in some of the rural population. ... Furthermore, most of the population growth expected in urban areas will be concentrated in the cities and towns of the less developed regions.\nTraditional urbanization involves a concentration of human activities and settlements around the downtown area. When the residential area shifts outward, this is called suburbanization. A number of researchers and writers suggest that suburbanization has gone so far as to form new points of concentration outside the downtown. This networked, poly-centric form of concentration may be considered an emerging pattern of urbanization. Los Angeles is the best-known example of this type of urbanization.\nCounter-urbanization is the process whereby people move from urban areas to rural areas. It first took place as a reaction to inner-city deprivation and overcrowding. The process involves the moving of the population away from urban areas such as towns and cities to a new town, a new estate, a commuter town, or a village. The first two of these destinations were often encouraged by government schemes whereas the latter two were generally the choice of more middle class, socially mobile persons on their own prerogative. With the improvement of inner-city transportation infrastructure, and more sustainable public transport, people no longer have to live close to their work, and so can easily commute each day from more distant living areas.\nThe creation of the internet has impacted the way that people interact, work, and spend their leisure time. Office work and data entry is becoming dominated by internet protocol and programs, and so it is not uncommon to find employees working from their homes. This is seen as ideal for many—being able to work from the comfort of home while completing the same duties as one would at an office appears to be a desirable prospect. This type of work has come to be known as telecommuting.\nThe idea of telecommuting is to replace the commute to a work or business by the transfer of information from a computer to another computer—it brings the work to the worker. As well as being convenient for workers, this system has many beneficial results on society as a whole. For one, it cuts back on traffic congestion, since fewer commuters have to travel to work on a daily basis. This also decreases the amount of pollution in the city\'s air. A healthier environment benefits every person living in the area, increases the attractiveness of the city, and improves the quality of life for the population.\nAll links retrieved January 12, 2016.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nThe history of this article since it was imported to New World Encyclopedia:']"	['<urn:uuid:d0ef86ca-a0bb-42c7-b63e-3ba4ef20091b>', '<urn:uuid:6d6519d6-278d-4468-925d-b06a9386a02e>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T04:31:27.499155	6	89	3541
96	filtration methods oxygen role bacteria growth water purification efficiency	Oxygen plays a crucial role in aquarium filtration and water purification through multiple mechanisms. In aerobic bacterial filtration, bacteria require dissolved oxygen to break down organic waste products. These aerobic bacteria operate optimally in temperatures between 20-40°C and pH levels of 6.5-7.5. The oxidation process, performed by bacteria like Pseudomonas and Achromobacter, converts organic waste into carbon dioxide, water, and ammonium. Subsequently, nitrifying bacteria (Nitrosomonas and Nitrobacter) convert ammonium into nitrite and then nitrate. In mud bed filtration systems, some aquarists choose not to use protein skimmers to maintain beneficial particulate matter in the water, though oxygen levels must still be maintained through proper water flow and plant life.	"['Mud bed filtration is similar to that of other sand based aquarium filtration systems, however the substrate is made of such a small grain size that it is very much like mud. A good filtration method to equate a mud bed to is the refugium.\nIn the refugium filtration method various types of macro algae are grown in the fine sand. The mud bed filtration system is effectively the same, however instead of sand mud is used instead. It is not just normal mud though, it is a mud which is full of various elements and minerals. These elements and minerals are slowly released from the mud into the water column.\nA mud bed system is normally run in an aquarium or some other container located under the main display aquarium. Some aspect of mechanical filtration is required to remove any large particles from entering into the mud filtration area. This can be performed at the end of the overflow(s) by passing the water through very small chunks of live rock etc. After the water has been through the mechanical filtration it enters the mud filtration aquarium. The macro algae consume nutrients from the water and various elements are slowly released into the water from the mud bed. The water then normally passes through some type of grid or through another chamber full of small pieces of live rock the purpose of which is to prevent any of the macro algae from leaving the mud area and blocking the return pump. Once the water has passed into the pump area it is returned to the display aquarium.\nIn a mud based system the lights are normally left on for 24 hours a day. This allows the macro algae to photosynthesise and grow. It is only when macro algae grow that they consume nutrients from the water. It is best to use lighting which is designed for the growth of plant life. A couple of fluorescent tubes will suffice, ensure as said that they are for plant life and have a Kelvin rating of around 6000. Lights with this Kelvin rating have more colour in the yellow/red area which benefits the plants.\nA normal rule of thumb for a mud based filtration system is to pass in the region of 10 times the total water volume per hour. This needs to be considered when designing the aquarium system as the total amount of water in the system will determine both the overflow size and the size of return pump required.\nAs with a refugium there may be a requirement to harvest the macro algae if it becomes too dense. Never pull the algae out by the roots – instead cut the plants back with an old pair of scissors or similar. Harvesting the algae will allow more light to penetrate into the areas where the algae grows. One point to remember is that you should not add this macro algae back into the aquarium as you may reintroduce the nutrients back into the water.\nA mud based system combined with macro algae removes a lot of the nutrients (nitrate, phosphate etc) and dissolved organic compounds as well as replenishing trace elements.\nA lot of aquarists who utilise a mud based system for filtration do not run a protein skimmer. It is not recommended to do so by these aquarists because of the amount of particulate matter which is extracted from the water by protein skimmers. When viewing a mud based system you can actually see the fine particulate matter in the water. The water is still clear, however it is full of fine matter which some life in the aquarium can use for energy.\nOf course there are also aquarists who do choose to run a skimmer, however the majority of these run the skimmer part-time (i.e. throughout the night, turned on/off via the use of a timer). The majority of these aquarists decide to run a skimmer as they are very wary of turning it off. I have run a system using a mud based method combined with live rock for many years without problems. The only thing you need to ensure when you run a system like this is that detritus is removed from the main display aquarium regularly and that weekly water changes are performed. A lot of aquarists who do not run a skimmer on their mud based systems do not have a sand bed in the display aquarium due to the build up of detritus which can occur. Instead they go ‘bare bottom’ in the aquarium. The glass which is visible at the bottom of the aquarium quickly becomes covered in coralline algae as well as other types of life so looks more natural as time passes.\nThe aquarists who decide not to run a skimmer have reported a higher level of particulate matter visible in the aquarium water which the corals, and other filter feeders consume. Because of the amount of particulate matter in the water column you should see good polyp extension from your corals as well as hopefully having success in keeping some of the harder to keep corals. Obviously you cannot just expect to be able to keep these more difficult corals just because you are running a mud based system – you must still ensure that you have optimal water parameters and that the requirements for the livestock in question are met.\nImplementing a mud based filtration system is fairly straightforward. It is best to have an aquarium which is split into three or four sections. This can be accomplished yourself using glass and baffles or you can have one made for you at your local fish shop.\nThe first area is where the live rock is placed and is where the water from the display aquarium, via the overflow(s), enters the filtration aquarium. The purpose of this section is to remove any large detritus from the water as well as break up any air bubbles. Using live rock in this area is a good idea rather than another type of media as it is a natural filtration medium and will actively help in filtering the water rather than hindering it as other types of media could do.\nThe second section is where the mud is placed as well as the macro algae (caulerpa etc). The mud is poured into this section and the macro algae planted within it. To give the macro algae a chance to put its roots down trap it gently under a piece of live rock. Once the macro algae takes hold the piece of live rock can be removed. It is advisable to add a few varieties of macro algae as some may not take root. Caulerpa is one species which does tend to do well and there are numerous varieties available. Because of the amount of water flow which can be created in the central chamber some people choose to create small containers 1 inch or so high and 1-2 inches apart in the bottom of the chamber. This prevents the mud from moving around and building up in a pile at one end of the chamber.\nThe third or fourth section is where the return pump is located and is protected from the second central section via a baffle of some type or even more chunks of live rock contained in a chamber. If you are running this mud aquarium as your sump then your heaters for example can also go in this area.\nA couple of fluorescent tubes will suffice for the lighting above the mud section and should remain on for 24 hours per day. As said attempt to use tubes which are designed for plant life as this will help the macro algae grow.\nBecause of the mud releasing essential elements into the water the mud bed will eventually expire, therefore it is recommended that half of the mud bed is replaced about every two years however I would recommend that you rely upon the manufacturers recommendations in this area.', 'Biological Filtration: What Exactly Does That Imply?\nThe aim of biological purification is to keep the water, in which our\nsplendid fish and plants live, at a certain quality in which they feel\nthenselves pleasantly comfortable. Therefore in other words: to strive for a\nbiological equilibrium in our aquarium. The need of a good biological filter\nis in my eyes for that reason essential. It ensures that the arisen and\nproduced organic waste products, by plant and animal, are demolished on a\njustified manner and are converted into harmless inorganic products.\nThe operative organism in a bio filter (biological filter) are of course\nFor them the organic waste products in the water are the ""food"" on which\nFor this reason biological purification is also called bacteriological\nWe know two types of environments which support the several bacteria that\ntake care of the break down of organic waste products, namely:\naerobe and anaerobe environments.\nAnaerobe bacteria can demolish organic waste products without the use of\nThese are bacteria which we mainly find in different kinds of aquarium\nAerobe bacteria are bacteria which demolishes the organic waste products\nusing dissolved oxygen in water. These are the operative bacteria in a\nbiofilter such as we know it. This process is also called aerobe\nmineralisation or aerobe purification.\nSome vital factors for the activity\nof aerobe bacteria:\n- Temperature (between 10°- 45°C / 50°-113°F optimum range: 20°- 40°C /\n- pH value (between 4.0 - 9.5, optimum between 6.5 - 7.5).\n- Oxygen, dissolved in water.\n- Building material for growth of the bacteria, such as organic and\ninorganic compounds containing: phosphorus, carbon, sulphfur, nitrogen,\ncalcium, sodium as well as certain trace elements such as potassium, iron,\nmanganese, copper, and so on.\n- absence of toxic substances.\nThe reproduction rate of bacteria is very high.\nThe doubling time varies depending on the type of bacterium: from 20 minutes\nto 30 hours.\nThis process we can subdivide into several processes, namely: oxidation and\nIn oxidation, using oxidizing bacteria (Pseudomonas, Achromobacter,\nArthrobacter, etc.) and oxygen, the organic waste products are demolished\nand converted into simple inorganic end products, namely: carbon dioxide\n(CO2), water (H2O) and ammonium (NH4+).\nIn a new biological filter a culture of oxidizing bacteria arises within\na relatively short time, verified by the fact of how quickly ammonium in the\nwater can be measured.\nAt the nitrification step, using nitrifying bacteria (Nitrosomonas and\nNitrobacter) and oxygen, the arisen ammonium is converted into nitrite\n(NO2-) and later into nitrate (NO3-).\nThe nitrification reactions are as follows:\n2 NH4+ + 3 O2\nà 2 NO2-\n+ 2 H2O + 4 H+. ( nitrite formation by\n2 NO2- + O2 à 2 NO3-.\n( nitrate formation by Nitrobacter)\n2 NH4+ + 4 O2\nà 2 NO3-\n+ 2 H2O + 4 H+.\nThe production rate of the culture of nitrification bacteria in a bio\nfilter has as a rule a relatively longer duration then the oxidizing\nbacteria culture, but this is also strongly dependent on temperature. At\nthe temperatures used in our aquaculture, the dividing speed of nitrifying\nbacteria will be higher than under the normal, lower temperatures.\nEssential for a good aerobe purification is therefore, a sufficent level\nof oxygen in the water, since this oxygen will be consumed in the several\nFor this reason the presence of plants in the aquarium or a good oxygen\nabsorption by the water is of substantial importance.\nNaturally, our fish also needs, of course, to have oxygen available for\nThe presence of nitrates and the absence of ammonium or nitrites in the\nout put water of the bio filter indicates a total biological purification,\nwhich means all organic waste products are converted. For that reason we\nspeak of ""a complete or adult"" biological filter.\nTips regarding the arrangment of a\nAs a filter material use filling/material that has a very large surface\narea with large pore size.\nThis is because on this filter material the so called biological skin is\nformed, which consists of humus like substances, in which the bacteria are\nThe larger the surface, the more bacteria that can be present and the better\n& faster the purification can take place.\nThe filter material must:\n- Chemical and mechanically be stable, this means, does not pulverize.\n- Be biologically inert for infestation by bacteria, organic waste\nproducts and end products.\n- Offer binding possibilities for the biological skin.\n- Divides the fluid flow evenly.\n- It may not be too small of structure, because of the possiblity that\nthe ""bio filter bed"" would clog up, which would reduce capacity/volume.\nA way to prevent filter clogging is to direct the water flow in the\nfilter from top to bottom with the top most filter material being a ""finer""\nor small size and the bigger material being consigned to the lower part of\nAlso is it important in a biological filter to use a prefilter to catch\nthe particle pollution in the water before it enters the biological filter\nThe flow speed of the water through the biological filter may not be too\nfast but also not too slow. An applied rule is that on average the total\naquarium contents must pass through the filter within 1 hour.\nMake sure that the water, pumped back into the aquarium, can absorb as\nmuch oxygen as possible by means of a bubblestone or a sprinkler above the\nwater surface in the aquarium.']"	['<urn:uuid:e5ff7b48-cdbe-48ec-a455-0718ec6a0197>', '<urn:uuid:de701421-4950-4069-8d40-40684429f996>']	open-ended	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-13T04:31:27.499155	9	109	2241
97	How do you start recording network traffic with tcpdump?	You can record network traffic using the command: tcpdump -ni eth0 -s 0 -w networking.pcap, where eth0 is the name of the network device.	"[""- Table of contents\n- PCAP and protocol analysis\n- Recording and viewing A-bis communication\nPCAP and protocol analysis¶\nYou can take protocol traces of the communication between OpenBSC, OsmoBSC or OsmoNITB and your BTS. This includes the signalling between BTS and BSC, but also includes the signalling with all the subscribers/phones currently using the Osmocom network.\npcap is a data format for captured packets of communication protocols. It is used by a library called libpcap, which in turn is used by popular network protocol analyzer projects such as tcpdump and wireshark.\nIn the Ethernet/Internet world, you typically capture packets from your Ethernet card using RAW sockets and promiscuous mode.\nWith GSM protocols such as classic A-bis iver E1, it is obviously not that simple - since they are at least traditionally not transported over IP.\nRecording and viewing A-bis communication¶\nMethod 1: Using tcpdump (Abis over IP)¶\ntcpdump -ni eth0 -s 0 -w networking.pcap\nwhere eth0 is the name of the network device connected to the same network as the BTS.\nIf you would like to filter on only Abis traffic, make sure you capture only tcp ports 3002 and 3003, as well as 23000 for SGSN/Gb traffic.\nThe osmo-nitb application inside openbsc provides a command line option to automatically create a PCAP file.\nThis method is the standard method when using any E1/T1 based A-bis interface, such as mISDN or DAHDI.\nIf you're using the kernel-based mISDN LAPD implementation, the resulting dump is only a subset of what is actually transmitted over the wire. Currently only Link Access Protol D-Channel (LAPD) messages are logged, the actual LAPD header is spoofed and only the TEI and SAPI information is valid. This is mostly due mISDN not providing us with a LAPD header/frame and the encapsulation we use for wiretap/pcap.\nFor the libosmocore based userspace LAPD implementation (always for DAHDI, in mISDN optional), you will see the full LAPD header.\nTo write the protocol dump simply invoke osmo-nitb:\n./osmo-nitb -p networking.pcap\nMethod 3: Using misdn_log¶\nThis is the preferred method in case you are using the mISDN input driver for OpenBSC, e.g. with a BS-11 or other E1 based BTS.\nIn order to obtain a A-bis capture and save it in a pcap file, please use the misdn_log tool (part of mISDNuser)\nthe following way:\nmisdn_log -c0 -w networking.pcap\nPlease make sure to first start osmo-nitb and only then start misdn_log\nViewing / wireshark settings¶\nWireshark already provides dissectors for the various protocols we use (LAPD, RSL, GSM-A, GSM-SMS...).\nThe LAPD protocol dissector needs some minor configuration. Go to Edit -> Preferences -> Protocols -> LAPD and check the checkbox saying Use GSM Sapi Values. Afterwards wireshark will be able to display\nAbis over IP (gsm_ipa)¶\nA-bis OML (gsm_abis_oml)¶\nMake sure you are selecting the OML dialect that matches your BTS vendor/model. This can be done in the\nA-bis OML dialect to be used preference of the OML dissector. You have the following options:\n- ETSI/3GPP TS 12.21 (just those common parts that are specified by 3GPP)\n- Siemens (for BS-11 and other Siemens BTSs)\n- ip.access (for nanoBTS and OsmoBTS)\n- Ericsson OM2000 (for Ericsson RBS2000 + RBS6000)\nA-bis RSL (gsm_abis_rsl)¶\nIf you're using Abis/IP with nanoBTS or OsmoBTS, you should check the use nanoBTS definitions protocol preference setting. It enables decoding of vendor-specific messages and information elements, such as the IPA CRCX/MDCX/DLCX.\nDumps for you¶\nHere are some dumps that might be useful. Make sure that you only provide data from your own network and equipment (no IMSI/IMEI you do not know...)""]"	['<urn:uuid:776f3a86-f3db-4605-9a87-108786995750>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T04:31:27.499155	9	24	594
98	compare treatment historical war crimes documentation germany japan 1990s site development	In the 1990s, Germany and Japan took contrasting approaches to documenting their World War II history. While Germany established the Topography of Terror Foundation in 1992 to create a permanent museum documenting Nazi crimes, Japan saw a rightward shift where the government began denying war responsibility in history textbooks and conservative groups launched counter-attacks against those demanding accountability for wartime actions. The Topography of Terror site actively preserved evidence of atrocities like the Gestapo headquarters cellar, while in Japan there was increasing support for nationalist interpretations of the war period.	"['Resurgence of the Right and Japan\'s World War II Accountability\nby Chung Chinsung\nAt major intersections throughout Japan one can see the rightist groups with patriotic banners across their chests. They use relentless microphone-amplified diatribes to drive home their message as they stand atop their black vans. Their posters declaring ""Worried about the Nation"" has become a fixture of the Japanese urban landscape. After the Second World War such figures were considered comical. They seemed but hot-blooded nationalists; marginal crackpots far from the norm and with minimal influence. But before we knew it, the vague sense that perhaps their ravings contained a kernel of truth spread through Japanese society. Ironically, the far right has gained considerable power in Japanese society by insistently holding up the ideal of protecting the imperial system at the very time that civil society has achieved remarkable maturity in Asian nations.\nModernization was carried out under the auspices of the Japanese government in tandem with the war preparations so central to pre-war militarism. The trend throughout the world between 1910 and 1930 was towards socialism and labor activism. Yet such movements were ruthlessly suppressed by the government in Japan. Along the way, the rightist movement that had emerged within civil society was absorbed directly into the government.\nAfter the Second World War, democratic reforms implemented during the occupation led to the revival of the left. After a brief period of reform in the immediate post-war period, the previously progressive political course went into reverse. The outbreak of the Korean War and the ensuing conservative mood offered the opportunity for the right to retake the stage. Although the rightists came back to life, they often lacked credibility. Like many Japanese social movements, their ideological and intellectual foundation was weak.\nSocial activists and the student movement were transformed into the political presence known as the New Left after the Liberal Democratic Party came to power in 1955. Japan was swept by a wave of social action in the sixties and seventies. Notable among them were the citizen\'s movement, the anti-Vietnam war protest movement, the Women\'s Liberation Movement, and others. Yet they soon faded from prominence, proving how short-lived citizen\'s movements can be. Civil society in post-war Japan developed on the two legs of economic growth and political stability. In recent years the central movements that have emerged are concerned primarily with the pursuit of quality of life. Such powerful groups as the food cooperative movement, consumer organizations and the pre-school education movement are distinctive in their ability to draw members from both the left and the right. These movements promote an improved and healthier life and consequently have a broad appeal. Consequently, the discourse of civil society in Japan has lost its ideological and theoretical dimension.\nThis situation started to shift again in the early 1990s when a series of international controversies rocked Japan. Korea led the nations of East Asia victimized by Japan during World War II, demanding recognition of, and settlement for, Japan\'s historical crimes. For example, the campaign to receive apology and reparations for the enslavement of Koreans as military comfort women grew into a Pan-Asian movement. Japan\'s ethical standing was seriously shaken by these charges and the climate at home rapidly became more supportive of domestic social movements. Progressive political movements that had just managed to scrape by as small-scale study groups before now teamed up with activist citizens\' groups from other Asian nations. They demanded a full accounting of Japan\'s wartime responsibility. The social engagement thus generated was unique for post-war Japanese society in that women\'s groups, organizations of ethnic Koreans, peace groups, intellectuals and lawyers all joined in the campaign. They were tied together by a common thread: criticism of the imperial system.\nConservative groups launched a determined counter attack. With vast resources at their disposal, the rightists were able to bring together politicians, intellectuals and youths for their purposes. These neo-nationalist groups held that Japan was fully justified in its actions during the war. The right denigrated the citizens\' groups and victims who came forth from other nations in East Asia and branded as treasonous the statements of domestic critics. Japanese political discourse lurched to the right. Although a healthy Japanese society had developed in the post-war period, ultimately, the lack of ideological foundations for the new order made possible the resurgence of the right.\nIn the early nineties the forces of the left and right counterbalanced each other in Japanese society. Recently, the balance has tipped increasingly in favor of the right. The rightists sharpened their rhetorical weapons in battles to suppress the demands made by former comfort women. Rightists also demonstrated their prowess in the battle over history textbooks. In the 1980s and early 1990s, the Japanese government had acknowledged certain controversial actions in World War II. Revisions appeared in government-approved history textbooks. In 1994-95, however, textbook policy took a turn to the right.\nThe government has denied Japanese responsibility for the Second World War in recent editions of some history textbooks and restored nationalist content. In addition, the obligatory singing of the newly approved national anthem ""Kimigayo"" in praise of the emperor, and raising the Hinomaru flag are required by law and rigorously enforced in formal events at many schools. A string of new laws, starting with the Emergency Security Law, has been implemented and serious debate concerning revision of Japan\'s Peace Constitution has ensued. As a result, rightist social movements have spread their wings and flown to every corner of the country. They carry out campaigns such as the demand for the revision of history textbooks and violently attack Japan\'s tradition of peaceful engagement with the world. Starting with the prime minister, a long line of officials now pays public visits to the Shinto holy place Yasukuni Shrine. The leaders and martyrs associated with Japan\'s conquest of Asia during the Pacific War are interred and honored there. Adjacent to the Yasukuni Shrine is the Yushukan Hall, a museum featuring exhibitions for the education of Japanese visitors about the ""glorious Japanese race"" and the new educational system.\nAt the same time, many citizens\' groups that demand accountability for Japan\'s wartime actions have turned their backs on issues such as the punishment of those responsible for the crimes of World War II and the legitimacy of the imperial system. Participants in citizens\' movements nowadays are often passive figures who have given up hope of successfully combating the rightists, but continue to find a certain personal meaning in ethical activity.\nThe long-term stagnation of the Japanese economy has nourished a mentality receptive to the arguments offered by the rightists since the 1990s. Japan has witnessed the demise of its legendary system of life-time employment. Foreign workers have entered Japan in growing numbers. The social disruptions resulting from such cataclysmic changes have proven difficult for most Japanese to absorb. At the same time, neighboring China is aiming the heavy cannons of diplomatic warfare at Japan over unresolved historical issues. The regional configuration of East Asia today demands a new role of Japan. Political conservatism and economic development can no longer simply work hand and hand without friction. Similarly, social movements in Japan cannot remain secondary activities subordinate to larger political priorities.\nIt is not immediately obvious whether the groups that once took such pride in their power such as the food cooperative movement or volunteer organizations will recognize their complicity in the swing to the right, or whether it is possible to have critical intellectuals in Japan.\nThis article appeared in Hankuk Ilbo on May 3, 2004. Chung Chinsung is Professor of Sociology, Seoul National University. In August 2004 she was appointed to a post at the United Nations Human Rights Commission.\nTranslated from Korean for Japan Focus by Emanuel Pastreich. Emanuel Pastreich teaches in the Department of East Asian Languages and Cultures at the University of Ilinois. His recent publications include work on the internet and globalization.', 'Topography of Terror\nThe Topography of Terror (German: Topographie des Terrors) is an outdoor and indoor history museum in Berlin, Germany. It is located on Niederkirchnerstrasse, formerly Prinz-Albrecht-Strasse, on the site of buildings, which during the Nazi regime from 1933 to 1945 was the SS Reich Security Main Office, the headquarters of the Sicherheitspolizei, SD, Einsatzgruppen and Gestapo.\nThe buildings that housed the Gestapo and SS headquarters were largely destroyed by Allied bombing during early 1945 and the ruins demolished after the war. The boundary between the American and Soviet zones of occupation in Berlin ran along the Prinz-Albrecht-Strasse, so the street soon became a fortified boundary, and the Berlin Wall ran along the south side of the street, renamed Niederkirchnerstrasse, from 1961 to 1989. The wall here was never demolished. The section adjacent to the Topography of Terror site is the longest extant segment of the outer wall, as the longer East Side Gallery section in Friedrichshain was part of the inner wall, not visible from West Berlin.\nThe first exhibitions of the site took place in 1987, as part of Berlin\'s 750th anniversary. The cellar of the Gestapo headquarters, where many political prisoners were tortured and executed, was found and excavated. The site was then turned into a memorial and museum, in the open air but protected from the elements by a canopy, detailing the history of repression under the Nazis. The excavation took place in cooperation with East German researchers, and a joint exhibition was shown both at the site and in East Germany in 1989.\nIn 1992, two years after German reunification, a foundation was established to take care of the site, and the following year, it initiated an architectural competition to design a permanent museum. A design by architect Peter Zumthor was chosen. However, construction was stopped due to funding problems after the concrete core of the structure had been built. This stood on the site for nearly a decade until it was finally demolished in 2004 and a new building begun.\nThe construction of the new Documentation Center according to a prize-winning design by the architect Ursula Wilms (Heinle, Wischer und Partner, Berlin) and the landscape architect Heinz W. Hallmann (Aachen) was finished in 2010. The new Documentation Center was officially opened on 6 May 2010 by Federal President Horst Köhler on the occasion of the 65th anniversary of the end of World War II. The new exhibition and documentation building and the redesigned historic grounds were opened to the public on 7 May 2010.\nAfter the demolition of the ruins in the 1950s, the area was used as a bumper car site and a dumping ground for rubble from the renovation of Kreuzberg. The plans for a memorial site on the former site of the Gestapo goes back to 1978, when Berlin architecture critic Dieter Hoffmann-Axthelm was one of the first to note, in essays and surveys, the significance of the former site of the Gestapo, SD and RSHA headquarters.\nThe first exhibition on the site\'s history was created for the 750th anniversary of Berlin in 1987. The research continued after it, leading to a documentation centre that collected some more evidence for the terror of the National Socialists in Germany. In 1992, a foundation was created for the construction and maintenance of the centre with an associated permanent exhibition. The managing director is Rabbi Andreas Nachama.\nA tender in 1993 to design the museum complex was won by the Pritzker Prize-winning Swiss architect Peter Zumthor. Based on the temporary exhibition building, his design was likened to the skeleton of a barracks, allowing light through the glazed gaps in the concrete beams. Although critically acclaimed, the structure proved expensive to build and when the original contractor became insolvent in the middle of construction, no other contractor willing to continue the project for the fixed fee could be found. With the city of Berlin unwilling to pay an additional three to five million Euros for a reduced design and funding from the federal government delayed until more progress was achieved, the site was left with just the concrete stairwells of the design. Having spent 13.9 million Euros already, these were demolished, despite the protests of Zumthor and other architects, in 2004.\nArchitectural design competition 2005\nIn June 2005 a new architectural design competition was launched following the aborted partial construction of Zumthor\'s design. Out of 309 submitted and 23 chosen drafts, architect Ursula Wilms from the Berlin architects office Heinle, Wischer and Partner and landscape architect Heinz W. Hallmann from Aachen won in January 2006 the final round. The draft included a two-storey, ashlar-formed, paned building with an available surface of 3,500 square metres. For the construction around €15 million was available. Another five to nine million Euro was used for the interior and the redevelopment of the historical site. These costs were defrayed jointly by both the federal government and the federal state of Berlin, each contributing 50%. The architects estimated construction costs at a maximum of €20 million and a construction period of two years.\nThe construction was finished on time and the new building was opened to the public on 7 May 2010.\nThe open-air exhibition in the trench alongside the excavated segments of cellar wall on Niederkirchnerstraße (formerly Prinz-Albrecht-Straße) was retained and sheltered with glass. The room for the permanent exhibition is 800 cubic metres and presents the development and functions of the security apparatuses during the Nazi regime. A room for events at the back of the building can accommodate 200 participants. In the southern part of the area outside is a copse of robinias, the remains of ""Harrys Autodrom"" from the 1970s, whereas the rest of the open space is covered with greywacke. Around the flat-roofed building is a façade made of metal lamellae, which opens the building in a way that it is possible to look out of it to the surroundings anywhere on the ground floor of the building. In the basement is the seminar centre, the library with about 25,000 volumes, the memorial department and offices for 17 employees of the Topography of Terror Foundation.\nWith the inauguration of the new Documentation Center, three permanent exhibitions are open to the public. All three are presented bilingually in German and English.\nTopography of Terror. Gestapo, SS, and Reich Security Main Office on Wilhelm- and Prinz-Albrecht-Straße\nThe ""Topography of Terror"" permanent exhibition was shown in the open air until the new documentation center opened. The thoroughly revised and redesigned ""Topography of Terror"" permanent exhibition is presented over 800 square meters in the new building. The focus of the exhibition is the central institutions of the SS and police in the ""Third Reich"" as well as the crimes they perpetrated throughout Europe. Attention to the Nazi regime\'s many victim groups will assume a central place alongside the portrayal of the system of terror.\nBerlin 1933–1945 Between Propaganda and Terror\nA permanent exhibition about the capital Berlin during the ""Third Reich"" will be on display in the exhibition trench alongside the excavated segments of cellar wall on Niederkirchnerstraße (formerly Prinz-Albrecht-Straße). It will address National Socialist policy in Berlin and its consequences for the city and its population.\nTopography of Terror Site Tour. The History of the Site\nWith the opening of the new Documentation Center, the grounds of the ""Topography of Terror"" are once again completely open to the public. The site tour, which mainly follows the exposed building remnants, encompasses 15 stations. Informational signs provide an overview of the historic location and the site\'s use during the Nazi period and the postwar era. The tour also integrates remains of the Berlin Wall, which have been designated a historic monument.\nSpecial and temporary exhibitions\nThe Face of the Ghetto. Pictures taken by Jewish Photographers in the Litzmannstadt Ghetto 1940–1944\nThis special exhibition will be presented in the Topography of Terror Documentation Centre from 23 June 2010 on. It was developed by Dr. Ingo Loose and Dr. Thomas Lutz in cooperation with the State Archive in Łódź.\nThe ""House Prison"" at Gestapo Headquarters in Berlin. Terror and Resistance 1933–1945\nA bilingual German-English exhibition on the ""House Prison"" at the Gestapo Headquarters was shown in a special open-air exhibition area and included the \'ground memorial\' including remains of former basement prison cells.\nWith altogether 400 photos and documents, for the first time the exhibition comprehensively related the history of the prison at Prinz-Albrecht-Straße 8 and reminded the fate of numerous detainees.\nThis presentation lasted from August 2005 to April 2008 on the site of the \'Topography of Terror\'.\nThe Trial of Major War Criminals in Nuremberg\nThis exhibition was presented on the occasion of the 60th anniversary of the Nuremberg Trials and comprised around 110 photo and 50 text documents as well as 15 audio stations. It outlined the genesis, process, ambition and importance of the trial led by the Allies at Nuremberg focussing on the accused, whose culpability for the war crimes is demonstrated.\nThe presentation was located on the construction hoarding at the area of the Topography of Terror from October 2005 to April 2007.\nThe ""People\'s Court"" - Hitler\'s Political Tribunal\nGerman-English documentation on occasion of the 70th anniversary of the foundation of the people\'s court.\nThe exhibition was developed in cooperation with the Memorial to the German Resistance.\nFire! Anti-Jewish Terror on ""Kristallnacht"" in November 1938\nThe exhibition was developed in cooperation with the Foundation Memorial to the Murdered Jews of Europe and the Stiftung Neue Synagoge - Centrum Judaicum. The cooperative project presented on the 70th anniversary of the Kristallnacht pogrom presents historical documentation of the attack, seen around the world, on German Jewry after five and a half years of Nazi dictatorship.\nThe presentation was displayed from November 2008 to March 2009 in the Centrum Judaicum in Berlin.\nThe library of the Topography of Terror Foundation is a special library focusing on the police, SS, Gestapo in the Third Reich and on the National Socialism in general. It currently comprises about 25 800 media elements, about 120 regularly and 100 closed magazines. It is situated around a fountain reminding of Zen gardens and freely accessible.\nMemorial Museums Department\nThe Topography of Terror Foundation provides comprehensive advice and coordination tasks in the field of national and international memorial sites. In Germany, the Memorial Museums Department is the central coordination office for memorial sites and initiatives for memorial sites and increasingly promotes the international collaboration.\nDocumentation Centre NS Forced Labor\nThe last well-preserved former Nazi forced labour camp is located in Niederschöneweide. In the Second World War it served as one of the more than 3000 collective accommodations dispersed throughout the city for forced labourers. The Documentation Centre on Nazi Forced Labor opened in the summer of 2006 on a part of historical grounds that once belonged to the camp and which are today protected as a monument. The Documentation Centre offers two permanent exhibitions: ""Forced Labour in the Daily Round 1938-1945"" and ""Between two stools. The History of the Italian Military Internees 1943-1945"". Entrance and guided tours are free.\n- Lucarelli, Fosco: Zumthor’s Topographie des Terrors (1993–2004): visual history of birth, growth and death of a project Archived 2015-04-02 at the Wayback Machine, 14 November 2011\n- Topographie des Terrors. ""Köhler weiht Dokumentationszentrum in Berlin ein"" stern.de, 6 May 2010 (in German)\n- Topography of Terror. Library Topography of Terror Foundation, 26 May 2010\n- Publisher: Topography of Terror Foundation, represented by Prof. Dr. Andreas Nachama: Topography of Terror. Gestapo, SS and Reich Security Main Office on Wilhelm- and Prinz-Albrecht-Straße. A Documentation 1. edition. Berlin 2010, ISBN 978-3-941772-07-6.\n- Publisher: Topography of Terror Foundation, represented by Prof. Dr. Andreas Nachama: Site Tour ""Topography of Terror"". History of the Site 1. edition. Berlin 2010, ISBN 978-3-941772-05-2.\n- Schaltzentrale der Hölle. Was passiert mit der ""Topographie des Terrors"" in Berlin? Documentary, Germany, 2004, 7\'08 Min., ZDF-aspekte, 20 July 2004\n- Dokumentationen des Terrors. News programme, Germany, 2007, 1\'52 Min., Production: ZDF-heute, first run: 2 November 2007\n|Wikimedia Commons has media related to Topographie des Terrors.|\n- Official site\n- Nazi Forced Labour Documentation Centre\n- ""Topography of Terror"". Exberliner. nd. Archived from the original on 17 January 2010.\n- ""Nazi control room reopens as Topography of Terror museum in Berlin"". The Guardian. 6 May 2010.']"	['<urn:uuid:b5ca95ca-e0c7-41ba-86f1-5f51af4cc3dd>', '<urn:uuid:724c3bc5-b4ee-4a7f-ba46-7e89d5aefaed>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-13T04:31:27.499155	11	90	3364
99	What tools does a Mobile Fab Lab include, and how should noise be managed?	Mobile Fab Labs include multiple tools such as 3D printers (both large and small), laser cutters with fume extractors, portable CNC routers, laptops, and various hands-on making tools. Regarding noise management, it's important to consider that CNC systems are typically the noisiest, with three separate noise-generating components: the spindle, vacuum table, and dust collector. To minimize noise disruption, louder machines should be placed against exterior walls when possible, and enclosed systems can help keep noise levels down. Separation of spaces using collapsible walls can also help control noise, especially in areas designated for computer design and teaching.	['Carnegie Science Center offers convenient and affordable packages to help you purchase and implement your own Mobile Fab Lab, complete with equipment, training, support, and curriculum.\nBring 3D printers, laser cutters, and digital fabrication technologies to students, educators, and families anywhere with a Mobile Fab Lab. Set up a makerspace in a classroom, a gymnasium, a cafeteria, or even outdoors for a day, a week, or just a few hours. The Mobile Fab Lab comes equipped with everything needed to inspire future engineers, scientists, and technologists to innovate ideas and solutions to hands-on challenges.\nUtilize a Mobile Fab Lab to offer STEM activities, hands-on programming, and engineering challenges for learners of all levels with the equipment, computers, and curriculum designed by Fab Lab Carnegie Science Center.\nSupport STEM learning at schools with engaging activities that encourage creativity, collaboration, communication, problem solving, and critical thinking. School administrators embrace the Mobile Fab Lab experience because it eliminates the need to transport students offsite, and students love it because it is fun!\nThe Mobile Fab Lab can support school curriculum with standards-aligned lesson plans, can integrate into existing project based learning modules, and can also draw a crowd at a school-wide STEAM night!\nThe Mobile Fab Lab van is wrapped with your organization’s logo and branding, making the van itself an important part of your outreach strategy. Your Mobile Fab Lab can become a revenue generating program as funders recognize the ability of the Mobile Fab Lab to visit rural or urban schools with little access to traditional makerspaces.\nDigital fabrication is about more than just the tools and technology, so to help kick-start your mobile making program, check out Carnegie Science Center’s Mobile Fab Lab Packages© which include intensive training, materials, curriculum, operational support and coaching, and over 8 days of in-person training.\nCall Jonathan Doctorick at 412.802.2146 or email firstname.lastname@example.org.\n|3D printers (large)||2||2|\n|3D printers (small)||10|\n|Laser cutter and custom cart||2||1|\n|Laser cutter fume extractor||2||1|\n|Portable CNC router||1||1|\n|Laptops and mice||24||18|\n|40” flat-screen TV||1|\n|3D printing pens and filament||8||4|\n|3D printer filament||✔||✔|\n|Hands-on making tools||✔||✔|\n|Cordless drills and bits||✔||✔|\n|12-pack laptop cases||2||2|\n|Consumables for laser cutter||✔||✔|\n|Tactile-assisted design kit||2||1|\n|12’ ramp for loading/unloading||1||1|\n|Organizer racks for van||✔|\n|DuroMax 8000-watt generator||✔|\n|Retractable, portable banner signs||2||1|\n|Mobile Fab Lab Intensive||✔||✔|\n|Van delivery and training||✔||✔|\n|Consultation services||2 years||1 year|\n|Onsite professional development||2||1|\n|Total days of training||11||9|\nTwo staff members travel to Carnegie Science Center* and train onsite for Mobile Fab Lab programming, including:\n*Travel costs not included in Mobile Fab Lab Van Package©\nCarnegie Science Center staff delivers finished van to location:\nPhone calls, emails, texts, video conferencing, on-going/on-demand communication to support daily lab operations and answer any questions.\nCarnegie Science Center’s expert Fab Lab team will design and deliver specific training for your organization based on your needs within your first year of operation. Carnegie Science Center staff will travel to your location and train up to 10 people on your Mobile Fab Lab equipment.\nFab Lab education facilitators at Carnegie Science Center have developed very successful Mobile Fab Lab Van Packages© complete with lesson plans, presentations, and digital templates to get you started delivering engaging programming right away.\nIn these short lessons, students will design and create their own project using digital fabrication technologies, learning 2D and 3D design principals with a 1:1 computer/student ratio.\nIn these lessons, students work in teams to problem-solve, design, prototype, and improve their solutions to solve a challenge using digital fabrication technologies.\nThese lengthy projects involve collaboration, creativity, and critical thinking to define and solve a challenge and can be approached individually or as a team. These projects are more student-centered and can take 6 hours to a week of class time to complete.', 'When planning a makerspace, most think about designing for creativity. It’s important to think about the types of projects that will be completed in the space, how many people will use the space at any given time, and the equipment on your must-have versus nice-to-have lists.\nHowever, there are also a few considerations that often go overlooked.\nSpace Utilization – Design, Fabrication, or Both?\nIt’s important to consider where lab users will be designing versus making. Will everything take place in the same space or will you have a separate design studio down the hall? Will your design computers be laptops or desktops?\nA separate design space should be close to the makerspace in order to minimize lengthy trips back and forth. If that is not possible, consider laptops that users can bring with them into the lab to make design changes quickly.\nFor educators, it’s also important to consider if you will be teaching in the design lab, the makerspace, or both. If you plan to lecture in the makerspace, consider the schedules of other classes that may use the space to minimize or avoid trying to lecture over the noise of the equipment.\nKey Takeaway: How you plan to use the space will determine your spatial flow.\nDirty vs. Clean\nMost makerspaces have two different types of equipment – dirty (makes a lot of dust and debris) and relatively clean. “Dirty” equipment often includes drill presses, table saws, other manual tools, and CNC lathes, mills, and routers. “Clean” equipment includes 3D printers, 3D scanners, laser cutters, vinyl cutters, vacuum formers, injection molders, and computers.\nIn an ideal situation, a makerspace has a designated dirty space and a designated clean space that are separated at the very least with a collapsible wall. Without separation, you risk dust and debris infiltrating expensive equipment such as laser tubes in laser systems or electronics bays of 3D printers.\nIf separation is not possible, it’s important to consider different types of equipment. For example, a smaller enclosed CNC machine may be a better option than a large format table CNC router because most of the dust is contained within the machine.\nKey Takeaway: Separate dirty from clean equipment if possible. If not, consider alternative equipment options.\nAnother reason to separate spaces is for noise control. This is especially common in labs that also feature designated computer design space and teaching space all within one lab. CNC systems will likely be the noisiest systems in the space. Many large format CNC systems have three separate noise generating components – the spindle, the vacuum table, and the dust collector. Find out how loud the system will be as a whole before purchasing.\nIf noise is a big concern, an enclosed system may be the answer. An enclosed machine will help keep noise levels down while lab users are concentrating on their designs.\nIt’s also important to consider the location of a lab within your building. If there are classrooms or workspaces nearby, consider their locations when choosing the layout of your space. In these cases, it’s usually recommended to place louder machines against exterior walls if possible.\nKey Takeaway: It’s easy to minimize noise disruptions with a little planning in advance.\nPower and Electrical Requirements\nOne of the most common mistakes to make when planning a makerspace is assuming all equipment will run off of standard 120V electricity.\nFor instance, some large format routers require 3 Phase power for a high frequency spindle and vacuum table pump. Many dust collectors require 240V. These power requirements, along with any other facility requirements, will typically be listed on equipment datasheets, so make sure you read these datasheets in detail.\nIf specialized power is required, contact an electrician to find out what it will cost to have that power installed. It’s important to finalize your lab layout as you work with an electrician because once specialized power is installed, it is difficult and expensive to rearrange it.\nMakerspaces that don’t allow for anything beyond standard 120V everywhere will need to be careful when selecting equipment to ensure there aren’t any surprises when the equipment arrives.\nKey Takeaway: Read equipment datasheets in detail to understand power requirements. Finalize your lab layout after working with an electrician.\nOften workbenches and storage solutions don’t get the attention they deserve. Furniture isn’t as exciting as equipment, but it is a critical part of the overall usability of the space.\nFurniture is not a place to skimp by building it yourself or using extra furniture from around the facility. This often results in a space filled with furniture that isn’t appropriate. Users may end up working hunched over because tables are too short or an expensive piece of equipment may be set on top of workbench that is not rated for its weight.\nStorage is another big concern. Keeping your lab space clean and organized not only makes it easier to work in, but also makes it safer. Know what you’ll need to store, how big it is, and how much of it you’ll need to have on hand at any given time.\nAnother common mistake when it comes to furniture is basing the decision on aesthetics over functionality. Everyone wants their space to look modern and attractive, but your decision criteria should be weighted towards durability.\nTo pick the correct furniture, think about the type of work that will be completed in the space. Will your users be using hand tools on your workbenches? Will they be soldering electronics? Will these activities take place while standing or seated? How will these workbenches hold up to these types of activities in the long run? Some tables and workbenches may start out looking great, but if they are not designed for this type of work, they’ll lose their luster and, more importantly, usability very quickly.\nThe best way to get a feel for your furniture needs is by visiting another space in your area that is using similar equipment to what you plan to put in your shop. While you are there, note the equipment and furniture and talk to the manager of this space about what they chose and why.\nKey Takeaway: Consider furniture with the right storage options and durability to fit your needs.\nDesigning for Mobility\nMobility is a popular trend in makerspaces, and understandably so.\nCheck to see if the furniture you are considering has a mobile option. Often casters can be added to workbenches to make the majority of furniture reconfigurable. The ability to push tables together for large groups and pull tables apart for working alone gives a makerspace additional flexibility, so that it can meet the needs of a variety of different types of users.\nMobility doesn’t just apply to furniture – equipment can be made mobile as well. For example, many 3D printers and laser systems can include a mobile cart option, allowing for even more flexibility within a makerspace.\nKey Takeaway: Mobile furniture and equipment offers an entirely new level of customization.\nJoin Make: Community Today']	['<urn:uuid:1a3a3fbd-9cbd-46ab-bb33-5831e388d7bd>', '<urn:uuid:6d23e283-4e99-43be-b3d5-27bf84b1c7bc>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T04:31:27.499155	14	97	1789
100	What is better for dental healing: titanium implants or platelet-rich fibrin?	Both titanium implants and platelet-rich fibrin (PRF) have beneficial properties for dental healing. Titanium implants can be designed with controlled porosity to match bone properties and can be functionalized to improve osseointegration. PRF accelerates both soft and hard tissue healing by releasing growth factors over time. The two approaches are often complementary - PRF can actually be used in conjunction with titanium implants to improve their integration and healing outcomes.	"[""by Keyword: Osseointegration\nRodríguez-Contreras A, Torres D, Rafik B, Ortiz-Hernandez M, Ginebra MP, Calero JA, Manero JM, Ruperez E, (2021). Bioactivity and antibacterial properties of calcium- and silver-doped coatings on 3D printed titanium scaffolds Surface & Coatings Technology 421\nOne of the major problems faced by metallic implants is the high probability of bacterial infections, with significant consequences for the patient. In this work, a thermochemical treatment is proposed to obtain silver-doped calcium titanate coatings on the Ti surface to improve the bioactivity of porous 3D-printed Ti structures and simultaneously provide them with antibacterial properties. A complete characterization of the new coating, the study of the ion release and the analysis of its cytotoxicity were carried out together with evaluation of the natural apatite forming in simulated body fluid (SBF). Moreover, the antibacterial properties of the coatings were assessed against Pseudomona aeruginosa and Escherichia coli as gram-negative and Staphylococcus aureus and Staphylococcus epidermidis as gram-positive bacterial strains. Ag ions were integrated into the Ca titanate layer and Ag nanoparticles were formed within the entire 3D Ti surface. Ca and Ag ions were released from both porous and solid samples into the Hanks' solution for 48 h. The treated surfaces showed no cytotoxicity and an apatite layer precipitated on the entire porous surface when the samples were immersed in SBF. The release of Ag from the surface had a strong antibacterial effect and prevented bacterial adhesion and proliferation on the surface. Moreover, the nanostructured topography of the coating resulted also in a reduction of bacterial adhesion and proliferation, even in absence of Ag. In conclusion, the cost-effective approach here reported provided protection against the most predominant bacterial colonizers to the Ti porous implants, while maintaining their bioactivity.\nJTD Keywords: 3d-printing, alkaline, antibacterial activity, arthroplasty, bacterial adhesion, biomaterials, generation, ions, nanoparticles, osseointegration, silver, surface-layer, titanium implants, toxicity, 3d-printing, Antibacterial activity, Biomaterials, Porous structures, Silver, Ti metal, Titanium implants\nOliver-Cervelló L, Martin-Gómez H, Reyes L, Noureddine F, Ada Cavalcanti-Adam E, Ginebra MP, Mas-Moruno C, (2021). An Engineered Biomimetic Peptide Regulates Cell Behavior by Synergistic Integrin and Growth Factor Signaling Advanced Healthcare Materials 10,\n© 2020 Wiley-VCH GmbH Recreating the healing microenvironment is essential to regulate cell–material interactions and ensure the integration of biomaterials. To repair bone, such bioactivity can be achieved by mimicking its extracellular matrix (ECM) and by stimulating integrin and growth factor (GF) signaling. However, current approaches relying on the use of GFs, such as bone morphogenetic protein 2 (BMP-2), entail clinical risks. Here, a biomimetic peptide integrating the RGD cell adhesive sequence and the osteogenic DWIVA motif derived from the wrist epitope of BMP-2 is presented. The approach offers the advantage of having a spatial control over the single binding of integrins and BMP receptors. Such multifunctional platform is designed to incorporate 3,4-dihydroxyphenylalanine to bind metallic oxides with high affinity in a one step process. Functionalization of glass substrates with the engineered peptide is characterized by physicochemical methods, proving a successful surface modification. The biomimetic interfaces significantly improve the adhesion of C2C12 cells, inhibit myotube formation, and activate the BMP-dependent signaling via p38. These effects are not observed on surfaces displaying only one bioactive motif, a mixture of both motifs or soluble DWIVA. These data prove the biological potential of recreating the ECM and engaging in integrin and GF crosstalk via molecular-based mimics.\nJTD Keywords: binding, biomaterials, biomimetic peptides, bone, cell adhesion, cell differentiation, differentiation, dwiva, multifunctional coatings, osseointegration, osteoblasts, rgd, surface, surface functionalization, Biomimetic peptides, Cell adhesion, Cell differentiation, Dwiva, Matrix-bound bmp-2, Rgd, Surface functionalization\nVidal, E., Torres, D., Guillem-Marti, J., Scionti, G., Manero, J. M., Ginebra, M. P., Rodríguez, D., Rupérez, E., (2020). Titanium scaffolds by direct ink writing: Fabrication and functionalization to guide osteoblast behavior Metals 10, (9), 1156\nTitanium (Ti) and Ti alloys have been used for decades for bone prostheses due to its mechanical reliability and good biocompatibility. However, the high stiffness of Ti implants and the lack of bioactivity are pending issues that should be improved to minimize implant failure. The stress shielding effect, a result of the stiffness mismatch between titanium and bone, can be reduced by introducing a tailored structural porosity in the implant. In this work, porous titanium structures were produced by direct ink writing (DIW), using a new Ti ink formulation containing a thermosensitive hydrogel. A thermal treatment was optimized to ensure the complete elimination of the binder before the sintering process, in order to avoid contamination of the titanium structures. The samples were sintered in argon atmosphere at 1200 °C, 1300 °C or 1400 °C, resulting in total porosities ranging between 72.3% and 77.7%. A correlation was found between the total porosity and the elastic modulus of the scaffolds. The stiffness and yield strength were similar to those of cancellous bone. The functionalization of the scaffold surface with a cell adhesion fibronectin recombinant fragment resulted in enhanced adhesion and spreading of osteoblastic-like cells, together with increased alkaline phosphatase expression and mineralization.\nJTD Keywords: Direct ink writing, Osseointegration, Recombinant protein, Thermoresponsive binder, Titanium, Titanium scaffold\nGuillem-Marti, J., Gelabert, M., Heras-Parets, A., Pegueroles, M., Ginebra, M. P., Manero, J. M., (2019). RGD mutation of the heparin binding II fragment of fibronectin for guiding mesenchymal stem cell behavior on titanium surfaces ACS Applied Materials and Interfaces 11, (4), 3666-3678\nInstalling bioactivity on metallic biomaterials by mimicking the extracellular matrix (ECM) is crucial for stimulating specific cellular responses to ultimately promote tissue regeneration. Fibronectin is an ECM protein commonly used for biomaterial functionalization. The use of fibronectin recombinant fragments is an attractive alternate to the use of full-length fibronectin because of the relatively low cost and facility of purification. However, it is necessary to combine more than one fragment, for example, the cell attachment site and the heparin binding II (HBII), either mixed or in one molecule, to obtain complete activity. In the present study, we proposed to install adhesion capacity to the HBII fragment by an RGD gain-of-function DNA mutation, retaining its cell differentiation capacity and thereby producing a small and very active protein fragment. The novel molecule, covalently immobilized onto titanium surfaces, maintained the growth factor-binding capacity and stimulated cell spreading, osteoblastic cell differentiation, and mineralization of human mesenchymal stem cells compared to the HBII native protein. These results highlight the potential capacity of gain-of-function DNA mutations in the design of novel molecules for the improvement of osseointegration properties of metallic implant surfaces.\nJTD Keywords: Fibronectin, Growth factor, Mutation, Osseointegration, Recombinant protein, Titanium\nGuillem-Marti, J., Boix-Lemonche, G., Gugutkov, D., Ginebra, M.-P., Altankov, G., Manero, J.M., (2018). Recombinant fibronectin fragment III8-10/polylactic acid hybrid nanofibers enhance the bioactivity of titanium surface Nanomedicine 13, (8), 899-912\nAim: To develop a nanofiber (NF)-based biomimetic coating on titanium (Ti) that mimics the complex spatiotemporal organization of the extracellular matrix (ECM). Materials & methods: Recombinant cell attachment site (CAS) of fibronectin type III8-10 domain was co-electrospun with polylactic acid (PLA) and covalently bound on polished Ti discs. Osteoblast-like SaOS-2 cells were used to evaluate their complex bioactivity. Results: A significant increase of cell spreading was found on CAS/PLA hybrid NFs, followed by control pure PLA NFs and bare Ti discs. Cell proliferation showed similar trend being about twice higher on CAS/PLA NFs. The significantly increased ALP activity at day 21 indicated an enhanced differentiation of SaOS-2 cells. Conclusion: Coating of Ti implants with hybrid CAS/PLA NFs may improve significantly their osseointegration potential.\nJTD Keywords: Electrospinning, Fibronectin, Hybrid nanofibers, Osseointegration, PLA, Recombinant protein\nHoyos-Nogués, M., Velasco, F., Ginebra, M. P., Manero, J. M., Gil, F. J., Mas-Moruno, C., (2017). Regenerating bone via multifunctional coatings: The blending of cell integration and bacterial inhibition properties on the surface of biomaterials ACS Applied Materials & Interfaces 9, (26), 21618-21630\nIn dentistry and orthopedics, it is well accepted that implant fixation is a major goal. However, an emerging concern is bacterial infection. Infection of metallic implants can be catastrophic and significantly reduce patient quality of life. Accordingly, in this work, we focus on multifunctional coatings to simultaneously address and mitigate both these problems. We have developed a tailor-made peptide-based chemical platform that integrates the well-known RGD cell adhesive sequence and the lactoferrin-derived LF1-11 antimicrobial peptide. The platform was covalently grafted on titanium via silanization and the functionalization process characterized by contact angle, XPS, and QCM-D. The presence of the platform statistically improved the adhesion, proliferation and mineralization of osteoblast-like cells compared to control surfaces. At the same time, colonization by representative bacterial strains was significantly reduced on the surfaces. Furthermore, the biological potency of the multifunctional platform was verified in a co-culture in vitro model. Our findings demonstrate that this multifunctional approach can be useful to functionalize biomaterials to both improve cell integration and reduce the risk of bacterial infection.\nJTD Keywords: Antimicrobial peptides, Cell adhesive peptides, Multifunctionality, Osseointegration, Surface functionalization\nFraioli, R., Dashnyam, K., Kim, J. H., Perez, R. A., Kim, H. W., Gil, J., Ginebra, M. P., Manero, J. M., Mas-Moruno, C., (2016). Surface guidance of stem cell behavior: Chemically tailored co-presentation of integrin-binding peptides stimulates osteogenic differentiation in vitro and bone formation in vivo Acta Biomaterialia 43, 269-281\nSurface modification stands out as a versatile technique to create instructive biomaterials that are able to actively direct stem cell fate. Chemical functionalization of titanium has been used in this work to stimulate the differentiation of human mesenchymal stem cells (hMSCs) into the osteoblastic lineage, by covalently anchoring a synthetic double-branched molecule (PTF) to the metal that allows a finely controlled presentation of peptidic motifs. In detail, the effect of the RGD adhesive peptide and its synergy motif PHSRN is studied, comparing a random distribution of the two peptides with the chemically-tailored disposition within the custom made synthetic platform, which mimics the interspacing between the motifs observed in fibronectin. Contact angle measurement and XPS analysis are used to prove the efficiency of functionalization. We demonstrate that, by rationally designing ligands, stem cell response can be efficiently guided towards the osteogenic phenotype: In vitro, PTF-functionalized surfaces support hMSCs adhesion, with higher cell area and formation of focal contacts, expression of the integrin receptor Î±5Î²1 and the osteogenic marker Runx2, and deposition a highly mineralized matrix, reaching values of mineralization comparable to fibronectin. Our strategy is also demonstrated to be efficient in promoting new bone growth in vivo in a rat calvarial defect. These results highlight the efficacy of chemical control over the presentation of bioactive peptides; such systems may be used to engineer bioactive surfaces with improved osseointegrative properties, or can be easily tuned to generate multi-functional coatings requiring a tailored disposition of the peptidic motifs. Statement of significance Organic coatings have been proposed as a solution to foster osseointegration of orthopedic implants. Among them, extracellular matrix-derived peptide motifs are an interesting biomimetic strategy to harness cell-surface interactions. Nonetheless, the combination of multiple peptide motifs in a controlled manner is essential to achieve receptor specificity and fully exploit the potentiality of synthetic peptides. Herein, we covalently graft to titanium a double branched molecule to guide stem cell fate in vitro and generate an osseoinductive titanium surface in vivo. Such synthetic ligand allows for the simultaneous presentation of two bioactive motifs, thus is ideal to test the effect of synergic sequences, such as RGD and PHSRN, and is a clear example of the versatility and feasibility of rationally designed biomolecules.\nJTD Keywords: hMSCs, Integrin-binding peptides, Osseointegration, RGD-PHSRN, Titanium"", '|Year : 2018 | Volume\n| Issue : 1 | Page : 23-26\nThe use of platelet-rich fibrin concentrate in tissue healing and regeneration in dentistry\nLídia Souza De Andrade1, Lenira Pelloso Leite1, Fernanda Britto De Melo Silva1, Rodrigo Figueiredo De Brito Resende2, Marcelo José Pinheiro Guedes De Uzeda2\n1 Department of Dental Clinic, School of Dentistry, Fluminense Federal University, Niteroi, Brazil\n2 Department of Oral Surgery, Dental Clinical Research Center, School of Dentistry, Iguaçu University, Itaperuna, Rio De Janeiro, Brazil\n|Date of Web Publication||13-Apr-2018|\nProf. Marcelo José Pinheiro Guedes De Uzeda\nRua Coronel Moreira Cesar 229/1120 - Niteroi, Rio De Janeiro\nSource of Support: None, Conflict of Interest: None\nPlatelet-rich fibrin (PRF) concentrate was developed in France in 2001 by Choukroun et al. Initially known in its platelet gel form, it can also be used as a membrane, being widely used to accelerate the healing of soft and hard tissues. The aims of this study are to report the applicability of platelet-rich fibrin in dental practice, seeking evidences of benefits in the regeneration and healing process of both soft and hard tissue. For this, a bibliographic survey of articles published between 2007 and 2017 was carried out using the descriptors: “Platelet-rich fibrin,” “oral surgery,” “fibrin” and “growth factors,” during the month of December 2017 resulting in nine selected articles according to the preestablished inclusion criteria. The results of the studies either showed that PRF presents effective and safe use when used in combination with biomaterials or isolated, in addition to presenting low risk associated with satisfactory clinical results.\nKeywords: Concentrate oral surgery, dentistry, growth factor, platelet-rich fibrin\n|How to cite this article:|\nDe Andrade LS, Leite LP, De Melo Silva FB, De Brito Resende RF, Guedes De Uzeda MJ. The use of platelet-rich fibrin concentrate in tissue healing and regeneration in dentistry. Int J Growth Factors Stem Cells Dent 2018;1:23-6\n|How to cite this URL:|\nDe Andrade LS, Leite LP, De Melo Silva FB, De Brito Resende RF, Guedes De Uzeda MJ. The use of platelet-rich fibrin concentrate in tissue healing and regeneration in dentistry. Int J Growth Factors Stem Cells Dent [serial online] 2018 [cited 2020 Dec 1];1:23-6. Available from: https://www.cellsindentistry.org/text.asp?2018/1/1/23/228979\n| Introduction|| |\nPlatelet-rich fibrin (PRF) is an immune and platelet concentrate, developed in 2001 in France by Choukroun. It is part of second-generation platelet concentrate and is widely used to accelerate soft- and hard--tissue healing process.,,\nPlatelets contain various growth factors and cytokines that play a key role in inflammation and bone healing. These growth factors are postulated as promoters of tissue regeneration.,\nPRF consists of a fibrin matrix polymerized in a tetramolecular structure that incorporates platelets, leukocytes, cytokines, and circulating stem cells. Slow fibrin polymerization, during PRF processing, leads to the intrinsic incorporation of platelet cytokines and glycan chains in the fibrin meshes. This result implies that PRF, unlike the other platelet concentrates, would be able to progressively release cytokines during fibrin matrix remodeling process.\nStudies show several benefits in using PRF. The concentrate can be used in platelet gel form, as a membrane and can also be used in a liquid (injectable) form, for this, the tubes used for blood collection should not contain additives, allowing the PRF to be obtained its liquid form ,, [Table 1].\nPRF releases a number of growth factors. Among them, we can mention the transforming growth factor-beta, platelet-derived growth factor, vascular endothelial growth factor, and matrix glycoproteins (such as thrombospondin-1) that are gradually released at least for 1 week. The increased concentration of these growth factors suggests an acceleration in the healing of soft and hard tissues.\nIn addition, several studies have suggested that PRF may stimulate osseous and soft-tissue regeneration while also reducing inflammation, pain, and unwanted side effects.\nBecause of these physiological properties, the use of platelet concentrates has become increasingly popular over the past 15 years to improve wound healing and improve the clinical benefits of bone replacement grafts.,\nThis study aims to critically discuss the applicability of platelet-rich fibrin in dental practice, emphasizing its benefits in the regeneration and healing process of soft and hard tissue.\n| Materials and Methods|| |\nThe present study was prepared from a bibliographic research by digital means during December 2017. The following databases were used: BVS, SCOPUS, SciELO, LILACS, and MEDLINE/PubMed. For this, the keywords “Platelet-Rich Fibrin,” “oral surgery,” “fibrin concentrate,” and “growth factors” were selected, following the inclusion/exclusion criteria [Table 2].\n| Results|| |\nInitially, 245 articles were found in the preestablished databases. The first evaluation resulted in the selection of 11 full-text articles. After careful reading, three of them were excluded. Thus, nine studies published were included in this review [Table 3].\n| Discussion|| |\nIn dentistry, PRF has been used as a treatment for gingival recession, furcation defects, maxillary rehabilitation before implant placement, restoration of peri-implant defects, as well as healing the tooth socket after extraction. PRF can regulate inflammation process and stimulate the immune response through chemotaxis., It can be used as unaltered mass, that is, as a clot or can be compressed into membrane without losing its properties.\nIn their study, Toffler et al. showed that, in conducting osteotomy-mediated sinus floor elevation using PRF, wound healing was faster, predictably increased maxillary sinus elevation and in the case of perforation, the fibrin matrix may assist in healing the wound. This study presented favorable results regarding the use of PRF for this type of procedure, with a high degree of safety and predictability. In addition, the PRF membrane, or plug, also provides protection for the sinus membrane during the use of an osteotome and in case of perforation, the fibrin matrix may assist in wound closure.\nAnother study that evaluated the use of PRF associated with bovine bone graft as grafting materials in sinus lifts concluded that PRF reduced the healing time, favoring optimal bone regeneration. In addition, platelet-rich fibrin has a gelatinous consistency that favors clot stability and when in the membranous shape, allows creating a natural “barrier effect” on the bone breaches that were opened in the surgical areas.\nPRF has also shown to be an effective modality in the regenerative treatment of degree II furcation defect. Sharma and Pradeep observed that there was greater reduction in probing depth, more relative vertical clinical attachment level and also relative horizontal clinical attachment level gain with significant bone defect filled with PRF in the treatment of degree II mandibular furcation defects.\nIn relation to periodontal regeneration in infrabony defects, demineralized freeze-dried bone allograft is still widely used. However, PRF showed promising results for periodontal regeneration in terms of clinical parameters and is comparable to demineralized freeze-dried bone allograft. Shah et al. showed that there is no difference in the clinical parameters between the PRF group and demineralized freeze-dried bone allograft group in periodontal infrabony defects in the experimental period of 6 months. Two positive results were also observed when associating the PRF to demineralized freeze-dried bone allograft. One study evaluated there was a significantly greater probing pocket depth reduction and clinical attachment gain. In agreement with this result, another study indicates that both PRF associated with anorganic bovine bone mineral and anorganic bovine bone mineral are effective in the treatment of intrabony defects. However, the addition of PRF to anorganic bovine bone mineral may lead to increased level of clinical binding gain.\nPRF offers several advantages when used in conjunction with bone grafts including promoting wound healing, bone growth and maturation, graft stabilization, wound sealing, hemostasis, and also improving the handling properties of graft materials.\nBaslarli et al. analyzed the effects of PRF on tooth socket of third molars in the mandible after exodontia in periods of 30 and 90 days. No significant results were found. However, several clinical studies have demonstrated the efficacy of PRF in the promotion of healing of tooth socket after exodontia; the PRF has, in fact, platelet growth factors that can improve the vascularization of the surgical site, promoting neoangiogenesis.,\nThere are several advantages of using PRF, such as easy and simplified chairside preparation of PRF, cost-effectiveness, release of relatively constant concentration of growth factors over a period of 7 days and also rapid and excellent healing of the periodontium. This natural material actually seems to accelerate the physiological wound healing, besides, in association with bone grafts, it seems to accelerate new bone formation. In the last years, PRF concentrates have been widely used as a supplement to tissue regeneration procedures.\n| Conclusion|| |\nThe results of the studies showed that PRF presents effective and safe use either when combined with biomaterials or isolates, besides presenting low risk associated with satisfactory clinical results. PRF has proven to be very advantageous for use in dentistry since most studies suggest benefits in the process of regeneration and healing of soft and hard tissue.\nFinancial support and sponsorship\nConflicts of interest\nThere are no conflicts of interest.\n| References|| |\nChoukroun J, Adda F, Schoeffler C, Vervelle A. Un opportunité in paro-implantologie: Le PRF. Implantodontie 2001;42:55.\nKurdukar AA, Kurdukar PA, Dani NH. Modified lateral positioned flap with platelet-rich fibrin graft for treatment of denuded root surfaces: A clinical study. Indian J Dent Res 2017;28:524-9.\n] [Full text]\nShah M, Patel J, Dave D, Shah S. Comparative evaluation of platelet-rich fibrin with demineralized freeze-dried bone allograft in periodontal infrabony defects: A randomized Controlled Clinical Study. Journal of Indian Society of Periodontology 2015;19:56-60.\nBaslarli O, Tumer C, Ugur O, Vatankulu B. Evaluation of osteoblastic activity in extraction sockets treated with platelet-rich fibrin. Med Oral Patol Oral Cir Bucal 2015;20:111-6.\nSharma A, Pradeep AR. Autologous platelet-rich fibrin in the treatmentt of mandibular degree II furcation defects: A randomized clinical trial. J Periodontol 2011;82:1396-1403.\nMourão C, Valiense H, Mourão N, Maia M. Obtention of injectable platelets rich-fibrin (i-PRF) and its polymerization with bone graft. Rev Col Bras Cir 2015;42:421-3.\nBorie E, Oliví DG, Orsi IA, Garlet K, Weber B, Beltrán V, et al\n. Platelet-rich fibrin application in dentistry: A literature review. International Journal of Clinical and Experimental Medicine 2015;8:7922-9.\nSezgin Y, Uraz A, Taner IL, Çulhaoglu R. Effects of platelet-rich fibrin on healing of intra-bony defects treated with anorganic bovine bone mineral. Braz Oral Res 2017;31:e15.\nMarrelli M, Tatullo M. Influence of PRF in the healing of bone and gingival tissues. Clinical and histological evaluations. European Review for Medical and Pharmacological Sciences 2013;17:1958-62.\nToffler M, Toscano N, Holtzclaw D. Osteotome-mediated sinus floor elevation using only platelet-rich fibrin: An early report on 110 patients. Implant Dentistry 2010;19:447-56.\nTatullo M, Marrelli M, Cassetta M, Pacifici A, Stefanelli LV, Scacco S, et al\n. Platelet Rich Fibrin (P.R.F.) in Reconstructive Surgery of Atrophied Maxillary Bones: Clinical and Histological Evaluations. International Journal of Medical Sciences 2012;9:872-80.\nBansal C, Bharti V. Evaluation of efficacy of autologous platelet-rich fibrin with demineralized-freeze dried bone allograft in the treatment of periodontal intrabony defects. Journal of Indian Society of Periodontology 2013;17:361-6.\n[Table 1], [Table 2], [Table 3]']"	['<urn:uuid:ec21d2a0-8c11-446a-bfe0-1b90c0cde219>', '<urn:uuid:ebc99230-ca47-4565-8ba1-1540ae1dd4b0>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T04:31:27.499155	11	70	3697
