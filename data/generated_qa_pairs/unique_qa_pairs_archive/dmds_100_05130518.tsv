qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	israelites numbers egypt mary magnificat compare	The Israelites and Mary expressed contrasting attitudes toward divine intervention. The Israelites complained about their situation and longed for Egypt, remembering 'the fish we used to eat in Egypt for nothing, the cucumbers, the melons' despite their previous slavery. In contrast, Mary's Magnificat showed revolutionary acceptance and praise, celebrating how God 'has brought down the powerful from their thrones, and lifted up the lowly.' While the Israelites' complaints led to divine punishment, Mary's acceptance led to her exaltation.	['Father Callan’s Commentary on Luke 1:26-38\nPosted by Dim Bulb on December 19, 2010\nText in red represent my additional notes.\nLuk 1:26 And in the sixth month, the angel Gabriel was sent from God into a city of Galilee, called Nazareth,\n26. And in the sixth month; i.e., after the conception of Elizabeth. Hence John the Baptist was just six months older than our Lord.\nNazareth. See on Matthew 2:23.\nIn the sixth month has been taken, along with other time references and the mention of Gabriel-in the Lucan Infancy Narrative, as suggesting an allusion to the “Seventy weeks of years” prophecy in Daniel 9:24-27. See here for more details.\nLuk 1:27 To a virgin espoused to a man whose name was Joseph, of the house of David: and the virgin’s name was Mary.\nTo a virgin espoused. From these words, some conclude that the Blessed Virgin was already really married to Joseph; others that she was only promised in marriage by her family. The latter opinion would seem more probable because otherwise she would have been actually living with St. Joseph, as was customary with those really married. See on Matthew 1:18.\nActually, the Jewish concept of espousal and marriage differs considerably from ours. As I noted in some comments on added to Maldonado’s commentary on Matt 1:18-24~The Jewish marriage process was twofold: (1) the espousal period, which could last up to a year. During this period the couple lived apart but were still considered to be husband and wife; it is for this reason that Matthew can speak of Mary in verse 18 as “espoused” (NAB, “betrothed”) and also refer to Joseph as her husband in verses 16 and 19; and call Mary “thy wife” (NAB “your wife’) in verse 20. (2) the ratification of the marriage took place when the groom went to the house where his s=espoused wife was living and brought her back into his own home. This is what is being referred to when the angel says fear not to take unto thee Mary thy wife (see verses 20 & 24). It is this process which is behind the marriage imagery of Matt 25:1-13; Jn 3:29-30.\nOf the house of David. St. Joseph and the Blessed Virgin were\nboth of the family of David. Our Lord was therefore truly”the Son of David” on his mother’s side.\nMary. This name is derived from the Hebrew “Miriam,” which signifies “Star of the Sea,” or, in Chaldaic, “Lady.” These represent common interpretations, however, we do not in fact know with certainty what the name means.\nLuk 1:28 And the angel being come in, said unto her: Hail, full of grace, the Lord is with thee: blessed art thou among women.\nAnd the angel being come in. It is the common opinion that our Blessed Lady was rapt in devout prayer when the angel entered her room.\nFull of grace. This is the Catholic translation of the Greek, κεχαριτωμενη, and is in conformity with the authority of the Latin Fathers, and of the ancient Syriac and Arabic versions of this passage. Protestants widely differ as to the meaning of the word. The Authorized Version translates it, “highly favored.” Literally,the term would seem to mean “highly pleasing,” or “highly\nfavored,” or “singularly endowed.” Since, therefore, it is grace alone which renders one highly pleasing in the sight of God, the translation “full of grace” would seem to be most correct. Just as Solomon was called “The Wise One,” and our Lord, “The Just One,” so here our Blessed Lady is addressed as “The Highly favored One,” i.e., favored above all others.\nThe Greek κεχαριτωμενη (Kecharitomene) is a present passive participle. As a present participle it refers to something which took place in the past, the effects of which continue. As passive in refers to something Mary has received. Since the word is based upon charis (grace), or, to be more exact, on charitoo (made graced), and since the word is passive, perhaps a better translation into English would be “filled with grace.”\nThe Lord is with thee. By “Lord,” we are here to understand YHWH (God); not our Lord, whom as yet our Blessed Lady had not conceived.\nBlessed art thou among women. These words, although found in most MSS., are wanting in the Vatican and a few other ancient MSS. Our Blessed Lady is here compared, not with the whole of mankind, but with all other women (but see Lk 1:48).\nLuk 1:29 Who having heard, was troubled at his saying and thought with herself what manner of salutation this should be.\nWho having heard, who rather “having seen”(ιδουσα) . It was a shock to our Lady’s modesty to see the angel in her room in the form of a young man. It was likewise a shock to her humility to hear the words of praise which he addressed to her.\nWas troubled, because she was most probably only promised in marriage by her family. Her heart and her own final promise of marriage had never been given, nor perhaps had she intended to give them to any man.\nFather Callan writes: It was a shock to our Lady’s modesty to see the angel in her room in the form of a young man. In fact, the text never describes what the angel looked like. He further writes: It was likewise a shock to her humility to hear the words of praise which he addressed to her. “Likewise?” In fact the text doesn’t attribute our Lady’s being “troubled” with the sight of the angel at all, rather, it is specifically and only his “saying” that “troubled” her. This stands in contrast to Zechariah who was troubled when he saw the angel (Lk 1:12) and, also, with the shepherds (Lk 2:9, implied), and the women at the tomb (Lk 24:4-5).\nThe Greek word ιδουσα usually refers to simple sight, however, it can also denote perception or recognition of some kind. Mary saw (recognized) that an angel-a messenger from God-appeared before her and on this basis was “troubled at his saying,” this did not deter her however, for she “thought with herself what manner of salutation this should be”. The Greek διελογιζετο (“thought with herself”) implies an intense, prolonged reflection. Here is how Thayer’s Greek Dictionary defines it: to bring together different reasons, to reckon up the reasons, to reason, revolve in one’s mind, deliberate. Except for here and in Lk 3:15, the word is always used by the third evangelist to designate something negatively done (see Luke 5:21; 5:22; 12:17; 20:14).\nLuk 1:30 And the angel said to her: Fear not, Mary, for thou hast found grace with God.\nFear not, Mary. The angel now speaks to her more familiarly, calling her by her name, which shows that she was no stranger to him, and he assures her that no evil can befall her since she is under divine protection.\nLuk 1:31 Behold thou shalt conceive in thy womb and shalt bring forth a son: and thou shalt call his name Jesus.\nBehold, thou shalt conceive in thy womb. The Blessed Virgin knew that the Prophet Isaias had foretold, seven hundred years before, that the Messiah should be conceived by a virgin (Isa 7:14), but how this should be, she did not know.\nJesus is the Greek form of the Hebrew Josue; it means Saviour. Jesus was to save his people from their sins (Matthew 1:21).\nBehold, thou shalt conceive in thy womb. Some see here and elsewhere in the annunciation an allusion to the Prophet Zephaniah. In Zeph 3:14 we read: Give praise, O daughter of Sion: shout, O Israel: be glad, and rejoice with all thy heart, O daughter of Jerusalem. “Rejoice” is the very first word the angel spoke to our Lady when he greeted her (translated “Hail” in Lk 1:28). Zeph 3:15 reads: The Lord hath taken away thy judgment, he hath turned away thy enemies: the king of Israel, the Lord, is in the midst of thee, thou shalt fear evil no more. Mary herself is told not to fear (Lk 1:30), and in her canticle she celebrates God’s overthrow of those hostile to his people (Lk 1:51-52). The words of Zephaniah here translated as the Lord is in the midst of thee is, literally, “the Lord is in your womb” calling to mind Lk 1:31.\nLuk 1:32 He shall be great and shall be called the Son of the Most High. And the Lord God shall give unto him the throne of David his father: and he shall reign in the house of Jacob for ever.\nLuk 1:33 And of his kingdom there shall be no end.\nHe shall be great, — (a) because He shall be called, and shall be in reality, the Son of God; (b) because He shall be the everlasting ruler of the people of God.\nThrone of David his father. Our Lord was to be the descendant of David, and He was to be David’s successor and the spiritual ruler of the chosen people of God. (see 2 Sam 7:12-16, Isa 9:5-6).\nIn the house of Jacob forever. Our Lord was to exercise dominion, not over a few tribes of Israel, as did some of the descendants of Jacob, but over all of the twelve tribes, and over all those who in future should be aggregated to His Church.\nLuk 1:34 And Mary said to the angel: How shall this be done, because I know not man?\nHow shall this be done, etc. By these words the Blessed Virgin did not express any doubt, as we know from verse 45; she wished to know only the manner in which the angel’s announcement was to be fulfilled.\nBecause I know not man. From these words, expressed in the present tense, the Fathers and commentators infer that the Blessed Virgin had made a vow of perpetual virginity. The use of the present tense seems to embrace all time, past, present, and future. See above, verse 29.\nLuk 1:35 And the angel answering, said to her: The Holy Ghost shall come upon thee and the power of the Most High shall overshadow thee. And therefore also the Holy which shall be born of thee shall be called the Son of God.\nThe Holy Ghost shall come upon thee, etc. The angel now assures our Lady that the Holy Ghost, without any detriment to her virginity, shall miraculously cause her to conceive a son. This work is appropriated to the Holy Ghost, because it is a work of love. The external works (opera ad extra) of the Holy Trinity are differently appropriated to the different persons; thus, power is appropriated to the Father, wisdom to the Son, love to the Holy Ghost. The appropriations are founded on the different relations to one another of the three Divine Persons of the Holy Trinity.\nIt is to be noted that this and the preceding verse afford a proof of the Virgin Birth of our Lord. Cf. on Matthew 1:18-25.\nThe holy; i.e., the holy offspring, “which shall be born,” —literally, “which is born ” (γεννωμενον); i.e., which is just about to be conceived. The phrase\n“of thee” is wanting in most Greek MSS.\nThe Holy Ghost shall come upon thee and the power of the Most High shall overshadow thee. See Acts 1:8~But you shall receive the power of the Holy Ghost coming upon you.\nLuk 1:36 And behold thy cousin Elizabeth, she also hath conceived a son in her old age: and this is the sixth month with her that is called barren.\nThy cousin Elizabeth. Elizabeth was of the daughters of Aaron and Mary was of the tribe of Juda; but since they are cousins, it must be that one of Elizabeth’s ancestors, on the maternal side, was married to a man of the tribe of Juda.\nShe also hath conceived, etc. The angel spoke these words to Mary, not because the latter had doubted, but only to strengthen her faith. Just as Elizabeth, who by age was sterile, had become fecund by the gift of God, so should Mary be able to conceive a son without any intervention on the part of man.\nThat is called barren; i.e., who is commonly known to be barren.\nLuk 1:37 Because no word shall be impossible with God.\nNo word; i.e., no promise of God is impossible of fulfillment;no thing is beyond God’s power. The words call to mind the announcement of the birth of Isaac in Genesis 18:14~Is there any thing hard to God?\nLuk 1:38 And Mary said: Behold the handmaid of the Lord: be it done to me according to thy word. And the angel departed from her.\nBehold the handmaid of the Lord. As soon as Mary understood that she was to conceive without detriment to her virginity, she at once humbly bowed to the will of God; and at that moment she conceived her Divine Son.\nBe it done unto me according to thy word. Like the previous verse, this one also calls to mind the birth of Isaac. In Genesis 21:1 the narrator describes the birth of Isaac in these words: and the LORD did unto Sarah as he had spoken.', 'Numbers 9-11, Psalm 43, Luke 1\nBe careful of what you pray for\nNumbers 9 – 11\nA lot of interesting things occur in Numbers, if we take our time to read it carefully. In Numbers 9:14, for example, we are told, “You shall have one statute for both the resident alien and the native.” This kind of thinking was unheard of and what made Israel so ahead of its time as they followed God ethical teachings. This would be a wonderful text for Israelis to wrestle with today, in a society where there are different laws for Jewish citizens and Arab workers.\nWe learn that God clearly shepherded the Israelites, moving before them with a cloud by day and a fire by night just as God did during the Exodus. There was no mistaking God’s leadership. In Numbers 10, we read more about God’s willingness to defend them from their enemies. “When you go to war in your land against the adversary who oppresses you, you shall sound an alarm with the trumpets, so that you may be remembered before the Lord your God and be saved from your enemies.” (Numbers 10:9) God will defend the Israelites from their oppressors.\nT.S Eliot writes in The Little Gidding section of The Four Quartets, “We shall not cease from exploration. And the end of all our exploring will be to arrive where we started and know the place for the first time.” I wonder if Moses felt that way after returning to the land where he met his father-in-law Jethro and married his daughter. In Numbers 10:29, Moses encounters Hobab, son of Reuel the Midianite, Moses’ father-in-law. His father-in-law is also called Jethro. Moses has clearly come full circle.\nIn Numbers 11, the Israelites continue to complain. God is not happy. Indeed “the fire of the Lord burned against them.” (Numbers 11:3) The minds of the Israelites drift back to Egypt. They visualize the food that they used to eat, even though most of them were slaves and probably ate simple rations.\n“We remember the fish we used to eat in Egypt for nothing, the cucumbers, the melons, the leeks, the onions, and the garlic…” they lament. (Numbers 11:5) Over the past few days, I have driven with cab drivers from Somalia and Ethiopia while visiting Nashville and Atlanta. I always ask cabbies about their country, religion, family and what brought them to the United Stations. I ask if they miss their home. Inevitably, they say that the miss their family and friends most. Some then speak about how they miss the traditional food of their country. It is clear that the Israelites clearly did.\nMoses then rants, attacking God like a malevolent boss. “Why have you treated your servant so badly,” cries Moses, proving once again that a good defense is a strong offense. “Why have I not found favor in your sight, that you lay the burden of all this people on me?” (Numbers 11:11-12) Moses asks God, “Did I conceive all this people? Did I give birth to them, that you should say to me, ‘Cary them in your bosom.’” (Numbers 11:11-12)\nThis is completely different from Jesus’s approach to leadership. In Matthew’s Gospel, where Jesus is depicted as a Second Moses, we are told that Jesus had “compassion” (Matt. 14:14, 15:32) for the multitude that followed him. Jesus delegated saying, “you give them something to eat.” (Matt. 14:16) Later, when feeding the four thousand, Jesus merely solicits food from the disciples and multiplies it. Either way, Jesus leads differently than Moses. In Moses’s defense, he has had to care for the massive crowd far longer than Jesus did. On many occasions, I have acted more like Moses than like Jesus.\nNumbers 11:16-17, 24-25 mirrors Exodus 18:17-26. In Exodus, Moses’s father-in-law Jethro sees his son-in-law burning himself out as the sole judge of Israel, deciding over each dispute or case that the Israelites bring to him. Jethro councils his son-in-law to select 70 “able men” “who fear God, are trustworthy, and hate dishonest gain” and set them over the Israelites to serve as judges. In Numbers, it is God who takes the initiative and instructs Moses to select 70 “of the elders of Israel” and to disperse some of the burden of leadership upon each of them. (Numbers 11:16-17) Either way, delegation is the only way to survive when we are doing God’s work and leading God’s people. It is a sin not to delegate.\nWe must, however, be careful of what we pray for. I tell people that had God answered all of my prayers, I would have married a girl named Nancy from my junior high school and starved our family by trying to support them as a professional soccer player with limited skills. God has a shrewd way of not answering some of our prayers for our own good.\nAfter lamenting their lack of food choice and meat, God delivers more food and this time meat to supply the Israelites. “You shall eat not only one day, or two days, or five days, or ten days, or twenty days, but for a whole month – until [meat] comes out of your nostrils and becomes loathsome to you,” says God. (Numbers 11:19-20) Who says that God does not have a sense of humor?\nGod will soon bury the Israelites in piles of quail two cubits deep. Surely, the writer of Numbers has a wonderful sense of humor. There is so much meat supply that much of it rotted. A plague broke out among the people. The place where God’s overabundance and the people’s bitter lament met is called Kibroth-hattaavah or Graves of graving. It is a place that we have all experienced when our bitterest prayers were answered by God. Be careful what you pray for and how you pray for it. It always pays to communicate respectfully with God and God’s people.\nWhile Moses took aside the seventy, two men remained in the Israelite camp – Medad and Eldad. God’s spirit rested upon them, and they prophesized. A young man informed Moses. Then Joshua, Moses’s field commander, said, “My lord Moses, stop them!” But Moses uttered famous words, “Would that all the Lord’s people were prophets, and that the Lord would put his spirit on them!” (Numbers 11:29)\nIn Judaism, a prophet is an individual who claims to have been contacted by God to speak for the divine, serving as an intermediary with humanity. His message is called prophecy. The English word prophet comes from the Greek word profetes meaning “advocate.” The Hebrew word is nevim. Its origins are found in Deuteronomy 18:18-19, where God said, “I will raise up for them a prophet like you from among their own people; I will put my words in the mouth of the prophet, who shall speak to them everything that I command. Anyone who does not heed the words that the prophet shall speak in my name, I myself will hold accountable.”\nThe prophet was thus considered the “mouth” of God. The navim were believed to have a hollowness or openness, which allowed them to receive the transcendental wisdom of God. In addition to writing and speaking on God’s behalf, the prophets often acted out prophetic messages to rebuke leaders and the Israelites. Jeremiah was a prime example. For now, it is enough to ponder that God’s message is transmitted by more than just those that the Church narrowly deputizes. The Roman Catholic Church, for example, does not accept the validity of any Anglican or Protestant clergy. Likewise, in the Colonial times, Church of England clergy did not accept the validity of other religious orders and allow them to preach from their pulpits. God is constantly expanding our horizons to allow us to see how the Spirit of God works through the Medads and Eldads of the world.\nThe most memorable line here is, “O send out your light and your truth; let them lead me.” (Ps. 43:3) Like the Israelites who relied on God’s cloud by day and fire by night to guide them, God’s wisdom is like a light that will guide our steps each day. Grounding ourselves in Scripture each morning is the surest way to take advantage of God’s willingness to lead us. Psalm 43:5 mirrors Psalm 42:5 exactly. A good thought is always worth repeating.\nI must confess from the start that Luke is my favorite Gospel. Right from the start, Luke informs us that he has investigated “everything carefully” and is writing “an orderly account” for “Theophilus,” who was either his patron who commissioned the gospel or a name that stands for every seeker of God. The name theophilus literally means “lover of God.” Hence, Luke is a historian. He wants to tell the story of Jesus and tell it as accurately as possible. Luke is writing to all of us who seek to know God’s ways.\nImmediately, Luke introduces us to Zechariah, a priest, and his wife, Elizabeth. Both are aged, have lived righteously, and somehow have been punished by being unable to bear children. Normally, this was associated with being disobedient or sinful. When an angel appeared to Zechariah as he was offering incense in the Holy of Holies, Zechariah “was terrified; and fear overwhelmed him.” (Luke 1:12) This is a true theophany or encounter with one of God’s ambassadors.\nWhen a priest was selected to enter the Holy of Holies to offer incense symbolizing the prayers of the people rising to God, a rope was tied to his leg in case he encountered God and was struck dead. He could then be dragged from the Holy of Holies and buried. Priests entered this chamber with fear and trembling for this is where God dwelt.\nThe angel of the Lord informed Zechariah that he would have a son, and his son would “be filled with the Holy Spirit.” (Luke 1:15) The Holy Spirit will appear constantly throughout Luke’s Gospel. It is the prime mover or lead character in this gospel. Watch for it! “With the spirit and the power of Elijah [John the Baptist] will go before [Jesus], to turn the hearts of parents to their children, and the disobedient to the wisdom of the righteous, to make ready a people prepared for the Lord.” (Luke 1:17) Zechariah’s son, John the Baptist, will thus serve like Elijah, the Hebrew prophet, who the Israelites believed would return before the Messiah came. Luke is carefully orchestrating his story.\nAn angel named Gabriel then appeared to Mary. He informed her that she “will conceive in [her] womb and bear a son” and call him Jesus. “He will be great, and will be called the Son of the Most High, and the Lord God will give to him the throne of his ancestor David. He will reign over the house of Jacob forever, and of his kingdom there will be no end.” (Luke 1:31-33)\nMary, like Zechariah, is incredulous. “How can this be, since I am a virgin?” she asks. (Luke 1:34) Whereas Gabriel struck Zechariah mute for his incredulity, the angel is kinder to Mary. He informs her that “the Holy Spirit will come upon you, and the power of the Most High will overshadow you; therefore the child to be born will be holy; he will be called Son of God.” (Luke 1:35)\nBoth unexpected and seemingly impossible pregnancies drive home the same point, which Luke highlights as he notes, “For nothing will be impossible with God.” (Luke 1:37) Whatever challenges you are facing, hold onto this hope.\nIf Luke’s Gospel were performed on stage, it would be a musical, not a play. Throughout this story, actors break into song. In today’s reading, Mary pauses and offers the Magnificat taken from the first word of the Latin translation, which reads, Magnificat anima mea Dominum…” “My soul magnifies the Lord…” (Luke 1:46) This song is the crown jewel of Anglican choral evensongs. Many of the greatest composers of Church music – Bach, Vivaldi, Bruckner, Rachmaninoff, Rutter, Tallis, Ralph Vaughan Williams, Tavener, Stanford and Howells – have set Mary’s words to music. The Magnificat is one of the eight most ancient hymns of the Church and is perhaps the earliest Marian hymn.\nIt forms part of the Daily Office for Roman Catholics, Lutherans and Anglicans. It is central for the service of Evening Prayer in The Book of Common Prayer, where it is usually paired with the Nunc Dimittis, which is also taken from Luke’s Gospel. The Eastern Orthodox Church sings it during Matins.\nThe Magnificat is a revolutionary hymn, which speaks about radical societal change. It is the battle cry of the poor and marginalized. God “has brought down the powerful from their thrones, and lifted up the lowly; he has filled the hungry with good things, and sent the rich away empty.” (Luke 1:52-53) God’s preferential concern for the poor, for women and for those who are marginalized by society will resound throughout every chapter of Luke’s Gospel. It is a gospel of liberation.\nZechariah’s turn comes soon after. No longer mute, he opens his mouth and sings, “Blessed be the Lord God of Israel…” He offers the Benedictus taken from the first word of the Latin translation. Anglicans sing or say this canticle in Morning Prayer or Matins, after the New Testament lesson. It was uttered on the occasion of the birth of Zechariah’s son, John the Baptist. It is a song of thanksgiving for the messianic hopes of the Jewish nation. The second part of the canticle focuses on Zechariah’s son, who will play a great part in the story of human Redemption by God. John the Baptist will be the last of the great prophets, who will preach remission of sins before the coming of the Messiah. His words echo Isaiah 40:3, which John later applied to his own mission in Luke 3:4.\nEarlier, Elizabeth exclaimed in a loud cry, “Blessed are you among women, and blessed is the fruit of your womb.” (Luke 1:42) These words are familiar to Roman Catholics who pray the rosary. Was this, too, a song for Luke’s musical, which was cut short? Or was Elizabeth the one who first offered the Benedictus, for this is how her exclamation begins. Were her words later attributed to Zechariah, as the words of Miriam were possibly attributed to Moses following the Exodus? (Exodus 15:1-18; 20-21) We simply do not know.\nWould that all the Lord’s people were prophets, and that the Lord would put his spirit on them! (Numbers 11:29)\nO send out your light and your truth; let them lead me. (Ps. 43:3)\nThe Holy Spirit will come upon you, and the power of the Most High will overshadow you; therefore the child to be born will be holy; he will be called Son of God. (Luke 1:35)\nFor nothing will be impossible with God. (Luke 1:37)\nHe has brought down the powerful from their thrones, and lifted up the lowly; he has filled the hungry with good things, and sent the rich away empty. (Luke 1:52-53)\nWhat prayers have you uttered that God saw wisely not to answer or answer as you had asked? How and when have you ranted at God? In what ways are you committing the sin of failing to delegate to others? How are you allowing God’s light to lead you each day? Is the Holy Spirit the prime mover in your life? If not, why not? Do you believe that with God all things are possible? How are you working to respond to the needs of the poor and the marginalized?\nGod of Heavenly Disruption, you turn human lives upside down, putting down those who have lorded over others and lifting up the marginalized and oppressed. Help us to partner with you in lifting up those in need and restoring hope to them. Give us the faith to trust that with you all things are possible. In Jesus’s name we pray. Amen.\n© The Rev. Marek P. Zabriskie\nRector of St. Thomas Episcopal Church\nFort Washington, Pennsylvania']	['<urn:uuid:8ad4705f-f4f7-4b86-8bd0-235628087671>', '<urn:uuid:9f27b2aa-69ed-4909-9ead-5ed82deec29e>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T05:18:19.158519	6	78	4884
2	where is sundarbans mangrove forest located and what species live there	The Sundarbans, the world's largest mangrove forest, is located where the Ganges-Brahmaputra Delta meets the water, spanning across Bangladesh and India. Both countries' portions of the forest are listed as UNESCO World Heritage sites. The Sundarbans serves as a vast wildlife preserve and provides critical habitat for numerous species, most notably the Bengal tiger (for which it is the largest remaining habitat) and the estuarine crocodile.	"['The Ganges-Brahmaputra Delta, also known as Ganges Delta, Sunderbans Delta or Bengal Delta is situated in south Asia. The delta plain is formed by the meeting of the rivers Ganges, the Brahmaputra and Meghna which stretches across 5 coriparian (or water sharing) countries: Bangladesh, India, Nepal, Bhutan and China. Together, these three rivers drain a catchment area of about 1.72 million sq km, at the southern side of the Himalaya. Out of this large catchment area, only 7% lies in Bangladesh.\n|Rivers||Total catchment area (sq km)||Catchment area (sq km)|\nBangladesh itself is a great delta formed by the alluvial deposits of these three mighty Himalayan Rivers. The mighty Ganges and Brahmaputra rivers are called the ""Padma"" (pronounced \'Podda\' in Bengali) and the ""Jamuna"" respectively in Bangladesh. Both of these massive rivers join several other smaller tributaries to eventually become the Lower Meghna, forming the great Gangetic Delta. This has a surface area of some 100,000 sq km, making it the world\'s largest Delta. It stretches from Meghna River (in Bangladesh) on the east to the Hughli River (in India) on the west. Approximately two-thirds of the delta is in Bangladesh, the rest lies in the state of West Bengal, India. Kolkata and Chittagong are the principal seaports on the delta.\nWhere the land ends, the Bay of Bengal begins. The delta is approximately 220 miles (350 km) wide along the coast. As the rivers empty it discharges large quantities of sediment into the Bay of Bengal therefore most of the sea adjacent to Bangladesh is quite shallow.\nThe Bengal delta occupies an unique position among the larger deltas of the world for its varied and complex river and drainage system. The whole delta is cross-crossed by innumerable large and small channels of which some are decaying, some are active, while some others are being drained only by the tidal flow. In the northeastern part of the delta there are some abandoned or partially abandoned courses of decaying rivers while the eastern and southeastern delta is characterised by the flow of active rivers with heavy discharges.\nAt its widest point near Bhola Island, the river stretches to a yawning 12km-wide breadth on its final leg towards the sea. Seen from a boat, the distinctions between land, river, ocean and sky become decidely uncertain.\nThe world\'s largest mangrove forest, the Sundarbans, is located where the land meets the water. The Sundarbans forest spans Bangladesh and India, with each country’s forest listed as a UNESCO World Heritage site. The Sundarbans - which is Bangla for \'beautiful forest\' – is also a vast wildlife preserve and provides critical habitat for numerous species, including the Bengal tiger and the estuarine crocodile. In fact, it is the largest remaining habitat of the Bengal tiger.\nAn extensive and intricate web of rivers is Bangladesh\'s most significant geographical feature. There are about 405 rivers – known as “nodi” in Bengali - in Bangladesh of which 57 are transboundary rivers (i.e. crosses national borders). Out of these 57 rivers, 54 are common with India and 3 with Myanmar (also known as Burma).\nThe Ganges River, India’s most sacred river (known as ‘Ganga’ in Indian languages), divides into many distributaries including the Hughli (also spelt Hooghly) River near Kolkata. It enters western region of Bangladesh, near Rajshahi, and forms another major distributary: Padma Nodi.\nElsewhere, the Brahmaputra River enters northern region of Bangladesh and forms the Jamuna Nodi. Both the Padma (pronounced ""Podda"" in Bengali) and Jamuna merge near the town of Paturia at the heart of the country and continue downstream as just the Padma Nodi until it reaches the coast, some 250 km away. About halfway down the Padma, near the town of Chandpur, the Meghna Nodi joins from the eastern side.\n|4. Ganges-Padma||378 (Ganges 258, Padma 120)|\n|7. Brahmaputra-Jamuna||276 (Jamuna 207)|\n|8. Old Brahmaputra (Mymensingh)||276|\nThe rivers bring down the rich alluvial soil that forms the Ganges Delta, and they provide the main means of transportation throughout the country. Rivers are also a source of hydroelectric power, a notable example being the Karnaphuli River in Chittagong in the southeast.\nPreparing a complete list of the rivers of Bangladesh is more or less tough as often a single river possesses different names at different places. Even a five or six-kilometre segment has a different name upstream or downstream. Also a single name is used for different rivers in different locations.\nBecause the rivers are subject to constant and often rapid change, Bangladesh\'s topography never remains the same for long.\nA classic example of this occurred in 1787 when the Tista River experienced massive flooding. Waters were diverted eastward, where they met and reinforced the Brahmaputra River. The swollen Brahmaputra then cut into a minor stream and by the early 1800s the minor stream had become the river\'s main watercourse, now called the Jamuna. The Brahmaputra, now a considerably smaller river below the juncture with the Jamuna, still flows along the old course.\nWater is Bangladesh’s blessing and curse. The entire low-lying region is plagued for nearly half the year by severe storm surges and powerful low-pressure cyclones which result with the monsoons arriving from the Bay of Bengal. The monsoon rains cause the country’s three major rivers, the Padma, Meghna and Jamuna, and their tributaries to swell resulting in devastating floods, heavy damage to crops and shelters, and loss of human life. During the rest of the year, the dry season brings almost no rainfall, and droughts threaten the livelihoods of people and the health of the natural environment.\nAlmost all of the country\'s rainfall comes during June to September, as northerly winds blow moisture up from the Bay of Bengal into the mountain wall that is the Himalayas. As the moisture accumulates, the skies eventually belch out their water over Bangladesh and the surrounding regions. Like many other areas that experience monsoonal rain, it seldom rains all day every day. But there can be spells where it drizzles for an entire week, and nearly everything becomes sodden, even if it\'s inside and well out of the rain.\nIf you ever wanted to experience the living reality of the idiom \'when it rains, it pours\' Bangladesh is the place to be. During the yearly south Asian monsoon, almost all the water collected by the Himalayas in Nepal, north/northeast India and Bhutan transits through Bangladesh on its journey to the Bay of Bengal, deposition life-giving minerals to the soil all along the Ganges Delta, the largest river delta in the world. It is here that the mountains literally crumble to the sea. This has resulted in Bangladesh\'s flatland alluvial topography, which is the defining characteristic of the country except in the hilly regions of the southeast and northeast.\n...As the rivers have gradually shaped and reshaped this land, they have also shaped the destinies of its people. It would be a mistake to picture the historic locations of Bangladesh\'s rivers according to current maps. For instance, the Brahmaputra used to flow east of Dhaka\'s present location before a major flood caused it to change course over a 30-year span during the mid-18th century. Simultaneously, the Ganges has also undergone similar changes as it used to flow through West Bengal via the Hooghly River (today much smaller than it used to be).\nAs the rivers have gradually shaped and reshaped this land, they have shaped the destinies of its people. It would be a mistake to picture the historic locations of Bangladesh’s rivers according to current maps. For instance, the Brahmaputra used to flow east of Dhaka’s present location before a major flood caused it to change course over a 30-year span during the mid-18th century. Simultaneously, the Ganges has also undergone similar changes, as it used to flow through West Bengal via the Hooghly River (today much smaller that it used to be).\nNowhere is this destiny more uncertain than in the country’s two disaster-prone areas. Firstly, the coast bordering the Bay of Bengal is vulnerable to tidal surges from cyclones. Secondly, the country’s char areas, or river islands, are also extremely prone to seasonal flooding. These islands lie mostly in the northern reaches of the Jamuna River of Rajshahi Division. Many inhabited islands are destroyed and reformed each year by flooding. Despite the fact most of the islands are little more than infertile sandbars, poverty forces millions of people to live on them under the risk that their houses could be swept away each year.\nIn the Lower Meghna region, another area of exposure lies directly adjacent to the Bay of Bengal. Here, two processes of land loss and land accretion happen simultaneously. While the Meghna tears away strips of land beneath the villages each year, its decreasing speed causes it to deposit massive amounts of Himalayan silt into the bay, forming new land that becomes populated almost immediately despite the fact that the precious land doesn’t become fully fertile for years. Some geologists even claim that Bangladesh is ‘gaining landmass’, putting the supposed doomsday scenario of climate change into question.\nM. H. Ali, author of ""Fundamentals of Irrigation and On-Farm Water Management, Volume 1"" (2010)\n© Londoni Worldwide Limited']"	['<urn:uuid:d49b9220-11d7-473b-a025-6868742c0e08>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	11	66	1522
3	innovative mining techniques benefits drawbacks sustainability perspective	Modern In-situ Recovery (ISR) mining offers notable sustainability benefits compared to conventional methods - it causes minimal surface disruption and eliminates the creation of large holes and debris that typically damage landscapes and threaten biodiversity. However, ISR has specific drawbacks - it requires permeable deposits and appropriate geological conditions to work effectively. From an environmental perspective, while ISR reduces visible surface damage, both methods pose risks of water contamination - traditional mining through erosion and chemical runoff, and ISR through the leaching solutions used. For ISR to be sustainable, extensive hydrogeological testing is needed early in projects to assess suitability.	"['CSA Global‘s Principal Resource Geologist, Dr Maxim Seredkin will present on ‘In-situ Recovery for Non-Uranium Metals’ between 18-25 May 2019 at the ALTA 2019 Nickel-Cobalt-Copper, Uranium-REE-Lithium Conference in Perth, Western Australia.\nALTA 2019 is a world-class annual metallurgical conference, now in its 24th year, and a leading platform for innovation. The emphasis of the program is practical rather than academic, and the themes running through the conference are the various aspects of technology and project development.\nThe conference will feature highly-focused programs, topical forums, panels and presentations by key international speakers and will consist of five conferences in one week:\n- Nickel-Cobalt-Copper (20-22 May) including Pressure Acid Leaching Forum & Panel\n- Uranium-REE (23 May) including Developments in IX Forum & Panel\n- Gold-PM (23 May) including Fit-for-Purpose Leaching Systems Forum & Panel\n- Lithium Processing (24 May) including Novel Lithium Processes Forum & Panel\n- In Situ Recovery (ISR) (24 May) including Enhancing ISR Permeability Forum & Panel\nIn-situ recovery (ISR) transfers hydrometallurgical processing of mineralised bodies to the subsurface to directly obtain solutions of commodities. As a result, there is little surface disturbance. For ISR to be successful, however, deposits need to be permeable. Furthermore, commodities need to be readily amenable to dissolution by leaching solutions over a reasonable period of time, with an acceptable consumption of leaching reagents.\nISR accounts for more than 50% of world uranium production. Other than uranium, the most common commodity extracted by ISR methods is copper. Copper is extracted by sulfuric acid. Several deposits were subject to ISR in Arizona. The Gumesevskoye deposit in the Urals and Morani deposit in Zambia are currently in operation by ISR, while the Florence ISR mine in Arizona is nearing production. Some deposits in South Australia (Moonta, Kapunda) are considered to have potential for ISR.\nAfter copper, the next most popular commodities for ISR are gold and its by-product silver. These commodities are extracted by chlorine or sodium hypochlorite. The Gagarskoe and Dolgy Mys deposits in the Urals are currently in operation. Several projects in Russia are nearing production and some projects in Australia and the USA are currently being assessed for ISR potential.\nISR is also likely to be used for extraction of nickel and cobalt in the near future. Several field cluster tests have been completed on four nickel-cobalt deposits in the Urals and Eastern Kazakhstan. Sulphurous anhydrite and sulfuric acid were used as alternate reagents for leaching with pure metals produced. Kazakhstan will potentially be the first country to start a nickel and cobalt ISR mine. Manganese is also a commodity which can be mined by a similar lixiviant. Pregnant solutions with this metal were also obtained in field tests.\nA successful push pull test for tungsten using a complex lixiviant with hydrochloric and ethanedioic acids was also recently completed in Kazakhstan.\nPotential methods for ISR of base metals and PGE have also been recently developed. Zinc can be leached by a lixiviant with sulfuric acid and chloride of sodium, while lead can be leached by methanesulfonic acid. PGE can be leached by a complex lixiviant with ammonium thiocyanate and ferric chloride. Successful tests have also been completed for boric acid and lithium.\nScandium is extracted as by-product from pregnant uranium solutions at Dalur in Russia. Successful tests have also been completed which show potential for extraction of rhenium, molybdenum, selenium, vanadium, rare earth, yttrium as by-products of pregnant uranium solutions.\nISR could be both an interesting and plausible alternative mining method for many commodities, however deposits require specific characteristics and conditions for its successful application. Hydrogeological and hydrometallurgical test work should be completed very early during project evaluation to assess the potential to use ISR methods.\nMEET OUR PRESENTER\nDr Maxim Seredkin\nPrincipal Resource Geologist\nDr Seredkin has more than 18 years’ experience in exploration, mining production and resource estimation. He has experience in a range of commodities and reporting codes (including JORC and NI43-101). Maxim is particularly sought after in industry and academia for his expertise in uranium, he has in-depth experience across a range of geological settings and deposit types. He has specialist skills in the application of in-situ recovery for uranium extraction based on 5 years of direct operational experience. In addition to uranium expertise, Maxim has experience in bauxite, rare earths and iron ore. Maxim has a strong scientific background, with research experience in ore genesis, petrology and the mineralisation of carbonatite, alkaline and ultramafic complexes, in hydrothermal-metasomatic gold and tungsten deposits.\nEmail us here to connect.', ""Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ...""]"	['<urn:uuid:318a23d2-1448-44be-9dbe-3dfeea411500>', '<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	7	100	2205
4	superstorm sandy damage great lakes east coast comparison wave impacts	While Superstorm Sandy primarily battered the east coast of the U.S., its damage extended to the Great Lakes region as the storm moved inland across the Midwest. The storm manifested as a hybrid of nor'easter blizzard and hurricane conditions in the northeast, while in the Great Lakes it caused extensive damage to structures like the Port Clinton Harbor East Jetty through oversized waves.	"['Contract awarded to repair Port Clinton Harbor East Jetty\nPORT CLINTON - The east jetty many residents and visitors use to fish, walk and take sunset pictures from will be closed this summer — but for good reason: $300,000 in renovations.\nThe U.S. Army Corps of Engineers (USACE), Buffalo District has awarded a $375,243 contract to GeoGradel of Toledo, to repair a sections of the Port Clinton Harbor East Jetty that was damaged during Superstorm Sandy.\nGeoGradel will begin work after the Fourth of July holiday and continue through the summer into early October.\nThe proposed project consists of repairs to the jetty head by replacing existing damaged structure with new rubble mound armor stone overlay, a 4-foot wide cast-in-place concrete walkway with new light foundation and returning the structure crest to the original elevation.\nIn October 2012, Superstorm Sandy battered the east coast of the U.S., but damage was felt on the Great Lakes as well as the storm moved inland across the Midwest.\n""The Port Clinton Harbor East Jetty experienced extensive damage during Sandy due to oversized waves and the extreme angle they were hitting the structure,"" said Capt. Kelly Polashenski, USACE project manager. ""Because the damage was caused by Sandy, the jetty was eligible for funding to repair the structure using Superstorm Sandy appropriations.""\nGeoGradel project manager Wayne Moon said the company would be using a crane barge, a tugboat and a material barge to work on the project.\nGeoGradel is also working on portions of the Lorain East Breakwater wall and at Edgewater Park in Cleveland, which were both damaged by Sandy, he said.\n""We\'ll put up a barricade 250 feet around from our work area and other signs detailing the project,"" Moon said Thursday.\nMoon expected to have an average of five workers on-site each day, five days a week. Moon noted the river is especially busy with boaters and ferries during the weekends, so they will avoid them.\nThe river\'s ""No Wake"" sign and ""Power Line"" signs will be temporarily removed, he said, but the laws still apply to those on the water.\n""We ask that boaters please exercise caution when in the area,"" Moon said.\nThe USACE began studying the east jetty in June 2010, with engineers noting that failure of either jetty would result in ""increased shoaling choking the mouth of the Portage River as well as increased wave action propagating up the river.""\nFerry service to the islands would be curtailed and effects on marina operations would be ""detrimental,"" the study noted.\n""The public park and beach are protected by the jetties from wave action due to a west to northwest wind and would likely suffer erosion,"" a release from USACE read.\n""I welcome this step in restoring and improving this vital harbor,"" Congresswoman Marcy Kaptur (D-Toledo) said. ""The Port Clinton Harbor is central to several local private marinas, almost eighty seasonal charter fishing boats and the local recreational boating community.""\n""Port Clinton\'s recreation and tourism industry is a key component of our state\'s economy. This important restoration project will help to protect the shoreline and ensure that the Port Clinton Harbor can continue to support the region\'s boating activity and tourism jobs,"" U.S. Sen. Sherrod Brown said.', 'NASA officials nicknamed Hurricane Sandy “Bride of Frankenstorm.” Strange behavior patterns—hitting the northeast as a mix of nor’easter blizzard and hurricane conditions created a powerful hybrid storm that affected many communities. In Maine, we felt the storm’s most severe impacts the night of the full moon on October 29th. Footage of storm surge on the news looked like the forceful wave action in “Thunder Hole” at Acadia National Park. Throughout New England, New York and New Jersey, many people were still without power when the nor’easter hit this week. Hurricane Sandy’s unusual hybrid classification and other factors set a precedent. Coupled with the tides of the full moon, storm surge was more intense, causing more flooding to occur. Are we likely to see and experience powerful hybrid storms like this in the future? What tools are available to predict storm surge?\nForecasters called Hurricane Sandy a “perfect storm.” View photos of the storm as seen from space. Last winter Strange Wetlands reported on the Red Cross/Red Crescent’sinvolvement in the IPCC report on the link between extreme weather disasters and climate change. This week Climate Central’s Surging Seas tool demonstrated how effects of climate change, including sea level rise and storm surge, made Hurricane Sandy worsethan it might have been otherwise.\nFederal agencies such as the U.S. Army Corps of Engineers and NASA have been measuring storm surge for many decades, since the 1960s (see above)—long before most people started talking about sea level rise. According to a NASA presentation on An Analysis of Storm Surge Attenuation using USGS, FEMA and NASA data, there is historical data to support the claim that wetlands significantly reduce storm surge. Wetland scientists in the 1960s asserted that 2.7 miles of wetlands reduced storm surge by 1 foot. More recent data from Hurricane Rita was used to assess the storm impacts to wetlands (such as causing wetland loss) as well as wetlands’ role in lessening the effects of storm surge. Therefore wetland losses along the Gulf of Mexico coastline in Louisiana, for instance, and along other areas of coastline on the eastern seaboard, intensified the amount of storm surge during recent hurricanes, such as Hurricane Irene and Sandy. (Fitzpatrick, et. al. 2008) Also see Storm Surge Reduction by Wetlands.\nWhile SLAMM—Sea Level Affecting Marsh Model—may be familiar to you, a tool used in analyzing sea level rise, especially with respect to wetlands, have you heard of SLOSH? Sea, Lake and Overland Surges from Hurricanes, (SLOSH) is a tool used to analyze storm surge. Hurricane Sandy’s storm surge was mapped before it made landfall. The SLOSH model was applied to Hurricane Sandy prior to Oct. 29th and it analyzed surges of various levels (2 feet, 3 feet, 4 feet, etc.) At most locations, meteorologists predicted unprecedented levels of surgeusing this tool and other analyses. Tools like SLOSH are only as good as the available data. Future budget cuts threaten data collection tools, such as ocean bouys. Other storm surge analysis tools were used to predict Sandy’s surge levels and ultimately, citizens were evacuated in areas where the path of the storm surge was predicted on the maps using those models.\nSome useful fact sheets and further reading on the importance of wetlands in preventing storm surge are linked below:\nStorm surge & wetlands in Louisiana (NWF fact sheet)\nMitigating Storm Surge with Vegetation & Wetlands (Army Corps of Engineers, 2007)\nAnalysis of Storm Surge Attenuation & Wetlands (NASA) (2008)\nThe potential of wetlands in reducing storm surge (Ocean Engineering, 2010)\nHurricane Sandy Geospatial Resources (NOAA Digital Coast, 2012)']"	['<urn:uuid:2ed30fa1-7978-45f0-b481-8ed4e88b103d>', '<urn:uuid:049783d6-d9ab-43db-803d-b695b12e035d>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T05:18:19.158519	10	63	1129
5	need info about building house in desert climate what protective elements modern architecture uses against harsh weather	Modern architecture in harsh desert climates employs several protective elements while maintaining connection to nature. The roof stands as the main symbol of protection, becoming abstracted and flattened to create a boundary between sky and house. In the Kaufmann House, which Neutra described as being built in 'the most uninhabitable site next to the moon', special equipment was needed along with integrated radiant heating and cooling systems. The house also featured protective elements like movable steel louvers that provided sun screening. While modern architecture emphasizes connection to nature through extensive use of glass, it still maintains necessary protection from rain, wind, and extreme temperatures through these various architectural features.	"['Richard Neutra: Chuey House\nSince the beginning, protection from the elements was the initial motivation for the invention of Architecture. In the past this has been accomplished rather well with walls and sloped roofs. The walls of pre-modern structures were almost always load-bearing. This required them to have a certain thickness. This thickness also allowed the walls to serve as great insulators from temperature fluxes. Small windows allowed a glancing connection to nature. This need for connection as well as protection has been a vital duality in architecture since it’s beginnings. The use of small windows and light-filled courtyards allowed the benefits of nature to inhabit the dwellings while leaving the discomforting elements out.\nIn the Modern Era it’s evident that the desire for a closer connection to nature took root. The increased use of glass combined with the separation of structure from facade (Le Corbusier’s Domino frame) allowed this development to occur. For mainly spiritual reasons, the modern movement sought to blur the lines between inside and out. They sought to allow buildings to breath from under the weight of gravity. Architecture must protect, but beyond that the Modern Architects devised a number of tricks to give the illusion of free exchange between inside and outside. In this world of connection the wall disappears and the roof stands out as the main symbol of protection.\n- Horizontal Thrust: The horizontal line is the line that goes along with nature. The vertical line declares its independence from nature. The emphasis of the horizontal can most clearly be seen in the early development of the Prairie Style by Frank Lloyd Wright. These early houses seek a connection with nature by blending into the horizontal countryside they inhabit. The house becomes less intrusive, less about being clearly man made, and more a part of their environment. From inside the low horizontal lines seen in railings and overhangs compliment the distant line on the horizon and include it as part of the aesthetic experience of the house. Naturally this would work best in the actual countryside where the horizon is uninterrupted. Prairie Houses in urban areas tend to lose this effect.\nMies Van Der Rohe: Farnsworth House demonstrating floor to ceiling glass and homogenous materiality.\n-Floor to ceiling glass: The use of floor to ceiling glass is the most obvious and effective way to establish a visual connection between inside and out by foiling any obstructions of view. The perfect example of this is at the Farnsworth House. The true effectiveness of the glass must be experienced in person to understand just how open and connected the house is with nature. The only cues that there is any disconnect from the outside is in the perception of wind. One can see the wind whipping the tree, but one can’t feel or hear it. Well-cleaned glass goes a long way in establishing a kinship with nature in domestic houses to a degree unimagined even a hundred years prior.\n-Homogenous use of inside and outside materials: When using large expanses of glass, the use of the same materials on the same plane in the exterior and the interior is very effective at creating the illusion of connection to the outside. There are many examples of this. The Farnsworth house, once again, uses travertine on the outside porch as well as in the inside living space. The rustic stone floor in the living room at Fallingwater seamlessly pours out onto the terraces.\nMVDR: Project for a brick country house. The extension of walls.\n-The extension of walls: Where an opaque wall abuts a large amount of glass, the extension of the wall to the outside creates an illusion of continuation from outside to inside. No built project can convey this idea better than the floor plan Mies Van Der Rohe drew for the Brick Country House. The plan is intentionally abstract to read, and the inside is difficult to distinguish from the outside. This is accomplished because the walls do not enclose space, they envelop around it, creating incidental room-like areas. These walls extend beyond their necessary edges and reach out into the landscape. They leave the impression of continuation into infinity. This is similar to what happens in Mondrian’s paintings: the lines slide past each other and beyond the painting implying a snapshot of an infinite condition. This outreach into nature is an embrace that invites nature to connect with the interior spaces.\nNeutra: Kaufmann House showing the overhang of roof and disappearing glass.\n-The overhang of roof: Similar to the extension of walls, the extension of roof into the outside can blur the boundary between inside and out. This technique is only truly effective when the overhang is flush with the interior ceiling. Richard Neutra’s houses are really good with the connective overhang. His most famous example of this roof projection can be found in his residence for Edgar J. Kaufmann (yes the same guy that commissioned Fallingwater). The floor to ceiling glass in the living room reveals an overhang that extends far beyond the glass wall. This effectively confuses the in/out boundary. Neutra’s use of beams on the roof that extend from inside to outside is also very effective in helping this illusion. Sometimes the beams continue beyond the roof and rest on thin columns. The columns are usually the same as the ones on the inside. All of these elements collaborate to create an embracing connection with nature.\nShigeru Ban: Curtain Wall House: Disappearing glass.\n-Disappearing glass: Glass walls that can be slid away on tracks or sunken or raised like garage doors also quite literally break the connection between inside and out. As I’ve mentioned in the previous post, the Tugendhat House by Mies Van Der Rohe had glass that could roll into the floor below and completely disappear. Neutra’s Kaufmann House has giant floor to ceiling glass planes that can slide away from the edges, completely opening them up. To dramatic effect the literally titled Curtain Wall House by Shigeru Ban is a completely open house that has large curtains on the perimeter. These curtains can be opened so that the house is a complete exterior space with railings around the edges. The desire by architects to destroy the barrier between inside and out cannot be accomplished more effectively than simply doing away with walls completely!\nFLLW: Second Jacobs House. The pool is the circle on the left of the glass side.\n-The pool: Many modern architects have used a wading pool that is half indoors and half outdoors to reinforce the idea of connection with nature. The Second Herbert Jacobs House of 1944 by Frank Lloyd Wright is one of the earlier examples I am aware of. Here, a circular pool is situated on the facade so that one half is indoors and the other is outdoors. Lily pads and swimming fish contribute to the effect of connecting pools. The Nesbitt House by Richard Neutra actually predates the Second Jacobs House by two years and also incorporates a half-in half-out wading pool in the entryway. The single large pane of glass above the pool is effective in blurring the exterior division of this pool.\nThese are just a few of the many aesthetic devices used to connect architecture with nature. The protective element will not be discussed in as much detail as the connective elements because the Modernists were trying to deemphasize this. Protection is found primarily in the roof, which became abstracted and flattened into its essential nature: the boundary between sky and house. Architecture will always need to protect its inhabitants from the rain and the wind and whatever else Mother Nature decides to throw at us. Because of this I do not believe that the spiritual desire for the connection will ever completely conquer the pragmatic need for protection. Next post, I will explore how all of these techniques are incorporated in the work of Albert Frey. Until then, faithful readers.....', 'It would begin with one full year of research. Every day, for four of those months, the architectural restoration team donned gloves and combed through the archives at the UCLA Research Library to solve the puzzle of Richard Neutra’s famed Kaufmann House, completed in 1946 and since fallen into disrepair. Floors were cracked, casework had been removed, portions of the land had been sold, and the square footage had nearly doubled through additions over the years.\nSans original plans, they visited the archives daily, redrawing everything original they could capture by hand as part of an intensive, five-year restoration project taken on by the new homeowners. ""There are truly iconic, important pieces of architecture, and this is one of those,"" says Ron Radziner, design partner at Marmol Radziner. ""This is one of the 20 best homes in this country, and it deserved that level of restoration.""\nThe year was 1993, and the internet was not what it is today, but they kept on. In addition to the work at UCLA, research also included time spent on-site, unearthing years of additions and modifications to the home in search of any clues they could find, such as the original mica plaster hidden behind an electrical box, identified through microscopic evaluation.\nIt would also include sifting through the archives of Julius Shulman, the world-renowned photographer who documented the home in its heyday and shared never-before-seen photos with the restoration team. It was those photos that would drive the entire restoration.\n""This is one of the 20 best homes in this country, and it deserved that level of restoration.""\n""After looking at the archives of Julius Shulman, it led us to better understand that it was quite a work of sculpture and much richer than really anyone knew, because nobody had seen Julius’ archives,"" says homeowner Brent Harris, who undertook the restoration with his former wife, Beth Harris. ""The decision was made when we saw the famous Shulman photo from 1947, the twilight photo. It was taken in one snap—a one-time exposure. It seemed like right place to take the home back to.""\nThe research team also discovered letters from Neutra to the original owner, Edgar Kaufmann, a wealthy department store owner who sought this property as a vacation home. He passed away in 1955, when the home was sold to a series of owners, including Barry Manilow. The letters helped solve what Harris describes as ""a gigantic, national scavenger hunt for the pieces that were gone."" They contained specs, sketches, and material details, which led to the team to identify the original buff stone. It originated from a quarry in Utah. Conquering all obstacles in pursuit of authentic restoration, the team trekked to the original source to procure the correct stone, which bears a sunset hue unlike the muddy shade of buff stone you\'ll find across the masses today.\nA final piece of the puzzle was consulting with several experts, including another architect who helped build Palm Springs, Albert Frey. ""He taught me the importance of materials,"" says Harris, who worked closely with Marmol Radziner throughout the restoration.\nIt wasn’t enough to just acquire the original stone; it would have to be set by the best masons they could find. It was initially installed using a dry stack technique—new to Neutra’s work—whereby mortar is applied to the rear of the stone, resulting in a natural look invisible to the eye. A mason worked at the site for a year and a half, chiseling and cutting blocks precisely in place to create a pleasing mosaic. While perfecting the brick took quite some time, just fewer than 10 percent of the brick was restored, demonstrating stone mason Clive Christie\'s commitment to his craft. Tops and bottoms were cut smooth to sit in horizontal position, allowing the sides and faces to be more organic. But the stone wasn’t the only meticulous detail of the renovation.\nThe fascia sent the team on wild goose chase—or rather a crimped metal chase. They had never before seen that vertical pattern in metal and set out on a mission to reproduce the material. After sending 3"" x 3"" samples to sheet metal fabricators nationwide, they found a clue. A fabricator in Kansas City, Missouri, recognized the pattern—and likely made the original. They resuscitated an old machine they hadn’t used in 30 years to replicate the design. ""We have worked on many renovations and have never done a project to that level of authenticity,"" says Radziner. ""This was about expressing what Neutra originally intended, not us.""\nAbout a decade before the home was designed, Kaufmann had commissioned another notable architect to design a home for him in Pennsylvania. The architect was Frank Lloyd Wright—and the project, Fallingwater. But unlike Frank Lloyd Wright, whose designs grow out of the landscape, Neutra’s designs descend upon it. The glass-and-steel home was, and still is, bold, given its harsh desert landscape—or as Neutra’s described it, the most uninhabitable site next to the moon. ""It’s a bit of a stretch, but I definitely felt it several times with the temperature and special equipment needed to bring this about,"" says Harris.\nThe Kaufmanns only lived in the home some 60 days a year, and Neutra designed it as such. While the integration of radiant heating and cooling at that time made the home somewhat of a prototype, the home could not sustain long-term habitation without the intervention of air conditioning given the omnipresence of wood. It is one of just a few modern-day amenities that have been thoughtfully incorporated into the restoration. Ductwork was carefully inserted below ground. Return air can flow beneath a bed, and in the dining area ceiling, where wood meets plaster, an air return is cleverly concealed.\n""We really were true to the original goals of Neutra,"" says Radziner. ""And we tried to very authentically recreate and restore what wasn’t there and make it authentic as humanly possible.""\nThe home is lauded in part because of Shulman’s work, but also because of its design, which has endured the test of time. Its pinwheel shape leaves the home without a backside, creating an organic orientation, photogenic from every angle.\n""Its place in history as a home—a pristine, modern sculpture in the raw desert—is incredible,"" says Radziner. ""As you walk around and experience it, it’s incredibly dynamic. The significance of this home in the fundamental sense is that it’s moving to people.""\nThe meticulous restoration included the sourcing of the original toilets and tables. The cutting-edge, movable and controllable steel louvers were recreated, serving both as privacy and sun screening for the top-floor, open-air ""gloriette""—Neutra’s solution for zoning that restricts second stories. Subcontractors were given rigorous tests to evaluate not only their abilities, but also their interest in the project. And while a pool house was added to include amenities that accommodate modern-day lifestyles, it was designed to complement—never to compete—with the restored home, which has since been designated by the Palm Springs City Council as a Class 1 Historic Site, the most prestigious historic designation. Says Harris, ""It pioneered the field for historic modern architecture.""\n""Its place in history as a home—a pristine, modern sculpture in the raw desert—is incredible.""\nRichard Neutra was a disruptor. He built what he described as a machine amidst a harsh desert landscape that defied all odds. And thanks to the team who respected its significance and dedicated so much of their lives to honoring the integrity of the design, it will endure—as good design does. Says Harris, ""My favorite part is seeing it through other people’s eyes. Ultimately, I’m really happy that it has inspired people to restore modern houses.""\nShop the Look\nRelated Reading: Iconic Perspectives: Richard Neutra’s VDL Studio & Residences\nArchitect: Richard Neutra\nConsultants: Cass Rogers (structural); Mel Bilow & Associates (mechanical); John Snyder & Associates (electrical); Seebohm Ltd. (architectural conservation); Reginald Hough (concrete); Eric Lamers and William Kopelk (landscape)\nGet the Renovations Newsletter\nFrom warehouse conversions to rehabbed midcentury gems, to expert advice and budget breakdowns, the renovation newsletter serves up the inspiration you need to tackle your next project.']"	['<urn:uuid:97e39df0-1f3b-42fd-b3c1-ced385b0499d>', '<urn:uuid:dff518a2-a27c-4743-84a4-c497aa403aa8>']	open-ended	with-premise	long-search-query	distant-from-document	three-doc	novice	2025-05-13T05:18:19.158519	17	109	2671
6	quantum computing applications computational advantages classical computers comparison	Quantum computing offers significant computational advantages over classical computers in specific applications. While classical computers perform linear calculations using binary bits (1 or 0), quantum computers use qubits that can exist in multiple states simultaneously through superposition, enabling parallel processing of all possibilities at once. This allows quantum computers to solve certain problems orders of magnitude faster - for example, using eight qubits to represent all numbers between 0 and 255 simultaneously, while classical computers need eight separate bits. This massive parallel processing capability makes quantum computers particularly powerful for complex calculations in fields like drug discovery, climate modeling, financial optimization, and cryptography. Recent demonstrations highlight this advantage - Google reported completing a calculation in minutes that would take classical computers 10,000 years, while Chinese researchers claimed achieving computations 100 trillion times faster than conventional computers.	"['IBM scientists say they have made two critical advances in an industrywide effort to build a practical quantum computer, shaving years off the time expected to have a working system.\n""This is critical,"" said Jay Gambetta, IBM\'s manager of theory of quantum computing. ""The field has got a lot more competitive. You could say the [quantum computing] race is just starting to begin... This is a small step on the journey but it\'s an important one.""\nGambetta told Computerworld that IBM\'s scientists have created a square quantum bit circuit design, which could be scaled to much larger dimensions. This new two-dimensional design also helped the researchers figure out a way to detect and measure errors.\nQuantum computing is a fragile process and can be easily thrown off by vibrations, light and temperature variations. Computer scientists doubt they\'ll ever get the error rate down to that in a classical computer.\nBecause of the complexity and sensitivity of quantum computing, scientists need to be able to detect errors, figure out where and why they\'re happening and prevent them from recurring.\nIBM says its advancement takes the first step in that process.\n""It tells us what errors are happening,"" Gambetta said. ""As you make the square [circuit design] bigger, you\'ll get more information so you can see where the error was and you can correct for it. We\'re showing now that we have the ability to detect, and we\'re working toward the next step, which would allow you to see where and why the problem is happening so you can stop it from happening.""\nQuantum computing is widely thought to be the next great step in the field of computing, potentially surpassing classical supercomputers in large-scale, complex calculations.\nQuantum computing would be used to cull big data, searching for patterns. It\'s hoped that these computers will take on questions that would lead to finding cures for cancer or discovering distant planets jobs that might take today\'s supercomputers hundreds of years to calculate.\nIBM\'s announcement is significant in the worlds of both computing and physics, where quantum theory first found a foothold.\nQuantum computing, still a rather mysterious technology, combines both computing and quantum mechanics, which is one of the most complex, and baffling, areas of physics. This branch of physics evolved out of an effort to explain things that traditional physics is unable to.\nWith quantum mechanics, something can be in two states at the same time. It can be simultaneously positive and negative, which isn\'t possible in the world as we commonly know it.\nFor instance, each bit, also known as a qubit, in a quantum machine can be a one and a zero at the same time. When a qubit is built, it can\'t be predicted whether it will be a one or a zero. A qubit has the possibility of being positive in one calculation and negative in another. Each qubit changes based on its interaction with other qubits.\nBecause of all of these possibilities, quantum computers don\'t work like classical computers, which are linear in their calculations. A classical computer performs one step and then another. A quantum machine can calculate all of the possibilities at one time, dramatically speeding up the calculation.\nHowever, that speed will be irrelevant if users can\'t be sure that the calculations are accurate.\nThat\'s where IBM\'s advances come into play.\n""This is absolutely key,"" said Jim Tully, an analyst with Gartner. ""You do the computation but then you need to read the results and know they\'re accurate. If you can\'t do that, it\'s kind of meaningless. Without being able to detect errors, they have no way of knowing if the calculations have any validity.""\nIf scientists can first detect and then correct these errors, it\'s a major step in the right direction to building a working quantum computing system capable of doing enormous calculations.\n""Quantum computing is a hard concept for most to understand, but it holds great promise,"" said Dan Olds, an analyst with The Gabriel Consulting Group. ""If we can tame it, it can compute certain problems orders of magnitude more quickly than existing computers. The more organizations that are working on unlocking the potential of quantum computing, the better. It means that we\'ll see something real that much sooner.""\nHowever, there\'s still debate over whether a quantum computer already exists.\nA year ago, D-Wave Systems Inc. announced that it had built a quantum system, and that NASA, Google and Lockheed Martin had been testing them.\nMany in the computer and physics communities doubt that D-Wave has built a real quantum computer. Vern Brownell, CEO of the company, avows that they have.\n""I think that quantum computing shows promise, but it\'s going to be quite a while before we see systems for sale,"" said Olds.\nIBM\'s Gambetta declined to speculate on whether D-Wave has built a quantum computing but said the industry is still years away from building a viable quantum system.\n""Quantum computing could be potentially transformative, enabling us to solve problems that are impossible or impractical to solve today,"" said Arvind Krishna, senior vice president and director of IBM Research, in a statement.\nIBM\'s research was published in Wednesday\'s issue of the journal Nature Communications.\nJoin the CIO Australia group on LinkedIn. The group is open to CIOs, IT Directors, COOs, CTOs and senior IT managers.', 'Quantum computing is a space of registering zeroed in on creating PC innovation dependent on the standards of quantum hypothesis (which clarifies the conduct of energy and material on the nuclear and subatomic levels). PCs utilized today can just encode data in bits that take the worth of 1 or 0—confining their capacity.\nQuantum computing, then again, utilizes quantum bits or qubits. It tackles the special capacity of subatomic particles that permits them to exist in more than one state (i.e., a 1 and a 0 simultaneously).\nQuantum registering takes advantage of the perplexing conduct that researchers have been noticing for a really long time in nature’s littlest particles – think molecules, photons or electrons. At this scale, the old-style laws of material science stop applying, and on second thought we shift to quantum rules.\nWhile analysts don’t comprehend everything about the quantum world, what they can be sure of is that quantum particles hold huge potential, specifically to hold and handle a lot of data. Effectively managing those particles in a quantum PC could trigger a blast of figure power that would incredibly propel development in many fields that require complex computations, similar to sedate revelation, environment demonstrating, monetary enhancement, or coordination.\nQuantum PCs perform computations dependent on the likelihood of an article’s state before it is estimated – rather than only 1s or 0s – which implies they can possibly process dramatically more information contrasted with traditional PCs.\nTraditional PCs complete coherent tasks utilizing the clear situation of an actual state. These are typically double, which means their tasks depend on one of two positions. A solitary state -, for example, on or off, up or down, 1 or 0 – is known as a little.\nIn quantum computing, activities rather utilize the quantum condition of an item to deliver what’s known as a qubit. These states are the vague properties of an article before they’ve been identified, as the twist of an electron or the polarization of a photon.\nRather than having a reasonable position, unmeasured quantum states happen in a blended ‘superposition’, similar to a coin turning through the air before it lands in your grasp.\nThese superpositions can be caught with those of different articles, which means their ultimate results will be numerically related regardless of whether we know yet what they are.\nThe complicated arithmetic behind these agitated conditions of trapped ‘turning coins’ can be connected to uncommon calculations to make short work of issues that would take an old-style PC quite a while to work out… on the off chance that they might at any point compute them whatsoever.\nSuch calculations would be helpful in tackling complex numerical issues, delivering hard-to-break security codes, or anticipating numerous molecule cooperations in substance responses.\nHow Does a Quantum Computer Respond?\nUtilizing qubits, quantum PCs could buzz through estimations that would require some investment — on the off chance that they could even complete them.\nFor instance, the present PCs utilize eight pieces to address any number somewhere in the range of 0 and 255. Because of highlights like superposition, a quantum PC can utilize eight qubits to address each number somewhere in the range of 0 and 255, all the while.\nIt’s an element like parallelism in registering: All prospects are figured without a moment’s delay rather than successively, giving colossal speedups.\nIn this way, while an old-style PC ventures through long division computations each in turn to factor a humongous number, a quantum PC can find the solution in a solitary advance. Blast!\nThat implies quantum PCs could reshape entire fields, similar to cryptography, that depend on figuring what is today inconceivably enormous numbers.\nHow Does Quantum Computing Work?\nTry not to hope to construct your own quantum PC like a DIY PC with parts searched from rebate canisters at the neighborhood gadgets shop.\nThe modest bunch of frameworks working today regularly require refrigeration that establishes working conditions only north of outright zero. They need that registering cold to deal with the delicate quantum expresses that power these frameworks.\nIn an indication of how hard building a quantum PC can be, one model suspends a molecule between two lasers to make a qubit. Attempt that in your home studio!\nQuantum processing takes nano-Herculean muscles to make something many refer to as a trap. That is when at least two qubits exist in a solitary quantum express, a condition now and again estimated by electromagnetic waves simply a millimeter wide.\nWrench up that wave with a hair a lot of energy and you lose snare or superposition, or both. The outcome is a loud state called decoherence, the identical quantum registering of the blue screen of death.\nQuantum computing benefits\nScientists have dove profoundly into the quantum PC versus old-style PC examination and have since a long time ago anticipated that quantum PCs could handle particular kinds of issues — particularly those including an overwhelming number of factors and expected results, similar to recreations or streamlining questions — a lot quicker than any traditional PC.\nBe that as it may, presently we’re beginning to see traces of this expected becoming reality.\nIn 2019, Google said that it ran an estimation on a quantum PC in only a couple of moments that would take a traditional PC 10,000 years to finish. Barely a year after the fact, a group situated in China made this a stride further, asserting that it had played out estimation in 200 seconds that would take a standard PC 2.5B years — 100 trillion times quicker.\nGenuine Example of a Quantum Computer\nGoogle (GOOG) is burning through billions of dollars on its arrangement to assemble its quantum PC by 2029. The organization has opened aground in California, called Google AI, to assist it with meeting its objective. Google has been putting resources into this innovation for quite a long time. Also, so have different organizations, like Honeywell International (HON) and International Business Machine (IBM). IBM hopes to hit significant quantum registering achievements before long.\nWhile a few organizations have assembled individual (albeit costly) quantum PCs, there is as yet nothing accessible on the business side. What’s more, there’s an advantage in quantum registering and its innovation, with JPMorgan Chase and Visa investigating the innovation. Once created, Google could dispatch a quantum figuring administration by means of the cloud.\nOrganizations can likewise get to quantum innovation without building a quantum PC. IBM plans to have a 1,000-qubit quantum PC set up by 2023. For the time being, IBM permits admittance to its machines assuming they’re important for its Quantum Network. Those that are essential for the organization incorporate exploration associations, colleges, and laboratories.\nMicrosoft likewise offers organizations admittance to quantum innovation by means of the Azure Quantum stage. This is not normal for Google, which doesn’t offer admittance to its quantum computers.']"	['<urn:uuid:0b2a2968-87d8-40db-bb86-fcee631de631>', '<urn:uuid:8ae0a21b-b64e-4df9-91bc-58f487f919d2>']	open-ended	direct	long-search-query	similar-to-document	three-doc	expert	2025-05-13T05:18:19.158519	8	136	2025
7	Which came first: ISO 55002:2018 or ISO 45001?	Both standards were released in 2018. ISO 45001 was released in March 2018, while ISO 55002:2018 was the second edition revising a 2014 version.	['A newly-revised international standard, ISO 55002:2018 – Asset Management – Management Systems – Guidelines For The Application Of ISO 55001, is closely intertwined with another standard developed by the International Organization for Standardization (ISO).\nThe mind is a powerful entity. While the fastest supercomputer in the world, the Tianhe-2, has a maximum processing speed of 54.902 petaFLOPS—petaFLOP meaning one quadrillion floating point calculations per second—the human brain is believed to operate at 1 exaFLOP—one billion-billion calculations per second. By firing synapses, one can plunge into creativity, computation, organization, and various other processes guided by thought.\nHowever, there’s a tangible world outside of the confines of our minds. In it, assets, represent potential or actual value to an organization, maintain key roles. More often than not, assets are physical.\nFor companies and organizations, depending merely on the power of one’s complex brain to adequately oversee and utilize all assets is often not enough. Instead, the communal knowledge of industry experts drives organizations towards success.\nAsset management enables an organization to realize value from its assets while achieving its organization objectives. In other words, asset management succeeds in taking that which is robustly formulated in the mind—assets and objectives associated with their use—and turns it into reality.\nThe application of asset management concepts is addressed in ISO 55002:2018. This standard is intrinsically tied to ISO 55001:2014 – Asset Management – Management Systems – Requirements.\nWhat is ISO 55001?\nIt should be noted that ISO 55002:2018, as implied by the “2” at the end of its designation, is part of a series of standards focused on asset management. ISO 55001:2014 is the seminal document in this series.\nISO 55001:2014 specifies the core guidelines for an asset management system. This involves considering the organization’s context, a commitment from the organization’s top leadership, established asset management objectives and plans to meet them, determining resources, establishing competencies, and evaluating the asset management system’s performance. Ultimately, this assists the organization in finding success in meeting its goals.\nOther than the organization itself, ISO 55001:2014 can be used by internal and external parties who need to assess the organization’s ability to meet legal, regulatory, and contractual requirements, as well as the organization’s own goals.\nWhat is ISO 55002 and How Does It Differ from ISO 55001?\nISO 55002:2018 supports ISO 55001:2014 thoroughly, in that its guidelines are intended to be used while applying an asset management system in accordance with ISO 55001:2014. Because of this, the ISO 55002:2018 document structurally mirrors the ISO 55001:2014 document closely, expanding upon guidelines necessary when implementing the core guidelines addressed in ISO 55001:2014. For example, in Subclause 4, “Context of the organization,” ISO 55002:2018 dives into external context (e.g. social and cultural, political, technological, and competitive and natural environment factors) and internal context (e.g. governance requirements, organizational culture, the size and complexity of the organization, and size of assets).\nAccording to ISO 55002:2018, the design and improvement of an asset management system is driven by four pathways of key importance, including the line of sight from the context of the organization to the asset management operation, objectives, decision-making criteria and processes, and risk management developed at all levels. ISO 55002:2018 was written with these four key elements in mind, and the guidelines it provides in applying asset management concepts are conductive to success due to their adherence to these concepts.\nUsers should note that ISO 55002:2018 cannot be used to provide financial, accounting, or technical guidance for managing specific asset types.\nFurthermore, while the majority of the ISO 55002:2018 document is aligned structurally with ISO 55001:2014, it strays from its companion’s composition by including eight annexes (Annexes A through H) that detail supplementary information on asset management concepts that may prove useful in its implementation. These annexes explain value as a fundamental in asset management, how to define the scope of an asset management system, the principles behind asset management decision making, and the objectives and structure of the strategic asset management plan (SAMP), among various other concepts.\nChanges to ISO 55002:2018\nISO 55002:2018 is the second edition of the standard for applying ISO 55001 asset management concepts, and it revises the 2014 edition to further clarify its guidelines and heighten connections between it and its related standard. For example, in the 2018 edition, linkages between clauses have been provided. Furthermore, the many annexes were added on cross-cutting concepts, and they serve to provide a combined guidance for specific asset management themes that are addressed throughout the ISO 55002:2018 standard.\nIn addition, the ISO 55000 fundamentals (value, alignment, leadership, and assurance) are now addressed in each clause, if applicable. Furthermore, the application of ISO 55001:2014 to the domains of asset management identified in ISO 55000 have been clarified, and the adequacy of this information to each domain is stated, whenever possible.\nThe standards in this series are available together at a discount as the ISO 55000 / ISO 55001 / ISO 55002 – Asset Management Package.\nISO 55002:2018 – Asset Management – Management Systems – Guidelines For The Application Of ISO 55001 is available on the ANSI Webstore.', 'What the New ISO 45001 Standard Means for your Organization\nWhat is ISO 45001?\nThe International Organization for Standardization (ISO) released the new health and safety management system standard, ISO 45001, in March 2018. This is the world’s first international standard dealing with health and safety. Created by occupational health and safety experts, the standard follows a generic management system approach similar to ISO 9001:2015 and ISO 14001:2015. The new global standard has an increased emphasis on management commitment, worker involvement and risk control, which is applicable to all organizational industry types and sizes.\nISO standards are developed by experts and committees to ensure it captures all necessary aspects and that it is fit for purpose. During the five years of developing ISO 45001, ISN participated as part of the Technical Advisory Group and even hosted the ISO Technical Committee at ISN’s HQ office in Dallas.\nFurther information on the requirements of the ISO 45001 standard and any actions organizations need to address to adhere to this standard can be found on the ISO website.\nThe Purpose of the Standard\nOrganizations are responsible for the occupational health and safety (OH&S) of all workers, including part-time and contracting personnel. Visitors who may be affected by a company’s scope of work should also be considered in an organization’s health and safety plan. The ISO 45001 standard includes being responsible for workers’ physical and mental health and promoting best practices throughout the organization.\nThe purpose of an OH&S management system that meets the requirements of ISO 45001 is for organizations to provide safe and healthy workplaces and prevent work-related injury and ill health while continuing to improve health and safety practices. Organizations can minimize or eliminate risks associated with hazards by taking effective preventative and protective measures and implementing ISO 45001 recommendations.\nEach element of the ISO 45001 standard is a reference to the Plan, Do, Check, Act (PDCA) cycle, which those in the health and safety field will most likely be familiar with from other ISO standards. The purpose of implementing this cycle to the standard as a whole and to each element is:\n- Plan: determine and assess OH&S risks and opportunities and establish objectives and processes necessary to deliver results in accordance with the organization’s OH&S policy\n- Do: implement the processes as planned\n- Check: monitor and measure activities and processes with regard to the OH&S policy and objectives and report the results\n- Act: take actions to continually improve OH&S performance to achieve the intended outcomes\nTimeline of ISO 45001 Implementation in Australia\nAustralia is one of the first countries to introduce the ISO 45001 OH&S management system country-wide. Like all new standards, ISO has ensured a generous grace period for organizations that are currently accredited in Australia to A/NZS4801 to migrate to the new ISO 45001 standard by October 2021. The ISO organization is promoting a positive collaboration culture amongst the standards until October 2021 to ensure organizations can promote a safe and positive culture through their management systems.\nISN has proactively created a timeline that offers a grace period to allow ISN Hiring Clients complete understanding of the new management systems and how it amends written program requirements within ISNetworld. During this time period, ISN has hosted Roundtables with Hiring Clients in Australia and the U.S. to ensure Hiring Clients understand the standard, the effects on their contractors and the next stages.\nSimilarities Between A/NZS4801 and ISO 45001\nWhile developing the ISO 45001 standard, the experts and technical committee took into consideration the elements of the A/NZS 4801 standard and the ability for organizations to comply with the new standards. There are a number of similarities between the standards, such as their overall intent to prevent injuries, illnesses and fatalities within the workplace. Other similarities exist, such as the PDCA cycle, policy requirements; identification of legal and other requirements, improvement objectives, awareness requirements, competency requirements, resources needed to support the system and requirements for monitoring, measuring and analyzing OH&S performance and improvement.\n|New ISN ISO 45001|\n|Current ISN A/NZS 4801|\nManagement System Protocols\n|OH&S Policy||Occupational Health & Safety Policy|\n|Risks and Opportunities||Hazard Identification & Risk Assessment|\n|OH&S Objectives||Key Performance Indicators|\n|Leadership & |\n|Leadership & Commitment|\n|Competence||Training & Competency|\n|Awareness and |\n|Document Control||Document Control|\n|Emergency Preparedness & Response||Emergency Preparedness |\n|Incidents & |\n|Audits||Inspections & Audits|\n|Management Review||Review of Mnaagement System|\n|Partly covered in OHS Meetings and|\nleadership & commitment\n|Performance Evaluation||Partly covered in Key Performance|\nDifferences Between A/NZS4801 and ISO 45001\nAlthough the ISO 45001 framework is based upon similar ISO standards, the structure differs from A/NZS4801. The new structure allows organizations to be more proactive in preventing accidents and injuries and evaluating hazard risks, while A/NZS4801 focuses mainly on hazard control.\nISO 45001 also evaluates workers’ involvement and responsibility to allow for broader employee participation within organizations. This looks at employees receiving relevant training and education in order to complete their tasks safely and to help create a successful safety program.\nSome other key differences between A/NZS 4801 and ISO 45001 include:\n- Management Representative (Clause 5.1) has changed from appointing a management representative and replaced by 13 responsibilities that need to be allocated to Senior Management\n- Health & Safety Policy (Clause 5.2) has additional requirements to be included in the policy.\n- Risks have been extended to go beyond hazards (Clause 6.1)\n- Psychosocial hazards (Clause 220.127.116.11) have been added to the standard\n- Documented information (Clause 7.5) changed to allow organizations to determine what documented information will be retained to demonstrate compliance (unless there is a legal requirement to retain certain documentation)\n- Procurement (Clause 8.1.4) requirements added as part of the overall standard\n- Evaluation of compliance (Clause 9.1.2) requires organizations to evaluate their compliance with legal and other requirements and retain documented information of the evaluation results.\nGet Support Complying with ISO 45001\nIf you are a Hiring Client interested to learn how the new ISO 45001 standard will impact you and your contractors’ use of ISNetworld, contact your ISN representative for more information.\nAre you a Hiring Client and interested to learn how ISN could help you manage your contractors and adhere to ISO 45001 standards? Request a demo of our contractor management system, ISNetworld.\nIf you are a contractor and need support submitting the new standards within ISNetworld, contact our customer support team for additional resources.']	['<urn:uuid:6084b19e-55cb-446d-bbdf-0bddd945c385>', '<urn:uuid:f69128e9-bce4-4874-9670-b9fef820c3ba>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T05:18:19.158519	8	24	1906
8	confucian temples present past persecution	Confucian temples exist across East Asia, including in Qufu, China and Hanoi, Vietnam, where people can pay respects to Confucius and his disciples. Historically, Confucianism faced persecution, from the First Emperor of China burning books and executing scholars, to the recent Cultural Revolution (1966-1976) when it was attacked as part of Mao's campaign against old traditions.	"['Over a year ago, a friend sent me a copy of Burton’s Watson’s translation of the Analects of Confucius, and after reading that book, my interest in Confucianism has gradually grown over time. It’s a subject I’ve touched upon many times on the blog, but the more I study it, the more I realize that Confucianism is far more complicated than what I assumed. Confucianism is something not oft studied in the West, and explanations of it usually begin and end with Confucius. But in fact Confucianism is a large and complex school of thought and its influence shouldn’t be understated. In the context of this blog, I have been recently reading about religion and politics in the Edo Period in Japan, 1600-1868, far removed from the stuff I usually study, but the book shows how religion and politics at the time were deeply intertwined. It was at this time that Neo-Confucianism rose to prominence and rivaled Buddhist thought, which had been the de facto state religion of Japan for 1000 years (with Shinto in a subordinate status).\nSeparately, I’ve also been reading an excellent introduction to Confucian thought by Professor Yao who helps explore the roots of Confucianism and how it developed over the centuries, the various strains of thought, approaches, interactions with other religions and so on.\nConfucianism is called rú jiào (儒教) in modern Mandarin, or jukyō in Japanese. The first character 儒 refers to the ancient priests of Shang-dynasty China, who gradually became scholars and engineers as their religious functions declined and they had to make themselves useful somehow. So the current meaning is that of a scholar. Thus, Confucianism in Asian culture means the “religion or teachings of the scholar”, but not in a sense of an elite class or aristocracy, as Western history sometimes portrays it. Instead, as Professor Yao describes Confucianism:\nConfucianism is a kind of humanism that seeks sacredness in an ordinary and yet disciplined life; or in Paul Rule’s words, it is a ‘secular religion, this-worldly in emphasis yet appealing to transcendent value embodied in the concept of “heaven”‘ (Rule, 1986: 31, as posted in page 45)\nBut as Professor Yao points out, Confucius himself didn’t invent the teachings, but rather articulated China’s idyllic past, reinterpreting ancient books such as the Book of Changes, the Book of Odes, etc. and with it provided a model for future generations to follow while adding innovations of his own, such as his deep emphasis on “humaneness” and love of learning. As history shows his followers carried the message onward, each in their own way, and through repeated government persecutions, and struggles with other schools, it recovered and thrived time and again sometimes interwoven with the government, other times acting as a watchdog toward it.\nHowever, as Professor Soothill points out in the same book:1\nA study of a religion which limits itself to the teachings of the early founders, and which ignores the present condition of its development, will give a very imperfect presentation of the religion as a whole. (Soothill, 1973: 21, printed in page 11)\nSo, one must look beyond Confucius and his immediate followers to appreciate Confucianism as a whole. So recently, I took an interest in the writings of Mencius, the second great Confucian scholar, which have been translated and posted in part by Professor A.C. Muller, including this excellent quotation:\n[6A:16] Mencius said: “There is a nobility that belongs to Heaven and a nobility that belongs to man. Humaneness, Rightness loyalty, truthfulness and a tireless delight in the good—these are the nobility of Heaven. Duke, Premier and Minister—these are the nobility of man.”\nThe conversations of Mencius often read like Socratic conversations, and are more detailed than those found in the Analects of Confucius, so you can get a genuine flavor of what Confucius taught through this famous disciple of his own grandson. Mencius helps to complete the picture, in my opinion.\nLater, as history shows, the First Emperor of China, Qin Shi Huang, followed the tenets of the rival school of Legalism instead, and Confucian scholars were often put to death and their books burned. It took centuries before the Confucian community could recover, but it went on to become the de facto state philosophy, and the civil entrance exams were implemented. In Confucius’s time, he taught that any person through education and self-cultivation could become noble, humane and a gentleman (or lady), and the civil entrance exams were revolutionary in allowing anyone in China who studied the Classics to take the exam and earn a position in the government. Compared to kingships, tribes and despots in other parts of the world, this was way ahead of the times. This model provided the inspiration for Korea, Japan and Vietnam as well with its appreciation of the learned scholar over the despot as a viable form of government, and civil entrance exams were implemented in these countries as well.\nAs Confucianism interacted with a fully matured religious Taoism and Buddhism, the complex tensions and exchanges in ideas led to the rise of Neo-Confucianism. The famous Neo-Confucian Zhu Xi (sounds like “joo shee”) organized Confucian texts in a way that remains today, while advancing new ideas with regard to cosmology and how the world works. His teachings provided tremendous inspiration for the Tokugawa Bakufu in Japan, as the books above show and for Neo-Confucians there.\nEven today, Confucianism is recovering from the persecution of the Cultural Revolution, and the stigma as a “reactionary and conservative” movement. But Professor Yao demonstrates Confucian thought has often been simultaneously conservative yet liberal and revolutionary. In the 21st century, Confucian scholars in Chinese communities everywhere, and in the West take inspiration from Confucius’s secular-yet-sacred teachings and try to instill moral conduct and humanness even in today’s technological world. Professor Yao’s book shows how Western missionaries who came to China in recent centuries often came away inspired to translate and bring the teachings back to the West, even while attempting to challenge it, including some well-known translators (e.g. Legge and Matteo Ricci among others).\nThis is a very brief, cursory look at the evolution of Confucianism, but shows how reverence for education, humane ethics and personal development are still alive in the world today and owe much to the many scholars throughout history who kept the teachings alive in the face of corruption and occasional persecution. Even now people can come to the Temple of Confucius in Qufu, China2 and pay respects to Confucius, his disciples, and all those who came after them.\nP.S. Scheduling error on my part so posting today instead of Sunday.\n1 Much of this would apply to the Buddha as well, and probably other religions too.\n2 Similar temples exist in the rest of East Asia. I’ve seen the one in Hanoi, Vietnam when I was studying abroad there many years ago.', ""Cultural revolution campaigns (1966-1976) criticize the old world and build a new world with mao zedong thought as a weapon, 1966 the cultural revolution. In 1966, china’s communist leader mao zedong launched what became known as the cultural revolution in order to reassert his authority over the chinese government. Media caption the moment john mcdonnell pulled out mao's little red book the little red book - or, to give its full title during the cultural revolution. Cultural revolution, 50 years on as the cultural revolution played out in china, mao zedong’s political teachings grew in popularity around the world. What was it and when did it begin the great proletarian cultural revolution was a decade-long period of political and social chaos caused by mao zedong’s bid to. Why did mao launch the cultural revolution a a struggle for power at the zunyi conference of 1935, mao had become the communists' virtually undisputed leader. Fifty years ago this month, mao zedong launched china’s cultural revolution – a decade of chaos, persecution, and violence, carried out in the name of ideology.\nThe cultural revolution had a massive impact on china from 1965 to 1968 the cultural revolution is the name given to mao’s attempt to reassert his beliefs in china. In 1969, mao declared the cultural revolution to be over, although various historians in and outside of china mark the end of the cultural revolution. It was the summer of 1966 and mao’s great proletarian cultural revolution fifty years after the start of the cultural revolution, in may 1966, yu. At the end of 1950s a full-scale retreat from the disastrous great leap forward program went underway and a period of recovery lasted for several years once again. The cultural revolution, formally the great proletarian cultural revolution, was a sociopolitical movement in china from 1966 until 1976 launched by mao.\nEvan osnos on the fiftieth anniversary of the cultural revolution what effect did mao’s campaign the cost of the cultural revolution, fifty years later. History is as complex as reality, they are or were reality it's extremely difficult to know what really happened and much harder to understand the true reason. Impact and effects of communist mao zedong in china the cultural revolution began in august of 1966 and continued the cult of mao, the cultural revolution.\nCultural revolution as a result of the failure on the great leap forward, mao zedong retired from the post of chairman of the people's republic of china. Mao's golden mangoes: the mango became an unlikely object of worship during the turmoil of the cultural revolution, which began 50 years ago this month. Both cultural revolution movements mao’s cultural revolution movement was the its irreplaceable destruction of china’s cultural heritage left chinese.\nCultural revolution: cultural revolution, upheaval launched by mao zedong during his last decade in power (1966–76) to renew the spirit of the chinese revolution. The most comprehensive and authoritative account of the cultural revo-lution yet to appear, mao’s last revolution, was recently published by the.\nThe battle for china's past: mao and the cultural revolution [mobo gao] on amazoncom free shipping on qualifying offers mao and his policies have long been. China’s cultural revolution amy freedman department of government supporting and instigating the cultural revolution mao was able to reassert his primacy, and he. Enjoy the best mao zedong quotes at brainyquote quotations by mao zedong, chinese leader, born december 26, 1893 share with your friends.\nXi jinping's uneasy relationship with mao's legacy, and the communist party's fear of a repeat of the cultural revolution help to explain the president's. Torture, mass murder, rape and cannibalism the horror of mao's cultural revolution 50 years on author xue xinran is still traumatised after the tyrant's brutal. The people's republic of china: iii the cultural revolution decade, 1966-76 in the early 1960s, mao was on the political sidelines and in semiseclusion. Historian yohuru williams gives a crash course on the cultural revolution led by communist leader mao zedong in china in the 1960s following the failure of chinese. Under mao zedong, china underwent a spasm of violence called the cultural revolution in the 1960s and 1970s, which threatened its ancient heritage.""]"	['<urn:uuid:5f0a065b-6d54-48f7-a081-f5265bc4a463>', '<urn:uuid:8c85144d-e646-427e-9831-9ff86bd790cf>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:18:19.158519	5	56	1837
9	Can you tell me what significant discovery Marie Tharp and Bruce Heezen made when they were studying the Atlantic Ocean?	Marie Tharp and Bruce Heezen discovered a V-shaped notch at the top of The Rift in the Atlantic, which contributed to a paradigm shift in earth science leading to the theory of plate tectonics.	['“Mapping the Deep” is a book which draws you in to the realm of the deep sea – far from shore. Imagine looking out of the window into a sky filled with the sea, we are taken into the world of the abyss. The author describes the book as a portrait of our understanding of the sea- with its distinct focus on the distant and the unfamiliar making it such a pleasure to discover. Laurence Madin of WHOI describes this popular science book as “The best account of discovery in oceanography I’ve ever read” and the chapters which follow are just as inspiring to read.\nSpace and the Ocean narrates the meeting of oxygen to hydrogen and the story of Albert Cheung, the graduate student who first observed water in space. The birth of the solar system and the possible role comets played in bringing water and organic matter onto the Earth are discussed to explain the origins the earth and its ocean basins. Then we start of on a wonderful narrative about the history of early bathymetric mapping and charting in Sounding the depths – from Charles Bonneycastle to the Challenger Expeditions to Atlantis. Introducing marine geologists Marie Tharp and Bruce Heezen’s discovery of the V-shaped notch at the top of The Rift in the Atlantic and Hess’s concept of seafloor spreading; the chapter leads to a paradigm shift in earth science to the theory of plate tectonics. Their first bathymetric map of the ocean and its geological features are described in A Map of the World. Turbidity currents and their extraordinary impact on the abyss and the early remote sensing satellites prior to the advent of sonar lead to Sandwell and Smith’s famous map of the oceans.The Seafloor at Birth goes on to discuss the history of multibeam sonar and the mapping of the US Exclusive Economic Zone (EEZ) and the quest to understand the birth of the seafloor.\nFrom history of deep sea ecology and Edward Forbes to the Porcupine and Challenger Expeditions to discovering the deep sea trenches, all can be found in Kingdom of the Holothurians. Islands in the Deep is the chapter on seamounts and their biology as discovered by Alvin and deep sea scientists. Life at hot springs and hydrothermal vents are discussed with a string of discoveries of chemosynthetic organisms in Life on a Volcano. Fantastic, Glistening Jellies discovers the pelagic realm and together with Animal Lights discuss the discoveries of mid-water biology. Greening the Ocean discusses the “paradox of the plankton”and man-made iron fertilization. Anthropogenic impacts are discussed leading on to the problem of over fishing in The Twilight of the Cod, which provides a comprehensive overview of cod fisheries collapse off Newfoundland and New England. Where the Water Goes discusses thermohaline circulation and research cruises which have taken place to further understand these water masses. The Climate Switch dives into the realm of climate science and without giving away the ending, Time and the Ocean looks at the lifespan of the oceans and the biosphere in time.\nOverall, this book is a must read for oceanographers of any discipline. Personally, I first read this book as a first year undergraduate student in Marine biology with oceanography and have very fond memories of it. It opened up a whole world full of deep sea oceanography, preparing me for lectures and the field. The book also provides a vibrant account of key developments in the field of marine geology, marine ecology and despite being over 15 years old it largely remains current account. It is highly recommended to popular scientists, students of oceanography and anyone who loves the sea. A Winner of Aventis Prize for Science Books 2001, Royal Society\nLinks to Book Reviews']	['<urn:uuid:43e600b4-ecd1-4459-ac29-f1f6c6f0f7b5>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	20	34	622
10	I'm looking for a simple point-and-shoot laser for scanning barcodes in my shop. What's interesting is that these devices came from something called a Fabry-Perot laser - could you tell me more about what made this invention so important?	The Fabry-Perot laser diode was revolutionary because it provided a low cost, simple to drive, and manufacturable source of spatially coherent light. This small semiconductor laser chip helped enable widespread commercial acceptance of laser technologies, leading to applications like bar-code scanners, compact discs, laser pointers, and fence-line monitors.	['Short Cavity Lasers\nWe specialize in frequency agile lasers for atomic, molecular, and optical (AMO) physics, with applications in cold atom experiments, spectroscopic sensing, and more. Our lasers are based on short cavities, which make them robust, reliable, and small. Our philosophy is to design and build the laser with its intended use in mind. For our short-cavity laser products this means locking to and scanning around specific atomic or molecular transitions and/or cavity modes with robust, vibration immune, and trouble free stability.\nTo realize frequency-agile, trouble-free stability we specifically avoid piezo and/or mechanically actuated gratings and mirrors. Such mechanical elements can enable larger wavelength tuning around a diodes gain curve, but this comes at a significant cost. Specifically, the mechanical mechanisms provide a pathway to couple vibrations into the laser cavity, add expense and complexity, provide a point of wear/fatigue which limits the lifetime, and necessitate an undesirably long laser cavity. The long laser cavity requirement is perhaps the most detrimental. This decreases the free spectral range, which limits the injection current mode hop-free tuning range. Injection current tuning is much faster (up to GHz) than any piezo or mechanical element, which means it is a potent knob for laser frequency control, whether it is for feedback during locking or frequency jumps during atom trapping and cooling. Specifically for locking, since piezo’s have a limited bandwidth (typically <10 kHz), they of course have limited utility when responding to acoustic challenges which can have frequency components of 10′s of kHz. The high-bandwidth injection current is the most potent technique for responding to such commonplace challenges, but, with traditional long-cavity designs the necessarily limited mode-hop-free tuning range dramatically limits the dynamic range of this approach. With our non-mechanical/piezo-less and short cavity designs we have both inherent stability and large mode-hop-free injection current tuning, providing trouble-free, very stable locking and injection-current control.\nWhile we understand that the mechanically and/or piezo actuated diode laser systems has been the AMO default for the last several decades, our laboratory and industrial experience has motivated a different approach. If your application does not require long-range tuning but rather requires robust and reliable locking and scanning around a specific frequency, then don’t incur the cost and substantial performance expense associated with prior mechanical approaches.\nIn the rest of this section we will very briefly review diode laser and then discuss our two short-cavity designs: fully integrated structures (DFB & DBR) and our Chip External Cavity diode Laser (CECL).\nThe Fabry-Perot Laser Diode: A Laser in “A Grain of Sand”\nMost scientific and AMO diode laser experiments utilize the ubiquitous Fabry-Perot semiconductor laser chip. This chip is a minute self-contained laser especially when compared to other non-semiconductor based laser cavities. The small size and scalability of manufacturing enabled wide-spread commercial acceptance of laser technologies. The Fabry-Perot cavity is typically a channel waveguide structure with lateral confinement either provided by a topological ridge (index guiding) or an electrode pattern which creates lateral charge density variations (gain guiding). The ends of the diode chip form the ends of the laser cavity. Typical dimensions for a spatially-single-mode device can be in the range of hundreds of microns long, by 3 microns wide, by 0.3 microns tall (this 3 × 0.3 µm cross section explains the elliptical output beams). With the invention of the Fabry-Perot laser diode the world now had a low cost, simple to drive, and manufacturable source of spatially coherent light. As is well known this technique (along with the related VCSL) brought on bar-code scanners, compact discs, laser pointers, fence-line monitors, and many more non-laboratory applications.\nAs is well known this technique (along with the related VCSL) brought on bar-code scanners, compact discs, laser pointers, fence-line monitors, and many more non-laboratory applications.\nWhile the Fabry-Perot laser diode has truly been disruptive, from an industrial and commercial point of view it’s most important attribute is spatial coherence, i.e., it was a bright and spatially well defined light source. From a scientific perspective, while the spatial coherence of lasers (i.e., all the photons are going in the same direction) is appreciated and utilized, the temporal coherence of lasers (i.e., all the photons have the same energy) is equally and often more important. The common Fabry-Perot laser cavity does not contain any frequency selective elements. The lateral and vertical spatial confinement can guarantee a single spatial mode, there is nothing to guarantee a single longitudinal mode. Stated simply, in the frequency domain the laser is multi-mode. For applications the require control of the temporal coherence, or frequency, a wavelength filter or wavelength selective element needs to be incorporated into the cavity.\nFully Integrated Single Mode Diode Lasers: The DFB & DBR\nThe largest commercial application for temporally coherent, i.e., frequency single-mode, lasers is wavelength division muliplexing (WDM) in telecommunications. This substantial market has driven the development of distributed feedback (DFB) lasers and distributed Bragg reflector (DBR) lasers. In these lasers a Bragg grating structure is integrated or incorporated directly into the semiconductor chip. For DFB lasers the grating section fully overlaps with the gain section and occupies the whole of the laser cavity. With DBRs the grating section is spatially separated from the gain region. Sometimes there can be multiple gratings or mutli-section gratings that enable longer vernier tuning, though these are more complicated to make and control. All of our products use a single grating element.\nThere is a vast body of prior work on different approaches for making the grating structure and even the design of the grating itself (e.g., single or multi-order gratings). Some examples include holographic patterning, direct e-beam writing, and nano-imprint lithography. With our DBR products that utilize chips produced by Photodigm the gratings are holographically written. Telecom DFB lasers are also typically holographically written. For shorter wavelength devices the grating pitch must decrease, even with the use of multi-order gratings. This explains why GaN based blue DFB or DBR lasers are not available.\nPhotodigm Spectroscopy-CertifiedTM DBR lasers\nBoth DFB and DBR lasers are extremely immune to vibrations and very rugged, stable devices. The cavity length is typically on the order of 1 mm, usually slightly shorter for DFBs than DBRs. Laser tuning is realized non-mechanically via temperature and injection current. The temperature is used to coarsely tune the wavelength, typically over a few nanometers. Once the laser wavelength is close to the atomic or molecular transition that is desired then the temperature typically does not need to be changed. Shorter range tuning, or what we often refer to as frequency (rather than wavelength) tuning, is realized via injection current. Injection current alters the lasers frequency both via a change in the charge density and local heating. The frequency of DFB and DBR lasers is considerably more sensitive to changes in current than a large cavity external cavity diode laser (ECDL). This is advantageous because it enables rapid detunings and servo-loop responses to external challenges. That said, this increased sensitivity also necessitates a low noise current source to ensure that current noise does not contribute to the laser linewidth. With our D2-105 current controller we typically observe DFB linewidths of ~1 MHz and DBR linewidths of ~700 kHz. The temperature sensitivity for these devices also necessitates a tight and fast thermo-optic design. Our D2-100 laser package has a two stage thermal loop design that provides fast local temperature control directly on the diode chip and stabilization of the outer housing to provide isolation from outside temperature variations.\nBecause the total cavity length is so small the free-spectral range is quite large. This enables long-range injection current tuning without mode-hops. For DFBs, since the injection current tunes both the gain and wavelength section, this is especially long (it can sometime be hard to realize a mode hop). For DBRs, since the wavelength section is not changed with injection current mode-hops can be realized, though they are very repeatably and controllable. For our 780 and 852 nm DBRs the injection current mode-hop-free tuning range is >30 GHz, more than enough to scan across the ~10 GHz wide hyperfine spectral features of the D2 lines in Rb or Cs. Furthermore, this large range enables the user to directly modulate the injection current for putting sidebands (out to several GHz) on the laser. This can replace external AO or EO modulators saving cost and complexity.\nWhile fully integrated lasers such as DFBs and DBRs provide excellent ruggedness, are very compact, and a long mode-hop-free injection current tuning, they do have limitations. First, the very short cavity, coupled with spontaneous emission noise, results in a larger linewidth (typically ~1 MHz). For some applications this is too broad. Secondly, a full grating design and manufacturing run is required for each wavelengths. If your experiment requires a “non-typical” wavelength then they might not be available. These two drawback motivated our development of our CECLs.\nChip External Cavity Laser (CECL): DFB/DBR Stability with ECDL Flexibility\nTo exploit the benefit of a short cavity, non-mechanical laser design, but with center wavelength flexibility and a more narrow linewidth than DFBs or DBRs, we developed our chip external cavity lasers (CECLs). These lasers exploit a Littrow external cavity architecture, wherein the wavelength selective element is outside of the semiconductor gain medium. For our designs this can be either a surface reflection grating or a volume Bragg grating. In this manner it is similar to conventional ECDLs. Where the design differs is that we engineered the cavity so that the external grating is mounted as close as possible to the gain chip, providing the advantages of a short cavity.\nThe total cavity length is on the order of millimeters, providing a free-spectral-range that is an order of magnitude larger than for conventional EDCLs. We also have removed all piezo and mechanical structures, since they provide a path for noise to couple into the cavity.\nAll of the cavity elements of the laser are mounted onto a monolithic ceramic block with telecom style alignment fixturing. The laser output is typically from the back facet of the Fabry-Perot diode. At the time of ordering the customer specifies the desired center wavelength, which we then fix during manufacturing. Because this is set at the time of manufacturing we can set this to anywhere within the diodes gain curve. Furthermore, since we are utilizing simpler Fabry-Perot diode chips we have access to a much wide range of wavelengths.\nThe simple, non-mechanical construction ensures that when the customer unpacks the laser and turns it on the frequency will be within a few GHz of their requested frequency. For alkali atoms like Rb and Cs we can guarantee that the laser will arrive on the D2 or D1 Doppler broadened transition. Slight adjustments to injection current will put it on top of the desired Doppler-Free hyperfine transition.\nBecause the cavity length is larger than with a DFB or DBR the linewidth is also less. Typical CECL linewidths are on the order of 50 kHz as measured by beating two independent lasers together. Furthermore, the linewidth is very Lorentzian, i.e., it is not a smaller line width that is rapidly “jittering” around, as with longer cavity lasers.\nThe laser tuning is controlled the same as with a DFB or DBR. Coarse wavelength tuning is done by temperature tuning the grating element. Short frequency tuning is done with high-bandwidth injection current. The injection current mode-hop-free tuning range is more than 10 GHz, sufficient for tuning across a hyperfine set in Rb or Cs and for providing tight, vibration tolerant locking.']	['<urn:uuid:e88bf2e4-82db-42c6-8bb7-40ecbf2fb970>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	39	48	1914
11	What rules do ballet schools have about clothing and personal appearance, and how strictly are these rules enforced in class?	Ballet schools have specific rules about clothing and appearance that are strictly enforced. The Royal Academy of Dance (RAD) and other ballet schools require students to wear specified dance apparel. Students must wear pink tights (which can vary in style from footed to transitional) and specific colored leotards. No skirts are allowed except in adult classes. Regarding personal appearance, no jewelry is permitted except small earrings, and hair must be secured without any loose strands around the face. A proper bun with hairnet, pins, and hairspray is typically required. If students are incorrectly dressed, they may be asked to observe class rather than participate. These rules are considered part of proper ballet etiquette and are necessary for maintaining discipline and proper training standards.	['What is the RAD?\nThe Royal Academy of Dance (RAD) is an international dance education and training organization, and examination board that specializes in Ballet. The RAD was established in London, England in 1920 as the Association of Operatic Dancing of Great Britain, and received its Royal Charter in 1936. HM Queen Elizabeth II is currently the patron of the RAD and Darcey Bussell CBE has served as President since 2012.\nThe RAD was created with the objective to improve the standard of ballet training in the UK and, in pursuit of that goal, a new teaching method and dance technique was devised for the Academy by a group of eminent European dancers. The RAD is one of the largest dance organizations in the world with over 13,000 members in 79 countries, including over 7000 who hold Registered Teacher Status. There are currently over 1,000 students in full-time or part-time RAD teacher training programs and each year the examinations syllabi are taught to over 250,000 students. For more information, go to www.rad.org.uk.\nHow are students placed in classes?\nStudents are placed in a level appropriate for both their age and level of physical and/or technical development and based on the syllabus of the Royal Academy of Dance. Our instructors are trained professionals who provide age appropriate dance training for all our young artists. Placement within a level is subject to change solely at the discretion of the Artistic Director.\nWhat do we mean by developmentally appropriate?\nIt is important to understand that ballet is a rigorous activity both physically and mentally. When young students begin ballet training, it is extremely important to remember that their bodies are still developing. At an early age the bones, ligaments, tendons, and muscles are not necessarily strong enough for the support of the body required by formal dance training. Strength, flexibility, coordination, confidence and the love for dance may be gained during these early years and will prepare the student for the demands of the art. For the same reason, students should not be put on pointe too soon. Eleven is generally an appropriate age for beginning pointe. Readiness for pointe is assessed on an individual basis taking into consideration age, years of training, strength and the understanding and physical demonstration of necessary skills and the required technique.\nWill I be able to see my child perform?\nStudents of the Alabama Ballet School present “Report of Progress” performances at the end of each year instead of a year-end recital. These reports are presentations performed at our dance studios, for each level, and are open to parents and family friends. They show a cross-section of class to illustrate the work that has been done throughout the year. Advanced level students will have an end-of-year performance on in the “black box” theatre at the ABCD.\nReports of Progress are presentations of each class for parents, family, and friends showing a cross section of class to illustrate the work that has been done throughout the school year. No separate costumes are required.\nThere will also be an optional performance opportunity at the end of the year. This year’s production will be Peter and the Wolf.\nStudents are encouraged to audition for community cast roles in George Balanchine’s The Nutcracker® and other productions of The Alabama Ballet. Auditions and rehearsals are held at the Alabama Ballet Center for Dance.\n- Observation of classes is only permitted during Parent’s Observation Week, or by special arrangement with the Artistic Director and faculty. It is highly inappropriate and disruptive to the class for parents and friends to wander the hallway looking in on classes. This is to ensure that the students stay focused on their class work and teacher, rather than visitors.\n- The ballet and school are not responsible for personal property. Please put your name in all your belongings.\n- All students are expected to wear the dance apparel specified. (see dress requirement) Dance clothes and shoes are to be kept clean and sewn.\n- All students are required to wear street clothes (a complete cover-up) and street shoes when entering and exiting the building. This preserves the life of our highly specialized dance floor as well as students’ dance shoes.\n- All parties picking up children or carpool participants must enter the building to pick students up at the end of classes. Please inform the school office if there is anyone with whom your student is not to leave the building. Do not park in the “drop-off” circle to come into the building. This causes traffic problems. If you need to come in, please park in a designated parking space.\n- The Alabama Ballet is a smoke-free environment. No gum chewing is allowed in the building. No food and drink in the studio except water.\n- Once class has begun, students are not permitted to enter without permission of the teacher.\n- Students are to use only the front school entrance to enter and leave the building except in case of emergency.\n- Students are not to open any outside doors in the building for any person other than a faculty or staff member. When in doubt, don’t let them in.\n- Students are to check the school message board daily for changes in the schedule, special events, messages, etc. You can also check the web site – www.alabamaballet.org – for additional information.\n- Except in the case of an emergency, office staff will not interrupt class for phone calls, messages, etc. Messages will be taken and given after class.\n- Absences must be reported to the office prior to class time.\n- Students may make up missed classes in a level equal or below their own. This must be coordinated with and approved by the school administrator and primary instructors.\n- A release of claims and treatment authorization form must be completed and on file before a student will be allowed to take class. This should be completed at the time of registration.\n- For all students – no skirts except those in the adult classes.\n- No jewelry (watches, necklaces, etc.) except small earrings are to be worn.\n- No loose hair around the face.\n- No plastic, rubber, cloth pants, or loose shirts are to be worn in class. With permission from the teacher, close fitting warmers may be worn in case of injury.\n- If a student is excessively tardy, incorrectly dressed or is disrupting the class, he/she may be asked to observe class rather than participate.\n- All dancers must wait for their parents inside the lobby.\n- No food or drink in the studios. You must clean up after yourself.\n- Parents, we respectfully request that you wait for your child in the lobby area and please monitor younger siblings. The lobby and hallways must remain quiet in order for the administrative employees to conduct their business.\n- We ask that all students do their best to keep the hallways, lobby and studios clean.\n- Every student, parent, and teacher in the school is expected to conform to the rules and regulations and to behave throughout the facility in a disciplined, responsible, and courteous manner. The school reserves the right to suspend or dismiss any student or family whose attitude, attendance or conduct is found to be unsatisfactory.', 'Ballet class etiquette is usually communicated clearly, and most dancers who begin as young children are taught the appropriate way to behave while in the classroom. Not only are rules—or etiquette—for class a sign of respect towards the teacher and the other students, they are necessary in order to progress through all the combinations that make up a full class. There is simply not time for a teacher to be reprimanding students or calling them back to attention every few minutes. However, you may begin taking lessons at a new studio or academy where the etiquette may differ from your former school, and it might be up to you to find out what the standard expectations are for students.\nMany schools have a dress code. Quite a few require pink tights. This sounds easy enough, but there can be many variations: students are clever at turning a rule on its head by changing it enough to say they are following the rules, when actually they are not. You need to know if pink tights means footed, or if transitional tights, stirrup tights, or footless tights are acceptable. Sometimes a student will have on transitional tights—which, when worn over the toes are considered footed—and have them rolled up to mid-calf. When it comes to class, pink can probably encompass ballet pink, classical pink, light pink, or European pink. (Or, as it was in my case as a kid, white tights dyed into a pink that came from red food coloring!) For performances, most teachers will be painfully specific about what color pink they want and if they want the tights to be mesh, seamed, or seamless; supplex, cotton, nylon, or a combination of fabrics; Capezio, Bloch, or Danskin. To make it a lot easier, some teachers will tell you a style number to be sure you get exactly the right thing. The Danskin mesh seamed tight in style 32 is very popular, for example.\nAlong with tights, some schools will require that you wear a specific color leotard or a specific style: camisole, tank, short sleeve, or long sleeve. A lot of schools will not be too picky about style as long as you wear the correct color, or a solid color. Make sure you are wearing shoes that are acceptable as well. Usually this is left to the discretion of the dancer, but some teachers don’t particularly care for canvas over leather, for instance. Some want you to have a full sole rather than a split sole. Just be certain you know if there’s a preference, and make sure your shoes have the elastics sewn securely. Elastics that are tied behind the ankle or kept on with paper clips or safety pins are irritating to most teachers. You’ve been warned.\nHair and Jewelry\nHair and jewelry are biggies in ballet class. To be safe, I would say to put your hair in a bun with a hair net and plenty of hair pins and hairspray, and don’t wear any jewelry at all. Here again, there will be variations of what is acceptable at your particular school. You may be allowed to wear your hair clipped with a great big barrette so it doesn’t flop at all when turning, and it might be fine to wear earrings as long as they don’t dangle. Sometimes earrings that slip through without a catch at the back can fly out during grand allegro or turns across the floor, so use good judgment here. Click here to see a great video by dancers at the Anaheim Ballet on how to make hair buns. And a fun place to find ballet needs is at bunheads.com.\nFinally, we come to the behavior that is expected in class.\n- No talking unless you have a question for the teacher (and questions are normally very welcome, especially in beginning to intermediate levels). This includes any kind of communication with others in class, so no miming or eye-rolling either!\n- Do not chew gum, eat, or drink during class.Sometimes you may be permitted to get a drink between barre and center, but it’s typically best not to leave the room to do so.\n- Do not arrive late. If you arrive during the plié combination you can usually catch up. Otherwise you need to check with the teacher to see if it’s okay for you to join or if they prefer that you just observe class.\n- Do not yawn.\n- Do not get impatient with yourself—this can be misinterpreted by the teacher who thinks you don’t care for their class or combination.\n- If you have an injury prior to class, let the teacher know that you may not do everything full out.\n- If you get injured during class or pull a muscle, let the teacher know. Get ice, if possible, and watch the rest of class from the sidelines.\n- Do not always stand in front. Take turns.\n- Do not always go first across the floor unless the teacher asks you to.\n- Do your best and have a positive attitude.\nOther things to note\nI encourage readers to add to the list if you can think of others I omitted. Of course, cells phones and ipods are also not good to bring into class. A few pet peeves of mine when I was teaching included students who had to go to the bathroom (although there might be exceptions, just don’t do this every single class), students who wanted to teach the class or recommend steps, and students who wore sweats over their tights once class began. Again, if you have an injury and you need to wear extra clothing for warmth, get permission from the teacher prior to class.\nThe nicest thing about class is the end. I always thought it very appropriate when I was dancing that we would let the teacher know we appreciated class by applauding at the end. Some teachers will tell young dancers that they should “give themselves a hand” so they get into the habit of clapping after class, but I believe that this part of ballet etiquette is more a show of respect for the authority and guidance of the teacher.']	['<urn:uuid:ea85ce77-83d0-462c-8592-66e987515153>', '<urn:uuid:ab105492-ab79-41a3-bffb-a3931f0d253c>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	20	123	2249
12	Which testing methods work better: protein mass analysis or peptide arrays?	Mass spectrometry-based approaches and peptide arrays each serve different purposes in protein/peptide analysis. Mass spectrometry techniques enable detailed 3D volume reconstruction and identification of specific protein modifications, as demonstrated by studies using imaging mass spectrometry of proteins. In contrast, SPOT-peptide arrays allow rapid, inexpensive and straightforward high-throughput screening of hundreds of synthetic peptides simultaneously to assess multiple activities like antimicrobial and immunomodulatory functions. The peptide array approach is more accessible as it can be implemented in most microbiology or immunology research laboratories without requiring specialists.	"[""Imaging mass spectrometry of proteins and peptides: 3D volume reconstruction. Andersson M, Groseclose MR, Deutch AY, Caprioli RM (2008) Nat Methods 5: 101-8 Integrating spatially resolved three-dimensional MALDI IMS with in vivo magnetic resonance imaging. Sinha TK, Khatib-Shahidi S, Yankeelov TE, Mapara K, Ehtesham M, Cornett DS, Dawant BM, Caprioli RM, Gore JC (2008) Nat Methods 5: 57-9 Lysine peroxycarbamates: free radical-promoted peptide cleavage. Masterson DS, Yin H, Chacon A, Hachey DL, Norris JL, Porter NA (2004) J Am Chem Soc 126: 720-1 Site-specific mapping and quantification of protein S-sulphenylation in cells. Yang J, Gupta V, Carroll KS, Liebler DC (2014) Nat Commun 5: 4776 The SH2 and SH3 domains of pp60src direct stable association with tyrosine phosphorylated proteins p130 and p110. Kanner SB, Reynolds AB, Wang HC, Vines RR, Parsons JT (1991) EMBO J 10: 1689-98 Crystal structure of cytochrome P450 14alpha -sterol demethylase (CYP51) from Mycobacterium tuberculosis in complex with azole inhibitors. Podust LM, Poulos TL, Waterman MR (2001) Proc Natl Acad Sci U S A 98: 3068-73 Mapping dominant-negative mutations of anthrax protective antigen by scanning mutagenesis. Mourez M, Yan M, Lacy DB, Dillon L, Bentsen L, Marpoe A, Maurin C, Hotze E, Wigelsworth D, Pimental RA, Ballard JD, Collier RJ, Tweten RK (2003) Proc Natl Acad Sci U S A 100: 13803-8 Characterization of an incompletely assembled major histocompatibility class I molecule (H-2Kb) associated with unusually long peptides: implications for antigen processing and presentation. Joyce S, Kuzushima K, Kepecs G, Angeletti RH, Nathenson SG (1994) Proc Natl Acad Sci U S A 91: 4145-9 MCL1 is phosphorylated in the PEST region and stabilized upon ERK activation in viable cells, and at additional sites with cytotoxic okadaic acid or taxol. Domina AM, Vrana JA, Gregory MA, Hann SR, Craig RW (2004) Oncogene 23: 5301-15 Overexpression of c-Myc and cell immortalization alters c-Myc phosphorylation. Lutterbach B, Hann SR (1997) Oncogene 14: 967-75 Novel tyrosine phosphorylations accompany the activation of pp60c-src during chemical carcinogenesis. Kanner SB, Gilmer TM, Reynolds AB, Parsons JT (1989) Oncogene 4: 295-300 Comprehensive proteomics analysis reveals new substrates and regulators of the fission yeast clp1/cdc14 phosphatase. Chen JS, Broadus MR, McLean JR, Feoktistova A, Ren L, Gould KL (2013) Mol Cell Proteomics 12: 1074-86 Efficient and specific trypsin digestion of microgram to nanogram quantities of proteins in organic-aqueous solvent systems. Strader MB, Tabb DL, Hervey WJ, Pan C, Hurst GB (2006) Anal Chem 78: 125-34 Reactions of prostaglandin endoperoxide synthase with nitric oxide and peroxynitrite. Goodwin DC, Landino LM, Marnett LJ (1999) Drug Metab Rev 31: 273-94 Systematic analysis of the role of CD19 cytoplasmic tyrosines in enhancement of activation in Daudi human B cells: clustering of phospholipase C and Vav and of Grb2 and Sos with different CD19 tyrosines. Brooks SR, Li X, Volanakis EJ, Carter RH (2000) J Immunol 164: 3123-31 Kinetics of B cell receptor signaling in human B cell subsets mapped by phosphospecific flow cytometry. Irish JM, Czerwinski DK, Nolan GP, Levy R (2006) J Immunol 177: 1581-9 Disruption of Myc-tubulin interaction by hyperphosphorylation of c-Myc during mitosis or by constitutive hyperphosphorylation of mutant c-Myc in Burkitt's lymphoma. Niklinski J, Claassen G, Meyers C, Gregory MA, Allegra CJ, Kaye FJ, Hann SR, Zajac-Kaye M (2000) Mol Cell Biol 20: 5276-84 Platelet-derived growth factor induces multisite phosphorylation of pp60c-src and increases its protein-tyrosine kinase activity. Gould KL, Hunter T (1988) Mol Cell Biol 8: 3345-56 Automatic quality assessment of peptide tandem mass spectra. Bern M, Goldberg D, McDonald WH, Yates JR (2004) Bioinformatics 20 Suppl 1: i49-54 Proteomic profiling of mucosal and submucosal colonic tissues yields protein signatures that differentiate the inflammatory colitides. M'Koma AE, Seeley EH, Washington MK, Schwartz DA, Muldoon RL, Herline AJ, Wise PE, Caprioli RM (2011) Inflamm Bowel Dis 17: 875-83 Mapping the N-terminal residues of Epstein-Barr virus gp42 that bind gH/gL by using fluorescence polarization and cell-based fusion assays. Liu F, Marquardt G, Kirschner AN, Longnecker R, Jardetzky TS (2010) J Virol 84: 10375-85 Pepitome: evaluating improved spectral library search for identification complementarity and quality assessment. Dasari S, Chambers MC, Martinez MA, Carpenter KL, Ham AJ, Vega-Montoto LJ, Tabb DL (2012) J Proteome Res 11: 1686-95 Proteomic consequences of a single gene mutation in a colorectal cancer model. Halvey PJ, Zhang B, Coffey RJ, Liebler DC, Slebos RJ (2012) J Proteome Res 11: 1184-95 Identification of phosphorylation sites within the signaling adaptor APPL1 by mass spectrometry. Gant-Branum RL, Broussard JA, Mahsut A, Webb DJ, McLean JA (2010) J Proteome Res 9: 1541-8 Proteolysis and mass spectrometric analysis of an integral membrane: aquaporin 0. Han J, Schey KL (2004) J Proteome Res 3: 807-12 IDPQuantify: combining precursor intensity with spectral counts for protein and peptide quantification. Chen YY, Chambers MC, Li M, Ham AJ, Turner JL, Zhang B, Tabb DL (2013) J Proteome Res 12: 4111-21 Protein identification using customized protein sequence databases derived from RNA-Seq data. Wang X, Slebos RJ, Wang D, Halvey PJ, Tabb DL, Liebler DC, Zhang B (2012) J Proteome Res 11: 1009-17 Proteomic parsimony through bipartite graph analysis improves accuracy and transparency. Zhang B, Chambers MC, Tabb DL (2007) J Proteome Res 6: 3549-57 Integrating genomic, transcriptomic, and interactome data to improve Peptide and protein identification in shotgun proteomics. Wang X, Zhang B (2014) J Proteome Res 13: 2715-23 IDPicker 2.0: Improved protein assembly with high discrimination peptide identification filtering. Ma ZQ, Dasari S, Chambers MC, Litton MD, Sobecki SM, Zimmerman LJ, Halvey PJ, Schilling B, Drake PM, Gibson BW, Tabb DL (2009) J Proteome Res 8: 3872-81 Identification of protein fragments as pattern features in MALDI-MS analyses of serum. Zimmerman LJ, Wernke GR, Caprioli RM, Liebler DC (2005) J Proteome Res 4: 1672-80 TagRecon: high-throughput mutation identification through sequence tagging. Dasari S, Chambers MC, Slebos RJ, Zimmerman LJ, Ham AJ, Tabb DL (2010) J Proteome Res 9: 1716-26 Activation and association of the Tec tyrosine kinase with the human prolactin receptor: mapping of a Tec/Vav1-receptor binding site. Kline JB, Moore DJ, Clevenger CV (2001) Mol Endocrinol 15: 832-41 Estradiol and phorbol ester cause phosphorylation of serine 118 in the human estrogen receptor. Joel PB, Traish AM, Lannigan DA (1995) Mol Endocrinol 9: 1041-52 Monoclonal antibodies for structure-function studies of (R)-3-hydroxybutyrate dehydrogenase, a lipid-dependent membrane-bound enzyme. Adami P, Duncan TM, McIntyre JO, Carter CE, Fu C, Melin M, Latruffe N, Fleischer S (1993) Biochem J 292 ( Pt 3): 863-72 Interaction of a permeant maleimide derivative of cysteine with the erythrocyte glucose carrier. Differential labelling of an exofacial carrier thiol group and its role in the transport mechanism. May JM (1989) Biochem J 263: 875-81 Characterization of functional domains within Smad4/DPC4. de Caestecker MP, Hemmati P, Larisch-Bloch S, Ajmera R, Roberts AB, Lechleider RJ (1997) J Biol Chem 272: 13690-6 Isoforms of selenoprotein P in rat plasma. Evidence for a full-length form and another form that terminates at the second UGA in the open reading frame. Himeno S, Chittum HS, Burk RF (1996) J Biol Chem 271: 15769-75 Ligand-induced desensitization of the human CXC chemokine receptor-2 is modulated by multiple serine residues in the carboxyl-terminal domain of the receptor. Mueller SG, White JR, Schraw WP, Lam V, Richmond A (1997) J Biol Chem 272: 8207-14 Nitric oxide trapping of tyrosyl radicals generated during prostaglandin endoperoxide synthase turnover. Detection of the radical derivative of tyrosine 385. Goodwin DC, Gunther MR, Hsi LC, Crews BC, Eling TE, Mason RP, Marnett LJ (1998) J Biol Chem 273: 8903-9\nHints: (1) double-click or double-tap to navigate to a node. (2) Grab a node and move it to arrange the graph."", 'Peptides are promising drug candidates because of their diversity, biocompatibility and spectrum of activities. Here, we describe a protocol for high-throughput screening of SPOT-peptide arrays to assess the antibiofilm, antimicrobial and immunomodulatory activities of synthetic peptides. It is a Protocol Extension of our previous Nature Protocols article, which describes the synthesis of SPOT-peptide arrays and assays for screening antimicrobial activity. This latest protocol allows the simultaneous assessment of hundreds of synthetic host defense peptides to define their overall activity profiles and identify candidate sequences that are suitable for further characterization and development as anti-infectives. When coupled with the SPOT-array technology for peptide synthesis, the described procedures are rapid, inexpensive and straightforward for peptide library screening. The protocols can be implemented in most microbiology or immunology research laboratories without the need for specialists. The time to complete each step ranges between 1 and 4 h with overnight pauses, and datasets related to the antibiofilm and immunomodulatory activities of a large set of peptide sequences can be generated in a few days.\nThis is a preview of subscription content, access via your institution\nAccess Nature and 54 other Nature Portfolio journals\nGet Nature+, our best-value online-access subscription\n$29.99 / 30 days\ncancel any time\nSubscribe to this journal\nReceive 12 print issues and online access\n$259.00 per year\nonly $21.58 per issue\nRent or buy this article\nPrices vary by article type\nPrices may be subject to local taxes which are calculated during checkout\nUS Centers for Disease Control and Prevention. Antibiotic Resistance Threats in the United States, 2019. https://www.cdc.gov/drugresistance/pdf/threats-report/2019-ar-threats-report-508.pdf (2019).\nBrown, E. D. & Wright, G. D. Antibacterial drug discovery in the resistance era. Nature 529, 336–343 (2016).\nHancock, R. E. W. The end of an era? Nat. Rev. Drug Discov. 6, 28 (2007).\nMacLean, R. C. & San Millan, A. The evolution of antibiotic resistance. Science 365, 1082–1083 (2019).\nO’Neill, J. Antimicrobial Resistance: Tackling a Crisis for the Health and Wealth of Nations. https://amr-review.org/sites/default/files/AMR%20Review%20Paper%20-%20Tackling%20a%20crisis%20for%20the%20health%20and%20wealth%20of%20nations_1.pdf (2014).\nThe World Bank Group. Drug-Resistant Infections: A Threat to Our Economic Future. https://www.worldbank.org/en/topic/health/publication/drug-resistant-infections-a-threat-to-our-economic-future (2017).\nRudd, K. E. et al. Global, regional, and national sepsis incidence and mortality, 1990–2017: analysis for the global burden of disease study. Lancet 395, 200–211 (2020).\nHarrison, J. J., Ceri, H. & Turner, R. J. Multimetal resistance and tolerance in microbial biofilms. Nat. Rev. Microbiol. 5, 928–938 (2007).\nKoo, H., Allan, R. N., Howlin, R. P., Stoodley, P. & Hall-Stoodley, L. Targeting microbial biofilms: current and prospective therapeutic strategies. Nat. Rev. Microbiol. 15, 740–755 (2017).\nFlemming, H. C. et al. Biofilms: an emergent form of bacterial life. Nat. Rev. Microbiol. 14, 563–575 (2016).\nMookherjee, N., Anderson, M. A., Haagsman, H. P. & Davidson, D. J. Antimicrobial host defence peptides: functions and clinical potential. Nat. Rev. Drug Discov. 19, 311–332 (2020).\nHancock, R. E. W., Haney, E. F. & Gill, E. E. The immunology of host defence peptides: beyond antimicrobial activity. Nat. Rev. Immunol. 16, 321–334 (2016).\nEtayash, H., Azmi, S., Dangeti, R. & Kaur, K. Peptide bacteriocins—structure activity relationships. Curr. Top. Med. Chem. 16, 220–241 (2015).\nZhang, R. et al. Structure-function relationships of antimicrobial peptides and proteins with respect to contact molecules on pathogen surfaces. Curr. Top. Med. Chem. 16, 89–98 (2016).\nde la Fuente-Nunez, C., Reffuveille, F., Haney, E. F., Straus, S. K. & Hancock, R. E. W. Broad-spectrum anti-biofilm peptide that targets a cellular stress response. PLoS Pathog. 10, e1004152 (2014).\nde la Fuente-Nunez, C., Cardoso, M. H., de Souza Candido, E., Franco, O. L. & Hancock, R. E. W. Synthetic antibiofilm peptides. Biochim. Biophys. Acta 1858, 1061–1069 (2016).\nHancock, R. E. W., Nijnik, A. & Philpott, D. J. Modulating immunity as a therapy for bacterial infections. Nat. Rev. Microbiol. 10, 243–254 (2012).\nHilchie, A. L., Wuerth, K. & Hancock, R. E. W. Immune modulation by multifaceted cationic host defense (antimicrobial) peptides. Nat. Chem. Biol. 9, 761–768 (2013).\nHancock, R. E. W. & Sahl, H. G. Antimicrobial and host-defense peptides as new anti-infective therapeutic strategies. Nat. Biotechnol. 24, 1551–1557 (2006).\nYeaman, M. R. & Yount, N. Y. Unifying themes in host defence effector polypeptides. Nat. Rev. Microbiol. 5, 727–740 (2007).\nEtayash, H., Pletzer, D., Kumar, P., Straus, S. K. & Hancock, R. E. W. Cyclic derivative of host-defense peptide IDR-1018 improves proteolytic stability, suppresses inflammation, and enhances in vivo activity. J. Med. Chem 63, 9228–9236 (2020).\nMookherjee, N. et al. Intracellular receptor for human host defense peptide LL-37 in monocytes. J. Immunol. 183, 2688–2696 (2009).\nYu, H. B. et al. Sequestosome-1/p62 is the key intracellular target of innate defense regulator peptide. J. Biol. Chem. 284, 36007–36011 (2009).\nChoi, K. Y., Chow, L. N. & Mookherjee, N. Cationic host defence peptides: multifaceted role in immune modulation and inflammation. J. Innate Immun. 4, 361–370 (2012).\nHaney, E. F., Straus, S. K. & Hancock, R. E. W. Reassessing the host defense peptide landscape. Front. Chem. 7, 43 (2019).\nHaney, E. F. et al. Computer-aided discovery of peptides that specifically attack bacterial biofilms. Sci. Rep. 8, 1871 (2018).\nHaney, E. F., Mansour, S. C., Hilchie, A. L., de la Fuente-Nunez, C. & Hancock, R. E. W. High throughput screening methods for assessing antibiofilm and immunomodulatory activities of synthetic peptides. Peptides 71, 276–285 (2015).\nHaney, E. F., Barbosa, S. C., Baquir, B. & Hancock, R. E. W. Influence of non-natural cationic amino acids on the biological activity profile of innate defense regulator peptides. J. Med. Chem. 62, 10294–10304 (2019).\nHilpert, K., Winkler, D. F. & Hancock, R. E. W. Peptide arrays on cellulose support: SPOT synthesis, a time and cost efficient method for synthesis of large numbers of peptides in a parallel and addressable fashion. Nat. Protoc. 2, 1333–1349 (2007).\nRabin, N. et al. Agents that inhibit bacterial biofilm formation. Future Med. Chem. 7, 647–671 (2015).\nHaney, E. F., Trimble, M. J., Cheng, J. T., Valle, Q. & Hancock, R. E. W. Critical assessment of methods to quantify biofilm growth and evaluate antibiofilm activity of host defence peptides. Biomolecules 8, 29 (2018).\nMünzker, L., Oddo, A. & Hansen, P. R. Chemical synthesis of antimicrobial peptides. Methods Mol. Biol. 1548, 35–49 (2017).\nKimmerlin, T. & Seebach, D. ‘100 years of peptide synthesis’: ligation methods for peptide and protein synthesis with applications to beta-peptide assemblies. J. Pept. Res. 65, 229–260 (2005).\nFields, G. B. Introduction to peptide synthesis. Curr. Protoc. Mol. Biol. Ch. 11, Unit 11.15 (2002).\nSüssmuth, R. D. & Mainz, A. Nonribosomal peptide synthesis-principles and prospects. Angew Chem. Int. Ed. Engl. 56, 3770–3821 (2017).\nJaradat, D. M. M. Thirteen decades of peptide synthesis: key developments in solid phase peptide synthesis and amide bond formation utilized in peptide ligation. Amino Acids 50, 39–68 (2018).\nStawikowski, M. & Fields, G. B. Introduction to peptide synthesis. Curr. Protoc. Protein Sci. Ch. 18, Unit 18.11 (2012).\nCoin, I., Beyermann, M. & Bienert, M. Solid-phase peptide synthesis: from standard procedures to the synthesis of difficult sequences. Nat. Protoc. 2, 3247–3256 (2007).\nBehrendt, R., White, P. & Offer, J. Advances in Fmoc solid-phase peptide synthesis. J. Pept. Sci. 22, 4–27 (2016).\nMäde, V., Els-Heindl, S. & Beck-Sickinger, A. G. Automated solid-phase peptide synthesis to obtain therapeutic peptides. J. Org. Chem. 10, 1197–1212 (2014).\nMerrifield, R. B. Solid-phase peptide synthesis. Adv. Enzymol. Relat. Areas Mol. Biol. 32, 221–296 (1969).\nda Silva, A. Jr. et al. Avian anticoccidial activity of a novel membrane-interactive peptide selected from phage display libraries. Mol. Biochem. Parasitol. 120, 53–60 (2002).\nPini, A. et al. Antimicrobial activity of novel dendrimeric peptides obtained by phage display selection and rational modification. Antimicrob. Agents Chemother. 49, 2665–2672 (2005).\nAshby, M., Petkova, A., Gani, J., Mikut, R. & Hilpert, K. Use of peptide libraries for identification and optimization of novel antimicrobial peptides. Curr. Top Med. Chem. 17, 537–553 (2017).\nWinkler, D. F., Hilpert, K., Brandt, O. & Hancock, R. E. W. Synthesis of peptide arrays using SPOT-technology and the CelluSpots-method. Methods Mol. Biol. 570, 157–174 (2009).\nWinkler, D. F., Andresen, H. & Hilpert, K. SPOT synthesis as a tool to study protein-protein interactions. Methods Mol. Biol. 723, 105–127 (2011).\nKaur, K., Ahmed, S., Soudy, R. & Azmi, S. Screening peptide array library for the identification of cancer cell-binding peptides. Methods Mol. Biol. 1248, 239–247 (2015).\nSoudy, R., Ahmed, S. & Kaur, K. NGR peptide ligands for targeting CD13/APN identified through peptide array screening resemble fibronectin sequences. ACS Comb. Sci. 14, 590–599 (2012).\nBluhm, M. E., Knappe, D. & Hoffmann, R. Structure-activity relationship study using peptide arrays to optimize Api137 for an increased antimicrobial activity against Pseudomonas aeruginosa. Eur. J. Med. Chem. 103, 574–582 (2015).\nKnappe, D. et al. Optimization of oncocin for antibacterial activity using a SPOT synthesis approach: extending the pathogen spectrum to Staphylococcus aureus. Amino Acids 48, 269–280 (2016).\nOmmen, P., Zobek, N. & Meyer, R. L. Quantification of biofilm biomass by staining: non-toxic safranin can replace the popular crystal violet. J. Microbiol. Methods 141, 87–89 (2017).\nToté, K., Vanden Berghe, D., Maes, L. & Cos, P. A new colorimetric microtitre model for the detection of Staphylococcus aureus biofilms. Lett. Appl. Microbiol. 46, 249–254 (2008).\nStiefel, P. et al. Is biofilm removal properly assessed? Comparison of different quantification methods in a 96-well plate system. Appl. Microbiol. Biotechnol. 100, 4135–4145 (2016).\nHarrison, J. J. et al. Microtiter susceptibility testing of microbes growing on peg lids: a miniaturized biofilm model for high-throughput screening. Nat. Protoc. 5, 1236–1254 (2010).\nAmsen, D., de Visser, K. E. & Town, T. Approaches to determine expression of inflammatory cytokines. Methods Mol. Biol. 511, 107–142 (2009).\nHuang, X. et al. Effect of arginine on the growth and biofilm formation of oral bacteria. Arch. Oral Biol. 82, 256–262 (2017).\nChen, P., Abercrombie, J. J., Jeffrey, N. R. & Leung, K. P. An improved medium for growing Staphylococcus aureus biofilm. J. Microbiol. Methods 90, 115–118 (2012).\nBowdish, D. M. et al. Impact of LL-37 on anti-infective immunity. J. Leukoc. Biol. 77, 451–459 (2005).\nTomita, T. et al. Effect of ions on antibacterial activity of human beta defensin 2. Microbiol. Immunol. 44, 749–754 (2000).\nHammond, A., Dertien, J., Colmer-Hamood, J. A., Griswold, J. A. & Hamood, A. N. Serum inhibits P. aeruginosa biofilm formation on plastic surfaces and intravenous catheters. J. Surg. Res. 159, 735–746 (2010).\nWiegand, I., Hilpert, K. & Hancock, R. E. W. Agar and broth dilution methods to determine the minimal inhibitory concentration (MIC) of antimicrobial substances. Nat. Protoc. 3, 163–175 (2008).\nMetcalf, T. U. et al. Global analyses revealed age-related alterations in innate immune responses after stimulation of pathogen recognition receptors. Aging Cell 14, 421–432 (2015).\nTer Horst, R. et al. Host and environmental factors influencing individual human cytokine responses. Cell 167, 1111–1124.e13 (2016).\nLi, Y. et al. A functional genomics approach to understand variation in cytokine production in humans. Cell 167, 1099–1110.e14 (2016).\nSchirmer, M., Kumar, V., Netea, M. G. & Xavier, R. J. The causes and consequences of variation in human cytokine production in health. Curr. Opin. Immunol. 54, 50–58 (2018).\nWu, B. C., Lee, A. H. & Hancock, R. E. W. Mechanisms of the innate defense regulator peptide-1002 anti-inflammatory activity in a sterile inflammation mouse model. J. Immunol. 199, 3592–3603 (2017).\nHaney, E. F. et al. Identification of an IDR peptide formulation candidate that prevents peptide aggregation and retains immunomodulatory activity. Pept. Sci. (Hoboken) 111, e24077 (2019).\nMartikainen, M. V. & Roponen, M. Cryopreservation affected the levels of immune responses of PBMCs and antigen-presenting cells. Toxicol. In Vitro 67, 104918 (2020).\nYang, J. et al. The effects of storage temperature on PBMC gene expression. BMC Immunol. 17, 6 (2016).\nLi, X., Zhong, Z., Liang, S., Wang, X. & Zhong, F. Effect of cryopreservation on IL-4, IFNγ and IL-6 production of porcine peripheral blood lymphocytes. Cryobiology 59, 322–326 (2009).\nDarveau, R. P. & Hancock, R. E. Procedure for isolation of bacterial lipopolysaccharides from both smooth and rough Pseudomonas aeruginosa and Salmonella typhimurium strains. J. Bacteriol. 155, 831–838 (1983).\nWe acknowledge the financial assistance of the Canadian Institutes of Health Research, Foundation grant FDN-154287 to R.E.W.H. for funding peptide research in our laboratory. H.E. is the recipient of a UBC Killam Fellowship and a Research Trainee Award from the Michael Smith Foundation for Health Research. R.E.W.H. is a Canada Research Chair in Health and Genomics and holds a UBC Killam Professorship.\nE.F.H. and R.E.W.H. have filed patents related to the antibiofilm and immunomodulatory functions of synthetic HDPs. These patents have been assigned to their employer, the University of British Columbia, and licensed to ABT Innovations Inc., in which R.E.W.H. has an ownership position.\nPeer review information Nature Protocols thanks Donald Davidson, Sam Walker and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nKey references using this protocol\nHaney, E. F. et al. Sci. Rep. 8, 1871 (2018): https://doi.org/10.1038/s41598-018-19669-4\nHaney, E. F., Mansour, S. C., Hilchie, A. L., de la Fuente-Nunez, C. & Hancock, R. E. W. Peptides 71, 276–285 (2015): https://doi.org/10.1016/j.peptides.2015.03.015\nHaney, E. F., Barbosa, S. C., Baquir, B. & Hancock, R. E. W. J. Med. Chem. 62, 10294–10304 (2019): https://doi.org/10.1021/acs.jmedchem.9b01344\nThis protocol is an extension to: Nat. Protoc. 2, 1333–1349 (2007): https://doi.org/10.1038/nprot.2007.160\nAbout this article\nCite this article\nEtayash, H., Haney, E.F. & Hancock, R.E.W. Assessing biofilm inhibition and immunomodulatory activity of small amounts of synthetic host defense peptides synthesized using SPOT-array technology. Nat Protoc 16, 1850–1870 (2021). https://doi.org/10.1038/s41596-021-00500-w']"	['<urn:uuid:9667c475-224f-46cd-87f1-b6557a03465e>', '<urn:uuid:97a9c36b-a717-4ddf-9377-0a17d98730c4>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T05:18:19.158519	11	85	3470
13	how long eaglets fed after leaving nest	When the juvenile eagles leave the nest, their parents continue to feed them for four to six weeks.	"[""Eaglets prepare to take wing in Pittsburgh, Harmar\nThere's a whole lotta flapping going on.\nThe three eaglets in the bald eagle nest in Pittsburgh's Hays neighborhood and the one eaglet in Harmar will spread their wings to leave the nests within the next month or so.\nThose four youngsters are the biggest crop of the once-endangered bald eagles born and raised in the Pittsburgh area in more than 150 years.\nPlus, there might be more: A pair of eagles that have been nesting for four years on private property in Crescent, near Beaver County, were last reported to be brooding eggs, according to the Pennsylvania Game Commission.\n“From a conservation point, four offspring is something we should be happy about,” said Brian Shema, director of conservation for the Audubon Society of Western Pennsylvania.\n“This is a first in our lifetime,” he said.\nThe birds are back because of cleaner rivers teeming with the bird's favorite food — fish — and a recovery from near extinction because of DDT and other factors.\nThirty years ago, when there were only three known nests left in the state, the game commission started to re-introduce the birds.\nThe efforts paid off as the bald eagle population has increased steadily and, in recent years, has topped more than 200 nesting pairs.\nBut not in the city limits of Pittsburgh.\nThat was until last year when the Hays couple successfully raised one young bird.\nThe Harmar nest produced young for the first time this year.\nAnd nest activity at both sites is ramping up.\nThe eaglets have grown quickly and healthy from white fuzz balls to dark, vulture-looking birds — but with the telltale large eagle beak.\nThey will don brown and white plumage for four to five years until their signature snowy head and tail feathers grow in.\nThe Pittsburgh birds can be seen on a webcam installed by PixController and the game commission romping, exercising their wings with increasing frequency, building up their muscles for flight, according to Dan Brauning, wildlife diversity division chief for the Pennsylvania Game Commission.\nFor added drama, the adolescent birds will fly-hop, catching some air for short bursts of flight, sometimes teetering on the edge of the nest.\nThe parents will do their part to encourage the young to fly by withholding meals and teasing them with food, according to Shema. The parents will also call to the young from a distance, he added.\nWhen the juvenile eagles leave the nest, their parents will continue to feed them for four to six weeks, Shema said.\nBut nest departures can be challenging for the young eaglets that sometimes get grounded.\nIn a U.S. Fish and Wildlife study of eaglets outfitted with radio transmitters, most young eagles ended up on the ground at some point after fledging, according to Brian Millsap, national raptor coordinator for the federal Fish and Wildlife Service.\n“But those scramble up small trees and get airborne again on their own in most cases,” he said. “This is pretty normal.”\nMary Ann Thomas is a staff writer for Trib Total Media. She can be reached at 724-226-4691 or firstname.lastname@example.org.""]"	['<urn:uuid:ff599df0-bad7-4050-a88b-5ca4f741ab59>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	7	18	522
14	As a naturopath researching Australian native plants, I'm wondering what specific antifungal properties lemon myrtle has compared to tea tree oil based on university research?	Research conducted at Charles Sturt University in Wagga Wagga in 2001 and 2002 demonstrated that lemon myrtle essential oil was more anti-fungal than tea tree while being equally anti-bacterial.	['AUSTRALIAN NATIVE shrub lemon myrtle was given the botanical name Backhousia citriodora in 1853 after the English botanist, James Backhouse.\nOriginally called ‘lemon-scented myrtle’ because of its strong lemon smell when the leaves are crushed, the name was shortened to ‘lemon myrtle’ by the native foods industry, which marketed the leaf for culinary use.\nDespite there being no written evidence of indigenous use of lemon myrtle, we can be quite sure that indigenous Australians long used the plant, both in food and as a healing tool.\nBy 1856 white settlers started exploring uses for lemon myrtle; notably documented by renowned Australian botanist Ferdinand von Mueller around the Moreton Bay area of southeast Queensland.\nBy 1888 the first lemon myrtle essential oil distillation is thought to have occurred. There were several small distillations of lemon myrtle essential oil from small wild populations of lemon myrtle trees, particularly around the Gympie area through to World War 1.\nWhen raw materials (lemon essences) were in short supply in World War 2, the tree was wild harvested again. Distillation again ceased after World War 2.\nCoastal rainforest in its native habitat of south-east Queensland was cleared very early during European settlement, resulting in the tree being rather rare to find in the wild.\nThis remarkably flavoursome herb was eventually planted out in small plantations in the mid 1990s.\nBy early 2000, one or two farmers had developed good cultivars of the tree and some simple but effective harvesting and processing equipment was also developed. Lemon myrtle leaves used as tea have become a growing niche market.\nToday lemon myrtle is sometimes referred to as the ‘Queen of the lemon herbs’ and is gaining recognition in the food, cosmetic and pharmaceutical industries as an ingredient to flavour everything from tea, meat and ice-cream through to its therapeutic benefits in soaps, shampoos and moisturisers.\nThe leaf is often used as dried flakes or in the form of an encapsulated flavour essence for enhanced shelf life.\nIncrease immunity to viruses and bacteria\nViruses and bacteria are quickly becoming resistant to traditional treatments and essential oils such as lemon myrtle can become key components in staying healthy. Lemon myrtle shares the antibacterial properties of lemon and tea tree oil and has been shown to be effective against even new strains of staph and many food-borne diseases.\nLemon myrtle essential is also antiviral and was demonstrated to be effective against listeria.\nResearch in Japan in 2000 revealed lemon myrtle to be effective against herpes virus. Use as a potent anti-bacterial and germicide to diffuse regularly to reduce coughs, cold and sinus issues.\nLemon myrtle essential oil possesses excellent antimicrobial properties; however the undiluted essential oil is toxic to human cells in vitro. When diluted to approximately one per cent, absorption through the skin and subsequent damage is thought to be minimal. Lemon myrtle oil has a high ‘Rideal–Walker coefficient,’ which is a measure of antimicrobial potency.\nEase fungal infections\nAlong with powerful antibacterial properties, lemon myrtle oil has also been shown to have antifungal properties as well.\nIn 2001 and 2002 at Charles Sturt University in Wagga Wagga it was demonstrated and published that Lemon Myrtle essential oil was more anti-fungal than tea tree and as anti-bacterial as tea tree.\nAnyone who suffers from repeated fungal infections such as athlete’s foot may find using lemon myrtle oil diluted in carrier oil helpful in reducing incidences of and preventing the infection. It may also be helpful in the treatment of candida overgrowth in the body as well.\nLemon myrtle oil has powerful anti-inflammatory properties that can help to fight both external and internal inflammation. It has a tonic effect on the digestive system and may be used for irritable digestive disorders and flatulence. Add a few drops to a stomach massage blend to assuage distension and discomfort.\nYou can make soothing massage oils by mixing five drops of lemon myrtle essential oil with 100mL of quality vegetable carrier oil.\nAdd cypress, manuka, juniper or frankincense for an effective pain queller. Massage this blend into aching joints or any inflamed areas.\nLemon myrtle, like tea tree, is an extremely powerful cleanser. Due to its extremely high citral content it can be harsh on the skin, however with adequate dilution in a carrier oil, usually one per cent, it can be safely used to treat many skin conditions.\nAny individual with extreme skin sensitivities should exercise precautionary principles before using this oil and maybe increase the dilution significantly.\nFresh-smelling lemon myrtle can revitalise dull or tired skin, and balance skin sebum production for those with oily skin.\nIts antibacterial action is effective in clearing skin eruptions such as acne - especially if it is caused by the overproduction of oil on the skin - as it cleanses pores, discourages the overgrowth of bacteria and reduces swelling and redness. Add a few drops of lemon myrtle oil to some distilled witchazel or detoxifying clay to treat breakouts.\nLemon myrtle’s anti-inflammatory properties will help relieve the itch of insect bites as well as repelling insects from attempting to feast on the skin in the first place.\nPainful eruptions on the skin\nMolluscum contagiosum is a viral skin infection that results in sometimes painful eruptions of the skin that can occur anywhere on the body and usually in children. Traditional treatment included abrasion of the skin to stimulate the body’s immune response.\nUse of lemon myrtle oil as a treatment for the skin lesions caused by this virus (MCV) has been researched and considered successful. Treating with lemon myrtle oil may prevent further spread of the infection and help ease some of the skin inflammation.\nBlend lemon myrtle with carrier oil and apply topically to the Mollusca daily until they shrink or go away.\nThe oil does not clear the virus but alleviates the symptoms and allows the body to heal properly. Interestingly, nine of sixteen patients who were treated with an unusually high strength of lemon myrtle oil (at 10 per cent) showed a significant improvement, compared to none in the control group. Professional guidance when using such a high dilution is recommended.\nEffects of citral, which is the main constituent in lemon myrtle, on an energy-intense diet model of obesity, were studied in India and published in the Indian Journal of Pharmacology.\nIt revealed that the citral-treated groups showed a dose-dependent reduction in body weight gain.\nThey significantly had lower fasting glucose levels, improved glucose tolerance, higher metabolic rate and smaller adipocytes after drug administration.\nWe can conclude from this research on citral, that the use of lemon myrtle could have similar effects on weight reduction. In addition it can improve insulin sensitivity and glucose tolerance.\nIn the current scenario of increasing prevalence of obesity and diabetes, citral-rich lemon myrtle may prove to be a prodigious agent in its management.\nThe Journal of Alternative and Complementary Medicine has published results of successful tests of lemongrass on anxiety. It is the constituent citral in this oil that is responsible for marked positive shifts in mood for people in depressive states and who suffer from anxiety.\nLemon myrtle that has a higher content of citral than lemongrass and could prove to be a helpful treatment for anxiety and depression.\nThese conditions have different causes, such as inflammation of the nerves in the brain, which prevents the proper usage of mood balancing hormones.\nNot only does lemon myrtle possess anti-inflammatory properties that soothe the inflamed nerves but also it is slightly sedative, so it helps to relax tension that can exacerbate anxiety. Soothing and calming, this oil improves sleep.\nLemon myrtle with its bright, cheerful scent lifts our spirits and counters exhaustion and shock.\nWhen choosing a mood-lifting mix, blend lemon myrtle essential oil with grounding vetivert or sandalwood and calming chamomile to use in a diffuser to help relax tension and lift mood.\nUse lemon myrtle in the yoga room to reduce the busyness of the mind and improve concentration; it relaxes but does not heavily sedate.\nThis oil clears the spirit of heavy feelings of weakness and brings clarity for those who feel lost on their spiritual journey. It encourages us to release trapped emotions and helps us to breathe freely, which is always rejuvenating.\nLemon myrtle is a popular ingredient in health care and cleaning products, especially soaps, lotions and shampoos. It can be used in a diffuser, a spray air freshener, pot-pourri, massage oil, carpet cleaning, or in the vacuum cloth bag, floor-washing water and dishwashing liquid. Use as a surface disinfectant or as an anti-microbial food additive. It is helpful in the preservation of foods that are prone to fungal infections. Wipe wooden boards that are used for processing meat.\nMix a few drops of this oil with vinegar and tea tree to create a spray that will disinfect surfaces anywhere in your house.\nDiffusing in the air should kill any airborne bacteria that may cause seasonal allergies and respiratory distress.\nAdd to a repellent blend to ward off moths and silverfish in clothes or food cupboards.\nLemon myrtle essential oil has many culinary uses and because of the pure oil’s potency, only a drop or two will be needed to flavour the products.\nIt is best suited to: vinaigrettes, mayonnaise, salad dressings, cold custards, sorbets, cheese cakes, ice creams and any product with a milk or oil base where no heat is applied after the oil has been added.\nTry as a lemon flavour replacement in milk-based foods, such as cheesecake, lemon flavoured ice cream and sorbet; it doesn’t have the curdling problem associated with lemon fruit acidity.\nLemon myrtle essential oil may be used in cooking oils such as olive, sunflower or macadamia oil, at a ratio of 8 to 12 drops per 1000mL of oil.\nLemon myrtle has interesting applications in alcoholic drinks; add to gins, cocktails and bitter tonics. To flavour drinks add 6 to 9 drops per 1000mL of liquid.\nName: Lemon Myrtle\nBotanical name: Backhousia citriodora\nAlso known as: sweet verbena myrtle\nSometimes confused with: Lemon ironbark (Eucalyptus staigeriana)\nThe Plant: Lemon myrtle is a flowering plant endemic to Australian subtropical rainforests of central and South Eastern Queensland with a natural distribution from Mackay to Brisbane. It can reach six metres in height, but is often smaller when cultivated and rarely exceeds five metres; usually with a dense canopy. The leaves are evergreen, opposite, lanceolate, and glossy green. The flowers are creamy-white, 5–7mm in diameter, produced in clusters at the ends of the branches from summer through to autumn. After petal fall the calyx is persistent.\nLemon myrtle is grown as a hardy ornamental plant that tolerates all but the poorest drained soils. It can be grown from tropical to warm temperate climates and may handle cooler districts, provided it can be protected from frost when young.\nThe Oil: The tree doesn’t smell a lot unless you tear the leaf, to open up the oil glands. Lemon myrtle contains the highest natural source of citral. Lemongrass and lemon myrtle essential oils both include noteworthy concentrations of limonene.\nExtraction: Lemon myrtle essential oil is extracted via steam distillation of the leaves and green small branches. In Australia the majority of commercial lemon myrtle is grown in Queensland and the north coast of New South Wales.\nIn plantation cultivation the tree is typically maintained as a shrub by regular harvesting from the top and sides; it is important to retain some lower branches when pruning for plant health. Mechanical harvesting is used in commercial plantations.\nThe harvested leaves are dried for leaf spice, or distilled for the essential oil. Controls are being developed for a significant fungal pathogen called myrtle rust (Uredo rangelii) that was detected in lemon myrtle plantations in January 2011. Myrtle rust severely damages new growth and threatens lemon myrtle production.\nScent: A strong top note, this essential oil releases a strong uplifting lemon aroma that is also rich and smooth to smell. More complex than lemon, lemon myrtle imparts its characteristically citrus notes immediately, but gives way to a smoky bush flavour.\nA universally pleasant scent for most people. It is also considered to have a ‘cleaner and sweeter’ aroma than comparable sources of citral - lemongrass and litsea cubeba - with more clarity than lemongrass. The oil has the highest citral purity so avoid blending with other citrus oils.\nBlends well with: woody, spicy, floral oils, consider cedarwood atlantica, geranium bourbon, ylang ylang, cinnamon, pimento or myrrh. Try it with cypress, eucalyptus varieties, niaouli, sandalwood or tea tree. Use sparingly to avoid overpowering.\nIndications: Disinfectant, antibacterial, antifungal, anti-viral, anti-inflammatory, anti-infectious, soothes hypertension. Inhibits formation of fat cells, increases metabolic rate, reduces weight gain and improves tolerance to a glucose load. Anti-depressive, sedative, improves concentration, deodorising, flea repellent.\nPrecautions: Contains citral, a strong sensitiser. Avoid use on broken skin. Avoid use during pregnancy; there is evidence that high citral oils can affect foetal development. Maximum dilution for topical 0.7 to 1 per cent.\nContraindications: Diabetic medications']	['<urn:uuid:df2b3aed-808d-4f42-9bdf-12876ae6e2a3>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	25	29	2163
15	How does the process of capturing facial expressions differ between creating a watercolor costume design and crafting a wax figure, and what specific details are emphasized in each medium?	In watercolor costume design, as shown in Somov's 1909 sketch of Pavlova as Columbine, the emphasis is on capturing the overall theatrical expression through paint on rough muslin. For wax figures, the process is much more intricate - it starts with clay sculpting where every mole and wrinkle is captured in perfect proportion, followed by wax carving where special surgical-like tools are used to etch authentic facial expressions, fine lines, and unique markings. The wax process also includes adding individual strands of hair, medical glass eyes, and porcelain teeth to achieve lifelike characteristics.	"['Pavlova as Columbine\nThis watercolor sketch by K. Somov was painted in 1909 and is clearly labeled as a costume design specifically for Pavlova in the role of Columbine, in the age-old tale of Harlequin, Pierrot, and Columbine. (See another costume, “Harlequinade” in “The Tsar’s Ballerina” Gallery). I was puzzled that I couldn’t find a single photograph of Pavlova wearing this costume. I now believe that it was never made.\nSomov was a Russian artist and costume designer in the circle of the famous impresario Sergei Diaghilev, who in 1909 was forming his groundbreaking dance company, the Ballets Russes. Pavlova danced for Diaghilev in Paris very briefly and was probably scheduled for later in this role. But after just one volatile season, the uncompromising ballerina and the unyielding impresario broke ties, and Pavlova went on to form her own company. Diaghilev was giving top billing to his male dancers, and Pavlova was never very good at sharing the spotlight with others! She would later dance other iterations of the Harlequin cycle, but never in this costume.\nNote: This triad of characters—originating in medieval day—is also the source of today’s DC Comics Harley Quinn. What is old is new again!\nAs long as there have been theaters and costume shops, there have been times when money, materials and time were scarce and opening night was just around the corner. A time-honored way to solve all these problems at once was to paint a design on rough muslin, imitating the intricate piecing and embellishing of a more laborious creation. Some of the costumes still in existence from the Ballets’ Russes are done in just this way. And much later, in 1946, when the Sadler Wells Ballet reintroduced “Sleeping Beauty” to a war-weary and devastated London, its costume shop did the same thing. I wanted to try this for my reimagined Columbine costume. I drew out all the pieces (sixteen in the lined bodice alone) from a sturdy cotton poplin, and with fabric paint and a small filbert brush painted each piece individually before sewing the whole thing together. Though the process was time-consuming, it certainly was a lot quicker than piecing hundreds of tiny fabric triangles together. I hope that when the stage lights come up and they bounce across my Pavlova in her iridescent colors, the audience will forgive me my little economies of money and time!\nPavlova Attends Knight Exhibit\nDame Laura Knight (1877-1970) was one of Britain’s most accomplished and popular figurative painters. The circus, the theatre, and the ballet were favorite subjects for her prodigious body of work in oils, watercolors, dry point and etching. Pavlova was often a model and muse. In 1920, the famous dancer attended an exhibition of Knight’s work at the Leicester Galleries in London. Ever the fashion plate, she was dressed in a cream silk suit, matching hat, and an ermine and mink shawl. She is photographed looking at Knight’s painting “Taking a Bow,” where she and her partner Alexander Volinine are subjects. Pavlova is acknowledging the audience not with the usual restrained curtsy, but with a traditional Russian bow, leaning far forward from the waist with arms flung out. The vivid colors of this delightful painting capture all the exuberance and drama of the performance.\nPavlova attends Knight Exhibit, Re-Imagined\nAs she has done so often, my friend Marina of All For Doll found the perfect piece of fur for me, complete with black-tipped “tails.” Marina is originally from Siberia, and she shares my passion for Russian ballet. I am indebted to her for many wonderful props and beautiful furs that she finds in her resale shop excursions. The suit is made from a piece of cream-colored silk from Elfriede’s Fine Fabrics. What makes it particularly special is that comes from a bolt of Japanese silk from 1945, brought into the shop by a US Army veteran who obtained it while serving in Occupied Japan after World War II. I was fortunate to get a yard of this historic fabric—as beautiful as the day it was rolled onto the bolt — and I’ve used it here and in other outfits. I love when there is a story within a story in the Pavlova Project!\nThe hat is a one-quarter scale adaptation from a people-sized pattern called “Eleanor” from Elsewhen sewing patterns. The elegant armchair was issued by the Tonner Doll Company.\nTwo Portraits of Pavlova, 1924\nSavely Sorine, like his better-known contemporary, John Singer Sargent, was a portrait painter of beautiful women: royalty, wealthy society ladies, literary figures, and beauties of theatre and dance. He created these two paintings of Pavlova, both in private collections. MOMA has one of his many drawings of the dancer, this one of her celebrity-status foot. Sorine was already a successful painter when he fled to Paris in the wake of the Russian Revolution, and eventually he made his way to New York City. It is not clear where and when Pavlova sat for these two portraits, but they were probably painted in the early to mid-1920s.The white costume is from the ballet Giselle, and captures the artistry of Pavlova. But the other portrait, where she wears fashionable street clothes, is particularly revealing because we get to see her relaxed, without her heavy stage makeup, and not in the overly-theatrical poses of the time. She gazes out at us as she leans against a wall, arms folded, her famously expressive hands beautifully rendered, and a facial expression of ever-so-slight amusement—or is she planning some mischief?\nTwo Portraits by Sorine, Re-Imagined\nI’ve chosen Tonner’s slightly amused-looking “Kit” sculpt, which I feel works well for Sorine’s portraits. I bought two identical dolls on Ebay before I even began the costumes. I wanted to capture the look and feel of the original paintings as well as I could. For the Giselle costume, I made a romantic length ballet skirt with four layers of tulle overlaid with silk organza. The bodice is silk dupioni, piped and overlaid with the same organza, with sheer sleeves. The original costume was designed by Russian artist Alexander Benois and his design has come down to our own day nearly unchanged.\nFor Pavlova in her brown suit, my friend Marina at All For Doll provided me with the pattern for the silk-lined coat and found the fur trim for me. Underneath the coat is a collared “flapper” style dress made of cream colored silk, adapted from a pattern by Magalie Houle Dawson of MHD Designs Miniature gold picture frames were made to my specifications by Jim Coates, who has a shop on Ebay. This diorama is one of my favorites in the whole Pavlova Project collection.\nThis lively pas de deux, with music by Paul Lincke and choreography by Ivan Clustine, delighted audiences from its premier in 1913. It remained in Pavlova’s repertoire until her death in 1931.\nThere were many versions of this dance, and as many costumes, but this lemony silk Directoire dress, designed by famed fashion artist Erte (Romain de Tirtoff, Russian, 1892-1990) was an audience favorite. Her partner wears black satin tails and a bright cumberbund accessorized by lace collar and cuffs.\nAmerican artist and dance historian Troy Kinney was best known for his works portraying dance performers, among them Ruth St. Denis, Sophie Pflanz, and of course, Anna Pavlova. He traveled the world to study various dance styles, and with his wife published two books on dance which are still respected works today.\nGavotte Pavlova, Re-Imagined\nThough Erte’s original gouache painting for this costume shows the dress in blue and red, it was eventually made up in bright yellow silk and either black or magenta lining (descriptions vary). I used a brillilant silk charmeuse lined with magenta china silk and trimmed with gold sequins, braid, and ribbons. Gold heels and a huge brimmed bonnet complete the costume. Volinin’s tuxedo is in my favorite silk/cotton blend from Elfriede’s, lined in the same magenta. Vintage lace accessorizes both costumes. I am once again indebted to Joe Kowalksi for the pattern for the man’s tuxedo.\n“Frenzied, reckless abandon, divine intoxication,” this pas de deux between Pavlova and the beefy Mordkin was a sensation when it opened in New York’s Palace Theater in 1910. With music by Alexander Glazunov and choreography by Marius Petipa, it quickly became an audience favorite, its populariy surpassed only by Pavlova’s famous Swan.\nThat year, the brilliant young sculptor Malvina Hoffman—later referred to as “America’s Rodin”— saw Pavlova and Mordkin perform Bacchanale, and their electrifying movements were burned into her memory. For the next four years she worked on arranging a meeting with the famous dancers, and in 1914 she had her opportunity. With sketchbook in hand, Hoffman attended several performances, and then had Pavlova and Mordkin come to her studio. There, as the dancers posed against a green backdrop, she created hundreds of drawings and photographs — works of art in their own right — that would eventually become one of her most famous works: Bacchanale Frieze. This was a long, continuous series of bas reliefs which in breathless detail captured twenty five key movements from the dance, as though the dancers were momentarily frozen and then moved to the next sequence, over and over again. Working first in clay, then casting in plaster and eventually bronze, the images were remarkable for capturing movement, narrative and music in a very unlikely medium. It is worthwhile to take a moment to look at this incredible work of art in Didi Hoffman’s website and blog. Didi Hoffman, is married to the great-nephew of the famous sculptor, and her website gives us a rare glimpse into the extraordinary collaboration and genuine friendship between these two historically significant women.\nFor some reason I have a lot of trouble with Greek and Roman tunics and togas. They should be easy: a rectangle with slits on the sides for the arms and an opening at the top for the head. But I struggle with this, and I can’t ever seem to get the drape, length, and folds just right.\nThese costumes were made with a gauzy dotted Swiss fabric which should drape better than it does. They were hand beaded with Swarovski crystals. The same crystals, in varying sizes, were also used to make the bejeweled grapes that both dancers wear in their hair. Red silk ribbon completes the look. Making these costumes was a long, tedious affair, but I’ve tried, successfully I hope, to capture some of the vivacity of the original ballet.\nIn 1922 ballet critic Valerian Svetloff commissioned several well-known artists to illustrate his chef-d’oeuvre, a limited-edition book he was writing about his muse, Anna Pavlova. Svetloff was convinced that she was the greatest ballerina who had ever lived. I have a 1974 Dover Publications re-issue of this book (the originals now fetch prices upwards of 1200 euros). Among the artists whose works appear are Leon Bakst, John Lavery, Georges Barbier and Konstantin Korovin, all of whom are mentioned elsewhere on this website. Less well-known is Aime Stevens, a painter of beautiful women, in the style of John Singer Sargent, who contributed two delightful Illustrations of Pavlova in her costume for “Syrian Dance.”\nThis short one-act ballet was part of a series of separate pieces loosely grouped together as “Oriental Fantasies.” Music was by Camille Saint-Saens and choreography by Mikhail Fokine, who had also partnered for Pavlova’s poignant signature role, “The Dying Swan.” This piece, in contrast, was exotic and seductive. It tells the story of the attempted seduction of a beautiful young slave girl at the bazaar. In the words of Pavlova’s music director, Walford Hyden, “Pavlova was completely transformed…her voluptuous poses and…undulating, sensuous Eastern expressions were as un-European as could be.”\nSyrian Dance, Re-Imagined\nI rarely know the original colors of the costumes I am re-creating, but this time I had two beautiful paintings by Stevens. To get the right color for the draping fabric and sleeves, I hand-dyed a piece of white silk netting, then added gorgeous beaded edging from my favorite shop, Elfriede’s Fine Fabrics. A red silk bodice, green epaulettes, and tiny rosette trim are all attached to the voluminous purple netting, all worn over gold trousers. Silk tassles from WomanShopsWorld — one of Etsy’s most delicious shops — and a purple cylindrical hat, complete the costume.\nThis Christmas card, sent by Pavlova to friends in 1923, was created by artist Eveline Von Maydell (1890-1962), who studied art in St. Petersburg before emigrating to the USA in 1922. There were large groups of Russian émigrés in Paris, London, and New York, who were referred to as “White Russians.” They were minor aristocrats, artists, intellectuals, writers, and others who had fled, often for their lives, from the Bolshevik regime in the newly formed Soviet Union. Quite often they gave themselves gratuitous titles. Pavlova, an émigré herself, would have known “The Baroness” through her contacts in the émigré community in New York. The card is a print of a black paper silhouette of Pavlova with a small dog sitting on its hind legs. The dog is quite possibly Pavlova’s beloved Boston terrier, Poppy, of whom we have many photographs.\nIt is hand-inscribed and signed in ink with the following caption: “With my warmest Christmas and New Year greeting, Anna Pavlova, New York, 1923.” There were perhaps hundreds of these cards sent by Pavlova, but this is one of the very few that has survived.\nChristmas Card, Re-Imagined\nI was very excited to find this image online, and I knew I wanted to re-create it for The Pavlova Project. The tutu is made with two different gauges of netting: a fine one for the first three layers and a larger gauge for the top layer. The little dog is needle felted. His head is too big (I’m still perfecting my needle felting skills) and his silver ruff matches the tiara on Pavlova’s head.', ""How to Make a Wax Figure\nEach life-size, life-like wax figure is made by our Hollywood Wax Museum team of seven different artists, and each one takes at least three months to create. Click the photos to see our step-by-step wax figure creative process.\nStep 1: Creating a Clay Sculpture\nArmed with photographs, actual measurements, and scanned images, the sculptor transforms a common lump of clay into a perfect likeness that captures the look and feel of life. From facial expressions right down to every mole and wrinkle, the clay head and face are created in perfect proportion.\nStep 2: Making a Mold\nOur fabricator forms an algenate mold from the clay sculpture. Then, he carefully pours a molten mixture of natural and petroleum-based wax inside. The wax is swirled, layer upon layer, until the mold is evenly coated with a two-inch thickness of wax. The mixture is left to thoroughly harden before the mold encasing the head is opened to reveal a rough waxen image.\nStep 3: Whittling the Wax\nUsing specially-designed instruments fashioned after surgical tools, the wax artist removes excess wax from the nostrils, ears and mouth. He then makes intricate refinements by delicately etching the wax for days. First, an authentic facial expression is created with the appropriate wrinkles, fine lines and unique markings in each forehead and face. The masterpiece is completed by carving the detailed features of the eyes, ears, nose and lips.\nStep 4: Bringing the Wax to Life\nFirst, the craftsman mixes colored wax and oil paint to create an exact skin tone match. He gently applies the color like make-up, adding details such as freckles, birthmarks and age marks. Next, hair is inserted with a special needle, one strand at a time, including eyebrows, lashes and facial hair. Finally, medical glass eyes and porcelain teeth are added to duplicate the person's actual characteristics.\nStep 5: Handling Hands & Other Parts\nThe wax artist creates hands using the same sculpting, molding, carving and painting techniques as crafting the head. But, there is an added challenge of matching the person's stance and hand gestures. Right down to each vein, knuckle and fingernail, our team matches the person's skin tone, paints the distinctive markings and inserts hairs with a tiny needle.\nStep 6: Crafting the Costume\nThe tailor places linen over the figure's body and creates a pattern for each piece of clothing. With the goal of complete authenticity in mind, the costumer locates fabrics and decorative materials (metal studs, buckles, beads, and sequins) to hand-make each item, including coats, dresses, boots, headdresses, belts, capes, chaps, bathing suits, and more. Cleverly-concealed Velcro closings are sewn in to help the costumer dress the figure.\nStep 7: Prepping the Props\nMatching the look and placement of every accessory is top priority for the prop master. Prop houses, vintage stores, auctions, and the studios, are resources for items such as military medals, crests, distinctive buttons, and logos. Often our prop master borrows an original sword, pair of sunglasses or other prop from the studio. He makes a mold and finishes the piece exactly as it appeared on screen.""]"	['<urn:uuid:4ff38041-9a7d-447e-870a-001e9905bf40>', '<urn:uuid:f8d17fc0-ecef-4059-9bb1-2d2bec8ef822>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:18:19.158519	29	93	2834
16	why china us space research partnership	The collaboration between China and the United States in space research serves multiple strategic purposes. In astronomy, partnerships like the Fermi LAT and FAST collaboration allow sharing of expensive facilities and enhance scientific discoveries. In quantum communications, collaboration is beneficial because China has demonstrated leadership in several quantum key distribution (QKD) technology areas, including space-based QKD using entangled photons and large terrestrial quantum networks, from which the US could learn engineering insights. Additionally, since both nations are increasingly dependent on space for data collection and movement, their shared interests in satellite communication systems could provide a foundation for future agreements on space activities that are mutually beneficial, such as limitations on disruptions of satellite communication systems.	['(Originally published by Stanford University)\nJune 1, 2018\nDuring the early morning hours of Feb. 17, 2018, Chinese scientists emailed data showing evidence of a rapidly spinning pulsar detected with China’s Five-hundred-meter Aperture Spherical Telescope (FAST) to the Fermi Gamma-ray Space Telescope–Large Area Telescope (LAT) team.\n“One of our collaborators in Germany, who was up at the time, used the FAST data to search in 10 years of Fermi data – and boom! There was the pulsar,” said Stanford physicist Peter Michelson, a member of the Kavli Institute for Particle Astrophysics and Cosmology (KIPAC).\nFAST had detected a faint pulsar with a spin period of just 5.19 milliseconds, and estimated to be 4,000 light-years away – likely the faintest millisecond pulsar ever detected. The discovery was the first of its kind from the collaboration between Fermi LAT and FAST, a partnership spearheaded by Michelson.\nSearching the sky\nWith Michelson as the principal investigator, the Fermi LAT team, an international collaboration, has discovered hundreds of pulsars since its launch 10 years ago this June. Pulsars are rapidly spinning neutron stars that release beams of electromagnetic waves as they rotate. Similar to the rotating beam of light from a lighthouse, the pulses of energy from pulsars occur at regular intervals ranging from milliseconds to seconds. Large radio telescopes detect pulses in the radio wave range of the electromagnetic spectrum while the Fermi LAT detects pulses in the gamma-ray range.\nThe partnership between Fermi LAT and China’s FAST significantly improves the ability of scientists to detect the faintest pulsars, called millisecond pulsars. The Fermi LAT can detect gamma-rays from suspected pulsars, but can’t determine the rotation period of a rapidly spinning pulsar. That’s where radio telescopes such as FAST come in. When directed to search for radio pulses from the regions of the sky where Fermi detected gamma-rays, FAST can determine the rotation period.\nBut that’s only if the radio telescope is sensitive enough to detect the radio pulses. FAST’s enormous 500-meter diameter dish makes it the most sensitive radio telescope on the planet for this purpose, which means FAST can detect pulsars that other radio telescopes overlook, such as the extremely faint millisecond pulsar detected in February.\nA universal effort\nThe Fermi LAT collaboration has been international from the start, involving hundreds of scientists from institutions in the United States, Japan, France, Italy and Sweden. Since its launch, scientists from China, Germany, Spain, South Africa and Thailand have joined the team.\nIn the spring of 2017, Michelson, who is also the Luke Blossom Professor in the School of Humanities and Sciences, spoke with Chinese physicist Xian Hou about initiating a collaboration with FAST. Hou is a collaborator on the Fermi LAT team and also a scientist at the Chinese Academy of Science’s Yunnan Observatory.\nTo kick off the collaboration, Fermi LAT scientists gave the Chinese team a list of locations in the sky where they had detected possible pulsars. The FAST team looked at a source that had previously been examined by Arecibo, a radio telescope in Puerto Rico operated by the University of Central Florida, but that failed to detect radio pulsations from the source. FAST’s more sensitive equipment succeeded, revealing one of the faintest pulsars detected to date.\nThe discovery demonstrated the capability of FAST to detect pulsars that are too faint to be detected by less-sensitive radio telescopes like Arecibo. “That was pretty exciting,” said Michelson.\nFrom a scientific standpoint, the finding is significant because it suggests future discoveries of many more pulsars, which together, Michelson explained, may help detect low-frequency gravitational waves traveling through the galaxy that can modulate the arrival times of pulsations from these sources.\nValuable global partnerships\nMichelson is proud of the team’s discovery but is most proud of the collaborative effort. “It’s not just the science. The part I think is important to me is that it’s truly an international collaboration,” he said. One of the reasons he thinks collaborations are so important: “Particularly with countries we sometimes have strained relations with, it’s important to work on things where you share a common purpose and there is a benefit to all involved. That’s important in the long run.”\nMichelson also sees cost benefits from international collaborations, especially in the field of astronomy, due to the expensive facilities required for experiments. “No one nation can afford to invest in all the experiments,” he said. “In astrophysics in particular, state-of-the-art facilities cost a lot. It’s important for scientists around the globe to share access to data from these facilities that will enable important science. Everyone can benefit from this.”\nAs a mentor to graduate students in Stanford’s Department of Physics, Michelson strives to teach his students the importance of international collaborations through working with Fermi. “It’s what science does beyond just doing science,” he said. “It connects cultures.”\n—Written by Kimberly Hickok', 'In 2019, a team of Chinese technicians, engineers, and scientists sent pairs of photons from a single satellite called Micius to two ground stations in China separated by over 1,120 kilometers. The photons were prepared in such a way that they carried information that remained perfectly correlated in spite of the distance between them. In addition, the two receiving stations in China were able to ensure that the two receivers could not be disrupted or deceived by any third party. The experiment demonstrated the ability to share secret cryptographic keys between the two locations in China, with no known means for a third party to covertly observe or copy them. Although the rate of the key exchange was too low for practical use, the achievement represented a step toward secret communications guaranteed by the laws of physics.\nSeveral countries have spent decades trying to find ways of moving data that are both cost-effective and secure by investing in quantum communication technology. The surge in China’s work in the field dates to 2013, when the release of classified information by Edward Snowden detailing U.S. intelligence capabilities caused deep concern in Beijing. “This incident has been so fundamental to Chinese motivations that Snowden has been characterized as one of two individuals with a primary role in the scientific ‘drama’ of China’s quantum advances, along with Pan Jianwei, the father of Chinese quantum information science,” the researchers Elsa Kania and John Costello concluded in a 2017 report.\nThe national-security implications of China’s interest in space-based quantum communications cuts several ways. The development of impenetrably secure communications links in China would be a loss for American intelligence organizations. On the other hand, China’s intensive efforts in using space for secure quantum-based communications may lead that nation to consider international agreements governing space activities as in their national interest. This strategic interest might be leveraged as part of a future U.S.-China agreement in managing competition in space. There are ample opportunities for collaboration in this field among the United States, Europe, Canada, Japan, Australia, and other democratic allies. China’s leading position in quantum data security suggests that U.S.-China collaboration—at least on basic science—would be a net benefit for the United States in understanding the state of the art.\nQuantum computing and secure communications\nData transmitted between two parties over the internet is subject to unwanted interception. The value of the internet depends on the fact that data sent between the sender and receiver can be securely encrypted. Encryption methods are based on exchanging secret keys used to encode data in a way that reveals minimal information to someone lacking the key. Keys can be as simple as a long random string of 0’s and 1’s. Common methods for sharing secret keys over the insecure internet are based on numerical computations that are easy to make in one direction but very difficult to make in the reverse direction—on our current computers. For example, multiplication of two prime numbers is easy, but given a very large integer, determining which primes were multiplied together to yield that integer is a difficult problem, and it gets rapidly more difficult the larger the integer. This is true even on today’s most powerful conventional computers.\nFundamentally new types of computing architectures based on interactions of quantum systems were first proposed in the 1980s. Within a few years, Peter Shor and other theorists proved that quantum computer algorithms, running on sufficiently large quantum computers, could in principle solve the extremely time-consuming numerical problems like factoring large integers much more quickly than a classical computer algorithm could.\nIn short, working quantum computing systems threaten to make useless current methods of encryption that provide the basis of internet commerce and digital communication. Though such systems are generally thought to be unachievable before 2030, when such quantum computers are available, decrypting some communications streams may become feasible if nothing is done to protect those streams. What’s more, any encrypted data that has been intercepted and stored will be vulnerable to decryption. That means any country that attains a quantum computing system of sufficient power in the future will be able to decrypt stored data from the current era that would otherwise remain impossible to decode. And the data at risk goes beyond national-security information to include genomic, medical, and financial data.\nThese concerns have spurred efforts in the United States to develop new encryption algorithms that are more resistant to known quantum computing-based decryption methods. These post-quantum cryptography (PQC) methods are being designed and evaluated to be run on current classical computers. The National Institute for Standards and Technology (NIST) is leading an evaluation of PQC alternatives and has recently published its latest list of top contenders.\nWhat is perfect security?\nIn discussions of cryptography, the sender and receiver of a message are usually referred to as Alice and Bob, respectively. They are assumed to have a public channel and an encrypted channel over which to send data. Proofs of security assume that an adversarial eavesdropper, called Eve, has access to both channels, as well as powerful computers at her disposal.\nIf Alice wants to transmit a message to Bob at some future time over a network on which Eve lurks, Bob can meet at Alice’s totally secure office. There they create two identical copies of a long string of completely random binary digits, called a key, and securely package one copy of the random key so that Bob can take it to his secure office. When Alice wants to send a message to Bob over the compromised channel, Alice converts her message to a string of bits using an agreed upon encoding scheme that does not need to be secret. She then chooses the first segment of the random key the same length as the binary form of her message, aligns the key bits with the message bits so that they are paired, and computes a bitwise exclusive “or” operation (XOR). A bitwise XOR operation of the two bits is a simple function that outputs a 0 if both inputs are 0, a 1 if one bit is 0 and the other is 1, and the “exclusive” part means that the output is 0 if both inputs are 1. The result is an encrypted string of bits that is equally likely to be any message. Alice sends this string of bits to Bob. Bob XORs the encrypted message with his copy of the key, and then he can convert the result back to text using the public encoding. Then they both discard the random key.\nThe procedure above provides no new information to Eve, even if she captures the entire string of encrypted bits. However, the requirements of this ideal scheme are immense: The random string of bits in the key must be truly random, a new key must be generated for each message since it is discarded after one use, the key must be as long as the message, and the key must be shared in perfect secrecy. This makes data exchange very inefficient. The requirement for perfect randomness cannot be met using readily-available computer random number generators: Only physical systems such as radioactive decay, or other quantum systems can generate truly random numbers. Encryption methods in use today make compromises to the idealized algorithm above in order to trade perfect security for efficiency.\nQuantum key distribution may offer secrecy with fewer such compromises. Quantum key distribution methods transmit random keys by encoding these strings of 0’s and 1’s into sequences of photons whose quantum states obey the rules of quantum mechanics. For single photons, these rules allow for photons to exist in a combination of two quantum states until they are detected by a device that can measure the states. Once detected by a particular kind of device, the photon will take on a definite state that is in part determined by the device itself. This close relationship between the photon and the measurement device is at the heart of QKD methods. Other QKD methods use pairs of photons which are generated to have perfect correlation between their states, regardless of their separate travel paths. Common to all QKD methods is the fact that an eavesdropper that detects the photons will either gain no information about the keys, or will signal to Alice and Bob that they have successfully intercepted the data. This allows Alice and Bob to make adjustments in order to complete the key exchange. In any case, the eavesdropper can never copy the quantum information. In classical information exchange over the internet, an eavesdropper can detect, copy, retransmit the 0’s and 1’s without changing how this information is later observed by Alice and Bob, thereby remaining invisible.\nThe ability to replicate classical digital data without error is a key enabler of the current internet, as it allows the same information to travel to multiple places for use. Since the quantum states of photons cannot be copied, this creates special challenges for quantum networking. However, by compromising on the perfect security of quantum information exchange at a few, well-trusted sites, quantum networks have been built.\nThe guaranteed secrecy of QKD systems threatens to make it impossible to spy on communication channels use by adversary countries. Whether these are channels that are already tapped, or ones that would be useful to tap in the future, improvements in communication security can potentially cut off information that might be useful in statecraft or to gain advantage in a military crisis.\nThis gives rise to two important reasons for pursuing QKD research. First, by understanding the weaknesses of QKD devices, one can guard against attacks on the integrity or reliability of one’s own QKD system. Second, if one can deny an adversary the secure use of QKD, it may drive that adversary to use less secure communications means, which may then be exploited. Thus, the development of QKD systems between geopolitical rivals will take on a measure-countermeasure character, in much the same way as military communications and sensing measures must overcome sophisticated electronic warfare countermeasures.\nNational efforts at QKD and QKD networks\nThe United States, Japan, Canada, Singapore, and Europe initially led the efforts in quantum key distribution. Initial research involved point-to-point QKD, but networks of quantum-secured information exchange is the real goal of these efforts. The first QKD network was established in Boston by DARPA in 2003 and, by 2004, ran between Harvard University, Boston University, and the offices of the research firm Bolt, Beranek, and Newman. Between 2008 and 2009, the European FP6 project integrated several QKD systems into one QKD backbone in Vienna. In 2010, researchers in Tokyo demonstrated a QKD network with encryption for video.\nToday, China has taken the lead in quantum key distribution: The largest demonstrated network is one that began operating in 2017 in China within the cities of Beijing, Jinan, Hefei, and Shanghai with a 1,200 mile quantum backbone network connecting them. In the United States, meanwhile, the U.S. firm Battelle, together with the Swiss company ID Quantique, is constructing a 400 mile link between Columbus, Ohio, and Washington, D.C..\nQKD research and development continues today, as part of broader developments in quantum technologies in Canada, the European Union, South Korea, Japan, the United Kingdom, the United States, Russia, China and other countries. Over the past 20 years, emphasis within the overall field of quantum technology has shifted, with the United States and other Western countries tending to focus on quantum computing and China putting greater emphasis on QKD. Though there are efforts underway in China to build advanced quantum computers, this difference of emphasis reflects the deep concern about internet security at the highest levels of Chinese leadership, while in the United States, quantum computing advances have been driven by large companies. As China, the United States, and other countries build a larger workforce with the experience in designing and building quantum information systems, it may be that all countries converge to a more broad-based quantum information technology base.\nCompanies based in China dominate applications for patents in quantum cryptography in the most recent period when data is available, from 2012 to 2016. Companies based in the United States and Japan dominated quantum cryptography patent applications between 2002 and 2010, but have since slowed considerably. In the field of quantum computing on the other hand, the United States, Japan, and Canada have applied for the great majority of patents from the period 2001 to 2016 and far more than China.\nAs commercial QKD component offerings grow, benefits will likely accrue to companies that can innovate while meeting or establishing industry standards. South Korea Telecom and ID Quantique have worked through the International Telecommunications Union to establish standards for quantum communications tools. The competition to help set standards is perhaps as consequential in this field as is any particular technological development. Companies that can adapt to voluntary international standards for technical devices and data can establish a market advantage as other companies and countries around the world begin to integrate that technology into their own infrastructure. Industries cooperate in setting standards by consensus, and to the extent that Chinese companies can bring real expertise and experience in quantum technology to international standards organizations, they will have a better chance that their technical approaches will be integrated into the standards and that they will be competitive in the long run.\nFinally, it is always useful to keep in mind throughout technical discussions of data security, that the weakest points in technological systems is often humans. Greed, fear, carelessness, lack of training or darker motives can open the most technically secure systems to risks. The methods of social engineering—manipulating the perceptions and behaviors of human users—that are core to cyberattack methods transfer directly to future quantum secure communications. Proper training and monitoring for insider threats will remain a key element of information security, regardless of any particular technology implementation.\nThe global quantum race\nThe need for varying levels of data security, up to and including near-perfect security, is driving countries around the world to invest in improvements in encryption based on both mathematics and on quantum physics. With quantum computers now in active production around the world, the risk to current internet encryption may arise a decade from now. This also raises immediate concerns about the long-term security of sensitive data that is already being intercepted. One possible future for information security is one that involves a hybrid of post-quantum cryptography based on mathematics and QKD based on physics, with the former providing security for authentication required over classical data channels.\nAs the United States increases its reliance on the secure flow of data, QKD will probably play a significant role. Therefore, it is important for the United States to develop a mix of technical infrastructures, such as satellites and fiber links. It also critical to understand the vulnerabilities of those QKD links. In cryptography, open analysis of methods has always led to better security, and the same is likely to be true for QKD. The United States would be best served through collaboration on quantum information among the national governments, business, and academic groups within the United States and between the US, Europe, South Korea, Australia, Japan, and other countries. This will minimize strategic surprise by maximizing the breadth and depth of U.S. understanding of quantum information science and engineering.\nChina has a demonstrated lead in demonstrations of several specific QKD technology areas, including space-based quantum key distribution using entangled photons launched from space. Since this method has some distinct advantages for very long-range secure information, China could become increasingly dependent on space-based QKD for securing data over long distances. This could provide the basis for a common interest in preserving the stability of satellite-based communications between the United States, China and other countries that are increasingly dependent on space.\nThe United States should also continue to engage in technical exchange and collaboration with China in the area of quantum information science for several reasons. First, China has a demonstrated lead in several QKD technology areas, including space-based QKD using entangled photons and large terrestrial quantum networks. The United States is likely to learn something about the engineering issues if not the physics. The second reason for collaboration has to do with the nature of QKD itself: It is provably secure. Unlike the codebreaking of WWII, which was so important to the Allies, certain QKD systems are impervious to eavesdropping. Understanding of the technology does not create a security vulnerability for either side. The final reason for collaboration is to maintain expert exchange between the two countries in the critical nexus of information technology, cybersecurity, and the uses of space. China could become increasingly dependent on space-based QKD for securing data over long distances. The United States has long been dependent on space for collecting and moving data. This convergence of needs could provide the basis for future agreements on activities in space that are mutually beneficial, such as limitations on disruptions of satellite communication systems.\nTom Stefanick is a visiting fellow at the Brookings Institution.']	['<urn:uuid:36c9eded-43f1-4b61-b85d-9880a52abe80>', '<urn:uuid:d0983495-8a6b-4fe3-8a25-e09a08c097fb>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	6	116	3645
17	What's more accurate: DNA match predictions or twin parentage?	Twin parentage determination through DNA is more accurate than general DNA match predictions. While Ancestry's predicted relationships are only estimates that can fall into multiple possible categories (for example, 575-1330 centimorgans could indicate first cousins, half-aunts/uncles, or great-grandparents), the DNA relationship between identical twins and their children is very precise - identical twins share exactly 100% of their DNA, and their children will share exactly 50% of their DNA with their siblings/cousins.	"[""In this post, I’ll help you understand the basics of the “predicted relationship” on Ancestry DNA. You will learn what the term means and how to use this information.\nWhether you are just starting out learning how to understand your DNA match list, or you have a specific DNA match in mind that you’d like to try to understand, you have come to the right place.\n- How Ancestry DNA determines the predicted relationship\n- Whether the predicted relationship is usually “right”\n- How you can use the predicted relationship and other information to figure out how you are really related to your match\nThe predicted relationship is a very important clue about how our DNA matches might be related to us. Understand how we are connected to our DNA matches is the best way to learn more about our ancestors from our DNA results.\nHow does Ancestry DNA determine the predicted relationship to a DNA match?\nIn order to determine exactly how you might be related to someone, Ancestry compares your DNA to the DNA of your potential relative to see how much genetic material you share. This shared DNA is measured in “centimorgans“.\nSimply put, centimorgans are used to measure genetic distance on a matching DNA segment (i.e. a “piece” of DNA that matches another piece of DNA that your that your relative has).\nWe generally share more numerous and longer DNA segments with closer relatives, and shorter and fewer DNA segments with more distant relatives.\nIt’s possible to share dozens of DNA segments with a DNA match, and it’s also possible to share only one DNA segment with a match. Generally speaking, the longer the longest shared segment, the more recent the common ancestor.\nWhile there is no hard and fast rule as to exactly how much DNA people with any given genealogical relationship (i.e. how you are related in your family tree) will share, there are clear patterns. There are generally accepted ranges of shared DNA for many of the most common relationships.\nWhat do I mean by a range of shared DNA? Let’s examine first cousins. Full first cousins, where both people share two grandparents, 575-1330 centmorgans (cMs) of DNA.\nSimple enough, right? Not so fast!\nPeople who shared between 575-1330 cMs of DNA could also be half-aunts or half-uncles, or even a great-grandparent or great-grandchild.\nAre predicted relationships on Ancestry usually “right”?\nAs you might have guessed by now, there is really no right or wrong when it comes to the relationship prediction. Since the relationship prediction is based on the amount of shared DNA, and except for the very closest relationships (such as a parent/child connection), there are at least a few different relationships that can fall within any given range.\nSometimes, there are several relationships – or even many, many potential relationship types that could fall within the range for any given relationship category.\nIn other words, the predicted relationship is only an estimate. It can’t really be wrong or right.\nWhen you are looking at a particular category of relationship and you notice that Ancestry has predicted that you might be related to a cousin in a particular way, it is important to understand that it is only an educated prediction. It is not necessarily evidence that you are actually related in that way.\nSometimes, a person who is predicted to be your first cousin will actually be a first cousin. Other times, they might be slightly more closely related, or slightly more distantly related.\nSo, what does “predicted relationship” on my DNA match list really mean?\nThe predicted relationship on your Ancestry DNA match list does serve a purpose. Even if you know that the relationship that is predicted isn’t 100% fact, it is helpful to have a general idea about how you may be related to someone.\nEven if sometimes it is a very general idea.\nFor example, if you have a DNA match who is predicted to be related to you at a 5th-8th cousin relationship distance, you can feel confident that this person is not your first or second cousin.\nConversely, if you have a person who is predicted to be related to you at a 1st-2nd cousin level, then you can feel confident that they are not distantly related to you. These examples are extreme, but helpful.\nWe have to do the “hard” work to figure out the nuance in between relationship possibilities. Our families aren’t set up neatly with only first, second, and third cousin categories.\nSomeone could be a first cousin twice-removed, or a second cousin once-removed, or even a half-second cousin twice-removed. As well all know, families are complicated.\nHow to know how you really might be related to a match on Ancestry DNA\nIn order to know how you are really related to a person on your DNA match list, you’ll need to know how to access the exact number of shared centimorgans (cMs).\nOnce you have this information, you can use Blaine Bettinger’s chart available in my post “Beginner’s Guide to Shared Centimorgans” to figure out the relationship possibilities that exist for that amount of shared DNA.\nIf both you and your DNA match have posted your family trees, it should be easy to know where to look to figure out your relationship. If you don’t, it’s still possible – keep reading!\nAfter you’ve determined the relationship possibilities for a given DNA match, you can then use more traditional methods to work out your genealogical relationship.\nLearning the following information about your DNA can help you eliminate relationship possibilities and know exactly where to look to find more information about how you and your match are connected:\n- How old is your match (obviously, a person who is younger than you can’t be a grandparent or a great-grandparent!)\n- Where does your match live? Where were they born? Were any of your family members born there, too? Did anyone in your family tree live in the same town?\n- For very close matches, comparing your “ethnicity” estimate with that of your match could possibly help you figure out which regions you share in common, and potentially which ancestor – this is not a foolproof method, however, but it can give you something to work with\n- Have you contacted your match to see what they know about their family? See tips on contacting DNA matches\nI hope that this post has helped you the basics about how Ancestry predicts how your DNA matches may be related to you, what exactly predicted relationship means, and how you can really find out how you are related to your DNA match.\nIf you have any questions about something that you read in this post, I’d love to hear from you in the discussion below.\nThanks for stopping by!\nMonday 12th of December 2022\nA recent DNA match showed a Fran as a 1st or 2nd cousin. We share 1029 cM and 15% shared DNA. By our reckoning, Fran is the daughter of our great uncle (my mother's uncle). However, I have two other 1st cousins (verified because I know them) who are children of my father's sisters and brothers. Both of these 1st cousins share fewer cMs (973 and 878) and a lower percentage of shared DNA (14% and 13%). How could I have more of a connection with the child of a great uncle (my mother's uncle) than to children of my father's siblings?\nThursday 8th of October 2020\nis it possible to share 3400 cMs over 38 segments with someone and not be related? I don't know the person but my DNA identified match insists that I am not his father.\nFriday 9th of October 2020\nHi Douglas, 3400 cMs is a very high number of centimorgans to share with a match. This is about 50% of our DNA, so we would expect a DNA match sharing at this level to be either a parent or child (since we get 50% of our DNA from our parents and pass 50% of our DNA down to our children). If the rest of your DNA matches are what you might expect (i.e. you have 1st, 2nd, and 3rd cousins descended from known ancestors), then you can exclude the possibility of a mistake in testing, which is extremely rare but possible. If you feel like your results are correct, you might be interested in reading this post: https://whoareyoumadeof.com/blog/what-to-do-if-you-get-a-surprise-dna-match/ If you tested on Ancestry DNA, you might also refer to this page: https://support.ancestry.com/s/article/Unexpected-DNA-Matches"", 'Why the children of the identical twin couples will be SIBLINGS\nWhy the children of the identical twin women that married identical twin men will be genetic SIBLINGS\n- Identical twin sisters Brittany and Briana Deane married identical twin brothers Josh and Jeremy Salyers earlier this month in Twinsburg, Ohio\n- Identical twins share 100 percent of the DNA\n- This means that all of the two couples’ children will be genetic siblings, sharing 50 percent of their genes with their brothers, sisters and first cousins\n- A geneticist explains the science behind the mind-bending Salyer genetic family tree\nIf the identical twin sisters who married identical twin brothers last weekend decide to start families, the couples’ children will all be genetic siblings.\nSiblings share about 50 percent of their DNA, except for twins, whose DNA sequences are a perfect 100 percent match.\nTypically, first cousins have just an average of 12.5 percent of their genes in common.\nBut not if you’re the future children of newlyweds Josh and Jeremy Salyers and Brittany and Briana Deane (now, all Salyers).\nA geneticist told Daily Mail Online about the bizarre twist of genetics – and, perhaps fate – that makes these two families more like one.\nTwin brothers Jeremy (far left) and Josh Salyers (second from right) married twin sisters Briana (second from left) and Brittany (far right) Deane. The couples’ children will be genetic siblings\nThe 39-year-old mother with a HEART-SHAPED WOMB: Russian…\nMedical miracle as mother with TWO wombs becomes pregnant in…\nShare this article\nTwins make everything more confusing for most of us, but they make everything much clearer for scientists.\nGeneticists in particular love identical twins, because their perfectly matched DNA offer a rare opportunity to rule out which traits are behavioral.\nThe rate of all twin births – both fraternal and identical – has risen dramatically since the 1980s, due mostly to the fact that women are having children later in life and getting IVF treatments.\nIn 1980, 19 out of every 1,000 births was a pair of twins. By 2016, every 33 out of 1,000 births resulted in twins.\nIdentical twins, however, are much more rare, and have pretty much stayed that way. Twins account for about three in every 1,000 births.\nIdentical twins like the Salyer brothers and the Deane sisters share 100 percent of their genes\nThe two twin couples were married this month at a twin festival in Twinsburg, Ohio. A twin lifestyle is important to them and they plan to raise all of their children together in one house\nAnd since the Salyers quaternary (yes, there is a term for a pair of identical twins married to a pair of identical twins) is made up of four people, but only two sets of DNA, there number of ways their DNA can combine to form their kids’ genes.\nHere’s how it works:\nHumans have around 30,000 genes. We have two copies of each of those genes in every chromosome, but the two corresponding copies vary slightly from one another.\nEach gene copy is the same same size and shape and sits in the same position on the chromosome. The two pairs will contain the same information, but in a different order.\nEnvironmental differences will result in some differences in each couples’ kids, but they plan to raise their families in the same house\nOur chromosomes come in 23 pairs with matching genes.\nJust about every cell in the body contains a full set of paired chromosomes.\nThat is, except for sex cells: Sperm and egg cells contain just one copy of every chromosome and that copy is essentially a mash up of the pair.\nThis way, when a sperm fertilizes an egg they make an embryo that has the correct number of genes and chromosomes.\nThe embryo will have one copy of each gene from the mother and one from the father, combined into a new set of 23 chromosomes.\n‘But identical twins are very special,’ explains Dr Alexandra Burt, who is co-director of the Michigan State University Twin Registry.\n‘Within the first two weeks of a pregnancy, a single [fertilized egg] divides, and they’ll both implant and become two people.’\nSo, in the case of the Deanes and the Salyers, ‘genetically, the two women are indistinguishable from one another, as are the two men,’ Dr Burt says.\n‘That means that each child will share as many genes with their aunt as they will with their mother. The children are cousins “in real life” but, in terms of their DNA, they are siblings.’\nTo be clear, this is not the same thing as genetic incest. Inbreeding birth defects happen because the mother and father share too much genetic material.\nThe male halves of each couple don’t share DNA with the female halves, so each pair can safely reproduce.\nThough they will be legal cousins, children of the separate couples will share roughly 50 percent of their DNA, like siblings.\nAnd Dr Burt says that there is no reason to believe that there are any genetic health risks to being the child of two identical twins.\nIn fact, even the respective children of one set of twins – whose other parents are not twins – will still be genetic half siblings, sharing as many genes as if they had one parent in common.\nGenes only go so far to shape who people become, however.\nNo matter how much DNA they share, children who grow up with the same set of parents will be shaped differently by that experience from their cousins growing up with a different – if genetically identical – set of parents.\nAlthough, that is complicated by the fact that the two Salyers couples intend to live in and raise their children in the same house.\nEven Dr Burt can’t predict what will come of that.\n‘That’s hardcore,’ she says.\nWhile there have plenty of studies on the offspring of identical twins, there aren’t enough children of two pairs of identical twins to predict much of what the two sets of Salyer kids might be like.\nSuch children are even rarer than their already rare parents, and how their lives pan out will be of great interest to geneticists like Dr Burt.\n‘Identical twins are really magical,’ she says.\n‘They’re clones of each other, created by nature, and you’ve got a case of two of these magical sets of human beings meeting and falling in love.\n‘If it didn’t happen in real life, you wouldn’t believe it could happen and you couldn’t write a better story than that.’\nSource: Read Full Article']"	['<urn:uuid:5d0b79a8-d38c-448d-a16b-4eb42c965d59>', '<urn:uuid:68f5857c-fabe-4a21-b195-ea5e3b44df4e>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T05:18:19.158519	9	72	2517
18	what lighting equipments needed decoration fish tank underwater plants	LED lights that replicate natural sunlight are recommended as they provide both visual appeal and the necessary light spectrum for plant growth. The intensity and duration of lighting needs to be carefully managed as it affects the overall health of fish and plants.	['Aquascaping is an art form that allows aquarists to create breathtaking underwater landscapes. In this article, we explore the concept of aesthetic aquascapes, where design principles of balance and beauty are applied. From selecting the right plants to arranging hardscape elements, we delve into the intricacies of creating visually stunning aquariums. Discover how to transform your tank into a mesmerizing aquatic masterpiece.\nTable Of Content\n- 1 Creating Stunning Aquascapes: Striking the Perfect Balance\n- 2 CLASSIC PLANTED AQUARIUM | Step By Step | Aquascaping\n- 3 Understanding Aesthetic Aquascapes: A Balance of Design and Beauty\n- 4 The Role of Hardscape in Aesthetic Aquascapes\n- 5 Selecting Suitable Flora for Aesthetic Aquascapes\n- 6 The Importance of Lighting in Aesthetic Aquascapes\n- 7 Creating Depth and Perspective in Aesthetic Aquascapes\n- 8 Aquascaping Techniques: The Art of Arrangement\n- 9 Maintenance and Care: Sustaining the Beauty\n- 10 Showcasing Your Aesthetic Aquascape\n- 11 FAQ\n- 11.1 How can I achieve a visually appealing aquascape while still providing a suitable habitat for my fish?\n- 11.2 What are some key design principles to consider when creating an aesthetic aquascape?\n- 11.3 Can you recommend any specific plants or decorations that will enhance the beauty and balance of my aquascape?\nCreating Stunning Aquascapes: Striking the Perfect Balance\nCreating Stunning Aquascapes: Striking the Perfect Balance\nDesigning a captivating aquascape requires striking the perfect balance between creativity and practicality. It is essential to curate a visually stunning underwater world that provides a thriving environment for fish and other aquatic organisms.\nFirst and foremost, consider the natural habitat of the fish species you intend to keep. Research their preferred water temperature, pH levels, and specific environmental needs. Incorporating appropriate plants, rocks, and driftwood in your aquascape can mimic their natural habitat and promote their well-being.\nNext, explore your creative side by experimenting with different layouts and themes. Use the rule of thirds to create visually pleasing compositions. Consider the heights, textures, and colors of the plants and decorations you choose. By using contrasting elements and focal points, you can create depth and add visual interest to your aquascape.\nLighting plays a crucial role in enhancing the beauty of your aquatic masterpiece. Opt for LED lights that replicate natural sunlight, as they provide both visual appeal and the necessary light spectrum for plant growth. Be mindful of the intensity and duration of lighting, as it can affect the overall health of your fish and plants.\nRegular maintenance is key to keeping your aquascape looking impeccable. Establish a routine for water changes, algae cleaning, and pruning plants to prevent overgrowth. This will help maintain the balance and ensure the longevity of your aquascape.\nRemember, creating a stunning aquascape is an ongoing process that requires patience, creativity, and constant attention. By striking the perfect balance between aesthetics and functionality, you can create a captivating underwater world that both you and your fish can enjoy.\nNote: The text above has been modified to include HTML tags around the important phrases.\nCLASSIC PLANTED AQUARIUM | Step By Step | Aquascaping\nUnderstanding Aesthetic Aquascapes: A Balance of Design and Beauty\nAesthetic aquascapes are more than just a visually appealing display of fish and plants in an aquarium. They are designed to create a harmonious balance between the elements of design and the natural beauty of aquatic life. In this section, we will explore the key aspects of designing for beauty and balance in aquascapes.\nThe Role of Hardscape in Aesthetic Aquascapes\nHardscape refers to the non-living elements in an aquascape, such as rocks, driftwood, or substrates. It serves as the structural foundation and creates the overall layout of the aquarium. By carefully selecting and arranging hardscape materials, aquarists can create visually appealing focal points and add depth to the aquascape.\nSelecting Suitable Flora for Aesthetic Aquascapes\nThe choice of plants is crucial in creating an aesthetically pleasing aquascape. Consider factors like growth rate, leaf structure, and coloration when selecting plants. Balancing contrasting plant shapes and textures can create a visually stunning display. Additionally, choosing plants that thrive in similar environmental conditions will ensure a healthy and vibrant aquascape.\nThe Importance of Lighting in Aesthetic Aquascapes\nLighting plays a vital role in enhancing the beauty of an aquascape. Proper lighting intensity and spectrum are essential for plant growth and coloration. By strategically placing lights and adjusting their intensity, aquarists can highlight specific areas and create dramatic effects. Additionally, using a timer to simulate natural day-night cycles will mimic the natural environment and promote the well-being of the aquarium inhabitants.\nCreating Depth and Perspective in Aesthetic Aquascapes\nDepth and perspective are crucial elements in designing an aesthetically pleasing aquascape. Using different sizes of plants, rocks, and positioning them at varying heights creates a sense of depth and scale in the aquarium. Creating a foreground, midground, and background allows for a visually appealing arrangement.\nAquascaping Techniques: The Art of Arrangement\nAquascaping techniques involve careful arrangement and positioning of elements within the aquarium. Techniques such as the rule of thirds, the golden ratio, and the focal point method can help create visually balanced and harmonious designs. By applying these techniques, aquarists can achieve a visually stunning aquascape composition.\nMaintenance and Care: Sustaining the Beauty\nKeeping an aesthetically pleasing aquascape requires ongoing maintenance and care. Regular pruning, fertilization, and water changes are essential to prevent overgrowth, nutrient deficiencies, and maintain water quality. Consistent care and attention will ensure the long-term beauty and health of the aquascape.\nShowcasing Your Aesthetic Aquascape\nOnce you have created a beautiful aquascape, it’s time to showcase it. Regularly photographing and sharing your aquascape on social media platforms or participating in aquascaping contests can inspire and connect you with the wider aquascaping community. Sharing your expertise and experiences can also promote growth and learning within the community.\nHow can I achieve a visually appealing aquascape while still providing a suitable habitat for my fish?\nTo achieve a visually appealing aquascape while still providing a suitable habitat for your fish, consider the following factors:\n1. Research: Before designing your aquascape, research the specific needs and preferences of the fish species you plan to keep. This will help you understand their natural habitat and behavior, enabling you to create a suitable environment.\n2. Aquarium size: Ensure that the aquarium you choose is appropriate for the fish species you intend to keep. Provide sufficient swimming space and consider the adult size of the fish to prevent overcrowding.\n3. Proper filtration: Invest in a high-quality filtration system to maintain water quality and clarity. The filter should be able to handle the biological load of your fish and remove any excess waste products.\n4. Lighting: Choose appropriate lighting for your aquascape, considering the needs of both the plants and the fish. Different species of plants have specific lighting requirements, so research accordingly. Additionally, some fish may prefer dimmer or brighter lighting conditions.\n5. Substrate: Select a substrate that suits the needs of your plants and fish. Some fish prefer sandy bottoms, while others may prefer gravel or rocks. Ensure that your chosen substrate does not have any sharp edges that could harm the fish.\n6. Plant selection: Choose aquatic plants that are compatible with the lighting, water conditions, and the fish species in your aquarium. Plants provide hiding places, oxygenate the water, and can contribute to a visually appealing aquascape.\n7. Hardscape: Use rocks, driftwood, and other hardscape elements to create natural hiding spots and territories for your fish. These elements also add visual interest to your aquascape.\n8. Water parameters: Maintain the appropriate water parameters (temperature, pH, hardness) for the specific fish species you are keeping. Regular testing and adjustments may be necessary to ensure a healthy environment for your fish.\n9. Maintenance: Regular maintenance is crucial to keeping your aquascape visually appealing and the fish healthy. Perform routine water changes, clean the substrate, and trim plants as needed.\n10. Observation: Continually monitor your fish’s behavior and make adjustments to the aquascape if necessary. Some fish may need additional hiding spots or space, while others may thrive with specific features in the tank.\nRemember, creating a visually appealing aquascape is essential, but it should never come at the expense of your fish’s well-being. Prioritize their needs and provide them with a suitable habitat that mimics their natural environment.\nWhat are some key design principles to consider when creating an aesthetic aquascape?\nWhen creating an aesthetic aquascape, there are several key design principles to consider:\nBalance: Achieving a sense of balance is crucial in aquascaping. This can be achieved through the strategic placement of both hardscape (rocks, driftwood, etc.) and plants. Balancing out the overall visual weight on both sides of the tank creates a harmonious composition.\nProportion: The proportions between different elements in the aquarium, such as the size of the hardscape, the height of the plants, and the ratio of negative space, play a significant role in the overall appeal. Paying attention to these proportions helps create a visually pleasing aquascape.\nContrast: Utilizing contrasting colors, textures, and shapes can add visual interest to an aquascape. For example, pairing vibrant green plants with dark colored rocks can create a striking contrast.\nRepetition: Incorporating repetition of certain textures, patterns, or plant species can add unity and cohesiveness to the aquascape. Repeating certain elements in a deliberate manner creates a sense of flow and continuity.\nFocal Point: Establishing a focal point helps draw attention and creates a visual hierarchy within the aquascape. This can be achieved by placing a unique or eye-catching element, such as a particularly striking plant or a prominent piece of hardscape, in a strategic location.\nPerspective: Consider the perspective from which the aquascape will be viewed. Take into account the viewing angle, how the arrangement will appear from different sides, and ensure that the overall composition looks appealing from the intended viewpoint.\nDepth: Creating a sense of depth in the aquascape can make it more visually engaging. This can be achieved by arranging plants and hardscape in a way that creates layers and varying heights, giving the impression of a three-dimensional space.\nNegative Space: Negative space refers to the areas of empty space in the aquascape. Leaving some areas empty helps provide a sense of balance and allows the focal points and elements to stand out more effectively.\nLighting: Proper lighting is essential for showcasing the colors and textures of the aquascape. Consider the intensity, color temperature, and distribution of light to enhance the overall visual impact.\nHealthy Environment: Lastly, maintaining a healthy environment for the aquatic life is of utmost importance. Be mindful of the needs and requirements of the fish and plants you choose, ensuring they thrive in the conditions provided.\nBy considering these design principles, you can create an aesthetic aquascape that is visually appealing, balanced, and provides a healthy environment for the aquatic life.\nCan you recommend any specific plants or decorations that will enhance the beauty and balance of my aquascape?\nWhen it comes to enhancing the beauty and balance of your aquascape, there are several plants and decorations you can consider:\n1. Anubias: These are hardy plants that can thrive in a wide range of aquarium conditions. They have broad leaves and come in various sizes and shapes, making them great for adding visual interest to your aquascape.\n2. Java Fern: Another popular choice, Java Fern is known for its unique leaf shape and low maintenance requirements. It can be tied to rocks or driftwood, adding a natural and lush look to your aquarium.\n3. Dwarf Hairgrass: This carpeting plant creates a beautiful foreground in aquascapes. With its vibrant green color and fine texture, it provides a sense of depth and dimension to the overall layout.\n4. Dwarf Sagittaria: This plant is ideal for midground placement in your aquascape. Its grass-like appearance and fast growth rate make it an excellent option for adding depth to your design.\n5. Driftwood: Adding driftwood to your aquarium can create a striking focal point and mimic a natural habitat. It also provides surfaces for attaching mosses and other plants.\n6. Rock formations: Incorporating rocks into your aquascape can add a sense of structure and stability. Choose rocks that are suitable for aquarium use and arrange them creatively to create caves, ledges, or other interesting formations.\nRemember to research the specific needs of the plants you choose, such as lighting and nutrient requirements, to ensure they thrive in your aquarium. Additionally, always properly clean and prepare any decorations before adding them to your tank to avoid introducing harmful substances.\nIn conclusion, designing aesthetic aquascapes for your fish tanks is not just about creating a visually pleasing setup, but also about achieving a harmonious balance that promotes the well-being of your aquatic pets. By incorporating elements such as natural landscapes, careful plant selection, and strategic placement of decorations, you can create a stunning underwater world that not only enhances the beauty of your space, but also provides a healthy and stimulating environment for your fish. Remember to take into account key factors such as lighting, water parameters, and the specific needs of your fish species when designing your aquascape. With a thoughtful approach and attention to detail, you can achieve a spectacular, Instagram-worthy aquarium that not only showcases your passion for the hobby, but also ensures the happiness and health of your aquatic companions. Dive into the world of aquascaping and unlock the transformative power of combining beauty and balance in your fish tank!']	['<urn:uuid:baf76f51-c43c-4a7f-bf31-7ecc5b243eb5>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	9	43	2242
19	How often must weather losses occur to trigger widespread rating changes?	S&P would not expect widespread rating changes on the insurance or reinsurance industry unless weather-related industry losses exceed the amount expected from a 1 in 250 year return period loss.	['The insurance and reinsurance industry is coping with the increasing number and frequency of extreme weather events and has processes in place to monitor the impact of climate change on weather losses, says rating agency Standard & Poor’s.\nIn a report published yesterday S&P looks at how insurers and reinsurers respond to the increase in frequency and severity of extreme weather events and how climate change is factored into their risk management and underwriting decisions.\n“We believe the industry has been, and remains, well prepared to deal with weather events of the magnitude the world has been experiencing in the past two years. For that reason, the ratings impact of these natural catastrophes has been limited,” comments Standard & Poor’s credit analyst Miroslav Petkov, in the report.\nS&P says that insurers and reinsurers have been generally able to cope with the impacts of severe and extreme weather over the last couple of years due to their risk diversification strategies as well as effective underwriting, risk management and risk mitigation.\nHowever, events in recent years while severe, have not resulted in historic sized losses, says S&P, and remained well within insurers and reinsurers expected loss levels and within their excess capital limits.\nS&P says that it would not expect to level widespread rating changes on the insurance or reinsurance industry unless weather-related industry losses exceed the amount expected from a 1 in 250 year return period loss.\nS&P notes that climate change is not explicitly factored into pricing or modelling of risks yet by the insurance and reinsurance industry. However it believes that re/insurers have the processes in place to monitor the impact of climate change on weather events and then to adjust premiums based on any gradual increase in weather-related claims in the future.\nEven those insurers and reinsurers that have invested the most in understanding climate change and its threat to the industry do not yet explicitly allow for it in their pricing and modelling, said S&P.\nDespite the growing consensus among scientists that climate change will result in more impactful weather events and ultimately greater economic and insured losses, there remains uncertainty about the exact impact. As a result, S&P does not expect the insurance or reinsurance industry will directly allow for the impact of climate change until consensus emerges over the potential impacts.\n“Our view is that climate change is another factor contributing to the challenges of modeling extreme weather events,” Petkov said. “For that reason, we take a favorable view of re/insurers that consider how climate change, despite its uncertainties, may affect extreme events in capital modeling and exposure management.”\nS&P notes that any sudden spike in the frequency or severity of weather events could test the insurance and reinsurance industry. Hence the need for both insurers and reinsurers to make sure they are at least aware of the potential worst case scenario for their portfolios of risk should the changing climate push weather extremes to the maximum degree.\nIn the future risk models will likely begin to factor in views of the potential futures for our climate depending on different models or forecasts for change. At this time insurers and reinsurers will be able to take a range of views on the long-term impact to regions and portfolios of risk and may begin to factor in some pricing increases.\nSome insurers and reinsurers are already doing this in-house, with their own analytics and modelling teams. S&P says that it takes a favourable view of those that consider the additional challenge that climate change poses to risk modelling and their exposure management.\nS&P notes that the industry would like more certainty in how it should factor climate change into its models, pricing and underwriting processes. Without certainty it is very difficult for insurers and reinsurers to do anything other than react to weather loss trends and adjust pricing based on loss experience.\nWith the recent growth of multi-year covers in reinsurance, stimulated as a response to alternative reinsurance capital and insurance-linked securities such as catastrophe bonds where multi-year is the norm, the need to have a more accurate way to factor the changing climate into pricing is heightened.\nClimate variability has increased, whether change related or not, we now experience years of extremely high losses from weather events and well below normal losses and the extremes seem to be growing in severity on either end.\nIt’s impossible to say whether this is climate change in action or simply climate variability, however the need to be able to forecast and model the potential variation in losses from year to year as the climate adjusts is important to both insurers and reinsurers.\nIt’s also important to the ILS market, with investors asking questions of ILS managers about climate change, variability and how these factors could affect the expected loss of an investment or fund. These questions are going to become louder and more frequent as the ILS market and use of instruments such as cat bonds continue to grow.\nS&P says that it stands ready to adjust its rating methodologies if further climate change impacts and threats to insurance and reinsurance markets become apparent:\nWe consider that the re/insurance industry is well prepared to deal with possible gradual increases in extreme weather events, whether or not they’re linked to climate change. As such, we don’t expect climate change per se to have a ratings impact over the next three to five years, unless it causes a sudden increase in the number and magnitude of extreme events. Meanwhile, we will continue to follow developments in the field of climate change. If needed, we’ll revise our base case assumptions and capital stresses for weather-related losses and reflect them in our rating process.\nSubscribe for free and receive weekly Artemis email updates\nSign up for our regular free email newsletter and ensure you never miss any of the news from Artemis.']	['<urn:uuid:b04e6d6d-6cdf-4301-b982-1d37251ffc07>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:18:19.158519	11	30	980
20	how emotions risk affect currency trading	Emotions play a crucial role in forex trading risks. When traders become fearful because the market moves against them, professionals stick to their trading method and close trades to limit losses, while novices often stay in losing trades hoping for recovery. Additionally, volatile markets can cause anxiety and impulsive decisions - traders may sell off immediately when values begin to fall, cutting potential profits. The key is to remain calm, avoid emotional responses, and follow a consistent trading formula without letting emotions influence decision-making.	"['Foreign exchange trading was once something that people only did when they needed foreign currency to use when traveling in other countries.\nThis involved exchanging some of their home country\'s currency for another at a bank or foreign exchange broker, and they would receive their foreign currency at the current exchange rate offered by the bank or broker.\nThese days, when you hear someone refer to foreign exchange trading or forex, they are usually referring to a type of investment trading that has now become common. Many people wonder how foreign currency trading, often shortened to forex trading, works because they\'re interested in learning how to trade currencies for themselves.\nJust like with trading stocks, forex traders can speculate on the fluctuating values of currencies between two countries, and it\'s done for profit.\nThe Forex Market for Beginners\nIt seems like something that most people would find easy, except, in this particular industry, there is a high rate of failure among new traders because there is quite a steep learning curve.\nEven traders that are aware of that tend to start out with the attitude of ""It happened to them, but it won\'t happen to me."" In the end, an average of 77% of these traders walk away empty-handed, not quite sure what happened to them, or maybe even feeling a bit scammed.\nForex trading is not a scam; it\'s just an industry that is primarily set up for insiders that understand it. The goal for new traders should be to survive long enough to understand the inner working of foreign exchange trading and become one of those insiders, and this will come with studying the market, understanding the terminology, and learning trading strategies.\nForex and Leverage\nThe number one thing that hangs most traders out to dry is the ability to use a trading feature called forex trading leverage. Using leverage allows traders to trade in the market using more money than what they have in their accounts.\nFor example, if you were trading 2:1, you could have a $1,000 deposit in your brokerage account, and yet control and trade $2,000 of currency on the market. Many forex brokers offer as much as 50:1 leverage. This can be dangerous, as new traders tend to jump in and start trading with that 50:1 leverage immediately without being prepared for the consequences.\nTrading with leverage sounds like a really good time, and it\'s true that it can increase how easily you can make money, but the thing that is less talked about is it also increases your risk for losses.\nIf a trader with $1,000 in their account is trading a specific currency pair with leverage of 50:1, this means they would be trading $50,000 on the market, with each pip being worth around $5. If the average daily move of a currency pair\'s price is 70 to 100 pips, in a day your average loss could be between $350 and $500. If you made a really bad trade, you could lose your entire account in two days, and of course, that is assuming that conditions are normal.\nMost new traders, being optimistic, might say ""but I could also double my account in just a matter of days."" While that is indeed true, watching your account fluctuate that seriously is very difficult to do.\nMany traders assume that they will not be emotionally shaken by volatile price changes, however, the reality proves otherwise. When they experience the loss of money in real-time they may act reflexively out of an irrational desire to quickly gain back what they have lost. This leads to rash judgment in which traders may take riskier trades which inevitably accelerates the losses.\nThe Market and Your Emotions\nAssuming that you can manage not to fall into the leverage trap, the next big challenge is to get a handle on your emotions. The biggest thing that you\'ll tackle is your emotion when trading forex. The forex market can behave like a rollercoaster, and it takes a steel gut to cut your losses at the right time and not fall into the trap of holding trades too long. Forex trading should be a formula and a method that is enacted consistently and without emotion.\nWhen traders become fearful because they have money in a trade and the market is not moving their way, the professional sticks to her trading method and closes out her trade to limit her losses. The novice, on the other hand, stays in the trade, hoping the market will come back. This emotional response can cause novice traders to lose all of their money very quickly.\nThe availability of leverage will tempt you to use it, and if it works against you, your emotions will weigh on your decision-making, and you will probably lose money. The best way to avoid all of this is to develop a trading plan that you can stick to, with methods and strategies you\'ve tested and that result in profitable trades at least 50% of the time.\nThe Bottom Line\nThe forex market works very much like any other market that trades assets such as stocks, bonds or commodities. The way you choose to trade the forex market will determine whether or not you make a profit. You might feel when searching online that it seems other people can trade forex successfully and you can\'t. It\'s not true; it\'s just your self-perception that makes it seem that way.\nA lot of people trading foreign exchange are struggling, but their pride keeps them from admitting their problems, and you\'ll find them posting in online forums or on Facebook about how wonderful they are doing when they are struggling just like you.\nUnderstanding the forex market and winning at trading forex online is an achievable goal if you get educated and keep your head together while you\'re learning. Practice on a forex trading demo first, and start small when you start using real money. Always allow yourself to be wrong and learn how to move on from it when it happens. People fail at forex trading every day because they lack the ability to be honest with themselves. If you learn to do that, you\'ll have solved half of the equation for success in forex trading.\nFrequently Asked Questions (FAQs)\nHow do you start forex trading?\nStarting with forex trading is similar to starting with stock trading, and the main thing you need to start is a brokerage account. However, the brokerage account you use to trade stocks might not let you trade forex markets, so you may have to open a new account with a forex broker. Other than that, you just need the capital required to meet any opening deposit minimums.\nWhat is a pip in forex trading?\nIn forex trading, a ""percentage in point,"" or ""pip,"" is how traders refer to the movement of the currency pairing being traded. It\'s a small movement, and it may be the smallest measurable movement, although some brokerages may measure partial pip movements. Pip size varies, depending on the pairing being traded, so learning the pip size must be part of your research when trading a new product. Pips aren\'t used in stocks, because all stock price movements are measured in dollars and cents.', 'The Forex Trading Risks\nYou Must be Aware of\nWhen you’re trading currencies on the foreign exchange you should be continually working on your loss control and refining your risk management methods. Doing this will strengthen your investment portfolio and keep losses as low as possible as well as protect your savings or investment capital. A major component of risk management is making entirely sure that you don’t invest more than you can afford to lose. Transferring a large portion of your investment capital, or savings, into a currency you believe is a surefire win is reckless. Forex trading is unpredictable and risky at the best of times, so investing more than you can afford to lose is dangerous and can wipe out all of your earnings in just moments.\nIn the same scope as risk management is moderating your use of broker leverage. Basically, broker leverage is a loan that a broker provides to an investor to use as capital for their investments on the exchange, but as with any and all loans, you’ll need to pay it back, or earn it back. The issue with broker leverage is that if you over borrow and use the funding on a currency that drops in value, then you’re left in debt, often hard-to-pay crippling debt.\nGoing beyond purely financial risk management, there’s also the management of your own emotions during your time trading. This means not being impulsive when there are price drops or gains. If a currency falls in value don’t immediately sell off every single one you own, either partially sell 10% to 15% to reduce losses, or don’t sell anything at all. Typically the highest returns are in the long game, don’t sell at the drop of a hat.\nIn order to be a successful forex trader you need to focus a large portion of your energy and mind towards risk management, it’s essential. Not only to keep your investments stable but for your own wellbeing. The best way to plan this out is to ensure you have a plan in place before you begin trading and do as much research on the volatility of each currency as you can. Remember, a currency’s history doesn’t tell it’s future, but it can sure help you make informed decisions.\nAs stated above, margin risk or broker leverage risk needs to be in your scope at all times when you’re trading. There can only really be two main outcomes when trading, profit or loss. Invest more and the profits are higher, but so are the losses if the market goes down.\nIn forex trading, your trade or investment amount is leveraged when it is enhanced by borrowed funds, whether they be from your broker or anybody else. A leveraged trade is most often used when an investor believes that there is great profit potential in a currency, and they don’t necessarily have the money to invest, or they don’t want to use it, so they turn to their broken for leverage.\nTypically a broker will allow leverage sizes of up to 100:1, though new traders and even experienced ones shouldn’t take advantage of the entire amount in the event the current values plummet, leaving you with not only zero profit, but major debts to pay off.\nVolatile Political & Economic Policies\nOne of the more dramatic risks you will have to watch out for is political volatility and economic instability. As the forex exchange encompasses almost every country in the world you’ll have a lot of information thrown at you on a daily basis. If this information is negative it has the potential to make currencies unattractive investments and severely stunt your profit possibilities.\nTimes that are the most likely to have a major effect on the worth and investment attractiveness of a forex currency are typically elections, both planned and unplanned, as well as international relations. This means that before investing in any currency, you should always check with a nation’s elections and their potential candidates.\nA major key aspect you must look out for with government changes are the policies of the new leadership. Things like trade, taxation and more can greatly sway currency pricing for forex trading, both directly and indirectly. With that said, allied nations undergoing leadership change will more than likely see currency values sway in accordance with each other. Although these changes and events are almost impossible to predict, it would be wise for forex investors to do some due diligence research prior to large investments in a currency.\nA great idea is to either create or find a political and economic calendar with information on your chosen currency’s home country. This can include things like electron dates, previous swings in forex currency values and even whether there are expected rises and falls in your currency during the end of financial years or government surplus and expenditure reports. Having news alerts switched on via your smartphone will also give you a heads up to when issues arise, potentially giving you insight into a buying opportunity.\nGovernment Borrowing and Interest Rates\nOne of the biggest risks, when forex trading is concerned, is government borrowing and treasury interest rates. If a country’s government over borrows or overspends, their currency could potentially becoming completely worthless in a matter of days.\nOn top of this, a nation’s interest rates will be charged based on the borrower’s risk. Country’s that are seen as higher risk, will be changed far higher rates than those who aren’t. This means that if you’re planning on investing in a country’s currency you should take a look at current interest rates from central banks. These banks are often the ones whose actions determine whether a country is attractive for investors or not.\nThis is particularly important as countries rely on companies setting up shop in order to prop up their economies and improve tax flow. If interest rates are too low or too high this will affect the attractiveness of the nation and reduce this tax flow and have an impact on the currency value, something you want to watch for.\nYour best bet for monitoring a country’s financial status, and the risk factor, is monitoring major central banks and their monetary movements within a country. This means when you spot a central bank propping up interest rates higher than typical levels it can mean that the country is being seen as a little risky, and therefore will affect your currency values.\nThe most important thing to take note of when correlating central bank interest rates with forex trading is that the higher a country’s interest rate, the higher the exchange rates. Interest rates will almost fall in line with exchange rates and have a major impact on your investment if the sways are too high.\nAnother good tip to remember is that higher interest rates will make your forex investment grow organically. So, if a country’s interest rate is high, or higher than usual, your investment will grow at that rate, but you must make sure that you sell before the interest rate reverses.\nFalls in Exchange Rates\nThis type of forex risk is mainly limited to businesses who operate in multiple countries and pay their employees in different currencies, but also for corporate investments. Typically, the exchange rate risk will hit a business when the currency they have a large stock of becomes worthless in their base or ‘home’ currency.\nSimilar to what happened in Zimbabwe in the early 90s, this risk becomes exceedingly serious when a currency’s exchange rate falls against the local currency of a business, or investor. You may have invested $5,000 in a foreign currency, and even if the value in the foreign country climbed, the final exchange rate in your home currency could wipe out all profits, and even the original amount itself.\nIt’s best to stay alert and well aware of exchange rates during your entire time investing in a certain currency as the final exchange rate will ultimately determine whether the forex investment is profitable or not.\nVolatile markets are ones which have major swings in price, growing exceedingly fast then falling far and growing once more. This, however, doesn’t mean a currency is bad per se, just that traders need to develop a process with which they implement to reduce risks. Typically volatility is seen as bad, and something to stay away from, but there are often high profits to be made within a volatile market.\nTo make the most of a volatile market, do your best to stay calm and don’t be impulsive. Purchase at a low point and monitor value swings and don’t sell all at one price point. Staggering your sell-offs in these markets is your best bet at minimising risk and reducing your total losses or increasing your total profits.\nAs volatile markets are typically a result of large media stories and other events, its best to keep an eye on the news and set up alerts for the currency you’re investing in. This will ensure traders don’t become anxious when values begin to fall and they sell off immediately, as there is most likely a lot of growth to come, and impulsively selling at a low will cut any profits you might have made.\nPurposeful devaluation is one of the biggest risks when forex trading is concerned as there is little that can be done on anyone’s part to prop the value back up. If your base country has devalued the currency of your investment currency then it’s almost permanently worthless.\nTypically these deviations happen during times of conflict between countries and can come about as a result of sanctions or other governmental policies. In the past devaluation has been implemented to block trading or either make trading fairer. In both circumstances, it can severely damage your investment returns.\nThere are also knock-on effects during a devaluation which can cause further monetary damage, and that can be the explosive rise of interest rates implemented to control inflation which will further decrease the value of your investment. In these circumstances, it is typically recommended you either sell to get any funds back, or hold out and wait for the long term.\nIf you’re a forex trader or a prospective one, your due diligence procedures should definitely cover whether a currency is at risk of devaluation and whether there are interest rate risks present in the near future that could damage your investment.\nCentral bank press releases, government policies, and the general news should give you enough of an insight into current relations to make an informed decision on whether it is a good idea to invest in a currency or not.\nIf you’ve made a successful bet as a forex trader, and your investment grows substantially you expect to be able to sell off and be paid the amount you’ve earned. When this doesn’t happen as it should, it’s called credit risk. It happens when a third-party, either your broker or a firm is unable to provide the funds you’ve made, either due to overspending or falling into bankruptcy.\nAs a forex trader, you should ensure that you do everything in your power to reduce the risks of being affected by an unstable firm or broker. This means making sure to check that your broker or their firm is licensed and follows rules and regulations in place. Also speaking with previous clients and checking online for reviews is a good idea.\nA few things you must check to mitigate your chances of dealing with unpaid investment returns are making sure your broker is regulated in accordance with the country they operate in, as well as making sure that they have a large reserve fund amount to cover losses.\nWith all of the above said, in order for you to become a successful and smart forex trader you should make sure that you have a good understanding of not only the big risks but all of the smaller ones too. Overlooking anything could leave you with major losses, in debt or entirely broke.\nFailing to correctly factor in the risks involved in trading will almost always have dangerous repercussions and leave you lost for what to do when difficulties arise. All of the risks listed above should have at least a minor action plan developed so you can find the best ways to avoid or reduce them.']"	['<urn:uuid:9e6ebc89-a4cc-41ee-8f45-d6bbba77a48b>', '<urn:uuid:53c3fcb7-89fa-467f-8c2c-4ee550ebef41>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	6	84	3277
21	What is the key difference between raster and vector maps in terms of image quality when zooming, and how does this affect their use in data visualization?	Raster and vector formats handle zooming very differently. Raster files, which are made of pixels like photos from your phone, become fuzzy and pixelated when zoomed in too much as the squares become visible. In contrast, vector files, which are based on mathematical formulas to maintain shape, can be scaled to any size without losing quality because they're made of paths and curves. This difference impacts their use in data visualization - raster files load faster but may lose quality when zoomed, while vector files take longer to load but maintain crisp appearance at any scale, making them beneficial for large-scale viewing. This is particularly important in map-making where different zoom levels may be needed.	['Map making is, almost by definition, data visualisation. Base maps aren’t just readily present, they are created off of data collected from urban planning departments, remote sensing companies, and physical geographers. Base maps are often overlaid with even more information: what buildings and business are we showing?, what neighbourhoods are people being directed to?, what borders are erected? The cartographer makes these decisions based off of their relevant importance to the map’s purpose and their own harboured bias. This post is meant to help clarify some basic and introductory concepts for understanding and communicating the data in maps.\n- Types of Data Used in Maps\n- Visualising Data\n- Legends, Charts & Supplemental Features\nTypes of Data in Maps\nMaps use geodata, meaning that this data has a spatial aspect to it. Some data might tell you how many evictions there are in a given time period, geospatial data will tell you where these evictions are happening. This spatial trend can help you discover targeted neighbourhoods. This could be addresses to places and latitude/longitude points, depending on how the data was collected and what was being recorded. Some examples of these file names and purposes are the following:\n- Raster – These files are used outside of geospatial technology and maps, but their relevance is worth bringing up. These pixel files, just like pictures taken off of your phone. You may notice that if you zoom in too much, you can see the squares materialise. The positive part of this is that raster files load much faster than vector files.\n- Vector – These files are based off of mathematical formulas to maintain the points, lines and polygons shape. Fonts and PDF files are examples of vector files. Although these files take more time to load, they can often be considered more beneficial if your zoom levels are large scaled.\n- Shapefiles – These are vector files–created by points, lines and polygons– are used in GIS software. These files usually have attributes to them as well.\n- MXD – These files have a map layout, description and other objects that can be saved on maps. These files can be used in GIS software.\n- KML – Keyhole Markup Language displays geographic data in an Earth browser, such as Google Earth. It has properties for place-markers, paths, polygons, styles for icons and overlays and much more.\n- TIGER – Topologically Integrated Geographic Encoding and Referencing describes land attributes. The US Census uses these files for the geospatial map\nThere are a lot more files that you may come across while making maps, but understanding these should give you a good place to start understanding what it is you’re working with.\nData visualisation is basically taking a whole bunch of numbers into a visual format that can be understood simpler, faster and by a wider community. I think the most important part of data visualisation is audience comprehension– what’s the point of you putting in time and effort if your efforts are being read as ambitious? Data visualisation on maps can be represented through a number of different ways. Here are just a few examples:\n- Choropleth maps use colour to show densities and scarcities in places. These maps are valuable when looking for concentrations and scarcities of a phenomena. When choropleth mapping strategically, the deconstruction of borders becomes apparent as well– more on this later, however. Common choropleth maps are population density maps.\n- Flow maps show movement of phenomena. This map is good for showing how much of what is going where. Examples of flows may be migration patterns and agricultural distribution.\n- Cartograms distort space by the phenomena density in a place. This map is good for giving truth about a place more so than a space. Some great examples of cartograms can be found here.\nDepending on what data you have and what spatial relationships you want to show will determine which type of map to use. Think first about what you to show and what you want your data to argue before you decide what kind of map you want to make. Don’t limit yourself! Think about combining these ideas too. Maybe you’re making a flow map that has a phenomena being sent at different scales, consider either changing the colours of the flows or graduating their size.\nLegends, Charts & Supplemental Features\nEqually as important to the map are the components that tell the reader just what they’re looking at. Legends are like cheat sheets that translate the map’s colours and symbols into an a linguistic format. Because legends tell people what your map is about, you should spend just as much time cleaning your map as you do constructing your map. The following image show an example of a poorly formatted (often default settings) of a legend and a manicured version.\nChart legibility is also critical in giving meaning to your map and data. When editing your charts, try to give the greatest amount of ideas with the least amount of ink in the smallest space. Creating a good, legible chart requires balancing substance, statistics and design. When constructing your chart, consider these key concepts for legibility and clarity:\n- Show the data – This sounds simple, but these can be easily filled with extraneous information you didn’t mean to make as important as the chart implies. Just tell the reader what they need to know about your map and let them move forward.\n- Tell the truth about the data – Maybe the data actually disproves your hypothesis– and that’s totally okay. It’s more transparent for you to bring forth all the information. Don’t make bias design decisions when communicating the data, you and your audience deserves the truth. If the data didn’t go the way you thought it did, maybe explore why?\n- Maximise dark ink – This is supplemental to your map, it isn’t the big show. Make it legible, but not to the point that it’s covering important parts of your map. Size is an important thing to note, try to keep a reasonable ratio between your map’s size and any supplemental aspect.\n- Minimise chart clutter – Sometimes it’s aesthetic, other times it’s distracting. Recall that the whole point of this is to supplement your map. If you have something that is too distracting or irrelevant, consider redesigning it or deleting it altogether.\n- Have a message – Your map has a purpose, and this supplemental piece should too. What is the purpose? What else do you want to say about your data? Perhaps there was a more niche set of information you delved into, or maybe there was blanket information that applied to the entire map extent. Be clear and precise.\nUnderstanding proper data visualisation is a crucial step in cartography and communication in general. A legible and succinct design that gets the message across is the goal when translating data into a visual. Good luck!', 'In last tutorial we learn how to start with new document , saving and opening it. Now before coming to Photoshop Working we will learn about some terms which will be regularly used in Photoshop or in many of photo editing software. Firstly we will understand what type of software is Photoshop.\nPhotoshop is a raster based software . Your images are built up with pixel in Photoshop . To do editing or make your image or subject editable you have to convert it to raster image. When you try to apply any editing tool on vector or bitmap, this box will appear on screen and then firstly you have to rasterize it by clicking on ok.\nHow it is different from vector?\nVector images are made up of path, path can be a line, square, circle or any shape by which you can make simple diagrams or complex drawings. It mainly contain solid color, as it is made up of some dots therefore it will not loose quality after full stretch also. But when it comes to raster it will image quality will come down on zoom as it is made up of pixel. However photo editing is possible in raster images only not vector.\nTUTORIAL IN DEPTH :\nHere is in detail tutorial, for those who want to dive deep in this topic. Above tutorial is for those who just want to know about rasterize and vector and are hobbyist, now this detailed tutorial is for you who scroll down to be professional.\nAs above I explained that Photoshop is a raster based software, now here we will learn about raster and vector in details.\nNow let start raster and vector image with an example image. Here below in image you can see two letters R and V, Text is a vector graphic only, but you can also make it raster. So here R is rasterized and V is vector. Now when I scale them , V didn’t pixilated and retain its crisp shape, but R which is rasterized looks fuzzy.\nSo it is a basic difference between raster and vector graphics.\nFirstly we will learn about Raster images ( also known as Bitmap).\nRaster images are also known as bitmap images. A raster or bitmap image use grid of colors, by which an image is represented, that grid is made up of pixels. Pixels is explained in detail in Image and resolution tutorial. Every pixel has its specific value and location in an image. Bitmap images are dependent on resolution, and it consist of a specific number of pixels. So because of that only a bitmap or raster image loose detail and looks fuzzy when scaled up on screen or looks pixilated when printed at higher resolution in compare to its original resolution.\nGif , png and jpeg are examples of raster based formats. Formats has been explained in Format Chapter.\nMore about Bitmap Images\nMany bitmap formats contains unique and unusual data structure in design. Normally it’s for to achieve some specific purpose of the format.\nFor example a common file format TIFF includes rudimentary header, but it stores much of its data in a series of tags , which is called as Image File Directories. It is not fixed in size or position.\nUnusual or unique features of these formats consist, that how they store data, and palette information in different files.\nThis is just brief description about it, as you don’t want know that much about this in designing work.\nNow coming to Advantages of Bitmaps :\n- Pixel Values can be modified individually or as a group by changing a pallet.\n- Raster or bitmap images can be edited and have a large scope to work with many tools in Photoshop and other editing software.\n- Bitmaps can be easily translate to devices like CRTs and Printers.\nDisadvantages of using bitmap files :\n- Bitmap files are very large in file size, mostly which images contains more number of colors. By compressing pixel data , size of images can be came down. But there is always a limit to compress images , as it can loss the details.\n- Bitmap or raster files will lose quality while scaling, and there is no way to solve this problem. Problem is not only with scaling up raster or bitmap images, while scaling down also it throw pixels and loose quality , end up with something different that original creation.\n- Bitmap image should be stored in same resolution in which it has to be printed. In image size and resolution I explained about this in details. Otherwise you will not get your desired pint output.\nNow let learn about vector images\nVector graphics are those which are made up of curves and lines , and defines by mathematical objects. Vector files consist of mathematical description of one elements of image , which are used by application , by which final output will be render. Corel and adobe illustrator are popular vector based software. Now main difference in raster and vector is that, raster is made up of pixels and vector is made up of image elements and objects. That’s why vector images are not resolution dependent, therefore can be scaled up to any size, and can be printed at any resolution without losing details and clarity. So that publish work is mostly done at vector based software, where you just want to show text and shapes.\nVector element can be scaled to any level and scaling vector image too large or too small didn’t affect its clarity. Now what can be a negative effect at the time of enlargement vector is , that if there is any gap in two endpoints, or joint point, it may be shown as a gap in time of enlargement.\nText as Vector\nOur fonts and text in software are vector based, which are made up of lines, and you can convert text into shapes in Photoshop, so you can edit them as vector shapes. It has been explained in Text Tool Chapter.\nAdvantages of Vector files\n- These files are useful to store images which can be composed by simple shapes, lines and polygons.\n- Vector files can be easily scaled and manipulate to have desired output in different output devices and mediums.\n- Objects in these files can be edited easily , any object can be removed or added without affecting other elements of image.\n- It is easy to save any vector file to a bitmap image.\nDisadvantages of Vector Files\n- It is not possible to store extremely complex images , such as photographs in Vector Files. As they are based on pixel information.\n- Sometimes appearance of vector images vary according to application used for rendering or displaying it. As vector image will be displayed differ in software in which you made it and other in which you are displaying it.\n- It may take a long time to reconstruct a vector file than a raster image, as in vector image every element should be drawn individually.\nFundamental Difference between Raster image and Vector files\n|Raster (bitmap) images||Vector Files|\n|A raster or bitmap file consist of exact pixel by pixel mapping of an image, it can be reconstruct or displayed by raster based application like Photoshop||Vector file contains mathematical description of elements which are used by its application to give final output.|\n|These images cannot be scaled without losing its quality.||Vector images can be scaled to any limit without leaving its quality.|\n|Bitmap or raster images are dependent on resolution.||Vector files , or graphics are resolution dependent.|\n|Raster images or bitmap use pixels to represent images.||Vector graphics are made up of objects or elements of particular application .|\n|Formats of bitmap are – .tiff, .jpeg, .png, .bmp, .gif||Vector files or images saved in format of their application in which they are drawn, these formats are .ai (illustrator) .cdr (corel)|\nSo here we comes to an end hopefully you understand that in which type of software you have to work , its nature and type of files which will be use in this.\nIf there is any problem regarding any topic just comment your question in that specific tutorial. Our team is always there to answer you, Top questions have been given to our expert team for making tutorials for that questions.\nSo ask us to help others.\nRasterize layer will be explain in next session in LAYERS tutorial.']	['<urn:uuid:62e47673-18b2-494a-aea9-2b9d67dc59cd>', '<urn:uuid:7c46d16c-4fc9-4fa3-b021-47d87bf5ce44>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	27	115	2570
22	working capital daily operations management excluded from ebitda calculation explain implications	EBITDA ignores working capital, which is crucial as it represents the cash required for daily business operations. The metric does not include working capital, interest rates, taxes, or capital expenditures, and its value can fluctuate in conjunction with these items. This omission is significant because when cash runs out, it needs to be reinvested into profitable investments as quickly as possible. Therefore, EBITDA's exclusion of working capital can mask important aspects of a company's operational health.	['The Definition of EBITDA\nThe acronym EBITDA stands for earnings before interest and taxes. This measure is more appropriate in many industries than others. However, it is misleading when used in industries such as oil and gas production or mining. Companies in those industries will likely perform better based on EBIT. In those cases, the investor should consider using the other metrics to gauge company performance. For more information, see the definition of EBITDA below. And keep in mind that EBITDA does not include depreciation or losses from asset value.\nAs an accounting metric, EBITDA does not include working capital, interest rates, taxes, or capital expenditures. As such, its value can fluctuate in conjunction with those items. In addition, an EBITDA value of less than zero may be a sign of trouble. Conversely, a positive EBITDA value does not mean that a company is healthy. Because this metric does not account for taxes and interest, it is difficult to convert earnings to cash.\nEBITDA is an important financial metric that enables investors to understand the financial health of a company before making strategic decisions or making financing arrangements. This measure is also useful in determining the company’s ability to repay debt, liabilities, and obligations. While this metric is a useful tool for investors, it has its limitations. Specifically, it is only useful for certain companies. Therefore, it is crucial to understand its definition and how it is used in financial transactions.\nEBITDA Formula and Calculation\nThe EBITDA, or earnings before interest and taxes, is a measure of the profitability of an operating business. EBITDA is a great way to compare businesses, and determining the amount you need to make a profit is vital. If you’re looking to improve your business, you should calculate EBI TDA regularly to see where you stand. This article will help you understand how this metric is calculate. Let’s get started!\nUnderstanding EBITDA Before Buying a Company\nIf you’re thinking about buying a company, understanding EBITDA is important. However, before you can decide whether EBITDA is worth your time, you need to know what it actually means. While this figure is not regulated by the GAAP, it is still useful to investors, lenders, and potential acquirers. Understanding EBITDA is important for a variety of reasons. It can help you understand how profitable a company is and how to make adjustments to its numbers in future years.\nEBITDA and Leveraged Buyouts\nIn the 1980s, EBITDA became popular as a measure for the ability of a company to repay its leveraged buyout debt. As a result, leveraged buyout investors began to use EBITDA to examine distressed companies and their repayment capacity. Since then, leveraged buyouts have been a popular method of investing. But how can you use EBITDA to assess a company?\nThe Drawbacks of EBITDA\nIn today’s market, many investors use EBITDA as a measure to compare companies. Often, this allows investors to see which company is growing faster in terms of revenue and product sales. For example, a company with similar revenue and product sales may have different capital structures and tax jurisdictions, so an analyst may compare them to determine which is the better value. However, there are some drawbacks to EBITDA.\nWhy EBITDA Ignores Costs of Assets\nA company’s EBI TDA is often a misleading measure of liquidity, because it ignores costs of assets that are on the balance sheet and on property, plant, and equipment. Depreciation and amortization are not reflective of management’s decisions in the current period, but they allow for easier comparisons. By including these costs, companies can show a clearer picture of their performance.\nEBITDA Ignores Working Capital\nWhile it may seem like the EBI TDA metric ignores working capital, it’s essential to remember that it’s still crucial. After all, working capital is a business’s cash required for daily operations. But what happens when the cash runs out? The answer lies in reinvesting it into profitable investments as quickly as possible. Fortunately, EBITDA can help you avoid this pitfall.\nThe Limitations of EBITDA\nA company’s cash flow is typically measured in terms of EBI TDA. This metric is very popular and provides a quick understanding of a company’s worth. Depending on the company’s industry, it can range anywhere from zero to many times its EBITDA. In addition, it is a very good indicator of financial condition in a sector that you’re familiar with. But there are several limitations to this measure.\nHow EBITDA Obscures Company Valuation\nUsing an EBI TDA multiple to value a company can be a helpful strategy. But not all EBI TDA multiples are created equal. In fact, they can vary greatly across industries. In this article, we’ll cover how to value a company using EBI TDA. Using a multiple to evaluate the value of a company is crucial, but only if you know how to interpret it.\nEBITDA Vs EBT and EBIT\nThere is a difference between free cash flow and operating cash flow. The former includes changes in working capital, which both use and provide cash. Using these figures is important to determine cash flow, as failing to account for these changes can mask signs of trouble ahead. As a result, EBI TDA is better used alongside other performance metrics. EBIT is a much better measure of a company’s performance than free cash flow.\nDifferences Between EBITDA and Operating Cash Flow\nOperating cash flow and EBI TDA are two different measurements of a company’s earnings. Both measures are important to understand, but each can have different implications. It’s important to know the differences between these two measures before you make a final decision. Listed below are the differences between EBI TDA and operating cash flow. If you’re looking for an investor, be sure to learn about EBI TDA and how to calculate it for your business.\nExamples of EBITDA\nIf you are looking for some examples of EBI TDA, you have come to the right place. Here you will find the basic definition of EBI TDA, as well as examples of how to calculate it. EBI TDA stands for “Earnings before Interest and Taxes.” This figure is generated after depreciation, amortization, and selling, general, and administrative expenses are deducted. Regardless of your business type, you will have to pay some form of business tax.\nWhat Is a Good EBITDA?\nMany business owners and investors alike are interested in the question of “What Is a Good EBI TDA?” Regardless of your goals, an understanding of this measure is critical. This ratio is crucial in determining the profitability of a business and can help you form an exit strategy. For example, EBI TDA can help convince a prospective buyer that a business is a good investment and set an appropriate asking price.\nWhat Is Amortization in EBITDA?\nWhat is amortization in EBITDA? EBI TDA stands for earnings before interest and taxes and is the measure of profitability in operating a business. Generally speaking, the higher the EBI TDA, the better. This measure also considers the impact of amortization. It’s important to understand the underlying principles of EBI TDA, which are to maximize profitability and minimize risk.']	['<urn:uuid:e80beef1-c77e-46df-b78c-5b839e6d20cb>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T05:18:19.158519	11	76	1189
23	who runs voting federal level canada explain	Elections Canada (officially the Office of the Chief Electoral Officer) is the non-partisan agency responsible for administering Canadian federal elections and referendums.	['- 1 Who are candidates for the electoral nominated by?\n- 2 Who is responsible for Elections Canada?\n- 3 Who is the youngest president to take office?\n- 4 What are the 4 political parties?\n- 5 Who is the leader of the federal government in Canada?\n- 6 How are elections called in Canada?\n- 7 How often are provincial elections held in Canada?\n- 8 How long can a Canadian prime minister serve?\n- 9 Which port is the largest and busiest in Canada?\nWho are candidates for the electoral nominated by?\nGenerally, the parties either nominate slates of potential electors at their State party conventions or they chose them by a vote of the party’s central committee. This happens in each State for each party by whatever rules the State party and (sometimes) the national party have for the process.\nWho is responsible for Elections Canada?\nThe Office of the Chief Electoral Officer (French: Bureau du directeur général des élections), commonly known as Elections Canada (French: Élections Canada), is the non-partisan agency responsible for administering Canadian federal elections and referendums.\nWho is the youngest president to take office?\nAge of presidents The youngest person to assume the presidency was Theodore Roosevelt, who, at the age of 42, succeeded to the office after the assassination of William McKinley. The youngest to become president by election was John F. Kennedy, who was inaugurated at age 43.\nWhat are the 4 political parties?\nToday, America is a multi-party system. The Democratic Party and the Republican Party are the most powerful. Yet other parties, such as the Reform, Libertarian, Socialist, Natural Law, Constitution, and Green Parties can promote candidates in a presidential election.\nWho is the leader of the federal government in Canada?\nThe Right Honourable Justin Trudeau, Prime Minister of Canada. Justin Trudeau (born December 25, 1971) is Canada’s 23rd Prime Minister.\nHow are elections called in Canada?\nIf a government loses a confidence motion, traditionally the prime minister will ask the governor general to call an election. The governor general when approached by the prime minister who has lost a vote of confidence will traditionally call an election.\nHow often are provincial elections held in Canada?\nIn Canada, the federal government and most provinces and territories have passed legislation setting fixed election dates so that elections occur on a more regular cycle (usually every four years) and the date of a forthcoming election is publicly known.\nHow long can a Canadian prime minister serve?\nThe prime ministerial term is not tied directly to the term of the House of Commons, which the Constitution sets as a maximum of five years from the most recent general election.\nWhich port is the largest and busiest in Canada?\nLying close to another CSA regional distribution centre, the Port of Vancouver in British Columbia is Canada’s largest port. Listed as the third-largest port in North America due to its tonnage capacity, the Port of Vancouver is the main trading port between Canada and the rest of the world.']	['<urn:uuid:c943aa1b-a207-4667-aa35-97c717a703ea>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	7	22	506
24	How do smart cities help the environment and health monitoring?	Smart cities contribute to environmental sustainability through optimized energy usage, efficient resource allocation, and sustainable power systems. They also enable 24/7 health monitoring of citizens 'in the wild' through digital technologies like wellness products and tracking apps, though this raises ethical concerns about data protection and regulation.	['Smart city programs all over the world have been rapidly rising in the past decade or two. It brings up new innovative technologies and possibilities but also risks and could have unpredictable outcomes and consequences. A good way to portray the size and growth potential of smart city market is that today over half of global GDP is produced by 380 cities and by 2025 the estimates are that 600 cities will be responsible for more than 60% of economic growth (Iliev, 2018). For the sake of comparison there were over 4000 cities with more than 100 000 inhabitants in 2015 and to this date that number has undeniably only grown bigger (Brilliant Maps, 2018).\nThe IoT (Internet of Things) has given a possibility for cities to make themselves more reachable and interconnected with its citizens and the technological advancements in data gathering and analyzing will lead to substantial cost savings and economic growth. This will also give outside investors a chance to be part of these rapidly growing programs. The effect of big and smart cities in global economy and future growth cannot be underlined enough. And if estimates come true, 66% percent of human population will be living in urban areas by the year 2050 and therefore there needs to be major accomplishments in technology and proper allocation of resources to handle this huge population (Low, 2018).\nWhat Makes a City Smart?A city might have a large population and a lot of big companies in technology and business, local public transportation and a good economical situation. But what makes a city smart is the communication, data collection and analyzation of this data and the use of it in daily life. Whether it is about optimizing the public transportation to efficiently match the needs of citizens and the current traffic flow or adjusting the lighting and heating in an office building based on the amount of natural light and warmth and more importantly the location of workers and the nature of work they are doing. The optimizing based on people’s location and the things they do extends all the way to water and energy usage and transportation and sewerage (Gardner, M., Brown, M. and Petel, G. 2018), (du Preez, 2017).\nAll these things have a huge impact on citizen well-being, cost-savings due to optimizing the usage of energy and water, transportation all-around and on environmentally sustainable life in a city. An optimal and desirable smart city is efficient, focused on making life better on all areas, non-discriminating, environmentally friendly and sustainable and has a wide range of possibilities on communication with the citizens and is continuously listening to ideas for improvement. That is a place everyone would be happy to live and should be a target for all governments and city councils all around the globe.\nStrategy of Smart City Investing\nAs a company investing in a smart city is a little different from usual as there is also the government and city council to cooperate with and that lays out some new challenges that separate the smart city investing from regular business to business or business to customer -based thinking. There is a lot of restrictions involving the procedures used, quality, sustainability, safety and privacy and all the work has to be done with close interactions and communication with the authorities and there needs to be a report on everything that is done or will be done and it has to be properly and with caution delivered to the authorities who verify and check everything to make sure it matches the restrictions (Gardner, M., Brown, M. and Petel, G. 2018).\nSince there is a lot of competition on the technological field concerning smart cities, the company absolutely needs to have a stand-out innovation plan that most proficiently fills the needs of the city and is sustainable, environmentally friendly and has an overall positive economic impact. And there must be a realistic plan to reach these requirements since failure in a matter of this significance and size is not really an option to the city and therefore the strict restrictions are in place.\nThere has been a lot of talk about the privacy concerns a smart city faces since the whole idea is based on data collecting and analyzing. One idea to most effectively keep the information safe is to cooperate with technology-businesses that have a lot of experience in protecting data and all kinds of challenges it creates like Google, Apple and Facebook. Cities using this data will have to find the happy medium in the depth of the data analyzing since eventually there will be a gray area of information that the citizens will not be willing to give up for use and don’t want to risk losing that information to third parties.\nA lot of the smart city ideas are based on following the locations of people. For example when a person wants to go to a business meeting in the center of the city with a car during the worst rush hour, an app could tell him where the nearest free parking spot is and where he has to turn to access this parking spot. Eventually this kind of location monitoring adds up to patterns that reveal extremely indistinguishable information about people that they certainly do not want to share. That kind of information can be where they live and work, what kind of hobbies they have, where do their friends live and what kind of medical issues they have. At some point in addition the law comes an obstacle in at least most of the modern states that value human rights and privacy (Gardner, M., Brown, M. and Petel, G. 2018).\nWays of Investing in a Smart City Ways of investing in a smart city vary on all levels of imagination. Whether it is about funding a sustainable green energy project just outside the city to cover a major part of the energy consumption in the city or developing a new kind of data storage system that can handle the massive amount of data collected and can withstand the growth of that data in the future as well (Iliev, 2018).\nMost likely the most used way of investing is and will be buying shares of the companies that have the connections and possibilities to in a major way affect some aspect of smart city technology and can be most profitable in the long run. An investor has to be extremely careful when investing in these companies though, since the smart city program is relatively new and we do not yet know how everything will turn out with it because there are no experiences from the past to compare to. In addition there is a massive amount of technology companies that want to be part of these programs but only a small proportion of them will succeed and have a profitable role in the smart city world. An investor needs to find the most potent, innovative, stable, ambitious and well-funded companies that have the necessary elements to be a successful operator in the field.\nEconomic Benefits of a Smart CityI\nmplementation of the Internet of Things allows a city to create variety of ways to both save and generate money through reducing energy and water usage and being more efficient in car-parking and speeding fines to name a few. Most importantly applying Internet of Things in the basic fundamentals of a city saves everyone’s time, money and effort and also the environment as the data collected allows cities to adjust their services and businesses to most efficiently do their work (Eichler, 2018).\nA significant effect on being a stable and profit generating city is that it attracts even more investors to make the city an even better place to live through continuous investment and new smart technology projects.\nThere has been concerns about the funding necessary for cities to start a program to implement the Internet of Things even though the overall amount of money invested in smart cities is exponentially growing. The reasons to this are mainly location- and state -based since the main proportion of funding comes from the local authorities (Eichler, 2018).\nIf a proper plan is introduced, a smart city project will get a big share of private funding, but as mentioned before, the privacy and security concerns need to be properly addressed and that requires the local authorities to have a substantial part in the developing and maintaining process, hence the need for local funding too. In addition the city itself is always held accountable for the safety and functionality of the technological systems and the area as a whole. Therefore it is understandable that the first thing to start a smart city program is to find a government-backed funding plan. Other significant reason for the hold-ups of smart city projects is the lack of infrastructure to back the systems needed to create a sufficient technological entity for the Internet of Things to be implemented (Eichler, 2018). Business Insider’s estimation of Internet of Things attracted investments. (Business Insider Intelligence)\nAs the technology grows and people keep on moving in large cities after work, family and way of life, the cities have to create more ways to save costs and at the same time be more reachable and environmentally friendly. This offers numerous chances for investors as technology companies implementing the Internet of things line up for cities to choose from. Investment choice has to be even more carefully planned than usual since the massive competition in the field compared to the amount of companies that will actually be used to build up the city’s communications- and data analyzing -systems.\nThere is a bright future in the technology-business considering the smart city programs. There is also a moral incentive as the smart city idea itself is to quarantee the well-being of the citizens and to provide sustainable and environmentally friendly services and power in addition to a huge impact on economic growth.\n- Iliev, I. (2018). Top Ten Reasons to Back Smart City Investment Opportunities. [online]\n- What Investment. Available at: http://www.whatinvestment.co.uk/top-ten-reasons-to-back-smart-city-investment-opportunities-2613969/ [Accessed 20 Sep. 2018].Low, I. (2018).\n- The Benefits of Smart Cities. [online] Globalsign.com. Available at: https://www.globalsign.com/en/blog/the-benefits-of-smart-cities/ [Accessed 20 Sep. 2018].Brilliant Maps. (2018).\n- The 4037 Cities In The World With Over 100,000 People. [online] Available at: https://brilliantmaps.com/4037-100000-person-cities/ [Accessed 20 Sep. 2018].\n- Gardner, M., Brown, M. and Petel, G. (2018). Smart Cities: A Look at Risks and Opportunities for Technology Companies. [online]\n- Wileyrein.com. Available at: https://www.wileyrein.com/newsroom-newsletters-item-Smart-Cities-A-Look-at-Risks-and-Opportunities-for-Technology-Companies.html [Accessed 22 Sep. 2018].\n- Eichler, W. (2018). Smart cities held back by ‘lack of funding’. [online]\n- Localgov.co.uk. Available at: https://www.localgov.co.uk/Smart-cities-held-back-by-lack-of-funding/44728 [Accessed 23 Sep. 2018].\n- du Preez, D. (2017). Deloitte – Smart cities investment requires some “creative thinking”. [online]\n- Diginomica government. Available at: https://government.diginomica.com/2017/06/01/deloitte-smart-cities-investment-requires-creative-thinking/ [Accessed 23 Sep. 2018].\n- Insider Intelligence, B. (2016). The US is investing $165 million into smart city solutions. [online] Business Insider. Available at: https://www.businessinsider.com/the-us-is-investing-165-million-into-smart-city-solutions-2016-10?r=US&IR=T&IR=T [Accessed 23 Sep. 2018].Page 2 of 2', 'Direct-to-consumer wellness products, location-tracking apps, and access to personal data on social networks present both exciting opportunities and significant ethical worries for researchers.\n“The digital revolution is rapidly influencing how health research is conducted. We can now passively observe and record people ‘in the wild’ and 24/7,” says Camille Nebeker, EdD, MS, founder and director of UC San Diego’s Research Center for Optimal Data Ethics.\nThe use of artificial intelligence and active assisted living robots in the health sector also is increasing. “While there is amazing potential, the digital health ecosystem is not consistently regulated. We are in the Wild West of digital health research,” says Nebeker.\nThe authors of a recent paper proposed steps the scientific community can take to ensure social media data are used ethically.1 The paper was prompted in part by the recent Cambridge Analytica scandal, involving allegations that the firm used data improperly obtained from Facebook to build voter profiles.\n“Many of my colleagues are conducting research using social media platforms,” says Nebeker, one of the paper’s authors.\nIRBs and researchers are struggling to navigate this new territory, sometimes unsuccessfully.\n“When something goes wrong, as it did with Cambridge Analytica, it compromises public trust and jeopardizes research that is in progress,” says Nebeker.\nThe following are two central ethical concerns:\n• Researchers may need to cover additional information during the informed consent process.\nCommercial products — such as fitness tracking devices — are used as measurement tools. This means privacy policies and terms of service should be considered.\n“These terms might influence the study risk assessment,” explains Nebeker. Potential research participants also need to factor in this information to make informed decisions.\n“In many cases, the terms of service directly conflict with the federal regulations for human subjects protections in that a participant, if harmed by the product, must agree to arbitration,” notes Nebeker.\n• Not all tech companies comply with federal requirements for research.\nFederal regulations for human subjects protections must be followed if research is funded by the U.S. Department of Health and Human Services. However, many tech companies that are involved in biomedical research are not regulated. “We need to develop common standards that govern digital health research,” says Nebeker.\nResearchers using social media data are operating in an unregulated environment. Thus, there is growing concern about potential harms. “This is another case of how technology has evolved faster than regulations,” says Sherry Pagoto, PhD, director at the University of Connecticut Center for mHealth and Social Media.\nPrivacy breaches are possible — intentional or not. “This poses risks to everyone involved: researchers, social media companies, and, most importantly, the general public,” says Pagoto.\nFor example, few Twitter users are aware that public social media posts can be used by researchers.2 Notably, the majority believe that researchers should not be able to use their tweets without consent. Also, users of commercial products do not always understand privacy implications.\n“We cannot fault them, though. These policies are very lengthy, and written in ways that are difficult to understand,” says Pagoto.\nThe following changes are needed, according to the study authors:\n• Public education on the research performed with social media data, why it is important, and how researchers protect user privacy.\n“Consultation with an expert in health tech ethics is critical if being proactive and diligent about human research protections,” says Nebeker.\nStakeholders — including researchers, IRBs, potential participants, or policymakers — may not be fully aware of how data are collected, used, or shared by social media platforms. “This lack of knowledge will influence risk assessment and information included in the informed consent process,” notes Nebeker.\n• Federal regulations on the use of social media data in research.\n“We can anticipate that the technology and research landscape will only continue to evolve, and rapidly,” says Pagoto. IRBs rely on federal regulations for guidance on the ethical conduct of research. These regulations are outdated as they pertain to the use of data generated by new technologies like social media. Thus, says Pagoto, “universities, funders, and researchers need to be more vigilant about potential harms and begin to craft guidelines for the purpose of self-policing. We need a code of conduct.”\n• “Tech ethicists” working alongside researchers as they attempt to use social media data.\nSomeone with tech ethics expertise could comment on the ethical implications specific to technology used in studies and conduct training for clinicians. “It would also be useful for these folks to advise on grant applications, even serving as consultants or co-investigators,” says Pagoto.\nSomeone on the IRB could take on this role. “But if limited expertise is available on campus, external expertise should be commissioned,” says Pagoto.\nIRBs also should have the expertise to properly review social media research. “Adequately attending to research ethics will require an investment,” says Pagoto. “We want to nudge institutions to make this investment.”\n- Pagoto S, Nebeker C. How scientists can take the lead in establishing ethical practices for social media research. J Am Med Inform Assoc 2019; 26:311-313.\n- Fiesler C, Proferes N. “Participant” perceptions of Twitter research ethics. Social Media + Society, March 10, 2018. Available at: http://bit.ly/2valr7G.']	['<urn:uuid:d5380911-fec3-483c-91ce-bac71cc3ad36>', '<urn:uuid:31f9e0fa-beb1-4e9f-9c19-d5b6873d9873>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	10	47	2684
25	What are the different ways to protect yourself from seasonal allergies when spending time outside during spring and summer, and what should you check before going outdoors?	To protect yourself from seasonal allergies outdoors, you should check several things before going outside. First, check daily air quality levels and air pollution forecasts in your area. For yard work, check local pollen counts and consider gardening in early morning or evening when pollen counts are lowest. Wear a particle mask when doing lawn maintenance to avoid breathing tiny particles. For bug protection, use unscented lotion repellents instead of aerosol sprays, wear long-sleeved shirts, long pants and socks, and stay indoors during sunrise and sunset when mosquitoes are most active. Summer is peak season for grass pollen, which is a leading cause of hay fever and asthma attacks, so these precautions are particularly important during warmer months.	"['Let’s Get Serious About Seasonal Allergies\n- Posted on: Jul 10 2019\nGrass pollen isn’t the only trigger for seasonal allergies, but it’s one that asthma patients and parents of children with asthma should be aware of.\nSummer is peak season for grass pollen, which is a leading cause of hay fever and asthma attacks for those with allergic asthma. 24 million Americans suffer from asthma, 6 million of these cases are children. During the summer months, children are out of school and playing outdoors with friends. Many families are planning family reunions, barbecues, and summer getaways. Environmental allergies not only disrupt the long-awaited leisure of these activities, but they present a certain risk for asthma sufferers who are unprepared. That is why it’s important to have asthma symptoms diagnosed and treated by an asthma doctor.\nAllergic vs. Non-Allergic Asthma\nPatients need to be familiar with the type of asthma they have; allergic or non-allergic. Allergic asthma is triggered by environmental factors such as pollen, pet dander, tobacco smoke, and mold exposure. Non-allergic asthma is often triggered by exercise and in some cases by acute or chronic health conditions affecting the lungs. Understanding you or your child’s asthma is the first step to preventing a serious attack. Avoiding exposure to substances and conditions that exacerbate asthma symptoms is top priority, however, this is nearly impossible for people who have asthma and seasonal allergies. An allergist may need to put preventative measures in place to help reduce and manage moderate to severe asthma symptoms related to seasonal allergies.\nFind Your Allergy Clinic\nAn allergy specialist is specifically trained to identify allergens and develop treatment strategies for asthma patients. Medication needs vary between quick-relief inhalers and a combination of inhalers and long-term controller medications. Visiting an allergy clinic is the best way to know that your asthma is being managed and monitored each year. Look up allergists in your area and choose an allergy clinic that’s right for you. Schedule visits with your allergist and your asthma doctor once a year, or more frequently if you notice worsening symptoms.\nBe Aware of Other Asthma Triggers\nAside from pollen and pet dander, people with allergic asthma should be aware of other insidious triggers like mold and dust mites. These indoor allergens are common year-round, but the abundance of these substances is highly altitude-dependent. Asthma patients are more likely to experience irritation during the warmer and damper months. Be sure to keep your home clean and moisture-free in the summer months, especially if you experience frequent rainy weather. Take care of structural leaks in pipes, ceilings, windows, and other fixtures as soon as possible. Keep carpets and rugs dry, and vacuum frequently to avoid the accumulation of pet dander, pollen, and other allergens.\nWhen Your Asthma Is Getting Worse\nIf you find yourself reaching for your asthma inhaler more than twice a week, or that your child’s wheezing spells are growing in frequency, get to the nearest asthma doctor immediately. These are not symptoms to ignore. Quick-relief inhalers are not intended for repeated use and should not replace regular care from a trained asthma specialist. Although you are familiar with your asthma and triggers, there could be unknown allergens in your environment causing more frequent flare-ups. Take caution and visit your allergy clinic if your symptoms increase.\nDo Your Research\nStay up to date on asthma news and treatments available. Know about the different asthma and allergy clinics in your area and find out what you can do to be prepared for seasonal allergies each year.\nPosted in: Allergy', 'American Lung Association"">\nWith springtime upon us, many look ahead with excitement to warmer weather, flowers in bloom, and more hours of daylight. Although for some, these signs of spring also bring the common frustrations associated with asthma and allergies. Don’t let asthma and allergy symptoms control your life! Keep these helpful hints from the American Lung Association in mind as you enjoy the budding new season.\nIdentify those triggers. Each season comes with its own set of potential asthma and allergy triggers. A trigger can be something in the environment like pollen, a chemical that you are exposed to, stress or emotions. In order to control your asthma, it is important to know what may trigger your asthma symptoms and how to limit your exposure to that trigger. Dust mite excretion, pollen and pet dander are all common allergens and are known triggers for people with asthma. Check here for a more detailed list of asthma triggers and work with your healthcare provider to identify what may be causing your asthma symptoms!\nGet ready, get set, get outside! With the warmer weather, come increased outdoor activity and the threat of new seasonal triggers. As flowers begin to bloom, pollen and air quality may play a role in your efforts to control asthma and allergy symptoms. Common springtime triggers found outside may include: pollen, flowers, weeds, trees, grass, pollution, and even temperature changes.\n• Check your outdoor air quality. If you plan to move your physical activity outside, remember to scope out the environment first and be aware of any obvious triggers. The quality of the air we breathe outdoors affects each of us and can be especially troublesome for people with allergies and asthma. Check daily air quality levels and air pollution forecasts in your area. Click here to check now. Review the American Lung Association website for more tips on how to protect yourself from unhealthy air.\n• Lawn & garden maintenance. Before working in the yard, check your local pollen count and consider gardening in the early morning or evening when the pollen count is at its lowest. Fertilizers and freshly cut grass can worsen asthma symptoms. When working the yard, consider wearing a particle mask (available at hardware stores) to keep from breathing in tiny particles.\n• It’s a bug’s life. Citronella candles and bug spray may keep mosquitoes at bay but can also trigger an asthma episode. It may help to stay several feet away from any strong smelling candles, and when using mosquito repellent, choose lotions that are unscented instead of aerosol sprays. Other tips that may help you avoid using repellant products are to empty flower pot liners or other containers holding water, wear long-sleeved shirts, long pants and socks when outside, and stay indoors at sunrise and sunset when mosquitoes are most active.\nKnow your hazards from the inside. Many Americans follow the long standing tradition of spring cleaning. Dusting, vacuuming and cleaning indoor surfaces can help to eliminate many potential asthma and allergy triggers, but can also introduce new ones into the home environment.\n• Beware of cleaning products. While cleaning the home is important and can eliminate triggers such as mold, ingredients in cleaning products can cause asthma and allergy symptoms. Help to reduce the impact of these chemicals by ensuring that there is good ventilation. This means that you may have to open a window or two to increase the air circulation in the home. Look for the Environmental Protection Agency (EPA) Design for the Environment (DfE) logo when you shop for products. EPA allows safer products to carry the DfE label. This mark enables consumers to quickly identify and choose products that can help protect the environment and are safer for families. For more information on what the DfE label means or for a list of approved products, visit the EPA website.\n• Get rid of pests (or at least decrease exposure). Pests, such a cockroaches and dust mites, can be triggers for many asthma and allergy sufferers. You may never have a pest-free home, but you can do your part to reduce your exposure to these tiny critters. Regular house cleaning and the use of dust-mite resistant pillow and mattress covers may eliminate the breeding grounds for these common household bugs.\nUse medications as prescribed. While limiting exposure to triggers can be helpful, you can never eliminate contact from all potential items that cause asthma and allergy symptoms. Always be sure to use your controller medications as prescribed, even if you are feeling well. Remember to keep your quick-relief medicine (i.e. Albuterol) close at hand in case of a flare-up. Other tools, such as a Peak Flow Meter and a written Asthma Action Plan, can compliment your medications and help to guide your outdoor plans. Check out additional information on asthma medicines.\nTalk with your healthcare provider. Be sure to keep him or her informed if you begin having trouble controlling your asthma or allergy symptoms during the spring months. With your provider’s help, you can create an asthma or allergy management plan to help keep you feeling healthy, active and well controlled.\nChildren are especially susceptible to pesticides and cleaning products. Find answers to the Top 10+ questions about household chemicals provided by the Environmental Protection Agency.\nAsthma and allergies shouldn’t keep you from enjoying springtime weather. For more information on lung health, contact the American Lung Association Help Line at 1-800-LUNGUSA (1-800-586-4872).']"	['<urn:uuid:31416ff0-3701-4977-9b60-9be16859ffe2>', '<urn:uuid:95831580-46a1-4c0c-b7a6-d913c0906a86>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T05:18:19.158519	27	118	1502
26	im planning to do exterior metal signs what kind of ink system lasts longest outside and gives best protection	For exterior metal signs, silicone-modified thermosetting enamels provide the best durability and protection. These systems substitute silicone polymer for phthalic anhydride in the formulation, resulting in excellent formability, abrasion resistance, chemical resistance, and heat resistance. In fact, similar coatings (silicone polyesters) are used in house siding with 20-year guarantees. However, these systems are extremely expensive and have poor shelf stability (less than a year).	['Various Ink Systems for Metals\nWritten October 17, 2019\nFirst, I want to touch briefly on the metals that are used in the screen-printing industry. Pound-wise, the major one would be steel as supplied by both domestic and foreign manufacturers. If you buy steel from the mill, you can specify cold-rolled dry steel. Dry means oil-free. It is relatively clean. You can purchase the steel in sheet or coil form if you have the equipment to do your own sheeting.\nIf ultimate corrosion- resistance is required, you can specify a surface-treated steel - phosphatized zinc grip, paint grip or some other chemical treatment. You may prefer to purchase precoated panels, sheets or coils. In this case, there is a paint coating applied at the mill. You can also specify what type of coating you might want, whether it be alkyd, polyester or the newer acrylics.\nIf you order precoated sheets, in most cases you are going to be ordering it roller-leveled–and this may pose some problems, because in the leveling or flattening of the sheets the surface may become contaminated with some sort of oil. It could merely be from fingerprints, but oil is a major problem.\nTo ensure proper adhesion, that surface must be completely free from oil and grease. If you were to purchase bare steel from a warehouse (and I would include aluminum here), it would probably come with a coating of oil applied to prevent rusting or corrosion while in the warehouse. This oil must be removed prior to printing. Other forms of steel used to a significant degree are stainless and galvanized and, to a lesser degree, template. Regardless of which one you utilize, cleanliness is vital. Aluminum is another major metal. It can be one of many different alloys supplied to your specification or to the specification of your customer. As in the case of steel, you may purchase it precoated and ready for decorating.\nAluminum can also be supplied in the form of cast or machine parts, or extruded, plain or even anodized. The aluminum most used for exterior signs generally are two types of alloys, either 6061T6 or 5052H38. These alloy grades are commonly used and specified by federal and state highway departments. To achieve proper adhesion, the aluminum, like steel, must be surgically clean. You can purchase aluminum that has been pretreated or has a chemical conversion coating known as Bondrite, Aladyne, Uradite or by other names. These are primarily used to prevent corrosion, and they assist in the adhesion of any paint or ink you might apply.\nAnodizing is another form of surface treatment for aluminum. It also provides corrosion-resistant surfaces on which you can obtain better ink adhesion. Anodizing is more expensive than other surface treatments, and it is used only when its properties are desirable to you or your customer. The anodic film on the surface is very brittle and reduces the ductility of the aluminum. Forming and cutting can become problems with anodized aluminum. You can create crazing or cracking of the film at points of high stress. Other metals used to some extent are copper and brass, with copper’s major use being in the printed circuit industry. But, like other metals, for proper adhesion both copper and brass must be free from any surface oil or other contaminants that might interfere with the performance of the end-product.\nPrecoated metal products may eliminate the need for you to handle the material in your shop. The surfaces have been treated and they have a coating applied to them. When they first came out, these products had the conventional alkyd-type coating. This presented no problem (as far as adhesion was concerned) with any of the conventional inks on the market. Then they went to polyester and made it a little more difficult for the screen printer. When they went to acrylic, it further magnified the problems. The acrylic surface is, as with the other coatings, a thermosetting system. Being thermoset, it has a very tight, hard surface on which it is very difficult to achieve excellent adhesion. If you prefer to buy the precoated material, it is necessary to pretest almost every load or shipment of that material.\nYou can have excellent results for months and then get in a new load of material whose surface could be of such a nature that the inks you used in the past no longer adhere. This is especially true in cases where you are going to be using very low temperature baking or air-drying. We have found in many cases that to achieve desired properties, you must reach a temperature of 127° C to 138° C (260° F to 280° F) before you can attain adhesion on some of the acrylic precoated surfaces.\nThe ink products used can be classified as either air-dried, thermoset or catalytic. The decoration or printing of metal products will involve one or all of the following (depending on the product end-use or customer specifications): If you are in the metal sign industry, you should always use a primer.\nThe primer can either be applied by roller coating, curtain coating, spraying or, in some cases, printing. It is usually formulated with vehicles that are going to exhibit excellent adhesion to the metal surface as well as excellent flexibility, enabling the final sign or product to withstand the shock of shearing, punching or subsequent forming. It will also contain either additives or pigments that resist lateral corrosion, as exemplified by the salt corrosion test. The next product used would be background coating. This product provides the background color to the sign. It also serves the purpose of protecting the primer. The primer resins that give you tenacious adhesion are not known for their exterior durability. They tend to chalk or degrade when exposed to ultraviolet light, so they must be protected.\nThe main purpose of the ink is either decorative or informative. It must be compatible with the previously applied coating and, to assure proper intercoat adhesion, it must be formulated specifically to meet the requirements you or your customer may have regarding permanence or fade resistance. In most metal decorating applications, the final printing operation is clear coating. The clear can be applied by roller coating, spray coating, curtain coating or printing. The main function of the clear is either to protect against the elements or for aesthetic purposes. Most of the clears used for exterior purposes contain ultraviolet inhibitors.\nThose are chemicals that will minimize or decrease the passage of UV light through the clear, which can degrade or fade the colors applied previously. Thus, the ultraviolet inhibitors can prolong the life of a specific product by up to 50%. If the application demands other properties, the clear may contain additives for abrasion resistance, or be formulated to resist specific chemicals or other production operations.\nThe basic considerations for the product you choose to use on a metal surface will be one or more of those listed below:\nThe application: You may choose a product that prints better than another, but will that product meet other requirements or specifications that you have in your own shop? Drying: Will your equipment have the necessary drying capabilities required by the product you choose to use? Are your ovens going to be hot enough? Are they going to be long enough? Are you going to have enough racks if the product is slower drying?\nAdhesion: In pretest application in your shop, is the adhesion suitable for the intended end-use or specifications? Whether the test be crosshatch tape, fingernail scratch, flex/bend/impact resistance or whatever, is the adhesion test going to be realistic for the end-use of the finished product? Formability: If the product is to undergo subsequent operations, will it perform as intended without cracking, chipping, or flaking? Will it take the stretch or the bend required? Abrasion resistance: Some specifications may require a tabor abrasion test. Will the system meet these requirements or other handling problems that you may encounter in your shop?\nChemical resistance: This brings us to the appliance field. Will the product meet appliance specifications and needs for resistance to chemicals? One of the appliance specifications requires an immersion in a 5% detergent/water solution for a period. The chemical resistance of the ink you choose is going to depend a lot on the specifications, so be sure that you know what those requirements are.\nHeat resistance: Space heaters, ovens, dishwashers, etc., generally require a good deal of heat resistance without a change of color or deterioration of the film. Can your product meet those requirements?\nDurability: Is the system, both vehicle and pigment, suited for your intended end-use? If it is set up for exterior exposure, how long will that system last? Any film of ink or coating immediately starts to degrade upon exposure to ultraviolet light, some faster than others. Any pigment will start to fade upon exposure to ultraviolet light, although at different rates - some fast, some extremely slow. The rate of fading or deterioration of the color will depend a great deal on the pigment, the degree of protection offered by the vehicle and the overprint or clear coat that you use.\nThe basic types of products available for use on metal surfaces include: Nitrocellulose lacquers, which dry by solvent evaporation. They may be modified with an oxidizing vehicle to enhance heat resistance and formability. Generally, the exterior exposure for nitrocellulose systems should be limited to six months to a year.\nThey will dry in 30 to 45 minutes at room temperature and in less than a minute at 82 degree C (180° F). Temperatures well in excess of 107° C (225° F) generally are not recommended for nitrocellulose, especially if you are going to put it through a multiple bake. Forming should be kept to a minimum unless a good, compatible primer is used. These lacquers generally have fair-to-good abrasion resistance; however, they are thermoplastic. Chemical and heat resistance would be considered very poor.\nDurability is only fair, but you can extend the life of a nitrocellulose system by choosing a specific clear, such as an enamel, to give you needed protection. For decorating anodized aluminum, there are specifically designed dye systems available; the aluminum, in this case, must be decorated after anodizing but prior to sealing. The lacquer system is made with dyes which penetrate the pores of the anodized surface. After sealing and removal of the residual on the surface, the result is dye encapsulated in the anodic film.\nThere is no ink on the surface, nothing to scratch off, scrape off or mar. The only way to destroy the color would be to destroy the anodic film. Permanency of the dye systems is generally acceptable in most applications; they will withstand up to one-year exterior exposure. They have been used on automotive trim (rather than an organic surface coating), grill trim and hubcaps, and are being used quite a bit on interior trim. The next surface ink is enamels. We could break down classification very simply: Fast-drying, slow drying or thermoset. Fast-drying inks are manufactured with alkyds, which dry by oxidization.\nBut to make the ink dry fast, these alkyds are highly modified with other resins which detract from the overall properties of the alkyds themselves. They are designed primarily to dry fast at room temperature or low heat. Again, they should not be subjected to extremely high baking cycles. They are similar in physical properties to the lacquers; in that they have limited exterior durability. Because they do contain a percentage of alkyds, they will last longer than nitrocellulose - one year is the most you should expect. They have limited formability because of the modifying resins. They tend to be a little brittle.\nAbrasion resistance and chemical resistance are less than you might desire for a good quality, long-life outdoor sign. Slower-drying alkyd systems not highly modified show considerable improvements in flexibility, formability, adhesion, abrasion and chemical resistance. These are the products primarily used on long-life outdoor signs today. This type of product will air-dry set-to-touch in four hours at room temperature or can be baked 30 minutes at 121° C (250° F) in the oven. Heat resistance is better than mentioned before, but with this type of system (which is still an alkyd system), the enamel will tend to yellow if you reach temperatures of 149° C (300° F) or more for any extended period. Thermosetting enamels are made with oil-free alkyds, but they are combined with a resin that will cross-link at temperatures in excess of 149° C (300° F).\nThe resins, either melamine or urea-formaldehyde resins, create the hard chemical-, abrasion-, and heat resistant surfaces you expect in a thermosetting enamel. The formability of the thermosetting system can be fair-to-good depending on the ratio of alkyds to cross-linking resins. Generally, the faster the cure the less the formability and flexibility. The exterior durability is good for systems that have been formulated for exterior exposure. If a thermosetting enamel was initially designed for appliance finishes or possibly the printed circuit area, it is not necessarily going to be suitable for exterior use. There is a class of thermosetting enamels that has a slightly different formulation from the one just discussed. Instead of using the most common ingredient in an alkyd resin (phthalic anhydride), a silicone polymer is substituted, which really increases the durability of the system.\nIt gives good formability, abrasion, chemical and heat resistance – everything you might desire. It is not widely used within the industry because of some problems that naturally occur with the product due to its reactivity. It has very poor shelf stability - less than a year. Similar coatings, silicone polyesters as they might be classed, have been used for house siding, etc., in one case a siding manufacturer guarantees his product for 20 years. It is an extremely durable, but extremely expensive system.\nThe acrylics are available in either air-dry or thermoset systems. The air-dry have good exterior durability, good color and retention. They have relatively good formability and are quick drying. They have one drawback in that they are thermoplastic like the lacquers, and to some extent like the fast-drying type of enamel. They cannot be stacked while they are hot; they must be stacked cool.\nIf you do not have high-temperature ovens and cannot bake, but you need to use a fast-drying ink that will stand up well outdoors, acrylics would be a good choice. Thermosetting acrylics have all the desirable properties: Adhesion, formability, abrasion resistance, heat resistance and durability. The one critical factor is their conversion temperature of 149° C to 176° C (300° F to 350° F). These temperatures or times can be decreased by using acid catalysts. The thermosetting acrylics must reach a conversion temperature to be completely cross-linked. They cannot be halfway or part way cross-linked or the whole sign will fall upon exterior exposure. It will not have chemical resistance or resistance to UV degradation.\nThe polymer is simply not formed unless it is baked sufficiently. Epoxies are available in both single-part thermosetting and two-part catalytic systems. Neither is suitable for exterior exposure beyond three to six months. Not that the film is going to fall apart or anything disastrous is going to happen - the film just chalks. The main usage in the metal sign industry is not an epoxy as described above, but a derivative with the backbone of epoxy and that would be as a primer.\nThe epoxy-ester is an alkyd formed with the epoxy backbone and is used primarily because of the tenacious adhesion and resistance to corrosion that epoxies have. In the appliance industry, epoxies are used because they have the gloss, resistance to chemicals, formability, flexibility, adhesion and heat resistance required for most applications. They exhibit excellent formability, much better than thermosetting enamels, and the single-part epoxy shows much better shelf stability than thermosetting enamels. Baking times and temperatures for thermosetting epoxies are not very different from those required for thermosetting enamels.\nOne of the primary reasons to go into ultraviolet curing of inks would be because of the almost instant drying, or curing, through cross-linking. These inks exhibit excellent chemical resistance as well as abrasion and heat resistance. However, the start of the art today leaves a lot to be desired in adhesion, formability - and because of these two drawbacks - durability.\nThe same UV ink may perform excellently on a piece of vinyl film or plastic, and may wear out your weatherometer, but on metal it may fail within 500 hours or less. These inks are now in general use on metal where the properties of formability and durability are not critical. I know several nameplate manufacturers who are designing nameplates using the UV system on metal. They may be doing something they are not telling us about, but they are completely satisfied.\nThere is a lot of work being done to improve the characteristics of UV ink on metal, but it is going to be a little while before all these problems are solved. In the meantime, there are conventional solvent-based primers or base coats that can be applied to metal that will accept and be compatible with UV inks so the UV inks can be applied over these primers or base coats.']	['<urn:uuid:83ec4e25-a35c-4b3a-99bc-e3b69e5f20d1>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	19	64	2883
27	wood bee carpenter bee and bumblebee colony structure social organization differences comparison	Carpenter bees and bumblebees have very different social structures. Carpenter bees are presocial, with only fertile females and males living together, sometimes forming working pairs of sister bees late in the season. They are non-colonial, with females sharing labor and caring for young. In contrast, bumblebees are eusocial insects with a more complex colony structure, featuring queens, male drones, and female workers. Bumblebee colonies have 50-400 individuals with clear division of labor, where queens and males are the only reproductive individuals while workers perform tasks like feeding young and foraging. Unlike carpenter bees, only bumblebee queens survive winter through hibernation.	"['Carpenter bees are their own specific category of bees, the genus Xylocopa, and they are not bumblebees, which have very different breeding and life cycles and social systems. Most species of carpenter bees have a glossy black abdomen and fuzzy thorax which may be honey coloured, black, or even bright electric blue, as in Xylocopa caerulea, which lives in southeast Asia. They are generally docile and non-aggressive, but the females are capable of delivering multiple stings, and they do not die when they sting, the way honeybees do.\nHere I wish to address a single specific species within the Xylocopa genus of bees, the Eastern Carpenter Bee, Xylocopa virginica virginica, which will be the only wood bee species encountered by most people living in North America. This is the species with which I have the longest acquaintance.\nEastern carpenter bees, ""wood bees"" to locals, are a presocial bee species, unlike the eusocial European honeybee. This means that, instead of having a single fertile queen, a few dozen male drones, and thousands of sterile female workers, the wood bee society consists only of fertile females and males who spend most of their lifespan together, divide labour, and to some extent take care of their hatched young. They are a non-colonial bee species, but late in the season after the males have died, sister bees will sometimes form working pairs, taking turns tunneling their nests, harvesting pollen for food, and protecting the nest. The generations of hatching bees are somewhat staggered, so that there may sometimes be a few hibernatory adult females in the nest along with the hatching larva, when the hatching begins, and these last lingering adults will only just begin to die out when the next generation is emerging from the pupal stage. The number of generations per season or year is limited by the local climate, and areas with a temperate autumn and winter will have fewer wood bees overall than more tropical zones.\nWood bees are the largest true bee species, and very few other hymenopterans are much larger- mostly hornets and ants living in tropical rain forest regions. The largest wood bee I have personally seen was nearly 5 cm long, and I initially mistook the sound of his wings for a hummingbird, or a helicopter built for Borrowers. He hovered directly in front of my face, investigating me, and upon verifying that the large creature was not a rival male, he took off in pursuit of an acorn which had fallen from a nearby oak.\nWood bee males are immense fun: they are stingless, they are easily identifiable by the large white spot on their forehead (a spot which the black-faced, sting-carrying females lack), and they are curious and have very poor vision by bee standards. This means that they will approach and investigate anything which moves, scaring the wits out of people frightened by bees, or chasing tossed pebbles and acorns. Two wood bee males facing off against one another is amusing, since neither of them has any natural weapons. They are all bluff, diving at one another in pint-sized games of chicken. Eventually, one leaves on his own accord, with no apparent indication why. Sometimes, they make such a fuss that nearby birds notice them, and one or both ends up on the menu for a flycatcher, shrike, or bee-eater.\nThe females have stingers, but it takes some serious provocation to incite a sting. A female wood bee\'s entire focus is on hollowing a piece of dry wood for her eggs, and leaving enough stored food available for the young when they hatch. For most of the breeding season, male and female wood bees will pair off monogamously, and the male will occasionally bring back plant pollen and nectar for the female, when he is not preoccupied with rival males. Male wood bees are among the most important pollinators in the United States Midwest, because the timing of their emergence tends to be earlier than the emergence of honeybees, meaning that they are able to pollinate flowers sooner in the season. The female bees, however, present some problems for the plants which the males pollinate: especially very late in the season, after the more fragile males have died off, the female bees will sometimes ""rob"" nectar from flowers, carving neat holes into the side of the flower and stealing the nectar from below, without ever interacting at all with stamens or pistil. The bee gets her meal, but the flower does not get pollinated, and now it has no nectar left to attract other pollinators. Eastern carpenter bees are not as severe robbers of nectar as most carpenter bee species, but this could be attributed to the abundance of red clover plants in the midwest, which have especially abundant and available pollen for bumblebees and carpenter bees, whose probosces are longer than honeybees\'.\nNectar robbing is not the only ecological problem offered by carpenter bees; the females also have pesky tendency to drill centimeter-wide, perfectly round holes into the undersides of wood beams on people\'s houses and barns. Every time a wood bee female creates a nesting tunnel, her daughters will continue using and deepening that tunnel for multiple generations, only changing locations when the current tunnel is too crowded by sisters from the same mother. The only way to completely eliminate this cycle, once it has begun, is to kill the entire local wood bee population at that nesting site, and this can only be guaranteed by coring out the nest tunnels and killing the larvae, or by completely replacing the damaged wood. There are alternatives, however, to using bee-killing chemicals which do not discriminate between bee species, and to replacing or carving into the wood on your front porch awning.\nIf you already have a wood bee problem at your house, your most immediately convenient option is to give them a better place to build their nests. Wood bees prefer unpainted, dry, unrotten wood over painted, wet, or rotten wood. They also prefer to avoid wood which has been treated for outdoor use. They create their tunnels almost exclusively on the undersides of wood, not on the flat sides which woodpeckers would be able to access easily. If you suspend a plank of wood horizontally someplace away from your house or other wooden structures, especially if the space is not directly exposed to rain and shear winds, then subsequent generations of wood bees will prioritize the plank over your house, which is inhabited and a high-traffic area which the bees would probably rather not inhabit anyway.\nAnother option is to make use of the abundant avian predators at your disposal: set out bird feeders to attract woodpeckers and other insect-eating birds. Bear in mind that woodpeckers will also do damage to wood, while seeking out their larval meal, and they won\'t restrict their pecking just to the nonliving wood which makes up your awning or the crossbeams in your barn; woodpeckers go for live trees, too, and they can do quite a bit of damage in large numbers. They are also damn noisy.\nIron Noder 2013, 13/30', '|Species||Over 250 species|\n|Niche||Temperate social stinging bee|\n|Length||Up to 1.6 inches (40 mm)|\n|Weight||Up to 0.03 ounces (0.85 g)|\n|Lifespan||Males & workers: 2 – 6 weeks, Queens: 1 year|\n|Conservation Status||Species vary from least concern to critically endangered|\n|Preferred Habitat||Temperate climates with suitable nesting sites|\n|Average Clutch Size||50 – 400|\n|Main Food Item||Nectar|\n|Predators||Badgers, predatory insects, birds & crab spiders|\nThe bumblebee is a group of over 250 species belonging to the genus Bombus. They are found mainly at northern latitudes in North America, Europe, Asia and northern Africa, as well as throughout South America. Bumblebees are characterized by their plump, furry appearance and vary in color from being entirely black to having bright yellow, red, orange, white, or even pink stripes.\nBumblebees are found at higher latitudes and altitudes than most other bees, although a few lowland tropical species do exist. In fact, two bumblebee species that inhabit Ellesmere Island in the high arctic have the northernmost range of any eusocial insect. Bumblebees are able to live in such cold climates thanks to a number of adaptations. For example, their thick pile of long bristles acts as insulation to prevent heat loss during winter. Bumblebees are also able to raise the temperature of their muscles by shivering and can absorb heat even from weak sunshine.\nBumblebees feed on nectar, which is a sugar-rich liquid produced by plants to attract pollinating animals. When bumblebees and other pollinators are foraging for nectar inside flowers, they brush against the reproductive organs of plants and pick up or deposit pollen, thus facilitating fertilization between different individuals. Nesting bumblebee females have specially-adapted pollen baskets on their hind legs, which are areas surrounded by hairs that are used to transport pollen back to the nest to feed bumblebee larvae.\nThe bumblebee has a long, hairy tongue called a proboscis, which is used to lap nectar from tube-shaped flowers. It is thought that the tip of the tongue acts as a suction cup, allowing nectar to be drawn up via capillary action. Different species have tongues of varying lengths, reflecting their adaptions to forage from different flower species. When bumblebees are in flight or at rest, the proboscis is kept folded up under the head.\nBumblebees are eusocial insects, which means they practice cooperative brood care, division of labor into reproductive and non-reproductive groups and have overlapping generations within a colony of adults. Division of labor creates a caste system in which queens and males are the only individuals to reproduce, whilst female workers carry out a number of tasks such as feeding young and foraging.\nQueens are the only bumblebee caste to survive the winter, but do so in hibernation underground. Once spring arrives, queens emerge and each find a place to build their colony, which may be underground in an old burrow or above ground in thick grass or a tree hole. Queens build wax cells in which to lay eggs that were fertilized before hibernation. These eggs develop into female workers, creating colonies that are typically composed of between 50 and 400 individuals. Unlike honeybees, bumblebee nests do not have an organized hexagonal structure, but instead consist of a random jumble of cells.\nAs a bumblebee colony increases in size, males and new queens begin to be produced. Although worker bees are capable of producing males by laying unfertilized eggs, queens suppress this behavior via physical aggression and pheromones in order to reduce reproductive competition. Queens are the only females that leave the nest to mate with males from other colonies, thus they are the only females able to lay fertilized eggs, which develop into queens and workers. Young queens mate in autumn before they begin feeding intensively to build up fat stores for hibernation. Males and workers die as the weather turns colder and queens begin their hibernation.\nFun Facts about Bumblebees\nBumblebees may look cute and fluffy, but their striking coloration hints at their ability to inflict a painful sting. This adaptation and others provide examples of a number of interesting biological concepts.\nAlong with all other members of the order Hymenoptera, bumblebee sex is determined by a system of haplodiploidy. In this system, unfertilized eggs only containing genetic information from females develop into haploid males. In contrast, fertilized eggs containing genetic information from both males and females develop into diploid females. The terms haploid and diploid represent the fact that males have half the number of chromosomes as females.\nHaplodiploidy is one factor proposed to have driven the evolution of eusociality in bumblebees and other species. This is because in colonies with one queen that mated with only one male, the relatedness between workers is 3/4. This is higher than the typical relatedness of 1/2 for siblings in species with other sex-determination systems. Thus, haplodiploidy is thought to favor kin selection, whereby the reproductive success of closely related relatives is favored over an individual’s own reproductive success as a strategy to propagate their genes in the next generation.\nThe bright color patterns of bumblebees are aposematic signals that warn other animals of their stinging ability. Different bumblebee species found in the same area tend to have similar color patterns, which is an example of Mullerian mimicry. This type of mimicry is adaptive as it means that other animals only have to learn one warning signal and will avoid all species that look like the one they were previously stung by.\nOther non-stinging animal species such as hoverflies and horseflies also exhibit similar color patterns to bumblebees, which is an example of Batesian mimicry. This type of mimicry is adaptive for non-stinging species as predators will avoid any species that looks like one that previously stung them, regardless of whether they are actually dangerous or not.\nThe Bumblebee’s Sting\nFemale bumblebees, including queens and workers, are able to sting repeatedly. Unlike honeybees, their stinger lacks barbs so is not left in the wound, thus allowing bumblebees to sting without inuring themselves. Although bumblebees are not aggressive, they may sting to defend their nest or if harmed. The ability to sting is also common in other members of the order Hymenoptera, including many wasp, ant and other bee species. This protective mechanism generally involves an injection of venom, which causes painful reactions of varying severity depending on the species.']"	['<urn:uuid:e78a3ff8-8e61-4fc5-b75a-9205171447f0>', '<urn:uuid:91f421f0-1565-4102-a311-30b297c58fd6>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T05:18:19.158519	12	100	2233
28	what are physical characteristics of black roof rat most common florida	Black rats or roof rats are 12 to 14 inches long, including the tail, which is longer than the body. They are great climbers and young rats can squeeze through any hole if they can fit their head inside.	"[""Poison a problem when our pests are someone else's dinner\nDuring winter, rodents make their way into the comfort of the warm, dark spaces in our homes. Poison baits and traps, which may seem like easy solutions, often have unintended consequences.\nOne of the telltale signs that you have rats or mice in your home is the scratching and gnawing sounds, especially at night.\nRodents have chisel-like incisors that grow continuously. Gnawing on hard nutshells and wood keeps their teeth trimmed. But they can also chew through walls and electrical wires.\nSt. Francis Wildlife frequently receives calls and emails asking for advice about how to evict these animals humanely.\nIt’s all about the food chain\nThe Southwest Florida Eagle Cam uses a camera perched 60 feet above the ground to give eagle lovers around the world a bird’s eye view of a famous nest in North Fort Myers. But this February, viewers were saddened by the death of a one-month-old eaglet.\nNecropsy results determined the cause of death was brodifacoum, a type of anticoagulant rodenticide or rat poison.\nAfter eating the rodenticide bait, rats and mice go back into the wild and can take more than a week to die. Weakened from internal bleeding, they are easy prey and a toxic ticking time bomb for any animal who preys on them.\nWhen I rescued a weak, emaciated Barn Owl, I took it immediately to North Florida Animal Hospital, but it died as soon as we arrived.\nDr. George Simmons said, “I bet I know what killed it.” He made an incision on the beautiful owl’s breast. The owl’s blood did not clot. “Secondary rat poisoning,” he said.\nOne sick rat is probably not enough to kill an adult Barn Owl, but after a few tainted meals, a lethal amount of poison can accumulate in its body. In the case of the tiny eaglet, a lethal dose would be much lower.\nBut walk into any home improvement or hardware store and you will find shelves stocked with these rodenticide baits as well as glue traps.\nBoth products are dangerous, inhumane, and lethal to non-target animals, including your own pets. Last year, St. Francis Wildlife received 18 animals — songbirds, owls, a hawk, and a gecko — stuck to glue boards or suffering from secondary poisoning. And these are just the ones we treated.\nAre there safe and effective ways to evict rats and mice?\nAbsolutely. Remove the attractants. Rake up fallen birdseed. Keep all perishable food in tightly closed canisters or in the fridge. Find out where rodents are entering, and seal up the entryway.\nRecently, I discovered a mound of sunflower seed shells on our laundry room floor. A critter had chewed through the plastic birdseed bin, and there were rodent droppings. We looked behind the washer and dryer, but could not find a hole.\nThen we set up a trail camera beneath the house. Inexpensive trail cams can be purchased today for less than $100. The next morning we had another mound of sunflower seed shells but also a movie showing exactly where the black rat had entered.\nThe black rat or roof rat is the most common rat in Florida. An exotic from southern Asia, it likely hitchhiked a ride on the first ship that sailed to the New World.\nThese rats are great climbers. Ours could have run along tree limbs then hopped on the house. Or maybe it was attracted by the seed falling from our bird feeders and then followed that scent to the source.\nAdults are 12 to 14 inches long, including the tail, which is longer than the body. Young rats can squeeze through a tiny hole. If it can fit its head inside, it can squeeze its body through.\nOur visitor had squeezed in and then shimmied up beside a water pipe. We filled in the small entrance hole around the pipe and covered the entire area beneath the house with quarter-inch, steel hardware cloth. Problem solved. No more unwelcome guests in our laundry room.\nIf rodents have already settled in your home or garage, humane live traps baited with peanut butter on a cracker can be effective. H.B. Sherman live traps, used by scientists worldwide, are manufactured right here in Tallahassee on Peddie Drive.\nFor sustainable rodent control, open a predator B&B in your backyard\nWe supplied the owl nest box, but they need to find their own breakfast. Screech, Barred, and Barn Owls all use tree cavities and nest boxes. One Barn Owl can catch 1,400+ rodents a year! Build your own from plans at: nestwatch.org.\nA tall snag provides a home for a raccoon family and a perch for Red-shouldered Hawks. We also put out the welcome mat for gray rat and red rat snakes (corn snakes) and have even rehomed a few beneath our house. Bon appétit, y’all!\nFor more ideas on safe and sustainable pest control check out RATS! (Raptors Are the Solution) at www.raptorsarethesolution.org.\nSandy Beck is the education director for St. Francis Wildlife. Contact her at firstname.lastname@example.org.\nNever miss a story: Subscribe to the Tallahassee Democrat.""]"	['<urn:uuid:cb737ca8-1fc5-4d26-b6ca-257556799cf1>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	11	39	851
29	what are venice bridges architectural significance and shoreline erosion processes effects	Venice's bridges hold significant architectural importance, particularly the Rialto Bridge from 1588 which connects two busy streets and a shopping district, and the Bridge of Sighs leading to the Prigioni prison cells. These structures are part of Venice's complex network of waterways spanning 118 islands. However, shoreline areas face continuous changes through various processes: sand movement occurs onshore during small wave action, offshore during large storms, along the shore due to longshore drift, and through wind transport. These processes can lead to shoreline retreat caused by multiple factors including glacial melting related sea level rise, changes in sediment supply, compaction and subsidence of wet sediment, and increased storm frequency.	"['Located in northeastern Italy, Venice is popularly known as the City of Canals. It has enthralled and amazed travelers from time immemorial. Unlike any other European city, it is built upon 118 islands and is interconnected via a network of waterways. To the outsider, it simply looks like a picture book, completely unreal, adorned with an appearance that is unchanged for over 600 years. It is a place where you can simply get lost without knowing where to begin your journey. Whether you choose to stroll by its canals or explore its secret corners, everything in this quaint little city is worth mentioning.\nLet’s Find Out 5 Amazing Tourist Attractions Of Venice:\n1. Grand Canal\nIn a city that is dominated by its waterways and where cars are completely banned, the canals are its lifelines. The Grand Canal is the main boulevard that meanders through the heart of the city.\nTake a gondola and ride through its waters to discover Venice in its full glory.\nLined with structures influenced by varied architectural influences ranging from Renaissance and Gothic to Romanesque, a tour of the Grand Canal will take you through a 3.8 kilometer stretch that connects Piazza San Marco with the mainland bridge and the rail station. On your tour, you can also see well preserved remains of grand palaces that once was the hub of royalty in Venice. Only four bridges cross the Grand Canal including the famous 16th century Rialto Bridge.\n2. St. Mark’s Basilica\nDating back to the 9th century, St. Mark’s Basilica is one of Venice’s most famous tourist attractions. Situated in the city’s principal thoroughfare, it consists of a 30 story Campanile and a huge basilica.\nThe unique mix of Byzantine and Gothic architecture is evident in every façade of the basilica and is a fine statement of the wealth accumulated by the military prowess of this fascinating city.\nThe cathedral’s walls and vault is covered with incredible mosaics from the medieval times and the jewel adorned gold screen of Pala d’Oro, an altarpiece located behind the tomb of St. Mark is considered one of the best depictions of Byzantine craftsmanship.\n3. Rialto Bridge\nCrossing the Grand Canal midway, the Rialto Bridge is the connection between two busy streets and a shopping district. Built in 1588, it is an architectural icon of Venice. The stone bridge is very famous amongst tourists and you will always find happy travelers posing on the bridge, ready to collect memories from their Venice trip. It is also a great place to pick up a few souvenirs from the shops in the adjoining area.\n4. Palazzo Ducale Or Doge’s Palace And Bridge Of Sighs\nPalazzo Ducale or Doge’s Palace was once the city’s center of power. The magistrates or doges, as they are referred to in Italian, used to rule Venice and this palace served as the doge’s residence.\nIt is an extraordinary architectural feat and visitors are awed by its opulence and lavishness.\nThe Porta della Carta, the gold vaulted Scala d’Oro and the Sala del Collegio are some of the finest chambers in the palace and have awed travelers from all over the world. Inside, you can see the works of famous artists such as Tintoretto, who made the famous ‘Paradise’, the world’s largest oil painting. And if you opt for a private tour, you are allowed to walk through the Bridge of Sighs which leads to the prison cells of Prigioni.\n5. Piazza San Marco or St. Mark’s Square\nA city dominated by canals and waterways hardly has a public square and this holds true for Venice as well. The trapezoid shape Piazza San Marco or St. Mark’s Square is therefore, the city’s only public square which has been the meeting place for centuries and acts as an epicenter to any tour of Venice.\nLocated in close proximity to St. Mark’s Basilica and Palazzo Ducale, it is a place that is always buzzing with activity.\nIt is a place which everyone passes by, whether it is the tourist enjoying Venice or an office goer fulfilling his daily duties. The area is perfect for strolls, sipping on a cup of coffee or enjoying a drink with friends. The sides of Piazza San Marco has been framed on three sides and it is lined with numerous cafes and restaurants that provide the perfect opportunity to enjoy the charming atmosphere of Venice. Hidden behind the framed arcades lie fashionable shops selling Italian designer clothing, bags, jewelry and many more coveted items. And if you are interested in viewing this busy street from the top, go to the Torre dell’Orologio.', ""Environmental Geology lecture outline\n- Shorelines and related environmental concerns. Some answers\nWhat are general\nenvironmental concerns associated with shorelines?\n- floods associated with large cyclonic\nstorms (e.g. hurricanes), and especially the damage caused by the storm surge (one of the biggest risk factors).\n- long term shoreline erosion/retreat (especially during large storms).\n- water quality and saltwater incursion\n(water, water all around and not a drop to drink).\n- maintenance of important biotic systems\n(e.g. spawning grounds for fisheries).\n- sites of damage from oil spills.\n- stability of navigation channels.\nStunning image of result of washover of Dauphin Island during Hurricane Katrina in 2005. Photo source: USGS - http://coastal.er.usgs.gov/hurricanes/coastal-change/overwash.html\nThe influence of three hurricanes on a barrier island complex. Image source USGS: http://marine.usgs.gov/coastalchangehazards/research/storm-impacts.html .\nEstimated recurrence intervals for hurricanes for different parts of the coast. Image from NOAA - http://www.nhc.noaa.gov/climo/ .\nWhat basic forces\nare involved in shaping shorelines?\n- waves: wave size and wave base, wave refraction,\nlongshore drift and movement of sediment.\n- tides: tidal range as indicator of power\nto move sediment.\n- rivers: supply of sediment, deltas, fresh\n- mass wasting: sea cliff retreat.\n- sea level change: local and due to tectonism\nvs. eustatic (global).\n- storms: hurricanes to storms.\n- biologic activity: reefs and mangroves\n- the effect of day to day events vs. that\nof large magnitude, infrequent events?\nAmount of subsidence\nfor a few major cities:\n- Venice, .22 m subsidence over 150 km2, high tides can flood the plazas, large\nflood gates being built.\n- Shanghai, 2.63 m over 121 km2.\n- Tokyo, 4.5 m over 3000 km2.\n- Houston, 2.7 m over 12,100 km2. Due in some\nlarge part to oil withdrawal.\n- New Orleans, 2 m over 175 km2.\n- source: Dolan, R. and Goodell, H., 1986,\nSinking Cities, American Scientist, vol. 74, p. 38-47\nHow do you determine\nand what are rates of shoreline retreat?\n- Consider that retreat during small storms versus big\nstorms is of course different.\n- Thus, your rate is time scale dependent meaning that you get different answers depending on the time frame you are measuring it over. There can be a problem\nof short sampling spans under representing the long term rate.\n- Air photos over time are enormously useful for gauging the changes.\n- Rates on east coast can be on the order\nof feet per year over the last century.\n- Rates of Great Lakes shoreline retreat are locally as much as 10 m/yr (Rahn, 1989). This is another important consideration - rates will very spatially, and can be much higher in one place than another.\nShoreline before Hurricane Ivan along part of the Chandeleur Islands off the Louisiana coast.\nIslands after Hurricane Ivan. Note lighthouse for position. Photo sources (and more information): USGS sitehttp://www.nwrc.usgs.gov/hurricane/postivanphotos.htm\nHow does sand\nmove in shoreline setting?\n- onshore during small wave action (visible\nbeach often grows in width in the summer).\n- offshore during large storms (storm set-up\nand bottom return flow is a primary mechanism).\n- from barrier island into lagoon during\nstorm break through and washover fan development.\n- along shore due to longshore drift.\n- back and forth through tidal channel.\n- from lagoonal and terrestrial environment\nto barrier island where trapped by vegetation.\n- by wind transport, which can be quite significant. Vegetation traps such wind-blow sediment.\nSchematic cartoon of sources that produce sediment and of processes that move sediment around in a low-slope coastal environment.\nHow can barrier\nisland morphology change with time?\n- whole complex can shift landward or seaward (see images of Dauphine Island above).\n- the complex can disappear or accrete to\n- tidal channels can migrate in the direction\nof longshore drift.\n- new tidal channels can get formed during\n- tidal channels can be clogged and closed\nby long shore drift.\n- storms can break through and form spillover\n- basic conclusion: these are dynamic places.\nWhat are possible\ncauses of shoreline retreat?\n- glacial melting related eustatic (global sea level) rise.\n- change (decrease) in sediment supply to shoreline due\nto sediment capture by dam basins.\n- compaction and subsidence of wet sediment\nbelow at depth.\n- change (increase) in storm size frequency and shore\n- local tectonic subsidence (sinking crust).\nare engineering responses to shifting shorelines.\n- groins and long shore drift.\n- breakwaters: designed to provide relief\nfrom the waves\n- disturbs sediment flow, sediment collects\nin back of it.\n- Santa Barbara built one in 1930, Miramar\nBeach Hotel 5 km down drift direction sued city.\n- response is often a distinct narrowing\nof beach (increase offshore transport of sand).\n- must be deeply anchored\n- jetties: designed to stabilize navigation\n- beach nourishment: trucking or pumping sand to the beach.\n- how long does it last.\n- cost as a subsidy to those living on coast.\nGroins, rip-rap and seawall employed to stop erosion. Image source USGS site: http://coastal.er.usgs.gov/coastal-classification/figures/fig11.html\n© Harmon D. Maher Jr..\nThis page may be used for non-profit educational purposes. For\nany other use please contact me.\nReturn to Environmental\nGeology course index.\nReturn to Harmon's""]"	['<urn:uuid:198b68b5-942a-4d6f-b0bd-863631e1deb9>', '<urn:uuid:c3c1f443-b0b8-4c93-a0dd-d7723a88501f>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	11	109	1621
30	I keep hearing about new car technology - what innovations are happening with car sensors, and how are they making parking easier these days?	A key innovation is sensor fusion, which combines information from multiple systems in vehicles. There's also increased installation of sensors for engine management and HVAC systems. In parking applications, these advanced technologies include ultrasonic detectors and guidance sensors that provide real-time information about vacant spaces through display screens and mobile apps. The system enables ticketless parking services, allowing drivers to pay online through their phones without leaving their cars, while also providing parking facility managers with usage reports for space optimization.	"['The global automotive sensors market generated the revenue of $25,723.8 million in 2019. Furthermore, the market is projected to advance with a CAGR of 7.4%, during the forecast period (2020–2030), and generate $58,215.3 million revenue in 2030. The powertrain sensors application category in terms of volume dominated the market in 2019.\nHowever, the ADAS application category is expected to generate the fastest growth in the automotive sensors market during the forecast period. At present, these sensors find their major application in level 1, 2, and 3 autonomous vehicles, and with the introduction of level 4 and 5 autonomous vehicles in 2023 and 2025, respectively, the demand for these sensors is expected to spur up significantly.\nTechnological advancement Is a Major Trend in the Market\nOne of the key trends witnessed in the automotive sensors market is the ongoing technological advancements. For instance, the concept of sensor fusion has gained much popularity in the automobiles. Sensor fusions are intended to combine the best information available from each of their systems installed in the vehicles. Moreover, the increasing installation of electronic devices in the vehicles has fueled the need for various automobile sensors for engine management systems; heating, ventilation, and air conditioning (HVAC) systems; and other application areas.\nIncreasing Adoption of ADAS Sensors Is Driving the Growth of the Market\nThe increasing demand for advanced driver-assistance system (ADAS) sensors is a major driver for the growth of the automotive sensors market. With the growing adoption of autonomous vehicles of different levels, these sensors are becoming more and more crucial for the vehicles. Autonomous vehicles have several functions, such as speech recognition, eye tracking, driver monitoring, virtual assistance, gesture recognition, and natural language interface, which are highly dependent on advanced automotive sensors. Further, the development of the ADAS, which includes camera-based machine vision systems, driver condition evaluation systems, sensor fusion engine control units (ECUs), and radar-based detection units, creates higher demand for ADAS sensors in the market.\nSegmentation Analysis of Automotive Sensors Market\nGeographical Analysis of Automotive Sensors Market\nAsia-Pacific (APAC), led by China, was the most dominant region in terms of volume in the automotive sensors market. Moreover, the region is expected to witness the fastest growth as well during the forecast period. The dominance of China is buoyed by its high production capacity as well as its continuous implementation of automotive innovations in terms of electric and autonomous vehicles. These vehicles require advanced and greater number of automotive sensors in the vehicles, which is a major driver for the market.\nCompetitive Landscape of Automotive Sensors Market\nThe global automotive sensors market is moderately fragmented in nature, with the top six players accounting around 60.0% of the total market share in 2019. Among the major players, Robert Bosch GmbH led the market in 2019. Moreover, the historical trend exhibits that the company consistently remained the market leader between 2015 and 2019, with significant fluctuations in share over the mentioned period.\nBrowse report overview with detailed TOC on ""Automotive Sensors Market Research Report: By Type (Pressure Sensor, Temperature Sensor, Position Sensor, Oxygen Sensor, Motion Sensor, Optical Sensor, Torque Sensor, Gas Sensor, Level Sensor), Application (ADAS, Powertrain, Chassis, Body), Technology (MEMS, Non-MEMS), Vehicle Type (Passenger Car, Commercial Vehicle), End Use (OEM, Aftermarket) – Global Industry Share, Size, Growth and Demand Forecast to 2030"" at:https://www.psmarketresearch.com/market-analysis/automotive-sensors-market\nIn December 2019, Continental AG introduced a contact sensor system that is able to pick up the sound signal of low-speed impacts and detect scratches and damages on the vehicle. The new sensor would improve the safety in automated parking, thereby assisting in the development in self-driving vehicles. Furthermore, in April 2019, NXP Semiconductors N.V. entered into a strategic collaboration with HawkEye Technology Co. Ltd. to develop autonomous vehicle in the Chinese market. Under the partnership, HawkEye Technology Co., Ltd. would offer its deep expertise in 77 GHz radar sensor to NXP Semiconductors N.V., in order to develop 77 GHz radar solutions for the Chinese automotive market.\nSome other players operating in the market are DENSO CORP., Valeo SA, Panasonic Corp., OmniVision Technologies Inc., ON Semiconductor Corp., Sensata Technologies Holding plc, TE Connectivity Ltd., NXP Semiconductors N.V., Analog Devices Inc., Continental AG, Delphi Technologies PLC, Infineon Technologies AG, Melexis NV, and Allegro MicroSystems LLC.', 'Dubai is famous for its advanced car parking management and guidance systems that keep an accurate tracking of large parking spaces like the ones at the airports, shopping malls or the dedicated parking facilities. This computerized system of monitoring the occupied and vacant spaces saves a lot of time and money, as no staff is needed to check the information and guide the drivers through the parking lot, the location of the vacant spaces automatically appearing on the screens.\nThere are various car parking systems that are used in the UAE, especially in crowded places in Dubai and Abu Dhabi. These include car parking management systems, automatic number plate recognition systems and parking guidance systems. These provide an easy and effective parking solution, as the sensor system transmits real time information about the available spaces and their location, directly to the screens located at the entrance and throughout the parking lot, so the incoming drivers can go directly towards them, avoiding congestion. The data is also transmitted to the central parking monitoring system to show the overall situation of the vacant spaces and to create reports of the parking facility usage, that are useful for managers in deciding whether there is a need for expanding or modifying the space.\nThe latest technology in parking systems is the use of mobile phone apps that allow you to get online real time information about the nearest available parking spaces as well as to pay for the parking, where is the case. This is the most efficient method and can save you a lot of effort and time.\nCar parking systems suppliers in the UAE provide all the software and equipment needed to implement modern and time saving solutions in the parking facilities of commercial places or institutions, as well as in private parking lots.\nDepending on the parking system needed or the available budget, the necessary equipment might include ultrasonic detectors, indoor display screens, outdoor display screens, LED indicators, node controllers, central controllers, main control stations, entry tickets dispensing machines, traffic barriers, exit manual cashiers, single space detection sensors, guidance sensors, automatic license plate reader and cameras.\nThese advanced parking systems eliminate the frustration of finding an available parking place and minimize queuing and congestion at the entrance and exit of a parking lot. Also, the option of checking for the nearest parking that has vacant spaces on a mobile app is very useful, as well as the possibility of paying for parking services online, with a credit or debit card, by just tapping your phone, in the comfort of your own car, without getting out and queuing at the paying kiosks. The license number plate information, together with your personal information and your payment details can be safely saved in the app for an even easier and faster payment.\nTicketless parking services are the best option for a business, since there is a reduced cost of equipment, consumables and maintenance and the overall client experience is improved, and for the drivers, because there is no need to get off the car and they can just pay online and leave.']"	['<urn:uuid:a250b0fe-98f4-4cfa-ba75-f96c3ee8901a>', '<urn:uuid:40d3dcaa-1b5b-494a-993a-2ab678864d34>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	24	81	1222
31	ruston gallery owner unusual creation request	They have received requests for funeral urns, and once created a pyramid for a math professor.	['Hands on the Wheel\nGonzales potter Kent Follette’s handcrafted life\nTo say that Kent Follette’s pottery is built on the foundation of south Louisiana’s timeless love for cooking and communal eating is as true as stating that the Mississippi River is muddy, but as far as influences go, Japan might hold the most sway over the veteran ceramicist’s work.\nThat’s where Follette, originally from New Orleans, spent the early 1950s, living in a traditional, 200-year-old Japanese otaku. Follette’s father was stationed there in what was then-called Washington Heights, a housing district for families of servicemen stationed in post-V-J Day Tokyo.\nGrowing up next to a geisha house and traversing the same parade grounds where the Imperial Army held marches just a few years prior, Folette says he first noticed textures and color as well as the way history married with art and how both could blend to form something altogether new.\nThe startling differences between Japan and the United States jolted his attention to design aesthetics for the first time.\n“I was in awe,” he says. “It was so different from America, from the people and the food, to the street markets, to the architecture — all these details were coming at me for the very first time.”\nAt elementary school in Tokyo, an artisan would visit his class each week to explain how a certain trade worked. One week it was an umbrella maker. The next, a carpenter, and so on. And during a visit to Camp Washington’s on-base hobby shop filled with sundry housewares, gadgets and diversions, Follette decided that whatever path he walked in life, he wanted to make things.\n“I take pleasure in being a craftsman,” says the 68-year-old potter, husband and father of two. “Art is one thing, but really I just think of myself as a guy who likes to work with his hands.”\nFollette’s hands are put to use at the potter’s wheel, and have been daily since leaving academia in 1980. From his home studio he produces roughly 50 pieces each week, just enough to keep up with demand.\nInspired by his father, who also cooked, Follette’s work is a blend of beauty and utility, a variety of multi-colored vases, mugs and pots bathed textures and inks. An ikebana wish bowl even celebrates his time spent in Japan.\n“You never really know how pieces will turn out until they are fired,” he says. “It’s a secret society what happens in the kiln.”\nWe visit in early November, when Follette’s focus is on the five festival shows where his creations will be featured and for sale before Christmas. It’s the edge of the holiday rush, but he is always ready.\n“I like the challenge of it — the craft of pottery,” Follette says. “It’s hard work. This is not magic. And honestly, I get excited every morning to create because a day doesn’t go by that I don’t learn something new.”\nPottery was brand new to Follette — who first attended Louisiana Tech as a painter — when he met his future wife, Libby, on campus in the late 1960s. Their courtship was swift, and after many years of marriage and operating a huge studio and gallery in Ruston, they moved to Gonzales to be closer to their daughter and grandson.\nFollette used the move to refine his purpose as an artist. He realized just how much he valued being a part of his customer’s everyday lives.\n“I wanted to simplify what I was doing,” Follette says. “No gallery commitments; sell directly to my customers; go to festivals and markets and meet the people.”\nIn 1994, Libby retired from her interior design career and her work in the family lumber business and began working with Kent full-time. An artist in her own right, she serves as a second set of eyes on all output, especially the elements of color and design. She’s “The Opinionator,” Follette jokes.\n“Some days she’ll look at my work and go ‘This is the best thing you’ve ever made,’ and other times it’s more like, ‘OK, let me go find a hammer’.”\nWhether a piece is successful or not — and most are — Follette enjoys the process of refining, because his career has become as comfortable to live in as his pottery is to hold and to use.\n“When I first began making pottery, I never thought of it as a career or a new direction for me,” Follette says. “It just felt like ‘this is who I am.’ It felt natural.”\nWhat do you do for fun when you are not working? I love to fish. It really is the best medicine for a potter.\nHow do you stay inspired in your craft after so many years? I stay inspired by looking at all types of pottery and using it as a starting point of my interpretation.\nWhat’s the oddest or most interesting thing someone has requested that you make for them? We have requests for funeral urns. Once we created a pyramid for a math professor.\nIf you hadn’t gotten into pottery and found that people responded well to your work, what do you think you’d be doing now? I was lucky to have discovered clay and pottery. At the time I was doing a lot of painting and had just discovered the welding torch and steel. If I wasn’t working in pottery, I could see myself as a sculptor of some other kind. I would definitely be involved in the arts. I’m just not sure I could work at anything else.\nThis article appears in the January-February 2017 issue of Louisiana Life.']	['<urn:uuid:c1d599e5-6def-4a1b-b9be-e8549f227683>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:18:19.158519	6	16	937
32	business intelligence data analysis tools benefits ethical considerations privacy data protection	Business Intelligence systems offer sophisticated data analysis tools that provide historical, current, and predictive views of business operations through tools like Business Activity Monitoring, Competitive Analysis, and Multidimensional Analysis. While these tools enable organizations to make informed decisions and improve operations, they must be implemented with strong ethical considerations around privacy protection. Organizations must obtain informed consent from participants, ensure data confidentiality, and use the collected information only for stated purposes. The data analysis must also be conducted with transparency and objectivity to avoid bias or manipulation of findings.	['Data Analysis is a complete and thorough review of all of the information being collected and warehoused by an organization. This is most commonly done by organizing and examining the data collected with the help of charts, graphs, and tables.\nThe data is processed in order to draw conclusions about the way an organization is operating and aid that organization in coming to a sound decision regarding their business. By dissecting and organizing data an organization is able to pin-point problem areas and move forward with the required adjustments accordingly. Many businesses have turned to the sophisticated Business Intelligence systems to handle their data analysis needs.\nThe organizations that have started using these systems have a tendency to be the leaders in their specific area of expertise. The Data Analysis tools that Business Intelligence systems offer are some of the most superior of their kind. It is no surprise that Business Intelligence systems are beginning to gain so much exposure.\nData Analysis in Business Intelligence\nData Analysis is a huge part of Business Intelligence. Business Intelligence programs are designed to collect the past as well as the present data of an organization, whether it is financial, productivity data, customer data, trend data ,or any other data company related data that needs to be modified in order to be clear and easy to follow.\nThe next step is to thoroughly analyze the data that has been collected. Some companies prefer to use charts and graphs others may use tables. No matter the means in which the data is analyzed it is done in a precise and detail oriented manner. Figuratively speaking, no stone is left unturned.\nModern Business Intelligence systems are designed to analyze and cross analyze large amounts of unstructured data. This can range from production metrics to customer attrition figures. These systems provide historical, current, and predictive views of business operations. The systems house data analysis tools that can be applied to virtually any area of an organization, and present the data in a clear, concise manner.\nBusiness Intelligence Systems Data Analysis Tools\nThere are many different tools used to analyze and organize data collected by the Business Intelligence systems. Depending on company needs Business Intelligence vendors design the systems to benefit that company and meet they company’s needs. Some of the tools included in a Business Intelligence system are; Business Activity Monitoring, Competitive Analysis, Finance and budgeting, Trend Analysis, and Multidimensional Analysis, and that is just to name a few.\nBusiness Activity Monitoring aids in monitoring business activities. Specifically it refers to the aggregation, analysis, and presentation of real time information about the activities within the organization. Normally it focuses on the activities surrounding the customers and partners of the company.\nThe goal of Business Activity Monitoring is to provide real time information regarding the status and results of various operations, processes, and transactions. A benefit from using Business Activity Monitoring is to enable an organization to make informed business decisions so that they can provide better services or even generate larger revenue.\nLet’s take a closer look at Competitive Analysis. An organization needs to be fully aware of their competition so that they know what if anything needs to be adjusted in their own company in order to remain a true competitor. Competitive Analysis or Competitor Analysis allows a company to assess the strengths and weaknesses or their competitors, whether they are current or even potential competitors.\nThis tool focuses on four key aspects, competitor’s objectives, competitor’s assumptions, competitor’s strategy, and competitor’s resources and capabilities. This data can provide an organization with clear idea of exactly what they are up against and help them to formulate a concrete plan to aid them in moving ahead of their competitors.\nBusiness Intelligence Systems Data Analysis Tools\nPerhaps one of the most important aspects of data analysis is the Finance and Budgeting tools. These tools deal with the management and control of a company’s finances. They allow an organization to view what areas need more financial focus then others and what areas are receiving too much.\nBy budgeting its finances an organization will be all around better equipped for unexpected expenses and rise of cost within the business landscape. This tool can also help a struggling company better distribute its funds and improve the value and financial well being of that business.\nTrend Analysis refers to collection information regarding patterns or trends in the information. While trend analysis is mostly used to predict future events, it could be used to estimate unclear events in the past as well. This tool gathers data spanning over the period of a few years and then searches for patterns within that data. This ultimately will help an organization to make an informed prediction about future events and decisions on how some less desirable patterns can be broken.\nFinally we will look at Multidimensional Analysis. This tool is used to group data into two main categories, data dimensions and measurements. A data set consisting of information for a number of years would be considered multidimensional. An example of this would be if information about an organization’s productivity was gathered for three or more years, the data set would be considered multi-dimensional because it contains more then a single year of data. This tool is helpful when a company wants to analyze data from several years or terms in order to get a better idea of where they have been successful and what areas need more fine tuning.\nData Analysis General Opinion\nMore and more organizations are relying on Business Intelligence to perform their Data Analysis. The day of the stressed out little guy trying to keep all of his drawn up charts and graphs in order are nearly gone. Managers and executives have found that the software applications designed by Business Intelligence system vendors, can perform the task of data collection and analysis in less time and more thoroughly then previous strategies.\nThese programs are capable of sorting, processing, and analyzing data from several years in the organizations past to the present, as well as make more precise predictions about future outcomes relating to that data. It is no big surprise that most companies who can afford to invest in such a system are beginning to sing its praise. Yet some still remain skeptical, this is largely due to confusion and misinformation about how Business Intelligence systems work.\nBusiness Intelligence Data Analysis benefits for Organizations\nLarge companies need to gather massive amounts of data for processing and analysis in order to make decisions regarding their organization’s present and future needs. To do this without the aid of a Business Intelligence system would be not only a near impossible task, but one that would be very time consuming. When you consider the importance of time, relevance, and thoroughness these organizations would benefit most from the Business Intelligence systems that provide Data Analysis tools.\nStill even mid-size organizations can benefit from these tools. No business is too small to need to keep an up-to-date record of their organizations data. Organizations ranging from schools to multi-million dollar enterprises can use data analysis to their advantage. These systems allow such organizations to remain fully updated on most important areas of their companies. So when we look at the big picture we can safely assume that today’s Business Intelligence had greatly improved the data analysis process.', 'Importance of ethics in marketing research. What Is Ethics in Research, And Why Is It Important? Research Ethics Explained 2022-10-03\nImportance of ethics in marketing research Rating:\nEthics play a crucial role in marketing research as they provide a moral framework that guides the conduct of researchers and the collection, analysis, and communication of data. Marketing research involves the gathering and analysis of data about consumers, markets, and competitors for the purpose of understanding and predicting their behavior. It is a powerful tool for businesses to make informed decisions about product development, pricing, promotion, and distribution. However, the gathering and use of this data also raise ethical concerns about privacy, confidentiality, and the potential for manipulation or abuse.\nOne of the main ethical considerations in marketing research is the protection of personal privacy. Marketing research often involves the collection of sensitive personal information, such as demographics, income, and purchasing habits. This information is valuable to businesses but can also be misused or abused if not handled responsibly. To protect the privacy of individuals, researchers must obtain informed consent from participants, ensure that the information is collected and used only for the stated purposes, and maintain the confidentiality of the data.\nAnother ethical concern in marketing research is the potential for manipulation or bias. Marketing research aims to provide accurate and unbiased information, but there is always the risk that the researcher may have a vested interest in the outcome of the study or may unintentionally introduce bias into the research process. To minimize these risks, researchers must adhere to strict standards of objectivity and transparency, disclosing any potential conflicts of interest and ensuring that the research design and data collection methods are rigorously tested and validated.\nFinally, it is important for researchers to be transparent and accountable in their communication of research findings. Marketing research is often used to inform business decisions, and it is essential that the results are accurately and clearly communicated to decision-makers. Researchers must be transparent about their methods and any limitations of the study, and they must be prepared to defend their findings and respond to criticisms or questions.\nIn conclusion, ethics play a critical role in marketing research, as they ensure that the research is conducted in a responsible and respectful manner and that the results are used ethically and transparently. By following ethical principles, researchers can build trust and credibility with their clients, participants, and the broader community, and contribute to the development of more informed and effective marketing strategies.\nThe Importance of Ethical Marketing\nThe involved set of guidelines and rules leads to a purely good, organized roadmap for everyone to follow. Ray Poynter Importantly, we all work cooperatively to develop codes that complement each other. Rebecca Clay, Advertising to children — Is it ethical? Shills are primarily for message-delivery, but they can also be used to drive up prices in auctions, such as Ebay auctions. The company said that it was gathering the data to provide to universities to help the students gain admission and financial aid. Such a situation, with three major players the producer, the customer and the market researcher often sets the stage for conflicts of interest which can give rise to ethical problems.\nUsually, one will find examples that represent the extremes of the situation so that a compromise can be reached that will hopefully include the wisdom gained from the previous situations. Note that this theory is not necessarily concerned with the welfare of others. They should avoid advertisements that promote undesirable customer behavior or hurt, confuse, and mislead consumers. The concentration is even more pronounced in the United States, where the 10 largest firms account for 64 percent of total U. Paolo Macchiarini guilty of fabricating research results and sentenced him to 16 years in prison.\nThere are quite many layered facets over the factor of confidentiality as various external factors such as competition from the other brands in the The research department 5 Understand the perils of the internet : The era of the internet and social media has its own merits and demerits and the researcher and his team have to be extra careful in understanding the perils of the same. The client then attempts to get the supplier to conduct the more elaborate study at the lower price. Along the same line of thinking, this theory assumes that the results of the current ethical dilemma will be similar to results in the examples. Allowing Subjectivity into the Research : Research suppliers must avoid using biased samples, misusing statistics, ignoring relevant data, and creating a research design with the goal of supporting a predetermined objective. Ethical misconduct poses a considerable risk.\nWhat Is Ethics in Research, And Why Is It Important? Research Ethics Explained\nIt might cause significant harm to animals, including human subjects, the public, and students. In terms of sustainability, we need to work out how we can make a difference and focus on that. To achieve sustainable growth, companies should commit to ethical marketing practices. This point system provides a logical and rational argument for each decision and allows a person to use it on a case-by-case context. The more we govern our profession towards high ethics and adherence to doing no harm, the less likely the government will feel the need to write laws that force governance. Companies today have more access to The. Negative Advertising Techniques, Such as Attack Ads: In negative advertising, the advertiser highlights the disadvantages of competitor products rather than the advantages of their own.\nEthical behavior is not, however, a one-way relationship. Marketing ethics, regardless of the product offered or the market targeted, sets the guidelines for which good marketing is practiced. The researcher still has an obligation to the respondent. Embrace, extend and extinguish ii. However when new rules are invented which shift power too suddenly or too far, other participants may respond with accusations of unethical behaviour, rather than modifying their own behaviour to suit which they might not be able to anyway. Pricing Ethics : List of Unethical Pricing Practices: i. Black Box Branding : Marketing research suppliers have discovered branding.\nThe language and expectations of data privacy have been updated, including an evolution from using consent as a foundation for activities to having privacy by design be the guiding principle. Marketing ethics fosters fairness and honesty in all advertisements. Power may be concentrated with the producer caveat emptor , but factors such as over-supply or legislation can shift the power towards the consumer caveat vendor. With companies having an unprecedented ability to collect and store massive amounts of customer data, the importance of ethics in marketing research has never been higher. Will give respondents the opportunity to refuse to participate in the research when there is a possibility they may be identifiable even without the use of their name or address e.\nMarketing Ethics: Meaning, Importance and Examples.\nTo this end, it recently enacted the following code of ethics: Companies Engaged in Data Collection : 1. In such cases, perhaps the judgement of the consumers will be the deciding factor. Unsolicited e-mail must not be sent to those requesting not to receive any further e-mail. She follows a balanced approach for the quality of content and its marketing. Importance of Language in Market Research: It goes without saying that language is the single most important factor affecting the ethical practice of market research. Most researchers may be wondering why they need extensive training in research ethics.\nA person who would like to terminate an interview or experiment may give short, incomplete answers or even false data. Conscious practices Companies may decide to engage in conscious practices to protect communities and the environment. Dozens of researchers commit ethical lapses, but many go unnoticed and unpunished. Marketers can uphold this principle by educating, protecting the privacy of, and respecting the civil and human rights of consumers. So, what is ethics in research, and why is it so crucial in research? This subject is an integral part of many marketing team conversations, planning meetings and strategies. Today an advertiser who fails to tell the truth not only offends against morality but also against the law.\nWhat is the Importance of Ethics in Research? 8 Reasons Explained\nThis uncertainty can lead to unexpected results, making the utilitarian look unethical as time passes because his choice did not benefit the most people as he predicted. Often, it is necessary to disguise the name of the research sponsor to avoid biasing the respondent. Openness: You have to be open to suggestions, new ideas, and criticisms whether constructive or not. It is only normal to discuss the ethical side of every research even before work commences. Having a preference per se is not unethical.']	['<urn:uuid:27b0bf0e-68a9-4160-8aed-45928dbd3350>', '<urn:uuid:09b5f561-b10d-4bc9-b8a0-d1d101337160>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:18:19.158519	11	89	2683
33	current musical style composition comparison matthew hindson amythyst kiah	Matthew Hindson incorporates death metal and techno elements into his classical compositions for symphony orchestras and ballet companies, while Amythyst Kiah combines blues rock with traditional music, particularly featuring arrangements of songs from traditional singers like Vera Hall in a blues-dance-rock style with her band Chest of Glass.	"['“Thanks @wollconmusic – you saved my teenaged years, I tell you that! Such an invigorating musical environment. It set me up for my entire musical future.”\nWe don’t think you can get a better testimonial than that. Professor Hindson is one of Australia’s great modern-day composers currently based near Geelong, Victoria where he composes musical masterpieces in his home studio that are performed world-wide.\nHe has composed music for all the symphony orchestras in Australia and many ballet companies across the world and is currently working on a ballet that is premiering in the U.S in March 2022.\n“I’ve worked really hard to get where I am and I love what I’m doing. I’ve got music in my head,” he says.\nHow it began at WollCon\nMatthew started learning violin at the age of 5 from Hiroko Primrose, behind the Wesley Church in Crown St. Wollongong – when the Wollongong Conservatorium of Music first began.\nDuring his time at WollCon three people stood out for Matthew. First, there was Mrs Powell, who took theory and musicianship classes.\n“How she managed to teach so many for so long is beyond me. She was a very nice person and very thorough, as well. I still have the handwritten report from one of my Music Theory exams from that time.”\nThere was also Mr John Stender, who conducted the Wollongong Conservatorium String Orchestra.\n“We did a lot of challenging repertoire with him and being in the orchestra engendered a real sense of community. I also did Chamber Music classes with him. Again, I don’t know where he got the patience for all of it from.”\nAnd finally Mr Powell, who was the CEO of the Wollongong Conservatorium when Matthew was deciding what to do with his future in music.\n“He was my first composition teacher and I still remember his lessons. It just goes to show that as a teacher, one never knows the future impact you might have on students.”\nFor Matthew his future was in music composition.\nIt was clear that Matthew loved playing music and being creative, but didn’t enjoy solo performing. He explains. “When I was in high school I also wrote computer games – computer programming is very similar to composing as it is about problem-solving.”\nFor Matthew the Conservatorium provided a nurturing place that was full of people as interested in music as he was. In addition to a very solid musical foundation, it provided a great social network that he found very stimulating.\n“It’s not just the teachers, but the other students as well that make a difference. My advice to music students today is to take as many opportunities to interact with others as you possibly can, including being part of ensembles and performances. You will learn so much and have fun at the same time.”\nHindson returning to WollCon\nMatthew Hindson will be returning to WollCon for a 50th birthday concert with the BlueScope Youth Orchestra, playing a new commission he’s been working on.\nIt will be not only a celebration of the group’s long standing partnership with the steel-making company, but also a celebration for one of its blast furnaces also turning 50th in 2022.\n“My first string quartet, “Industrial Night Music”, was directly inspired by the steelworks at Port Kembla and Whyalla. So it can definitely be a source of inspiration in all manner of ways.”\nMatthew has a particular interest in death metal and techno in his compositions and often brings these aspects into the classical world – which not many people do.\n“In my musical ideas for this new commission, I will have the clanking and clunking of machinery in different patterns, but all working in sync.”\nMatthew was the perfect alumni to be asked to commission a piece for WollCon’s 50th birthday given when he finished high school he started as a computer programmer at BHP in Port Kembla.\n“I also worked in the actual Blast Furnace at the steelworks at Whyalla, so I know what the blast furnace is all about!”\n“The other thing about steelmaking is that it is a monument to human achievement and endeavour. I mean, it’s a massive thing in itself, dedicated to turning iron ore from the ground into an incredibly important part of our civilisation – steel. We couldn’t do without it. So I will be trying to reflect that in the composition. A sense of majesty, of grandeur and great power.” Said Matthew.', 'On a cool November evening, the crowd of regulars filters in at the Down Home -- Johnson City, Tennessee’s beloved listening room and bar. The performer waiting to the side of the stage is no stranger to this crowd, cycling between tuning her guitar and greeting friends as they make their way to their seats. When Amythyst Kiah takes the stage and the warm applause settles, she lays down a thumping bass line with her acoustic guitar. Soon, a few bright treble notes layer in, building up a minor chord that completes the gritty and skillful backdrop. Kiah begins to sing with a relaxed sense of ease and a steely intention, and the listeners lean in. “Ooh, Lordy, my trouble so hard / Don’t nobody know my trouble but God.” Though few audience members would know the song’s origin, the emotion moving in it is familiar and immediate.\nTrouble was a familiar subject for Adele “Vera” Hall, a singer who learned African-American spirituals and blues in her family and community in rural Alabama. When Hall sang on record for folklorists John and Ruby Lomax in 1939, she had already endured the death of her husband -- a coal miner who died in a gunfight more than a decade earlier. Hall made her own way, earning a living as a cook and washerwoman, and since her childhood days, she was known to be one of the finest singers in the area. When Hall sang “Trouble So Hard,” perhaps she knew that future generations of singers like Amythyst Kiah would put the song to good use, just as Hall had throughout her life. Of the hundreds of singers John Lomax documented for the Library of Congress, he remarked that Vera Hall had the ""loveliest untrained voice [he] had ever recorded.""\nPraise from folklorists like Lomax is not what makes Hall’s singing so valuable. For those hearing Kiah perform, the testament is in the air and among them, a strong voice reaching back through generations to present a song that folks can still relate to.\nKiah is an important and innovative presence in contemporary traditional music. Describing herself as a “Southern Gothic, alt-country blues singer/songwriter,” Amythyst has a repertoire that honors tradition while crossing genres to illuminate many common threads. A theme of “vocal integrity” unites her varied influences which include Son House, Dolly Parton, Sister Rosetta Tharpe, and Florence and the Machine. Accompanying her singing with guitar and clawhammer banjo, Kiah stands out among Southern artists, a uniqueness which has led her to perform at national venues such as the Smithsonian Folklife Festival and on programs like Music City Roots. Amythyst released a solo record titled Dig in 2013 and her current project brings together traditional and original music set for a five-piece blues rock band -- Amythyst Kiah and Her Chest of Glass. The group will release their debut EP in Fall 2016.\nTell me some about your upbringing in Chattanooga and how your first music came about.\nI grew up in the suburbs, so I was close to the mall and all that -- suburban sprawl kind of thing -- that’s sort of where I grew up. I played basketball and did the typical suburban life stuff. But when I was 13, I’d been really interested in wanting to play an instrument for a while, and my parents wanted to encourage me to play an instrument, play a team sport, and make good grades -- to be a well-rounded individual. Once I got my guitar, I started getting into writing and really getting into listening to music -- a lot of rock, singer/songwriters, that kind of thing -- and so, during that time, I pretty much dropped sports, transferred to a creative arts high school. I wish I could have gone there so much earlier, but I got there when I got there and it was a great change. I got really heavy into writing and playing music when I was in high school and I was a closet musician. I played a couple of talent shows, but I really just played for fun and I kind of kept to myself as a kid.\nActually, my first performance where it was a large group that was very much validating my existence as a musician -- other than my dad saying that he liked what he heard -- I wrote a song for my mom ... for her funeral. It was a few months before I graduated from high school that she died, so I wrote a song for her and sang it at the funeral. That was an eye-opening experience for me: Maybe I could write songs and people would actually want to hear them. I had a lot of good feedback.\nWe ended up moving to Johnson City when I was about 19, just to kind of start over. I transferred from the college I was going to in Chattanooga to East Tennessee State University and had absolutely no idea what I was going to do for my career. I didn’t really have a path. I just knew I was supposed to go to college. I was reveling in all these really cool classes like philosophy. I was really enjoying myself, but not knowing what I wanted to do. So I ended up auditioning for bands the next semester -- the Fall before, I had taken a bluegrass guitar class and I didn’t know anything about traditional music. I just liked the idea that I could take a music class where they appreciated learning by ear because, when I took classical guitar in high school, what discouraged me was the fact that I had to learn how to sightread and do all the formations and all that kind of stuff.\nSo I took that bluegrass guitar class with Jack Tottle, who is an amazing human being; it really meant a lot to be able to take that course because it changed my perspective on a lot of things. From there, I joined a Celtic band and did Celtic rhythm guitar and then I found my place in old-time music. That was around the time I learned about the Carolina Chocolate Drops -- I had taken Ted Olson’s class about American folk music and was fascinated with the intermingling, how multicultural the music actually was. I think part of my hesitance to finding my place was that I’ve always listened to all kinds of different music, and sometimes, if you don’t see people like you, sometimes you wonder or people make you feel like, “Well, do I belong here?” I was having those kind of feelings with some people. I’d had lots of praise and lots of support, so I’m really grateful for that, but there were always those few little people who put doubts in my head about my presence.\nOnce I read about the history of this music and how Blacks and whites both played this music -- that this is something that is integrally a hybrid -- I was like, “Well, hell, I have just as much right to be here as anyone else!” From then on, I was just like, “I’m doing it.” Roy Andrade reached out to me and asked me to be in the first-ever Old-Time Pride Band because he heard my voice and felt like he really wanted me to be part of it, and from then on I just did old-time. I ended up switching my major, once they got it approved, to Bluegrass and Old-Time Country Music Studies, and I graduated in 2012. During that time, I picked up solo gigs alongside the school band stuff, so that’s all part of my transition from playing mainly contemporary stuff into solely old-time stuff for a while, and now I’ve transitioned back into doing contemporary stuff. But it still has that old-time, roots influence.\nI really admire your music, because your personality is so evident in it and your own experience has shaped it. That’s so much a part of good music -- period -- but also traditional music. I was reading through your list of influences, and there wouldn’t have been a Sister Rosetta Tharpe or an Ola Belle Reed or so many of these figures if they hadn’t taken a step to put their own personality and experience in the music. You said you’re doing a lot more songwriting now and transitioning with your new band, Amythyst Kiah and Her Chest of Glass. Talk about that.\nIt’s interesting because, first of all, the guys in the band, they’re also part of another local band here called This Mountain and they’re an interesting mix. It’s kind of in the middle of folk and rock. They’re a hard sound to describe. They remind me of Radiohead -- alternative rock with acoustic instruments in it. This Mountain asked me to open for them a couple of years ago at the Hideaway over here. That was my first time playing a solo show in Johnson City. That turned out really well, and then they asked me to play with them at a festival in Savannah, Georgia, called Revival Fest because two guys in their band weren’t going to be able to make it. So we got together, put together a 30-minute set, then we went and played in Savannah.\nWe were well-received and I thought, “This is pretty cool!” Basically, the music in this band, a lot of it is stuff that I played acoustic, but with electric arrangements. For our EP, there are three songs that I’ve written that are going to be on there, and then there’s some stuff that I’ve done that come straight from old-time. Not all of them transitioned over, but two big ones are Vera Hall songs -- she’s really become one of my favorite singers. I’d like to take more of her songs and do more work with them. We do “Another Man Done Gone” and we do “Trouble So Hard” in the band. Obviously she sang a cappella and, as a guitarist, I always feel like I need to add some guitar stuff, so I added guitar arrangements to both of those songs. Then when I brought them to the band. At that point in time, they had mainly just been solely following me on what I do on guitar because I establish rhythm, bass line, and the riff. When they came in, they were kind of just following me, which is fine, but then we got to the point where it’s like, “Hey, what if we did the intro of a song with just piano or just drums?”\nSo I’ve gotten into arranging songs more because I have to remind myself that I’ve got other instruments here now. I don’t have to do everything. It’s nice because you get four different perspectives on the same song and it really opens you up in new ways, maybe trying things that you never thought you’d try before. But the way everything kind of flows right now is that it’s blues rock, but it’s also danceable -- it’s like blues-dance-rock. I’ve gotten into writing songs in a blues style mainly because, for me, songwriting has always been very difficult. I can write a poem -- I can write a short prose piece or a poem piece, but when it comes to putting it to music, I think of melody and chord arrangements first. That’s what happens when I listen to a song. Once I come up with the melodies, I’m like, “What the hell am I going to sing about?” because I feel like I’ve already expressed my feelings in this melody and in this song, so what else do I need to say? So that’s always been difficult.\nBut, when I got into blues, I started realizing that this is perfect. The main focus is on the emoting, and you’ve got a few choice words to describe what you’re feeling. For me, I like singer/songwriter stuff and the storytelling aspect of that, but I guess my brain doesn’t necessarily work in telling stories. I more or less like to express feelings. With a song like “Hangover Blues,” I’ll create three verses and they tell a really short story. I don’t know if it’s an attention span thing or what it is, as far as words go. I feel like sometimes, if I write too many words, it might take away from the emoting of the music. It’s something I always struggle with.\nThat’s so characteristic of really good traditional songs like the blues that you’re talking about -- that economy of words and expressing the feeling with your voice.\nThat’s where I feel most at home.\nSo much of your songwriting is about speaking your truth. You talk about writing for your mother and what a brave step that was. What kinds of emotions do you find yourself writing about now?\nIn the beginning, a lot of the stuff I would write about would be kind of along the lines of “me against the world.” Those aren’t songs that I’ve recorded because I wrote them years and years ago. But, as time has gone on -- especially after playing old-time music -- a lot of the songs I was drawn to were about loss and heartache, death ... lots of things that affect us to the core as humanables. There’s something very cathartic about playing a really sad song because, when it’s finished, it’s almost like you’re dealing directly with something that’s kind of scary and that you know is going to happen at some point in your life. To go through all those emotions in song is the safest way to be able to experience those things. It’s almost like preparing yourself, reminding yourself that bad shit happens, but at the same time, you come out of the song, and you can appreciate what you do have a lot more.\nSo now, the new songs that I’ve written, they’re actually a little more lighthearted than the stuff I’ve written in the past. “Hangover Blues” is one that’s on the EP and it’s about recovering from a hangover, but also being like, “I had a damn good time and I would do it again.” That’s one of my more lighthearted songs. Then “Wildebeest” is inspired from the sort of quintessential blues theme of “My woman pissed me off and I want to get back at her.” It’s a jealous lover kind of song. That one’s got some little parts in there that are meant to be lighthearted and comedic, but at the same time, the title also ties into the idea that, even though we are human beings, despite living in I guess what you would call a civilized society, we still have these primal urges. It’s a reminder of the fact that we are animals. I feel like keeping that in mind -- that we are susceptible to those things -- I feel like expressing that helps check the ego a little bit. The idea that people don’t see that they’re part of nature baffles me. You can be spiritual and still realize that you’re also part of nature. But some people separate themselves from their environment and, when people do that, you see what happens: Mountains get removed, tree forests are cut down because people don’t see themselves within the cycle of life. Just because we have logic and cognitive thought doesn’t mean that we live above and beyond everything. We are, in essence, destroying ourselves by doing this. Songs like “Wildebeest” ... I like to remind people that we’re very much part of something much bigger.\nYou can take this wherever you want to, but I’m wondering what you hope to accomplish through your music. You’ve talked about expressing your emotions and I think you represent a lot of communities in an innovative way, and also you are honoring these traditions and carrying them forward. What impact do you hope to make with your music?\nThis is something I’ve been thinking about a lot lately because music has always been something very personal. Sometimes it’s easy for me to get lost in my own brain and not necessarily think about the kind of impact that I’m having, but I’m thinking a lot more about that lately. For me, being a queer woman of color in Appalachia, pulling from these different roots-based ideas and then making these connections with electric music and traditional acoustic music and bridging the gap there, as an Appalachian person, I feel like I can bring a perspective to a wider audience and hopefully inspire people that look like me or love like me to tap in and be like, “Hey, this is really cool. This is something that I could do.” I just feel like, in a lot of ways, intersectionally, I’m the exact opposite of what would be considered typical for what I’m doing and I needed to see someone like that when I was playing music. I just feel like I want to, in some way, inspire other people. Their voice should be heard, and that contributes to the diversity of the people who are from our area.\nSam Gleaves is a folk singer and songwriter from Southwest Virginia. His latest record, Ain’t We Brothers, is made up of stories in song from contemporary Appalachia, produced by Cathy Fink.']"	['<urn:uuid:1db2f94f-1065-4382-95b0-088cb8a908f4>', '<urn:uuid:a5853b64-8a1a-48e7-9f7d-80b604c49dae>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T05:18:19.158519	9	48	3642
34	what the difference yellow mustard vs dijon mustard taste texture	Regular (yellow) mustard and Dijon mustard have several key differences. Regular mustard is brighter yellow and more acidic due to its vinegar content. Dijon mustard is creamier in texture and uses verjuice (acidic juice from unripe grapes) instead of vinegar. Dijon is also spicier because it's made with brown and black mustard seeds, while regular mustard uses milder yellow and white mustard seeds.	['A maple, brown sugar and Dijon glazed ham served with a pineapple salsa\nIf you’ve been with us for a while then you know that both of us are married to men who are not Greek. Although they have both embraced our Greek culture, religion, heritage, customs, language and traditions, they are xeni (the Greek way of say not Greek). Both men are a beautiful medley of English, Irish and Scottish descent and with a Canadian lineage that goes back a few generations at least.\nJohn is Billie’s husband, and he is a gem. As a child he spent a lot of his time with his maternal grandparents, Viola and Eldon, surrounded by love, kindness and food that his grandma served with pride. This was a very traditional home where the man of the house worked and the woman kept the home pristine and comfortable, and every night dinner was a committed affair. The table was always set perfectly, manners were paramount, and meals began with a prayer and thanks. John relished the time he spent with his grandparents and thrived under their care. Sleepovers with grandpa and grandma were the highlight of young John’s week. It’s no wonder that when John and Billie were married, Viola and Eldon were the two people who walked him down the aisle. They were so proud.\nAlthough John’s grandmother cooked daily, and elaborately some of the time, some of John’s favourite recipes were as simple as could be. They were simple, but to a young boy’s palate, they were the most delicious things on earth. Sometimes he was served peanut butter and margarine sandwiches cut into squares with the crust removed. Other times he would be invited to help himself to the pile of cream cheese and maraschino cherry sandwiches that his grandma would prepare for church luncheons.\nOther times Viola’s meals were fancier than sandwiches. On special occasions, she would spend hours in the kitchen and one of the meals that she often made was a ham. Viola’s ham recipe looked as though it came straight out of a cooking magazine from the 70’s. She slathered the ham with brown sugar and yellow mustard and used toothpicks to decorate it with canned pineapple rings and maraschino cherries. As a child John would anxiously anticipate the ham and his grandma would always make a point of giving him a piece with a generous presence of the sweet and sticky glaze.\nEldon and Viola were married for 70 years and had a wonderful life together. Even now that they are gone they remain an inspiration and example of commitment, love and family.\nThis recipe for maple glazed smoked ham served with a pineapple salsa is John’s adaptation of his grandma’s classic recipe. Although the original was (and still is) great, he was inspired to create something a little different. Instead of decorating the ham with pineapple rings, he has made a pineapple salsa with a bit of heat; that way every bite of ham can be complemented by the fresh taste of the pineapple. Although Viola sweetened her ham with brown sugar, John uses both maple syrup and brown sugar and the old-fashioned Dijon mustard brings a bit of spice.\nWhat is ham? And what are the different types of ham available?\nYou can find several varieties of ham in the grocery store or at the butcher. Hams are cut from the rear leg of a pig and although you can buy fresh ham from the butcher, if you cooked it it would really just taste like most other cuts of pork. What makes ham unique is that it is typically cured, baked and smoked – giving it a unique taste and texture that is hard to replicate.\nIf you purchase a ham that is cured or cured and smoked you can actually eat it cold. That’s why you can add cold ham that you purchase sliced at the deli counter to your sandwich. A larger ham however that will be served as a meal should be heated through, as it is in this recipe.\nIt can be daunting to make a ham selection if you are unfamiliar with this choice of meat. The best advice we have is to speak to your local butcher, but in the meanwhile, here are a few things to keep in mind:\n- When purchasing your ham be sure to know if it is partially cooked or fully cooked; this will impact your cooking time\n- You can buy half hams which are either shank-end or butt-end hams. The shank end ham is what we have used in this recipe and is the lower end of the leg. The butt-end tends to be more tender and easier to carve since it only has one bone.\n- You can purchase bone-in or boneless hams. We prefer the bone-in hams because they have more flavour than the boneless varieties which are processed and in turn lose not only the bone but also some of the flavour and texture.\n- There are many specialty hams available, including prosciutto, Smithfield ham, Serrano ham and more\n- Picnic ham, which you may have heard of, is actually not ham at all, but is cut from the front leg.\nWhat is the difference between Dijon mustard and regular (or prepared or yellow) mustard?\nNot all mustards are created equal! On a very basic level, regular (also known as prepared or yellow ) mustard is a much brighter yellow than Dijon mustard is. Regular mustard is also more acidic as it contains vinegar. Dijon on the other hand is made with a liquid called verjuice, which is an acidic juice made from unripe grapes. Because it uses less of the liquid, Dijon is also creamier in texture. Dijon mustard is also spicier because unlike regular mustard which is made with milder tasting yellow and white mustard seeds, Dijon is made with brown and black mustard seeds which are more intense in flavour.\nHaving said that, you can actually use any mustard that you like (or have) for this glaze. John’s grandmother always used prepared mustard when she made ham and it was very delicious.\nIn this recipe for Maple glazed ham, we used whole grain mustard, which is also often called old-fashioned Dijon. In this condiment the mustard seeds are not ground to a fine paste, leaving instead a seedy texture. We love the flavour and complexity of whole grain mustard and use it regularly. It works quite well in this glazed ham recipe.\nPin this recipe if you like it!\nLooking for more meals to help you celebrate? How about these special recipes:\nWe love hearing from you! If you have made our recipes, including our Maple Glazed ham; or if you have a question or comment, or simply want to say Hi!, please leave a comment below!\nMaple glazed smoked ham\n- Large roasting pan\n- Brush for applying glaze\n- Meat thermometer\n- 1 ham, bone-in, hickory smoked, fully cooked we used a 5.38 kg ham\n- vegetable oil for greasing roasting pan\nFor the glaze\n- 1 cup (250 mL) maple syrup\n- 1/2 cup (100 grams) brown sugar\n- 4 tbsp dijon mustard, old-fashioned\n- 2 tsp Worcestershire sauce\nFor the salsa\n- 2 cups finely diced pineapple\n- 1/2 jalapeno, finely diced you could add more to taste\n- 1/4 medium red onion, cut into small dice\n- 1 tbsp parley, finely chopped\n- 1/2 lime\n- Preheat your oven to 350 degrees Fahrenheit.\n- Prepare your ham by patting it dry. Then, score the ham all over, especially where you have a layer of fat, into a criss-cross shape. Use a sharp knife and cut approximately 1/4 inch thick.\n- Grease the bottom of a large roasting pan with a bit of vegetable oil. Place your ham into the center of the pan with the cut side down and the bone pointing upwards.\n- Roast, uncovered for the amount of time that it will take for your ham to warm through. Note, for a fully cooked and smoked bone in ham you will need about 15 - 20 minutes per pound to get to the safe internal temperature of 325 degrees Fahrenheit (163 degrees Celsius).\n- While your ham is roasting, prepare the glaze and the pineapple salsa (see below)\n- 30 minutes before your ham is ready remove it from the oven. You will notice that there is juice which has collected in the bottom of the roasting pan. Using a baster, baste your ham with this juice.\n- Next, using a large basting brush, coat the ham with the glaze, being sure to coat all visible sides of the ham with the glaze (do not coat the underside of the ham with glaze).\n- Once you have brushed the glaze all over the ham, pour the rest of the glaze mixture evenly over the ham.\n- Return the roasting pan with the ham back to the oven and bake for the final 30 minutes. Use a meat thermometer to ensure that the internal temperature of the ham is 325 degrees Fahrenheit (163 degrees Celsius).\n- Remove the roasting pan from the oven and allow your ham to rest for about 10 minutes before slicing it.\nFor the glaze\n- In a medium sized bowl combine the maple syrup, brown sugar, Dijon mustard and Worcestershire sauce.\n- Whisk all the ingredients together until well combined. Set aside.\nFor the salsa\n- In a medium sized bowl combine the pineapple, jalapeno, red onion, parsley and the juice from 1/2 a lime.\n- Mix well and taste. Adjust with more jalapeno and/or lime juice if desired. Set aside.\n- Transfer your ham to a cutting board for carving or to a serving platter. If desired (highly recommended) drain the drippings that are in the bottom of your pan through a fine sieve. Serve this sauce with your ham as a gravy.\n- Serve the ham with this gravy and with the pineapple salsa.']	['<urn:uuid:f24de61d-adc3-48ef-b4ac-ad9c00f1b633>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	10	63	1674
35	what environmental benefits green roofs provide urban areas and how does paul mccartney blackbird song promote equality	Green roofs provide crucial environmental benefits in urban areas, including improved insulation, reduced power consumption, stormwater retention, better air quality, and increased biodiversity in areas where vegetation is scarce. The Beatles' Blackbird song, written by Paul McCartney, promotes equality by serving as a metaphorical message of hope for those fighting against racism and discrimination. The song was inspired by the Little Rock Nine incident and continues to resonate with current civil rights movements, encouraging people to rise above oppression and fight for their rights.	"['Art has a way of retaining its value and meaning through the ages. And the song Blackbird by the Beatles is one of them. This is because Paul McCartney wrote the song as a symbol of hope for those who were fighting against racism and for civil rights in the United States. However, Blackbird’s meaning still stands strong today and it falls perfectly into context with the Black Lives Matter Movement.\nThe meaning of Blackbird\nBlackbird is one of the most memorable songs The Beatles ever wrote. Even though it was at the end of The White Album’s B-Side, it is one of The Beatles’ finest work. In an interview with GQ, Paul McCartney (the songwriter) explained that the Civil Rights Movement of the 60s inspired him to write Blackbird‘:\nI was sitting around with my acoustic guitar and I’d heard about the civil rights troubles that were happening in the ’60s in Alabama, Mississippi, Little Rock in particular.\nThe incident in Little Rock that Paul McCartney is talking about is the Little Rock Nine. They were a group of nine black students who faced discrimination due to segregation after enrolling in the all-white Little Rock Central High School in 1957.\nFollowing that, the Supreme Court’s historic Brown vs. the Board of Education decision came, which changed the Civil Rights Movement in the US forever.\nInspired by this, McCartney wanted to write a song that would become a symbol of hope:\nI just thought it would be really good if I could write something that if it ever reached any of the people going through those problems, it might give them a little bit of hope. So, I wrote Blackbird.\nHere is McCartney meeting with two of the Little Rock Nine:\nIncredible to meet two of the Little Rock Nine–pioneers of the civil rights movement and inspiration for Blackbird. pic.twitter.com/QrnOQnqrFX\n— Paul McCartney (@PaulMcCartney) May 1, 2016\nBlackbird isn’t for an actual bird\nThe word ‘Blackbird’ isn’t meant to depict an actual bird. In fact, the word ‘bird’ refers to a girl in England. Therefore, McCartney’s meaning behind the song was to tell a black girl to take her broken wings and ‘fly’:\nBlackbird singing in the dead of night\nTake these broken wings and learn to fly.\nAll your life\nYou were only waiting for this moment to arise.\n50 years have passed, and Blackbird is still relevant today, with the Black Lives Matter Movement.\nHow this song matters to Black Lives Matter\nPolice brutality and racism in the United States are still left unchecked. And that is what makes the murder of Ahmaud Arbery and George Floyd possible. However, songs like Blackbird can give the protesters hope that this is their moment to fight for their rights.\nBlackbird is a song that is deep and metaphorical and can in fact be used in any circumstance. It speaks that even if you’re not free, oppressed, persecuted, or living in darkness, you should always try to rise and follow the light. As McCartney wrote:\nBlackbird fly, Blackbird fly\nInto the light of the dark black night.\nSo, Blackbird provides the perfect motivation for the Black Lives Matter Movement. Despite the racism and police brutality in the United States, it is their time to shine for what is right. And that is what makes Blackbird by The Beatles extremely relevant to this age.\nThis is why Blackbird is one of the 10 most covered songs of all time. Even though it’s been more than 50 years, Blackbird by The Beatles still resonates with the plight of black people in the United States, making it perfect for Black Lives Matter Movement.', ""Sustainability Series: Green Roofs\nArticle information and share options\nWith increased urbanization, green roofs are becoming a major trend. They offer many advantages to building owners and occupiers, the general public, and the environment: better insulation, reduced power consumption, stormwater retention, improved air quality, and a biodiverse environment offering aesthetic diversity. However, green roofs may introduce property considerations and concerns that owners and occupiers should be aware of when reviewing their property and business risks.\nGreen roofs comprise specific vegetation designed and installed on a series of components, including moisture retention material, a drainage system, a root barrier, and a protective layer for both the insulation and underlying roof surface.\nProperty Risk Considerations\nLet's take a closer look at a few features that will likely be of interest to commercial property insurers when evaluating the building's overall risk profile.\nDownload our guide\n1. Potential for water leakage damage\nThe irrigation pipework may be susceptible to freezing. Leaks in waterproofing membranes due to root growth or temperature fluctuations may expose the underlying roof structure to damage or corrosion. Drainage systems may become blocked by soil and vegetation. These risks are particularly relevant in the construction phase.\nLoss Prevention Tips: Penetrations in the roof should be minimized. It is important to include leak detection systems below the waterproofing membrane. Size gutters to accommodate both rainfall and irrigation runoff and maintain them regularly, with specific inspection points included. Consider the impact of roof gradient on the density of growth media and its propensity to shift or slide during heavy rain events.\n2. Fire load\nGreen roofs and their supporting components (e.g., vegetation and waterproofing membrane) typically add combustible loading to a roof and increase the potential for ignition from exposing fires or other ignition sources. In some cases, the design of a green roof may encompass space for recreational activities, introducing ignition sources such as lighting, electrical installations, barbecues/grills, and smoking.\nLoss Prevention Tips: Limit the overall fire risk by maintaining adequate moisture content of vegetation, regular removal of dead vegetation, and careful design and placement of noncombustible fire breaks. Include manual firefighting equipment such as fire hydrants, fire hose reels, and portable fire extinguishers, along with access provisions for the fire brigade. Potential ignition from electrical equipment, smoking, or other sources should be identified and carefully managed.\n3. Susceptibility to collapse\nGreen roofs introduce live loads associated with landscaping, precipitation, induced saturation due to irrigation, and periodic replacement of new soil and growth of vegetation.\nLoss Prevention Tips: The load carrying ability of concrete roofs versus all other types (e.g., long span steel) should be considered over the roof lifecycle. Some roofs may be susceptible to deformation over time, affecting the operational efficiency of certain components that originally functioned properly. Future changes may affect the structural integrity and should undergo formal review. Concrete roofs are generally more resilient than other roofs to changes that increase live loading.\n4. Damage from natural hazards\nGreen roofs are susceptible to natural hazard perils such as seismic forces and wind uplift pressures. Vegetation or potentially the entire roof system may be damaged, requiring replacement.\nLoss Prevention Tip: The system, including any moisture/root barriers, should be properly secured to structural elements (growth media should not be relied upon). Green roofs should not generally be installed in areas with elevated wind exposures such as coastal areas subject to hurricanes.\nGreen Roofs provide many environmental benefits, especially in urban locations where vegetation may be scarce. However, if a green roof is to be installed, ensure that the considerations outlined above are incorporated in the design, installation, and future maintenance requirements to mitigate the potential for loss.""]"	['<urn:uuid:d8e7b6c7-a8cf-477d-8afa-64e82fcdcc47>', '<urn:uuid:adefe97e-66fa-495a-b3a2-8e24a422fce1>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	17	84	1216
36	Which cheese needs caves for production, Roquefort or Parmesan?	Roquefort cheese must be ripened in the caves of Roquefort-sur-Soulzon in the Combalou Mountains, while Parmigiano Reggiano is matured in cellars, not caves.	"['Legend has it that a young shepherd was guarding his herd of ewes near the Grotte du Combalou, a large cliff face that dominates the village of Roquefort-sur-Soulzon in the beautiful Aveyron region of south central France. He was just about to prepare his midday meal when he noticed a beautiful young lady in the distance. Fascinated, he attempted to follow her. He left his dog to look after the herd and hid his lunch—of bread and cheese—in the cool, damp rocks of a grotte (""cave"" in French). Sadly, the shepherd never found the young woman and returned to his herd disappointed.\nMonths later he came upon the lunch that he had hidden. The bread had decomposed, leaving the cheese with streaks of blue veins. In a move most of us today would question, he tried the abandoned cheese and found the taste remarkable. Thus Roquefort cheese was born. It did not take long for the shepherd to share this alchemy of milk, bread, air and time with his fellow herdsmen. Soon many caves had been converted into cabanes en bois or ""huts of wood,"" so named for the oak planks built in the interior of the caves where cheeses are left to ripen.\nRoquefort cheese is said to date back to the time of Pliny the Elder in ancient Rome (circa 75 AD), and the center of Roquefort cheesemaking has always been Roquefort-sur-Soulzon. The cheese\'s name-controlled AOC status is probably the oldest in all of France. Today it\'s made with pure sheep\'s milk which is inoculated with a special type of blue mold, penicillium roqueforti, obtained from bread left to mold in the caves of the Combalou Mountains, where the original Roquefort cheese was accidentally created. Ripening is of paramount importance and takes a minimum of three months. Cheese that can legally be sold as Roquefort is ripened in only one area—in the caves of Roquefort-sur-Soulzon in the chalky Combalou Mountain area. The region has an abundance of caves due to a geological accident that occurred long ago. Then an extensive clay bed partially collapsed in the same region, creating vertical faults and fissures in the caves which provide ventilation, a necessity for making the cheese.\nRoquefort begins, as all cheese does, at dairy farms, where it\'s salted and then pierced with needles. It is then placed in the caves, on top of oak planks in long rows filling the natural caves. There the cheese matures under the careful watch of maîtres-affineurs, or master ""maturers."" The well-known green-blue marbled appearance of Roquefort develops when the cool, damp air is swept into the caves through the fissures, developing the roqueforti mold in the cheese\'s pierced channels.\nTo prevent Roquefort from drying out and to ensure that its deliciously unctuous texture remains well preserved, it should always be kept in its original wrapping or protected by a sheet of aluminum. Since most of us don\'t have a damp, cool cellar in which to store our cheese, keep your Roquefort on the lowest shelf of the refrigerator. As with other fine cheeses, avoid drastic changes in temperature, as this can ruin the cheese.\nRoquefort has a tingly pungent taste, a distinct bouquet, and a flavor that combines the sweet burnt-caramel taste of sheep\'s milk with the sharp, metallic tang of the blue mold. When destined for a cheese board, Roquefort should be brought to room temperature at least one hour before being served, just like a good wine. Only at room temperature will it offer the full splendor of its aroma, softness, and sophisticated flavor. Serve it after dinner with sweet fruit and wine – a perfect dessert.\nExperience International Variety\nYou might receive a Gaperon, originating in France during the 14th Century, an\nauthentic Lancashire by Ruth Kirkham, and an Italian Taleggio matured in the\ncaves of Valsassina…all in one shipment!', 'There are really stringent laws governing what kind of cheese can be called Parmigiano Reggiano, so in fact the answer to the question should be: Only one kind – Parmesan cheeses produced in the regions which are covered by the Parmigiano Reggiano PDO (protected designation of origin). In reality, however, it is a little more complicated! Let me try to explain about the variations that can be found amongst cheeses that all proudly bear the PDO stamp which proclaims to the world that they are genuine Parmesan cheeses from the PDO region, which includes Parma, Reggio Emilia, Modena, Mantua and Bologna.\nIt may surprise you to know that there are about 420 creameries within this designated region, and these “parmesan factories” receive their milk from over four thousand farms every day. Inevitably, there will be a large variation in the end product from all these dairies, due to the season, altitude, breed of cattle and expertise of the cheese-maker. Another factor that hugely influences the final cheese is the period of maturation; the minimum time required for a cheese to fulfil the stringent appellation requirements is 12months, but some cheese wheels spend up to 36 months in the maturation cellars, during which time there is a very noticeable change in the taste and character of the cheese.\nFactors which influence the final product are:\nMaturation: at 12 months this medium-fat semi-hard cheese will have a pale cream colour, taste slightly salty, slightly acid and slightly sweet, and have a wonderful nutty fragrance – Parmigiano is never a smelly cheese! There is a slight grainy texture, one of the distinctive characteristics of this King of Cheeses. At 18 months, the texture has changed as more crystals develop and the straw colour of the cheese is a shade darker; the flavour is becoming more savoury and the fragrance has become a little fruity. The flavours and aroma of the cheese continue to deepen and mature, and the colour gets progressively darker. By 30+ months the cheese is fully mature, a golden straw colour with many crystals and can have woody, spicy, and dried-fruit flavours on your palate. The rind will be really hard at this stage.\nAltitude: At higher altitudes, the dairy herd has access to sweeter, greener grass (Parmigiano herds are never fed anything other than grass!) and purer water, resulting in the cheese from the mountains (Parmigiano di Montagna) having a subtly deeper flavour. Many cheese connoisseurs also believe that cheeses made in Spring and Autumn are also superior due to the improved feed at these times of the year. Since each wheel is date-stamped, it is easy for cheese buyers to select cheeses made at these time of the year.\nThe Herd: if you are faming cattle for the meat market you choose good beef producing breeds, and likewise milk producers for the famous Parmigiano cheese production rely on superior milk-producing cows. In this region the most favoured breed is the Alpine Brown, bred exclusively in the mountainous areas. Recently “red cows” as they are locally known are making a coming back. The Rossa di Parma is native cow of the area and it produces a superior milk. These animals produce the very best balance of quality and quantity of milk – a really superior product just perfect for the production of a really superior cheese.\nSo, to get back to the original question – there is only one type of cheese that may be called Parmigiano Reggiano, produced in the areas covered by the PDO, but within the parameters set there can be fairly wide variations in the appearance, aroma and taste (and price!) of your slice of Parmigiano.\nSubmit your review']"	['<urn:uuid:a1e1fa96-9952-4547-b827-b8b2228b73aa>', '<urn:uuid:17133bf9-f8e6-43f1-bff4-835bb407e0a5>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T05:18:19.158519	9	23	1254
37	preferred return hurdle minimum return before carry	A preferred return hurdle means that Limited Partners (LPs) must receive a certain minimum annualized return before the General Partner (GP) can receive carried interest. For example, with an 8% preferred return hurdle, LPs must receive an 8% annual return on their capital before the GP gets any carry.	"['What is a preferred return hurdle?\nA preferred return hurdle is a component of the fund manager’s carried interest.\nTo read more, please click on the ""Read More"" link below and to the right.\nA preferred return hurdle is layered on top of the GP’s carry, and injects a time value of money consideration. A preferred return hurdle basically says that the LPs receive a certain minimum annualized return before the GP can receive carry. An 8% preferred return hurdle means that the LPs must receive an annualized 8% return on their capital before the GP receives its carry. Using our very basic example, an 8% annual return on a $100 million fund means that the LPs must receive $8 million per year before the GP can receive carry. So, after 10 years, the LPs must receive $80 million in returns (above the $100 million original investment) before the GP can receive carry. As our fund only returned $70 million in profit, the GP is not entitled to carry. If, on the other hand, the fund returned $200 million ($100 million of profit and $100 return of LP capital), then the GP would be entitled to carry. For more on preferred return hurdle, please see my post “LP Corner: Fund Terms – Preferred Return Hurdle and GP Catchup.”\nThe key take-aways are (1) GPs who have carry with no preferred return hurdle are motivated solely by a multiple of invested capital, and (2) GPs who have carry with a preferred return hurdle are motivated both by a multiple of invested capital and the time value of money.\nWhy does this matter? A few reasons, but primary among them is that most LPs use IRR either as the primary metric or as one of the primary metrics to evaluate the performance of their overall investment program and their private equity investment program.\nMost institutional LPs evaluate their fund investments using IRR\nIn my experience, most institutional LPs evaluate their private equity programs (and their fund investments) based on an annualized rate of return, or IRR. For example, Yale Endowment’s 2017 Endowment Update Report (obtained here: http://investments.yale.edu/endowment-update) focuses on percentage return. CalSTRS measures its private equity portfolio performance based on a rate of return (a dollar-averaged internal rate of return, see https://www.calstrs.com/private-equity-portfolio-performance). CalPERS evaluates their private equity program on both IRR and investment multiple (see https://www.calpers.ca.gov/page/investments/asset-classes/private-equity/pep-fund-performance). I have discussed IRR in a couple of posts: LP Corner: Fund Performance Metrics - Internal Rate of Return (IRR) Part One and LP Corner: Fund Perormance Metrics - Internal Rate of Return - Part Two. I don’t believe any one metric (IRR, multiples, PME) fully captures return, an so I analyze fund performance using a combination of metrics: IRR, multiples (especially DPI) and PME.\nSince most institutional LPs use IRR as the primary, or a primary, return metric, private equity funds should be structured to have IRR be a primary metric to reward GPs with carry. Most buyout funds have a preferred return hurdle, so that’s good. Many growth equity funds also have a preferred return hurdle. However, most venture capital funds do not have a preferred return hurdle. Why?\nVenture Capital Funds Have Long Lives.\nEarly-stage venture capital firms invest in companies that are truly starting up. It can take 5 to 10 years (or longer in some cases) for these companies to mature to the point that they go public or are sold.\nThe NVCA Yearbook for 2015 contained a chart based on data from Adams Street Partners (based on 2010 analysis of dissolved funds) showing that the median life span of an IT fund in their analysis was over 14 years, and that 10% of funds in their analysis had life spans that were 19 years or longer. I believe that this is only stretching out longer. I have served on Limited Partner Advisory Committees for funds that were close to 20 years old!\nBecause of the long hold times (which longer for early stage venture capital than for buyout or growth equity), GPs of venture capital firms, especially ones with whole-fund carry, may have to wait a very long time to ever see carry. If a preferred return hurdle is added to the carry, it would stretch even longer the period before the GP can receive carry. This is the main argument why venture capital funds do not have a preferred return hurdle.\nAlignment of Interests\nAnother issue is alignment of interests between the GP and the LPs. As a venture capital fund ages and reaches the end of its initial 10 year term, GPs that don’t have a preferred return hurdle don’t need to consider time value of money. What this means is that if the fund has a number of portfolio companies that are growing slowly, the GP may be motivated (by the prospect of carry) to let the companies continue on for a period of time to see what happens (oh, and possibly continue to charge a management fee). This creates a misalignment of interest between the GP and the LPs.\nIf that same GP had a preferred return hurdle, the GP would have to weigh selling now vs letting the company continue on for a period of time. The GP would have to consider the time value of money, which better aligns the interests of the GP and the LPs.\nThe arguments for an 8% preferred return hurdle for VC funds:\n- Better alignment of interests. Because IRR is one of the main metrics to evaluate venture capital funds, having a preferred return hurdle better aligns LP-GP interests.\n- It will make VCs focus more on the time value of money.\n- It might lead to shorter venture capital fund life spans.\n- It would delay carry being paid to VCs (especially for funds with whole-fund carry)\n- Most buyout funds have a preferred return hurdle, so why not venture capital funds?\nThis is one of a series of posts on fund terms. Other posts include:']"	['<urn:uuid:e236834f-be47-4e2f-afc9-7bb9987e8ab6>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	7	49	999
38	What are the performance capabilities of military aircraft in terms of flight duration, and how do solar-powered alternatives change this?	The V-22 Osprey operates with conventional fuel limitations, requiring sufficient power to sustain hover and landing operations, particularly when carrying heavy cargo loads. In comparison, solar-powered aircraft technology offers the potential for much longer flight durations - as demonstrated by Solar Impulse 2's completion of a 43,000 km flight using only solar energy. Solar power allows for extended operation periods that are primarily limited by the pilot rather than fuel capacity, though the technology still requires further development to withstand high pressure, extreme temperatures, and fluctuating weather conditions.	"['CAMP FOSTER, Okinawa -- The Navy is tweaking takeoff and landing procedures to increase the cargo load of its helicopter of the future -- the tilt-rotor V-22 Osprey. Helicopter Test Squadron 21 performed nine days of tests in October aboard the aircraft carrier USS Eisenhower, said Naval Air Systems Command spokesman Billy Ray Brown. The team flew 25.6 flight hours, performing 69 short take-offs and minimum run-on landings. This maneuver is used to transition from forward flight to a landing when there may not be sufficient power available to sustain a hover as might be the case when the helicopter/airplane hybrid is at high gross weight. Testing was a success in that engineers were able to complete the assessment plan without missed or canceled events, Brown said. However, in-depth analysis is still required before the maneuvers can be added to an Osprey pilot\'s repertoire. Results, expected in late spring, will be used to expand the envelope for the Marine Corps and Air Force variants of the V-22, known as the MV-22 and CV-22 respectively, as well as a future variant that will be employed by the Navy. ""Throughout the lifetime of an aircraft, there are lessons learned and new tactics and techniques which are identified,"" Brown wrote in a statement to Stars and Stripes. ""Before they are included in the standardized flight operating procedures, the tactics and techniques are tested by the developmental test team to develop how and under what conditions a pilot could safely employ them."" Rolling take-offs and run-on landings would allow the aircraft to take off and land at a higher gross weight due to the ""aerodynamic advantage"" known as ""translational lift,"" Brown said. Further examination is required to determine adequate control margins and the effects of changes in movement on the flight deck as well as atmospheric conditions. When the final parameters and procedures have been developed and evaluated, they will be incorporated into the Osprey\'s Naval Air Training and Operating Procedures Standardization program, or NATOPS. NATOPS manuals contain standardized operating procedures, training requirements, aircraft limitations and the technical data necessary for safe and effective operation. The Osprey, created through a partnership between Bell Helicopter and Boeing, is the military\'s only tilt-rotor aircraft. Following a checkered development history that saw deadly crashes and questions raised about its abilities, the Osprey has performed admirably in Iraq, Afghanistan and Libya and in humanitarian relief operations across the Asia-Pacific. Pilots like the aircraft for its speed, versatility, maneuverability, cargo capacity and durability. The Osprey is designed to carry two pilots, two crewmembers and 24 troops, Brown said. During the October tests, the aircraft did not carry a troop complement. Weight was controlled using a water tank.\nNavy Hopes Landing Tweaks Will Increase Osprey Cargo Capacity\nYou May Also Like\nPresident Trump has met three times with North Korean dictator Kim Jong Un during his presidency.\nGet the Military Insider Newsletter\nGet the latest on pay updates, benefit changes and award-winning military content. Right in your inbox.\nView more newsletters on our Subscriptions page.\nVerify your free subscription by following the instructions in the email sent to:\nMost Popular Military News\nTen more states have met the criteria to lift the travel restrictions the Pentagon set in place in March.\nLawmakers in the Senate are proposing a new hurdle in the Army\'s race to replace its outdated physical fitness test.\nIt\'s the first time since the creation of Tricare Select that users will have to pay a monthly fee.\nU.S. Army paratroopers from the 82nd Airborne will be the first unit to receive the service\'s new Infantry Assault Vehicle.\nThe U.S. Space Force has determined how it will be organized, right down to the squadron level.\nLatest Military Videos\nAir Mobility Command aircrew and medical personnel conducted the first operational use of the Negatively Pressurized Conex to transport...\nThe Air Force’s “Singing Sergeants” band collaborated with Lee Greenwood and Home Free for a special performance of one...\nCvoky, a 120-pound Belgian Malinois, who serves as a U.S. Air Force military working dog, was recently rushed to...\nU.S. Soldiers of the 1st Battalion, 5th Infantry Regiment, 1st Brigade Combat Team, 25th Infantry Division, fire 60mm mortars...\nAmphibious Reconnaissance Marines with the 31st Marine Expeditionary Unit conduct clandestine subsurface infiltration during an amphibious reconnaissance operation on...\nApproximately 5,500 soldiers will participate in Exercise Lightning Forge at various training areas on Oahu.\nA 21-year-old soldier from Texas was killed Friday in southwestern Afghanistan, Pentagon officials announced Saturday night.\nThe national command was 4,000 enlistments behind its annual goal by the end of June.\nThe carrier has been operating in the Eastern Pacific and was in the vicinity of Pearl Harbor at the time.\nAir Force News\nThe NPC can carry up to 28 COVID-19 patients without spreading the disease to the aircrew.\nThe latest accident marks the fourth crash of a U.S. Air Force fighter jet since May.\nGen. Goldfein said if the chance to make things right for all airmen falls by the wayside, it will be the Air Force\'s fault.\nMarine Corps News\nMacander said her role and history-making command came with its own set of challenges.\nThe Marine Corps has acknowledged a host of problems with an investigation into a midair collision off the coast of Japan.\nCoast Guard News\nThe boats were spotted at about 1 a.m. as they crossed from Mexican territorial waters toward the United States.\nThe Coast Guard changed its policy in 2019 to give commanders discretion as to how to handle such imagery.\nEight members of Coast Guard Station Brunswick were recognized Monday for their exemplary performance during rescue efforts.', 'The key distinction between drones and other aircraft is their ability to fly and operate autonomously, hence the name unmanned aerial vehicles (UAV). With neither the risks nor limitations of having a human onboard, drones can reach higher altitudes and operate for extended periods of time, while controls are “manned”, or monitored, by remotes and computer systems on the ground. The combination of autonomy and advanced technology has allowed drones to take on a variety of functions, such as carrying and delivering goods, surveilling areas of land, collecting data and capturing images for both military and civilian missions (1).\nEngineering a power source that can last for days/weeks and not burden the device with excessive weight is a constant challenge in aeronautics. Drones for civilian use are usually powered by removable battery packs that last for less than an hour before returning to the ground for replacement (2). Those used for extended surveillance and reconnaissance require much larger power sources to carry the weight of multi-rotors and actuators (i.e. cameras, weapons, radars, sensors), which in total can range from below 2kg and over 600kg (1). For high endurance missions lasting days or weeks, 80-90% of the gross weight is in fuel capacity and 10-20% of the weight in the actuators (1).\nSolar-powered drones have proven to be a much more efficient source of energy for drones traveling long distances and over extended periods of time. The use of solar cells not only offers a constant stream of energy to operate the vehicle, but also greatly reduces the overall weight. AeroVironment, a leading energy and aeronautics company in California, designed some of the most groundbreaking solar-powered aircrafts—Solar Challenger, Pathfinder, and Pathfinder-Plus (3). Although these were manned aircrafts, AeroVironment also has a division for unmanned aircraft systems, which may foreseeably be integrated with their solar-energy initiatives.\nSolar-powered drones still require research and experimentation before they can be fully implemented. However, the opportunities and benefits are clear and the industry has become increasingly competitive, with energy, aeronautic innovation, and even digital media companies involved. Solar Impulse is one company working towards clean energy aircrafts. Most recently, the company’s Solar Impulse 2 completed a 43,000 km flight on no fuel and solar energy alone. Bertrand Piccard, the 58-year-old seasoned pilot, told reporters, “We have shown that the plane could fly forever. The limit is the pilot.” (4) Facebook and Alphabet, Google’s parent company, are developing technologies in their projects known as Aquilas and Google Titan to deliver internet connection to any part of the world that still lacks access (5). These missions require devices to sustain itself at high altitudes, 18,000 to 27,000 meters above the ground, and adapt to unexpected weather conditions for a continuous 3 months; current tests remain far from the goal, as Aquila has flown up to 655 meters above ground and for a 96-minute period (6).\nThe vision for solar-powered drone technology has yet to be perfected and authorized under government regulation. As mentioned earlier, photovoltaic technology has proven to be sufficient for fueling aircrafts without interfering with the vehicles’ aerodynamics. However, the conditions for which drones are expected to experience include high pressure, extreme temperatures, and fluctuating weather conditions. With this in mind, researchers have identified that only 5 out of 20+ technologies (crystalline silicon (c-Si), gallium-arsenide (GaAs), amorphous silicon, copper-indium-gallium-selenide and thin gallium-arsenide based photovoltaics) can withstand the conditions described above (7). Furthermore, research around photovoltaic technology specifically for drones requires a different standard for measurements and data collection because the optimal solar panel for powering a drone is not necessarily the technology with the highest energy yield, but the highest power-to-mass and power-to-area ratios (PUAV) (7). Currently, solar-powered drone technology is somewhat dependent upon existing photovoltaic research, until more data can be collected and solar energy systems can be understood in collaboration with aerial vehicles.\nFinding a balance between mass and power sources in drone technology is not the only challenge that innovators are faced with. Researchers and engineers are also racing against the clock as more competition enters the industry and increasing government regulation is put in place. The U.S. Federal Aviation Administration’s (FAA) UAS Rule (Part 107), due to take effect on August 29th, 2016, limits vehicles’ weight at 25kg and altitude at 121.92 meters off the ground unless approved under a certificate of waiver (8). Increasing government regulations may hinder the progress in solar-powered drone research, especially for purposes of testing high endurance and high altitude vehicles. On the other hand, as more companies and organizations occupy the aerial landscape, partnerships across sectors will rise. Facebook and Alphabet are just two of many companies that have found opportunities through such partnerships.\nIncorporating solar energy with drone technology has shown benefits beyond providing clean energy. Solar-powered systems will allow vehicles to spend longer durations in the air and maintain constant surveillance, leading to greater data and accuracy of information collected. These aspects are appealing to industries that may not have had a part in drone technology before, like those in media or telecommunications. Although this new area of research may lead to some interferences from competitors and regulators, other businesses and governments can also offer great networks and opportunities for solar-powered drone technology.\n(1) Gupta, S., Ghonge, M., Jawandhiya, P. M. “Review of Unmanned Aircraft System (UAS)” International Journal of Advanced Research in Computer Engineering & Technology (IJARCET), vol. 2, no. 4, 2014. Accessed on 5 Aug 2016.\n(2) Pullen, John Patrick. “This Is How Drones Work.” TIME. TIME Inc., 3 Apr. 2015. Web. 1 Aug. 2016.\n(3) AeroVironment. AeroVironment, Inc., 2016, www.avinc.com. Accessed 1 Aug. 2016.\n(4) Burgess, Matt. “What’s next for Solar Impulse? Pilots reveal where their iconic plane is going to take them now.” WIRED. Condé Nast Publications, 27 July 2016. Web. 3 Aug. 2016.\n(5) Cuthbertson, Anthony. “Google Tests Solar-Powered ‘5G’ Internet Drones” Newsweek. Newsweek LLC. 1 Feb. 2016. Web. 1 Aug. 2016.\n(6) Vanian, Jonathan. “Facebook’s Solar-Powered Drone Just Hit a Big Milestone” Fortune. Time Inc. 21 July. 2016. Web. 1 Aug. 2016.\n(7) Alta Devices. “White Paper: Selecting Solar Technology for Fixed Wing UAVs” 2015. pdf. 3 Aug. 2016.\n(8) Small UAS Rule, Federal Aviation Administration § 107 (2016). Print.\nImage: © Ivan Cholakov | Dreamstime.com - <a href=""https://www.dreamstime.com/stock-photo-drone-over-us-city-surveillance-flying-image57023398#res14972580"">Drone over US city</a>\nThe idea of using sunlight and radiant heat to source electrical and heating/cooling systems is no new phenomenon. Solar powered technology has been developing for decades, since the introduction of solar photovoltaics (PV), a medium of conductive materials (i.e. silicon, cadmium, gallium) designed to convert absorbed sunlight directly into electricity (1). Solar engineers later found that the alternate form of solar energy, radiant heat, could generate solar thermal electricity (STE) through concentrated solar power (CSP) technology. CSP structures are strategically placed to reflect and focus sunlight upon a “heat transfer fluid” (i.e. molten salt, synthetic oil), which then transfers the energy to an engine that produces electricity (1). Although solar technology has existed for several decades, solar engineers continue to make material, installation and distribution improvements on existing PV and CSP structures to stimulate and sustain growing demands for renewable solar technology.\nBetween solar PV and CSP technology, households and businesses prefer PV energy conversion systems because they are easier to install, show rapid returns on investment, and are compatible with existing policies/markets (1). CSP technology remains as a major source of renewable energy, but requires much more area and maintenance, making it most effective in arid climates.\nAt 2015 year-end, the worldwide capacity for solar PV was 227 gigawatts, approximately 185 million solar panels, while that of STE was 4.8 gigawatts (3). The U.S. alone increased capacity by more than 10 gigawatts in 2015 and totaled over 800,000 distributed PV systems installed (4). These trends were attained through technological advancement, as well as increasing dialogue around the subject of renewable energy in politics, businesses, and civil engineering. The past decade has shown rising numbers of operating solar energy systems in the U.S. and worldwide due to both scientific and systemic progress.\nMaterial and Manufacturing Improvements\nSolar CSP and PV systems are both constructed with large amounts of metal, where CSP structures have also shown a “high metal depletion burden”, “greater than for other power generators” (1). PV systems generate the most waste during the manufacturing process of using crystalline silicon to build PV modules (1). Developments in solar technology materials and manufacturing processes are foreseeable, as it is in solar technology companies’ best interest to find alternative materials and designs that will increase efficiency and durability.\nSolar Energy Systems for the Long Haul\nSolar power systems’ variability to sun exposure, based on geography, climate, and date, is a continuous challenge for engineers. To account for periods and locations that receive minimal to zero sunlight, solar power engineers have experimented with different materials and chemicals’ power load capacities, battery and storage technology, and transmission systems (1). A broader approach is a systemic change that places solar and wind energy on the power grid, alongside existing electricity grids that are primarily powered by coal and fossil fuels, also known as system value (SV) (5).\nProgress in solar power technology has been driven by increasing awareness of climate change, initiatives to counter act global warming, and economic incentives through lower utility costs and public policy standards. Solar engineers are expected to meet the rising demands of government officials, consumers, and businesses by mid-century (5). Engineers and manufactures will be held responsible for increasing production in the coming years, while continuing to develop solar technology that will maximize energy efficiency, sustainability, and distribution.\n(1) Hertwich, E.G., Alosisi de Larderel, J., Arvesen, A., Bayer, P., Bergesen, J., Bouman, E., Gilbon, T., Heath, G., Pena, C., Purhit, P., Ramirez, A., Suh, S. Green Energy Choices: The benefits, risks and trade-offs of low-carbon technologies for electricity production.\n(2) “Renewables: About solar photovoltaics.” International Energy Agency. n.d. 14 Jun. 2016.\nRenewable Energy Policy Network for the 21st Century. Renewables 2016: Global Status Report. REN21, 2016. 13 Jun 2016.\n(3) Cantwell, Maria, Sen. Hearing on Near-Term Outlooks for Energy and Commodity Markets, U.S. Senate Committee on Energy & Natural Resources. 366 Dirksen Senate Office Building, Washington D.C. 19 Jan. 2016. Opening Statement. 13 Jun 2016.\n(4) Mueller, Simon. Next Generation Wind and Solar Power, From cost to value. International Energy Agency and Clean Energy Ministerial, 2016. 14 Jun. 2016\nImage: © Tangencial | Dreamstime.com - <a href=""http://www.dreamstime.com/stock-photos-solar-thermal-power-plant-image23345363#res14972580"">Solar thermal power plant</a>']"	['<urn:uuid:c1f00a49-ab3a-4479-87ee-57bd59294eb1>', '<urn:uuid:0913cb95-c253-4c6c-8417-adb35ebbe7de>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	20	88	2672
39	garden tools arthritis manual reel difficulty	Push-type reel mowers require manual effort which could be challenging for those with arthritis, while Fiskars offers specially designed PowerGear tools that have been awarded the Arthritis Foundation's Ease-of-Use Commendation. These tools amplify cutting force by up to 5 times, reducing strain on joints for people with arthritis.	"[""Edibles with Ease: When to Get Growing from Seeds or from Starts? Read more »\nIn my side yard which is mostly shade, I have tried a variety of perennials that thrive in a woodland setting. Read more »\nMake your garden even more welcoming to birds and butterflies: turn it into a certified wildlife habitat. Read more »\nThe StaySharp™ Max Reel Mower combines patent-pending technology with superior ergonomics to deliver best-in-class cutting perf... Read more »\nKeep your lawn and your shoes clean and free of clippings by adding our innovative, sturdy Grass Catcher to your StaySharp™ Ree... Read more »\nThe Salsa Rain Barrel System makes it easy to collect up to 58 gallons of water for your garden and lawn. Our rain barrel is ma... Read more »\nMake the most of National Craft Month by preparing some craft kits for your children - let them explore color, texture and dif... Read more »\nThis is the second how-to in a series focused on getting the most out of your basic paper punches. Read more »\nSpring brings in the most wonderful colors and here is a fun way to add a touch of color to your gifts! Read more »\nOur ProCision™ Rotary Bypass Trimmer features a unique dual-rail system that stabilizes the rotary blade, eliminating wiggle fo... Read more »\nPerfect for a wide range of crafting and mixed media tasks, our Amplify® Mixed Media Shears sense blade separation and force th... Read more »\nAdd distinctive style to craft projects of all kinds with a Squeeze Punch that makes every embellishment up to 2X easier to pun... Read more »\nMy idea is to show everyone that they can make something cute and fashionable without spending a lot of money. Read more »\nEmbellishing a plain shirt using a reverse appliqué technique is easy - and your kids will love their personalized outfit! Read more »\nThis year, it seems like spring is way overdue at our house. Read more »\nPerfect for tight, precise cuts, our Amplify® RazorEdge™ Fabric Shears sense blade separation and force the blades back togethe... Read more »\nPerfect for a wide range of sewing and quilting tasks, our Amplify® RazorEdge™ Fabric Shears sense blade separation and force t... Read more »\nPerfect for users with larger hands or anyone who needs to make long cuts through multiple layers, our Amplify® RazorEdge™ Fabr... Read more »\nI always look forward to school being out for the summer (more so than my children, probably!) and the change of pace means we... Read more »\nThis fun project is a great way to send a little love note to your child. These lunchbox notes can be slipped into a backpack... Read more »\nHere is a fun craft for St. Patrick’s Day that is not only adorable, it makes kids stop and think about how lucky they are. Read more »\nChildren love our Blunt-tip Kids Scissors for the handle that’s shiny, bright and smooth, not “sticky” or “bumpy.” Teachers and... Read more »\nOur Big Kids Scissors take the basic design of our teacher-recommended Kids Scissors and enlarge them for kids that are a littl... Read more »\nOur Student Scissors are larger than our Kids Scissors but smaller than adult scissors, perfect for those older children who ar... Read more »\nIntroduced to the world as a quality fabric scissors, the Original Orange-Handled Scissors redefined the standard for cutting p... Read more »\nThe first time you try our PowerGear® Super Pruner/Lopper, you’ll be amazed — but it’s not magic, it’s gears. Our patented gear... Read more »\nOur Comfort Loop Rotary Cutter with a 45 mm blade makes cutting a wide variety of quilting materials comfortable and easy. A cu... Read more »\nDon't miss your chance to win a complete prize pack valued at nearly $200!\nCommunity gardens aren’t just allotments — they’re urban farms, great places to share gardening skills and crops.\nKeep all of your tools performing at their best.\nLiterally! But it doesn’t have to be. With a little forethought, gardening can be an enjoyable activity for anyone who loves plants and being outdoors.\nFirst and foremost, pick the right tool. Twelve of Fiskars garden tools in the PowerGear line were studied by Georgia Tech Research Institute and awarded the Ease-of-Use Commendation by the Arthritis Foundation. The smart design of these pruners, loppers, shears and weed remover give gardeners with arthritis more power while reducing strain on joints. For example,\nThe PowerGear Pruner amplifies cutting force by as much as 5 times, making it easy to cut through branches even when you don’t have much hand strength.\nThe great thing about container gardens is that they’re raised off the ground. If you have arthritis in your back, or it just hurts to constantly bend down, raise your pots up even higher with shelving units or plant stands. A potting bench that allows you to re-pot your plants while standing up is often helpful. On the flipside, when dealing with large pots, it may also be easier for you to garden if you’re sitting down. Pull up a chair or use a storage caddy that allows you to take a seat on it once you’ve gotten the tools you need.\nIt’s also important to pick the right time of day to garden. If you feel stiff in the morning, then wait until your joints have loosened up before you start to garden. When cold weather aggravates your arthritis, wait for a warmer day or wear gloves to help keep your hands warm and protected. Additionally, consider talking to your doctor about some stretches or exercises that you can do to help your body become more limber.\nSimilarly, it’s a good idea to avoid repeating the same motion for more than 20-30 minutes, and stop the second you start to feel a little pain or stiffness. So, if you start off pruning some of your roses, take a break to water your containers, pull out a few weeds that have sprouted up, and then go back to pruning.\nFinally, remember that you are in control of your container garden, not the other way around. You don’t have to grow annuals that need to be pulled out and replaced multiple times a year. Instead, pick slow-growing, low-maintenance perennials. They’ll need to be pruned and repotted less frequently, giving you more time to enjoy your garden and less time wasted on potentially painful garden chores."", 'Choosing a mower\nThe two principal types of mowers available for use on home lawns include ""reel"" and ""rotary"" lawn mowers. Regardless which type is chosen, it is very important to keep the lawn mower blades as sharp as possible. Dull lawn mower blades tear grass blades rather than cutting them. This results in injured plants.\nRotary lawn mowers\nRotary lawn mowers have a whirling horizontal blade. Rotating blades have the potential for throwing stones or other small objects in the path of the mower. Therefore, rotary lawn mowers can be more dangerous to operate than reel lawn mowers.\nRotary lawn mowers cut the grass by impact, similar to the way a machete works. This causes a rougher, more uneven cut than reel lawn mowers. However, rotary lawn mowers do an acceptable job on virtually any lawn. In addition, they are easier to maintain then reel lawn mowers and can cut taller grass.\nMulching lawn mowers\nMulching or recycling lawn mowers are now available. A mulching lawn mower recuts grass clippings into smaller pieces and redistributes them uniformly back into the lawn for rapid decay. When the turf is wet, clumping is common with mulching lawn mowers because of the excess water present.\nPush-type reel lawn mowers\nThere is a growing movement among people with small, easily managed yards, back to ""old-fashioned"" push-type reel lawn mowers. Newer versions are smaller, lighter-weight, and easier to push than the old clunkers you may have used as a youngster. They have the advantage of quiet, fuel-free operation, and can provide good exercise at the same time.\nReel lawn mowers\nReel lawn mowers have a series of revolving blades that cut against a stationary bedknife, much like a scissors. While reel lawn mowers provide the finest quality of cut available, they are expensive, not easily adjusted, and require specialized equipment for sharpening. Also, they can\'t be used where stones, twigs, or other debris are a problem because of potential damage to the cutting units. They are also not well adapted for use in higher heights of cut.\nKeep all mowing equipment in good working condition. Consult lawn mower manual for annual routine maintenance practices.\nAs a general rule, sharpen and adjust lawn mower blades frequently to assure a clean cut. A dull rotary lawn mower blade frays the ends of grass blades and results in brown tips, which are unsightly and increase the plants\' susceptibility to certain plant diseases.\nModern lawn mowers are equipped with certain safety devices which should not be removed or tampered with. One safety feature is a blade that stops turning when the operator\'s hands are removed from the controls. Another is a rear-mounted rope pull for starting the mower. This reduces the chance of getting one\'s foot caught in the blades during starting.\nMany rotary lawn mowers have bagging attachments which affect mower safety. The manufacturer may have a special attachment which is required for safe mowing without a bag. Similarly, discharge chute shields on non-bagging lawn mowers should not be removed.\nSteep slopes are both difficult and dangerous to mow. These slopes are good candidates for alternative groundcovers or a no-mow grassed slope that will stabilize the bank and eliminate the need for routine maintenance such as mowing.\nAdditional mowing safety practices\n- Pick up rocks, wires, and sticks before mowing to prevent them from becoming dangerous projectiles.\n- Keep all spectators away from the lawn mower while it is in operation.\n- Children should not be allowed to operate lawn mowers until they can handle them safely and easily.\n- Always consult the owner’s manual for precautions and instructions on using lawn mowers safely.\n- Use earplugs when operating noisy power lawn mowers.\n- Wear sturdy shoes that protect the feet.\n- Always push the lawn mower forward. Never pull it backward in a motion where a slip will allow your foot under the mower deck.\n- Mow across a slope, not up and down as good footing is important in steep slopes.\n- Delay mowing when grass is wet. Dry grass is less apt to plug lawn mowers and footing is better on dry grass.\nReviewed in 2018']"	['<urn:uuid:1358b5c7-4a17-41eb-b1c1-5d79817a57ab>', '<urn:uuid:de541514-8c05-4843-9abf-170dfd6dff6a>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-13T05:18:19.158519	6	48	1780
40	How do movie ratings work in America vs Britain?	In America, the MPAA (made up of six major movie studios) uses a board of 8-13 parents with children aged 5-17 to assign ratings, without input from child development experts. Their main goals are protecting movies from restrictions and attracting audiences. In Britain, the British Board of Film Classification (BBFC) handles ratings and is generally more tolerant of sexual scenes than America, while America is more tolerant of violence. The BBFC actively consults with 10,000 people for feedback on their ratings and can request cuts to films, as they did with The Hunger Games.	"['By Kristelle Lavallee, staff member at Boston Children’s Hospital’s Center on Media and Child Health\nAre you looking to take the family to a movie but aren’t sure whether your child should see The Hunger Games (PG-13) or Bully (unrated)? If you base the decision on the Motion Picture Association of American (MPAA) ratings, the answer seems pretty cut and dry—“maybe” to Hunger Games, and “no” to Bully. But are the movie ratings the best guide to making healthy media choices for your children?\nBased on the best-selling novel, The Hunger Games is a fantasy story where teenagers are pitted against each other in a battle to the death broadcast on live TV. In contrast, Bully is a “slice of life” documentary about peer-on-peer bullying in American schools.\nBoth movies center on children and teenagers, but the fictional Hunger Games, portraying “intense violent thematic material and disturbing images” (MPAA’s description) was given a PG-13 rating, while the documentary Bully with “some language” (MPAA again) was rated R. The producers of Bully knew that accepting an R rating would greatly limit the film’s impact as an educational tool for young viewers, so they chose to release it unrated. But when a film is released without an MPAA rating, it comes at a price: Fewer theaters are willing to show it, and those that do will treat it the same way they treat films unrated for extreme violence or sex.\nClearly the MPAA has a lot of power in terms of how many people will be able to see a film, but who really applies these standards, and what criteria do they use?\nThe MPAA is made up of six major American movie studios and serves as the “voice and advocate” for the film and television industry. That means the MPAA is the tool that media producers use to govern themselves and represent their interests to the government and public. Since 1968, the MPAA has assigned movie ratings, which it claims “exists to give parents clear, concise information about a film’s content, in order to help them determine whether a movie is suitable for their children.”\nSounds like a useful tool. But it is important to remember that the primary goals of the MPAA studios are to protect their movies from external restrictions and to attract as many ticket buyers as possible. The MPAA decides the criteria on which ratings decisions are made, chooses the members of the ratings board, and administers the ratings in partnership with the National Association of Theater Owners, many of whom will not show any film that is unrated.\nSo who does the MPAA employ to make these decisions? Parents. Film ratings are not assigned by child psychologists, educators, pediatricians, or anyone with child development credentials, but by a full-time board of eight to 13 parents whose only qualifying credential is that they have at least one child between the age of 5 and 17. (And even that is loosely applied—for an in-depth look at the ratings process, see the documentary This Film is Not Yet Rated.)\nAccording to the MPAA, the job of the ratings board is to “reflect what they believe would be the majority view of their fellow American parents in assigning a rating to a film.” In short, our film ratings are determined by the opinions of a few parents on what their peers will find socially acceptable.\nBut research has shown that this system has been inconsistent. For example, one study found that the amount of violent content allowed in PG-13 rated films significantly increased from the late 1980s until the mid 2000s, resulting in a shift of one full rating level every 11 years. Both this and other studies concluded that child development and health care professionals should participate in assigning ratings, so the decision-making can combine parental opinions with research-based guidance.\nSince each child will interpret and react to on-screen action differently, parental input about what is and isn’t best for children to see is still essential. A film like Bully may be considered R material by the MPAA, but can be accepted, even embraced by parents who understand the film’s important message. At the same time, despite the MPAA’s reassurance that viewing does not require adult supervision, The Hunger Games story of children being forced to kill other children may be too potent for young viewers, even those that have read the book. These nuances aren’t captured by ratings.\nFor that reason, the ratings system can be used, at best, as a starting point—and then parents can take into account the individual child, media content and expert advice to help them choose what to share with their children.\nUpdate added after original publishing:\nOn April 5th, The Weinstein Company announced that the documentary, Bully, will be released with a PG-13 rating. The announcement follows an agreement that Weinstein Co. made with the MPAA to re-cut the film. As a result, Bully editors removed the audio on three obscenities, however, the controversial school bus scene that features three F-words (uttered by children) remained untouched. The new cut of Bully makes the film an exception to the MPAA’S standard of giving an R rating to any film that features the F-word more than twice.\nThe new PG-13 rated version of Bully allows all children to see the film without an adult. The rated version will also be shown in most major theater chains, which typically have policies against showing unrated movies. Bully opens nationwide on April 13th.\nFor other helpful resources on making these choices, please visit the Center on Media and Child Health at: http://www.cmch.tv/\nYou can also ask Children’s Hospital Boston’s own ‘Mediatrician’ at: www.askthemediatrician.org\nFor useful parent-friendly film reviews, try: http://www.commonsensemedia.org/movie-reviews/', 'The complaint was received by the British Board of Film Classification (BBFC), which has revealed the Woman in Black, starring Daniel Radcliffe, was the most complained about film last year.\nA viewer raised concerns with the BBFC about the danger of depicting children playing on railway tracks, in what was the first complaint received by the BBFC since the Railway Children was classified U in 1970, a rating which means it is generally suitable for children aged four and over.\nDavid Cooke, director of the BBFC said the film had ""always been a U on every platform."" In the board\'s annual report it said it judged that as the Railway Children is set in the Edwardian period, access to trains was very different from today, and also the film showed the “potential harm to children if proper care is not taken” near railways.\nThe Woman in Black, which saw Daniel Radcliffe step out of his infamous Harry Potter character to take on the role of haunted solicitor Arthur Kipps, attracted a raft of complaints for its theme of supernatural deaths. A total of 134 people complained that it was too dark and unsettling for its 12A certificate.\nDavid Cooke, director of the BBFC, said: “It was a much bigger number than what we get for other films.\n“There’s usually a buzz of controversy in something that may confound expectations in some way. The fact that people expected to see Harry Potter was one of them. It’s also a strange and scary film.”\nThe board judged that as the film was based on a well-known book studied in schools, and was the basis of a stage play, the rating was appropriate.\nBut Mr Cooke said the most controversial film the board had had to consider in the last five years was The Dark Knight starring Heath Ledger as the Joker, which he said “confounded expectation because it was darker than people were expecting.”\nWhen people were later asked in a consultation about the 12A classification of the film 69 per cent agreed with it.\nThe Woman in Black is being used as part of an ongoing consultation by the BBFC with 10,000 people where they ask for feedback on the ratings they have given. In the last year the board classified 87 films with a U rating, and 150 as PG.\nA total of 234 were given a similar 12A rating to the Woman in Black, while 315 were rated as a 15 and 64 for over 18s.\nMr Cooke said they had not rejected any films in the last year but had requested cuts to some.\nSpeaking at the launch of the BBFC’s annual report, he said the film adaptation of the Hunger Games, where the BBFC requested 20 seconds of cuts for it to receive its 12A rating, required the most contact with producers who asked for advice before officially submitting it to the board. The cuts were made to the \'cornucopia\' scene where the children initially dash for weapons and start to attack each other.\nThe BBFC would not give any hints on the second instalment of the Hunger Games, starring Jennifer Lawrence but said they ‘may have seen’ it but that it had not yet been formally classified.\nMr Cooke said The Human Centipede 2 was one of the films that ‘stuck in his mind.’\nHe said: “It was partly about media effects. We initially rejected it and it was going to appeal. Then quite late in the day the team came to us with a proposal for cuts. We agreed to a longer series of cuts.”\nHe said that generally the British industry seemed to be \'slightly more\' tolerant of sexual scenes than the American industry, but the reverse was true for violence.']"	['<urn:uuid:e2649922-782d-464e-9342-a30a1ddc4a99>', '<urn:uuid:694a6789-1eac-49cc-8840-81fa2a739a04>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T05:18:19.158519	9	94	1580
41	winter landscape photographer camera exposure techniques white balance settings fog mist conditions	For winter landscape photography, proper exposure and white balance are crucial. In foggy and misty conditions, apply positive exposure compensation starting at +0.7EV to preserve pale, misty tones rather than getting muddy grey. For snow scenes, you'll need to overexpose even more, anywhere from +2/3 to 2 stops, as cameras tend to underexpose bright scenes. Regarding white balance, early hours and sunset can produce blue casts, while regular daylight hours may give warm tones. To compensate, use 'cloudy' or 'shade' white balance modes, though auto white balance may give inconsistent results as lighting conditions change.	['53 essential photo ideas for winter\nJust because it’s miserable outside doesn’t mean you can’t take great winter photos. With a bit of imagination and perhaps some warm clothes, there is no reason your photography has to hibernate for the winter.\nWinter offers some wonderful picture-taking opportunities, both outdoors and in, and we hope this list of 53 photo ideas inspires you to crawl out from under your duvet. Snow, rain and frost make great subjects, and the constantly changing light can be both a challenge and a revelation.\nOf course, the short winter days mean you might only have a few precious hours of light to work with, but there are just as many photo opportunities after dark, whether you’re out in the city streets or in the comfort of your own home.\nWith our collection of winter photo ideas, shooting tips and gear guides, you’ll soon be wishing that this cold spell will last forever… OK, maybe not!\nBRAVE THE OUTDOORS: CELEBRATE WINTRY WEATHER!\nBad weather can often lead to great photography. We can’t always rely on picture-postcard snow, but storms, rain and fog can be just as good.\nTip 1: Fog, Mist and Frosty Mornings\nYour best chance to catch mist and fog is early in the morning, before the sun as had time to burn it off. You could check the forecast the day before, but it’s difficult to predict the best conditions. Get into the habit of carrying a camera with you everywhere, if you can, and set off for work a few minutes early just in case you see the perfect scene on the way in. Use a telephoto lens to flatten perspective and reduce objects to simple shapes. To preserve the pale, misty tones, apply a little exposure compensation, starting at +0.7EV and adding more if necessary. If you don’t, the camera’s system will make the mist come out a muddy grey.\nTip 2: Sunrise and sunset\nWhether you’re fetching the paper or taking the dog for a walk, take your camera. Sunsets can be disappointing, but winter dawns often bring the most spectacular red and pink tones, captured here in this picture by David Queenan. You have to shoot quickly, though, as the effect can disappear in minutes. If you have time to plan, look up sunrise and sunset times, and make sure you’re in a good location.\nTip 3: Protect your camera\nSea spray can leave gritty deposits on your lens, so consider fitting a UV filter. It won’t affect the pictures, but will protect the lens. Strong winds off the sea can carry sand into your camera’s crevices so put your camera in a plastic bag and fix the opening around the lens barrel with a rubber band. Always use a shoulder strap in case the camera slips out of your hands, and keep spare batteries in an inside pocket – extreme cold reduces their capacity.\nTip 4: Blue skies and snow\nSometimes, the contrast between colour and no-colour is really effective – the combination of blue skies and snow in this shot by David Queenan is a classic example. Look for simple compositions, and be careful with the exposure. You’ll need to apply positive EV compensation to make the snow come out white and not grey.\nTip 5: Stormy winter skies\nThis shot, by Sebastian Kraus, gets its power partly from the composition and partly from that dramatic, leaden sky. To capture this effect you’ll need to use a graduated filter and, maybe, do a little work in Photoshop. Remember to treat the sky as part of the composition, not just an empty space. The shapes of the clouds are compositional elements too, and the extent to which you darken (or ‘burn in’) the sky will affect the balance of the picture too. Dodging and burning is normally considered to be a black-and-white technique, but it’s every bit as effective in colour photography too.\nTip 6: ND filters\nThe movement in these waves has been captured with a shutter speed of 1/2sec. If the light is too bright, use an ND (neutral density) filter. To control the bright skies, Jon Sargisson used a graduated ND filter.\nQuick tips for using filters\nTip 7: Square filters\nSquare, not circular, filter systems let you use the same holder and filters with all your lenses. All you need are different adaptor rings.\nTip 8: Get a polariser\nPolarising filters intensify blue skies and cut down glare and reflections from water.\nTip 9: Grads are great\nGraduated filters tone down bright skies on overcast or stormy days.\nTip 10: NDs for blur\nNeutral density filters let you use longer exposure times to create blurry effects.\n10 common landscape photography mistakes every photographer makes (and how to fix them)\nThe 10 Commandments of Landscape Photography (and how to break them)\n14 photo editing tips and tricks every landscape photographer must know\nHow to photograph anything: best camera settings for landscape photography\non Wednesday, February 15th, 2012 at 11:11 am under Photography Tips.\nTags: digital cameras, flash photography tips, hot, photo ideas, photography tips, winter photography tips', 'Photography has a lot of bitter-and-sweet challenges; and one of them is shooting white stuff properly. By properly we mean with the appropriate tonal range that enables winter landscapes to look rich in texture and full of character.\nHere you’ll find some valuable information for you to make the most out of your winter landscape photography, as well as some nice winter photography ideas and tips and one simple yet challenging assignment on abstract winter landscape imagery.\nYour Camera Doesn’t Like Bright Stuff\nCamera manufacturers have made a huge effort for developing precise tools for measuring light. The problem is that these all aim to achieve perfection, and when it comes to bright stuff, your camera will try to dim down the reading.\nIn photography there is something usually known as “exposing to the right”, and that is the habit of slightly overexposing light in a photograph. This is an absolutely common practice when it comes to winter landscape photographs, and here is how it works:\nWhen you read light with your camera’s built-in light meter, you’ll see that it will make a very conservative reading of the light in front of it. This means that after shooting a centered exposure you’ll notice that the white areas actually look gray, and even a bit dull. Here is when the overexposure technique comes into play. There is no exact science and your decisions will be driven but the amount of brightness in front of you. But you can overexpose +2/3 of a stop to even 2 stops, again, that will depend on you.\nThe reason why is so important to expose to the right while making snow landscape photos is because it is easier for your sensor to recover information from bright zones rather than from the darks and shadows. Therefore, you’ll be compensating all the blown highlights in the raw development phase of your workflow.\nThis results in properly lit images with information across the whole frame. From shadows to lights, you’ll have rich texture, giving that highly desired dramatic punch from fine art landscape photographs.\nWhite isn’t That White\nWhen it comes to winter landscape photographs, white will be all over the place, and you’ll highly notice something odd. Your photos will have warm or cold color casts that doesn’t really resemble perfect whiteness.\nThis can be corrected by using the white balance setting on your camera. If you go out there shooting in the early hours of the day or during sunset, you’ll likely get some blue casts, and if you shoot during the regular hours of the day, you might get some warm tones as well.\nFor these situations in which you start getting too blue casts, make sure to compensate with “cloudy” or “shade” white balance modes. That will give you a better result in camera. You can also set your camera’s white balance mode to “auto”, but mind that this will give you inconsistent results as the sun or clouds move around you. Of course, the best way to compensate white balance is during the raw development stage of the photographer’s workflow, but there’s nothing wrong with giving well-crafted inputs to it.\nBeyond that, winter landscape photography shares the same principles of regular landscape imagery. Therefore, scouting, tripods and ND filters is mandatory. Well, that is enough about technicalities, let’s get some fun with these winter photo ideas:\nGet a Different Perspective by Using a Drone\nDespite being so popular, drones are still offering extremely unique ways of seeing life around us. When it comes to winter landscapes colors get sort of muted out, making everything closer to graphic design or painting. Even the most common objects around us look compelling from a higher point of view.\nSome photographers like the bird-like vantage point they get with drones, and some others like shooting straight down to the ground. No matter which one is your favorite, you’ll get a more compelling approach towards winter photography with these amazing tools. Oh, and moving around with snow is harder, so kudos to drones that enable us to move quicker around the woods.\nStart Using Color Accents\nAs said before, winter photography offers a lot of graphic-design-like layouts. This happens because there is always a strong contrast between both extremely bright and dark zones in the frame. This is notoriously good looking; but sometimes things can get a bit more interesting when a colored accent pop’s inside the frame. And we are talking about bright colors like red and orange. Take that into account and your compositions will have huge impact from here on!\nHigh-Keys are BAE\nHigh-key exposures always give a dream like nature to photographs, but they usually are better executed during controlled lighting situations. That obviously means studio photography, but we nature lovers hate being inside a studio. Snow photography will give you absolute freedom for high-key images, just mind the exposure compensation we mentioned before.\nWhat About that Assignment?\nAfter these brief yet insightful winter photography tips and ideas you’ll be better known for creating some beautiful photographs. In order to think a bit outside the box, we invite you to seek some nice abstractions around you.\nYou probably won’t find using a telephoto lens in a regular landscape photo ideas blog post elsewhere, but using a long lens is the best way for spotting abstract images in nature.\nThe important thing about Abstract photos is that they don’t totally reveal the complete nature of an element, while still keeping the power of telling enough about the element.\nWrapping it Up!\nRemember that cameras will always try to achieve perfection, but that doesn’t necessarily mean interesting or creative photographs. Any photograph created with purpose should not only compel the audience, but be able to reflect the photographer’s vision of the world. We do hope that this post might be able to trigger some new talented photographers out there; and don’t expect that winter landscapes will come to you; you need to go out there and discover them by yourselves!']	['<urn:uuid:780b6ebc-7903-4232-9ec6-90e9a02268b1>', '<urn:uuid:af5d5c11-61b6-4675-8774-a2b73c7f7c60>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:18:19.158519	12	95	1856
42	What are some of the main things that scientists have to deal with in their daily work, even when trying to get exciting results?	Scientists have to deal with 'grunge work' in their daily activities, including cleaning their own equipment and sometimes even hitting their fingers with hammers. While this work isn't always exciting, the eventual reward makes it worthwhile.	['If your goal is to get an A+ on your project, win awards at the science fair, and learn many new things about science, then choose a topic that interests you. If you find after some work that your topic is not as interesting as you originally thought, if possible stop and select another one. Since it takes time to develop a good project, it is unwise to repeatedly jump from one topic to another. But if you are required to stick with your first choice, make the best of it. After all, a scientist’s work—and you are a scientist—isn’t exciting all the time. There is the grunge work –they even clean their own equipment and hit their fingers with hammers—YIKES!! But the reward, no matter how long it takes, makes it all worth while. Approach your project with the idea that you are going to uncover some very interesting facts that you didn’t know. Your reward for your good work just might be that A+.\nThings to consider:\nThe objective of a science project is to learn more about science\nSuccessful projects do not have to be complex\nExcellent projects can be developed that answer simple questions by doing simple experiments\nCommon Science Fair Categories and Topics\nAgriculture. The science that deals with farming and raising livestock.\nAstronomy. The study of celestial bodies.\n1. Celestial motion. The study of the motion of celestial bodies.\n2. Natural satellites. Celestial body that moves in a circular path about another. Earth’s natural satellite is the Moon.\n3. Artificial satellites. Manmade objects that move in a circular path about celestial bodies.\n4. Stellar science: The study of stars, including their composition, magnitude, classification, structure and groupings.\nBiology. The study of living things.\n1. Botany. The study of plants and plant life. Subtopics include the following:\na. Anatomy. The study of the structure of plants, such as seeds, flowers, leaves, and stems.\nb. Behavior. The study of actions that change the relationship between a plant and its environment, such as responses to light, gravity, and touch called respectively phototropism, geotropism, and thigmotropism.\nc. Physiology. The study of life processes of plants, including germination, transpiration, transportation of nutrients, and plant growth.\n2. Ecology. The study of relationships of living things to other living things and to their environment.\n3. Food science. The study of food, including the causes of food decay and the nature of food, such as nutritional value.\n4. Microbiology. The study of microscopic organisms.\n5. Reproduction. The study of reproduction, either sexual or asexual.\n6. Zoology. The study of animals and animal life. Subtopics may include:\na. Anatomy. The study of the structure and use of animal body parts, including eyes, heart, lungs, ears.\nb. Behavior. The study of actions and reactions of humans and animals using observational and experimental methods.\nc. Physiology. The study of the life processes of animals including respiration, circulation, nervous system, and metabolism.\nChemistry: The study of what substances are made of and how they change and combine.\n1. Chemical changes. The study of the changes made when the particles of one or more substances combine or break apart and recombine in a new way to produce one or more different substances.\n2. Physical changes. A change that only affects the appearance of matter but has no effect on what it is made of.\na. Crystallography. The study of the formation of crystals as well as the crystals themselves.\nb. Mixtures. The study of the properties of the whole and/or parts of a mixture, which is a combination of substances in which each part retains it own identity.\nc. States of matter: A study of how the particles of a substance change forming three common states on Earth: gas, liquid, solid.\nd. Viscosity. The study of how easily fluids can flow.\nEarth Science. The study of the parts of Earth: the atmosphere, the lithosphere, and the hydrosphere.\n1. meteorology: The study of weather, climate, and the earth’s atmosphere.\n2. oceanography: The study of the oceans and marine organism.\n3. physiography: The study of the physical features of Earth’s surface.\nEngineering: The study of applying scientific knowledge for practical purposes.\n1. chemistry technology engineering: The branch of engineering concerned with the application of chemistry in the production of goods and services that mankind considers useful.\n2. food technology engineering: The branch of engineering concerned with the application of food science to the selection preservation, processing, packaging and distribution of safe, nutritious, and wholesome food.\n3. product-development engineering: The branch of engineering concerned with designing, developing, and testing new products.\n4. structural engineering: A branch of engineering concerned with designing as well as testing the strength of structures, including building, bridges, and dams.\nMathematics: The use of numbers and symbols to study amounts and forms.\n1.angular apparent measurements: A measurement in degrees of how far or how large objects appear to be.\n2. ratio: A pair of numbers used to compare quantities, such as speed.\nPhysics: The study of forms of energy and the laws of motion.\n1. energy: Energy is the capacity to make things change, and the process of making them change is called work, which is accomplished when a force (F) (a push or a pull on an object) causes an object to move, which also means the process of transferring energy. Subtopics may include the following:\na. heat: The energy that is transferred between objects because of differences in their temperature.\nb. radiation: Energy in the form of waves that can travel through space; also called radiant energy.\nc. sound: Waves produced by vibrating material that can only travel through a medium (any solid, liquid, or gas).\n2. mechanics: The study of objects in motion and the forces that produce the motion. Subtopics may include the following:\na. buoyancy: The upward force of a fluid on an object placed in it.\nb. periodic motion: Any type of motion that successively repeats itself in equal intervals of time.\nIt is important that you enter your project in the correct category. Since science fair judges are required to judge the content of each project based on the category in which it is entered, you could be seriously penalized if you enter your project in the wrong category.\nSome topics can correctly be placed in more than one category; for example, the study of bacteria could be biology, food science or microbiology. If you are in doubt about the category of your project, find the topic in a text book to identify the category it is listed under. If you still have doubts, ask for professional help, such as your teacher or me here—-ASK JANICE\n|Janice VanCleave’s Guide to More of the Best Science Fair Projects|']	['<urn:uuid:0c39c001-5654-4cfe-97f9-c6a49ddc0c35>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	24	36	1118
43	What rights did Jensen's family claim under NY Workers' Compensation?	Jensen's family was awarded workers' compensation benefits through the NY Workers' Compensation Commission after Jensen broke his neck and died while working on a ship for Southern Pacific Company. However, this award was later found to conflict with federal maritime law.	['Southern Pacific Co. v. Jensen (US 1917)\nFacts: Jensen was an employee of the Southern Pacific Company (SP). He was working on a ship when he broke his neck and died. SP objected to the award from the NY Workers’ Comp. Commission that was awarded to his remaining family, arguing the award violated Art. 3, § 2 of the Const., conferring admiralty to federal courts. NY SC upheld the award.\nHolding: -P is liable to employees for on the job injuries under the Federal Employers’ Liability Act (FELA).\n-As applied here, the Workmen’s Compensation Act (of NY) conflicts w/ the general maritime law under the Const.\n-Everything about this case is maritime and w/i admiralty jurisdiction\n-If NY can subject foreign ships coming into her ports to such obligations as those imposed by her compensation statute, other states may do likewise. The necessary consequence would be the destruction of the very uniformity in respect to maritime matters which the Const. was designed to establish, and freedom of navigation b/w states and w/ foreign countries would be seriously hampered and impeded\n-The remedy of the Compensation Statute is unknown to common law, therefore is not allowed to be adjudicated by states under the saving to suitors clause\n-Exclusive jurisdiction of all civil cases of admiralty and maritime jurisdiction is vested w/i the federal district courts\n- Southern Pacific Co. v. Jensen, 244 U.S. 205 (1917)\n- FACTS: Respondent’s decedent was a worker at a pier where the petitioner’s ship was docked. The decedent’s job was to load the ship with lumbar. He was killed while performing his job. Respondent filed a workers’ compensation claim against the petitioner in NY state court for wrongful death benefits for her deceased husband.\n- ISSUE: Is the NY workers’ compensation statute preempted by the federal admiralty law?\n- HELD: The state workers’ compensation statute is unconstitutional to the extent that it conflicts with the Constitutional grant of admiralty jurisdiction to the federal system.\n- REASONING: The work\nof a stevedore is maritime in nature; his employment was a maritime\ncontact; the injuries which were received were maritime; the rights and\nliabilities of the parties in connection therewith were matters clearly\nwithin the admiralty jurisdiction. The remedy given by the statute is\nunknown to the common law and is not saved to suitors from the grant of\n- Justice Holmes, dissenting: the remedy that is available is created by the NY common law. The mode of obtaining the remedy is what is different. This does not make the NY workers’ compensation statute incompatible with admiralty jurisdiction. The maritime law is not a corpus juris – it is a very limited body of customs and ordinances of the sea. The common law must be allowed to supplement the maritime law.\n- Significance of the case: this was the first case where the Supreme Court held that Art. III, § 2 provided jurisdiction and the power to create admiralty law.\n- Key language for determining whether federal maritime law preempts: if the claim is maritime, if there is a federal statute or a need for a federal common law rule to promote maritime shipping and commerce, then state law is preempted.']	['<urn:uuid:747a317a-ed00-4b8c-9ed9-29ab7c562783>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	10	41	533
44	How do consumer attitudes affect food technology and environmental outcomes?	Consumer attitudes show a complex dynamic where there's concern about genetically modified foods' safety, particularly among Japanese consumers who are uneasy about potential health risks, despite these foods being popular with producers. This selective consumer behavior influences food development and regulatory processes. Meanwhile, consumer purchasing patterns in both developed and emerging countries are driving increased land use for agriculture, leading to biodiversity loss and reduced ecosystem capacity to absorb CO2. This shows how consumer behavior impacts both food technology adoption and environmental degradation.	"['What are the best dietary practices and farming methods to promote health? The answer may depend on whether one looks at the health of individuals or the health of the planet (planetary health or PH). PH will equate to a healthy ecosphere fostered by dietary/farming practices that are less resource-intense, potentially decreasing starvation and carbon emissions. Best practices also may depend on whether by health one means lack of observable disease (such as obesity, nutritional deficiency, diabetes, or cancer), optimal health (also known as wellness), or longevity.\nNutritional genomics offers a way to optimize human health and the quality of life. It is an attractive endeavor, but one with substantial challenges. It encompasses almost all known aspects of science, ranging from the genomes of humans, plants, and microorganisms, to the highest levels of food science, analytical science, computing, and statistics of large systems, as well as human behavior. This paper describes the underlying biochemistry that is targeted by the principal issues in nutritional genomics, which entails genomics, transcriptomics, proteomics, and metabolomics.\nThe production of genetically modified foods for an increasingly informed and selective consumer requires the coordinated activities of both the companies developing the transgenic food and regulatory authorities to ensure that these foods are at least as safe as the traditional foods they are supplementing in the diet.\nShokuhin Eiseigaku Zasshi. Journal of the Food Hygienic Society of Japan\nWe performed experiments on in vitro digestion of newly expressed proteins by SGF (simulated gastric fluid) and SIF (simulated intestinal fluid) to assess the allergenicity of food components derived from biotechnological modification. For newly expressed proteins, we chose CP4-EPSPS (5-enolpyruvylshikimate-3-phosphate synthase from Agrobacterium sp. strain CP4) and Cry1Ab derived from Bacillus thuringiensis subsp. kurstaki strain HD-1. The former is expressed in GM-soybeans and the latter is expressed in GM-corns.\nRisk Analysis: An Official Publication of the Society for Risk Analysis\nEmpirical examinations of the ""social amplification of risk"" framework are rare, partly because of the difficulties in predicting when conditions likely to result in amplification effects will occur. This means that it is difficult to examine changes in risk perception that are contemporaneous with increases and/or decreases in social or media discussion of the risks associated with a particular risk event.\nSafety assessment of genetically modified food crops is based on the concept of substantial equivalence, developed by OECD and further elaborated by FAO/WHO. The concept embraces a comparative approach to identify possible differences between the genetically modified food and its traditional comparator, which is considered to be safe. The concept is not a safety assessment in itself, it identifies hazards but does not assess them. The outcome of the comparative exercise will further guide the safety assessment, which may include (immuno)toxicological and biochemical testing.\n[Nihon Kōshū Eisei Zasshi] Japanese Journal of Public Health\nCurrent and future trends regarding genetically modified (GM) crops and food stuffs were reviewed, with a particular focus on public acceptance and safety assessment. While GM foods, foods derived from biotechnology, are popular with growers and producers, they are still a matter of some concern among consumers. In fact, our recent surveys showed that Japanese consumers had become uneasy about the potential health risks of genetically modified foods.\nThe genetically modified (GM) crops cultivated at present have new properties of benefit to agriculture. It is expected that in the future GM crops will also be cultivated with more complex genetic modifications that are aimed at improving the nutritional and health value to the consumer. The safety assessment of GM foods before market approval is based on a comparison of the characteristics of the GM food with those of the conventional counterpart. Identified differences are thoroughly tested for their toxicological and nutritional consequences.\nThis synopsis reviews published in vivo studies on possible health consequences of genetically modified food and feed where the ingredients in question have consisted of genetically modified plant materials. The following, however, have not been taken into consideration:--ingredients consisting of genetically modified microorganisms or parts of animals/fish--ingredients produced by/from genetically modified organisms but without any DNA present--studies on consequences for the environment or biodiversity--in vitro studies or computer simulations.\nFood and Chemical Toxicology: An International Journal Published for the British Industrial Biological Research Association\nThe most important results from the EU-sponsored ENTRANSFOOD Thematic Network project are reviewed, including the design of a detailed step-wise procedure for the risk assessment of foods derived from genetically modified crops based on the latest scientific developments, evaluation of topical risk assessment issues, and the formulation of proposals for improved risk management and public involvement in the risk analysis process.', 'Biodiversity under attack\nBiodiversity researchers sound a warning: Global biodiversity is being destroyed by rapid economic and population growth, which is undermining progress.\nNew technologies such as field robots are already making agriculture more efficient and sustainable. New habitats for insects and other animals are being created with flower strips or mixed cultivation of crops. This contributes to the protection of biological diversity. But can all these measures halt the global loss of biodiversity? Apparently not. This is the result of a study presented by researchers from the German Centre for Integrative Biodiversity Research (iDiv) and the Martin Luther University Halle-Wittenberg (MLU), published in the journal ""Nature Ecology & Evolution"".\nEnvironmental degradation increases despite effective land use\nAlthough land use has become more effective over the years, overall environmental degradation has also increased. ""The world\'s population and economy are growing so fast that they are compensating for the progress made,"" explains Alexandra Marques from the iDiv research centre and the University of Halle. The team led by Alexandra Marques and Henrique Pereira has been investigating how land use has affected biodiversity and ecosystem services in 2000 and 2011, and how this has changed over the years. Data on bird populations, land use and CO2 sequestration were linked to economic models.\nGrowing land use through growing consumption\nThe main focus was on the role of population and economic growth in this context. The result: the growth of population and economy has led to increased land use all over the world. The reason for this is the associated increase in consumption. The production of the corresponding goods requires more and more land. As a result, nature has to give way to fields and plantations, which in turn is detrimental to biological diversity and endangers ecosystems.\nTropics hardest hit by species loss\nAccording to the researchers, almost every purchase of a food product would indirectly influence nature from a distance. The developed countries are outsourcing 90% of the destruction caused by the consumption of agricultural products to other parts of the world. According to the study, the number of bird species threatened with extinction by land use increased by up to 7% between 2000 and 2011. The loss of biodiversity takes place almost exclusively in the tropics, as the researchers state.\nEcosystems bind less CO2\nDuring the same period, the loss of land reduced the global potential of ecosystems to absorb CO2 from the air by 6%. The loss of land reduced the global potential of ecosystems to absorb CO2 from the air by 6%. A quarter of the decline is due to changes in the use of land for agriculture and forestry in Europe and North America. The researchers believe that the industrialized countries must take responsibility. They need to pay greater attention to the long-distance responsibility for the destruction of biodiversity and to the effects of their own climate policy, they say. ""We need an environmental policy that considers climate change and the change in biological diversity together,"" recommends Pereira.\nConcepts against population growth\nConsumption increased not only in the industrialized countries, but also in other countries of the world during the period under study, thus correcting the image of the ""culprits"" of biodiversity loss: ""It is not either the North or the South - it is both. The emerging countries are currently overtaking the industrialized countries as the main cause of biodiversity loss,"" says Pereira. The researchers come to the conclusion that in order to protect biological diversity, nature conservation policy needs concepts against population growth and for sustainable consumption.']"	['<urn:uuid:12193d29-9ca0-4afa-973e-79472b1d0ec9>', '<urn:uuid:e6d1d990-dc68-455e-90cd-efb37090a78d>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:18:19.158519	10	83	1351
45	pershore abbey original size how big compared worcester cathedral who bought it when monasteries ended	Pershore Abbey was once larger than Worcester Cathedral in size. When the monasteries were abolished, the parishioners bought the church for £400.	['PERSHORE TOWN – WORCESTERSHIRE\nSituated 9 Miles from Worcester and 6 Miles from Evesham it can be reached via the B4084 road.\nThis small town of about 8000 population has been known in ancient times as “Persere”*\nand “ Pearsore”.\n*Domesday – Old English Persh “ Osier willow used for basket\nweaving “ Old English ora “river bank.”\nSet on the banks of the river Avon it offers much for the visitor.\nPershore is known for its Georgian architecture and interesting to note that most of the\npremises in Bridge Street and Broad Street are listed buildings.\nBesides its past and present connection with market gardening and growing of many varieties of plums it once had an important\nconnection with the wool trade by way of sorting and grading.\nThere are a wide variety of shops offering a range of goods from gifts and souvenirs to\ndomestic provisions with many offering local produce.\nWith its many eating places, Hotels, Pubs, Cafes, and Take-Aways there is ample\nprovision for the visitor.\nNearby Bredon Hill at nearly 1000ft high is the largest outlying hill of the Cotswolds, one of the 176 classified Marilyns in England. A folly in the form of a tower sits on the summit and is called Parsons Folly.\nBredon Hill at 299 metres high has no other hills in its way or that are higher, if you go due east, until you reach the Ural Mountains in Russia.\nAs in ancient times an important place to visit is Pershore Abbey, and is believed\nto have been founded by Ethelbert , King of Mercia , in AD689.\nA historical notice of the founding of the abbey is to be found in Leyland’s “Collectanea” in\nwhich reference is made to Abbess Hilda‘s death in AD680.\nIt was in those days that the monasteries of Pershore and Gloucester were founded by Oswald\nand Ostric. Oswald founding Pershore , Gloucester being founded by Ostric.\nIt was in 976 that it was destroyed by Aelhire and was restored by Ethewald and Wada and\nOddo in 983.\nOddo brought the relics of St.Edburga from Winchester to Pershore and although at the\ntime the church was dedicated to St. Mary, St.Peter and Paul ,Edburga and the Holy Cross\nit eventually became just The Holy Cross but this just applied to the nave. It is believed\nto have become Benedictine in 984.\nMuch of the structures were destroyed by fire and storms during the 13th century and no\nrecords exist of the conventual buildings. At the time of the abolition of the monastries\nthe parishioners bought the church for £400\nThe Abbey is now much reduced to from its original size (once in excess of Worcester Cathedral),\nit is however still a formidable and attractive building with beautiful stained glass and\nmemorials which include a war memorial to those killed in two world wars.\nThe font is believed to be Norman and is decorated with carvings. Outside buttresses added\nto the main structure now give support to the building and were added in 1913. In 1996/97\nwhen a new floor was being laid, Saxon foundations were found.\nMany events take place in the abbey besides worship. Recitals and concerts are held regularly,\nit is possible to see a list of these displayed in the abbey.\nWhilst visiting it is worth taking a short walk in the abbey park, here the stump of a\ntree has been carved, it is truly a work of art which was completed by the sculptor Tom\nPershore was once two parishes , The Holy Cross and St. Andrew’s the parishes only\nbeing divided by the width of a road , now day’s the parishes have been unified into\nThe Holy Cross.\nThe church of St Andrew is now redundant but is now a visitor’s centre and during\nthe summer months serves teas.\nThe indoor retail market is signed off the High Street, established in 1976,it is a popular\namenity, visitors from far and wide take advantage of the opportunity to browse through\nthe 34 stalls which range from food to clothes to gifts and household items, it is open\non Wednesdays, Thursdays, Friday’s and Saturday’s 9am to 5pm.\nCOMMUNITY ARTS CENTRE\nThe Community Arts Centre - named No. 8 is situated in the High Street and\nis close to the junction of Broad Street. It was inspired by the local community, founded\nby the local community and is now supported by them.\nIt offers the opportunity for visitors to enjoy a diverse arts experience; its friendly\natmosphere has attracted many to enjoy the facilities that include a cinema screening mainstream,\nart house & children’s films as well as being a live events venue programming\nopera, theatre, music and dance.\nThere is also the chance to see a continuous programme of art and craft exhibitions that\nare set within the coffee bar. Number 8 also serves Homemade cakes, tea and coffee, these\nare available Monday to Saturday 10am – 4pm.\nPERSHORE LEISURE CENTRE\nPershore Leisure Centre was rebuilt in 2002 the centre is modern, bright and spacious,\nclose to the town centre and to the Riverside Gardens it ideally situated for the visitor\nwishing to take advantage of its facilities.\nComprising of a 25m Swimming Pool and a Fitness room that includes more than 30 items\nof Cardio-vascular resistance equipment. It also has a Health Suite – Sauna, Steam\nRoom and Spa. As well as these a Dance Studio and a Sports Hall is on site. The Pershore\nLeisure Centre also manages the sports facilities at Pershore High School, these are available\nfor hire, on offer is: 1 Astro – turf Pitches, 2 Grass Pitches, 4 Tennis & 3\nNetball courts, 4 Court Sports Hall.\nPERSHORE RIVERSIDE GARDENS\nThe Riverside Gardens provide a picnic area with seating enabling visitors to enjoy a\nrelaxed view of the River Avon with its river traffic consisting of boats and barges.\nPershore Bridge is just a short distance along Bridge Street (B4084 ) towards Evesham;\nthe historic bridge was the scene of skirmishes during the civil war and scars from this\nare to be seen in the stonework.\nThe bridge contains stone from the former castle at nearby Elmley Castle which was\ndestroyed in the War of the Barons during the reign of Henry the III. A small car park\nand picnic site gives visitors the chance to stay a while.\nPERSHORE HORTICULTURAL COLLEGE\nPershore Horticultural College is found by continuing on the B4084 towards Evesham. Founded\nin 1974 its purpose was to train Horticulture workers, it rapidly gained international\nrecognition and has attracted many overseas students.\nSince 1990 it has been connected to the Royal Horticultural Society (R.H.S) and has featured\nin its annual shows. The Campus is situated on a site of 60 hectares and provides unrivalled\nresources for students.\nWith its Gardens and conservation areas occupying 15 hectares and hard-landscape workshop\nand studios, crop production facilities include a further 12 hectares of fruit and computer\ncontrolled glasshouses plus a nursery and a specialist plant unit, it is not surprising\nthat its high reputation is spreading internationally.\nOn the 1st of August 2007 the college merged with the Warwickshire College.\nFor the visitor there are regular open days and events, for information about this Telephone\nREAL FLOWER PETAL CONFETTI COMPANY\nThe Real Flower Petal Confetti Company can be found by continuing along the B4084 towards\nEvesham in the small village of Wick, it is here that this company is based. Natural confetti\nis produced from the petals of delphinium flowers. The flowers are grown and harvested\nby hand on the farm at Wick, no artificial drying or colouring is used in the preparation\nand allows them to biodegrade naturally as blossom.\nThe flowers are usually in bloom at the end of June or early July depending on the amount\nof sunshine in the preceding months and should last until the early part of August. Visitors\nare of course welcome, but it must be stressed that the company cannot offer the facilities\nthat one might expect at a tourist site as it is still a working arable farm that happens\nto grow beautiful fields of flowers. It is possible to view the fields and to purchase\npacks of confetti.\nThe idea of producing confetti was conceived by the owner Charles Hudson, he was on his\nway home on a late spring afternoon when he paused outside the local church yard. Strewn\naround the lych-gate was a sodden mass of coloured paper these being the remains of paper\nconfetti, this had turned the church entrance into an eyesore.\nUpon his arrival home Charles , who was carrying a bouquet of flowers for his wife,\nwas inspired when a petal fell from the bouquet , for it was then that the idea for petal\nconfetti was born.\nHe experimented with many flowers following this incident and eventually found that delphiniums\nwere the most natural in their ability to retain shape and colour by using the sunshine\nand fresh air in a system that he had evolved.\nNew varieties have been produced using wild flower colour essences for subtle colour mixes\nto match any brides wedding dress.\nCharles Hudson’s Wyke Manor estate is now the British centre for real flower confetti\nand every year he plants an original design in a 10 acre field, his 2004 Delphinium Union\nJack was filmed from the air by national press and TV.\nIt was visited by the Guinness Book of records and the flower flag is in\nthe record book for posterity.\nThe confetti was the choice of Madonna , Catherine Zeta Jones and Gabby Logan at']	['<urn:uuid:206051ab-2128-447d-9e00-16bca3914bdb>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	15	22	1606
46	How do graph databases help with social networks and which search method suits them?	Graph databases excel at handling social network connections by efficiently establishing relationships between users and their friends' connections. For social networks, Breadth-First Search (BFS) is the preferred algorithm as it's specifically useful for finding people within a specific distance and locating neighbor nodes, making it ideal for discovering connections in social network services.	"['A Comparison Of NoSQL Database Management Systems And Models\nNoSQL databases try to offer certain functionality that more traditional relational database management systems do not. Whether it is for holding simple key-value pairs for shorter lengths of time for caching purposes, or keeping unstructured collections (e.g. collections) of data that could not be easily dealt with using relational databases and the structured query language (SQL) – they are here to help.\nIn this DigitalOcean article, we are going to try to introduce you to various popular NoSQL database management systems and explain their purposes and functionality, so that you can decide which one to use, or if they even suit your application\'s needs -- at all.\n1. Database Management Systems\n2. NoSQL Database Management Systems\n- Key / Value Based\n- Column Based\n- Document Based\n- Graph Based\n3. Key / Value Based NoSQL Database Management Systems\n- Popular Key / Value Based Databases\n- When To Use\n4. Column Based NoSQL Database Management Systems\n- Popular Column Based Databases\n- When To Use\n5. Document Based NoSQL Database Management Systems\n- Popular Document Based Databases\n- When To Use\n6. Graph Based NoSQL Database Management Systems\n- Popular Graph Based Databases\n- When To Use\n7. NoSQL DBMSs In Comparison To Relational DBMSs\n- When To Use NoSQL Databases\nDatabase Management Systems\nDatabases are logically modeled storage spaces for all kinds of different information (data). Each database, other than schema-less ones, have a model which provides structure for the data being dealt with. Database management systems are applications (or libraries) which manage databases of various shapes, sizes, and sorts.\nNote: To learn more about Database Management Systems, check out our article: Understanding Databases.\nNoSQL Database Management Systems\nIn the past decade or so, relational database management systems have been the choice of many developers and system administrators for a variety of applications, for a variety of reasons. Despite not being exactly flexible, the powerful nature of many RDBMS allowed complex database set-ups to be created, queried and used. This was more than enough for many requirements, since it was not until long ago that different needs started to rise.\nThe term ""NoSQL"" was coined over a decade ago, funnily enough as a name to yet-another relational database. However, this database had a different idea behind it: eliminating the use of the standardised SQL. In the next years to come, others picked up and continued to grow this thought, by referring to various other non-relational databases as NoSQL databases.\nBy design, NoSQL databases and management systems are relation-less (or schema-less). They are not based on a single model (e.g. relational model of RDBMSs) and each database, depending on their target-functionality, adopt a different one.\nThere are almost a handful of different operational models and functioning systems for NoSQL databases.:\n- Key / Value:\ne.g. Redis, MemcacheDB, etc.\ne.g. Cassandra, HBase, etc.\ne.g. MongoDB, Couchbase, etc\ne.g. OrientDB, Neo4J, etc.\nIn order to better understand the roles and underlying technology of each database management system, let\'s quickly go over these four operational models.\nKey / Value Based\nWe will begin our NoSQL modeling journey with key / value based database management simply because they can be considered the most basic and backbone implementation of NoSQL.\nThese type of databases work by matching keys with values, similar to a dictionary. There is no structure nor relation. After connecting to the database server (e.g. Redis), an application can state a key (e.g.\nthe_answer_to_life) and provide a matching value (e.g.\n42) which can later be retrieved the same way by supplying the key.\nKey / value DBMSs are usually used for quickly storing basic information, and sometimes not-so-basic ones after performing, for example, a CPU and memory intensive computation. They are extremely performant, efficient and usually easily scalable.\nNote: When it comes to computers, a dictionary usually refers to a special sort of data object. They constitutes of arrays of collections with individual keys matching values.\nColumn based NoSQL database management systems work by advancing the simple nature of key / value based ones.\nDespite their complicated-to-understand image on the internet, these databases work very simply by creating collections of one or more key / value pairs that match a record.\nUnlike the traditional defines schemas of relational databases, column-based NoSQL solutions do not require a pre-structured table to work with the data. Each record comes with one or more columns containing the information and each column of each record can be different.\nBasically, column-based NoSQL databases are two dimensional arrays whereby each key (i.e. row / record) has one or more key / value pairs attached to it and these management systems allow very large and un-structured data to be kept and used (e.g. a record with tons of information).\nThese databases are commonly used when simple key / value pairs are not enough, and storing very large numbers of records with very large numbers of information is a must. DBMS implementing column-based, schema-less models can scale extremely well.\nDocument based NoSQL database management systems can be considered the latest craze that managed to take a lot of people by storm. These DBMS work in a similar fashion to column-based ones; however, they allow much deeper nesting and complex structures to be achieved (e.g. a document, within a document, within a document).\nDocuments overcome the constraints of one or two level of key / value nesting of columnar databases. Basically, any complex and arbitrary structure can form a document, which can be stored using these management systems.\nDespite their powerful nature, and the ability to query records by individual keys, document based management systems have their own issues and downfalls compared to others. For example, retrieving a value of a record means getting the whole lot of it and same goes for updates, all of which affect the performance.\nFinally, the very interesting flavour of NoSQL database management systems is the graph based ones.\nThe graph based DBMS models represent the data in a completely different way than the previous three models. They use tree-like structures (i.e. graphs) with nodes and edges connecting each other through relations.\nSimilarly to mathematics, certain operations are much simpler to perform using these type of models thanks to their nature of linking and grouping related pieces of information (e.g. connected people).\nThese databases are commonly used by applications whereby clear boundaries for connections are necessary to establish. For example, when you register to a social network of any sort, your friends\' connection to you and their friends\' friends\' relation to you are much easier to work with using graph-based database management systems.\nKey / Value Based NoSQL Database Management Systems\nKey / Value data stores are highly performant, easy to work with and they usually scale well.\nPopular Key / Value Based Databases\nSome popular key / value based data stores are:\nIn-memory K/V store with optional persistence.\nHighly distributed, replicated K/V store.\n- Memcached / MemcacheDB:\nDistributed memory based K/V store.\nWhen To Use\nSome popular use cases for key / value based data stores are:\nQuickly storing data for - sometimes frequent - future use.\nSome K/V stores (e.g. Redis) supports lists, sets, queues and more.\n- Distributing information / tasks:\nThey can be used to implement Pub/Sub.\n- Keeping live information:\nApplications which need to keep a state cane use K/V stores easily.\nColumn Based NoSQL Database Management Systems\nColumn based data stores are extremely powerful and they can be reliably used to keep important data of very large sizes. Despite not being ""flexible"" in terms of what constitutes as data, they are highly functional and performant.\nPopular Column Based Databases\nSome popular column based data stores are:\nColumn based data store based on BigTable and DynamoDB.\nData store for Apache Hadoop based on ideas from BigTable.\nWhen To Use\nSome popular use cases for column based data stores are:\n- Keeping unstructured, non-volatile information:\nIf a large collection of attributes and values needs to be kept for long periods of time, column-based data stores come in extremely handy.\nColumn based data stores are highly scalable by nature. They can handle an awful amount of information.\nDocument Based NoSQL Database Management Systems\nDocument based data stores are excellent for keeping a lot of unrelated complex information that is highly variable in terms of structure from one another.\nPopular Document Based Databases\nSome popular document based data stores are:\nJSON-based, Memcached-compatible document-based data store.\nA ground-breaking document-based data store.\nAn extremely popular and highly-functional database.\nWhen To Use\nSome popular use cases for document based data stores are:\n- Nested information:\nDocument-based data stores allow you to work with deeply nested, complex data structures.\nOne of the most critical functionalities of document-based data stores are the way they interface with applications: Using JS friendly JSON.\nGraph Based NoSQL Database Management Systems\nGraph based data stores offer a very unique functionality that is unmatched with any other DBMSs.\nPopular Graph Based Databases\nSome popular graph based data stores are:\nA very fast graph and document based hybrid NoSQL data store written in Java that comes with different operational modes.\nA schema-free, extremely popular and powerful Java graph based data store.\nWhen To Use\nSome popular use cases for graph based data stores are:\n- Handling complex relational information:\nAs explained in the introduction, graph databases make it extremely efficient and easy to use to deal with complex but relational information, such as the connections between two entities and various degrees of other entities indirectly related to them.\n- Modelling and handling classifications:\nGraph databases excel in any situation where relationships are involved. Modelling data and classifying various information in a relational way can be handled very well using these type of data stores.\nNoSQL DBMSs In Comparison To Relational DBMSs\nIn order to draw a clear picture of how NoSQL solutions differ from relational database management systems, let\'s create a quick comparison list:\nWhen To Use NoSQL Databases\n- Size matters:\nIf will be working with very large sets of data, consistently scaling is easier to achieve with many of the DBMS from NoSQL family.\nNoSQL databases are usually faster - and sometimes extremely speedier - when it comes to writes. Reads can also be very fast depending on the type of NoSQL database and data being queried.\n- Schema-free design:\nRelational DBMSs require structure from the beginning. NoSQL solutions offer a large amount of flexibility.\n- Automated (or easy) replications / scaling:\nNoSQL databases are growing rapidly and they are being actively built today - vendors are trying to tackle common issues and one of them clearly is replication and scaling. Unlike RDBMSs, NoSQL solutions can easily scale and work with(in) clusters.\n- Multiple choices:\nWhen it comes to choosing a NoSQL data store, there are a variety of models, as we have discussed, that you can choose from to get the most out of the database management system - depending on your data type.', 'Neo4j includes a growing, open library of high-performance graph algorithms that reveal the hidden patterns and structures in your connected data.\nIn this series on graph algorithms, we’ll discuss the value of graph algorithms and what they can do for you. Last week, we explored how data connections drive future discoveries and how to streamline those data discoveries with graph analytics.\nThis week, we’ll take a detailed look at the many graph algorithms available in Neo4j and what they do.\n15 Graph Algorithms Optimized for the Neo4j Graph Platform\nUsing Neo4j graph algorithms, you’ll have the means to understand, model and predict complicated dynamics such as the flow of resources or information, the pathways that contagions or network failures spread, and the influences on and resiliency of groups.\nAnd because Neo4j brings together analytics and transaction operations in a native graph platform, you’ll not only uncover the inner nature of real-world systems for new discoveries, but also develop and deploy graph-based solutions faster and have easy-to-use, streamlined workflows. That’s the power of an optimized approach.\nHere is a list of the many algorithms that Neo4j uses in its graph analytics platform, along with an explanation of what they do.\nTraversal & Pathfinding Algorithms\n1. Parallel Breadth-First Search (BFS)What It Does: Traverses a tree data structure by fanning out to explore the nearest neighbors and then their sub-level neighbors. It’s used to locate connections and is a precursor to many other graph algorithms.\nBFS is preferred when the tree is less balanced or the target is closer to the starting point. It can also be used to find the shortest path between nodes or avoid the recursive processes of depth-first search.\nHow It’s Used: Breadth-First Search can be used to locate neighbor nodes in peer-to-peer networks like BitTorrent, GPS systems to pinpoint nearby locations and social network services to find people within a specific distance.\n2. Parallel Depth-First Search (DFS)What It Does: Traverses a tree data structure by exploring as far as possible down each branch before backtracking. It’s used on deeply hierarchical data and is a precursor to many other graph algorithms. Depth-First Search is preferred when the tree is more balanced or the target is closer to an endpoint.\nHow It’s Used: Depth-First Search is often used in gaming simulations where each choice or action leads to another, expanding into a tree-shaped graph of possibilities. It will traverse the choice tree until it discovers an optimal solution path (e.g., win).\n3. Single-Source Shortest PathWhat It Does: Calculates a path between a node and all other nodes whose summed value (weight of relationships such as cost, distance, time or capacity) to all other nodes are minimal.\nHow It’s Used: Single-Source Shortest Path is often applied to automatically obtain directions between physical locations, such as driving directions via Google Maps. It’s also essential in logical routing such as telephone call routing (least-cost routing).\n4. All-Pairs Shortest PathWhat It Does: Calculates a shortest path forest (group) containing all shortest paths between the nodes in the graph. Commonly used for understanding alternate routing when the shortest route is blocked or becomes sub-optimal.\nHow It’s Used: All-Pairs Shortest Path is used to evaluate alternate routes for situations such as a freeway backup or network capacity. It’s also key in logical routing to offer multiple paths; for example, call routing alternatives.\n5. Minimum Weight Spanning Tree (MWST)What It Does: Calculates the paths along a connected tree structure with the smallest value (weight of the relationship such as cost, time or capacity) associated with visiting all nodes in the tree. It’s also employed to approximate some NP-hard problems such as the traveling salesman problem and randomized or iterative rounding.\nHow It’s Used: Minimum Weight Spanning Tree is widely used for network designs: least-cost logical or physical routing such as laying cable, fastest garbage collection routes, capacity for water systems, efficient circuit designs and much more. It also has real-time applications with rolling optimizations such as processes in a chemical refinery or driving route corrections.\n6. PageRankWhat It Does: Estimates a current node’s importance from its linked neighbors and then again from their neighbors. A node’s rank is derived from the number and quality of its transitive links to estimate influence. Although popularized by Google, it’s widely recognized as a way of detecting influential nodes in any network.\nHow It’s Used: PageRank is used in quite a few ways to estimate importance and influence. It’s used to suggest Twitter accounts to follow and for general sentiment analysis.\nPageRank is also used in machine learning to identify the most influential features for extraction. In biology, it’s been used to identify which species extinctions within a food web would lead to biggest chain-reaction of species death.\n7. Degree CentralityWhat It Does: Measures the number of relationships a node (or an entire graph) has. It’s broken into indegree (flowing in) and outdegree (flowing out) where relationships are directed.\nHow It’s Used: Degree Centrality looks at immediate connectedness for uses such as evaluating the near-term risk of a person catching a virus or hearing information. In social studies, indegree of friendship can be used to estimate popularity and outdegree as gregariousness.\n8. Closeness CentralityWhat It Does: Measures how central a node is to all its neighbors within its cluster. Nodes with the shortest paths to all other nodes are assumed to be able to reach the entire group the fastest.\nHow It’s Used: Closeness centrality is applicable in a number of resources, communication and behavioral analysis, especially when interaction speed is significant. It has been used to identifying the best location of new public services for maximum accessibility.\nIn social network analysis, it is used to find people with the ideal social network location for faster dissemination of information.\n9. Betweenness CentralityWhat It Does: Measures the number of shortest paths (first found with Breadth-First Search) that pass through a node. Nodes that most frequently lie on shortest paths have higher betweenness centrality scores and are the bridges between different clusters. It is often associated with the control over the flow of resources and information.\nHow It’s Used: Betweenness Centrality applies to a wide range of problems in network science and is used to pinpoint bottlenecks or likely attack targets in communication and transportation networks. In genomics, it has been used to understand the control certain genes have in protein networks for improvements such as better drug- disease targeting.\nBetweenness Centrality has also be used to evaluate information flows between multiplayer online gamers and expertise sharing communities of physicians.\nCommunity Detection Algorithms\nThis category is also known as clustering algorithms or partitioning algorithms.\n10. Label PropagationWhat It Does: Spreads labels based on neighborhood majorities as a means of inferring clusters. This extremely fast graph partitioning requires little prior information and is widely used in large-scale networks for community detection. It’s a key method for understanding the organization of a graph and is often a primary step in other analysis.\nHow It’s Used: Label Propagation has diverse applications from understanding consensus formation in social communities to identifying sets of proteins that are involved together in a process (functional modules) for biochemical networks. It’s also used in semi- and unsupervised machine learning as an initial preprocessing step.\n11. Strongly ConnectedWhat It Does: Locates groups of nodes where each node is reachable from every other node in the same group following the direction of relationships. It’s often applied from a depth-first search.\nHow It’s Used: Strongly Connected is often used to enable running other algorithms independently on an identified cluster. As a preprocessing step for directed graphs, it helps quickly identify disconnected groups. In retail recommendations, it helps identify groups with strong affinities that then are used for suggesting commonly preferred items to those within that group who have not yet purchased the item.\n12. Union-Find / Connected Components / Weakly ConnectedWhat It Does: Finds groups of nodes where each node is reachable from any other node in the same group, regardless of the direction of relationships. It provides near constant-time operations (independent of input size) to add new groups, merge existing groups and determine whether two nodes are in the same group\nHow It’s Used: Union-Find / Connected Components is often used in conjunction with other algorithms, especially for high-performance grouping. As a preprocessing step for undirected graphs, it helps quickly identify disconnected groups.\n13. Louvain ModularityWhat It Does: Measures the quality (i.e., presumed accuracy) of a community grouping by comparing its relationship density to a suitably defined random network. It’s often used to evaluate the organization of complex networks and community hierarchies in particular. It’s also useful for initial data preprocessing in unsupervised machine learning.\nHow It’s Used: Louvain is used to evaluate social structures in Twitter, LinkedIn and YouTube. It’s used in fraud analytics to evaluate whether a group has just a few bad behaviors or is acting as a fraud ring that would be indicated by a higher relationship density than average. Louvain revealed a six-level customer hierarchy in a Belgian telecom network.\n14. Local Clustering Coefficient / Node Clustering CoefficientWhat It Does: For a particular node, it quantifies how close its neighbors are to being a clique (every node is directly connected to every other node). For example, if all your friends knew each other directly, your local clustering coefficient would be 1. Small values for a cluster would indicate that although a grouping exists, the nodes are not tightly connected.\nHow It’s Used: Local Cluster Coefficient is important for estimating resilience by understanding the likelihood of group coherence or fragmentation. Analysis of a European power grid using this method found that clusters with sparsely connected nodes were more resilient against widespread failures.\n15. Triangle-Count and Average Clustering CoefficientWhat It Does: Measures how many nodes have triangles and the degree to which nodes tend to cluster together. The average clustering coefficient is 1 when there is a clique, and 0 when there are no connections. For the clustering coefficient to be meaningful, it should be significantly higher than a version of the network where all of the relationships have been shuffled randomly.\nHow It’s Used: The Average Clustering Coefficient is often used to estimate whether a network might exhibit “small-world” behaviors which are based on tightly knit clusters. It’s also a factor for cluster stability and resiliency. Epidemiologists have used the average clustering coefficient to help predict various infection rates for different communities.\nThe world is driven by connections. Neo4j graph analytics reveals the meaning of those connections using practical, optimized graph algorithms including the ones detailed above.\nThis concludes our series on graph algorithms in Neo4j. We hope these algorithms help you make sense of your connected data in more meaningful and effective ways.\nLearn about the power of graph algorithms in the O’Reilly book,\nGraph Algorithms: Practical Examples in Apache Spark and Neo4j by the authors of this article. Click below to get your free ebook copy.\nGet the O’Reilly Ebook\nCatch up with the rest of the graph algorithms in Neo4j blog series:\nAbout the Author\nAmy E. Hodler , Analytics & AI Program Manager\nIn her career, Amy has consistently helped teams break into new markets at startups and large companies including EDS, Microsoft and Hewlett-Packard (HP). She most recently comes from Cray Inc., where she was the analytics and artificial intelligence market manager.\nAmy has a love for science and art with an extreme fascination for complexity science and graph theory. When the weather is good, you’re likely to find her cycling the passes in beautiful Eastern Washington.']"	['<urn:uuid:333368df-425c-4d54-a2dc-cb720aa6ac03>', '<urn:uuid:c810dab5-06bf-4782-b51f-1523fbea7fe4>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	14	53	3754
47	traveling abroad need medical care what services foreign patients hospital suwon	The hospital offers several services for foreign patients including: free airport pickup, private coordinator service, hotel reservations, tourist attraction discounts, Russian and European cuisine for inpatients, free WiFi, private wards for foreign patients, private doctor service, free telemedicine, happy call service, and medical documentation in foreign languages.	['[General Hospital] The Catholic University of Korea St.Vincent’s Hospital\n|Fields||General Hospital,Dermatology/ Plastic Surgery,Dentistry,Ophthalmology,Woman’s Clinic,Otolaryngology|\n|Address||93, Jungbudae-ro, Paldal-gu, Suwon-si, Gyeonggi-do, Kroea|\nSpecialized Medical Dept.\nProstate cancer, Kidney cancer, Bladder cancer, Ureter cancer Kidney cancer Bladder cancer Ureter cancer\nFamily Medicine, Infectious Disease Internal Medicine, Endocrinology, Rheumatology, Gastroenterology, Cardiology, Nephrology, Oncology, Hematology, Pulmonology, Anesthesia & Pain Medicine, Radiation Oncology, Pathology, Urology, O & G, Plastic Surgery, Pediatrics & Adolescent Medicine, Neurology, Neurosurgery, Ophthalmology, Radiology, Liver-Pancreas-Gallbladder Surgery, Colon & Rectal Surgery, Gastrointestinal Tract Surgery, Breast/Thyroid Surgery, Emergency Medicine, Otolaryngology, Rehabilitation Medicine, Neuropsychiatry, Orthopedics, Laboratory Medicine, Dentistry, Dermatology, Nuclear Medicine, Thoracic & Cardiovascular Surgery\nEstablished in Jun 1967 under the banner of “Patient-Oriented Medical Service that Gives Trust and Hope”, Catholic University of Korea St. Vincent Hospital is a medical institution affiliated with Catholic University to materialize the spirit of Jesus Christ. As one of those eight affiliated hospitals (Seoul, Yeouido, Uijeongbu, Bucheon, Incheon, St. Paul, Dajeon) that the Catholic University of Korea Catholic Medical Center leads, we contribute to the largest medical network in Korea to act as the medical backbone. St. Vincent Hospital comprises 31 departments and 800 sickbeds employing 1,500 and counting faculty members. The medical staff members have accumulated experiences in overseas leading medical institutes and operate the state-of-the-art medical devices including da Vinci SI, Tomotherapy, PET-CT, 3.0 T MRI, 64-channel CT and more. To keep up with the fast-developing medical technologies, to specify the treatment by body part and disease and to secure medical department-oriented and integrated treatment, it is extending its intensive treatment in the specialized treatment centers and detailed clinics including the Cardiovascular Center, MIS Center, Tomotherapy Center, Diabetes Center, Gastrointestinal Endoscopy Center, Obesity Surgery Center, Sleep Disorder Epidemiology Center, Lung Cancer Clinic, Breast/Thyroid Clinic. In addition, it has introduced, for the first time within Suwon area, the state-of-the-art da Vinci SI Robotic Surgery System equipped with cutting-edge newest technologies, which is a robotic laparoscopic surgery system that may be integrated to urology, O & G, surgery, otolaryngology, and the cancer treatment implemented in thoracic & cardiovascular surgery to improve the patient’s post-op value of life by facilitating rapid recovery and by minimizing scar, etc.\n|Scale||All (Num)||1,750||BED (Num)||796|\n|DOCTOR (Num)||329||Nurse (Num)||639|\n|The business year|\n|Aggregate Amount Insured|\n|Medical quality certification||\nMedical institution, certified by the Ministry of Health and Welfare\nSpecial hospital Gyeonggi-do medical tour excellent cooperation organization\nSubsidiary services for foreign patients\n- Free Airport Pickup Service\n- One to one Private Coordinator Service\n- Hotel Reservation Service\n- Discount and Connection Service for Main tourist attraction\n- Russian and European Cuisine Service for inpatient\n- Free Wifi Service for inpatient\n- Operating private wards only for Foreign patients\n- Doctor’s room and Private doctor Service only for Foreign patients\n- Free Telemedicine and Happy Call service\n- Providing medical documentation in a foreign language\nMedical Care Costs\nRobotic surgery (Prostate cancer, Kidney cancer, Bladder cancer, Ureter cancer)\n- Robotic surgery/laparoscopic surgery that leads to MIS!\nThe da Vinci Si Robotic Surgery System is one of the robotic laparoscopic surgery systems. It inserts through a small incision cut at the lesion site the robot arm and an imaging camera for 3D magnification, which the physician remotely controls at consol. The robotic surgery/laparoscopic surgery has advantages in that, compared to existing laparotomy, it minimizes bleeding, reduces post-op pain, lowers the complication rate, minimizes post-op scars, shortens hospitalization period and promotes fast recovery. This technique is particularly excellent in terms of the restoration of various functions, which will greatly improve the patient’s quality of life.\n93, Jungbudae-ro, Paldal-gu, Suwon-si, Gyeonggi-do, Kroea']	['<urn:uuid:0b5f6eef-35c3-40af-8736-ba8625006126>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	11	47	593
48	How does linen perform as a travel-friendly material, and what environmental benefits does it offer for sustainable clothing?	Linen is excellent for travel as it's a natural fiber that's lightweight and easy to fold. Modern synthetic blends have improved linen's performance, creating slacks and skirts that resist wrinkling when sitting. From an environmental perspective, linen is highly sustainable as it's grown without chemicals or pesticides and is naturally moth-resistant. It requires minimal water to produce, making it an environmentally responsible choice. The fabric is also breathable and cool to wear, adding to its practical benefits for both travel and sustainability.	"['Clothes That Travel Well\nIn a perfect world, we\'d all have closets that work like a ""Star Trek"" transporter room and send our clothes wrinkle-free from home straight to our travel destination. Until then, we\'re stuck with folding, rolling and layering our items in creative ways to minimize creases. Fortunately, there are a number of easy-care fabrics that are up to the task of globe-trekking.\nNatural fibers such as silk, cotton, rayon and linen are popular favorites with travelers because they\'re lightweight and easy to fold. Examples include silk blouses, t-shirts, wool sweaters, spandex dresses and denim. The welcome addition of synthetic blends has resulted in silks that are machine washable and linen slacks and skirts that won\'t generate ""lap wrinkles"" the first time you sit down.\nIf you\'re going to a hot climate and will be spending lots of time outdoors, you need to remember that you can actually get a sunburn through certain items of clothing. New to the market and sold through apparel venues such as TravelSmith and Magellan\'s are shirts, blouses, shorts and cover-ups that have Ultraviolet Protection Factor (UPF) woven into the finish and linings to protect your skin in much the same way as sunscreen. And speaking of sun, a cap or a folding floppy hat won\'t take up much room in your checked bag or carry-on.\nMicrofibers and Synthetics\nGarments made from microfibers and synthetics have structure and durability which is combined with a soft drape that moves with the body. This is a result of a super-fine weave that gives fabrics such as acrylic nylons, matte jersey, and Tencel a breathable texture that often incorporates stain repellents as well. Synthetics are also in widespread use as leather and suede substitutes. Not only do both of these pack well but they\'re also a lot less expensive than the real deal and easier to replace if they get stolen during your trip.\nIt\'s important to put as much consideration into your footwear choices as you do for your travel wardrobe. If you simply have to take a pair of boots, keep in mind that they\'re a hassle to remove for airport security if you wear them and are heavy space-hogs if you put them in a suitcase. A better choice is a pair of loafers or slip-ons that can also come off easily for airplane naps. To accommodate foot-swelling issues, go for elastic gores and adjustable straps. In addition, pack shoes with good traction if you\'ll be walking a lot.\nColors and Patterns\nYou\'ll have more options available if you build your travel wardrobe around two or three neutral colors that can be combined with complementary and contrasting shades. Strive as well for separates that can be mixed and matched. Scarves, vests and shawls are ideal accessories because they pack flat; scarves and shawls can also be rolled. Incorporate a few patterned shirts and skirts. As travel experts such as Susan Foster and Judith Gilford recommend, patterns are better at concealing spills and stains than solids and, thus, extend their usage on a trip.\n- ""The Packing Book: Secrets of the Carry-on Traveler""; Judith Gilford; 2006\n- ""Smart Packing for Today\'s Traveler""; Susan Foster; 2008\n- ""Clothing: Fashion, Fabrics & Construction""; Jeanette Weber; 2006\n- ""Claire Shaeffer\'s Fabric Sewing Guide""; Claire Shaeffer; 2008\nLeaf Group is a USA TODAY content partner providing general travel information. Its content is produced independently of USA TODAY.\n- iron on orange cloth image by Nikolai Sorokin from Fotolia.com', 'Choosing environmentally friendly fabrics is an essential step towards reducing your impact on the environment. The main factors to consider are the use of biodegradable materials, the production process, and the disposal of waste. To find out more about environmentally friendly fabrics, keep reading.\nAn excellent method to lessen the influence of your apparel is to use environmentally friendly fabrics. These fabrics are made from natural, recyclable materials and are often biodegradable. These materials also use fewer resources in production.\nLinen is grown without chemicals or pesticides and is naturally moth-resistant. It’s also soft and absorbent. Some fabrics, like linen, require minimal water to produce.\nBamboo is an eco-friendly fabric. It doesn’t require much water to grow and feels silky. Bamboo proliferates as a plant. The bamboo plant may be picked without harming it.\nAnother type of fabric that is biodegradable is modal. It is made from spinning cellulose from beech trees. It is more potent than traditional rayon and uses 10-20 times less water.\nHemp is another eco-friendly fabric. It doesn’t require much water to grow and is considered carbon-negative. It also helps to fertilize poor soil.\nUnlike cotton, hemp is a plant that requires little water and does not require pesticides. It also provides a great source of oxygen, making it an eco-friendly fabric.\nHemp is also a naturally antibacterial, hypoallergenic, and thermoregulating material. It is also durable and breathable and has high UV resistance. Hemp also provides excellent warmth and moisture retention, making it an ideal fabric for humid climates.\nHemp is a fast-growing plant that requires less water and land to produce. It also has a large tap root that penetrates deep into the soil, preventing soil erosion and aerating the ground for future crops.\nHemp is also a great source of carbon sequestration and traps about 162 kg of carbon per tonne of hemp harvested. It is a much higher yield than cotton.\nIndustrial hemp has been used for various purposes, including food, medicine, and textiles. It is also the oldest cultivated plant in the world. Its fibers have been used to weave flax since the Neolithic period.\nThe plant is fast-growing, uses little water, and is pest resistant. It can be harmful if not grown in a controlled environment. Buying fabrics made from bamboo is a great way to help the environment.\nBamboo lyocell or bamboo viscose can be used to create bamboo cloth. The latter is made using an automated procedure. Chemical processes are used to develop the latter.\nBamboo lyocell is made in a closed-loop process, which means that the chemicals used are recycled and not disposed of into the environment. Compared to the rayon production process, this method is also environmentally friendly.\nBamboo viscose is a newer version of rayon. In this process, bamboo is mechanically crushed and processed into fibers. These fibers are then exposed to sulfuric acid, which softens the strands.\nThe bamboo fiber is then processed into yarn. The yarn is spun into fabric, then woven into valuable products.\nA great way to lessen your carbon footprint is to fill your wardrobe with eco-friendly fabrics. All-natural, non-toxic ingredients and dyes are used to create these textiles. Additionally, they generate less water and energy. Your apparel will last longer as a result.\nSome of the most popular fabrics in the fashion industry include cotton, nylon, and rayon. The latter two can contain poisonous compounds that are bad for your health.\nMany eco-conscious brands now use recycled fabrics, natural fabrics, and other eco-friendly materials. They are also more durable than conventional cotton and often require less water and energy to produce.\nAnother great way to support the environment is to use fabrics that are 100% biodegradable. They are also hypoallergenic and durable.\nOther fabrics that are considered eco-friendly include hemp, linen, and silk. Silk is a strong, durable, absorbent, and excellent material. It is also very soft against the skin. It can last up to 10 years if it is correctly cared for.\nCradle to Cradle(r) certification\nMBDC certification allows companies to assess their products’ environmental and social responsibility. It determines various factors, such as how a product uses energy, water, and materials. It is also essential to identify strategies for social responsibility.\nProducts are evaluated for material health, circularity, and potential renewable energy impacts. MBDC certification has several different levels. The lowest level, or Silver, awards a product with third-party verification that it meets the Cradle to Cradle Certified Product Standard. The highest level, Bronze, grants a product with a second-party assurance that it meets the Cradle To Cradle Certified Product Standard. Products are then subject to regular standards review and assessment.\nCompanies that produce cradle-to-cradle products can increase their profits. Cradle-to-Cradle Certified Products are advantageous for both the environment and business. Outcomes are assessed for their material health, renewable energy impacts, social fairness, and ethical treatment of workers.\nAn eco-friendly wardrobe is essential to anyone concerned with the environment. Fortunately, there are many options for sustainable fabrics. These fabrics can be made from organic or recycled materials and are often produced by sustainable brands. They also require less energy and water to grow.\nOne of the most commonly known eco-friendly fabrics is cotton. Cotton is grown using less water and requires less energy than other textiles. Cotton is also known for its durability. However, when cotton is overfarmed, the pollution from the farming process affects the environment.\nAnother type of eco-friendly fabric is organic cotton. Organic cotton is produced without pesticides and requires less water than conventional cotton. Similarly, hemp is a harmful carbon fiber, which requires less water to grow.\nAnother popular eco-friendly fabric is linen. Linen is made from flax, a plant that does not require much water to grow. It is also breathable and cool to wear.']"	['<urn:uuid:7df06f06-eae9-4707-80c9-3d3e401a3824>', '<urn:uuid:282b7ab9-a556-4cbc-a29f-86f2f41d32f2>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	18	82	1542
49	What modern decoration techniques replace acid etching?	Acid etching, which used to be big in the industry, is being replaced by techniques such as abrasive etching and shrink labels to create effects. Additionally, transparent pressure-sensitive labels are popular, especially for spirits, as they allow the product's clarity to show through.	['Package designers and glass manufacturers are advancing both the form and function of glass packages. From strong, lightweight bottles to ergonomic features and more eco-friendly decorating techniques and blown-in accoutrements, today’s glass packages are very different from those from a few years back.\nAlso, the past decade’s glass packaging shortages seem to be mere reflections in the industry’s rearview mirror. Glass packaging is a more available and versatile option in much of the country.\nTo help package designers who are rethinking glass as a packaging option, Package Design sat down with two executives from Owens-Illinois Inc. (O-I), a major supplier of glass packaging: Raul Paredes, manager of new product development, and Miguel Yanez, marketing director for North America.\nPD: What trends are you seeing in glass package design?\nRaul Paredes: We’re actually seeing a divergence of trends. The first one is more efficient structural design, where a lot of packaging operations are moving away from containers that require special handling. They want bottles and containers that will run very quickly down fill lines and different handling lines.\nThe second trend is brand owners who want the container to become part of the product offering. Instead of the package just being a vessel, it’s integrated into the brand’s image. Proprietary bottles can become the signature element of an iconic brand.\nThere, efficiency may take a little bit of a backseat so that functionality comes to the forefront. These packages may focus on features such as an ergonomic grip. They may be designed to be easy to open, easy to close, and easy to identify both on the shelf and in use. An example is the growing use of specific necks on bottles so bartenders can recognize products in a very fast-paced, busy environment. Here, it’s important for the bottles to be really distinguishable; bartenders are trying to conduct their business with clientele and can’t always look down.\nMiguel Yanez: We’ve also seen interest in “retro propositions,” which includes the use of amber glass for protection against UV light. Returnable systems for glass are becoming cool again, too, for their sustainability benefits.\nWhich shapes and structures are trending up?\nParedes: It’s very category specific. For beer packaging, we’re seeing stubby bottles begin to appear where the long-neck had been the predominant design. I think these shorter, more robust containers feel a little bit nicer in the hand. The larger diameter also seems more drinkable. It’s certainly more nostalgic in its look, as well, calling to mind 1950s design. Structures for spirits, again, are all about grippability and recognition.\nSquare jars are giving packaged foods a more premium look. There’s more facing for on-the-shelf presentation and a bigger billboard for labeling. There’s also an opportunity to have more of a footed design, and that often adds to an upscale look.\nWe’re also starting to see more effort put in developing jars that are very easy to grip. I expect to see this spread across the food categories and grow, along with an interest in the torque removal needed for closures.\nHow is glass package decoration changing?\nParedes: In the U.S., we’re noticing a tendency to move away from more caustic decorative techniques. The use of heavy metals in decorating is almost unheard of today. Some of this change probably came from government regulation, but it also changed because of some self-regulation.\nAnother design technique that’s fading is acid etching. It used to be big in the industry. Other techniques, such as abrasive etching or using shrink labels to create effects, are being used.\nFor spirits, brand owners like the no-label look of transparent pressure-sensitive labels. They like how the label carries the branding information, yet still lets the clarity of their vodkas or the ambers of their whiskeys and rums to show through. Using transparent packaging to highlight the product is a consistent trend that’s as important as ever and we don’t see it going away.\nClosures are another tool package designers are using to decorate their bottles. Designers who understand closure needs for their products and the different closure styles can certainly improve their products’ shelf impact and consumer interface.\nIn the last five years, we’ve seen several CPGs move away from glass to PET in an attempt to lower their packaging’s carbon footprint. Your thoughts on that?\nYanez: We definitely hear a lot of noise from alternative packaging manufacturers about what they think is information the consumer should know. What we’ve found is that when you do the exercise of apple to apples, glass currently plays an advanced and favorable position against alternative packaging.\nGlass recycling rates, today, are some of the highest for any packaging material. Ratios for other materials such as aluminum and PET might look close. In reality, they relate to a mixed process of recycling and down-cycling.\nWhen a bottle of glass is melted, it can be made into another glass bottle with the exact characteristics. With a plastic bottle, you can only do that so many times before you can’t use it to hold food or beverages. Also, every 10% of recycled glass used in production results in an approximate 5% reduction in carbon emissions, and energy savings of about 3%. Every 1.0 kg of recycled glass used replaces 1.2 kg of virgin raw materials that would otherwise need to be extracted; plus, it can be recycled ad infinitum with the same material properties.\nOn top of all these benefits, glass packaging has powerful returnable systems. When glass is reused, the difference in CO2 footprint and value for this sustainability proposition becomes huge. It’s more than double between glass and alternative packaging.\nParedes: There are also new handling efficiencies in the packaging industry that allow the use of lighter weight packages [in filling and case-packing lines]. Before, we might have over-engineered our packaging to accept any type of harsh environment [without breakage]. Now, we’re doing a better job of putting weight in where it’s needed. The industry is helping by giving us feedback on the lightweight packages’ fill rates and handling.\nWhat other trends do you see coming down the proverbial turnpike?\nParedes: The trend for easy-open closures is not going away. There’s an aging population in the U.S., and package designers want to accommodate that marketplace. Also, anything we design for the aging markets should help us reach the younger markets, especially children who are just learning to self-serve.\nI’m also seeing an emergence of decorative push-ups in glass bottles. Here, the bottle structure might be very simple, but the designer personalizes the package with a custom bottom plate. What’s wonderful about this type of decoration is that the consumer views the branding application through the top to the bottom, so it serves as an invitation.\nPractically, it’s also a relatively inexpensive way to personalize a bottle. The bottom of the bottle is created by an independent hockey-puck-like piece of the mold. So you can get this special type of structural branding without having to own the entire mold.\nYanez: The packaged goods consumer is very different from the consumer of five or 10 years ago. We commissioned an Omnibus survey in April 2011 that showed that 69% of U.S. consumers would choose glass if they could buy their favorite foods or beverages in any type of container. Ninety percent said they agree that glass is the healthiest packaging, because it doesn’t leach into the product. Sixty-eight percent believe that glass containers are the most environmentally friendly packaging material, and 85% say food and beverages taste better in glass.\nThese consumers want packaging that better serves the flavor and quality of their foods and beverages, and this will work in favor of glass.\nFor articles on similar topics, visit the Glass Channel.']	['<urn:uuid:29a594b1-7591-4dba-9e50-b2b89f93923a>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:18:19.158519	7	43	1287
50	vinyl fence vs wood fence maintenance costs which one requires more work	Wood fences require more maintenance work. While vinyl fences are generally low maintenance, wood fences need regular application of water-repellent preservatives or sealants (costing $10.99 to $18.99 per gallon) to prevent rotting. However, if a vinyl fence yellows due to UV exposure, it will require repainting once or twice a year, turning it into a high-maintenance fence.	"[""Should I Repair or Replace My Vinyl Fence?\nEvery time I drive through north Texas ranch country, I’m amazed by the endless rows of pristine white vinyl fences that outline the region’s prime horse-ranching real estate. Vinyl fencing, made from extruded polyvinyl chloride (PVC), offers an attractive residential fencing solution as well. The rigid vinyl is low maintenance, impervious to insect damage, and just a bit flexible, which helps it withstand high winds without breaking.\nVinyl fencing has some downsides, however. Low quality vinyl can turn brittle in a couple of years, holes drilled in a vinyl fence can cause splitting and permit water to seep in, and a small grass fire can quickly turn a vinyl fence into a melted pretzel. Deciding whether to repair or replace a vinyl fence depends on factors ranging from the age and quality of the fence to the type and extent of damage. If repairs encompass anything more than a patching a simple crack, your best bet is to hire a knowledgeable vinyl fencing contractor.\nShoddy Workmanship Equals an Unsightly Fence\nUnlike wood fences that can be slightly out of kilter and still look respectable, vinyl fences that lean or slant often look worse than no fencing at all. With perfectly chiseled pickets and rails, vinyl fences must be laser straight in order to look their best. Big box stores will sell vinyl fencing to anyone with a credit card, but installing a vinyl fence isn’t a do-it-yourself project.\nYellowing and Stains\nDiscoloration is a known problem with some types of vinyl fencing. The PVC-extrusion industry is constantly working to improve its products, and there are some wonderful vinyl fences on the market. However, old and poor-quality vinyl fences are susceptible to yellowing and developing of unsightly stains due to hard water and mildew. Rust-removal products (as long as they’re vinyl-safe) will eliminate most hard water stains, and you can remove mildew stains with a diluted bleach-and-water solution. Treat the fence with a sealant after you clean it, however, so that the stains will not recur.\nYou can’t remove yellowing, which is caused by exposure to the sun’s harsh UV rays, because it’s more than surface deep ... but you can paint over it. The problem with this solution is that your maintenance-free fence is now a high-maintenance fence that requires regular repainting. If you don’t mind taking the time once or twice a year to maintain the fence, you can keep it looking relatively good. If you don’t want the hassle, though, a new fence is probably in order.\nIt's Important to Set Fence Posts Right\nFence posts are slightly different from decorative rails, panels, or pickets, because they form the structural foundation for the rest of the fence. Poor workmanship during post-setting, such as the failure to secure the posts with concrete or align them, is correctable by pulling the old posts and setting new ones. As long as the existing rails, panels or pickets are in good shape, you can probably reuse them but plan to install new posts. For a vinyl fence to look sharp, its posts must be precisely aligned.\nPosts Cracking due to Water\nMany homeowners think ground-level cracks in vinyl fence posts are the result of high winds or rowdy kids, but if the fence itself isn’t leaning, the cracks are more likely due to water entering the posts and then freezing and expanding. Uncapped post tops are an obvious problem, but ill-fitting caps and small holes in the posts can also let in enough water to create a freezing/cracking problem.\nOnce water gets into a hollow post, it has nowhere to go since the bottom of the post sits in concrete. Therefore, the post continues to collect more water whenever it rains or you water the lawn. When Old Man Winter arrives, the water freezes, cracks the post, and then quietly drains away as the weather warms. If a crack is small, you can fix it with a vinyl patch kit. Pinpoint where the water is coming in and seal that spot as well, so you don’t have another crack to patch next spring.\nPicket and Panel Replacement\nIf only a few panels or pickets are missing or damaged, you can save some bucks by replacing only those items, although you could run into trouble finding matching replacements if the fence is more than a couple years old. If you’re replacing parts of a vinyl fence that’s yellowed or stained, keep in mind that the new white pieces will stand out like a sore thumb against the older discolored fencing.\nFence Replacement Tips\nIf you decide your old fence isn’t worth the time or money to repair, keep a few things in mind before buying a new vinyl fence:\n- Check your local codes. Some communities restrict fence height, while others do not permit solid-panel fences that completely block all view of the yard.\n- Look for vinyl fencing that contains titanium oxide, which prevents discoloration.\n- Hire a professional to install the fence, check out his references, and make sure the fence and the installation come with a warranty.\n- Installing a gate in a vinyl fence is a complicated process, requiring steel and concrete reinforcement, because vinyl posts alone, do not have the strength to support a gate.\n- Buy the best quality vinyl fence you can afford. A premium vinyl fence will boost curb appeal and will retain its good looks for decades.\nRecycle that old vinyl fence – don’t contribute to the growing landfill problem. Vinyl, which is partially derived from natural gas, is recyclable and can be remanufactured into many useful items. By recycling your old fence, you’ll be doing your part to reduce the demand on our natural resources.\nUpdated March 22, 2018.\nGlenda Taylor writes for Networx.com.\nLooking for a Pro? Call us(866) 441-6648"", 'How To Care For a Wood Fence\nOutdoor Japanese Rock Garden in Private Contemporary Home\nThis modern style Japanese rock garden is in keeping with the natural stone elements of the home. The plants provide a bit of greenery, as well as, soften up the space. The surrounding wood fence adds another layer of texture and provided privacy to the space.\nWooden Fence With Painted Graphic Pattern\nAdd an artistic focal point outdoors with a graphic painted fence treatment. This classic midcentury pattern was made with a printable template taped up to the fence and painted with an aged look.\nTropical Plants Among White Fence\nA wood fence can be a great way to give a home\'s property a definitive border. It keeps both kids and pets safely in and stray animals out. It also changes the look of the house. But over time, wood fences rot away. However, you can make wood fences last longer with a little care.\nTypically, wood fences, especially those going up around newly constructed homes, are made of cedar or redwood.\n""They are more resistant to rot and decay than other species,"" says Jack Holt, owner of Holt Lumber in Fresno, Calif. ""They\'re not structurally as strong as Douglas fir, which is used in framing a house, but for anything that is exposed to the elements, those are the two species of wood used.""\nIn buying wood for fences, people should get pressure-treated wood for posts, says Ethan Elaison, co-owner of Elaison Lumber in Fresno. Posts should be pressure-treated because they go into the ground, where they are susceptible to insects and moisture.\n""When they go through the mill, they are chemically treated to resist bugs and water damage,"" Elaison says. Addressing concerns about the effects of arsenic and other chemicals used to treat the wood leaching into the ground, Elaison says they ""are not anything to worry about. The chemicals aren\'t extremely potent, especially lately. The chemicals have been toned down a little.""\nPressure-treated posts, which are indicated with a label or stamp, cost about $10 each.\nIs Your Wooden Fence Too Far Gone? 02:27\nPickets don\'t need to be pressure-treated because they are above ground, say Holt and Elaison. Fence pickets cost between $1.70 and $3.\nWithin the first year of installation, the wood may warp or split as it dries, says John Elaison, co-owner and founder of Elaison Lumber. In five to eight years, the fence may start to lean, but it should last 10 to 15 years overall. To keep the wood from rotting, apply a wood preservative to seal it. ""It\'s like wax on a car,"" Holt says. ""It helps protect it from the elements, from water, from sprinklers.""\nWater-repellent preservatives and sealants are sold at home-improvement and hardware stores. They cost $10.99 to $18.99 for a gallon. Exterior stains, which usually also are sealants, are another option if homeowners want their fence to coordinate with the exterior of their homes. Semi-transparent stains costs $17 to $18 per gallon.\nIf wood has been pressure-treated, a sealant isn\'t necessary, as the wood already has been injected with preservatives, John Elaison says.\nHomeowners also should keep sprinklers from wetting the fence, which causes the wood to rot faster. And bushes and vines should be kept off the fence.\n""The less weight on the fence, the better,"" Ethan Elaison says. ""The vine will help keep it moist and help it rot faster.""']"	['<urn:uuid:afa4bb52-9687-4cd6-91b4-591c5676274c>', '<urn:uuid:d45f311a-5e54-4055-b5ea-13ed96edd6b0>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T05:18:19.158519	12	57	1546
51	What should I clean a snake bite with?	You should clean the bitten area using betadine, but never use water.	['All of us do not want to have an experience with a snake bite, especially the venomous ones. But, whether the one which bites you is venomous or not, experts said that you must treat all of the snake bites as it is venomous. Since you cannot use your own judgment to decide whether the one which has bitten you is venomous or not.\nThe first thing to do is going to the hospital or calling for the emergency help. But while waiting for the medical help, whether it is you who are bitten, the first aid you can do are:\n- Someone does a CPR if needed\n- Immobilize the bitten area\n- Help the victim to keep calm\n- Make a mental picture of the snake: its color, shape etc.\n- Write the time when you are bitten.\nHow long does the venom stay in your system?\nThe faster an antivenin is given, the better. There is no a certain period of time for the venom to affect your whole body systems. Some of them may take a few hours, the others take several days to react. Why? Because the “outcome” is affected by combinations of variables:\n- The snake itself: its type, body temperature, mood.\n- The venom itself: its type, quantity (amount of venom) and toxicity level.\n- The victim: his or her allergic reaction, and health.\nYes, it does. Snake venoms consist of many enzymes, and these enzymes bring different effects. Some of them may damage your system and lead to death. One example is cholinesterase (neurotoxins), it attacks the nervous system and blocks the receptors site in the diaphragm which leads to a malfunction of muscles. If it continues, the patient will die in 30 minutes unless right after the bite is given, she or he got the proper antivenin and being put on the artificial respirator.\nThat is just an enzyme, unfortunately, a snake venom consists of more than one enzymes.\nDue to how dangerous the snake venom is when a person is bitten, the best treatment is an antivenin and other medical treatments from the professionals. But after being bitten, there are several things you or people around you can do while waiting for the medical help.\nWhat to do\n- Keep calm , being panic just increase the heart rate and make the venom spread faster.\n- Stop any activity immediately and limit the movements.\n- Clean the bitten area using betadine , but never use water.\n- If available, use a splinting to immobilize the bitten area. But do not make it too tight, it may affects the blood circulation.\nWhat not to do\nThere are also things you must not do after you are bitten by a venomous snake:\n- No alcohol, caffeine etc.\n- Do not apply any icing treatment to the bite, several studies prove that icing makes the venom spread faster.\n- Do not cut or make any incision on the bitten area. It will waste the time you have, lead to infection and cause an unwanted blood loss.\n- Do not try to suck the venom. It is dangerous.\n- Do not burn the bitten area, it is useless since once the venom has entered the body, it has entered the bloodstream.\n- Do not fall asleep. It will reduce your awareness.\n- Do not apply any medication until you get the medical help.']	['<urn:uuid:499cf603-11a8-4995-a413-b1b56b8783be>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	8	12	569
52	senior yoga benefit pain and falls	Yoga offers dual benefits for seniors: it can help manage arthritis pain and significantly reduce fall risk. According to medical experts, yoga is a type of exercise that focuses on tendons and ligaments and is designed for preventive joint health. Research has shown that practicing Hatha Yoga biweekly for 8 weeks reduced falls by 48% in older adults. This is particularly important since falls are the primary cause of injury death in adults 65+ and are especially dangerous for those with conditions like arthritis.	"['An estimated 50 million Americans are living with arthritis, and while the pain, stiffness and joint deformities that often go along with it can be debilitating, medical experts say there are treatments that can bring relief to help sufferers live full and productive lives.\nThere are more than 100 different forms of arthritis, according to the Arthritis Foundation. The most common types are osteoarthritis, caused by ""wear and tear"" on joints, and rheumatoid arthritis, caused when the body attacks its own tissues, leading to inflammation of the joints.\nAlong with commonly prescribed painkillers, surgeries and other medical interventions, there are also a number of complementary approaches specialists use that they say can decrease inflammation, reduce pain and promote overall health.\nABC News asked doctors who practice integrative medicine, a field focused on blending conventional medicine with complementary treatments, to weigh in on what options they recommend for arthritis.\nA few of the approaches they discuss on the following pages work for several types of arthritis because they target inflammation, a characteristic some forms of the condition have in common. Other approaches are more targeted. But no matter what the approach, experts stress before beginning any therapy, people should consult their doctors because not every option is appropriate for everyone with arthritis.\n""Certain anti-inflammatory ingredients can be incorporated into the diet, such as tumeric and ginger,"" said Dr. Ashwin Mehta, medical director of integrative medicine at the University of Miami\'s Miller School of Medicine. ""Ginger and tumeric are powerful anti-inflammatory ingredients we can recommend pretty much to anybody. They are very safe and have no potential medication interactions or complications.""\nThere have been few clinical trials looking at the effectiveness of ginger and tumeric on inflammation, but there are some laboratory data that suggest both can be helpful.\n""Studies have been done on ginger and tumeric and have shown some anti-inflammatory effects, so there is at least some basic science to suggest these might be helpful,"" said Dr. Joanne Jordan, director of the Thurston Arthritis Research Center at the University of North Carolina School of Medicine in Chapel Hill.\nOther food additives are considered to have anti-inflammatory properties, such as garlic, cinnamon and soy.\nBut while he recommends ginger and tumeric to anyone with arthritis, Mehta said before incorporating others into the diet, people should consult a physician, because certain ingredients may work better with certain symptoms. Others, he added, may interact with certain medications.\nCutting back on refined sugars can also reduce inflammation, Mehta added. That dietary tip will help with all types of arthritis.\nIn some people, foods that cause allergic reactions may be foods that support inflammation, so arthritics with any sensitivity to foods should avoid those foods.\nAnother ingredient doctors recommend is capsaicin, found in very hot peppers. It\'s often used in topical ointments or creams made specially for pain relief.\n""It\'s helpful for osteoarthritis and rheumatoid arthritis when it\'s used as a topical remedy, although some people do eat it in the form of red, hot chili peppers,"" said Dr. Lawrence Taw, clinical professor at UCLA\'s Center for East-West Medicine. ""It tends to help arthritis that is worse when it\'s cold, windy and damp.""\nAlthough some people may believe having arthritis means doing exercise will cause further damage to the joints and others may find it too painful to be active, doctors stress that staying active is key to managing the symptoms of arthritis.\n""Physical activity and exercise are very important, especially something like yoga that would target the joints,"" said Mehta. ""Yoga is a type of exercise that focuses on the nuts and bolts that hold everything else together, like the tendons and ligaments, and it\'s designed for preventive joint health.""\n""Exercises like yoga and tai chi incorporate physical stretching and deep breathing,"" Taw added. ""Gentle water therapy is a very good exercise for people with arthritis since it is easy on the joints.""\nStretching helps, but as with any exercise, it should never be performed to the point where it causes pain.\nExercise can also help people lose weight, which is very effective at reducing joint inflammation, he said.\n""Even if you lose as little as four pounds, the stress and pressure you put on your knee joints is lessened and you really end up feeling better.""\n""For every person with arthritis, we recommend one nutritional or dietary change, one physical activity goal or exercise treatment and a mindfulness program of some kind,"" Mehta said.\nMehta explained that a benefit of mindfulness programs is that they teach people how to relax, which can be especially helpful for people with rheumatoid arthritis.\n""It can take 30-plus minutes to get up out of bed in the morning because of stiffness, so there\'s a tendency to stay in bed and not be motivated. These programs help people get motivated and they enlist the mind to help cope with the debilitating effects,"" he said.\nAcupuncture and Temperature Remedies\n""Acupuncture has been found to be effective for osteoarthritis of the knee and hip as well as for rheumatoid arthritis, and a small study found it helps arthritis in patients with systemic lupus erythematosus,"" Taw said.\nOn its web site, the American College of Rheumatology explains that studies have found acupuncture to be effective at relieving pain related to osteoarthritis, and it may be that the needle contact with the skin is what causes the decreased pain. But they go on to say that acupuncture is safe in combination with conventional treatments.\nAnd since symptoms of different types of arthritis may be triggered by the temperature or the weather, experts may recommend heat or cold therapy.\nOsteoarthritis and rheumatoid arthritis tend to be worse in cold weather, and the pain associated with lupus can be worse in warm weather.\n""Simple things like taking warm showers and using heating pads can relieve symptoms, and if the pain is worse with warm weather, we recommend cooling measures,"" Taw said.\nIf recommended by a health care provider, arthritis specialists say these and other non-conventional therapies can offer arthritis sufferers additional ways to keep their pain and inflammation in check and live free of the disabling effects of this common disorder.', 'View basket (0 items $0.00)\nYoga for Osteoporosis - Focus on Fracture Prevention\nCan yoga reduce the risk of osteoporotic fractures by reducing fall risk? Indeed, a growing body of research shows that yoga can improve balance. Now, a new study finds that practicing yoga can also reduce the risk of falling.\nFalling becomes an increasing risk as we age, affecting 30% of people over age 65 years and 50% for those over 80. For people with osteoporosis, falls are particularly troublesome, as most osteoporotic fractures are caused by a fall.\nIn fact, a growing number of researchers believe that the emphasis in osteoporosis prevention should be shifted from focusing on building bone mass to efforts to prevent falls. And indeed, efforts to build balance and proprioception have shown great promise for preventing fall.\nStudy Shows Yoga Prevents Falls for Older Adults\nA study published in Complementary Therapies in Medicine, examined older, rural, adults attending Hatha Yoga classes and the effect it had on their incidence of falling.\nThe 38 study participants (average age of 70 years old), attended Hatha Yoga classes biweekly for 8 weeks. All of the participants were assigned “homework” of doing five minutes of meditation at home; half of them were also randomly assigned to do an additional 10 minutes of yoga.\nResearchers compared participants’ incidence of falling during the six months prior to and the six months following the start of yoga classes and found that the number of falls in older adults dropped 48 percent in the six months after yoga classes began.\nThere was no significant difference between the group that did and didn’t do additional “homework” yoga. Lead researcher, Irene Hamrick, has a theory why: “We have always harped on practice, along the lines of what we suggest for building muscles. But maybe the brain-nerve-muscle connection that is built in classes just needs to be stimulated periodically, and the classes are enough to do that.”\nWhile the study was small, the results are very promising: falls are the primary cause of injury death in adults 65+, as well as the most common cause of nonfatal injuries and hospital admissions for trauma, and yoga can have a sizable impact on the preventing falls.\nYoga for Older Adults in Rural Areas\nOne additional noteworthy aspect of this yoga study was that it was done in a rural area of Western Wisconsin; there have only been a handful of studies addressing yoga practice in rural demographics. A yoga therapy workforce survey found that, yoga therapists were less likely to work in rural settings. This could be a contributing factor to why this underserved population isn’t taking advantage of the many therapeutic benefits of yoga in the way that more urban centers are.\nAs the Baby Boomer generation matures, rural areas in particular see an increased number of older adults at greater risk of falling. Hopefully the results from this yoga study pave the way for increased interest in yoga and increased accessibility to it for older adults living in rural areas.']"	['<urn:uuid:1bfbb0d4-c7da-4b8b-9872-e4b421e46954>', '<urn:uuid:17388b32-9b59-441c-8eed-7c1d1d7f8b56>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	6	84	1533
53	wired infrastructure dependent versus adhoc networks deployment characteristics compare	Infrastructure-dependent networks and ad-hoc networks have distinct deployment characteristics. Infrastructure networks require wired backbone networks and access points (APs) to function, often relying on high-speed optical fiber for the backbone. In contrast, ad-hoc networks can operate without any fixed infrastructure, allowing direct communication between stations. This is particularly useful in scenarios where infrastructure is not available. For example, Winlink can operate in both modes - it typically uses infrastructure with Remote Message Servers (RMS) connected to Central Message Servers (CMS) via internet, but can also work in local stand-alone mode for peer-to-peer messaging when infrastructure is knocked out.	"['Do you know what Winlink is and what it’s used for? Have you heard folks talking about Winlink and wondered what the heck is this system? Here’s a little help from Wikipedia:\nWinlink, also known as the Winlink 2000 Network, is a worldwide radio messaging system that uses amateur-band radio frequencies to provide radio interconnection services that include email with attachments, position reporting, weather bulletins, emergency relief communications, and message relay. The system is built and administered by volunteers and administered by the Amateur Radio Safety Foundation Inc., an American charitable entity and 501c(3) non-profit organization.\nOK, that doesn’t really get down to the brass tacks for us here in our local area. What do WE do with Winlink and what is it really?\nWell, it is indeed a worldwide radio messaging system. It provides Remote Message Servers (RMS’s) around the world that operate on a wide variety of frequencies and modes. The RMS’s provide direct connection to a Central Message Server (CMS) via internet. CMS’s are effectively e-mail servers associated with the Winlink system. There are five redundant CMS locations around the world including San Diego (California), Brentwood (Tennessee), Perth (Australia), Halifax (Canada), Wien (Austria). These servers constantly synchronize with each other. Now, we can ALSO operate in “local” stand alone mode and create our own CMS or CMS’s. This is especially helpful in case of a scenario where infrastructure, including internet, is knocked out and allows us to use the great features of the Winlink system to manage local message traffic.\nBut how does this really work? Well, your objective as a “client” would be to connect to an “RMS”. Making a connection to an RMS results in the Winlink system doing two things:\n- If you have message traffic to send, it will accept the traffic from you.\n- If there is traffic waiting to be delivered to you, it will indeed be delivered be delivered to you. Once those transactions are complete, they system will disconnect.\nOK, now you’re asking about frequency bands, modes, etc. Can’t blame you. The reality is that there are RMS’s operating on HF, 2-meters, 220 MHz, 440 MHz, and even on MESH networks. We actually have RMS’s here locally in Anchorage and the MatSu areas available on ALL these bands. On HF, the normal mode of operation is Pactor (although other modes are used, such as Winmor). On 2-meters, 220 MHz, and 440 MHz, the mode used is packet, at either 1200 or 9600 baud. Then of course there are the RMS’s operating on our MESH network at 3.4 GHz.\nThis is all great, but what about the darn software I have to run? Well, the Winlink system will indeed work with a number of “client” software packages, but the package of choice, at least our choice, is Winlink Express. Winlink express allows use of all the features of the Winlink system as a whole, including use of the various modes on different bands to get the job done.\nSo, what are the “local” RMS stations you ask? Here’s a list of local stations (and this will change somewhat as we develop and evolve the system):\n- HF (Center frequencies 3589.0 KHz, 7075.9 KHz, 7101.7 KHz, 10143.7 KHz – PACTOR 1, 2, and 3): WL7CVG – Located at Alaska Native Medical Center. This station is connected to the worldwide Winlink CMS system.\n- VHF (144.90 MHz, 1200 Baud Packet): WL7CVG-10 – Located on Elmendorf AFB at the EARS club station. This station is connected to the worldwide Winlink CMS system.\n- VHF (144.98 MHz, 1200 Baud Packet): WL7CVG-10 – Located at AARC Club Station – This works with our LOCAL MESH-synced messaging server and is NOT connected to the worldwide Winlink CMS system at this time.\n- VHF (144.98 MHz, 1200 Baud Packet): KL7AA-9 – Located at the ANMC Tuttle Drive warehouse. Station – This works with our LOCAL MESH-synced messaging server and is NOT connected to the worldwide Winlink CMS system at this time. We anticipate moving this RMS to another 2-meter frequency and shifting it to 9600 Baud Packet).\n- VHF (223.66 MHz, 9600 Baud Packet): WL7CVG-10 – Located at AARC Club Station – This works with our LOCAL MESH-synced messaging server and is NOT connected to the worldwide Winlink CMS system at this time.\n- UHF (441.175 MHz, 9600 Baud Packet): Located on Elmendorf AFB at the EARS club station. This works with our LOCAL MESH-synced messaging server and is NOT connected to the worldwide Winlink CMS system at this time.\nIn addition to sending messages using the Winlink system’s RMS/CMS features, there’s also an ability to move traffic peer-to-peer, or in other words, one station directly to another.\nAnother new feature we now have is the ability to interconnect all our local RMS’s via our Mesh network. This is transparent to the end user, but you just need to know that any message sent to a local Mesh-synced RMS is available for pickup via any other local Mesh-synced RMS.\nIf you’d like to learn more about Winlink, the Mesh Network, or our Anchorage ARES effort, join us at the club station on Working Wednesdays. We’re usually opened up by 1800L.', 'Presentation on theme: ""Week 3 Lecture 1 Mobile Health (M-Health). What is Mobile Health (M-Health)? One way to achieve tele-healthcare [WiKi definition] mHealth (also written.""— Presentation transcript:\nWhat is Mobile Health (M-Health)? One way to achieve tele-healthcare [WiKi definition] mHealth (also written as m- health or mobile health) is a term used for the practice of medicine and public health, supported by mobile devices. The term is most commonly used in reference to using mobile communication devices, such as mobile phones, tablet computers and PDAs, for health services and information.mobile phonesPDAshealth services It is based on Wireless & Mobile technologies (such as Wireless LAN, WiMax, Cellular networks, Satellite, Sensor Networks, Ad hoc Networks, etc.)\nThinking: How to monitor a patient whenever he/she goes to? When the patient is at home -- can use telephone (dial-up) – too slow -- Comcast – Cable Modem - Fast -- ADSL (Asymmetric Digital Subscriber Line) from AT & T -- Bluetooth + Internet (the above 3 also need Internet) When the patient is in a building -- Wireless LAN (also called WiFi) -- Wired LAN (high speed!) When the patient is driving /walking -- Cell phone -- WiMax -- Mobile Ad hoc Network, Vehicle Ad hoc Network, etc.\nWhat wireless & mobile networks can be used? Wireless is convenient – anywhere, anytime No cable needed However, in the backbone networks, to achieve high- speed, long-distance transmission, in many times we may still use Internet with wires (most likely optical fiber) Wireless Networks are much slower (most times) than wired ones (especially optical fiber); But WiMax can achieve >100Mbps in good conditions. Wireless also brings much higher data transmission errors or data loss due to noise / interference In the remaining contents we will discuss popular wireless\nSridhar IyerIIT Bombay5 Wireless networks Access computing/communication services, on the move Wireless WANs Cellular Networks: GSM, CDMA Satellite Networks: Iridium Wireless LANs WiFi Networks: 802.11 Personal Area Networks: Bluetooth Wireless MANs WiMaX Networks: 802.16 Mesh Networks: Multi-hop WiFi Adhoc Networks: useful when infrastructure not available\nMobile communication Wireless vs. mobile Examples stationary computer laptop in a hotel (portable) wireless LAN in historic buildings Personal Digital Assistant (PDA) Integration of wireless into existing fixed networks: Local area networks: IEEE 802.11, ETSI (HIPERLAN) Wide area networks: Cellular 3G, IEEE 802.16 Internet: Mobile IP extension\nSummary Relative coverage, mobility, and data rates of generations of cellular systems and local broadband and ad hoc networks.\nSridhar IyerIIT Bombay10 Wireless frequency allocation Radio frequencies range from 9KHz to 400GHZ (ITU) Microwave frequency range 1 GHz to 40 GHz Directional beams possible Suitable for point-to-point transmission Used for satellite communications Radio frequency range 30 MHz to 1 GHz Suitable for omnidirectional applications Infrared frequency range Roughly, 3x10 11 to 2x10 14 Hz Useful in local point-to-point multipoint applications within confined areas\nSridhar IyerIIT Bombay11 Frequency regulations Frequencies from 9KHz to 300 MHZ in high demand (especially VHF: 30-300MHZ) Two unlicensed bands Industrial, Science, and Medicine (ISM): 2.4 GHz Unlicensed National Information Infrastructure (UNII): 5.2 GHz Different agencies license and regulate www.fcc.gov - US www.etsi.org - Europe www.wpc.dot.gov.in - India www.itu.org - International co-ordination Regional, national, and international issues Procedures for military, emergency, air traffic control, etc\n12 Communication Process Information Source Transmitter Receiver Noise Source Noise Source Destination Communication Channel 0 1 0 0 1 1 0 0 1 0 Input Signal: Source Data: Noise: Output Signal: Sampling Times: 0 1 0 0 1 1 0 0 1 0 Source Data: 1 1 0 0 1 1 1 0 1 0 Data Received: Bits in error S N S+N Decision threshold\n13 Wireless Transmission and Receiving System Modulator Destination Error Control Encoder Source Decoder (Decompress) Source Encoder (Compress) Error Control Decoder Information Source Demodulator Transmitter Receiver Communication Channel\nSridhar IyerIIT Bombay14 Antennas An antenna is an electrical conductor or system of conductors to send/receive RF signals Transmission - radiates electromagnetic energy into space Reception - collects electromagnetic energy from space In two-way communication, the same antenna can be used for transmission and reception Omnidirectional Antenna (lower frequency) Directional Antenna (higher frequency)\nSridhar IyerIIT Bombay15 Signal propagation Propagation in free space always like light (straight line) Receiving power proportional to 1/d² (d = distance between sender and receiver) Receiving power additionally influenced by fading (frequency dependent) shadowing reflection at large obstacles refraction depending on the density of a medium scattering at small obstacles diffraction at edges reflectionscatteringdiffractionshadowing refraction\nSridhar IyerIIT Bombay16 Bandwidth Amount of data that can be transmitted per unit time expressed in cycles per second, or Hertz (Hz) for analog devices expressed in bits per second (bps) for digital devices KB = 2^10 bytes; Mbps = 10^6 bps Link v/s End-to-End\nSridhar IyerIIT Bombay17 Latency (delay) Time it takes to send message from point A to point B Latency = Propagation + Transmit + Queue Propagation = Distance / SpeedOfLight Transmit = Size / Bandwidth Queueing not relevant for direct links Bandwidth not relevant if Size = 1 bit Software overhead can dominate when Distance is small RTT: round-trip time\nSridhar IyerIIT Bombay19 Wireless LANs Infrared (IrDA) or radio links (Wavelan) Advantages very flexible within the reception area Ad-hoc networks possible (almost) no wiring difficulties Disadvantages low bandwidth compared to wired networks many proprietary solutions Infrastructure v/s ad-hoc networks (802.11)\nSridhar IyerIIT Bombay20 802.11 variants MAC MIB DSSS FH IR PHY WEP LLC MAC Mgmt 802.11b 5,11 Mbps 802.11g 20+ Mbps 802.11a 6,9,12,18,24 36,48,54 Mbps OFDM 802.11i security 802.11f Inter Access Point Protocol 802.11e QoS enhancements\nSridhar IyerIIT Bombay21 Infrastructure vs. Ad hoc networks infrastructure network ad-hoc network AP wired network AP: Access Point Source: Schiller\nSridhar IyerIIT Bombay22 Components of IEEE 802.11 architecture The basic service set (BSS) is the basic building block of an IEEE 802.11 LAN The ovals can be thought of as the coverage area within which member stations can directly communicate The Independent BSS (IBSS) is the simplest LAN. It may consist of as few as two stations ad-hoc network BSS2 BSS1\nSridhar IyerIIT Bombay23 Distribution System Portal 802.x LAN Access Point 802.11 LAN BSS 2 802.11 LAN BSS 1 Access Point 802.11 - infrastructure network (PCF) Station (STA) terminal with access mechanisms to the wireless medium and radio contact to the access point Basic Service Set (BSS) group of stations using the same radio frequency Access Point station integrated into the wireless LAN and the distribution system Portal bridge to other (wired) networks Distribution System interconnection network to form one logical network (EES: Extended Service Set) based on several BSS STA 1 STA 2 STA 3 ESS Source: Schiller\n24 W-LAN Transmission Rates 11 Mbps 8 % of coverage area 1 Mbps 47 % of coverage area Low probability of having good link!! Lucent ORiNICO 802.11b outdoors, no obstruction—ideal conditions!\nSridhar IyerIIT Bombay26 Motivation for 802.16 Broadband: A transmission facility having a bandwidth sufficient to carry multiple voice, video or data, simultaneously. High-capacity fiber to every user is expensive. Broadband Wireless Access: provides “First-mile” network access to buildings. Cost effective and easy deployment.\nSridhar IyerIIT Bombay27 IEEE 802.16 WirelessMAN air interface for fixed point to multi-point BWA Broad bandwidth: 10-66 GHz Channel as wide as 28 MHz and Data rate upto 134 Mbps MAC designed for efficient use of spectrum Bandwidth on demand QoS Support\nSridhar IyerIIT Bombay29 802.16: Summary Higher throughput at longer ranges (up to 50 km) Better bits/second/Hz at longer ranges Scalable system capacity Easy addition of channels maximizes cell capacity Flexible channel bandwidths accommodate allocations for both licensed and license-exempt spectrums Coverage Standards-based mesh and smart antenna support Adaptive modulation enables tradeoff of bandwidth for range Quality of Service Grant / request MAC supports voice and video Differentiated service levels: E1/T1 for business, best effort for residential\nSridhar IyerIIT Bombay30 Mesh and Adhoc Networks\nSridhar IyerIIT Bombay31 Multi-Hop Wireless May need to traverse multiple links to reach destination Mobility causes route changes\nSridhar IyerIIT Bombay32 Route discovery B A S E F H J D C G I K Node H receives packet RREQ from two neighbors: potential for collision Z Y M N L [S,E] [S,C]\nHeterogeneous Cellular Networks Seamless mobility across diverse overlay networks “vertical” hand-offs software “agents” for heterogeneity management IP as the common denominator? High-tier Low-tier Satellite High Mobility Low Mobility Wide Area Regional Area Local Area\nBASIC ARCHITECTURE BACKBONE TELEPHONE NETWORK (HLR) Mobile Switching Center Visitor Location Register Mobile Terminal (MT) MSC VLR Local SignalingLong Distance Signaling (MSC) (VLR) Home Location Register\nCellular Concept The most important factor is the size and the shape of a CELL. A cell is the radio coverage area by a transmitting station or a BS. Ideally, the area covered a by a cell could be represented by a circular cell with a radius R from the center of a BS. Many factors may cause reflections and refractions of the signals, e.g., elevation of the terrain, presence of a hill or a valley or a tall building and presence in the surrounding area. The actual shape of the cell is determined by the received signal strength. Thus, the coverage area may be a little distorted. We need an appropriate model of a cell for the analysis and evaluation. Many posible models: HEXAGON, SQUARE, EQUILATERAL TRIANGLE.\nCell R (a) Ideal Cell(b) Actual Cell R R R R (c) Different Cell Models Cell Shape']"	['<urn:uuid:322116ab-d980-4847-9b1b-1a0458937522>', '<urn:uuid:1327ec47-5049-4150-9f18-b8275a79edcf>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-13T05:18:19.158519	9	98	2412
54	I've heard some people talk about a Celtic tree calendar with months named after trees - is this actually something the ancient Celts used?	No, this is a false belief. The Celtic tree calendar is a modern invention, not something the ancient Celts actually used. While the Celts did have a calendar, they did not name their months after trees. The actual Celtic calendar (the Coligny calendar) had month names that translated to terms like 'Summer's End', 'Horse Month', and 'Hot Month'. This tree calendar myth originated in the nineteenth century and was later popularized by Robert Graves, but there is no historical evidence that the Celts ever named their months after trees.	"['The Celtic Tree Calendar\nModern pagans are wont to promote a certain ""Celtic Tree Calendar"" which has thirteen months, named for trees, based upon the ogham alphabet. Some of them assert that the Celts followed this calendar, and that proof of this can be found in the existence of ogham, and in the works of Robert Graves. Unfortunately, this is as false as the assertion that the druids built Stonehenge.\nFirst, it should be stated that like most (if not all) human civilizations, the Celts had a calendar--one which existed before the Romans, or at least independent of the Romans. It is known from Irish literature that there were four important holidays in the year--Samhain, Imbolc, Beltane, and Lughnassadh--and from archaeological findings (namely stone chambers, circles, etc.) it is possible they adopted the solstices and equinoxes, though these were not as important, but considered ""mid"" days (hense ""midwinter,"" ""midsummer"").\nWe are given more evidence through the existence of the Coligny calendar--twelve months plus an intercalary month, based on the Metonic cycle, syncing up both the solar and lunar calendars, but relying more on the moon. The names of the months are not derived from trees, however, but instead translate names such as ""Summer\'s End (?)"" (November?), ""Horse Month"" (July?), and ""Hot Month"" (September?). They had two ""weeks""--the first with fourteen days, the second with 14 or 15, depending on the apperance of the (full? new? this is still debated) moon. The year began on the first of Samhain--November 1. (For more, see the Coligny Calendar.)\nHowever, in the nineteenth century antiquarian Edward Davies, on looking at a 16th century history of Ireland called Ogygia by one Ruairí Ó Flaitheartaigh, suddenly decided that the Celts had a thirteen month calendar named for trees. Now, Edward Davies was one of Iolo Morgannwg\'s partners in the ""Druidic Revival"" of the late 18th/early 19th centuries, in which unforunately many documents were forged as ancient. This Ogygia, however, was not a forgery--Ó Flaitheartaigh\'s history (which names Ireland after a mythical island in the Odyssey so as to avoid persecution by the English) does contain a discussion of ogham, ultimately derived from the ogham tract in the Book of Ballymote. However, Ó Flaitheartaigh does not mention calendars, only trees. It was Davies who insinuated there might be a calendar, and Graves who devised this calendar based on the 13 consonants and 5 vowels of Ó Flaitheartaigh. (Ogham actually contains 25 letters, but the last 5 are considered ""extra"" letters for foriegn words, and likely point to a foreign origin for the alphabet.)\nWorse yet, people--including Graves--have now taken to calling ogham ""Ogygia"" after Ó Flaitheartaigh\'s book, ignoring the actual reason for his naming of the book. They even say ""Ogygia"" is a special, secret form of druidic ogham--which is laughable, as the druids never wrote down their teachings, much less carved them into stone.\nAt any rate, the dating of this calendar--as devised by Robert Graves and subsequently used by many--is as follows:\nThere are several problems with this calendar. First, he begins the year on December 23, not November 1--this is plainly wrong. He does this so as to coincide with the birth of the sun god. However, the Celts did not worship the sun per se, much less as a male sun (the word for ""sun"" in Irish is grainne--feminine, and the name of a goddess). Even if sunworship were involved, the year still began on Samhain, not midwinter. Now, he is somewhat correct in having months of 28 days; but the Celts would actually vary the lenght of the months, depending on the moon--Graves does not do this, but keeps the plan rigid.\nAnd, as stated before, there is no evidence that the Celts ever named their months after trees. It just didn\'t happen. This is the result of Davies and Graves remembering that the Celts followed a lunar year, and picking and choosing the facts to suit their purpose, a problem epidemic to The White Goddess, Graves\' wildly-popular book on the tree calendar, tree alphabet, and goddess worship. Graves even goes so far as to say that these 13 trees were also 13 signs of the zodiac--and that this constitutes the original Celtic Astrology. It does not. (read ""The Fabrication of \'Celtic\' Astrology"" by Peter Beresford Ellis.)\nThere have been a number of books based on Graves and his ""theories""--the absolutely dreadful 21 Lessons of Merlyn, various ""Celtic Wiccan"" books published by Llewellyn, and the ridiculous Handbook of Celtic Astrology by Helena Patterson--which takes this fallacy to a whole new level.\nI wish to note a delicious irony: While researching his book, Graves wrote to Robert MacAlister, at the time the greatest scholar on ogham and Irish in general--moreover, he was the president and editor of the Irish Texts Society, still the standard in translation. MacAlister warned Graves not to take the tree alphabet too seriously, as it only went back to the sixteenth century and not before. Graves ignored him. This shocked MacAlister, and many scholars for:\nIf Robert Graves thought the tree alphabet tradition only went back to the thirteenth century AD (the Book of Ballymote is actually late 14th Century), and that is precisely what MacAlister was warning him about, for we cannot trace it back beyond that time, how is he conjuring its use and claiming it as a mystical druidic calendar used in pre-Christian times?--Peter Berresford Ellis.\nPresumably Dr MacAlister must also have been surprised at Robert Graves\' argument\' because Robert Graves\' own grandfather, when President of the Royal Irish Academy, was leading authority on Ogham. Charles Graves had already dismissed the \'tree alphabet\' as entirely spurious. MacAlister, like all Celtic scholars who had ventured into the field of Ogham, was well aware of Charles Graves\' work and, indeed, MacAlister cites it in his own study.\nFor debunking the myth:\nBoutet, Michel-Gérald. ""Celtic Astrology: A modern Hoax""\nEllis, Peter Berresford. ""The Fabrication of \'Celtic\' Astrology.""\n----- The Druids. Wm. B. Eerdmans Pub. Co., 1995.\nMarkale, Jean The Pagan Book of Halloween. Inner Traditions, 2001. (Depsite the title, this is actually a good exploration of Samhain and the Celtic calendar, and includes a helpful section on the Coligny calendar).\nCross & Slover. Ancient Irish Tales. Barnes and Noble 1996.\nThe origins of this myth:\nGraves, Robert. The White Goddess. Noonday Press, 1992. (reprint)\nNichols, Ross. The Book of Druidry. HarperCollins, 1992. (reprint)\nPatterson, Helene. The Handbook of Celtic Astrology. Llewellyn, 1994.\nPlus just about any book or website on ""Celtic Wicca""\nBack to ""C"" | Back to JCE\nMary Jones © 2004']"	['<urn:uuid:5502a7a3-9651-425c-9e2c-0f97983751d2>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	24	89	1094
55	planning visit civil war landmarks tell me when did general robert e lee give up fighting	General Robert E. Lee surrendered his Army of Northern Virginia to General Ulysses S. Grant on April 9, 1865, which was Palm Sunday. This surrender took place in a farmhouse near Appomattox, Virginia, and effectively brought an end to four years of Civil War conflict.	['Let us say a prayer for Appomattox.\nNext year is the 150th anniversary of the end of the Civil War, and perhaps for a brief moment our attention will turn from the swirl of minutiae that consumes our lives and allow us to reflect on the lessons to be found in this small town tucked into the rolling hills of central Virginia.\nAppomattox. The name rolls off the tongue with surprising ease and no small amount of delight. It was in a farmhouse near here on April 9, 1865, Palm Sunday, that Gen. Robert E. Lee surrendered his Army of Northern Virginia to Gen. Ulysses S. Grant, effectively bringing a halt to four years of bloody conflict.\nA large and active group of people in the town of Appomattox and the county of the same name are preparing for the sesquicentennial. But the truth is, there is a far more important issue here than how many people show up in April to hear speeches at a restored village owned by the National Park Service.\nThis modest and welcoming community is gripped with an identity crisis. It is in part tied to the unraveling that haunts so much of rural America. The factory jobs have all but left. The new economy has yet to show up. Many residents say they would just as soon not depend on Civil War tourism or be a bedroom community to Lynchburg, about 22 miles west, but there may be no other choices.\nWhat’s at stake is more than just paychecks and longer commutes. Appomattox is like Selma or Yalta in that it is a small place where an event so important happened that the name has become a shorthand for something larger than itself. The notion of surrender, which is why Appomattox matters, makes us uncomfortable. We don’t like to quit, to give up, even when it’s done with honor and accepted with grace. But sometimes it’s necessary to stop what we’re doing, accept that the plan isn’t working, and do something else. That is what is slowly taking place in Appomattox, another surrender, different from the first but also filled with the uncertainty of what comes next.\nThe surrender here takes many forms, large and small. People want a steakhouse, preferably with the stamp of approval conferred by a national chain. They’re getting a Hardee’s. They had hoped a major hotel would be built in time for the events next year. That won’t happen by then, if ever. At Appomattox County High School, the staff spent the summer changing its block-letter “A” logo on everything from sticky notes to uniforms after the licensing agency representing the University of Arizona sent the school a cease-and-desist letter claiming potential confusion among consumers. “Our lawyer said we should change it rather than litigate it,” said Chris Dodge, the athletic director.\nPeople in Appomattox keep two lists of what their community lacks. The first carries a touch of envy. There is no Target here, no Starbucks, no cinema or bowling alley, no hospital. The new-car dealers pulled out a few years back. The second list, said with pride, includes a lack of traffic, crime and drama. There are no bars, pawnshops or even a jail.\nBut it’s not just a community defined by deficits. There’s a good bookstore and cafe and plenty of local hardware stores. People work hard, speak their minds and watch their tongues. It’s the sort of place where a clerk in the county’s revenue office can create a budget crisis by recording $638,000 in the wrong place, and her error is forgiven as a mistake anyone could have made. And the land here is beautiful, a well-worn quilt of meadows and forests and small crossroads. At dawn, with the fog lying low in the fields and the quiet of a new morning, it is easy to squint your way past the problems of today into a simpler time long past.\nVisitors to the town, population 1,733, are welcomed by a sign that shows portraits of Lee and Grant and proclaims the community as “where our nation reunited.” It’s a good slogan, but not quite true, more of a fudge than a lie. The surrender didn’t happen in Appomattox. It happened three miles away at McLean House in what is known as Appomattox Court House, the county seat in 1865. It was already a community in decline, bypassed by the railroad that swung to the south. When the courthouse there burned in 1892, the replacement was built in what is now the town of Appomattox, and the fading village of Appomattox Court House was all but abandoned.\nThe distance between the park and the town is a buffer that adds to the peace and serenity of the surrender grounds. But it’s also a barrier. Unlike in Gettysburg, where the national military park spills into the town, in Appomattox, the history is by default and design kept at arm’s length.\nThe largest reason for that detachment now sits vacant next to the Dairy Queen and across U.S. 460 from the Wal-Mart that may or may not have been built on a significant Civil War site. For 37 years, Thomasville Furniture Industries operated an enormous factory here, 14 football fields under roof, that at its peak employed 1,000 in a county of 15,000. After a series of layoffs, it closed in 2011, part of the bankruptcy of the parent company, Furniture Brands International. The machinery was auctioned off in April. Even today, nearly everybody has a connection to the factory, which anchored Appomattox’s sense of self-reliance and was a comforting constant until it wasn’t.\n“Appomattox put all its eggs in one basket with Thomasville,” said Paul Harvey, the mayor. He’s a dentist who grew up here, moved to Richmond, then returned in 1998 to a community that wasn’t much different from the one he left. “Now, we’re really trying to play catch-up.”\nBefore Thomasville’s closing, neither the town nor the county made much effort to recruit business or even figure out how to cash in on tourism. Mostly, the two elected boards fought, often over the town’s efforts to expand its boundaries and tax base through the addition of water and sewer service. The county pushed back, encouraged by residents who didn’t want to pay city taxes. It’s not exactly North vs. South, but Appomattox developed a reputation for petty fighting.\n“Companies aren’t interested in places that are feuding,” said Ronnie Spiggle, chairman of the county board. He has a unique vantage point of what has taken place and the efforts of late to repair the relationship, including the county allowing the town to annex the property where the Hardee’s is being built. Spiggle was mayor for 28 years, and his ex-wife is on the town council.\nSpiggle is also a funeral director, with the solicitude and bearing of a man who takes care of the living and the dead. I asked him what Appomattox wanted to become, and he said nobody really knew, but it was a constant source of friction, with one side wanting development, the other side wanting no part of that, and a hard road to the compromise necessary to take advantage of what was going to happen next April with or without a clear roadmap.\n“To have the war end here makes it a special place,” Spiggle said. “We sort of take it for granted. We have one chance next year to put our best foot forward and to show people a community that makes people want to come back.”\nThe pivot to an economy built around history and tourism, tying the future to the past, is tricky. It’s also compounded by the fact that for all the peace found at Appomattox Court House and the pleasure of hearing a reenactor in a soft accent tell stories of days gone by, attendance is a little shaky. That’s what worries the mayor, that the window to build a new Appomattox around history is already closing.\nAccording to the National Park Service, the golden days of visitation were in the mid-1990s after the release of the Ken Burns documentary “The Civil War.” More than 105,000 people came through the park’s visitor center in 1994. It dropped to the low 60,000s but has increased in recent years to just under 75,000, thanks in part to the opening of the Museum of the Confederacy’s satellite facility in 2012. Attendance is off this year, possibly because some visitors are delaying a trip until next year, when a full slate of events is scheduled, many coordinated with a local group called the Appomattox 1865 Foundation.\nRusty Small is on the foundation board. He is also lead pastor at Liberty Baptist Church, which sits at the corner of Church Street and Lee-Grant Avenue (not far from Booth Drive). His father came to Appomattox from North Carolina to open the Thomasville plant, and Small has watched the boom and bust from both a practical and theological perspective. It’s not that communities choose to die, he said, rather that “they choose not to live and take the steps to vibrancy.”\nSmall worries that the lack of infrastructure in place means his town won’t be able to fully capitalize on the opportunity at hand. “The park is going to be ready. The foundation is going to be ready. The community not so much,” he said. “I don’t even want to think about it. It hurts too much. I feel as though the 1865 story is our story, and it is our county’s identity. I can’t conceptualize what it means to live in a divided country. We think we live in a polarized nation, but there was a time when the U.S. was going to be a divided nation, and to think that last bit of that drama was played out here. It can’t be overstated.”\nA few months ago, the Appomattox Town Council pushed through a local excise tax on cigarettes, which could bring in $20,000. In 2013, it raised the meals tax to 8 percent, for an additional $240,000. Some proceeds are going to beautification. Another part is to keep property taxes low, and a third portion is to fund economic development, hopefully to build modest momentum to carry over after the big whoop-de-do.\nIn 1965, as many as 20,000 people were expected for the centennial, but only 5,000 showed up, and they listened in the soft rain to a handful of speeches by historians and watched a grandson of Grant and a great-grandson of Lee shake hands.\nAt the time, there was another surrender taking place across the South. The Civil Rights Act of 1964 was enacted. Public schools in neighboring Prince Edward County, which had closed to avoid integration, reopened under court order that September. Appomattox never closed its schools, and many black students from Prince Edward moved in with relatives here and continued their education at Carver-Price High School, which is now a small museum, next to an auto parts store along Confederate Boulevard. The museum has limited resources and an uneasy relationship with the Civil War tourism being pushed in many quarters with such vigor.\nI met Ora McCoy at the museum on a sweltering day in June, and she apologized for the air-conditioning being on the fritz. She graduated from the school in 1960, with the intention of becoming a beautician. But she then went to work at the post office, eventually retiring as postmaster of Appomattox. “We can’t get people to invest in our community,” she said. “It’s almost like we’re stuck in the Civil War.”\nAccording to the National Park Service, blacks account for less than one percent of visitors at Appomattox Court House, and many of the black residents of Appomattox with whom I spoke don’t care much for the national park that defines their community to the outside world. It wasn’t exactly their surrender, and the reunification that followed was often a cruel reminder of promises not kept. In addition, the focus on one event so far in the past can distract from the world as it is.\n“It’s important to keep in mind that the history didn’t stop there,” said Nicole Cabell, who graduated this year from Appomattox County High School and will attend Howard University this fall. “History is still being made here.”\nThe past can be an anchor and a millstone, and perhaps it’s unrealistic for the people of Appomattox to have a uniform opinion on the direction to move forward. But in the absence of consensus, a lot of lofty ideas get thrown around. They include developing a municipal water park, like the big one in Roanoke, and creating an outlet mall, like the shopping extravaganza near Colonial Williamsburg.\nPenny Searcy laughed at that second idea. She is president of Penelope Inc., which started here and has eight stores across Virginia that sell women’s jewelry and clothing. Its headquarters and distribution center are in Appomattox’s mostly vacant industrial park, but Searcy closed the store here four years ago. The inventory just moved quicker at other stores, where there is more traffic, particularly in the off-season for the park, she said.\n“We don’t matter to national retailers,” she said without a trace of anger or even resignation. That can be hard for people to accept, she said, but confronting the truth is an important part of community progress. She came to Appomattox nearly 30 years ago when her husband was transferred here with Thomasville, but her connection goes back to a great-grandfather, a Confederate soldier who surrendered at Appomattox Court House. “I’ve come full circle about living here,” she said. “I don’t see Appomattox as a dying town. We just keep on keeping on despite losing everything.”\nMany tourists initially confuse the old courthouse on Court Street in Appomattox for the one that it replaced more than 120 years ago, and a historical marker is out front to set them straight. It is a handsome brick building with white columns, formal yet approachable in the way we would like our justice to be. The interior looks much the same as it must have looked when court was held here, but the building is now the home of the Appomattox Courthouse Theatre.\nI stopped by one evening and took a seat on one of the benches in the back. The cast and crew were trickling in for a dress rehearsal of “The Hobbit,” which played to full houses over two weekends in June. Many of the young people were wolfing down dinner, clutching paper bags from Panera Bread and Sheetz and Cook-Out, totems of the world beyond Appomattox. There was a buzz of activity, fueled by caffeine, fries and a shared mission of something larger than any one person.\n“The Hobbit” is about a lot of things, but it starts with an essential act of courage, of going on a journey even when the final destination is uncertain. Valerie Daugherty, the director, said the themes — the safety of the shire, the sense of place and the need to confront our fears — are the threads that run through Appomattox. She loves the place and thinks it can thrive again if it embraces change. But she is also a realist. She moved back to Lynchburg two years ago, giving in, with some regret, to the convenience of a bigger city. “Of all the places I lived, I was the most proud to live here,” she said.\nKen Otterbourg lives in Winston-Salem, N.C., where he writes frequently about business and politics.\nE-mail us at firstname.lastname@example.org.\nFor more articles, as well as features such as Date Lab, Gene Weingarten and more, visit The Washington Post Magazine.\nFollow the Magazine on Twitter.\nLike us on Facebook.']	['<urn:uuid:2cbc44be-c050-41f4-af63-1cfdaadd5361>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	16	45	2623
56	I've heard that there's a government office in England that looks at healthcare costs - what's the range of money they consider reasonable to spend on improving someone's health for a year?	According to modelling studies, the value ranges from £10,000 to £70,000 per QALY (Quality Adjusted Life Year). Survey research methods mostly resulted in values between £18,000 and £40,000, although some methods produced much higher values.	"[""The social value of a QALY: raising the bar or barring the raise?\n1 Yunus Centre for Social Business & Health, Research Institutes, 3rd Floor Buchanan House, Glasgow Caledonian University, Cowcaddens Road Glasgow, G4 0BA, UK\n2 Institute of Health & Society, Newcastle University, UK\n3 Newcastle University Business School, UK\n4 Centre for Social and Economic Research on the Global Environment, University of East Anglia, UK\n5 Department of Economics, University of Warwick, UK\n6 Angela Robinson, Health, Policy and Practice, University of East Anglia, UK\n7 School of Economic and Social Studies, University of East Anglia, UK\n8 Department of Economics, University Pablo de Olavide, Sevilla, Spain\n9 Health Economics Research Unit, University of Aberdeen, UK\n10 Phil Shackley, Sheffield Vascular Institute, University of Sheffield, UK\n11 Richard Smith, London School of Hygiene and Tropical Medicine, UK\nBMC Health Services Research 2011, 11:8 doi:10.1186/1472-6963-11-8Published: 11 January 2011\nSince the inception of the National Institute for Health and Clinical Excellence (NICE) in England, there have been questions about the empirical basis for the cost-per-QALY threshold used by NICE and whether QALYs gained by different beneficiaries of health care should be weighted equally. The Social Value of a QALY (SVQ) project, reported in this paper, was commissioned to address these two questions. The results of SVQ were released during a time of considerable debate about the NICE threshold, and authors with differing perspectives have drawn on the SVQ results to support their cases. As these discussions continue, and given the selective use of results by those involved, it is important, therefore, not only to present a summary overview of SVQ, but also for those who conducted the research to contribute to the debate as to its implications for NICE.\nThe issue of the threshold was addressed in two ways: first, by combining, via a set of models, the current UK Value of a Prevented Fatality (used in transport policy) with data on fatality age, life expectancy and age-related quality of life; and, second, via a survey designed to test the feasibility of combining respondents' answers to willingness to pay and health state utility questions to arrive at values of a QALY. Modelling resulted in values of £10,000-£70,000 per QALY. Via survey research, most methods of aggregating the data resulted in values of a QALY of £18,000-£40,000, although others resulted in implausibly high values. An additional survey, addressing the issue of weighting QALYs, used two methods, one indicating that QALYs should not be weighted and the other that greater weight could be given to QALYs gained by some groups.\nAlthough we conducted only a feasibility study and a modelling exercise, neither present compelling evidence for moving the NICE threshold up or down. Some preliminary evidence would indicate it could be moved up for some types of QALY and down for others. While many members of the public appear to be open to the possibility of using somewhat different QALY weights for different groups of beneficiaries, we do not yet have any secure evidence base for introducing such a system.""]"	['<urn:uuid:85d499c0-4f07-499d-b1ee-09de3e318e1c>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	32	35	508
57	learning flower history why did tulips become expensive netherlands story	Tulips became extremely expensive in Holland during the 1630s due to a widespread mania. The craze began when tulips became a status symbol among the rich and famous in Holland, who purchased bulbs directly from Constantinople at extravagant prices. The mania soon spread to common people, middle class, and merchants. By 1634, the situation was so extreme that ordinary industry was neglected as everyone became involved in the tulip trade. Prices reached astronomical levels - in 1635, forty tulip bulbs cost 100,000 florins. People from all classes sold their properties and invested in tulips, leading to a massive economic bubble that eventually burst, causing widespread financial ruin in the Dutch economy.	['Lessons from Tulipomania in Sixteenth Century.\nQuis furor, ô cives!.\nWhat Fury? Citizens!Lucan\nToday’s story comes from a book titled, MEMOIRS OF EXTRAORDINARY POPULAR DELUSIONS AND THE Madness of Crowds. Written by CHARLES MACKAY, LL.D. in 1852.\nThe tulip was introduced into western Europe about the middle of the sixteenth century. The word Tulip is of Turkish origin, meaning a turban.\nConrad Gesner, first saw a beautiful Tulip in the year 1559, in a garden at Augsburg. The garden belonged to one Counsellor Herwart, who had a collection of such exotic flowers and plants. He had originally received his first Tulip bulbs from a friend at Constantinople.\nFrom 1559 to 1570, tulips were much sought after by the rich and famous in Holland and Germany. Rich people at Amsterdam purchased the bulbs directly from Constantinople, and paid the most extravagant prices for them.\nThe first Tulips were planted in England were brought from Vienna in the year 1600, and the popularity of Tulips kept increasing. If you had any taste, if you were anything – you had to have those Tulips. Tulips were the status symbol of the times.\nThe craze for the tulips soon caught the common man, the middle class, and merchants and shopkeepers. Every one wantedTulips, and were willing to pay the preposterous prices, not to sell it again for profit, but to keep it in their living rooms and offices to show off.\nOne would think that there must have been some magical powers in this flower to have made it so valuable in the eyes of rich, famous, educated and Europeans!\n“The tulip next appeared, all over gay, But wanton, full of pride, and full of play; The world can’t shew a dye but here has place; Nay, by new mixtures, she can change her face; Purple and gold are both beneath her care, The richest needlework she loves to wear; Her only study is to please the eye, And to outshine the rest in finery.” By Abraham Cowley\nIn 1634, the rage among the Dutch to possess Tulips was so great that the ordinary industry of the country was neglected, Every one was involved in the tulip trade. As the mania increased, prices increased, until, in the year 1635, one paid 100,000 florins to buy forty bulbs of Tulips.\nIn comparison, Here is a cost of other essentials of that time:\nFour fat Ox (Male Cows): 480 Florins\nEight fat Pigs: 240 Florins\nTwelve fat sheep: 120 Florins\nFour tuns of beer: 32 Florins\nTwo tuns of butter: 192 Florins\nOne thousand lbs. of cheese: 120 Florins\nA complete bed: 100 Florins\nA suit of clothes: 80 Florins\nA silver drinking-cup: 60 Florins\nAgain, 40 bulbs of Tulips cost Over 100,000 Florins!\nFlorins are coins from that era in the 12th to 16th century.\nPeople who had not lived in Holland, and were returning when this foolishness was at its maximum, were sometimes led into awkward positions by their ignorance.\nThere is this amusing story from Blainville’s Travels. One time this wealthy merchant, who showed off his rare tulips, received a very valuable consignment of merchandise from the Levant. The news of the shipment was brought to him by a sailor. The merchant, to reward him for his news, gave him a present of a fine red herring for his breakfast. The sailor apparently loved his onions, and seeing a bulb that looked very much like an onion lying upon the counter of this merchant’s office, thought – This onion bulb is, very much out of its place among silks and velvets! He slipped the bulb into his pocket, as a relish for his herring. Without thinking much of it, he proceeded to the quay to eat his breakfast.\nIt did not take too long for the merchant to realize his Tulip Bulb was missing. It was the most valuable kind, the Semper Augustus, worth three thousand florins at the time.\nThe whole office and the household of the merchant were instantly in an uproar; they searched everywhere for the precious Tulip Bulb, but it was nowhere to be found. At last, someone thought of the sailor. And the unhappy merchant took to the streets, his alarmed household followed him. There was a mob, now in search of the Sailor.\nThe ignorant sailor had no idea. He was found quietly sitting on a coil of ropes, eating the last bites of what he still enjoyed as the most unusually delicious “onion”. The merchant demanded the sailor pay back for his prized Tulip. At the time, the cost of his breakfast would have paid the salary for the entire crew of the ship for 12 months!!\nWhile his tulip was quite delicious with his red herring, the most unfortunate part of the business for him was, that he remained in prison for some months on a charge of a felony by the merchant.\nThere is a similar story of an English traveller, which I think is even funnier. This gentleman was an amateur botanist, He happened to see a tulip-root lying in the conservatory of a wealthy Dutchman. Having absolutely no idea how much a Tulip costs, he took out his penknife, and peeled off its coats, trying a few experiments to learn more about the “bulb”. He made many observations while continuing to peel this bulb, slashing it in half. And Suddenly, the owner pounced upon him, and, with fury in his eyes, asked him if he knew what he had been doing? “Peeling a most extraordinary onion,” replied the Botanist.\n“It is 4000 Florins.” said the Dutchman. The botanist had no idea, yet he was extremely remorseful. But the Dutchman was so angry, he would have none of the apologies. He dragged the Botanist in the streets by his collar and took him to the Magistrate, followed by a mob of persons. When brought into the presence of the magistrate, the Botanist was ordered to pay four thousand florins to the Dutchman; He was sent to prison until he found money for the payment of this amount.\nThe demand for tulips of a rare species increased so much in the year 1636, that regular stores for their sale were established on the Stock Exchange of Amsterdam, in Rotterdam, Harlaem, Leyden, Alkmar, Hoorn, and other towns. And as with the Stock Markets, Gambling began. The traders started dealing large sums of money in tulips, making use of all the means they so well knew how to employ, to cause fluctuations in prices.\nAt first, as in all these gambling mania, confidence was at its height, and every body made money. The tulip-traders speculated in the rise and fall of the tulip stocks, and made large profits by buying when prices fell, and selling out when they rose. Many individuals grew suddenly rich.\nEvery one imagined that the passion for tulips would last for ever, and that the wealthy from every part of the world would send to Holland, and pay whatever prices were asked for them.\nNobles, citizens, farmers, mechanics, sea-men, Professors, Scientists and People of all classes converted their properties into cash, and invested it in Tulips. Houses and lands were offered for sale at ruinously low prices, or assigned in payment of bargains made at the tulip-mart.\nForeigners became smitten with the same frenzy, and money poured into Holland from all directions. The prices of the necessaries of life rose again by degrees: houses and lands, horses and carriages, and luxuries of every sort, rose in value with them.\nAt last, however, the bubble burst. Someone began to see that this folly could not last forever. Rich people no longer bought the flowers to keep them in their gardens, and offices but to sell them again at profits. It was seen that somebody must lose fearfully in the end. As this conviction spread, prices fell, and never rose again. Confidence was destroyed, and a universal panic seized upon the dealers.\nA had agreed to purchase ten Semper Augustines from B, at four thousand florins each, at six weeks after the signing of the contract.\nB was ready with the flowers at the appointed time; but the price had fallen to three or four hundred florins, and A refused either to pay the difference or receive the tulips.\nDefaulters were announced day after day in all the towns of Holland.\nHundreds and Thousands of people suddenly found themselves in possessions of a few bulbs, which nobody would buy, even though they offered them at half the prices they had paid for them.\nMany who, thought they became rich for the short term by owning the Tulip, were suddenly cast back into their original poverty.\nRich became poorer and people from all walks of life saw their fortunes of his house ruined, beyond redemption. The Dutch economy was in shambles for many years to come.\nWhen the first alarm subsided, the tulip-holders in the several towns held public meetings to figure out a solution. It was generally agreed, that perhaps the government could rescue them. The government at first refused to interfere but advised the tulip-holders to agree to some plan among themselves.\nSeveral meetings were held for this purpose, but no one could convince the people. At last, the government agreed that all contracts made in the height of the tulip mania, or prior to the month of November 1636, should be declared null and void, and that, in those made after that date, purchasers should be freed from their engagements, on paying ten percent to the vendor.\nThis decision did not help the vendors who had their tulips on hand. Tulips which had, at one time, been worth six thousand florins, were now to be procured for five hundred; so that the composition of ten percent was one hundred florins more than the actual value. Actions for breach of contract were threatened in all the courts of the country.\nThe matter was finally sent to the Provincial Council at the Hague, thinking that they would invent some measure by which credit should be restored. The members of the council continued to deliberate week after week, and at last, after thinking about it for three months, declared that they could offer no final “helpful” decision!!\nThey advised, however, that, in the mean time, every vendor should, in the presence of witnesses, offer the tulips to the purchaser for the sums agreed upon. If the purchasor refused to take them, they might be put up for sale by public auction, and the original contractor held responsible for the difference between the actual and the stipulated price.\nThere was no court in Holland that would enforce payment. The question was raised in Amsterdam, but the judges unanimously refused to interfere, on the ground that debts contracted in gambling were no debts in law.\nThe Dutch economy took a long time to recover.\nThe example of the Dutch was imitated in England. In the year 1636 tulips were publicly sold in the Exchange of London, and the Traders tried their best to raise the Prices to the fictitious value, they had acquired in Amsterdam. In Paris also the traders tried to create a tulipomania. In both cities, they only partially succeeded.\n“Shorting” in stock market as explained by Wimpy in Popeye.\n“I’d gladly pay you Tuesday for a hamburger today”Wimpy. From Popeye.\nSo Indeed, Wimpy cannot get this hamburger. But you sure can borrow a stock and sell it Short.\nJerry Seinfeld Experiment: Don’t break the chain. Post #32.']	['<urn:uuid:aa478075-ad55-4a97-bee2-bccd26f30474>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	10	111	1923
58	As someone studying migration patterns, I'm curious about how the early development of soccer in England relates to European immigration to America - what were the parallel timelines and movements of people during the 1800s?	In the 1800s, while soccer was taking shape in England with the creation of the Football Association in 1863 and the standardization of rules at the Freemason's Tavern, European immigration to America was also experiencing significant changes. The period saw large waves of European immigrants, particularly from the 1880s through 1924, with the largest numbers arriving between 1900-1914. While soccer was becoming a popular form of entertainment for the British working class, immigrants from various European countries were moving to America's industrial areas, with Poles, Jews, Italians, and other groups providing the workforce for American industry. The sport itself would later follow these migration patterns, arriving in American industrial centers in the 1920s through immigrant workers in mills, plants, and factories.	['THE SHORT VERSION:\nWhile the exact history of soccer is ambiguous, many believe:\n- The game traces its origins back to rudimentary ball games played in China and Rome.\n- Soccer as it’s known today developed over centuries in England.\n- Freemason’s Pub in where many of the modern rules were laid down.\nFootball, or better known as soccer in America, is the world’s most popular sport with over 4 billion fans. But what’s the story behind this game? Who invented it? Where did it originate? And how did it find its way to the United States?\nIts history is muddy and while some details are up for debate, below is a brief overview of soccer’s history.\nAlthough a far cry from today’s version of the game, some historians believe early games played with a ball occurred over 3,000 years ago in Mesoamerican culture.\nChina had a rudimentary game called Cuju that involved kicking a ball on a square surface through goals made from bamboo back in the second and third centuries. Also, the ancient Greeks had ball games as did the Romans, and some allege the Romans brought these games to the British island.\nWhile the exact history of how soccer as we know it got started, historians think the game developed in England in the 12th century. Back then, the game was played in the streets and players could use their hands and their feet to move the ball. Historians say these early games were so violent, players sometimes died during competition, and eventually the game was banned by King Edward III in 1365 and later by King James I in 1424 and it remained so for several centuries.\nIn 1863, in London, the first Football association came into being and during the meeting, they established rules such as the rule against using hands to advance the ball, and the rules regarding the size and weight of the ball. Also, during this meeting, the game was divided into clear delineations: association football and rugby, which had different rules such as allowing for the kicking of shins, tripping and carrying the ball.\nEven though the game was taking shape, it still didn’t resemble what we see today. The game continued to evolve with new rules added and changed, such as the number of players allowed in the field during play. During this evolution, the idea of using your head became part of the game.\nAs the game continued growing, it became a popular form of entertainment for the British working class and would draw huge crowds to see big matches. And, while football clubs existed, they were unorganized and it’s difficult to tell which was the official first football club. However, historians believe the Football Club formed in 1824 in Edinburgh was the first official club.\nIt wasn’t until October 1863 that what we call modern soccer was formed. Eleven representatives from the football clubs and schools that played the sport met at the Freemason’s Tavern to lay down standard fundamental rules, and the result was the creation of the Football Association. It was in December of that same year that Rugby Football and Association football went their separate ways marking a clear separation between rugby and soccer. Then in 1869, the Football Association put a stop to players using their hands to advance the soccer ball and the game grew more popular especially with soldiers and sailors who helped spread the sport around the world.\nSoccer in the United States\nLike soccer in Europe, the origins of the game here in the US are fuzzy. Some believe the game came to the US via Ellis Island in the 1870s, however, other historians say the game came to America back in the 1850s in New Orleans when Scottish, German, Irish, and Italian immigrants settled there.\nHowever, it wasn’t until the 1920s that the game truly ‘arrived.’ Back then, there was a boom in manufacturing in the United States and with plenty of immigrant workers ready to go to work in the mills, plants, and factories it was only natural they brought their favorite game with them. Then, in 1921 the American Soccer League (ASL) was formed with the founding teams coming from blue collar places like Bethlehem, Pa. and Fall River, Mass.\nAlthough the sport grew, infighting caused the collapse of the ASL and the game lost popularity for about 40 years.\nThen, in 1967, a group of entrepreneurs started the National Professional Soccer League (NPSL) hoping to revitalize the sport, but they failed and the group disbanded after only one season.\nIn 1975, Pelé - who was considered the greatest player of all time - signed to play with the New York Cosmos, which was a team owned by Warner Communications. It was his signing that catapulted soccer back into the spotlight in the US and drove it into the mainstream. And while interest in soccer fell after Pelé’s retirement in 1977 and as the economy tanked, it would come back about 10 years later when FIFA, the world’s governing body of the sport, brought the World Cup competition to the US.\nAfter the World Cup, soccer continued to grow in popularity and Major League Soccer was formed, which endures to this day.', 'Although the territory of the United States was originally settled in ancient times by the Asian ancestors of modern Native Americans, European immigrants of the seventeenth through early twentieth centuries dominated the landscape and brought with them the culture and institutions to which other modern immigrants have had to adapt.\nEuropean immigration to the New World of the Western Hemisphere had its origins in the Age of Exploration that began with Spanish and Portuguese voyages of discovery in the fifteenth and sixteenth centuries. The creation of European colonies in the Americas, as expressions of political power and as business opportunities, stimulated both forced and free migration from Europe. European immigration has been almost constant since the early seventeenth century, but it has waxed and waned with changing economic, social, demographic, and political conditions on both sides of the Atlantic Ocean.\nTransatlantic migration can be seen as an extension of long-standing patterns of movement within Europe that stretch back to the Middle Ages. Due to better technology, improved farming practices, and a warming of the climate, medieval populations expanded, putting pressure on existing arable land. Encouraged by rulers, nobles, and kings, who would often remit certain feudal duties, peasant populations migrated to virgin lands. This occurred within the core regions of western Europe, but there were significant movements of population from Germany and Flanders into less populated areas of central and eastern Europe. Due to persecutions that stemmed from the onset of the bubonic plague during the mid-fourteenth century, Jewish populations migrated to Poland and Lithuania, where they received improved treatment and a measure of religious freedom.\nAs urban areas across the continent grew during the early modern period, they increasingly drew populations from the countryside. During the early modern period populations displaced by war or religious persecution also migrated throughout Europe. These included\nMigration within Europe was a necessary precursor to transatlantic migration. Studies of immigrants from the colonial period onward have indicated that a majority of individual European immigrants had some previous migration experience, either regionally or within Europe, prior to coming to North America. In a study of the British colonies in North America after the Seven Years’ War (also known as the\nInternal migrations within Europe increased the likelihood of individuals making longer and more permanent journeys for several reasons. First, it gave them access to new economic opportunities and altered their economic worldviews. Most local peasant and subsistence economies in Europe prior to migration were perceived as zero-sum games in which those who attained greater material wealth did so only at the expense of their neighbors. Migration changed this view and opened up the possibility of expanding one’s material universe and realizing economic possibilities that were previously unattainable.\nBy the mid-eighteenth century, the growth of European industrialization drew many out of the countryside and into factories, mills, and mines. This process affected western Europe most directly, but its indirect effects were felt throughout the Continent. By the nineteenth century, industrialization was evident throughout central Europe and even in Russia and the Balkans by the end of the century. This movement drew large numbers of peasants out of rural villages and into cities, but the new industrial jobs provided by this economic change could not keep pace with the expanding size of the rural population or with the number being displaced from the land. Rural populations continued to grow throughout the nineteenth century because of the cessation of major war, the introduction of new crops such as the potato, and improved health and sanitary conditions. This put additional land pressure on the rural populations, something that was exacerbated in some areas by inheritance patterns in which land was divided evenly among peasants’ heirs.\nIn central and eastern Europe, the movement of peasants was kept in check throughout the seventeenth century by quasi-feudal laws that bound the peasants to the land. Throughout the nineteenth century, however, these laws were gradually done away with in an effort to modernize agriculture. The peasants were emancipated in Prussia in 1807, Austria-Hungary in 1848, Russia in 1863, Romania in 1864, and the Balkans after 1878 as\nThe usual method of peasant emancipation was to convert labor duties into cash rents and–much like the earlier enclosure movement in England–to restrict peasant access to pastures, woodlands, or other resources once used in common. As one Polish scholar put it, “peasant emancipation took the shackles off the peasants’ feet–and took the shoes as well.” The result was a sudden need for money in village economies where cash had rarely been used. This impelled peasants to migrate in search of work, and as they did so they found not only the ability to pay rents but the possibility of bettering their economic status.\nNorth America’s abundance of resources and its relatively smaller and less densely concentrated population began attracting immigrants during the early seventeenth century. By the time of the American Revolution (1775-1783), the average American had more personal freedom and a better standard of living than counterparts in Europe, even in the better-off countries of western Europe. Throughout its history, average wages in the United States have always been higher than in Europe. Moreover, due to Indian removal policies and westward expansion during the nineteenth century, the United States offered an abundance of farm and grazing land that was both relatively inexpensive and highly productive.\nAmerica attracted three main types of immigrants. The first are “settler immigrants,” who come with the intention of settling permanently in the New World. They usually bring all or most of their immediate and extended family members and thus cut their strongest ties to their home villages. Historically this pattern was often associated with those who came to America with the specific intention of taking up farms. Bringing additional family members was beneficial as an additional source of farm labor. The majority of settler immigrants from Europe during the nineteenth and early twentieth centuries were from northern and western Europe.\nLabor seekers are the second type of immigrants–those who come to find jobs that pay good wages. Labor-seeking immigrants have made up and continue to make up the largest numbers of immigrants to the United States. Typical labor-seeking immigrants are men aged between sixteen and forty-five who come for unskilled or semiskilled work. A significant number of\nThe first significant European immigration to the New World came from the\nBetween the Seven Years’ War and the American Revolution, immigration to the British colonies grew dramatically, rising to approximately 15,000 per year. Germans and Swiss made up the largest single group, numbering about 125,000, followed by Protestant Irish (55,000),\nDuring the Revolutionary War and in the decades of economic readjustment and wars in Europe that followed American Independence, immigration decreased dramatically, especially from its traditional sources in the British Isles, though some German immigrants continued to arrive. During the conflict, a significant number of Europeans with military experience arrived to provide critical assistance to the American colonists, with French, Germans, Poles, and Hungarians the most prominent among them.\nImmigration began to increase once again during the 1820’s in response to the end of the\nEuropean immigrants arriving at Ellis Island in 1902. Located in New York Harbor, Ellis Island was the primary port of entry for European immigrants between 1892 and 1954.\nAmong immigrants arriving before the Civil War, three groups predominated:\nFollowing the Civil War, Germans and Irish continued to arrive in large numbers, but new nationalities also began to appear on American shores as well: Norwegians, Swedes, Danes, Czechs, Hungarians, and Poles. Immigration after the war represented the last great wave of settler immigrants who arrived to look for farms in the Midwest and Great Plains. Thereafter, good land became increasingly difficult to acquire, though agricultural colonization continued in the arid lands of the west and cut-over regions of the Great Lakes.\nBeginning in the 1880’s and continuing through the passage of restrictive immigration laws in 1924, the largest wave of immigration in history arrived on North America’s shores. The largest number of arrivals came in the period from 1900 to 1914. Arrivals fell off sharply during World War I. The largest number of immigrants came in 1907, when approximately 1.3 million arrived during that year alone.\nAlthough immigrants continued to arrive from western Europe and Scandinavia, this wave of immigration was dominated by east-central and southern European. Beginning in the eastern marches of the German Empire, “immigration fever” spread eastward into Austria-Hungary, Romania, and the western regions of Russia. Italy also sent massive numbers of immigrants, and while many came from northern Italy, southern Italians and Sicilians dominated Italian arrivals. From east-central Europe, Poles were the largest single group, arriving from the German, Russian, and Austrian empire. Jews were a close second–although many came from Austria-Hungary, Germany, and Romania, Russian Jews formed the largest contingent. A host of smaller groups also came–Hungarians, Lithuanians, Ukrainians, Carpatho-Rusins, Slovaks, Czechs, Romanians, Slovenes, Croatians, Serbs, Macedonians, Bulgarians, and Greeks.\nIn contrast to earlier waves of immigrants, the Europeans who came between 1880 and 1924 were predominantly labor-seeking immigrants. However, some did come within family units and some did settle on farms. It was primarily industrial work that drew them to the United States, and they settled in the areas of heaviest industrial activity–New York, New Jersey, Pennsylvania, and the Midwest and Great Lakes states.\nWage labor immigrants from east-central and southern Europe provided the workforce for America’s industry, and by the turn of the century dominated both heavy and light industry in most sectors. Jews and Italians were prominent in the needle trades. Poles, Italians, Slovaks,\nBetween World War II and the reform of U.S. immigration laws in 1965, the United States admitted between two and three million European immigrants. Many were political refugees, with Jewish\nFollowing the major reform of U.S. immigration laws in 1965, a steady flow of immigration from Europe developed. Family reunifications, the need for work, political oppression, and the collapse of communism during the 1980’s and 1990’s have been some of the major factors in this continuing stream. Some traditional sending countries continued to provide large numbers of immigrants.\nFollowing the fall of Eastern Europe’s communist governments during the last decade of the twentieth century and the wars and ethnic cleansing in the former Yugoslavia, a large number of Russians, Jews,\nEuropean immigration has continued into the first decade of the twenty-first century, when Europeans were still making up between 15 and 20 percent of the immigrants admitted to the United States. This pattern appeared likely to continue into the foreseeable future.\nBailyn, Bernard. The Peopling of British North America: An Introduction. New York: Vintage, 1986. Useful study of early British immigration to North America. Daniels, Roger. Coming to America: A History of Immigration and Ethnicity in American Life. Princeton, N.J.: Visual Education Corporation, 1990. Comprehensive survey of the major immigrant groups in the United States, emphasizing numbers of immigrants, their settlement patterns, and socioeconomic issues. Erickson, Charlotte. American Industry and the European Immigrant, 1860-1885. Cambridge, Mass.: Harvard University Press, 1957. Excellent study of the employment of European immigrants during the Civil War and postwar eras. _______. Invincible Immigrants: The Adaptation of English and Scottish Immigrants in Nineteenth Century America. Leicester, England: Leicester University Press, 1972. Important survey of the second peak period of British immigration, with useful data and appendixes. Greene, Victor R. A Singing Ambivalence: American Immigrants Between Old World and New, 1830-1930. Kent, Ohio: Kent State University Press, 2004. Comparative study of the different challenges faced by members of eight major immigrant groups–Irish, Germans, Scandinavians and Finns, eastern European Jews, Italians, Poles and Hungarians, Chinese, and Mexicans–through one of the longest peak periods of immigration. Meltzer, Milton. Bound for America: The Story of the European Immigrants. New York: Benchmark Books, 2001. Very readable history of European immigration to the United States, written for young-adult readers.\nCzech and Slovakian immigrants\nEuropean revolutions of 1848\nFormer Soviet Union immigrants\nPilgrim and Puritan immigrants\nRussian and Soviet immigrants\nYugoslav state immigrants']	['<urn:uuid:ad70ddea-045c-4036-83bd-65242d360cff>', '<urn:uuid:73191b61-16d9-45d5-bbde-3e6b36d139be>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:18:19.158519	35	121	2869
59	Why are some old power plants getting paid to stay open?	Some power plants receive guaranteed payments (called 'reliability must run' payments or RMRs) when grid operators determine that closing the plant would affect grid reliability and that existing solutions like upgrades and demand response aren't enough. These payments, funded by consumers, ensure the plants continue operating until other solutions can be implemented.	['The Sustainable FERC Project’s priority is to accelerate the deployment of clean energy resources into America’s electricity transmission grid. Part of our priority is ensuring that older, dirtier and costlier power plants – nearly all of which are powered by coal and oil –do not block the entry of these new resources onto the grid.\nHundreds of these power plants are slated to close in the next several years because of lower energy demand, cheaper natural gas, and new U.S. Environmental Protection Agency (EPA) standards for mercury and other toxic air pollutants. Most of these plants are located in the mid-Atlantic, Midwest and Southeast.\nAs owners announce plant retirements, grid planners at Regional Transmission Organizations (RTOs) work through the process of evaluating whether the closures will affect grid reliability. Sometimes the plants can close with virtually no impact on grid reliability – especially since most regions of the country have had a healthy surplus of power capacity. In other cases the RTOs may need to accelerate transmission line upgrades or take other steps to make up for the retiring plants, such as moving power into areas of potential shortfall or activating demand response (customer reductions in load) to reduce energy demand on hot days.\nHowever, sometimes an RTO or utility concludes that accelerating upgrades and existing demand response aren’t enough and that a power plant can’t close until after other solutions are implemented. In these cases, the owners will receive guaranteed payments—ultimately funded by consumers—to continue plant operation. In the world of transmission system planning, these payments are called “reliability must run” payments, or “RMRs” for short.\nAlthough much of the work related to reliability around power plant retirement takes place at the local and regional level, the Federal Energy Regulatory Commission (FERC) impacts determinations about reliability and power plant retirement in a few different ways. It regulates the technical rules RTOs and utilities use to plan for power plant retirements. FERC also reviews proposed RMR contracts. Finally, FERC advises the U.S. Environmental Protection Agency on reliability determinations that provide the basis for power plant owners to request additional time for complying with EPA’s mercury and air toxics standards (MATS) rule.\nPower Plant Retirement Impacts Analysis and Reasonableness of RMR Payments\nThe Sustainable FERC Project has two concerns when an owner announces a power plant retirement and the RTO assesses the impacts of the plant’s closure on grid reliability. The first concern is to make sure the RTO/utility has evaluated the grid impacts of the planned retirement accurately and reasonably. Doing so involves delving deeply into the “black box” of transmission planning, which we and our allies navigate with the help of transmission experts. The second area of concern arises after the RTO/utility has decided that an RMR contract is necessary for some period of time. RMR payments, which can total tens of millions of dollars annually, can be economic lifelines for plant owners, especially during periods when the plants otherwise can’t compete in the energy markets. In some cases, the owners end up making more money than before the “reliability need” determination.\nAlthough sometimes justified, these payments can create several problems. They virtually assure more pollution than otherwise would exist if they closed and they limit opportunities for cleaner generation and energy efficiency to compete. To minimize the costs of RMR payments, the Sustainable FERC Project and our allies examine them in detail and determine if either the RTO or the power plant owner is proposing to include improper or unnecessary costs in the scope of the contract.\nThe FERC Project’s Power Plant Retirement/Reliability Advocacy\nThe Sustainable FERC Project and its allies are engaged on these issues at the Federal Energy Regulatory Commission (FERC), in coal-heavy regions such as the RTOs Midcontinent Independent System Operator, Inc. (MISO) and PJM Interconnection LLC (PJM), and in the southeast at the Southeast Regional Transmission Planning Process. For example, we ask FERC to carefully assess all power plant requests for extensions of time to comply with the U.S. EPA mercury and air toxics standards. We also work with partners to propose demand response or energy efficiency that may avoid or reduce the amount of time necessary for the plants to operate past their desired retirement dates. Finally, we work to improve RTO rules governing power plant retirements, by including more public access to information, requiring early notice of plant retirements and reasonable, consumer-friendly RMR payment terms, and consideration of alternatives to RMR payments.\nThe Project doesn’t oppose all payments to coal plants that continue to operate for reliability reasons. But we want to be sure that grid operators and FERC fully consider all reasonable and reliable alternative solutions to continued operation, including cleaner efficiency, demand response and generation. To do otherwise would, in the words of the Federal Power Act, be unjust, unreasonable and discriminatory, and it will create more pollution.']	['<urn:uuid:856c513f-8dfc-4a3d-a92c-e94217601477>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	11	52	808
60	lossless compression data archiving vs lossy compression multimedia implications	Lossless compression maps input to smaller output while preserving all original information, making it essential for data archiving and backup where exact 1:1 copies are needed, used in formats like 7Z, RAR, and ZIP. In contrast, lossy compression removes less relevant information to achieve better compression ratios, making it suitable for multimedia like MP3 audio and JPEG images, but causing permanent information loss that accumulates with multiple save cycles.	"['For example, an audio CD can contain approx 72 minutes of uncompressed hi-quality stereo sound and any CD player or computer system can replay it, but because of speed limitations, CD quality audio is normally converted into the lower quality and smaller file size MP3 format before it is delivered over the internet.\nAnother example is a DVD video disc which usually contains compressed quality pictures and sound and requires specialist compression software (and often hardware) to replay.\nWhat is optimising?\nOptimising is the process of preparing a source/""raw"" media file (typically video or audio) for playback on a target system, and usually involves converting it into a smaller (and usually lower quality) format. Optimising is normally required for 2 primary reasons ...\n- To reduce a file size so that it will take up less space on a storage device (such as DVD disc or iPod)\n- To reduce its file size/data rate so that it will transfer faster over a network such as the internet\nThe process of optimising is often called ...\n- optimising (the complete process of preparing a file for a delivery technology, and which may include encoding and compression)\n- encoding (the process of converting one data file file format into another and which usually involves compression)\n- compressing (the process of using a codec to reduce the data rate/file size of a file)\nIncidentally, the process of uncompressing a file for playback can be called ...\nTranscoding is the digital to digital conversion of one file format to another without involving an optimising (compression etc) process. Transcoding is often carried out in order to convert a file from an obsolete format to a current one. Transcoding may incur a quality loss.\nWhat is a codec?\n""Codec"" is short for compression-decompression algorithm. It is a piece of software which is employed to compress (reduce in size) a media file (most commonly video, audio or animation) and then at a later time decompress it to allow it to be played or viewed. This process is useful for 2 primary reasons ...\n- To allow more data to be stored on a given storage device (such as a hard drive, memory stick or optical disc etc).\n- To increase the speed of transmission across a network (such as the internet).\nThe term ""codec"" is usually used in reference to audio and video media files, but still-image compression methodologies, such as jpeg, can also be called codecs.\nLossy and lossless\nA codec may be lossy or lossless.\n- Lossless. This is a process whereby clever algorithms (such as Flac) are employed to ""compact"" the data in a file without losing any information. Lossless compression can reduce the file size but not as much as lossy.\n- Lossy. These codecs find ways to throw away information which is considered ""less-essential"". File size reduction can be dramatic (10 times smaller) but the effect on quality is usually noticeable.\nHard and soft codecs\nA ""hard(ware) codec"" is integrated into the electronics of a hardware processor/device such as a DVD player, camcorder, MP3 player or video capture card.\nA ""soft(ware) codec"" is usually installed onto a computing system (usually into the operating system folder) and runs on the computers CPU. For example, QuickTime mostly comprises a collection of soft codecs which run on a PCs CPU and which can be employed by any application to decompress, edit and replay files. Applications which employ soft codecs include, browsers, video editing, and music making applications.\nThere are many many codecs out there and new and improved ones are being developed all the time. Examples codecs include ...\n- JPeg (images)\n- MPeg3 (audio)\n- MPeg 2 (broadcast digital TV)\n- H264 (Blu-Ray video)\nThe distinction between file formats and codecs can be tricky to understand. We have already established what a codec is.\n(Media player) File formats\nFile formats are usually associated with Media Player Software such as QuickTime, DivX or Windows Media Player. Media Player software usually comprises 3 elements ...\n- a collection of codecs, which are usually stored in the operating system folder\n- a browser plug-in interface\n- a stand-alone player interface or utility/application.\nFor example, if you have a media file with the .mov extension, then it has been compressed with a codec that QuickTime has, and saved in the QuickTime file format. The same applies for Windows Media with its .wmv file format. The .avi file format is the old Windows format.\nIf a file has been saved in a proprietary media player file format it can only be played by that media player. It is not always apparent from the file format which codec is being used, only that it can replayed by a particular media player or plug-in.\nHow do I know what codec is being used if a media file is in a player file format?\nIf you encounter a video file with the .wmv (Windows Media) or .mov (QuickTime) extension you won\'t necessarily know what codec its been compressed with but you will know (version permitting) which player software can replay it.\nOpen the file in the player and choose the option to look at information about the file.', 'is a free cross-platform file compression utility that provides an\nportable GUI for many Open Source technologies like 7-Zip, FreeArc,\nPAQ, UPX... free alternative to WinRar, WinZip and\n- Create 7Z, ARC, BZ2, GZ, *PAQ, PEA, QUAD/BALZ, TAR, UPX, WIM, XZ, ZIP\n- Open and extract ACE, ARJ, CAB, DMG, ISO, LHA,\nRAR, UDF, ZIPX files and more, over 200 archive types\nFeatures of PeaZip includes: archives\nopener and extractor, batch creation and extraction\narchives at once, convert files, create\nself-extracting archives, split and join files,\nstrong encryption with two factor authentication, encrypted password\nmanager, secure deletion, find\nduplicate files, calculate hash and checksum,\nexport job definition as script.\nLossy and lossless compression\nWhat is file compression,\ntypes of algorithms\nFile compression is the process of\ncreating a smaller output from a larger input, in order to sparing\nstorage space, or saving bandwidth to share the data, or reduce the\ninput to a suitable size for a media of distribution (CD, DVD, mail\nattachment size limit...).\nHow lossy and lossless compression works\nData compression can be defined lossy\nin terms of\nreversibility of the compression process due loss (or preservation) of\noriginal information in the process. The two types of algorithms have\ndifferent pros and cons, and different field of application.\nLossless compression definition, file archiving\nstatistical models to map the input to a smaller\noutput eliminating redundancy in the data.\nIn this way the output carry\nexactly all the information featured by the input in less bytes, and\ncan be expanded when needed to a 1:1 copy of the original data\n(restoring exactly the original content), which\nis a fundamental property for storing some types of data - i.e. a\nsoftware, a database.\nFor this reason\nlossless compression algorithms are used for data backup and for archive\nused in general\npurpose archive manager utilities, like 7Z, RAR,\nand ZIP, where an\nexact and reversible image\nof the original data must be saved.\nExamples of lossless compression algorithms are Deflate (used i.e. for\nZIP and GZ formats), BZip2 (used in BZ2 format), PPMd (RAR, 7Z\nformats), LZMA / LZMA2 (7Z / XZ format).\nSome graphic file fomats (notably, PNG files and deflated TIFF) uses\nlossless compression, which usually results in less compression but no\nimage quality degradation after multiple cycles of modification and\nsaving of the picture, making this kind of image format suitable as\nintermediate save files for image editing tools.\n|Lossless compression is fully invertible,\nas 1:1 copy of original content input is stored in the smaller,\nefficiently encoded output, so it is usually suitable for backup, file\narchiving and other applications where any loss of information is not\nLossy compression definition, multimedia data\nor less relevant information (not just redundant data) and removing it.\nUnlike the lossless compression, the amount of information to compress\nis effectively reduced.\nThe loss of information / content is irreversible, and depending from\nthe nature of the algorithm, will likely happen each time the content\nis modified and saved to a lossy file format - e.g. when editing a\nlossy jpeg images, and saving it multiple times to intermediate work\nIn this way data\ncompression ratio is improved but at the cost of making lossy\nnon reversible process - as it comes at the cost of losing part of the\ninformation - and making it a suitable choice only when it is not\nintended, by design, to restore the original content again.\nis consequently not suitable for general purpose file\n(as in example losing a single byte of an executable file would make it\nnot working), but it works very well when loss, reducing less\ninformation, is acceptable, as for graphic and multimedia files\n- in example for MP3\nlosing audio information below the audibility threshold, or losing not\nvisible details in JPEG\nimages, or both in compressed video formats such as MPEG (AVI, MKV, MPG, MP4...).\nMost common lossy compression algorithms are consequently usually fine\ntuned for the specific pattern of a multimedia data type.\nloss is destructive for the ability of 1:1 reversal of\nthe algorithm (the information is permanently lost), but it is not\nprejudicial for the ability of end users to receive meaningful\ninformation - intelligible audio, clear picture or video.\nDue the lossy nature of those compression schemes, however, usually\nprofessional editing work is performed on non compressed data (i.e. WAV\naudio, or TIFF images) or data compressed in a lossless way (i.e. FLAC\naudio, or PNG images) every time it is feasible so saving the work in\nprogress multiple times does not result in losing bits of the\ninformation each time, with progressive degradation of quality -\nreserving use of lossy compression to final step for creating a\nreasonably sized output to distribute for media consumption.\ncompression and lossy\ndefinitions on Wikipedia.\n> Archiving > What are lossy & lossless compression\nTopics and search suggestions about how lossy compression and lossless\ncompression works, what are reversible and non reversible algorithms,\ninvertible file compressio / archiving and non-invertible destructive\nmultimedia compression, compare definitions and compressed file types:\n| what is\nclassification of compression algorithms,\nloss of information,\nfile compression utility,\n| what is\nlossy / lossless differences,\nnon invertible algorithms,\nhow file compression works,\napplication of lossless compression,\nand disadvantages of\nlossy & lossless compression,\ntypes of compression strategies,\nwhat are invertible functions,\nwhat is file compression,\nfile compression applications,\napplications of lossy compression,\nfile compression software,\ncompression algorithm families,\nclassification of compression']"	['<urn:uuid:eb880f4c-33e0-47c2-81a6-542211ea0949>', '<urn:uuid:3c850513-5841-463f-943c-2277e5733572>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:18:19.158519	9	69	1772
61	I'm planning to make snow candles for the first time - what supplies do I need for snow candle making, and what fire safety measures should I have in place at home?	For snow candles, you need snow, wax, dye, scent, wire core wicking, a spoon, an ice pick or wire, and a bucket for packing snow. You'll also need basic candle-making supplies and a thermometer to heat wax to 160°F. For fire safety, ensure your home has working smoke detectors with regularly tested batteries (recommended to change twice yearly with Daylight Savings Time), and if you use fossil fuel heating, install carbon monoxide detectors. Keep a fire screen nearby if using near a fireplace, and never leave candles burning unattended.	"[""Snow Molded Candles\nBy Bob Sherman\nThis article was originally written in 1998 and has been rewritten, modernized, and modified for this web site.\nSnow candles are a great way to get free form shapes, as no two are alike. They are very similar to sand candles, but with a more irregular form because the snow melts as the hot wax is poured in. Oddly enough I can't find a good photo of a snow candle at the moment. This photo shows a snow molded candle but because of not packing the snow tightly enough it has a more freeform shape and looks like a lava flow.\nPLEASE NOTE! - Candle making can be dangerous if proper safety procedures are not followed. Please read these Safety Rules before attempting any candle making projects.\nThe most important thing needed for snow candles is lots of snow. The up side of this is that snow is free. The down side is that if you don't get snow where you live, they are pretty much impossible to make. This photo shows ideal conditions for making snow candles. A spoon, ice pick (or wire), and the basic candle making products - wax, dye, scent, etc... will be also be needed.\nThe mold is made from snow. For best results it should be tightly packed, and near where you will melt your wax. Use your hand, stick, cup, or other object to create a cavity in the snow. Allow six inches of snow below the mold cavity to remain. This step is similar to making sand candles, and the shape of the finished candle will depend on the shape of the mold cavity.\nAlthough not as much fun as pouring directly into a snow drift, this works best when the snow is packed into a bucket as it allows you to pack it tighter.\nGenerally any pillar candle formula should work well. My preferred wax formula for these is:\nCut some wire core wicking. It will need to be at least a few inches longer than the depth of the mold cavity you have made in the snow.\nPoke an ice pick or wire into the snow to make the wick hole. Position one end of the wick into this hole, making sure it extends two to three inches below the mold cavity. Position a dowel or stick across the top of the cavity and wrap the wick around it to hold the wick in place.\nHeat the wax to 160 degrees F. Add dye and scent as desired.\nHold the spoon into the mold cavity. Slowly pour the wax onto the spoon, allowing it to splatter as it is poured. This will prevent the hot wax from boring a hole through the bottom of the mold cavity. Allow to cool.\nRelief holes should be poked near the wick as the candle cools, and one or more repours will be needed to fill the shrink void.\nOnce the candle is fully cooled, remove it from the snow. Trim the wick and level the base.\nVarying the pouring temperature will vary the irregularity of the finished candle. The cooler your pouring temperature the closer the finished candle will match the original shape of the mold cavity. Pouring at hotter temperatures will cause a wilder look.\nIf your snow is very fluffy or you only have a little snow fill a bucket with snow to increase the depth you have to work with. This bucket technique is also handy if temperatures are too low to work outside comfortably, since you can bring it indoors.\nThese candles are fun and easy. No two come out alike, and you are limited only by your imagination on the basic shapes.\nSupport Free Projects\nYou can help ensure the continued availability and production of free candle making projects and instructions by telling your friends about them. The more popular they are the more we can produce so tell your friends, mention them on message boards, link to them from your web site, etc... More information is available here.\nDisclaimer: The information presented here is accurate to the best of my knowledge and common candle making practices as of the time of this writing Originally published in the late February 1998 and updated in October 2007 and July 2011. The author and the publisher accept no liability for the use or misuse of any of the information presented in this article. This article is presented for informational purposes and is used at your own risk.\nAuthor: Bob Sherman\nPublisher: Bobby's Craft Boutique Inc.\nThis article is provided free of charge for use. Candles may be made and sold using this design royalty free, however no portion of this article may be reproduced for publication elsewhere without express permission from Bobby's Craft Boutique Inc. with the following exceptions:\n- Non profit organizations such as religious groups, scouts, 4h, etc... may use this information without permission for printed materials provided it is used without modification and credit is given to both the author and onestopcandle.com\n- Reprinting to the web is prohibited without permission, however web sites wishing to link to this article may do so without permission."", ""CONSTRUCTION INDUSTRY INFORMATION PORTAL\nHomepage For Construction Industry Professionals\nInformation resources for construction industry employment and operations\nHoliday Safetyby Susan McElrath\nPrinter Friendly Version\nFrom now until New Year's Day there are all sorts of things to celebrate. Unfortunately, decorations, food, and alcohol all present dangers, particularly to young children. A few common-sense precautions can help keep your holidays safe and happy.\nCandles, colored lights, Christmas trees, artificial snow, and plants add a lot of beauty to festive occasions. Unfortunately, young children (pets, too!) often like to play with these. Kids may even try to taste these lovely decorations, or experiment with that candle flame. Make sure decorations are kid-safe or out of reach. Watch out especially for small ornaments, button batteries, and festive yet poisonous plants such as mistletoe, poinsettia, and Jerusalem cherry. (Hard or chewy candy can choke children, so keep it away from them, too.)\nCandles are also popular, but don't let yours go up in smoke! Speaking of candles, have you ever let one get too close to some greenery, paper, or a tablecloth? You know what can happen. Keep an eye on those candles. Never leave them burning unattended, and never place them too close to other objects. (Light candles only in containers meant for lighted candles--many candles are not meant to be lit in their original, usually decorative, containers.) Keep those lighters and matches out of children's reach, too.\nIf you use outdoor lights, make sure they're approved for outdoor use. For indoor and outdoor lights, inspect them for broken or missing bulbs, and check the cords for fraying before you use them. Don't place cords where they can trip people, and don't run them under furniture, rugs, carpeting or other objects, or around doors and windows.\nA crackling fire in the fireplace is a welcome sight. Keep yours safe. Make sure the fireplace and chimney are clean when you begin using it each year. Call a professional to clean and inspect your flue. Keep the area in front of the fireplace free of combustible material, and use a screen to keep sparks from flying out. Again, keep young children and pets away from the fireplace.\nThere's plenty to eat during the holiday season. Unfortunately eating food that isn't prepared right or that has been sitting around too long can turn happiness to misery. Foodborne illness can make you very sick. It can also kill. You should always be aware of food safety precautions and follow them no matter what time of year. If you are hosting a celebration, be sure to follow safe handling, cooking, and re-heating practices, especially for meats and eggs. These precautions can usually be found printed on the food packages and in published literature, such as cookbooks or health department brochures. Don't take a chance by eating food that is not fully cooked, seems to have been sitting out too long, or that contains raw eggs--not even cookie dough!. The risk of serious illness is just not worth it.\nMany holiday celebrations include alcohol consumption. If you do drink, never drink and drive. Watch out for guests and make sure everyone is riding with a sober driver. Provide plenty of alternatives to alcohol such as soft drinks, coffee, tea, and cocoa. Stop serving alcohol a few hours before your party ends. Also, never leave drinks around where youngsters and pets can get them. Alcohol can make them very sick. Collect used glasses and cups quickly and lock up your supply.\nFinally, make sure your home is equipped with smoke detectors and, if your home uses a fossil fuel heat source, carbon monoxide detectors. Test the batteries frequently--some people find it helpful to replace the batteries twice a year when Daylight Savings Time begins and ends. If your home has been vacant for several days test the detector batteries when you return. The low-battery alarm may have sounded while you were away and could be silent when you return. The battery may be dead and you won't know unless you check. Don't let accidents spoil your holidays. Plan now to play it safe this year and every year.Happy Holidays!""]"	['<urn:uuid:8eb528a1-56a9-4053-8cd2-1a8fb8004a55>', '<urn:uuid:065f2b6e-9278-4f8c-b394-38ec91a06655>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	32	89	1553
62	project completion pattern over time	Projects typically follow a pattern where they make very little progress at the beginning, then ramp up steeply in the middle, and again make slow progress at the end of the project.	"['How to Review Subordinates\' Work\nWhen you assign work to subordinates, you monitor many aspects of their progress, but you are mainly concerned about whether the work gets done right and is done on time.\nGet Work Done on Time\nGetting their work done on time is the easier of the two to monitor. That\'s because time is easier to measure than quality. You do need to set clear requirements, including milestones and deadlines, but you also need to allow some flexibility. Plan their schedules, or have them plan their schedules, to include some slack time. There always will be some unforeseen events, so it is prudent to build at least a little slack into a schedule. The amount of slack that you allow in their schedule will vary from individual to individual with the more senior and talented individuals needing less slack.\nUsing a Progress Report to Track Schedule\nThe most common method of tracking whether work is done on time is the periodic report, usually done on a weekly basis for small projects and on a monthly basis for larger projects. When you have your subordinates submit written progress reports, one of the items on which they will report is their adherence to the schedule. One common method used to quickly indicate status is stoplight colors. If the project is on schedule, they would show a green symbol. If the project is late, they would show that as a red symbol.\nSometimes yellow is also used to indicate when there is a small problem that is not severe enough to warrant the red color.\nYou also want them to report anything that is blocking their progress. This section should include what they have tried to clear the blocker and the results of those efforts. If they are requesting assistance from you, or from someone outside their sphere, that should be clearly indicated.\nDon\'t forget to have them also include their accomplishments. If they have done something that has cleared a blocker, prevented the problem, or in some other way helped keep the schedule intact that should be noted in their progress report.\nFinally, the progress report should show the percent complete. Most projects make very little progress at the beginning, then they ramp up steeply, and at the end of the project again make slow progress. The project schedule that you and your employee have developed should account for this. Whether or not you have factored this into the schedule, you need to evaluate the progress reported by the employee. The percent planned complete should reflect, in general, how much of the time allotted to the project has passed.\nIn a project with a three-month schedule, the employee should report the planned complete percentage of approximately 1/3 at the end of the first month. If they reported a number much smaller or much larger it would be prudent for you to look a little deeper. Also, you need to make a judgment about the accuracy of what they report for an actual percent complete. Based on your knowledge of the project and its overall status you should be able to judge, approximately, whether the percent complete reported by the employee is accurate or not.\nFor instance, if they report a planned complete figure of 60% and an actual complete percentage of 65%, but you know the project has been plagued with difficulties, you should investigate the report in more detail.\nGet Work Done Right\nIt\'s more difficult to measure and monitor whether or not your employee is getting the work done right. That\'s because it\'s harder to define and quantify what ""done right"" means. Here again, as with schedules, you need to set clear and specific expectations and objectives. And while with schedules you allow some flexibility and slack you need to be less flexible with regard to quality. Good is good.\nDon\'t lose sight of your target. When we talk about ""done right"", remember you\'re going for ""excellent"" not ""perfect"". And don’t forget to focus on results rather than the process.\nThe best way to track your employees’ progress with respect to getting the work done right is frequent meetings and making liberal use of ""show and tell."" When an employee reports having made appreciable progress, have them show you what they\'ve done. At the beginning of the project, the work may be a little rough and only partially complete. It is to be expected. However, as time passes, the employee’s demonstrations should show progress with the work becoming smoother and more complete.\nThe key during these quality reviews is to make sure the employee’s estimate of the quality of the work done so far matches your own. If it appears to you that the quality of the work done to this point is still rough and preliminary, you need to discuss the difference with the employee and reset their expectations. If you don\'t, when they think they\'ve achieved finished quality, you\'re likely to assess the work is only moderately finished.\nWhen reviewing work that you have delegated to subordinates, remember that you are still responsible for their work. So make sure the work is done on time and done right as discussed above. At the first sign of difficulty, you need to step in and adjust the employee’s standards to conform to your own.']"	['<urn:uuid:75315c33-e2e3-41be-a825-26a821e2cfc6>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	5	32	888
63	What makes minerals solidify at different temperatures when hot molten rock cools down, and is there a specific order to this process?	According to Bowen's reaction series, minerals crystallize in a specific sequence as magma cools. There are two simultaneous sequences: the discontinuous series and the continuous series. In the discontinuous series, minerals high in iron and magnesium crystallize first, starting with olivine at very high temperatures, followed by pyroxene, then amphibole, and finally biotite. In the continuous series, plagioclase minerals form, starting with calcium-rich plagioclase at high temperatures, gradually changing to sodium-rich plagioclase as temperatures drop. At the coolest temperatures in both series, potassium feldspar, muscovite, or quartz may form.	"[""Back To CourseEarth Science 101: Earth Science\n23 chapters | 178 lessons\nDr. Gillaspy has taught health science at University of Phoenix and Ashford University and has a degree from Palmer College of Chiropractic.\nIn this lesson, we will talk about Bowen's reaction series and magmatic differentiation, which are basically two attempts to explain or predict how igneous rocks form. We recall that rocks made from the cooling and solidifying of magma are called igneous rocks. So the Bowen's reaction series and magmatic differentiation pertain to these types of rocks.\nSo who was Bowen and why do people who know a lot about igneous rocks know his name? Well, Norman Bowen is well known in geological circles because of some experimenting he did back in the 1920's and 30's. Through his experiments, he discovered that minerals crystallize differently as they cool. The result of his research gave us what we call the Bowen's reaction series, which we can define as the crystallization sequence from magma as cooling occurs.\nAs Bowen dug into his understanding of the crystallizing process, he realized that there are two sequences that minerals can follow. These are the discontinuous series and continuous series, as we see here on this diagram of the Bowen's reaction series. The discontinuous series is seen on the left and it contains minerals high in iron and magnesium. We also see that the series progresses with a drop in temperature.\nWhen we follow this branch, we see that at very high temperatures, olivine is the first mineral to form. In other words, olivine minerals, which are high in iron and magnesium, tend to crystallize at very high temperatures. Then, as the magma begins to cool, some of the olivine becomes pyroxene. As we progress in the sequence with more cooling, the pyroxene turns into amphibole and finally the amphibole turns into biotite.\nYou might want to use an acronym to remember the steps of the sequence, such as 'Olive Pits Are Bitter.' Each step of the discontinuous series represents a very distinct change with the creation of a new mineral, so the change is not a smooth continuous flow, but instead a discontinuous process, hence the name. With the formation of biotite, the discontinuous series officially ends, but there can be more to it if the magma has not completely cooled and depending on the chemical characteristics of the magma. For instance, the hot liquid magma can continue to cool and form potassium feldspar, muscovite or quartz.\nYou might want to use a technique to remember these final minerals as well. For example, you could use the acronym 'P.M. Quiet.' The 'P.M.' is useful because these minerals are formed late in the sequence, just like the p.m. hours are late in the day. And, 'Quiet' is useful as a memory jogger because these minerals are formed during the coolest or 'quietest' temperatures of the sequence.\nThe continuous reaction series is going on at the same time as the discontinuous series and we see it here as the right branch. With the continuous branch we see the reaction has more of a flow or 'continual' reaction taking place, hence the name for this series. With the continuous series we see plagioclase minerals. It starts with the highest temperature mineral, which is calcium-rich plagioclase.\nAs the magma cools down, the calcium is replaced with sodium. But this happens in somewhat of a flow with the calcium and sodium mixing in a continual series, so a plagioclase in the middle of the series could be thought to have about 50% calcium and 50% sodium. At the bottom of the series we see sodium-rich plagioclase.\nBy remembering that the 'c' in 'calcium' comes before the 's' in 'sodium,' you can recall that calcium-rich plagioclase is at the top of this series and sodium-rich is at the bottom. Then, as we saw with the other branch, as the temperatures continue to cool and the chemical characteristics continue to change, we see the formation of potassium feldspar, muscovite or quartz - 'PM Quiet.'\nNow let's switch gears and take a look at the second process for predicting how igneous rocks form that we mentioned at the beginning of this lesson, which is known as magmatic differentiation. This can be defined as the process that explains how different igneous rocks can form from a single magma melt.\nSo we already learned that as magma starts to cool, crystals form out of the magma. But what we must also consider is that when these solid crystals form, they become denser. Dense things are heavier, so they tend to sink; just as a dense rock sinks if you drop it into a pond of water. These heavier crystals sink to the bottom of the liquid magma and take with them some of the available minerals that were in the original magma.\nThis changes the chemical composition of the remaining magma. This process continues with more crystals forming and settling out of the magma, further changing the composition of the remaining magma. After a while, we are left with layers of chemically-different igneous rocks that have settled out of the original magma. So with magmatic differentiation we can have many different igneous rocks forming from the same initial magma melt.\nLet's review. Bowen's reaction series can be defined as the crystallization sequence of magma as cooling occurs. It has two parts, the discontinuous series and the continuous series. Both branches progress with a drop in temperature. With the discontinuous series, we see that olivine is the first mineral to form, and it forms at a very high temperature. As the magma cools we see the formation of pyroxene, amphibole and finally biotite. You can use the acronym 'Olive Pits Are Bitter' to recall this sequence.\nWith the continuous series we see calcium-rich plagioclase forming at the highest temperatures, then as the magma cools the calcium is continually replaced with sodium until we have sodium-rich plagioclase. As the magma continues to cool and the chemical characteristics continue to change after both series, we see the formation of potassium feldspar, muscovite or quartz. Using the acronym 'P.M. Quiet' may help you recall these minerals.\nMagmatic differentiation is a process that explains how different igneous rocks can form from a single magma melt. As crystals solidify in the magma, they sink and settle out of the liquid magma. This changes the composition of the remaining magma and leaves us with layers of chemically-different igneous rocks that have settled out of the original magma melt.\nTo unlock this lesson you must be a Study.com Member.\nCreate your account\nDid you know… We have over 49 college courses that prepare you to earn credit by exam that is accepted by over 2,000 colleges and universities. You can test out of the first two years of college and save thousands off your degree. Anyone can earn credit-by-exam regardless of age or education level.\nTo learn more, visit our Earning Credit Page\nNot sure what college you want to attend yet? Study.com has thousands of articles about every imaginable degree, area of study and career path that can help you find the school that's right for you.\nBack To CourseEarth Science 101: Earth Science\n23 chapters | 178 lessons""]"	['<urn:uuid:79e3a2b4-9fe0-40cb-943d-c518a1316664>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	22	89	1210
64	what construction materials were used de havilland dh4 postal plane compared mosquito bomber	The de Havilland DH-4 used in postal service had a tubular frame covered by sheets of canvas, while the Mosquito was constructed using a balsa/plywood 'sandwich' construction, specifically designed to minimize the use of strategic metal resources.	"['Once upon a time, the world moved at a slower pace than it does today. No mass media, no 24-7 news channels, and no next-day mail delivery service were available. But with the advent of the aircraft as a functional operational machine, the world changed completely in an instant. In the past, mail was delivered on horses, trains, boats and even primitive automobiles and/or four-wheeled trucks, these methods of delivery took days, weeks or even months in some instances; but with the invention and development of the airplane, mail delivery reached a new dimension. Thus the airplane had a direct effect on how people could communicate throughout great expanses of territory. They shortened, not the distance between sender and receiver, but the time the mail took from getting from the originating party to the end user. In the course of the early aircraft-supplied mail delivery system, four very distinct aircraft stood out from the pack. These four represented the epitome of air cargo delivery in an age of constant development and improvements.\nIn the spring of 1911, an early sample of the Wiseman-Cooke airplane was the first flying machine to deliver mail in the United States, when pilot and aviation pioneer Fred Wiseman carried a pack of letters from Petaluma to Santa Rosa in California. The complete eighteen-and-a-half mile trip was covered by Wiseman in two full days. Many mechanical difficulties, common on those early flying machines, delayed his trip. When he was airborne, the Wiseman-Cooke plane could only muster speeds just short of seventy mile per hour. Slightly built and very similar in airframe construction to the famous Wright Brother’s Flyer, the Cooke was powered by a Hall-Scott V8 engine modified to give the 670 lbs airframe enough speed to clear the ground. The next generation of mail delivery airplanes instituted a big move forward with the inception of the Curtiss JN-4, also called the Jenny. The Jenny was an advanced version of an early Curtiss JN model used mainly as a training aircraft during the Great War by the British Royal Flying Corps. Introduced in mid 1915, the JN-4 had a fuselage of 27′-4″ in length with a height of 9′-10.5″. Total wing area for the Jenny was 352 sq ft. A Curtiss designed OX5 in-line piston engine, capable of generating nearly seventy miles per hour, powered the JN-4. After the War ended in August 1918, the United States Postal Office adopted the Jenny as it’s first official air mail carrier plane. But the Jenny’s relatively small operational range, (it could operate only about one hundred and seventy five miles without refueling and maintenance); made it ill-suited for long-range mail delivery. It also did not help that the Jenny’s payload capacity was only three hundred pounds. Soon after its incorporation into the US Mail System, the Jenny was retired from front line service in less than a year.\nWhen the US Postal Service bought the JN-4s, they also acquired a small group of de Havilland DH-4 airplanes from the US Army Signal Corp supply depot. The Airco, (or de Havilland), DH-4 was a two-seater daylight medium bomber produced in Great Britain. The DH-4 had an airframe 30′-8″ in length and a height of 10′-5″. When in combat, the DH-4 was armed with a single 7.7 mm Vickers machine gun mounted on the front of the cockpit, and another Vickers gun placed in the back of the fuselage for defensive cover – features removed for civilian operations. The DH-4 could carry up to 460 lbs of bombs internally, making the cargo payload a more manageable one. The plane was powered by one Roll-Royce Eagle VIII Vee piston engine capable of providing the aircraft with top speeds of just under 143 mph. The de Havilland’s operational range was an improvement over the other aircraft examples utilized by the Postal Service; it could operate at a range of 435 miles without any stops. As soon as they arrived, and after re-fitting, the DH-4 entered front-line service with the Postal Office. This plane was exactly what the mail service was looking for. It could carry a relatively large payload for long distances. But, as with all of the aircraft of the time, it fell victim to the newer, improved and less expensive aircraft coming along.\nThese two above mentioned aircraft represented a leap forward in aviation design. They were basically a tubular frame covered by sheets of canvas. The first departure from this design concept adopted by the Service was an impressive, albeit, dangerous one. The first US Postal Service all-metal aircraft was Germany’s Junkers JL-6 plane. First developed for military use in March 1917; the aircraft never saw significant combat in the Great War. A civilian version was introduced in the spring of 1919. It were to be the world’s first all metal monoplane use to ferry civilian passengers, doing so from the mid 1920 onward. But the JL-6 was a flawed design. Its electrical wire system was not properly insulated causing the plane to catch fire on mid-air. Many attempts were made to correct the problem, and all were unsuccessful, this fact lead the Postal Service to retire the JL-6 from front line service in the summer of 1921.\nToday, the United States Postal Service utilized the latest commercial aircraft available and the best that technology can offer, this with the sole purpose of providing the customer with the best delivery capability the Service can offer. But in pioneer days of aviation, the Service needed to adapt promptly to new technology, new operational system, and by trial and error; they did. These four distinct planes, each of one served the Service in its own capability, proved that the aircraft was indeed, a practical and affordable mean of mail transportation, and on those days, this was a leap forward.\n– Raul Colon', ""De Havilland B.35 Mosquito, RAF Museum, London and ‘The People’s Mosquito’ Project\nTags: 'Freeman's Folly', 'Milestones of Flight' Gallery, 'Peoples Mosquito', 'stealth' characteristics, 1996, 25th November 1940, 4000 lb bomb, a gathering of senior officers, a popular movement, Air Chief Marshal Sir Wilfred Rhodes Freeman, Air Member for Research and Development, Air Ministry, aircraft, anti-aircraft units, anti-aircraft units around the country, anti-shipping strikes, Australia, Aviation, B.35, balsa/plywood 'sandwich' construction, Baronet, Barton, Battle of Britain Memorial Flight, BBMF, Berlin, bomber, Bomber Command, British Aerospace, British Aerospace-owned, built in secrecy, Canada, Canberra, converted from B.35 bomber version, Czechoslovakia, day intruder, De Havilland, de Havilland DH.98 Mosquito, defend Great Britain, DH.98, Doncaster, DSO, engine feathered, England, English Electric, English Electric Canberra, enormous range, Europe, European skies, extreme manoeuverability for a twin, fastest bomber in RAF service, fastest warplane Europe, fastest warplane in Europe, fighter, Fighter Command, fighter-bomber, flight status, flight test programme, flyable Mosquito, FRAeS, furniture trades, gave their lives in the service of their country, Germany, Great Britain, gun turrets, Hendon, Hertfordshire, honouring those who gave their lives, KGCB, London, low-level attack aircraft, MC, mosquito, Mosquito night-fighters, Mossie, museum, Museums, near-400 mph speed, new jet aircraft, New Zealand, NF.30, night bomber, night fighter, No 616 Squadron, opposing German fighters, P-51 aircraft, photo-reconnaisance units, photo-reconnaissance, photo-reconnaissance version, Prague, presenting it to the nation, prototype Mosquito, public memorials, radar operators, RAF, RAF Coastal Command, RAF Finningley, RAF's Battle of Britain Memorial Flight, RAuxAF, restored aircraft may fly in the next year, restoring a Mosquito to flight status, Rolls-Royce, Rolls-Royce Merlin, Royal Air Force, Royal Air Force Museum, RR299, Salisbury Hall, Second World War, Specification B.1/40, strategic metal resources, superb night-fighter, target tugs, the outcome of WW2, TT.35, UK subcontractors, upward rolls, USA, USAAF, W4050, warbird, without defensive armament, wood-working, WW2\nThis aircraft has it all – speed, power, grace. It was built against all the odds, when the Air Ministry just couldn’t understand the concept of a bomber without defensive armament or gun turrets. It was only due to the staunch support of the Air Member for Research and Development, Air Chief Marshal Sir Wilfred Rhodes Freeman, Baronet, KGCB, DSO, MC, FRAeS, RAF, that the De Havilland DH.98 Mosquito was built at all; indeed, its detractors often refered to it as ‘Freeman’s Folly’. The prototype Mosquito, W4050, was built in secrecy close to Salisbury Hall, Hertfordshire, and took off from a small field nearby on 25th November, 1940, to start its flight test programme. It was only because of its mainly balsa/plywood ‘sandwich’ construction, with wood being used wherever possible so using relatively little strategic metal resources, that it got the green light under Specification B.1/40. W4050 soon amazed a gathering of senior officers and others by exhibiting near-400 mph speed, extreme manoeuverability for a twin, and performing upward rolls with one of its two Rolls-Royce Merlin engines feathered! It became the fastest warplane in Europe – of ANY type – a title it held for nearly 18 months; indeed, it stayed the fastest bomber in RAF service until the 1950s, and the advent of the twin-jet English Electric Canberra.\nEveryone now wanted the Mosquito! RAF Coastal Command for anti-shipping strikes, Fighter Command as a superb night-fighter and day fighter-bomber, Bomber Command as a hard-hitting low-level attack aircraft and later a night bomber with the capacity to carry a 4,000 lb bomb to Berlin and return (and do it TWICE in one night!). Photo-reconnaisance units prized it for its enormous range – able to penetrate as far as Prague, Czechoslovakia in later versions – and its ability to outmanoeuver opposing German fighters. Even the USAAF wanted the Mossie, going so far as to suggest a one for one swap involving P-51 aircraft!\nProduction was split amongst many UK subcontractors in the former wood-working and furniture trades, and rapidly spread to Canada and Australia. Even after the end of the war, Mosquito night-fighters, such as the NF.30, were used to defend Great Britain until the new jet aircraft could be developed (No. 616 Squadron, RAuxAF, at RAF Finningley, near Doncaster, was an example of this). The final versions produced were target tugs, which provided fast towing and target facilities for anti-aircraft units around the country; some of these were converted from B.35 bomber versions, (as seen in the above photograph) which was taken in the Royal Air Force Museum’s ‘Milestones of Flight’ Gallery at Hendon. Inadvertently, the Mossie must have given radar operators a harder time than they might have expected, because its wooden structure conveys certain ‘stealth’ characteristics.\nDue to the tragic loss of the British Aerospace-owned ‘RR299’ at Barton, in 1996, there is no longer a flyable Mosquito in European skies (up to three restored aircraft may fly in the next year or so, one in New Zealand, one in the USA and one in Canada). However, there is big news this month; a popular movement has been formed with the avowed intent of restoring a Mosquito to flight status, and then presenting it to the nation – in the shape of the RAF’s Battle of Britain Memorial Flight – so that future generations can marvel at the sight and sound of a warplane which did so much to influence the outcome of WW2. I would urge you all to support the efforts of the ‘Peoples Mosquito’ organization (links at the bottom of this page), as being not only the right thing to do, but as a way of honouring those who gave their lives in the service of their country. Bomber Command, especially, has been under-represented until recently in the way of public memorials, and this would be a wonderful way of righting that omission. As I was saying to a friend, ‘It’s Mosquito time’!\n12 comments on “De Havilland B.35 Mosquito, RAF Museum, London and ‘The People’s Mosquito’ Project”\nThis month’s offerings!""]"	['<urn:uuid:5e0e5059-deef-42a3-b0cd-920c790bbf09>', '<urn:uuid:333b1fe5-470b-48b1-865d-3c4e01d0dd84>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-13T05:18:19.158519	13	37	1942
65	historic fountains drinking mechanism changes functionality	The drinking mechanisms of these historic fountains have evolved over time. The Clifton Down Fountain in Bristol originally featured a font with a scalloped edge basin supported by a decorative pedestal, offering drinking cups suspended by chains from a central urn, plus a ground-level basin for dogs. Similarly, the Frazer Fountain in Sydney initially had cups dangling from its large water basin, with bronze taps shaped like dolphins. However, both fountains saw changes in their drinking mechanisms - the Bristol fountain's original font was replaced with a bubbler-type fountain around 1980 (now non-existent), while the Sydney fountain's basin and taps were replaced with a bubble stream design in 1934 due to hygiene concerns about communal cups.	['Clifton Down Fountain\nLocation: Bristol, England\nLocated on Sion Hill at the junction of Gloucester Row and Observatory Road this drinking fountain was erected in 1866. It can be found on the south side of Clifton down near the suspension bridge.\nSeated on a two tiered square granite plinth, drinking fountain number 8 from Walter Macfarlane & Co.’s catalogue was manufactured at the Saracen Foundry at Possilpark in Glasgow, the most prolific architectural iron founders in the world. The structure is 9 feet 6 inches high and consists of four columns, from the capitals of which consoles with griffin terminals unite with arches formed of decorated mouldings.\nOn each side, rope moulded cartouches within each lunette host images with arched faceplates containing inscriptions using raised metal letters. The coat of arms of the City and County of Bristol with its Latin motto Virtute Et Industria, translated as By Virtue And Industry, contains an inscription stating: Jesu, Like Streams From God Thy Spirit Give. This legend is also shown above a shield containing a cockle shell and the initials MCB. The third and fourth sides of the fountain contain the same Bible quotation i.e. Coming To Thee That We May Drink And Live. Amen. Jn XV 26 & VII.37. The third shield contains an inscription, To The Glory Of God In The Use Of Man & Beast; and the fourth shield displays a handshake accompanied by the phrase, The Receiver To The Givers Ever So We May Go. The structure is surmounted by an open filigree dome, the finial being a crown with a pattée cross.\nUnder the canopy there was originally a font (casting number 7) 5 foot 8 inches high. The basin which had a scalloped edge and decorative relief was supported by a single decorative pedestal with four pilasters and four descending salamanders, a symbol of courage and bravery. A central urn with four consoles offered drinking cups suspended by chains. The terminal was a crane. A small basin at ground level was available for the use of dogs.The font was later (circa 1980) replaced with a bubbler type fountain which no longer exists.\nSymbolism was popular in Victorian times. Griffins are symbolic of guardians of priceless possessions, salamanders display bravery and courage that cannot be extinguished by fire, and cranes are recognized as a symbol of vigilance.\nThe structure was recorded as a British listed building, category Grade II, on 4 March 1977.\n- Capital: The top of a column that supports the load bearing down on it\n- Cartouche, a structure or figure, often in the shape of an oval shield or oblong scroll, used as an architectural or graphic ornament or to bear a design or inscription\n- Console: a decorative bracket support element\n- Filigree, fine ornamental work\n- Finial, a sculptured ornament fixed to the top of a peak, arch, gable or similar structure\n- Fret, running or repeated ornament\n- Griffin, winged lion denotes vigilance and strength, guards treasure and priceless possessions\n- Lunette, the half-moon shaped space framed by an arch, often containing a window or painting\n- Pattée cross, a cross with arms that narrow at the centre and flare out at the perimeter\n- Pedestal, an architectural support for a column or statue\n- Pilaster, a column form that is only ornamental and not supporting a structure\n- Plinth, flat base usually projecting, upon which a pedestal, wall or column rests\n- Terminal, statue or ornament that stands on a pedestal\nPosted on April 21, 2015, in Architecture, BrItish Listed Building, Cast Iron, Drinking Fountain, England, English Listed Building, Saracen Foundry and tagged Clifton suspension bridge, Gloucester Row, John XV 26 & VII.37, Observatory Road, Sion Hill, Virtute Et Industria. Bookmark the permalink. Leave a comment.', 'City of Sydney\n- GPO Box 1591Sydney 2001\n- 02 9265 9333\n- [email protected]\nThe excellent example of early, decorative drinking sources in Sydney has been revitalised to its former glory.\nFrazer Fountain, one of Sydney’s first water bubblers located in Hyde Park, has been restored to its former glory.\nWhile most are no longer functional, these fountains are excellent examples of some of Sydney’s early, decorative drinking sources.\nRestoration work on the fountain started in early 2016 and was completed in late 2016.\nWe installed a new base, plinth and steps. We also repaired the cracks in the mortar and revitalised the intricate filigree works of the John Frazer fountain, ensuring its continuing place in our city’s rich history.\nThe fountain, along with its pair, quenched the thirst of many Sydneysiders at a time when sources of clean drinking water were few. It was not only a source of clean drinking water, but it also became one of the park’s most-loved early features.\nGifted to his fellow citizens by John Frazer, a wealthy philanthropist and one of the city’s early entrepreneurs, the fountain was commissioned in 1881 and installed in 1882. The fountain is one of a pair of decorative, canopied drinking fountains. Its twin, designed in the Italian renaissance style, was installed on Prince Albert Road in 1884, where it still stands today.\nFirst installed at the Hyde Park entrance today known as Whitlam Square, Frazer Fountain was moved near the Pool of Reflection in 1917 to make way for the Emden Gun. The gun from the SMS Emden of the Imperial German Navy was sunk by the original HMAS Sydney after a 90-minute sea battle during World War 1. In 1934 the fountain was moved once again to its current location near College Street when Hyde Park south was remodelled.\nJohn Frazer emigrated from Ireland in 1842 as a carpenter and joiner with little wealth. Forward thinking and industrious, he became an extremely successful and influential businessman, owning one of central Sydney’s largest mercantile stores. Over his life he amassed a fortune of £405,000 – the equivalent of about $47.5 million today.\nThe original design of the fountain featured cups dangling from the large water basin for people to take a drink. Every detail in the design was considered, with the taps made of bronze in the shape of dolphins.\nHygiene standards were less stringent in the 19th century, but over time the city council realised the health hazards of communal cups. In 1934 it replaced the basin and taps with the new bubble stream design – today’s bubbler.\nBucking the usual trend of importing ready-made structures from overseas, the fountains were designed by Thomas Sapsford, the architect who also designed Sydney Town Hall’s Centennial Hall. He worked alongside local sculptor Lawrence Beveridge, who carved both structures using the finest-available Pyrmont sandstone.\nVisit the John Frazer fountains and the many other water features in our local area with our newly updated ‘Walk on Water’ self-guided walking tour and the Sydney Culture Walks app.\nStonemasons who are working to restore Frazer Fountain have uncovered a time capsule from 1934.\nCity Art: Frazer Fountain Hyde Park\nOffice of Environment and Heritage: Frazer Memorial Fountain']	['<urn:uuid:6dcf016e-0db8-4536-954e-5b02d9bf5635>', '<urn:uuid:ec650163-c40b-4e7c-b883-c225feed8164>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T05:18:19.158519	6	116	1163
66	soft power success challenges global influence	Soft power has seen varying levels of success across different countries. While some nations like Japan have achieved cultural appeal through hands-off government approaches, and countries like Brazil and South Africa have used soft power for regional influence, there are significant challenges. Autocratic regimes like China, Russia, Saudi Arabia, and Iran are increasingly capable of exerting influence beyond their borders, potentially undermining democratic institutions. For instance, China's Confucius Institutes, while promoting Chinese language and culture globally, have raised concerns about transparency and links to state propaganda, while Russia has been noted for cyber-subversion and disinformation methods that are spreading globally.	"['Soft Power Takes Center Stage\nLooking for all things soft power? Check out our roundup of content related to this important area of public diplomacy. Start with a general discussion of Joseph S. Nye, Jr.\'s concept and continue with case studies of individual countries.\nAll About Soft Power\n- Q&A with Joseph S. Nye, Jr. On the evolving concept of ""soft power"" and key challenges facing U.S. public diplomacy.\n- Soft Power: The Inspiration Behind Henry David Hwang\'s New Play Tony Award-winning playwright David Henry Hwang on his play with a musical told from a Chinese perspective for American audiences.\n- CPD-BBC: Does Soft Power Really Matter? An edited transcript of the CPD-BBC Forum held at USC, asking the hard questions about soft power.\n- 2017 Soft Power 30. The third edition of Portland Communications\' annual report was produced in partnership with CPD.\n- 5 Reasons Why Think Tanks Are Soft Power Tools. Katarzyna Rybka-Iwańska suggests an increasing need to recognize the value and impact of think tanks.\n- Ambassador Tom Fletcher Speaks About Soft Power. Ambassador Tom Fletcher offers insights into the results of the 2017 Soft Power 30 report.\n- Citizen Diplomacy and Soft Power by Timothy Jenkins explains how citizen diplomats are a critical part of their country\'s soft power.\n- Putting a Hard Edge on Soft Power. Philip Seib warns against over-emphasizing ""cutesy projects"" over lasting change.\n- De-Americanizing Soft Power Discourse? Daya Thussu examines of the shifting global landscape of soft power, away from the U.S. and toward “the rise of the rest.”\n- The Hard Truth About Soft Power. CPD Advisory Board Member Markos Kounalakis and Ambassador Andras Simonyi penned this issue of CPD Perspectives.\nCountry Case Studies\nThe Tumultuous Story of Brazil\'s Soft Power. The hosts of the ""Explaining Brazil"" podcast talked with Daniel Buarque on the country\'s diminishing influence in the international sphere.\nSouth Africa\'s Soft Power Future. How South Africa uses soft power in foreign policy to maintain regional hegemony amidst a troubled economy and government.\nLeveraging Intellectuals for Soft Power by Alfredo Cumerma. Thought leaders can serve as cultural ambassadors, as Norberto Fuentes has for Cuba. How can the U.S. follow suit to develop soft power?\nTokyo 2020 and Japan\'s Soft Power by Yukari Easton. Japan\'s cultural appeal is at it\'s best when the government takes a hands-off approach, says Easton.\nThe Public Diplomacy of Emerging Powers Part II: The Case of Indonesia by Ellen Huijgh. In part II of the ""emerging power"" series, Huijgh looks at Indonesia\'s influence in the international realm.\nThe Public Diplomacy of Emerging Powers: Part I: The Case of Turkey by Ellen Huijgh and Jordan Warlick. Part I of the ""emerging power"" series which looks at the soft power of Turkey.\nU.S. Soft Power From the First Amendment. David Ensor on how the VOA\'s style of reporting strengthens U.S. soft power.\nSoft Power and Public Diplomacy: The Case of the European Union in Brazil by María Luisa Azpíroz. Written in both Spanish and English, Azpiroz\'s paper considers the EU\'s efforts in Brazil.\nRising Soft Powers: India. This CPD e-book features an exclusive interview with Indian diplomat and author whose book was adapted into the film Slumdog Millionaire, and more.\nRising Soft Powers: China. A 2015 e-book featuring CPD blogs, essays and interviews on the Confucius Institutes, U.S-China military-to-military exchanges and ""Panda diplomacy.""\nCartoons and a Country\'s Attractiveness: The Case of Russia. CPD Blogger Anna Velikaya makes the case for cartoons as soft power.\nIcons are courtesy of Flat Icon Basic License and are attributed to Freepik.', 'Like a growing number of countries, Canada is contending with some vexing challenges to its democracy posed by ambitious and more internationally active authoritarian states. It is not just one country that presents such challenges. China, Russia, Saudi Arabia and Iran are among the autocratic regimes quite capable of exerting influence beyond their national borders, and they are doing so in many democratic settings. Nor is it a single sector or institution that is vulnerable. Media, universities and academic publishers, as well as the technology sector and electoral systems, are among those at risk in this era of hyper-globalization.\nDemocracies have been slow to come to grips with the fact that interaction with autocracies in such contexts is not necessarily neutral or benign. Russia’s corrosive manipulation of the information environment and the election processes in any number of instances, including in the United States, is indicative of the problem.\nThe most determined autocratic powers use a variety of activities to manipulate democratic institutions. Canadian society is already contending with this multifaceted challenge.\nChinese Canadians have raised concerns about the Chinese government’s impact on their own freedom of expression. The Canadian experience is part of a global pattern in which the space for external criticism of the Chinese authorities has shrunk. Within China, the authorities brook no dissent. Abroad, Beijing seeks where possible to exclude from public discussion topics on which it prefers to hear only silence, part of an exertion of influence my colleagues and I at the National Endowment for Democracy describe as “sharp power.” This desire was brought into focus in June 2016 during a news conference in Ottawa, when China’s foreign minister, Wang Yi, scolded a Canadian reporter for asking Stéphane Dion, then Canada’s foreign minister, a question about human rights in China.\nThe authoritarian challenge to democratic standards is also present in the educational sphere. China’s engagement in the Canadian education sector has skyrocketed. Canada was host to 186,000 Chinese students as of December 2017. Confucius Institutes and Confucius Classrooms, which are part of the Chinese government’s overseas propaganda efforts, offer Chinese language and culture courses at universities and schools around the world. As of early 2019 there are 12 Confucius Institutes and 36 Confucius Classrooms in Canada, controversial for their opacity and links to China’s state propaganda apparatus. As I noted in testimony presented to the Canadian Senate in November, “Little about [the Confucius Institutes] is transparent. It is hard to say, for instance, what amount of Chinese government money goes to host universities. It’s also unclear what level of control universities have over curricula within these institutes.” Unlike freestanding organizations such as France’s Alliance Française or Germany’s Goethe-Institut, the Confucius Institutes are embedded within educational institutions, most of which are dedicated to the type of free intellectual inquiry that is not possible at the Confucius Institutes themselves.\nSaudi Arabia has tried to limit expression. In August 2018, Riyadh effectively sought to weaponize the more than 8,000 Saudi students studying in Canada in reaction to Foreign Minister Chrystia Freeland’s straightforward tweets about evident human rights abuses in Saudi Arabia. The horrific murder of Jamal Khashoggi in Istanbul at the hands of the Saudi state in October brought the depths of the kingdom’s repression into unambiguous and sharp relief. For their part, the Iranian authorities have sought to extend censorship to film showings in Canada, among other instances of extraterritorial repression.\nAs they do in many places, the leading authoritarian regimes increasingly reach beyond their borders to intimidate or silence dissidents. The University of Toronto’s Citizen Lab identified what it believes are Saudi state efforts to digitally track and intimidate Canadian residents. Beijing, too, has engaged in such repression in Canada. In 2018, a report issued by a coalition led by Amnesty International Canada chronicled harassment ranging from digital disinformation activities to direct threats aimed at Taiwanese, Tibetans and Uighurs in Canada. Within China, Tibetans and Uighurs face an unprecedented crackdown; in Xinjiang, the latter are being subjected by Beijing to some of the most horrifying, systematic repression since the darkest days of the 20th century. Beijing’s arbitrary methods of “rule by law,” rather than rule of law, have been on display in the Canadian context in the recent case of Huawei Technologies Company executive Meng Wanzhou.\nRussia has received a good deal of attention for its harmful intrusions into democratic space, and deservedly so. Its cyber-subversion and disinformation methods are metastasizing globally. The Russian authorities have tested, refined and applied much of their playbook of repression at home, on Russian journalists and activists.\nA concerted response is needed to the challenges presented by such authoritarian regimes. Any effort to strengthen resilience and defend democracy’s integrity must be durable. It will require effective cooperation and dedicated channels for information sharing.\nOn this count, the nascent Rapid Response Mechanism (RRM) initiated in 2018 under Canada’s G7 presidency to defend against foreign threats holds promise and could offer a valuable model of cooperation for future efforts to defend democracy and the ideas that underlie it. The RRM initiative seeks to strengthen the leading democracies’ “coordination to identify and respond to diverse and evolving threats…including through sharing information and analysis, and identifying opportunities for a coordinated response.”\nThe challenge is multidimensional and therefore requires a full-spectrum response, including the active participation of civil society, which must play an important role in ensuring that democracies maintain their own standards and principles as a way of safeguarding themselves against corrosive authoritarian sharp power.\nDo you have something to say about the article you just read? Be part of the Policy Options discussion, and send in your own submission. Here is a link on how to do it. | Souhaitez-vous réagir à cet article ? Joignez-vous aux débats d’Options politiques et soumettez-nous votre texte en suivant ces directives.']"	['<urn:uuid:d9ef236a-5889-46f4-bcf6-0598158668db>', '<urn:uuid:672cbf55-72b8-4fcd-a488-035881c1c7b4>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:18:19.158519	6	100	1557
67	construction site environmental controls material recycling	Construction sites require comprehensive environmental controls and material recycling strategies. For erosion control, sites must prevent soil loss during excavation, install silt fencing around excavation areas, protect topsoil piles, and maintain stormwater pollution prevention measures like inlet protection and vehicle tracking controls. Regarding materials, the sustainable approach involves investigating ways to accumulate salvaged and waste materials for recycling, deconstructing existing structures with recoverable materials for recyclers, and considering donating surplus materials to nonprofit organizations like Habitat for Humanity. These practices help both prevent water pollution and promote resource conservation.	"['Sustainability is the process applied to our quest to sustain economic growth while maintaining our long-term environmental health. Sustainability means designing structures that take advantage of technological advancements to create eco-friendly products. Inert-gas-filled insulated windows, engineered-wood products made from scrap wood shavings, sawdust and assorted wood fibers, and thermal break window frames that keep the cold and hot air out are all examples of sustainable products. These products provide the owner and occupants with the following benefits:\n- Reduced maintenance and replacement costs over the life of the building\n- Energy conservation\n- Improved occupant health\n- Productive working environment\nSustainable products incorporated into the building should follow these selection guidelines:\n- Recycled content\n- Natural, plentiful, or renewable materials\n- Products manufactured by a resource-efficient process\n- Locally available products\n- Salvaged, refurbished, or remanufactured products\n- Reusable or recyclable products\n- Durable products\nUsing sustainable materials can also improve the indoor working environment and save money. Consider these advantages:\n- Materials that emit few or no carcinogens or irritants, as demonstrated by the manufacturer’s long-term testing results\n- Minimal chemical emissions from volatile organic compounds (VOC) that out-gas (continue to emit chemical vapors after installation)\n- Moisture-resistant materials that are not easily susceptible to mold growth\n- Materials that are easily maintained and require simple nontoxic cleaners\n- Equipment systems that promote healthy indoor air quality (IAQ) by identifying indoor air pollutants\n- Products and systems that help reduce water consumption\nThe sustainable approach to design would include requirements to do the following:\n- Simplify construction details.\n- Utilize repetitive details and components.\n- Standardize design components.\n- Incorporate accurate dimensions in the design, as some product and material sizes many have been reduced.\n- Simplify building systems so future expansion projects can take advantage of simplified designs or components.\n- Consider occupant safety and worker productivity gains in the new design.\n- Investigate more efficient and environmentally sensitive ways to bring underground utilities into the site with the least disruption to the existing terrain.\n- Consider other ways of disposing of site drainage onsite rather than offsite.\n- Adjust new site contours to provide for a balanced site where no offsite fill or off-site disposal of surplus soils is required.\n- Optimize dimensions to utilize a standard product size.\n- Minimize plumbing pipe and HVAC ductwork bends to reduce liquid and air friction.\n- Select fittings and fasteners that permit quick assembly.\n- Select sealants with the least environmental impact and longest life.\n- Investigate ways to accumulate salvaged and waste materials for recycling.\n- Consider donating surplus materials to a nonprofit organization such as Habitat for Humanity.\n- Deconstruct all existing structures with substantial recoverable materials and dispose of them to recyclers.\nWhen designing a new green structure, a number of goals must be set. The site must meet or exceed standards for sedimentation control and erosion:\n- Prevent the loss of soil during excavation and construction due to surface water drainage; keep dust down and cover large stored piles of earth to prevent wind erosion.\n- Prevent the silting up of existing storm drains in the immediate area by constructing erosion and silt fence enclosures around areas to be excavated.\n- Prevent the siltation of existing nearby streams or waterways by installing erosion and silt fencing around those streams adjacent to areas to be excavated.\n- Protect topsoil piles for reuse. (Topsoil piles are generated early in the construction process as soil is stripped during rough grading operations; respreading is one of the last operations, commencing as landscaping is put in place.)\nThe site utilities should reduce soil erosion during excavation of trenches and contain storm water runoff:\n- Plan infiltration swales and basins during trenching operations to contain surface water.\n- Retain or recharge existing water tables by minimizing site disturbances; leave as many trees as possible; use existing vegetation and retain natural contours.\n- Consider a design to store roof runoff when the building has been completed; it can be used as gray water or reclaimed wastewater.\n- Investigate a small onsite, state-of-the-art treatment plant to recycle reclaimed water.\nAn open-space and landscaping program can accommodate the following:\n- Protect trees during construction; they enhance property values and lower cooling loads.\n- Consider indigenous landscaping; it supports natural wildlife and plantings and lowers the level of irrigation as well as the need to fertilize and apply chemical treatment.\n- Minimize pesticide use by installing weed cloth; use mulches and planting species that create dense planting beds.\nSite -circulation and transportation programs should meet these objectives:\n- Encourage carpooling.\n- Provide areas for people to store bicycles during working hours.\n- Encourage the use of public transportation by instituting a program of incentives.', ""Rain and melting snow create runoff as they move across the landscape to lakes, streams, rivers, and wetlands. In urban areas, most stormwater falls on streets, sidewalks, parking lots, roofs, and other hard surfaces and carries pesticides, fertilizers, oils, metals, salt, bacteria, litter, yard waste, sediment, and other materials down storm drains and into lakes and streams. Unlike wastewater, most stormwater is not treated before it empties into nearby bodies of water. Not surprisingly, stormwater runoff is a leading source of water pollution.\nIn addition to the contaminants it carries, stormwater runoff from hard, urban surfaces travels faster and in large quantities, the force of which damages rivers, streams and wetlands and can destroy habitats for fish and other aquatic life. Impervious surfaces also prevent stormwater from filtering into the ground and recharging groundwater.\nA public entity's stormwater system (ditches, stormwater ponds, curbs and gutters, storm drains, catch basins, storm sewer pipes, and so on) is called a municipal separate storm sewer system (MS4). This does not include systems where wastewater and stormwater are combined. The MPCA regulates MS4 systems as mandated by the federal Clean Water Act with the goal of improving water quality by reducing pollutants in stormwater discharges. MS4 regulations require that owners/operators educate citizens on stormwater management issues and include them in solving stormwater problems, make plans to detect and stop illicit discharges, control runoff on construction sites, and more.\nCurrent challenges in municipal stormwater\nIn most towns and cities, there's a storm drain on almost every street corner. That's a lot of openings to the storm sewer—a direct path to lakes and streams. So keeping contaminants out of stormwater requires effort from everybody: keeping leaves and grass clippings away from curbs, using phosphorus-free lawn fertilizer, picking up litter when you see it, and directing your rain gutters onto your lawn or into a rain barrel. Some of the larger challenges include:\nChloride (salt). The salt applied to roads, parking lots, and sidewalks during our icy winters contains chloride, a water pollutant. When snow and ice melts, the salt goes with it, washing into storm drains and then lakes and rivers. It takes only one teaspoon of road salt to permanently pollute five gallons of water. Once in the water, there is no way to remove the chloride, and at high concentrations, chloride can harm fish and plant life.\nLawn care practices. Homeowners use up to 10 times more chemical pesticides per acre on their yards than farmers use on their crops, according to the U.S. Fish and Wildlife Service. A lot of that fertilizer will end up in stormwater. Unraked leaves get into the gutter. Patios and driveways add to the hard surfaces over which stormwater runs, rather than soaking into the ground. Better lawn care practices keep contaminants out of stormwater.\nFacilities in specific industries, that store materials, waste, or equipment outdoors, are subject to industrial stormwater regulations administered by the MPCA. These facilities must take steps to monitor and manage stormwater on their properties where stormwater may come into contact with harmful pollutants including toxic metals, oil, grease, de-icing salts, and other chemicals. Industries subject to stormwater regulation include:\n- Lumber and wood companies, paper mills, chemical and petroleum businesses, concrete companies, foundries, and other metals operations\n- Hazardous waste treatment, storage, or disposal facilities\n- Landfills that receive industrial waste\n- Recyclers and salvage yards\n- Steam electric power generating facilities\n- Facilities for vehicle maintenance, equipment cleaning, and airport deicing operations\n- Sewage treatment works\n- Light manufacturing facilities\nWhen stormwater drains off a construction site, it carries sediment and other pollutants that can harm lakes, streams, and wetlands. The U.S. Environmental Protection Agency estimates that 20 to 150 tons of soil per acre are lost every year to stormwater runoff from construction sites.\nTo protect Minnesota waters, the MPCA issues permits to construction site owners and operators to prevent stormwater pollution during and after construction. As part of the permit application, site owners/operators must create a stormwater pollution prevention plan that explains how they will control stormwater. The agency oversees an average of more than 2,000 construction sites each year.\nCurrent challenges in construction stormwater\nGiven the thousands of construction sites operating around the state at any time, it's a challenge to stay on top of violations. The top five construction stormwater permit violations:\nMissing or inadequate soil stabilization. Mats, mulches, blankets, and other strategies keep disturbed soils in place during construction.\nMissing silt fence or other perimeter controls. Perimeter controls such as silt fence and biorolls prevent runoff and allow water through while sediment settles out.\nMissing or inadequate inlet protection. Inlet protection captures sediment before it enters storm sewers.\nVehicle tracking. Vehicles driving off the site can track sediment onto paved surfaces. Rock pads and similar practices knock dirt off tires before it can be carried off the construction site.\nStormwater protection practices not maintained. For instance, it's not enough to just install a silt fence. It must be checked to ensure it hasn't been breached or knocked down.""]"	['<urn:uuid:87dc776d-1577-43c5-924a-15b33db12340>', '<urn:uuid:475e7920-dc72-4e27-99a0-0acf3c44dcf9>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:18:19.158519	6	89	1630
68	i need equipment to make sanitizer and water what pump makes both hand sanitizer and moves water in buildings	Both AODD pumps and Booster pumps can handle these applications. AODD pumps can pump virtually any liquid safely and efficiently, including hand sanitizer, while Booster pumps are commonly used in low-contamination water systems to move water through buildings by increasing fluid flow and pressure.	"[""The COVID-19 pandemic is the largest global health crisis the world has faced in over a century. The only solution to this crisis is working together.\nWhile frontline medical workers treat patients and essential workers keep store shelves stocked, some companies are carving out a role for themselves to help flatten the curve and keep people safe.\nUntil recently, many facilities that use air-operated double-diaphragm (AODD) pumps were producing items like pharmaceuticals, personal care products and different types of food and beverages. Now, some of those companies have repurposed their pumps to produce hand sanitizer. If your company has the ability to switch its facilities to produce sanitizer, you need some guidelines.\nIf your company is looking for a way to help with the crisis and has the ability to switch its facilities to produce sanitizer, you need some guidelines.\nHere are 10 things to consider when you’re switching your AODD pumps to hand sanitizer production:\nBefore switching to a new process, you need to make sure the unit is clean. The best way to do this is to clean the surfaces of the pump’s wetted side completely.\nRemember: Outer surfaces of the pump will see the environment while the inner workings of the pump will be exposed to the product.\n2. Erratic Operation\nIf your pump isn't working properly, check to make sure:\n- The pump is close enough to the source\n- The pump is close enough to the discharge\n- The pilot valve is running correctly\n- The product being pumped is thin enough—if not, turn the air pressure up\n3. Check Ball Chatter\nIf you’re hearing noise from the check area, slow the unit down or add head by slowly closing a valve until the noise stops.\n4. Open Flaps\nIf the flaps keep sticking open, use a ball valve.\nRemember: Flaps are designed to move solids and ball valves are designed to move liquids and slurries.\nIf the pump’s diaphragms aren’t working properly, you probably have the wrong elastomer for the job. Check the pump manufacturer’s chemical compatibility guide.\nTip: Torque the diaphragm plate during assembly. Not torquing could cause leaks.\n6. Preventative Maintenance\nDevelop a preventative maintenance plan that includes regular service rather than waiting for a failure. For quick and easy maintenance, check with your pump manufacturer or local distributor for complete air side and wet side repair kits.\n7. Optimize Performance and Efficiency\nYour pump size and settings may not be appropriate for all liquids being pumped. Make sure your pump is sized correctly for your application requirements.\nTip: If you choose a pump one size larger than what you need, it won’t have to work so hard. Consequently, it will pump more efficiently and its life span will likely be lengthened.\n8. Choose the Proper Pump\nThough AODD pumps are very versatile, they are not necessarily “one-size-fits-all” when it comes to applications. Choosing the proper pump can solve common problems. As a general rule:\n- If you’re pumping slurries (like hand sanitizer with gel agents), choose a heavy-duty ball valve pump.\n- If you’re pumping solids, choose a heavy-duty flap valve pump.\n- If you need to contain harsh chemicals, choose a containment-duty ball valve pump.\n- If you're pumping hand sanitizer of any kind, the pump must be FDA-compliant.\n9. Proper Application Setup\nWhen you’re producing hand sanitizer, it may be beneficial to add some accessories to your pump:\n- Pulsation dampeners: All AODD pumps create pulsations of product fluid as they operate. Pulsation dampeners are easily installed tools for a more consistent flow.\n- Filter regulators: AODD pumps are powered by air, which can be warm, wet, dirty and, often, unregulated when leaving an air compressor. The combination of dirty and unregulated air can damage and/or shorten the pump’s life. To alleviate this issue, filter regulators remove solid and liquid contaminants while controlling the air flow to increase the service life of AODD pumps and other air system components.\nTip: To make sure your pump can handle the liquids used to produce hand sanitizer, consult your pump manufacturer’s chemical compatibility guide.\n10. Proper Routine Cleaning/Sanitization\nMake sure you’re following the proper protocols for your application, whether cleaning and sanitization take place after every shift or in 24-hour intervals. With few working parts, AODD pumps are easy to clean. Some manufacturers offer exclusive designs that allow for in-line cleaning. Check your manufacturer’s guidelines.\nTip: Some pump components can’t handle high temperatures or pressure washing. Check with your manufacturer or distributor to make sure you're using the proper cleaning/sanitization methods. And make sure to always check the pump manufacturer's chemical compatibility guide to ensure the cleaning materials are compatible with the materials of the unit.\nUsing AODD Pumps to Help With Hand Sanitizer Production\nAODD pumps can pump virtually any liquid safely and efficiently.\nIf you've decided to rely on the versatility of your AODD pump to produce hand sanitizer, make sure your pump can handle the application. Follow the manufacturer's guidelines for proper use and industry standards for appropriate cleaning and sanitization.\nIf you have a SANDPIPER pump, check the chemical compatibility guide and consult SANDPIPER’s experienced engineers and after-sales support team for guidance and troubleshooting tips. Or, feel free to leave your question as a comment below."", 'The Booster pump is a centrifugal pump since it uses centrifugal force and one or more impellers to pump fluid. This pump is a mechanical device that increases fluid flow and pressure. A boost pump is also known as a pressure pump. It provides the additional kick you need to raise your fluid pressure to the desired level. These pumps can handle both gases and liquids, with different designs depending on the fluid. Boosters may convey high-pressure gas, fill cylinders, increase gas pressure, and remove gas from the system.\nIn low-pollution water systems and services, booster pumps are often employed. Boost pumps are used in a variety of military, municipal, and commercial applications. These pumps are utilized in a variety of sectors, including food processing and mining. A booster pump is a device that connects the water source to endpoints such as taps, faucets, and showers. A booster pump draws water from a source and compresses it to the necessary pressure at the ends.\nBooster pumps aren’t the only equipment that transports water around in a system. To boost fluid pressure, they act in tandem with a source pump. Booster pumps come in very handy in a variety of situations. Moving water through a high-rise structure or uphill, for example, necessitates a high-pressure head, which these pumps can provide. Sprinklers and high-pressure spraying systems may also require a bump in the pressure head since they operate on high water pressure.\nA Booster pump boosts the flow of water from your faucet or shower head by increasing the pressure and volume. It’s a pain to live with low water pressure. You know what I’m talking about if you’ve ever tried to shower under a trickle of water and had to move in circles simply to get wet. Low water pressure may make even simple things like showering and brushing your teeth difficult, but a booster pump could be the answer.\nA Booster pump boosts the flow and pressure of low-pressure water. It gives you the additional push you need to get your water pressure to where you want it. A water booster pump creates enough pressure to carry water from a storage tank or throughout a home or business.\nBooster pumps are used to maintain optimal water pressure in tall or high-set structures, as well as in places where water pressure is low or fluctuates. The pump takes water from the water supply line when the pressure in the tank declines, raising the pressure in the tank and increasing the water pressure in the building. A pump, motor, pressure tank, and controller make up a conventional booster pump system.\nHOW BOOSTER PUMP WORKS\nA Booster pump functions in the same way as a fan does. It has an impeller that increases water flow and pressure in the same way that a fan’s blades increase air velocity. In some cases, a booster pump’s objective is to raise the fluid’s pressure while also raising the flow rate. This pump is made to work in conjunction with an existing pump.\nA Booster pump raises water pressure by pumping water through hydraulic lines at a quicker rate. However, keep in mind the pump’s characteristics as the pressure required for water movement increases, the flow velocity drops. Consider putting your thumb through a yard hose. The water flows out at a higher pressure, but the flow rate is limited by the thumb.\nBooster pumps function in a similar manner. They deliver the most water at the lowest pressure at the maximum flow rate. When water flows freely from the pump, it moves at a faster rate. A booster pump’s impeller pushes water that enters via the inlet and departs through the output. A motor drives the impellers. The way booster pumps take in and put out water is different.\nA spinning propeller is used in certain booster pumps, whereas an oscillating diaphragm is used in others. In pumps with oscillating diaphragms, water is pushed by two oscillating or revolving plates, one with cups and the other with indentations. The cups are crushed as the plates contact, pushing the water out.\nAs the plates open, more water is drawn in. The casing or body of a pressure pump is used to protect water from splashing outside the pump. If the pump is subjected to a heavy load, it protects the internal components. The body of the booster pump is composed of stainless steel. The impeller is a rotating component of the booster pump.\nThere are a lot of fixed blades on it. This impeller rotates the water axially and radially using centrifugal force. The impeller of the pump has these blades. These spin in lockstep with the impeller. The primary function of the impeller blades is to enhance the speed of water or other working fluids. An input valve allows water to flow into the pump. The outlet valve, on the other hand, allows the pressurized water to escape.\nAPPLICATIONS OF BOOSTER PUMP\nBooster Pumps are utilized when the usual system pressure is low and has to be raised. Priming is the process of introducing fluid into the pumping chamber in order to establish the pressure differential required for pumping at rated capacity. Self-priming pumps generate and maintain enough vacuum to pull fluid into an intake without the need for external help.\nBooster pumps are typically employed in low-contamination water systems or applications. They are utilized in a number of sectors, including aerospace, mining, and food processing, and have a variety of commercial, municipal, and military uses.\nBooster pumps are capable of transporting a variety of materials. Abrasive, acidic, caustic, combustible, or dangerous compounds are used in some booster pumps. Others employ wastewater, salt water, sewage, sludge, ash, or food-processing rendering wastes. To raise system pressure, liquids, liquids containing solids (slurry), and liquid metal are also utilized. Other media include gasoline, diesel fuel, oil, lubricants, chemicals, and coolants, depending on the use.\nAluminum, Brass, Bronze, Cast iron, Plastic, and Stainless steel are all common materials for booster pumps. AC or DC voltage; Pneumatic or Hydraulic systems; Gasoline, Diesel fuel, or Natural gas; steam or water; and solar power are all examples of power sources. Depending on the orientation of the media, booster pumps include a Pump Stator / Rotor assembly that can be positioned vertically or horizontally. The pump end of a close-coupled pump is mounted on the motor shaft. Frame-mounted pumps have the pump end positioned on a bearing frame.\nBooster pumps carry water from a lake, pond, or storage tank for usage in a house or business facility by increasing the low flow of water in water systems or industrial equipment. A pump would be required for a residence that does not receive enough water pressure from the water source. To deliver the water to the top of history, the hotel requires a massive commercial booster pump.\nThe Booster pump additionally recompresses the water in the tank before delivering it to your faucet or residence. Water gathers in the tank in a rain collection system, for example. You must pump water from the tank into your home to utilize it for flushing toilets or washing linens. Booster pumps come with a number of extra functions. Some come with thermal overload protection or are designed to run indefinitely. Others are self-priming or can function for long periods of time without the use of pumped fluid or external lubrication.\nMulti-stage pumps transport compressed fluid from an initial stage to subsequent chambers or stages of pressurization, allowing for higher pressure levels than single-stage pumping. A pressure gauge is usually found on both single-stage and multi-stage pumps. Some booster pumps are made to work in specific conditions. Explosion-proof pumps, for example, contain elements that might ignite the transfer medium or the surrounding environment.\nIn collaboration with Dickow Pumps, Kiron Hydraulic Needs Private Limited offers a full solution for your Booster pump system requirements. These pumps are made in accordance with industry standards to guarantee that they work well. We have a significant presence not only in the country but all over the world as a consequence of our unwavering attitude toward our business and devoted approach to our clients.']"	['<urn:uuid:078b7aa9-36d4-46b1-9646-3e602c290bd1>', '<urn:uuid:f0836595-993a-42d7-8fae-44accccd8792>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T05:18:19.158519	19	44	2238
69	I repair equipment - which needs more frequent lubrication: boat dock chains or bike chains?	Boat dock chains need regular lubrication to prevent rust, while bike chains only need relubrication every 50 miles (with olive oil) or longer with specialized lubes.	"[""Looking for 10 boat dock repair tips? Boat docks, like the boats they house, need annual repairs to keep them in working condition. Many people unfortunately let their old boat docks go, and they become a sunfish's favorite spot to hide. If you’re not ready to let the lake claim your boat dock, then here are some repair tips to keep your dock in working condition.\n- Use treated lumber. If your boat dock wasn’t constructed using treated lumber, this is an excellent place to begin making repairs. Make sure to remove all un-treated lumber and replace it with water-treated lumber. This will extend the life of your dock and remove the need for major repairs down the road.\n- Seal your wood. To repair your boat dock, you will need to keep it regularly sealed with deck sealer. Make sure that you seal your dock at least once a year. If you can, it’s a great idea to remove your dock from the water, let it thoroughly dry, then seal it.\n- Check wooden platforms. If your boat dock is set on wooden platforms, you should definitely check these out. You won’t need scuba diving gear to dive under the water and inspect these, though! Just grab a pair of goggles and check each platform with your hands. If you suspect an issue, make a note of which platform is giving you problems so you can replace it.\n- Repair wooden platforms. If you don’t have the manpower to replace a platform on your boat dock, you may look at the option of repairing it. Marine putty can be used to repair holes in the bottom of boats, and works extremely well in patching holes in your platforms. If your platform is beyond repair though, you may need to look at completely replacing it.\n- Replace old platforms. If you have an issue with one of the platforms which your boat dock rests on, you may have to replace it. Replacing a platform may seem like a major task at first glance, but it’s actually fairly simple. You can begin by installing your new platform next to where your old one is. Next, simply remove the old platform with a saw. You should do these steps in this order because if you simply remove the old one first, then your boat dock may shift.\n- Perform maintenance on metal platforms. Some boat docks, especially older ones, use metal pipes for platforms. To maintain the life of these platforms, you should regularly treat them with a rust-resistance substance, such as a commercial lubricant. If you run into problems, such as holes or rusting, you can remove these older areas and weld a new piece of metal to these rough spots.\n- Check any barrels. If your boat dock uses barrels as a support system, make sure to check each of these barrels to see how buoyant it is. Barrels are fairly easy to replace. It is likely not worth trying to repair them, since water has a tendency to seek out places where it’s not allowed!\n- Treat any chains. If you are using chains to connect your barrels or keep your boat dock in place, it’s a good idea to regularly treat them with lubricant. This will keep rust off of them and extend their life span. You may also need to do some minor welding to repair old, rusted chains.\n- Check the screws. If you have a loose plank and can’t seem to figure out the problem, you may have a missing screw. Screws or other small products are usually the first things to go on your boat dock. Make sure that you check all of your screws, including those on the sides, top and bottom of your boat dock. If you suspect any issues with your screws, don’t be afraid to change them!\n- Repair worn-out rope. Many boat docks also use rope to keep the boat tied up. If you have a frayed line, this is a quick fix. Take a soldering iron or another heat source and warm up these frayed ends. This should melt them back together and keep your ropes working!\nWhat Others Are Reading Right Now.\nActing, comedy and strong spirits converge in Speakeasy. When host Paul F. Tompkins interviews entertainers—Key and Peele, Alison Brie, Rob Delaney, Zach Galifianakis—about all sor …\n10 Things Women Expect Men to Know How To Do\nTo make ladies swoon or at least not cringe, you’d better be able to handle the following…\nWe all love fine food—and the people who make it! Eats introduces you to those folks, taking you into the kitchens of all kinds of culinary luminaries. From BBQ to vegan, eco-frien …"", ""Your frame and wheels might get all the attention on the weekly group ride, but the true hero of your bike is the chain. Here’s all you need to know to keep it running smoothly, plus five reasons you should keep it in top shape.\n1. It’ll Make You Faster\nIf you’re getting edged at the town-limits sign on your weekly group ride or coming up just short at the finish of your local crit, don’t reach for the EPO. There’s a cheaper, more ethical solution.\nA clean, properly lubed chain will save about 10 watts over a poorly maintained chain, according to Jason Smith of Friction Facts, a Colorado-based research firm. For the average rider, that accounts for about 4 percent of lost power. If you’re only racing to the midride coffee stop, think of it this way: You’ll finish the ride fresher, having exerted less energy, if you lube the chain.\n2. You’ll Save Money\nA clean chain won’t just prevent you from wasting energy. It’ll also save you from wasting money. Dirty chains can damage drivetrain components as the dirt and grime slowly wear away cogs and chainrings.\n“Would you rather pay $30 for a new chain now or $400 for a new drivetrain later?” asks Eric Fostvedt, mechanic for the professional Axeon Cycling Team.\n3. It’s Easy\nEvery mechanic and rider has his or her favorite way to clean a chain. Park Tool master mechanic Calvin Jones uses, of course, Park Tool’s Chain Gang filled with degreaser.\nIf you don’t want to buy a special piece of equipment, try using a thick-bristled paintbrush to coat the chain and cogs with degreaser. Richard Belson, a mechanics instructor at the United Bicycle Institute, suggests dousing the chain in a self-cleaning lube, such as White Lightning Clean Ride. Just be sure to wipe off all the excess lubricant with a rag afterward. Too much lube can be a bad thing, especially when it’s picking up road grime.\n4. You Don’t Have to Do It (That) Often\nMost experts agree you should replace your chain after 2,000 to 3,000 miles of riding. The easiest way to see if you need a replacement: Use a chain checker, such as this one from Park Tool.\nBarring that, break out your tape measure. On a new chain, you should be able to measure exactly 12 inches between two chain pins. If the distance measures as little as an eighth of an inch more than that, replace the chain. You’ll be amazed at how much smoother your pedal stroke will get.\n5. You Can Get Creative with Lube\nPro tip: Some of the most effective bike-chain lubricants aren’t actually bike-chain lubricants. Plain old food-grade paraffin canning wax is a great way to keep your chain rolling fast and smooth, but it requires taking the chain off your bike and dipping it into the melted wax. The effect lasts quite a while, but few riders want to deal with the hassle.\nAnother excellent but unorthodox product is olive oil, says Smith. On the downside, it’ll last only about 50 miles or so. If you’re riding a century, be sure to save some of your salad dressing from the lunch stop for the ride home.\nIf a road rider wants a fast, simple liquid lube for clean conditions, Smith recommends Squirt, Rock ’n’ Roll Gold, or Silca NFS Pro. For mountain biking and cyclocross riding in wet or muddy conditions, try a thicker lube, such as Finish Line Wet, White Lightning Wet, or Phil Wood Tenacious Oil. Smith suggests keeping three to four different types of lube in your tool kit so you’ll be ready for nearly every weather and road condition you encounter.\nSubscribe to Outside\nSave 72% and Get the Special Women's Issue!""]"	['<urn:uuid:ca8aaa9b-b871-432d-931f-e36b7197c27a>', '<urn:uuid:0c87edc1-90ac-4398-848f-328e1d212694>']	factoid	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T05:18:19.158519	15	26	1420
70	What methods are being used to authenticate and preserve ancient texts, and what disagreements exist between institutions and scientists about the proper handling of these historical documents?	Various methods are being employed to authenticate and preserve ancient texts. Archaeological discoveries, such as tablets and prisms found in places like Babylon and Sippar, help verify historical accuracy through physical evidence. For example, the discovery of tablets mentioning officials like Nebo-Sarsekim provides direct confirmation of historical records. Regarding preservation, institutions like the Israel Antiquities Authority collaborate with organizations such as the Library of Congress and the Getty Conservation Institute, using techniques like Japanese tissue paper with methyl cellulose for reinforcement. However, significant disagreements exist between institutions and scientists about proper preservation methods. Scientists from the German Federal Institute for Materials Research and Testing argue that current exhibition practices are causing accelerated degradation of documents like the Dead Sea Scrolls. They criticize the lack of a sophisticated monitoring system and claim that some conservation methods, particularly the use of methyl cellulose adhesive, may be causing irreversible damage. These scientists advocate for halting exhibitions until better monitoring systems are established, while institutions defend their practices as meeting high conservation standards.	['This article was first published in the Summer 2007 issue of Bible and Spade.\nNebo who? You mean you don’t remember Nebo-Sarsekim? No wonder, because if you consult your concordance, you will find that he is referred to but once in the Old Testament. Nebo-Sarsekim was a high Babylonian official named in Jeremiah 39:3. The mention of this individual in the Hebrew Bible is yet another example of an obscure “factoid” which demonstrates the historical accuracy and eyewitness nature of the Biblical record.\nThe time was the ninth day of the fourth month of the 11th year of the reign of Zedekiah (Jer 39:2), i.e., July 18, 587 BC. The place was Jerusalem. The event was the fall of Jerusalem and the Southern Kingdom of Judah to the Babylonians under Nebuchadnezzar after a siege of two and a half years, a very sad time in the history of God’s people. After the city wall was broken through,all the officials of the king of Babylon came and took seats in the Middle Gate: Nergal-Sharezer of Samgar, Nebo-Sarsekim a chief officer, Nergal-Sharezer a high official and all the other officials of the king of Babylon (Jer 39:3).\nThe tablet mentioning Nebo-Sarsekim was found in Sippar, an ancient Babylonian city 20 mi (32 km) southwest of modern Baghdad and 35 mi (57 km) north of Babylon. In the late 19th century, tens of thousands of cuneiform tablets were recovered from the site and brought to the British Museum (Gasche and Janssen 1997). Later, in 1920, the Nebo-Sarsekim tablet, only 2.13 in (5.5 cm) wide, from the same site, was acquired by the museum. Credit: Ian Jones/Telegraph\nMichael Jursa, associate professor at the University of Vienna, made the discovery. Since 1991, he has been sifting through the approximately 130,000 inscribed tablets at the British Museum to ferret out data on Babylonian officials. On July 5, 2007, just another day in the tablet room, Jursa made the find of a lifetime when he discovered the Biblical name (Reynolds 2007). The tablet is so well preserved that it took him only minutes to decipher (Alberge 2007). It was a mundane receipt acknowledging Nebo-Sarsekim’s payment of 1.7 lb (0.75 kg) of gold to a temple in Babylon. Dated to the tenth year of Nebuchadnezzar (595 BC), eight years before the fall of Jerusalem, the tablet reads in full:\n[Regarding] 1.5 minas [0.75 kg] of gold, the property of Nabu-sharrussu-ukin, the chief eunuch, which he sent via Arad-Banitu the eunuch to [the temple] Esangila: Arad-Banitu has delivered [it] to Esangila. In the presence of Bel-usat, son of Alpaya, the royal bodyguard, [and of ] Nadin, son of Marduk-zer-ibni, Month XI, day 18, year 10 [of] Nebuchadnezzar, king of Babylon (Reynolds 2007).\nThe Hebrew spelling of the name is slightly different from the cuneiform, but there is no question that it is the same person. Although the NIV translates Nebo-Sarsekim’s title as “chief officer,” the literal translation is “chief eunuch,” exactly the same as in the tablet.\nReflecting on his discovery, Jursa commented, “It is very exciting and very surprising. Finding something like this tablet, where we see a person named in the Bible making an everyday payment to the temple in Babylon and quoting the exact date is quite extraordinary” (Alberge 2007). Irving Finkel, Assistant Keeper in the Department of the Middle East at the British Museum, stated,\nThis is a fantastic discovery, a world-class find. If Nebo-Sarsekim existed, which other lesser figures in the Old Testament existed? A throwaway detail in the Old Testament turns out to be accurate and true. I think that it means that the whole of the narrative [of Jeremiah] takes on a new kind of power” (Reynolds 2007).\nThis is not the first archaeological discovery relating to the fall of Jerusalem. Nebuchadnezzar, king of Babylon at the time and leader of the invading force, is well known to us. His palace has been excavated in Babylon and many of his records recovered (Wiseman 1985; Klengel-Brandt 1997: 252–54). One such record, the Babylonian Chronicle for the years 605–595 BC, describes the fall of Jerusalem to Nebuchadnezzar in 597 BC, the so-called “second deportation,” the first being in 605 BC when Daniel and his companions were taken to Babylon (Dan 1:1–5). The tablet was found in Babylon and purchased by the British Museum in the 19th century. The section pertaining to the fall of Jerusalem reads,\nYear 7 [597 BC] in Kislev the king of Babylonia [Nebuchadnezzar] called out his army and marched to Hattu [the west]. He set his camp against the city of Judah and on the second Adar [March 16] he took the city and captured the king [Jehoiachin]. He appointed a king of his choosing there [Zedekiah], took heavy tribute and returned to Babylon (Millard 1997: 468).\nBabylonian Chronicle for the year 605–595 BC. First published by Donald J. Wiseman in 1956, it records the last (21st) year of the reign of Nabopolassar and the first 11 years of his son Nebuchadnezzar. Among Nebuchadnezzar’s accomplishments was the capture of Jerusalem, dated precisely to March 16, 597 BC. The document is on display in the British Museum, London. Credit: Michael Luddeni\nThe Bible describes the same events in some detail. When Nebuchadnezzar besieged and captured Jerusalem in 597 BC, Jehoiachin was on the throne. He took Jehoiachin, the royal family and important men in the kingdom to Babylon. He then placed Jehoiachin’s uncle, Mattaniah, on Judah’s throne and changed his name to Zedekiah (2 Kgs 24:11–17). Jehoiachin was a young man of 18 when he became king of Judah. He reigned but three months before being carried off to Babylon, where he lived out the rest of his days (2 Kgs 24:8, 12, 15; 25:27–30). Four tablets found in Nebuchadnezzar’s palace name Jehoiachin and his family as among those who were receiving rations from the king (Weidner 1939; Wiseman 1985: 81–82).\nRation record from Babylon mentioning Jehoiachin. During Robert Koldeway’s excavations at Babylon at the turn of the 20th century, he discovered what archaeologists call the “Northern Palace,” most likely the royal residence of King Nebuchadnezzar. Koldeway found there a number of cuneiform-inscribed clay tablets dating to the years 594–569 BC. They list kings captured from throughout the ancient Near East who were living in the palace and receiving rations of grain and oil from the king. Four of the tablets list rations for “Jehoiachin, king of Judah” and his family. These tablets are today in the Pergamum Museum, Berlin. Credit: Walter Pasedag\nIn his campaign against Jerusalem in 589–587 BC, Nebuchadnezzar laid waste all the fortified cities of Judah. While Jerusalem was being besieged, Jeremiah delivered a message to king Zedekiah concerning the fate that awaited him (Jer 34:1–7). The passage ends with the statement that the only fortified cities that were still holding out, apart from Jerusalem, were Lachish and Azekah (Jer 34:7). A group of 21 letters found at the site of Lachish date from this time period. They appear to be military communiqués sent to the military commander at Lachish from another outpost. The final words of Letter 4 echo those of Jeremiah 34:7: “we are watching the [fire] signals of Lachish…for we cannot see Azekah” (Pardee 2002: 80).\nLachish Letters 1 (left) and 2 (right), on display in the British Museum, London. A British expedition to Lachish under the direction of James Starkey in the 1930s discovered 21 letters in a chamber of the city gate written on flat, broken pieces of pottery (ostraca). The stratum where they were found and the shape of the script indicates that they date to the time of the Babylonian invasion of Judah, 589–587 BC. Their content reflects the turmoil that was taking place in Judah at that time. Credit: Michael Luddeni\nAnother official named in Jeremiah 39:3 is also known from cuneiform sources as Nergal-Sharezer. Jeremiah 39:13–14 goes on to say that he was instrumental in releasing Jeremiah from the Courtyard of the Guard where he was being held as prisoner. Nergal-Sharezer was married to Nebuchadnezzar’s daughter Kashshaia and later became king of Babylon (559–556 BC), the Neriglissar of the classical sources (Wiseman 1985: 10–12; Leick 1999: 122).\nIn addition to Nebo-Sarsekim, Nebuchadnezzar and Nergal-Sharezer, the name of yet another Babylonian official has turned up in a Babylonian text. His name is Nebuzaradan. He played a significant role in the events of 587 BC. Nebuzaradan was a high-ranking military official, called “Captain of the Guard” in the Biblical text, perhaps reporting directly to Nebuchadnezzar. He was responsible for supervising the burning of the city (2 Kgs 25:8–9; Jer 39:8a 52:12–13), tearing down the defenses (2 Kgs 25:10; Jer 39:8b 52:14), deporting 832 captives to Babylon (2 Kgs 25:11; Jer 39:9; 52:15, 29), plundering the Temple (2 Kgs 25:15; Jer 52:17–19), and rounding up Judean officials to appear before Nebuchadnezzar (2 Kgs 25:18–21; Jer 52:24–27).\nNebuchadnezzar gave Nebuzaradan specific instructions to deal kindly with Jeremiah (Jer 39:11–12). He released Jeremiah and gave him a choice of either going to Babylon or remaining in Judah (Jer 40:2–4). Jeremiah chose to remain in Judah, and joined the newly appointed Judean leader Gedaliah at Mizpah (Jer 39:14; 40:5–6). Five years later, Nebuzaradan returned to Jerusalem and deported another 745 people to Babylon (Jer 52:30). A clay prism found in Nebuchadnezzar’s palace, dating to ca. 570 BC, contains a list of court officials. Among them is Nebuzaradan, with the title “Chancellor” or “Chief Baker” (Wiseman 1985: 73–75).\nWe sometimes wonder why such mundane details as the names of Babylonian officials are recoded in God’s word. Yet, it is these seemingly insignificant bits of information that demonstrate the historical accuracy of the Biblical text and how that accuracy has been preserved through the centuries.\nRecommended Resources for Further Study\n2007 Museum’s tablet lends new weight to Biblical truth. Times Newspapers, http://www.timesonline.co.uk/tol/comment/faith/article2056362.ece (accessed July 21, 2007).\nGasche, Hermann, and Janssen, Caroline\n1997 Sippar. Pp. 47–49 in The Oxford Encyclopedia of Archaeology in the Near East 5, ed. Eric M. Meyers. New York: Oxford University.\n1997 Babylon. Pp. 251–56 in The Oxford Encyclopedia of Archaeology in the Near East 1, ed. Eric M. Meyers. New York: Oxford University.\n1999 Who’s Who in the Ancient Near East. New York: Routledge.\n1997 The Babylonian Chronicle (1.137). Pp. 467–68 in The Context of Scripture 1, ed. William W. Hallo. Leiden, The Netherlands: Brill.\n2002 Lachish Ostraca (3.42). Pp. 78–81 in The Context of Scripture 3, ed. William W. Hallo. Leiden, The Netherlands: Brill.\n2007 Tiny tablet provides proof for Old Testament. Telegraph Media Group, http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2007/07/11/ntablet111.xml (accessed July 21, 2007).\nWeidner, Ernst F.\n1939 Jojachin, König von Juda, in Babylonischen Keilschrifttexten. Pp. 923–35 in Mélanges Syriens Offerts à Monsieur René Dussaud 2. Bibliothèque Archéologique et Historique 30. Paris: Librairie Orientaliste Paul Geuthner.\nWiseman, Donald J.\n1985 Nebuchadrezzar and Babylon. Oxford, England: Oxford University.\nPlease help ABR continue to post these free articles by making a donation of any size today. Donate', 'Dead Sea Scrolls – Scientists In Berlin Criticize Israeli Cultural Authorities For Treatment Of Sacred Documents\nLast week, a peer-reviewed journal called the Restaurator published a controversial article about the Dead Sea Scrolls written by two Berlin-based scientists who charge that these sacred documents are not receiving proper care from the Israeli cultural institutions responsible for their well-being.\nThe article’s abstract does not mince words:\n“Examination of the properties of the scrolls proves that frequent travel, exhibitions and the associated handling induce collagen deterioration that is covered up by the absence of a proper monitoring program.”\n“I want the scrolls to be protected,” says Ira Rabin, who co-authored the piece entitled “Dead Sea Scrolls Exhibitions Around The World: Reasons For Concern” with her colleague Oliver Hahn at the German Federal Institute for Materials Research and Testing.\nThe 20-page document specifically criticizes the Israel Antiquities Authority and the Israel Museum in Jerusalem, who hold responsibility for a majority of the Dead Sea Scrolls. Both defend their treatment of the scrolls (detailed below).\nBut first, the criticisms. Rabin and Hahn argue in the Restaurator that:\n1. The Dead Sea Scrolls are being exhibited far too much, and that the consequent travel and handling is seriously accelerating their degradation. The authors show that there’s been a substantial increase in international exhibitions in the past two decades.\n2. No scientifically sophisticated system is yet in place to monitor the degradation state of the Dead Sea Scrolls. The authors argue that all exhibitions should be stopped until a rigorous analytical monitoring system is established and can prove that the frequent traveling is not unduly exacerbating the fragile state of these documents.\n3. Current strategies to conserve the scrolls may, in some cases, be worsening their state. In particular the authors take issue with the reinforcement of the scrolls with Japanese tissue paper. This process requires the use of an adhesive called methyl cellulose.\nRabin and Hahn argue that use of the adhesive brings the scrolls in contact with unnecessary moisture. They say this wetness is causing collagen in the animal skin parchment to unravel irreversibly from its triple helix formation into an amorphous gelatin. They include an image of this gelatinization to make their point.\nThe article’s first author, Rabin, worked as a consultant at the Israel Antiquities Authority (IAA) from 2005-2006. From multiple accounts, Rabin and the IAA did not part on congenial terms. In 2007, Rabin received a letter from the IAA’s lawyers (seen by Artful Science) threatening legal action if she presented work from her time at the IAA at a conference or in a publication, based on an employment contract she had once signed.\nAs such, the criticism in the current Restaurator article is based on publically available information, the authors’ research on Dead Sea Scrolls from the Israel Museum in Jerusalem (some of which underwent conservation treatments at the IAA), and the authors’ research on a Dead Sea Scroll collection housed at the University of Manchester. The Manchester collection contains small fragments of the scrolls that don’t go on tour and are primarily used for research purposes, Rabin says.\nIn the past year, Rabin and Hahn have also published two scientific papers (here and here) on their Dead Sea Scroll research at Manchester. (They are developing an X-ray fluorescence technique to help resolve a long standing debate about where and how the scrolls were produced.)\nAnd now the responses:\nThe Israel Museum’s press officer sent a short email noting that the organization’s curators and conservators “adopt the best available tools to ensure the safekeeping of these works” and “ascribe to the highest standards” but she did not provide any further explanation about the tools or the standards. She also noted that the scrolls’ condition is “assessed on an ongoing basis” and that they are only allowed to travel “when the Museum can be satisfied regarding the conditions for transport, handling, and display.” No specifics were given.\nIn a phone conversation, the IAA’s head scroll conservator, Pnina Shor, said that the IAA’s conservation strategies were developed in collaboration with the Library of Congress and the Getty Conservation Institute. She also defended the exhibitions. When the scrolls are on the road, Shor said the IAA ensures that “the environmental conditions in display cases are exactly the same as the conditions in the vault” except for the amount of illumination the scrolls get.\nMuseums spend a lot of effort evaluating what acceptable light levels are, given that light damage is a prime culprit in the destruction of many kinds of art and artifacts, from Van Gogh pigments to plastic sculptures.\nShor said that the IAA has set a maximum amount of illumination the scrolls can receive to 15,000 lux per year. “The scrolls are made of organic material [animal skin]. We know they deteriorate no matter what we do or do not do. It is part of our duty to both ensure the safety of our heritage and to share it with the public.”\nShe acknowledged that an analytical monitoring system that could systematically evaluate degradation was not yet in place. However she pointed out that the IAA is working on a high tech multispectral imaging system, with collaborator Greg Bearman who has developed similar systems for NASA’s jet propulsion lab.\nLast year Bearman, Shor and their colleagues published two proof-of-principle articles (here and here) about how the monitoring system might work. The articles showed data for artificially-aged parchments, but not the Dead Sea Scrolls. Rabin and Hahn criticize the technique in the Restaurator article, saying that they don’t think it is going to work for the Dead Sea Scrolls.\nBearman told Artful Science in an email that Rabin and Hahn don’t fully understand the technique being developed and he added that his team has been collecting data on the scrolls for eight months. He also noted that experiments are ongoing this summer but he did not know when they would be publishing the results. He added that the team was also considering a whole battery of other analytical techniques, such as Raman spectroscopy and scanning electron microscopy.\nPresented with this rebuttal, Rabin responded that she’ll need to see a peer-reviewed article that shows a monitoring technique that works on the Dead Sea Scrolls before she’ll be convinced. She added that even if a fully functioning monitoring system is on the horizon, the scrolls should not be traveling until such a watchdog system is in place.\nAs for the article’s third point, that using Japanese tissue paper and methyl cellulose to reinforce the fragile scrolls is causing harm in some cases: The IAA’s Shor pointed out that this is a well-accepted strategy in the document conservation world, citing the Library of Congress as an institution that uses the method.\nIrene Brückle, an editor of the Restaurator who has written books on document conservation, said the Japanese tissue technique is indeed industry standard, but she also added that there’s no one-size-fits-all strategy for document conservation. Decisions are often made on a case-by-case basis.\nThe current controversy is actually just one of many that have swarmed around the scrolls. Discovered by nomads in desert caves starting from 1947, the Dead Sea Scrolls then found themselves in a new humid environment that kick-started their degradation.\nIll-fated conservation strategies of the 1950s and 60s only made things worse. As the IAA’s Shor has written:\n- The scrolls were at first unknowingly handled inappropriately and kept in an uncontrolled environment. Moreover, in the first years, adhesive tape used to join fragments and seal cracks caused irreversible damage. The scrolls were then moistened and flattened loosely between plates of window glass and sealed with adhesive tape. The ageing of the adhesives and the pressure of the glass caused the skins to darken – to the extent that some of the texts are no longer legible – and the edges to gelatinize.\nConservators started the herculean task of removing the tape in the 1970s and by 1991 they were placed in a climate-controlled storeroom. Conservators also began reinforcing the scrolls with various strategies, including with Japanese tissue paper. And then, as the Restaurator article points out, exhibitions of the scrolls increased substantially.\nIn the end, the current debate boils down to the precautionary principle. Rabin and Hahn think all exhibitions should be stopped until it can be scientifically proven that the traveling isn’t further harming the scrolls. The IAA and Israel Museum says they have the situation under control and that the benefits of exhibition outweigh the possible risks.\nAs it always does, time will tell which approach is wisest.']	['<urn:uuid:c23210c8-56de-4a23-8944-81b04a7d9ec4>', '<urn:uuid:80570ddf-0cd7-4d77-b63c-9f4362b421db>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	27	169	3250
71	What lightweight clothing works best for multiple travel situations?	For versatile travel clothing, it's recommended to focus on items that can serve multiple purposes. Long-sleeved shirts with button-up sleeves can be adjusted for different temperatures, while convertible pants can be used as both long pants and shorts. For layering, consider lightweight options like fine wool cardigans, soft shell jackets, or lightweight fleeces that can be combined for warmth or worn separately. On river cruises and similar trips, women can mix and match separates like tops with skirts/pants instead of single-use outfits, while men can combine items like polo shirts with nice jeans or khakis. The key is choosing breathable, quick-drying fabrics that don't crease easily and can be dressed up or down as needed.	"[""So you've booked that dream river cruise to Europe. You've chosen your state room, purchased flights, and have already bought a guidebook (or two). Now comes perhaps the hardest part about getting ready for a river cruise: figuring out what to pack!\nMost people go on European river cruises in the spring and summer months, with a handful cruising in the winter to visit Europe's famous Christmas markets.\nWhat you pack will of course depend on which season you're traveling in. But here are my tips for what to pack for a river cruise in Europe after going on 4 such cruises myself.\nCommon questions about packing for a river cruise\nDo I have to dress up?\nThe top question I get about river cruises is related to how “fancy” you need to get onboard. There are generally no formal nights on river cruise ships. There may be one Captain's Dinner night where you're advised that shorts and tank tops are not appropriate, but otherwise the cruise will be fairly casual.\nMost people will dress up a little bit for dinner in the main dining room – but by “dress up” I mean put on a clean pair of pants and maybe swap out the sneakers for something slightly nicer.\nFor ladies, dinner attire can be anything from a nice pair of capris/pants and a top to a sundress. Cocktail dresses would be considered very fancy for a river cruise.\nFor men, most will wear pants or a nice pair of jeans and a polo shirt (or similar). You won't find many men wearing shorts at dinner, but there aren't generally any rules against them.\nFor the rest of the time on your cruise, dressing for comfort is key!\nWhat kind of shoes are best?\nWhat kind of shoes you want to bring will largely depend on your cruise itinerary. Obviously if you're doing a more active river cruise, then you'll probably want a pair of good sneakers, or at least some good walking sandals.\nIn fact, you probably want those things anyway, as walking tours are a common excursion on any river cruise.\nLately, my go-to casual shoes for both myself and my husband have been SUAVS Zilker knit shoes. These lightweight, flexible shoes are super packable, breathable, and basically feel like you're just wearing socks. They're also machine washable, too, which is a big bonus.\nCan I do laundry?\nIf you're on a longer river cruise (or if your cruise is part of a longer trip around Europe), you might need to do laundry at some point. While there aren't any self-service laundry facilities on river cruise ships, there will always be an onboard laundry service.\nLaundry prices on a river cruise are comparable to prices in higher-end hotels, with every clothing item costing a certain amount to be washed, dried, and pressed. I would personally only pay this much to have laundry done if it's absolutely necessary, but the service IS there.\nAnd you can expect your laundry to be returned to you in 24 hours.\nWhat's provided in the room?\nAmenities will vary slightly based on which cruise line you're sailing with, but in general you can expect to find bath robes, slippers, a hair dryer, and toiletries like shampoo and conditioner in your room.\nDo I need outlet/voltage adapters?\nSOME river cruise lines will offer two types of outlets in every state room (usually a US plug and a European one). Others will just have the European ones, meaning traveling with an outlet adapter is never a bad idea. I like this one.\nAs for voltage, most electronics (like your phone charger, Kindle charger, camera charger, etc.) are built these days to handle any voltage between 110 and 250. So the ONLY instance where you might need a separate voltage adapter is if you're using a curling iron, straightener or hair dryer of your own that's single-voltage from the US, and your ship only has European plugs. (But you can easily avoid this issue by purchasing dual-voltage gadgets – here's a dual-voltage hair dryer, and here's a dual-voltage curling iron.)\nWhat to pack for a river cruise in Europe\nI've traveled on river cruises in spring, summer, and winter. The packing list below represents what I recommend packing for a river cruise in Europe in either spring, summer, or early autumn.\n(If you're headed to Europe on a river cruise in winter, check out the bottom of this post for suggestions on how to tweak this list for winter!)\nTo be completely honest with you, your suitcase for a river cruise doesn't really matter. The beauty of a river cruise is that you only need to unpack once, and then you can shove that suitcase under your bed for the duration of your cruise (and yes, most river cruise lines design their beds so that full-size suitcases slide easily under them!).\nI went with my Osprey rolling bag for this trip. It’s a 60-liter bag with sturdy wheels and removable backpack straps (though I always leave the backpack straps at home). Osprey makes amazing soft-sided bags – this particular one has been traveling around the world with me for five years and is still in fantastic shape.\nIf a hard-sided suitcase is more your speed, I'm a fan of spinner luggage from Samsonite.\nRELATED: The Best Luggage and Bags for Travel\nI utilized my favorite packing cubes (GREAT for keeping tops and bottoms organized) and and my new SegSac (perfect for socks and undies) for packing. As a bonus, these cubes can help you organize things in drawers when you unpack onboard, too.\nRiver cruise clothing (for women)\n(Remember, this specific river cruise packing list is based on a spring/summer river cruise. Your list might vary depending on what kind of excursions you're planning to do!)\n- 1 pair of stretchy jeans\n- 1 pair of long leggings\n- 1 pair of capris/shorter leggings\n- 1 pair of shorts\n- 5 short-sleeved tops\n- 2 tank tops\n- 1 long-sleeved layering shirt\n- 1 cropped sweater\n- 3 dresses (I really like this maxi dress for travel)\n- 1 maxi skirt (this one is the red one you see in this post)\n- 1 set of pajamas\n- 4 pairs of socks (including compression socks for the plane)\n- 10 underwear (including my favorite ExOfficio bikini briefs)\n- 3 bras\n- 1 pair of moisture-wicking shorts to wear under dresses\n- 1 swim suit (since the ship had a small heated pool)\n- 1 casual jacket\n- 1 raincoat (I love this Columbia one)\n- 1 scarf (check out my favorite hidden pocket scarf!)\n- 1 hat (either a baseball cap or packable sunhat)\n- A dual-voltage hair dryer (because even though river cruise ships will supply one, I like to have my own!)\nMy top tip for packing light is to stick to mostly neutral colors and one or two accent colors that you can mix and match. This way, you can create multiple outfits with fewer articles of clothing.\nI generally pack this amount of clothing whether I'm traveling for 2 weeks or 2 months – but the river cruises I've been on in Europe have usually been 8-9 days.\n- 1 pair of sneakers or walking sandals (I love my Teva Verra sandals)\n- 1 pair of Crocs flats\n- 1 pair of nicer sandals\nWhatever you decide to pack, just be sure you have at least one pair of shoes that you'll be comfortable walking in. River cruises offer a lot of free walking tours, and you don't want to have to skip them because of poor shoe choice!\n(If you're going in the winter – like on a Christmas market cruise – I would swap out the sandals for a warm pair of boots like these Omni-Heat boots by Columbia.)\nMy carry-on backpack\nMy new favorite backpack that's both functional AND stylish for the streets of Europe is the Pacsafe Citysafe CX. This backpack has anti-theft features like slash-proof straps, lockable zippers, and RFID protection, but also looks really great.\nInside this bag:\n- My laptop\n- Kindle Paperwhite\n- Electronics chargers, extra batteries/SD cards, etc.\n- Belkin power strip/surge protector*\n- Outlet adapter*\n- My glasses and extra contacts\n- A travel umbrella (this rainbow one is super cute!)\n*I recommend these for any trip anywhere! They make it easy to charge any type of device that you've brought along, from cameras to iPads to Kindles.\nI'm still in love with my Donner bag by Overland Equipment, but unfortunately this exact bag isn't sold any longer. If you want something similarly roomy and useful for traveling, though, check out the Travelon Anti-Theft Bucket Bag. If you prefer using a small backpack as your handbag, I love this Pacsafe Citysafe convertible backpack.\nIn my purse (on travel days):\n- My camera\n- Extra camera battery and lens cleaner (my LensPen is great!)\n- A small power bank so my phone never runs out of juice\n- Small zippered pouch with Dramamine, chapstick, Advil, etc.\n- Tiny hair brush with attached mirror\n- Business cards (because you never know who you’ll meet!)\nNOTE: I don't ever recommend carrying your passport on you at all times. Unless you're told you need your passport when you're off the ship, just leave it in the safe in your room.\nMore river cruise posts\n- 7 Reasons You Should Go on That River Cruise This Year\n- 6 Highlights from an Active River Cruise with Avalon Waterways\n- Cruising Solo: What It’s Like to Go on a Viking River Cruise Alone\n- Christmas Markets with Viking River Cruises: What’s It Really Like?\n- Exploring Russia with Viking River Cruises: The Highlights\n- Highlights from a Danube River Cruise in Eastern Europe with AmaWaterways\nWhat about for men?\nIf you're a man planning to go on a river cruise (or, probably more likely, a wife wondering what to pack for your husband), here's what I would suggest:\n- 2 pairs of nice jeans\n- 1 pair of khakis or dress pants\n- 1-2 pairs of shorts or hiking pants\n- 4-5 t-shirts\n- 2 polo shirts\n- 1 dress shirt (optional)\n- 1 fleece or sweatshirt\n- Raincoat (especially if traveling in spring)\n- Underwear for 10 days\n- 5-6 pairs of socks\n- 1 pair of sneakers\n- 1 pair of nicer shoes\n- 1 hat\nWhat if I'm going on a winter river cruise?\nIf you're doing a Christmas market cruise, I would do the following to amend this list:\n- Nix a couple of the dresses and/or add another pair of pants or leggings\n- Leave the shorts at home\n- Pack a couple more sweaters\n- Bring boots instead of sandals\n- Pack a medium-weight winter jacket (it doesn't get *that* cold in most parts of Europe in December – when I went on a Christmas market cruise, it was in the 40s/50s (F) most of the time); a down (or down alternative) jacket would be perfect.\n- Pack a hat and gloves, just in case\nDo I need anything else?\nIt’s not a tangible item, but I also always recommend packing a good travel insurance policy when you travel! That way everything from lost luggage to a bad accident is covered – because you just never know what might happen. I usually recommend Wold Nomads for affordable basic travel insurance.\nREAD NEXT: Christmas Markets with Viking River Cruises\nHave you ever been on a river cruise? What kind of things did YOU pack?\nPin it for later:"", 'Tips for Packing Light When Travelling\nFirst, Decide What to Pack\nWhen travelling, it\'s always a challenge to fit everything you want to bring into a suitcase. While there are certainly tricks to squeezing everything into a small space, the easiest way to pack light is to take less stuff in the first place!\nIn order to do this, assemble all the clothing you think you\'ll need during your holiday, before you start packing.\nNow, before you put one single thing in your case or backpack, take a look at what you\'ve laid out and pick out some items you can wear on the journey. This approach will avoid the most common travel mistake—packing everything you\'ll need, then wearing something else as a “going away” outfit. That outfit likely won\'t be worn again till you come home, and it\'s just dead weight! Sure, the climate where you’re going may be very different to where you start, but that doesn’t have to be a problem if you think laterally.\nSay it’s cold at home and you’re going somewhere tropical. If you wear a thick sweater or padded jacket on your outward trip, it’ll be useless at your destination. Instead, wear a long-sleeved tee-shirt, and add a fine wool cardigan or sweater, a rainproof/windproof jacket and a pashmina or scarf.\nEach of these things, individually or in combination, could be useful on your holiday (nights can be cold, even in the tropics!).\nChoose ""Double Duty"" Clothes\nApplying the principle of ""double duty"" like this is the main secret to saving space. Avoid items of clothing that can only be worn one way, because chances are they’ll only be worn once or twice and won’t “earn their keep”. For women, stick to separates you can mix and match to produce different outfits. If you\'re going to one major evening event, then an evening gown is fine - but for something like a cruise, where you have several special nights, bustiers and skirts/pants will give you more different outfits in far less space.\nOn a recent trip to the Middle East, our best investment (for both myself and my guy) was undoubtedly convertible long pants (“double duty” again!). Be careful with fabrics—many convertible styles are made of quick-dry synthetic fabrics, which claim to be cool but often aren\'t! These are made of soft synthetic material. That means they won\'t crease like cotton and will dry much faster. Most importantly, they are also breathable, unlike some travel pants which can be sweaty. Columbia convertible travel pants\nPacking for a Cruise\nOur other most-worn items were lightweight long-sleeved shirts with button-up sleeves. Roll the sleeves up when it\'s hot, roll them down when it\'s cool—and unlike my husband, I had the added bonus of being able to tie mine as a midriff top!\nIf you’re really worried about the cold, today\'s thermal underwear is featherlight and scrunches down into nothing—however it may be too warm for milder climates. Unless you\'re going somewhere really cold, consider taking lightweight singlet tops or t-shirts which can be worn under or over shirts as an extra layer.\nFor a multi-purpose jacket, a soft shell or lightweight fleece is a good buy.\nNow you’ve chosen your clothes, it’s time to pack them.\nRoll, Don\'t Fold!\nRolled clothes are easier to fit into odd spaces in your suitcase, and won\'t crease nearly as much as if you fold them. Pack big items first, and fill in the gaps with rolled-up undies or socks.\nDon\'t roll up belts or ties. Leave them unfurled, and tuck them around the edges of the suitcase. That way they\'ll take up almost no space at all, and you\'ll be able to find them easily too.\nA Different Approach for Touring\nWhile rolling maximises the amount of clothing you can squeeze into your case, searching for clothes can turn your whole case into a glorious muddle. If you\'re touring, that means you\'ll have to do a total repack every morning!\nThe solution is travel packs. You can buy specialist travel cubes, but I just buy the biggest size of Ziplock bag from the supermarket. They\'re not nearly as sturdy as the proper travel packs, but I always carry a few spares in case I burst one or two. Sort your clothes into categories, and roll and pack each category into a separate bag.\nBecause you can\'t make use of every nook and cranny, using bags means you won\'t fit quite as much into your case. But the joy of being able to get up and pack your case in minutes for that early morning start—priceless!\nI don\'t recommend the vacuum bags (where you suck out the air after packing), unless you don\'t mind walking around in wrinkled clothes. Besides, if I\'m touring I don\'t have time to muck around with sucking out the air every morning!\nShoes are always a problem, because they’re heavy and take up space. If you can, wear your heaviest/bulkiest shoes on the outward journey, so you don’t have to fit them in your luggage. You may be reluctant to do that (who wants to wear heavy shoes on a long trip?), but you can take a pair of light slippers or socks that you can change into on the plane (you can stash your shoes in the overhead locker). Just make sure your shoes are not too tight, or you may have trouble getting them on again at the end of the trip!\nNever pack empty shoes. Find small items to put inside them (socks, hairbrush, pens, ties etc). Put your shoes in shoebags so they don’t dirty your clothes.\nIf you\'re staying in hotels, there\'s no need to take towels. Sure, hotels say you shouldn\'t use their towels at the pool or beach—but that’s only because they’re worried about them getting lost. I’ve used room towels at beaches and pools all over the world, and never had a problem.\nIf you don’t want to take the risk, ask the hotel if they have pool or beach towels—many do.\nIf you feel absolutely lost without your own towel, pack a microfibre travel towel instead (but check the size - most are much smaller than a regular towel).\nToiletries for Travel\nAt the time of writing, there are severe restrictions on the amount of liquid you can carry when travelling. Your liquid or gels must all be packed in 3-oz bottles, and they all have to fit in one small pouch. For most people, that\'s mostly a problem when it comes to toiletries.\nThe solution is to look for toiletries that aren\'t gel or liquid. For instance, I don\'t pack cleanser or toner - instead, I take make-up remover wipes. Sunscreen and self-tanning wipes are good, too. Dove or Neutrogena soap is solid, and just as gentle on your skin as a liquid cleanser.\nThe principle of double duty applies here, too. I always pack a conditioning shampoo, so I don’t need conditioner, and I choose a body sunscreen which is also a good moisturizer.\nFor the face, I take a good quality SPF15 tinted moisturiser, so I don’t need separate face moisturiser, sunscreen and foundation. A bronzer can be used as blusher and eyeshadow. Soft eyeliner and lipliner pencils take up no space and can be used as eyeshadow and lip tint.\nIf you\'re staying at hotels, you may not need to pack shampoo, conditioner or body lotion at all, as there\'s likely to be a free supply in the room. It\'s worth asking.\nIf you\'re decanting, be careful not to overfill the bottles, because the contents will expand with the changes in air pressure—and you don’t want sunscreen all over your clothes. Always put your toiletries inside a plastic bag, in case of spillage.\nThe restrictions on toiletries can be a nuisance, but you can also be seen as an opportunity! I\'ve never travelled with lots of toiletries, even on a long holiday. For me, part of the fun of going overseas is trying new things, so I love the excuse to buy some French shower gel or an Italian face cream!\nFor my last overseas trip, which lasted six weeks, I travelled with one medium-sized wheelie duffle bag and a carry-on bag. My husband had one medium wheeled suitcase and a shoulder tote. That trip took us from the heat of Africa to the chill of Northern Europe, and we never had any trouble keeping warm (or cool).\nWe’ve all heard the saying “pack everything you think you need, then halve it”. I’ve never been able to apply that rule—but then again, thanks to my own “rules”, I’ve never had to!\nMore Tips on Packing Light\n- 7 Reasons to Travel With One Bag\n7 Reasons to Travel With One Bag. There is a great debate among travelers about how to pack for a trip. Should you pack everything but the kitchen sink so you\'re sure to have everything you need, or strip down all the superfluous ...\n- Rick Steves\' Europe: Packing Light and Right\nPlanning tips to help maximize your time and money spent in Europe.\n- Travel Light with One Bag!\nleisure and business travel packing list - travel light - carry-on luggage\n- How to Pack Light\n- Cruise Packing Tips\nCruise Packing Tips from Cruise Diva\'s Cruise Planner\n© 2007 Kate Swanson']"	['<urn:uuid:68b74f17-6fc0-4d90-9fce-c5cd28963b8b>', '<urn:uuid:9476bd2e-9c37-40ff-a83b-a3536d051bc7>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-13T05:18:19.158519	9	115	3492
72	initial steps introduce newborn baby dog first time home arrival	When bringing a newborn home, first introduce the dog to a blanket or baby outfit with the baby's scent while the mother is still in hospital. When arriving home, the mother should give the dog attention while the father holds the baby in a separate room. Then introduce the dog to the baby while keeping the dog on a leash and the baby at a distance, repeating this process several times during the first couple of days.	['Children and dogs seem to be natural pals, but the introduction of a new baby into a family that already has a dog takes some preparation and planning. Many dog owners (and cat owners too) dote on their pets and lavish them with a lot of attention. But smart parents-to-be need to make the effort to prepare their best furry buddy for the needed adjustments.\nEasing the Transition\nTo reduce the upset, making changes to your usual routine before your new baby arrives can be beneficial. For example:\n- Teach your dog to be quiet around the house and get him used to staying still. When you’re dealing with a baby, this will be very useful.\n- Have a dummy run and see how your dog might react to a baby. It sounds mad, but wrap a doll in a blanket, nurse it and give it attention.\n- Encourage your dog not to go upstairs, especially if your nursery will be there.\n- Watch out for any behaviour or habits that could cause difficulties, like frequently jumping up or pulling on the lead. If you do notice these habits, slowly work on them, using treats as rewards.\nIt is important for dogs to understand and obey their owners, and that they can follow simple commands such as sit and stay. Once they have these behaviours down pat, you should participate in practice sessions with your dog to get it used to the presence of a baby.\nWhile it may feel a bit silly, many parents find that rocking, cuddling or talking to doll while asking the dog to remain in a sitting position can help the pet to master good behaviours. After laying the ‘baby’ down, you should reward your dog’s obedience with affection or a small treat.\nBabies make noises and movements that dogs don’t always understand, but by introducing them gradually, dogs can learn to accept new babies. If your dog is good natured, arrange visits with the small children of friends or family to help with the transition. Short, well-controlled interactions can help dogs to get used to the loud and unpredictable nature of small children.\nIt’s also important that your dog gets accustomed to a bit less attention than it might be used to before a new baby’s arrival. For a few months before the due date, you may want to gradually decrease the time you spend interacting with your dog each day, since even the best-intended pet owners are bound to be a bit short on time once the baby is born.\nSimilarly, areas of the house that your dog will be kept from after the baby’s arrival, such as the nursery, should be made off-limits beforehand. Every step that can be made to get the dog accustomed to the changes in advance will make life that much easier once your baby has joined the family.\nBringing Baby Home\nWhile Mum is still in the hospital, a blanket or baby outfit that has your newborn’s scent should be introduced to your dog, helping it to become familiar with the baby. On the day that your baby comes home, Mum should plan to give the dog a little extra attention immediately upon arrival, letting Dad hold the baby (preferably in a separate room).\nWhen you do introduce your dog to your baby, have him on a leash and keep your baby at a distance at first. Repeat this process several times during the first couple of days, and don’t leave your baby alone with your dog until you’re completely at ease. If you still don’t feel 100% comfortable, then you could consider putting up a child safety gate for added protection. <#70#>John Lewis<#> stock an affordable range of safety gates.\nThe important thing is to treat your dog in the same way as normal. Feed him at the same times and take him for his usual walks. In most cases, babies and dogs do get on fine, but if your dog has shown previous behaviours such as aggression, killing cats or other animals, or is known to not get on with children, then seek expert advice.\nHealth-wise, children can’t catch dog diseases and are protected if dogs have been immunised, so they’re safe from that point of view. The only thing to remember is that they can catch worms and get flea bites, so ensure you have your dog wormed and keep it flea-free.\nBuying a Dog\nIf you’re thinking of buying a dog and already have a family, then it’s still important to ensure they’ll all get on. Before you part with your cash, check that:\n- The dog has no record of aggression or killing animals.\n- The dog does get on with children.\n- If it’s a puppy, consider how you’ll train it.\nTaking your children to see the dog will clearly help identify which animals they’ll get on with, but if you’ve got a baby, bringing the dog home to see how you all cope before making a firm decision can be useful. First impressions are important, and if the dog seems to take a dislike to your children, then you may have to find another dog.\nWhich Breeds Are Best?\nAll dogs living with children need stringent training and handling, but some breeds are considered to be more child-friendly or suitable than others. The article Safe Dog Breeds For Children on the SaferPets website gives some useful pointers and valuable tips.']	['<urn:uuid:7326af64-2c3c-4173-8437-16716c76a00d>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T05:18:19.158519	10	77	910
73	Which is more accurate for measuring hot spots: infrared cameras or thermocouples?	Both technologies have specific advantages. Infrared cameras allow rapid scanning of electrical systems and can provide visual displays of heat patterns, making them ideal for non-destructive detection of electrical problems. However, thermocouples, particularly those with heat shields like the Xtracto-Pad, can provide more accurate temperature measurements since they eliminate interference from radiant heat sources. Traditional exposed thermocouples can give inaccurately high readings by measuring both tubeskin temperature and radiant heat, while infrared pyrometers may read up to 40°F higher than actual temperature if not properly calibrated for furnace emissivity. For optimal monitoring, using both technologies together is recommended - the thermocouple system allows quick comparison and adjustment of the pyrometer's settings for better accuracy.	"['Using Thermal Images for Electrical Inspections\nHeat: The Symptom of a Faulty Electric System\nAn infrared electrical inspection of your facility can provide you with valuable information about the condition of your electrical system and equipment that can save thousands of repair dollars and hundreds of hours of lost productivity. Even worse, unchecked electrical system failures can result in fires or dangerous explosions. Infrared inspections are designed to identify potential equipment failures before they happen so that appropriate preventative action can be taken. The decreased liability of a thermally inspected electrical system even encourages some insurance providers to offer discounted rates.\nWhen an electrical system begins to have problems, it gives off heat as a byproduct of electrical resistance. Infrared cameras record and visually display this emitted heat to show the exact location of the electrical defect. Thermal imaging simplifies diagnosis and repairs while protecting adjacent electrical components from hot spots. Routine thermal electrical inspections will reduce repair costs and decrease lost production hours that result during unexpected down time.\nA thermal scan of your electrical system increases efficiency, greatly reduces the risk of fires, and allows preventative and predictive maintenance to be scheduled during planned down-times. In business, this alone can mean mean big cost savings.\nMore About Predictive Maintenance\nThermal Imaging has proven to be an ideal inspection method for all types of predictive electrical maintenance. Infrared technology gives thermographers the ability to “see”, measure, and record temperatures on defective components and the normal wear, chemical contamination, corrosion, fatigue and faulty assembly that occurs in many electrical systems. The molecular friction produced by problem electrical components is visible within the infrared spectrum and recordable by infrared cameras. Overheating can occur in any electrical component including generators, transformers, pole top connections, insulators, disconnects, jumpers, shoe connections, fuse connections, switchgear, starters, contactors.\nOhm’s law states that power dissipated in the form of heat exponentially increases as the current increases. This means that the amount of heat generated in a conductor is proportional to its resistance and to the square of the current it carries, while the temperature rise depends on the rate at which the heat is dissipated through convection, radiation, and conduction.\nBy allowing a small amount of current to pass through electrical devices, localized ""hot spots"" can be detected using a thermal camera making thermal imaging a non-destructive method of detecting problems on electrical devices.\nHow Electrical Inspectors Use Thermal Imaging Technology:\n- Connectors, Relays, and Switches: Find poorly secured, corroded, or current overloaded hardware\n- Semiconductors: Find poorly bonded, die attached, open, shorted, or leaky active devices\n- Circuit Boards: Find overstressed components, plated through holes, poor heat sinks, and bad solder joints\n- Discrete Components: Find overstressed transformers, capacitors, and resistors\nStandards for Thermal Electrical Inspections\n7.1.1. Current-carrying capacity of a bus - The current-carrying capacity of a bus is limited by the temperature rise produced by the current and other factors. Buses for generating stations and substations are generally rated on the basis of the temperature rise which can be permitted without danger of overheating equipment terminals, bus connections, and joints. ANSI C37.20C-1974 (IEEE standard 27-1974) permits a hottest spot temperature rise for plain copper buses of 30 ºC (54 ºF) above an ambient temperature of 40 ºC (104 ºF), with a hottest spot total temperature limit of 70 ºC (158 ºF). The standard allows a hot spot temperature rise of 65 ºC (117 ºF) for metal-enclosed applications where silver contact surfaces are used on connections and a hot-spot temperature rise of 45 ºC (81 ºF) for silver-surfaces terminals of outgoing circuits. Aluminum used for bus work ordinarily has a conductivity of 63 percent, as compared to copper at 99 percent. For a given current rating and for equal limiting temperatures, the area of an aluminum bus will be about 133 percent of the area of a copper bus.\n7.1.2. Allowable current density in a bus - Allowable current density in a bus is the amount of current that the bus can carry per square inch or cross-sectional area without exceeding the permissible temperature rise. For both ac and dc buses, densities may vary from values of 9.3. x 105 and 1.09 x 106 A/in2 (600 and 700 A/in2) in heavy current-carrying copper buses to 1.86 x 106 and 2.17 x 106 A/in2 (1,200 and 1,400 A/in2) in light buses under favorable conditions. For aluminum, densities of 75 percent of the above values are usually permitted.\n7.1.3. Current-carrying capacity of conductors - No method has been generally accepted by the industry for the calculation of the current-carrying capacity of conductors for overhead power transmission lines. However, Reclamation designs its ACSR transmission line capacity in accordance with tables, charts, and procedures in the ""Aluminum Electrical Conductor Handbook,"" 1971 edition or specific instructions from the conductor manufacturer.\n7.2. Temperature of Connections - The principal function of an electrical connection is to satisfactorily carry the electrical load over its entire service life. The electrical load can be expected to have daily fluctuations from no load to full load and frequently to very heavy overloads, thus causing wide fluctuations of operating temperature. In addition, the ambient temperature can be expected to fluctuate between daily extremes and between seasonal extremes. The effect of this heat cycling on a poorly designed or improperly installed connection is frequently progressive deterioration and ultimate failure of the connection or associated equipment. Therefore, temperature rise provides an important and quite convenient method of monitoring the condition of electrical connections.\n7.4. Thermographic (IR) Surveys - Sophisticated infrared equipment in vans or helicopters is now widely used throughout the electric power industry for rapid scanning of substation and switchyard bus and equipment as well as transmission lines. Available equipment can survey an average-sized switchyard in about 30 minutes. Experience in the industry has proven the technical worth and economic justification of infrared testing as a preventive maintenance tool.\n7.4.1. Thermal Imaging devices - Thermal imaging devices are useful in permitting a rapid scan of switchyard buses and equipment terminals to detect temperature differences. Infrared signals from photovoltaic detectors are electronically amplified and transmitted to a small television viewing screen producing an image corresponding to the thermal patterns within the viewed scene. Temperature differences are shown in varying shades of gray or in different colors. An accessory instant camera may be used to provide a permanent record of temperature differences.\n7.4.3. Scheduling of infrared surveys - Scheduling of infrared surveys to detect hot spots in electrical connections and equipment is an important facet of an effective infrared testing program. Such surveys should be conducted during periods of high system loads, which circuits in a given station are sufficiently loaded to reveal existing or potential hot spots. Subsequent surveys should be scheduled to test transfer of auxiliary buses under loaded conditions. Each hot spot detected by an infrared survey should be physically inspected and evaluated. Judging the severity of a faulty connection by temperature readings alone can be misleading. A reading of 50 ºC (90 ºF) above ambient in one case can be as serious as one 180 ºC (325 ºF) above ambient, especially if the load is low in the first case. A follow up inspection of suspect and/or repaired connections should be performed with a hand-held infrared thermometer, temperature-sensitive tape, or other suitable means.', 'Hot spots can considerably reduce the life of furnace tubes. To monitor hot spots, tubeskins should be equipped with a reliable, robust temperature measurement system. Multipoint thermocouple assemblies can quickly alert operators when hot spots occur. In addition, product innovations increase accuracy and practically eliminate the need for large welds, which stresses furnace tubes and shortens their service life even more.\nTemperature is the most important factor affecting tube life in syngas plant reformer furnaces. A typical lifetime is 10 years (100,000 hours) of service at its maximum design temperature. When operating temperatures climb above that maximum, the overall furnace tube life decreases.\nEven when temperatures are closely controlled, different parts of a furnace tube can experience thermal differences due to:\n- Flame impingement\n- Uneven gas flow\n- Uneven process flow\n- Catalyst deactivation/poisoning\nThese and other temperature excursion events result in hot spots. Even tiny ones affect the entire tube, as the material at those spots age faster and are more prone to failure than cooler parts. Depending on the conditions, the operator may need to throttle back the rate of the unit, thereby decreasing production. Or, the furnace will experience a methane slip due to reduced firing, necessary to keep the hot spots within the tube wall temperature limits (TWTL).\nMonitoring Hot Spots\nKeeping operating conditions within the TWTL minimizes failures, optimizes efficiency, and maximizes tube life. A common way to monitor individual hot spots is with an infrared pyrometer. Most of these devices control for emissivity but still require tedious hand calculations.\nA better solution is to use both a pyrometer and an online thermocouple measuring system. The latter allows a quick comparison of temperature readings with those of the pyrometer, as well as an easy adjustment of the pyrometer’s emissivity setting to better simulate the current conditions in the furnace. It is not unusual for older pyrometers to read 40°F higher than the actual temperature if they are not properly set for the real furnace emissivity. In the case of rate-limited operations due to the TWTL of a hot spot, temperature information needs to be accurate to avoid unnecessary production losses.\nAccurate Tubeskin Temperature Measurement with Thermocouples\nThermocouple assemblies provide accurate online temperature measurements of furnace tubes. However, in-place accuracy is an issue when a thermocouple measures both the temperature of the tubeskin (desirable) and the radiant heat source (undesirable).\nAny protrusion on the furnace tube collects and radiates heat into the tube. Hence, exposed thermocouples receive heat from both sources, giving inaccurate higher readings. Taking into account this pain point, Gayesco International – now a division of WIKA USA – developed the Xtracto-Pad®. This tubeskin thermocouple has a heat shield over the measuring tip. The extremely opaque insulation ensures that the thermocouple gets its heat input only from the tubeskin and does not act as a fin for receiving radiant heat. In addition, since the measuring junction is protected, the already durable thermocouple has an even longer service life.\nWIKA USA continues to develop and improve temperature measurement solutions for challenging applications in syngas plants, refineries, and the chemical and petrochemical industries. Contact our expertsfor more information.']"	['<urn:uuid:cfcd4133-464f-407d-ba3a-f042d6ba6b68>', '<urn:uuid:41e88120-70c4-47f4-a702-44043261bfdb>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T05:18:19.158519	12	114	1740
74	best meat cooking techniques home kitchen healthy preparation methods cancer risk reduction	For meat preparation, several cooking techniques are available: grilling uses direct high heat that creates distinct flavors through charring, roasting employs lower-temperature indirect heat that maintains moisture and ensures even cooking, and braising combines searing with slow cooking in liquid to tenderize meat. However, WHO research has identified health concerns: processed meats (including hot dogs and ham) have been linked to bowel, pancreatic, and prostate cancer, while red meats are classified as probably carcinogenic. According to the Global Burden of Disease project, diets high in processed meat result in 34,000 cancer deaths annually worldwide. While health experts advise against large quantities of red and processed meat, these warnings have traditionally focused on heart disease and obesity risks rather than cancer.	"['If you\'re bored in the kitchen, try these 5 cooking techniques.\nAre you growing weary of consistently preparing the same dishes in the same ways? Have you ever thought about different ways to prepare your favorite dishes so that you can get diverse flavors and textures? Foods that we find repetitious or uninteresting frequently have less to do with recipes or seasonings and more to do with how we prepare them. Your ability to prepare meals, your pleasure of food, and your creativity in the kitchen can all be improved by learning new cooking methods and techniques.\nHere are some various cooking techniques you can use.\nGrilling is a dry heat cooking technique that involves applying a lot of direct heat to the food\'s surface from above, below, or to the side, searing the outside. It is frequently used to swiftly cook meat or veggies. Typically, while grilling, the food is placed on a grill, a cast iron pan, or a grill pan with raised ridges that resemble the wires of an open grill. Foods can be exposed to extremely high temperatures while grilling, frequently reaching 500 degrees F. Due to the charring of the ingredients, the cuisine is able to develop distinct, intense flavor profiles and a lovely\nWhen the heat source is from above, creating heat transfer through thermal radiation, broiling is a type of grilling. Similar to grilling, broiling cook’s food with intensely hot, direct heat and can result in the same charred flavor. Foods should be carefully examined during broiling because they might burn easily (particularly on the top side). The broiling can transform into baking because the meals will cook in the hot air of the oven rather than the direct heat from the broiler because the oven regulates the temperature and can prevent high temperatures.\nAnother dry heat cooking technique is roasting, which employs hot air produced by indirect heat from a source (flame or oven) to cook the food uniformly on all sides. It requires more time to adequately cook the items to the appropriate end point because it uses lower temperatures to complete the process. To guarantee uniform cooking, meat can occasionally be placed on a rotisserie. Both meats and vegetables can be roasted. For various meat cuts or intended flavor profiles, different temperatures are employed. Roasting maintains moisture of the food being cooked so that the meat remains juicy, often the meat is also basted during cooking to prevent drying out and maintain the moisture levels.\nUsing both dry heat and wet heat cooking techniques, braising is a type of cooking. It begins with high-temperature searing in a skillet and is then slowly cooked in some liquid, frequently in a Dutch oven or slow cooker. Heat, time, and moisture are all used in braising to cook the meal. Tougher chunks of beef are frequently used in braising to help break them down into more tender pieces. Another method of braising is to use a pressure cooker. In a braised dish, the flavors of the food and the liquid in which it is being cooked can be combined.\n5) Pan frying, stir-frying, or sautéing\nWet heat cooking methods like sautéing, stir-frying, and pan-frying all employ small amounts of fat or oil in specific pans to cook food over medium to high heat. A French cooking technique known as sautéing involves the food ""jumping"" or ""being tossed"" as it cooks. To facilitate quicker cooking, foods are frequently chopped into smaller pieces. Food is being cooked and browned directly on a hot pan while keeping its flavor and texture. Chinese cooking style known as stir frying involves cooking food in a tiny amount of extremely hot oil, typically in a wok. The wok\'s design makes it possible to prepare food quickly. Pan frying is typical. Refers to fast frying huge chunks of food in oil on both sides. The pan-fried food may occasionally be transferred to the oven to complete cooking. This procedure depends on oil or fat to convey the heat to the meal in all techniques of frying', 'Bacon, Hot Dogs Can Cause Cancer\nEating processed meat can cause cancer, World Health Organization experts said Monday.\nProcessed meat is meat that has been preserved by curing, salting, smoking, drying or canning.\nExperts from the WHO\'s International Agency for Research on Cancer (IARC) in Lyon, France, studied 800 cases. IARC researchers linked processed meat, such as hot dogs and ham, to bowel cancer, as well as pancreatic cancer and prostate cancer.\nThe IARC has included processed meat in its group 1 list, for which there is ""sufficient evidence"" of links to cancer. Tobacco, asbestos, and diesel fumes are also on the group 1 list.\nWHO experts also called red meat, including beef, lamb and pork, ""probably"" carcinogenic to humans.\nDr. Kurt Straif is with the IARC. He said in a statement that the risk of cancer increases with the amount of meat a person eats. A person who consumes 50 grams of processed meat per day – about two pieces of bacon – increases his or her risk of bowel cancer by 18 percent.\nHealth experts in some countries already advise against eating large amounts of red and processed meat. But those advisories had been centered on the increased risks of heart disease and obesity.\nMeat industry groups are protesting the WHO study. They say that meat is part of a balanced diet. They also say the causes of cancer are broad, and include environmental and lifestyle factors.\nThe WHO report cited the Global Burden of Disease project, which estimates that diets high in processed meat lead to 34,000 cancer deaths per year worldwide.\nI\'m Ashley Thompson.\n1.processed meat 加工过的肉类\nA membranous case, often made of animal intestine, used to contain sausage or other processed meat.\n2.sufficient evidence 充分的证据；强有力的证据\nWe\'ll certainly consider a claim if there\'s sufficient evidence.\n3.red meat 红肉（猪羊牛肉）\nShe\'s a demi-veg. She doesn\'t take red meat.\n4.balanced diet 均衡饮食\nEating a hard boiled egg or an orange as a snack can contribute to a well balanced diet.\n1.Experts from the WHO\'s International Agency for Research on Cancer (IARC) in Lyon, France, studied 800 cases. IARC researchers linked processed meat, such as hot dogs and ham, to bowel cancer, as well as pancreatic cancer and prostate cancer.\nResearch finds genetic link to some types of gum disease.\nThey will use only small computer devices that link to the Internet.\n2.Dr. Kurt Straif is with the IARC. He said in a statement that the risk of cancer increases with the amount of meat a person eats. A person who consumes 50 grams of processed meat per day – about two pieces of bacon – increases his or her risk of bowel cancer by 18 percent.\nthe risk of ...的风险；...危害\nHe is not willing to run the risk of losing his money.\nHe has distinguished himself many times at the risk of life since he joined the army.\n他参军以来, 出生入死屡建功劳 。\n世界卫生组织国际癌症研究所（IARC）的专家们在法国里昂针对800多起案例进行了分析 。专家研究后得出结论：热狗和火腿等加工肉类与肠癌、胰腺癌以及前列腺癌有一定的联系 。\n国际癌症研究所将加工肉类列入了可充分证明致癌的第一类物品名单中 。该名单中还包括香烟、石棉和柴油尾气 。\n世界卫生组织专家表示红肉可能会致癌 。红肉是指牛肉、羊肉和猪肉等 。\n库尔特·斯特夫博士在国际癌症研究所任职 。他曾在一份声明中称，罹患癌症的风险随着食用肉类的数量而不断增加 。一个人每天食用50克加工肉 -- 也就是相当于两片培根，那么他（她）患结肠癌的风险就会增加18% 。\n一些国家的卫生专家早已警告不要食用太多红肉和加工肉类 。只是这些健康警告围绕的主题一直是心脏病和肥胖症 。\n肉类行业对世界卫生组织这一项研究表示抗议 。他们表示，肉类是饮食均衡中不可缺少的一部分 。他们还称，导致癌症的原因很多，比如说环境和生活方式等因素 。']"	['<urn:uuid:a5ed795c-0033-47a1-acf2-cfe6bfd6f4c6>', '<urn:uuid:6b6155d9-a160-4d8f-802b-d0bcdf204dce>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	12	120	1180
75	what ways implement reflective practice professional development team collaboration methods benefits	Reflective practice can be implemented in professional development and team collaboration through various methods and approaches. These include keeping reflective journals to document ideas and experiences, forming peer groups for regular collective reflection, and using cooperative inquiry methods for structured group learning. In team settings, professionals can engage in collaborative planning sessions where they experiment with new approaches, share experiences, and support each other's learning. This can involve jigsaw reading activities, group discussions, and collaborative lesson planning. The benefits include enhanced self-awareness, improved organizational learning, better monitoring and evaluation, and the development of new skills through social construction of meaning. These methods also help address issues of power relationships and conflict while supporting continuous professional growth.	['How can I make a difference in the world? What is “good change” and how do I contribute to it?\nWhat is reflective practice?\nReflective practices are methods and techniques that help individuals and groups reflect on their experiences and actions in order to engage in a process of continuous learning. Reflective practice enables recognition of the paradigms – assumptions, frameworks and patterns of thought and behaviour – that shape our thinking and action. It also allows for the exploration of broader questions, such as:\n- What are the paradigms that shape not just our own actions, but development as a whole?\n- How does our position relate to the assumptions we make? Are these constructive or destructive to our goals?\n- How are our goals themselves limited by our paradigms?\nBy trying out methods of reflection and personal inquiry we can nurture greater self-awareness, imagination and creativity, as well as systemic, non-linear modes of thinking and analysis.\nWhat use is reflective practice to a development professional?\nReflective practice can help us understand our own intentions, values and visions and support us to work in a challenging field where our ethics and morals may be tested, where power relations may be decidedly unequal, and where we may be working in emotionally and physically demanding environments.\nMany of us keep coming back to fundamental questions: how can I make a difference in the world? What is “good change” and how do I contribute to it? How do we sustain ourselves and keep going, when the going gets rough? How can we position ourselves effectively within a change process, and avoid becoming part of the problem? Practicing reflection can help us answer these questions and others throughout our lives and careers.\nMore development professionals could benefit from adapting creative and innovative approaches to reflective practice – many of which are already used in fields of qualitative research, education, health care, social work, psychology and management. Opening spaces for reflection offers the possibility of transforming not only individual experience, but also the patterns and relationships within groups, organisations and systems, and ultimately those systems themselves.\nHow can I use reflective practice in my work?\nReflective practice can be a particularly powerful tool for organisational learning and in monitoring and evaluation. It can also be used for addressing issues of position, conflict, resistance and power relationships, which are often present in development, but seldom dealt with directly. Reflective practice, whether named as such or not, is already an important dimension of\n- participatory and qualitative research\n- gender and power analysis\n- social constructivism and feminist standpoint theory\n- methods of facilitation and community development work\n- monitoring and evaluation\n- organisational learning and change, and capacity development\n- attention to power and relationships in aid.\nMethods for reflective practice\nKeeping a reflective journal – sometimes also called a learning journal – is a way to reflect through documenting ideas, feelings, observations and visions. It can be done on paper or on a computer. Keeping a reflective journal can help you to\n- focus your thoughts and develop your ideas\n- develop your voice and gain confidence\n- experiment with ideas and ask questions\n- organise your thinking through exploring and mapping complex issues\n- develop your conceptual and analytical skills\n- reflect upon and make sense of experiences and the processes behind them\n- express your feelings and emotional responses\n- become aware of your actions and strategies\n- develop your writing style and skills, and explore different styles of writing\n- develop a conversation with others.\nWhen keeping a reflective journal, these tips may be useful:\n- write for yourself, and write every day\n- be informal, using language you are comfortable with\n- write by hand if you prefer\n- write in your own language\n- be relaxed and comfortable\n- try sitting in different places and positions\n- use diagrams and drawings\n- record not just events but reflection on process\n- ask questions and challenge assumptions\n- connect personal and professional experiences to concepts and theories.\nMore details on these and other tips can be found in Jenny Moon’s 2004 book A Handbook of Reflective and Experiential Learning.\nPeer groups and Co-operative Inquiry\nA group of peers who meet on a regular basis to learn and reflect together can be a powerful supporting element of individual reflective practice. The group, which decides together how to use and organize its time, may discuss work-related issues, share learning journal excerpts or try out a form of collective reflective practice.\nCo-operative Inquiry is a reflective practice method for groups which was initially developed by John Heron to support the reflective practice of participatory researchers. Heron, a pioneer in the development of participatory methods in the social sciences, describes the theory and practice of the method in his 1996 book, Co-operative Inquiry: research into the human condition. It involves a group working through a structured, four-stage cycle of action and reflection, through which group members move towards developing new ways of acting.\nMethods from research and other fields\nReflective practice, reflexivity and first person inquiry are used in research to explore issues of power and positionality and to make the role and assumptions of researchers more explicit and integral to their analysis. There are many approaches to this, which include methods from qualitative ethnographic and anthropological research, participatory and action research, and feminist research.\nThere are also many different reflective practice methods and approaches from management science, experiential and transformational learning, and organisational learning and change. Several of these are reviewed in the 2010 IDS Bulletin Reflecting Collectively on Capacities for Change.\nWithin development and action research, the field of embodied learning and reflection is growing. Many practices in this field are based on the pioneering work of Brazilian director and activist Augusto Boal, who developed Theatre of the Oppressed in the 1970s. They include methods for bodywork and movement, and approaches such as Forum Theatre and Theatre for Development.', 'Best practices – Team planning for methods and approaches\nJim Fuller, Sponge ELT\nExperimenting with different methods and approaches is something that all teachers should do as it aids in expanding their repertoire of teaching skills, raises awareness of potentially beneficial viewpoints, and increases their ability to use principled eclecticism in the modern classroom. However, while this idea of experimental practice is not new, it is often largely left to diploma-level courses or for those teachers that are generally considered ‘the ideas people’. One way in which trainers might wish to introduce or encourage other teachers to work with different methods and approaches is through team planning. Why? Well, team planning offers a safe place for teachers to socially construct meaning regarding principles and ideas, allows for them to be supported in their learning and interpretation of the new method or approach, and increases the likelihood of the ideas actually being used, so long as what teachers plan can be used in their classrooms.\nObjective: The objective of this activity is to raise awareness of the principles and ways to implement various methods and approaches to language teaching.\nMaterials: These will be largely dependent on the method or approach you would like your teachers to experiment with.\nSession 1: Introduction to new method or approach\n- Provide teachers with some information on the new method or approach. Ideally, this would be done through a jigsaw reading and then followed up by each ‘expert’ sharing their information with the group.\n- Teachers then need to use the information they have learnt to answer a number of questions related to the new method or approach.\n- Teachers should then be encouraged to reflect on their own practice and see if they use the method or approach, where it might be applicable, and if it would work in all their classrooms.\nSession 2: Planning using the new method or approach\n- Revisit the principles of the new method or approach\n- Teachers then should be presented with a set of criteria for what a (insert method or approach) lesson should include. Alternatively, teachers can create the criteria themselves.\n- From here teachers then need to plan a lesson using these criteria. Teachers should use materials that they currently use (if applicable) so that once they finish planning the lesson, they can trial the lesson in their classes.\n- Before ending the session, teachers should share their lessons in plenary. Teachers should be encouraged to ask questions and clarify points.\nFollow-up: How did it go?\n- After teachers have completed the lesson, if there is space for another workshop, get teachers to openly reflect on how the lesson went, things they learnt, and what they would like to do differently. If there is no room for another workshop, you can simply sit down with each teacher for a short discussion.\nYour role as the trainer is facilitative and guiding. Teachers should be given room to interpret the principles and underlying beliefs of the method or approach collaboratively, with you guiding them when they steer too far off the mark. Furthermore, try to avoid planning the lesson with them, where possible – jump in only when something seems wrong or there is going to be obvious failure or issues.']	['<urn:uuid:bf0543ad-7326-41a5-9459-408a6b20a5b7>', '<urn:uuid:e6eea643-6fd2-4c1f-9c0c-bfc9fb47f8d7>']	open-ended	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-13T05:18:19.158519	11	116	1541
76	I'm a cotton farmer. When's the best time to check leaf nutrients?	Research in Virginia has shown that petiole and leaf nutrient concentrations have a higher correlation to yield during the first week of bloom than later sampling intervals. During this period, samples should be taken from the fourth mature leaf below the apical meristem down the main stem.	['Through fluid nitrogen-sulfur fertilizer formulationsThrough fluid nitrogen-sulfur fertilizer formulations. .\n1. Evaluate granular and fluid nitrogen (N) sources with varying sulfur (S) application rates on in-season NDVI measurements, petiole and leaf S status during the first week of bloom, and lint yield of cotton in the upper southeast coastal plain. 2. Determine the effect of high N:S ratios in side-dress fluid N sources at varying N application rates on NDVI, petiole and leaf N:S ratios, and lint yield in the upper southeast coastal plain.\nMaterials and Methods\nExperimental Design. Three trials were implemented across the upper southeast coastal plain cotton production region in 2016. The trial locations were Tidewater Agricultural Research Center in Suffolk, VA (Suffolk), Everett Farms in Southhampton County, VA (SHC), and the Peanut Belt Research Station in Lewiston NC (LEW). The study was a randomized complete lock design with four replications of each treatment. Treatments were applied to four row plots measuring 35 ft. in length and 12 ft. wide. Prior to planting a composite soil sample was taken at 0-6, 6-12, 12-24, and 2436 inch depths and analyzed for soil ammonium and nitrate concentrations using a 2M potassium chloride (KCl) extraction procedure. All other nutrients, except N and S, were applied based on soil test recommendations and/or extension recommendations for North Carolina and Virginia cotton production.\nThere were a total of seventeen fertilizer treatments (Table 1). The primary fluid N sources were urea ammonium nitrate solutions (28-32% UAN) and granular urea. A bulk blend of granular urea and ammonium sulfate (AMS) was applied at 100 pounds of N per acre with S rates ranging from 0 to 30 pounds S per acre (Treatments 2-5). The granular fertilizers were spread uniformly over the plot area by hand. The total N application rates for the fluid NS sources were 60, 100, and 140 lbs. N per acre (Treatments 6-17). Nitrogen was applied in split applications with 20 pounds N per acre applied at planting and the remaining N applied at the 1st square stage of development. To achieve the varying rations of N:S solutions, UAN was mixed with ammonium thiosulfate (ATS) (120-026S). Ammonium thiosulfate was chosen for its high S concentration over AMS solutions (8-0-0-9S). The four ratios applied were S,8:1,4:1, and 2.66:1. The analyses of each fluid N-S side-dress fertilizer material were 32-0-0, 24-0-0-3S, 24-0-0-6S, representing fluid fertilizer formulations currently feasible using UAN and ATS solutions (Tom Fairweather, personal communication). The fluid fertilizers were applied using pressurized carbon dioxide system mounted on a four (36-inch) row applicator outfitted with coulters/fertilizer injection knives approximately 6 inches to the side of the row. All N sources were treated with the urease inhibitor N-(n-butyl) thiophosphoric triamide (NBPT),to minimize any ammonia volatilization from surface applied urea based fertilizers.\nPreplant Soil Nitrate and Ammonium Sampling. Prior to planting, soil samples were taken from each site. Six to ten cores were taken per location. Samples were taken at 0-6, 6-12, 12-24, and 2436 inch increments to be analyzed for ammonium and nitrate. The soils were air-dried and extracted with 2M KCl.\nAmmonium and nitrate concentrations from each sampling depth were determined using colormetric analysis using a Lachat Quickchem 8500 (Lachat Instrument, Denver, CO).\nNormalized Difference Vegetative Index Measurements. Remote sensing measurements were initiated one week after side-dress N applications using a Greenseeker handheld crop sensor (Trimble Navigation Limited, Sunnyvale, CA). The Greenseeker measures the normalized difference vegetative index (NDVI), a measurement of the reflected near infrared and red light from the crop canopy. Greenseeker measurements were taken for 5 weeks after N application. Normalized difference vegetative index measurements were taken on the second row of the plot and the sensor height was 36 inches above the canopy. Normalized difference vegetative index measurements were taken on the second row of the plot and the sensor height was 36 inches above the canopy. Reflectance measurements, like NDVI, have been correlated to chlorophyll content and N status in multiple crops. Given that S deficiencies occur in the upper crop canopy, NDVI may also be sensitive to variations in S status in crops.\nTissue and Petiole Sampling during Bloom. During first week of bloom twenty-four cotton petioles and leaves were sampled from the first and fourth rows of each plot. Research in Virginia has shown that petiole and leaf nutrient concentrations have a higher correlation to yield during the first week of bloom than later sampling intervals. The petiole and leaf were sampled from the fourth mature leaf below the apical meristem (bud) down the main stem. Cotton petioles and leaves were separated immediately to ensure accuracy of nutrient concentrations. Petioles and leaves were dried at 65 degrees C and ground to pass 1 mm and 0.5 mm sieve sizes for petioles and leaves, respectively. The petioles were analyzed for nitrate-N, phosphorus (P), potassium (K), and sulfur (S) and leaf tissue underwent a complete nutrient analysis at Water’s Agricultural Laboratory (Camilla, GA).\nDefoliation and Lint Yield. Defoliation timing of cotton varies, depending on the growing season and development of the crop. The cotton during this trial was defoliated when 40-60% of the bolls were opened. This timing was based upon treatments with a total N application rate of 100 pounds N per acre. High N application rate can delay maturity in cotton. However, given that the upper southeast coastal plain falls within the northern latitudes of the U.S., cotton production, any delay in maturity can be detrimental to the crop yield. This range in percentage of open bolls will allow for some adjustment based on the development of the crop during the study, but falls within the recommendations for defoliating cotton in Virginia and North Carolina (Edmisten, 2012; Wilson 2015).\nThe cotton from the center two rows was harvested with a Case International two row cotton picker. Seed cotton was weighted from each plot, then a onepound subsample was taken and ginned to determine lint turnout. Lint yield will be calculated from the percent turn out from the ginning process and seed cotton weights from the harvested rows. The ginned lint was sent to the USDA cotton quality lab in South Carolina for lint quality analysis. The lint was analyzed using a High-Volume Instrument (HVI), where length (stable), strength, micronaire, color, and leaf (trash) grade were determined.\nStatistical Analysis. The statistical analysis of the experiment included a regression analysis and analysis of variance (ANOVA), conducted using SAS 9.2 (SAS Institute, 2009). The data set was separated in order to conduct the appropriate statistical analyses, based on treatment design, to determine the effect of S fertility on NDLVI, petiole and tissue S concentrations, and lint yield quality. The first dataset compared granular NS fertilizers to fluid N/S fertilizers with varying S application rates at a fixed N rate of 100 pounds N per acre. The treatment design for the first dataset was a 2X4 factorial design with two N/S fertilizer sources and four S rates.\nThe N/S fertilizer sources were blends/ formula-tions of granular urea+AMS and fluid UAN+ATS solutions. The S rates were 0, 10, 20, and 30 pounds S per acre for each fertilizer source. Fisher’s LSD mean separation procedure was used to detect treatment differences at: a = 0.05 level of significance. Regression analyses were used to describe the response of each dependent variable to increasing S application rates. The second dataset compared the interaction of N rate and S rate using N-S fluid fertilizers with varying N:S ratios. The treatment design for the second dataset was a 3X4 factorial design with three total N rates and four N:S fluid fertilizers.\nThe three total N application rates were 60, 100, and 140 pounds N per acre and the four N:S fluid ratios were 32:0, 8:1 4:1 and 2.66:1. Fisher LSD mean separation procedure was used to detect treatment differences at a = 0.05 level of significance.\nResults and Discussion\nGeneral comments. The 2017 growing season had more favorable weather for cotton production than 2016, particularly during the boll opening /defoliation period of the growing season. Yields in Virginia averaged over 1,100 pounds of lint per acre and were the second highest production year in Virginia’s history. Yields across all locations were high with the highest yields reported at the Southampton location in 2017. There was no substantial period when rainfall or soil moisture was limiting at any location. Due to seed availability of Phytogen 499 WRF, the variety planted across all locations in 2017 was Phytogen 450 W3FE. This variety was chosen due to its similar maturity and performance in the 2016 Virginia cotton official variety trials. However, this variety was variable in its performance across locations and its environment. Large differences were found in plant available nitrogen (PAN) (soil ammonium-N + nitrate-N) in the top 36 inches of soil at each location (Table 2). The Lewiston location had the highest PAN among the sites with 12.3 ppm ammonium N and 15.2 ppm nitrate N, which would be equivalent to 55 lbs. N per acre of PAN prior to planting at this site. This is over 50% of the recommended N rate for cotton in Virginia. Southampton had the lowest PAN of any location with an equivalent N rate of 14 lbs. N per acre prior to planting.\nNormalized Difference Vegetative Index (NDVI). Normalized difference vegetative index at TAREC was very similar to that measured in 2016.\nDifferences in NDVI were more were more prevalent early in the growing season, differences becoming limited later in the growing season (Table 3). Once the cotton closed the row middles the biomass, with a handheld sensor, saturated NDVI, thus only the unfertilized control was significantly different from the fertilized treatments. Visually, some greenness differences were observed visually, however, the handheld sensor and/or NDVI was not able to differentiate the visual differences. A sensor mount on a UAV might be a better option for measuring reflectance on cotton, as the higher altitude will prevent sensor saturation due to a large canopy near to the sensor.\nGranular vs. Fluid N/S Formulations.\nTissue N and S concentrations varied in response to N/S formulation and S application rate among locations. The TAREC location was the only location where petiole nitrate-N concentrations were significantly higher with the fluid N/S formulations compared to the broadcast formulations with 10,262 and 4,737 ppm nitrate–N, respectively. However, petiole and leaf S concentrations were significantly higher when a broadcast system was used at two of the three locations (Figures 1 and 2). When evaluating S application rate across S fertilizer formulations there was no interaction between formulation or application rate. Petiole and leaf S concentrations increased with increasing S application rates in a quadratic function for all locations (Figures 3 and 4). Petiole S concentrations ranged from 400 to 600 ppm S for the 0 lb S per acre rates over 900 to 1,400 ppm S at the 30 lbs. S per acre rate (Figure 3). For leaf tissue S concentrations, the 0 lb S per acre rate ranged from 0.3 to 0.4 to 0.5% and 0.8% for Lewiston, TAREC, and Southampton, respectively (Figure 4). There were no samples taken from any location where leaf S concentrations fell below current deficiency threshold of 0.25% (Figure 4). When N application rate was kept constant, the petiole nitrate-N: petiole S and leaf N: leaf S rations decreased as S application increased, regardless of N/S formulation (data not shown). Nitrogen to S ratios for petiole and leaf tissue were not a good predictor of lint yield in 2017 as they were in 2016. This may be due to the limited response to application of N/S during 2017 and no ratio exceeding 30:1 for petiole nitrate-N:petiole S as in 2016 when a yield penalty was observed. More data are needed to develop N:S ratios for cotton during bloom for petiole and leaf tissue samples. Lewiston was the only location in 2017 where differences among S application rates and NS formulations were observed (Figure 5). However, the 0 and 10 pound S application rates produced the highest lint yields during 2017, and lint yield decreased slightly with increasing application rates (Figure 5). The fluid N/S formulation with 0lb S per acre application rate produced the highest yields at Lewiston and was significantly higher than the 20 and 30 lb S application rates regardless of formulation (Figure 5). At the Southhampton and TAREC locations no differences in S application and N/S formulations were observed for lint yield (Figure 6). However, at TAREC yields followed a similar trend with deceasing lint yields with increasing S application rates. The Southhampton location increased slightly with increasing S application rates and was also the highest yielding site in 2017 (Figure 6). The limited response to S application rates may be due to soil type and leached S in a subsoil level that is accessible. Since all soil samples were saved during the study, they will be sent out for analysis of S concentrations down to depth.\nNS Combinations at Various N Rates.\nThe second half of the study focuses on the use of fluid N/S formulation with differing N/S ratios to supply adequate S at lower N application rates. All three locations were very responsive to fluid N/S formulations with increasing petiole S (Figure 7) and leaf S concentrations (data not shown) as the S analysis of the fluid N/S fertilizer increased. Petiole nitrate-N: Petiole S and leaf N:leaf S ratios were significantly different at two out of three locations for the petiole ratio and three out of three for the leaf ratio (Table 4). The petiole N:S ratio was very low at 60 lbs N per acre application rate, and overall, lower than the other locations. Leaf N:S ratios were consistent across locations in 2017, which may be indicative that leaf N:S would be a more consistent predictor of S status. Lower N application rates and higher S analysis of the fertilizer resulted in lower petiole and leaf N:S ratios across all locations (Table 4). The increase in leaf and petiole S concentrations indicate that cotton was taking up the added S, however no lint yield responses were observed by increasing the S percentage in the fertilizer blend. Lewiston was the only location in 2017 where ling yields differed among the three N application rates (Figure 8). The 100 lbs. N per acre rate maximized yields at the Lewiston location, whereas no response to N application rate was observed at TAREC from 60-140 lbs. N per acre (Figure 8). There was a slight increase in lint yields at the Southhampton location, though this increase was not statistically significant (Figure 8). When lint yield was regressed over all N application rates and N/S combinations during 2017, the optimum N rate was found to be 100 lbs N per acre (Figure 9). However, each location had unique N response curves and yield levels during the 2017 growing season. There was no response to increasing the S content of fluid UAN solutions during the study in 2017. All locations in 2016 had a slight increase in lint yield from 32% UAN to 24-0-0-3S solutions. Soil types at each location during 2017 were heavier in texture and may have had S contributions from the subsoil where leached S may be retained. All soil samples were from the past six site years. This will allow inferences to be made about S contributions from the subsoil and predict when applying S will be needed to achieve optimum lint yields in cotton.\nOverall, the 2017 growing season was marked by favorable weather conditions and high lint yields across locations. The cotton variety selected for 2017 showed increased variability in lint yields across environments, though lint yields were satisfactory. Also, this variety did not perform well in the 2017 Virginia cotton variety testing program, thus in 2018 another variety may be selected that is a more consistent high yielding variety across environments. Tissue S concentrations were very responsive to N and S application rates during 2017. The broadcast (dry) N/S management system produced higher petiole and leaf S concentrations compared to fluid N/S sources. Nitrogen rate was the most limiting nutrient at each location during the study and lint yields were maximized at 100 lbs. N per acre across locations, regardless of yield potential. No response to S was observed during 2017. This may be due to varietal S usage or the mining of S from heavier textured subsoil where S has been leached from sandy loam surface horizons. Soil samples taken for PAN analysis will be sent off to determine S availability in lower soil horizons. The differences observed between 2016 and 2017 indicate environment and soil type play critical roles for N/S management in cotton. Sandier soils will be selected during the 2018 growing season to ascertain the effect of soil type on N/S management in the upper Southeast coastal plain.\nDr.W. Hunter Frame is Field /Crops Agronomist, Department of Crop and Soil Environmental Sciences, Tidewater Agricultural Research and Extension Center, Suffolk, Virginia 23437.']	['<urn:uuid:64cd3672-11f1-4674-8ea3-99ff38314123>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:18:19.158519	12	47	2820
77	agricultural data sharing benefits risks	Agricultural data sharing through platforms like OpenTEAM offers benefits including improved soil health management, carbon measurement, and access to decision-support tools. However, there are also risks that must be managed: farmers' data must be protected with security safeguards against unauthorized access, modification, or disclosure. Additionally, data cannot be used for unlawful activities like commodity market speculation, and farmers must be notified of any third-party data sharing and have the option to opt out.	"['FREEPORT, MAINE and WASHINGTON, D.C. — Wolfe’s Neck Center for Agriculture & the Environment, together with founding collaborators Stonyfield Organic, the USDA’s The Land-Potential Knowledge System (LandPKS) is a simple free and open source software tool and mobile application developed by USDA/ARS that supports land use planning and management. It includes modules that allow non-soil scientists to (a) determine the sustainable potential ... project and Foundation for Food and Agriculture (FFAR), today announced the launch of OpenTEAM, the first Open source is a publicly accessible software design that can be modified and shared by multiple users. This allows the source code to be inspected and enhanced by anyone. Because open source utilizes multiple collaborators, it allows for more control, increased security and stab... technology ecosystem in the world to address soil health and mitigate climate change. OpenTEAM is projected to provide quantitative feedback on millions of acres of farmland by 2024.\nOpenTEAM, or Open Technology Ecosystem for Agricultural Management, is a farmer-driven, Interoperability is a characteristic of a product or system whose interfaces are completely understood to work with other products or systems, at present or in the future, in implementation or access, and without any restrictions. While the term was initially defined for informat... platform to provide farmers around the world with the best possible knowledge to improve soil health.\nCurrently, farmers are faced with an ever-expanding assortment of decision-making software; however, these tools often do not “communicate” with each other, making it difficult to transfer, share or use by farmers and scientists or in supply chains. With OpenTEAM, farmers are not only in control of their own data, but also able to enter data once to access all available tools in the OpenTEAM collaborative.\nOpenTEAM offers field-level carbon measurement, digital management records, remote sensing, predictive analytics and input and economic management decision support in a connected platform that reduces the need for farmer data entry while improving access to a wide array of tools. The platform will support adaptive soil health management for farms of all scales, geographies and production systems. OpenTEAM will also accelerate scientific understanding of soil health by providing more high-quality data to researchers collaborating on the project.\nTo date, more than one dozen organizations have joined to develop, fund, and implement OpenTEAM. These include The Soil Health Partnership; General Mills; Colorado State University/USDA-NRCS Comet Farm; Applied GeoSolutions, LLC; DNDC Applications, Research and Training; Dagan, Inc.; Michigan State University Global Change Learning Lab; Purdue University Open Technology and Systems Center (OATS); University of British Columbia Center for Sustainable Food Systems; Regen Network; Our.Sci; Quick Carbon is a growing academic research initiative and protocol for rapidly assessing soil carbon contents across landscapes. Quick Carbon creates an accessible measurement system that empowers individuals to generate reliable soils carbon data for the purpose of ecological u... at Yale F & ES; U.S. Cover Crop Council decision tools; Sustainability Innovation Lab at Colorado (SILC); The University of Colorado Boulder; and A free and open-source web-based farm management and record keeping system that also provides a platform for the creation of custom modules and integration across decision tools, observation tools and agricultural internet of things devices..\nWolfe’s Neck Center will coordinate OpenTEAM from its headquarters on more than 600 acres of conserved landscape and farmland on the coast of Maine. Implementation and demonstration will begin in fall 2019. Field testing will continue in the 2020 growing season across the U.S. and international OpenTEAM Hubs will serve as a primary testing ground for the OpenTEAM platform, and their experience and feedback will inform OpenTEAM’s user interface design and calibration of the tools that are part of the OpenTEAM platform. Hubs are a center for a representative group of fa... farm networks.\n“At Wolfe’s Neck Center, we are collaborating to create solutions that address climate change through regenerative agriculture,” said Dave Herring, executive director, Wolfe’s Neck Center for Agriculture & the Environment. “OpenTEAM pairs agriculture with open source technology to accelerate soil health right here in Maine and around the globe.”\nThe more than $10 million public-private collaboration is made possible by a $5 million grant from FFAR, with more than $5 million matching contributions coming from across the network, including a $200,000 grant from The Stonyfield Foundation and $200,000 in in-kind contributions and a grant from Stonyfield Organic.\n“Optimizing soil management practices not only improves soil health, but also protects the environment,” said FFAR’s Executive Director Sally Rockey. “At scale, OpenTEAM can improve soil management practices for farmers around the globe and mitigate the effects of climate change.”\n“Stonyfield is strongly committed to reducing our greenhouse gas emissions,” said Britt Lundgren, director of organic and sustainable agriculture at Stonyfield Organic Yogurt. “Over half of our emissions come from agriculture, so in order to hit our target we know we need to work with the farms who provide our ingredients and help them reduce their emissions and sequester more carbon. OpenTEAM will enable us to do this, and track farms’ progress so we can be confident we’re hitting our goals.”\n“We are inspired by the level of collaboration, leadership and vision our OpenTEAM partners have provided,” said Dorn Cox, PhD, research director, Wolfe’s Neck Center. “Through the power of open technology, we aim to make what was invisible visible and in so doing the unvalued valuable.', ""A list of definitions that are relevant to the agricultural sector. For example: what types of data are going to be collected, what is considered to be personal data, agricultural data, individual farm data, raw data, aggregated data, data originator, data provider...\nThe Codes of Conduct\nThe tool features the following clauses from which the users should be able to select a clause if they think it is relevant and proceed to a checkout where the selected clauses can be used as an output to a document.\nThese clauses are not intended to be exhaustive and are no substitute for a robust institutional framework to guide and operationalize decision making concerning privacy, ethics.\nClick the + sign next to Clauses that are relevent to you to start building your Code of Conduct\nIt should be mentioned that farmers own information generated on their farming operations. In particular whoever has produced/collected the data either by technical means or by himself, or who has commissioned data providers for this purpose, has a leading role in controlling the access to and use of data from their business and to benefit from sharing the data with any partner that wishes to use their data. Providers should preserve the ability of the farmer to determine who can access and use individual farm data. However, it would be good for the farmer to agree upon data use and sharing with the other stakeholders with an economic interest, such as the tenant, landowner, cooperative, owner of the precision agriculture system hardware, and/or an Ag Tech Provider(ATP) etc.\nCollection, access and use of farm data should be granted only with the affirmative and explicit consent of the farmer. Via a contractual arrangement the collection, access, storage and use of agricultural data can be occurred only with the explicit informed permission of the data originator. Consent must be freely given, specific, informed and unambiguous. In order to obtain freely given consent, it must be given on a voluntary basis. The element “free” implies a real choice by the farmer. Any element of inappropriate pressure or influence which could affect the outcome of that choice renders the consent invalid. For consent to be informed and specific, the farmer must at least be notified about the provider’s identity, what kind of data will be processed, how it will be used, to whom it will be disclosed and the purpose of the processing. The farmer must also be informed about his or her right to withdraw consent anytime.\nProviders must only collect, use and share farm data for the purposes that have made clear to the farmer. No reuse of data is allowed for different purposes than those that had been originally agreed.\nData originators(farmers) must be notified that their data is being collected and about how and to whom the farm data will be disclosed.\nData originators(farmers) should be notified about what types of farm data is being collected, as well the purposes for which agribusinesses, Ag.Tech Providers(ATP’s) collect, use and share data in a more transparent way (e.g. algorithms).In addition information should be provided about how farmers can contact e.g. the ATP’s with any inquiries or complaints, be also aware of the third parties to which their data is disclosed and any risks that may affect farmers who share data with the providers.\nAll agribusinesses’, ATP’s policies, principles and practices should be transparent and consistent with the terms and conditions in the legal contacts. No contract change can be effective without the other party’s agreement.\nWithin the context of the agreement and retention policy, the data originator (farmer) should be able to have the following rights:\n- Right to portability: Data providers should be responsible for making individual farm data easily available to farmers.\n- Farmers should be able to retrieve their individual farm data in both processed(cleaned) and unprocessed form for storage or use in other systems, with the exception of the data that has been made anonymous or aggregated and is no longer specifically identifiable.\n- Right to remove, destroy, erase (the right to be forgotten) or return data to the data originator.\nProviders should recognise the originator’s right to benefit or be compensated for the use of data they originated.\nAn agribusiness, an Ag.Tech.Provider( ATP) will not sell and/or disclose individual farm data to a third party without first securing a legally binding commitment to be bound by the same terms and conditions as the ATP has with the farmer. Farmers must be notified if such sale is going to take place and have the option to opt out or have their data removed prior to that sale. An ATP will not share or disclose original farm data with a third party in any manner that is inconsistent with the contract with the farmer. If the agreement with the third party is not the same as the agreement with the ATP, farmers must be presented with the third party’s terms for agreement or rejection.\nEach agribusiness, ATP should provide for the removal, secure destruction and return of individual farm data from the farmer’s account upon the request of the farmer or after a pre-agreed period of time. The ATP should include a requirement that farmers have access to the data that an ATP holds during that data retention period.\nFarmers must be given the possibility to opt out of the contract and terminate the collection and usage of their data provided that it's stated in the contract and the data originator is informed about the consequences. Procedures for termination of services should be clearly defined in the contract.\nData should not be used for unlawful or anti-competitive activities, such as misuse of power and information for farmers?? A prohibition on the use of farm data by the agribusinesses, ATP’s to speculate in commodity markets.\nThe contract should mention farmer’s privacy, security and confidentiality responsibilities and measures that data users/providers should take. Farm data should be protected with security safeguards against risks such as loss or unauthorised access, destruction, use, modification or disclosure. Notification policies and measures in the event of a breach should be established.\nTerms of liability should be defined. The contract should also acknowledge the rights of all parties to protect sensitive information via restrictions on further use or processing. Protection of sensitive data such as personal/financial data, confidential information, trade secrets, intellectual property rights against tampering should be ensured.\nProviders should be responsible for making a clear contract and easily understandable to farmers. Contracts for ag data should use simple and plain language.\nIn addition contracts will clearly specify: (1) important terms and definitions, (2) the purpose of collecting, sharing, and processing data, (3) rights and obligations of parties related to data, (4) information related to storage and use of ag data; (5) verification mechanisms for the data originator; and (6) transparent mechanisms for adding new uses.\nData certification schemes develop transparency and trust around data uses. How to monitor a code or accreditation requirements. Establishment of an independent Supervisory Authority that could evaluate, if the contracts comply with these principles. Compliance with the codes of conduct should be rewarded. All stakeholders who respect these principles should submit their contracts and policies for evaluation by an audit team of an independent accredited organization. Upon evaluation a certificate of compliance will be issued.\nAll stakeholders who work and develop Codes of Conduct shall comply with local and international laws.\nThis online tool was created by GODAN, CTA and GFAR initially from the review of the existing codes of conduct, voluntary guidelines and principles relevant for farm data sharing. It is produced in the context of a consultative process taking place in the GODAN/CTA Sub-Group on Data Codes of Conduct and around a planned global collective action on Empowering Farmers through Equitable Data Sharing.\nThe initial draft was circulated for input and feedback during the first KTBL workshop on codes of conduct and also during the virtual Kampala Hackathon and incorporated into this edition.\nThe proposed clauses have been selected from the US American Farm Bureau Federations’ Privacy and Security Principles for Farm Data, the EU Code of conduct on agricultural data sharing by contractual agreement and the Australian Farm data code during the consultation as a first starting point for developing an ag code of conduct.\nIt is an evolving tool, with recommendations for a general, scalable and further customisable code of conduct template that best addresses farmers’ needs around fair and responsible data sharing.\nThe next stage is to consult externally for further input.""]"	['<urn:uuid:09557d1a-c813-46b3-86f7-ee1afce9a6ef>', '<urn:uuid:a119849c-b63a-46d1-9e08-62573783a4cd>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	5	73	2304
78	I'm interested in ocean-bottom seismic technology - what makes it particularly useful for collecting data in areas with many oil production facilities in the Gulf of Mexico?	Ocean-bottom-cable (OBC) and ocean-bottom-sensor (OBS) technologies are useful in congested production areas because ocean-floor sensors, once deployed, are immobile and can be positioned close to platforms, well heads, and other obstructions that interfere with towed-cable operations. Additionally, these technologies allow for four-component (4-C) data acquisition, enabling reservoir imaging with both P-P and P-SV wavefields.	"['Geophysics research at the Bureau concentrates on developing multicomponent seismic technology that can be used to better characterize geologic systems. Research effort focuses on design of vector seismic sources, optimization of data-acquisition and data-processing procedures, and unified interpretation of P- and S-wave images. Researchers use 9-component (9-C) seismic data whenever possible as these data allow all seismic wave modes (P, SH, SV, C, fast-S, slow-S) to be used in subsurface imaging. Research involving 3-C and 4-C seismic data is also a key focus area, even though SH and SV modes are not available with these data, as 3-C and 4-C data are acquired by companies more often than are 9-C data.\nImaging Deep Gas Targets with Multicomponent Seismic Data\nOperators across the Gulf of Mexico (GOM) are targeting deeper and deeper drilling objectives. For deep targets to be evaluated, seismic data are required that have longer and longer source-receiver offsets. Most shallow-water operators in the GOM consider 30,000 ft (9 km) to be the greatest target depth to be drilled for the next several years. For geology at depths of 9 km to be imaged, seismic reflection data must be acquired with offsets of 9 km or more.\nThis long-offset requirement is difficult to achieve using towed-cable seismic technology in areas that are congested by production facilities, which is the situation for many shallow-water blocks across the northern GOM shelf. Ocean-bottom-cable (OBC) and ocean-bottom-sensor (OBS) technologies are logical options for long-offset data acquisition in congested production areas because ocean-floor sensors, once deployed, are immobile and can be positioned close to platforms, well heads, and other obstructions that interfere with towed-cable operations. An additional appeal of OBC seismic technology is that four-component (4-C) data can be acquired, allowing targeted reservoir intervals to be imaged with P-SV wavefields, as well as P-P wavefields. Once 4-C seafloor receivers are deployed, source boats can maneuver along a receiver line to generate P-P and P-SV data from long-offset distances.\nThis research is designed to investigate the value of long-offset multicomponent seismic data for studying deep-gas geology across the northern shelf of the GOM. The term long offset means that 4-C OBC data are processed using uniformly sampled source-receiver offsets ranging from 0 to 10 km. The study area is a large, 3,200-mi2 (8,200-km2) section of the Louisiana shelf noted for prolific gas production. Data consist of parallel north-south and parallel east-west 2-D4C profiles spaced at intervals of 2 mi. P-P and P-SV images produced from these long-offset reflection data are interpreted to determine the relative depth-imaging capabilities of each seismic mode. Analysis of these long-offset data shows that the P-P mode contains reflection signals from depths of 60,000 ft (18 km), which is deeper than any reported seismic reflection effort in the GOM basin. Equally important, the critical P-SV mode has a reflection signal from depths of 42,000 ft (13 km).\nAssessing Deep-Water Gulf of Mexico Gas-Hydrate Systems with Multicomponent and Multi-frequency Seismic Data\nThe Exploration Geophysics Laboratory (EGL) is a subcontractor to Louisiana State University in this study. Research is funded by the U.S. Department of Interior, Minerals Management Service. The study will investigate several seafloor fluid-gas expulsion sites in the deep-water area of Green Canyon in the northern Gulf of Mexico. Sites will be selected that are traversed by one or more of WesternGeco\'s deep-water, multiclient, 4-C OBC seismic lines. Multifrequency imaging of gas-hydrate systems associated with these expulsion chimneys will be done by acquiring 2- to 16-kHz chirp-sonar data using an autonomous underwater vehicle (AUV) and 500- to 2,000-Hz data with the Naval Research Laboratory Deep-Towed Acoustic/Geophysics System (DTAGS). These AUV and DTAGS images will then be compared with 4-C OBC images along each profile that is analyzed.\nProcessing and Interpreting Multicomponent Seismic Data: Gulf of Mexico Seafloor Observatory\nThe Exploration Geophysics Laboratory (EGL) is a subcontractor to the Mississippi Mineral Resources Institute (MMRI) in this study. MMRI has received funding from a variety of sources (DOE, MMS, NOOA) to construct a seafloor observatory across a gas-hydrate system in Mississippi Canyon Block 118. EGL\'s responsibilities are to process and interpret seismic data acquired by the horizontal and vertical arrays of 1-C and 4-C seismic sensors that will be deployed across the observatory. Downgoing P waves from three types of sources will be used to illuminate gas-hydrate strata: conventional surface-positioned air-gun arrays, noise from passing ships, and wind-driven ocean-surface waves. Data will be acquired in a continuous round-the-clock mode when wind-driven surface waves are the imaging source. Data acquisition will be repeated at appropriate calendar-time intervals to investigate dynamic processes that occur in gas-hydrate systems.\nCO, DO: Horizontal array (400 m long)\nDC: Data Cable\nDRS: Data recovery ship\nDSS: Data storage system\nSSD: Station service device\nVA1: Water-column vertical array (200 m)\nVA2: Subsea vertical array (150 m)\nW: Weight (disposable)\nThe principal components of the seafloor observatory that will operate in Mississippi Canyon Block 118.\nDistinguishing Fizz-Gas and Commercial-Gas Reservoirs\nThis investigation applies 4-component ocean-bottom-cable (4-C OBC) seismic data across gas-producing trends of the Gulf of Mexico to the problem of distinguishing between commercial-gas reservoirs and low-saturation, noncommercial, fizz-gas reservoirs. Attributes of the P-SV mode are combined with calibration log data to classify high-amplitude P-P reflection anomalies into ""high-saturation"" and ""reduced-saturation"" categories. Two reservoir examples are provided as Figures 1 and 2. One example is a commercial-gas reservoir; the other is a fizz-gas reservoir. The gas saturation will not be defined for either example so the difficulty of distinguishing between the two types of targets can be appreciated.\nResearch data utilized in the study are provided by Devon Energy, Seitel Data, and WesternGeco. Funding is provided by EGL sponsors.\nElastic-Wavefield Seismic Stratigraphy: A New Seismic Imaging Technology\nClick here for Technical Progress Report (May 2004) posted on DOE Website\nThe Exploration Geophysics Laboratory (EGL) is partnering with Fasken Oil and Ranch Ltd. and Vecta Technology to develop a new seismic interpretation technology, Elastic Wavefield Seismic Stratigraphy. This technology is based on the physics that each mode of an elastic wavefield can, and often does, image a suite of stratal surfaces differently than do the other elastic modes. Shear (S) modes can image seismic sequences and facies not observed in the compressional (P) mode, which is the only elastic-wave mode used in conventional seismic stratigraphy. In a homogeneous Earth, a full-elastic (9-component) seismic wavefield yields three S-wave modes: SH-SH (horizontal shear), SV-SV (vertical shear), and P-SV (converted shear). In an anisotropic Earth, each of these S modes splits into S1 (fast-S) and S2 (slow-S) modes controlled by the principal axes of anisotropy. Thus, there is a rich source of stratigraphic information in a full-elastic wavefield that is not being utilized in conventional P-wave seismic stratigraphy studies. The objectives of this research are to create compelling examples that prove that different stratal surfaces are imaged by different elastic-wave modes, to develop systematic relationships between petrophysical properties and combinations of elastic-mode sequences and facies, and to demonstrate how this new seismic imaging technology should be applied to improve geologic understanding of oil and gas systems.\nExploring for Subtle Mission Canyon Stratigraphic Traps with Elastic-Wavefield Seismic Technology\n(A study funded by the U.S. Department of Energy and Vecta Exploration)\nThis study is funded by the U.S. Department of Energy and Vecta Exploration. In its subcontract to Vecta Exploration, Inc., the Exploration Geophysics Laboratory (EGL), is developing a new seismic technology to explore for subtle Mission Canyon oolitic limestone reservoirs in the Williston Basin. This technology will be based on the acquisition and application of full-elastic (9-component) seismic data. Mission Canyon reservoirs are elusive targets when exploration is based on conventional compressional (P) wave seismic data. The attraction of 9-component (9-C) seismic data is that three shear (S) wave modes can also be used for target imaging: SH-SH (horizontal shear), SV-SV (vertical shear), and P-SV (converted shear) modes. Work at EGL has shown that each mode of an elastic wavefield can, and often does, image stratal surfaces across a target interval differently than do other elastic modes. Thus, any of the S modes can depict seismic sequences and seismic facies that are not observed using P waves. This rich, expanded source of stratigraphic and lithofacies information in full-elastic seismic wavefields needs to be utilized in Mission Canyon exploration. The objectives of this study are to acquire, process, and interpret 9C3D seismic data across Mission Canyon plays, develop relationships between drilling objectives and elastic-wavefield attributes, drill confirmation wells, and then share research findings so that full-elastic seismic technology can be applied to improve oil exploration across other areas.\nNarrow Azimuth Migration\nSergey B. Fomel, principal investigator\nWe are developing the theory, numerical algorithm, and prototype implementation of narrow-azimuth migration, a powerful and efficient method for 3-D seismic imaging of marine streamer data.\nNarrow-azimuth migration takes advantage of the narrow-azimuth character of data acquired by marine streamers to increase the efficiency of seismic imaging by wavefield extrapolation methods. A new theory is being developed to account for inadequate approximation in common-azimuth migration and corrected for small rotation in the offset azimuth of the propagating wavefield. We anticipate that this theory will lead to a noticeable improvement in current seismic imaging technology. An additional increase in efficiency comes from implementing the narrow-azimuth algorithm on parallel computer clusters.\nDifferential Azimuth Moveout\nSergey B. Fomel, principal investigator\nWe are developing the theory, numerical algorithm, and prototype implementation of differential azimuth moveout, a new powerful method for regularizing 3-D seismic reflection data.\nIn theory, differential azimuth moveout is represented by a partial differential equation, whose role with respect to integral (Kirchhoff) azimuth moveout is similar to the role of the wave equation with respect to Kirchhoff migration. In practice, we anticipate differential azimuth moveout to behave as a compact, local, accurate, and efficiently computed regularization operator. The operator is applied iteratively to produce regular output from irregular input.\nPetrophysical Analysis of Multicomponent Seismic Data\nSergey B. Fomel, principal investigator\nWe are developing new technology for improved multicomponent seismic data analysis. The technology is based on creating angle gathers for both P-wave and S-wave amplitude-versus-angle (AVA) analysis using new powerful wave-equation imaging approaches. Unlike traditional amplitude-versus-offset (AVO) gathers, AVA gathers operate directly in the true reflection angle coordinates, providing a direct assessment for inverting P-wave and S-wave reflectivity for seismic lithology, porosity, and pore fluid content. A joint inversion of multicomponent data in the true angle coordinates opens new possibilities for direct detection of natural gas resources by discriminating between oil, water, and gas content in complex reservoirs during both exploration and production. This technology facilitates exploration in complex geologic areas, improves oil-and-gas reservoir characterization, increases the accuracy of petrophysical attributes estimation, and decreases the much higher costs of exploratory drilling and failed secondary recovery injection projects.\nUtilizing 3-D wave-equation migration for seismic imaging is a new approach that has shown great promise in imaging complex deep-water Gulf of Mexico structures. Previously only so-called Kirchhoff methods could be used to generate common-reflection point (CRP) gathers in offset domain and iteratively improve the velocity model used for imaging. Recent research has established a new approach to generating angle-domain common image gathers (ACIG) directly from 3-D wave-equation methods. ACIG\'s can be used to update the initial velocity model, and they form the basis for a novel method of 3-D amplitude analysis. This technology can be used for gas exploration in deep complex structures over 10,000 ft, where conventional, single-traveltime-arrival, Kirchhoff imaging fails to provide an accurate structural image and wave-equation imaging provides much higher structural resolution and amplitude fidelity. The geophysicist can then obtain higher resolution petrophysical information, linking the accurate seismic amplitude to reservoir properties such as porosity, sand/shale content, water/oil saturation, Vp/Vs ratio, etc.\nSeismic Imaging by Riemannian Wavefield Extrapolation\nSergey B. Fomel, principal investigator\nWe are developing the theory, numerical algorithm, and prototype implementation of seismic imaging by Riemannian wavefield extrapolation (RWE). RWE was proposed by Sava and Fomel (2004) for imaging steeply dipping and overturning reflections in geologically complex exploration areas, such as deep subsalt structures in the Gulf of Mexico.\nRWE belongs to the class of wave-equation extrapolation methods that are known to handle accurately large-velocity contrasts, multipathing, and band-limited wave propagation effects. Instead of conventional downward extrapolation, RWE employs a coordinate transformation to extrapolate waves numerically in a direction close to the preferential direction of natural wave propagation. As a result, one can accurately image large propagation angles, including overturning salt-flank reflections, using inexpensive extrapolation operators.\nDevine Test Site\nA Public-Domain Geophysical Field Laboratory Operated by the Exploration Geophysics Laboratory (EGL)\nThe 100-acre Devine Test Site (DTS) is located less than 50 miles southwest of San Antonio, Texas, in Medina County, Texas (Maps 1 and 2, below). The site is managed by the Exploration Geophysics Laboratory (EGL), an Industrial Associate Program at the Bureau of Economic Geology. It is a state-of-the-art public-domain geophysical research facility for academia and industry donated to UT in 1998 by British Petroleum (BP). The test site is used for surface-based seismic and potential-field experiments performed in conjunction with downhole and crosswell experiments.\nThe size and shape of the 100-acre field laboratory and the adjacent area for which surface-access rights can be negotiated with property owners is shown on Map 3. BP drilled three test wells (2, 4, 9) during its 12 years of ownership, which are cased to 3,000 ft. Wells 2 and 4 are completed with fiberglass casing to allow for testing of borehole electromagnetic (EM) sources and receivers. These wells are popular with logging companies as resistivity tools can be tested without the complications of steel casing attenuating EM wave propagation. Four shallow (100-200 ft) steel-cased holes are available for borehole-based seismic energy sources and other instrumentation. The wells are in excellent condition. Permanent concrete and gravel pads provide consistent vibrator coupling in a wide range of weather conditions.\nSignificant upgrades to the site have been made as a result of generous donations by the SEG Foundation. The site now has electricity, flood lights for nighttime use, a water well and water lines, new and refurbished chain fences, new entry gate, tractor mower, and storage sheds.\nThe stratigraphic section (shown below) breached by the site\'s 3,000-ft wells is well suited as a field laboratory for geophysical experiments. A key attribute of the site, which appeals greatly to developers of downhole geophysical instruments, is its stable geologic condition. The nearest oil and gas production is several miles away, which ensures that no fluid-exchange processes are occurring in rock facies immediately around the wellbores. Petrophysical properties of the formations, therefore, are well calibrated by numerous historical well logs preserved in the public database. Equipment manufacturers and researchers can also quickly determine if a new tool, a new measurement procedure, or a new petrophysical analysis procedure is providing correct data.']"	['<urn:uuid:46205e76-7946-4048-b84b-18eb91776161>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	27	54	2455
79	How does digital technology improve mining operations, and what environmental damage does mining cause to landscapes and ecosystems?	Digital technology like Maptek BlastLogic improves mining by providing a single platform to manage drill and blast processes, enabling better data integration, increased drilling accuracy, and efficient analysis. It allows real-time feedback, easier communication between departments, and detailed control of key performance indicators. On the environmental side, mining causes severe damage including huge holes in landscapes, erosion, loss of biodiversity, and contamination of soil and water. It leads to deforestation, destruction of habitats, and can turn areas into lifeless wastelands. The extraction process can disrupt ecosystems and threaten wildlife populations.	"[""Digitalisation of drill and blast\nBarrick Pueblo Viejo has improved day-to-day operations and provided greater business intelligence for future mining excellence through implementing Maptek BlastLogic.\nInnovation through digitalisation and technology is a key focus for Barrick. This is strongly evident at its Pueblo Viejo operation, an open pit gold mine in the Dominican Republic, particularly in the drill & blast department.\nThe Maptek blast design and reconciliation solution, that includes BlastLogic and Vulcan Drilling Designer, has given Pueblo Viejo a single platform to manage its drill & blast process from design to performance measurement. Integrated tracking of vibration, fragmentation, performance, cost and inventory of each blast facilitates continuous improvement from blast to blast and informs future planning.\n'Digitalisation is key to continuous improvement. Automating data generation enables innovation around operational processes that optimise the business', said Alejandro Rosario, Senior Drill & Blast Engineer at Barrick Pueblo Viejo.\nDirect benefits include increased drilling accuracy, efficient back analysis, enhanced online data management, cost savings for explosives, and a faster and easier improvement process.\n'As results of each blast are fed back automatically, we spend less time figuring out what we need to change in our designs. This enables us to apply improvements in real time', said Rosario.\nPrior to the implementation of BlastLogic, blast data existed in multiple silos. All machinery collected and compiled drill information, but a lot of blast data was recorded by hand and kept separate from the drill data. Accessing the disparate data was difficult and analysis was time-consuming.\nThe BlastLogic database now contains all related drill & blast data, enabling faster, easier, more effective analysis.\nImportantly, digitalisation enables benchmarking so that the effect of changes can be measured. For example, with all drill data integrated to blasting results, improvements in accurate charge placement are easily realised, delivering better fragmentation. Blasting can be performed with smarter and safer sequences.\nReal-time feedback of an electronic detonator misfire allows immediate identification of issues and avoids unexpected outcomes. Accuracy is critical for safety in dealing with explosives.\nThe administration of tasks and resources has also been enhanced, data analysis time has been reduced, and business intelligence can be applied to reduce costs and time spent. This has been achieved by substantially shifting the amount of engineering time from data entry, formatting, reporting and meetings to planning, compliance and improvement.\nAccess to digital drill and blast data allows easier communication of accurate information between departments. Data integration has provided:\n- Detailed control of drill & blast KPIs (daily, weekly, monthly, etc.), with parameters such as total metres drilled, re-drilling, over-drilling, penetration rate, powder factor, fragmentation, total explosive usage, tonnage removed.\n- The ability to audit contractors and the overall blasting process.\n- Synchronisation of drill & blast tasks into a single platform, with pattern design, charge plan, tie-up design, drilling, QA/QC, observations, fragmentation and vibration control.\n- Data and process transparency, including online access for managers and an increased ability to check indicators.\n- Correlation of data in 3D for simple analysis and summary dashboards to track technical, economic and operational opportunities for drill & blast. For example, drill depth and collar accuracy, redrills, over/under drilling and associated costs.\nRosario concluded, 'We ultimately chose BlastLogic for more than just the software capabilities. Along with the technology, we have access to a phenomenal support team helping us customise the platform specifically for our operations. We are paying for more than a fixed solution, one that can grow with our needs and is backed by excellent customer service.'\nSenior Drill & Blast Engineer\nBarrick Pueblo Viejo Mine"", ""Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ...""]"	['<urn:uuid:f6b19ab6-d8ef-451a-8a16-eda26a13bdd9>', '<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	18	90	2049
80	traditional 401k vs roth 401k retirement savings women	Traditional 401(k) and Roth 401(k) plans have key differences that are particularly relevant for women savers. With traditional 401(k)s, contributions come from pre-tax income and taxes are paid upon withdrawal during retirement. In contrast, Roth 401(k) contributions are made with after-tax dollars but withdrawals are tax-free. This can be especially beneficial for women, who typically save 43% less than men for retirement and have longer life expectancies. The Roth 401(k) also offers more flexibility with early withdrawals without penalties, which could help women who may need to exit the workforce temporarily to care for family. Both plans can include employer matching, but the Roth 401(k) has contribution limits of $6,000-$7,000 per year and income restrictions since it was designed for middle-class savers.	"[""Women face unique savings challenges, and many employers and wealth managers do not adequately address the differences between genders. By better understanding the savings challenges faced by women, employers will not only be able to help female employees meet their savings objectives, but also benefit from an increased ability to attract, retain and motivate their female workforce. Women can also improve their own financial wellness by prioritizing it as they would their families’ health and wellness. Employers and wealth managers have a great opportunity to help women see that importance.\nIn general, women save significantly less than men over their working lives. According to a 2018 study conducted by Prudential, the average woman has saved 43% less for retirement, compared to the average man. This greatly increases the likelihood of a woman living in poverty in retirement and is exacerbated by women’s longer life expectancies. Factors contributing to the savings gap include:\n1) Pay gaps between men and women\n2) A greater tendency for women to exit the workforce to care for children or parents\n3) A lack of confidence in making financial decisions\n4) Not taking on enough risk to meet their financial goals\n5) Focusing on short-term financial issues rather than long-term saving\nAlthough some employers are taking action to resolve the first two issues, employers and wealth managers must be more innovative to address the others. While improved financial knowledge is commonly viewed as a key to improved savings rates, Mercer has found that perceived financial knowledge, or ‘financial courage’, correlates more closely with improved savings outcomes. Women tend to have lower financial courage scores than men, even when they have comparable financial knowledge, which can lead to inertia in decision-making, failure to optimize savings rates, and poor financial wellness. For example, based on Mercer’s When Women Thrive research, 55% of women in the US believe they know less than the average investor, compared with 28% of men.\nDifferences in contribution levels are not the only hindrance to savings accumulation for women. In fact, women and men generally contribute to retirement plans in similar proportions, but women are less likely to pursue aggressive investments. Women have less comfort with the volatility in riskier investments, resulting in diminished investment returns over a lifetime. For instance, a 2015 BlackRock survey that found that the average female investor keeps 68% of her portfolio in cash, versus 59% for the average man, leading to diminished long-term returns.\nOur research also finds that short-term financial issues are more important to many women than long-term saving, particularly for those in a weaker financial position. Boosting long-term savings rates is crucial to improving women’s financial well-being.\nSolutions for Employers and Wealth Managers\n1) Focus on improving financial courage\nOne tactic to improve financial courage among women is to offer options that require limited financial knowledge but provide incremental wins, such as setting default options for contributions and investments. By giving women the opportunity to take gradual steps towards improving their savings rates, they receive positive feedback, driving them to become more engaged with their finances, building financial courage and spurring better decision-making in the future. Wealth managers and robo sites need to encourage systematic and regular contributions to savings and retirement accounts.\n2) Offer tailored retirement solutions to women\nSome organizations have begun to tailor their pension offering to women to improve their savings adequacy. Customized offerings that account for differences in needs and behaviors across genders correlate to higher participation and savings rates for women. Financial education and gender-specific communications are needed to show women the value of pursuing more aggressive investments and to direct women to programs that will help them improve their savings outcomes. These programs are growing in popularity, but from a low base, as according to Mercer’s When Women Thrive research, globally just 9% of organizations offer women-focused retirement and saving programs.\n3) Help women meet short-term financial needs\nExpanding the focus beyond saving for retirement to design programs that help employees to manage their finances holistically, through services such as debt management, budgeting and financial coaching, will help to improve results. This is where a financial advisor ought to be stepping up to the responsibility of addressing the holistic financial needs of clients, not merely their retirement savings.\n4) Present goals-based investing as the path to financial security\nAs women struggle to find time to focus on themselves, employers and wealth managers need to find a way to capture their attention to address their savings and investments. Appealing to a women’s interest in financial security would provide that urgency. Goals-based investing would be a natural fit for women to see the connection between their investment strategy and a secure future.\nThe Payoff for Employers\nEmployers who are able to improve savings outcomes for women will see significant benefits in their workforce. First, there is an opportunity for improved engagement and productivity, as the average woman spends 15 hours a week at work worrying about her finances. Next, offering savings programs that are tailored to women is critical to improving gender equity in the organization. This can have a tremendous impact on engaging and leveraging the female workforce, triggering better overall results for the organization.\nEmployers also have a unique opportunity to improve retention among female employees. A joint survey by Mercer, InHerSight and Ellevate Network revealed that women who are financially stressed are more likely to leave their job for a new company, higher education or unemployment. Finally, financial wellness also contributes to overall health and wellness. Women who report high levels of financial stress often have higher rates of absenteeism and increased healthcare costs. Alleviating that stress can reduce those costs.\nThe Payoff for Wealth Managers\nWealth managers who work with couples need to ensure the woman is fully integrated in the daily decision-making. Insisting on solely meeting and speaking with the male counterpart ensures that in the case of divorce or death, the female client will go elsewhere where she can build her own relationship with the advisor.\nEnsuring your female clients have been well-guided (not patronized) will ensure stickier assets and strong potential for referrals.\n Prudential. 2018. “The Financial Wellness Census”\n BlackRock. March 5, 2015. “American Women Face Saving for Retirement “Gender Gap” with a Lasting and Harmful Impact”\n Mercer’s Inside Employees’ Minds – Women and Wealth, 2017\nReferences to Mercer shall be construed to include Mercer LLC and/or its associated companies. © 2019 Mercer LLC. All rights reserved. This contains confidential and proprietary information of Mercer and is intended for the exclusive use of the parties to whom it was provided by Mercer. Its content may not be modified, sold or otherwise provided, in whole or in part, to any other person or entity without Mercer's prior written permission. Mercer does not provide tax or legal advice. You should contact your tax advisor, accountant and/or attorney before making any decisions with tax or legal implications. This does not constitute an offer to purchase or sell any securities. The findings, ratings and/or opinions expressed herein are the intellectual property of Mercer and are subject to change without notice. They are not intended to convey any guarantees as to the future performance of the investment products, asset classes or capital markets discussed. For Mercer’s conflict of interest disclosures, contact your Mercer representative or see www.mercer.com/conflictsofinterest.\nThis does not contain investment advice relating to your particular circumstances. No investment decision should be made based on this information without first obtaining appropriate professional advice and considering your circumstances. Information contained herein may have been obtained from a range of third party sources. While the information is believed to be reliable, Mercer has not sought to verify it independently. As such, Mercer makes no representations or warranties as to the accuracy of the information presented and takes no responsibility or liability (including for indirect, consequential, or incidental damages) for any error, omission or inaccuracy in the data supplied by any third party.\nNot all services mentioned are available in all jurisdictions. Please contact your Mercer representative for more information. Investment management and advisory services for US clients are provided by Mercer Investment Management, Inc. (MIM) and Mercer Investment Consulting LLC (MIC). MIM and MIC are federally registered investment advisers under the Investment Advisers Act of 1940, as amended. Registration as an investment adviser does not imply a certain level of skill or training. The oral and written communications of an adviser provide you with information about which you determine to hire or retain an adviser. MIM and MIC’s Form ADVs Part 2A & 2B can be obtained by written request directed to: Compliance Department, Mercer Investments, 701 Market Street, Suite 1100, St. Louis, MO 63101. Certain regulated services in Europe are provided by Mercer Global Investments Europe Limited, Mercer (Ireland) Limited and Mercer Limited. Mercer Global Investments Europe Limited and Mercer (Ireland) Limited are regulated by the Central Bank of Ireland. Mercer Limited is authorized and regulated by the Financial Conduct Authority. Registered in England and Wales No. 984275. Registered Office: 1 Tower Place West, Tower Place, London EC3R 5BU."", 'Recent surveys show that only 23-percent of millennials save a portion of their income. Of this 23-percent, the research shows that Americans between the ages of 21 and 37-years old prefer safe investment vehicles, and tend to avoid investing in high-risk assets, such as stocks.\nSaving is a critical part of building our future. With so many Americans living from paycheck to paycheck, it makes one wonder how these people will survive in their retirement years. If you have no clue on saving and investing, and you’re in the millennial age group, it’s vital that you understand the importance of putting money away at the end of the month.\nInvesting for Dummies – What You Need to Know\n- 1 Investing for Dummies – What You Need to Know\n- 2 The Beginners Dream – The 401(k)\n- 3 How 401(k) Works\n- 4 The Roth 401(k) Explained\n- 5 Traditional 401(k) Vs. Roth 401(k)\n- 6 Limitations to the Roth 401(k)\n- 7 Including the Roth 401(k) in Your Investment Strategy\n- 8 Tips for Selecting a Roth 401(k)\n- 9 The Roth 401(k) – Key Takeaway\nWhen people think of investors, most think of men in their early fifties, dressed in a $3,000 suit, puffing on a Cuban cigar while they stare at a ticker board in their 5th avenue corner office. However, this image of “success” does not fit everyone, and you don’t need to be a master of the universe that’s working on Wall Street to take advantage of saving and investing your money.\nSo, where do you start if you’re a newbie saver or investor? The idea of having savings and investments is to put your money to work for you, instead of working for your money. Your income is a representation of the hours you spend toiling at your job. For most of us, we won’t be earning millions of dollars a year, and every penny we save counts toward achieving our retirement goals.\nInvesting allows you to choose a vehicle and park your money in this asset, allowing it to grow over a specific term. When the contract ends, you receive your initial starting capital, plus a return on your money for investing it in the asset class.\nThere are thousands of investment vehicles available – So, which one will suit you? Speak to any professional investor, and they’ll tell you the most critical part of investing is understanding risk versus reward.\nInvesting in risky assets, such as stocks, may yield fantastic returns that could double or triple your money in as little as a quarter – but it could also wipe out all of your money in a matter of minutes if the market goes the wrong way. This example is a clear indicator of the risk versus reward conundrum.\nOn the flipside, a savings account is low-risk, but the rewards aren’t high, and you’ll be lucky if you earn any interest on your money at all – all you’re doing in this example is building your capital base.\nThe Beginners Dream – The 401(k)\nWe’ve all heard about the 401(k) investment plan. The chances are your company uses this financial vehicle to provide its employees with a means to invest in their retirement.\nSimply put, the 401(k) plan allows you to take a portion of your income and apply it to an investment vehicle that’s managed by a professional investor at a credible financial institution. In some cases, your employer will match your monthly contribution into the fund, allowing you to benefit from a doubled savings rate.\nThe asset management firm then takes the pooled funds and invests it into a portfolio of assets, which may contain property, stocks, bonds, and other financial vehicles. The fund manager spreads the risk across many different asset classes to “diversify” the portfolio. This strategy means that should one asset class fail; the other assets will not be at risk.\nDepending on the management company, the 401(k) investor may have the option to choose a low, medium, or high-risk profile for their investments. This tiered risk structure defines the types of assets held in the portfolio.\nFor example, a low-risk 402(k) may invest the funds in property and government bonds. These assets are traditionally low-risk, but they also offer low returns. The flipside would be a portfolio of assets containing stocks, business loans, and equity in companies.\nThis strategy is far riskier, and should the company fail, you 401(k) might lose a substantial amount of money. However, if it plays out the way the advisor expects, you could be looking at an excellent return on your capital.\nHow 401(k) Works\nThe 401(k) is not rocket science, and portfolio managers offer the product to people who are not savvy or accredited investors. The idea behind the 401(k) is the fact that you are not a professional investor. Therefore, you should trust a qualified and experienced investor to handle your money for you. As a result, you pay a percentage of your earnings to the management team in return for their services.\nYou make a monthly contribution from your paycheck towards your 401(k) plan, and continue to do so until your retirement. When it time to leave the workforce and rely on your nest egg, you withdraw your money to fund your retirement years.\nThe 401(k) has some rules you’ll need to follow to make the most out of your investment. 401(k) plans require you to leave your money in your 401(k) account for a specified duration of time, (anywhere from 20 to 40-years.)\nThere are plenty of 4012(k) plans available, each with different characteristics. The primary difference and concern for newbie investors is the tax they have to make on their contributions and distributions into and out of the fund. The popular plans for Americans include the traditional 401(k), as well as the Roth 401(k) – but what’s the difference between the two?\nThe Roth 401(k) Explained\nThe Roth 401(k) gets its moniker from Senator William Roth, who decided to create the plan some twenty years ago. Bills vision was to turn Americans back toward a culture of saving rather than spending. At the present moment, Americans are experiencing the highest consumer debt levels in the history of the country, along with the lowest savings rate on record.\nSenator Roth’s idea was to put the American people back into a position where they can benefit from making monthly contributions to their retirement – without limiting their taxable income.\nTraditional 401(k) Vs. Roth 401(k)\nThe primary difference between a Roth and traditional 401(k) resides in its taxation of contributions and distributions.\nThe Roth 401(k) model as outlined in section 402A of the Internal Revenue Code, was a part of the Economic Growth and Tax Relief Reconciliation Act of 2001. The act cut income tax rates in the wake of the 2001 recession, creating the Roth 401(k) to increase tax-deductible payments into IRA accounts.\nContributions to a traditional 401(k) come from the employee’s pre-tax income, and Roth 401(k) contributions come from the net income of the employee after paying taxes.\nWhen it comes time to distribute the funds in the 401(k) account, the traditional model requires you to pay taxes on your gains. The Roth 401(k) model differs in the sense that there is no taxation on the profits you make with your money, providing a tax-free payout.\nTherefore, with the Roth 402(k), you pay more taxes now but receive a significant tax break down the line when it’s time to cash out for your retirement.\nAs an example, a 35-year-old woman with an annual income of $80,000 – contributes $5,500 to a Roth 401(k) every year until she retires at the age of 67, may end up with the investment yielding $518,000 in savings at a return of 6-percent per annum.\nWith a traditional 401(k) model, the woman is now eligible for a 25-percent tax rate on her profits. However, with a Roth 402(k), the woman now owes the IRS nothing. While this may sound like a dream come true, it’s important to point out at this time that she was in a higher tax bracket while working, paying 30-percent of her income and investments to the taxman.\nHowever, since the post-withdrawal tax on Roth 401(k) plans are tax-free, she benefits from a $130,000 saving, allowing her to benefit from more money at her retirement.\nWith traditional 401(k) models, you are not allowed to withdraw your capital until your fixed retirement date. Should you choose to violate this part of the contract and withdraw your funds early, the contract will have clauses stating the financial services provider will penalize you for early withdrawal.\nWith a Roth 401(k) model, you can withdraw all or a portion of your funds at any stage of the investment term, with no penalty fee. This feature makes a Roth 401(k) the ideal model for anyone looking to save for their kids’ college tuition or a down payment on some real estate.\nResearch shows that millennials prefer Roth 401(k) plans to the traditional model, and they are more likely to make contributions to the Roth 401(k) than baby boomers or GenXers. It’s important to note that not all companies offer the Roth model, but for those that do, they find that more than 60-percent of employees opt for the Roth 401(k) over the traditional model.\nLimitations to the Roth 401(k)\nThe tax incentives for the Roth 401(k) are so generous that the government decided to place a cap on contributions to this plan. Holders of Roth 401(k) accounts may contribute up to a maximum of $6,000 per year to the investment, with those individuals over the age of 50-years allowed contributions up to $7,000. You can fund a Roth 401(k) account for 2019 through to the April 2020 tax deadline.\nThe Internal Revenue Service also limits who can apply for a Roth 401(k). The Roth 401(k) launched as a financial savings vehicle for the middle class, allowing them to make returns on their money to fund their retirement. The IRS does not feel that high-earning individuals qualify for this model, and restricts contributions to a Roth 401(k), or denies the use of the vehicle to high-income earners.\nThe early withdrawal benefit and tax-free incentive of a Roth 401(k) are attractive, but it’s critical to understand that this only applies to contributions, not to earnings. Should you choose to withdraw earnings before the end of the contract maturity date, you’ll experience two penalties; a 10-percent penalty, plus the taxes you pay on the amount withdrawn.\nThe “pay now, save later” feature of Roth 401(k) plans allows you to build up your capital paying tax in your current tax bracket. When you retire in a higher tax bracket, you don’t owe the taxman any backdated amounts on your previous taxable income.\nIncluding the Roth 401(k) in Your Investment Strategy\nThe Roth 401(k) is best suited for young people entering the workforce. During the early years of your career, the chances are that you’ll be in a much lower tax bracket than when you reach the peak of your career in your forties and fifties.\nTherefore, you benefit from contributing to your Roth 401(k) in the formative years of your career, as the tax you pay on contributions is lower than in the later years. Since the Roth model does not require you to pay any tax on your funds when you withdraw during your retirement, you benefit from the lower-taxed contributions early in your career.\nTips for Selecting a Roth 401(k)\nThere are thousands of brokers online offering a variety of 401(k) plans. Should your employer offer a 401(k), you can choose to take up their offer – or go out on your own with an external plan from a broker. It’s important to note that you should start by investigating your employer’s plan. Most employers match the contribution made by their employee, allowing you to benefit from a double savings rate.\nHowever, if you have to search for a plan, make sure you take the track record of the firm you’re investing with into account. The last thing you want is to invest in a 401(k) managed by reckless or inexperienced financial managers.\nThe Roth 401(k) – Key Takeaway\nThe Roth 401(k) combines the best features of traditional 401(k) plans, along with added tax-break incentives to form a hybrid investment vehicle suitable for those newbie investors that want to take a hands-off approach to money management and investing.']"	['<urn:uuid:62e31df7-3aa0-43a3-8b88-be423f37d093>', '<urn:uuid:ba417fd4-e464-4ff7-81ee-d9967e5a3cc1>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T05:18:19.158519	8	122	3595
81	rock stress measurement depth tarim basin bingham pit compare	In the Tarim Basin, rock stress can be measured up to 7 kilometers (4.3 miles) depth using the anelastic strain recovery method, while the Bingham Canyon Copper Mine pit has a maximum depth of about 720 meters. This shows a significant difference in the depths at which rock measurements are being taken in these two locations. The Tarim Basin measurements represent a breakthrough in deep rock stress assessment, while the Bingham Pit, despite being one of the world's largest open-pit mines, operates at much shallower depths.	['Measuring unobservable forces of nature is not an easy feat, but it can make the difference between life and death in the context of an earthquake, or the collapse of a coal mine or tunnel.\nTo manage the risk of such events, researchers often rely on estimating a quantity called rock stress.\n“Rock stress—the amount of pressure experienced by underground layers of rock—can only be measured indirectly because you can’t see the forces that cause it,” explains Hiroki Sone, an assistant professor of civil and environmental engineering and geological engineering at the University of Wisconsin-Madison. “But instruments for estimating rock stress are difficult to use at great depths, where the temperature and pressure increase tremendously.”\nAddressing this challenge, Sone and his colleagues in China and Japan have now pushed the limits of rock stress measurements that don’t require temperature-sensitive instruments to new depths, from a previous maximum of 4.5 kilometers (2.8 miles) to a whopping 7 kilometers (4.3 miles).\nIn a study published in July 2017 in Scientific Reports, the researchers used rocks sampled from a well bore of that depth to show that stress estimates obtained by the so-called anelastic strain recovery method were consistent with a visual analysis of borehole wall images, a reliable but often infeasible approach that requires a specialized scanner.\nThe scientists conducted their proof-of-principle study in the Tarim Basin in northwest China, an area almost two-thirds the size of Alaska that is surrounded by K2, the world’s second highest mountain after Mount Everest, and several other mountain ranges. The region is well known to historians because of its association with the Silk Road, an ancient trade route between China and the Mediterranean.\nToday, in addition to historians and mountain climbers, petroleum companies have taken an interest in Tarim Basin, as it contains some of the largest oil and gas resources in Central Asia. These companies want to understand the region’s geology to assess whether drilling may trigger seismic activity, given that many smaller earthquakes have occurred in the surrounding mountains.\nFor Sone and his colleagues, this presented a unique opportunity to advance the methodology for measuring rock stress.\n“We wanted to test the reliability of the anelastic strain recovery method at up to 7 kilometers depth because its main advantage is that you only need to sample and analyze the rock itself,” Sone says. “It estimates stress indirectly by measuring how much the rock sample expands in different directions after it has been recovered.”\nWith that kind of depth, the recovery process—pulling a large enough rock sample out of a borehole—can take a few days, which is why the researchers were excited to prove that the method still worked.\nFor the first time, they measured rock stress even when sensors weren’t attached to the sample until 65 hours after coring and found that the results matched a conventional image analysis of the borehole wall, obtained with a resistivity scanner. While the visual method also worked in this case, it can be infeasible at such great depths because of the scanner’s temperature limitations.\nIn addition to proving the easier method’s validity at greatly increased depth, the study resolved a longstanding geological puzzle in the Tarim Basin: The rock stress in Earth’s outer shell—which consists of many large pieces of cooler rock (tectonic plates) floating on a very thick layer of hot magma—differs between the Basin’s periphery and its interior.\nOther scientists had found evidence for this difference before, but the current study confirmed it.\nIn the interior of Tarim Basin, tectonic plates are relatively stable, even though they crash and fold up against each other in the periphery, explaining the observed seismic activity. This translates to a lower risk of earthquakes in the interior and informs a petroleum company’s decisions about the depth at which boreholes should be stabilized to minimize the risk of structural collapse.\nFor earth scientists, the new study is an important validation of a more practical method for estimating rock stress. “These new results give us confidence that we can use the anelastic strain recovery method at greater depths than we thought possible,” Sone says. “As long as the rock deforms the same amount in vertical and horizontal directions, this method is much easier to apply when very high temperatures and pressures in the Earth’s crust challenge the other options in our toolbox.”\nDongsheng Sun et al. Stress state measured at ~7 km depth in the Tarim Basin, NW China, Scientific Reports (2017). DOI: 10.1038/s41598-017-04516-9', 'The Bingham Canyon Copper Mine, Utah, USA For this exercise, you will use Google Earth and Microsoft Excel to calculate the quantity of rock removed from the Bingham Copper Pit, one of the world’s largest open-pit mines. In addition, you will be able to estimate the percentage of copper ore disseminated in these rocks and the quantity of rock that must be removed for each ton of copper produced.\nTo locate the Bingham Copper Pit, enter the following coordinates into the query bar of Google Earth:\n40.520978, -112.150242 (don’t forget the negative sign in front of the longitude coordinate!)\nFig. 1. Google Earth image of the Bingham Canyon copper mine (“Bingham Pit”), Utah, USA.\nCongratulations! You have landed at the bottom of the Bingham Copper Pit southwest of Salt Lake City, Utah, USA! Move your cursor around in the bottom of the main pit to locate the minimum elevation and write it down in your notes.\n1. Zoom out so you can examine the full extent of the mine. As you zoom out, notice that the image of the eastern half of the mine pit and the area east of it are much higher resolution than the western mine pit and areas west of it. The area east of the mine is a heavily populated portion of the Salt Lake Valley and has been photographed at high resolution by the National Aerial Photography Program whereas the mountains west of the mine are largely uninhabited, so have not been a priority of the NAPP program.\n2. The north-south boundary separating high and low resolution imagery is a convenient marker we can use to help construct a topographic profile across the mine.\n3. To construct a topographic profile, begin by enabling the metric measures in Google Earth. This is accomplished by clicking on ‘Tools’ on the menu bar at the top of the page, the clicking on ‘Options’. When the ‘Options’ dialogue opens, you will see in the lower left of the dialogue an ‘Elevation’ option. Choose Meters, Kilometers, then ‘Apply Settings’.\n4. Next, you should start up the Microsoft Excel program as you will be entering data generated from Google Earth into this spreadsheet. When the spreadsheet opens, label one column ‘Distance (m)’ and one column ‘Elevation (m)’.\n5. Next, return to the Google Earth window and enable the ‘Measure’ tool by clicking on the ‘Tools’ menu, the clicking the word ‘Measure’ (or simultaneously using ‘ctrl + 6’ keys on your keyboard). When the ‘Measure’ dialogue box appears, use the drop down window to choose meters as the units of distance. Also select the tab labeled ‘Path’.\n6. Note that the ‘Measure’ icon on your image appears as a box with tick marks along the mid-points. This box can be used to add points along your profile with approximately equal spacing; simply click the first point, then move the box until one of its tick marks is aligned with this point; now click again. The next point is located approximately the distance from the center of the box to a tick mark. This distance varies depending on the level to which you are zoomed into the image. In addition, from point to point, the distance is not precisely the same, but it is close enough for the exercises we are attempting to conduct. If you add a point and want to delete it, simply right click on your mouse and it will disappear.\n7. You can determine the approximate spacing to each point by noting the cumulative distance along your profile and entering this distance at each point into the spreadsheet. You will construct an east-west profile across the center of the Bingham Pit. Begin by clicking a point along the east or west edge of the mine pit. When you click this point, note the distance along your profile (the first point will be 0) and the elevation of the point (read along the bottom of the image next to the latitude and longitude). Enter the distance and the elevation into their respective columns in your spreadsheet. Repeat this process until you have traversed the mine pit from north to south.\n8. Note that the excavation is approximately cone-shaped. As a consequence, we can estimate the volume of rock removed from the mine by calculating the volume of this cone. Use the Internet to locate the formula for the volume of a cone. Using this formula, calculate the volume of rock removed from the pit by determining the maximum depth of the pit from rim to center. Answer the following questions to help you determine the volume and mass of rock removed from this pit:\na. What is the maximum elevation (meters) of the pit rim? ca. 2,401 m (NOTE: this value may vary somewhat depending on where along the pit rim students take their measurement)\nb. What is the minimum elevation (meters) in the bottom of the pit? Ca. 1,681 m (NOTE: this value may vary somewhat depending on where within the pit students take their measurement)\nc. What is the depth of the main pit (meters)? ca. 720 m\nd. What is the radius of the main pit (meters)? ca. 1,490 m\ne. Using the formula for the volume of a cone, what is the volume of rock removed from this pit? ca. 1,672,388,443 m3 (NOTE: quantity may vary somewhat depending on input values, but order of magnitude should be about 1.6 x 109 m3)\nf. You have already learned in lab and lecture that density = mass ÷ volume. If the density of rock in Bingham Canyon is 2.8 g/cm3, calculate the mass of rock removed from this pit. Note that you will need to do some conversions to get proper units in your answer.\n(NOTE: 1 m3 of rock with 2.8g/cm3 density has a mass of 2.8 metric tons; therefore the mass from volume above is 4,682,687,641 tonnes.)\ng. Since mining began in 1904, 15.4 million metric tons of copper have been smelted from this mine. What percentage of the total rock mass mined from the pit was copper? ca. 0.33%\nh. How many metric tons of rock must be mined to yield one metric ton of copper? ca. 300T of rock must be mined for each tonne of copper.\ni. Did this calculation surprise you? Explain your answer. Students should elaborate a response here.\nj. Note that reported ore grade of the Bingham Pit is 0.6%. How does this compare to your estimate above? Discuss potential reasons for this disparity. My estimate is about half that reported. Most likley due to errors in estimates above; could also be due in part to fact that early ores mined from the pit were very high grade, the average 0.6% grade takes into account these very rich ores. The ore grade continues to become lower over time as the richest portions of the deposit are mined.']	['<urn:uuid:ff0badca-383b-46f7-8940-7ce7654a8104>', '<urn:uuid:77d0a382-1eb6-4b40-b817-a6481bdad3c6>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-13T05:18:19.158519	9	86	1892
82	need help designing org chart what basic steps should follow	The basic steps to design an organizational chart are: 1. Know your cause to implement - determine who and what purpose the chart is for, 2. Have all the information from the nooks - gather correct and complete information about management levels and resources, 3. Decide the platform you're going to use - select appropriate chart-making software, 4. Plan for a volatile chart - ensure it can cope with organizational changes.	['Organizational Charts: The best way to design your structure\nOrganizational Charts are something that we have been using all this time to represent data in a more efficient way. We use them in office presentations, school and college exams, and to understand things in a better way.\nAs we have been using them so frequently, it is important for us to get to the depth of organizational charts. We should know its origin, its formal way of representing it, and its importance in the field of graphical representation.\nAn organizational chart is a diagram that shows the graphical relation between components of a structure or organization. It also depicts the relative ranks and positions of its parts. It creates an uncomplicated visual illustration of the hierarchy that makes up the organization.\nThese organizational charts are also called Organograms, Hierarchy charts, Org charts, etc. They have many uses and can be structured in different ways and used as managing tools, planning purposes, or as a staff directory.\nGoing Down the Memory Lane of Organizational Charts\nDaniel McCallum, a Scottish-American engineer, is credited for designing the first org chart for American business in 1855. It was an embellished structure of New York and Erie Railroad; it was drawn by George Helt Henshaw. He believed that designing structures could bring the division of responsibilities and make the working transparent. His idea of org charts was successful in laying out his plan as through them a sense of order and control could be established.\nBut, the term organizational chart came into practice in the early 20th century. In 1914, Willard C. Brinton, a consulting engineer, used the term for the first time in his textbook – Graphic Methods for Presenting Facts. He declared that these charts should be more widely used, they are not as common as they should be.\nOrg Charts were mostly used by the people of engineering groups till the 1920s, after that they started to become prominent in the corporate world. The less popular term which is ‘Organigram’ became more common in the 1960s.\nOrganigraphs are very much similar to org charts, but it is dissimilar to it in the sense that it is less linear than an Organigram and serves to describe alliances and opportunities among departments or supply chains.\nDifferent Types of Org Charts in Use\nOrganograms have a number of formats which can be made; we can decide one depending upon our requirements. These described below are the main genus of charts used.\n1. Hierarchical Charts\nIn a hierarchy, one group is at the top, and the subsequent groups which are less important or have less power are put below them in a pyramidal way.\nIt is almost like a monarchy where the king is at the top with the maximum power and the people with weak command come under them.\nIn such a type of hierarchy, members are in direct contact with the people who report directly to them and the ones they report directly to.\nIf this made for a company where many employees work in different or similar position; the CEO would come at the top, and the people with the same skill-set, position, and specialization are grouped together.\n2. Divisional Chart\nIt depicts a company organized along a product line or specific geography. Divisional set up is used when one part is sufficiently free from another.\nOne example is that it can be used in a car company and the divisions can be SUVs, sedans, hatchback, or electric cars.\n3. Matrix Chart\nThese are meant to combine hierarchical and divisional structures to work on certain specific projects. Matrix org charts come into use when you want to have more than one reporting point, and the scope of changing your division or multitasking with more than one division is acceptable.\nIt helps in better and more open communication which simultaneously creates a superior dynamic work environment where shifting of resources is easy.\nBut if not utilized properly, it can create mental problems due to workload and frustration.\n4. Flat Chart\nSuch type of org charts are also known as Horizontal Org Chart. It shows less number of levels of management. In structures like these, the groups below have more responsibility and are more involved in decision making.\nEveryone is treated equally, job skills and resources are shared, and there are no direct lines of command. It is mostly opted by small businesses & sometimes by the ones on the large scale too.\nThe need for the different layouts arose because we could not handle all types of structures with a single one. So there might be some limitations to a certain type of chart, there are some pros and cons to every type of chart we are using.\n- In matrix chart, you have more than one reporting point; this might create some confusion and conflicts of interests though it is helping with cross-department relationships.\n- In a hierarchical approach, the structure is built upon the qualities of clarity and firmness, but its potential drawback may be rigidity. It might not be flexible when we want it to. The layers of management sometimes also bloat the organization.\n- A flat or horizontal structure is good with fewer levels as it builds up a sense of teamwork among the workers. But there is always a chance of conflicts as fewer people are doing the work.\nSome Mainstream Uses of Org Charts\n- Due to the modern technology, the org charts are not only catching dust in the folders, they are now accessible to you anytime, anywhere on your smartphones. It has some basic uses which we are going to discuss here-\n- Describe Management Structure: As we have already discussed, the prime focus of these charts is to embellish the structure of an institution and the relationship between companies and the various components of it.\n- To Enhance Conveyance Between Employees or Groups: It helps to perceive the entire working of the company properly such as who reports to whom, adding photos can also help.\n- Planning: Drawing is the best way to plan things. You can easily figure out the need for engaging more employees, switching roles and cut down on capital misuse.\n- The Personal Details of Your Employee: You can add photos, telephone numbers, or email id to maintain a directory of your employee which will help for smoother communication between the head and his employee and among employees of the company.\nThe Best Way to Plan an Organizational Chart\nNowadays, Organizational Charts are used in every business presentation and are very useful in describing your point, you should, therefore, be comfortable in designing and understanding them. If you do not know to work with the organizational charts, then you might be left behind in the race of the corporate world.\nAlso, everybody has different requirements and expectations for their job. So, you need to mould according to the kind of chart you want and modify the basics.\nThus, it is important that you understand the basics of an org chart.\n1. Know Your Cause to Implement\nKeep in mind for who you want to make the chart, for what purpose. If you are charting it for the whole company or just a department or a group. Think about the best for your company and your purpose.\n2. Have All the Information from the Nooks\nWhile drafting the chart, you need information about the topic of your chart; you cannot design it without having the correct and whole information.\nFirst get to know all the important points in mind which will form the base of your chart, define an initial point for you to begin, know the different levels of management, and collect all the resources.\n3. Decide the Platform You Are Going To Use\nThere are many platforms which provide a user-friendly, powerful chart making software like Lucidchart, PingBoard, etc. So, decide what is best for you and get going to work.\n4. Plan for a Volatile Chart\nIn this fast-changing world, nothing remains constant, neither do these organizational charts. You should always design a chart which can cope with the updations with things changing in the organization. These things become a lot easier with online collaborative platforms.\nOrg Charts form the foundation of any organization; it depends on us – how well do we manage the thing.\nYour chart should depict the structure of your organization; it may be a pyramid, cake, or in the form of circles. Choose the software tool which offers templates to simplify designing the chart.\nYour product which you are going to sell should be visually appealing to its buyers; here are some tips to add the color to your org chart.\n- Breaking the load into pieces would allow you to manage the work more efficiently. Divide your areas into different categories.\n- Include details of departments or employees like contact, name, photo, location for increased efficiency and less confusion.\n- You can use colors and shapes to make it attractive visually.\nSome Secret Tips to a Flawless Org Chart\nAfter learning all these basic steps to an ideal org chart, there are still some minor points left which can further allow you to hone your work.\n1. Choosing format\nYou should choose the format according to the space available to you; it may be vertical or horizontal. When you input the person with lower authority or position in a chart, they should be connected with lines box to box.\nTry to keep the boxes of the same size throughout the chart, but it can be lenient if you want to keep the box larger of the one at the top.\n2. What content is required and what to include\nFor what specific task we are going to use our chart becomes the main cause in determining the content.\nIf the prime reason for your chart is to get the key positions for information on the structure of the company, then the person holding the responsibility does not matter much. It is preferred to put the title before the mane in such cases; it helps in changing the person’s name if he is shifted to another department, changed his role, or got promoted because the employee may change but the position is going to remain the same.\nWe can add additional data like telephone number, head-shots, e-mail, etc.\n3. Displaying non-traditional roles\nIf a person holds two or more positions, it is best to write both positions in the chart and show the person occupying both of them. Try not to make your chart complicated keeping simple and easy to read; you are always advised to avoid exotic formatting. It increases the functionality of the chart.\nIf there is an employee who works under more than one manager, then you can connect it to all the managers with a dotted line. However, it is better to draw separate charts in such cases.\n4. Choose the best orientation\nFigure what type of orientation is going to suit the best for you. We generally make it in a pyramidal fashion, but earlier, the inverted pyramid was more common.\nOne side to other like left to right flow is not that popular, but you can also go for it if you want. Choose the orientation which reflects your point the best.\nTry to adjust the whole chart on a single page. Go for horizontal arrangement at the top and vertical after that to fit all its content in a single page.\n6. Modernize with online Org Charts\nDistributing your chart online is very helpful; you can interact with the contents of the chart. Finding names or any other information becomes easy with online charts, you can directly contact anybody just by clicking on the name, and the e-mail would be directly linked to it.\nYou can also add hyperlinks to the employee’s record in the chart; you can make your chart an interface for additional informational.\n7. Break it into smaller parts\nA large chart is complicated and difficult to understand, what we can do is break it into smaller charts each with a reasonable size and link them back together. By linking them together, no matter what you can always find your way back to where you started.\nNothing is perfect! Not Even Org Charts\n- With this fast-changing era, nothing is bound to be perfect or error-free, with this pace of development and updates even org charts lack behind and show certain limitations.\n- They become outdated frequently if the turnover of the company is high. Online charts are a better option in this scenario, but even they lack behind if not tended regularly.\n- You can describe only formal relationships with them, but it’s actually the informal things which get the work done.\n- They can show the sequence of authority but not the exercise of the power or how the things are done.\n- As the chart shows the status of employees in different positions and status, a feeling of superiority or inferiority might develop among them.\n- In some cases, Organigraphs which are a close relative of org charts prove to be more useful in illustrating other departments.\n- They only include the workers in an organization, not the customers.\nThe End Note\nThis was all the information regarding organizational charts, its history, appreciation, drawbacks, steps to make a decent chart, and its corporate use. These charts are very useful in depicting data and give you an edge over other chart makers if you follow the steps correctly and try to make it as appealing as possible.\nThere is much online software available to further ease your work; these prove to be very helpful from designing the layout for the final finish. It depends on you, how you use them to your benefit.']	['<urn:uuid:0d4e892f-4e26-4e74-92b7-c76baf45ce27>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	10	71	2311
83	why do elvis sun discs have circular dents	The circular indentations, known as 'push marks,' were created during the manufacturing process. Sun Records used a method of aligning the stamper (the negative used to make vinyl discs) over three pins. These marks were transferred to the vinyl while it was still warm.	['In the course of writing about Elvis Presley memorabilia the other day, I found myself spending a lot of time looking at Elvis Presley records, particularly the 45s and 78s he cut at Sun Records for Sam Phillips.\nOne of the terms that kept popping up in the descriptions written about the King’s 45 rpm singles was “push marks.” I could see them beneath the label (look for the circular indentations to the left of the word “That’s” in the photo above-left, and to the right of the word “Kentucky” in the photo above-right), but it took a while to find an authoritative source to explain how they got there.\nMusic appraiser Stephen M.H. Braitman, who gave us a fascinating interview about the British Invasion last summer, pointed me to a site called WorldLingo and this description of record manufacturing: “Each stamper [the negative used in a hydraulic press to make the actual vinyl disc] was next centre punched; methods used included aligning the final locked groove over three pins or tapping the edge while rotating under the punch until the grooves could be seen (through a microscope) to move constantly towards the centre.”\n“Sun Records,” says Braitman, “obviously used the method of aligning the stamper over three pins, which, on the final vinyl result, looks like ‘push mark’ impressions.” In other words, the impression was made while the disc was still warm, transferred from the stamper to the vinyl.\n“To the best of my understanding,” says Boija, “the push marks stem from the so-called stampers, the metal plates used when manufacturing the records. You can see them quite clearly on one of the preserved stampers for Elvis’s debut, Sun 209, which Joseph Pirzada in the U.K. owns.” See photo at left.\n“Now, when one checks the early Sun 45 rpm releases one by one,” Boija continues, “it is easy to note that many—but not all—have push marks. Why is that? Well, the scarcity of the majority of the pre-Elvis Sun 45s makes it difficult to verify, but my theory, based on some of Sam Phillips’s correspondence with the companies that made the stampers, is that the releases that were anticipated to only have limited commercial success were done in small quantities—often in increments of 500—locally in Memphis. Those pressings always have the so-called push marks.\n“But when a record became a hit, neither the Memphis pressing plant (Plastic Products on Chelsea Ave.) nor the local distributor (Music Sales, also on Chelsea) had the capacity to merchandise and distribute satisfactorily outside of the South. The first national hits like ‘Bear Cat’ by Rufus Thomas, Jr. (Sun 181, released March 1953) or ‘Just Walkin’ In The Rain’ by the Prisonaires (Sun 186, July 1953) were mainly pressed and distributed in Los Angeles. The former I have never seen with push marks, the latter both with and without, depending on which pressing plant was used.\n“This is the explanation for why genuine Sun releases from the 1950s can be originals both with and without push marks, but in the case of Elvis, push marks have become synonymous with being the real deal—it takes both skill and experience to identify the genuine non-push marked originals.\n“The very last number I’ve seen with push marks,” says Boija, “is Sun 236 by Jimmy Haggett, a single that sold so poorly most collectors have never seen it. Released in December 1955, it probably wasn’t run in more than 500 copies. I’ve seen two copies and both had push marks. Apart from that, Sun 231 by Charlie Feathers, another ultra-rare release from December 1955, also has push marks, as so do most earlier releases, unless they were large hits pressed out of town from the start, mainly for Elvis, Johnny Cash, and Carl Perkins.”\nAs an aside, Boija has some advice for those who might ignore Sun’s 78s, which are easily identified by their small center hole, the rooster logo above it, and the horizontal bar that reads “RECORD COMPANY.” Sun 45s, he says, were made from “the poorest quality vinyl and were pressed with a groove width designed to handle the thicker 78 rpm stylus, as compared to the thinner ‘Microgroove’ needle that was introduced with the LP format.” So, when you play your Elvis 45s on modern equipment, Boija says, the result is what’s known as “the Sun hiss. Most collectors have never realized that if they play the records with the correct needle, they sound much better. In terms of dynamics, sound quality, and clarity, not even a mint 45 comes close to the 78s. A pristine 78 even beats the CD reissues.”']	['<urn:uuid:d09ae2fd-a84e-42d3-ae4d-625ef3b34d3c>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	8	44	769
84	water temperature effects pizza dough fermentation poolish	Water temperature plays a crucial role in both regular pizza dough fermentation and poolish pre-ferments. For regular dough, water temperature affects fermentation speed - cold water (40-80°F) is used for slow fermentation while hot water (115-130°F) enables quick fermentation within hours. When using poolish pre-ferment method, which has about 70% hydration, the dough requires less fermentation time compared to other methods and produces a softer, more open crumb structure with a sweeter flavor and hint of yeast. The water temperature needs to be carefully controlled in both cases to achieve the desired fermentation results and final dough characteristics.	"[""My practice is to adjust finished dough temperature based on the type of dough I plan to make and the fermentation method I plan to use. For example, if I want to make a typical cold fermented dough, such as a NY style or an American style dough, I try to adjust the water temperature to give me a finished dough temperature of between 75-80 degrees F. The range recommended for finished dough temperature for professionals is 80-85 degrees F (see, for example, Tom Lehmann's instructions given in Reply 3 at http://www.pizzamaking.com/forum/index.php/topic,7953.msg68396/topicseen.html#msg68396\n). The reason for the difference is that commercial coolers are more efficient for cooling than home refrigerators (they run several degrees cooler than a typical home refrigerator). If I want to make an emergency dough that is to be used within a few hours, I will use a water temperature that can be around 120 degrees F.\nThe range of water temperatures I use can be very wide. For example, if I want to make a cold fermented dough that will last a few weeks in the refrigerator, I would use a water temperature of in the 40s degrees F. See, for example, Reply 29 at http://www.pizzamaking.com/forum/index.php/topic,3985.msg36081.html#msg36081\nwhere I used water at 44.1 degrees F to make a dough that was cold fermented for about 12 days. I once even conducted an experiment in which all of the formula water was in the form of ice cubes, and another member, Les, perfected a method in which all or nearly all of his formula water was in the form of crushed ice. I have also tried using frozen and refrigerated flour to try to achieve a lower finished dough temperature. Using refrigerated flour is a method that Bev Collins, who worked for years in the R&D group at Domino's, often recommended (see, for example, Reply 26 at http://www.pizzamaking.com/forum/index.php/topic,4625.msg40011/topicseen.html#msg40011\n). (I think I can find the links on these methods if you are interested.)\nOn the other end of the spectrum, if I want to make a dough that can be used within, say, an hour, I would use a water temperature of close to 120 degrees F and maybe closer to 130 degrees F. See, for example, Reply 12 at http://www.pizzamaking.com/forum/index.php/topic,2250.msg19793/topicseen.html#msg19793\nwhere I used water at about 115-120 degrees F to make a dough that was used to make the pizza within an hour total, from beginning to end.\nProfessionals generally adjust water temperature for their cold fermented doughs. There are even charts that they can use that tell their workers what water temperature to use based on their particular mixers (and, specifically, their friction factors) and temperature conditions. You can see an abbreviated version of such a chart on the last page of the General Mills piece at http://www.gmflour.com/gmflour/PDFs/Website%20A49104%20Just%20Crust%20Brochure.pdf.\nAbsent such a chart, there are formulas for calculating water temperature to use to achieve a desired finished dough temperature based on flour temperature, room temperature, and mixer friction factor. Tom Lehmann discusses these formulas in an article at http://www.pmq.com/mag/2003spring/tom_lehmann.shtml.\nI hasten to add, however, that the methods described by Tom work best for straight doughs in a commercial setting, as I discussed at Reply 18 at http://www.pizzamaking.com/forum/index.php/topic,10152.msg88757/topicseen.html#msg88757.\nSince most of my doughs are straight cold fermented doughs, I find the water temperature calculations useful in even a home setting and use them pretty much for all of my straight cold-fermented doughs.\nI tend to agree with Bill/SFNM that your acquaintance was perhaps thinking of doughs other than pizza dough. However, the notion of keeping things cold does have merit although I did not find that using cold yeast and cold flour (even frozen) had a lot of value in my dough making, at least for the quantities of dough that I make. I have found that both yeast and cold flour can approach room temperature quite quickly. That is especially true where I am in Texas in the summer. In my setting, if I don't use water cold enough, I run the risk of my dough fermenting faster than I would like.\nAs you can see, water temperature is not just an abstract notion.\nEDIT (1/25/13): Since the link to the above Lehmann article is no longer operative, see the Wayback Machine link to the same article at http://web.archive.org/web/20070502014430/http://www.pmq.com/mag/2003spring/tom_lehmann.shtml\nEDIT 2 (3/22/13): EDIT (2/4/2013): For an updated link to the General Mills brochure, see http://www.professionalbakingsolutions.com/water-temperature-chart"", 'Biga and poolish are two pre-ferment methods used in making pizza dough. Each produces different flavors and textures.\nPizza enthusiasts are continuously searching for new ways to enhance this italian classic. The pre-ferment method has been gaining popularity, and two common methods are frequently used for making pizza dough- the biga and poolish methods. These methods have been used for years in bread baking and have recently found their way into pizza crusts.\nBoth employ a pre-ferment process where a small portion of flour, water, and yeast, are mixed and then left to ferment before being added to the pizza dough. The purpose of this is to extend fermentation time, allowing flavors to intensify and the dough to have a more open and airy texture when baked. Although both methods are similar, there are notable differences in how they are made and the resulting pizza crust’s flavor and texture. In this article, we will explore the differences between biga and poolish pizza and what makes them unique.\nUnderstanding Biga And Poolish Pizza Dough Starters\nUnderstanding biga and poolish pizza dough starters starter is a mixture of flour, water, and yeast that aids in bread making. A biga is a firm starter that’s originated from italy and is fairly similar to sourdough. On the other hand, a poolish is a loose starter method originating from france.\nBoth sourdough starters help in the fermentation of the pizza dough, resulting in flavorful crusts. Unlike biga, poolish requires less time to ferment. Biga results in a firmer, more structured texture, whereas poolish produces a softer, more open crumb. When contrasting the two, it’s crucial to remember recipe proportions and time since they have different recipes and processes.\nBy using either biga or poolish, you can achieve excellent results in your pizza-making processes.\nThe Science Of Pizza Crust\nThe perfect pizza crust is a delicate balance of chemistry and physics. Using biga and poolish starters can affect the final texture and flavor. Gluten and yeast play important roles in creating the right consistency. The hydration level can also change the crust’s texture and flavor.\nIt’s essential to understand these elements to make the perfect crust. By experimenting with different ingredients and techniques, you can create a crust that’s unique to you. Whether you prefer a crispy or chewy crust, understanding the science behind it can help you achieve the perfect slice every time.\nBiga Vs. Poolish: Which One Delivers The Perfect Crust?\nBiga and poolish are popular starters used in pizza crusts. To achieve the perfect crust, texture, and flavor are key. Biga, with its longer fermentation, has more depth of flavor but requires more time. Poolish, on the other hand, produces a lighter crust with a shorter fermentation time.\nChoosing between the two depends on the pizza style and toppings you’re using. Thin and crispy crusts pair well with biga, while poolish is best for thicker crusts. Making the perfect biga or poolish crust at home requires attention to detail and the right technique.\nWith some tips, practice, and patience, you can achieve a restaurant-quality pizza crust at home.\nWhat Is Biga And Poolish In Pizza Making?\nBiga and poolish are pre-ferments used in pizza dough. They are a combination of water, flour, and yeast left to ferment for a specific time, generating an intense flavor in the final dough. The difference between the two is the hydration of the dough and the percentage of flour.\nWhat Is The Difference Between Biga And Poolish?\nBiga is prepared with a low hydration dough, around 50-60% and is left to rise overnight at room temperature, whereas poolish contains a higher hydration dough (around 70%) and is left to rise for a shorter time. Biga gives a slightly denser texture with a crispy crust, while poolish gives a more tender crumb.\nHow Do Biga And Poolish Affect Pizza’S Flavor?\nBiga and poolish affect the flavor of the pizza dough positively. The biga produces a rich, complex, and slightly sour flavor, whereas the poolish unleashes a sweeter flavor with a hint of yeast. They also help in gluten development, making the dough more extensible and elastic, allowing it to rise better during the baking process.\nCan You Make A Pizza Dough Using Both Biga And Poolish?\nYes, you can mix both of these pre-ferments together to produce an even more complex dough with a unique flavor profile. Many pizzerias use a mix of biga and poolish, and it is called “bigoli. ” It gives the pizza dough a distinct tangy flavor and an airy, irregular texture.\nCan You Substitute Biga Or Poolish With Instant Yeast In Pizza Making?\nWhile biga or poolish gives an authentic and unique taste, in case you don’t have much time to make these pre-ferments, you can use instant yeast instead. However, the taste may differ, and it would not give you the expected flavor profile of the biga or poolish dough.\nOverall, both biga and poolish are effective starter doughs in pizza making, each with their own unique benefits. Biga offers a longer fermentation process that results in a more complex flavor, while poolish provides a quicker method with a softer and lighter dough.\nUltimately, the choice between the two should be based on personal preference and the desired outcome of the pizza. Consider the type of pizza being made, ingredients, and desired texture and flavor. Experimenting with different recipes and techniques can lead to discovering the perfect combination that satisfies individual taste buds.\nAs with any craft, pizza making requires practice and experimentation, so don’t be afraid to try new things. Making a delicious pizza can be a fun and rewarding experience for anyone, whether using biga or poolish as the starter dough.']"	['<urn:uuid:c4cb773e-beb0-44a8-9ff8-ca3e93ce1299>', '<urn:uuid:b1aece86-5784-4995-8f43-c0f3abf0980b>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	7	98	1679
85	nintendo wii remote vs radio baton comparison differences in functionality tracking technology cost	The Wii remote and Radio Baton have several key differences. The Radio Baton uses FM transmitters in two batons to coordinate X, Y, and Z position relative to a surface. The Radio Baton's two-baton system allows control of balance of instruments, while the Wii only controls tempo. The Wii remote likely uses a combination of internal sensors (gyro and/or accelerometers) with IR, evidenced by an IR window in the remote and a long receiver bar placed in front of the TV. The Wii remote is a mass-market device expected to cost around $30, making it much more accessible than experimental alternatives.	['Before Nintendo imagined mass-market gamers waving around a wireless remote to conduct music, digital music pioneer Max Mathews devised the Radio Baton:\nRadio Baton at the Electronic Music Foundation site\nMax Mathews page at CSounds.com, which contains extensive photos, documentation, and even software for the Radio Baton (unfortunately, several of those links appear to be broken)\nIn contrast to Nintendo’s Wii remote, the Radio Baton uses very basic technology: FM transmitters in two batons coordinate X, Y, and Z position relative to a surface. It’s an idea that’s indebted in some ways to the Theremin, and like the Theremin creates the challenge of performing without physical resistance or feedback on your location (well, unless you count air and looking at the surface, respectively). The payoff, though, is freedom of gesture and expression. Unlike the Wii, too, the two-baton system lets you control balance of instruments; the Wii does tempo only.\nI don’t know whether Nintendo’s engineers were familiar with this device; I’m assuming not. But Mathews certainly has proven to be ahead of his time, as millions of mass-market devices executing basically the same idea ship worldwide in the fall.\nRelated: Electric Violins, IBM Mainframes, and Playboy (yes, that story really is about Max Mathews; read it if you don’t believe me!)\nThe Radio Baton was probably the first real baton controller, derived from Max Mathews’ early “Daton” controller which sensed X/Y position by impacting the surface (so, therefore, not really a baton so much as a kind of digital drumstick). But it was not the only such device.\nAs reader Crix points out, you shouldn’t miss Buchla’s Thunder and Lightning controllers, designed by legendary synth designer Don Buchla and apparently still available for sale. (The website promises a new Lightning III model, but I can’t find details on it.) At last check, they cost about US$2000, but they have gotten artistic use, notably by San Francisco interactive performing artist Pamela Z. Unlike Mathews’ design and the Nintendo controller, the Buchla batons support only X and Y positioning, not 3D space, though they can coordinate the position of two batons at once. Despite that, the Buchla’s engineering principle is closer to Nintendo’s: a receiver positions the devices in X/Y space based on transmitters in the batons. The Wii controller likely coordinates internal sensors (gyro and/or accelerometers) with IR, because Nintendo’s product shots clearly show an IR window in the remote, just as on a TV remote, and a long bar acting as a receiver placed in front of the TV. The long bar would be necessary only if Nintendo were using the IR data as a way of coordinating exact position in X/Y space. So there you have it: your US$30 (or whatever it will cost) Nintendo remote uses the same technology used by experimental artists in a US$2000 system from one of the great pioneers of electronic music. Who would’ve thought.\nMathews and Buchla are just two among many others who have done something like this; see the article below for an extensive history of alternative controllers, current as of 1999, by Joseph Paradiso, noting a number of other batons (along with lots of other interesting controllers). Now you just have to add Nintendo to the mix.\nElectronic Music Interfaces, by Joseph Paradiso: story at the MIT Media Lab site, and (apparently updated from the same article) Baton Interfaces from American Innovations in Electronic Musical Interfaces in NewMusicBox\nBuchla Lightning II Product Page, from Buchla and Associates (as shown below from Buchla’s site featuring Pamela Z, though, oh, I wish I had a better image. Here’s what Pamela Z looks like in real life, at least.)\nI’m sure there are more, so chime in!']	['<urn:uuid:ba1b831d-0872-4974-b738-356711e65697>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	13	101	615
86	when was indus hospital built open	The hospital's ground-breaking ritual was performed on July 1st, 1983 by Raja Virbhadra Singh, then Chief Minister of Himachal Pradesh. The hospital was opened for the public on June 13th, 1993 and was inaugurated by Sri Bali Ram Bhagat, who was the Governor of the state at that time.	['INDUS Hospital ShimlaShimla is the residential place for many and the city is also visited by thousands of outsiders every year. It is important to have proper healthcare services in Shimla so that local people as well as tourists get diagnosed and treated well whenever they have any health related issue. Shimla houses many private hospitals and nursing homes and each one of them offers quality services at reasonable price. INDUS Hospital in Shimla is a renowned hospital located at an amazing location so that people could easily reach here for getting treated fast.\nAbout INDUS Hospital ShimlaINDUS Hospital Shimla has a capacity of 90-beds. The hospital is nestled among tall cedar, oak and Pine trees along with grooves of Rhododendrons. The hospital is located at a height of 7500 ft very close to the Jakhoo Hill among the panoramic landscapes of Himalayan Hills.\nThe INDUS Hospital was established by Dr. Balak Verma who wanted to show his love and respect for his parents Sri Jindu Ram Verma and Srimati Purbu Devi Verma. He also got the help from his wife Dr. Indira Verma in his project. Dr. Verma had been in the USA for more than 27 years and always wanted to serve the common people.\nRaja Virbhadra Singh, the then Chief Minister of the state of Himachal Pradesh had performed the ground-breaking ritual for INDUS Hospital in 1983 on 1st July. The hospital was opened for the public in the year 1993 on 13th June. The hospital was inaugurated by Sri Bali Ram Bhagat, the then Governor of the state.\nINDUS Hospital has its name originated from the names of two countries, INDIA and the USA.\nINDUS Hospital MissionINDUS Hospital Shimla was built with an aim to provide high quality health care to the people of Shimla, and the entire state. The hospital is dedicated to the people of the country. Providing safe and affordable health care services is the ultimate mission of this hospital. Modern healthcare equipments and technologies are used to treat patients at this hospital. The main focus is to provide world class health care to the middle and lower-middle class people.\nDepartments of INDUS HospitalHere are the various departments of INDUS Hospital and their services.\nGeneral medicine services for common ailments as well as special and rare diseases are also provided at this department.\nSurgery Department is divided into many sections like Trauma Surgery, Thoracic Surgery, Gynecology, Urological Surgery, Cardio Vascular Surgery, Plastic Surgery etc. Each of these divisions is headed by experienced and well-qualified department heads and specialized doctors and support staffs work here.\nThe operation theaters are well equipped with all types of modern equipment. The guidelines for infection control are strictly complied with. Seamless monitoring facilities are there during post-operative recovery.\nThis department offers services to a wide number of patients who look for alternative medicine. The head is a renowned Ayurvedic Doctor. The Department was setup in January 2013.\nThe department has services for pediatrics, neonatal and adolescent medicine. To ensure any type of abnormality or growth problems in the babies could be detected early, emphasis is given so that babies are brought to the Well Baby Clinic of this hospital.\nPediatrics and Adolescent Department\nProper Immunizations are also emphasized.\nMaternity and Gynecology DepartmentQualified consultants work here to provide obstetrical, maternal and antenatal services.\nThis department is run under the supervision of experienced dentists and dental surgeons. All types of dental procedures (minor and major), artificial dentures, oral problem and surgery are done at this department. Emergency services are available 24 hours.\nDentistry and Oral Surgery Department\nOrthopedic surgeries of general type as well as rare types are performed under the supervision of specialist orthopedic surgeons.\nOrthopedic Surgery Department\nPhysiotherapy department of this hospital works under a specialized doctor. Physiotherapy Department remains opened on all working days and in-patient and out-patient services are offered here for rehabilitation of various types like neurological, orthopedic, pulmonary, cardiovascular, pediatric etc.\nFacilities of INDUS Hospital\nVarious services and facilities offered at INDUS Hospital are:\n- Out-patient department\n- In-patient department\n- General Medicine\n- Pediatric Care\n- ICU (Intensive & Critical Care Unit)\n- Cardiopulmonary Unit\n- Emergency Services (24 hours)\n- 24 hours Pharmacy\n- Oral surgery and dentistry\n- Gynecology and Obstetrics Unit\n- General and Endoscopic surgery\n- Oncology Surgery\n- Urological Surgery\nThe Diagnostic Services\n- Laboratory services\n- Cardiopulmonary Laboratory for non-invasive surgery\n- Radio Diagnosis and Imaging\nUltra Sonography and Radio Diagnosis ServicesThe department has latest equipments and uses state-of-the-art techniques to perform common and special sonography and radio diagnosis services. Qualified and well-trained technicians work in this department.\nThe services available are:\n- MRI scanning for whole body\n- CT scanning for whole body\n- Special Radiography\n- USG & CT\n- X-Rays with Double Contrast\n- Traditional Radiography\n- Special Radiography\nNon-invasive services at Cardiopulmonary Laboratory\n- Color Doppler\n- Stress EKG\n- Holter Monitoring\n- Digital EKG\n- Automated Biochemistry\nAt the clinical laboratories of INDUS Hospital, all types of common tests are performed and for special tests it has arrangement with the National Reference Laboratories.\nAccommodations at INDUS Hospital Shimla\nAccommodations in this hospital are of different types. They are:\n- Critical care rooms\n- Special wards\n- Deluxe rooms\n- Private rooms\nPatient as well as service areas are provided with central heating system. The system has thermostatic controls for individual sub-units. In each of the floors, oxygen is provided through pipes. Rooms and wars of the patients have attached toilets and they are also provided with intercom facilities.\nDeluxe rooms have carpets on the floors and they are provided with colored TV.\nThe critical care unit has 8 beds in it. All types of latest medical equipments like ventilators, cardiac monitoring system, piped oxygen, compressed air, central suction etc. are present in the critical care unit.\nQuality Assurance of INDUS HospitalINDUS Hospital ensures Quality Assurance standards to be maintained. The activities for Quality Assurance followed in this hospital are:\n- Incident Reports\n- Patients’ Bill of Rights\n- Medical or Surgical Audits\n- Mortality or Morbidity Review\n- Patient Feedback\nLocation of INDUS Hospital Shimla\nNear Chotta Shimla\nPhone No.: 0177 284 1401 / 02 / 03 / 04\nFax: 0177 284 2017\nEmail Id: chairman@INDUShospital.org / ceo@INDUShospital.org / INDUS@INDUShospital.org\nFrom the ISBT (Inter State Bus Terminal), the driving distance of the hospital is only 6 km and from Railway Station it is only 7 km. From Shimla Airport, one can reach to this hospital in 45 minutes through car journey. The hospital is easily accessible from all sides of Shimla through all-weather roads.\n- SHARE THIS\n- TWEET THIS\n- SHARE THIS\n- LOVE THIS 0']	['<urn:uuid:d036e706-5079-4dfd-96c1-8fc3eac54d94>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	6	49	1115
87	What makes China a major player in solar panel manufacturing, and how do these panels perform when installed on water bodies?	China is the world's leading exporter of solar cells, with significant growth in recent years. In 2021, China exported 3.201 billion solar cells worth US$28.460 billion, and from January to October 2022, exports increased to 3.294 billion units valued at US$40.033 billion. When these solar panels are installed on water bodies, they show improved performance due to the cooling effect of water, which increases efficiency by 16%. This water-cooling effect also helps reduce losses and extends the plant's life. The installations can be placed on lakes, water basins, or reservoirs, making them particularly valuable in areas with limited land availability.	"['DUBLIN--(BUSINESS WIRE)--The ""Research Report on China\'s Solar Cell Export 2023-2032"" report has been added to ResearchAndMarkets.com\'s offering.\nChina is the world\'s leading exporter of solar cells, exporting a large number of solar cells every year. According to the publisher\'s analysis, in 2021, China exported 3.201 billion solar cells, up 17.56% year-on-year, with an export value of US$28.460 billion, up 43.79% year-on-year. From January to October 2022, China exported 3.294 billion solar cells, up 24.06% year-on-year, with an export value of US$40.033 billion, up 75.58% year-on-year.\nIn 2018-2022, the average price of China\'s solar cell exports showed an overall change trend of first decrease and then increase. In 2018-2020, the average price of China\'s solar cell exports fell continuously, from US$12.18 each in 2018 to US$7.27 each in 2020. In 2021-2022, the average price of China\'s solar cell exports rose continuously. From January to October 2022, the average price of China\'s solar cell exports was US$12.15 each, up 41.53% year-on-year.\nIn 2021, China exported solar cells to more than two hundred countries and regions around the world. The publisher\'s analysis shows that India, Turkey, Vietnam, South Korea, Germany, Malaysia, Cambodia, Thailand, the Netherlands and the Philippines are China\'s major solar cell export destinations by export volume. Among them, India is the largest exporter of solar cells from China.\nIn 2021, China exported 868 million solar cells to India, accounting for 27.11% of the total solar cell exports in that year, with an export value of US$3.914 billion, accounting for 13.75% of the total export value. By export value, then the Netherlands is the largest country in China\'s solar cell exports, and in 2021, China\'s solar cell exports to the Netherlands amounted to US$5.990 billion, accounting for 21.05% of the total exports.\nChina is a major global producer and exporter of solar cells, and the publisher expects China\'s solar cell exports to continue to rise from 2023-2032 as the global new energy sector grows.\n- China\'s Solar Cell Export Status and Major Sources in 2018-2022\n- What is the Impact of COVID-19 on China\'s Solar Cell Export?\n- Which Companies are the Major Players in China\'s Solar Cell Export Market and What are their Competitive Benchmarks?\n- Key Drivers and Market Opportunities in China\'s Solar Cell Export\n- What are the Key Drivers, Challenges, and Opportunities for China\'s Solar Cell Export during 2023-2032?\n- What is the Expected Revenue of China\'s Solar Cell Export during 2023-2032?\n- What are the Strategies Adopted by the Key Players in the Market to Increase Their Market Share in the Industry?\n- What are the Competitive Advantages of the Major Players in China\'s Solar Cell Export Market?\n- Which Segment of China\'s Solar Cell Export is Expected to Dominate the Market in 2032?\n- What are the Major Adverse Factors Facing China\'s Solar Cell Export?\nKey Topics Covered:\n1. 2018-2022 China Solar Cell Export Analysis\n1.1. China\'s Solar Cell Export Scale\n1.1.1. China\'s Solar Cell Export Volume\n1.1.2. China\'s Solar Cells Export Value\n1.1.3. China Solar Cells Export Price\n1.2. China\'s Main Export Destinations of Solar Cells\n1.2.1. By Export Volume\n1.2.2. By Export Value\n2. 2018-2022 China\'s Export Analysis of Monocrystalline Silicon Solar Cells\n2.1. Export Volume of Monocrystalline Solar Cells\n2.2. Export Value of Monocrystalline Solar Cells\n2.3. Export Price of Monocrystalline Solar Cells\n2.4 Export Destinations of Monocrystalline Solar Cells\n2.4.1. By Export Volume\n2.4.2. By Export Value\n3. 2018-2022 China Polycrystalline Solar Cells Export Analysis\n3.1. Export Volume of Polycrystalline Solar Cells\n3.2. Export Value of Polycrystalline Solar Cells\n3.3. Export Price of Polycrystalline Solar Cells\n3.4 Export Destinations of Polycrystalline Solar Cells\n3.4.1. By Export Volume\n3.4.2. By Export Value\n4. 2018-2022 China Amorphous Silicon Thin Film Solar Cells Export Analysis\n4.1. Export Volume of Amorphous Silicon Thin Film Solar Cells\n4.2. Export Value of Amorphous Silicon Thin Film Solar Cells\n4.3. Export Price of Amorphous Silicon Thin Film Solar Cells\n4.4. Export Destinations of Amorphous Silicon Thin Film Solar Cells\n4.4.1. By Export Volume\n4.4.2. By Export Value\n5. 2018-2022 China Other Solar Cells Export Analysis\n5.1. Other Solar Cells Export Volume\n5.2. Other Solar Cells Export Value\n5.3. Other Solar Cells Export Price\n5.4. Other Solar Cells Export Destinations\n5.4.1. By Export Volume\n5.4.2. By Export Value\n6. 2018-2022 China Solar Cells Major Export Destinations Analysis\n6.4. South Korea\n6.7. Other Export Destinations\n7. China\'s Export Outlook for Solar Cells, 2023-2032\n7.1 Factors Affecting China\'s Solar Cell Exports\n7.1.1 Favorable Factors\n7.1.2. Unfavorable Factors\n7.2. China\'s Solar Cell Export Forecast, 2023-2032\n7.2.1. Export Volume Forecast\n7.2.2. Major Export Destinations Forecast\n7.2.3. Major Exported Solar Cell Types Forecast\nFor more information about this report visit https://www.researchandmarkets.com/r/y842eh\nLaura Wood, Senior Press Manager\nFor E.S.T Office Hours Call 1-917-300-0470\nFor U.S./ CAN Toll Free Call 1-800-526-8630\nFor GMT Office Hours Call +353-1-416-8900', 'Utilising the Incessant Solar Energy to Human Advantage\nHumans and nature are inextricably coupled - it underpins our economy, our society, indeed our very existence. Since ages, humans have been possessing the knowledge of how to best exploit the nature to reap of maximum benefits. Humans have explored atmosphere, heat, soil, water, and sunlight to their advantage. However, the sun has been the most crucial natural resource for the life on earth. The use of solar energy for electricity generation has been around for several decades and the way of application still continues to evolve on the back of technology intervention. With the world emphasizing green energy alternatives to meet sustainability targets set for 2050, the preference for clean fuel power generation is on a constant rise. The market for floating solar panels is thus witnessing an impressive growth outlook banking on its key advantages. As their installations do not require massive land spaces and the overall projects are cost-effective, floating solar panels are expected to witness notable demand during the forecast period. Floating solar panel projects can even be set up on unused spaces on water bodies, which will remain the top advantageous factor driving the growth of the floating solar panels market.\nSolar Energy Technology Capturing Global Interests, Pushing Prospects of Floating Solar Panels Market\nAccording to Renewable Energies Info, the earth’s surface receives 120,000 terawatts of solar irradiation, which represents 20,000X power than the whole planet needs. Moreover, National Aeronautics and Space Administration (NASA) states that the sun still has around 6.5 billion years of life, which makes solar energy the most desirable, abundant, and freely available natural resource for electricity generation. Indeed, in rather less time, solar technology in some countries has evolved to compete with conventional sources of electricity generation. In just a few decades’ time, it will most likely be the mainstream sustainable energy system for the world. Given the rapidly growing energy requirements, exploding investments in the renewables sector, governmental and non-governmental efforts in promoting solar energy technology, falling technology prices, and strengthening network of solar panel installers, the floating solar panels market is poised for a promising growth outlook.\nCovering Water with Solar Panels Is a Power Move\nFor the technology to be effective, large structures of panels, and arrays are necessary in order to capture enough sunlight - and water bodies naturally provide a plenty of space. This requires large amounts of space, and given the fact that around 70% of the world’s surface is covered by water, it is unsurprising that companies have sought to develop floating solar energy farms. Instead of leaving water bodies open and letting the sun evaporate the water away, cover them with panels that would both shade the precious liquid, and hoover up solar energy. The turbulent oceans are however not the suitable location for large solar arrays. Reservoirs, and lakes that can provide calmer waters need to safely operate a fleet of floating solar energy farms.\nUnlike traditional solar panel installations that are affixed to roofs, or ground mounts, floating panel installations are placed right on lakes, water basin, or reservoirs. Building the sun-sucking tech on water is an ingenious way of freeing up land. In places where there is scarcity of available land, building of medium, and large-scale plants on water surfaces is the correct, viable choice. This is the case for Japan, which hosts 73 of the world’s 100 largest floating solar plants accounting for 246MW of solar capacity.\nInnovations Continue to Pour in Solar Industry\nThe largest floating solar panels could generate huge amounts of clean energy at a reasonable cost while saving a lot of water (the main requirement for flora, and fauna to survive) by reducing evaporation. Covering waterways would, in a sense, make solar panels water-cooled, boosting their efficiency. An increase of 16% in efficiency is due to the cooling effect of water on panels, reduce losses and extend the life of the plant. Greater efficiency offsets the higher costs of installation (meet decarbonisation goals as well). According to estimations of Nature Sustainability, the net present value of canal-spanning solar systems could be as much as 50% higher than solar farms on nearby land.\nThe deployment of floating solar panels is growing in popularity around the world. The outlook for floating solar panels is positive and set to break free of its niche perception. More than 35 countries today have an estimated 350 floating solar panels. Southeast Asia has roughly two thirds of the total floating solar panel installations globally, mostly in China, India, South Korea, Taiwan, Thailand, and Vietnam. While China, and India together account for six of the world’s 10 biggest floating solar projects in the various stages of their development, South Korea accounts for two as of now. Researchers at University of California have introduced a new creation – anti-solar panels that can potentially harness solar power at night for energy generation round the clock. This is being increasingly perceived as the future of solar industry.\nKey Players in Floating Solar Panels Market\nCanadian Solar, Conergy, First Solar, Hanwha Q Cells, GCL-SI, JA Solar, JS Solar, JinkoSolar, Kyocera, LONGi Solar, Mage Solar, MUST Solar, Panasonic, REC Solar, Risen Energy, Samsung, Schott, Seraphim Energy Group, Sharp, Suntech, SunPower, Talesun Solar, Trina Solar, and Yingli Solar constitute some of the key market players competing in the global floating solar panels landscape.\nKey Elements Included In The Study: Global Floating Solar Panels Market\nPost Sale Support, Research Updates & Offerings\nWe value the trust shown by our customers in Fairfield Market Research. We support our clients through our post sale support, research updates and offerings.\nConsidering the volatility of business today, traditional approaches to strategizing a game plan can be unfruitful if not detrimental. True ambiguity is no way to determine a forecast. A myriad of predetermined factors must be accounted for such as the degree of risk involved, the magnitude of circumstances, as well as conditions or consequences that are not known or unpredictable. To circumvent binary views that cast uncertainty, the application of market research intelligence to strategically posture, move, and enable actionable outcomes is necessary.Read more']"	['<urn:uuid:fd5f1f99-226d-4c72-b1ff-47b3c0c5e8a7>', '<urn:uuid:58dfefec-8461-49de-817b-1c01f3709d3c>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:18:19.158519	21	100	1821
88	I'm studying environmental science and wondering - how do wind power companies' conservation strategies differ from what other renewable energy developers are doing to protect wildlife?	Wind power companies use specific technological solutions, including detection and deterrent systems for raptors and bats, along with the Landscape Assessment Tool for early screening of wildlife conflicts. They also collect and analyze wildlife impact data through the AWWIC system. In contrast, other renewable energy developers are urged by CLEANaction to focus on broader strategies like avoiding protected areas and Key Biodiversity Areas, applying stringent environmental impact assessments, and ensuring traceability of raw materials. Wind power's approach is more technologically focused and species-specific, while other renewables take a broader ecosystem-based approach, though both aim to achieve positive environmental outcomes.	"[""AWWI drives the implementation of innovative strategies in the conservation and research communities and the wind development field.\nFor more than 20 years, wind energy companies have undertaken studies to assess risk and impacts to wildlife from wind energy projects. Some data are publicly available but other data are privately owned. AWWI is building the American Wind Wildlife Information Center (AWWIC, pronounced ‘a-wic'; formerly know as the Research Information System), a first-of-its kind initiative, which will securely house wind-wildlife data and make it available for scientific analysis. Collection of existing data through the AWWIC is the most promising and efficient way to increase overall understanding of risks and impacts. The scientific analysis of the data in the AWWIC, which will be guided by AWWI, will:\nTo minimize risk, wind project developers and decision makers need cost-effective tools that enable them to screen locations for potential wildlife and habitat conflicts early in the siting process. AWWI and The Nature Conservancy (TNC) built and maintain the Landscape Assessment Tool (LAT), a GIS-based, publicly-available mapping tool that collates and displays biological information that is relevant to wind-energy development. Users can display web-based maps featuring information culled from over 1,000 data layers relevant to the identification of potential wildlife and environmental impacts and to landscape-level screening for potential project sites. The LAT is a currently the only national decision support tool for energy development, with a focus on the data most relevant to wind energy.\nAWWI’s collaborative network offers a unique opportunity to bring biologist together with engineers to explore challenges and opportunities related to technological solutions to wind-wildlife impacts. By creating a forum for this interaction, AWWI serves as a catalyst for new and exciting ideas that have the potential to drastically reduce the impacts of wind energy on sensitive species.\nAWWI conducted a Request for Information (RFI) to gather information from vendors that have technology products available for immediate testing and/or deployment. The results of this RFI are compiled in a Technology Catalog made available to AWWI Partners and Friends.\nAWWI’s Technology Innovation program coordinates testing, evaluation, and verification of available and in-development technologies intended to detect and deter raptor and bat species at wind energy facilities.\nScience for Policy & Practice: Through research and tool development, AWWI provides the science that wind-wildlife stakeholders need to make decisions in the face of uncertainty. More\nInformation Exchange: Through our Partners and Friends and the National Wind Coordinating Collaborative, AWWI provides a forum to share wind-wildlife solutions and ensure that information empowers action that leads to sustainable results. More\nAWWI’s success is based on wide collaboration with all who hold a stake in the success of wind energy and the protection of wildlife and habitat. By working together, we can move toward a common goal. Here is how you can help:\nOffer your input and ideas to build the AWWI collaboration or support AWWI as a partner organization. To learn more about opportunities to engage with AWWI, contact us.\nWind Energy and Wildlife. We need both for a healthy, sustainable planet. Your donation will support AWWI’s work which generates technological innovation, policy-relevant science, and outreach and education."", 'SHIFT TO RENEWABLE ENERGY MUST PROTECT NATURE, SAYS CLEAN ENERGY COALITION\nNew report finds all forms of renewable power are better for nature than fossil fuels.\nA transition focused on wind and solar PV can result in significantly reduced environmental impacts compared to other renewable energy types.\nThe rapid transition to renewable energy offers opportunities to reset the broken relationship between energy production and nature, according to a new report by the Coalition Linking Energy And Nature for action ().\nIn its first major report, CLEANaction confirms that even when the full range of environmental impacts - from sourcing raw materials to final operation - is considered, generating and storing energy from renewables is far less environmentally damaging than using fossil fuels. The report argues that a transition focused on wind and solar can result in significantly reduced environmental impacts compared to other renewable energy types, although other renewables can be the most appropriate solution depending upon the local circumstances.\nTo limit global warming to 1.5ºC above pre-industrial levels and avoid the worst risks of climate change, renewable energy will need to account for more than 90 per cent of electricity generation by 2050 according to International Energy Agency. The report explains how, in a vital decade for action, the potential for negative impacts from the energy transition can be carefully managed to ensure that renewable energy technologies causing the least damage to nature are prioritized.\nCLEANaction is a coalition of NGOs, leading businesses, government bodies and financial institutions established in recognition of the urgent need for a global and just transition to a low-impact and nature-sensitive renewable energy system. The founding members are WWF, IRENA, ICLEI, The Nature Conservancy, Birdlife International and the Alliance for Rural Electrification (full list of members in the notes below).\nDean Cooper, WWF global energy lead, said: “Climate change is causing havoc across the globe and, together with habitat loss and overexploitation, is a leading cause of reduced biodiversity worldwide. This loss of biodiversity, and of the natural carbon uptake and storage enabled by healthy ecosystems, exacerbates the climate crisis and threatens the health and wellbeing of humankind around the world. We all must urgently shift to 100% renewable energy, but we must take care to protect and enhance nature at the same time.”\nExisting global-scale mapping of sites for wind and solar indicate there is enough energy available in areas which have low conflict with biodiversity to achieve projections from the International Energy Agency for a power system consistent with holding global temperature rise to below 1.5ºC. This Paris Agreement goal is a crucial threshold to avoid the most catastrophic impacts of climate change on people and nature.\nSignificant changes are also needed in how we source and trace materials, such as rare earth elements, for developing our energy infrastructure. A new, circular economic model is essential to reduce environmental impacts, according to the report.\nA circular economic and energy efficient model should prioritize the reduction of primary materials, as well as reuse and recycle to minimize further extraction and impacts related to the disposal of end-of-life equipment. When mining occurs, rigorous environmental and social safeguards must be in place to avoid the degradation of natural habitat and other harms, according to the report.\nTo achieve a clean energy future that takes full account of the impact on nature, CLEANaction is urging governments to:\n- Undertake strategic-level energy planning at national or regional scales to identify potential energy savings, suitable renewable energy sources, and sites for energy expansion in areas of low biodiversity sensitivity.\n- Consider the impact on nature at the earliest stage of integrated clean energy planning, taking account of the full value chain (from sourcing material to disposal).\n- Develop national regulatory schemes that require energy developers to contribute to national conservation targets.\n- Invest in timely nature-sensitivity mapping to help direct technology siting through proper data, and require industry to avoid protected areas, Key Biodiversity Areas, and other areas of particular sensitivity and value.\n- Apply stringent environmental impact assessment processes and required standards to all new developments according to best practice.\n- Adopt a circular economy approach with optimised energy efficiency, to maximise reuse of energy materials, and minimise demand for natural resources.\nCLEANaction is calling on renewable energy investors and developers to:\n- Integrate biodiversity, social and environmental risks early into renewable energy planning and investment decisions.\n- Apply effective biodiversity safeguards and environmental impact assessment procedures to avoid and minimise impacts, and offset any residual impacts to achieve net-positive outcomes.\n- Ensure there is traceability of raw materials and account for supply chain impacts within corporate commitments to nature.\n- Apply a circular approach to minimise the use of primary materials and maximise the reuse and recycling of materials.\n- Strengthen corporate disclosure and reporting on biodiversity, environmental and social impacts.\nNotes to editors:\n- CLEANaction members include the Alliance for Rural Electrification (ARE), Birdlife International, Iberdrola, ICLEI-Cities Biodiversity Center, the International Renewable Energy Agency (IRENA), The Nature Conservancy (TNC), Nordic Energy Research, Orsted, the PanAfrican Climate Justice Alliance, Spoor and WWF. .']"	['<urn:uuid:1835cb57-0931-402a-b978-e17f8b5cb178>', '<urn:uuid:6b9dd57e-b4be-408c-afd0-c65877bd0f29>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T05:18:19.158519	26	99	1370
89	What makes matrix management notoriously difficult?	Matrix management is notoriously difficult because it creates culture clashes, with accountabilities running both vertically (to the team) and horizontally (to the cross-cutting theme).	['[This article was originally published on Simon Maxwell’s blog on 20 February 2020.]\nA cross-cutting issue in a think-tank is one which involves several departments or teams. Historically, poverty and globalisation have been examples. ‘Africa’ has been another. ‘Europe’ is one we struggled with in my time at ODI, needing input on aid, trade, finance, and humanitarian action, among others. Climate change is a current example, and the inspiration for this piece. It goes without saying that climate change needs to be on everyone’s agenda – and that, given the complexity of the issue, people need to work together.\nCross-cutting issues are difficult to manage because they complicate management and create conflicting accountabilities. They are also often unfunded. The result can be that a leadership team decides on a cross-cutting issue, even identifies one of its number to be ‘in charge’, but then finds that people in the various teams are too busy with other projects, facing different incentives, or more accountable to their own team leaders than to the person responsible for the cross-cutting theme.\nWhat can be done? This was the question I posed to young think-tank leaders at the WinterSchool for Thinktankers 2020, held in Geneva in February, and jointly sponsored by On Think Tanks, foraus and the The Think Tank Hub. We had already worked on a case study on linking research and policy in the field of climate change, and I then presented them with the following. It is the latest in a series of ‘pizza night’ cases: the first four (from 2018) deal with various aspects of think tank governance, the next (from 2019) deals with planning policy-relevant research (with climate change as an example). Observant readers will note that all the cases begin and end in the same way: the challenge is embedded in the body of the text.\nThe office was closed and everyone had left, but in one corner of the building a light still burned. Cecilia wanted to go home to her family, it was pizza night, and she had hardly seen the children all week. But there was a Board meeting coming up, and as Director of the think tank, it was preying on her mind.\nAt the last Board meeting, Cecilia had presented her plan for a climate change task force, focused on the Glasgow COP in 2020. The Plan was exciting and forward-looking, and the Board had loved it. But now she was worried. Time was slipping past, and the ambition she had laid out was not being fulfilled. Her colleagues faced too many distractions, and too few incentives. The Board was worried, too. ‘Cecilia’, the Chair had said, ‘You are the manager as well as the leader. You need to think about the levers you can pull, and manage your way out of this’. Cecilia sighed. The Chair was right. She needed an action plan of her own to drive change internally. But was this a strategy problem, she wondered, or business planning, or budget, or HR? What were her levers, actually? And how could she use them?\nCecilia pulled off a page from her pad, and wrote a heading: ‘How to deliver the climate action plan’. She needed to fill that in, but it was too late to do more. She thought of the pizza and her mouth began to water. Margarita, she wondered? Or Quattro Staggioni? It was time to go home. Cecilia rose, stretched, and switched off the light.\nIt was interesting that the first response of the young think tank leaders, paraphrasing, was ‘well, just tell them to do it’. It’s an obvious answer, and sometimes it works. An anecdote: I once sat next to Paul Kagame, President of Rwanda, at a lunch in Davos, and asked him how he had brought about what many considered a difficult change, then in the news. ‘I just told them’, he said.\nNot being either Paul Kagame, or a former General or President of any kind, I found myself telling the young leaders about Charles Handy’s wonderful book on Understanding Organisations, first published in 1976, a constant companion when I was teaching mid-career professionals at the Institute for Development Studies (IDS), Sussex in the 1980s and 1990s, and a useful resource in later roles.\nIf you want to change something, Handy said, think about your source of power and influence. Is it physical power, based on your size and strength? Is it resource power, based on your control of resources? Is it position power, in which ‘the manager is by right allowed to order people to do so-and-so’? Is it expert power, because you know best? Is it personal power, derived from charisma? Or is it negative power, the capacity to stop things happening?\nPhysical. Resource. Position. Expert. Personal. Negative. Which of these powers do think tank leaders have, and which are they prepared to use? Not physical, obviously; and not negative, given that the project is to get something done rather than stop something happening. Probably not expert, given that think tanks are stuffed full of experts. Probably not charisma either, given that think tank leaders are mostly researchers (that’s a joke against researchers, but it reminds me of the definition of an economist, which is ‘just like an accountant, but without the charisma’. Oh dear: I am both a researcher and an economist.).\nThe young thinktankers zeroed in on position power, but think tank leaders face difficult constraints, and position power is not always what it is cracked up to be. For example, fund-raising is often devolved to individuals or teams, which means that their first loyalty is to their external funders – well, anyway, it means that meeting contract deadlines is a higher priority than satisfying the latest whim of the leadership team. Further, creating cross-departmental work streams immediately imposes a kind of matrix structure on the organisation, with accountabilities running both vertically (to the team) and horizontally (to the cross-cutting theme). Matrix management is notoriously difficult, creating culture clashes, as Handy describes.\nThese are not problems limited to think tanks, by the way. I have written about the problem of cross-sectoral planning in the context of food security, drawing on experience from rural development, multi-sectoral nutrition planning, farming systems research, poverty planning, and industrial organisation. I also thought a lot at ODI (London) about role cultures and task cultures: people mostly wanted the problem-focused characteristics of the latter, but found themselves working in the more hierarchical structures of the former.\nThere’s a literature on delivering strategic change, of course. Duncan Green cited this diagram by Delores Ambrose in a recent blog (Figure 1), making the point that vision was of little use on its own, without strategy, skills, incentives, resources and an action plan. Personally, I find the Dorling Kindersley management handbooks a useful introduction to lots of topics. There is a Dorling Kindersley compendium called The Essential Manager’s Handbook, which provides useful advice. There is a separate little volume on Strategic Management, and another on Strategic Thinking: both deal with implementation challenges relevant here, like managing resistance to change, using targets and motivating people.\nIn the case of think-tanks, clues to the answer are given in the case study: strategy, business planning, budget, and HR are all involved. Charisma plays a role too. Some pointers:\nFirst, ‘cross-cutting’ is not the same as ‘mainstreamed’. Mainstreaming is a frequent recourse for organisations trying to highlight a common issue: gender is a frequent case, difficult enough in its own terms, and often ineffective without a strong, practical management push. Cross-cutting implies something different, not just that the issue should be a priority across an organisation, but also that teams should collaborate. Ideally, there should be shared frameworks and analysis across the organisation; even, allowing for differences of opinion, a shared ‘narrative’.\nSecond, it definitely helps if the think tank director articulates a cross-cutting priority and is insistent. I remember Mike Faber, Director of IDS, Sussex in the mid-1980s, drawing our attention to the growing crisis in Africa, when structural adjustment became a big issue, and asking us to focus on the topic. No resources were available, but most of us adjusted our work programmes. That was when I began to work on food security in Africa. So a combination of charisma and position power had an effect.\nThird, the cross-cutting theme needs to be visible in the five-year strategy, because it sends a signal and creates expectations, not least among the Board. A strategy document may also commit to creating an innovation fund, which can provide seed-funding for new cross-cutting themes.\nFourth, it is important to have the right person leading cross-cutting work. This should be someone senior enough to work across programmes and teams, probably a member of the senior leadership team; conceivably, for a major issue like climate change, the chief executive personally. Appointing someone too junior can mean that the issue disappears in management terms: for example, it may not appear with sufficient regularity on the agenda of senior management meetings.\nFifth, work on the cross-cutting theme should not just be driven top-down, but should also engender and build on energy from below. Creating a task force may be a good way to do this, with representatives of different teams. However, beware endless meetings and high transactions costs. An internal series of brown-bag lunches can be a good idea. In the case of climate change, internal change may also drive externally-focused research and policy work: develop an environmental plan, monitor and reduce flights, recycle, reduce bottled water, even (in one case with which I was associated) reduce or ban meat from the canteen.\nSixth, more detailed research and policy planning takes place in the context of the annual business plan. The structure of these varies, but it is usual to have chapters for both teams and cross-cutting themes, and for all of these to specify the big questions, the work streams and the expected outputs. Normally, also, the business plan will identify which staff will contribute to different activities. The plan never survives contact with reality, of course, especially if there is no funding, but again provides a template against which to assess progress. If an innovation fund has been created, then the business plan can be the point at which resources are committed.\nSeventh, personal incentives need to be built into individual work plans, both for the cross-cutting theme leader and for individual researchers. Usually, a sketch of next year’s work programme is produced at the time of the annual appraisal. Does this include work on the cross-cutting theme? Are there specific outputs? Is it clear to the individual researcher that increments, bonuses and promotions are dependent on delivering for the cross-cutting theme as well as other work?\nEighth, management can help create opportunities to work with others on the cross-cutting theme. A model I have used is one of hubs and spokes: a wheel where each researcher pursues their own line of work, the individual spoke, but all come together at the hub from time to time, for joint projects. For example, there could be a shared series of public meetings, or a co-edited edition of a journal, a book even, or a briefing paper summarising the state of thinking on the cross-cutting theme. It is better, of course, if these are funded, and there may be scope to apply for new funding. However, cheap options need to be kept in mind: it costs very little, for example, to organise a series of public meetings. Joint activities need to be in the business plan. In all cases, working together helps to create a shared narrative.\nNinth, don’t forget to celebrate success – at the Board, in staff meetings, on the website, in annual reports, and in one-to-one encounters. Even better if the think tank’s work can be recognised as having produced real change.']	['<urn:uuid:71cadc9c-5f22-42a1-8b83-d55eab313e42>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	6	24	1975
90	I'm researching quality management systems and would like to understand how the Gemba principles connect with visual control methods and standardization in improving operational efficiency?	Gemba principles connect with visual control and standardization through several key aspects. Visual control methods require simple indicators that immediately show whether processes are in standard condition, with reports reduced to one page when possible. These connect to standardized tasks, which form the foundation for continuous improvements by maintaining predictable, regularly timed processes. When abnormalities occur, the Gemba principle requires going to the actual place first to check the situation, supporting both visual control and standardization efforts.	"['Fig.1.2. 14 Principles of 4P\n1P: Long Term Philosophy\nPrinciple 1: Base your management decisions on a long term philosophy, even at the expense of short-term financial goals\n- Have a philosophical sense of purpose that supersedes any short-term decision making. Work, grow and align the whole organisation toward a common purpose that is bigger than making money. Understand your place in the history of the company and work to bring the company to next level. Your philosophical mission is the foundation for the other principles.\n- Generate value of the customer, society and the economy - it is your starting point. Evaluate every function in the company in terms of ability to achieve this.\n- Be responsible. Strive to decide your own fate. Act with self reliance and trust in your own abilities. Accept responsibility for your own conduct and maintain improve the skills that enable you to produce added value.\n2P: The Right Process Will Produce the Right Results\nPrinciple 2: Create continuous process flow to bring problems to the surface\n- Redesign work process to achieve high value-added, continuous flow. Strive to cut back to zero the amount of the time that any work project is sitting idle or waiting for someone to work on it.\n- Create flow to move material and information fast as well as to link processes and people together so that problems surface right way.\n- Make flow evident throughout your organizational culture. It is the key to a true continuous improvement process and to developing people.\nPrinciple 3: Use ""Pull"" system to avoid overproduction\n- Provide your downline customers in production process with what they want, when they want it and in the amount they want it. Material replenishment initiated by consumption in the basic principle of just-in-time.\n- Minimize your work in process and warehousing of inventory by stocking small amounts of each product and frequently restocking based on what the customer actually takes away.\n- Be responsive to the day-by-day shifts in customer demand rather than relying on computer schedules and systems to track wasteful inventory.\nPrinciple 4: Level out the workload (heijunka). (Work like a tortoise, not the hare)\n- Eliminating waste is just one-third of the equation for making lean successful. Eliminating overburden to people, equipment and eliminating unevenness in the production schedule is just as important - yet generally not understood at companies attempting to implement lean principles.\n- Work to level out the workload of all manufacturing and service processes as an alternative to the start/stop approach of working on projects in batches that is typical at most companies.\nPrinciple 5: Build a culture of stopping to fix problems, to get quality right at the first time\n- Quality of the customer drives your value proposition\n- Use all the modern quality assurance methods available\n- Build into your equipment the capability of detecting problems and stopping itself. Develop a visual system to alert team or project leaders that a machine or process needs assistance. Jidoka (machines with human intelligence) is the foundation for ""building in"" quality.\n- Build into your organisation support systems to quickly solve the problems and put in place countermeasures.\n- Build into your culture the philosophy of stopping or slowing down to get quality right the first time to enhance productivity in the long run.\nPrinciple 6: Standardized tasks are the foundation for continuous improvements and employee empowerment\n- Use stable, repeatable methods everywhere to maintain the predictability, regular timing and regular output of your processes. It is the foundation for the flow and pull.\n- Capture the accumulated learning about a process upto a point in time by standardizing today\'s best practices. Allow creative and individual expression to improve upon the standard; then incorporate it into the new standard so that when a person moves on you can hand off the learning to the next person.\nPrinciple 7: Use Visual Control so no problems are hidden\n- Use simple visual indicators to help people determine immediately whether they are in standard condition or deviating from it.\n- Avoid using a computer screen when it moves the worker\'s focus away from the workplace.\n- Design simple visual system at the workplace where the work is done, to support flow and pull.\n- Reduce your reports to one piece of paper whenever possible, even for your most important financial decisions.\nPrinciple 8: Use only reliable, thoroughly tested technology that servers your people and process\n- Use technology to support people not to replace people. Often it is best to work out process manually before adding technology to support the people.\n- New technology is often unreliable and difficult to standardize and therefore endangers ""flow"". A proven process that works generally takes precedence over new and untested technology.\n- Conduct actual tests before adopting new technology in business processes, manufacturing systems or products.\n- Reject or modify technologies that conflict with your culture or that might disrupt stability, reliability and predictability.\n- Nevertheless encourage your people to consider new technologies when looking into new approaches to work. Quickly implement a thoroughly considered technology if it has been proven in trials and it can improve flow in your processes.\n3P: Add value to your organisation by Developing your people and partners\nPrinciple 9: Grow leaders who thoroughly understands the work, live philosophy and teach it to others.\n- Grow leaders within, rather than buying them from outside the organisation.\n- Do not view the leader\'s job as simply accomplishing tasks and having good people skills. Leaders must be role models for the company\'s philosophy and the way of doing business.\n- A leader must understand the daily work in great detail so that he or she can be a best teacher of your company\'s philosophy.\nPrinciple 10: Develop exceptional people and teams who follow your company\'s philosophy\n- Create a strange, stable culture in which company values and beliefs are widely shared and lived out over a period of many years.\n- Train exceptional individuals and teams to work within the corporate philosophy to achieve exceptional results. Work hard to reinforce the culture continually.\n- Use Cross functional teams to improve quality and productivity and enhance flow by solving difficult technical problems. Empowerment occurs only when people use the company\'s tools to improve company.\n- Make an ongoing effort to teach individuals how to work together as teams together toward common goals. Team work is something that has to be learned.\nPrinciple 11: Respect your extended network of partners and suppliers by challenging them and helping them improve\n- Have respect for your partners and suppliers and treat them as an extension of your business.\n- Challenge your outside business partners to grow and develop. It shows that you value them. Set challenging targets and assists your partners in achieving them.\n4P: Continuously Solving Root Problems Drives Organisational learnings\nPrinciple 12: Go and see for yourself to thoroughly understand the situation (Genchi Genbutsu)\n- Solve problems and improve processes by going to the source and personally observing and verifying data rather than theorizing on the basis of what other people or the computer screen tell you.\n- Think and speak based on personally verified data.\n- Even high-level managers and executives should go and see things for themselves, so they will have more than a superficial understanding of the situation.\nPrinciple 13: Make decision slowly by consensus, thoroughly considering all options; implement decisions rapidly\n- Do not pick a single direction and go down that one path until you have thoroughly considered alternatives.\n- Nemawashi is the process of discussing problems and potential solutions with all of those affected, to collect their ideas and get agreement on a path forward. This consensus process, though time - consuming, helps broaden the search for solutions, and once a decision is made, the stage is set for rapid implementation.\nPrinciple 14: Become a learning organisation through relentless reflection (hansei) and continuous improvements(Kaizen)\n- Once you have established a stable process, use continuous improvement tools to determine the root cause of inefficiencies and apply effective countermeasures.\n- Design processes that requires almost no inventory. This will make wasted time and resources visible for all to see. Once waste is exposed, have employees use a continuous improvement process (kaizen) to eliminate it.\n- Protect the organisational knowledge base by developing stable personnel, slow promotion and very careful succession systems.', ""Applying lean principles and kaizen rapid a gemba walk, debrieﬁng waste, mapping the process, brainstorming improvement ideas, and creating a list. Gemba kaizen: a commonsense low-cost approach to management user review - not available - book verdict in this sequel to his popular business/quality management book. What is kaizen 13 the golden rule of gemba management, called the 5-gemba principles, is as follows: when a difficulty (abnormality) arises, go to gemba first. The methods presented in gemba kaizen reveal the toyota way is the first book for a general audience that explains the management principles and business. Gemba kaizen - free download as powerpoint presentation (ppt), pdf file 5-gemba principles 1 when a trouble (abnormality) happens, go to gemba first.\nGemba kaizen, the management approach for competitive success whilst protecting the environment masaaki imai the five gemba principles fall under this category. Foundations for gemba teams:this course presents the principles of operational excellence in both private and public sector the course covers kaizen™ tools and. Kaizen basic education | lean manufacturing february 26, 2015 february 6 (process-oriented principles) the author behind japanese gemba kaizen web. Kaizen principles kaizen is interview with mr masaaki imai, the author of gemba kaizen creative safety supply 8030 sw nimbus ave beaverton, or 97008. The kaizen principle is a japanese management philosophy which literally translates to continuous improvement this article describes in detail the kaizen principle.\nGemba and kaizen are japanese words the former means “real place” and the latter means “improvement” or “change for the better kaizen practices focus. The ten basic kaizen principles 1 throw out all your old fixed ideas on how to do things 2 no blame - treat others as you want to be treatedtreat others as you want. Get better at getting better: the kaizen it's also important to keep the basic principles in mind while you're working—you don't gemba kaizen: a commonsense.\nBased on masaaki imai\\'s teachings on \\'kaizen\\' and \\'gemba kaizen\\' and workplace organization is a set of basic management principles that are widely adopted. Kaizen institute consulting group cursul bazele kaizen™ este primul din seria practicant a gemba kaizen this course presents the principles of. Kaizen institute consulting group home page this course presents the principles of operational excellence in both private and gemba kaizen buy now. Utility of gemba kaizen model on public sector this study examined the utility of gemba kaizen model in government kaizen approach premises on principles.\nWe love to go to the gemba and watch the value gemba principles is a great my faults lie in having too much of a problem view instead of a kaizen view. Applying the kaizen method and the 5s technique in the activity of post-sale services • gemba – the real place where each employee works. Masaaki imai (1930) is one of the founders and a very famous advisor in the field of the kaizen, gemba kaizen, lean manufacturing and quality philosophies especially.\nKaizen (改善), is the japanese word for improvement in business, kaizen refers to activities that continuously improve all functions and involve all employees. Gemba is one of the key principles of lean manufacturing and the toyota production system get your free introduction to kaizen. Application and usefulness of 5s and kaizen for library space kaizen, 5s, pdca cycle, tqm, gemba, 1 kaizen principles and its impact on the library. Are the trademarks of kaizen institute the 5 gemba principles 1 when an abnormality occurs, go to gemba first 2 check with gembutsu (machine, material. Management but rather should be regarded as sound principles to be improvement kaizen kaizen kaizen kaizen kaizen kaizen the the gemba gemba kaizen. What is kaizen the definition of and kaizen gemba is a philosophy in lean real processes and day-to-day undertaking of lean principles the benefits of lean.\nLean training, consulting companies – a business blog for kaizen institute india that is including in all types of lean information business blogs also lean to be. Founding principles useful terms of gemba kaizen may be understood simply as eliminating wastes and striving for continuous improvement at the actual place or. Praise for gemba kaizen it’s exciting to see an updated version of a classic book, gemba kaizen, which shares a wealth of new healthcare examples and case studies. Kaizen secret is to go to gemba after going to gemba, what to do observe we have so for discussing the philosophies and principles of kaizen. Gemba walk gemba walks denote the action of going to see the actual process, understand the work, ask questions, and learn it is also known as one fundamental part.""]"	['<urn:uuid:ef63feb6-1c5c-4f68-b541-240cb870a2fd>', '<urn:uuid:af75466f-68a6-4d6e-ad99-b4055a5204d2>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-13T05:18:19.158519	25	77	2166
91	I need help dating an old artifact - what exactly is carbon dating?	Carbon dating is a radiometric dating technique that uses naturally occurring radioactivity. It works by measuring carbon-14, which is present in all living organisms but begins to decay after death. This method can date biological tissues up to 50,000-60,000 years old, though it's most accurate for younger samples due to their higher carbon-14 content.	"['Half life dating formula\nThe term is commonly used in nuclear physics to describe how quickly unstable atoms undergo, or how long stable atoms survive, radioactive decay. The term is also used more generally to characterize any type of exponential or non-exponential decay. For example, the medical sciences refer to the biological half-life of drugs and other chemicals in the human body. The converse of half-life is doubling time. The original term, half-life period , dating to Ernest Rutherford \'s discovery of the principle in , was shortened to half-life in the early s.\nHalf Life Calculator\nUnstable nuclei decay. However, some nuclides decay faster than others. For example, radium and polonium, discovered by Marie and Pierre Curie, decay faster than uranium. That means they have shorter lifetimes, producing a greater rate of decay. Here we will explore half-life and activity, the quantitative terms for lifetime and rate of decay.\nWhy do we use the term like half-life rather than lifetime? The answer can be found by examining Figure The time in which half of the original number of nuclei decay is defined as the half-life , t 1 2 t 1 2. After one half-life passes, half of the remaining nuclei will decay in the next half-life. Then, half of that amount in turn decays in the following half-life.\nNuclear decay is an example of a purely statistical process. A more precise definition of half-life is that each nucleus has a 50 percent chance of surviving for a time equal to one half-life. If an individual nucleus survives through that time, it still has a 50 percent chance of surviving through another half-life. Even if it happens to survive hundreds of half-lives, it still has a 50 percent chance of surviving through one more. Therefore, the decay of a nucleus is like random coin flipping.\nThe chance of heads is 50 percent, no matter what has happened before. The probability concept aligns with the traditional definition of half-life. Provided the number of nuclei is reasonably large, half of the original nuclei should decay during one half-life period. The following equation gives the quantitative relationship between the original number of nuclei present at time zero N O N O and the number N N at a later time t. The decay constant can be found with the equation.\nWhat do we mean when we say a source is highly radioactive? Generally, it means the number of decays per unit time is very high. We define activity R to be the rate of decay expressed in decays per unit time. In equation form, this is. The SI unit for activity is one decay per second and it is given the name becquerel Bq in honor of the discoverer of radioactivity.\nThat is,. Activity R is often expressed in other units, such as decays per minute or decays per year. The definition of the curie is. Radioactive dating or radiometric dating is a clever use of naturally occurring radioactivity. Its most familiar application is carbon dating. Carbon is an isotope of carbon that is produced when solar neutrinos strike 14 N 14 N particles within the atmosphere. Radioactive carbon has the same chemistry as stable carbon, and so it mixes into the biosphere, where it is consumed and becomes part of every living organism.\nCarbon has an abundance of 1. Over time, carbon will naturally decay back to 14 N 14 N with a half-life of 5, years note that this is an example of beta decay. When an organism dies, carbon exchange with the environment ceases, and 14 C 14 C is not replenished. Carbon dating can be used for biological tissues as old as 50 or 60 thousand years, but is most accurate for younger samples, since the abundance of 14 C 14 C nuclei in them is greater.\nOne of the most famous cases of carbon dating involves the Shroud of Turin, a long piece of fabric purported to be the burial shroud of Jesus see Figure This relic was first displayed in Turin in and was denounced as a fraud at that time by a French bishop. Its remarkable negative imprint of an apparently crucified body resembles the then-accepted image of Jesus. As a result, the relic has been remained controversial throughout the centuries.\nCarbon dating was not performed on the shroud until , when the process had been refined to the point where only a small amount of material needed to be destroyed. Samples were tested at three independent laboratories, each being given four pieces of cloth, with only one unidentified piece from the shroud, to avoid prejudice. All three laboratories found samples of the shroud contain 92 percent of the 14 C 14 C found in living tissues, allowing the shroud to be dated see Figure Carbon has a half-life of If 1 kg of carbon sample exists at the beginning of an hour, b how much material will remain at the end of the hour and c what will be the decay activity at that time?\nThe decay constant is equivalent to the probability that a nucleus will decay each second. As a result, the half-life will need to be converted to seconds. Another way of considering the decay constant is that a given carbon nuclei has a 0. The decay of carbon allows it to be used in positron emission topography PET scans; however, its As a result, one would expect the amount of sample remaining to be approximately one eighth of the original amount. The Calculate the age of the Shroud of Turin given that the amount of 14 C 14 C found in it is 92 percent of that in living tissue.\nHere, we assume that the decrease in 14 C 14 C is solely due to nuclear decay. We enter that value into the previous equation to find t. Our calculation is only accurate to two digits, so that the year is rounded to That uncertainty is typical of carbon dating and is due to the small amount of 14 C in living tissues, the amount of material available, and experimental uncertainties reduced by having three independent measurements.\nThere are other noncarbon forms of radioactive dating. Rocks, for example, can sometimes be dated based on the decay of U U. The decay series for U U ends with P b P b , so the ratio of those nuclides in a rock can be used an indication of how long it has been since the rock solidified. Knowledge of the U U half-life has shown, for example, that the oldest rocks on Earth solidified about 3. Learn about different types of radiometric dating, such as carbon dating.\nUnderstand how decay and half-life work to enable radiometric dating to work. Play a game that tests your ability to match the percentage of the dating element that remains to the age of the object. Skip to main content. Sections Learning Objectives. Learning Objectives Learning Objectives By the end of this section, you will be able to do the following: Explain radioactive half-life and its role in radiometric dating Calculate radioactive half-life and solve problems associated with radiometric dating Section Key Terms activity becquerel carbon dating decay constant half-life radioactive dating.\nTips For Success A more precise definition of half-life is that each nucleus has a 50 percent chance of surviving for a time equal to one half-life. Figure In one half-life t 1 2 t 1 2 , the number decreases to half of its original value. Half of what remains decays in the next half-life, and half of that in the next, and so on. This is exponential decay, as seen in the graph of the number of nuclei present as a function of time.\nIn equation form, this is Radiometric Dating Radiometric Dating Radioactive dating or radiometric dating is a clever use of naturally occurring radioactivity. The shroud first surfaced in the 14th century and was only recently carbon dated. It has not been determined how the image was placed on the material. Butko, Wikimedia Commons.\nWorked Example Carbon Decay Carbon has a half-life of Print Share. Texas Gateway: Related Items. No resources. Texas Education Agency N. All Rights Reserved.\nAlthough we now recognize lots of problems with that calculation, the age of We next define the half-life, τ1/2, the time necessary for 1/2 of the. A radioactive half-life refers to the amount of time it takes for half of the original isotope to decay. For example, if the half-life of a gram sample is 3 years.\nPetrology Tulane University Prof. Stephen A. Nelson Radiometric Dating Prior to the best and most accepted age of the Earth was that proposed by Lord Kelvin based on the amount of time necessary for the Earth to cool to its present temperature from a completely liquid state. Although we now recognize lots of problems with that calculation, the age of 25 my was accepted by most physicists, but considered too short by most geologists.\nThe ease of using the formula for radioactive decay with common elements found in nature make it a powerful tool. Radiometric dating is the technique of using isotopic ratios of common elements to determine the age approximate of materials associated with the element, such as trees, rock strata, fossils, human artifacts and the like.\nYou can calculate half life if you know how much of the substance is left after a certain time, though typically it works the other way - the half life is known, and used to calculate age. Half life is defined as the time after which half of a sample of a radioactive material will have decayed.\nRadioactive Half-Life Formula\nA technician of the U. Geological Survey uses a mass spectrometer to determine the proportions of neodymium isotopes contained in a sample of igneous rock. Cloth wrappings from a mummified bull Samples taken from a pyramid in Dashur, Egypt. This date agrees with the age of the pyramid as estimated from historical records. Charcoal Sample, recovered from bed of ash near Crater Lake, Oregon, is from a tree burned in the violent eruption of Mount Mazama which created Crater Lake. This eruption blanketed several States with ash, providing geologists with an excellent time zone.\nRADIOMETRIC TIME SCALE\nUnstable nuclei decay. However, some nuclides decay faster than others. For example, radium and polonium, discovered by Marie and Pierre Curie, decay faster than uranium. That means they have shorter lifetimes, producing a greater rate of decay. Here we will explore half-life and activity, the quantitative terms for lifetime and rate of decay. Why do we use the term like half-life rather than lifetime? The answer can be found by examining Figure The time in which half of the original number of nuclei decay is defined as the half-life , t 1 2 t 1 2. After one half-life passes, half of the remaining nuclei will decay in the next half-life.\nCarbon is a radioactive isotope of carbon, containing 6 protons and 8 neutrons, that is present in the earth\'s atmosphere in extremely low concentrations.\nRadiometric dating is a means of determining the ""age"" of a mineral specimen by determining the relative amounts present of certain radioactive elements. By ""age"" we mean the elapsed time from when the mineral specimen was formed. Radioactive elements ""decay"" that is, change into other elements by ""half lives.\n5.7: Calculating Half-Life\nAll absolute isotopic ages are based on radioactive decay , a process whereby a specific atom or isotope is converted into another specific atom or isotope at a constant and known rate. Most elements exist in different atomic forms that are identical in their chemical properties but differ in the number of neutral particles—i. For a single element, these atoms are called isotopes. Because isotopes differ in mass , their relative abundance can be determined if the masses are separated in a mass spectrometer see below Use of mass spectrometers. Radioactive decay can be observed in the laboratory by either of two means: The particles given off during the decay process are part of a profound fundamental change in the nucleus. To compensate for the loss of mass and energy , the radioactive atom undergoes internal transformation and in most cases simply becomes an atom of a different chemical element. In terms of the numbers of atoms present, it is as if apples changed spontaneously into oranges at a fixed and known rate. In this analogy , the apples would represent radioactive, or parent, atoms, while the oranges would represent the atoms formed, the so-called daughters. Pursuing this analogy further, one would expect that a new basket of apples would have no oranges but that an older one would have many. In fact, one would expect that the ratio of oranges to apples would change in a very specific way over the time elapsed, since the process continues until all the apples are converted. In geochronology the situation is identical.\nHow do you calculate half life of carbon 14?\nArchaeologists use the exponential, radioactive decay of carbon 14 to estimate the death dates of organic material. The stable form of carbon is carbon 12 and the radioactive isotope carbon 14 decays over time into nitrogen 14 and other particles. Carbon is naturally in all living organisms and is replenished in the tissues by eating other organisms or by breathing air that contains carbon. At any particular time all living organisms have approximately the same ratio of carbon 12 to carbon 14 in their tissues. When an organism dies it ceases to replenish carbon in its tissues and the decay of carbon 14 to nitrogen 14 changes the ratio of carbon 12 to carbon Experts can compare the ratio of carbon 12 to carbon 14 in dead material to the ratio when the organism was alive to estimate the date of its death.\nHalf-life and carbon dating\nLove-hungry teenagers and archaeologists agree: But while the difficulties of single life may be intractable, the challenge of determining the age of prehistoric artifacts and fossils is greatly aided by measuring certain radioactive isotopes. Until this century, relative dating was the only technique for identifying the age of a truly ancient object. By examining the object\'s relation to layers of deposits in the area, and by comparing the object to others found at the site, archaeologists can estimate when the object arrived at the site. Though still heavily used, relative dating is now augmented by several modern dating techniques. Radiocarbon dating involves determining the age of an ancient fossil or specimen by measuring its carbon content.\nThe following tools can generate any one of the values from the other three in the half-life formula for a substance undergoing decay to decrease by half. Half-life is defined as the amount of time it takes a given quantity to decrease to half of its initial value. The term is most commonly used in relation to atoms undergoing radioactive decay, but can be used to describe other types of decay, whether exponential or not. One of the most well-known applications of half-life is carbon dating. The half-life of carbon is approximately 5, years, and it can be reliably used to measure dates up to around 50, years ago. The process of carbon dating was developed by William Libby, and is based on the fact that carbon is constantly being made in the atmosphere.\nPerhaps the most widely used evidence for the theory of evolution through natural selection is the fossil record. The fossil record may be incomplete and may never fully completed, but there are still many clues to evolution and how it happens within the fossil record. One way that helps scientists place fossils into the correct era on the geologic time scale is by using radiometric dating. Also called absolute dating, scientists use the decay of radioactive elements within the fossils or the rocks around the fossils to determine the age of the organism that was preserved. This technique relies on the property of half-life. Half-life is defined as the time it takes for one-half of a radioactive element to decay into a daughter isotope.']"	['<urn:uuid:e7259130-fdac-4314-9235-47af842225e8>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:18:19.158519	13	54	2688
92	what prevents suture needle twisting during surgery	The grooved face of each blade in the needle holder prevents the needle from twisting or swiveling while being driven through tissue.	['Also found in: Dictionary, Thesaurus, Legal, Encyclopedia, Wikipedia.\na surgical forceps used to hold and pass a suturing needle through tissue. Also called suture forceps.\nneedle holdersmall surgical forceps with locking handles; used to carry a suture needle\n1. a sharp instrument for suturing or puncturing.\n2. to puncture or separate with a needle.\nstainless steel needles with silver-plated handles 0.5 to 1 inch long, which are inserted into tissues at those points on the skin surface which are considered relevant to the problem being treated.\none with a handle used in ligating blood vessels.\na long, hollow needle for removing fluid from a cavity.\naspiration biopsy needle\na needle to which suction can be applied in order to withdraw a core of tissue from a solid organ.\nsurgical needles with suture material fused to the end, which is less traumatic to tissues then suture doubled back through the end of a needle. See swage (2).\na noncutting, blunt-pointed needle used in general surgery and for suturing liver and kidney. Needle pricks are less likely. This is an issue in modern surgery on humans.\na needle used as a cannula, as for introduction of an intravenous catheter or for passing a suture thread.\none used in removing a cataract.\na two-way needle or cannula which permits flushing and aspiration of liquid cataract material. See also discission.\nsee needle holder (below).\nthe sensation perceived by the operator when the insertion of an acupuncture needle reaches the acupuncture point.\nStipa spp. Called also spear grass.\na form of flat suture needle.\na strong scissor-type instrument used to hold a suture needle while pushing it through tissue. The handles are ratcheted and have to be squeezed to release the needle. The face of each blade is grooved so that the needle will not twist or swivel while being driven. The natural action is for a right-handed surgeon.\na hollow, sharp-pointed needle to be attached to a hypodermic syringe for injection of solutions.\na slender knife with a needle-like point, used in ophthalmic operations.\na long-handled, slender steel needle, having an eye in its curved end, used for passing a ligature underneath an artery.\npuncture of a mass, tissue or fluid accumulation in order to relieve pressure or to collect sample for field or laboratory examination.\nreverse cutting needle\na curved cutting needle with the cutting edge on the back of the curve rather than on the concave surface.\nused for aspiration of bone marrow.\nround bodied needle\na noncutting surgical needle used for suturing tissues that separate easily such as intestine, liver, lung and fascia. Called also taperpoints.\na flat, rather than round, special cutting needle for ophthalmic work.\nneedle stick injury\naccidental puncture of the skin by needles while in use or as a result of inappropriate disposal with the risk of introducing infectious agents.\none with a shoulder that prevents too deep penetration.\na special, heavy duty needle with a palm-fitting handle, for sewing with tape.\na suture needle with a flattened shaft, so that it is three times as wide as it is thick, and a point which has a gradually diminishing triangular cross-section, a cutting point. Modern design has a circular cross-section and a short cutting tip.']	['<urn:uuid:1af6fae1-57e9-49ed-84af-e2480651247a>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:18:19.158519	7	22	538
93	I keep hearing about something called PubPeer helping to find problems in research papers - what exactly is this and how does it help science?	PubPeer is an anonymous post-publication peer-review website where scientists can comment on published research papers and identify potential problems. It has become an important tool for correcting issues in scientific literature. Along with social media, PubPeer has accelerated the process of identifying and correcting problems in published research. This was demonstrated in the case of the brain tumor research paper, where PubPeer comments first identified potential image manipulation issues, ultimately leading to the paper's retraction. Scientists view this as a positive development, as it allows for continuous scrutiny of published research by the scientific community.	['Last September, a group of 27 researchers led by scientists at the Baylor College of Medicine in Texas published a paper in Nature reporting a new technique that would allow immune cells to cross the blood-brain barrier and home in on hard-to-reach brain tumors. After garnering more than 50 comments on the anonymous post-publication peer-review website PubPeer, the article was retracted today (February 20).\nIn the paper, oncologist Nabil Ahmed, Heba Samaha, a research associate at Children’s Cancer Hospital Egypt 57357 who worked at Baylor for several years, and colleagues revealed a potential solution for the difficult task of getting the immune cells used in immunotherapy to brain cancers. The researchers reported that by engineering T cells with a “homing system” to bind firmly to molecules on the surface blood vessels—and adding a chimeric antigen receptor (CAR) that could identify cancer cells—they were able to successfully treat glioblastoma, an aggressive form of brain cancer, in mice.\nAt first, these findings were met with positive attention. The study was discussed in an associated Nature News & Views piece, appeared as a research highlight in an associated journal, Nature Immunology, and received a press release, media coverage, and several citations. “The results were very encouraging,” Samaha said in the press release. “We observed that T cells with both the homing system and CAR substantially shrunk tumors in all treated animals.”\nBut starting last October, a few weeks after the paper was posted online, comments about potential image manipulation in the article began to appear on PubPeer, and the issue quickly caught the attention of scientists on social media.\nGaetan Burgio, a geneticist at Australian National University who posted a widely circulated Tweet about the manuscript, notes that the extent of alleged image duplication identified in this paper was “quite exceptional.”\n“If you look at the [PubPeer] comments, [there are] claims of duplications for pretty much every single figure . . . and raw data that did not match figures in the paper,” he tells The Scientist.\nBrian Ferguson, an immunologist at the University of Cambridge, says he was also stunned by the PubPeer posts, which, he adds, “showed suggestions of image manipulation to a degree that I hadn’t seen in any paper before.”\nAround a week after comments started appearing on PubPeer, Nature added an Editor’s Note to the study, alerting readers that the journal had opened an investigation into the concerns raised about the data presented in the paper.\nAccording to the retraction notice posted today, the authors are pulling the paper “due to issues with figure presentation and underlying data.” All the authors, except Samaha, the first author, agreed with the retraction.\n“Unfortunately, issues were identified in the presentation of several figure panels and the underlying data [in the paper],” Ahmed writes in an email to The Scientist. “I promptly notified the office of research at Baylor College of Medicine, which is looking into the cause of these issues.”\nSamaha did not respond to The Scientist’s requests for comment.\nAccording to Dana Benson, the director of communications at the Baylor College of Medicine, the institution’s Committee on Scientific Integrity (COSI) reviews all allegations of scientific misconduct. In an emailed statement to The Scientist, she added that these evaluations “take time and these proceedings are strictly confidential.”\nSome commenters on both PubPeer and social media suggested that the duplicated images should have been caught by peer reviewers. But Burgio believes that the responsibility lies with the publisher, not the reviewer. “I think it’s on the publisher to ensure that the paper doesn’t contain any image duplication or plagiarism,” he says. “It’s unfair to rely solely on the reviewer to police the paper.”\nNature currently conducts random spot checks of images in manuscripts prior to publication. “If concerns about a figure in a Nature paper are raised, we have software tools that enable us to evaluate images in detail,” a Nature spokesperson writes in an emailed statement to The Scientist. (The spokesperson also noted that the journal could not comment on individual articles for confidentiality reasons).\nSome journals, such as the Journal of Cell Biology and The EMBO Journal, have implemented procedures to screen figures in every article prior to publication. “This [practice] is absolutely admirable,” Ferguson says. “How to screen really carefully for image manipulation prior to publication is something that most journals will have to address.”\nFor now, Ferguson notes that social media and post-publication peer review websites such as PubPeer have accelerated that process of identifying and correcting issues in the scientific literature. “This is a good example where post-publication peer-review has had a clear impact,” he adds. “I believe that will continue to happen—because there are individuals looking for this stuff all the time.”']	['<urn:uuid:f689f072-244d-4dc7-9a63-06d8f044d295>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	25	95	786
94	How are mussels used as fishing bait, and what risks do they pose as invasive species in waterways?	Mussels are excellent fishing bait, particularly effective when used in cocktail combinations with crab, worms, or other shellfish. They're especially good for catching cod, coalfish, and flatfish, with their bright yellow-orange color standing out against dark backgrounds. They need to be properly prepared and tied to the hook, as they're soft bait that can be easily nibbled off. However, as invasive species, certain types of mussels like zebra and quagga mussels can be harmful when introduced to new waterways. These invasive mussels can attach to boat hulls and underwater infrastructure, causing millions of dollars in damage. They can survive several weeks out of water and form dense clumps on solid objects, making them particularly problematic when transferred between different bodies of water.	"['Common mussels are easily gathered from most rocky shorelines, sheltered harbour walls and muddy estuaries. They make excellent bait, so why don’t more anglers use them?\nMany would say that they take too long to prepare for the hook and it’s easier to buy worms wrapped in paper. True, but after a little practice it is possible to clean several in the time it takes to prepare a peeler crab, which, by the way, needs just as much thought when it comes to collection, storing and preparation. Yet mussels can outfish crab baits.\nThe biggest problem is finding mussels that are firm and fleshy, rather than a watery thin mess that offers little to put on the hook. Being filter feeders the fatter specimens are nearly always found lower down the tidal range closer to the bottom, preferably in a muddy estuary or harbour. These are submerged longer and therefore feed for greater periods in between tides on natural food particles that live in the silt and mud.\nAlong the North East coast it is cod, coalfish and various flatfish that are the main species taken on mussel baits. A big, and I mean big, cocktail of a crab and mussels is a killer for cod, especially from the rock edge marks, while smaller cocktail baits are effective for coalfish.\nBoth species will take mussel baits but it is more usual to use it as a cocktail along with a crab, worms or other shellfish such as razorfish. Many anglers claim it is the best bulking bait. Mussels make a good visual bait; a big ball of bright, yellow-orange mussels stands out against a dark background of rock and kelp. As the bait breaks down it releases small fish-attracting particles, as well as a scent trail, that drifts downtide.\nSmaller mussel baits on a size 1 or smaller hook can be deadly for dabs, especially when tipping off the bait with a tiny sliver of mackerel or squid. It will pick up the odd flounder and plaice as well. It is also a deadly bait for fishing down the side of a pier where it can lure big bags of coalfish, small codling, pouting and other species.\nNever underestimate how close fish move inshore and be aware that you don’t have to cast out to the horizon to catch them. In the past when coalfish were more plentiful, anglers used to groundbait before fishing a match by crushing up a big bag of mussels and possibly adding hardback crabs as well then hanging the bait bag over the pier side. If the match was not pegged they would throw shelled mussels into their swim the day before and make sure they got up early enough the next day to claim their spot.\nBeing a soft bait it needs to be tied on to the hook prior to casting. You can fish close in by wrapping it around the hook, although failure to tie it on can result in small fish quickly nibbling the bait off the hook.\n1. You need is a reasonably sharp knife with a rounded end, plus a bait container. Hold the mussel in one hand with the pointed end towards you and insert the knife into the shell about halfway down\n2. Push the knife through to the other side of the mussel, then away from your hand towards and around the rounded end of the shell and sever the tendons holding the shell together. Now open the shell by hand\n3. Scoop out meat and tougher fringes and sever remaining tendons. Once you have enough, tip onto newspaper to get rid of excess liquid. Keep in newspaper and put into a container to use, or freeze in small quantities\n4. Hooking your bait is quite simple: Push the hook point through the foot end of the mussel and then up the shank of the hook…\n5. Now wrap it around hook shank a few times. If you want a big bait, push it up past the hook eye onto the line, then add more mussels\n6. Secure with fine elastic cotton. Don’t tie it on too tightly because the cotton will cut through the soft tissue and the mussels won’t stay on the hook\nWrap the empty shells in several carrier bags before putting them in the dustbin, otherwise after hot weather they will smell. If going bait digging or fishing the same or next day take them with you and tip them in a rock pool where small blennies and crabs pick them clean when the tide floods.', '- What are aquatic invasive species?\n- Help stop aquatic invasive species\n- Report aquatic invasive species\nWhat are aquatic invasive species?\nAquatic invasive species can harm native species by bringing disease and altering natural ecosystems. This can have a negative effect on fisheries, tourism, health and recreation. The Yukon Invasive Species Council defines an invasive species as:\n""An organism (plant, fungus, or bacterium) that is introduced into an ecosystem and has negative effects on the economy, environment, and/or health. The term “invasive” is reserved for the most aggressive species, which reproduce rapidly and cause major changes to the areas where they become established.""\nHow are aquatic invasive species spread?\nPeople spread aquatic invasive species when they:\n- dump their aquariums illegally; and\n- move from place to place. Aquatic organisms can cling to boats, fishing gear and footwear and unknowingly be transported from one area to another.\nAre there aquatic invasive species in Yukon?\n- Most Yukon lakes and waterways appear to be free of invasive species.\n- Some species of fish have been introduced to Yukon waters but are not considered invasive. These include rainbow trout, stickleback, Arctic char and goldfish.\nWhat aquatic invasive species should I be aware of?\nZebra and quagga mussels\nZebra mussels and quagga mussels attach to boat hulls and underwater infrastructure and can cost millions of dollars in damage. Adult mussels can live several weeks out of the water and be transferred from one body of water to another.\nIdentify zebra and quagga mussels\nZebra and quagga mussels:\n- are small and only up to 3 cm;\n- are shaped like propeller blades;\n- have yellow or brownish shells;\n- can have light-coloured zebra stripes; and\n- form dense clumps and firmly attach themselves to solid objects.\nDidymo algae is found in Yukon. It is considered an invasive species in other parts of Canada, but may be native to Yukon. Further research will provide us with an answer.\nThis algae can form large, slippery mats that cover stream beds. These mats can damage organisms in Yukon waters and degrade fisheries by changing the natural ecosystem and disrupting fish rearing habitat. As stream levels drop, didymo mats resemble and are mistaken for toilet paper.\nThis YouTube video produced by New Zealand Fish and Game explains how didymo can form large algal blooms.\nIdentify didymo algae\n- is brownish-yellow to white and form in clumps or ropes. It isn\'t didymo if it is green or dark brown or black or transparent;\n- has a rough texture like wet cotton wool when pulled apart. It isn\'t didymo if it has a slimy texture or falls apart when handled; and\n- is found in clumps on rocks or plants, or floating in the current. It isn\'t didymo if it is attached to the bottom with roots.\nFind out more about aquatic invasive species in Yukon\n- Yukon Fishing Regulations summary\n- Yukon Invasive Species Council\n- Protect Your Waters and Stop Aquatic Hitchhikers\nReports on Yukon aquatic invasive species\nHelp stop aquatic invasive species\nOne of the most important things you can do to stop aquatic invasive species is to not move water, fish, plants or aquatic life from one water body to another. If you boat, fish or use Yukon waters you should follow the Clean, Drain, Dry guidelines to stop aquatic invasive species.\nNote: It’s illegal to bring live water creatures into Yukon waters or to transfer them among bodies of water without a permit. This includes live fish, uncured fish eggs, crayfish, leeches and water plants.\nClean, Drain, Dry guidelines\nClean mud, weeds and aquatic life from equipment before leaving the area\nMud and weeds can keep invasive species cool and moist, allowing them survive a long time.\nDrain water from bilges, pumps, coolers and buckets before leaving the area\nDraining water makes it harder for aquatic invaders to survive and be transferred to new waters.\nDry boat and gear\nDry your boat and equipment before heading to another body of water. It can take 5 days in the sun to dry. If your equipment won\'t dry before you head onto another body of water, you should:\n- power wash your boat and trailer;\n- freeze small items like footwear, waders and reels in a bag overnight until solid; or\n- soak gear in very hot water for 30 minutes.\nChoose your gear wisely\n- Studded or soft rubber waders are the best choice. Felt and other porous boot soles can trap moisture and spread aquatic invaders.\n- Didymo algae, also known as ""rock snot"" can live several weeks when attached to felt soles.\n- If you have felt-soled waders, it\'s best to freeze them solid overnight.\nReport an aquatic invasive species\nHelp stop the spread of aquatic invasive species by reporting them when you spot them. If you think you\'ve spotted an aquatic invader email email@example.com or phone 1-800-661-0408, ext. 5721.\nSee the next section for more details.\nReport aquatic invasive species\nIf you think you\'ve spotted an aquatic invader:\n- Report the incident to the Fisheries Unit by calling 1-800-661-0408, ext. 5721 or emailing firstname.lastname@example.org.\n- If possible, take a photograph and a small sample for identification in a secure plastic bag, and note the exact location.\n- Prevent its spread with Clean, Drain, Dry.\n- If you find a potential invasive species on equipment after leaving a site, throw it in the garbage and not down the drain.\nIf you have further questions about aquatic invasive species in Yukon, email email@example.com or phone: 867-667-5721, toll free in Yukon: 1-800-661-0408 ext. 5721.']"	['<urn:uuid:cae389ea-2066-4c3b-90cc-9d8aafd0f721>', '<urn:uuid:15a8dd31-31f3-4f8d-bdd8-a1005d92f0b6>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:18:19.158519	18	122	1694
95	infrastructure requirements soundloc vs sirfusion compare	SoundLoc requires no special infrastructure and works using just built-in speakers and microphones on ordinary laptops, while the SiRFusion platform similarly requires no additional infrastructure installations or complex surveys of buildings. Both systems achieve their positioning capabilities using existing hardware in the devices.	"['STMicroelectronics (NYSE: STM), a global semiconductor leader serving customers across the spectrum of electronics applications and the leading supplier of MEMS (Micro-Electro-Mechanical Systems) for consumer and portable applications, will demonstrate what the company says is the world’s most accurate indoor positioning of a mobile phone at the World Mobile Congress, Barcelona.\nComprising a smartphone combining the leading-edge SiRFusionTM location technology from CSR plc and an optimized MEMS (Micro-Electro-Mechanical-Systems) inertial measurement unit from ST, the demonstration shows that location can be reliably determined to within a few metres even in the absence of GNSS (Global Navigation Satellite Systems) satellite signals.\nAccurate indoor location will be the key enabler for future Location-Based Services (LBS), which are widely expected to be the next “killer application” in the mobile world. However, a challenge has been the problem of locating the mobile device with sufficient accuracy and reliability in urban canyons and indoor environments where the signals from the GNSS satellites cannot be received. In these cases, Wi-Fi and stored databases have been used so far to calculate the approximate location but the resulting accuracy has not been sufficient for many of the LBS services that are envisaged. For example, in the future, visitors in a museum will have the option to download detailed information about an exhibit simply by pointing their mobile towards it but for this to be possible the location of the device must be known to within a few meters, an accuracy not achievable using Wi-Fi alone.\nAt Mobile World Congress, ST will demonstrate the feasibility of achieving the required order of accuracy by combining the SiRFusion location architecture recently introduced by CSR plc with a sophisticated MEMS inertial module that offers ten degrees of freedom. With the high accuracy of the module, the mobile can be reliably located to within less than a few meters, an order of magnitude better than can be achieved using Wi-Fi alone. The SiRFusion location platform requires no additional infrastructure installations or complex surveys of buildings.\n“This solution demonstrates that reliable pedestrian dead-reckoning achieved by combining ST’s industry-leading motion sensors with our SiRFusion location platform is the final piece of the indoor navigation jigsaw,” said Kanwar Chadha, chief marketing officer for CSR and founder of SiRF. “This will allow a new wave of location-based services that users will be able to enjoy both indoors and outdoors.”\nThe sensor used in the demonstration is a multi-sensor Inertial Measurement Unit combining 3-axis sensing of linear, angular and magnetic motion with a pressure reading used to determine the elevation, which enables 3-dimensional location in multi-story buildings. Utilizing motion sensors, Wi-Fi measurements and signals from Global Navigation Satellite Systems (GNSS) satellites, CSR’s SiRFusion™ solution fuses data from the sensor unit with other data to dramatically improve the accuracy and coverage of the position calculation.\n“Our motion sensors enabled the big revolution that allowed people to interact more easily or intuitively with their cell phone. Now we are at the dawn of a new age where optical image stabilization and location-based services will drive the adoption of new high-performance motion sensors. And ST is ideally positioned to drive these two emerging applications,” said Benedetto Vigna.', 'The global positioning system, or GPS, has its limitations—namely, it cannot work indoors. Potential solutions for indoor positioning continue to fire up the imaginations of scientists. The latest news involves a form of echolocation. MIT Technology Review reported on the approach for indoor localization based on sound. Ruoxi Jia and team at the University of California, Berkeley developed a simple, cheap mechanism, said Technology Review, that can identify rooms based on a relatively small dataset. Their paper describing their system was submitted on July 16 to the arXiv server; authors are Ruoxi Jia, Ming Jin, and Costas J. Spanos of the University of California, Berkeley. They call their system SoundLoc. In their paper, ""SoundLoc: Acoustic Method for Indoor Localization without Infrastructure,"" they described SoundLoc as ""a room-level localization system that exploits the intrinsic acoustic properties of individual rooms."" Their SoundLoc method is based on the extraction of acoustic features of rooms. The team said they can acquire RIRs [room impulse responses]by using built-in speakers and microphones on laptops.\nAlso, a ""noise adaptive reverberation extraction algorithm"" was developed for feature extraction from the noisy RIRs. How it works: ""Emit a sound and then listen for the return which will be distorted in a way that depends on the size and shape of the room, the materials on the walls and floors as well as the furniture and people within it,"" said Technology Review.\nThe researchers tested their system in ten rooms on the Berkeley campus. Data was taken using the built-in microphone and speakers on an ordinary laptop. ""The laptop produces a set of sound waves and then listens for the echo. They took 50 samples at each location, which included background noise such as footsteps, talking and heating and ventilation sounds. They processed this data to find the echo fingerprint for each room,"" said Technology Review. -\nThe team said there was a 97.8 percent accuracy in identifying the individual rooms. They wrote, ""The acoustic features we extracted are shown to be distinctive, robust and efficient to compute. 97.8% of overall accuracy has been achieved for 10 rooms\' identification.""\nOf what use is their research? MIT Technology Review commented how ""Jia and co are particularly interested in using the technique to reduce the energy consumption in buildings. Some 40% of energy usage in the US comes from commercial and residential buildings. If those buildings are empty, then that represents a significant waste.""\nThe authors said in their paper that ""Identifying locations of occupants is beneficial to energy management in buildings. A key observation in indoor environment is that distinct functional areas are typically controlled by separate HVAC and lighting systems and room level localization is sufficient to provide a powerful tool for energy usage reduction by occupancy-based actuation of the building facilities.""\nExplore further: One-way transmission system for sound waves\nMore information: SoundLoc: Acoustic Method for Indoor Localization without Infrastructure, arXiv:1407.4409 [cs.HC] arxiv.org/abs/1407.4409\nIdentifying locations of occupants is beneficial to energy management in buildings. A key observation in indoor environment is that distinct functional areas are typically controlled by separate HVAC and lighting systems and room level localization is sufficient to provide a powerful tool for energy usage reduction by occupancy-based actuation of the building facilities. Based upon this observation, this paper focuses on identifying the room where a person or a mobile device is physically present. Existing room localization methods, however, require special infrastructure to annotate rooms.\nSoundLoc is a room-level localization system that exploits the intrinsic acoustic properties of individual rooms and obviates the needs for infrastructures. As we show in the study, rooms\' acoustic properties can be characterized by Room Impulse Response (RIR). Nevertheless, obtaining precise RIRs is a time-consuming and expensive process. The main contributions of our work are the following. First, a cost-effective RIR measurement system is implemented and the Noise Adaptive Extraction of Reverberation (NAER) algorithm is developed to estimate room acoustic parameters in noisy conditions. Second, a comprehensive physical and statistical analysis of features extracted from RIRs is performed. Also, SoundLoc is evaluated using the dataset consisting of ten (10) different rooms. The overall accuracy of 97.8% achieved demonstrates the potential to be integrated into automatic mapping of building space.']"	['<urn:uuid:8250c792-be2f-4a93-8573-72866905e1d9>', '<urn:uuid:faeec573-e009-453d-8c0f-285d3ca14a18>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T05:18:19.158519	6	43	1222
96	How do lab-engineered crops compare with traditional breeding techniques?	Lab-engineered crops use biotechnology techniques like Agrobacterium tumefaciense transformation with specific vectors to directly alter genetic material, while traditional breeding relies on open-pollination and careful selection of desired traits over generations. The tissue-culture based transformation achieves 35% regeneration frequency under selection, while traditional heirloom varieties maintain natural genetic diversity through seed saving and generational passing.	"['Transgenic tomato (Solanum lycopersicum Mill.) regeneration by comparing different transformation techniques\nMetadataShow full item record\nProduction of saline-tolerant high-yielding crop has become most important in the current global scenario. Biotechnological approaches are extensively employed for introducing foreign genes into high-yielding genotypes. However, the success of genetic manipulation of a plant depends on the transformation as well as regeneration frequencies of the explants. This study presents a comparison of tissue-culture based and non-tissue-culture based transformation protocol for tomato (Solanum lycopersicum Mill.) using the Agrobacterium-mediated transformation of two different explants by three different vectors. Whole seed and cotyledonary leaves were used as explants of tomato varieties, named, BARI tomato-2, BARI tomato-3, BINA tomato-2, BINA tomato-3 and Bahar. Efficient and reproducible regeneration protocol was evaluated. Regeneration media with 2 mg/l BAP supplementation was found best for maximum number of shoot formation for all five varieties tested. For transformation, Agrobacterium tumefaciense strain LBA4404 harboring recombinant vectors, pBI121, pK7WG2_AtNHX1_1.6 and pK7WG2_OsNHX1_1.6 having kanamycin selectable marker gene were used. The parameters optimized here include optical density, infection time, co-cultivation period and pre-culturing of explants for all the varieties. In tissue-culture based transformation of cotyledonary leaves, transformation with bacterial solution (OD600 0.6-0.8) with infection time 30 min followed by 24 hrs of co-cultivation showed higher regeneration response under selection for all the varieties except BINA tomato-3 which gave its best result at OD600 1.0-1.2 with 48 hrs of co-cultivation period. Regeneration frequency of putative transformants appeared more than 35% under selection though the non-transformed plants gave more than 80% without selection (control experiment). This elaborate culture procedure resulted in low transformation frequency leading to necrosis of the regenerants, which urged to switch to the non-tissue-culture based transformation. During in planta transformation of whole seeds, OD600 1.1-1.4 with 30 minutes of incubation period and 24 hours of co-cultivation period was optimum for achieving more than 90% transformation efficiency in transient GUS expression for all the varieties. After optimization of in planta protocol by pBI121, the other vectors were employed for transformation. The vector pK7WG2_AtNHX1_1.6 has achieved maximum percentage of regeneration frequency over the other two vectors throughout the parameter optimization. Wounding of seeds resulted in reduction of germination percentage of infected seeds for all the tested varieties. The pre-culture period of 24 hours prior to infection were considered effective for regeneration of both explants. The process of transformant detection was carried out in three distinct ways, such as, antibiotic selection, GUS assay and leaf disc senescence assay. Kanamycin at a concentration of 50 mg/l found to be efficient for this purpose. In addition to bioassay, micropropagation of putative transformed leaves observed to regenerate during antibiotic selection confirms stable transformation. The percentage of transformation efficiency diverges with tomato genotype, explant, infecting vector and method of transformation. The in planta protocol ensured generation of putative transgenic plants with considerable ease in a short time and is not genotype-dependent. The putative transformed plants obtained from in planta transformation observed to tolerate upto 100 mM of salt. Successful transformation of tomato will be greatly aided by genotype-specific determination of crucial parameters on improving in vitro regeneration after transformation, followed by growth into a whole plant transformant through acclimatization to net house conditions. Further, the study offers independence to select suitable tomato variety, explant, vector and method of transformation to generate transgenic salt tolerant tomato.', ""Uncovering the Truth: Are Heirloom Seeds Genetically Modified and What Does That Mean for Our Food Supply?\nHeirloom seeds are an important part of our food system. These are the types of seeds that have been passed down for generations and are used for planting crops. With traditional cultivation methods, farmers and gardeners can select the plants with the desired traits and use those particular seeds to grow more plants with similar characteristics. Heirloom seeds represent a part of our cultural identity while preserving access to certain plant varieties.\nBut this isn’t the only type of seed available on the market today. Recent advancements in genetic engineering have made it possible to create genetically modified (GM) seeds, which have been altered by scientists to have specific traits, such as disease resistance or improved nutritional benefits. With GM technology becoming more advanced and available, many people are naturally asking if heirloom seeds are genetically modified as well.\nWhat are Heirloom Seeds?\nHeirloom seeds are open-pollinated varieties that have been raised for generations by gardeners and farmers. They are the foundation of biodiversity and food security, and are not genetically modified. These varieties allow for the preservation of important traits such as flavour, nutrition and adaptability to various environments.\nIn this article, we'll explore what heirloom seeds are and why they're important.\nHeirloom seeds are defined as open-pollinated seeds that are at least 50 years old. Heirloom plants have been passed down from generation to generation, some with origins of centuries ago. Unlike hybrid or modified varieties, heirloom seeds have not been bred for certain traits or crossbred with any other type of seed. They are the most naturally diverse types of seed, and unlike genetically modified organisms (GMOs), they have not had their genetic makeup altered or tampered with in any way.\nHeirloom plants have the genetic diversity to acclimate to the natural conditions of their environment. As long as each season’s crops are planted in areas far enough away from larger-scale conventional farms and fields, this seed biodiversity can be maintained, preserving heirloom species for generations to come.\nFor more information on heirlooms and how they compare to hybrids and GMOs, please refer to our article where we discuss further details on this topic.\nHeirloom seeds are open-pollinated varieties that have been passed down from generation to generation for many years. Unlike modern hybrids, these varieties are not genetically modified and have remained unchanged through the years. Heirloom seeds are characterized by their excellent taste and aroma, as well as their adaptability to different climates. They offer a far greater diversity of flavors than commercially propagated plants, and a much wider range of colors, shapes and sizes.\nFor thousands of years, people relied on heirloom plants for their survival. Unaltered heirloom plant varieties were carefully selected by growers depending upon their desired traits – such as hardiness or medicinal effect – or simply based on the delicious flavor of the fruits they might produce.\nPreserving these heirloom seed varieties is an important way to ensure food security. With climate change having an increasingly major impact on what grows where and when, heirloom seed lines can help us better equip ourselves to handle changes in weather patterns while being better stewards of our land over time. By learning from the past, we may be able to more easily adapt to changing situations in the future:\n- Preserving heirloom seed varieties.\n- Selection of heirloom plants based on desired traits.\n- Adapting to changing weather patterns.\n- Being better stewards of our land over time.\nGenetically Modified Organisms (GMOs)\nGenetically Modified Organisms (GMOs) are living organisms that have had their genetic material altered using biotechnology. This includes organisms created through genetic engineering, which is a method of introducing specific traits or traits from other organisms by directly altering the genetic code of an organism. GMOs are commonly used in agriculture to increase yields, reduce disease, and to modify plant characteristics.\nIn this article, we will look at GMOs in relation to heirloom seeds.\nGenetically Modified Organisms (GMOs) are living organisms whose genetic material was artificially manipulated in a laboratory through genetic engineering. This process most often involves inserting genes into the organism’s DNA to introduce new traits or increase productivity.\nWhile GMOs have been used to farm crops since 1994, controversy still remains over their potential risks to the environment and human health. As such, consumers have become increasingly interested in avoiding GMOs through buying labeled non-GMO products, or growing and harvesting heirloom seeds.\nThe term heirloom seed is used to refer to varieties that have not been altered by modern hybridization. They are generally open-pollinated and can be passed down for generation for generation as each harvest will yield identical or similar plants as their parents. Therefore, it is important to note that heirloom seeds are not themselves genetically modified; however, certain organizations may perform additional tests on the nutritional content of these seeds prior to sale in order verify their origin and identity.\nPros and Cons\nGenetically modified organisms (GMOs) are organisms whose genetic material has been altered using genetic engineering. This technology has been used in the production of some plant and animal species – primarily those used in agriculture. With their increased yields, improved nutritional value, pest resistance, and many other advantages, GMOs have become an increasingly popular choice among farmers and consumers alike. However, GMOs also present potential risks to both the environment and human health which have caused some controversy over their use. Below is a discussion of the various pros and cons associated with genetically modified organisms:\n- GMOs have the potential to produce higher yields than traditional crops. For example, genetically modified maize plants can produce significantly larger yields due to increased pest resistance or enhanced efficiency of photosynthesis;\n- GMOs are less susceptible to extreme weather conditions, meaning that farmers can benefit from better crop security;\n- GMOs can be tailored to have improved nutrition by introducing genes from different organisms into a single crop;\n- They may also be able to resist drought allowing for easier establishment of crops in arid regions;\n- Herbicides used on GM crops are typically more environmentally friendly since they target specific weeds instead of destroying all vegetation around it;\n- In some cases GM crops require fewer applications of insecticides or herbicides meaning less damage to surrounding wildlife;\n- GMOs can help reduce food insecurity due to shorter maturation times which equate with more efficient production cycles.\n- Like with any technology there is a potential for unintentional harm to the environment or human health if not adequately regulated;\n- There are concerns that GMO products could contaminate nonGM crops leading to damage of biodiversity in agricultural ecosystems;\n- Some argue that GMO products should be labeled so that consumers know what they’re eating;\n- The gene exchange between GMOs and wild species may cause harm to both as well as lead to ecological imbalance due to significant population changes in comparison with pre existing populations.\nAre Heirloom Seeds Genetically Modified?\nHeirloom seeds have been around for centuries and are considered to be some of the oldest types of seeds available. Heirloom seeds are passed down from generation to generation and are open pollinated, meaning the seeds will produce the same variety of plants as their parent plants.\nBut the question remains: Are Heirloom Seeds Genetically Modified (GMO)? To answer this question, let's take a look at the pros and cons of Heirloom Seeds:\nDifferences Between Heirloom and GMO Seeds\nHeirloom seeds, also called heritage seeds, are a type of open-pollinated seed. They are created through the natural process of pollination by insects, birds, wind, or other natural methods. Heirloom seed varieties are open to pollination and genetic variation, like other types of traditional open-pollinated seed.\nGenetically modified organisms (GMOs) on the other hand, are created in a laboratory setting using advanced biotechnology techniques. The technique involves taking genetic material from one organism and transferring it into another organism in order to impart desirable traits such as increased yield or resistance to certain pests or herbicides.\nWhen it comes to heirloom versus GMO seeds there are several key differences between them:\n- Heirloom seeds come from plants that have been naturally pollinated over hundreds of years resulting in numerous varieties of vegetables and fruits; while GMOs originate from plants with experimental genes that have been artificially inserted and modified using sophisticated biotechnology techniques.\n- Heirloom seeds are open to pollination resulting in genetic variability; while genetics of GMO crops remain consistent due to their uniformity produced through experimentation.\n- Heirloom varieties retain their original genetics after growing them out for many generations; while some GMO crops may not contain the expected levels of gene modification over multiple crop cycles even if traditional breeding has not taken place during this time.\n- Heirlooms offer unique flavors and genetic characteristics; while GMOs offer convenience and uniformity instead where taste is often compromised for production yields and/or resistance to specific insect pests or weeds/herbicides treatments.\nAre Heirloom Seeds Safe?\nHeirloom seeds are safe to use and consume because they have not been compromised by any form of genetic engineering. Heirloom plants are traditional varieties of crops that are at least fifty years old, but some heirlooms may be centuries old. This means that no human interference has taken place in the seed's makeup or growing requirements, as opposed to modern horticultural techniques such as hybridization or genetic engineering (GE).\nUnlike GE seeds, heirloom varieties do not contain altered DNA from other organisms and are not genetically modified (GMO) in anyway. This means that heirlooms have a level of trustworthiness associated with them – it is easier to trust their safety and reliability because people can be assured that these plants haven’t been altered in a lab. Plus, the “living history” contained within these plants adds a certain value that other plant varieties simply cannot replicate.\nOverall, heirloom seeds are a safe option for anyone looking to cultivate plants without worrying about potential human health impacts caused by using GE seed varieties or consuming GM products. As long as you source your heirloom seeds from legitimate vendors, you can enjoy their unique flavor profiles and beautiful ornamental properties without compromising your health or peace of mind.\nIn conclusion, heirloom seeds are not genetically modified. They are open-pollinated, meaning that the plant produces its own seeds, passed on through generations of farmers and gardeners from one season to the next. While these seeds might contain some genetic variation inherited over time from naturally-occurring mutagens like radiation or ultraviolet light, it is more likely that the variation was introduced deliberately by humans through breeding practices such as cross-pollination.\nThe bottom line is that heirloom seeds are not genetically modified, so you can safely purchase them for your own garden or to save for future generations.\nWhat are heirloom seeds?\nHeirloom seeds are open-pollinated seeds that have been passed down from generation to generation, often for hundreds of years. Heirloom seeds are prized for their flavor, nutrition, and history, as well as their ability to produce plants with the same characteristics as the original. Heirloom seeds are not hybrids, which are created by crossing two different varieties of plants.\nWhat is genetic modification?\nGenetic modification, also known as genetic engineering, is a process by which the genetic material of an organism is altered in a way that does not occur naturally. This process involves the manipulation of the organism’s DNA, which can be done in a number of ways. The most common methods of genetic modification involve the insertion of a gene from one organism into the genome of another. This gene may be from a different species, or even from a different organism within the same species.\nWhat are the benefits of heirloom seeds?\nHeirloom seeds offer a number of benefits to gardeners and farmers alike. Heirloom seeds are open-pollinated varieties of plants that have been passed down through generations and have been grown for many years. Heirloom seeds are prized for their unique characteristics, such as taste, color, size, and shape. One of the primary benefits of heirloom seeds is that they are more resilient and adaptable to environmental changes than hybrid or genetically modified seeds.\nWhat are the disadvantages of heirloom seeds?\nHeirloom seeds have been around for centuries, but they have their drawbacks. Here are some of the disadvantages of heirloom seeds: 1. Heirloom seeds are not always easy to find. Heirloom seeds are not widely available in stores, so you may have to search for them online or at specialty seed stores. 2. Heirloom seeds may not be as productive as hybrid or GMO seeds.""]"	['<urn:uuid:db64218a-f55c-42ff-a32a-939124317c1e>', '<urn:uuid:e1dc0cd0-7792-458b-a816-73bba2652b97>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:18:19.158519	9	55	2669
97	I'm interested in taking up sewing as a hobby - what are the basic techniques I need to know for both clothing construction and simple craft projects like making a pet accessory?	For basic clothing construction, one of the fundamental techniques is weaving, which creates fabric by interlacing two sets of threads - the warp (running lengthwise) and the weft (running horizontally). The three basic types of weaves are plain, twill, and satin. For craft projects like making a pet accessory such as a chef hat, you'll need to know how to measure and cut pattern pieces, gather fabric using a large stitch setting, create seams, and attach different pieces together. You can either use a sewing machine or hand stitch these projects. Basic supplies needed include fabric (medium weight cotton works well), sewing pins, needles, and optionally, interfacing for lightweight fabrics. These techniques can be used for both machine-washable items and decorative pieces.	"[""Introduction: How to Make a Chef Hat Toque Pattern for Large or Small Pets\nLast year my son mentioned he would love to have a chef hat or toque. I did not have a pattern and when the pet contest posted, I thought it was a good opportunity to experiment with a self made pattern for pets. I figured there was little waste and time if it did not turn out. This instructable is the results of my endeavor. Now that I know my pattern worked I will make one for my son. Follow through and lets make a chef hat for a pet.\nStep 1: Supplies\nThis is what you will need:\nFabric scraps of your choice preferably cottonmedium weight works great\nInterfacing for light weight fabric ( light weight fusible)\nMake a pattern I will show you how\nSewing pins and sewing needle\nHand stitch or sew with a sewing machine\nFabric paint optional\nStep 2: Pattern\nI looked at chef hats for pets to see what style I liked. I found the hat I liked and tried to make one like it. Keep in mind the wider the hat band the taller the hat will be. Also the larger the hat circle the fuller the top of the hat will be. You will notice from the pictures I made 2 small hats. The polka dot hat had a smaller band and the top of the hat was not as full. I did not like that style as much as I did the plain white hat. You will notice the large chef hat is made slightly different than the smaller ones. Scooby does not have much of a crown for the hat so we modified the pattern by sloping the hat band that connects to the top of the hat so the fullness of the top of the hat could be shaped to come forward. This corrected the problem.\nI used lids to determine the size of hat to make but you can use a tape measure if you like. I placed the lid on top of the small pets head between the ears to determine how large to make the gathered section of the hat. For the large dog I placed the lid on his head and estimated where I wanted the band to be. For large or small pets I tripled the circle and cut one circle out of a piece of sheet or old fabric to see how it fit after I gathered it. I enlarged or reduced the pattern from there.\nCut two pattern pieces from the circle.\nDon't cut the band until we make the gathered hat piece.\nPress if needed.\nStep 3: Press and Sew Round Pattern Piece\nNow would be the time to iron the interfacing onto the round fabric piece on the wrong side of one piece. If you are using light weight fabric.\nPlace the two circle fabric pieces with wrong sides together.\nUsing the largest stitch setting; stitch around the circle placing the presser foot at the edge of the circle.\nStitch again placing the presser foot next to the seam you just stitched.\nPick up the top 2 threads as shown and carefully pull the threads to gather the hat top.\nGather the hat until it is about 1/4 inch larger than the desired size of hat using the lid to measure.\nStep 4: Cut Press Pin Hat Pieces\nMethod for the hat band pattern:\nMeasure around the gathered hat top.\nAdd 1inch for the length of the rectangle. Mark this on some paper.\nDetermine how wide you want the band and add 1/4 inch for the seam allowance. I made mine 3 1/2 inches for a small animal ( but decided I liked it slightly wider because it made the hat taller) and 5 1/2 inches for a large animal. The small animal was the cat and a tea cup puppy ?\nFold the band with wrong sides together lengthwise and press.\nPin the band to the round hat piece allowing 1/2 inch tail as shown.\nContinue around; pinning the band to the hat piece. You should end up with about 1/2 inch tail.\nStitch the band together.\nStep 5: Sew the Band to the Hat Top\nI accidently stitched the band to the hat top before I stitched the band together as you can see from the previous picture.\nStitch the band to the hat top as shown.\nFold the band in half and turn under a small hem and then re-pin the band to the hat top (over the previous stitching as shown. It is OK if one of the previous stitching still shows because the other one will secure the stitch when sewn next. You just pull out the second stitch line that shows. This will make a nice finished hat.\nStep 6: Decorate or Not\nYou can decorate by using lapels, paint names or words, or make a fish or hot dog. I will share how I made a hot dog. I thought it would be cute to paint or embroidery human finger prints on a pet hat.\nWhat you will need for a hot dog:\nBrown fabric scrap\nYellow fabric scrap\nNeedle and thread\nStep 7: Strap to Secure the Chef Hat\nFor the strap I used elastic so it would be easy to put on the animal with little fuss. The next time I make one I will use a wider elastic because I think it would fit the head a little nicer.\nStep 8: Hot Dog Pattern\nLay the chef hat over the brown fabric.\nThe length of the hot dog should extend slightly over the length of the chef hat adding 1/2 inch for seams.\nCut the yellow cheese after the hot dog is made.\nStep 9: Sewing the Hot Dog\nFold the brown fabric 1/4 inch down on each end as shown.\nWith right sides together fold lengthwise in half and stitch as shown.\nTurn right side out.\nStitch one end by gathering as shown.\nFill with fiber fill using a straw to pack it inside.\nGather remaining end as shown.\nIt should look like last picture.\nStep 10: Cheese\nCut the cheese a little larger than you think you need.\nPlace the hot dog over the cheese and fold it as shown to see if that is the size you want.\nTrim it down as needed.\nFold it over the hot dog and stitch it to the hat top.\nStep 11: Flaunt Your Chef Hat and Cook Up Some Snacks\nStep 12: Final Thoughts\nIt does not take very long to make one of these. Hand stitching them does not take that long if you do not own a sewing machine. You can machine wash them on gentle cycle and line dry.\nThis is an entry for the pet contest, if you would like to vote for it please come back when the vote button is in the top right corner. Thanks so much for your support. Have a happy spring!\nI wish to thank our sponsors' for their contributions.\nkristiannemari made it!\nWe have a be nice policy.\nPlease be positive and constructive."", 'The vertical threads are called the warp and the horizontal threads are called the weft. Weaving is done on a machine called a loom. On a loom, the warp beam holds the warp threads on which the textile will be woven.\nWhat are terms used in weaving?\nSelvage: The woven edge of a fabric. Selvage Loops: The extensions of the weft beyond the selvage from the weft tension being too loose. Shed: The opening created on a loom where the weft passes. Shot (pick): A single pass of weft through the shed.\nWhat are the two types of weaving?\nThree types of weaves: plain, twill, and satin. Encyclopædia Britannica, Inc. The manner in which the yarns are interlaced determines the type of weave. The yarn count and number of warp and filling yarns to the square inch determine the closeness or looseness of a weave.\nWhat are the 2 directions of weaving called?\nWarp and weft are the two basic components used in weaving to turn thread or yarn into fabric. The lengthwise or longitudinal warp yarns are held stationary in tension on a frame or loom while the transverse weft (sometimes woof) is drawn through and inserted over and under the warp.\nWhat are the two most common types of weave?\nWhat are some of the most common weaves?\n- Plain Weave. Plain weave is the simplest weave. …\n- Basket Weave. A basketweave fabric is an alternative form of the plain weave. …\n- Twill Weave. Twill weave is among the most commonly used weaves in textile processing. …\n- Satin Weave.\nWhat is weaving used for?\nWeaving is a process used to create fabric by interlacing threads. Ancient examples date back 12,000 years. Woven fabric fragments composed of natural fibers like linen and wool have been found in places as diverse as Egypt, Peru, China, and Turkey. Weaving uses two types of threads: the warp and the weft.\nWhat are ends and picks in weaving?\nIn general, weaving involves using a loom to interlace two sets of threads at right angles to each other: the warp which runs longitudinally and the weft (older woof) that crosses it. One warp thread is called an end and one weft thread is called a pick.\nWhat is the common lines used in weaving?\nTwill is among the most widely used weaves within textile production. Easily identified by its pattern of diagonal lines, twill weave is used to create strong fabrics such as tweed, gabardine, and of course, denim.\nWhat are the weaving techniques?\nSome of the most popular weaving techniques are the plain, twill, and five-harness satin weave. The plain weave is the simplest weaving technique and is accomplished by passing the weft thread over and under each warp thread at a right angle.\nWhat is a weaving loom called?\nTapestry looms include the simplest of looms, the frame loom. Frame looms do not have any ability to create a shed, and a tapestry you create on a frame loom is constrained to the size of the frame. Some larger types of tapestry looms hold longer warps and offer methods of creating a shed.\nHow can we define the term weaving cycle?\nOne complete cycle of shedding, filling insertion, beat-up, and warp let-off.\nWhat is another term for weft?\nthe yarn woven across the warp yarn in weaving. Synonyms: cream, pickaxe, selection, pick, choice, plectrum, picking, woof, filling, pickax, option, plectron, fill.\nWhat are the basic weaves used in fabric construction?\nThe basic weaves include plain (or tabby), twills, and satins.\nWhat 2 basic methods are used to construct knitted fabrics?\nThe two basic methods of constructing knits are weft (or filling) knits and warp knits. Each method produces a variety of types of knit fabrics.\nWhich machine is used for weaving?\nA loom is a device used to weave cloth and tapestry. The basic purpose of any loom is to hold the warp threads under tension to facilitate the interweaving of the weft threads.']"	['<urn:uuid:7a0a57bc-b371-4067-b02d-947f8f2cb583>', '<urn:uuid:61e83556-ec4b-48ce-b097-5e727483c734>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T05:18:19.158519	32	122	1863
98	How hot does it get during electric arc zinc spraying?	During electric arc spraying, the arc melts the zinc wires at temperatures higher than 7200º F.	['The following article will cover the process of welding and restoring corrosion resistance.\nRestoring Corrosion Resistance to Welds and Zinc Rich Coatings\nThe heat generated from the welding process vaporizes the protective zinc coating near the weld. Even though the remaining zinc continues to provide some protection to the zinc-free areas, the appearance is poor and the zinc-free area will rust when exposed to the environment. The same applies to the interior (ID) of the tube, except exposure to the environment is minimal.\nThere are several zinc rich paints available which will restore corrosion protection to the weld areas on both the ID and OD, with varied application methods.\nNote: It is necessary to take the proper steps in cleaning the surface of weld slag and oil to insure a strong mechanical bond.\nSome of the coatings are for use “as is” and some are for use as a base coat in powder coating applications.\nWhen coating the weld area of a tube to be used in the shiny galvanized state, a standard cold galvanizing compound should be used. There are numerous manufacturers and various brand names commercially available. The key to choosing a corrosion resistant cold galvanizing compound is the percentage of zinc dust present in the compound. Some of the recommended compounds are listed in the table below.\nRecommended Cold Galvanizing and Color Match Coatings\n|Galvilite® Cold Galvanizing Compound||95%||Spray or brush||ZRC Worldwide||Stock|\n|Matchmaker||0%||Spray||Seymour of Sycamore||Stock|\nSalt spray tests have shown that painting or powder coating over zinc increases the rust-free product life. For customers who powder coat, and are concerned about additional corrosion protection in the weld zones, cold galvanizing compounds are available that are heat resistant (up to 750° F), and act as a primer for powder coating. We suggest the two manufacturers in the table below. For those manufacturers who paint the tubing, all of the cold galvanizing compounds in both tables will provide corrosion protection in the weld zone where the zinc has been vaporized.\nHeat-Resistant Cold Galvanizing for Powder Coating Applications\n145 Enterprise Drive\nMarshfield, MA 02050\nM.A. Bruder & Sons\n600 Reed Road\nBroomail, PA 19008\n*Galvilite is available directly from Allied Tube & Conduit®. Galvilite is a registered trademark of ZRC Worldwide.\nRe-coating the ID of a tube to prevent corrosion where the weld zone heat has deteriorated the coating is quite similar to the method used in rust proofing a car. A spray gun with an extension is inserted, typically through a hole, and the coating is sprayed in a pattern covering 360 degrees of the interior.\nAll of the cold galvanizing compounds described in the OD Applications section of this bulletin can be used for re-coating of the ID surface.\nThermal Spray Process\nThermal spraying of zinc will completely restore the corrosion resistance of galvanized tube where welding has burned the zinc off the surface of the tube and where the weld metal is exposed. The resulting sprayed fabrication will have the same or better corrosion resistance in the thermally sprayed weld area as it does in the unaffected tube.\nThermal spraying is a simple process where zinc wire is continuously melted and compressed air atomizes the molten metal and projects it onto a prepared surface.\nConceptually, it is similar to spray painting, except the paint is liquid zinc. Like painting the atomized metal hits the surface of the part and sticks. Unlike welding there is no metallurgical bond occurring between the liquid zinc and metal surface. The zinc bonds mechanically to the metal surface.\nWire Flame Spray\nThe Wire Flame Spray Process applies coatings of any metal that can be drawn into a wire and has a melting point below that of a combustion flame.\nDrive rolls feed the wire through a flame spray gun to its nozzle. There it is continuously melted in an oxygen and fuel-gas flame. Compressed air atomizes the molten metal and projects it onto a prepared surface. This method is the best choice for all-purpose spraying. The zinc coating can be applied fast and at a low cost.\nThe Electric Arc Process\nElectric Arc Spraying applies zinc in wire form. Push/pull motors feed two electrically charged wires through the arc gun to contact tips at the gun head. An arc is created that melts the wires at temperatures higher than 7200º F. Like the wire flame process, compressed air atomizes the molten zinc and projects it onto a prepared surface. This process is excellent for applications where wide, large surfaces must be sprayed. The arc system can produce a spray pattern ranging from 2 to 12 inches and can spray at extremely high speeds.\nThe most important aspect of thermal spraying is surface preparation. Proper surface preparation requires removal of any foreign material and roughening of the surface. The recommended preparation is to grit blast the surface. This is highly effective in removing foreign material and roughening the surface.\nGrit blasting is fast, although it does require containment of the blast stream in either a blast cabinet or by use of a “Vacu-Blast” unit, which captures most of the abrasive and recycles it. In typical galvanized tube fabrication only small sections around weld areas need be cleaned; therefore, the “Vacu-Blast” system is a likely candidate for surface preparation.\nThe corrosion resistance of thermal spray obviously depends on how thick the zinc coating is applied. Theoretically there is no thickness limitation. Practically, there are reasonable limits that vary depending on the application. In a very broad sense, coatings can be applied to a thickness greater than .100 and as thin as .002 to .003 inches. The average thickness for zinc thermal spray is .003 to .006 inches.\nAllied´s coating process uses 99.9% pure zinc. The zinc wire used in the thermal spray process is 99.9% pure. As a result the combination process is much purer than other galvanizing methods.\nThermal spraying does generate particles of zinc oxide which must be captured. Therefore, spraying must be done while pointing the spray gun in the general direction of a dust recovery unit. The dust recovery unit has to be of the dry cartridge type. Several recovery systems are available and selection depends on the size of the recovery area needed.\nIn a typical manufacturing process, after fabrication the welded areas and non-coated components would be grit blasted and thermal sprayed. Due to the secondary processing and equipment it may be necessary to create an additional work area.\nRestoring the corrosion resistance of welds on galvanized tube using thermally sprayed zinc will restore the full corrosion resistance of a welded structure to the same level as Allied´s galvanized tube itself. Thermally sprayed restoration of the zinc coating results in a superior product when compared to the same product when restoration of corrosion resistance is achieved by using “cold galvanizing” inorganic zinc coatings.\nIt should be noted that since there are no organic solvents associated with thermal spraying the coating is suitable for immediate powder coating without fear of bubbling or blistering due to solvent evolution.']	['<urn:uuid:b7b956c7-3ac9-4131-ad53-4cbe652f7b8e>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:18:19.158519	10	16	1167
99	main symptoms achalasia diagnosis methods	The main symptoms of achalasia include dysphagia, regurgitation of food, and retrosternal chest pain. Diagnosis is primarily made through endoscopy, radiographic studies, and manometry. Specifically, esophagogastroduodenoscopy (EGD) is used to rule out other conditions, barium esophagogram may show a classic 'bird's beak' appearance, and high-resolution manometry helps confirm the diagnosis.	['Current status of achalasia management: a review on diagnosis and treatment\n- First Online:\n- Cite this article as:\n- Tuason, J. & Inoue, H. J Gastroenterol (2017) 52: 401. doi:10.1007/s00535-017-1314-5\n- 660 Downloads\nAchalasia is a rare esophageal motility disorder that is characterized by loss of peristalsis and failure of relaxation of the lower esophageal sphincter (LES), particularly during swallowing. This review focuses on the diagnosis of esophageal motility disorders as defined by the Chicago Classification ver 3.0, and presents management options with regard to per-oral endoscopic myotomy (POEM) as the treatment of choice.\nA concise review of literature was performed for articles related to the management of achalasia, and this was contrasted with our institution’s current practice.\nAchalasia is still incompletely understood, and management is focused on establishing a proper diagnosis, and relieving the obstructive symptoms.\nAchalasia should be considered when dysphagia is present, and not otherwise caused by an obstruction or inflammation, and when criteria is met as per the Chicago Classification ver 3.0. Lowering LES tone and disruption of LES can be accomplished by various methods, most notably pneumatic balloon dilatation and surgical myotomy. POEM has been gaining momentum as a first line therapy for achalasia symptoms, and can be considered an important tool for motility disorders of the esophagus.\nKeywordsAchalasia Peroral endoscopic myotomy Esophageal motility disorders Dysphagia\nAchalasia is a neurodegenerative motility disorder of the esophagus that results in deranged peristalsis and loss of lower esophageal sphincter function, especially during swallowing [1, 2, 3, 4]. The incidence is still rare, approximately 1.6 per 100,000 [5, 6]. It occurs equally in males and females, without racial predilection, and across all ages, with a peak incidence between the ages of 30 and 60 .\nThe disease manifests as symptoms of dysphagia, regurgitation of food and retrosternal chest pain . Even with advancements in diagnostic tools and accurate screening methods, the etiology remains unclear. The general consensus of the literature suggests that achalasia represents a family of disorders rather than a single disease with a fixed pathophysiologic profile [9, 10]. The current understanding suggests that three factors determine the clinical phenotype including genetic predisposition, environmental triggers, and autoimmune myenteric plexitis [11, 12, 13, 14]. The exact pathophysiology of the disease is not completely understood, and is beyond the scope of this review.\nFirst-line treatment options have varied in the past, and well-established modalities such as pneumatic balloon dilation and surgical myotomy have been the mainstays over the past decade. Newer treatment options such as the peroral endoscopic myotomy (POEM) procedure have been practiced with increasing frequency in Asia and other parts of the world , but has yet to gain universal acceptance, though the data appears promising. POEM has the potential to be the preferred modality for achalasia and related esophageal motility disorders when personnel and logistics allow .\nThis review focuses on the diagnosis of achalasia based on the Chicago Classification ver. 3.0. A concise review of literature was conducted, focusing on the diagnosis and management of achalasia, and included papers discussing newer treatment modalities. We review the treatment options for achalasia and other motility disorders of the esophagus, and we draw special attention towards the POEM procedure as the primary management, and include our experience with its implementation.\nWhen approaching a patient with dysphagia as the chief complaint, a high index of suspicion for achalasia must be maintained. Evaluation must rule out common disorders, such as gastroesophageal reflux disease, mechanical obstruction, or malignancy . Once such disorders are ruled out, work-up should then focus on motility disorders of the esophagus. A few technological advances have enhanced the diagnostic work-up for achalasia , but the modalities utilized most frequently include endoscopy, radiographic studies, and manometry. Esophagogastroduodenoscopy (EGD) is performed to rule out erosive gastroesophageal reflux disease, esophageal cancer, and structural lesions, such as strictures, webs, or rings. A normal EGD should not dissuade a clinician from making the diagnosis because up to 40% of patients with achalasia will have normal endoscopy . Barium esophagogram may reveal a classic “bird’s beak” appearance, esophageal dilation, or a corkscrew appearance with aperistalsis. In our institution, esophagogastroduodenoscopy, high-resolution manometry, barium esophagogram, as well as CT scan imaging have become the routine for patients presenting with dysphagia and a clinical history suggestive of an esophageal motility disorder.\nInterpretation of HRM studies\nMotility disorders of the esophagus based on Chicago Classification v3.0 \nDisorders with EGJOO\nAchalasia type I: classic achalasia\nAchalasia type II: with pan-pressurization\nAchalasia type III: spastic achalasia\nMajor disorders of peristalsis\nDistal esophageal spasm\nMinor disorders of peristalsis\nIneffective esophageal motility\nAnother diagnostic modality that can determine the dynamic profile of the esophagus is the functional lumen imaging probe (FLIP), used mostly at the esophagogastric junction . This device consists of a distensible bag surrounding a catheter with impedance electrodes that measures cross-sectional area and impedance patterns . The advantage is that the results can be observed in real time, and therapy can be tailored appropriately to the findings . Information regarding FLIP is still limited, and it is unknown whether its application will be valuable as an alternative or an adjunct to manometry.\nThe pathophysiology of the disease is not fully understood, therefore treatment modalities can neither reverse nor prevent the neurodegeneration of the myenteric plexus . The primary goal of management should focus on early diagnosis and prevention of late complications, in order to preserve esophageal structure and function . Available treatments aim to relieve esophageal outflow obstruction by lowering LES tone, or disrupting the LES, to facilitate passage of food across the LES. Available methods have used pharmacologic or interventional means. O’Neill et al. mentioned that patient preference, patient symptoms, and local expertise should guide the choice as to which modality to utilize .\nMedical management has been used to relax smooth muscles, reduce LES pressure, and provide symptom relief, however the effects are limited clinically, and produce side effects. These therapies are now reserved for short-term treatment in patients who cannot tolerate invasive methods, or as a bridge to more definitive treatment . Endoscopic botulinum toxin injection is still an accepted method for symptom relief, however the effects are often short lived [28, 29]. As with oral pharmacologic agents, Botox injection should be reserved for patients who cannot tolerate more invasive methods.\nThe mainstay of recent achalasia management focuses on the disruption of the LES by interventional means. Endoscopic pneumatic balloon dilatation (PD) positions a balloon across the LES and inflates it, effectively rupturing the muscle of the affected segment. Graded PD is performed by an initial dilation of 3.0 cm, progressing slowly at 0.5-cm intervals to reach a target of 4.0 cm. This is performed periodically with 4–6 weeks between each dilation. This modality is stated to have fewer major complications and deaths as compared to surgical myotomy . Surgical LES myotomy also disrupts the LES fibers with a longitudinal incision. The preferred method has been the laparoscopic approach, or laparoscopic Heller myotomy (LHM), due to decreased morbidity and faster recovery [31, 32]. Though this method effectively relieves the obstruction, it also increases the risk of gastroesophageal reflux, and most surgeons incorporate an anti-reflux procedure by way of a partial fundoplication [33, 34]. Studies initially showed superiority of surgical myotomy in terms of efficiency and durability compared to single balloon dilatation [35, 36]. Further comparison of these two modalities has shown similar outcomes in treatment success rates after 2-year follow-up when a graded approach to PD has been used [32, 37, 38]. Pneumatic balloon dilatation and surgical myotomy therefore remain the universally accepted treatments for achalasia presently, and patients should be followed up long term .\nThe advent of the peroral endoscopic myotomy (POEM) procedure by Inoue et al. in 2008 provided another means of treatment that has been presented worldwide . Initially described by Pasricha et al. , this form of natural orifice transluminal endoscopic surgery (NOTES) has provided adequate symptom relief, shown similar short-term results, and is less invasive as compared to laparoscopic Heller myotomy . Short-term results have proven that POEM is a safe and effective approach for esophageal achalasia [43, 44, 45]. The patient is placed in a supine position under general anesthesia . An endoscopic submucosal injection of dyed saline is followed by a mucosal incision, and subsequent creation of a submucosal tunnel to expose the circular muscle fibers at a length 2–3 cm beyond the LES. Selective circular muscle myotomy is then carried out under direct visualization, and the mucosa is closed with clips . Advancements in technology as well as refinements in technique have led to safer procedures, as well as decreased operative time. The water-jet assisted triangle knife (Olympus, Tokyo, Japan) used in the creation of the submucosal plane, as well as the selective myotomy, provides a reduction of procedure time . The use of the double-scope POEM technique provides a safe and reliable method for ensuring adequate gastric myotomy length during POEM, and may help to ensure proper outcomes . These advancements along with the increase in operator expertise are what drive the use of POEM to be more widespread.\nPOEM has been used for all types of esophageal motility disorders, including, but not restricted to, advanced sigmoid achalasia and spastic motility disorders of the esophagus [50, 51]. POEM has even been performed successfully on refractory cases of achalasia following previous balloon dilation or laparoscopic Heller myotomy [52, 53, 54, 55]. These cases should only be treated by highly experienced operators . The advantage of POEM over surgical myotomy is that the length of the myotomy from the esophageal to gastric side can be adjusted freely to suit the individual case. This may make POEM a more appropriate, safe, and effective option for patients with sigmoid esophagus or history of surgical myotomy . POEM has proven to be similar to LHM in terms of Eckardt score reduction, complications, operative time, and length of hospital stay [57, 58, 59]. POEM has also proven to be equally effective to pneumatic dilatation . It is important to note, however, that data regarding the long-term effects of POEM are still limited, and randomized controlled trials comparing POEM with other treatment modalities are needed. This has been the barrier to the worldwide acceptance of the procedure, and its use has been confined to high-volume centers, though it is still believed that POEM represents a paradigm shift in the treatment of achalasia . Post-procedure complications still arise, as with the other treatment modalities. On objective testing, the rate of gastroesophageal reflux disease (GERD) after POEM is 20–46%, which may not be significantly different from that observed after LHM with Dor fundoplication [62, 63].\nThe different motility disorders of the esophagus are based on manometric study, barium esophagogram, and endoscopic findings and are further subdivided into categories depending on the area of involvement of each disease entity. It then follows that a standard-length myotomy through the LES is adequate treatment for classic achalasia (type I), achalasia with pan pressurization (type II), and esophagogastric junction outlet obstruction. An extended myotomy of the esophageal body and the LES should be considered for vigorous achalasia (type III) with premature contractions. Diffuse spasm and jackhammer esophagus cases can be managed with a focused myotomy of the esophageal body, and the LES may be preserved. In some cases, the specific motility disorder may not be clearly identified at the time of the procedure. It is important to note the presence of spastic simultaneous contractions, or non-peristaltic contractions, as these intraoperative findings may also affect the length of the myotomy. Previous interventional procedures, as well as anatomy of the esophagus itself will also affect the decision on myotomy length as well as placement. Further long-term studies are needed to properly evaluate the outcomes of cases that follow this algorithm and more studies are needed in order to determine what constitutes an adequate myotomy, whether that be measurable by pressurization studies (EndoFLIP) or subjective symptom relief based on the Eckardt score. The purpose of this algorithm is not to create a guideline for clinical practice, but to assist the decision-making process, and clarify the primary focus of the intervention.\nAchalasia remains a difficult to diagnose condition and a high index of suspicion must be maintained when patients present with dysphagia. Other causes of mechanical obstruction must be ruled out, and proper diagnostics must confirm a motility disorder of the esophagus. Treatment focused on disrupting the LES and lowering LES tone has been the mainstay of management, and has traditionally utilized pneumatic balloon dilatation and surgical myotomy. Peroral endoscopic myotomy is an effective treatment option for all types of motility disorders of the esophagus. With time and physician experience, POEM may become the ideal management for achalasia and its use may change the way we approach this rare disorder.\nCompliance with ethical standards\nConflict of interest\nThe authors declare there are no conflicts of interest.']	['<urn:uuid:e8f421a1-a191-4c19-b842-2e221a81877d>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	5	50	2144
100	brahms early career performing teenage years	As a teenager, Brahms helped support his family financially by performing in various venues including inns, brothels, taverns and along city docks. While he is believed to have started composing early in life, he destroyed his early compositions and didn't become famous as a composer until 1853 when he toured as an accompanist.	"[""1. Peter Ilyich Tchaikovsky (the traditional Western spelling) was born in 1840 in Votkinsk, Russia. He began taking piano lessons in 1845; however, formal music education was not available in Russian schools at this time so his parents never considered that he might pursue a career in music. Instead, they prepared him for a life of civil service; he began his formal education at the Imperial School of Jurisprudence in 1850, which he attended for nine years.\nArchive for the 'General' Category\nTags: peter ilyich tchaikovsky, Russian composers, Russian music, tchaikovsky, tchaikovsky sheet music, The Five, The Mighty Handful, The Nutcracker\nTags: Beethoven, Brahms, Brahms sheet music, Clara Schumann, Johannes Brahms, Liszt, New German School, Robert Schumann, Wagner, War of the Romantics\nby Jacy Burroughs\n1. Johannes Brahms was born on May 7, 1833. His father was a town musician who played a variety of instruments, mostly horn and double bass.\n2. Brahms began playing piano at the age of 7. By the time he was a teenager, he was helping the family financially by performing in inns, brothels, taverns and along the city docks. Brahms is also believed to have begun composing early in his life, but destroyed his early compositions. He did not become famous as a composer until April and May of 1853, when he was on a concert tour as accompanist to the Hungarian violinist Eduard Reményi.\n3. In 1853, Brahms met Robert Schumann. Schumann was so impressed with Brahms’ compositions that he wrote an article in his Neue Zeitschrift für Musik, praising the young composer’s genius and heralding him as the one who could overthrow the New German School of Liszt and Wagner.\nTags: musical theater, musical theater productions, musicals, networking, pit musicians, pit orchestra, pit orchestra musicians\nby Ryan Blauvelt\nPlaces. Lights up. Downbeat. It’s the order of things for playing in a pit orchestra for musical theater productions. The culmination of many hours of rehearsals, score study, listening sessions, and custom edits in order to bring together cast, crew, and of course musicians for a couple weeks of “showtime!”\nIt’s spring and that means musical theatre productions. Playing musicals is something I’m pretty good at, especially when it comes to last minute notice. I’ve been hooked on playing musicals ever since getting picked to play drum set in my high school’s first musical production, “Guys and Dolls”. It was a lot of pressure being in the driver’s seat for a music ensemble. Also, being depended on by actors and actresses for steady tempos, sharp hits, and joke reinforcement adds another layer of complexity. But I loved the thrill of learning so much music very quickly and making quick changes to fine-tune my contribution to a well-performed show. Continue reading ‘Playing in a Pit Orchestra: Anecdotes and Advice from a Percussionist’\nTags: Dave Brubeck, Dave Brubeck Quartet, jazz, Jazz Appreciation Month\nby Jacy Burroughs\nIn honor of Jazz Appreciation Month, I decided to write a post about one of the jazz greats. I am not a jazz musician and unfortunately, my classical music education barely scraped the surface of jazz. However, the question of “Who to write about?” was an easy choice, because I share an alma mater with the legendary Dave Brubeck. We both went to University of the Pacific, formerly College of the Pacific. I even had the chance to meet him my freshman year, well, more like run into him…literally. I was coming out of a class and there he was, right in front of me. Dave Brubeck. I must have looked shocked and embarrassed and he just smiled and asked, “How are you?” I will never forget that smile. He must have been 86 then. Continue reading ’10 Interesting Facts about Dave Brubeck’\nTags: johann sebastian bach, johann sebastian bach music\nby Jacy Burroughs\n1. Johann Sebastian Bach was born on March 21, 1685 in Eisenach, Germany in the province of Thuringia. His father, Johann Ambrosius, was a town musician. During this period, music was a trade just like metalwork or shoe making. And for the Bachs, music was the family business, stretching back several generations. Continue reading ’10 Interesting Facts About Johann Sebastian Bach’\nOriginally posted on Take Note:\nBy Kevin Harper\nHistory of Fake Books and Lead Sheets\nImagine this: you’re a famous jazz player; you’re busy on the road going from gig to gig. One day you come up with a great tune and want to write it down and orchestrate it for your ensemble, but orchestration takes a long time. So instead, you write down the melody and then write out the general chords and any potential rhythms. When you read it during the gig (for the first time no doubt!) you and your bandmates have a general outline of what needs to happen – everything else is improvised. Because improvisations are different everytime, writing down the “correct” way of playing any tune in the old days was impossible.\nAs jazz grew in popularity, everyone wanted to hear all the popular songs, but the problem was that many of these tunes were hard to find or…\nView original 1,609 more words\nTags: composer, Digital Print Publishing, publishing models, self-publishing\nOn January 15, 2015 during the New Music Gathering at the San Francisco Conservatory of Music, Sheet Music Plus CEO Jenny Silva gave a presentation on publishing models for the independent composer. Her presentation, available as a PDF by clicking on the link below, discusses the various methods through which composers may publish their music and the advantages and disadvantages to each. It is a must read for any composer, new or seasoned, wishing to gain more exposure for his/her work.\nTags: Amadeus (film), Classical music, Freemason, Leopold Mozart, Mozart, Nannerl Mozart, Salieri, Wolfgang Amadeus Mozart\nby Jacy Burroughs\nWolfgang Amadeus Mozart\n(27 January 1756 – 5 December 1791)\n1. Mozart was baptized as Johannes Chrysostomus Wolfgangus Theophilus Mozart. (Imagine trying to learn to write that name!) His first two names, Johannes Chrysostomus, represent his saint’s name, following the tradition of the Catholic Church. This saint’s name was in all likelihood chosen because Mozart’s birthday, January 27th, was the feast day of Saint John Chrysostom. Wolfgangus, or Wolfgang in German, means Continue reading ’10 Interesting Facts About Wolfgang Amadeus Mozart’\nTags: cantata, Choir, choir music, choral music, choral sheet music, church music, Easter music, Joel Raney, Joseph M. Martin, Lloyd Larson, Pepper Choplin, sacred choral music, Thomas Fettke, worship music\n1. The Easter Story by Thomas Fettke & Thomas Grassi\nPerformance Time: 35 minutes\nThe Easter Story combines a profound narrative by Ken Bible with musical selections from Tom Fettke and Thomas Grassi. Fettke and Grassi have drawn upon several folk music sources, including hymns from The Sacred Harp and The Columbian Harmony, and spirituals, carols and songs from English, Irish and Hebrew traditions. The narrative is steeped in scripture, helping to relive the events in Jerusalem leading up to Jesus’ death and resurrection.\nThis cantata recommends using two narrators, but can be performed with one. It is beautiful whether accompanied by piano or orchestra. Continue reading ‘Top 5 New Cantatas for Lent and Easter’\nTags: Christmas gifts, Daniel Ho Ukulele, Disney, Finale SongWriter, gifts, gifts for musicians, holiday gifts, Jamey Aebersold, metronome, Teach Yourself to Play Ukulele, tuner, ukulele\nThis great gift idea is perfect for anyone who wants to learn how to play an instrument but doesn’t know how to get started. It comes with everything needed to start jamming away immediately, including the most important part – the instrument! Ukuleles are fun and easy to play, and this pack comes with a high-quality Firebrand wood ukulele complete with carrying case and extra set of strings. Also included is Alfred’s Teach Yourself to Play Ukulele Course (book, CD & DVD), the bestselling instructional series for beginners of all ages. The course covers everything from basic fundamentals (such as how to hold the instrument) to songs in various musical styles – you can be your own teacher and learn at your own pace.\nE-Commerce Content Manager and Violinist\nContinue reading ‘Top Ten Holiday Gift Ideas for Musicians’""]"	['<urn:uuid:5a70ef9b-7f2f-4d51-a650-9b27650e1dfe>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T05:18:19.158519	6	53	1351
